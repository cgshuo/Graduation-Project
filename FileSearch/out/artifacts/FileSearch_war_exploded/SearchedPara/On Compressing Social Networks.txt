 Motivated by structural properties of the Web graph that support efficient data structures for in memory adjacency queries, we study the extent to which a large network can be compressed. Boldi and Vigna (WWW 2004), showed that Web graphs can be compressed down to three bits of storage per edge; we study the compressibility of social networks where again adjacency queries are a fundamen-tal primitive. To this end, we propose simple combinatorial for-mulations that encapsulate efficient compressibility of graphs. We show that some of the problems are NP-hard yet admit effective heuristics, some of which can exploit properties of social networks such as link reciprocity. Our extensive experiments show that social networks and the Web graph exhibit vastly different compressibil-ity characteristics.
 H.2.8 [ Information Systems ]: Database Applications X  Data Min-ing ; G.2.2 [ Mathematics of Computing ]: Graph Theory X  Graph algorithms Algorithms, Experimentation, Measurement, Theory Compression, Social networks, Linear arrangement, Reciprocity We study the extent to which social networks can be compressed. There are two distinct motivations for such studies. First, Web
Work done in part while visiting Yahoo! Research.
Supported in part by a grant from Yahoo! Research. Supported in part by NSF grant NSF CNS-0721491.
 Copyright 2009 ACM 978-1-60558-495-9/09/06 ... $ 5.00. properties require high-speed indexes for serving adjacencies in the social network: thus, a typical query seeks the neighbors of a node (member) of a social network. Maintaining these indexes in mem-ory demands that the underlying graph be stored in a compressed form that facilitates efficient adjacency queries. Secondly, there is a wealth of evidence (e.g., [17]) that social networks are not ran-dom graphs in the usual sense: they exhibit certain distinctive local characteristics (such as degree sequences). Studying the compress-ibility of a social network is akin to studying the degree of  X  X an-domness X  in the social network. The Web graph (Web pages are nodes, hyperlinks are directed edges) is a special variant of a social network, in that we have a network of pages rather than of peo-ple. It is known that the Web graph is highly compressible [6, 11]. Particularly impressive results have been obtained by Boldi and Vi-gna [6], who exploit lexicographic locality in the Web graph: when pages are ordered lexicographically by URL, proximal pages have similar neighborhoods. More precisely, two properties of the or-dering by URL are experimentally observed to hold:
These two empirical observations are exploited in the BV-algorithm to compress the Web graph down to an amortized storage of a few bits per link, leading to efficient in-memory data structures for Web page adjacency queries (a basic primitive in link analysis). Do these properties of locality and similarity extend to social networks in general? Whereas the Web graph has a natural lexicographic order (by URL) under which this locality holds, there is no such obvi-ous ordering for social networks. Can we find such an ordering for social networks, leading to compression through lexicographic locality?
Our main contributions in the paper are the following. We pro-pose a new compression method that exploits link reciprocity in social networks (Section 3). Motivated by this and BV, we formu-late a genre of graph node ordering problems that distill the essence of locality in BV-style algorithms (Section 4.1). We develop a sim-ple and practical heuristic based on shingles for obtaining an effec-tive node ordering; this ordering can be used in BV-style compres-sion algorithms (Section 4.3). We then perform an extensive set of experiments on large real-world graphs, including two social net-works (Section 5). Our main findings are: social networks appear far less compressible than Web graphs yet closer to host graphs and exploiting link reciprocity in social networks can vastly help its compression.
Prior related work falls into three major categories, namely, com-pressing Web graphs, compressing indexes, and graph ordering prob-lems.

Adler and Mitzenmacher introduced the idea of finding pages with similar sets of neighbors in the context of compressing Web graphs, and obtained some hardness results for compression in this context [1]. Randall et al. [22] suggested lexicographic ordering as a way to obtain good Web graph compression, utilizing both sim-ilarity and locality. Raghavan and Garcia-Molina [21] considered a hierarchical view of the Web graph to achieve compression; see also Suel and Yuan [27] for a structural approach to compressing Web graphs. A major step was taken by Boldi and Vigna [6], who both developed a generic Web graph compression framework that takes into account the locality and similarity of Web pages and ob-tained very strong compression performance; our work is based on this framework. Boldi and Vigna [7] also developed  X  -codes, to exploit power law distributed integer gaps. Buehrer and Chellapilla [11] used the frequent pattern mining approach to compress Web graphs; using this, they were able to achieve a compression of un-der two bits per link. Recently, Boldi, Santini, and Vigna [5] stud-ied the effectiveness of various orderings, including Gray ordering, in compressing the transpose of the Web graph.

The problem of assigning or reassigning document identifiers in order to compress text indexes has a long history. Blandford and Blelloch [4] considered the problem of compressing text in-dexes by permuting the document identifiers to create locality in an inverted index. Silvestri, Perego, and Orlando [26] proposed a clustering approach for reassigning document identifiers. Shieh et al. [24] proposed a document identifier reassignment method based on a heuristic for the traveling salesman problem. Recently, Sil-vestri [25] showed that assigning document identifiers to Web doc-uments based on URL lexicographic ordering improves compres-sion.

There are many classical node ordering problems on graphs. The minimum bandwidth problem, where the goal is to order the nodes to minimize the maximum stretch of edges, and the minimum lin-ear arrangement problem, where the goal is to order the nodes to minimize the sum of stretch of edges, have a rich history. We re-fer to [13] and the online compendium at www.nada.kth.se/  X  viggo/wwwcompendium/node52.html .
In this section we outline the compression framework used in the rest of the paper. The framework is based on the algorithm of Boldi and Vigna for compressing Web graphs [6]; their algo-rithm achieved a compression down to about three bits per link on a snapshot of the Web graph. We henceforth refer to this as the BV compression scheme, which we first describe. Next, we describe what we call the backlinks compression (BL) scheme, which tar-gets directed graphs that are highly reciprocal.
 Notation . Let G = ( V, E ) be a directed graph and let | V | = n . The nodes in V are bijectively identified with the set [ n ] = { 1 , . . . , n } of integers. For a node u  X  V , let out( u )  X  V de-note the set of out-neighbors of u , i.e., out( u ) = { v | ( u, v )  X  E } . Likewise, let in( u ) denote the set of in-neighbors of u . Let outdeg( u ) = | out( u ) | and indeg( u ) = | in( u ) | . If both ( u, v )  X  E and ( v, u )  X  E and u &lt; v , then we call the edge ( v, u ) to be re-ciprocal . For a node u  X  V , let rec( u ) be { v | ( v, u ) is reciprocal } . Let lg denote log 2 .

We will encode all integers using one of three different encod-ing schemes, namely, Elias X  X   X  -code,  X  -code, and Boldi X  X igna  X  -code with parameter 4 (which we found to be the best in our ex-periments) [7]. These integer encoding schemes encode an inte-ger x  X  Z + using close to the informatic-theoretic minimum of 1 + b lg( x ) c bits. For example, the number of bits used by the  X  -code to represent x is 1 + 2 b lg x c . We refer to [28] for more background on these codes.
BV incorporates three main ideas. First, if the graph has many nodes whose neighborhoods are similar, then the neighborhood of a node can be expressed in terms of other nodes with similar neigh-borhoods. Second, if the destinations of edges exhibit locality, then small integers can be used to encode them (relative to their sources). Third, rather than store the destination of each edge sep-arately, one can use gap encodings to store a sequence of edge des-tinations. Given a sorted list of positive integers (say, the destina-tions of edges from a node), we write down the sequence of gaps between subsequent integers on the list, rather than the integers themselves. The idea is that even if the integers are big (requiring many bits to record), the gaps between integers on the list could be recorded with fewer bits.

We now detail the BV scheme for compressing Web graphs. The nodes are Web pages and the directed edges are the hyperlinks. First, order Web pages lexicographically by URL. This assigns to each Web page a unique integer identifier (ID), which is its position in this ordering. Let w be a window parameter; for the Web, BV recommend w = 8 .

Let v be a Web page. Its encoding will be as follows. 1. Copying. Check if the list out( v ) of v  X  X  out-neighbors is a small variation on the list of one of the w  X  1 preceding Web pages in the lexicographic ordering. Let u be such a prototype page, if it exists. 2. Encoding. Encode v  X  X  out-neighbors as follows. If the copy-ing step found a prototype u , then use lg w bits to encode the (back-ward) offset from v to u , followed by the changes from u  X  X  list to v  X  X . If none of the lg w preceding pages in the lexicographic or-dering offers a good prototype, set the first lg w bits to all 0 X  X , then explicitly write down v  X  X  out-neighbors. (BV also optimize further by storing a list i, i + 1 , . . . , j  X  1 , j of consecutive out-neighbors by storing the interval [ i, j ] instead.) Note that locality and similarity are captured by the copying step. By using clever gap encoding schemes (using the integer codes mentioned earlier) on top of the basic method above, BV obtain their best results. Note that the availability of a natural ordering on the Web pages facilitates exploitation of locality. For more details, we refer to the original paper [6] and [19, Chapter 20]. This general method of compression has two nice properties. First, it is dependent only on locality in some canonical ordering. Second, adjacency queries (fetch all the out-neighbors of a given node) can be served fairly efficiently. Given a Web page whose out-neighbors are sought, we enumerate these out-neighbors by decod-ing backwards through the chain of prototypes, until we arrive at a list whose encoding begins with at least lg w 0 X  X . While in princi-ple this chain could be arbitrarily long, in practice most chains are short. For instance, few chains would go backwards across URL domains.
We now describe a slightly different compression scheme that is motivated by the observed properties of social networks. This scheme, called BL , incorporates an additional idea on top of BV, namely, link reciprocity . In the BL scheme, reciprocal links are encoded in a special way. Since social networks are known to be mostly reciprocal (if Alice is Bob X  X  friend, then Bob is very likely to be Alice X  X  friend), this will turn out to be advantageous.
Suppose we obtain an ordering of the nodes in the graph through some process to be discussed later. We will identify each node in the graph with its position in this ordering. Let v be a node. Its encoding will consist of the following. 1. Base information. The outdegree | out( v ) | , minus 1 if v has a self-loop, and minus the number of reciprocal edges from v . Also, include a bit specifying if v has a self-loop. 2. Copying. The node u that v uses as a prototype to copy from: as u  X  v in the ordering, u is encoded as the difference between u and v . If u = v , then no copying is performed. Otherwise, a bit is added for each out-neighbor of u , representing whether or not that out-neighbor of u is also an out-neighbor of v . 3. Residual edges. Let v 1 , . . . , v k be the out-neighbors of v that are yet to be encoded after the above step. Let v 1  X  X  X  X  X  X  v write one bit stating if v &gt; v 1 or v &lt; v 1 . Then we encode the gaps | v 1  X  v | , | v 2  X  v 1 | , . . . , | v k  X  v k  X  1 | . 4. Reciprocal edges. Finally, we encode the reciprocal out-neighbors of v . For each v 0  X  out( v ) such that v 0 &gt; v , we en-code whether v 0  X  rec( v ) or not using one bit per link and discard ( v , v ) .

Note that reciprocal edges are succinctly encoded by the last step. Thus, this method potentially outperform BV in terms of com-pression. However, it has a drawback: unlike in BV, adjacency queries may be slower. This is because BV limits the  X  X ength X  of prototype chains but we do not impose such a limit in BL. If the compressed representation of a network bottlenecks adjacency query serving, then a limit on the length of copying chain can be introduced in BL as well.
In both the BV and BL schemes, the ordering of nodes plays a crucial role in the performance of the compression scheme. The performance of suggests that the lexicographic ordering of URL X  X  for the Web graph is both natural and crucial, begging the question: can we find such orderings for other graphs, in particular, social networks ? If we could, we would be able to apply either the BV or the BL scheme. In this section we study ordering problems that are directly motivated by the BV and BL compression schemes.
We first formalize the problem of finding the best ordering of nodes in a graph for the BV and BL schemes. As we saw earlier, both algorithms benefit if locality and similarity are captured by this ordering. This leads to the following natural combinatorial op-timization problem, which we call minimum logarithmic arrange-ment .

P ROBLEM 1 (ML OG A). Find a permutation  X  : V  X  [ n ] such that P ( u,v )  X  E lg |  X  ( u )  X   X  ( v ) | is minimized. The motivation behind this definition is to minimize the sum of the logarithms of the edge lengths according to the ordering (where the length of the edge u  X  v is |  X  ( u )  X   X  ( v ) | ). Notice this cost repre-sents the compression size of the length of the edge in an encoding that is information-theoretically optimal (or nearly so).
Also note that if the term inside the summation were just |  X  ( u )  X   X  ( v ) | , then this is the well-known minimum linear arrangement (ML IN A) problem. ML IN A is NP-hard [14]; little, however, is known about its approximability. The best algorithm [23] approx-imates ML IN A to O ( practical for large graphs. From the standpoint of the hardness of approximation, only the existence of a PTAS has been ruled out [3]. One cannot hope to use an approximate solution to ML to solve ML OG A since we can show (see Appendix A) that these problems are very different in their structure.

In actually compressing the graph, it is more efficient to com-press the gaps induced by the neighbors of a node. Suppose u &lt; v 1 &lt; v 2 and ( u, v 1 ) , ( u, v 2 )  X  E . Then, compressing the gaps v  X  u and v 2  X  v 1 is always and could be far less expensive than compressing the lengths of the edges, namely, v 1  X  u and v  X  u . For this reason, we introduce a slightly modified problem, called minimum logarithmic gap arrangement . Let f  X  ( u, out( u )) be the cost of compressing out( u ) under the ordering  X  , i.e., if u 0 = u, out( u ) = { u 1 , . . . , u k } with  X  ( u 1 )  X  X  X  X  X  X   X  ( u
P ROBLEM 2 (ML OG G AP A). Find a permutation  X  : V  X  [ n ] such that P u  X  V f  X  ( u, out( u )) is minimized.
Once again, as a problem, ML OG G AP A turns out to be very dif-ferent from ML IN A and ML OG A (see Appendix A).

Both formulations ML OG A and ML OG G AP A capture the essence of obtaining an ordering that will benefit BV and BL compres-sions. We believe a good approximation algorithm for either of these problems will be of practical interest.
We show that ML OG A is hard in general. The proof is in Ap-pendix B.
 T HEOREM 3. ML OG A is NP-hard on multi-graphs.
 While we are currently unable to show that ML OG G AP A is NP-hard, we can show that its  X  X inear X  version (i.e., without the log-arithms), ML IN G AP A, is indeed hard. The proof is in Appendix C.
 T HEOREM 4. ML IN G AP A is NP-hard.
 We can also show a lower bound on the solution to ML OG A for expander-like graphs, suggesting that they are not compressible with constant number of bits per edge via BV/BL schemes. The proof is in Appendix D.
 L EMMA 5. If G has constant conductance, then the cost of ML on G = ( V, E ) is  X ( | E | log n ) . If, instead, G has constant node or edge expansion, then the cost of ML OG A on G is  X ( n log n ) .
In this section we propose a simple and practical heuristic for both ML OG A and ML OG G AP A problems. Our heuristic is based on obtaining a fingerprint of the out-neighbors of a node and or-dering the nodes according to this fingerprint. If the fingerprint can succinctly capture the locality and similarity of nodes, then it can be effective in BV/BL compression schemes.

To motivate our heuristic, we recall the Jaccard coefficient J ( A, B ) = | A  X  B | / | A  X  B | , a natural notion of similarity of two sets. Let  X  be a random permutation of the elements in A  X  B . For a set A , let M  X  ( A ) =  X   X  1 (min a  X  A {  X  ( a ) } ) , the smallest element in A ac-cording to  X  ; we call it the shingle . It can be shown [10] that the probability that the shingles of A and B are identical is precisely the Jaccard coefficient J ( A, B ) , i.e., Instead of using random permutations, it was shown that the so-called min-wise independent family suffices [10]; in practice, even pairwise independent hash functions work well. It is also easy to boost the accuracy of this probabilistic estimator by combining multiple shingles obtained from independent hash functions.
The intuition behind our heuristic is to treat the out-neighbors out( u ) of a node u as a set and compute the shingle M  X  of this set for a suitably chosen permutation (or hash function)  X  . The nodes in V can then be ordered by the shingles. By the prop-erty stated above, if two nodes have significantly overlapping out-neighbors, i.e., share a lot of common neighbors, then with high probability they will have the same shingle and hence be close to each other in a shingle-based ordering. Thus, the properties of lo-cality and similarity are captured by the shingle ordering heuristic. (Gibson, Kumar, and Tomkins [15] used a similar heuristic, but for identifying dense subgraphs of large graphs.)
In this section we show some theoretical justification for the shingle ordering heuristic: using shingle ordering, it is possible to copy a constant fraction of the edges in a large class of ran-dom graphs with certain properties. The well-known preferential attachment (PA) model [2, 8], for instance, generates graphs in this class. Our analysis thus shows that it is indeed possible to obtain provable performance guarantees on shingle ordering with respect to copying (hence compression) in stylized models.

We first prove the following general statement about the suffi-cient conditions under which using shingle ordering can copy a constant fraction of edges. The proof is given in Appendix E.
T HEOREM 6. Let G = ( V, E ) be such that | E | =  X ( n ) and  X  S  X  V such that (i) | S | =  X ( n ) , (ii)  X  v  X  S,  X  v 0  X  S, v 6 = v 0 , s.t. | out( v )  X  out( v (iii) there exists a constant k , s.t.  X  v  X  S, outdeg( v )  X  k , (iv)  X  v  X  S,  X  w  X  out( v ) , indeg( w )  X  n 1 2  X  .

Then, with probability 1  X  o (1) (over the space of permutations), at least a constant fraction of the edges will be  X  X opied X  (even with a window of size 1) when using the shingle ordering.

It is trivial to note that this holds even for undirected graphs; indeed, each undirected edge { u, v } can be substituted by two di-rected edges ( u, v ) , ( v, u ) . Then, for each node, its original set of neighbors will be the same as its new sets of in-and out-neighbors.
We now show the main result of the section: using shingle order-ing it is possible to copy a constant fraction of the edges of graphs generated by the PA model.

T HEOREM 7. With high probability, the graphs generated by the PA model satisfies the properties of Theorem 6.

P ROOF . We start by removing the nodes incident to multi-edges or self-loops  X  there are o ( n ) such nodes and their incident edges
This can be easily shown by noting that the expected number of multiple edges and self-loops added by the n th inserted node is O ( m 3 /n 1 / 2  X  ) , conditioned on the fact that the highest degree at that point is O ( n 1 / 2+ ) whp [12]. Then, by Markov X  X  inequality the claim follows.
 Also, we remove all nodes of degree &gt; k , for some constant k  X  by [9] only k n edges and nodes will be removed this way.
The resulting graph will thus have at most n nodes and at least (1  X  2 k ) mn  X  (1  X  2 k ) n edges. Also its maximum degree will be k . By averaging, a graph having these three properties will contain at least (1  X  2 k ) n 2 k nodes of degree at least 2 .
Now take all the nodes v in this graph incident to a neighbor of degree  X  2 . There are  X  (1  X  2 k ) n 2 k such neighbors and each of them will be connected to at most k such v  X  X . Thus, the number of these v  X  X  is at least  X ( n/ (2 k 2 )) =  X ( n ) . The set of these v  X  X  is the set S of Theorem 6.
 As our experiments show, shingle ordering allows both BL and BV schemes to take significant advantage of copying.
In this section we describe the experimental results. The goal of our experiments is two-fold: (1) study the performance of BV/BL schemes using the shingle ordering on social networks; (2) obtain insights into the differences between the Web and social networks in terms of their compressibility. First we begin with the descrip-tion of the data sets we use for our experiments. Next we discuss the baselines we use (to compare against shingle ordering). Finally we present and discuss our experimental results.
For our experiments, we chose four large directed graphs: (i) a 2008 snapshot of LiveJournal (a social network site, livejournal. com ) and an induced subgraph of users, called LiveJournal (zip), for whom we know their zip codes; (ii) monthly snapshots of Flickr (a photosharing site, flickr.com ) from March 2004 until April 2008; (iii) the host graph 2 of a 2005 snapshot of the .uk Web graph; and (iv) the host graph of a 2004 snapshot of the IndoChina ( .in,.cn ) Web graph.
 LiveJournal (zip) 1,314,288 8,040,562 79.0 IndoChina-host 19,123 233,380 10.6
In Table 1, we summarize the properties of the graphs we have considered. Notice the magnitude of the reciprocity of the social networks (LiveJournal and Flickr); the BL scheme will critically leverage this property.
We use the following orderings as our baselines to compare against the shingle ordering. (1) Random order. We use a random permutation of all the nodes in the graph. (2) Natural order. This is the most basic order that can be de-fined for a graph. For Web and host graphs, a natural order is the URL lexicographic ordering (used by BV). For a snapshot of Live-Journal, a natural order is the order in which the user profiles were
Host graph refers to a directed graph whose nodes are the hosts and an edge exists from host a to host b if there is a Web page on host a that points to a Web page on host b . crawled. For Flickr, since we know the exact time at which each node and edge was created, a natural order is the order in which users joined the network. (3) Gray order. Consider the binary adjacency matrix induced by, say, the natural order. Now, a node v precedes node w in the Gray ordering if the row of v precedes the row of w (both interpreted as binary strings) in the canonical Gray code [5, 22]. (4) Geographic order. In a social network, if geographic infor-mation is available in the form of a zip code, then this defines a geography-based order. Liben-Nowell et al. [18] showed that about the 70% of social network links arise from geographical prox-imity, suggesting that friends can be grouped together using geo-graphical information. Notice that this only defines a partial order (i.e., with ties). (5) DFS and BFS order. Here, the orderings are given by these common graph traversal algorithms. We also try the undirected versions of these traversals, where the edge direction is discarded.
To test the robustness of shingle ordering, we also use an order-ing obtained by two shingles instead of just one, where the second shingle is used to break ties produced by the first. We call this the double shingle ordering. When only one shingle was used, ties were broken using the natural order or if specified, the Gray order.
Our performance numbers are always measured in bits/link.
In Table 2, we present the results of the different compression/orderings on our graphs. This table shows that double shingle ordering pro-duces the best or the near-best compression, for both BV and BL. In some cases, it cuts almost half the number of bits used by the natural order. Also we note that the improvement of BL over BV is significant for networks that are highly reciprocal, i.e., social net-works. Finally, the numbers show interesting similarities between social networks and host graphs. In both cases, their compressibil-ity using the best compression (BL with double shingle order) is on par with each another.

It is interesting to note that the best compression rates for the host graphs are similar to that of the social networks, even though the former are much smaller in size than the latter. For comparison, we note how the snapshot of the UK domain (IndoChina domains) that we used to obtain the host graph, was found to be compressible to 1.701 (1.472) bits/link (see [6] and http://law.dsi.unimi. it/ ). This seems to indicate that the host graphs are very hard to compress.
 IndoChina-host 7.328 7.260 7.082 (  X  ) 7.080 (  X  ) Table 3: Performances of the compression techniques using Gray orderings to break ties.

Next we present the effect of breaking ties in shingle and dou-ble shingle orderings using the Gray ordering (Table 3). Modest improvements are obtained by this method for some graphs. Once again, LiveJournal does not appear to be amenable to the shingle approach.

We also note how (Table 4) the BFS/DFS orderings are always suboptimal, almost as bad as a random order. In Table 5, we show the performance of geographical ordering on the induced subgraph of LiveJournal, restricted to users in US with a known zip code. We see how ordering by zip code (i.e., in such a way that people at small geographic distance are close to each other in the ordering) is much worse than ordering by shingle, suggesting that geographic ordering is perhaps not useful for compression.
In Figure 1, we see how the different ordering and compression techniques achieve different results on the monthly snapshots of the Flickr social network. The upper half of the figure shows how the Flickr network grew over time. Here, we see that BL with shingle ordering beats the competition uniformly over all the snapshots. We also see an interesting pattern: BL obtains a better compression rate, with each of the orderings. It is remarkable to note that even though the number of edges in Flickr grew by an enormous number between March 2005 and April 2008, the compressibility of the network (under a variety of schemes and orderings) has remained robust.
Figures 2 and 3 show one reason why the shingle ordering helps compression: in the LiveJournal, IndoChina-host and UK-host graphs the number of small gaps is higher with shingle ordering than with any other ordering. The notable exception is the LiveJournal graph, where the natural ordering is marginally better.

In Figure 2, the upper panel represents the number of gaps ( y -axis) of a certain length ( x -axis) for the LiveJournal graph. The lower panel represents a sub-sampled version of the same data: for each length i we deleted the length = i point with probabil-ity  X (1 /i ) . This way, in expectation, the number of points in each interval 10 k , . . . , 10 k +1 is the same. The bottom panel is more readable. Recall that in LiveJournal, the natural (crawl) ordering beats the shingle ordering by a small amount.

In Figure 3, the upper (lower) panel represents the number of gaps ( y -axis) of a certain length ( x -axis) for (top to bottom) for the UK-host and the IndoChina-host. These are the sub-sampled versions of the actual data. Note that in both cases, shingle ordering is the best, i.e, the shingle ordering creates many more gaps of small length than the other orderings  X  the smaller the length of a gap, the fewer bits it takes for encoding.

From these, we see that shingle ordering reduces the lengths of gaps. As we argued earlier, shingle ordering also helps the BV and BL schemes exploit copying. These two benefits together appear to be the main reasons why shingle ordering almost always outper-forms many other orderings.
IndoChina-host 9.224 10.543 7.340 7.367 7.120 9.753 (  X 
We investigate what causes social networks to be far less com-pressible than Web graphs. We ask the question: is the densest portion of a social network far more compressible than the rest of the graph? To study this, we analyze k -cores of the LiveJournal so-cial network. Recall that a k -core of a graph is the largest induced subgraph whose minimum degree is at least k . For each k , the k -core of LiveJournal was extracted and compressed by itself. Then, the k -core edges were were removed from the original LiveJour-nal, which was also compressed by itself. The results are shown in Figure 4. It is clear that as k increases, the k -core gets easier to compress but at the same time the remaining graph gets harder and harder to compress. This suggests that the low-degree nodes in social networks are primarily responsible for its incompressibility. On a separate note, k -cores can also be used to compress the so-Figure 3: Gap distribution in UK-host and IndoChina-host graphs. cial network. This is done by representing all the nodes in a k -core by a single virtual node, and compressing the k -core graph and the remainder graph (with the virtual node) separately. For the Live-Journal graph, for k = 50 , we obtain 9.435 bits/link compression. This is a mild improvement over the best numbers in Table 2.
We have considered the compression of social networks, and have found both key similarities and differences between this prob-lem and the related, previously well-studied problem of compress-ing Web graphs. As with Web graphs, it appears desirable to take advantage of lexicographic locality and similar neighborhoods for compression of social networks. However, it is less clear that there is a natural ordering for social networks comparable to the URL ordering of Web pages. Instead, we have shown that shingle order-ing, which is based only on the local linkage of the graph, performs better than other seemingly natural orderings. We also highlight the critical role of reciprocal edges in social network graphs.
We have shown how the optimization of locality-based compres-sion schemes can be formulated as variations of the well-known minimum linear arrangement problem. These variations have their own subtle properties; approximation algorithms (or proving the hardness of approximation) remain interesting open questions. We have also considered lower bounds on compression for general classes of graphs, which suggest that the amazing compression ratios ob-tainable on the Web may be due to its particular structure, and may not be available for social graphs even given an optimal lexico-graphic ordering. [1] M. Adler and M. Mitzenmacher. Towards compressing web [2] R. Albert and A.-L. Barabasi. Emergence of scaling in [3] C. Amb  X  uhl, M. Mastrolilli, and O. Svensson.
 [4] D. Bladford and G. Blelloch. Index compression through [5] P. Boldi, M. Santini, and S. Vigna. Permuting web graphs. In [6] P. Boldi and S. Vigna. The webgraph framework I: [7] P. Boldi and S. Vigna. The Webgraph framework II: Codes [8] B. Bollobas and O. Riordan. Mathematical results on [9] B. Bollob  X  as, O. Riordan, J. Spencer, and G. E. Tusn  X  ady. The [10] A. Broder, M. Charikar, A. Frieze, and M. Mitzenmacher. [11] G. Buehrer and K. Chellapilla. A scalable pattern mining [12] A. Flaxman, A. M. Frieze, and T. I. Fenner. High degree [13] M. Garey and D. Johnson. Computers and Intractability: A [14] M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some [15] D. Gibson, R. Kumar, and A. Tomkins. Extracting large [16] J. H  X  astad. Some optimal inapproximability results. JACM , [17] R. Kumar, J. Novak, and A. Tomkins. Structure and [18] D. Liben-Nowell, J. Novak, R. Kumar, P. Raghavan, and [19] C. Manning, P. Raghavan, and H. Sch  X  utze. Introduction to [20] C. McDiarmid. Concentration for independent permutations. [21] S. Raghavan and H. Garcia-Molina. Representing web [22] K. H. Randall, R. Stata, J. Wiener, and R. Wickremesinghe. [23] S. Rao and A. W. Richa. New approximation techniques for [24] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. Chung. [25] F. Silvestri. Sorting out the document identifier assignment [26] F. Silvestri, R. Perego, and S. Orlando. Assigning document [27] T. Suel and J. Yuan. Compressing the graph structure of the [28] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Figure 5: An example showing the difference between ML OG and ML IN A .
 The graph in Figure 5 is an example showing that the ML IN and the ML OG A problems can have different optimal solutions: there is no ordering that minimizes both the objective functions si-multaneously. The best solutions for ML IN A have cost 19 whereas the best solutions for ML OG A have cost lg 180 . It can be checked that among the optimal ML IN A orderings (with cost 19), the best for ML OG A has cost lg 192 (e.g, the ordering 4 , 5 , 3 , 2 , 6 , 1 , 0 ). Among the optimal ML OG A ordering (with cost lg 180 ), the best for ML IN A has cost 20 (obtained by swapping 3 and 5 in the pre-vious ordering).

It is easy to similarly show that ML OG G AP A can have different solutions from both ML IN A and ML OG A. For instance, consider a star with three leaves. The optimum ordering for ML OG G will place the center of the star as the first (or the last) node of the ordering, yielding a total cost of 0 . On the other hand this solution is suboptimal for both ML IN A and ML OG A, either of which would place the center of the star as the second (or the third) node in the ordering.

We prove the hardness of ML OG A using the inapproximability of M AX C UT . Our starting point is that M AX C UT cannot be ap-proximated to a factor greater than 16 17 + unless P = NP [16]. In the reduction below we have not attempted to optimize parameters.
We start from a M AX C UT instance ( G ( V, E ) , k ) , where the ques-tion is if there is a cut of size at least k in G . Let | V | = n and | E | = m . We build the graph G 0 composed by a clique of size n 100 and a disjoint copy of the complement of G denoted  X  ther, we add an edge between each node of the clique and each node of  X 
G . Each edge of the clique will have multiplicity n 500 + 1 and all other edges will have unit multiplicity.

Now we would like to answer the following question Q : is it possible to find an ordering of G 0 with an ML OG A cost smaller than a given Z ? We show that answering questions of the form Q would allow us to approximate the corresponding M AX C UT n
First, note that in any ordering of G 0 for which the answer for Q is yes when Z = C + X  X  k lg n 100 , the nodes in the clique must be adjacent. Otherwise, at least one edge of the clique will be enlarged by at least 1 . In this case, the overall cost of the clique edges will be at least X  X  ( n 500 + 1)(lg n 100 ) + ( n 500 + 1) lg( n which is X +  X ( n 400 ) . This is larger than the cost allowed by the question Q .

We show that if the answer to Q when Z = C + X  X  k lg n 100 is positive, then there is a cut in G of size at least k (1  X  otherwise there is no cut of size k . As this allows approximations of M AX C UT to a factor better than 16 / 17 , this shows that we can have an algorithm to answer questions of the form Q only if P = NP, proving the hardness of ML OG A. From our previous argument, we now need only consider ordering of G 0 where the clique nodes are laid out consecutively. Each such ordering naturally gives a cut of the original graph, and the cost of the ML OG A objective function is equal to C + X  X  P { u,v } X  E ( G ) lg |  X  ( u )  X   X  ( v ) | . Consider the edges in G (corresponding to the missing edges in G 0 ) that pass over the clique. Each of these edges will have length at least n and hence the cost of the ML OG A objective function is smaller than C + X  X  k lg n 100 = Z . Hence if the there is a cut of size at least k in G , then the answer to Q is yes.

On the other hand, each of the other missing edges will have length at most n , (the order of G ), and hence have cost at most lg n . As the M AX C UT cost k is at least m 2 , if G does not have a cut of size at least k (1  X  1 50 ) , then the smallest that the ML objective function can be is for n sufficiently large. This proves the claim.

We start from the (directed) ML IN A problem, which is known to be NP-hard [14]. Let ( G ( V, E ) , k ) be an ML IN A instance where the question is if there is a linear arrangement whose sum of edge lengths is  X  k . Let n = | V | and m = | E | . We create the instance of the (directed) ML IN G AP A problem as follows.

The graph G 0 will be composed of n 0 = n c +1 + 2 m nodes, for a large enough constant c . For each node v  X  V ( G ) , two directed cliques K v, 1 and K v, 2 of equal sizes n c will be created. Also, a clique of n nodes d v, 1 , . . . , d v, 2 n (the  X  X eer nodes X  of v ) will be created for each v  X  V ( G ) . Each node in K v, 1 and each node in K v, 2 will point to node d v,i for all i = 1 , . . . , deg( v ) and vice versa.

The set E ( G 0 ) will contain 2 m other edges, that we call the  X  X riginal X  edges. In particular, for each edge ( v, u )  X  E ( G ) the each node d v,  X  will have outdegree  X  n .

Given an arbitrary node v , consider the following ordering of its two cliques and of its peer nodes: the first clique laid out on n consecutive nodes, followed by its 2 n peers, and finally the second clique (using a total of n c + n nodes); we call this ordering good . Let F be the cost of the edges of the cliques, and the edges from the cliques to the peers, in this ordering. Note that F can be trivially computed in polynomial time. Now we ask: is there an ordering with ML IN G AP A cost at most nF + 3 K (2 n c ) + 3 mn 2
If there is an ML IN A ordering  X  of cost at most K , then it is easy to find an ML IN G AP A ordering of cost at most T . If v is the first node of  X  , place the first clique of v followed by the peers of v and the second clique of v at the beginning. Then do the same for the second node of  X  , and so on, until all nodes have been placed. Now we compute the total ML OG G AP A cost. We have a fixed cost of nF (the ordering of the  X  X odes structures X ) for the non-original edges. As for the original edges, note that each node from which an original edge starts has out-degree 1 , thus encoding the  X  X ap X  induced by that edge has the same cost of encoding its length. Note that the number of cliques that an edge (that had length ` in  X  ) passes over in the new ordering is 2 ` and each such clique has size n . Thus, the cost in the new ordering of the edge will be at most ` 2 n c +  X  , where  X  is an error term that equals n 2 (the total number of peer nodes). Now for any edge of length in the ML IN A, there are three gaps of cost at most ` 2 n c + n 2 . The total cost will thus be at most nF + 3 K (2 n c ) + 3 mn 2 = T .
 Now suppose we have an ML IN G AP A ordering with cost at most T . We then show that there is an ML IN A ordering of cost at most K . To show this, we first prove that for each v , the ordering will be such that (i) the distance between any two nodes of K v, 1 K v, 2 ) will be at most n c + n 4 , i.e., the cliques will not be spread out, (ii) the distance between each single peer of v and its nearest node of K v, 1 ( K v, 2 ) will be at most n 4 .

Suppose this statement is true. We show by contradiction that there must exist an ML IN A ordering of cost at most K . First, notice that the minimum cost that we have to pay for the edge between nodes in V ( G 0 ) that are generated from one node v is at least F , since in any ordering the gaps are of length at least 1 and for any ordering, the sum of the cost of the backward edges is at least that of in the good ordering. Furthermore, it follows from (i) and (ii) that in all valid solutions, for each v  X  V ( G ) , each peer node of v must be placed at distance at most n c + 2 n 4 from each clique node of v . Now, the number of nodes of the cliques generated by v is 2 n c so it is necessary that each peer node has to be placed after at least n c  X  2 n 4 nodes of one of its two cliques and before n  X  2 n 4 nodes of its other (as each peer node has to be at distance  X  n c + 2 n 4 from each node of its cliques). Hence, the total cost for any ordering of cost K + 1 for the ML IN A problem is at least nF + 3( K + 1)(2 n c  X  4 n 4 ) &gt; T , a contradiction.
To finish the proof, we show (i): if the maximum distance be-tween two nodes in any of the K v cliques is &gt; n c + n total cost of the ordering is &gt; T . Indeed if the distance between any two nodes of K v is more than n c + n 4 , then the cost for the edges between the clique and peer nodes of v will be  X  F + n c +4 where the first term of the sum follows since all the gaps are of length at least one, and there are at least n c + n backlinks. The sec-ond term of the sum is the added cost due to the spread of the clique, which is  X  n 4 , and since, say, the rightmost node of the clique must go across all the non-clique nodes between clique nodes, for a total of at least n c  X  1 links. Hence, the cost of the ordering would be  X  nF + n c +4  X  n c &gt; T , contradicting the validity of the solution since K = O ( n 2 ) .

Finally we have to prove (ii): for each v  X  V ( G ) , no peer node of v is at distance  X  n 4 from each of the cliques of v . Proceeding as before, we lower bound the cost of the ordering for the edges between the nodes of the peers and the cliques of v . The cost of the ordering will be F plus the cost due to the enlargement of the gaps between v and K v . Thus, the total cost of the ordering is  X  nF + n c +4 &gt; T , again a contradiction.

Let G = ( V, E ) be a simple graph with no isolated nodes and let | E | = m . For the edge expansion case, note that for any S  X  V such that | S | = n/ 2 , we have that  X ( n ) edges are in the cut ( S, G \ S ) . If  X ( n ) edges are in the cut then there are  X ( n ) edges of length at least will have cost  X (log n ) . The edge expansion claim follows. The node expansion case is analogous.

Now suppose that the conductance is  X (1) . First note that the total degree of the graph is  X  2 n  X  2 , since the graph has to be connected. We claim that there is an [ S, T ] cut with m  X  n + 1 &lt; | S | X  m . To see this, consider any line embedding of the graph. Scan the nodes from the left and stop as soon as the total degree of the nodes from the nodes seen so far to the unseen nodes is more than m . Remove the current node, whose degree is  X  n  X  1 , letting S be the set of nodes seen up until the current node. Then, | S | X   X ( m ) , and at least  X ( m ) edges are in the cut. Hence, at least  X ( m ) of those edges will have length  X ( be  X ( m log m ) =  X ( m log n ) .

We need the following concentration inequality, proved in a stronger form by McDiarmid [20].

T HEOREM 8. Let X be a non-negative random variable not identically 0, which is determined by an independent random per-mutation  X  , satisfying the following for some c, r &gt; 0 : interchang-ing two elements in the permutation can affect X by at most c , and for any s , if X  X  s , then there is a set of at most rs coordinates of  X  whose values certify that X  X  s . Then, for any 0  X  t  X  E [ X ] ,
Pr[ | X  X  E [ X ] | &gt; t + 60 c p rE [ X ]]  X  4 exp
Using this, we prove Theorem 6. Given an ordering and an arbi-trary node v , we say that the edge ( v, w ) is shingled if the position of v is determined by w , i.e., if the minimum out-neighbor of v , according to the random shingle permutation, is w . Also, we say that a node v is shingled by w if w is the minimum out-neighbor of v according to the random shingle permutation. A node v  X  S is good if there is a node v 0  X  S , v 6 = v 0 , such that v and v shingled by the same node.

Let X be the number of good nodes. By property (ii), each node v in S has a common out-neighbor with at least another node in S . As all nodes in S have outdegree bounded by k and hence with probability 1 / (2 k  X  1)  X  1 / (2 k ) , one of their common out-neighbors will be the smallest of both their out-neighborhoods ac-cording to the random shingle permutation, i.e., they will be shin-gled together. Thus, E [ X ]  X | S | / (2 k ) .
 We will first assume X  X   X ( | S | ) whp, and claim that at least  X ( | S | ) edges are copied. Indeed, partition the good nodes in S according to their shingling node. Each part will contain at least two nodes (by the definition of good nodes), and in each part all the nodes, but the first, will copy their edge pointing to their shingling node. Thus, the fraction of good nodes in a part copying at least one of their edges is  X  1 / 2 . The claim follows.

To obtain the high probability lower bound on X we use The-orem 8. Note that here we only have the random shingle permu-tation (i.e., no random trials). In order to use Theorem 8 we have to choose suitable c, r . Using property (iv), we can upperbound the effect on X of a swap of two elements with c = 2 n 1 2 see this, the only nodes that can change their good or bad statuses are the in-neighbors of the two swapped nodes and these can be upperbounded by 2 n 1 2  X  . If a node v  X  S is good, then there exists a node v 0  X  S with the same shingling node w . Thus, to certify that v is good it suffices to reveal the positions of the nodes in N + ( v )  X  N + ( v 0 )  X  v is good iff w is the first of the nodes in N + ( v )  X  N + ( v 0 ) . As the degrees of v, v 0 are bounded by k , we can safely choose r = 2 k . By plugging c, r into Theorem 8 we get the high probability lower bound on X .
