 Manually crafted combinatorial features have been the  X  X e-cret sauce X  behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Cross-ing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be ei-ther dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units .

Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to im-prove existing products, as well as to speed up the devel-opment of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowl-edge.
 Neural Networks, Deep Learning, Convolutional Neural Net-work (CNN), Deep Crossing, Combinatorial Features, DSSM, Residual Net, CNTK, GPU, Multi-GPU Platform
Traditional machine learning algorithms are supposed to make the best use of all input features to predict and classify new instances. However simply using raw features rarely provides optimal results. Therefore in both industry and academia, there exists a large body of work on engineering the transformation of raw features. One major type of trans-formation is to construct functions based on a combination of multiple features, and use their output as the input to a learner. These combinatorial features 1 are sometimes called cross features , or multi-way features .

Combinatorial features are powerful tools, especially in the hands of domain experts. In our own experiences with a major sponsored search engine, they are among the strongest features in many models. In the Kaggle community, top data scientists are the masters of crafting such features, even crossing three to five dimensions 2 . The intuition and the ability to create effective combinatorial features is an essen-tial ingredient in their winning formulas. In the computer vi-sion community, SIFT-like [13] features were the key drivers behind the then state-of-the-art performance of ImageNet competitions. SIFT features are extracted on image patches and are special forms of combinatorial features.
 The power of combinatorial features comes with high cost. There is a steep learning curve to climb before an individual can start to create meaningful features. As the number of features grows, managing, maintaining, and deploying them becomes challenging. In web-scale applications, finding ad-ditional combinatorial features to improve an existing model is a daunting task due to the large search space, and the slow turnaround in the training and evaluation cycle given billions of samples.

Deep Learning [9, 20, 16] carries the promise of learning from individual features without manual intervention [17, 21]. Speech [8, 4, 18] and image recognition [11] were among the first to demonstrate this potential. Convolution kernels learned from specific tasks through deep Convolutional Neu-ral Networks (CNN) have replaced the hand-crafted SIFT-like features as the state-of-the-art in image recognition. A similar model [3] has been applied to NLP applications to build language processing models from scratch without ex-tensive feature engineering.

Deep Crossing extends the success of deep learning to a more general setting where individual features are of a differ-ent nature. More specifically, it takes in individual features such as text, categorical, ID, and numerical features, and searches for the optimal combinations automatically based on the specific task. Furthermore, Deep Crossing is designed to handle web-scale applications and data sizes. This is not only because the authors are primarily interested in such applications, but also because there are not many options for general purpose models operating at this scale. It is
See Sec. 4 for a formal definition and concrete examples.
See github.com/owenzhang/Kaggle-AmazonChallenge2013 for an example of multi-way features in Owen Zhang X  X  win-ning solution for the Amazon X  X  Employee Access Challenge. worth noting that Deep Crossing does in some way generate combinatorial features during the learning process, although the output of Deep Crossing is a model that does not have explicit representations for these features.
The idea of learning deep neural network without man-ually crafted features is not new. In early 80s, Fukushima [6] reported a seven-layer Neocognitron network that rec-ognized digits from raw pixels of images. By utilizing a partially connected structure, Neocognitron achieved shift invariance which is an important property for visual recog-nition tasks. CNN was invented by LeCun et. al. [12] in the late 90s with a similar architecture, especially the par-tially connected convolution kernels. Despite the solid foun-dation of CNN as a recognition engine, classifiers based on SIFT-like [13] features dominated image recognition for nearly a decade. In 2012, Krizhevsky et. al. proposed the AlexNet [11] that beat the SIFT-based baseline error rate by almost 11 absolute percentage points. Recently, a 152-layer Residual Net [7] won both the ImageNet and the MS COCO Competitions in 2015.

The evolution of deep CNN is both inspiring and encour-aging. It demonstrated that deep learning is able to im-prove even in systems in which the best manual features have been finely tuned over a decade. In other words, even the most experienced domain experts could miss the deep in-teractions among features that were captured by deep CNNs using task-specific filters. Realizing this has profound impli-cations on our work on Deep Crossing.

Deep Semantic Similarity Model (DSSM) [10] learns the semantic similarity between a pair of text strings, each rep-resented by a sparse representation called tri-letter grams. The learning algorithm optimizes an objective function based on cosine distance by embedding the tri-letter grams into two vectors. The learned embedding captures the semantic meaning of words and sentences, and has been applied to sponsored search, question answering, and machine transla-tion with strong results.

Factorization Machines (FM) [15] in their general form models d-way interactions among individual features. In the presence of very sparse inputs, FM shows better results than SVMs but it X  X  unclear how it performs on dense features.
For NLP tasks, [3] built a unified neural network archi-tecture that avoids task-specific feature engineering. Deep Crossing aims at a broader range of input features.
Deep Crossing is discussed in the context of sponsored search of a major search engine. Readers can refer to [5] for an overview on this subject. In brief, sponsored search is responsible for showing ads alongside organic search results. There are three major agents in the ecosystem: the user, the advertiser, and the search platform. The goal of the platform is to show the user the advertisement that best matches the user X  X  intent, which was expressed mainly through a specific query. Below are the concepts key to the discussion that follows.
 Query: A text string a user types into the search box Keyword: A text string related to a product, specified by Title: The title of a sponsored advertisement (referred to as Landing page: A product X  X  web site a user reaches when Match type: An option given to the advertiser on how Campaign: A set of ads that share the same settings such Impression: An instance of an ad being displayed to a user. Click: An indication of whether an impression was clicked Click through rate: Total number of clicks over total num-Click Prediction: A critical model of the platform that Sponsored search is only one kind of web-scale application. However, given the richness of the problem space, the various types of features, and the sheer volume of data, we think our results can be generalized to other applications with similar scale.
This section defines and compares combinatorial features and individual features, using examples listed in Table 1. The features in the table are available during run-time when an ad is displayed (an impression). They are also available in offline logs for model training, etc. Each individual feature X i is represented as a vector. For text features such as a query, one option is to con-vert the string into a tri-letter gram with 49 , 292 dimensions as in [10]. Categorical input such as MatchType is repre-sented by a one-hot vector, where exact match (see Sec. 3) is [1 , 0 , 0 , 0], phrase match is [0 , 1 , 0 , 0], and so on.
There are usually millions of campaigns in a sponsored search system. Simply converting campaign ids into a one-hot vector would significantly increase the size of the model (see Sec. 5.1). One solution is to use a pair of companion features as exemplified in the table, where CampaignID is a one-hot representation consisting only of the top 10 , 000 campaigns with the highest number of clicks. The 10 , 000 slot (index starts from 0) is saved for all the remaining cam-paigns. Other campaigns are covered by CampaignIDCount , which is a numerical feature that stores per campaign statis-tics such as click through rate . Such features will be referred as a counting feature in the following discussions. All the fea-tures introduced so far are sparse features except the count-ing features.
Given individual features X i  X  R n i and X j  X  R n j , a com-binatorial feature X i,j is defined in R n i  X  R n j . Combinatorial features also have sparse and dense representations. An ex-ample of sparse representation is a CampaignId  X  MatchType feature, which is a one-hot vector of 10 , 001  X  4 = 40 , 004 dimensions. An example of a dense representation counts how many ad clicks for a specific CampaignId and MatchType combination. The dimension for the dense representation is the same as its sparse counterpart.

Deep Crossing avoids using combinatorial features. It works with both sparse and dense individual features, and supports a broad range of feature types described above. This gives users the freedom to use the features of their choice from their specific application. While collecting fea-tures and converting them into the right representations still requires a significant amount of effort, the work stops at the level of individual features. The rest is taken care of by the model.
Fig. 1 is the model architecture of Deep Crossing, where the input is a set of individual features. The model has four types of layers including the Embedding , the Stacking , the Residual Unit , and the Scoring Layer . The objective func-tion is log loss for our applications but can be easily cus-tomized to soft-max or other functions. Log loss is defined as: logloss =  X  1 where i indexes to the training samples, N is the number of samples, y i is the per sample label, and p i is the output of the one-node Scoring Layer in Fig. 1, which is a Sigmoid function in this case. The label y i in the click prediction problem is a user click.
Embedding is applied per individual feature to transform the input features. The embedding layer consists of a single layer of a neural network, with the general form: where j indexes the individual feature, X I j  X  R n j is the input feature, W j is an m j  X  n j matrix, b  X  R n j , and X is the embedded feature. When m j &lt; n j , embedding is used to reduce the dimensionality of the input feature. The per element max operator is usually referred to as a rectified linear unit (ReLU) in the context of neural networks. The output features are then stacked (concatenated) into one vector as the input to the next layer: where K is the number of input features. Note that both { W j } and { b j } are the parameters of the network, and will be optimized together with the other parameters in the net-work. This is an important property of embedding in Deep Crossing. Unlike the embedding only approaches such as word2vec [14], it is an integral part of the overall optimiza-tion process.

It should be pointed out that the size of the embedding layers has significant impact on the overall size of the model. Even for sparse features, the m j  X  n j weight matrices are inherently dense. This is why Deep Crossing uses the com-panion features in Sec. 4 to constrain the dimensionality of high cardinality features. For the click prediction experi-ments in Sec. 7, the total size of the input features is around 200 , 000, and the m j for the high dimensional features such as query and keyword are uniformly set to 256. Features with dimensionality lower than 256 are stacked without em-bedding. An example is Feature #2 in Fig. 1.
The residual layers are constructed from the Residual Unit in Fig. 2. The Residual Unit is the basic building block of the Residual Net [7] that claimed the world record in the ImageNet contest. Deep Crossing uses a slightly modified Residual Unit that doesn X  X  use convolutional kernels. To our knowledge, this is the first time Residual Units have been used to solve problems beyond image recognition.
The unique property of Residual Unit is to add back the original input feature after passing it through two layers of ReLU transformations. Specifically: where W { 0 , 1 } and b { 0 , 1 } are the parameters of the two lay-ers, and F denotes the function that maps the input X I of the Residual Unit to the output X O . Moving X I to the left side of Eq. 4, F (  X  ) is essentially fitting the residual of X
O  X  X I . In [7] the authors believed that fitting resid-uals has a numerical advantage. While the actual reason why Residual Net could go as deep as 152 layers with high performance is subject to more investigations, Deep Cross-ing did exhibit a few properties that might benefit from the Residual Units.
Before Deep Crossing, we tried a number of model archi-tectures with deep layers but none of them provided signif-icant gains over a model with two to three layers to justify the added complexity. Deep Crossing is our strongest model that easily beats the performance of its shallower counter-parts.

Deep Crossing was applied to a wide variety of tasks. It was also applied to training data with large differences in sample sizes. In all cases, the same model was used without any adjustment in the layers, nodes, and type of nodes. It X  X  likely that the Residual Units are implicitly performing some kind of regularization that leads to such stability. It is worthwhile to compare Deep Crossing with DSSM. Fig. 3 is the architecture of a modified DSSM using log loss as the objective function. The modified DSSM is more closely related to the applications of click prediction. It keeps the basic structure of DSSM on the left side of the green dashed line, but uses log loss to compare the predic-tions with real-world labels. DSSM allows two text inputs, each represented by its tri-letter gram vector. DSSM has the distinct property of delaying the feature interaction (or crossing) to the late stage of the forward computation. Be-fore reaching the Cosine Distance node, the input features are fully embedded through multiple layers of transforma-tions on two separate routes. In contrast, Deep Crossing adopts at most one layer of single-feature embedding, and starts feature interaction at a much earlier stage of the for-ward computation.

DSSM and its convolutional variation (CDSSM [19]) have been proven by many real-world applications to be strong learning machines especially tuned for a pair of text inputs. Still, when comparing Deep Crossing and DSSM in Sec. 7.1 on two text only tasks, the former consistently outperforms the latter. Besides the superior optimization ability of the Residual Units, introducing the feature interaction early on in the forward computation seems to play an important role.
Deep Crossing is implemented on a multi-GPU platform powered by the Computational Network Toolkit (CNTK)[1] 3 , which shares the same theoretical foundation of computa-tional network with TensorFlow 4 .
Building the Deep Crossing model is quite straight for-ward with CNTK. The script below defines a few Macros that will be used later.
 ONELAYER(Dim_XO, Dim_XI, XI){ } ONELAYERSIG(Dim_XO, Dim_XI, XI){ } ONELAYERRELU(Dim_XO, Dim_XI, XI){ } RESIDUALUNIT(Dim_H, Dim_XI, XI){ } The Macro ONELAYER creates a weight matrix W and a bias vector b for one layer of a network. Macros ONELAYERSIG and ONELAYERRELU apply element-wise Sigmoid and ReLU func-tions to the output of ONELAYER and build a full Sigmoid layer and a ReLU layer, respectively. Comparing side-by-side ONELAYERRELU and Equ. 2, XI , XO , Dim_XI , and Dim_XO are X I j , X O j , n j , and m j , respectively. The RESIDUALUNIT uses a hidden layer of Dim_H , but the output has the same dimension as Dim_XI . The script in Fig. 4 has the details of the actual Deep Crossing model when applied to the features in Table 1. As can be seen from the script, the full model has five steps. Each step has been described in Sec. 5. Since the MatchType feature M has only 4 dimensions, it goes di-rectly to the stacking stage without embedding. This is why the embedding step and the stacking step are regarded as a single, combined step. Dim_E and Dim_CROSS* are the num-bers of nodes in the embedding layer and the hidden layer inside the Residual Unit, respectively. Dim_L=1 for log loss. Some details such as feature I/O and normalization are not included here for simplicity 5 . http://www.cntk.ai https://www.tensorflow.org
The full Deep Crossing model will be available at ## The Deep Crossing Model ## Step 1: Read in features, omitted ## Step 2A: Embedding Q = ONELAYERRELU(Dim_E, Dim_Query, Query) K = ONELAYERRELU(Dim_E, Dim_Keyword, Keyword) T = ONELAYERRELU(Dim_E, Dim_Title, Title) C = ONELAYERRELU(Dim_E, Dim_CampaignID, CampaignID) ## Step 2B: Stacking # M = MatchType, CC = CampaignIDCount Stack = RowStack(Q, K, T, C, M, CC) ## Step 3: Deep Residual Layers r1 = RESIDUALUNIT(Dim_CROSS1, Dim_Stack, Stack) r2 = RESIDUALUNIT(Dim_CROSS2, Dim_Stack, r1) r3 = RESIDUALUNIT(Dim_CROSS3, Dim_Stack, r2) r4 = RESIDUALUNIT(Dim_CROSS4, Dim_Stack, r3) r5 = RESIDUALUNIT(Dim_CROSS5, Dim_Stack, r4) ## Step 4: Final Sigmoid Layer Predict = ONELAYERSIG(Dim_L, Dim_Stack, r5) ## Step 5: Log Loss Objective CE = LogLoss(Label, Predict) CriteriaNodes = (CE) Figure 4: A Deep Crossing Model with five layers of Residual Units, described in the CNTK modeling language
The experiments were carried out on a GPU cluster that is optimized for CNTK for rapid, no-hassle, deep learning model training and evaluation. The cluster, which has high-throughput distributed storage, virtual file systems, and fault tolerance, is managed by specially designed automated clus-ter management and job/container scheduling software. Dur-ing experimentation, each GPU machine in the cluster con-tained four K40 GPU cards. Infiniband is used to connect nearby GPU machines for high-speed data transmission be-tween GPUs across machines.

To speed up the experiments, we have exploited the block-wise model-update filtering (BMUF) distributed training algorithm implemented in CNTK, originally proposed by Chen and Huo [2]. The BMUF algorithm improves over traditional model averaging (MA) and alternating direction method of multipliers (ADMM) while keeping the advantage of the low communication cost of these methods. While MA and ADMM can be easily scaled out, they often under per-form single-GPU SGD with regard to the accuracy achiev-able with the trained model, and require different learning rate schedules than that of single-GPU SGD. The BMUF algorithm, however, does not have these drawbacks. Chen and Huo [2] reported a 28X speedup with 32 GPUs while also achieving better accuracy than the single-GPU SGD on large-scale speech recognition experiments. In our experi-ments reported in this paper, we reduced training time from 24 days with a single-GPU, to 20 hours by exploiting 32 GPUs across 8 machines.
Deep Crossing was trained and evaluated on impression and click logs of a major search engine. Table 2 lists the data sets used for the experiments in this section. Each https://github.com/Microsoft/CNTK/wiki/Examples.
 Please check the site for the latest.

Data Set Group Task Type Rows Dims text cp1 tn s G 1 CP1 train 194 98.5 text cp1 vd G 1 CP1 valid 49 98.5 text cp1 tt G 1 CP1 test 45 98.5 text cp1 tn b G 1 CP1 train 2,930 98.5 text cp2 tn G 2 CP2 train 518 98.5 text cp2 vd G 2 CP2 valid 100 98.5 text cp2 tt G 2 CP2 test 93 98.5 all cp1 tn s G 3 CP1 train 111 202.7 all cp1 vd G 3 CP1 valid 139 202.7 all cp1 tt G 3 CP1 test 156 202.7 all cp1 tn b G 3 CP1 train 2,237 202.7 Table 2: Data sets used in this paper, where Rows (in mil-lions) is the number of samples in the data set, while Dims (in thousands) is the total number of feature dimensions experiment will use a combination of training, validation, and test data sets, referred by the names in the  X  X ata Set X  column. Only data sets in the same  X  X roup X  are compat-ible, meaning that there is no overlap in time for different  X  X ypes X . For example, a model trained with all_cp1_tn_b (the last row in the table) can be validated with all_cp1_vd and tested with all_cp1_tt since they all belong to the G_3 group.

The  X  X ask X  of a data set is either CP1 or CP2 . The two tasks represent two different models in the click prediction pipeline. In this case, CP1 is the more critical model of the two, however the models are complementary in the system.
The Deep Crossing model described in Fig. 4 is used for all the experiments. Model parameters are also fixed to 256, 512, 512, 256, 128, and 64 for Dim_E , and Dim_CROSS[1-5] , respectively. Readers are encouraged to experiment with these parameters including the number of layers for their specific applications to achieve optimal results.

Note that all experimental results comparing with base-lines are statistically significant due to the sheer volume of samples used in the test data. We will not call out individ-ually for simplicity.
As briefly mentioned in Sec. 5.3, we are interested in com-paring DSSM and Deep Crossing in the setting that DSSM is specialized. To create an apples-to-apples comparison, we trained a DSSM and a Deep Crossing model on data for CP1 and CP2 , but limited the Deep Crossing model to the same data as DSSM (i.e., both use a pair of inputs that include query text and keyword or title text, each represented by a tri-letter gram vector).

In the first experiment, click prediction models were trained on task CP1 on the two data sets listed in Table 3. On both data sets, Deep Crossing outperforms DSSM in terms of rel-Table 3: Click prediction results for task CP1 with a pair of text inputs where performance is measured by relative AUC using DSSM as the baseline Table 4: Click prediction results for task CP2 with a pair of text inputs where performance is measured by relative AUC using the production model as the baseline ative AUC as explained in the table. Note that the DSSM model used here is the log loss version detailed in Sec. 5.3.
In the second experiment, both models were trained on task CP2 on the text_cp2_tn dataset, and tested on the text_cp2_tt dataset. Table 4 shows the performance re-sults for DSSM, Deep Crossing, and the model currently run-ning inside our production system. The production model is trained on a different data set, but is tested with the same data ( text_cp2_tt ) using the prediction output logged dur-ing run-time. It can be seen that Deep Crossing performs better than DSSM but worse than the production model. This is expected since the production model uses many more features  X  including combinatorial features  X  and has been refined over many years. Still, by just using individual query and title features, Deep Crossing is only about one percent-age point away from the production model.

While we have demonstrated Deep Crossing X  X  ability to learn from simple pairs of text inputs, this is not its main goal. The real power of Deep Crossing is to work with many individual features, as we will see in the subsequent experi-ments.
We now consider the performance of Deep Crossing on task CP1 (training set all_cp1_tn_s ) with about two dozen features 6 including those listed in Table 1. The experiments in this sub-section don X  X  have external baselines, instead we will only compare Deep Crossing X  X  performance with differ-ent combinations of features. Instead of assessing the per-formance relative to other methods, the goal here is to see Deep Crossing in action, and demonstrate its performance can change significantly as features are added and removed. We will compare the performance of Deep Crossing with production models in the next sub-section, using the same rich set of features.

In the first experiment, we ran Deep Crossing several times with different sets of features turned on and off. Count-ing features (see Sec. 4 for definition) are always turned off, and will be turned back on in the next experiment. The All_features model in Fig. 5 has all the remaining features turned on. As expected, it has the lowest log loss among all the models in this experiment. The Only_Q_K model has the highest log loss. This model uses only query text and keyword text, which is similar to the models in the previous sub-section.

Note that the log loss in Fig. 5 is relative log loss de-fined as the actual log loss divided by the lowest log loss of the All_features model over all epochs. The gap between Only_Q_K and All_features in terms of relative log loss is about 0 . 12. This is roughly a 7  X  8% improvement in terms of AUC, which is huge given that an improvement of 0 . 1  X  0 . 3% in AUC is usually regarded as significant for click prediction
Unfortunately we were not able to disclose the full feature set but we did our best to make sure the lack of such details has minimal impact on the discussions of the results. Figure 5: Deep Crossing X  X  performance on task CP1 with different individual feature sets where performance is mea-sured by relative log loss on the validation set over the train-ing epochs (relative log loss is defined in Sec.7.2) models. Without_Q_K_T is a model with query text, key-word text, and title text removed, which means that taking away the majority of text features will increase relative log loss by 0 . 025. This is a half of the increase with the model Without_position , which removes the position feature.
In the second experiment, we study how the counting fea-tures interact with the rest of the individual features. As discussed in Sec. 4, counting features play an important role in reducing the dimension of high cardinality features. Our full feature set has five types of counting features. In this experiment we just turned on one of them (referred as the selected counting feature hereafter) to demonstrate the ef-fect.

As can be seen from Fig. 6a, the model Counting_only with only the selected counting feature is very weak com-pared with the All_without_counting model, which has all of the features except the counting features. Fig. 6b shows the result after adding the selected counting feature, where the new model All_with_counting reduced the rel-loss for both Fig. 6a and Fig. 6b is the lowest log loss of the All_without_counting model over all epochs.
Up to this point, the question remains whether Deep Cross-ing can actually beat the production model, which is the ul-timate baseline. To answer this question, we trained a Deep Crossing model with 2.2 billion samples, using a subset of the raw features from the production model. As reported in Table 5, the new model has comfortably outperformed the production model in offline AUC on task CP1 (results for task CP2 are not available as of the time of publication). It was trained on the task CP1 ( all_cp1_tn_b ) data set , and tested on all_cp1_tt . The production model was trained on a different (and bigger) data set but was tested with the same data. As in the comparison with the task CP2 model in Sec. 7.1, the production AUC was based on the prediction output logged during the run-time.

The above result is quite significant since the Deep Cross-Table 5: Click prediction model compared with the produc-tion model on task CP1 where performance is measured by relative AUC using production model as the baseline ing model used only a fraction number of features, and took much less efforts to build and maintain.
Deep Crossing enables web-scale modeling without exten-sive feature engineering. It demonstrated that with the recent advance in deep learning algorithms, modeling lan-guage, and GPU-based infrastructure, a nearly dummy solu-tion exists for complex modeling tasks at large scale. While such claims require further testing, it does resonate with a key benefit of deep learning envisioned by the pioneers, which is to free people from the tedious work of feature en-gineering.

Deep Crossing was initially developed in the context of paid search ads, which is a web-application with massive data. When applied to other domains, we expect that most properties will still hold. This is because of the extensive coverage of feature types and the general model representa-tions.

Besides saving effort in building large models, Deep Cross-ing also helps to decouple the application domains with mod-eling technologies. To hand craft combinatorial features re-quires extensive domain knowledge. But if modeling com-plexity is shifted from feature engineering to modeling tech-nologies, the problem is opened up to the entire world of modeling experts, even those not working in the same do-main. As evidence, the BMUF [2] distributed training algo-rithm developed by a few researchers in speech recognition helped Deep Crossing to fill the last bit of performance gap with a production model for sponsored search. While we don X  X  have an explicit economic study, the compound effect of less efforts in feature engineering and getting more help from others is expected to be significant for a broad range of applications.

Deep Crossing is by design a multi-way fusion engine for heterogeneous features. As we apply it to more application domains, it will be interesting to see how this aspect is fully manifested. The authors would like to thank Chris Basoglu, Yongqiang Wang, Jian Sun, Xiaodong He, Jianfeng Gao, and Qiang Huo for their support and discussions that benefited the development of Deep Crossing. [1] A. Agarwal, E. Akchurin, C. Basoglu, G. Chen, [2] K. Chen and Q. Huo. Scalable training of deep [3] R. Collobert, J. Weston, L. Bottou, M. Karlen, [4] G. E. Dahl, D. Yu, L. Deng, and A. Acero.
 [5] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [6] K. Fukushima. Neocognitron: A self-organizing neural [7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual [8] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r.
 [9] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast [10] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and [11] A. Krizhevsky, I. Sutskever, and G. E. Hinton. [12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [13] D. G. Lowe. Object recognition from local [14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [15] S. Rendle. Factorization machines. In Data Mining [16] J. Schmidhuber. Deep learning in neural networks: An [17] F. Seide, G. Li, X. Chen, and D. Yu. Feature [18] F. Seide, G. Li, and D. Yu. Conversational speech [19] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. [20] D. Yu and L. Deng. Deep learning and its applications [21] D. Yu, G. Hinton, N. Morgan, J.-T. Chien, and
