 1. Introduction 1.1. Motivation XML is becoming the standard for integrating and exchanging data over the Internet and within intranets, covering the complete spectrum from largely unstructured, ad hoc docu-ments to highly structured, schematic data. A number of XML query languages have been proposed, such as XPath, XML-QL, or the recently announced W3C standard XQuery. These languages combine SQL-style logical conditions over element names, content, and attributes with regular-expression pattern matching along entire paths of elements. The re-sult of a query is a set of paths or subgraphs from a given data graph that represents an XML document collection; in information retrieval (IR) terminology this is called Boolean Retrieval.

This search paradigm makes sense for queries on largely schematic XML data such as electronic product catalogs or bibliographies. It is of very limited value, however, for searching highly heterogeneous XML document collections where either data comes from many different information sources without a global schema or most documents have an ad 522 SCHENKEL, THEOBALD AND WEIKUM hoc schema or DTD with element names and substructures that occur only in a single or a few documents. The latter kind of environment is typical for document management in large protein structures, and, of course, also for the Web. For example, a bank has a huge number of truly semistructured documents, probably much larger in total size than the production data held in (object-) relational databases; these include briefing material and the minutes of meetings, customer-related memos, reports from analysts, financial and business news articles, and so on. Here, the variance and resulting inaccuracies in the document structures, v ocabulary, and document content dictate ranked retrieval as the only meaningful search paradigm.

The result of a query should be a list of potentially relevant XML documents, elements, or subgraphs from the XML data graph, in descending order of estimated relevance. This intranet search, but this technology does not at all consider the rich structure and semantic annotations provided by XML data. Rather state-of-the-art IR systems restrict themselves to term-frequency-based relevance estimation (Baeza-Yates and Riberto-Neto 1999) and/or link-based authority ranking (Brin and Page 1998, Kleinberg 1999).

This article presents a query language, coined XXL (for Flex ible X ML Search L anguage), and the prototype implementation of the XXL search engine, as steps towards more power-ful XML querying that reconciles the more schematic style of logical search conditions and pattern matching with IR-style relevance ranking. Aiming at simplicity and with focus on simple but widely usable search templates, our approach has adopted a core of essential fea-tures from XQuery-style languages and has enhanced it with a similarity operator, denoted  X  ,o n element names and contents. The evaluation of similarity conditions is based on a quantified ontology for element names and contents in combination with term-frequency-based IR-style estimations for element contents. The assessments of  X  X ocal X  similarity tests are combined into  X  X lobal X  relevance rankings using simple probabilistic arguments.
Even though some of our results have been previously published (Theobald and Weikum 2000, 2002a, Schenkel et al. 2003), this article is the first that completely covers the XXL Search Engine in all its aspects, including the underlying ontological model, query evalua-tion, and implementation issues. It further presents new experimental results with the INEX benchmark that were not published outside the INEX community before. 1.2. XML data model In our model, a collection of XML documents is represented as a directed graph where the nodes represent elements, attributes and their values. For identification, each node is assigned a unique ID, the oid . There is an directed edge from a node x to a node y if  X  y is a subelement of x ,  X  y is an attribute of x ,  X  y contains the content of element x ,or  X  y contains the value of attribute x .
 SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 523 Additionally, we model a link from one element to another (this can be an ID/IDREF link, an XLink or an XPointer) by adding two directed edges in opposite directions between the corresponding nodes; while this model covers simple and extended XLinks, our system currently supports only simple XLinks. We call the resulting graph the XML data graph for the collection.

Figure 1 shows the XML data graph for a collection of two XML documents similar to those in the INEX collection: a journal document with an XLink pointing to an article document. Each node that contains an element or attribute name is called an n-node (shown as normal nodes in figure 1), and each node that contains an element content or attribute v alue is called a c-node (dashed nodes in figure 1). To represent mixed content, we need a local order of the child nodes of a given element. Figure 1 shows an example for a sentence that is distributed over several c -nodes. 1.3. Outline The rest of the article is structured as follows. We discuss related work in Section 2. In Section 3 we introduce our ontology model. Section 4 presents our query language XXL, and Section 5 describes architecture and core components of our search engine. Section 6 presents details of the query evaluation in the XXL Search Engine. Finally, Section 7 shows the effectiveness of our search engine with example results from the INEX benchmark. 2. Related work Ranked retrieval on XML data has been quite popular in recent years. The first approaches carried over keyword-based search from the Web to XML data and did not provide structural constraints. Among the more recent retrieval engines of this kind are XRANK (Guo et al. 2003) and XSearch (Cohen et al. 2003). XRANK uses a two-dimensional proximity measure and a pagerank-like authority ranking to increase result quality for keyword-based queries; XSearch allows to restrict a keyword-based query to elements with certain tag names, but does not support more complex structural constraints.
 The majority of recent approaches support both keyword-based and structural constraints. Extending existing XML query languages such as XML-QL (Deutsch et al. 1998) or XQuery (Boag et al. 2002) with text search methods has been suggested by Chinenyanga and Kushmerick (2001) Fuhr and Gro X johann (2001) Hayashi et al. (2000) Theobald and Weikum (2000). The simultaneously developed languages XIRQL (Fuhr and Gro X johann 2001) and XXL (Theobald and Weikum 2000) (the latter is our own approach) have been designed to support ranked retrieval. A restricted approach along these lines is Hayashi et al. (2000), which assumes advance knowledge of the document structure and provides similarity search only on element contents, not on element names. The ELIXIR system (Chinenyanga and Kushmerick 2001, Chinenyanga and Kushmerick 2002) provides an e xtension of XML-QL by content-based conditions that are evaluated using the WHIRL system (Cohen 1999). The TeXQuery project (Amer-Yahia et al. 2004), extends XQuery with extensive keyword-based search conditions on the content of XML elements. To our 524 SCHENKEL, THEOBALD AND WEIKUM knowledge, none of the above approaches uses ontological information in their similarity metrics.

Structural similarity of path patterns within XML documents has been investigated by some authors. Schlieder and Meuss (2000) extend the vector space model to term vectors SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 525 consisting of structured terms by generalizing tf*idf-based term weights to structured terms. The language ApproXQL (Schlieder 2002) considers structural similarity between query graphs and XML data graphs based on graph transformation and matching. Guha et al. (2002) apply a tree distance measure to implement approximate joins on XML data. Amer-Yahia et al. (2002) consider a broader set of transformation rules, including ontological-based generalization of tag names (but without discussing the problem of quantifying ontolog-ical relationships), edge generalization, and subtree promotion. However, in all these ap-proaches, ranked results are computed based on the cost of the graph transformations; the general problem of embedding a query tree into a data tree is NP-complete. In contrast, our XXL search engine aims to exploit similarity conditions on element names and contents, which can be evaluated much more efficiently.

Index structures for the support of XML path queries have been intensively studied in the literature. The most efficient path indexes use encoding schemes for trees (e.g., Grust 2002, Zezula et al. 2003), but are inherently limited to tree-structured XML without links. Recently, index structures have been published that support XLinks and XPointers, e.g. APEX (Chung et al. 2002), the Index Definition Scheme (Kaushik et al. 2002), the D ( k ) Index (Qun et al. 2003), HOPI (Schenkel et al. 2004), and the FliX framework (Schenkel 2004). All these approaches don X  X  provide explicit support for ranked retrieval. Our XXL search engine can make use of all of them.

There are many recent papers on XML query processing and optimization. A special focus has been on the efficient evaluation of query twig patterns (see, e.g., Bruno et al. av ariant of Fagin X  X  Threshold algorithm to return only the most relevant results. Another important aspect in the literature have been selectivity estimation for queries on XML (see, e.g., Aboulnaga et al. 2001, Chen et al. 2001, Wu et al. 2002) and statistical summaries of XML data (Polyzotis and Garofalakis 2002a, 2002b). None of these approaches considers similarity search.

The interest in ontologies has been recently revived with the recent discussion about the  X  X emantic Web X . In contrast to the extremely ambitious early AI approaches toward b uilding universal ontologies (see, e.g., Lenat and Guha 1990, Russel and Norvig 1995), publications do not consider the quantification of relationships which was first introduced by Rubenstein and Goodenough (1965) as a result of a manual annotation process. The first automatic approaches for term similarity concentrated on exploiting statistical correlations between terms (Lesk 1969), especially for the problem of query expansion (Qiu and Frei 1993, 1995). Early work on similarity measures for ontologies concentrated on the graph structure of the ontology (Rada et al. 1989, Sussna 1993, Leacock and Chodrow 1998, Wu and Palmer 1994, Hirst and St-Onge 1998, Richardson et al. 1994, Agirre and Rigau 1996, Lewis 2002). Since the mid of the 90ies, researchers started connecting both worlds, statistics on a large corpus (Resnik 1995, Jiang and Conrath 1997, Lin 1998, Resnik 1999). A detailed comparison of similarity measures for WordNet can be found in Budanitsky and Hirst (2001), McHale (1998) and Jamasz and Szpankowicz (2003) compare measures 526 SCHENKEL, THEOBALD AND WEIKUM based on WordNet with similar measures for Roget X  X  Thesaurus (Jamasz and Szpankowicz 2001). 3. Ontology-based similarity Ontologies have been used as a means for storing and retrieving knowledge about the words used in natural language and relations between them. This section presents an overview of our ontological model with its quantified relationships; more details can be found in Schenkel et al. (2003).

In our approach we consider an ontological term t as a pair t = ( w, s ) where w is a word ove ra n alphabet and s is the word sense (short: sense) of w , e.g. t1 = (star, a celestial body of hot gases) t2 = (heavenly body, a celestial body of hot gases) t3 = (star, a plane figure with 5 or more points) which concepts are related, we introduce semantic relationships between concepts that are derived from common sense. We say that a concept c is a hypernym ( hyponym )o fa concept c if the sense of c is more general (more specific) than the sense of c .W e also consider holonyms and meronyms, i.e., c is a holonym ( meronym )of c if c means something that is a part of something meant by c (vice versa for meronyms).

Based on these definitions we now define the ontology graph O = ( V O , E O ) which is a data structure to represent concepts and relationships between them. This graph has concepts between them. In addition, we label each edge with a weight and the type of the underlying relationship. The weight expresses the semantic similarity of two connected concepts.
To fill our ontology with concepts and relationships we use the voluminous electronical thesaurus WordNet (Fellbaum 1998) as backbone. WordNet organizes words in synsets (i.e., sets of words with the same sense) and presents relationships between synsets without any quantification.

Fo r quantification of relationships we consider frequency-based correlations of concepts using large web crawls. In our approach, we compute the similarity of two concepts using correlation coefficients from statistics, e.g. the Dice or Overlap coefficient (Manning and Schuetze 1999). Figure 2 shows an excerpt of an example ontology graph around the first sense for the word  X  X tar X .

Fo rt wo arbitrary nodes u and v that are connected by a path p = u = n 0 ... n k = v , be the product of the weights of the edges on the path: SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 527 this formula is that the length of a path has direct influence on the similarity score. We may additionally restrict the type of edges that are allowed on the path (e.g., allow only hyponym edges to have positive similarity only to more specific concepts). The similarity sim( u ,v ) of two nodes u and v is then defined as the maximal similarity along any path between u and v : However, the shortest path (the path with the smallest number of edges) need not always be the path with the highest similarity. Thus, we need an algorithm that takes into account all possible paths between two given concepts, calculates the similarity scores for all paths, and 528 SCHENKEL, THEOBALD AND WEIKUM chooses the maximum of the scores for the similarity of these concepts. This is a variant of the single-source shortest path problem in a directed, weighted graph. A good algorithm to algorihm (Cormen et al. 2001) that takes into account that we multiply the edge weights on the path and search for the path with the maximal weight instead of minimal weight.
Furthermore, as words may have more than one sense, it is a priori not clear in which sense a word is used in a query or in a document. To find semantically similar words, it is fundamental to disambiguate the word, i.e., to map it to the concept that corresponds to its current sense. In our work we compute the correlation of a context of a given word and the context of a potential appropriate concept from the ontology (e.g., the concepts that contain the word). Here, the context of a word are other words in the proximity of the word in the query or document, and the context of a concept is built from the words of closely related nodes of the concept in the ontologies (like hypernyms and hyponyms). After removing stopwords, we determine the most similar concept out of the candidate concepts by computing the correlation of the contexts using one of the correlation measures discusses above or (as we do in our implementation) by computing the cosine similarity of the corresponding term vectors.

Note that the information in a query may not always be sufficient for a successful dis-ambiguation, for example when a user submits only a single keyword. In such a situation, it may be helpful to ask the user select the  X  X ight X  concept out of the candidates, maybe by presenting her all possible candidates together with a subset of results for the query with that concept. However, we have not yet implemented this solution in the XXL search engine (but the COMPASS search engine (Graupmann et al. 2004) delevoped in our group provides a similar user interaction for disambiguation). 4. The Flexible XML Query Language XXL The Flex ible X ML Search L anguage XXL (Theobald and Weikum 2000, Theobald and W eikum 2002a) has been designed to allow SQL-style queries on XML data. We have adopted several concepts from XML-QL (Deutsch et al. 1998), XQuery (Boag et al. 2002) and similar languages as the core, with certain simplifications and resulting restrictions, and have added capabilities for ranked retrieval and ontological similarity search. As an e xample for an XXL query, consider the following query that searches for publications about both information retrieval and databases: SELECT $T // output of the XXL query FROM INDEX // search space WHERE ~article AS $A // search condition The SELECT clause of an XXL query specifies the output of the query: all bindings of a set of element variables. The FROM clause defines the search space, which can be a set of URLs or the index structure that is maintained by the XXL engine. The WHERE clause specifies SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 529 e xpression is a regular expression over elementary conditions and an elementary condition refers to the name or content of a single element or attribute. Regular expressions are formed The operator  X  %  X  X  sa wildcard for any single element, and  X # X  (which is equivalent to (%)* ) keyw ord AS and a variable name that binds the end node of a qualifying path (i.e., the last element on the path) to the variable, that can be used later on within path expressions, with the meaning that its bound value is substituted in the expression.

In contrast to other XML query languages we introduce a new operator  X   X   X  X  oe xpress semantic similarity search conditions on XML element (or attribute) names as well as on XML element (or attribute) contents. As an example for this, consider the elementary con-dition  X  article in the example query. It is satisfied by elements with the name article , bu t also by other elements with names that are semantically similar to article , like book , report , and so on. The relevance for such elements corresponds to the semantic similarity of their tag name to article as reported by the ontology.

The result of an XXL query is a subgraph of the XML data graph, where the nodes are annotated with local relevance probabilities called similarity scores for the elementary search conditions given by the query. These similarity scores are combined into a global similarity score for expressing the relevance of the entire result graph. See Section 6 for more details on this process. 5. The XXL search engine 5.1. Architecture The XXL Search Engine (Theobald and Weikum 2002b) is a client-server system with a Java-based GUI. It uses an Oracle 9i database for storing XML documents (see Secion 5.3). The architecture of our search engine is depicted in figure 3, it consists of the following core components: 530 SCHENKEL, THEOBALD AND WEIKUM  X  The Crawler : This is a standalone, multithreaded Java application that imports XML and HTML files into the search engine. It can either operate on a local filesystem or on the Web, in both cases following links found within the documents. The documents are decomposed into database tables (see Section 5.3 for details). The crawler uses a simple timestamp-based heuristics to decide if a document that was encountered before has changed in the meantime. Newly found or updated documents are automatically fed to the index subsystems.  X  The Element Content Index (ECI) : The ECI contains all terms that occur in the content of elements and attributes, together with their occurrences in documents; it corresponds to a standard text index with the units of indexing being elements rather than complete documents.  X  The Element Path Index (EPI) : The EPI contains the relevant information for evaluating simple path expressions that consist of the concatenation of one or more element names and path wildcards #.  X  The Ontology Index (OI) : The OI implements the ontology graph presented in Section 3.  X  The Query Processor : This component makes use of the three indexes to efficiently and effectively evaluate XXL queries.  X  The V isual XXL Interface : This Java applet provides a user friendly, browser-based in-terface for formulating XXL queries. Additionally, it presents the XML fragments that are the result of a query.

More details on the index structures are provided in Section 5.2. Important aspects of the query processor are highlighted in Section 6. 5.2. Index structures The XXL Search Engine provides appropriate index structures, namely the element path index (EPI), the element content index (ECI), and the ontology index (OI), that support the ev aluation process. 5.2.1. The ontology index The OI supports finding words that are semantically related in descending order of similarity so that they can be consumed as needed by the query tag names, all edge types may be used, whereas for finding similar content, search may be restricted to hyponym edges to avoid topic drift. Finally, the OI provides methods for disambiguation, i.e., find the current sense of a word among all candidate senses, given the context of the word. 5.2.2. The element content index The ECI supports the evaluation of complex logical atomic formula, the ECI returns elements whose content is relevant with respect to that atomic formula together with a relevance score. The current implementation makes use SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 531 stemming and stopword removal. For a given combination of terms, Oracle Text returns rows that contain all of the terms, in descending order of a tf/idf-based score. Since in our database schema, each row (which is a  X  X ocument X  for Oracle Text) corresponds to element frequency of the terms. This is much more appropriate to XML documents with their explicit structure than simply considering the frequency of a term in a complete XML document (as the standard tf/idf measure does). Note that even though Oracle Text provides ab uild-in ontology, we make use of the OI to expand queries in order to have full control ove r the generated score. 5.2.3. The element path index The EPI provides efficient methods to find children, parents, descendants and ancestors of a given node, to test if two arbitrary nodes are connected, and to determine the distance of two nodes. It provides a general Java interface that in principle allows all existing path indexes to be applied.

When the XML data graph forms a tree, we use the well-known pre-and postorder scheme by Grust (2002) and Grust and van Keulen (2003). This path index computes the pre order pr e ( e ) and the post order post ( e ) for each element e of a single XML document without links, by traversing the document in depth-first order. All XPath axes can be evaluated post ( y ).

However, if the document collection contains links, this scheme can no longer be applied as the XML data graph no longer forms a tree. For such settings that occur frequently with documents from the Web, the XXL Search Engine provides the HOPI index (Schenkel et al. 2004) that utilizes the concept of a 2-hop cover of a graph. This is a compact representation of connections in the graph developed by Cohen et al. (2002). It maintains, for each node v the transitive predecessors and successors of v .F or each connection ( u ,v )i n the XML data graph G ,w e choose a node w on a path from u to v as a center node and add w to L out ( u ) v , hence the name of the method. If we additionally store the distances of u to the nodes in L v among all i  X  I ,o f dist( u , i ) + dist( i ,v ).

As the optimal choice of center nodes is NP-complete, Cohen et al. (2002) apply some well for very large graphs because it requires that all connections in the graph are computed in advance. The HOPI index provides a divide-and-conquer algorithm for building the 2-hop cover of an XML data graph that first partitions the graph into fragments whose connections the cover for the complete graph. More technical details of HOPI, including experimental results with real-life and synthetic data, can be found in Schenkel et al. (2004). 532 SCHENKEL, THEOBALD AND WEIKUM
Fo r heterogeneous XML collections, none of these path indexes is perfectly suited. There-fore, the XXL Search Engine provides the FliX framework (Schenkel 2004) for indexing paths that supports large, heterogeneous document collections with many links, using the e xisting path indexes as building blocks. It first divides the document set into carefully chosen fragments (so-called meta documents). After that, an index is built for each meta document, using the  X  X est X  available indexing strategy given the characteristics of the meta document. XPath axes like descendants or ancestors are then evaluated first on the local indexes (which will probably return the  X  X est X  results, i.e., elements that are connected with short paths). After that, results spanning multiple meta documents are evaluated by following links between meta documents at run-time. 5.3. Implementation issues Our prototype implementation of the XXL Search Engine stores XML data in an Oracle 9i database with the following relational database schema (denoting primary keys as underlined columns): URLS (urlid , url, lastmodified) NAMES(nid , name) NODES(oid , urlid, nid, pre, post, depth) EDGES(oid1 , oid2 ) LINKS(oid1, oid2 ) CONTENTS(oid , urlid, nid, content) LIN (oid1 , oid2 , distance) LOUT(oid1 , oid2 , distance)
Here, NODES , EDGES and CONTENTS store the actual XML data, URLS contains the urls of all XML documents known to the system, and LINKS holds the links between XML element, the corresponding entry in CONTENTS stores the concatenated contents of the 1.1 gigabytes of data which is about two times the size of the original (XML) data. While this replication increases the overall storage usage, it heavily decreases query evaluation time. Additionally, this is the only way to apply Oracle Text to the contents which would be impossible without this kind of replication.

LIN and LOUT store the L in and L out sets used by the HOPI index. Here, a tuple ( v, w, d ) pre and post order numbers as well as the depth of the node in the tree are augmented to the NODES table. The Ontology Index is represented by the following three relational tables: CONCEPTS (cid , concept, description, freq) WORDS (cid , word ) RELATIONSHIPS(cid1 , cid2 , type, freq, weight) SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 533 Currently the index stores hypernym, hyponym, holonym, and meronym edges, but it can be easily extended to support new edge types. The entries in the ontology index are extracted from the well-known electronic thesaurus WordNet (Fellbaum 1998). Frequencies and weights are computed as shown in Section 3. 6. Query processing in the XXL search engine As mentioned in Section 4, an XXL query is of the form SELECT S FROM F WHERE W1 AND ...AND Wk . The evaluation of the search conditions in the WHERE clause consists of the following main steps:  X  The XXL query is decomposed into subqueries W1 ,..., Wk .  X  X  global evaluation order for evaluating the various subqueries is chosen.  X  X  or each subquery, first a local evaluation order for evaluating the components of the subquery is chosen. Then, subgraphs of the data graph that match the query graph are computed, exploiting the various indexes to the best possible extent.  X  The subresults are combined into the result for the original query. 6.1. Query decomposition As an example for an XXL query, consider the following XXL query that asks for the titles of scientific articles about information retrieval and databases: SELECT $T FROM INDEX WHERE ~article AS $A AND $A/~title AS $T AND $A/#/~section ~ "IR &amp; database" The Where clause of an XXL query consists of a conjunction "W1 And ...And Wn" of subqueries Wi , where each subquery has one of the following types:  X  Pi , i.e., a regular path expression,  X  Pi AS $ A , i.e., a regular path expression where the node at the end of the path is bound to variable $ A , content condition. where each Pi is a regular path expression over elementary conditions, $ A denotes a element v ariable to which the end node of a matching path is bound, and condition gives a content X  based search condition using a binary operator. In our example, the first two subqueries are 534 SCHENKEL, THEOBALD AND WEIKUM of the second form, whereas the third subquery is of the third form. From the definitions of variables we derive the variable dependency graph that has an edge from $ V to $ W if the path bound to $ W contains $ V .W e require the variable dependency graph of a valid XXL query to be acyclic. In our example, the variable dependency graph contains an edge $ A  X  $ T because $ A is used in the definition of $ T .

Each subquery corresponds to a regular expression over elementary conditions which can be described by an equivalent non-deterministic finite state automaton (NFSA). Figure 4 shows the automata for the subqueries of the example query and the variable dependency graph for this query. Note that the path wildcard operator # in the third subquery has been converted to the equivalent form (%)* . 6.2. Global evaluation order To ev aluate an XXL query, we first choose an order in which its subqueries are evaluated. This order must respect the variable dependency graph, i.e., before a subquery that defines av ariable is evaluated, all subqueries that define variables used in this subquery must be ev aluated. As this may still leave us some choices how to order subqueries, we estimate the selectivity of each subquery using simple statistics about the frequency of element names and search terms that appear as constants in the subquery. Then we choose to evaluate estimated size of the intermediate result).

In our example, we may compare the frequency of element names that are similar to article with the combined frequency of the terms IR and database in either the whole collection or in the content of elements whose name is similar to section . This is done by first computing all element names with similarity above a given threshold and then querying Oracle Text for the number of elements with these names that contain the terms. As there will probably be much more articles in the collection than sections that contain the terms, SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 535 we may choose to first evaluate the last subquery, i.e., find sections that contain the terms, and then the first subquery, i.e., limit the results to sections within articles. The second subquery can be evaluated last, as it is only needed to compute the results that are returned to the user. 6.3. Subquery evaluation Each subquery is mapped to its corresponding NFSA. A result for a single subquery, i.e. a r elevant path ,i sa path of the XML data graph that matches a state sequence in the NFSA by multiplying the local relevance scores of all nodes of the path. In addition, all variables that occur in the subquery are assigned to one node of the relevant path.

A result for the query is then constructed from a consistent union of the variable as-signments and a set of relevant paths (one from each subquery) that satisfies the variable assignments. The global relevance for such a result is computed by multiplying the local relevances of the subresults.
 subquery X  X  NFSA are matched with elements in the XML data graph. The XXL prototype supports two alternative strategies: in top-down order the matching begins with the start matching begins with the final state(s) and then proceeds towards the start state.
As an example, we show how the NFSA shown in figure 5 is evaluated in top-down order on the data shown in that figure: Step 1: The first elementary search condition contains a semantic similarity search condition on an element name. Thus, we consult the ontology index to get words which are similar to article , yielding the word paper with sim ( paper, article ) = 0.9. The first part of our result graph is therefore an n -node of the data graph named article , and it is assigned a local relevance score of 0.9.
 Step 2: To be relevant for the query, a node from the result set of Step 1 must also have a child node with name bdy .A sa result of Step 2, we consider result graphs formed by such nodes and their respective child.
 Step 3: The next state in the NFSA corresponds to a wildcard for an arbitrary path in the data graph. Explicitly evaluating this condition at this stage would require an enumeration of the (possibly numerous) descendants of candidate results found so far, out of which only a few may satisfy the following conditions. We therefore proceed with the next condition in the NFSA and postpone evaluating the path wildcard to the next step. The following condition is again a semantic similarity condition, so we consult the ontology index to get words which are similar to section . Assume that the ontology index returns the word sec with a similarity score of 0.95. There are no n-nodes in the data that are named section ,b ut we can add n-nodes named sec to our preliminary result with a local relevance score of 0.95.
 Step 4: In this step we combine the results from steps 2 and 3 by combining n-nodes that are connected through an arbitrary path. The local relevance score for the results corresponds 536 SCHENKEL, THEOBALD AND WEIKUM to the inverse of the length of the path between the nodes: the longer the path is, the lower the relevance score.
 Step 5: The final state of the NFSA contains a content-based semantic similarity search condition which must be satisfied by the content of a sec -element in the result set of
Step 4. We first decompose the search condition that may consist of a conjunction of search terms into the atomic formulas (i.e., single terms). For each atomic formula we consult the ontology index for similar words and combine them in a disjunctive manner.
We then use a text search engine to evaluate the relevance of each element X  X  content which is expressed through an tf/ief-based relevance score. This score is combined with the ontology-based similarity score to the relevance score of the atomic formula. Finally, similarity condition.
 In our example, the shaded nodes in figure 5 form a relevant path for the given NFSA. SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 537 7. Experimental results with INEX In this section, we present the results of our XXL search engine with the INEX bench-W eikum (2002a). We start with a short summary of the INEX benchmark in Section 7.1. In Section 7.2, we present the results with keyword-only queries ( X  X O X -topics in INEX terminology), showing the overall quality of our approach as well as the additional benefit from applying ontology-backed query expansion. In Section 7.3, we discuss the results for queries with constraints on both content and structure ( X  X AS X -topics). 7.1. INEX overview The INEX benchmark (Kazai et al. 2003) is an excellent testbed for information retrieval engines on XML data. It provides a large collection of XML documents with rich textual components, two sets of query topics (with and without structural constraints), and several metrics for evaluating the results.
 topic. Then the potentially relevant components from each pool are assessed by a human who assigns an e xhaustivity v alue and a specificity v alue which are both in the range 0 X 3. Exhaustivity describes the extent to which the component discusses the topic of request, specificity describes the extent to which the component focusses on the topic of request. call/precision metrics is applied. In order to apply this metric, the assessors X  judgements have to be quantised onto a single relevance value. Two different quantisation functions have been used: 1. Strict quantisation is used to evaluate whether a given retrieval approach is capable of 2. In order to credit document components according to their degree of relevance (gener-538 SCHENKEL, THEOBALD AND WEIKUM
Using one of these quantisation functions, each document component in a result set is assigned a single relevance value using the human X  X ased relevance assessment. 7.2. CO-topics To automatically transform a CO-topic into an XXL query we consider the keywords given for the query. As there is no way to automatically decide how to combine these keywords (conjunctively, disjunctively or mixed) in an optimal manner, we chose to combine them conjunctively. To include results that are semantically similar to the keywords, we add our similarity operator  X  .F or CO-topic 98 with keywords information exchange , XML and information integration , this process yields the following XXL query: SELECT * FROM INDEX WHERE article/# ~ "(information exchange)
At query evaluation, each keyword in the query is (conceptually) replaced by the dis-junction of itself and all its related terms: SELECT * FROM INDEX WHERE article/# ~ ("information exchange" | "data exchange" | "heterogeneous data") &amp; ("XML" | "semistructured data") &amp; ("information integration" | "information sharing")
Fo r the unexpanded query we obtain 7 results with an average precision of 0.0002 for the strict quantisation and with an average precision of 0.0043 for the generalized quantisation. Fo r the expanded query we obtain 28 results with an average precision of 0.0002 for the strict quantisation and with an average precision of 0.0065 for the generalized quantisation. So even this straightforward query expansion helped to slightly increase result quality.
However, if we carefully look at the topic, it turns out that a recombination of the query ke ywords could return better results. (Note that such an optimization is not allowed for the  X  X fficial X  INEX runs as all queries had to be generated automatically.) Thus, we reformulate the unexpanded query: ("information exchange" | "information integration") &amp; "XML"
The expanded query then has the following structure: (("information exchange" | "data exchange" | "heterogeneous data") | SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 539 ("information integration" | "information sharing")) &amp; ("XML" | "semistructured data")
Figure 6 shows the precision-recall curves with the strict quantization for CO-topic 98 figure 7 shows the precision-recall curves with the generalized quantization for this topic with reformulation, but without expansion (left) and with query expansion (right). For both metrics, the expanded queries have a much higher average precision, and they deliver much more relevant results than the unexpanded queries. However, especially for the generalized 540 SCHENKEL, THEOBALD AND WEIKUM metrics, recall is still quite low; we attribute this to the fact that many elements have been assessed as (to some extent) relevant that don X  X  contain all or even any of the ontologically e xpanded keywords. As XXL uses conjunctive keyword conditions, it considers all those elements as non-relevant, leaving them out of the result list.

Fo r the complete set of all 36 CO-topics, 1 the results clearly indicate that ontological curves with strict quantisation for our search Engine (drawn in bold) in comparison to the others that participated in INEX, figure 9 shows them with generalized quantization. Note that the absolute average precisions are quite low for all systems; this is mostly due to the SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 541 difficulty to select the right result granularity (depending on the topic, an article element may not be a good result even though it contains all keywords, because its content is too general; analogously, the content of a section element may be too narrow even though containing all keywords. Choosing the right granularity for the result is currently an active research problem in XML retrieval.

Our search engine took about 15 minutes to generate the results for all 36 CO-topics; this time was dominated by the queries to OracleText and the generation of the results in the INEX format.

The results clearly show that ontology-based query expansion for keyword-based XML retrieval provides much better average precision and better recall. 7.3. CAS-topics CAS-topics are represented by an expression in an XPath-like query language. We map this e xpression in a straightforward way to a corresponding XXL expression, adding semantic similarity conditions to all element names and keywords that appear in the XPath expression. Fo r CAS-topic 63, the query //article[about(., "digital library")] is mapped to the following XXL query: SELECT $A FROM INDEX WHERE article AS $A 542 SCHENKEL, THEOBALD AND WEIKUM AND $A ~ "digital library"
AND $A/#/p ~ "authorization &amp; (access control) &amp;
As an example for the ontology-based query evaluation on CAS-topics, figure 10 shows the generalized results for CAS-topic 63 with the original query (left) and with the expanded query (right). The strict evaluation results in an average precision of 1.0 in both cases. This experiment shows that the XXL search engine is able to evaluate conditions on XML structure as well as conditions on XML contents. In addition, the ontology X  X ased query expansion for the content condition provides much better average precision and better recall. Due to the regular structure of the INEX data, XXL could not make much use of its structural similarity features. We expect that INEX X  upcoming heterogeneous data track will allow us to demonstrate the strength of XXL in that field. 8. Conclusions and future work Querying large collections of highly heterogeneous, richly structured XML documents is an important task where existing XML query languages like XQuery and keyword-based search engines are failing. The XXL Search Engine presented in this article is an important step towards solving this problem. Combining keyword-based search with structural conditions and semantic similarity, its query language XXL is more expressive than all existing query languages for XML. The results obtained with the INEX benchmark clearly indicate that e xploiting semantic similarity generally increases the quality of search results.
Our ongoing and future work includes generalizing query semantics, exploiting user feedback, and further optimizing query performance. More specifically, we plan to gen-eralize the semantics of the path wildcard operator # to include more general notions of connectivity like those discussed by Amer-Yahia et al. (2002). To further extend the result quality, we plan to add a relevance feedback step to incrementally increase the quality over time, including a user-specific personal ontology. Finally, we plan further studies on query optimization heuristics. This includes finding good global and local evaluation ordering of subqueries and elementary search conditions based on selectivity estimations as well as algorithms to quickly return the best results for the query (without having to compute all results first).
 Note References SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 543 544 SCHENKEL, THEOBALD AND WEIKUM SEMANTIC SIMILARITY SEARCH ON SEMISTRUCTURED DATA 545
