 Ata Ebrahimzadeh, Reza Ghazalian n 1. Introduction
Automatic digital communication signal recognition is a technique that recognizes the type of the received signal at the receiver. It plays an important role in military and civil domains.
For example, in military applications, it can be employed for electronic surveillance, interference recognition and monitoring.
The wide range of civilian applications includes spectrum management, network traffic administration, signal confirmation, software radios, intelligent modems, cognitive radio, etc. Due to the increasing usage of digital signals in novel technologies such as software radio, the recent researches have been focused on identifying these signal types.

Generally, the digital signal type identification methods fall into two main categories: decision theoretic (DT) methods and pattern recognition (PR) methods. DT methods use probabilistic and hypothesis testing arguments to formulate the recognition problem ( Sue et al., 2008; Nolan et al., 2002; Wei and Mendel, 2000 ). The major drawbacks of DT methods are their very high computational complexity, lack of robustness to the model mismatch as well as careful analysis, which are required to set the correct threshold values ( Hsue and Soliman, 1990 ). The PR methods, however, do not need such careful treatment. They are easy to implement. The PR methods can be further divided in two main subsystems: the feature extraction subsystem and the classifier subsystem. The former extracts the features (e.g. histograms, spectral characteristics, instantaneous characteristics, combination of second and fourth order moment, symmetry, etc.), and the latter determines the membership of signal (e.g. neural networks, K-nearest neighbor, fuzzy logic classifier, etc.) ( Hsue and Soliman, 1990; Mobasseri, 2000; Azzouz and Nandi, 1995; Swami and Sadler, 2000; Lopatka and Macrej ,2000; Chani and Lamontagne, 1993; Hagedorn et al., 1997; Nandi and Azzouz, 1998; Louis and Sehier, 1994; Mingquan et al., 1996; Zhao et al., 2003; Mustafa and Dorolovacki, 2004; Avci et al., 2007; Zadeh and Aredebilipour, 2008; Zadeh and Seyedin, 2006; Zadeh and Movahhedi, 2007 ).
From the published works, it appears clear that in the design of a system for automatic recognition of digital signal type, there are some important issues, which, if suitably addressed, lead to the development of more robust and efficient recognizers. One of these issues is related to the choice of the classification approach to be adopted. Literature review shows that despite its great potential, the application of different supervised classifier has not received the attention it deserves in the modulation classification. Therefore, in this paper we investigated the performances of: (a) several multilayer perceptron (MLP) neural networks with multiple training algorithms, (b) radial basis function (RBF), (c) probabilistic neural network (PNN), (d) support vector machine (SVM). Choosing the right feature set is still another issue. In this paper, a suitable set of the instantaneous character-istics, the higher order moments up to eighth and the higher order cumulants up to eighth are proposed as the effective features. Turning back to the digital signal recognition systems,found that: (1) feature selection is not performed in a completely automatic way and (2) the selection of the best free parameters of the adopted classifier are generally done empiri-cally (model selection issue). Another issue that is addressed in this paper is optimization. Using the optimizer, the most appropriate parameters of the classifier and the best feature subset are selected.

The rest of the paper is organized as follows: Section 2 presents the feature extraction. Section 3 describes the classifier. Section 4 explains the optimization of the recognizer. Section 5 shows some simulation results. Finally, Section 6 concludes the paper. 2. Feature extraction
There are four main types of digital modulation: FSK, ASK, PSK and QAM ( Proakis, 2001 ). These different types of modulation have their recognition of is a critical problem. In this paper we have considered a variety of communication signals: FSK2, FSK4, ASK4, ASK8, PSK2, PSK4, PSK8, QAM8, QAM16, QAM32, QAM64 and V32.
To simplify the indication, these signals are substituted with P ,
P ,P 4 ,P 5 ,P 6 ,P 7 ,P 8 ,P 9 ,P 10 ,P 11 , and P12, respectively.
Based on the extensive experiments and researches, a suitable set of the instantaneous characteristics, the higher order moments up to eighth and the higher order cumulants up to eighth are proposed as the effective features. These features can be used to define the characteristics of the probability density function of the signal. The behavior of the higher order moments and cumulants to various transformations is an important factor in determining how useful these quantities may be to characterize signals in systems. The only effect of translation on the received signal is only the mean changes. The variance and all the higher order moments or cumulants remain unaffected. The rotation of the received signal X  X  constellation, due to multipath or other distor-tions, affects the relative variances and higher order moments or cumulants, though certain other parameters such as the eigenva-lues and the covariance matrix are invariant to rotation. Following phrases describe briefly these features. 2.1. Higher order statistics
Probability distribution moments are a generalization of concept moment of a random variable is given by ( McCullagh, 1987 )  X  where m is the mean of the random variable, and f (.) the probability density function of random variable s .Thedefinitionforthe i th moment for a discrete-valued signal with finite number of levels is given by  X  where N is the data length. In this study signals are assumed to be zero mean. Thus  X  as follows:
M pq  X  E  X  s p q  X  s  X  q  X  4  X  where p is called the moment order and s * stands for complex conjugation of s . Assume a zero-mean discrete signal sequence of the form s  X  a + jb , using the definition of the auto-moments, the expressions for different orders may be easily derived. istic function ^ f  X  t  X  X  E f e jts g X  5  X 
Taylor series, one obtains log ^ f  X  t  X  X  k 1  X  jt  X  X  X  k r  X  jt  X  distribution) of s . The symbolism for p th order of cumulant is similar to that of the p th order moment. More specially
C pq  X  Cum s , ... , s |fflffl{zfflffl} up to (and including) n Cum  X  s 1 , ... , s n  X  where the summation index is over all partitions v  X  ( v 1 for the set of indices (1,2, y , n ), and q is the number of elements in a given partition. Assume n  X  3. In such a case, the available set (2), (3)} leading to q  X  3. Therefore can be computed. For example digital communication signals that are considered. Table 1 shows the some of the theoretical values of the higher order statistics for a number of the considered digital signal types. These values are computed under the constraint of unit variance in noise free and normalized by theoretical signal power, i.e., these values are obtained assuming the signal is clean and of infinite length.
However, in practice, signals are usually subject to some type of distortion, either inside the transmitter or during transmission, and are of finite length. Figs. 1 and 2 show some of the higher order features for a number of the considered digital signal types. 2.2. Instantaneous features
Instantaneous features are suitable for signals which contain hidden information in a single domain, instantaneous amplitude, instantaneous phase or instantaneous frequency ( Lopatka and
Macrej, 2000 ). In this paper, we have used the following instantaneous features for discrimination of FSK signals: af : standard deviation of the absolute value of the normal-ized-centered instantaneous frequency over non-weak segments of the intercepted signal where f N  X  i  X  X  f c  X  i  X  = r s , f c ( i )  X  f ( i ) m f bol rate of digital sequence, N sf the number of samples in { f This feature could differentiate between the modulation schemes without frequency information and the FSK modulation schemes and also between FSK2 and FSK4. s fn : standard deviation of the direct value of the normalized-centered instantaneous frequency, evaluated over the over non-weak segments of the intercepted signal fn is used to discriminate between FSK2 and FSK4 signals. 3. Classifier In this paper, several supervised classifiers have been used.
Following phrases describe briefly these classifiers. 3.1. MLP neural networks
An MLP neural network consists of an input layer (of source nodes), one or more hidden layers (of computation nodes) and an output layer ( Haykin, 1999 ). The issue of learning algorithm and its speed is very important for MLP. Among the learning algorithms of MLPs, back propagation (BP) algorithm is still one of the most popular algorithms. In BP a simple gradient descent algorithm updates the weight values w  X  t  X  1  X  X  w ij  X  t  X  e @ E @ w where w ij represents the weight value from neuron j to neuron i , e is the learning rate parameter, E represents the error function.
However, under certain conditions, the BP network classifier can produce non-robust results and easily converge to local minimum.

BP with momentum (BPM) ( Rumelhart and McClelland, 1986 ) makes use of gradient descent with a momentum term to smooth out oscillation. It adds an extra momentum parameter, m to the weight changes. Eq. (14) gives the weight update for BP with momentum
D w ij  X  t  X  1  X  X  e d E d w Quick prop (QP) algorithm does not need much computation. QP calculates the update for each weight separately using a
Newton-like method to minimize the error in that weight dimension ( Fahlman, 1988 ). The weight update rule for QP is given by following expression w  X  t  X  1  X  X  w ij  X  t  X  X  D w ij  X  t  X  X  15  X 
D w ij  X  t  X  X  g ij  X  t  X  g g  X  t  X  X  where w ij represents the weight value from neuron j to neuron i , E represents the error function. There are some heuristics that are decay, given by g  X  t  X  X  g ij  X  t  X  X  0 : 0001 w ij  X  t  X  X  18  X 
Also the derivative of the sigmoid function f is altered to be f u  X  o j  X  1 o j  X  X  0 : 1  X  19  X 
The Levenberg X  X arquardt (LM) algorithm ( Hagan and Menhaj, 1994 ) uses the approximation to the Hessian matrix in the following Newton-like update: w  X  t  X  1  X  X  w ij  X  t  X  X  J T J  X  m I 1 J T e  X  20  X  where J is the Jacobian matrix, e a vector of network errors and m a constant. 3.2. Radial basis function neural networks training efficiency are a good candidate to perform a nonlinear mapping between the input and the output vector spaces. RBFNN is a fully connected feed forward structure and consist of three layers namely, an input layer, a single layer of nonlinear processing units, and an output layer. The network structure is shown in Fig. 3 . Input layer is composed of input nodes that are equal to the dimension of the input vector x . The output of the j th hidden neuron with Gaussian transfer function can be calculated as where h j is the output of the j th neuron, x A R n 1 is an input vector, c j A R n 1 is the j th RBF center, s the center spread parameter, which controls the width of the RBF, and : U : 2 represents the Euclidean norm. The output of any neuron at the output layer of RBF network is calculated as y i  X  where w ij is the weight connecting hidden neuron j to output neuron i and k the number of hidden layer neurons.
 the weights in the output layer, the centers of the RBFs and spread parameter of the Gaussian function. 3.3. Probabilistic neural networks suitable for classification problems. An example of a probabilistic neural network is shown in Fig. 4 . PNN networks have three layers: input, pattern and summation. The input layer has as many elements as there are individual parameters needed to describe the samples to be classified ( Specht, 1990 ). The pattern layer organizes the training set such a way that an individual processing element represents each input vector.

The pattern units in the probabilistic network are used to store pattern examples, taken directly from the training data. The entire set of training data is used, and so the number of units in the first hidden layer is set equal to the number of training cases. The summation layer has as many processing elements as there are classes to be recognized and simply collects the outputs from all hidden neurons of each respective class. The products of the summation layer are forwarded to the output (one neuron for each data class), where the estimated probability of the new pattern being a member of that data class is computed. The transfer function is radial basis function for the first layer and is competitive function for the second layer. Only the first layer has biases.

Training of the probabilistic neural network is much easier than with back-propagation. It can be simply finished by setting the weights of the network using the training set. The outputs of summary layer are binary values. If y i is larger than input of other neurons (which means that this input pattern belongs to class i ), y is set to 1, otherwise it is set to zero. 3.4. Multi-class SVM-based classifier
Support vector machine (SVM) is a supervised machine learning method that was originally developed for binary classification problems, but it can be used for multi-class classification ( Burges, 1998 ). Following phrases, briefly, describe the binary SVM classifier and multi-class SVM-based classifier.
Binary SVM performs classification tasks by constructing optimal separating hyper-planes (OSH). OSH maximizes the margin between the two nearest data points belonging to two separate classes. Suppose the training set, ( x i , y i ), i  X  1,2, y
A { 1,+1} can be separated by the hyper-plane w T x + b  X  0, where w ! is the weight vector and b the bias. If this hyper-plane maximizes the margin, then the following inequality is valid for all input data: y  X  w x i  X  b  X  Z 1 , for all x i i  X  1 , 2 , ... , l  X  23  X 
The margin of the hyper-plane is 2 = : w ! : . Thus, the problem is the maximizing the margin by minimizing : w : 2 subject to (23). This is a convex quadratic programming (QP) problem and
Those training points, for which the equality in (23) holds, are called support vectors (SV) that can satisfy a i g 0.
For input data with a high noise level, SVM uses soft margins that can be expressed as follows with the introduction of the non-negative slack variables x i , i  X  1, y , l : y  X  w x i  X  b  X  Z 1 x i for i  X  1 , 2 , ... , l  X  24  X 
To obtain OSH, it should be minimizing the F  X  1 2 : w : 2
C S
In the nonlinearly separable cases, the SVM maps the training points, nonlinearly, to a high-dimensional feature space using
Some of the kernel functions of SVMs are as follows:(1) Gaussian radial basis function (GRBF): K  X  x i , x j  X  X  exp  X  : x i h  X  g x ! i U x functions.

The performance of SVM can be controlled through the term C and the kernel parameter, which are called hyper-parameters. These parameters influence the number of the support vectors and the maximization margin of the SVM.

There are two widely used methods to extend binary SVMs to one-against-all (OAA) method. Suppose we have a P -class pattern recognition problem, P -independent SVMs are constructed and each of them is trained to separate one class of samples from others. When testing the system after all the SVMs are trained, a sample is input to all the SVMs. Suppose this sample belongs to class P only the SVM trained to separate class P 1 from others can have a positive response. Another meth od is called the one-against-one (OAO) method. For a P -class problem, P ( P 1)/2 SVMs are con-structed and each of them is trained to separate one class from result of these SVMs. In this paper we have used the OAA method. 4. Swarm intelligence and optimization
Although all of these features may carry good classification information when treated separately, there is little gain if they are combined together (due to the sharing the same information content). Then we should select the suitable features subset. On the other hand, selection of the optimum values of the classifier can improve its performance. Optimization of parameter settings for a pattern classifier is usually carried out with a human in the loop-choosing a set of parameter values, training the classifier and then determining its performance on a set of test data. This is a very time-consuming process and, when the number of parameters is moderately large, it precludes the exploration of more than a tiny fraction of parameter space. The idea here is to employ a particle swarm optimization (PSO) to provide a broader and more efficient search of parameter space so as to obtain close to optimum performance. Therefore in this paper we have used the PSO for feature subset selection and model selection simultaneously. 4.1. PSO
Particle swarm optimization (PSO) ( McCullagh, 1987 )isan emerging population-based meta-heuristic that simulates social behavior such as birds flocking to a promising position to achieve precise objectives in a multidimensional space. PSO performs searches using a population (called swarm) of individuals (called particles) that are updated from iteration to iteration. Each individual is considered as a volume-less particle (a point) in the N -dimensional search space. At step t , the i th particle is represented as X i ( t )  X  ( x i 1 ( t ), x i 2 ( t ), y , x position (the position giving the best fitness value) of the i th particle is recorded and represented as P i ( t )  X  ( p i 1 The index of the best particle among all the particles in the population (global model) is represented by the symbol g . The index of the best particle among all the particles in a defined topological neighborhood (local model) is represented by the index subscript l . Likewise, the velocity (i.e., distance change), which is also an N -dimension vector, for particle i at iteration t can be described as V i ( t )  X  ( v i 1 ( t ), v i 2 ( t ), ality is measured by means of one or more fitness functions defined in relation to the considered optimization problem. During the search process, the particles move according to the following equations: v  X  t  X  X  w i v in  X  t 1  X  X  c 1 rand 1  X  X  x  X  t  X  X  x in  X  t 1  X  X  v in  X  t  X  X  26  X  where n is the dimension (1 r n r N ), c 1 and c 2 are personal and social learning factors, rand 1() and rand 2() are two random functions in the range [0,1], and w is the inertia weight. For the neighborhood ( l best) model, the only change is to substitute p for p gn in equation for velocity. This equation in the global model is used to calculate a particle X  X  new velocity according to its previous velocity and the distance of its current position from its own best experience ( p best) and the group X  X  best experience ( g best). 4.2. Optimization of the recognizer
As mentioned we use the PSO for feature subset selection and model selection simultaneously. The choice of the fitness function is an important issue, because it is on this basis that the PSO evaluates the goodness of each candidate solution for designing our recognition system. In this paper we have used the recognition accuracy as the fitness function
Fitness function  X  y t y where y t and y f denote the number of true and false classifica-tions, respectively. Another problem is the particle representa-tion. This problem is related to the classifier and its parameters that should be optimized. Therefore the particle is comprised of multiple parts: the input features, and the classifier parameters. Fig. 4 illustrates the representation of a sample particle where the
SVM with RBF kernel is selected as the classifier. The classifier parameters are: C and g . The dimension of particle is n +2, where n is the total number of the input features (variables) of a data set.
The value of n variables ranges between 0 and 1. If the value of a variable is less than or equal to 0.5, then its corresponding feature is not chosen. Conversely, if the value of a variable is greater than 0.5, then its corresponding feature is chosen ( Fig. 5 ).
The procedure describing the proposed optimization system for the basic discrimination case between two classes is as follows: 1) Initialization: 5. Simulation results assumed that carrier frequencies were estimated correctly (or to be known). Each signal used in this study was generated using
MATLAB. We assumed that carrier frequencies were estimated correctly and the signals heterodyned down. Thus, we only considered complex baseband signals. The carrier frequency is equal to 10.7 MHz, sample frequency is 85.4291 kHz and the symbol frequency is 10 kHz. A total of 3000 samples per signal type were created and stored. The digital information (message) is generated randomly for every trial to ensure results are independent of the message transmitted. Estimating moment and cumulant values for all signal types are based on the theoretical formulas that are explained in Section 2. For this process, only the moments and cumulants that show some special characteristics as class features are selected. The estimation is done on a subset of 600 samples per scheme, out of the total 3000 samples per scheme data set. Two different cases are examined.
First, the signals are generated noise-free. Second, the signals are distorted by additive white Gaussian noise (AWGN) according to
SNRs, 3, 0, 3 and 6 dB. 5.1. Performance of different classifiers different classifiers. In this step we have used all of the features.
First, we have compared the performances of different learning algorithms of an MLP neural network. Therefore we have considered two structures: NN1 (with single hidden layer) and
NN2 with two hidden layers. NN1 has single hidden layer with 40 neurons. NN2 has two hidden layers with 60 40 neurons. Other parameters for these architectures are the same. Tables 2 and 3 show the recognition accuracies of these neural networks with different training algorithms. It can be seen that for NN1 and NN2 structures, the MLP with QP learning algorithm has the higher recognition accuracy (RA) than others. However, this performance is weak at lower SNRs. The LM algorithm has better performance than others. Increasing the number of hidden layers causes the RA to be better especially in the lower SNRs.
 classifiers. Table 4 shows the performance comparison of RBF (with 40 neurons), PNN (with 40 neurons) and SVM (with GRBF kernel). It can be found that SVM outperforms other classifiers at all the SNRs, especially at low SNRs.
 theory. These cause it to have high accuracy and excellent generalization capability. The RBF classifier is located at the later row, i.e., its recognition accuracies are higher than other neural network classifiers. When the SNR is low, neural network classifiers show poor performance while in higher SNR the accuracies are higher. The construction of neural network in low SNRs is not proper, which results in low generalization ability. In higher SNRs, the features are proper and closer to the noiseless state and it is easier to construct the neural network and results in high recognition probability.

In order to compare the performance of SVM with different kernels, we have considered the penalty parameter C  X  10, and the kernel parameters are selected as s  X  1, d  X  3, g  X  0.1 and Z  X  2. These values are selected based on the try and error method. Table 5 shows the recognition accuracies of the recognizer with different kernels at each SNR. From these results it can be seen that the performances of the recognizer with different kernels are generally very good even at very low SNRs. This is due to two facts: proposed features and proposed classifier. The chosen features have effective properties in signal representation. On the other hand, the SVM-based classifier has high generalization ability for classification of the radio signals even at low SNRs. Also, it can be found that the classifier with GRBF kernel has higher recognition accuracy than others. Then we have chosen the classifier with this kernel as the main classifier of the proposed technique and optimize this classifier in order to improve its performance. Fitness Function Evaluation Digital modulated signal dataset
The feature subset represented by particle
It should be mentioned that the percentage of RA at each SNR is the mean values that are located in the diagonal of confusion matrix of each SNR. The confusion matrix is a table summarizing the tendency of the recognizer to classify a recognized modulation into a correct class or into any of the other possible (wrong) classes. For example, the confusion matrix of the recognizer with
GRBF kernel at SNR  X  0 dB is given in Table 6 . It can be found that there is a tendency for P 1 modulation to be mostly confused with
P and P 5 modulations, P 2 modulation with P 1 modulation, etc. P and P 4 modulations are the hardest to be classified (92%), followed by P 8 modulation (93%). 5.2. Performance of hybrid intelligent system
We have shown in the previous results the good performances generally achieved with the nonlinear SVM classifier based on the
Gaussian kernel. Then we describe the proposed optimization system with this particular kernel. The particle therefore, is comprised of hyper-parameters of GRBF kernel and features subset. Therefore we apply PSO for finding the optimum penalty parameter ( C ) and kernel parameter ( s ), and features subset.
Selection of the parameters of SVM is an optimization problem with constraints. The research space of these parameters is C
A [1:1000], s A [0.01:0.001:1000]. The number of features varies from 1 to 28. In this section we find the effect of the optimization of the parameters.

Concerning the PSO algorithm, we considered the following: swarm size  X  50, inertia weight  X  0.6, acceleration constants equal to the unity and maximum number of iterations fixed at 500, size of local neighborhood equal to 2. These values usually are determined by the trial and error method.

Tables 7 X 9 show the performance of the hybrid intelligent system at each SNR for different feature subsets selected by the optimizer. Comparison between these tables indicates that: (a) performance of the four features selected by the optimizer is less than the performance of the system with seven features and eighteen features, (b) difference in the performances of the recognizer with seven features (selected using the optimizer) and 20 features (selected using the optimizer) is negligible. Therefore it can be said that selection of the seven features using the optimizer provides the desirable performance and that there is no need to consider more features. The important features that have important roles for recognition are: C 40 ,M 41 ,M 61 ,C 63 and C 82 .

It states that the optimized recognizer has a recognition accuracy higher than 98% for SNR 4 0 dB. It can be found that the optimization, generally, improves the performances of the identifier for all SNRs; especially at low levels of SNR. Table 8 shows the confusion matrix of optimized recognizer at SNR  X  0 dB.
Comparison between Tables 10 and 6 states that the optimization improves the recognition of the modulations that their recogni-tion was hard. 5.3. Analysis of the effect of parameters of swarm intelligence
As mentioned, the selection of the PSO parameters is an important issue. We have analyzed the sensitivity of the hybrid intelligent PSO X  X VM system with respect to the inertia weight w and the two acceleration constants c 1 and c 2 , which control the behavior, and thus, the goodness of the PSO search process. In the first step, we fixed c 1 and c 2 to 1.6 and we varied w in the range [0,1]. In order to compare the performance of the recognizer, we have considered the recognition accuracy at the lowest SNR.
Table 11 shows the results. It is found that the best and the worst classification accuracies were obtained for w  X  0.6 and w  X  0.8, respectively. In the second step, we fixed w  X  0.6 (corresponding to the best obtained accuracy) and we varied c 1 and c 2 in the range [1,2]. Table 12 shows the results. In this case, the recognition accuracy was less affected by the variation in these parameters. Indeed, it fluctuated from 95.50% for c 1  X  c 95.05% for c 1  X  c 2  X  1.2. As this empirical analysis shows, the PSO optimizer appears more sensitive to the inertia weight parameter than the other two parameters. However, even when nonstandard parameter values are adopted, the achieved accuracies keep still above those yielded by the reference classifiers. 5.4. Performance comparison
As mentioned in Mobasseri (2000) , direct comparison with other works is difficult in signal type recognition. Table 13 shows the comparison among the important previous papers and the hybrid proposed system. In comparison with other works, the proposed recognizer has many advantages. This system includes a variety of digital signal types. Because of the proposed feature set and structure of the classifier, performance of the system is very high even at very low level of SNRs. Using the optimization technique improves efficiently the performance of the recognizer. The optimized recognizer has a success rate of around 95% at SNR  X  3 dB. The performances of the recognizer are higher than 98% for SNR 4 0 dB. These performances are achieved with fewer samples. 6. Conclusion
Automatic recognition of digital modulations plays an im-portant role in various applications such as software radio. This paper has studied the process of designing of an accurate system for recognition of digital modulations, from simple structure to complex constellation. In this study, the features and the classifier effect on the recognition accuracy are shown. We have studied the performance of different neural networks. It is concluded that there are limitations on generalization ability in low SNRs. For example, the best RA is achieve by RBF neural network around 87% at SNR  X  3 dB. Then we investigated the performance of support vector machines that are based on statistical learning theory. It is observed that this type of classifier achieves RA around 90% at SNR  X  3 db. Therefore in low levels of SNR the performance of SVMs are better than other classifiers. However, in higher SNRs the accuracies are higher. The construction of neural network in low SNRs is not proper, which results in low generalization ability. In higher SNRs, the features are proper and closer to the noiseless state and it is easier to construct the neural network and results in high recognition probability. In the next step we have optimized the structure of recognizer and proposed a hybrid intelligent system. The results indicate that optimization improves the performance of recognizer signifi-cantly. For example, around 95% RA is achieved at SNR  X  3dB and 98% at SNR  X  0 dB. It is worth that these recognition accuracies are achieved with lowest samples and feature subsets. References
