 Effective ranking functions are an essential part of com-mercial search engines. We focus on developing a regres-sion framework for learning ranking functions for improv-ing relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of rel-evance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user clickthroughs of search results or converted from the absolute relevance judgments. We propose a novel optimiza-tion framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objec-tive functions involving prefer ence data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods.
 H.3.3 [ Information Systems ]: Information Search and Retrieval X  Retrieval functions ; H.4.m [ Information Sys-tems ]: Miscellaneous X  Machine learning Algorithms, Experimentation, Theory ranking function, machine learni ng, absolute relevance judg-ment, relative relevance judgment, preferences, clickthroughs, functional gradient descent, re gression, gradient boosting Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Research and experiments in information retrieval have produced many fundamental methodologies and algorithms enabling the technological advances in the current commer-cial search engines. Ranking functions are at the core of search engines and they directly influence the relevance of the search results and users X  search experience. In the past, many models and methods for designing ranking functions have been proposed, including vector space models, prob-abilistic models and the more recently developed language modeling-based methodology [17, 16, 2]. In particular, learn-ing ranking functions within the framework of machine learn-ing have attracted much interest long before the recent ad-vances of Web search [10, 6, 5, 11, 22, 19]. The trend con-tinues to this day and several methods have been proposed incorporating many of the recent advances in machine learn-ing such as SVM and gradient boosting [7, 4, 19].
Machine learning approaches for learning ranking func-tions, in particular, the supervised learning approaches, en-tails the generation of training data, in the form of labeled data explicitly constructed from relevance assessment by hu-man editors. As an example, labels or grades such as perfect, good, or bad, can be assigned to documents with respect to a query indicating the degree of relevance of documents. With labels associated with query-document pairs, we are using the absolute relevance framework where judgments are made with respect to whether a document is or is not relevant to a query. Acquiring large quant ities of absolute relevance judgments, however, can be very costly because it is neces-sary to cover a diverse set of queries in the context of Web search. An additional issue is the reliability and variability of absolute relevance judgments.

One possibility to alleviate this problem is to make use of the vast amount of data recording user interactions with the search results, in particular, user clickthroughs data [1]. Each individual user click may not be very reliable, but the aggregation of a great number of user clicks can provide a very powerful indicator of relevance preference. In this regard, Joachims and his coworkers have developed meth-ods for extracting relative relevance judgments from user clickthroughs data [13, 14, 20, 15, 21]. Particularly, the relative relevance judgments are in the form of whether a document is more relevant than other documents with re-spect to a query. The benefit for using relative relevance judgments are the potential unlimited supplies of user click-throughs data and the timeliness of user clickthroughs data for capturing user searching behaviors and preferences. The drawback for using relative relevance judgments are that user clickthroughs data tend to be quite noisy, especially we also need to deal with fraudulent clicks. Although there have been some research on how to extract relative relevance judgments from user clickthroughs data, much research is still needed to make the extraction process more effective.
Once relative relevance judgments are extracted from user clickthroughs data, the next question is how to use them for the purpose of learning a ranking function. This falls under the general framework of learning ranking functions from preference data and several algorithms have been proposed in the past: Joachims and his coworkers used RankSVM based on linear SVM for learning ranking functions. To in-corporate nonlinear interactions of features in RankSVM, ei-ther more complicated features need to be devised or some kind of kernels used [13, 14, 15]. RankNet, developed by a group from Microsoft Research, proposed an optimiza-tion approach using an objective function based on Bradley-Terry models for paired comparisons and explored neural networks for learning the ranking functions [4]. The clos-est to our proposed method is RankBoost discussed in [8], using ideas of Adaboost for learning ranking functions for preference data. The choice for selecting weak learners for RankBoost as discussed in [8] is very limited and is less flexi-ble to deal with the complicated features used in Web search context.

The main contribution of our work is the development of a learning framework for preference data using regression as the basic ingredient. For example, the ranking functions are represented as a combination of regression trees when we use gradient boosting for regression [9]. More interestingly, our experimental results also show that even with absolute relevant judgments, it is more advantageous to first convert them into preference data and apply our proposed methods than to treat the ranking problem with absolute relevant judgments as a regression problem.

The rest of the paper is organized as follows: section 2 develops the main algorithmic contribution of the paper; we start with a brief review of the basic idea of gradient descent in function spaces [9]. We then propose an objective func-tion, the optimization of whic h will lead to the construction of the ranking function. We apply functional gradient de-scent methodology to the objective function and transform the problem of learning ranking functions as a sequence of problems of learning regression functions. For concreteness, we use gradient boosting regression as an illustration of the general methodology. In section 3, we present a detailed ex-perimental study using data from a commercial search en-gine. In the last section, we make some concluding remarks and also point out several directions for future research.
Our basic premise is that ranking and regression are fun-damentally different problems, but regression can be used to solve ranking problems using preference data. Our main contribution is a framework for solving ranking problems with regression using relative judgments and the regression methods used can be be chosen to tailor to the specific ap-plications. For concreteness, we will discuss the framework using gradient descent, and we start with a brief introduc-tion of gradient descent in function spaces [9]. We then propose a new objective function for learning ranking func-tions using preference data and develop an algorithm that adapts functional gradient de scent for optimizing the pro-posed objective function.
We first give a brief discussion of gradient descent for un-constrained optimization of a multivariate function [3]. To this end, suppose we want to solve min x  X  X  d F ( x ) , where F ( x )isa d -variable function. The idea of gradient descent is to start with an initial guess x 0 of a minimizer, and at each step compute the gradient of the objective function F at the current iterate x k ,say  X  F ( x k ), and use the negative gradient as the search directi on to obtain the next iterate where  X  k is the step size which can be chosen, for example, by a line-search.

In the context of regression, we are given a training set { ( x i ,g i ) } N i =1 , and we seek to find a function h such that g  X  h ( x i ) ,i =1 ,...,N . For simplicity we use the square loss function, i.e., we measure the discrepancy between g and h ( x i )by( g i  X  h ( x i )) 2 . Then we need to find a function h ( x ) to solve the following minimization problem, where H is a pre-defined function class such as the class of polynomials not to exceed certain degree. We can apply gradient descent in function space to minimize the functional L ( h ), i.e., compute the gradient of L ( h )withrespectto h at the current iterate h k ( x ) and form the next iterate as The problem is that, we can not compute  X  L ( h k ( x )) at all x , but rather we can only compute it at a finite sample, The crucial idea of functional gradient descent is to find a function that interpolates/approximates the above sample values and therefore obtain an approximation of the nega-tive gradient  X  X  X  L ( h k ( x )) to form the next iterate. As an illustration, we explain the det ails of the algorithm when the interpolation/approximation is done by fitting a regression tree to the sample values, as is easily seen other regression methods can also be used here [9]. We summarize the above in the following and label it as GBT (Gradient Boosting Trees).

Algorithm. (Gradient Boosting Trees [9]) 1. Initialize h 0 ( x )= N i =1 g i /N . 2. For k =1 ,...,M : (number of trees in gradient boost-ing)
There are two parameters M , the number of regression trees and  X  , the shrinkage factor that need to be chosen by the user. In general, we use cross-validation for choosing the two parameters.
As we mentioned before, the relative relevance judgments are in the form of whether a document is more relevant than other documents with respect to a query. We encode this information as follows: given the feature vectors for two query-document pairs x and y (see Section 3 for details on extraction of query-document features), we use x y to mean that x is preferred over y , i.e., x should be ranked higher than y . Simply put, this means that the document represented by x is considered more relevant than that rep-resented by y with respect to the query in question.
We denote the set of available preferences based on the relative relevance judgments as We formulate the problem of learning ranking functions as computing a ranking function h  X  X  , H a given function class, such that h match the set of preferences, i.e., h ( x h ( y i ), if x i y i ,i =1 ,...,N ,asmuchaspossible. We propose to use the following objective function to measure the risk of a ranking function h , the motivation is that if for the pair x i ,y i , h matches the given preference, i.e., h ( x i )  X  h ( y i ), then h incurs no cost on the pair, otherwise the cost is given by ( h ( y i )  X  Direct optimization of the above can be difficult, the basic idea of our regression framework is to fix either one of the values h ( x i )or h ( y i ), e.g., replace either one of the function values by its current predicted value, and solve the problem by way of regression.

Remark. To avoid obtaining an optimal h which is con-stant, we actually need to optimize, for 0 &lt; X   X  1, Our implementation in the sequel corresponds to setting  X  to be a fixed constant.

To this end, we use the idea of functional gradient descent as reviewed in the previous section. We consider as the unknowns, and compute the gradient of R ( h )with respect to those unknowns. The components of the negative gradient corresponding to h ( x i )and h ( y i ), respectively, are Both of the above equal to zero when h matches the pair x i ,y i , and therefore, in this case no modification is needed for the components corresponding to h ( x i )or h ( y i ). On the other hand, if h does not match the pair x i ,y i ,the components of the gradient are The above tells us how to modify the difference of function values, to know how to modify the function itself we need to translate those gradient components into modification to h . We adopt the following simple approach: we set the target value for x i as h ( y i )+  X  and that for y i as h ( x i fixed  X  . Then, we obtain the following set of data that need to be fitted at each iteration, where h does not match pair x i ,y i .

When some feature vectors x i or y i canappearmorethan once in S , there will be several components of the negative gradient of R ( h ) that will involve x i or y i . When translat-ing the gradient components to modification of h ,wemay end up with inconsistent requirements. One approach will be to compute an average taking into account of all the re-quirements. This is a local approach using information in the training data related to the feature vectors in question. A better alternative approach is to add all the different and potentially inconsistent requirements in the training set, and let the regression methods such as GBT to handle the in-consistency using more global information based on all the training data. We summarize the algorithm, again using GBT for regression as an illustration, as follows which we label as GBrank.
 Algorithm. (GBrank) 1
Start with an initial guess h 0 ,for k =1 , 2 ,... , 1) using h k  X  1 as the current approximation of h ,wesep-2) fitting a regression function g k ( x ) using GBT and the 3) forming (with normalization of the range of h k )
Remark. We want to point out that the above framework is generic in the sense that it can use any application-specific regression method for learn ing the regression function g
If the preferences x i ,y i are converted from absolute rel-evance judgments, we multiple  X  by the absolute value of grade difference between x i and y i
We carried out several experiments illustrating the prop-erties and effectiveness of GBrank. We also compared its performance with some existing algorithms such as GBT and RankSVM.
We first describe how the data used in the experiments are collected.
As we mentioned before, each query-document pair is rep-resented by a feature vector. For query-document pair ( q, d ), afeaturevector x =[ x Q ,x D ,x QD ] is generated and the fea-tures generally fall into the f ollowing three categories:
The preference data for training are extracted from the follow two sources: absolute relevance judgments arising from editorial labeling, and relative relevance judgments ex-tracted from user clickthroughs data.
A set of queries are sampled from query logs, and a cer-tain number of query-document pairs are labeled according to their relevance judged by human editors. A 0-4 grade is assigned to each query-document pair based on the degree of relevance (perfect match, excellent match, etc), and the nu-merical grades are also used as the target values for GBT re-gression. We use a data set from a commercial search engine which contains 4,372 queries and 115,278 query-document pairs.

We use the above labeled data to generate a set of pref-erence data as follows: given a query q and two documents d x and d y . Let the feature vectors for ( q, d x )and( q, d x and y , respectively. If d x has a higher grade than d include the preference x y while if d y has a higher grade than d x , we include the preference y x . For each query, we consider all pairs of documents within the search results ex-cept those with equal grades. This way, we generate around 1.2 million preferences in total.

As we will see later, the labeled data not only provide us with preference data, they also allow us to compare GBrank based on converted preference data and GBT regression us-ing the labeled data.
We also examined a certain amount of clickthroughs data and extracted a set of preference data as follows. For a query q , we consider two documents d 1 and d 2 among the top 10 results from Yahoo! web search. Assume that in the clickthroughs data, d 1 has c 1 clicks out of n 1 impressions, and d 2 has c 2 clicks out of n 2 impressions. We want to consider document pairs d 1 and d 2 for which either d 1 or d significantly better than the other in terms of clickthroughs rate. To this end, we assume that clicks in user sessions obey binomial distribution. Denote t he binomial distribution by We apply likelihood ratio test (LRT) and compute, We consider a pair d 1 and d 2 when the above is greater than a threshold, and we say the pair is significant. Among the significant pairs, we apply rules similar to those in [15] such as Skip-Above to extract preference data. In total we extracted 20,948 preferences.
The output of GBrank is a ranking function h which is used to rank the documents x according to h ( x ). Therefore, document x is ranked higher than y by the ranking function h if h ( x ) &gt;h ( y ), and we call this the predicted preference. We propose the following three metrics to evaluate the per-formance of a ranking function with respect to a given set of preferences which we considered as the true preferences.
Notice that precision at 100% corresponds to the percent-age of contradicting pairs.
 (right)
The following questions guide the design of the experi-ments we carry out: 1. What is the convergence behavior of GBrank, will the 2. Quantitatively, what is the effect of the training data 3. When we have absolute relevance judgments, we can 4. RankSVM based on linear SVM is a popular method For GBT and RankSVM, we tuned the parameters to get the best performance. For GBrank, we just used the parameters tuned for GBT: we use 100 trees each with 15 leaf nodes, and the shrinkage factor is set to be 0.05. All the experi-ments were conducted on a 2.4Ghz 4-cpu AMD server with 4G RAM. GBT training will take about 15 minutes while the training time for GBrank would be a few hours depend-ing on the number of iterations and number of preferences. The testing time for GBT is only a few minutes and that for GBrank would be the number of iterations multiplied by the above time for GBT. Both training and testing time for GBrank could be significantly reduced by using less number of trees for each iteration, e.g. single tree instead of GBT. Our more recent experiments showed that GBrank using sin-gle tree with a few hundred iterations achieves comparable dcg-5 while reducing the time complexity a great deal.
For the first two questions, we generate the training and testing data as follows: we randomly split the labeled data described in section 3.1.1 by query into training set (60% of labeled queries, 71,338 query-documents and 753,976 pref-erences) and testing set (the remaining 40% queries, 43,940 query-document pairs and 465,893 preferences).

From the left panel of Figure 1, we can see that the num-ber of contradicting pairs monot onically decreases iteration by iteration during training, which indicates the convergent trend of GBrank. As for now, we always use a validation set Table 1: Number of contradicting pairs (CP) and precision (Prec) at K %forGBrankandGBT %K num pairs GBrank GBT 10% 46590 225 0.9952 949 0.9796 20% 93179 1095 0.9883 3313 0.9644 30% 139768 5695 0.9593 8718 0.9376 40% 186358 15324 0.9178 18340 0.9016 50% 232947 28117 0.8793 30865 0.8675 60% 279536 46334 0.8434 43776 0.8342 70% 326126 61189 0.8124 63711 0.8046 80% 372715 80726 0.7834 82976 0.7774 90% 419304 101601 0.7577 103530 0.7531 100% 465893 123939 0.7340 126188 0.7291 to decide when to stop the iteration. The middle panel of Figure 1 shows the number of contradicting pairs on the test-ing data first decreases and then gradually increases after a certain number of iterations. Since we also have available the absolute relevance judgments, we plot the dcgs against each iteration in the right panel of Figure 1. The dcg plot is almost a mirror image along the horizontal axis of the contradicting pairs plot on testing data. We mention that we have observed similar trends on other data sets we have tested.

In order to demonstrate the effect of training data size, we randomly sample increasing percentages of training data and generate the corresponding pairwise preference data as described in section 3.1.1. The experimental results on the same testing data are reported as follows:
The left panel of Figure 2 shows the number of contradict-ing pairs in the testing data decreases with the increasing size of training data. The dcg-5 for different training data size was shown in the right panel of Figure 2, which indi-cates a strongly positive correlation between the dcg gain and the increase of training data size.

Although our major concern is about GBrank using rela-tive relevance judgments, it is ac tually rather instructive to see how it compares with GBT when trained on the same absolute relevance judgments, of course for GBrank, we will need to convert the absolute relevance judgments to rela-tive relevance judgments. This experiment is aimed at the third question. One interesting observation is that GBrank underperforms GBT when the training data size is small. One plausible explanation for this is that to rank objects according to a set of preferences, there needs to be enough overlaps among the preferences, for small amount of data, the overlaps are weak and hence the poorer performance. Why would GBrank outperform GBT when there are plenty of training data? We suspect the deeper reason lies at the fundamental difference between a ranking problem and a regression problem. GBT is des igned for regression prob-lems and is therefore not necessarily the optimal choice for ranking problems.

We elaborate on this point a bit more now: ranking web search results is fundamentally a preference learning prob-lem rather than a regression problem, i.e., we just want to rank more relevant results higher than those less relevance ones, we do not need to care about predicting the grades of the documents very accurately. Let us illustrate this using a simple example [19]. Consider two queries q 1 :  X  X arvard uni-versity X  and q 2 :  X  X ollege of san mateo X . q 1 is more popular than q 2 generating about 13 million search results while re-sults for q 2 are two orders of magnitude less. For simplicity we consider a ranking function h ( x )usingasinglefeature x which counts the number of inbound links to a document. Now let us examine the top three results d i 1 ,d i 2 ,d i for each of the query. Assume d i 1 is ranked perfect, d ranked excellent, and d i 3 is ranked good, and we convert the labels to numerical values as follows, Since q 1 is very popular, each of the top three results gener-atehighfeaturevalues,say, x = 100000 , 80000 , 50000 while for q 2 the corresponding feature values are x = 1000 , 800 , 500 . Assume x is negatively correlated with the label, i.e., small x values tend to indicate better relevance, then we will need to find a monotonically decreasing function h such that for q and for q 2 , The major issue comes from the difference of popularity of the queries. This could partially explain why ranking func-tions could be better learned using GBranking with pref-erence data than GBT regression with absolute relevance judgments.

In our experiments, we exclude all tied data (pairs of docu-ments with the same grade) when converting preference data from the absolute relevance judgments, which is a significant information loss especially when the training data are small. Adding those tied data will certainly increase the overlaps among the training preferences. Including those tied data in GBrank learning will be part of our future work.
We now turn to the precision at K %metric: Table1 presents the number of contradicting pairs and precision at K % for GBT learned with all of training data and GBrank learned with the corresponding preference data. This again shows that GBrank outperforms GBT with respect to the precision at K %metric.

To further explore the third and the last questions, we con-duct an experimental comparison among GBrank, GBT, and RankSVM in a 5-fold cross-validation setting. Again, the 5-fold splitting is on queries using the data described in section 3.1.1. Figure 3 and 4 show the results using the two met-rics, dcg-5 and number of contradicting pairs, for GBrank, GBT, and RankSVM, from which we can see GBrank is the best performer and RankSVM is worse than both GBrank and GBT. Average over the 5-folds, dcg-5 for GBrank is 1 . 2% better than GBT with p -value equal to 0 . 0005, and 5 . 7% better than RankSVM with p -value close to zero. As a baseline comparison, the dcg-5 difference among the top search engines on this data set is about 2-3%.
We also use the preference data extracted from user click-throughs data as described in section 3.1.2. The compari-son is for RankSVM and GBrank in a 5-fold cross validation setting. For this data set, we can no longer use GBT since we do not have the absolute relevance judgments. Tables 2 and 3 present the results with respect to the number of contradicting pairs metric as well as the precision at K % metric. Both tables again show that GBrank outperforms RankSVM.
 Figure 3: DCG for GBrank, GBT and RankSVM in 5-fold cv Figure 4: Number of contradicting pairs for
GBrank, GBT, and RankSVM in 5-fold cv
In this paper we proposed a general regression framework for learning ranking functions from preference data. In par-ticular, we developed GBrank, a specialization of our frame-work using gradient boosting trees as the regression method. When only preference data are available, GBrank provides a more flexible and effective solution to the problem of learn-ing ranking functions. Even when absolute labels are avail-able, our experiments suggest that it is preferable to first convert them into preference data and apply GBrank over them than directly apply GBT to the original aboslute la-bels.

There are several directions we can pursue to further en-hance our approaches: 1) when converting absolute rele-vance data, we can overweigh the document pairs with larger grade difference. 2) weigh each error term in the loss func-tion defined in Equation (1) with the DCG difference. Specif-ically, assume we have two documents: d 1 and d 2 .Atthe current iteration, d 1 and d 2 were ranked at position i and j respectively, where i&lt;j . Suppose the resulted predicted preference contradicts with the true preference. Their dcg contribution with respect to the wrong ordering would be ordering is therefore | G ( d 1 )  X  G ( d 2 ) | [ 1 log 2 i ing training, we can weigh each error term according to that difference. When the absolute r elevance judgments are not availabe, we can just remove | G ( d 1 )  X  G ( d 2 ) | .3)Wemen-tioned that we can also include the tied data, pairs x i ,y with the same grade. One way to do that is to add the fol-lowing to the set in Equation (2) to construct the training set for computing the regression function at each iteration, 4) as we mentioned before, our framework including GBrank is very flexible for combining relative and absolute relevance judgments. With any query-document feature vector x i and its grade g i , we just need to add ( x i ,g i )tothesetinEqua-tion (2), and there is no need to modify the objective func-tion. Such flexibility is desirable considering there are many queries having a single document with absolute relevance judgment(or documents with same absolute relevance judg-ment), and we could not extract any preference data from that.

Acknowledgment. We thank Tong Zhang for suggesting a modification of the objective function (1) and Alex Simma for the use of LRT. [1] R. Atterer, M. Wunk, and A. Schmidt. Knowing the [2] A. Berger. Statistical machine learning for [3] D. Bertsekas. Nonlinear programming .Athena [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. [5] H. Chen. Machine Learning for information retrieval: [6] W. Cooper, F. Gey and A. Chen. Probabilistic [7] D. Cossock and T. Zhang. Subset ranking using [8] Y. Freund, R. Iyer, R. Schapire and Y. Singer. An [9] J. Friedman. Greedy function approximation: a [10] N. Fuhr. Optimum polynomial retrieval functions data [11] F. Gey, A. Chen, J. He and J. Meggs. Logistic [12] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [13] T. Joachims. Optimizing search engines using [14] T. Joachims. Evaluating retrieval performance using [15] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and [16] J. Ponte and W. Croft. A language modeling approach [17] G. Salton. Automatic Text Processing. Addison [18] H. Turtle and W. B. Croft. Inference networks for [19] H. Zha, Z. Zheng, H. Fu and G. Sun. Incorporating [20] Diane Kelly and Jaime Teevan. Implicit Feedback for [21] F. Radlinski and T. Joachims. Query chains: Learning [22] C. Zhai and J. Lafferty. A risk minimization
