
Jiangtao Ren 1 , Zhengyuan Qiu 1 ,WeiFan 2 , Hong Cheng 3 , and Philip S. Yu 4 Feature selection is an important data processing step in high dimensional data learning tasks. Traditionally, feature selection methods use information from  X  X abeled data X  to find the most informative or most useful feature subsets [1,2], but the information in the  X  X nlabeled X  data is not used. When the size of the  X  X abeled X  data is limited, it is difficult to select an ideal feature subset only on  X  X abeled X  data. R ecently, there have seen consider able interests in learning with labeled and unlabeled data [3]. In many learning tasks, the effectiveness of semi-supervised learning has been demonstrated [4]. Zhao and Liu [5] intro-duced a semi-supervised feature selectio n algorithm based on spectral analysis. Later, they exploited intrinsic properties underlying supervised and unsuper-vised feature selection algorithms, and proposed a unified framework for feature selection based on spectral graph theory [6]. Yet these algorithms are  X  X ilter X  models which 1) do not select feature subset for specific learning method and 2) sample selection bias is ignored. It is important if one can employ certain strategy to extend the initial training set with unlabeled data to overcome the biased distribution problem, and in the same time, perform a  X  X rapper type X  feature selection for specific learning model.

In this paper, we introduce a  X  X rapper-type X  forward semi-supervised feature selection framework. It uses the mech anism of random selection on unlabeled data to form new training sets, and the most frequently selected feature is added to the result feature subset in each iteration. With the introduction of randomly selected data with predicted labels, the s ufficiency and diversity of the training sets can be improved, which in return helps to choose the most discriminative features.

In order to evaluate the effectiveness of our framework, we give formal anal-ysis as well as conduct extensive experimental study on the algorithm. First, bipartite graph has been employed to formally show that unlabeled data is help-ful in feature selection. Secondly, we have conducted extensive experiments with extremely few labeled insta nces (such as, only 6 labeled instances) which can re-flect the scenario of data limitation. The r esults of these experiments show that the proposed  X  X rapper-type X  framework, generally, outperforms the traditional feature selection method and state-of-the-art  X  X ilter-t ype X  semi-supervised fea-ture selection algorithms [5] by 1% to 10% in accuracy. It performs especially well when the size of the labeled data set is very small. Supervised sequential forward feature s election (SFFS) is one of the most widely used feature selection algorit hms. Conceptually, it is an iterative process starting with an empty feature subset. In each iteration, one feature is chosen among the remaining features. To determine which feature to add, it tests the accuracy of a model built on the incremented feature subset. The feature that results in the highest accuracy is selected. Normally, t he process terminates when no additional features could result in an improvement in accuracy or the feature subset already reaches a predefined size. Since this process can be easily implemented and is usually quite effective, it remains one of the widely adopted supervised feature selection methods. In this work, we extend it to take unlabeled data into account. 2.1 Our Approach We propose a new framework of forward feature selection, which performs feature selection on both labeled and unlabeled data. Our algorithm uses SFFS and wrapper model to select startf n features initially, then the startf n features are used to train a classifier, the classifier is then used to predict the labels of the unlabeled data. Then the randomly selected samplingRate % unlabeled data with predicted labels is combined with labeled data to form a new training set. Afterwards, the new training dataset is used to select fnstep features based on SFFS and the learner. The random selectio n and feature selection process repeat samplingT imes times and samplingT imes groups of features are selected. And we count the frequency of every feature in the samplingT imes groups of features, and the one with the most frequency is added to form a new feature subset. This process repeats until the size of the feature subset reaches a predefined number. The algorithm is described in detail in Figure 1. L denotes the labeled data, U denotes the unlabeled data; sizeF S denotes the predefined number of se-lected features; samplingRate denotes the sampling rate according to the un-labeled data with predicted labels; samplingT imes denotes the randomly sam-pling times; maxIterations denotes the max iteration times; startf n denotes the start feature number; fnstep denotes the number of features selected in every step. resultfs denotes the output feature subset. In our algorithm,  X * X  denotes the features reduction operator. 2.2 Method Analysis The classifier  X  X rapped X  in the feature selection algorithm is used to predict the labels of the unlabeled instances as well as to evaluate the effectiveness of the chosen feature subset. Obviously , a more accurate classifier can select more effective feature subsets to represe nt the target distribution. Next, we will demonstrate why the use of unlabeled da ta can improve the  X  X ccuracy X  of the classifier at each step of feature selection.

Feature selection can be formulated as: f 1 ( X )= f ( X ) subject to make X as small as possible, where f 1 and f are the target functions on chosen feature subset X and full feature set X , respectively. When unlabeled examples are used in feature selection, its process is similar to  X  X o-training X , where X and X are interpreted as the two  X  X iews X  in co-training. The classifier constructed in one view is expected to be helpful for the other.

Now we can adopt the bipartite graph model to facilitate our interpretation on why  X  X rapper classifier X  with unlabeled examples can improve the performance of feature selection. The training process can be regarded as a bipartite graph G
D ( X ; X ) [7]. In Figure 2, each node on the left-hand side denotes one instance in X view and the one on the right-hand side denotes the same instance in X view. Any two instances in the labeled dataset will be connected by an edge if they belong to the same class, and those edges are shown in solid lines. Let S be a finite dataset that consists of the labeled dataset, then G S is a bipartite graph in S whose components show the concept distribution in G S .Inthecase of unlabeled instances, each vertex on the left, depending on the its prediction on X view, will be connected to a vertex on t he right. Thus, it is connected to the most probable category.

Next, we analyze why unlabeled data ca n improve the generalization accu-racy of the classifier on the platform of bipartite graph. Let G D be the bipartite graph in the all-real-data distribution D . As we know, the components in G D reflect the concept distribution on the real dataset, and we can achieve com-pletely correct prediction if we get the components in G D . But it is impossible to achieve this when the dataset D is infinite. The work of co-training model is to find components that are much  X  X imilar X  to those in G D by the use of unlabeled data. Given S , as the predictions on the unlabeled data increase, the edges will be added to the bipartite graph and the number of components will example, vertex 3 has a predicted labe l the same as vertex 2, and vertex 4 has a predicted label the same as vertex 5), unl abeled vertex 3 can be connected to ver-tex 1 and 2 to form a new component, an d unlabeled vertex 4 can be connected to vertex 5, 6 and 7 to form another new component. Then the number of com-ponents is reduced to 2, as illustrated in Figure 2(b). Blum and Mitchell [7] had demonstrated that the components in G S will be similar to those in G D if the unlabeled set is large enough. Thus, they capture the real concept distribution of the real dataset and give the cla ssifier higher prediction accuracy.
Based on the above analysis, it is cl ear that the accuracy of the wrapper classifier can be improved by the iterati on process of FW-SemiFS. In other words, the effectiveness of FW-SemiFS can be improved by applying unlabeled data. In order to evaluate the effectiveness of our proposed algorithm, we have con-ducted extensive experiments on several datasets. Table 1 summarizes the infor-mation of the datasets that we used. In our experiment, the labeled data and unlabeled data are randomly selected from the whole dataset, and the left is used as testing data. 3.1 Experiment Settings and Evaluation Method In FW-SemiFS experiments, the parameters startf n , fnstep , samplingRate , and samplingT imes are set to 5, 6, 50%, and 10, respectively. For compari-son, SFFS and Semi-supervised Laplacian Score (called SLS for short) are also conducted. SFFS is a supervised feature selection algorithm described in the previous section; and SLS is a semi-supervised feature selection algorithm which is similar to Zhao and Liu X  X  algorithm [5], but it uses Laplacian score as its rank-ing criterion. We chose three machine-l earning models, NaiveBayes, NNge, and k-NN, to evaluate the effectiveness of t he algorithms. Specifically, NNge is the nearest neighbor like algorithm using non -nested generalized examples; k-NN is k nearest neighbor classifier whose parameter k is set to 5.

For FW-SemiFS and SLS, we employ both the labeled dataset and unlabeled dataset to perform semi-supervised feature selection. But for SFFS approach, we only employ the labeled dataset to perfo rm feature selection. After selecting the feature subset ResultF S , we construct the classifier only with the labeled dataset and ResultF S , and then the unseen testing dataset is employed to evaluate the classification accuracy.
 3.2 Empirical Results Table 2 shows the experiment results of the SFFS, SLS and FW-SemiFS methods when the size of the selected feature subset is 10, as well as the classification accuracy without feature selection (de noted as  X  X ull X  in the table). The numbers are shown in bold when it is the highest one among  X  X ull X ,  X  X FFS X ,  X  X LS X  and  X  X emiFS X . From table 2, we could find that the accuracies of FW-semiFS are higher than that of two other feature selection algorithms in 13 out of 21 cases.
For further view of the algorithm comparison, we conduct statistical analysis of the experiment results. For every dataset and every learning algorithm, we run SFFS, SLS and FW-SemiFS respectiv ely according to 16 different feature subset sizes (from 5 to 20), and then get 16 groups of feature subsets and related accuracies. Each group has t hree feature subsets and three related classification accuracies according to SFFS, SLS and FW-SemiFS, respectively. After that, we calculate the mean and standard deviation of these 16 accuracies for each group, which are listed in table 3.
 From Table 3, we can see that the mean of the classification accuracies of FW-SemiFS are higher than that of two others most of the time. On German and Ionosphere datasets, the means of FW-SemiFS are the highest ones for NaiveBayes, NNge, and k-NN learners, along with small standard deviation; Although, for German dataset, the standard deviation is larger than that of two other algorithm, the mean of FW-SemiFS is much higher than that of two others. The similar phenomena can be also observed for Sonar and wdbc datasets. We have explored the use of unlabeled examples to facilitate  X  X rapper-type X  forward semi-supervised feature select ion. The proposed algorithm works in an iterative procedure. In each step, unlab eled examples receive labels from the classifier construct ed on currently selected feat ure subset. Then a random sam-ple of now  X  X abeled X  unlabeled examples is concatenated with the training set to form a  X  X oint X  dataset, where a wrapper-based feature selection is performed. Experiment results show that the proposed approach, generally, can obtain 1% to 10% higher accuracy than other supervi sed and semi-supervised feature se-lection algorithms.
 Future Work. The work discussed in this paper represents techniques based on random selection mechanism. In the future, we plan to extend the techniques by using prediction confidence as a criterion to select those unlabeled data which is most probable to have corr ectly predicted labels.

