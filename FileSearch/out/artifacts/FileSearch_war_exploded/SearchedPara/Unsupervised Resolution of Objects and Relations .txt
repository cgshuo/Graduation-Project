 Web Information Extraction (WIE) sys-tems extract assertions that describe a rela-tion and its arguments from Web text (e.g., (is capital of , D . C ., United States) ). WIE systems can extract hundreds of millions of assertions containing millions of different strings from the Web ( e.g. , the T EXT R UNNER system (Banko et al., 2007)). 1 WIE systems often extract assertions that describe the same real-world object or relation using different names. For example, a WIE system might extract (is capital city of , Washington , U . S . ) , which describes the same relationship as above but contains a different name for the relation and each argument.

Synonyms are prevalent in text, and the Web cor-pus is no exception. Our data set of two million as-sertions extracted from a Web crawl contained over a half-dozen different names each for the United States and Washington, D.C., and three for the  X  X s capital of X  relation. The top 80 most commonly extracted objects had an average of 2.9 extracted names per entity, and several had as many as 10 names. The top 100 most commonly extracted re-lations had an average of 4.9 synonyms per relation.
We refer to the problem of identifying synony-mous object and relation names as Synonym Res-olution (SR). 2 An SR system for WIE takes a set of assertions as input and returns a set of clusters, with each cluster containing coreferential object strings or relation strings. Previous techniques for SR have focused on one particular aspect of the problem, ei-ther objects or relations. In addition, the techniques either depend on a large set of training examples, or are tailored to a specific domain by assuming knowl-edge of the domain X  X  schema. Due to the number and diversity of the relations extracted, these tech-niques are not feasible for WIE systems. Schemata are not available for the Web, and hand-labeling training examples for each relation would require a prohibitive manual effort.

In response, we present R ESOLVER , a novel, domain-independent, unsupervised synonym resolu-tion system that applies to both objects and relations. R
ESOLVER clusters coreferential names together us-ing a probabilistic model informed by string similar-ity and the similarity of the assertions containing the names. Our contributions are: 1. A scalable clustering algorithm that runs in 2. An unsupervised probabilistic model for pre-3. An empirical demonstration that R ESOLVER
The next section discusses previous work. Section 3 introduces our probabilistic model for SR. Section 4 describes our clustering algorithm. Section 5 de-scribes extensions to our basic SR system. Section 6 presents our experiments, and section 7 discusses our conclusions and areas for future work. The DIRT algorithm (Lin and Pantel, 2001) ad-dresses a piece of the unsupervised SR problem. DIRT is a heuristic method for finding synonymous relations, or  X  X nference rules. X  DIRT uses a depen-dency parser and mutual information statistics over a corpus to identify relations that have similar sets of arguments. In contrast, our algorithm provides a for-mal probabilistic model that applies equally well to relations and objects, and we provide an evaluation of the algorithm in terms of precision and recall.
There are many unsupervised approaches for ob-ject resolution in databases, but unlike our algo-rithm these approaches depend on a known, fixed schema. Ravikumar and Cohen (Ravikumar and Co-hen, 2004) present an unsupervised approach to ob-ject resolution using Expectation-Maximization on a hierarchical graphical model. Several other re-cent approaches leverage domain-specific informa-tion and heuristics for object resolution. For ex-ample, many (Dong et al., 2005; Bhattacharya and Getoor, 2005; Bhattacharya and Getoor, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously ( e.g. , co-authors of the same publication). While this is useful information when resolving authors in the ci-tation domain, it is extremely rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolv-ing relations. See (Winkler, 1999) for a survey of this area.

Several supervised learning techniques make en-tity resolution decisions (Kehler, 1997; McCallum and Wellner, 2004; Singla and Domingos, 2006), but of course these systems depend on the availability of training data, and often on a significant number of labeled examples per relation of interest. These approaches also depend on complex probabilistic models and learning algorithms, and they have order O ( n 3 ) time complexity, or worse. They currently do not scale to the amounts of data extracted from the Web. Previous systems were tested on at most a few thousand examples, compared with millions or hun-dreds of millions of extractions from WIE systems such as T EXT R UNNER .
 Coreference resolution systems (e.g., (Lappin and Leass, 1994; Ng and Cardie, 2002)), like SR sys-tems, try to merge references to the same object (typ-ically pronouns, but potentially other types of noun phrases). This problem differs from the SR problem in several ways: first, it deals with unstructered text input, possibly with syntactic annotation, rather than relational input. Second, it deals only with resolv-ing objects. Finally, it requires local decisions about strings; that is, the same word may appear twice in a text and refer to two different things, so each occur-rence of a word must be treated separately.
 The PASCAL Recognising Textual Entailment Challenge proposes the task of recognizing when two sentences entail one another, and many authors have submitted responses to this challenge (Dagan et al., 2006). Synonym resolution is a subtask of this problem. Our task differs significantly from the tex-tual entailment task in that it has no labeled training data, and its input is in the form of relational extrac-tions rather than raw text.

Two probabilistic models for information extrac-tion have a connection with ours. Our probabilistic model is partly inspired by the ball-and-urns abstrac-tion of information extraction presented by Downey et al. (2005) Our task and probability model are dif-ferent from theirs, but we make many of the same modeling assumptions. Second, we follow Snow et al. X  X  work (2006) on taxonomy induction in incorpo-rating transitive closure constraints in our probabil-ity calculations, as explained below. Our probabilistic model provides a formal, rigorous method for resolving synonyms in the absence of training data. It has two sources of evidence: the similarity of the strings themselves (i.e., edit dis-tance) and the similarity of the assertions they ap-pear in. This second source of evidence is some-times referred to as  X  X istributional similarity X  (Hin-dle, 1990).

Section 3.2 presents a simple model for predict-ing whether a pair of strings co-refer based on string similarity. Section 3.3 then presents a model called the Extracted Shared Property (ESP) Model for pre-dicting whether a pair of strings co-refer based on their distributional similarity. Finally, a method is presented for combining these models to come up with an overall prediction for coreference decisions between two clusters of strings. 3.1 Terminology and Notation We use the following notation to describe the proba-bilistic models. The input is a data set D containing extracted assertions of the form a = ( r, o 1 , . . . , o where r is a relation string and each o i is an object string representing the arguments to the relation. In our data, all of the extracted assertions are binary, so n = 2 . The subset of all assertions in D containing a string s is called D s .

For strings s i and s j , let R i,j be the random vari-able for the event that s i and s j refer to the same entity. Let R t i,j denote the event that R i,j is true, and R f i,j denote the event that it is false.
A pair of strings ( r, s 2 ) is called a property of a string s 1 if there is an assertion ( r, s 1 , s 2 )  X  D or ( r, s 2 , s 1 )  X  D . A pair of strings ( s 1 , s an instance of a string r if there is an assertion ( r, s 1 , s 2 )  X  D . Equivalently, the property p = ( r, s 2 ) applies to s 1 , and the relation r applies to the instance i = ( s 1 , s 2 ) . Finally, two strings x and y share a property (or instance) if both x and y are extracted with the same property (or instance). 3.2 String Similarity Model Many objects appear with multiple names that are substrings, acronyms, abbreviations, or other sim-ple variations of one another. Thus string similarity can be an important source of evidence for whether two strings co-refer. Our probabilistic String Sim-ilarity Model (SSM) assumes a similarity function sim( s 1 , s 2 ) : STRING  X  STRING  X  [0 , 1] . The model sets the probability of s 1 co-referring with s 2 to a smoothed version of the similarity: The particular choice of  X  and  X  make little differ-ence to our results, so long as they are chosen such that the resulting probability can never be one or zero. In our experiments  X  = 20 and  X  = 5 , and we use the well-known Monge-Elkan string similarity function for objects and the Levenshtein string edit-distance function for relations (Cohen et al., 2003). 3.3 The Extracted Shared Property Model The Extracted Shared Property (ESP) Model out-puts the probability that s 1 and s 2 co-refer based on how many properties (or instances) they share. As an example, consider the strings  X  X ars X  and  X  X ed Planet X , which appear in our data 659 and 26 times respectively. Out of these extracted assertions, they share four proper-ties. For example, ( lacks, Mars, ozone layer ) and ( lacks, Red Planet, ozone layer ) both appear as assertions in our data. The ESP model determines the probability that  X  X ars X  and  X  X ed Planet X  refer to the same entity after observing k , the number of properties that apply to both, n 1 , the total number of extracted properties for  X  X ars X , and n 2 , the total number of extracted properties for  X  X ed Planet. X 
ESP models the extraction of assertions as a generative process, much like the URNS model (Downey et al., 2005). For each string s i , a certain number, P i , of properties of the string are written on balls and placed in an urn. Extracting n i assertions that contain s i amounts to selecting a subset of size n i from these labeled balls. 3 Properties in the urn are called potential properties to distinguish them from extracted properties.

To model coreference decisions, ESP uses a pair of urns, containing P i and P j balls respectively, for the two strings s i and s j . Some subset of the P balls have the exact same labels as an equal-sized subset of the P j balls. Let the size of this sub-set be S i,j . The ESP model assumes that corefer-ential strings share as many potential properties as possible, though only a few of the potential proper-ties will be extracted for both. For non-coreferential strings, the number of shared potential properties is a strict subset of the potential properties of each string. Thus if R i,j is true then S i,j = min( P i , P j ) , and if R i,j is false then S i,j &lt; min( P i , P j ) .
The ESP model makes several simplifying as-sumptions in order to make probability predictions. As is suggested by the ball-and-urn abstraction, it assumes that each ball for a string is equally likely to be selected from its urn. Because of data sparsity, almost all properties are very rare, so it would be dif-ficult to get a better estimate for the prior probability of selecting a particular potential property. Second, it assumes that without knowing the value of k , ev-ery value of S i,j is equally likely, since we have no better information. Finally, it assumes that all sub-sets of potential properties are equally likely to be shared by two non-coreferential objects, regardless of the particular labels on the balls, given the size of the shared subset.

Given these assumptions, we can derive an ex-pression for P ( R t i,j ) . First, note that there are tions for s i and s j . Given a particular value of S i,j the number of ways in which n i and n j assertions can be extracted such that they share exactly k is given by
Count( k, n i , n j | P i , P j , S i,j ) = By our assumptions, P ( k | n i , n j , P i , P j , S i,j ) =
Let P min = min( P i , P j ) . The result below fol-lows from Bayes X  Rule and our assumptions above: Proposition 1 If two strings s i and s j have P i and P j potential properties (or instances), and they ap-pear in extracted assertions D i and D j such that | D i | = n i and | D j | = n j , and they share k extracted properties (or instances), the probability that s i and s j co-refer is: P ( R t i,j | D i , D j , P i , P j ) = Substituting equation 1 into equation 2 gives us a complete expression for the probability we are look-ing for.

Note that the probability for R i,j depends on just two hidden parameters, P i and P j . Since we have no labeled data to estimate these parameters from, we tie these parameters to the number of times the respective strings s i and s j are extracted. Thus we set P i = N  X  n i , and we set N = 50 in our experi-ments. 3.4 Combining the Evidence For each potential coreference relationship R i,j , there are now two pieces of probabilistic evidence. Let E e i,j be the evidence for ESP, and let E s i,j be the evidence for SSM. Our method for combining the two uses the Na  X   X ve Bayes assumption that each piece of evidence is conditionally independent, given the coreference relation:
Given this simplifying assumption, we can com-bine the evidence to find the probability of a cofer-ence relationship by applying Bayes X  Rule to both sides (we omit the i, j indices for brevity): P ( R t | E s , E e ) = 3.5 Comparing Clusters of Strings Our algorithm merges clusters of strings with one another, using one of the above models. However, these models give probabilities for coreference deci-sions between two individual strings, not two clus-ters of strings.

We follow the work of Snow et al. (2006) in in-corporating transitive closure constraints in proba-bilistic modeling, and make the same independence assumptions. The benefit of this approach is that the calculation for merging two clusters depends only on coreference decisions between individual strings, which can be calculated independently.

Let a clustering be a set of coreference relation-ships between pairs of strings such that the corefer-ence relationships obey the transitive closure prop-erty. We let the probability of a set of assertions D given a clustering C be:
The metric used to determine if two clusters should be merged is the likelihood ratio, or the prob-ability for the set of assertions given the merged clusters over the probability given the original clus-tering. Let C 0 be a clustering that differs from C only in that two clusters in C have been merged in C , and let  X  C be the set of coreference relation-ships in C 0 that are true, but the corresponding ones in C are false. This metric is given by: P ( D | C 0 ) /P ( D | C ) =
The probability P ( R t i,j | D i  X  D j ) may be supplied by the SSM, ESP, or combination model. In our ex-periments, we let the prior for the SSM model be 0.5. For the ESP and combined models, we set the prior to P ( R t i,j ) = 1 Our clustering algorithm iteratively merges clusters of co-referential names, making each iteration in S := set of all strings For each property or instance p , 1. Scores := {} 2. Build index mapping properties (and instances) 3. For each property or instance p : 4. Repeat until no merges can be performed: time O ( N log N ) in the number of extracted as-sertions. The algorithm requires only basic assump-tions about which strings to compare. Previous work on speeding up clustering algorithms for SR has ei-ther required far stronger assumptions, or else it has focused on heuristic methods that remain, in the worst case, O ( N 2 ) in the number of distinct objects.
Our algorithm, a greedy agglomerative clustering method, is outlined in Figure 1. The first novel part of the algorithm, step 3, compares pairs of strings that share the same property or instance, so long as no more than Max strings share that same property or instance. After the scores for all comparisons are made, each string is assigned its own cluster. Then the scores are sorted and the best cluster pairs are merged until no pair of clusters has a score above threshold. The second novel aspect of this algorithm is that as it merges clusters in Step 4, it merges prop-erties containing those clusters in a process we call mutual recursion , which is discussed below.
This algorithm compares every pair of clusters that have the potential to be merged, assuming two properties of the data. First, it assumes that pairs of clusters with no shared properties are not worth comparing. Since the number of shared properties is a key source of evidence for our approach, these clusters almost certainly will not be merged, even if they are compared, so the assumption is quite rea-sonable. Second, the approach assumes that clus-ters sharing only properties that apply to very many strings (more than Max ) need not be compared. Since properties shared by many strings provide lit-tle evidence that the strings are coreferential, this as-sumption is reasonable for SR. We use Max = 50 in our experiments. Less than 0.1% of the properties are thrown out using this cutoff. 4.1 Algorithm Analysis Let D be the set of extracted assertions. The follow-ing analysis shows that one iteration of merges takes time O ( N log N ) , where N = | D | . Let NC be the number of comparisons between strings in step 3. To simplify the analysis, we consider only those properties that contain a relation string and an argu-ment 1 string. Let A be the set of all such properties. NC is linear in N : 4
Note that this bound is quite loose because most properties apply to only a few strings. Step 4 re-quires time O ( N log N ) to sort the comparison scores and perform one iteration of merges. If the largest cluster has size K , in the worst case the al-gorithm will take K iterations. In our experiments, the algorithm never took more than 9 iterations. 4.2 Relation to other speed-up techniques The merge/purge algorithm (Hernandez and Stolfo, 1995) assumes the existence of a particular attribute such that when the data set is sorted on this attribute, matching pairs will all appear within a narrow win-dow of one another. This algorithm is O ( M log M ) where M is the number of distinct strings. However, there is no attribute or set of attributes that comes close to satisfying this assumption in the context of domain-independent information extraction.

There are several techniques that often provide speed-ups in practice, but in the worst case they make O ( M 2 ) comparisons at each merge iteration, where M is the number of distinct strings. This can cause problems on very large data sets. Notably, McCallum et al. (2000) use a cheap comparison metric to place objects into overlapping  X  X anopies, X  and then use a more expensive metric to cluster ob-jects appearing in the same canopy. The R ESOLVER clustering algorithm is in fact an adaptation of the canopy method; it adds the restriction that strings are not compared when they share only high-frequency properties. The canopy method works well on high-dimensional data with many clusters, which is the case with our problem, but its time complexity is worse than ours.
 For information extraction data, a complexity of O ( M 2 ) in the number of distinct strings turns out to be considerably worse than our algorithm X  X  com-plexity of O ( N log N ) in the number of extracted assertions. This is because the data obeys a Zipf law relationship between the frequency of a string and its rank, so the number of distinct strings grows linearly or almost linearly with the number of assertions. 5 4.3 Mutual Recursion Mutual recursion refers to the novel property of our algorithm that as it clusters relation strings to-gether into sets of synonyms, it collapses proper-ties together for object strings and potentially finds more shared properties between coreferential object strings. Likewise, as it clusters objects together into sets of coreferential names, it collapses instances of relations together and potentially finds more shared instances between coreferential relations. Thus the clustering decisions for relations and objects mutu-ally depend on one another.

For example, the strings  X  X ennedy X  and  X  X res-ident Kennedy X  appear in 430 and 97 assertions in our data, respectively, but none of their ex-tracted properties match exactly. Many properties, however, almost match. For example, the asser-tions (challenged , Kennedy , Premier Krushchev) and (stood up to , President Kennedy , Kruschev) both appear in our data. Because  X  X hallenged X  and  X  X tood up to X  are similar, and  X  X rushchev X  and  X  X re-mier Krushchev X  are similar, our algorithm is able to merge these pairs into two clusters, thereby creat-ing a new shared property between  X  X ennedy X  and  X  X resident Kennedy. X  Eventually it can merge these two strings as well. While the basic R ESOLVER system can cluster syn-onyms accurately and quickly, there is one type of error that it frequently makes. In some cases, it has difficulty distinguishing between similar pairs of ob-jects and identical pairs. For example,  X  X irginia X  and  X  X est Virginia X  share several extractions be-cause they have the same type, and they have high string similarity. As a result, R ESOLVER clusters these two together. The next two sections describe two extensions to R ESOLVER that address the prob-lem of similarity vs. identity. 5.1 Function Filtering R
ESOLVER can use functions and one-to-one rela-tions to help distinguish between similar and identi-cal pairs. For example, West Virginia and Virginia have different capitals: Richmond and Charleston, respectively. If both of these facts are extracted, and if R ESOLVER knows that the  X  X apital of X  relation is functional, it should prevent Virginia and West Vir-ginia from merging.

The Function Filter prevents merges between strings that have different values for the same func-tion. More precisely, it decides that two strings y 1 and y 2 match if their string similarity is above a high threshold. It prevents a merge between strings x 1 and x 2 if there exist a function f and extractions f ( x 1 , y 1 ) and f ( x 2 , y 2 ) , and there are no such ex-tractions such that y 1 and y 2 match (and vice versa for one-to-one relations). Experiments described in section 6 show that the Function Filter can improve the precision of R ESOLVER without significantly af-fecting its recall.

While the Function Filter currently uses func-tions and one-to-one relations as negative evidence, it is also possible to use them as positive evidence. For example, the relation  X  X arried X  is not strictly one-to-one, but for most people the set of spouses is very small. If a pair of strings are extracted with the same spouse X  e.g. ,  X  X DR X  and  X  X resident Roosevelt X  share the property ( X  X arried X ,  X  X leanor Roosevelt X ) X  X his is far stronger evidence that the two strings are identical than if they shared some random property.

Unfortunately, various techniques that attempted to model this insight, including a TF-IDF weighting of properties, yielded essentially no improvement of R
ESOLVER . One major reason is that there are rel-atively few examples of shared functional or one-to-one properties because of sparsity. This idea de-serves more investigation, however, and is an area for future work. 5.2 Using Web Hitcounts While names for two similar objects may often ap-pear together in the same sentence, it is relatively rare for two different names of the same object to appear in the same sentence. R ESOLVER exploits this fact by querying the Web to determine how often a pair of strings appears together in a large corpus. When the hitcount is high, R ESOLVER can prevent the merge.

Specifically, the Coordination-Phrase Filter searches for hitcounts of the phrase  X  s 1 and s 2  X , where s 1 and s 2 are a candidate pair for merging. It then computes a variant of pointwise mutual information, given by coordination score ( s 1 , s 2 ) = The filter prevents any merge for which the coor-dination score is above a threshold, which is de-termined on a development set. The results of Coordination-Phrase filtering are discussed in the next section. Our experiments demonstrate that the ESP model is significantly better at resolving synonyms than a widely-used distributional similarity metric, the co-sine similarity metric (CSM) (Salton and McGill, 1983), and that R ESOLVER is significantly better at resolving synonyms than either of its components, SSM or ESP.

We test these models on a data set of 2.1 million assertions extracted from a Web crawl. 6 All models ran over all assertions, but compared only those ob-jects or relations that appeared at least 25 times in the data, to give the ESP and CSM models sufficient data for estimating similarity. However, the mod-els do use strings that appear less than 25 times as features. In all, the data contains 9,797 distinct ob-ject strings and 10,151 distinct relation strings that appear at least 25 times.

We judged the precision of each model by manu-ally labeling all of the clusters that each model out-puts. Judging recall would require inspecting not just the clusters that the system outputs, but the en-tire data set, to find all of the true clusters. Be-cause of the size of the data set, we instead esti-mated recall over a smaller subset of the data. We took the top 200 most frequent object strings and top 200 most frequent relation strings in the data. For each one of these high-frequency strings, we man-ually searched through all strings with frequency over 25 that shared at least one property, as well as all strings that contained one of the keywords in the high-frequency strings or obvious variations of them. We manually clustered the resulting matches. The top 200 object strings formed 51 clusters of size greater than one, with an average cluster size of 2.9. For relations, the top 200 strings and their matches formed 110 clusters with size greater than one, with an average cluster size of 4.9. We measured the re-call of our models by comparing the set of all clus-ters containing at least one of the high-frequency words against these gold standard clusters.

For our precision and recall measures, we only compare clusters of size two or more, in order to focus on the interesting cases. Using the term hy-pothesis cluster for clusters created by one of the models, we define the precision of a model to be the number of elements in all hypothesis clusters which are correct divided by the total number of elements in hypothesis clusters. An element s is marked cor-rect if a plurality of the elements in s  X  X  cluster refer to the same entity as s ; we break ties arbitrarily, as they do not affect results. We define recall as the sum over gold standard clusters of the most num-ber of elements found in a single hypothesis cluster, divided by the total number of elements in gold stan-dard clusters.

For the ESP and SSM models in our experiment, we prevented mutual recursion by clustering rela-tions and objects separately. Only the full R E -SOLVER system uses mutual recursion. For the CSM model, we create for each distinct string a row vec-tor, with each column representing a property. If that property applies to the string, we set the value of that column to the inverse frequency of the property and zero otherwise. CSM finds the cosine of the an-gle between the vectors for each pair of strings, and merges the best pairs that score above threshold.
Each model requires a threshold parameter to de-termine which scores are suitable for merging. For these experiments we arbitrarily chose a threshold of 3 for the ESP model (that is, the data needs to be 3 times more likely given the merged cluster than the unmerged clusters in order to perform the merge) and chose thresholds for the other models by hand so that the difference between them and ESP would be roughly even between precision and recall, although for relations it was harder to improve the recall. It is an important item for future work to be able to esti-mate these thresholds and perhaps other parameters of our models from unlabeled data, but the chosen parameters worked well enough for the experiments. Table 1 shows the precision and recall of our models. 6.1 Discussion ESP significantly outperforms CSM on both object and relation clustering. CSM had particular trouble with lower-frequency strings, judging far too many of them to be co-referential on too little evidence. If the threshold for clustering using CSM is increased, however, the recall begins to approach zero.
ESP and CSM make predictions based on a very noisy signal.  X  X anada, X  for example, shares more properties with  X  X nited States X  in our data than  X  X .S. X  does, even though  X  X anada X  appears less of-ten than  X  X .S. X  The results show that both models perform below the SSM model on its own for object merging, and both perform slightly better than SSM on relations because of SSM X  X  poor recall.

We found a significant improvement in both pre-cision and recall when using a combined model over using SSM alone. R ESOLVER  X  X  F1 is 19% higher than SSM X  X  on objects, and 28% higher on relations.
In a separate experiment we found that mutual re-cursion provides mixed results. A combination of SSM and ESP without mutual recursion had a preci-sion of 0.76 and recall of 0.59 on objects, and a pre-cision of 0.91 and recall of 0.35 on relations. Mutual recursion increased recall and decreased precision for both objects and relations. None of the differ-ences were statistically significant, however.
There is clearly room for improvement on the SR task. Except for the problem of confusing similar and identical pairs (see section 5), error analysis shows that most of R ESOLVER  X  X  mistakes are be-cause of two kinds of errors: 1. Extraction errors . For example,  X  X S News X  gets extracted separately from  X  X orld Report X , and then R ESOLVER clusters them together because they share almost all of the same properties. 2. Multiple word senses . For example, there are two President Bushes; also, there are many terms like  X  X resident X  and  X  X rmy X  that can refer to many dif-ferent entities. 6.2 Experiments with Extensions The extensions to R ESOLVER attempt to address the confusion between similar and identical pairs. Experiments with the extensions, using the same datasets and metrics as above, demonstrate that the Function Filter (FF) and the Coordination-Phrase Filter (CPF) boost R ESOLVER  X  X  performance.
FF requires as input the set of functional and one-to-one relations in the data. Table 2 contains a sam-pling of the manually-selected functions used in our experiment. Automatically discovering such func-tions from extractions has been addressed in Ana-Maria Popescu X  X  dissertation (Popescu, 2007), and we did not attempt to duplicate this effort in R E Table 3 contains the results of our experiments. With coordination-phrase filtering, R ESOLVER  X  X  F1 is 28% higher than SSM X  X  on objects, and 6% higher than R ESOLVER  X  X  F1 without filtering. While func-tion filtering is a promising idea, FF provides a smaller benefit than CPF on this dataset, and the merges that it prevents are, with a few exceptions, a subset of the merges prevented by CPF. This is in part due to the limited number of functions available in the data. In addition to outperforming FF on this dataset, CPF has the added advantage that it does not require additional input, like a set of functions. We have shown that the unsupervised and scalable R
ESOLVER system is able to find clusters of co-referential object names in extracted relations with a precision of 78% and a recall of 68% with the aid of coordination-phrase filtering, and can find clus-ters of co-referential relation names with precision of 90% and recall of 35%. We have demonstrated significant improvements over using simple similar-ity metrics for this task by employing a novel prob-abilistic model of coreference.

In future work, we plan to use R ESOLVER on a much larger data set of over a hundred million as-sertions, further testing its scalability and its abil-ity to improve in accuracy given additional data. We also plan to add techniques for handling mul-tiple word senses. Finally, to make the probabilistic model more accurate and easier to use, we plan to investigate methods for automatically estimating its parameters from unlabeled data.
 This research was supported in part by NSF grants IIS-0535284 and IIS-0312988, DARPA contract NBCHD030010, ONR grant N00014-05-1-0185 as well as gifts from Google, and carried out at the Uni-versity of Washington X  X  Turing Center. We thank Doug Downey, Michele Banko, Stef Schoenmack-ers, Dan Weld, Fei Wu, and the anonymous review-ers for their helpful comments on previous drafts.
