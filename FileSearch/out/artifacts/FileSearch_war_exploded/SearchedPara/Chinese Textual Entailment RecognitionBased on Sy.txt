 The Recog n izing Textual Entailment (RTE) challenge focuses on detecting the directi onal entailment relationship between pairs of text expressions, denoted by T (the entailing  X  X ext X ) and H (the entailed  X  X ypothesis X ). We say that T entails H if human reading T would typically infer that H is most likely true.

RTE is proposed as a generic task that captures the semantic inference demand with a wide range of natural language applications. For example, a question an-swering system needs to recognize the text whether entail s a hypothesized answer , e.g., given the question  X  Which te am wo n the N BA championship in 2012 -2013? X , the text  X  James led Miami Heat to their second straight title on June 20,2013 X  entails the hypothesized answer form  X  Miami Heat win the championship in 2012 -2013 X .

RTE has attracted extensive attention ever since it was prop osed, and research-ers have developed many methods to solve thi s problem. These methods can be roug hly classified into five categories. 1. Logic -based recognition Approaches [2, 3] . These approaches map the language expression T (Text) and H (Hypothesis) to logi cal meaning representations  X  T and 
 X  H , then check if  X  H can be infer r ed from  X  T using many kinds of entailment rules and common sense knowledge B possibly by invoking theorem provers. But it is very difficult to convert the language expressions into l ogical forms concern-ing limited performance of current natural language processing tools. 2. Decoding based recognition approaches [4,11]. Given many entailment rules like the following forms,
These approaches are to search for a sequence of rule applications that turn T ex-pression (or its syntactic or semantic representation) to H expression. If such a se-practice. 3. Transformation -based approaches [23 , 24 , 25]. Transformation -based approaches and Hypothesis identical. After the rule set has been exhausted (when either no more changes can be effected by apply rules, or some heuristic limited is reached ), if the Text and Hypothesis match, the entailment pair is labeled as  X  X ntails X , and if they don X  X , it is labeled as  X  X ot entailment X . 4. Alignment and similarity measures based recognition approaches . These ap-proaches are to measure a kind of similarity or distance between text T and hypoth-the similarity or distance with a threshold. The similarity measure used can be structure [9, 10], or latent semantic representations [12, 13]. And quite a few suc-cessful approaches also treat RTE as an alignment problem [8, 21]. Thesaurus like 
WordNet, Hownet [20] can be useful in these approaches. 5. Machin e learning based recognition approaches [5, 6, 14, 15 ] . These approaches t rea t the entailment judgment problem between two texts T and H as a binary clas-sification problem, t hen supervised machine learning methods can be used to make chine learning algorithm train a classifier on manu a lly classified vectors corre-pairs as correct or incorrect textual entailment pairs by examining their features. This paper explores the method s for reco gnizing Chinese textual entailment. The difficult problems for this task include the lack of Chinese textual entailment rules, and word segmentation as well as other language processing errors, and it is also very hard to convert the language expressions T and H into logical forms to infer H from T. Therefore, we use a machine learning based Chinese textual en-tailment recognization method in which a new syntactic tree clipping and match-ing feature we presented is combined with other traditional different si milarity the  X  X inimum information tree X , the matching between T and H can be more accuracy and tolerant of word segmentation error. The experimental result shows that the c lipping on syntactic tree structure is effective for Chinese textual entail-ment recognization.

The remainder of this paper is composed as follows. In section 2 we introduce the rules to clip a syntactic tree structure to a  X  X inimum information tree X  and th e similarity measure between different  X  X inimum information tree X . In section 3 we present other features and machine learning methods used in our system. In section 4 we show the experimental results on the test data and give some analysis. Finally, we su mmarize our work and outline some ideas for future research. Our approach also treat recognizing Chinese textual entailment as a bi nary classi-fication problem. We believe that a Hypothesis H with  X  X imilar X  content to the Text T is more likely to b e entailed by that Text T than one with  X  X ess similar X  content, therefore using matching similarity between T and H s hould be a n im-portant feature for entailment classification. In this paper we match T and H at different levels in cl uding lexical level, sy ntactic level, and shallow semantic level. At syntactic level, firstly we clip and transform the two original syntactic trees of T and H into  X  X inimum syntactic trees X , then search for their common structure and compute matching similarity . 2.1 Clipping synta ctic tree The main idea of syntactic tree clip ping is to delete meaningless nodes by aggre-gating those no d es of syntactic tree . Based on syntactic tree , the first operation is to aggregate the common subsequence into one node. Secondly, aggregate those str ings which can be treated as  X  X ommon similar subtrees X . Finally, w e will get a tree with minimum information by saving relate d links of notes and deleting re-dundant information (nodes without any operation). 2.1.1 Common subsequence aggregation In this ste p , we aggregate all common nodes by searching all subsequences. After this step, some entities can be extracted to reduce the Chinese word segmentation errors and the syntactic tree will be less complex . The following example (Marked as Example 1) is taken from NTCIR -10 X  X  data:
T :  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  4  X  X  X  X  X  X  X  X  X  6300  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  .
 H :  X  X  X  X  X  X  X  X  X  X  X  4  X  X  X  X  X  X  X  6300  X  X  X  X  X  X  X  .
 Two syntactic trees of T and H in the example is as following(ignore all punctua-tions)  X 
C ommon subs equence can fix the errors as E xample 1 shows . In this example,  X   X  X  X  X  X  X   X  should not be separated into three nodes, and we need to treat them as an e ntirety. But even two equal s ingle node s couldn X  X  be treat ed as common subsequence . A fter aggregating the nodes, th e  X  X inimum information trees X  would be as follows : 2. 1.2 Common similar subtree aggregation We define the similar subtrees as such kind s of trees that h ave similar format and generated from syntactic analysis . T hose similar subtrees will be a ggregated dur-ing the step of syntactic tree clip ping . Our approach judges those similar subtrees by single word X  X  overlap . W ith constraint in syntactic structure and co re words X  similarity we can determine whether these subsequences should be aggregated or not . Another example (Marked as Example 2) selected from NTCIR -10: Two syntactic trees of T and H in this example are as follow ing(ignore all punctu-ations)  X  Here  X   X  X  X  X  X  X  X  X  X  X   X  is a common subsequence,  X   X  X  X   X  and  X   X  X  X   X  wou ld be name entities appear ing in each text. The left one with  X   X   X  and  X   X   X  and  X   X  X  X   X   X  co nsist a subtree try to com pare with the subtree which consists of  X   X   X  a nd  X   X  X  X   X  and  X   X  X  X  X   X  . The score of similarity greater than threshold after calcula-tion, then aggregate those nodes into one. The final  X  X inimum information trees X  are: What we had mentioned before, the equation for calculating the similarity be-tween different subtrees is defined as : Here 0  X   X  ,  X   X  1 ,  X  +  X  =1 . Via manual work, when  X  =0 .55 and  X  =0 .45 this similarity work best in distinguishing the synonymy between two subtrees. 2.2 Th e syntactic tree clipping algorithm T he algori thm to clip the original syntactic trees of T and H into minimum infor-mation trees is as follows.
 2.3 The similarity computing be tween minimum information trees
Although using clipping method will decrease the number o f nodes, we still can X  X  just employ statist ical features to make the entailment prediction correctly . The minimum information trees still keep semantic features. The r efore, the fol-lowing Equation (2) for the similarity calculation is necessary: In the equation , SBV stands for the syntactic subject -predicate dependency rela-tion while only considering thos e nod es around root one : NED stands for the result of named entity discrimination:
NED T he sta tistical machine learning models are trained to classify whether T in a giv-en text -hypothesis pair entail H , and some traditional features including statistical and lexical semantic features are also used in these models . The following table 2 and table 3 illustrate these features and c omputational equations for them. W hat need to be illustrated is that although the equations of Ho wN et and Tongyicilin similarity are same , the same formula is only to sum the similarity which has already been calculated. The value of similarity in different feature s calculates in different way s . 4.1 Data and evaluation standards The National Institute of Informatics (NII) of Japan organized the NTCIR [19] RITE (Recognizing Inferenc e in TExt) competition [17] since 2011. RITE is to evaluate one system X  X  ability about recognizi ng specific entailment relationship between two sentences. Therefore, we use NTCIR -10 RITE evaluation d ataset as our experimental data , in which 814 text -hypothesis sentence pairs will employed between every sentence pair has been already labeled as entailment or not entail-ment. We also use Precision, Recall and F -measure as system perfor mance evalu-ation criterion. They are defined as follows. 4.2 Experimental result and analysis We implemented two systems NLPWM -01 and NL PWM -02 for experimental evaluation. NLPWM -01 use s all features mentioned in section 3, and NLPWM -02 add ed further the minimum information tree si milarity feature of text pair . We define those T -H test text pair s in which the e ntailment relationship exists as posi-tive instances, and others as negative instances . Being similar to confusion matrix, Table 4 presents the prediction numbers of two systems for positive and negative test instances respectively. 
F rom Table 4 we can see that the precision increase s after adding the minimum information tree feature, especially for negative test text pairs . The reason for performance improvement can be explain ed as: 1) After clipping the syntactic tree , the noise in recognizing process decreased ; 2) Equation 2 solved some pr oblems such as the only difference in subject or object, reverse of subject and object, and so on.
 Figure 5 shows the achieved F -measure values by different models including Decision Tree, SVM and Na X ve Bayes based on Gaussian distribution machine learning methods [18]. Different machine learning methods cause different result s . SVM which is regard ed as the best supervise d learning method in general text classification didn X  X  obtain the best result. Meanwhile, na X ve Bayes appro ach gets the highest score in both prediction. And apparently, when adding the minimu m information tree feature, the perform ance is always improved under these differ-ent prediction models .

Figure 6 demonstrates the effect of different features in Na X ve Bayes decision model . Features are put into vector in an order from bottom to the top. The m ore features join the vector, the better accuracy would be.

For all features as showed in figure 6, minimum information tree feature makes minimum in formation tree feature is effective in textual entailment classification . Table 5 compare the performance of our systems with those participated NTCIR -10 challenge.
 formance criterion s, MacroF1 is the average value of Y -F1 and N -F1. Items start-ing with  X  Y  X  show the prediction performances of different systems fo r positive negative instances . 
Compare d with bcNLP -CS -BC -03, NLPWM -02 should increase the value of recall in not entailment pairs. And 56.27% shows the weak ness our system copes with not entailment pairs. This points our research direction and inspires us to f ind more reasonable expression on semantic features. T his paper proposed a machine learning based method for Chinese textual entail-ment recogniz ation task . This method integrated lexical, syntactic, and shallow semantic levels of language matching features together. To construct syntactic matching feature, a syntactic tree clipping algorithm is presented to form mini-mum information trees of T and H for matchi ng. The experimental result shows the minimum information tree feature is effective.

The approach still has two deficiencies: First, the time complexity in clipping algorithm isn X  X  good enough; Second, the lack of resource make the system weak in dealing w ith not entailment pairs
Our future work is two -fold. The judgment on not entailment pairs is not suc-cessful, w e therefore should find new feature or new method to express entail-ment relationship better . Meanwhile we also need to focus on the multi -directi on in Chinese textual en tailment recognizing, and this challenge requires entailment system developed in a more robust and reasonable way.
 This work was sponsored by the National Natural Science F oundation of China (No. 61163039, No. 611630 36, No. 61363058 ) and the Young Teacher Research Ability Enhancement Project of Northwest Normal University of China ( NWNU -LKQN -10 -2 ). We thank the anonymous reviewers for their insightful comments.
