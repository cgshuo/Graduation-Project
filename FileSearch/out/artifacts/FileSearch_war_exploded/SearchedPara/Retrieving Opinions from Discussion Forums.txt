 Understanding the landscape of opinions on a given topic or issue is important for policy makers, sociologists, and int el-ligence analysts. The first step in this process is to retriev e relevant opinions. Discussion forums are potentially a goo d source of this information, but comes with a unique set of retrieval challenges. In this short paper, we test a range of existing techniques for forum retrieval and develop new re-trieval models to di ff erentiate between opinionated and fac -tual forum posts. We are able to demonstrate some signif-icant performance improvements over the baseline retrieva l models, demonstrating that this as a promising avenue for further study.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval[Retrieval models] Experimentation, Performance Opinion retrieval, social media, relevance model In this paper, we investigate the retrieval of opinions from social media sources. Formally, given an information need, we retrieve documents that contain topically relevant expr es-sions of opinion. In this context, information needs are ex-pressed as longer grammatical queries, for example; What is causing the real estate crisis in the USA? Note that there are many factual answers to this question, and many more opinions on these answers. In this paper, we present initial research on this problem and explain its idiosyncrasies.
There are a number of activities that could take advantage of high quality opinion retrieval. Some examples include: Each of these high level tasks requires some post-processin g of retrieved documents to extract relevant opinions from do c-uments, such as passage extraction or summarization. In this paper, we focus on the information retrieval technique s, in which the system returns a high quality, ranked set of topically relevant documents containing expressions of op in-ions. Information extraction methods can then be applied to these documents to produce the final output for the par-ticular task.

Retrieving opinions from discussion forums on particular topics and questions is very di ff erent from sentiment analysis of product reviews [13]. Product reviews follow template-l ike language with a small set of attributes expressing pros and cons, which have been successfully tackled with dictionar-ies such as SentiWordNet [2]. In contrast, language that expresses opinions about politics, economics, and ethics i s much more subtle and diverse, and is hard to detect using dictionary-based approaches, as is demonstrated in this pa -per. Generally, opinion retrieval caters to two typical use cases, poll and exploration. For opinion polls, the task is to estimate the fraction of users who agree or disagree with an opinion. This paper addresses the exploration use case, where a portfolio of opinionated answers to a question are to be collected.

Retrieval over discussion forums poses challenges across a number of dimensions. The data is frequently agrammatical and the vocabulary is often invented on the spot. A par-ticular problem we encounter involves the disambiguation of pseudonyms for public figures. The culture of quoting in replies leads to an extreme duplication of content. Al-though most forum posts express opinions, users often quote Wikipedia and news articles in place of an answer. Selection of material is one way of expressing opinion, but we are pri-marily interested direct expressions of first hand opinions.
Previous studies of social media retrieval have used variou s techniques to cope with these aspects of social media search . For example, the thread and conversation structure have been shown to aid retrieval by providing context for short postings [17, 6]. External information sources have also be en shown to be able to improve retrieval performance [7]. In addition, a wide variety of techniques have been proposed to cope with the low quality and high degree of repeated content in microblogs [12, 3]. Sentiment analysis, the act o f determining both if a document expresses an opinion and its polarity is an active area of research [13, 15]. Akkaya et al. [1] study context-based classifiers that distinguish subje ctive from objective use of opinionated words. Okamoto et al. [14] investigates methods of incorporating sentiment anal -ysis into sentence retrieval using contextual data. Huang and Croft [9] explore di ff erent query expansion methods for opinion retrieval on blog data.

In this short paper, we analyze the e ff ectiveness of some of these techniques for opinion retrieval, leading to a retrie val benchmark for opinion retrieval in forum data.

The remainder of this paper is laid out as follows. We be-gin with a description of the query models for forum retrieva l in Section 2.1 and focus on methods for opinion retrieval in Section 2.2. The discussion forum collection and the test sets of opinion-focused queries is described in Section 3. W e detail experimental evaluation in Section 4 before conclud -ing the paper. We start by investigating a wide variety of e ff ective retriev al models. Baseline retrieval models include the query likeli -hood (QL) [16] and the sequential dependence model (SDM) [11 ]. Further, the sequential dependence model is extended using pseudo relevance feedback (RM3) [10].

Recent research has shown that performing query expan-sion using data extracted from Wikipedia can dramatically improve performance for a variety of information retrieval problems [7, 18]. We investigate two methods of expand-ing queries using Wikipedia data. First, we perform pseudo-relevance feedback using RM3 [7, 10] over Wikipedia article s to determine expansion terms (WikiRM3). Second, we use Wikipedia titles and redirection pages to determine synony m sets for query terms (WikiRD) using the method proposed by Xue and Croft [19]. WikiRD candidates are accepted based on the context model score, as proposed by Dang and Croft [5]. When searching for people X  X  opinions on a given topic, we want to bias against forum posts that are objectively talkin g about the topic. In this line of research we mainly care for the valence of the sentiment, independent of its polarity. W e evaluate opinionated expansion methods for SDM, as well as in combination with the forum retrieval methods from Section 2.1.

We explore three query-independent expansion methods which have shown good results for blog sentiment retrieval [9]: The first method (Seed) expands with a fixed set of sentiment seed words:  X  X ood, nice, excellent, positive, fo r-tunate, correct, superior, bad, nasty, poor, negative, un-fortunate, wrong, inferior X . The second method (OP) ex-pands queries with terms that are both frequent in the fo-rum data, and present in the external opinionated corpora General Inquirer 1 and Opinion Finder X  X  subjectivity Lexi-con. 2 Thirdly, we explore a supervised method (SV) which selects terms from the target collection. Candidate terms are selected from the top k documents for each query in a set of training queries. The terms are weighted to maximize the mean average precision over the training queries. As this method requires training data, we employ 2-fold cross-validation within each test collection.

Next, we explore query-specific expansion methods. We start with a variant on pseudo-relevance feedback methods, where each candidate expansion term is required to be presen t in a term list or dictionary. We separately evaluate the re-sources used in Huang X  X  &amp; Croft X  X  work (FiltO) and subjec-tive words in SentiWordNet (FiltS) [2]. 3
Usually, pseudo relevance feedback associates an expan-sion term t for a given query q with a probabilistic weight p trieval probability of the target corpus C .Werecognizethat SentiWordNet contains many words that indicate subjectiv-ity in product reviews but not so much in ethical discus-sions. Instead of adding terms to bias towards subjectiv-ity, it is possible to bias against objectivity. In this stud y, we choose to use news data to discover query-specific objec-tive terms. We study a modified relevance model we call Anti-news Relevance Model (ARM) that requires expansion terms to be in SentiWordNet but also biases against words that are common in a news collection N .Weformalizethis as a likelihood ratio test and expand terms with weights w
C ( t | q )= uses the top words selected by ARM, but weights terms ac-cording to the standard RM weighting p C ( t | q ). Similar to SV, each of these models require training data, we employ 2-fold cross-validation within each test collection
All sentiment expansion methods are intended to be used in a weighted combination with the original query and (non-opinionated) relevance model. All word counts are smoothed with Dirichlet smoothing; all expansion weights are normal -ized to sum to 1, projecting probabilities and likelihood ra -tios onto the same range. To investigate opinion retrieval, we use a large collection of forum data, collected by the Linguistic Data Consortium (LDC). 4 The set of forums crawled discuss a range of issues, with topics ranging from political and news events to med-ical subjects. In this paper, we use a subset consisting of approximately 262 , 000 threads, containing almost 5 . 5mil-lion posts. The subset contains 591 million terms, with a vocabulary of 1 . 1millionuniqueterms.

In the evaluation of opinion retrieval models, we use two query test sets. The first is compiled and judged by LDC to which we refer to as  X  X 1-Eval X . This test set contains 146 questions. A total of 2172 relevant passage-level judgment s are identified by LDC from a pool of rankings that did not include our methods. Since our task is to retrieve documents for passage extraction, we use the best passage judgment as ameasureofdocumentrelevance. http://www.wjh.harvard.edu/  X inquirer http://www.cs.pitt.edu/mpqa
Single word expressions with PosScore + NegScore  X  0.25. http://www.ldc.upenn.edu/ Table 1: Retrieval performance measured using P@10, and nDCG@10. Significant improvement with respect to the SDM baseline is marked + .

We independently compile an additional development test set of 42 opinionated queries from questions posed on Ya-hoo! Answers 5 that have elicited opinionated responses on Yahoo! Answers. The associated user posted answers are not used in the this study. All queries were modified to be grammatically correct questions and are available online. For example: Q1 Who is responsible for the deteriorating economy? Q2 What can be done about Somali Pirates?
Relevance judgments on the forum data are created by us using a pooling method of several supervised and unsuper-vised retrieval methods including approaches in Section 2. 1. Below are some excerpts from relevant forum posts for the above queries.
 Q1 ... the market didn X  X  go down because of the Q2 ... Secondly, allow me to put my position across. The
For each development query, a pool of the top ten returned posts from each retrieval model were judged for three-value d topical relevance. Across the 42 queries, 2099 forum posts were identified as relevant or partially relevant. While jud g-ing a pool of only highly ranked posts yields a relatively shallow pool of judgments, it is enough to di ff erentiate per-formance between some of the retrieval methods. In accor-dance with this annotation process, we focus our analysis on P@10 and nDCG@10.

In addition to the forum data, information extracted from several external data sources are used to complement re-trieval models. A dump of almost 10 million articles from Wikipedia, downloaded in June 2012, is used in some query expansion techniques. As a large, recent collection of news articles, we use a collection of around 134 million news docu -ments, originally collected for use in the TREC 2012 Knowled ge-base Acceleration Track [8]. All retrieval models were implemented using the Indri Searc h Engine [4]. Results based on the pool of judgments of the http://answers.yahoo.com/ http://www.ciir.cs.umass.edu/  X dietz/forum-opinion/ Table 2: Retrieval performance among opinionated expan-sion methods. Significance over SDM baseline is marked + . development set from all retrieval experiments and o ffi cial judgments compiled by LDC on P1-Eval are shown in Ta-ble 1. Statistical di ff erences in this work are tested using a two-tailed paired t-test with level  X  =5%.

The absence of initial training data for forum retrieval prohibits fine-grained parameter tuning for each retrieval model. Instead we use default parameters which achieved good performance on ad-hoc TREC collections: Dirichlet smoothing parameter  X  =2500andsequentialdependence parameters 0.85, 0.1, 0.05.

For the Development query set, we can see that for P@10 and nDCG@10 the combination of the SDM, RM3, and WikiRD is the most e ff ective model, significantly outper-forming SDM on P@10. Metrics on the P1-Eval query set yield low numbers, because the majority of the top 10 re-trieved documents are unjudged. Therefore we also observe only very small di ff erences between each of the models. How-ever, we can clearly see that WikiRD degrades average per-formance over the P1-Eval query set, inspection of per-quer y results reveals that this technique dramatically harms the performance of just 10% of query set, relative to SDM.
Results on opinionated expansion methods are presented in Table 2. Mixing weights between original query, RM, and opinionated expansion model are learned together with the numbers of expansion documents and terms via 2-fold cross-validation. On both data sets we observe a consistent improvement with opinionated filtering of relevance model expansion terms. We note that combining SDM with each query independent expansion model deteriorates performan ce, relative to SDM alone. The query-dependent expansion tech-niques, however, show some small, but promising improve-ments over the SDM baseline.

We show detailed results and expansion terms for the test question, What is the republicans solution for the healthcare system? ,inTable3.Wedisplaycumulativeperformancein-creases when adding RM, FiltS, and WikiRD to SDM. We observe this improvement in more than half of the develop-ment queries, and this improvment is further reflected in as asignificantimprovementovertheP@10metric.

Inspecting the lists of expansion terms from the di ff er-ent methods for this example query, we confirm that FiltO selects very opinionated expansion terms, and ARM iden-tifies forum-typical expressions. FiltS includes many topi -cal words. However, even in combination with opinion ag-nostic RM3, FiltS achieves better performance than FiltO and ARM on the complete test set. We observe that query-specific expansion techniques perform consistently better than query-independent expansion techniques. In this paper, we present initial research on retrieving opi n-ions in discussion forum data. We encountered several in-teresting problems relating to the nature of social media. In particular, we found several issues relating to the noisy nature of forum data, duplication of content, and verbatim replication of news and Wikipedia articles.

We observe that many successful retrieval models for ad-hoc web retrieval do not outperform baseline retrieval mod-els for this task, indicating that this problem deserves atten-tion. We achieved good results with a filter-approach based on a list of sentiment words from product reviews, although they contain many words of topical relevance. Methods for subjectivity word sense disambiguation [1] may help distin -guish those cases. However, we note that even in combina-tion with RM3 models, expansion using mix of topical and opinonated words (FiltS) achieves better performance than filtering with strictly opinionated words (FiltO).
This initial research demonstrates that opinion retrieval remains an open problem for information retrieval. Future work will include further study on how to promote  X  X ood X  and suppress  X  X ad X  expansion terms from external corpora, such as news and Wikipedia articles.
 This work was supported in part by the Center for Intelli-gent Information Retrieval, in part under subcontract #19-000208 from SRI International, prime contractor to DARPA contract #HR0011-12-C-0016, and in part by NSF grant #CNS-0934322 Any opinions, findings expressed in this ma-terial are the authors X  and do not necessarily reflect those o f the sponsor.
 [1] Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
 [2] Stefano Baccianella, Andrea Esuli, and Fabrizio [3] Jaeho Choi, W. Bruce Croft, and Jin Young Kim. [4] W. Bruce Croft and Jamie Callan. The Lemur Project. [5] Van Dang and W. Bruce Croft. Query reformulation [6] Jonathan L. Elsas and Jaime G. Carbonell. It pays to [7] Jonathan L. Elsas, Jaime Arguello, Jamie Callan, and [8] J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, [9] Xuanjing Huang and W. Bruce Croft. A unified [10] Victor Lavrenko and W. Bruce Croft. Relevance based [11] Donald Metzler and W. Bruce Croft. A Markov [12] Donald Metzler, Congxing Cai, and Eduard Hovy. [13] Tetsuya Nasukawa and Jeonghee Yi. Sentiment [14] Takayoshi Okamoto, Tetsuya Honda, and Koji Eguchi. [15] Bo Pang and Lillian Lee. Opinion mining and [16] Jay M. Ponte and W. Bruce Croft. A language [17] Jangwon Seo, W. Bruce Croft, and David A. Smith. [18] Yang Xu, Gareth J.F. Jones, and Bin Wang. Query [19] Xiaobing Xue and W. Bruce Croft. Generating
