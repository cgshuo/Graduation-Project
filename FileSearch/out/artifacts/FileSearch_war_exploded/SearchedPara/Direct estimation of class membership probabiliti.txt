 Kazuko Takahashi  X  Hiroya Takamura  X  Manabu Okumura Abstract Accurate estimation of class membership probability is needed for many applica-tions in data mining and decision-making, to which multiclass classification is often applied. Since existing methods for estimation of class membership probability are designed for binary classification, in which only a single score outputted from a classifier can be used, an approach for multiclass classification requires both a decomposition of a multiclass classifier into binary classifiers and a combination of estimates obtained from each binary classifier to a target estimate. We propose a simple and general method for directly estimating class membership probability for any class in multiclass classification without decomposition and combination, using multiple scores not only for a predicted class but also for other proper classes. To make it possible to use multiple scores, we propose to modify or extend represen-tative existing methods. As a non-parametric method, which refers to the idea of a binning method as proposed by Zadrozny et al., we create an  X  X ccuracy table X  by a different method. Moreover we smooth accuracies on the table with methods such as the moving average to yield reliable probabilities (accuracies). As a parametric method, we extend Platt X  X  method to apply a multiple logistic regression. On two different datasets (open-ended data from Japanese social surveys and the 20 Newsgroups) both with Support Vector Machines and naive Bayes classifiers, we empirically show that the use of multiple scores is effective in the estimation of class membership probabilities in multiclass classification in terms of cross entropy, the reliability diagram, the ROC curve and AUC (area under the ROC curve), and that the proposed smoothing method for the accuracy table works quite well. Finally, we show empirically that in terms of MSE (mean squared error), our best proposed method is superior to an expansion for multiclass classification of a PAV method proposed by Zadrozny et al., in both the 20 Newsgroups dataset and the Pendigits dataset, but is slightly worse than the state-of-the-art method, which is an expansion for multiclass classification of a combination of boosting and a PAV method, on the Pendigits dataset.
 Keywords Multiclass classification  X  Class membership probabilities  X  Accuarcy table  X  Logistic regression  X  Direct estimation  X  Multiple classification scores 1 Introduction When a classifier predicts a class for an evaluation sample, an accurate estimation of probabil-ity with the sample belonging to the predicted class (class membership probability) is needed in many applications in natural language processing, pattern recognition, and other impor-a real-world example for human decision-making, we describe the need of class membership probabilities in  X  X n automatic occupation coding system X  in social surveys. The occupational coding is a task for various statistical analyses in sociology, in which researchers assign the most appropriate occupational code to each occupational data collected as responses to open-ended questions in social surveys [ 9 ]. Because the total number of occupational codes is in the hundreds or thousands according to a coding structure used in a survey, the task is extremely troublesome for human annotators (coders). To help coders, we have developed an automatic occupational coding system with machine learning [ 32 ], as well as with a system called the NANACO system [ 33 ], which displays outputs from the automatic system as candidates of occupational codes. The NANACO system is currently being applied in important social sur-veys in Japan such as in the JGSS 1 and in the 2005 SSM survey 2 [ 34 ]. The coders have asked us to supply a measure of confidence for the first-ranked candidates and for other ranked candidates 3 to help the candidates make better decisions.

Although the class membership probabilities can be estimated easily using classifica-tion scores outputted from a classifier (hereafter referred to as scores ), 4 [ 19 ] and estimates should be calibrated because they are often quite different from true values. There are several approaches for estimating an accurate class membership probability for binary classification. Representative methods are PETs by Provost and Domingos [ 25 , 26 ], Platt X  X  method [ 24 ], and various methods by Zadrozny et al. [ 16 , 39 , 42 ]. PETs are applied to decision trees, smoothing each class membership probability in decision trees with the Laplace method without prun-ing [ 25 , 26 ]. Platt X  X  method is basically applied to Support Vector Machines (SVMs), using a sigmoid function to transform scores into the probabilities [ 24 ]. The methods by Zadrozny et al. are a  X  X inning X  method for naive Bayes classifiers [ 39 ], Isotonic regression via a PAV algorithm (hereafter referred to as a PAV method ) for SVMs and naive Bayes classifiers [ 42 ], and a Probing algorithm to reduce learning as an estimator of class membership probability on an extended line of binning [ 16 ]. Other methods without transforming a score to class membership probability are bagging and boosting [ 16 , 20 , 25 , 26 , 38 ]. Furthermore, combi-nation methods of bagging or boosting and a method for estimation by transformation are effective, and B-PETs (Bagging PETs) is a representative method for decision trees [ 23 , 26 ]. A combination of boosting and a PAV method (hereafter referred to as a PAV boosted method ) is also effective [ 20 , 42 ].

A problem in these existing methods is that because these existing methods are designed for binary classification, an approach for multiclass classification, which is often applied to many applications in data mining and decision-making, requires both a decomposition of a multiclass classifier into binary classifiers and a combination of estimates obtained from each binary classifier to a target estimate [ 41 , 42 ]. We propose a simple general method for directly estimating class membership probability for any class in multiclass classification without decomposition, using multiple scores not only for a predicted class but also for other appropriate classes. Using multiple scores is effective for an accurate estimation of class membership probability, when a multiclass classifier outputs a score for each class. The reason for the effectiveness is that a predicted class is determined not by the absolute value of the score, but by its relative position among the scores. For example, even if the score for the first-ranked class (hereafter referred to as the first-ranked score ) is low, as long as the score for the second-ranked class (hereafter referred to as the second-ranked score )isvery low (the difference between the two classes is large), the classification output will be reliable. In contrast, even when the first-ranked score is high, if the second-ranked score is equally high (a negligible difference in score between the two classes), then the classification output is unreliable. Therefore, the class membership probabilities are likely to depend not just on the score for the target class, but also on other scores.

Because only a single score can be dealt in any existing method, we need novel methods for using multiple scores. Modifying or extending representative existing methods, we propose two kinds of methods using multiple scores as follows. One of these two is a non-parametric method, which we construct referring to a binning method by Zadrozny et al. However, we create an  X  X ccuracy table X  by different a method from their method, and moreover smooth accuracies on the original accuracy table with methods such as the moving average to yield reliable probabilities (accuracies). The other is a parametric method, we construct by extend-ing Platt X  X  method to apply a multiple logistic regression.

In the binning method by Zadrozny et al., various bins were generated depending on the number of samples included for a bin, and they experimentally selected the most effective table [ 39 ]. Similarly, in the method using an accuracy table, we need to select the best accuracy table of all generated tables. we also experimentally select the best accuracy table. Another problem in using multiple scores is that we need to select an effective combination of scores whether parametric or not. We also experimentally solve this problem.

The remainder of the paper is organized as follows. In Sect. 2 , we describe the existing methods for estimating class membership probabilities. In Sect. 3 , using multiple scores, we propose both a non-parametric method and a parametric method for directly estimating the class membership probabilities of any predicted class in multiclass classification. In Sect. 4 , we describe experimental settings and in both Sects. 5 and 6 , we show that the use of multiple scores is effective for any class in multiclass classification on different datasets. We show that the proposed methods are effective for the first-ranked class and the other classes in Sects. 5 and 6 , respectively. In Sect. 7 , to show the effectiveness of our proposed method, we compare our best proposed method with both an expansion for multiclass classification of a PAV method and an expansion of a PAV boosted method. Finally, in Sect. 8 , we conclude this paper and discuss some directions for future work. 2 Related work In Sect. 2 , we first describe Platt X  X  method, the binning method, and a PAV method (Isotonic regression via a PAV algorithm) for binary classification. Next, we describe how to reduce multiclass classification to binary classification by Zadrozny et al. 2.1 Platt X  X  method To estimate class membership probabilities for SVMs, Platt [ 24 ] proposed using a sigmoid function P ( f ) = 1 / { 1 + exp ( Af + B ) } with the score f because f is substituted into a monotonically increasing sigmoid function. The parameters A and B were estimated with the maximum likelihood method beforehand. To avoid overfitting the train set, Platt used an out-of-sample model. Using five types of datasets from Reuters [ 11 ] and other sources, a good experimental result was obtained for probability values. Bennett [ 3 ], however, using the Reuters dataset showed that the sigmoid method could not fit the Naive Bayes scores. 5 Bennett proposed other sigmoid families in approximating the posterior distribution given naive Bayes classifiers log odds approximation. Zadrozny et al. [ 42 ] also showed that Platt X  X  method could not be applied for some datasets 6 and proposed a different approach. 2.2 Binning method Zadrozny et al. [ 39 ] proposed the discrete non-parametric binning method, which estimates class membership probabilities by referring to the  X  X ins X  made for naive Bayes classifiers beforehand. In the notable binning method, Zadrozny et al. used the first-ranked scores of training samples, rearranged training samples according to their scores, and then made bins with equal samples per bin. The method is described as follows. First, samples are rearranged in order of the values of their scores, and intervals are created to ensure that the number of samples falling into each bin equals a fixed value. For each bin Zadrozny et al. computes lower and upper boundary scores. Next, the accuracy of samples in each bin is calculated. Finally, an evaluation of the new sample is done using the score of the sample to find a matching bin, and the accuracy of the bin is then assigned to the sample. Using KDD X 98 datasets, a good experimental result was obtained in terms of certain evaluation criteria such as MSE (mean squared error) or average log-loss (bin = 10). The method has a problem, however, in answering how the best number of bins can be determined. On an extended line of binning, Zadrozny et al. proposed the  X  X robing X  algorithm for reducing the error rate for binary classification [ 16 ]. 2.3 PAV method (Isotonic regression via a PAV algorithm) Based on a monotonic relationship between classifier scores and accuracies, Zadrozny et al. [ 42 ] next proposed a method via a PAV (Pair-Adjacent Violators) algorithm, which has been widely researched for problems of Isotonic regression. A PAV method (Isotonic regression via a PAV algorithm) may be interpreted as a method in which each bin exist for each sample. As a result of experiments for SVMs and naive Bayes classifiers, a PAV method performed slightly better than the sigmoid method, while the PAV method always worked better than the binning method.

Niculescu-Mizil et al. [ 19 ] compared 10 classifiers for calibration using Platt X  X  method with a PAV method using 8 datasets from UCI and other sources, and showed that Platt X  X  method was most effective when the data were small, while a PAV method was more powerful when there was sufficient data to prevent overfitting. Jones et al. [ 12 ] compared a PAV method with the sigmoid method, and showed that the sigmoid method outperformed the PAV method by root-mean squared error and log-loss. The reason for the outperformance was that the PAV method tended to overfit. As for boosting, Niculescu-Mizil et al. showed that both Platt X  X  method and the PAV method improved the probabilities predicted by any boosting model, while Logistic Correction worked well only with a weak boosting model such as decision stumps [ 20 ]. 2.4 Expansion for multiclass classification To extend binary classification for multiclass classification, Zadrozny proposed using a code matrix, M  X  X  X  1 , 0 , + 1 } k  X  l ,where k is the number of classes and l is the number of binary classification problems, which is a generalization of the Error-Correcting Output Code (ECOC) scheme proposed by Dietterich and Bakiri 7 [ 41 ]. Table 1 shows a sample of the all-pairs code matrix in the three classes. First Zadrozny decomposed a multiclass classifier into binary classifiers by a one-against-all code matrix or an all-pairs code matrix. Next Zadrozny calibrated the predictions from each binary classifier and combined them by some of a least-squares method, a coupling method, 8 and a normalization method 9 to obtain a target estimate. In this expansion of binary classification for multiclass classification, there remained the question whether the calibration of the class membership probability estimation given by the binary classifiers improves the calibration of the probabilities for multiclass classification [ 41 ].

Zadrozny et al. applied the expansion algorithm of binary classification for multiclass classification for naive Bayes classifiers with a PAV method, boosting, and a PAV boosted method, respectively [ 42 ]. Zadrozny et al. used the 20 Newsgroups dataset [ 2 ] 10 for inves-tigating the effectiveness of an expansion of the PAV method, while they used the Pendigits dataset [ 2 ], 11 which has ten classes, for comparison of four methods including an expansion of naive Bayes classifiers. Through experiments, Zadrozny et al. used one-against-all code matrix because the previous research [ 27 ] had found that one-against-all had performed as well as other code matrices for the 20 Newsgroups dataset in terms of the error rate [ 42 ]. Experiments showed that an expansion of a PAV boosted method was the best in these meth-ods in terms of both MSE and the error rate, but it was slightly better than an expansion of boosting, which was about better than an expansion of a PAV method. Although an expansion of a PAV method was better than that of naive Bayes classifiers in terms of MSE, it was not good in terms of the error rate. 12 3 Proposed method We propose two kinds of methods using multiple scores. One is a non-parametric method using the accuracy table described in Sect. 3.1 , and the other is a parametric method applying a logistic regression as described in Sect. 3.2 . 3.1 The method using an accuracy table We show Table 2 as an example of an accuracy table, which shows the accuracy values using two scores, a pair of first-ranked scores f 1 , and the second-ranked scores f 2 ,onSVMs.In f 2 in SVMs, a cell does not exist, nor does an accuracy when f 1 is smaller than f 2 .Ifin a test sample, f 1 , f 2 is 0 . 8,  X  0 . 6, respectively, by the method using an accuracy table, the class membership probability for the first-ranked class of the sample is estimated from the accuracy table as 0 . 67.
 The process of the method using an accuracy table is as follows.
 STEP 1 Create cells for an accuracy table.
 STEP 2 Smooth accuracies.
 STEP 3 Estimate class membership probability for an evaluation sample. 3.1.1 STEP1 To generate an accuracy table, we need a pair of scores and classification status (incor-rect/correct) for each sample. To obtain these pairs, we divide the whole of the training dataset into two datasets: (a) a training dataset to make  X  X n accuracy table X  and (b) a test dataset for the table. We employed cross-validation. For example, in a five-fold cross-validation, we divide the whole training data into five groups, and use four-fifths of the data to build a classifier with the remaining one-fifth of the data used to output scores from the classifier; we repeat the process four more times (a total of five times) by changing the data used for training and outputting the score to make an accuracy table.

We create cells for an accuracy table as follows. First, the score is used as an axis, divided into even intervals. For example, the size of an interval may be 0.1 on SVMs. When using multiple scores, this step takes place for each score. When we use the first-ranked scores and the second-ranked scores, we split a rectangle into several intervals. Second, we decide, on the basis of the score, to which cell each training sample belongs. Finally, we check the classification status (correct/incorrect) of the training samples in each cell and calculate the accuracy of that cell, that is, its ratio of correctly classified samples. In this method, an accuracy table can be made for any number of scores (dimensions) used because the training samples do not need be sorted according to their scores to create cells. However, this proposed method has a similar problem as Zadrozny X  X  binning method in that we can discover the best size of cell intervals only by experiments. Furthermore, because the number of samples for a cell may be different, the reliabilities of accuracies in cells are probably variant. To solve the problem of reliabilities, we use coverage for each cell as weight. 3.1.2 STEP2 The original accuracy table generated above does not yield reliable probabilities (accuracies), when there are no or very few samples for some cells. Therefore, we propose smoothing on the original accuracy table. There are simple smoothing methods such as Laplace X  X  law (Laplace) and Lidstone X  X  law (Lidstone) [ 13 ]. In this paper, we denote, for an observed cell c ( f ) in which f is the classification score, the number of training data samples that appear in the cell by N ( c ( f )) , and denote the number of correctly classified samples within all the samples in the cell by N p ( c ( f )) . The smoothed accuracy P Lap ( f ) is formulated as P The value of  X  for Lidstone was determined for each accuracy table, using cross-validation within the training data.

In both Laplace and Lidstone, accuracies are smoothed using solely the samples in the cell in question. However, further examination of the entire accuracy table shows that nearby cells fairly often have similar accuracies. Therefore, using the accuracies of cells near the target cell should be effective. We apply some smoothing methods such as the moving average method (MA) and a median method (Median) [ 1 ], which use values near the smoothing value target. The MA and Median are computed according to the following formula: where Nb ( c ( f )) is the set of cells that are adjacent to cell c ( f ) whose accuracy can be defined (i.e., there is at least one sample), and n gives | Nb ( c ( f )) |+ 1. Furthermore, we propose an extended MA, the moving average with coverage method (MA_cov), in which cells with many samples are more weighted in accuracy computation because the accuracy of those cells are more reliable.
 where C ( c ( f )) is the number of the samples in the cell c ( f ) divided by the number of all the samples. In this paper, we simply use the cells neighboring the target cell as surrounding cells. For example, in the case of using the first-ranked score and the second-ranked score, we use nine cells; the up cell, down cell, left cell, right cell, the diagonal cells and the target cell. 3.1.3 STEP3 We first calculate the classification scores of the sample, then find the range or cell that corresponds to the scores, and output the values associated in the range or cell in the accuracy table. 3.2 The method applying a logistic regression In the method applying a logistic regression, we extend the sigmoid function used Platt X  X  method to the formula: where f i represents the i th-ranked classification score. The parameters A i (  X  i )and B are estimated with the maximum likelihood method. The process of the method applying a logistic regression is as follows.
 STEP 1 Estimate parameters with the maximum likelihood method.
 STEP 2 Estimate class membership probability for an evaluation sample. 3.2.1 STEP1 We need a pair of scores and classification status (incorrect/correct) for each sample to estimate parameters A i (  X  i )and B in the formula ( 4 ) with the maximum likelihood method. With the same method mentioned in STEP1 in Sect. 3.1 , to obtain these pairs, we divide the whole of the training dataset into two datasets: (a) a training dataset to generate a classifier for estimating parameters and (b) a test dataset for estimating parameters. We employed cross-validation by the same method mentioned in STEP1 in Sect. 3.1 . 3.2.2 STEP2 in the formula ( 4 ) for the classification scores and obtain the value of P Log ( f 1 ,..., f r ) . 4 Experiments 4.1 Purposes of experiments The purposes of the experiments were to show that the simple and general proposed method using multiple scores described in Sect. 3 was effective in multiclass classification and that the best proposed method was more effective than existing methods. For these aims, in multiclass classification, we needed to discover not only an effective method for estimation, but also an effective combination of scores used. We simply assumed that the best method for some predicted class, e.g., the first-ranked class, was also the best for another class in multiclass classification. First, in Sect. 5 , we conducted experiments to discover the best method of all for the predicted first rank class, in which we could more easily control combinations of scores used because the number of the combinations of scores used was smaller than in the predicted other rank classes in multiclass classification. Next, in Sect. 6 , based on results of the experiments in Sect. 5 , we conducted experiments to discover the best combination of scores used for the other classes, using the best method for the first-ranked class. 4.2 Experimental settings 4.2.1 Classifier We used SVMs, and also used naive Bayes classifiers for experiments to show the generality of the proposed method. The reason why we selected SVMs was that SVMs are widely applied to many applications in various areas for excellent effectiveness [ 8 , 11 , 31 , 37 ]. Because SVMs are a binary classifier, we used the one-versus-rest method (the one-against-all method) [ 15 ] to extend SVMs to a multiclass classifier. 13 Following Takahashi et al. [ 33 ], we set the SVMs kernel function to a linear kernel. 4.2.2 Dataset We used two different datasets: the JGSS dataset, which is Japanese survey data, and UseNet news articles (20 Newsgroups), which were also used in Zadrozny et al. experiments [ 42 ]. 14 First, we used the JGSS dataset (23,838 samples) taken from respondents who had occupa-tions [ 33 ]. Each instance of the respondents X  occupational data consists of four answers:  X  X ob task X  (open-ended), and  X  X ndustry X  (open-ended), both of which consisted of much shorter texts than usual documents, and have approximately five words in each, and  X  X mployment status X  (close-ended), and  X  X ob title X  (close-ended). We used these features for learning. The number of categories was nearly 200 and by past occupational coding, each instance was encoded into a single integer value called an occupational code. We used JGSS-2000, JGSS-2001, JGSS-2002 (20,066 samples in total) for training, and JGSS-2003 (3,772 samples) for testing. The reason why we did not use cross-validation is that we would like to imitate the actual coding process; we can use the data of the past surveys, but not of future surveys. To generate an accuracy table, we used a five-fold cross-validation within the training data; we split 20,066 samples into five subsets of equal size, used four of them for temporary training and the rest for outputting the pairs of the scores and the status, and repeated this process five times with different combinations of four subsets. We used all the pairs to make an accuracy table.

The second dataset, the 20 Newsgroups dataset, consists of 18,828 articles after duplicate articles are removed. The number of categories is 20, corresponding to different UseNet discussion groups [ 21 ]. We employed a five-fold cross-validation. 4.2.3 Cell intervals We experimentally determined the best cell intervals in the method using an accuracy table. For these experiments, we created some accuracy tables with different cell intervals: 0.05, 0.1, 0.2, 0.3, and 0.5, etc. For example, Table 3 shows the relationships of cell intervals and the number of cells in the case of the first-ranked scores used. 4.2.4 Evaluation metrics We used cross entropy of test data to evaluate each method according to Langford and Zadrozny [ 16 ] and Niculescu-Mizil and Caruana [ 4 , 19 , 20 ] in Experiment 1. Cross entropy number of test instances, p i is the estimation of class membership probability of the i th test instance, and y i is the status (incorrect/correct) of the i th test instance, which is defined to be 1 if the status is correct, and 0 if the status is incorrect. Smaller values of cross entropy are considered to be better. As an evaluation method in Experiment 2, we used a reliability diagram, a ROC (receiver operating characteristic) curve and an AUC (area under the ROC curve), reliability for each coverage, accuracy for each threshold, and the ability to detect misclassified samples. 5 Experiments for the first-ranked class in multiclass classification 5.1 Experiment 1: comparison of the methods 5.1.1 Effectiveness of the proposed method for creating cells Before Experiment 1, we conducted simple experiments to confirm the effectiveness of the proposedmethodformakingcellswithequalcellintervalsbycomparingtheproposedmethod with the method with equal samples for each cell. We used the values without smoothing and used only the first-ranked scores. Table 4 shows the results in the best cases by changing the number of cells from 7 to 60. The tendencies in other cases with the different number of cells were much the same as in Table 4 . A possible reason is that although a method dividing cells with equal samples is generally better than a method diving them with equal cell intervals, the method dividing them with equal samples has a large bias from the point of estimating density [ 14 ]. Thus, we confirmed that the effectiveness of the proposed method for creating cells. 15 5.1.2 Evaluations by cross entropy Tables 5 and 6 show cross entropy of the JGSS dataset and the 20 Newsgroups dataset by the proposed methods as well as by other method for different numbers of used scores and different intervals on SVMs, respectively. 16 The Lidstone column shows the result when the predicted optimal value of  X  is used. The dash ( X ) indicates that we cannot compute cross entropy for those cases because the argument of the log function in some cells was 0. We discuss the results in Tables 5 and 6 . First, for the SVMs, the best case for both datasets was the method using both the first-ranked score and the second-ranked score, in which we applied the moving average with coverage method (cell intervals = 0.1). Second, for each method we obtained better cross entropy when we used multiple scores than when we used a single score. In particular, using both the first-ranked score and the second-ranked score was the best for both datasets. The reason for using both scores is that in multiclass classification, the probability of the first-ranked class depends not only upon the first-ranked scores, but also upon the second-ranked scores as mentioned in Sect. 1 . Third, in the case of using multiple scores with smaller cell intervals (e.g., 0.1), smoothing methods such as MA or MA_cov, which use the accuracies of cells near the target cell, were more effective than the other methods.

To show the generality of the above conclusions, we conducted experiments of the same kind as in the previous experiments, except for Lidstone, using naive Bayes classifiers for the 20 Newsgroups dataset. In Table 7 , the dash ( X ) indicates the same meaning as in Tables 5 and 6 . We obtained the same results as shown in Tables 5 and 6 . First, the best of all cases was the method using both the first-ranked score and the second-ranked score, in which we smoothed using the moving average method with a larger number of cells (e.g., 30). Second, for each method we obtained better cross entropy when we used multiple scores than when we used a single score. Third, in the case of using multiple scores with a larger number of cells (e.g., 30), smoothing methods such as MA or MA_cov were effective.

We obtained results of the method where we applied a logistic regression using multiple scores as shown in the second parts of Tables 5 and 6 . In the method applying a logistic regression, we also showed that the values of cross entropy were better when we used multiple scores than when we used a single score. The method applying a logistic regression showed an average performance in the methods in the two tables in each dataset.

Finally, we observed that it is more difficult to estimate class membership probabilities for the JGSS dataset than for the 20 Newsgroups from the results in both Tables 5 and 6 ,and that it is more accurate to estimate class membership probabilities on 20 Newsgroups with SVMs than with naive Bayes classifiers from the results in both Tables 6 and 7 . 5.2 Experiment 2: evaluation of the proposed method 5.2.1 Reliability diagram, ROC curve and AUC We used the reliability diagram, the ROC curve, and the AUC to evaluate the proposed method for the first-ranked class, which was the best method of all cases described in Sect. 4.1 .To plot a reliability diagram, we used an average of the estimates of the samples in each interval estimates as a true value ( y ). Figure 1 shows three reliability diagrams in the JGSS dataset. In the proposed method, all points were near the diagonal line. In the reliability diagram, when a point is the farthest from the diagonal line, the performance is the worse. As a whole, the proposed method was better than both a method without smoothing and the method applying a logistic regression. Figure 2 shows that this tendency was the same as on the 20 Newsgroups dataset.

Both Figs. 1 and 2 show three ROC curves in each dataset. A ROC curve evaluates classifier performance in terms of a FPF (false positive fraction) ( x ) and a TPF (true positive fraction) ( y ). Therefore, on a ROC curve, when the ROC curve is nearer to the upper left line, a method is better. Table 8 shows the AUC in both Figs. 1 and 2 . As for the AUC, when the value of the AUC is larger, a method is better. The proposed method was the best of the three methods in both datasets. 5.2.2 Ability to detect misclassified samples We ordered all test instances by ascending order of the estimated class membership proba-bility and counted the number of error samples in the set of samples with low probability. We compared our method with the raw score method [ 30 ], in which the distance from the sep-aration hyperplane is used instead of the probability. We evaluated these methods by the ratio of the detected errors. Figure 3 shows the number of error samples detected by the proposed method and those by the raw score method in both datasets. The proposed method always surpassed the raw score method in both datasets. In the 20 Newsgroups dataset especially, the proposed method performed better when coverage was lower, which is desirable for us, since, in practice, we would like to find as many errors as possible by manually checking only a small amount of data. The reason for the difference of the two methods is not clear in the JGSS dataset. A possible explanation would be that the JGSS dataset has many very short samples, among which there are only a few active features. Those short samples do not have enough information for precise probability estimation.

Through experiments on two different datasets, for the first-ranked class in multiclass classification, the use of multiple scores was more effective than the method using a single score, whether parametric or not. In particular, the method using the best accuracy table was the most effective. The best accuracy table was created by using both the first-ranked score and the second-ranked score and was smoothed with the moving average with coverage method or the moving average method. The cell interval of the best accuracy table was 0 . 1, where the number of cells for the first-ranked class was thirty. However, because we experimentally discovered the best accuracy table, better accuracy tables may exist than this accuracy table. Therefore we need to discover theoretically the best accuracy table, using information criteria such as AIC (Akaike Information Criteria) [ 29 ]. The method using an accuracy table was not stable, and depended on cell intervals and smoothing methods, while the method applying a logistic regression was stable, which was slightly worse than the method using the best accuracy table. 6 Experiments for the other classes in multiclass classification 6.1 Candidates of effective combination of scores used For the other classes in multiclass classification, the problem of which scores should be combined with a predicted class X  X  score is more complicated than for the first-ranked class. If a score is strongly related with class membership probabilities for a predicted class, an absolute value of a correlation coefficient between a score with a predicted class X  X  status (incorrect/correct) might be large. To obtain candidates of effective scores, we computed correlation coefficients between scores with each of the nineteen predicted classes X  status fromthesecond-rankedtothetwentieth-ranked. 17 Table 9 showsthatforthepredictedclasses X  status, the first-ranked score was the most effective in the twenty scores including a predicted class X  X  score, and that the predicted class X  X  score itself was more effective. For the predicted classes X  status, the front-ranked score or the next-ranked score was sometimes effective. For example,thenumberofthefirst-rankedclasseswiththelargestabsolutecorrelationcoefficient between a score with a predicted class X  X  status is sixteen, which is 80% of all classes, in the 20 Newsgroups dataset. A possible explanation would be that because the first-ranked class X  X  score is the largest of all the scores and any class X  X  score is always less than the first-ranked class X  X  score, any predicted class X  X  status might be depend on the first-ranked class X  X  score.
As candidates of effective combinations of multiple scores used for the other classes, we selected three kinds of combinations below and conducted experiments with these combina-tions.

Combination A a predicted class X  X  score and the first-ranked score (The number of scores
Combination B from the first-ranked score to a predicted class X  X  score 18 (The number
Combination C a predicted class X  X  score, the front-rank of a predicted class X  X  score and 6.2 Experiment 1: comparison of the combinations of scores used Although the method using an accuracy table was most effective for the first-ranked class when we used the best accuracy table, generating a best accuracy table suitable for each class in multiclass classification, respectively, might be difficult because the tendency of fluctuation in any of the ranked scores is different from each other. 19 In contrast, the method applying a logistic regression was more stable. Therefore, for the other classes, we first compared three combinations of scores, using the method applying a logistic regression to discover the best combination of scores in Sect. 6.2 . Then, by the best combination of scores, we conducted experiments to confirm the effectiveness of the method applying a logistic regression, compared with the method using an accuracy table in Sect. 6.3 . To obtain class membership probabilities for the other classes, we used appropriate values of f i in formula ( 4 ) according to each combination method. 6.2.1 Evaluations by cross entropy Figure 4 shows values of cross entropy in the JGSS dataset and the 20 Newsgroups dataset by the three combination methods in Sect. 6.1 as well as the method using only a single score of a predicted class ranked from the second class to the twelfth class on SVMs, respectively. 20 First, for the second-ranked class, all the methods using multiple scores were more effective than the method using a single score in both datasets. Although Combination A and Combi-nation B were always effective, Combination C was as bad as the method using a single score for the predicted classes ranked lower than the second-ranked class in both datasets. For any class, Combination A was the most effective of all three combinations and the method using a single score in both datasets. Second, for classes ranked lower than the second-ranked class, values of cross entropy were small, and as the rank of a predicted class became lower, dif-ferences among the combinations became smaller in both datasets. A possible reason could be that for classes ranked lower than the second-ranked class, values of cross entropy by any combination are small because predicted classes tend to be inclined toward the incorrect, 21 and class membership probabilities by any combination are generally smaller. 6.2.2 ROC curve and AUC Figure 5 shows the ROC curves of the three combination methods and the method using a single score for classes of the second-ranked and the third-ranked in the JGSS dataset and in the 20 Newsgroups dataset, respectively. 22 Table 10 shows the AUC in Fig. 5 . In each class, the methods using multiple scores were more effective than the method using a single score for any predicted class except Combination C of the third-ranked class in the JGSS dataset. In particular, Combination A was always effective in the JGSS dataset, while Combination C (for the second-ranked class) and Combination B (for the third-ranked class) were effective in the 20 Newsgroups dataset. 6.2.3 Changing size of training data for estimating parameters To investigate the robustness of the method using a single score, we compared values of cross entropy in the combination of scores with the method using a single score for the second-ranked class where a difference of cross entropies of the methods was largest, changing the size of the training data to estimate parameters in the formula: 1000, 2000, 4000, 10000, and 20000 in the JGSS dataset. Figure 6 shows the values of cross entropy for both Combination A and the method using a single score in each size of a training data. First, Combination A was always more effective than the method using a single score, and the difference of cross entropies of the methods became larger as the size of the training data became larger. Second, the method applying a logistic regression was stable and effective if the size of the training data to estimate parameters in the formula was reduced to only 1000.

The use of multiple scores was also effective in estimating class membership probabilities for the other classes with the method applying a logistic regression on SVMs. In particular, the use of both a predicted class X  X  score and the first-ranked score was most effective, and the use of scores from the first-ranked to a predicted class was more effective. The use of multiple scores was more effective for the higher-ranked class than for the lower-ranked class. 6.3 Experiment 2: comparison of the methods In Sect. 6.3 , to confirm the effectiveness of the method applying a logistic regression for the other classes, we compared the method with the method using the best accuracy table with the best combination of scores, that is, Combination A on SVMs. As the method using the best accuracy table, we used two kinds of accuracy tables. One was the best accuracy table accuracy table for each predicted class (hereafter referred to as an improved best accuracy table ), which was hardly generated because best cell intervals were different depending on the rank of a predicted class and datasets. For convenience sake, the best accuracy table for the first-ranked class was also used for the other classes.

Figure 7 shows values of cross entropy by the three methods for higher-ranked classes from the second-ranked to the fifth-ranked in the JGSS dataset and in the 20 Newsgroups dataset. First, the method applying a logistic regression was the most effective of the three methods for any class in both datasets except for both the second-ranked class in the JGSS dataset, and the third-ranked class in the 20 Newsgroups dataset. However, as for AUC for the first-ranked class in the JGSS dataset, the method applying a logistic regression was better than the method using the best accuracy table and was slightly worse than the method using the improved accuracy best table. 23 Second, for an average of values of cross entropy from the second-ranked to fifth-ranked, the method applying a logistic regression was most effective in the three methods. 24 Although we did not conduct experiments for classes ranked lower than the fifth-ranked class, the effectiveness of the method applying a logistic regression might become higher than the method using an accuracy table because generating an improved accuracy table might be more difficult. Therefore, if we can hardly generate the best accuracy table, we had better use the method applying a logistic regression for estimating class membership probabilities for the other classes. Finally, the values of cross entropy by the methods using an accuracy table also became small as the rank of a predicted class became lower. The reason is the same as that described in Sect. 6.2 . 7 Comparison of the proposed method with an expansion for multiclass classification of a PAV method Finally, we compared our best method in total with an expansion for multiclass classification of a PAV method by Zadrozny et al. [ 42 ] described in Sect. 2.4 in the same experimental setting conducted by Zadrozny et al. As our best method in total, we considered the following method by applying a logistic regression using multiple scores. For the predicted first rank class, we used scores for both the first-ranked class and the second-ranked class. 25 For the predicted other classes, we used scores for both a predicted class and the first-ranked class as described in Sect. 6.2 . 7.1 Dataset We used both the 20 Newsgroups dataset and the Pendigits dataset. The Pendigits dataset consists of 7,494 training samples and 3,498 test samples, and we used the dataset according to the same method as defined. In the 20 Newsgroups dataset, we employed a five-fold cross-validation, which Zadrozny et al. also applied. 7.2 Evaluation metrics We used both MSE and the error rate as well as Zadrozny et al. MSE is the mean of the the probability estimated for sample x and class c and T ( c | x ) is defined to be 1 if the actual label of x is c and 0 otherwise [ 42 ]. Therefore, a method is better when MSE is smaller. The error rate is defined as the number of samples incorrectly classified divided by the number of all samples, by classifying x as positive if P ( c | x )&gt; 0 . 5[ 42 ]. Therefore, a method is better when the error rate is smaller. MSE is an adequate metric for assessing the accuracy of estimates of class membership probability [ 39 ], while the error rate is a metric for assessing the accuracy of classifying the samples correctly [ 41 ]. The error rate is sufficient if we are only interested in classifying the samples correctly and do not need accurate estimates of class membership probabilities [ 41 ]. 7.3 Classifier We used SVMs, while Zadrozny et al. used naive Bayes classifiers. The reason why the classifiers used were different was that we obtained the best values by SVMs described in Sect. 4.1 , while Zadrozny et al. obtained the best values by naive Bayes classifiers [ 42 ]. More strictly speaking, Zadrozny et al. used naive Bayes classifiers in the 20 Newsgroups dataset because in their previous experiments for binary classification, PAV NB (naive Bayes classifiers calibrated by a PAV method) had been more effective than Sigmoid NB (naive Bayes classifiers calibrated by a sigmoid function), Sigmoid SVM 26 (SVMs calibrated by a sigmoid function) and PAV SVM (SVMs calibrated by a PAV method)in terms of both MSE and the error rate [ 42 ]. In the Pendigits dataset, Zadrozny et al. used both naive Bayes classifiers and the boosted naive Bayes classifiers described in Sect. 2.4 . 7.4 Evaluations by MSE and error rate Table 11 shows MSE and the error rate of both our best method for SVMs (Logistic regression SVM) and Zadrozny et al. best method (PAV NB) 27 on both the 20 Newsgroups dataset and the Pendigits dataset. All numbers of the methods except for Logistic regression SVM 28 are the numbers shown by Zadrozny et al. [ 42 ]. In terms of MSE, our best method was more effective than PAV NB in both datasets. As for the error rate, our best method was better than PAV NB in the 20 Newsgroups dataset and was much the same as that in the Pendigits dataset.

Table 12 shows MSE and the error rate of both our best method for SVMs and the PAV boosted method for naive Bayes classifiers (PAV BNB) on the Pendigits dataset. All numbers of the methods except for Logistic regression SVM are also the numbers shown by Zadrozny et al. [ 42 ]. PAV BNB was superior to our best method in the Pendigits dataset. 29 In terms of MSE, our best method was slightly worse than PAV BNB. Boosting was effective for an expansion for multiclass classification of a PAV method on the Pendigits dataset. 30 We did not conduct experiments whether boosting for our best method was effective because our aim was to discover a simple and general method without requiring ensemble learning for accurate estimation of class membership probability. However, for more accurate estimation, we need to investigate the effectiveness of bagging and boosting for our proposed method using various datasets. 8 Conclusions In this paper, we proposed a simple and general method for directly estimating class member-ship probability for any class in multiclass classification, using multiple scores for not only a predicted class, but also for other proper classes. To make it possible to use multiple scores, we proposed modifying or extending representative existing methods. As a non-parametric method, referring to an idea of the binning method by Zadrozny et al., we created an accu-racy table by different method from their method. Moreover we smoothed accuracies on the table with the moving average method to yield reliable probabilities used as accuracies. As a parametric method, we extended Platt X  X  method to apply a multiple logistic regression. On two different datasets, which were open-ended data from Japanese social surveys and the 20 Newsgroups, with both SVMs and naive Bayes classifiers, we empirically showed that the use of multiple scores was effective in the estimation of class membership probabilities in multiclass classification in terms of cross entropy, a reliability diagram, a ROC curve and AUC, and that the proposed smoothing method for the accuracy table worked quite well. For example, for the first-ranked class, the method using the best accuracy table used both the first-ranked score and the second-ranked score, smoothed with the moving average method with coverage and set with cell intervals to 0 . 1 (set the number of cells to thirty), was most effective. For the other classes, the method applying a logistic regression using scores for both the predicted class and the first-ranked class was the most effective. Finally, we showed empirically that in terms of MSE, our best proposed method was better than an expansion for multiclass classification of a PAV method proposed by Zadrozny et al., in both the 20 Newsgroups dataset and the Pendigits dataset, but was slightly worse than the state-of-the-art method, which was an expansion of a PAV boosted method, on the Pendigits dataset. In the method using an accuracy table, discovering the best accuracy table is important. In future work, we would like to discover effective cell intervals or the effective number of cells for an accuracy table, using information criteria such as AIC [ 29 ]. We would also like to conduct experiments of bagging and boosting for our proposed method on various datasets. Finally, we need to show theoretically the effectiveness of our proposed method. References Author Biographies
