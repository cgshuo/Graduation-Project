 Recent advances in digital broadcasting and re-cording allow fans access to an unprecedented amount of sports video. The growing need to manage and search large video collections presents a challenge to traditional information retrieval (IR) technologies. Such methods cannot be directly applied to video data, even when closed caption transcripts are available; for, unlike text docu-ments, the occurrence of a query term in a video is often not enough to assume the video X  X  relevance to that query. For example, when searching through video of baseball games, returning all clips in which the phrase  X  X ome run X  occurs, results primarily in video of events where a home run does not actually occur. This follows from the fact that in sports, as in life, people often talk not about what is currently happening, but rather, they talk about what did, might, or will happen in the future. 
Traditional IR techniques cannot address such problems because they model the meaning of a query term strictly by that term X  X  relationship to other terms. To build systems that successfully search video, IR techniques should be extended to exploit not just linguistic information but also ele-ments of the non-linguistic context, or situation , that surrounds language use. This paper presents a method for video event retrieval from broadcast sports that achieves this by learning a situated model of meaning from an unlabeled video corpus. 
The framework for the current model is derived from previous work on computational models of verb learning (Fleischman &amp; Roy, 2005). In this earlier work, meaning is defined by a probabilistic mapping between words and representations of the non-linguistic events to which those words refer. In applying this framework to events in video, we follow recent work on video surveillance in which complex events are represented as temporal rela-tions between lower level sub-events (Hongen et al., 2004). While in the surveillance domain, hand crafted event representations have been used suc-cessfully, the greater variability of content in broadcast sports demands an automatic method for designing event representations. 
The primary focus of this paper is to present a method for mining such representations from large video corpora, and to describe how these represen-tations can be mapped to natural language. We focus on a pilot dataset of broadcast baseball games. Pilot video retrieval tests show that using a situated model significantly improves perform-ances over traditional language modeling methods. Building situated models of meaning operates in three phases (see Figure 1): first, raw video data is abstracted into multiple streams of discrete fea-tures. Temporal data mining techniques are then applied to these feature streams to discover hierar-chical temporal patterns. These temporal patterns form the event representations that are then mapped to words from the closed caption stream. 2.1 Feature Extraction The first step in representing events in video is to abstract the very high dimensional raw video data into more semantically meaningful streams of in-formation. Ideally, these streams would corre-spond to basic events that occur in sports video (e.g., hitting, throwing, catching, kicking, etc.). Due to the limitations of computer vision tech-niques, extracting such ideal features is often in-feasible. However, by exploiting the  X  X anguage of film X  that is used to produce sports video, informa-tive features can be extracted that are also easy to compute. Thus, although we cannot easily identify a player hitting the ball, we can easily detect fea-tures that correlate with hitting: e.g., when a scene focusing on the pitching mound immediately jumps to one zooming in on the field (Figure 1). While such correlations are not perfect, pilot tests show that baseball events can be classified using such features (Fleischman et. al., in prep). work that is domain specific; i.e., it is the only as-pect of the framework designed specifically for use with baseball data. Although many feature types can be extracted, we focus on only two feature types: visual context, and camera motion. Visual Context Visual context features encode general properties of the visual scene in a video segment. The first step in extracting such features is to split the raw video into  X  X hots X  based on changes in the visual scene due to editing (e.g., jumping from a close up of the pitcher to a wide angle of the field). Shot detection is a well studied problem in multimedia research; in this work, we use the method of Tardini et al. (2005) because of its speed and proven performance on sports video. 
After a game is segmented into shots, each shot is categorized into one of three categories: pitch-ing-scene, field-scene , or other . Categorization is based on image features (e.g., color histograms, edge detection, motion analysis) extracted from an individual key frame chosen from that shot. A de-cision tree is trained (with bagging and boosting) using the WEKA machine learning toolkit that achieves over 97% accuracy on a held out dataset. Camera Motion Whereas visual context features provide informa-tion about the global situation that is being ob-served, camera motion features afford more precise information about the actions occurring in the video. The intuition here is that the camera is a stand in for a viewer X  X  focus of attention. As ac-tion in the video takes place, the camera moves to follow it, mirroring the action itself, and providing an informative feature for event representation. 
Detecting camera motion (i.e., pan/tilt/zoom) is a well-studied problem in video analysis. We use the system of (Bouthemy et al., 1999) which com-putes the pan, tilt, and zoom motions using the pa-rameters of a two-dimensional affine model fit to every pair of sequential frames in a video segment. The output of this system is then clustered into characteristic camera motions (e.g. zooming in fast while panning slightly left) using a 1 st order Hid-den Markov Model with 15 states, implemented using the Graphical Modeling Toolkit (GMTK). 2.2 Temporal Pattern Mining In this step, temporal patterns are mined from the features abstracted from the raw video data. As described above, ideal semantic features (such as hitting and catching) cannot be extracted easily from video. We hypothesize that finding temporal patterns between scene and camera motion features can produce representations that are highly corre-lated with sports events. Importantly, such tempo-ral patterns are not strictly sequential, but rather, are composed of features that can occur in complex and varied temporal relations to each other. For example, Figure 1 shows the representation for a fly ball event that is composed of: a camera pan-ning up followed by a camera pan down , occurring during a field scene, and before a pitching scene . 
Following previous work in video content classi-fication (Fleischman et al., 2006), we use tech-niques from temporal data mining to discover event patterns from feature streams. The algorithm we use is fully unsupervised. It processes feature streams by examining the relations that occur be-tween individual features within a moving time window. Following Allen (1984), any two features that occur within this window must be in one of seven temporal relations with each other (e.g. be-fore, during, etc. ). The algorithm keeps track of how often each of these relations is observed, and after the entire video corpus is analyzed, uses chi-square analyses to determine which relations are significant. The algorithm iterates through the data, and relations between individual features that are found significant in one iteration (e.g. [BEFORE, camera panning up, camera panning down ]), are themselves treated as individual fea-tures in the next. This allows the system to build up higher-order nested relations in each iteration (e.g. [DURING, [BEFORE, camera panning up, camera panning down ], field scene ]]). The tempo-ral patterns found significant in this way are then used as the event representations that are then mapped to words. 2.3 Linguistic Mapping The last step in building a situated model of mean-ing is to map words onto the representations of events mined from the raw video. We equate the learning of this mapping to the problem of estimat-ing the conditional probability distribution of a word given a video event representation. Similar to work in image retrieval (Barnard et al., 2003), we cast the problem in terms of Machine Transla-tion: given a paired corpus of words and a set of video event representations to which they refer, we make the IBM Model 1 assumption and use the expectation-maximization method to estimate the parameters (Brown et al., 1993): 
This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above. For every shot classified as a pitching scene , a new instance is created in the paired corpus corresponding to an event that starts at the beginning of that shot and ends exactly four shots after. This definition of an event follows from the fact that most events in baseball must start with a pitch and usually do not last longer than four shots (Gong et al., 2004). 
For each of these events in the paired corpus, a representation of the video is generated by match-ing all patterns (and the nested sub-patterns) found from temporal mining to the feature streams of the event. These video representations are then paired with all the words from the closed captioning that occur during that event (plus/minus 10 seconds). Work on video IR in the news domain often fo-cuses on indexing video data using a set of image classifiers that categorize shots into pre-determined concepts (e.g. flag , outdoors , George Bush, etc.). Text queries must then be translated (sometimes manually) in terms of these concepts (Worring &amp; Snoek, 2006). Our work focuses on a more auto-mated approach that is closer to traditional IR tech-niques. Our framework extends the language modeling approach of Ponte and Croft (1998) by incorporating a situated model of meaning. 
In Ponte and Croft (1998), documents relevant to a query are ranked based on the probability that each document generated each query term. We follow this approach for video events, making the assumption that the relevance of an event to a query depends both on the words associated with the event (i.e. what was said while the event oc-curred), as well as the situational context modeled by the video event representations: The p(word|caption) is estimated using the lan-guage modeling technique described in Ponte and Croft (1998). The p(word|video) is estimated as in equation 1 above.  X  is used to weight the models. Data The system has been evaluated on a pilot set of 6 broadcast baseball games totaling about 15 hours and 1200 distinct events. The data represents video of 9 different teams, at 4 different stadiums, broadcast on 4 different stations. Highlights (i.e., events which terminate with the player either out or safe ) were hand annotated, and categorized ac-cording to the type of the event (e.g., strikeout vs. homerun) , the location of the event ( e.g., right field vs. infield ), and the nature of the event (e.g., fly ball vs. line drive ). Each of these categories was used to automatically select query terms to be used in testing. Similar to Berger &amp; Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled ran-domly from this distribution. This gives us a set of queries for each annotated category (e.g., strikeout :  X  X iss, chasing X ; flyball:  X  X ly, streak X ). Although much noisier than human produced queries, this procedure generates a large amount of test queries for which relevant results can easily be determined (e.g., if a returned event for the query  X  X ly, streak X  is of the flyball category, it is marked relevant). validation during which five games are used to train the situated model while the sixth is held out for testing. Because data is sparse, the situation model is trained only on the hand annotated high-light events. However, retrieval is always tested using both highlight and non-highlight events. Results Figure 2 shows results for 520 automatically gen-erated queries of one to four words in length. Mean average precision (MAP), a common metric that combines elements of precision, recall, and ranking, is used to measure the relevance of the top five results returned for each query. We show re-sults for the system using only linguistic informa-tion (i.e.  X  =1), only non-linguistic information (i.e.  X  =0), and both information together (i.e.  X  =0.5). non-linguistic information is expected given the limited training data and the simple features used to represent events. Interestingly, using only lin-guistic information produces similarly poor per-formance. This is a direct result of announcers X  tendency to discuss topics not currently occurring in the video. By combining text and video analy-ses, though, the system performs significantly bet-ter (p&lt;0.01) by determining when the observed language actually refers to the situation at hand. We have presented a framework for video retrieval that significantly out-performs traditional IR meth-ods applied to closed caption text. Our new ap-proach incorporates the visual content of baseball video using automatically learned event represen-tations to model the situated meaning of words. Results indicate that integration of situational con-text dramatically improves performance over tradi-tional methods alone. In future work we will examine the effects of applying situated models of meaning to other tasks (e.g., machine translation). 
