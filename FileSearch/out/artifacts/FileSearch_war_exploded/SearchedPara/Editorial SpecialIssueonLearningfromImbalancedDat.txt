 The class imbalance problem is one of the (relativ ely) new problems that emerged when mac hine learning matured from an em bry onic science to an applied tec hnology , amply used in the worlds of business, industry and scien tific researc h. Although practitioners migh t already have kno wn about this problem early , it made its app earance in the mac hine learn-ing/data mining researc h circles about a decade ago. Its im-portance grew as more and more researc hers realized that their data sets were imbalanced and that this imbalance caused sub optimal classification performance. This increase in interest gave rise to two workshops held in 2000 [1] and 2003 [3] at the AAAI and ICML conferences, resp ectiv ely . These workshops and the ensuing e-mail discussions and in-formation seeking requests that follo wed them allo wed us to note two poin ts of imp ortance: 1. The class imbalance problem is perv asiv e and ubiqui-2. Despite the fact that two workshops have already been The purp ose of this special issue is to comm unicate and presen t some of the latest researc h carried out in this area while reviewing other imp ortan t recen t dev elopmen ts in the field. In this Editorial, we begin by reviewing the class im-balance as well as an arra y of general solutions that were previously prop osed to deal with it. We then discuss the progression of ideas starting at the 2000 workshop to to-day. In order to giv e a comprehensiv e picture of the state of the art in the field, we giv e a short overview of the pap ers that were presen ted at the 2003 workshop as well as a short description of the pap ers con tained in this volume. The ex-cellen t overview pap er by Gary Weiss [55] published in this volume will complete this short picture. The class imbalance problem typically occurs when, in a classification problem, there are man y more instances of some classes than others. In suc h cases, standard classi-fiers tend to be overwhelmed by the large classes and ignore the small ones. In practical applications, the ratio of the small to the large classes can be drastic suc h as 1 to 100, 1 to 1,000, or 1 to 10,000 (and sometimes even more). (See, for example, [41], [57]). As men tioned earlier this problem is prev alen t in man y applications, including: fraud/in trusion detection, risk managemen t, text classification, and medical diagnosis/monitoring, but there are man y others. It is worth noting that in certain domains (lik e those just men tioned) the class imbalance is intrinsic to the problem. For exam-ple, within a giv en setting, there are typically very few cases of fraud as compared to the large num ber of honest use of the offered facilities. Ho wever, class imbalances sometimes occur in domains that do not have an intrinsic imbalance. This will happ en when the data collection pro cess is limited (e.g., due to economic or priv acy reasons), thus creating  X  X r-tificial X  imbalances. Con versely , in certain cases, the data abounds and it is for the scien tist to decide whic h exam-ples to select and in what quan tity [56]. In addition, there can also be an imbalance in costs of making differen t errors, whic h could vary per case [3].
 A num ber of solutions to the class-im balance problem were previously prop osed both at the data and algorithmic lev-els. At the data lev el, these solutions include man y differen t forms of re-sampling suc h as random oversampling with re-placemen t, random undersampling, directed oversampling (in whic h no new examples are created, but the choice of samples to replace is informed rather than random), di-rected undersampling (where, again, the choice of examples to eliminate is informed), oversampling with informed gen-eration of new samples, and com binations of the above tec h-niques. At the algorithmic lev el, solutions include adjusting the costs of the various classes so as to coun ter the class imbalance, adjusting the probabilistic estimate at the tree leaf (when working with decision trees), adjusting the deci-sion threshold, and recognition-based (i.e., learning from one class) rather than discrimination-based (two class) learning. Man y of these solutions are discussed in the pap ers pre-sen ted in the workshops [1][3] or are referred to in the activ e bibliograph y on the topic 1 . It is interesting to review briefly the typ es of problems that 1 http://www.site.uotta wa.ca/  X nat/Resea rch/ class imbalance bibli.h tml have been considered by the researc hers on the class imb al-anc e problem in the past few years. At the first workshop 2 , in July 2000, the two issues that receiv ed the greatest amoun t of atten tion were: 1. Ho w to evaluate learning algorithms in the case of class 2. The relationship between class imbalances and cost-With regard to the first issue, it was emphasized that the use of common evaluation measures suc h as accuracy can yield misleading conclusions. It was suggested that more accurate measures suc h as ROC curv es and Cost Curv es be used. Some of the ma jor pap ers on this topic that came out at the time of or since the first workshop are: [44][18][13][20]. In fact, a workshop on ROC Analysis in AI is being held in August 2004 [4]. In addition, an evaluation measure was prop osed for the case where only data from one class is avail-able.
 With regard to the second issue, a close connection was rec-ognized between re-sampling approac hes and cost-sensitiv e approac hes [53][15][12][52]. Cost-sensitiv e learning or mea-sures assume that a cost-matrix is kno wn for differen t typ es of errors or even examples, whic h can be used at classifica-tion time [53][15]. Ho wever, we often do not kno w the cost matrix. Furthermore, cost-sensitiv e learning does not mo d-ify the class distribution of the data the way re-sampling does. Finally , cost-sensitiv e learning is not encum bered by large sets of duplicated examples, whic h, as discussed in [34] can also be a dra wbac k. Practically , it is often rep orted that cost-sensitiv e learning outp erforms random re-sampling (e.g., [29][12]).
 Other issues discussed at that workshop were the facts that: The three years separating the two workshops saw a great amoun t of activit y. The impact of the discussions held at the 2000 workshop was clearly seen on the first issue, in the fact that most of the researc h conducted in the class imbalance area after that first workshop made use of ROC curv es for evaluating the results. In addition, there was a bias towards various over and under-sampling tec hniques. Decision trees remained a popular classifier for researc h.
 With regard to the relationship between cost-sensitiv e learn-ing and re-sampling, the impact was not as direct or clear, but it remained presen t in the form of two emerging ideas: 2 These remarks have been adapted from [28]. Regarding the idea of clev er re-sampling and com bination metho ds, [10] sho wed that creating syn thetic examples of the minorit y class that spread the decision regions, thus mak-ing them larger and less specific while undersampling the ma jorit y class to various degrees gave very effectiv e results. Regarding the second idea, the problem was considered in parallel in both the con text of cost-sensitiv e learning and that of re-sampling. We would like to poin t the reader to the ICML workshop on cost-sensitiv e learning [2] and an on-line bibliograph y on cost-sensitiv e learning 3 . Within cost-sensitiv e learning, various ways of adjusting the probabilistic estimate at the tree leaf (when working with decision trees) were explored so as to giv e a more direct and more flexible approac h to the treatmen t of differen t parts of the decision space [58]. A similar idea was used in the con text of re-sampling, with recourse to unsup ervised learning [25]. This work also gave rise to the question of how related the class imbalance problem is to the problem of small disjuncts, pre-viously discussed in the literature. This last issue was also link ed to the small sample versus imbalance problem dis-cussed at the last workshop.
 The inter-y ears also saw some researc h on one-class learn-ing as well as on a new twist of one-class learning that can be though t of as extreme semi-sup ervised learning. Suc h tec hniques can be used in the con text where unlab eled data are readily available, while lab eled data for at least one of the classes are missing. In suc h cases, iterativ e tec hniques suc h as Exp ectation Minimization (EM), have been used to assign/estimate the missing-class lab els [33][36]. This essen tially man ufactures the missing portion of the train-ing data, whic h can then be used in the standard induc-tion pro cess. The imp ortan t differen tiator from other semi-sup ervised problems (e.g., [40]) is that there are no lab eled seed data to initialize the estimation mo del for the missing class. First, a hin t at the fact that researc h on the class imbal-ance problem is starting to mature is the fact that a big prop ortion of the 2003 workshop was occupied by the com-parison of previously prop osed schemes for dealing with the class imbalanced problem [9][14][37]. These pap ers looked at random oversampling, oversampling with artificially gen-erated samples, random undersampling, and directed un-dersampling. In conjunction with sampling, these pap ers also considered probabilistic estimates, pruning, threshold adjusting and cost-matrix adjusting. In addition, the pa-per by [60] prop osed sev eral new directed undersampling sensitiv e.h tml schemes whic h were compared to eac h other and to ran-dom undersampling. [47] consider two differen t metho ds for balancing the class distribution in the data set, and then extend that scenario to only one-class learning. They sho w that one-class SVM learning can be beneficial for certain domains (an extended version of their pap er app ears in this Volume). Though all these pap ers shed some ligh t on the way various metho ds compare, there is no single final word on the question. In other words, a num ber of tec hniques were sho wn to be effectiv e if applied in a certain con text, where the breadth of the con text ma y vary . It is worth noting that there was much more discussion at ICML 2003 on sampling metho ds as compared to the AAAI workshop. That brings us to question: Is sampling becoming a de facto standar d for countering imb alanc e? In addition to sampling, an imp ortan t question to consider, particularly when applying mac hine learning to a real-w orld problem, is the cost asso ciated in acquiring the data. Giv en these costs, a  X  X udgeted X  sampling approac h is required. In the opening talk, Foster Pro vost discussed differen t costs in pro curing the data, and learning from it [43]. His presen ta-tion suggested that when using ROC as the performance cri-teria, a balanced distribution is mostly the preferred choice. He also addressed the question: Given a budget for data procurement, what class distribution should be used? He prop osed a novel budget-sensitiv e progressiv e sampling ap-proac h, whic h is not worse than choosing a balanced or nat-ural distribution.
 Although the issue of how to evaluate classifiers in cases of class imbalances seemed settled by the adoption of ROC curv es and, to a lesser exten t, Cost Curv es, the question resurfaced at the 2003 workshop when Charles Elk an [16] poin ted out that ROC curv es are unable to deal with within-class imbalances and differen t within-class misclassification costs. Ho wever, if there are well defined sub categories of one class, the evaluation set can be resampled in prop ortion to true sub-class prop ortions and their costs. Elk an sug-gested that the issue of how to evaluate classifiers in cases of class-im balances should be revisited in accordance with this question. Another issue with regard to the evaluation of classifiers is concerned with the distribution that should be used in the testing set. It is often assumed that the target distribution should be used, but the issue with this solution is the fact that the target distribution is usually unkno wn. One interesting criticism raised in conjunction with this se-ries of pap ers, however, is worth noting here: too much reliance of the class-im balance researc h comm unit y on C4.5 [45]. It was argued, in particular, that C4.5 is not the best classifier for dealing with class imbalances and that the com-munit y should focus on it less. Tw o ensuing questions re-lated to that issue were whether some classifiers are insen-sitiv e to class imbalances [24]; and whether classification really is the task to focus on, or whether it would be best to perform ranking (through probabilit y estimation) [16]. In another invited talk, Naoki Ab e presen ted various strate-gies for selectiv e sampling based on query learning [5]. This aids in selecting data near the decision boundary . He pro-posed cost-prop ortionate sampling metho dologies. He also discussed a cost-sensitiv e ensem ble learning metho d called Costing [59], whic h achiev ed significan t impro vemen ts over random resampling metho ds. [32] also discussed using se-lectiv e sampling as a part of activ e sampling before learning a one-class classifier. In that work, it was sho wn that data selection driv en by the uncertain ty of a classifier and/or dis-tance from the target class pro vide viable (although some-what classifier dep enden t) approac hes.
 A num ber of pap ers discussed interaction between the class imbalance and other issues suc h as the small disjunct [27] and the rare cases [23] problems, data duplication [34], and overlapping classes [54]. It was found that in certain cases, addressing the small disjunct problem with no regard for the class imbalance problem was sufficien t to increase per-formance 4 . Though in other cases, [41] found that handling the small disjuncts was not sufficien t. The metho d for han-dling rare case disjuncts was found to be similar to the m-estimation Laplace smo othing, but it requires less tuning. It was also found that data duplication is generally harmful, although for classifiers suc h as Naiv e Ba yes and Perceptrons with Margins, high degrees of duplication are necessary to harm classification [34]. It was argued that the reason wh y class imbalances and overlapping classes are related is that misclassification often occurs near class boundaries where overlap usually occurs as well.
 Tw o of the workshop pap ers also presen ted novel approac hes suc h as [61] who prop osed a feature selection approac h specif-ically tuned to the class imbalance problem (an expanded version of their work app ears in this volume and will be dis-cussed in more detail belo w) and [57] who prop ose to mo dify the Kernel function or matrix of an SVM by adapting it lo-cally based on the data distribution. In this section, we summarize the most recen t dev elopmen ts in the area of class imbalances by describing briefly the con-tributions to this volume along with the con text in whic h they fall. Gary Weiss [55] presen ts an overview of the field of learning from imbalanced data. He pays particular atten-tion to differences and similarities between the problems of rare classes and rare cases. He then discusses some of the common issues and their range of solutions in mining im-balanced datasets. The rest of the con tributions are made to three subareas of the class imbalance problem: Sampling , One Class Learning , and Featur e Sele ction . The comp elling question, giv en the differen t class distribu-tions, is: What is the correct distribution for a learning al-gorithm? It has been observ ed that naturally occurring dis-tribution is not alw ays the optimal distribution [56]. In ad-dition, the imbalance in the data can be more characteristic of the  X  X parseness X  in feature space than the class imbalance [10].
 Random undersampling can poten tially remo ve certain im-portan t examples, and random oversampling can lead to overfitting. In addition, oversampling can intro duce an ad-ditional computational task if the data set is already fairly large but imbalanced. Ho w much to oversample or un-dersample is usually empirically detected. There has been a progression in sampling metho ds to focus on particular ma jorit y or minorit y class samples. Another interesting paradigm of researc h utilizing (adaptiv e or random or fo-4 An expanded version of [27] app ears in this volume and will be discussed further belo w.
 cused) sampling is evolving under the multiple classifier sys-tems or ensem bles domain [8][12][11][46][5 1][17][31][59] . Various pap ers in this special issue focus on utilizing sam-pling metho ds directly or as a part of ensem ble learning. Batista et. al [6] presen t a comparison (and com bination) of various sampling strategies. They note that com bining fo-cused over and undersampling, suc h as SMOTE+T omek or SMOTE+ENN is applicable when the data sets are highly imbalanced or there are very few instances of the minorit y class. Guo and Viktor [21] prop ose another tec hnique that mo difies the boosting pro cedure  X  DataBo ost. As com-pared to SMOTEBo ost, whic h only focuses on the hard mi-norit y class cases, this tec hnique emplo ys a syn thetic data generation pro cess for both minorit y and ma jorit y class cases. Ph ua et. al [42] com bine bagging and stac king to iden tify the best mix of classifiers. In their insurance fraud detec-tion domain, they note that stac king-bagging achiev es the best cost-sa vings. Jo and Japk owicz [30] shed some new and differen t ligh t to the problem of class imbalance in a data set. They suggest that small disjuncts (due to class im-balance) in C4.5 decision trees and bac kpropagation neural net works are resp onsible for performance degradation. The (often) negativ e impact of class imbalance is comp ounded by the problem of small disjuncts, particularly in small and complex data sets. They prop ose use of cluster-based over-sampling to coun ter the effect of class imbalance and small disjuncts. When negativ e examples greatly outn um ber the positiv e ones, certain discriminativ e learners have a tendency to over-fit. A recognition-based approac h pro vides an alternativ e to discrimination where the mo del is created based on the examples of the target class alone. Here, one attempts to measure (either implicitly or explicitly) the amoun t of sim-ilarit y between a query ob ject and the target class, where classification is accomplished by imp osing a threshold on the similarit y value [26].
 Mainly , two classes of learners were previously studied in the con text of the recognition-based one-class approac h X  SVMs [50][49] and auto enco ders [26][38] X  X nd were found to be comp etitiv e [38].
 An interesting asp ect of one-class (recognition-based) learn-ing is that, under certain conditions suc h as multi-mo dalit y of the domain space, one class approac hes to solving the classification problem ma y in fact be sup erior to discrimina-tiv e (two-class) approac hes (suc h as decision trees or Neural Net works) [26]. This is supp orted in the curren t volume by [48], who demonstrate the optimalit y of one-class SVMs over two-class ones in certain imp ortan t imbalanced-data domains, including genomic data. In particular, [48] sho ws that one class learning is particularly useful when used on extremely unbalanced data sets comp osed of a high dimen-sional noisy feature space. They argue that the one-class approac h is related to aggressiv e feature selection metho ds, but is more practical since feature selection can often be too exp ensiv e to apply . Feature selection is an imp ortan t and relev ant step for min-ing various data sets [22]. Learning from high dimensional spaces can be very exp ensiv e and usually not very accurate. It is particularly relev ant to various real-w orld problems suc h as bioinformatics, image pro cessing, text classification, Web categorization, etc. High dimensional real-w orld data sets are often accompanied by another problem: high skew in the class distribution, with the class of interest being rel-ativ ely rare. This mak es it particularly imp ortan t to select features that lead to a higher separabilit y between the two classes. It is imp ortan t to select features that can capture the high skew in the class distribution. The ma jorit y of work in feature selection for imbalanced data sets has focused on text classification or Web categorization domain [39][19]. A couple of pap ers in this issue look at feature selection in the realm of imbalanced data sets, alb eit in text classifica-tion or Web categorization. Zheng and Srihari [62] suggest that existing measures used for feature selection are not very appropriate for imbalanced data sets. They prop ose a fea-ture selection framew ork, whic h selects features for positiv e and negativ e classes separately and then explicitly com bines them. The authors sho w simple ways of con verting existing measures so that they separately consider features for neg-ativ e and positiv e classes. Castillo and Serrano [7] do not particularly focus on feature selection, but mak e it a part of their complete framew ork. They use a multi-strategy clas-sifier system to construct multiple learners, eac h doing its own feature selection based on genetic algorithm. Their pro-posed system also com bines the predictions of eac h learner using genetic algorithms. To summarize the Editorial, we attempted to (briefly) chart out the progress in related areas of learning from imbal-anced data sets by outlining some of the trends since the AAAI 2000 workshop. The problem of class or cost imbal-ance is prev alen t in various real world scenarios. As this field slo wly matures, novel questions and problems stem requiring equally novel solutions. We hop e that this Issue stim ulates new directions and solutions that can lead to both theoret-ical insigh t and practical applications. We thank the review ers for their useful and timely commen ts on the pap ers submitted to this Issue. We would also like to thank the participan ts and attendees of the previous work-shops for the enligh tening presen tations and discussions. [1] In N. Japk owicz, editor, Proceedings of the AAAI X 2000 [2] In T. Dietteric h, D. Marginean tu, F. Pro vost, and [3] In N. V. Cha wla, N. Japk owicz, and A. Ko lcz, editors, [4] In C. Ferri, P. Flac h, J. Orallo, and N. Lac hice, editors, [5] N. Ab e. Invited talk: Sampling approac hes [6] G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard. [7] M. Castillo and J. Serrano. A multistrategy approac h [8] P. K. Chan and S. J. Stolfo. Toward scalable learning [9] N. V. Cha wla. C4.5 and imbalanced datasets: Investi-[10] N. V. Cha wla, L. O. Hall, K. W. Bo wy er, and W. P. [11] N. V. Cha wla, A. Lazarevic, L. O. Hall, and K. W. [12] P. Domingos. Metacost: A general metho d for mak-[13] C. Drummond and R. Holte. Explicitly represen ting ex-[14] C. Drummond and R. Holte. C4.5, class imbalance, [15] C. Elk an. The foundations of cost-sensitiv e learning. In [16] C. Elk an. Invited talk: The real chal-[17] W. Fan, S. Stolfo, J. Zhang, and P. Chan. Adacost: [18] T. Fawcett. ROC graphs: Notes and [19] G. Forman. An extensiv e empirical study of feature se-[20] J. Furnkranz and P. Flac h. An analysis of rule evalua-[21] H. Guo and H. L. Viktor. Learning from imbal-[22] I. Guy on and A. Elisseeff. An intro duction to variable [23] R. Hic key. Learning rare class footprin ts: the reflex al-[24] R. Holte. Summary of the workshop. [25] N. Japk owicz. Concept-learning in the presence of [26] N. Japk owicz. Sup ervised versus unsup ervised binary-[27] N. Japk owicz. Class imbalance: Are we focusing on the [28] N. Japk owicz and R. Holte. Workshop rep ort: Aaai-[29] N. Japk owicz and S. Stephen. The class imbalance prob-[30] T. Jo and N. Japk owicz. Class imbalances versus small [31] M. Joshi, V. Kumar, and R. Agarw al. Ev aluating boost-[32] P. Juszczak and R. P. W. Duin. Uncertain ty sampling [33] A. Ko lcz and J. Alsp ector. Asymmetric missing-data [34] A. Ko lcz, A. Cho wdh ury , and J. Alsp ector. Data du-[35] M. Kubat and S. Mat win. Addressing the curse of im-[36] B. Liu, Y. Dai, X. Li, W. S. Lee, and P. Yu. Building [37] M. Malo of. Learning when data sets are imbalanced and [38] L. M. Manevitz and M. Yousef. One-class SVMs for [39] D. Mladenic and M. Grob elnik. Feature selection for [40] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitc hell. [41] R. Pearson, G. Goney , and J. Shwaber. Im balanced [42] C. Ph ua and D. Alahak oon. Minorit y rep ort in fraud [43] F. Pro vost. Invited talk: Cho osing a marginal [44] F. Pro vost and T. Fawcett. Robust classification for im-[45] J. Quinlan. C4.5: Programs for Machine Learning . [46] P. Radiv ojac, N. V. Cha wla, K. Dunk er, and [47] B. Raskutti and A. Ko walczyk. Extreme re-balancing [48] B. Raskutti and A. Ko walczyk. Extreme rebalancing for [49] B. Sch  X olk opf, J. C. Platt, J. Sha we-T aylor, A. J. [50] D. Tax. One-class classific ation . PhD thesis, Delft Uni-[51] K. M. Ting. A comparativ e study of cost-sensitiv e [52] K. M. Ting. An instance-w eigh ting metho d to induce [53] P. Turney . Typ es of cost in inductiv e concept learning. [54] S. Visa and A. Ralescu. Learning imbalanced and over-[55] G. Weiss. Mining with rarit y: A unifying framew ork. [56] G. Weiss and F. Pro vost. Learning when training data [57] G. Wu and E. Y. Chang. Class-b oundary alignmen t [58] B. Zadrozn y and C. Elk an. Learning and making deci-[59] B. Zadrozn y, J. Langford, and N. Ab e. Cost-sensitiv e [60] J. Zhang and I. Mani. knn approac h to unbalanced data [61] Z. Zheng and R. Srihari. Optimally com bining positiv e [62] Z. Zheng, X. Wu, and R. Srihari. Feature selection for
