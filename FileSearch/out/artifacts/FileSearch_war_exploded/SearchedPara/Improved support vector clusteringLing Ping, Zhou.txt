 1. Introduction
Support vector clustering (SVC) is an unsupervised learning algorithm inspired by support vector technique ( Ben-Hur et al., 2001). Among a number of clustering approaches, SVC is the one with some special advantages. Firstly it can handle arbitrary-shaped clusters through its boundary-based clustering model.
Secondly it has the ability to deal with structured data by employing Kernel function. Thirdly, as an algorithm based on support vector technique, SVC X  X  solution is globally optimal and is of generalization. These merits promote SVC X  X  promising success in diverse applications like marketing analysis, medicine predic-tion, relational data analysis, etc. ( Ben-Hur et al., 2000 ). Two bottlenecks, however, degrade SVC X  X  popularity: pricy cost and poor labeling piece. To solve two bottlenecks, some literatures did extensive study; four representatives of modifications are as follows: support vector graph (SVG) ( Ben-Hur et al., 2001 ), proximity graph (PG) ( Yang et al., 2002 ), gradient descendent (GD) ( Lee and Lee, 2005 ), and cone cluster labeling (CCL) ( Lee and
Daniels, 2006 ). The first three methods adopt some simulation operations, thus they have lower cost while higher error. CCL improves clustering results with the price of large increase of cost. So existing modifications can only solve one of two bottlenecks.
This paper presents an improved SVC algorithm, iSVC, to address two bottlenecks simultaneously. iSVC X  X  crucial compo-nents are a reduction strategy and a new labeling approach.
Reduction strategy aims to increase efficiency. It extracts a desired subset based on which the clustering model is formulated.
Through cooperating with a modified objective, the resultant model has subtle loss of quality. In a simple but effective way, the new labeling approach labels data according to the geometric property of feature space. The geometric property is proofed to guarantee the approach X  X  validation. Empirical and theoretical evidence show the improvement of iSVC in performance and efficiency over its counterparts, and shows iSVC X  X  fine clustering ability when compared with popular clustering methods. In the following sections, SVC, its modifications and their shortcomings are overviewed firstly, and then the details of iSVC are described in sequence. 2. Overview 2.1. SVC
SVC process can be divided into two pieces: optimization piece and labeling piece. Given the n -dimension dataset X , X ={ x the optimization piece aims to look for a minimum hyper sphere containing all data in a new space, that is, the feature space. The optimization objective is expressed as follows:
Therein F is the non-linear map from the input space to the feature space, x i the slack variable, a the sphere center, R the sphere radius, and C the penalty parameter. The Wolfe dual is as follows: max b s : t : 0 r b i r C ; With Kernel trick k ( x i , x j )= / F ( x i ), F ( x j ) written as follows: max b
Points with b i =0 are inner-boundary points. Points with 0 o b i o C are support vectors (SVs); they describe cluster contours. k is Gaussian Kernel: k ( x , y ) = exp ( q : x y q is the scale parameter. From (3), there is a = P i b i F ( x distance of x to a is computed as follows: R  X  x  X  2  X  : F  X  x  X  a : 2  X  k  X  x ; x  X  2
Then it comes to the labeling piece, which is named as complete graph (CG) since it identifies clusters by computing a complete graph and taking connected components of the graph as clusters. The complete graph is represented by an adjacent matrix, which is formulated based on an observation that given a pair of data points that belong to different components, any path that connects them must exit from the sphere. So, that path contains a segment of points y such that R ( y ) 4 R , sphere radius R is computed by introducing a SV into R ( x ). This leads to an adjacency matrix A :
A  X 
Therein line [ x i , x j ] represents the line segment between x x . Then clusters are defined as the connected components of the graph induced by A . To compute A , the path between two points must be investigated to decide whether it is located outside the sphere. Because such an investigation is an infinite process, it is simplified to a finite process: sample some points randomly from the path; if any one sample is located outside the sphere, the path is not within cluster contours and the two points belong to different clusters. Clearly random sampling incurs errors, like two cases shown in Fig. 1 . In case (a), x and y belong to the same cluster, while there is a sampling point, z , outside the cluster contour; in case (b), x and y have different label, but the selected sampling point z is all by chance located within cluster contour. 2.2. Modifications and shortcomings Existing modifications of SVC all focus on the labeling piece. Four mentioned representatives are introduced in brief.
SVG builds the adjacent matrix on SVs instead of on all data. It induces the connected components of SVs and labels data according to its near SV. SVG consumes less cost than CG, but it has a decrease in clustering accuracy. PG is similar with SVG in building adjacent matrix for SVs, but it employs approximation operations to induce connected components of SVs. This algorithm takes less cost than SVG, yet its clustering results are poorer than SVG. Rather than using SVs to learn cluster information, GD creates some stable equilibrium points (SEPs), and each SEP represents data within its neighborhood. Then GD computes the adjacent matrix of SEPs and learns connected components of SEPs. Data are labeled the same membership as its SEP. SEPs hold much local information, so GD is expected to give more accurate clusters than CG. However the process of searching SEPs is an iterative process, which is not steady in convergence speed and convergence quality. That leads to GD being uncontrollable. To detect clusters above three methods computes adjacent matrix in the way of formula (5), therefore the randomness is not removed.
CCL removes randomness. It constructs cone-shaped neighbor-hoods for SVs, and then observes the intersecting situation among neighborhoods to decide adjacent matrix of SVs: two SVs belong to the same cluster if their cones are intersected. In CCL, the adjacent matrix is more definite and accurate than that of CG, so CCL has the higher accuracy. The disadvantage lies in CCL X  X  expensive cost, which is used in constructing cones, observing intersecting situations, parameter searching.

It finds existing modifications only overcome one of two bottlenecks. It is necessary to propose a two-goal algorithm. This paper does this topic. 3. The optimization piece of iSVC
In iSVC X  X  optimization piece, reduction strategy is performed on the whole dataset firstly, and then the modified objective is optimized on the subset. Details are mentioned below. 3.1. Reduction strategy 3.1.1. Schr  X  odinger equation
Reduction strategy is based on the Schr  X  odinger equation ( Horn and Gottlieb, 2002 ), so this equation is introduced firstly. The Schr  X  odinger equation describes the law of energy conservation of a particle, written as follows: H c  X  x  X  s 2 2 r 2  X  P  X  x  X  c  X  x  X  X  e c  X  x  X  X  6  X  Therein e is the energy, P ( x ) the Schr  X  odinger potential, r the Laplacian, and s the variance parameter. c ( x ) expresses the state of a quantum system, so c ( x ) can be explained as the wave function of particle. Usually, given P ( x ), the equation is solved to find solution c ( x ), so as to know particle orbits.

When applied in machine learning community, c ( x ) can be regarded as data probability distribution function, and its maxima associate with cluster centers. Instead of looking for c ( x ) X  X  maxima, potential P ( x ) X  X  minima are probed since in the equation there is dual relation between P ( x ) and c ( x ). Another important reason is that P ( x ) has much more robustness towards the variation of s than c ( x ). That reduces cost of parameterization. For the given c ( x ), P ( x ) is solved as follows: P  X  x  X  X  e  X  s 2 2 c  X  x  X  r 2 c  X  x  X  X  7  X 
Usually c ( x ) is simulated by the empirical distribution: c  X  x  X  X  Then (7) is rewritten as
P  X  x  X  X  e n = 2  X  1 2 s 2 S i Therein n is data dimensionality. It can further require min
P ( x )=0, so e is computed: e  X  min s 2 2 c  X  x  X  r 2 c  X  x  X   X  min n = 2 If the dataset is finite, it forms a list of { P ( x i )} values:
P  X  x  X  X  e n = 2  X  1 2 s 2 From geometry meaning, P ( x ) X  X  minima tell cluster centers, so P ( x ) X  X  maximum indicate the boundary information of clusters.
That inspires us to use the Schr  X  odinger equation to define reduction strategy. 3.1.2. Reduction strategy iSVC develops clustering model from a subset. To obtain a desired model, the subset is required to meet two conditions. The one is that subset should contain data that make significant contributions to model formulation. Here, clustering model is SVs since subsequent operations are based on SVs. Then points located around cluster contours should be included in the subset.
Secondly the subset should cover all clusters. This condition is easy to be satisfied in supervised learning where label informa-tion is known, yet it is difficult in unsupervised learning. This paper employs the Schr  X  odinger equation to explore data position information and to find the qualified subset.
 As stated in Section 3.1.1, P ( x ) values reveal data location.
Then, points with top P ( x ) values tend to be around cluster boundaries; they should be included in the subset. Points with small P ( x ) values tend to be located in cluster central zones; they need to be sampled according to some ratio, so that all clusters can be covered. Then the reduction strategy corresponds to following steps: (1) Sort { P ( x i )} values in the descending order: { P
P ( i +1) , where P ( i ) = P ( x i ), ( i =1, y , N ). into L intervals ( L = N / G ): (3) In each interval, some data are sampled randomly at a certain follows:
The sampling ratio decreases with J . Z J is ever-decreasing and it makes most data of top intervals are selected, and a few data of other intervals are selected. The  X  X ax X  mechanism ensures each interval has at least one member included in the subset. The resultant subset contains rich boundary data and center-zone data. Center-zone data helps the subset cover all clusters. G balances the clustering quality and the cost. The higher the
G , the bigger the subset size. That leads to better results possibly, but asks for more computation cost. G can be specified by memory or time requirement. In the following experiments, it finds
G =0.1 N is a fine setting since other settings around this setting did not cause big change of results. 3.2. Modified optimization objective
The reduction strategy reduces the optimization from the whole set to the subset. It causes information loss inevitably. To compensate this loss, this section adjusts optimization objective in the way that max s : t : 0 r b i r C ; P i b i  X  1 ; d ij  X  Given k ( x i , x i )=1, the objective is reduced to min b [ k ( x , x j ) d ij /4 C ]. The matrix version is as follows: min b T K d 4 C b s : t b
T e  X  1 ; 0 r b i r C ; d  X  diag  X  d ij  X  ; d ij  X  1 i  X  j
Therein K is the Kernel matrix, b the Lagrange multiplier vector, and M the subset size. From mathematics perspective, diagonal matrix d /(4 C ) is a positive definite matrix. According to the property of positive definite matrix, K d /(4 C ) is positive definite, so it can be adopted into optimization process correctly. 4. The labeling piece of iSVC
In this piece iSVC performs the new labeling approach, whose idea is to cluster SVs firstly; then construct a classifier based on labeled SVs; finally label other data using the classifier. The details are as follows: (1) Create affinity matrix H with respect to SVs according to
Gaussian Kernel: H ij = k ( v i , v j ), with v i and v j
Normalize H into H 0 : H 0 = L 1/2 H L 1/2 , where L = diag ( S
Do eigenvalue decomposition on H 0 , and take top g eigenvectors as columns to form matrix H 00 . (4) Perform K-means on rows of H 00 ; the cluster number is initialized as g . g is specified by the number of eigenvalues that are larger than 1 ( Zheng et al., 2007 ). (5) Label v as the i th row X  X  cluster membership. (6) Label other data in terms of its nearest SV X  X  label.

In step (6), below metric : : n is used to search the nearest SV: : x y :  X  1
The validation of this approach is supported by the geometric properties of Gaussian Kernel feature space, which is discussed next. 4.1. Geometric properties of Gaussian Kernel feature space In feature space, data are mapped into the hyper sphere.
Supposing this sphere as S , and center as a . Since k ( x , x )= surface of the unit ball. Suppose this ball as B , then B  X  X  center is the original O . Thus data actually spread on the cap-like surface intersected by S and B , as shown in Fig. 2 . At the same time SVs are on the surface of S , so they are on the intersection hyper line of S This paper proposes that those SVs with the same cluster membership appear together on the rim. Before giving proof of this consequence, some statements of feature space ( Lee and Daniels, 2006 ) are mentioned first.

Denote V ={ v i } as the set of SVs. In Lee and Daniels (2006) , for that is + ( F ( v i ) Oa 0 )= + ( F ( v j ) Oa 0 ). For any v with F ( v ) as the vertex, O F ( v ) vector as axis, and the basic angle. If two cones are intersected, it means two SVs belong to the same cluster. Whether a point is within v  X  X  cone decides whether it has the same label as v . SV X  X  cone corresponds to a small sphere in input space with v as the center and : v F 1 ( a 0 ) : as the radius. So the computation of feature space is converted into the computation of input space. Those statements are summarized as the below lemma.

Lemma. For any x A X , v A V , let y = + ( F ( v ) Oa 0 ), there is + ( F ( v ) O F ( x )) o y 3 : v x : o : v F 1 ( a 0 ) : 3
F ( v ) X  s cone . 3 x is within v  X  s small sphere 3 x and v have same cluster label . Based on this lemma, this paper proposes the following.
Corollary. In feature space of Gaussian Kernel, SVs are collected in terms of clusters on the intersection hyper line of S and B. Proof. The geometric meaning of Corollary is illustrated in Fig. 3 .
Use the reduction to absurdity to proof it. Assume that SVs of different clusters appear disordered. That is, the SVs belonging to one cluster are scattered by SVs of other clusters. So for v v 3 A V , where v 1 , v 2 A Cluster1, v 3 A Cluster2, their images appear common data and v 1 as the SV, and there is + ( F ( v 1 ) O F ( v + ( F ( v 1 ) Oa 0 ). For a point F ( v 3 ) that is located between F ( v F ( v 2 ) on a half of hyper circle, there is + ( F ( v 1 + ( F ( v 1 ) O F ( v 2 )). Therefore + ( F ( v 1 ) O F ( v According to the lemma, it means v 1 and v 3 have the same label. That is contrary to the unknown. So the assumption is false. The proof is completed. &amp; 4.2. Validation of new labeling approach
From the corollary, + ( F ( v 1 ) O F ( v 2 )) decides whether v belong to the same cluster. It means that SVs X  distribution has the manifolds decided by angel information, and angel can be a good measure to detect SVs clusters. In new approach, H collects the angle information of feature space. According to Ng et al. (2001) , through eigendecomposing H 0 , the rescaled H matrix, the resulted eigenvectors form data X  X  new representations, which reveal distribution manifolds. That is, these new representations mean the movement of data in terms of their angle manifolds. Then, data groups that are detected based on these new representations account to accurate clusters. It indicates the new labeling approach is valid. 4.3. Scale parameterization
For SVC, q is crucial because the final labeling is based on cluster contours and q is the key to determine smoothness and shape of contours. SVC has to pay cost to parameterize q adaptively. For iSVC, q is not so crucial because the new labeling approach partitions SVs by learning their manifolds and is not based on investigating cluster contours. So iSVC has a relatively loose range of q . This paper gives a heuristic. (1) For point x its distance list in the ascending order: { d ij 9 d ij 4 = d d = : x i x j : ,( j =1, y , N ). (2) Compute gap ( i )=max ( j =2, y , N ) . (3) r = ave { gap ( i )}. (4) q =1/( r 2 5. Time complexity analysis
First consider time consumption of iSVC optimization piece. It
SVC and iSVC on real datasets is recorded, where  X  X ize X  and  X  X ubsetSize X  are the whole set size and the subset size. Datasets are taken from UCI ( http://archive.ics.uci.edu/ml/ ).
Now discuss the second piece. For clarity, the new labeling approach is named as NLA. NLA X  X  time is spent on two aspects.
The one is on eigenvalue decomposition, with O (( N sv ) 3 being the number of SVs. The another is on labeling other data, with O (( N N sv ) N sv ). So NLA X  X  one-run time is O (( N cost is compared with CG, SVG, PG, GD, and CCL.

Table 2 lists time complexity of methods, where T one is one-run time, and T all is entire running time. z is the sampling number for
CG, SVG, PG, and GD; N sep is the number of SEPs for GD; t is the iteration times to converge to a SEP. PG uses Delaunay triangulation ( Lee and Schachter, 1980 ) as the simulation approach to learn connected components, and EN is the number of edges of Delaunay triangulation. These methods do parameterization through cross validation ( Kohavi, 1995), and L is the fold number of cross validation. CCL runs over a Kernel scale parameter list, and L 2 is the length of that list.

For T one , CCL shows the best efficiency. NLA X  X  behaviors follow it closely. The exceeding cost of NLA over CCL is consumed on clusteringSVs.PG,SVG,GD,andCGw itness the gradually decreasing efficiency. That is coincided with their approximation extent. But for
T , the advantage of NLA is clear. The scale parameterization makes
NLA not suffer from cross validation and scale searching. In the following experiments, it finds th is parameterization can produce competitive results in most cases. Thus theoretically NLA X  X  practical total time performance is the b est among its counterparts. Table 3 gives the time cost of these methods in some real datasets. These results coincide with above analysis. 6. Experiment analyses 6.1. Test reduction strategy
Empirically, one 2-dimension dataset is tested. For datasets shown in Fig. 5 , the subset produced by reduction strategy is plotted in Fig. 6 . Obviously, the subset meets two required conditions to include all boundary information and cover all clusters. Fig. 7 shows SVs yielded by iSVC X  X  optimization process.
For comparison, SVC is performed on the whole set, with the resulted SVs shown in Fig. 8 . It can be seen that Fig. 7 gives good cluster contours, and it shares most SVs with Fig. 8 . The model constructed on the reduced subset is very close to the standard model. That demonstrates the fine effect of the strategy.
Theoretically quality of reduction strategy is reflected by the subset quality: whether the subs et can contain important informa-tion of the whole set as much as possible. This question is converted to checking the quality of the Kernel matrix that is used in the optimization objective. Suppose the sub-Kernel matrix used by iSVC is subK ; the Kernel matrix used by SVC is K .If subK contains most information of K , then it means the reduction strategy is of fine performance. Here, based on subK , this paper generates a simulation matrix for K .Thequalityof subK is observed through computing the similarity between the simulation matrix and K .

Inspired by reduced support vector machine (RSVM) ( Yuh-Jye and Olvi, 2001 ), the Nystr  X  om method ( Christopher and Matthias, 2001) is used to generate simulation matrix. Denote the subset-based rectangular matrix as recK ; recK consists of rows of K that is respected to the subset. The Nystr  X  om method defines below appK as the approximation of K : appK  X  recK T  X  subK  X  1 0 recK  X  16  X 
Denote the reduced subset as sub 1; its square sub matrix and rectangular sub matrix as subK 1 and recK 1. Then the approxima-two types of subset. 6.1.1. Compare with RSVM X  X  subset
RSVM is another method using a subset to generate model. The subset used by RSVM is random subset. Denote the random subset as sub 2; its square sub matrix and rectangular sub matrix as subK 2 and recK 2. This subset leads to approximation: appK 2  X  X  recK 2  X   X  subK 2  X  1  X  recK 2  X  .

Two measures are computed to check the similarity between original matrix and its approximation: maxEvDif , the max difference of their eigenvalues; and relTrace , the relative differ-ence of their traces: relTrace  X  trace  X  K appK  X  trace  X  K  X   X  17  X 
If two measures are small, it means appK gets to K closer and then subK holds most information of K . Table 4 lists experiment results on real datasets. To get fair comparison, results are the average of 30 runs. Clearly measure values of appK 1 are quite small and much lower than appK 2. It demonstrates subK 1 is better than subK 2 and its subset sub 1 is better than sub 2. Since literature (Yuh-Jye and Olvi, 2001 ) explains the random subset can produce fine model, it shows the reduced subset is of good performance. Consequently, the reduction strategy is of fine performance. 6.1.2. Compare with CVM X  X  subset
Core vector machine (CVM) ( Ivor et al., 2005 ) is a speeding algorithm that generate SVM model on a subset. Its customized subset is formulated by an iterative process. It starts from a two-point subset; then adding the point that is furthest to the current sphere into the subset; then pursues the smallest sphere on the updated subset. Such a process stops when the current sphere could  X  X ontain X  all data under a e -approximation. e controls the loose extent of the sphere radius. Here CVM subset formulation method is compared with the reduction strategy.

CVM and iSVC find their subsets in different manner. We conclude theoretical comparison between them in Table 5 .Tobe frank, CVM holds a narrow insight to iteratively search  X  X urrently X  furthest point, which leads to the resultant subset hard to understand. That is, it is hard to know what types of data are included in the final subset. It is true that CVM X  X  work is not so bad asks model to be well-fitting the dataset. In this aspect, reduction strategy of iSVC does a reliable job by producing an intelligible subset. That is, it is known that the subset contains boundary points and some center-zone points. Note that CVM needs to adjust a important parameter e and it adopts simulation operations when searching the furthest point. All these affect algorithm X  X  work.
Therefore, even in cost CVM takes advantage, for clustering, iSVC is a more pleasant choice than CVM for subset formulation.
 Now give empirical comparison. Denote the subset found by
CVM as sub 3, and its square sub matrix and rectangular sub matrix as subK 3and recK 3. Its approximation is appK 3  X  X  recK 3  X   X  recK 3  X  . Table 6 lists comparison between sub 1and sub 3, where T 1 and T 3 are running seconds of optimization on two types of subset.
These results coincide with above analysis. 6.2. Test modified optimization objective
To check the quality of the modified objective, this section performs SVC and its three variants. SVC1 consists of reduction strategy, modified objective and CG. SVC2 consists of reduction strategy, original objective and CG. SVC3 consists of random subset, original objective and CG. The classical SVC is conducted on whole set, whose results are taken as benchmark results. Set the sampling number as 15. Table 7 records the number of SVs (#SV) and the clustering error ratios (err%) of four methods. For three variants, the numbers of SVs that are shared between them and SVC are recorded in column  X  X hared% X .

From Table 7 , following conclusions are obtained: (i) The three reduced algorithms produce less SVs than SVC and have higher error ratios, which is natural since they are based on less data. (ii)
For  X  X hared% X , SVC2 does best because it is most close to SVC among three variants. SVC3 X  X  behaviors are affected by random subset.
SVC1 uses modified objective, which also brings difference. (iii) For  X  X rr% X , another measure to check algorithm quality, SVC1 is very competitive with, even better than, SVC2. It means the modified objective cooperates well with the reduced subset, and although
SVs yielded by SVC1 are some different from SVC, it produces satisfying results. That demonstrates the quality of the modified objective. (4) For  X #SV X , SVC1 shows higher sparseness than SVC2.
That is the reason of using the modified objective in iSVC. 6.3. Test new labeling approach To check new labeling approach, compare it with CG, SVG, PG,
GD, and CCL. Here CG runs with two sampling numbers: CG1, with sampling number z =15, and CG2 with z =30. SVG, PG, and GD set z as 15. All labeling methods use the original optimization that is conducted on the whole set. Denote the algorithm that consists of original optimization plus new labeling approach as SVC4. Note that SVC4 uses scale parameterization. Table 8 records the clustering error ratios.

From Table 8 , it is clear that for CG1 and CG2, with sampling number increasing, clustering accuracy goes up, which is natural since higher sampling number can reduce randomness. Of course this improvement is at the price of the increase of cost. SVG is an approximation of CG1, so its work is not as good as CG1. PG follows SVG due to its high approximation extent. As mentioned before, GD X  X  behaviors exhibit much unsteadiness. Compared with CG1, it does well in Sonar, Iris, but does poorly in Wine and
Diabetes. CCL and SVC4 show obvious outperformance thanks for they removing randomness. Basically, CCL is a costly but qualified algorithm. SVC4 is competitive with CCL, which shows new labeling approach has fine clustering ability. In some cases CCL outperforms SVC4 because CCL can use a desired scale by searching a long scale list, while SVC4 sets scale using the heuristic. The heuristic can not necessarily work well in all cases. Wine dataset, which has 178 13-dimension data, is an example. But the distance between CCL and SVC4 is small. Basically, SVC4 achieves good results with less cost, so it is more welcome than
CCL in practice. 6.4. Test iSVC performance
To test iSVC, this section compares iSVC with some clustering methods: K-means ( Bradley and Fayyad, 1998 ), Girolami (Girolami, 2002 ), NJW ( Ng et al., 2001 ), and NI ( Ding et al., 2007). Girolami is a Kernel-based method depending on the expectation-maximum process. NJW is a representative of spectrum clustering methods. NI is an agglomerative clustering method. It defines the metric according to information entropy, and takes this information-based distance as clustering criteria to accumulate sub-clusters gradually. These methods set their parameters with 10-fold cross-validation. Here, three UCI data-sets, two relatively big-sized datasets ( Ivor et al., 2005 ), and US
Postal Code set ( LeCun, 1995) are tested here. US Postal Code has 9298 data that are divided into then classes from  X 0 X  to  X 9 X . Here some data randomly sampled from several classes form the samples selected from each class.

From experiments recorded in Table 9 , following conclusions are obtained: (i) First three datasets are used to compare iSVC with SVC1 and SVC4. It finds that iSVC is better than SVC1 and not as good as
SVC4. The outperform ance of SVC4 over iSVC lies in that the former conducts optimization in the whole set while the latter does it in subset. The outperformance of iSVC over SVC1 is due to that the former uses the new labeling appr oach that upgrades clustering accuracy. (ii) Among 5 methods, NJW i s the best one empirically. It presents the optimal results in 11 of 14 cases and does well in other cases. NJW X  X  good work is attributed to its clustering mechanism to derive data inherent distribution directions firstly, and then group data according to these directions. Distribution directions correspond to cluster structures, so clustering based on this information will produce correct clusters. Of course NJW must pay huge cost to achieve good behaviors. (iii) iSVC does improve SVC and follows the optimal result closely; and in rest 5 cases, the difference between iSVC X  X  results and the optimal results is small. Generally speaking, iSVC achieves fine results with low cost, thus it is more feasible than its peers. (iv) K-means does a moderate job due to its heavy dependence on hard rigid partition idea. (v) NI has sharp fluctuations in behaviors. The reason lies in its agglomerative clustering mechanism: it is fairly easy to be affected by data input order. Even the information-based metric helps a lot to provide accurate distance information, NI still does unstable work. 7. Conclusion
A new improved support vector clustering (iSVC) algorithm is presented in this paper. iSVC consists of a reduction strategy and a new labeling approach. Reduction strategy extracts a qualified subset, where a modified objective is optimized to obtain cluster contour information. The strategy is based on the Schr  X  odinger equation, which cooperates with the modified objective to develop competitive model with less cost. The new labeling approach is based on the geometric property of feature space. It labels data in a simple way without randomness and complex operations. Experiments demonstrate good behaviors of iSVC to overcome two bottlenecks, and fine clustering performance in applications.
 Acknowledgement This paper was supported by the National Natural Science Foundation of China under Grant nos. 60673099, 60873146; the National High Technology Research and Develop-ment Program( X 863 X  X rogram) of China no. 2007AA04Z114, 2009AA02Z307; the Key laboratory for Biological Recognition Technology of Jilin Province no. 20082209; the 3rd-phase construction item of Jilin University  X 211 Project X ; the Key Laboratory for Symbol Computation and Knowledge Engineering of the National Education Ministry of China.
 References
