
The ultimate goal of data mining is to extract knowledge from massive data. Knowledge is ideally represented as human-comprehensible patterns from which end-users can gain intuitions and insights. Yet not all data mining methods produce such readily understandable knowledge, e.g., most clustering algorithms output sets of points as clusters. In this paper, we perform a systematic study of cluster descrip-tion that generates interpretable patterns from clusters. We introduce and analyze novel description formats leading to more expressive power, motivate and define novel descrip-tion problems specifying different trade-offs between inter-pretability and accuracy. We also present effective heuristic algorithms together with their empirical evaluations.
The ultimate goal of data mining is to discover useful knowledge, ideally represented as human-comprehensible patterns, in large databases. Clustering is one of the major data mining tasks, grouping objects together into clusters that exhibit internal cohesion and external isolation. Unfor-tunately, most clustering methods simply represent clusters as sets of points and do not generalize them into patterns that provide interpretability, intuitions, and insights.
So far, the database and data mining literature lacks sys-tematic study of cluster description that transforms clusters into human-understandable patterns. For numerical data, hyper-rectangles generalize multi-dimensional points, and a standard approach in database systems is to describe a set of points with a set of isothetic hyper-rectangles [1, 16, 18]. Due to the property of being axis-parallel, such rectangles can be specified in an intuitive manner; e.g.,  X 3.80  X  GPA 4.33 and 0.1  X  visual acuity  X  0.5 and 0  X  minutes in gym per week  X  30 X  intuitively describes a group of  X  X erds X .
Patterns are models with generalization capacity, as well as templates that can be used to make or to generate things. The rectangle-based expressions are interpretable models; as another practical application, they can also be used as search conditions in SELECT query statements to retrieve (generate) cluster contents, supporting query-based iterative mining [13] and interactive exploration of clusters.
To be understandable, cluster descriptions should appear short in length and simple in format. Sum of Rectangles ( SOR ), simply taking the union of a set of rectangles, has been the canonical format for cluster descriptions in the database literature. However, this relatively restricted for-mat may produce unnecessarily lengthy descriptions. We introduce two novel description formats, leading to more expressive power yet still simple enough to be intuitively understandable. The SOR  X  format describes a cluster as the difference of its bounding box and a SOR description of the non-cluster points within the box. The kSOR  X  for-mat allows describing different parts of a cluster separately, using either SOR or SOR  X  descriptions. We prove that the kSOR  X  -based description language is equivalently ex-pressive to the (most general) propositional language [18].
Meanwhile, cluster descriptions should cover cluster contents accurately, which conflicts with the goal of min-imizing description length. The Pareto front for the bicrite-ria problem of optimizing description accuracy and length, as illustrated in Figure 3, offers the best trade-offs between accuracy and interpretability for a given format. To solve the bicriteria problem, we introduce the novel Maximum Description Accuracy (MDA) problem with the objective of maximizing description accuracy at a given description length. The optimal solutions to the MDA problems with different length specifications up to a maximal length con-stitute the Pareto front. The maximal length to specify (20 in Figure 3) is determined by the optimal solution to the Minimum Description Length (MDL) problem, which aims at finding some shortest perfectly accurate description that covers a cluster completely and exclusively. Previous re-search only considered the MDL problem; however, per-fectly accurate descriptions can become very lengthy and hard to interpret for arbitrary shape clusters. The MDA problem allows trading accuracy for interpretability so that users can zoom in and out to view the clusters.

The description problems are NP-hard. We present heuristic algorithms Learn2Cover for the MDL problem to approximate the maximal length, and starting from which DesTree for the MDA problems to iteratively build the so-called description trees approximating the Pareto front. The resulting descriptions, in the format of SOR or SOR  X  , can be transformed into shorter kSOR  X  descriptions with at least the same accuracy by FindClans, taking advantage of the exceeding expressive power of kSOR  X  descriptions. Contributions. (1) Introduction and analysis of novel de-scription formats, SOR  X  and kSOR  X  , providing enhanced expressive power. (2) Definition and investigation of a novel description problem, MDA, allowing trading accuracy for interpretability. (3) Presentation and evaluation of effec-tive description heuristics, Learn2Cover, DesTree and Find-Clans, approximating the Pareto front.
 Related work. [1] studies grid data and defines a cluster as a set of connected dense cells. Their proposed Greedy Growth heuristic constructs an exact covering of a cluster with maximal isothetic rectangles. In the heuristic, a yet-uncovered dense cell is arbitrarily chosen to grow as much as possible along an arbitrarily chosen dimension and con-tinue with other dimensions until a hyper-rectangle is ob-tained. A greedy approach is then used to remove redun-dancy from the set of obtained rectangles. This special case of cluster description is related to the problem of cover-ing rectilinear polygons with axis-parallel rectangles [11], which is NP-complete [17], no polynomial time approxima-tion scheme [3], and usually studied in 2-dimensional space in the computational geometry community (e.g., [15]). [16] also studies grid data but generalizes the description problem studied in [1] by allowing covering some  X  X on X  X  care X  cells to reduce the cardinality of the set of rectangles. Their proposed Algorithm BP bases on Greedy Growth to generate the initial set of rectangles, then performs greedy pairwise merges of rectangles without covering undesired cells and with limited  X  X on X  X  care X  cells for use.
Greedy Growth and BP explicitly work on cluster de-scription. However, despite the grid data limitation, they address the MDL problem solely while our focus is on the more useful and practical MDA problem. In addition, they only use the SOR format while we study and apply novel formats with more expressive power.

Similar to [1] and [16], [18] is motivated by database applications too but with a focus on the theoretical formula-tion and analysis of concise descriptions. [18] also formally defines the general MDL problem for given language ( L -MDL) and proves its NP-completeness. As the initial work of this study, [9] discusses cluster description formats, prob-lems and algorithms at the introductory level. [10] extends the description problem to the classification problem.
Axis-parallel decision trees [5] can be related to clus-ter description technically as they provide feasible solutions to the MDL and MDA problems even if with different ob-jectives. Consider a closed rectangular instance space, the leaf nodes of a decision tree correspond to a set of isothetic rectangles forming a partition of the training data (and the instance space). Stipulating rectangles to be disjoint, deci-sion tree methods can be considered addressing a partition-ing problem (with the additional constraint of partitioning the instance space) while cluster description is essentially a covering problem allowing overlapping. Partitioning prob-lems are  X  X asier X  than covering problems in the sense that they have a smaller search space. Algorithms for a parti-tioning problem usually work too for the associated cov-ering problem but typically generate larger covers. In de-cision tree induction, the preference for shorter trees co-incides with the preference for shorter description length in the MDL problem. As in the MDA problem, decision tree pruning allows trading accuracy (on training data) for shorter trees, and the technique can be applied to generate feasible solutions for the MDA problems.

Indirectly related work in the theory community exists. [6] studies the red blue set cover problem. Given a set of red and blue elements and a family which is a subset of the power set of the element set, find a subfamily of given cardinality that covers all the blue elements and the min-imum number of red elements. [7] studies the maximum box problem. Given two finite sets of points, find a hyper-rectangle that covers the maximum number of points from one designated set and none from the other. Both problems are NP-hard and related to the MDA problem (with preci-sion at fixed recall of 1 and recall at fixed precision of 1 as the accuracy measures respectively, see  X  3.1), except that the former is given an alphabet (family) and restricted to the SOR format, and the latter is limited to use one rectangle.
The above discussed research more or less roots in the classical minimum set cover and maximum coverage prob-lems. The former attempts to select as few as possible sub-sets from a given family such that each element in any sub-set of the family is covered; the latter, a close relative, at-tempts to select k subsets from the family such that their union has the maximum cardinality. The simple greedy al-gorithm, iteratively picking the subset that covers the maxi-mum number of uncovered elements, approximates the two NP-hard problems within (1+ ln n ) [14] and (1  X  1 e ) [12] re-spectively. The ratios are optimal unless NP is constrained in quasi-polynomial time [8]. The minimum set cover and maximum coverage problems are related to the MDL and MDA (with recall at fixed precision of 1 as the accuracy measure) problems respectively except that they are given an alphabet (family) and restricted to the SOR format. Organization of the paper. In Section 2 we introduce and analyze the description formats. In Section 3 we formalize the description problems. We present heuristic algorithms in Section 4, report empirical evaluations in Section 5, and conclude the paper in Section 6.

In this section, we study alphabets, formats, and lan-guages for cluster descriptions in depth so as to gain insights into the description problems with different given formats.
Given a finite set of multi-dimensional points U as the universe, and a set of isothetic hyper-rectangles  X  as the alphabet, each of which is a symbol and subset of U con-taining points covered by the corresponding rectangle. A description format F allows certain Boolean set expressions over the alphabet. All such expressions constitute the de-scription language L with each expression E  X  L being a possible description for a given subset C  X  U . The vocab-ulary for E , V E , is the set of symbols in E . We summarize the notations used and to be used for easy lookup. D : data space; D = D 1  X  D 2  X  ...  X  D d U : data set; U  X  D R : rectangle or the set of points it covers; R  X  U  X  : alphabet; a set of symbols with each as a rectangle V
E : vocabulary of expression E ; set of symbols used in E || E || : length of expression E ; || E || = | V E | B u : bounding box for u ; u is a set of points or rectangles C : set of points in a given cluster; C  X  U C  X  : set of points in B E F, X  : expression in format F with vocabulary in  X  L F, X  : language comprising all E F, X  expressions
Note that R ( E ) is overloaded to denote a rectangle (ex-pression) or the set of points it covers (describes). The dis-tinction should be made clear by the context.  X  is often left unspecified in E F, X  and L F, X  when assuming some default alphabet (to be discussed shortly).  X  +  X ,  X   X   X ,  X   X   X  and  X  are used to denote Boolean set operators union, intersection, difference and complement.
 Two descriptions E 1 and E 2 are equivalent, denoted by E 1 = E 2 , if they cover the same set of points. Logical equivalence implies equivalence but not vice versa. || E || indicates the interpretability of E for a given for-mat. There are two simple ways of defining || E || , absolute length and relative length. Absolute length is the total num-ber of occurrences of symbols in E ; relative length is the cardinality of V E . Neither alone captures the interpretability of E perfectly. The former overestimates the repeated sym-bols; the latter underestimates the repeated symbols. The two converge if the repeated symbols are few, which we ex-pect to be the case for cluster descriptions. We define || to be the relative length of E for the ease of analysis.
Description accuracy is another important measure for the goodness of E , which we will discuss in more details in section 3. For the use of this section, we conservatively de-fine the  X  X ore accurate than X  relationship for descriptions. A description for C is more accurate than the other if it cov-ers more points from C and less points from C  X  . Definition 2.1 ( more accurate than ) Given E 1 and E 2 as descriptions for a cluster C ,wesay E 1 is more accurate than E 2 , denoted by E 1  X  accu E 2 ,if | E 1  X  C | X | E 2 and | E 1  X  C  X  | X | E 2  X  C  X  | . ( E 1  X  C  X  E 2  X  C )  X  ( E 1  X  C  X   X  E 2  X  C  X  )  X  E 1  X 
A description problem, viewed as searching, is to search good descriptions that optimize some objective function in a given description language. A more general description language implies more expressive power. Language L 1 is more general than language L 2 if L 1  X  L 2 . To character-ize expressive power more precisely, we define the  X  X ore expressive than X  relationship for languages. L 1 is more ex-pressive than L 2 if for any description of cluster C in L there is a shorter and more accurate description in L 1 . Definition 2.2 ( more expressive than ) Given two descrip-tion languages L 1 , L 2 and a cluster C ,wesay L 1 is more expressive than L 2 , denoted by L 1  X  exp L 2 , if for any de-scription E 2  X  L 2 , there exists some description E 1  X  with || E 1 || X || E 2 || and E 1  X  accu E 2 with respect to C .
A more expressive language is guaranteed to contain  X  X etter X  expressions with respect to length and accuracy. Certainly, L 1  X  L 2  X  L 1  X  exp L 2 . Yet to restrict the search space, we do not want languages to be unnecessarily general. This concern carries on through the following dis-cussions on alphabets and formats, by which languages are specified.
Unlike the set cover problem, alphabet  X  is not explicitly given in cluster description. Assuming given  X  , a descrip-tion problem, MDA or MDL, can be considered searching through a given language L for the optimal expression. We call such problems L -problems; in particular, L -MDA or L -MDL. The L -problems, to be detailed shortly, are variants of the set cover problem. We brief alphabets mainly for the purpose of gaining insights into the description problems by relating their L -problems to the classical set cover problem.
 X  is potentially an infinite set since for a set of points, there are an infinite number of covering rectangles with each being a candidate symbol. A simple finite alphabet can be defined as the set of bounding boxes for the subsets of B C , i.e.,  X  most = { B S | S  X  B C } . The alphabet is fi-nite since there is a unique bounding box for a set of points.  X  most is the most general alphabet relevant to the task of describing C ; however, it can be unnecessarily general for some L -problem with a mismatched format. It is desirable to have some most specific sufficiently general alphabets.
Let f ( L -p ) denote the feasible region of an L -problem L -p ; certainly, f ( L -p )  X  L .Wesay  X  is sufficiently general means  X  does not lose to  X  most if used in the L -p case. On top of being sufficiently general,  X  is most specific if removing any element from it,  X  would not be sufficiently general anymore. In the following, we define some alpha-bets with this desireable property.
  X  mix = { B S | S  X  C  X  S =  X  X 
 X  pure (  X   X  pure ) contains pure rectangles covering points from C ( C  X  ) only. Symbols in  X  mix (  X   X  mix ), however, can be mixed allowing points from C and C  X  to co-exist. Ap- X  most . Multiple subsets may match to the same bounding box, e.g., B 13 = B 123 in Figure 1, where B 13 is short for B { 1 , 3 } and so on. The figure illustrates  X  pure and  X  mix
Examples of some L -problems with matching alpha-L L kSOR  X  , X  k -MDA are also such examples. Due to the page limit, we omit further explanations.

Such a desirable  X  is assumed given by default if it is left unspecified in E F, X  or L F, X  . Although these default alpha-bets are made specific, they can still be prohibitively large (e.g., O ( | 2 C | ) and not scalable to real problems. Therefore, it is practically infeasible to generate  X  and apply existing set cover approximations on description problems.
We require descriptions to be interpretable. For descrip-tions to be interpretable, the description format has to have a simple and clean structure. Sum of Rectangles ( SOR ), de-noting the union of a set of rectangles, serves this purpose well and has been the canonical format for cluster descrip-tions in the literature (e.g., [1, 16]). For better interpretabil-ity, we also want descriptions to be as short as possible. To minimize the description length of SOR descriptions has been the common description problem.

Nevertheless, there is a trade-off between our prefer-ences for simpler formats and shorter description length. Simple formats such as SOR may restrict the search space too much leading to languages with low expressive power. On the other hand, if a description format allows arbitrary Boolean operations over a given alphabet, we certainly have the most general and expressive language containing the shortest descriptions, but such descriptions are likely hard to comprehend due to their complexity in format despite their succinctness in length. Moreover, not well-structured com-plex formats bring difficulties in manipulation of symbols and design of efficient and effective searching strategies.
Clearly, we require description languages with high ex-pressive power yet in intuitively understandable formats. In the following, we explore several alternative description formats beyond SOR , in particular, SOR  X  and kSOR  X  . While SOR takes the form of R 1 + R 2 + ... + R l ,a SOR  X  description, in describing C , takes the set difference between B C and a SOR description for C  X  .
 Definition 2.3 ( SOR  X  description ) Given a cluster C ,a SOR  X  description for C , E SOR  X  ( C ) , is a Boolean expres-sion in the form of B C  X  E SOR ( C  X  ) , where E SOR ( C a SOR description for C  X  .

In addition, a SOR  X  description for C , E SOR  X  ( C ) , is an expression in the form of either E SOR ( C ) or E L
SOR . In describing a cluster C , SOR and SOR tions together nicely cover two situations where C is easier to describe or C  X  is easier to describe. Different data distri-butions, which are usually not known in advance, favor dif-ferent formats. In Figure 2, consider C 2 as a single cluster to be described with perfect accuracy, certainly SOR  X  de-scriptions are favored. The shortest E SOR ( C 2 ) has length 4 whereas the shortest E SOR  X  ( C 2 ) has length 2.
SOR  X  descriptions have a structure as simple and clean as SOR descriptions. In addition, the added B C draws a big picture of cluster C and contributes to interpretability in a positive way. The two formats together also allow us to view C from two different angles. Note that the special rectangle B C is required for the format, it is not included in the default alphabets either counted in || E || for simplicity.
SOR  X  descriptions generally serve well for the purpose of describing compact and distinctive clusters. Neverthe-less, arbitrary shape clusters are not uncommon and for such applications, we may want to further increase the expressive power of languages by allowing less restrictive formats. For example, if some parts of cluster C favor SOR and some other parts favor SOR  X  , then SOR  X  is too restrictive to consider different parts separately. Instead, it can only pro-vide a global treatment for C . To overcome this disadvan-tage, we introduce kSOR  X  descriptions.
 Definition 2.4 ( kSOR  X  description ) Given a cluster C ,a kSOR  X  description for C , E kSOR  X  ( C ) , is a Boolean ex-pression in the form of E SOR  X  ( C 1 )+ E SOR  X  ( C 2 )+ E
Clearly, kSOR  X  descriptions generalize SOR  X  de-scriptions by allowing different parts of C to be described separately; and the latter one is a special case of the for-mer one with k =1 . In Figure 2, C 1 favors SOR whereas C 2 favors SOR  X  . The shortest E SOR ( C ) and E SOR  X  ( C have length 5 and 4 respectively. kSOR  X  is able to provide local treatments for C 1 and C 2 separately and the shortest E be found much more effective than other simpler formats; but how expressive is L kSOR  X  precisely? In the following, we compare it with the propositional language ,themost general description language we consider (as in [18]). Definition 2.5 ( propositional language ) Given  X  as the al-phabet, L P, X  is the propositional language comprising ex-pressions allowing usual set operations of union, intersec-tion and difference over  X  .

Consider any E  X  L P, X  k . Since E can be re-written as E DNF in disjunctive normal form with the same set of vocabulary, thus || E || = || E DNF || and E = E DNF . Each disjunct in E DNF is a conjunction of literals and each literal takes the form of R or  X  R where R  X   X  k . Consider any dis-junct E j in E DNF , since axis-parallel rectangles are inter-section closed, E j can be re-written as E j , which takes one of the following three forms: (1) E j = R 0 ;(2) E j = R 0 the three cases || E j || X || E j || and E j = E j . For case 3, due to the generalized De Morgan X  X  law, E B For case 1 and 2, we suppose R 0  X   X  k . Then for case 1, E j is a SOR description. For case 2, due to the generalized De Morgan X  X  law, E j = R 0  X   X  ( R x + R y + ... + R z )= R 0  X  ( R x + R y + ... + R z ) , which is a SOR  X  description.
Then, E j can be re-written as an equivalent SOR  X  de-scription with length  X || E j || . We do this for every disjunct of E DNF , then E can be re-written as a kSOR  X  descrip-tion E  X  L kSOR  X  , X  k with || E || X || E || and E = E ,
Note that for case 1 and 2, we have supposed R 0  X   X  k , which may not hold since  X  k is not intersection closed. As a simple counter-example, the intersection of two bounding boxes may not be a bounding box. However, we note that for the two cases, the purpose of E j is to describe R 0  X  C 0  X  C , thus R 0 = B C 0  X   X  k . As expressions of length 1 describing C 0 , R 0  X  accu R 0 because ( R 0  X  C 0 = R 0 (
R E j , then E j be re-written as a more accurate SOR  X  description with length  X || E j || . Wo do this for every disjoint of E DNF then E can be re-written as a kSOR  X  description E  X  L
Theorem 2.6 does not hold if description length || E || is defined as the absolute length, in which case E =( R 1 + R 2 )  X  R 3 in L P has length 3 but the equivalent E =( R 1 R 3 )+( R 2  X  R 3 ) in L kSOR  X  has length 4. Nevertheless, the general conclusion persists, that is, L kSOR  X  is a very expressive language close or equal to L P .

Despite its exceptional expressive power, the kSOR  X  format is very simple and conceptually clear, allowing only one level of nesting as the SOR  X  format. It is also well-structured to ease the design of searching strategies.
Assuming some given default alphabet, previous re-search studied cluster description as searching the shortest expression in L SOR, X  pure , the simplest and least expressive language we discussed in this section. We study the same problem but considering other more expressive languages L
SOR  X  and L kSOR  X  , and our main focus is on the problem of finding the best trade-offs between accuracy and inter-pretability, as to be introduced in the following section.
A description problem is to find a description for a clus-ter in a given format that optimizes some objective. In this section, we introduce cluster description problems with dif-ferent objective measures.
We want to describe a given cluster C with good inter-pretability and accuracy. Simple formats and shorter de-scriptions lead to improved interpretability. We have stud-ied alternative description formats that are intuitively com-prehensible. Within a given description format, description length is the proper objective measure for interpretability.
In addition to interpretability, the objective of minimiz-ing description length can also be motivated from a  X  X ata compression X  point of view. There are many situations when we need to retrieve the original cluster records; e.g., to send promotion brochures to a targeted class of customers, to perform statistical analysis, or in a query-based itera-tive mining environment as advocated by [13], to resume the mining process from stored temporary or partial results. Cluster descriptions provide a neat and standalone way of  X  X toring and retrieving X  cluster contents.

In DBMS systems, an isothetic rectangle can be spec-ified by a Boolean search condition such as 1  X  D 1  X  10  X  ...  X  5  X  D pound search condition for the points in the cluster, which can be used in the WHERE clause of a SELECT query state-ment to retrieve the cluster contents entirely. In this sce-nario, the cluster description process resembles encoding and the cluster retrieval process resembles decoding. The compression ratio for cluster description E can be roughly defined as | E | / ( || E || X  2) , as each rectangle takes twice as much space as each point. The goal of large compres-sion ratio leads to the objective of minimizing description length. Meanwhile, shorter length also speeds up the re-trieval process by saving condition checking time [18].
Accuracy is another important measure for the good-ness of cluster descriptions. An accurate description should cover many points in the cluster and few points not in the cluster. To precisely characterize description accuracy, we borrow some notations from the information retrieval com-munity [2] and define recall and precision for a description E of cluster C .
If we only consider recall , the bounding box B C could make a perfectly accurate description; if we only consider precision , any single point in C would do the same. The F -measure considers both recall and precision and is the harmonic mean of the two.
A perfectly accurate description with f =1 has recall = 1 and precision = 1. In a perfectly accurate SOR or SOR  X  description, all rectangles are pure in the sense that they contain same-class points only.

The F -measure does not fit situations where users want to specify constraints on either recall or precision .Wein-troduce two additional measures to provide this flexibility. The first is recall at fixed precision ; often, we want to fix precision at 1. The second is precision at fixed recall ;of-ten, we want to fix recall at 1. The measures can be found useful in many situations. If we can afford to lose points in C much more than to include points in C  X  , then we can choose recall at fixed precision of 1 to sacrifice recall and protect precision as in the maximum box problem [7]; in the opposite situation, we can choose precision at fixed recall of 1 as in the red blue set cover problem [6].
Description length and accuracy are two conflicting ob-jective measures that cannot be optimized simultaneously. The Pareto front for the bicriteria problem, as illustrated in Figure 3, offers the best trade-offs between accuracy and interpretability for a given format. To solve the bicriteria problem and obtain the best trade-offs, we introduce the novel Maximum Description Accuracy (MDA) problem. Definition 3.1 ( Maximum Description Accuracy problem ) Given a cluster C , a description format F , an integer l , and an accuracy measure, find a Boolean expression E in format F with || E || X  l such that the accuracy measure is maximized.

The optimal solutions to the MDA problems with differ-ent length specifications up to a maximal length constitute the Pareto front. The vertical lines in Figure 3 illustrate the feasible regions of the MDA problems, whose union is the feasible region of the bicriteria problem. The maximal length is the length of some shortest description with perfect accuracy, which is 20 in Figure 3. It is pointless to spec-ify larger lengths as the best accuracy has been achieved. To determine this maximal length, we define the Minimum Description Length (MDL) problem, whose objective is to find some shortest description that covers a given cluster completely and exclusively, i.e., with f =1 .
 Definition 3.2 ( Minimum Description Length problem ) Given a cluster C and a description format F , find a Boolean expression E in format F with minimum length such that ( E  X  C = C )  X  ( E  X  C  X  =  X  ) .

The optimal solution to the MDL problem gives the max-imal length to specify in solving the MDA problems. The feasible region of the MDL problem is also illustrated in Figure 3. Previous research only considered the MDL prob-lem with SOR as the description format. However, in prac-tice, perfectly accurate descriptions can become lengthy and hard to interpret for arbitrary shape clusters. The MDA problem allows trading accuracy for interpretability so that users can zoom in and out to view the clusters. From a  X  X ata compression X  point of view, description requiring perfect accuracy resembles lossless compression while description allowing lower accuracy resembles lossy compression.
Assuming some given default alphabets as previously discussed, the L -problems, easier than their counterparts, can be related to some variants of the set cover problem. In particular, L SOR, X  pure -MDL corresponds to the minimum set cover problem, which is known to be NP-hard. Other L -MDL problems are harder in the sense that they have larger search spaces with more general languages.

Given the recall at fixed precision of 1 accuracy measure, the L SOR, X  pure -MDA problem corresponds to the maxi-mum coverage problem, which is known to be NP-hard. Given the precision at fixed recall of 1 accuracy measure, the L SOR, X  pure -MDA problem corresponds to the red blue set cover problem, which is also NP-hard [6]. Given the F -measure, the decision version of the L SOR, X  pure -MDA problem is reducible to the decision problem of either of the two. Other L -MDA problems are harder in the sense that they have larger search spaces.

As we have seen, the cluster description problems, with different format and accuracy measure specifications as dis-cussed, are all NP-hard. We present efficient and effective heuristic algorithms for these problems in the next section. In this section, we present three heuristic algorithms. Learn2Cover solves the MDL problem approximating the maximal length. Starting with the output of Learn2Cover, DesTree iteratively builds the so-called description tree for the MDA problems approximating the Pareto front. Find-Clans transforms the output descriptions from DesTree into shorter kSOR  X  descriptions without reducing the accuracy.
Given a cluster C , Learn2Cover returns a description E for C in either SOR or SOR  X  format with f =1 .For this purpose, it suffices to learn a set of pure rectangles covering C and a set of pure rectangles  X  covering C  X  completely. Learn2Cover is carefully designed such that and  X  are learned simultaneously in a single run; besides, the extra learning of  X  does not come as a cost but rather a boost to the running time.
 Sketch of Learn2Cover. To better explain the main ideas, we give the pseudocode for the simplified Learn2Cover and its major procedure cover() in the following.

In preprocessing(), the bounding box B C is determined; the points in B C are normalized against B C and sorted along a selected dimension D s . Ties are broken arbitrarily.
Algorithm : Learn2Cover foreach ( o x  X  B C ) { // processedinsortedorder
Procedure : cover ( , o x ,  X  ) foreach ( R  X   X  ) { foreach ( R  X  &amp;&amp; R is not closed ) { foreach ( R  X  ) { // processed in ascending order of cost At the moment we suppose there are no mixed ties involv-ing points from different classes. In general, the choice of D s does not have a significant impact if the data is not ab-normally sparse; thus D s can be arbitrarily chosen. Never-theless, Learn2Cover offers an option to choose D s with the maximum variance, which makes the algorithm more robust against some rare, malicious cases such as large mixed tie groups. Learn2Cover is deterministic once D s is chosen. Initially =  X  and  X  =  X  .Let o x be the next point from B
C in the sorted order to be processed. cover( , o x , or cover(  X  , o x , ) is called upon depending on o x  X  C or C  X  . The two situations are symmetric.

Suppose o x  X  C , procedure cover( , o x ,  X  ) chooses a non-closed R  X  covering no points covered by rec-tangles in  X  and with the minimum covering cost with respect to o x to expand and cover o x . Otherwise, a new rectangle R new minimally covering o x will be created and added to (line 12). A rectangle is closed if it cannot be expanded to cover any further point without causing a covering violation , i.e., covering points from the other class (lines 1, 2, 3). Violation checking can be expensive; therefore, we always calculate cost ( R, o x ) first. If there is a non-closed R with cost ( R, o x ) = 0, we need to extend R only along D s to cover o x , in which case violation check-ing is unnecessary (lines 4, 5, 6, 7). Otherwise rectangles are considered in ascending order of cost ( R, o x ) for viola-tion checking. The first qualified rectangle will be used to cover o x (lines 8, 9, 10, 11). In the following we discuss covering cost and covering violation in more details. Covering cost and choice of rectangles. The behavior of Learn2Cover largely depends on how the covering cost cost ( R, o x ) is defined, i.e., the cost of R to cover point o This cost should estimate the reduction of the potential of R to cover further points after o x . Intuitively, we want to choose R with the minimum increased volume, so that rec-tangles can keep maximal potential for future expansions without incurring covering violations.

Yet there are more issues to be concerned beyond this basic principle. First, when calculating the increased vol-ume for R , we should not consider D s . Since points are sorted on D s and processed in the sorted order, the exten-sion of R along D s is the distance it has to travel to cover further points after o x . To keep R short on D s does not help to keep its potential for future expansions. In Figure 4(a), if we considered D s , R 1 would have the biggest increased volume and not be chosen to cover o x . But this saved space would be part of the expanded R 1 in covering any point af-ter o x , and whether R 1 had been expanded already to cover o x or not would not make a difference. This suggests that the increased volume of R 1 with respect to o x should be 0, ignoring D s in the calculation.

Second, if the expanded R has a length of 0 or close to 0 in any dimension, its volume and increased volume will be 0 or close to 0, which makes R a favorable choice. But R may have traveled far along some other dimensions to cover o ; its expansion potential would thus be limited. In Figure 4(a), both R 1 and R 3 require the same increased volume of 0 to cover o x since R 1 has to be extended only along D s and R 3 has a length of 0 in one of its dimensions. However, R 3 has to travel far along some dimensions other than D s whereas R 1 does not. Moreover, R 2 does not have the in-creased volume of 0, but it seems not to be a worse choice than R 3 because o x is more local to R 2 than R 3 . Therefore, cost needs to fix the illusion of 0 increased volume and take into account the locality of rectangles.

In the following, we propose a definition for cost ( R, o with these issues in consideration. Let l j ( R ) denote the length of rectangle R along dimension D j , and R denote the expanded R in covering o x . aveIncV ol ( R, o x )=( vol ( R )  X  vol ( R )) 1 / ( d  X  1) cost ( R, o x )= aveIncV ol ( R, o x )+ dist ( R, o x )
D s is ignored if we project R and o x onto the sub-space D \ D s . vol ( R ) is the volume of the projected R ; aveIncV ol can be viewed as the increased volume aver-aged on each dimension; dist is precisely the Euclidean dis-tance from the projected o x to the projected R . cost is the sum of aveIncV ol and dist , i.e., we assign equal weights to both components of cost . According to this definition of cost ( R, o x ) , the choices in Figure 4(a) would be R 1 and R 3 in priority order.

Sometimes there are no good rectangles available, in which case forcing greedy expansions may deteriorate the overall performance. Learn2Cover provides an expansion control parameter to limit the maximum distance each di-mension can travel at each expansion. Since data is normal-ized, the default choice of 0.5 means that each expansion cannot exceed half of the span of B C in each dimension. The parameter is user-specified but not sensitive. Without expansion control, Learn2Cover works generally well; but it may help in cases of extremely sparse or malicious datasets. Figure 4(b) is a real run of Learn2Cover on a toy dataset. Dark and light points denote points in C and C  X  respec-tively. Rectangles are numbered in ascending order of their creation time. Note that on processing A , a better choice of R 3 was made while R 4 was also available. R 3 had cost of 0 with D s ignored. If R 4 had been chosen to cover A ,it would have been closed before covering B .
 Covering violation and correctness. Learn2Cover is re-quired to output pure rectangles, any covering of inter-class points will be considered as a violation.
 BP considers all inter-class points in violation checking. In Learn2Cover, since points are processed in the sorted or-der, the only points that could lead to violations in the ex-pansion of R i  X  (  X  ) are currently processed points in R ist to help each other in violation checking to dramatically reduce the number of points in consideration. A simple aux-iliary data structure is maintained to avoid the possible per-formance deterioration in the presence of extremely dense and big rectangles. We omit the details due to the page limit.
Learn2Cover outputs pure rectangle collections and  X  covering every point in C and C  X  respectively. We examine procedure cover() to argue the correctness. From the definition of cost , cost ( R, o x )=0 if and only if the projected o x is in the projected R . In such case, if R  X  (  X  ) and o further point without covering o x , which causes a violation and R will thus be closed (line 3). If R  X  (  X  ) and o as (line 6) without causing any violation. If there existed o causing a violation, o must have been processed before o x with cost be closed. In line 10, R is expanded to cover o x after vi-olation checking. In line 12, R new is a degenerate rectan-gle covering only one point o x . Thus, upon completion of Learn2Cover, each o x  X  B C is covered without violation.
In the sketch of Learn2Cover, we have assumed there were no mixed ties involving points from different classes. This case may happen and even frequently on grid data. Mixed tying points may cause covering violation to one another. Learn2Cover identifies mixed tie groups in the preprocessing() step, and then some extra checking is per-formed on processing o x belonging to a mixed tie group.
The MDA problem is to find a description E with a user-specified length l for cluster C maximizing a given accu-racy measure. Our algorithm DesTree takes the output from Learn2Cover, or  X  whose cardinality approximates the maximal length, iteratively builds a so-called description tree approximating the Pareto front.

Description trees are tree structures resembling dendro-grams to provide overviews on alternative trade-off descrip-tions of different lengths. Accordingly, DesTree resembles agglomerative hierarchical clustering to iteratively merge child nodes into parent nodes until a single node is left. Each node in a description tree represents a rectangle; and a normal merge operation produces a parent node that is the bounding box of its child nodes. The tree grows bottom-up along a series of merge operations.
 Each horizontal cut in the tree defines a set of rectangles. For the so-called C -description tree, a cut set constitutes the vocabulary for a SOR description of C ; for the so-called C  X  -description tree, a cut set constitutes the vocabulary for a SOR description of C  X  leading to a SOR  X  description of C . The cardinality of a cut set, the description length, equals to the number of links being cut. Each cut offers an alternative trade-off between description length and accu-racy. The higher in the tree we cut, the shorter the length and the lower the accuracy.

Consider a SOR ( SOR  X  ) description, merging two rec-tangles into their bounding box may cause precision ( recall ) to decrease; removing a rectangle may cause recall ( preci-sion ) to decrease. Both operations trade the accuracy mea-sure, say f , for shorter length and we want to consider both. To integrate the removal operation in building description trees, we add a symbolic node, the empty set  X  , into the leaf nodes and define the merge operator as follows. Definition 4.1 ( merge ) R i merge R j = R parent = (1) bounding box for R (2)  X  otherwise
DesTree is a greedy approach starting from the input leaf nodes, a set of pure rectangles or  X  generated by Learn2Cover, building the tree in a bottom-up fashion. Pairwise merge operations are performed iteratively, and the merging criterion is the biggest resulting accuracy mea-sure. The C -description tree and C  X  -description tree are built separately in the same fashion.

Figure 5 exemplifies a description tree. R 1  X  R 4 are the input rectangles. R 1 and R 2 are chosen for the first merge to give R 5 . The second merge of R 4 and  X  results in the removal of R 4 . R 6 , the parent node of R 5 and R 3 , merges with  X  to give the symbolic root. The lowest cut cut 0 is or  X  . Each cut corresponds to a SOR or SOR  X  description. Take cut 2 as an example. For a C -description tree, cut 2 corresponds to E SOR ( C )= R 5 + R 3 ;fora C  X  -description tree, it corresponds to E SOR  X  ( C )= B C  X  ( R 5 + R 3 ) scription trees are not necessarily binary. A merge could result in more rectangles fully contained in the parent rec-tangle. Nonetheless, the merging criterion discourages branchy trees and Figure 5 is a typical example.

The merging process can be simplified for some accu-racy measures. Given recall at fixed precision of 1 ,forthe C -description tree, only merge operation (2) (the removal operation) needs to be considered, and the root is always for the C  X  -description tree, only merge operation (1) (the normal merge operation) needs to be considered, and the root is always B C  X  . For both cases, precision is guaranteed to be 1 and recall reduces along the merging process.
Given precision at fixed recall of 1 ,forthe C -description tree, only merge operation (1) needs to be considered, and the root is always B C ;forthe C  X  -description tree, only merge operation (2) needs to be considered, and the root is always  X  . For both cases, recall is guaranteed to be 1 and precision reduces along the merging process.

We can easily prove the accuracy measure, recall at fixed precision of 1 or precision at fixed recall of 1 , reduces monotonically along the merging process in DesTree. With respect to the F -measure, though evident in experiments, it is non-trivial to construct a proof of the same property.
FindClans takes as input a cut (denoted by T in the following) from a description tree representing a SOR or SOR  X  description, outputs a kSOR  X  description with shorter length and equal or better accuracy.
 The algorithm is based on the concept of clan .Let SOR V be a SOR description with vocabulary V , e.g., SOR V = R 1 + R 2 for V = { R 1 ,R 2 } . Intuitively, a clan N  X  T is a group of rectangles that dominate (densely pop-ulate) a local region, so that by replacing them as a whole, SOR N can be rewritten as a shorter and more accurate SOR  X  description for the targeted points in the region. Definition 4.2 ( clan ) Given T as a cut from a C ( C  X  ) description tree, N  X  T is a clan if | N | X  X  N | &gt; 1 and B (
SOR N  X  C  X  ) , where B N is the bounding box of N and N a set of rectangles associated with N called the replacement of N .Wealsoreferto | N | X  X  N | X  1 as N.score .
 Note that the purpose of SOR N is to describe SOR N  X  C ( SOR N  X  C  X  )if T is from a C ( C  X  ) -description tree. N.score is the possible length reduction offered by a single clan N since SOR N will be replaced by B N  X  SOR N . Two clans N 1 and N 2 are disjoint if N 1  X  N 2 =  X  . For a set of mutually disjoint clans, Clans , the total length reduction is at least N i .score where N i  X  Clans .

Figure 6 uses the example in Figure 2 and illustrates how a clan can help to rewrite a SOR description represented by T into a shorter kSOR  X  description.
 Suppose we have found Clans for T and T is from a C -description tree, it is straightforward to rewrite the input SOR description E SOR ( C )= SOR T into a kSOR  X  de-scription as illustrated in Figure 6. For each N  X  Clans , we simply replace SOR N in SOR T by the shorter and more accurate ( B N  X  SOR N ) .

If T is from a C  X  -description tree, the input SOR  X  de-scription is E SOR  X  ( C )= B C  X  SOR T . For each N  X  Clans , we replace SOR N in SOR T by B N and add back SOR N . As an example, let E SOR  X  ( C )= B C  X  SOR T = B N = { R 2 ,R 3 ,R 4 ,R 5 } with replacement N = { R 1 } , then E = T.score =4  X  1  X  1= || E SOR  X  ( C ) || X  X | E kSOR  X  ( C 6  X  4=2 . It is easy to verify that after all such replace-ments, the resulting E kSOR  X  ( C ) is shorter and more ac-curate than E SOR ( C ) or E SOR  X  ( C ) with respect to any accuracy measure we discussed.

All we need is to find Clans . To simplify the task, we define N , the replacement of N , to contain each rec-tangle R  X   X  ( ) overlapping with B N if T is from a C ( C  X  ) -description tree. Then, it is guaranteed that B (
SOR N  X  C  X  ) since N contains pure rectangles completely covering B N  X  C  X  ( B N  X  C ). Thus given a candidate clan N , N is uniquely determined and N is a clan if | N | X  X  N | &gt;
Algorithm Findclans, as presented in the following, con-tinues to call procedure findAClan() to find a clan N in the updated T and insert it into Clans . findAClan() first checks each pair of rectangles in T and finds theN with the highest score. bestN is used to keep track of the best stage of theN , which continues to grow greedily one more R  X  ( T  X  theN ) at a time resulting in the largest score increase, until no more rectangles available. bestN is re-turned if it is a clan; otherwise, NULL .
 Algorithm : FindClans ( T )
Clans  X  X  X  ;
N  X  findAClan ( T ) ; while ( N != NULL ) { return Clans ;
Procedure : findAClan ( T ) find theN ; // consider each pair in T bestN  X  theN ; while ( theN  X  T ) { if ( bestN.score  X  1) else
We experimentally evaluated and compared our meth-ods against CART (Salford Systems, Version 5.0) and BP [16]. While decision tree classifiers, as argued in re-lated work, can be applied to the MDA and MDL problems, BP addresses the MDL problem only. We implemented Learn2Cover, DesTree, FindClans, and BP. For BP, we also implemented Greedy Growth and a synthetic grid data gen-erator. To make our experiments reproducible, real datasets from the UCI repository [4] with numerical attributes and without missing values were used, where data records with the same class label were treated as a cluster. Note that, in the broad sense, a cluster can be used to represent an arbi-trary class of labeled data that require discriminative gen-eralization. The notion of rectangle can be extended (but not implemented in this version) to tolerate categorical at-tributes; e.g., to use sets instead of intervals. Rectangles do not provide generalization on the categorical attributes.
To approximate the Pareto front, our approach starts with applying Learn2Cover for the MDL problem, then DesTree for the MDA problems to build description trees. Decision tree classifiers provide feasible solutions to both the MDL and MDA problems. We compared Learn2Cover and De-sTree with CART on UCI datasets. In each experiment we described one class of points C , considering all points from other classes within B C , the bounding box for C ,as C  X  Learn2Cover vs. CART. In each experiment, B C was fed to Learn2Cover and CART. The CART parameters were set such that a complete tree without misclassifications could be built. The entropy method was used for tree splits.
For each dataset, Table 1 presents the class label, the di-mensionality, the cardinalities of C and C  X  , and the results. For Learn2Cover and CART, a/b denotes the cardinalities of the two sets of rectangles covering C and C  X  respec-tively. For CART, a and b correspond to the numbers of leaf nodes of the two classes. For Learn2Cover, they corre-spond to | | and |  X  | . The smallest a or b is highlighted in bold font. We observe that, on average, Learn2Cover needs only half of the description length required by CART. Re-sults from other UCI datasets as well as synthetic datasets are not presented. However, the above observation holds consistently through all the experiments. The output of Learn2Cover was further fed into Find-Clans for additional length reduction. In Table 1,  X  c de-notes the additional reduction achieved by FindClans com-paring to the shortest length (in bold font). The effective-ness of FindClans depends on the size and distribution of the input data; bigger and more complex datasets are likely to exhibit more clans, leading to more length reduction. We observe that, FindClans further reduces the shortest descrip-tion length by about 20% on average and up to 50%. DesTree vs. CART. In each experiment, DesTree took as input or  X  returned by Learn2Cover. For CART, the complete tree without misclassifications was pruned step by step. In each step, the misclassifications for both classes were counted to calculate the F -measure. The number of rectangles covering the target class C or C  X  was also recorded as the description length.

Figure 7 demonstrates the results of DesTree ( C -description tree) and CART (target class C ), for clarity, on only two of the UCI datasets used in Table 1. f and l denote the F -measure and description length respectively. As ex-pected, for both methods, f decreases monotonically with decreasing l . However, the DesTree results clearly domi-nate the ones from CART. For each l , DesTree achieves a significantly higher f , and each f a significantly smaller l . Again, this observation holds consistently for all other experiments not presented, including ones on other UCI datasets and synthetic datasets, as well as C  X  -description tree experiments on both data types. While the focus of our study is on the MDA problem, BP addresses the MDL problem only. In this series of ex-periments, we compared Learn2Cover with BP on synthetic datasets as BP works on grid data. Our data generator fol-lows exactly what [16] does for BP. It takes as input the dimensionality, the number of intervals of each dimension, and the density. Dense cells are randomly generated in a grid with specified density, then one of them is randomly selected to grow a connected dense cell set as cluster C ,the rest of the dense cells in B C constitute C  X  .

In our experiments with BP, we did not limit the number of  X  X on X  X  care X  cells for use and allowed BP to find the best possible results. Since BP only generates one set of rectan-gles, we used from Learn2Cover for the comparison. We studied the averaged percentage length reduction compared to BP for varying dataset size and dimensionality. We ob-served that in all cases Learn2Cover clearly outperformed BP, gaining 20% to 50% length reduction. As a general ten-dency, the reduction increased with increasing complexity of data. FindClans further improved the results, gaining an additional 25% length reduction on average.
 BP starts with the maximal rectangles generated by Greedy Growth in a greedy manner. The  X  X on X  X  care X  cells may come too late to be helpful, as illustrated in Figure 8.
Runtime was not a major concern of this study. We did not integrate the X -tree index, on which BP relies to reduce the violation checking time. Without indexing for both, we observed that Learn2Cover ran faster than BP by one to two orders of magnitude. Recall that Learn2Cover can signifi-cantly reduce the number of points in consideration for vi-olation checking. If we assume constant number of rectan-gles, Learn2Cover has a worst case runtime of O ( | B C | 2 )
DesTree and FindClans also have quadratic worst case runtime in the number of input rectangles, O ( | | 2 ) or O ( |  X  | 2 ) for DesTree and O ( | T | 2 ) for FindClans respec-tively. In particular, for DesTree, the accuracy calculation results of all possible pairwise merges of rectangles can be reused in each iteration, recalculation is needed only for the resulting parent rectangle, which takes linear time. For FindClans, in each call of procedure findAClan(), the results from  X  X ind theN  X  (line 1) can be reused.
In this paper, we systematically studied rectangle-based discriminative data generalization in the context of cluster description, which transforms clusters into patterns and pro-vides the possibility of obtaining human-comprehensible knowledge from clusters. In particular, we introduced and analyzed novel description formats, SOR  X  and kSOR  X  , providing enhanced expressive power; we defined the novel Maximum Description Accuracy (MDA) problem, allowing users to specify different trade-offs between interpretabil-ity and accuracy; we also presented heuristic algorithms to-gether with their experimental evaluations.

The concept of cluster in our study, in the narrow sense, is the output from clustering algorithms. Our study is mo-tivated from and can find most applications in describing such clusters. In the broad sense, a cluster can be used to represent an arbitrary class of labeled data that require dis-criminative generalization.

Last but not least, cluster descriptions are patterns which can be stored as tuples in a relational table, so that a cluster-ing and its associated clusters become queriable data min-ing objects. Therefore, this research can serve as a first step for integrating clustering into the framework of induc-tive databases [13], a paradigm for query-based  X  X econd-generation X  database mining systems.

