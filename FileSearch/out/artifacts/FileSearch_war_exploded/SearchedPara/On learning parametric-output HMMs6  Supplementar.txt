 We now give a detailed account for the theorems stated in section 4. 6.1. Preliminaries I In what follows we use the following notation: For an its Frobenius matrix norm is k A k F = k vec( A ) k 2 . Recall the definition of g  X  :  X g is defined by: Weiss (2012, Theorem 1) is our main tool in proving the error bounds given here.
 Lemma 1. Let Y = Y 0 ,...,Y T  X  1  X  Y T be the out-ometrically ergodic with constants G, X  as in (1). Let Lipschitz with respect to the Hamming metric on Y T . Then, for all &gt; 0 , P ( | F ( Y )  X  E F | &gt; T )  X  2 exp  X  bounding the variance of our estimators.
 Lemma 2. Let f ( y ) : R  X  R + be a function of the observables of an n states geometrically ergodic HMM with constants ( G, X  ) and Assume the HMM is started with the stationary distri-bution  X  . Then
Var consecutive observations ( y,y 0 ) such that Then
Var 6.2. Accuracy of  X   X  ,  X   X  ,  X   X  and  X   X  tinuous case, let us first examine the accuracy of the later. The following results shows that geometric er-to the true values.
 be given by (3) and  X  by (4) with their empirical esti-mates given in (5). Then Furthermore, for any &gt; 0 , and
P k  X   X   X   X  k 2 &gt; Proof. First note that w.r.t the Hamming metric, spectively. In order to prove (32) note that E [ k  X   X   X   X  k 2 2 ] = X Since P m k =1  X  k = 1 we get the desired bound. P served sequence from a continuous observations HMM respectively. Then for any &gt; 0 , and all k  X  [ n ]. Thus by Lemma 1 and the union bound we have Since we have
P  X   X   X   X  putting 0 = / 1-Lipschitz so by Lemma 1 and the union bound we have Since k  X   X   X   X  k 2 2 = X we have putting 0 = /n in (40), the claim in (38) follows. 6.3. Proof of theorem 1 -Strong consistency We now prove the strong consistency of our estimators stated in Theorem 1.
 Proof. For the discrete case, by Lemma 3, the expecta-respectively.
 Now, the function f : R m  X  R n given by f ( x ) = given by x  X  =  X  when  X   X  =  X  . Since  X   X  R m + by assumption, the argument above shows that almost asymptotic strong consistency of  X   X  is established. program x | Kx  X  h | x subject to Gx  X  g , Dx = d , is continuous under small perturbations of K,h,G,D,d  X   X   X   X  almost surely, we also have  X  A a.s.  X  X  X  A . For the continuous observations case, note that  X   X  and  X  A are also solutions of quadratic programs. Also note that  X   X   X   X  and  X   X   X   X  almost surely. Thus we have that  X  A a.s.  X  X  X  A and  X   X  a.s.  X  X  X   X  as above. 6.4. Proof of Theorem 2: Bounding the error implies that k  X   X   X   X  k  X  = O P (1 / a change of variables, without the normalization P x i = 1, and write it as x between  X  and  X  .
 Observe that  X  satisfies the system of linear equations We need T sufficiently large so that, with high prob-ability, max k 1  X   X  .
 By taking T &amp; 4 g  X  /a 2 1 we have 1 / (1 + ) = 1  X  + O ( 2 ) gives Note that since B  X  =  X  , the leading order correction for  X  is simply where the matrix  X  B = diag(1 /  X  1  X   X  2 ...  X   X  n ; thus,  X  B also has n non-zero singular values follows from its definition combined with our Assumption 2d that B has rank n . Then and hence, out loss of generality, assume that  X  1 = min j  X  j and analyze the worst-case setting. This occurs when the cides with the standard basis vector e 1 . Then, |  X  1 | X  min  X  j  X  a 0 provided that In the unlikely event that (i) the vector  X  is uniform (  X  j = 1 /n for all j ), (ii) the matrix cal singular values, we need the equation analogous to (44) to hold for all n coordinates. By a union bound argument, an additional factor of log n in the number non-negativity of the solution x .
 write x  X   X   X  =  X  = X Since both the { u i } and the { v i } are orthonormal, Bounding k  X   X   X   X  k 2 2 via Lemma 3 and noting that the result in (22) follows. 6.5. Preliminaries II The remaining estimators ( X   X  for the continuous obser-vations case, and  X  A for both the discrete and contin-uous observations cases) are obtained as solutions for quadratic programs. Let us take for example the QP for calculating  X   X  with continuous observations HMM, given in (23). For this case, the QP is equivalent to subject to x  X  0 and P i x i = 1.
 tion of the above QP would simply be the true  X  . In the solutions of such a quadratic program are affected by errors in  X  .
 More generally, we are concerned with two QPs both subject to Gx  X  g , Dx = d . We assume that the solution to the first QP is the  X  X rue X  value while the the estimate error is equivalent to bounding the error between the solutions obtained by the above two QPs, where  X  M and  X  h are perturbed versions of M and h . Given that, note that only the objective function has been perturbed, while the linear constraints remained unaffected. We may thus apply the following classical programs.
 Theorem 7. (Daniel, 1973) Let  X  =  X  min ( M ) be the smallest eigenvalue of M , and let = max {k  X  M  X  Eqs.(46) and (47), respectively. Then, for &lt;  X  , In the following we will obtain bounds on and  X  for the different estimators and invoke the above theorem. 6.6. Proof of Theorem 3: Bounding the error Proof. Note that in the notation given in Theorem 7, we have h =  X  | K and  X  h =  X   X  | K . Since we assumed that the output density parameters are known exactly we have no error in M = K | K .
 It is immediate that and From Lemma 4 we have while by Theorem 7 we have Since k  X  k 2  X  1, the claim follows.
 As a side remark we note that the form of (24) is some-what counter-intuitive, as it suggests a worse behavior to a more peaked  X  and hence lower-variance  X  den-however that as numerical simulations suggest we typ-ically have timate in (24) and the bound is reasonable after all. much like the matrix B in the discrete outputs case. 6.7. Proof of Theorem 4: Bounding the error Let  X  A be the solution of were known exactly, the above QP could be written as min Q ( A ) = min where M = C | C and h = C | vec(  X  ). Its solution mization problem is perturbed to min  X  Q ( A ) = min where  X  M =  X  C |  X  C , and  X  h =  X  C | vec(  X   X  ). tion problem we follow the same route as above. Thus eigenvalue of M . Regarding the latter, by definition,  X  gular value of C . A simple exercise in linear algebra yields The following lemma provides bounds on k  X  M  X  M k 2 and on k  X  h  X  h k 2 .
 Lemma 5. Asymptotically, as T  X  X  X  , and P Since each of k  X   X   X   X  k  X  and k  X   X   X   X  k  X  are O
P (1 / ically negligible as compared to each of the first two P  X  h  X  h  X  ), which are asymptotically negligible, (  X 
M  X  M ) ij, X  X  = ( X   X  j  X   X  j )  X   X  X arguments as for h , (52) follows.
 We can now prove Theorem 4: Proof. ( of Theorem 4 ) Lemma 3, together with (22), implies that with high probability, and Inserting these into (51) and (52) yields, w.h.p., By Theorem 7, we have that where k A k F  X  claim follows by substituting the bounds on in (53) and noting that  X  2 1 (  X  B )  X   X  2 1 ( B ). 6.8. Proof of Theorem 5: Bounding the error Let  X  A be the solution of  X  C min  X  Q ( A ) = min where  X  M =  X  C |  X  C , and  X  h =  X  C | vec(  X   X  ). above QP and the unperturbed one.
 First note that Next we give the analogue of lemma 5.
 Lemma 6. Asymptotically, as T  X  X  X  , k  X  h  X  h k 2 . P and turbed due to errors in  X   X  with order in  X   X   X   X  we find that where in the last inequality we used the fact that F for Lemma 5 and noting that a 0 1 we get (57) and (58).
 We now come to the proof of Theorem 5.
 Proof. ( of Theorem 5 ) Lemma 4, together with (24), implies that with high probability, and Inserting these into (57) and (58) yields, w.h.p., By Theorem 7, we have that where k A k F  X  claim follows by substituting the bounds on in (59) and noting that  X  2 1 (  X  F )  X   X  2 1 ( F ). with the help of the matrix K (instead of  X  with Lipschitz any more but O ( L 2 /T )-Lipschitz with L = gration may be computational intensive, choosing be-between working with limited number of samples and computational efficiency. 6.9. Proof of Theorem 6: Perturbations in the We give here the proof for the perturbation in the ma-trix F . The proof for perturbations in the matrix K is similar.
 P As the two first terms already considered we focus on the last term. It can be shown that: Thus  X  b  X  b {k  X   X   X   X  k  X  , F } we have (  X 
K  X  K ) ij, X  X  = ( X   X  j  X   X  j )  X   X  X Again considering only the terms including P P  X 
K  X  K Repeating the analysis in the proofs for Theorems 3,
