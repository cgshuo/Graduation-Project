 given by: distribution and the projected distribution,  X  p G , defined by D (  X  p ||  X  p G ). D (  X  p ||  X  p G ) = X P Primal optimization problem. P (  X , X  ) denotes the primal cost function. The constraints of the combinatorial optimization problem are given by: The combinatorial optimization problem is given by The  X  -relaxed primal optimization problem is Proposition 1 The non-convex primal and the  X  -relaxed primal are equivalent. primal are identical.
 The convex relaxation of the primal optimization problem is Dual Derivation. The dual variables are defined by :  X  Set cover constraints in Eq. (4):  X   X  R V + .  X  Running intersection property in Eq. (7):  X   X  R V .  X  Edge constraints in Eq. (8):  X   X  R E +  X  R E + .  X  Clique constraints in Eq. (9):  X   X  R D + .
 Therefore, the dual space is represented by (  X , X , X , X  ).
 with the following dual constraints on the Lagrange multipliers Eq. (10) and Eq. (11) respectively in deriving the dual as follows: It is decomposed in three parts defined in Eq. (19), Eq. (20) and Eq. (21) respectively : where q (  X , X , X , X  ) = inf q (  X , X , X , X  ) =  X  sup q (  X , X , X , X  ) = X dual constraints defined in Eq. (16) is given by Proposition 2 If k = 1 , the convex relaxation in Eq. (14) is equivalent to Eq. (12). the number of elements in the feasible edges is only 1, i.e.,  X  ( C,D )  X  X  , | C  X  D | = 1. On solving the dual variables, the optimal dual solution is given by where H ( { i } ) =  X   X  p i ( x i ) log(  X  p i ( x i )).
 The optimal solution to the dual problem is given by Approximate Greedy Primal Solution. We describe an algorithm to project from the average  X  X etNumberConnectedComponents X  gives the number of connected components in the graph using Algorithm 1 Approximate Greedy Primal Solution of Vertices n , set of cliques D and integer m such that 0 &lt; m  X  T Output: approximate discrete primal feasible solution  X  m after m iterations of Algorithm 1 order = Sorted indices in the descending order  X   X  m repeat until decomposable = true , treewidth = k , numConnectedComponents = 1, i = length( order ) Generating Decomposable Covariance Matrices. In order to generate a covariance matrix A random positive definite covariance matrix,  X  0 is generated as follows: with higher values of d .
 onto a decomposable graph G as follows: decomposable graph, G .
 and the projected covariance matrix  X : where A is the adjacency matrix of the decomposable graph G onto which  X  0 was projected. that is factored in G  X  X  : X  X  V . Therefore, for any multivariate decomposable Gaussian graphical model, G : we consider only the covariance matrix. [2] M. C. Golumbic. Algorithmic Graph Theory and Perfect Graphs . North Holland, 2004. [3] S. L. Lauritzen. Graphical Models . Oxford University Press, Oxford, 1996. [4] A. Schrijver. Combinatorial optimization: Polyhedra and efficiency . Springer, 2004.
 K. S. Sesh Kumar sesh-kumar.karri@inria.fr Francis Bach francis.bach@inria.fr Graphical models provide a versatile set of tools for probabilistic modeling of large collections of inter-dependent variables. They are defined by graphs that encode the conditional independences among the random variables, together with potential func-tions or conditional probability distributions that encode the specific local interactions leading to globally well-defined probability distributions (see, e.g., Bishop et al. , 2006 ; Wainwright &amp; Jordan , 2008 ; Koller &amp; Friedman , 2009 ).
 In many domains such as computer vision, natu-ral language processing or bioinformatics, the struc-ture of the graph follows naturally from the con-straints of the problem at hand. In other situa-tions, it might be desirable to estimate this struc-ture from a set of observations. It allows (a) a sta-tistical fit of rich probability distributions that can be considered for further use, and (b) discovery of structural relationships between different variables. In the former case, distributions with tractable in-ference are often desirable, i.e., inference with run-time complexity that does not scale exponentially in the number of variables in the model. The sim-plest constraint to ensure tractability is to impose tree-structured graphs ( Chow &amp; Liu , 1968 ). However, these distributions are not rich enough, and following earlier work ( Malvestuto , 1991 ; Bach &amp; Jordan , 2002 ; Narasimhan &amp; Bilmes , 2004 ; Chechetka &amp; Guestrin , consider models with bounded treewidth , not simply by one (i.e., trees), but by a small constant k . Beyond the possibility of fitting tractable distribu-tions (for which probabilistic inference has linear complexity in the number of variables), learn-ing bounded-treewidth graphical models is key to designing approximate inference algorithms for graphs with higher treewidth. Indeed, as shown by Saul &amp; Jordan ( 1995 ); Wainwright &amp; Jordan ( 2008 ); Kolmogorov &amp; Schoenemann ( 2012 ), approxi-mating general distributions by tractable distributions is a common tool in variational inference. However, in practice, the complexity of variational distributions is often limited to trees (i.e., k = 1), since these are the only ones with exact polynomial-time structure learning algorithms. The convex relaxation designed in this paper enables us to augment the applicability of variational inference, by allowing a finer trade-off between run-time complexity and approximation quality.
 Apart from trees, learning the structure of a di-rected or undirected graphical model, with or without constraints on the treewidth, remains a hard problem. Two types of algorithms have emerged, based on the two equivalent definitions of graphical models: (a) by testing conditional independence relationships (see, e.g., Spirtes et al. , 2001 ) or (b) by maximizing the log-likelihood of the data using the factorized form of the distribution (see, e.g., Friedman &amp; Koller , 2003 ). In the specific context of learning bounded-treewidth graphical models, the lat-ter approach has been shown to be NP-hard ( Srebro , 2002 ) and led to various approximate algorithms based on local search techniques ( Malvestuto , 1991 ; Deshpande et al. , 2001 ; Karger &amp; Srebro , 2001 ; Bach &amp; Jordan , 2002 ; Shahaf et al. , 2009 ; Gogate et al. , 2010 ; Sz  X antai &amp; Kov  X acs , 2011 ), while the former approach led to algorithms based on independence tests ( Narasimhan &amp; Bilmes , 2004 ; Chechetka &amp; Guestrin , 2007 ), which have recovery guarantees when the data-generating distribution has low treewidth.
 In this paper, we make the following contributions:  X  We provide a novel convex relaxation for learning bounded-treewidth decomposable graphical models from data in polynomial time. This is achieved by posing the problem as a combinatorial optimization problem in Section 2 , which is relaxed to a convex optimization problem that involves the graphic and hypergraphic matroids, as shown in Section 3 .  X  We show in Section 4 how a supergradient ascent method may be used to solve the dual optimization problem, using greedy algorithms as inner loops on the two matroids. Each iteration has a run-time complexity of O ( k 3 n k +2 log n ), where n is the num-ber of variables. We also show how to round the obtained fractional solution.  X  We compare our approach to state-of-the-art meth-ods on both synthetic datasets and classical bench-marks in Section 5 , showing the gains of the novel convex approach. In this section, we first review the relevant concepts of decomposable graphs and junction trees; for more details, see Bishop et al. ( 2006 ); Wainwright &amp; Jordan ( 2008 ); Koller &amp; Friedman ( 2009 ). We then cast the problem of learning the maximum likelihood bounded treewidth graph as a combinatorial optimization prob-lem. 2.1. Decomposable graphs and junction trees We assume we are given an undirected graph G defined on the set of vertices V = { 1 , 2 , . . . , n } . Let C ( G ) denote the set of maximal cliques of G (which we will refer to as cliques). We consider n random variables X 1 , . . . , X n (referred to as X ), associated with each vertex indexed by V . For simplicity, they are assumed to be discrete, but this is not a restriction as maximum likelihood will use only entropies that can be extended to differentiable entropies ( Cover &amp; Thomas , 2006 ). The distribution p ( x ) of X is said to factorize in the graph G , if and only if it factorizes as a product of potentials that depend only on the variables within maximal cliques.
 A graph is said to be decomposable if it has a junction tree, i.e., a spanning tree whose vertices are maximal cliques of G (i.e., C ( G ) is the vertex set) such that:  X  the junction tree connects only cliques that have a common element ( clique tree property),  X  for any vertex i  X  V , the subgraph of cliques con-taining i is a tree ( running intersection property). Let T ( G ) denote the edges of the junction tree over the set of cliques C ( G ). When the graph G is decomposable, the distribution p ( x ) of X fac-torizes in G if and only if it may be written as where p C ( x C ) denotes the marginal distribution of random variables belonging to C  X  C ( G ) and p
C  X  D ( x C  X  D ) denotes the marginal distribution of ran-dom variables belonging to the separator set C  X  D , with ( C, D )  X  T ( G ). See Figure 1 . The treewidth of G is the maximal size of the cliques in G , minus one. An alternative representation of decomposable graphs may be obtained by considering hypergraphs . Hyper-graphs are defined by a base set V and a set of hyper-edges, i.e., subsets of V . A hypergraph is said to be acyclic if and only if the resulting graph obtained by connecting all elements within an hyperedge is decom-posable. Unfortunately, the nice properties of acyclic graphs do not transfer to acyclic hypergraphs. Partic-ularly, the matroid property, which allows exact greedy algorithms, does not hold. In Section 3.2 , we will use a lesser known more general notion of acyclicity that will lead to exact greedy algorithms. 2.2. Maximum likelihood estimation Given N observations x 1 , . . . , x N of X , we denote the corresponding empirical distribution of X by  X  p ( x ) =
P N i =1  X  ( x = x i ). Given the structure of a decom-posable graph G , the maximum likelihood distribution that factorizes in G may be obtained by combining the marginal empirical distributions on all maximal cliques and their separators. The goal of structure learning is to maximize the log-likelihood with respect to the graph as well. As shown by Malvestuto ( 1991 ) and Srebro ( 2002 ), this corresponds to minimizing the fol-lowing combination of entropies with respect to the graph (see a proof in the supplementary material): where H ( S ) is the empirical entropy of the random variables indexed by the set S  X  V , defined by H ( S ) = P x sum is taken over all possible values of x S . Note that in this paper, we will not be us-ing a traditional model selection term (see, e.g., Friedman &amp; Koller , 2003 ) as we will only consider models of low tree-width (thus with a bounded number of parameters). 2.3. Combinatorial optimization problem We now consider the problem of learning a decompos-able graph of treewidth less than k . We assume that we are given all entropies H ( S ) for subsets S of V of cardinality less than k + 1.
 Since we do not add any model selection term, without the search space to the space of maximal junction trees , i.e., junction trees with n  X  k maximal cliques of size k + 1 and n  X  k  X  1 separator sets of size k between two cliques of size k +1. Our natural search spaces are thus characterized by D , the set of all subsets of size k + 1 of V , of cardinality n k +1 , and E , the set of all potential edges in a junction tree, i.e., E = { ( C, D )  X  D  X  D , C  X  D 6 =  X  , | C  X  D | = k } . The cardinality of E is n k +2 . k +2 2 (number of subsets of size k + 2 times the number of possibility of excluding two elements to obtain a separator).
 A decomposable graph will be represented by a clique selection function  X  : D  X  { 0 , 1 } and an edge selection function  X  : E  X  { 0 , 1 } so that  X  ( C ) = 1 if C is a maximal clique of the graph and  X  ( C, D ) = 1 if ( C, D ) is an edge in the junction tree. Both  X  and  X  will be referred to as incidence functions or incidence vectors , when seen as elements of { 0 , 1 } D and { 0 , 1 } E . Thus, minimizing the problem defined in Eq. ( 1 ) is equivalent to minimizing, P (  X ,  X  )= X with the constraint that (  X ,  X  ) forms a decomposable graph.
 At this time, we have merely reparameterized the problem with the clique and edge selection functions. We now consider a set of necessary and sufficient con-ditions for the pair to form a decomposable graph. Some are convex in (  X ,  X  ), while some are not. The latter ones will be relaxed in Section 3 . From now on, we denote by 1 i  X  C the indicator function for i  X  C (i.e., it is equal to 1 if i  X  C and zero otherwise).  X  Covering V : all vertices, must be covered,  X  Number of edges : exactly n  X  k  X  1 edges are selected,  X  Number of cliques : exactly n  X  k cliques are selected,  X  Running intersection property : the subtree of cliques containing any vertex i  X  V must be span-ning, i.e., the number of edges has to be equal to the number of nodes minus one, i.e.,  X  i  X  V ,  X  Edges between selected cliques : we must have the following link between  X  and  X  ,  X  Acyclicity of  X  :  X  has to be the incidence vector of a forest, i.e.,  X  Acyclicity of  X  :  X  has to be the incidence vector of an acyclic hypergraph, i.e., The above constraints encode the classical definition of junction trees. Thus our combinatorial problem is exactly equivalent to minimizing P (  X ,  X  ) defined in Eq. ( 2 ), subject to the constraints in in Eq. ( 3 ), Eq. ( 4 ), the constraint in Eq. ( 9 ) that  X  represents an acyclic hypergraph is implied by the other constraints. Figure 2 shows clique and edge selections in blue which satisfy all these constraints and hence represent a de-composable graph. The clique and edge selections in red violate at least one of these constraints. We now provide a convex relaxation of the combina-torial problem defined in Section 2.3 . The covering constraint in Eq. ( 3 ), the number of edges and the number of cliques constraints in Eq. ( 4 ) and Eq. ( 5 ) respectively, and the running intersection property in Eq. ( 6 ) are already convex in (  X ,  X  ).
 The non-convex constraint in Eq. ( 7 ) that  X  C  X  D ,  X  ( C ) = max D  X  X  , ( C,D )  X  X   X  ( C, D ) may be relaxed into:  X  Edge constraint : selection of edges only if the both the incident cliques are selected, i.e.,  X  Clique constraint : selection of a clique if at least an edge incident on it is selected, i.e., We now consider the two acyclicity constraints in Eq. ( 8 ) and Eq. ( 9 ). 3.1. Forest polytope Given the graph ( D , E ), the forest polytope is the convex hull of all incidence vectors  X  of subforests of ( D , E ). Thus, it is exactly the convex hull of all  X  : E  X  { 0 , 1 } such that  X  satisfies the constraint in Eq. ( 8 ). We may thus relax it into:  X  Tree constraint : While the new constraint in Eq. ( 12 ) forms a convex constraint, it is crucial that it may be dealt with em-pirically in polynomial time. This is made possible by the fact that one may maximize any linear function over that polytope. Indeed, for a weight function w : E  X  E  X  R , maximizing P ( C,D )  X  X  w ( C, D )  X  ( C, D ) is exactly a maximum weight forest problem, and its so-lution may be obtained by Kruskal X  X  algorithm, i.e., (a) order all (potentially negative) weights w ( C, D ) and (b) greedily select edges ( C, D ), i.e., set  X  ( C, D ) = 1, with higher weights first, as long as they form a forest and as long as the weights are positive. When we add the restriction that the number of edges is fixed (in our case n  X  k  X  1), then the algorithm is stopped when ex-actly the desired number of edges is selected (whether the corresponding weights are positive or not). See, e.g., Schrijver ( 2004 ).
 The polytope defined above may also be defined as the independence polytope of the graphic matroid, which is the traditional reason why the greedy algorithm is exact (see, e.g., Schrijver , 2004 ). In the next section, we show how this can be extended to hypergraphs. 3.2. Hypergraphic matroid Given the set of potential cliques D over V , we con-sider functions  X  : D  X  { 0 , 1 } that are equal to one when a clique is selected, and zero otherwise. Ideally, we would like to treat the acyclicity of the associated hypergraph in a similar way than for a regular graph. However, the set of acyclic subgraphs of the hyper-graph defined from D does not form a matroid, and thus the polytope defined as the convex hull of all inci-dence vectors/functions of acyclic hypergraphs may be defined, but the greedy algorithm is not applicable. In order to define what is referred to as the hypergraphic matroid , one needs to relax the notion of acyclicity. We now follow Lorea ( 1975 ); Frank et al. ( 2003 ); Fukunaga ( 2010 ) and define a different notion of acyclicity for hypergraphs. An hypergraph ( V, F ) is an hyperforest if and only if for all A  X  V , the number of hyperedges in F contained in A is less than | A |  X  1. A non-trivially equivalent definition is that we can se-lect two elements in each hyperedege so that the graph with vertex set V and with edge set composed of these pairs is a forest.
 Given an hypergraph with hyperedge set D , the set of sub-hypergraphs which are hyperforests forms a ma-troid. This implies that given a weight function on D , one may find the maximum weight hyperforest with a greedy algorithm that ranks all hyperedges and select them as long as they do not violate acyclicity (with the notion of acyclicity just defined and for which we exhibit a test below).
 Testing acyclicity. Checking acyclicity of an hyper-graph ( V, F ) (which is needed for the greedy algorithm above) may be done by minimizing with respect to A  X  V The hypergraph is an hyperforest if and only if the minimum is greater or equal to one. The minimiza-tion of this function may be cast a min-cut/max-flow problem as follows ( Fukunaga , 2010 ):  X  single source, single sink, one node per hyperedge in F , one node per vertex in V ,  X  the source points towards each hyperedge with unit capacity,  X  each hyperedge points towards the vertices it con-tains, with infinite capacity,  X  each vertex points towards the sink, with unit ca-pacity.
 Link with decomposability. The hypergraph ob-tained from the maximal cliques of a decomposable graph can easily be seen to be an hyperforest. But the converse is not true.
 Hyperforest polytope. We can now naturally de-fine the hyperforest polytope as the convex hull of all incidence vectors of hyperforests. Thus the constraint in Eq. ( 9 ) may be relaxed into:  X  Hyperforest constraint : 3.3. Relaxed optimization problem We can now formulate our combinatorial problem as follows: All constraints except the integrality constraints are convex. Let  X  -relaxed primal be the partially relaxed primal optimization problem formed by relaxing only the integral constraint on  X  in Eq. ( 14 ), i.e., replacing  X   X  { 0 , 1 } D by  X   X  [0 , 1] D . Note that this is not a con-vex problem due to the remaining integral constraint on  X  , but it remains equivalent to the original problem as the following proposition shows (see proof in the supplementary material): Proposition 1 The combinatorial problem in Eq. ( 14 ) and the  X  -relaxed primal problem are equivalent.
 The convex relaxation for the primal optimization problem formed by relaxing the integral constraint on both  X  and  X  can now be defined as It is not tight anymore in general, i.e., the minimum value of the convex problem may be smaller than the minimal value of the non-convex problem. However, when k = 1 (i.e., trees), our relaxation is tight and we recover the results of the Chow-Liu algorithm (see proof in supplementary material): Proposition 2 If k = 1 , the convex relaxation in Eq. ( 15 ) is equivalent to Eq. ( 14 ). We now show how the convex problem may be mini-mized in polynomial time. Among the constraints of our convex problem in Eq. ( 14 ), some are simple lin-ear constraints, some are complex constraints depend-ing on the forest and hyperforest polytopes defined in Section 3 . We will define a dual optimization problem by introducing the least possible number of Lagrange multipliers (i.e., dual variables) (see, e.g., Bertsekas , 1999 ) so that the dual function (and a supergradient) may be computed and maximized efficiently. We in-troduce the following dual variables:  X  Set cover constraints in Eq. ( 3 ):  X   X  R V + .
H ( C )  X  X  X  Running intersection property in Eq. ( 6 ):  X   X  R V .  X  Edge constraints in Eq. ( 10 ):  X   X  R E +  X  R E + .  X  Clique constraints in Eq. ( 11 ):  X   X  R D + . Therefore, the dual variables are (  X ,  X ,  X ,  X  ). We may then compute the dual function Q (  X ,  X ,  X ,  X  ) by intro-ducing the Lagrange multipliers defined above. As shown in the supplementary material, it is decom-posed in three parts defined in Eq. ( 16 ), Eq. ( 17 ) and Eq. ( 18 ) respectively (see Figure 3 ):
Q (  X ,  X ,  X ,  X  ) = q 1 (  X ,  X ,  X ,  X  )+ q 2 (  X ,  X ,  X ,  X  )+ q The dual functions q 1 (  X ,  X ,  X ,  X  ) and q 2 (  X ,  X ,  X ,  X  ) may be computed using the greedy algorithms defined in Section 3.1 and Section 3.2 ; q 1 can be evaluated in O ( r log( r )), where r is the cardinality of the space of cliques, D , i.e., n k +1 and q 2 can be evaluated in O ( m log( m )), where m is the cardinality of fea-sible edges, E , i.e., n k +2 . k +2 2 . This complexity is due to sorting the edges and hyperedges based on their weights (the costliest operation in each iteration). This leads to an overall complexity of O ( k 3 n k +2 log n ) per iteration of the projected supergradient method which we now present.
 Projected supergradient ascent. The dual opti-mization problem defined by maximizing Q (  X ,  X ,  X ,  X  ) can be solved using the projected supergradient method. In each iteration t of the algorithm, the dual timation of q 1 and q 2 by solving Eq. ( 16 ) and Eq. ( 17 ) respectively. In the process of solving these equations, timated and allows the computations of the supergra-dients of Q (i.e., opposites of subgradients of  X  Q ) (see, e.g., Bertsekas , 1999 ). As shown in Algorithm 1 , a step is made toward the direction of the supergradient and projection onto the positive orthant is performed for dual variables that are constrained to be nonneg-ative. With step sizes  X  t proportional to 1 / algorithm is known to converge to a dual optimal so-lution ( Nedi  X c &amp; Ozdaglar , 2009 ) at rate 1 / over, the average of all visited primal variables, i.e., be approximately primal-feasible (i.e., it satisfies all the linear constraints that were dualized up to a small constant that is also going to zero at rate 1 / convergence to primal feasibility is illustrated in Fig-ure 6 (a), where, on one of the synthetic examples from Section 5 , the different constraint violations. Note that these are not the number of each of these constraints violated but the maximum value by which they are violated. It can be observed that the constraint viola-tions reduce to zero over iterations.
 Approximate greedy primal solution. Given an (approximate) primal-feasible but fractional solution ( X   X  ,  X   X  t ) of our convex optimization problem, we pro-pose to find an integral candidate by discarding  X   X  t , and (a) ordering the components of  X   X  t in decreasing order and (b) selecting the cliques C greedily with higher values of  X   X  t ( C ) first, while maintaining decomposabil-ity of the resulting graph. The time complexity of the rounding algorithm is O ( n k +2 ). This is due to de-composability test with run time complexity O ( n k +1 ), that is performed while adding O ( n ) cliques. See more details in the supplementary material. In this section, we show the performance of the pro-posed algorithm on synthetic datasets and classical benchmarks.
 Decomposable covariance matrices. In order to easily generate controllable distributions with en-tropies which are easy to compute, we use several de-composable graphs and we consider a Gaussian vec-Algorithm 1 Projected Supergradient
Initialize  X  0 = 0,  X  0 = 0,  X  0 = 0,  X  0 = 0 for t = 0 to T do 1 end for tor with covariance matrix  X , generated as follows: (a) sample a matrix Z of dimensions n  X  d  X  with en-tries uniform in [0 , 1] and consider the matrix  X   X  =
ZZ  X  + (1  X  d d  X  ) I , (b) normalize  X   X  to unit diagonal, and (c) find the projection of  X   X  onto the graph G , i.e., find the covariance matrix that factorizes in G gence. The last operation may be performed in closed form ( Lauritzen , 1996 ), while the first operation leads to strong correlations when d is large, and small cor-relations when d is small.
 We use the graph structures representing a chain junc-tion tree as in Figure 4 and a star junction tree as in Figure 5 to analyze the performance of our algorithm for decomposable covariance matrices generated with different correlations.
 Table 1 and Table 2 show the performance of our algo-rithm on these two graphs. Decomposable covariance matrices are generated as above with different values of the correlation parameter d (all averaged over ten different random covariance matrices). We show the difference between the cost function in Eq. ( 2 ) and the optimal entropy, i.e., the one of the actual structure represented by the covariance matrices. The differ-ences in the table are multiplied by 10 3 for brevity. The first column  X  X ual represents the optimal value of our convex relaxation (obtained from the dual func-optimal value by replacing the hyperforest constraint by the simply  X   X  [0 , 1] D . We can see from the two tables, that the two values are strictly negative (i.e., we indeed have a relaxation) and that the hyperforest constraint is key to obtaining tighter relaxations. Note that the associated solutions are only fractional. The third column  X  X rimal represents the cost function after the rounding step; it is compared in the fourth column with a simple greedy algorithm that sorts all mutual information and keep adding the cliques with largest mutual information as long as decomposability is maintained. Although the relaxation is not tight, our rounding scheme leads empirically to the optimal solution when the correlations are strong enough (i.e., large values of d ) and outperforms the simple greedy algorithm.
 16 -1.9  X  0.2 -3.4  X  2.7 0 25.6  X  1.2 32 -2.9  X  0.5 -3.2  X  0.3 0 57.3  X  1.5 16 -1.2  X  0.5 -3.1  X  0.3 0 26.3  X  1.5 32 -6.8  X  0.4 -8.5  X  1.2 0 58.3  X  1.9 Performance Comparison. We compare the qual-ity of the graph structures learned by the proposed algorithm with the ones produced by Ordering Based Search (OBS) ( Teyssier &amp; Koller , 2005 ), the combina-torial optimization algorithm proposed by Karger and Srebro (Karger+Srebro) ( Karger &amp; Srebro , 2001 ), the Chow-Liu trees (Chow-Liu) ( Chow &amp; Liu , 1968 ) and different variations of PAC-learning based algorithms (PAC-JT, PAC-JT+local) ( Chechetka &amp; Guestrin , 2007 ). We use a real-world dataset, TRAF-FIC ( Krause &amp; Guestrin , 2005 ) and an artificial dataset, ALARM ( Beinlich et al. , 1989 ) to compare the performances of these algorithms.
 This ALARM dataset was sampled from a known Bayesian network ( Beinlich et al. , 1989 ) of 37 nodes, which has a treewidth equal to 4. We learn an ap-proximate decomposable graph of treewidth 3. The TRAFFIC dataset is the traffic flow information ev-ery 5 minutes for a month at 8000 locations in Cal-ifornia ( Krause &amp; Guestrin , 2005 ). The traffic flow information is collected at 32 locations in San Fran-cisco Bay area and the values are discretized into 4 bins. We learn an approximate decomposable graph of treewidth 3 using our approach. Empirical entropies are computed from the generated samples of each data set and we infer the underlying structure from them using our algorithm. Figure 6 (b) and Figure 6 (c) show the log-likelihoods of structures learnt using various al-gorithms on Traffic and Alarm datasets respectively. These figures illustrate the gains of the convex ap-proach over the earlier non-convex approaches. In this paper, we have provided a convex relaxation for learning the maximum likelihood decomposable graph with bounded treewidth, in polynomial-time, which empirically outperforms previously proposed al-gorithms. We are currently exploring two avenues for improvements: (a) design sufficient conditions for tightness of our relaxation, following the recent litera-ture on relaxation of variable selection problems (see, e.g., Cand`es &amp; Tao , 2005 ), and (b) use heuristics to speed-up the algorithms to allow application to larger graphs.
 Bach, F. R. and Jordan, M. I. Thin junction trees. In Adv. NIPS , 2002.
 Beinlich, I. A., Suermondt, H. J., Chavez, R. M., and Cooper, G. F. The ALARM Monitoring System: A
Case Study with Two Probabilistic Inference Tech-niques for Belief Networks. In Proc. of Euro. Conf. on AI in Medicine , 1989.
 Bertsekas, D. P. Nonlinear Programming . Athena Sci-entific, 1999.
 Bishop, C. M. et al. Pattern recognition and machine learning . Springer New York, 2006.
 Cand`es, E. J. and Tao, T. Decoding by linear program-ming. IEEE Trans. Inf. Theory , 51(12):4203 X 4215, 2005.
 Chechetka, A. and Guestrin, C. Efficient principled learning of thin junction trees. In Adv. NIPS , 2007. Chow, C. I. and Liu, C. N. Approximating discrete probability distributions with dependence trees. IEEE Trans. Inf. Theory , 14:462 X 467, 1968.
 Cover, T. M. and Thomas, J. A. Elements of infor-mation theory . John Wiley &amp; Sons, 2006.
 Deshpande, A., Garofalakis, M. N., and Jordan, M. I. Efficient stepwise selection in decomposable models. In Proc. UAI , 2001.
 Frank, A., Kir  X aly, T., and Kriesell, M. On de-composing a hypergraph into k connected sub-hypergraphs. Discrete Applied Mathematics , 131(2): 373 X 383, 2003.
 Friedman, N. and Koller, D. Being Bayesian about network structure. a bayesian approach to structure discovery in bayesian networks. Machine learning , 50(1):95 X 125, 2003.
 Fukunaga, T. Computing minimum multiway cuts in hypergraphs from hypertree packings. Integer Prog. Comb. Optimization , pp. 15 X 28, 2010.
 Gogate, V., Webb, W. A., and Domingos, P. Learning efficient Markov networks. In Adv. NIPS , 2010. Karger, D. and Srebro, N. Learning Markov net-works: maximum bounded tree-width graphs. In
Proc. ACM-SIAM symposium on Discrete algo-rithms , 2001.
 Koller, D. and Friedman, N. Probabilistic graphical models: principles and techniques . MIT press, 2009. Kolmogorov, V. and Schoenemann, T. Generalized sequential tree-reweighted message passing. ArXiv e-prints , May 2012.
 Krause, A. and Guestrin, C. Near-optimal nonmyopic value of information in graphical models. In Proc. UAI , 2005.
 Lauritzen, S. L. Graphical Models . Oxford University Press, Oxford, 1996.
 Lorea, M. Hypergraphes et matroides. Cahiers Centre Etud. Rech. Oper , 17:289 X 291, 1975.
 Malvestuto, F. M. Approximating discrete probabil-ity distributions with decomposable models. IEEE Trans. Systems, Man, Cybernetics , 21(5), 1991. Narasimhan, M. and Bilmes, J. PAC-learning bounded tree-width graphical models. In Proc. UAI , 2004. Nedi  X c, A. and Ozdaglar, A. Approximate primal so-lutions and rate analysis for dual subgradient meth-ods. SIAM J. Opt. , 19(4), February 2009.
 Saul, L. and Jordan, M. I. Exploiting tractable sub-structures in intractable networks. In Adv. NIPS , 1995.
 Schrijver, A. Combinatorial optimization: Polyhedra and efficiency . Springer, 2004.
 Shahaf, D., Chechetka, A., and Guestrin, C. Learning thin junction trees via graph cuts. In Proc. AIS-TATS , 2009.
 Spirtes, P., Glymour, C., and Scheines, R. Causation, prediction, and search , volume 81. MIT press, 2001. Srebro, N. Maximum likelihood bounded tree-width Markov networks. In Proc. UAI , 2002.
 Sz  X antai, T. and Kov  X acs, E. Discovering a junction tree behind a Markov network by a greedy algorithm. ArXiv e-prints , April 2011.
 Sz  X antai, T. and Kov  X acs, E. Hypergraphs as a mean of discovering the dependence structure of a discrete multivariate probability distribution. Annals OR , 193(1), 2012.
 Teyssier, M. and Koller, D. Ordering-based search: A simple and effective algorithm for learning bayesian networks. In Proc. UAI , 2005.
 Wainwright, M. J. and Jordan, M. I. Graphical mod-els, exponential families, and variational inference.
Found. and Trends in Machine Learning , 1(1-2),
