
Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. 
However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD prob-lems. Individually making each classification learner cost-sensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classi-fier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the un-derlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, Meta-
Cost, is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well. 
Classification is one of the primary tasks of data mining [18]. It has also been a subject of research in machine learning, statistics, pattern recognition, neural networks and other areas for several decades. As a result, many well-developed approaches to it now exist, including rule induction [20, 121, decision tree induction [8, 231, instance-based learning [ll, 11, linear and neural classifiers [3], Bayesian learning [17, 161, and others. In classification problems, the goal is to correctly assign examples (typically described as vectors of attributes) to one of a finite number of classes. Most of the currently-available algorithms for classification are designed to minimize zero-one loss or error rate: number of incorrect predictions made or, equivalently, the probability of making an incorrect prediction. This implicitly assumes that all errors are equally costly, but in most KDD applications this is far from the case. For example, in database marketing the cost of mailing to a non-respondent is very small, but the cost of not mailing to someone who would respond is the entire profit lost. 
In general, misclassification costs may be described by an arbitrary cost matrix C, with C(i, j) being the cost of predicting that an example belongs to class i when in fact it belongs to class j. The realization that in real-world applications non-uniform costs are the rule rather than the exception has led in recent years to an increased interest in algorithms for cost-sensitive classification. (Some of these will be discussed in the section on related work; Turney [25] provides an online bibliography on the topic.) Substantial work has gone into making individual algorithms cost-sensitive. Doing this for all algorithms available in the literature would be a very time-consuming enterprise, and often it is far from obvious how best to perform the conversion. 
A potentially better solution would be to have a procedure that converted a broad variety of error-based classifiers into cost-sensitive ones. To our knowledge, the only currently available procedure of this type is stratification-changing the frequency of classes in the training data in proportion to their cost [8, 9, 221. However, this approach has several shortcomings. 
It distorts the distribution of examples, which may seriously affect the performance of some algorithms. It reduces the data available for learning, if stratification is carried out by undersampling. It increases learning time, if it is done by oversampling. Most seriously, it is only applicable to two-class problems and to multiclass problems with a particular type of cost matrix, one where C(i, j) = C(j) (i.e., where the cost of misclassifying an example is independent of the predicted class). 
In this paper we propose a new procedure for cost-sensitive classification that attenuates or eliminates the disadvantages of stratification. This procedure, called MetaCost, is based on wrapping a  X  X eta-learning X  stage around the error-based classifier in such a way that the classifier effectively minimizes cost while seeking to minimize zero-one loss. The next section describes in detail MetaCost and the reasoning leading to it. The following section describes an extensive empirical evaluation of MetaCost, where it is compared with stratification and where its properties, in particular its scalability, are investigated. The paper concludes with a discussion of related research, directions for future work, and a summary of results. In order to obtain a procedure for making error-based classifiers cost-sensitive, let us start with some basic notions. If, for a given example z, we know the probability of each class j P(j]z), the Bayes optimal prediction for z is the class i that minimizes the conditional risk [17]: 
The conditional risk R(i]z) is the expected cost of predicting that z belongs to class i. The Bayes optimal prediction is guaranteed to achieve the lowest possible overall cost (i.e., the lowest expected cost over all possible examples z, weighted by their probabilities P(x)&gt;* C(i, j) and P(jlz) together with the rule above imply a partition of the example space X into j (possibly nonconvex) regions, such that class j is the optimal (least-cost) prediction in region j. The goal of cost-sensitive classification is to find the frontiers between these regions, explicitly or implicitly. This is complicated by their dependence on the cost matrix C: in general, as misclassifying examples of class j becomes more expensive relative to misclassifying others, the region where j should be predicted will expand at the expense of the regions of other classes, even if the class probabilities P(jlz) remain unchanged. In fact, we do not know what the optimal predictions are even for the pre-classified examples in the training set; depending on the cost matrix, they may or may not coincide with the classes that the examples are labeled with. If the examples in the training set were relabeled with their optimal classes according to the cost matrix given, an error-based classifier could be applied to learn the optimal frontiers, because the examples would now be labeled according to those frontiers. In the large-sample limit, a consistent error-based learner would learn the optimal, cost-minimizing frontiers. With a finite sample, the learner should in principle do no worse at finding these frontiers than it would at finding the optimal zero-one loss frontiers given the original training set. The MetaCost procedure is based on this idea. In order to relabel the training examples with their  X  X ptimal X  classes, we need to find a way to estimate their class probabilities P(jlz). Note that this is different from finding class probabilities for unseen examples, and that the quality of these estimates is important only insofar as it influences the final frontiers produced; probability estimates can be quite poor and still lead to optimal classification, as long as the class that minimizes conditional risk given the estimated probabilities is the same that minimizes it given the true ones [16]. One possibility would be to use standard probability estimation techniques, such as kernel density estimation [17]. However, successful learning of a cost-sensitive classifier using this approach would require that the machine learning bias (i.e., the implicit assumptions) of both the classifier and the probability estimator be valid for the application domain. Strictly speaking, this is impossible unless the classifier and the density estimator are the same, and a mismatch between probability estimation and classification stages has indeed been found to hurt performance in a context similar to the present one ([13]; see also related work section below). For example, decision tree and rule inducers are some of the most effective learners for very-high-dimensional domains like those often found in KDD, but these are precisely those domains were commonly-used probability estimation techniques like kernel densities and mixture models are least effective. Our assumption here will be that the user has chosen a particular classifier because its characteristics are well suited to the domain, and that we should therefore also use that classifier and no other. 
Many classifiers yield class probability estimates as a by-product of learning, but these are often very poor. For example, most decision tree and rule learners work by attempting to drive class probabilities to zero or one within each leaf or rule, and the resulting estimates are correspondingly off [7]. B ecause of this, and because some classifiers may not produce class probabilities, MetaCost allows their use, but does not require it. A more robust and generally-applicable method for obtaining class probability estimates from a classifier is suggested by recent research on model ensembles [lo, 141. Many authors (e.g, Breiman [5]) have found that most modern learners are highly unstable, in that applying them to slightly different training sets tends to produce very different models and correspondingly different predictions for the same examples, while the overall accuracy remains broadly unchanged. This accuracy can be much improved by learning several models in this way (or using other variations) and then combining their predictions, for example by voting. Thus MetaCost estimates class probabilities by learning multiple classifiers and, for each example, using each class X  X  fraction of the total vote as an estimate of its probability given the example. (Not all learners are unstable in the fashion described; methods for applying 
MetaCost to such learners are discussed in the section on future work.) Specifically, MetaCost uses a variant of Breiman X  X  [5] bugging as the ensemble method. In the bagging procedure, given a training set of size s, a  X  X ootstrap X  resample of it is constructed by taking s samples with replacement from the training set. Thus a new training set of the same size is produced, where each of the original examples may appear once, more than once, or not at all. This procedure is repeated m times, and the resulting m models are aggregated by uniform voting (i.e., when an unclassified example is presented, the ensemble labels it with the class that is predicted by the greatest number of models). MetaCost differs from bagging in that the number n of examples in each resample may be smaller than the training set size s. This allows it to be more efficient. If the classifier being used produces class probabilities, a class X  X  vote is estimated as the unweighted average of its probabilities given the models and the example. Also, when estimating class probabilities for a given training example 2, MetaCost allows taking all the models generated into consideration, or only those that were learned on resamples the example was not included in. The first type of estimate is likely to have lower variance, because it is based on a larger number of samples, while the second is likely to have lower statistical bias, because it is not influenced by the example X  X  own class in the training set. 
In short, MetaCost works by: forming multiple bootstrap replicates of the training set, and learning a classifier on each; estimating each class X  X  probability for each example by the fraction of votes that it receives from the ensemble; using Equation 1 to relabel each training example with the estimated optimal class; and reapplying the classifier to the relabeled training set. Pseudo-code for the MetaCost procedure is shown in Table 1. Note that, if the cost matrix changes, only the final learning stage needs to be repeated, and this is equivalent to a single run of the error-based classifier. The question of whether MetaCost reduces cost com-pared to the error-based classifier and to stratification was studied empirically using 28 benchmark databas-es from the UC1 repository [4]. The C4.5 decision tree learner [23] was used as the error-based classifier be-cause of its de facto role as a standard for empirical comparisons. The C4.5RULES post-processor, which converts C4.5 X  X  decision trees to sets of  X  X F . . . THEN . . .  X  rules, was also used, since it tends to improve accu-racy and produces simpler, more comprehensible results [23]. In what follows, the C4.5-C4.5RULES combina-tion will be referred to by the abbreviation  X  X 4.5R. X  Except where noted, all experiments were carried out by randomly selecting 2/3 of the examples in the database Inputs: S is the training set, L is a classification learning algorithm, 
C is a cost matrix, m is the number of resamples to generate, n. is the number of examples in each resample, p is Z+ue iff L produces class probabilities, 
Q is True iff all resamples are to be used for Procedure MetaCost (S, L, C, m, n,p, q) For i = 1 to m Let ,S X  X  be a resample of S with n examples. Let &amp;Ii = Model produced by applying L to Si. For each example x in S For each class j 
Let x X  X  class = argmini CP(j]x)C(i, j). Let M = Model produced by applying L to S. Return M. for training the classifiers, and using the remaining l/3 for measuring the cost of their predictions. The results reported are the average of 20 such runs. We first report the results of experiments on 15 multiclass databas-es, followed by experiments on 12 two-class databas-es. Lesion studies and a scaling-up study using a larger database complete this section. 3.1 Multiclass Problems Experiments were conducted with two different types of cost model. In the first, each C(i,i) was chosen at random from a uniform distribution in the [0, lOOO] interval, and each C(i, j) for i # j was chosen at random from the fixed interval [0, lOOOO]. Different costs were generated for each of the 20 runs conducted on each database; thus the standard deviations reported incorporate the effects of varying the cost matrix. In the second experiment, each C(i,i) was chosen as before, Iris 652f30 618f26 641f25 513f16 470f22 454f18 469f20 345f20 LED 2016f92 2181f78 1897581 1484f67 835f44 814f52 727f21 393f16 Splice 659f12 602f13 715flO 556fll 412f12 424f9 419fll 342f4 Wine 685f24 647f33 670f29 558zt25 461f18 475f13 455f14 264f13 but C(i, j) was chosen with uniform probability from the interval [0, 2OOOP(i)/P(j)], where P(i) and P(j) are the probabilities of occurrence of classes i and j in the training set. Thus the expected value of C(i, j) was 1000 P(i)/P(j). Th is means that the highest costs are for misclassifying a rare class as a frequent one, and inversely for the lowest. This mimics the situation often found in practice (e.g., in the database marketing domains mentioned before) where the rarest classes are the ones that it is most important to identify correctly. 
When this is the case, a low error rate can be achieved simply by ignoring the minority classes, but the cost will be high. Cost-sensitive learning is thus particularly important in these problems. As before, a different cost matrix was generated for each run. arbitrary multiclass cost matrices, we followed Breiman et al. X  X  (81 suggestion of making C(j) = xi C(i]j), where C(j) is the cost of misclassifying an example of class j, irrespective of the class predicted. The training set was then resampled so as to make each class X  X  probability equal to P X (j) = C(j)P(j)/ Cj C(j)P(j). (See P, PP. 112-1151 for a detailed justification of this procedure.) This was done in two different ways: by undersampling and by oversampling. In the undersampling procedure, all examples of the class j with highest P X (j) are retained, and a fraction 
P X (i)/P X (j) of the examples of each other class i is chosen at random for inclusion in the resampled training set. Although this is probably the most frequently used type of cost-based stratification, it has the disadvantage of reducing the data available for learning, which may increase cost. Thus oversampling is sometimes used instead. In this alternative, all examples of the class j with lowest P X (j) are retained, and then the examples of every other class i are duplicated approximately 
P X (i)/P X (j) times in the training set. This avoids the loss of training data, but may significantly increase learning time, particularly for superlinear algorithms. only the models it was not used to train, using C4.5R X  X  rule class probabilities, and generating 50 resamples, each of size equal to the original training set X  X  (i.e., g = 
False, p = True, m = 50 and n = s in Table 1). The results obtained are shown in Table 2, and graphically in Figure 1. (Results obtained using other variants of MetaCost are reported in the section on lesion studies.) In the fixed-interval case, neither form of stratification is very effective in reducing costs, which is perhaps not surprising given that the approximations made in order to apply them are far from true. In contrast, MetaCost reduces costs compared to C4.5R and undersampling in all but one database, and compared to oversampling in all but three. In the probability-dependent case, which more closely matches the assumptions used to apply stratification, both undersampling and oversampling reduce cost compared to C4.5R in 12 of the 15 databases. MetaCost does better, achieving lower costs than C4.5R and both forms of stratification in all 15 databases. Globally, the average cost reduction obtained by MetaCost compared to C4.5R is approximately twice as large as that obtained by undersampling, and five times that of oversampling. In both sets of experiments, the costs obtained by MetaCost are lower than those of each of the other three algorithms with confidences exceeding 99% using sign and Wilcoxon tests. These results support the conclusion that MetaCost is the cost-3 1500 Figure 1: Comparison of MetaCost X  X  costs (z axis) with those of C4.5R, undersampling and oversampling (y axes). Each point corresponds to a database and type of cost matrix. Points above the y = 2 line are those where MetaCost outperformed the alternative classifier. reduction method of choice for multiclass problems. 
In two-class problems where C(l, 1) = C(2,2) = 0, stratification can be applied without any approximation by making C(1) = C(2, l), C(2) = C(1,2) and proceeding as before. Letting/l be the minority class and 2 the majority class, experiments on two-class databases were conducted using the following cost model: C(l, 1) = C(2,2) = 0; C(l, 2) = 1000; C(2,l) = 1000 T, where T was set alternately to 2, 5, and 10. 
Note that the absolute values of C(2,l) and C(1,2) are irrelevant for algorithm comparison purposes; only their ratio r is significant. The results obtained, using the same settings for MetaCost as before, are shown in Table 3, and graphically in Figure 1. Oversampling is not very effective in reducing cost with any of the cost ratios. Undersampling is effective for r = 5 and r = 10, but not for r = 2. MetaCost reduces costs compared to C4.5R, undersampling and oversampling on almost all databases, for all cost ratios. In all cases, the costs obtained by MetaCost are lower than those of each of the other three algorithms with confidences exceeding 99% using sign and Wilcoxon tests (except for the sign test for undersampling with r = 10, where the confidence is 98%). These results support the conclusion that MetaCost is the cost-reduction method of choice even for two-class problems where stratification can be applied without approximation. 
Several questions arise in connection with MetaCost X  X  results. How sensitive are they to the number of resamples used ? Would it be enough to simply use the class probabilities produced by a single run of the error-based classifier on the full training set? Would 
MetaCost perform better if all models were used in relabeling an example, irrespective of whether the example was used to learn them or not? And how well would MetaCost do if the class probabilities produced by C4.5R were ignored, and the probability of a class was estimated simply as the fraction of models that predicted it? This section answers these questions by carrying out the relevant experiments. For the sake of space, only results on the two-class databases are presented; the results on multiclass databases were broadly similar. Table 4 reports the results obtained for r = 2, 5 and 10 by the following variations of 
MetaCost: using 20 and 10 resamples instead of 50 (labeled  X  X =20 X  and  X  X =lO X ); relabeling the training examples using the class probabilities produced by a single run of C4.5R on all the data (labeled  X  X 4 Probs X ); ignoring the class probabilities produced by 
C4.5R (labeled  X  X -1 Votes X ); and using all models in relabeling an example (labeled  X  X ll MS X ). In each case, 
Underspl all other settings are those used in the previous sections. Some of the main observations that can be made by comparing Table 4 with Table 3 are: l In the m = 50 to m = 10 range, cost increases l Ignoring C4.5R X  X  class probabilities increases cost l However, using multiple runs to estimate class l Using all models decreases cost for T = 10 but 
To summarize, the one crucial element of MetaCost is the use of multiple runs to estimate class probabilities, but a number of runs as low as 10 is sufficient for excellent performance. The use of the error-based learner X  X  class probabilities is beneficial but not critical, as is estimating an example X  X  class probabilities excluding the models it was used to learn. 3.4 Scaling Up An obvious potential disadvantage of MetaCost as used so far is that it increases learning time compared to the error-based classifier. In the databases used in the previous sections, where all learning times are on the order of seconds or fractions of a second, this is arguably immaterial. But in larger databases this might become a serious limitation. Although MetaCost only increases time by a fixed factor (the number of resamples, approximately) and so its asymptotic time complexity is of the same order as the error-based classifier X  X , the increased constant may be critical in large-scale applications. One solution lies in the fact that, since the multiple runs of the error-based classifier required to form the probability estimates are completely independent of each other, they can be trivially parallelized, reducing the time increase factor Table 5: Costs and CPU times (in minutes and seconds) of C4.5R, oversampling and six variants of MetaCost on the shuttle database. to about two. But a potential solution that does not require parallel processing is to use resamples that are smaller than the original training set (in addition to using a relatively small number of resamples, for example 10). Smaller resamples may result in higher costs, but conceivably still lower than those obtained with the error-based learner or with stratification, and therefore still worthwhile. Further, the increase in cost caused by learning the class probabilities on smaller samples may be offset or exceeded by the reduction obtained by the use of multiple models. Indeed, this idea is behind Breiman X  X  [6] successful  X  X asting X  method for scaling up learners. At the same time, reducing resample sizes will reduce running time, by a factor that will be particularly significant if the error-based learner used has superlinear running time. For example, if the classifier X  X  running time is quadratic in the number of examples, using a tenth of the examples will reduce running time for each resample by a factor of 100. If 10 resamples are used, this will make the CPU time of the probability estimation phase an order of magnitude smaller than that of a single run of the error-based classifier on all the data, and therefore insignificant. 
To test whether this approach is feasible, experiments were carried out using the largest database available in the UC1 repository: shuttle [4]. The goal in this database is to diagnose the state of the space shuttle X  X  radiators from a set of sensor readings. This problem is well suited for testing cost-sensitive algorithms, because there is a large majority of one class (the  X  X ormal X  state) and it is easy to obtain very low error rates [12], but presumably the cost of missing one of the rare anomalous states is potentially much higher than that of a false alarm, making error rate an inappropriate measure of performance. 
There are seven possible states, and nine numeric readings. The database contains 43500 examples from one shuttle flight and 14500 from another. The first flight was used for training, and the second for testing. All runs were carried out on a 300 MHz Pentium computer. Costs were set to C(i,i) = 0 for all i, and to C(i,j) = lOOOP(i)/P(j) for all i # j. It was not possible to obtain results for oversampling because C4.5R running on the expanded training set exceeded the available memory. (Judging from the results in the previous sections, there is a high probability it would do worse than undersampling.) Table 5 shows the costs and running times for C4.5R, oversampling, and MetaCost with several combinations of m (number of resamples) and n (resample size, as a function of the training set size s). As before, MetaCost was used with p = Due and q = False (see Table 1). Undersampling is fast, but it does not reduce cost. MetaCost with 10 resamples each one tenth the size of the original training set reduces cost by over an order of magnitude, and its running time is very similar to C4.5R X  X . Increasing the number of resamples and the resample sizes predictably increases running time, but does not further reduce cost. Reducing resample size to l/100 of the training set size increases cost by 17%. 
The poor results obtained by undersampling suggest that MetaCost X  X  excellent performance is not just due to the problem being  X  X asy, X  requiring only a small number of examples to achieve low costs. However, as a further check we repeated the experiment with 10% class noise added to training and test sets (i.e., with 10% probability an example X  X  class was changed to a different one, with all classes having the same probability of being the new one). The results are also shown in Table 5. Costs and running times are now much higher for all algorithms, and undersampling is now effective at reducing C4.5R X  X  cost, but MetaCost is again by far the best performer. As before, increasing m above 10 produces no significant improvements. (For n = $ and n = s C4.5R exceeded the available memory.) Remarkably, MetaCost with m = 10 and n = h is over an order of magnitude faster than C4.5R, while reducing cost by over an order of magnitude. 
This result, which may appear surprising at first, is due to the fact that estimating class probabilities by averaging several models learned on small subsamples has the effect of filtering out noise, producing a cleaner dataset, on which C4.5R runs much faster than on the original one. Examining the algorithms X  output shows that MetaCost induces a single short rule for each anomalous state and makes the normal state the default, while C4.5R X  X  output is over an order of magnitude larger. C4.5R is known to be inefficient on large noisy databases [12], so these results may not generalize to other error-based learners. However, although preliminary, they are a strong indication that 
MetaCost can be applied effectively to large databases, reducing cost without significantly increasing running time. They also suggest that in noisy databases MetaCost may additionally be useful as a method for speeding up learning. Cost-sensitive learning is the subject of a burgeoning literature, which space does not allow us to review here. We point the reader to [15] for a brief review, and to [25] for an online bibliography. This section discusses those elements of previous research that are most closely related to MetaCost. 
Chan and Stolfo [9] have proposed a variation of stratification that involves learning multiple classifiers on stratified subsamples of the database, which are then combined by a classifier that uses the others X  outputs as inputs. This method does not produce a single model, and thus its output is hard to understand, while MetaCost produces a single model of similar complexity to that of the error-based classifier. Compared to stratification by undersampling, Chan and Stolfo X  X  [9] method avoids the loss of training data, but is also only applicable (in its present form) to two-class problems. It is more ad hoc than stratification, lacking large-sample guarantees or clear foundations for its resampling scheme. Unlike MetaCost, it requires repeating all runs every time the cost matrix changes. It has only been tested in a single domain (credit-card fraud detection). 
Ting and Zheng [24] have proposed a cost-sensitive variant of boosting (an ensemble method) for decision trees. It is significant here because it should be easily adaptable to other error-based learners, and like MetaCost creates a cost-sensitive learner from multiple runs of an error-based one. However, like Chan and Stolfo X  X  [9] method, it does not produce a single comprehensible model. Compared to regular boosting, it is also more ad hoc, lacking its guarantees of near-optimal performance on the training data. It requires repeating all runs every time the cost matrix changes. Based on published results, it appears to produce substantially smaller cost reductions than MetaCost. 
MetaCost X  X  architecture is in some respects similar to that of CMM [13], a meta-learner that combines multiple models into a single one. While MetaCost X  X  goal is to reduce costs, CMM X  X  goal is to increase comprehensibility, while retaining some of the accuracy gains of multiple models. MetaCost uses the model ensemble to relabel training examples, while CMM uses it to label additional artificially-generated examples. A combination of the two may conceivably bring together the advantages of both. One item for future work is to carry out experiments on additional large databases, and using other error-based learners besides C4.5R. Of particular interest would be to apply MetaCost to algorithms that are not unstable with respect to variations in the training set, like &amp;nearest neighbor [ll] and naive Bayes [16]. In its present form, MetaCost may not be very effective with these algorithms, but an alternative is readily suggested by the results of [2] and [26]. Their method consists of learning multiple models using different subsets of the attributes, instead of different subsets of the examples. K-nearest neighbor and naive Bayes are unstable with respect to changes in the attributes used, and this method was indeed found to reduce those algorithms X  errors to an extent similar to bagging X  X  with other learners. It is readily incorporated into MetaCost. 
The application of MetaCost to large databases may be improved by stratifying the subsamples used, and/or by using partitioning instead of bagging. This would be similar to the first phase of Chan and Stolfo X  X  [9] method, and might avoid losing useful information in some of the resamples. It might be necessary, however, to correct the probability estimates obtained for the effects of stratification. 
An interesting comparison that has not yet been per-formed is to use an  X  X ff-the-shelf X  probability estimator (e.g., kernel densities) for the first phase of MetaCost instead of multiple runs of the error-based classifier. Al-though unlikely to be generally useful, given previous results [13], this may be successful for some domains and some combinations of probability estimator and classi-fier. More generally, the effect on MetaCost X  X  perfor-mance of the quality of the probability estimates used needs to be investigated. This is best done using syn-thetic data, for which we know the true class probabili-ties for every example. It would also be interesting to do an ROC analysis of MetaCost, by varying the probabil-ity thresholds at which an example X  X  relabeling changes from one class to another [21]. The current version of MetaCost is based on bagging. An alternative method for constructing model ensem-bles is boosting [19]. Boosting often achieves lower er-ror rates than bagging, and using it in MetaCost might produce corresponding reductions in cost. KDD applications have often been hindered by the lack of powerful cost-sensitive learners. Converting individual error-based learners into cost-sensitive ones is a tedious and sometimes difficult process, but the general-purpose alternative of stratification is of limited applicability, and has a number of other disadvantages. In this paper we proposed and evaluated MetaCost, a new procedure for making error-based classifiers cost-sensitive. MetaCost is based on relabeling training examples with their estimated minimal-cost classes, and applying the error-based learner to the new training set. Experiments show that MetaCost systematically reduces cost compared to error-based classification and to stratification, often by large amounts, and that MetaCost can be efficiently applied to large databases. This research was partly funded by the PRAXIS XXI program. The author is grateful to all those who provided the datasets used in the empirical study. WI w I251 [26] Z. Zheng. Naive Bayesian classifier committees. 
