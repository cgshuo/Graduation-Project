 A posting list consists of a term t and n  X  1 postings, each containing the ID of a document where t occurs, and other information required by the search engine X  X  scoring function, e.g. the frequency of t in each document [ 6 ]. Posting list caching can reduce the amount of disk I/O involved [ 13 , 14 ] in query processing, affords higher cache utilisation and hit rates than result caching [ 12 ], and can combine terms to answer incoming queries.
 Our Contribution: We show that static caching of posting lists can be modelled in a principled manner using constrained combinatorial optimisation (CCO), a standard method that has yielded great improvements in many fields [ 9 , Chap. 35], and we provide a principled investigation of whether CCO would yield better solutions (preferably using modest extra computational resources) than greedy methods. Using simulated query logs for a range of cache sizes, we perform a sequence of experiments that show that results using combinatorial optimisation is comparable to the greedy baseline of Baeza-Yates et al. using 200-1000 MB cache sizes, with some modest improvements for queries of length two to three. Much prior work has been devoted to caching posting lists [ 3  X  6 , 10 , 11 , 13  X  15 ]. Zhang et al. [ 15 ] benchmark five posting list caching policies and find LFU (least frequently used  X  cache members are evicted based on their infrequency of access) to be superior, and that cache hit rates for static posting list are similar to the LFU, but with less computational overhead. An integrated cache that merges posting lists of frequently co-occurring terms to build new posting lists in the inverted index is used by Tolosa et al. [ 14 ]. Using a cost function that combines disk lookup and CPU time, the integrated cache improves performance over standard posting list caching by up to 40 %. Combinatorial optimisation for caching has not been investigated to the same degree: Baeza-Yates et al. [ 5 ] cache query terms based on their frequencies in a query log, and obtain reduction in memory usage without increasing query answer time. Baeza-Yates et al. [ 3 ] extend this approach by caching query terms using (i) their frequency in a query log weighted by (ii) their frequency in a collection. Posting lists with the highest weight are then cached. This method obtains higher hit rates than their approach in [ 5 ], dynamic LRU (least recently used) and dynamic LFU for all cache sizes. We propose an extension of [ 3 ] which uses a principled method to select posting lists for static caching. Next, we describe the original method by Baeza-Yates et al., and our extension. Greedy Posting Lists Caching. Consider a list of queries, each of which consists of one or more terms and a cache of finite capacity. Let F the number of queries that contain term t in some query log Q number of documents that contain t in some collection C .A greedy strategy to posting list selection chooses the query terms (representing posting lists) with the highest F q ( t ) until cache space is exhausted as in [ 5 ]. However, Baeza-Yates et al. [ 3 ] observe a trade-off between terms with high F these have long posting lists that consume substantial cache space. They address this trade-off by using the ratio F q ( t ) /F d ( t ), called static caching by (i) calculating QTFDF of each t  X  Q L  X  in decreasing value of QTFDF and (iii) caching the terms with the highest QTFDF until cache space is exhausted. The method of [ 3 ] is thus a clever variation of the profit-to-weight ratio approach first used by Dantzig [ 8 ]. [ 3 , 4 ]: given a knapsack with capacity c and n items c v ,...,v n and weights w 1 ,...,w n , take the items that maximise the total value without exceeding c . An item can only be selected once and fractions of items cannot be taken. As the knapsack optimisation problem is NP-hard and cannot in general be solved optimally using a greedy strategy [ 7 , Chap. 16], we next describe how to formulate posting list selection as a combinatorial optimisation problem which, in theory, would find an approximately optimal solution. Combinatorial Optimisation for Posting Lists Caching. We formalise the observation of [ 3 ] that a trade-off exists between F terms should be cached that yield the highest possible F constraint that the total size of the posting lists of cached terms should not exceed cache size. This is a classic CCO problem (a fact already noted by [ 3 ], but without formalisation or reported experiments). We cast posting list selection as an integer linear program of the form: where n i =1 v i x i is the objective function , n i =1 w i  X  n are constraints where x i represents a term t i (a posting list). A solution is a setting of the variables x i ;a feasible solution is a solution that satisfies all constraints; and an optimal solution is a feasible solution with maximal value of the objective function. We consider only optimal solutions here. Equation ( 2 ) states that the total weight of the selected terms cannot exceed c ,andEq.( 3 ) that each term is either selected or discarded. We set v i and refer to the method described here as the CCO method.
 We emphasise two points. First, the CCO method maximises the chance of a query term cache hit, but does not consider disk I/O a factor. We can do this using a multi -objective CCO problem where one objective function seeks to minimize disk I/O (using the length of the posting list of x a second objective function that seeks to maximise the number of cache hits. Second, if a term is selected its entire posting list is loaded. Another approach is to allow fractions of posting lists to be loaded and access the main index as needed. This may be useful if e.g. each posting list is sorted so access to the main index is reduced. We leave both topics as future work. Query logs from large search engines are typically not publicly available in large numbers. Instead, we construct simulated query logs using (i) the method of Azzopardi et al. [ 2 ] and (ii) random sampling from a large synthetic query log. Known-item Queries. We construct synthetic query logs containing known-item queries using the method of [ 1 , 2 ] as follows: We first select a document d from the collection (with uniform probability), then select a query length l and then select l terms t 1 ,...,l from the document language model (LM) of d probability p ( t i |  X  d ) and add t i to q . p ( t i |  X  likelihood estimate of a term occurring in a document and (ii) a background model p ( t ) (maximum likelihood estimate of t in the collection). Estimating (i) is done using one of two LMs [ 1 ]. The popular LM is given by where t i ,t j are terms in d k and n ( t i ,d k ) is the term-frequency of t discriminative LM is given by where b ( t j ,d k ) = 1 if term t j occurs in d k .
 Sampling from a Large Query Log. We use the anchor text query log from ClueWeb09 1 as starting point, which contains 500M triplets of the form &lt; URL , anchortext , fq &gt; where fq is the frequency of the tuple &lt; URL , anchortext &gt; . From this query log, we sample with replacement to generate new query logs. We describe how we simulate repeated queries and how we measure perfor-mance. We evaluate the CCO method against the greedy baseline of Baeza-Yates et al. [ 3 ], using the number of cache hits as our cache performance measure. Simulating Repeated Queries. The method of Sect. 4 generates queries occurring exactly once. To generate repeated queries in the synthetic query sets we do as follows: after simulating a query, we generate a random number r in the interval (0; 1) and compare it to a threshold  X  .If r&gt; X  we duplicate the query. We fix  X  =0 . 44 meaning that  X  56 % of the queries have multiple occur-rences [ 4 ]. We simulate queries of length l =1 , 2 , 3 and generate m = 5 queries from each document. For query logs simulated using the method in Sect. 4 ,we cannot control repeated queries.
 Experimental Settings. We experiment with cache sizes of 200, 600 and 1000 MB (cache sizes can vary between 100 MB to 16 GB [ 14 ]) and fix the size of a posting to 8 bytes. We use ClueWeb09 cat. B.  X  a domain-free crawl of ca. 50 million web pages in English  X  indexed using Indri 5.8 with no stemming and with stop words removed as collection. We simulate query logs of 1M, 5M and 10M queries using each of the two LMs from Sect. 4 and the method from Sect. 4 with the anchor text as queries. As in [ 3 ], we estimate F query log and F d ( t ) from the collection. Each CCO problem is solved using SYMPHONY 2 (extensive experiments and tuning using lp solve consistent improvements). We count a cache hit for a query iff at least one of its terms is found in the cache (see [ 15 ] for alternative definitions). A single hit is sufficient for efficient retrieval as we need only traverse that term X  X  posting list, and scan the forward index of each document to determine if remaining query terms are found. Counting query hits using this linear scan approach is less efficient than posting list intersection, but in this preliminary work, it allows us to test the merit of our method. We show results for the 5M and 10M query logs generated using the method from Sect. 4 in Table 1 . Results for all other query logs are qualitatively similar. We do not report CPU or memory consumption as this cost is likely minimal compared to indexing and retrieval costs. Across all query logs, query lengths (qlen) and cache sizes, the overlap coefficient is &gt; 85 % and both CCO and the baseline cache contain approximately the same number of terms. For qlen=1, CCO and the baseline perform nearly identically for all query logs. For qlen=2, the discriminative query log gives rise to the largest differences between CCO and the baseline though these differences are negligible relatively to the total number of cache hits. For the popular query log, the differences are substantially smaller. The observations for qlen=3 are identical to those for qlen=2. We have investigated static posting list caching as a constrained combinatorial optimisation (CCO) problem and have evaluated this theoretically principled method against the greedy method of Baeza-Yates et al. [ 3 ]. We found both methods performed similarly for all cache sizes, with some modest gains for the CCO method. The high values ( &gt; 85 %) of the overlap coefficient in all experi-ments suggest that both methods mostly identify the same high-frequency query terms and that differences in cache hits can be attributed to a small set of infre-quent terms. However, while combinatorial optimisation gives, in theory, optimal solutions, in practice the quality of the solution also depends on the problem, the solver and the settings of the solver X  X  parameters. In future work, we will investigate (i) how this impacts posting list selection, (ii) if CCO can obtain consistent performance improvements for domain-specific query logs, and (iii) the use of multi-objective CCO to balance disk I/O with cache hits.
