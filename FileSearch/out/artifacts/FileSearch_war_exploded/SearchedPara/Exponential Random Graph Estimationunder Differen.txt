
Authors X  note: The original published version of this pa-per contained a regrettable error in the proof of -differential privacy of the  X  X hain mechanism X . This version of the paper corrects the error by replacing the chain mechanism with an ( , X  ) -differentially private mechanism, inspired by [17], for estimating the alternating graph statistics discussed herein. The effective analysis of social networks and graph-structured data is often limited by the privacy concerns of individuals whose data make up these networks. Differential privacy offers individuals a rigorous and appealing guarantee of pri-vacy. But while differentially private algorithms for comput-ing basic graph properties have been proposed, most graph modeling tasks common in the data mining community can-not yet be carried out privately.

In this work we propose algorithms for privately estimat-ing the parameters of exponential random graph models (ERGMs). We break the estimation problem into two steps: computing private sufficient statistics, then using them to es-timate the model parameters. We consider specific alternat-ing statistics that are in common use for ERGM models and describe a method for estimating them privately by adding noise proportional to a high-confidence bound on their local sensitivity. In addition, we propose an estimation algorithm that considers the noise distribution of the private statistics and offers better accuracy than performing standard param-eter estimation using the private statistics.
 H.2.7 [ Database Administration ]: Security, integrity, and protection; H.2.8 [ Database Management ]: Data Mining Differential privacy; Exponential random graph model
The explosion in the collection of networked data has fu-eled researchers X  interest in modeling networks and predict-ing their behavior. However, for important application areas such as disease transmission, network vulnerability assess-ment, and fraud detection (among others), networks contain sensitive information about individuals and their relation-ships. It is difficult for institutions to release network data and it remains difficult for researchers to acquire data in many important application domains.

Recently, a rigorous privacy standard, differential privacy [9] was proposed that allows for formal bounds on the disclo-sure about individuals that may result from computations on sensitive data. Differential privacy provides each partic-ipant in a dataset with a strong guarantee and makes no assumptions about the prior knowledge of attackers.
Since its introduction, differentially private algorithms have been developed for a wide range of data mining and anal-ysis tasks, for both tabular data and networked data. For networks, existing work has focused on algorithms for accu-rately releasing common graph statistics under differential privacy [13, 17, 25, 26, 28, 31]. However, graph statistics are only one aspect of social network analysis and are often most useful in conjunction with some paradigm for model-ing structural features of graphs. Privately modeling graph data has only rarely been explored by researchers; we are aware only of work using the Kronecker model [20] under differential privacy [22].

In this work, we study the differentially private use of the classic exponential random graph model (ERGM) [21, 30, 27]. ERGMs are a powerful statistical modeling tool that al-lows analysts to analyze a network X  X  social structure and for-mation process. In social science and related fields ERGMs have been successfully applied to many scenarios, such as co-sponsorship networks [5], friendship networks [12], and corporate and inter-organizational networks [21].

Our goal is to accurately support parameter estimation for ERGMs under differential privacy, focusing on a specific set of model parameters of recent interest to researchers: the alternating statistics . These sophisticated statistics rep-resent more structural information than traditional star and triangle counts, and have been shown to lead to much better modeling results [30, 27, 14, 12].

Our adaptation of differential privacy to graphs protects relationships of individuals by limiting the influence on the output of any single relationship (edge) that is created or removed from the network. 1 A standard algorithm that im-plements this idea is the Laplace mechanism [9], which adds random noise to the output. The amount of noise required is related to the maximum difference in the output due to a single edge addition or removal for any possible network (this is the global sensitivity of the function producing the output). For ERGM estimation, this requires calculating the exact change in the ERGM parameter estimates as a result of changing an edge. Unfortunately, the global sensi-tivity for most ERGM parameters is either hard to compute in general, or too high, so that using noise calibrated to the global sensitivity is not acceptable.

To overcome this obstacle, we decompose private ERGM estimation into two separate steps. We first privately com-pute the sufficient statistics for ERGM estimation (typically the model statistics required by model description) and then estimate the parameters using only these sufficient statistics. Since the estimation process uses only the differentially-private statistics, and there is no additional access to the original graph, the output of estimation is also differentially private. In practice, the estimation algorithm is executed either on the server side (by the data owner) or on the client side (by the analyst). In either case, it does not violate the privacy condition to release both the statistics and the derived ERGM parameters.

Challenges arise in both steps of our approach. While prior work has proposed mechanisms for various graph statis-tics, common ERGM models use unique statistics, e.g., al-ternating graph statistics [30], which are a complex aggre-gation of a series of basic graph statistics. We describe new approaches for privately computing these statistics. The second parameter estimation step could be implemented us-ing standard methods [29, 3] while treating the privately-computed statistics as if they were the true statistics. In-stead, we propose a novel parameter estimation method based on Bayesian inference, which considers the noise dis-tribution from which the private statistics are drawn and produces more accurate parameter estimates.
 Contributions  X  In Section 3, we describe ( , X  )-differentially private al-gorithms for estimating two key statistics: alternating k -triangle and alternating k -twopath. The algorithms add noise proportional to a high-likelihood bound on the local sensitivity of the statistics. Unlike global sensitivity, lo-cal sensitivity is determined by the current graph instead of worst-case graphs and can be much lower. Our algo-rithms use a technique formalized in [17] and inspired by the Propose-Test-Release approach [8].  X  We describe a new Bayesian method for ERGM param-eter estimation (in Section 4) that is designed for the noisy sufficient statistics produced by a differentially private algo-rithm. While it is possible to use a standard algorithm for estimation, our inference takes the unknown network as a hidden variable and can result in estimates with lower error.  X  We study a set of ERGM models based on model terms consisting of alternating graph statistics [30] (in Section 5). Our experiments on both synthetic and real graphs show
This is one of the most common interpretations of differen-tial privacy for graphs, called edge differential privacy [13]. Node differential privacy is stronger, but often hurts util-ity. Our results for edge-differential privacy can easily be extended to k -edge privacy to protect multiple edges. that our techniques significantly reduce noise over baseline approaches.
A graph G = ( V,E ) is defined as a set of nodes V and relationships E : V  X  V  X  X  0 , 1 } . A common representation of a graph is as an adjacency matrix x , where x ij  X  { 0 , 1 } indicating whether there is an edge from node i to j . Let f (  X  ) define a vector of graph statistics called the model terms ; the concrete values of f ( x ) are the model statistics . Formally, the ERGM with parameter vector  X  defines a probability distribution over graphs in the space X (typically the set of all simple graphs with n vertices): Z  X  is a normalizing constant to make p ( x ) a true probabil-ity distribution, parameterized by  X  . If x 0 is the observed graph and X represents the random variable defined by the distribution above, our goal is to tune the parameter vector  X  , s.t. the expected value of f ( X ) is equal to observed statis-tics, meaning E  X  ( f ( X )) = f ( x 0 ), which intuitively puts the observed graph in the  X  X enter X  of space of possible graphs implied by the model. For example, the simplest ERGM uses the number of edges as the only model term. If m 0 is the total number of edges in x 0 , the  X  , which enables the expected number of edges of ERGM equal to m 0 , is given by [24]: Estimating  X  . The optimal  X  maximizes the likelihood of x 0 given  X  [24], i.e., arg max  X  p ( x 0 |  X  ). Unfortunately, most ERGMs do not have an analytical or closed-form estimate for the optimal  X  . Thus, numerical solutions are proposed in the literature, such as Markov chain monte carlo maximum likelihood estimation [29] and Bayesian inference [3]. An interesting property of these inference methods is that the algorithm does not require access to the input graph itself, i.e., the sufficient statistics for the parameter estimation are just the model statistics. This feature enables us to decom-pose the private inference problem into two steps, allowing analysts to see only the sufficient statistics.
 ing query of a specific graph pattern. Common patterns include triangles, stars and loops [21]. Recent research has introduced alternating statistics for k -star, k -triangle and k -twopath, which can represent structural properties of a graph better than traditional star and triangle counts [30]. Many works have explored these statistics since they were proposed, and they are an active and promising form of ERGM [30, 27, 14, 12]. Our work is focused on these alter-nating statistics (defined precisely in Section 3) which have not been studied before under differential privacy. A wide variety of other model terms are used with ERGMs; our gen-eral approach is compatible with other terms but they are beyond the scope of this work.
Differential privacy is traditionally defined over a tabular based database D consisting of records, each of which de-scribes an individual. When querying the database, differ-ential privacy protects individuals by restricting the impact on the output of any individual who opts into or out of the database. Two such databases that differ by one record are called neighbors .
 Definition 2.1 (Differential Privacy[7]) . Let D and D 0 be neighboring databases and K be any algorithm. For any subset of outputs O  X  Range ( K ), the following holds: If  X  = 0, K is standard -differentially private. Otherwise, K is relaxed ( , X  )-differentially private.

The input privacy parameter (and  X  if using the relaxed definition) are non-negative and are used to measure the degree of privacy protection. Smaller means better privacy as exp( ) is close to one.

In this paper, our database is a graph describing rela-tionships among individuals. Our purpose is to protect rela-tionships among individuals so we adapt differential privacy, following [13, 17, 26, 31, 28], by defining a neighboring graph as a graph that differs by one edge.
 Differential privacy can be achieved by adding noise to the output of algorithms according to privacy parameters and query sensitivity . The global sensitivity of a query is the maximum possible difference in the output when evaluating the query on two neighboring graphs. E.g., the query asking for the maximum degree of a graph has global sensitivity 1, because adding or removing one edge changes any degree by at most 1. Let Lap ( b ) be a Laplace random variable with mean 0 and scale b .
 Definition 2.2 (Laplace mechanism [9]) . Given query f on graph x , the following algorithm K ( f,x ) is -differentially private: where global sensitivity
A basic property we rely on is that post-processing a noisy, differentially-private output using any algorithm that does not access the original data cannot alter the privacy guaran-tee [19]. Past research has shown that post-processing the noisy output can, however, have significant impact on utility. In addition, composition rules for differential privacy allow us to compute the privacy standard that results from the combined release of multiple query answers or releases. Pre-cisely, if each release is i -differential privacy, the combined is then P i i -differential privacy.

In our perturbation step, we will use the composition rule to add noise to multiple model terms. In the parameter estimation step, we run post-processing.
 Some common graph analyses have high global sensitivity, requiring the Laplace mechanism to add enormous amounts of noise. For example, consider the simplest ERGM model above where  X  is calculated by (2). On a graph where m 0 = 0 or a graph where m 0 = n 2 ,  X  can change drastically with the addition or deletion of one edge. In other words, the global sensitivity is very high for this function. But the fact is that most real graphs are nothing like these extremes. Thus, by only focusing on the input graph X  X  neighbors, the local sensitivity [25] can be much smaller.
 Definition 2.3 (Local sensitivity[25]) . Given query f and graph x , local sensitivity LS f ( x )
However, one cannot achieve differential privacy by adding noise proportional to the local sensitivity because local sen-sitivity itself could disclose information. The authors of [25] proposed using a smooth upper bound on the local sensi-tivity, the smooth sensitivity . Intuitively, smooth sensitivity tries to  X  X mooth X  out the difference between local sensitiv-ities of two neighbors, so that it is itself not sensitive. Let d ( x,x 0 ) be the distance between two graphs, i.e. the number of edges in which they differ.
 Definition 2.4 (Smooth bound and smooth sensitivity[25]) . Function S f : X  X  R defines a  X  -smooth bound of local sensitivity on query f if The  X  -smooth sensitivity of f is a  X  -smooth bound, and
Calculating the smooth sensitivity for a function may be easy (in cases like the median of a list of numbers [25]) but could be quite difficult for other functions, requiring complex proofs and nontrivial algorithms [17].
In this section we provide methods for privately comput-ing alternating graph statistics. In Sec. 3.1 we define three alternating statistics and show that one of them (alternat-ing k -star) has a constant global sensitivity. This means the Laplace mechanism to be applied with relatively small error. However, alternating k -triangle and alternating k -twopath both have high global sensitivity. In Sec 3.2 we show that we cannot resort to smooth sensitivity, as calcu-lating the smooth sensitivity is NP-hard in both cases. To address this challenge, we adapt a technique which calibrates noise to a private, high-likelihood upper bound on the local sensitivity [17]. That bound is produced using the global sensitivity of the local sensitivity function. If, however, the global sensitivity of that function is high, the technique can be repeatedly applied, using a high-likelihood bound on the local sensitivity of the local sensitivity function. We describe these  X  X irst-order X  and  X  X econd-order X  algorithms with static error calculation in Sec 3.2, and then analyze the local sen-sitivity of alternating k -triangle and alternating k -twopath in Sec. 3.3.
The three alternating graph statistics, alternating k -star, alternating k -triangle and alternating k -twopath, are essen-tially complex aggregations of traditional k -star, k -triangle and k -twopath statistics. Instead of considering a vector of k terms, the alternating statistics aggregate over the terms but enforce alternating signs between each consecutive term, to weaken the correlation among different terms and effectively reduce the weight on higher terms near k .
 Alternating k -star. The k -star is a counting query of a star pattern in the graph, where each star contains k edges, i.e., S k = P i d i k where d i is the degree of node i .
 Definition 3.1 (Alternating k -star [30]) . With parameter  X   X  1, alternating k -star S is defined as
The  X  parameter here is a good way to control the geo-metrical weights on all k -stars.
 Alternating k -triangle. A k -triangle is a graph pattern in which k triangles share a common edge. The k -triangle query asks for the total number of k -triangles in the graph. Define the shared partner matrix C , where each entry ( i,j ) in C is the count of shared partners between nodes i and j , mathematically C ij ( x ) = P l x il x lj . Formally, k -triangle T is defined: Alternating k -triangle is defined similarly as alternating k -star, using parameter  X  : Definition 3.2 (Alternating k -triangle [30]) . With param-eter  X   X  1, alternating k -triangle T is: Alternating k -twopath. A k -twopath graph pattern is very similar to k -triangle, except it does not require the shared edge required by the k -triangle statistic. Using the shared partners matrix C above, the counting query for k -twopath U k is: And alternating k -twopath is: Definition 3.3 (Alternating k -twopath [30]) . With param-eter  X   X  1, alternating k -twopath U is
Alternating k -star S is the only statistic that can be read-ily solved using existing privacy mechanisms. Because the degree sequence is a sufficient statistic for S , one natural ap-proach is to use the mechanism described by Hay et al [13] to compute a private degree sequence from x , and then use it to compute S by Eq. (3). But, in fact, it can be shown that the global sensitivity of S is at most 2  X  . Thus, Laplace noise may be a better choice (  X  is usually set to a small in-teger in practice). We make empirical comparisons between these methods in Section 5.
 Lemma 3.4. The global sensitivity of alternating k -star is at most 2  X  .
Because the global sensitivity of alternating k -triangle and k -twopath can be as large as O ( n ), we would like to use a method which adds noise scaled to the local sensitivity, or a quantity close to the local sensitivity. One approach is to compute a smooth bound on the local sensitivity, however, the following lemma shows the NP-hardness of computing such a bound for these two statistics: Lemma 3.5. Computing the smooth sensitivity for both al-ternating k -triangle and alternating k -twopath is NP-hard. We therefore employ a technique inspired by the Propose-Test-Release framework [8], and formalized by Karwa et al [17], where it was used to estimate k -triangles. The tech-nique first computes a private over-estimate of the local sen-sitivity, one that is higher than the local sensitivity with high probability. That becomes a safe sensitivity value for cali-brating Laplace noise, however, the result satisfies only the weaker notion of ( , X  )-differential privacy.

Let f ( x ) be the sensitive function/query. We use LS f, 1 to denote the local sensitivity of f , which is a function of the input graph x .
 Algorithm 1 Local sensitivity bounding algorithm (First order) Require: input graph x , query f and , X  2:  X  y 1 = LS f, 1 ( x ) + Lap ( GS ( LS f, 1 ( x )) / ) + a  X  GS ( LS 3:  X  y = f ( x ) + Lap (  X  y 1 / ) 4: return  X  y,  X  y 1
In Algorithm 1,  X  y 1 is the private bound on the local sensi-tivity, computed by adding scaled noise to LS f, 1 ( x ), as well a positive offset, so that the bound is higher than LS f, 1 with high probability. Notice that the scale of the noise is determined by the global sensitivity of the local sensitivity, GS ( LS f, 1 ( x )).

If GS ( LS f, 1 ( x )) is large, it may cause  X  y 1 to be a significant over-estimate of LS f, 1 ( x ). We can repeat this approach by using a safe upper bound of the local sensitivity of LS f, 1 as presented below. Thus, Algorithm 1 bounds the first-order local sensitivity and the following algorithm bounds the second-order local sensitivity.
 Algorithm 2 Local sensitivity bounding algorithm (Second order) Require: input graph x , query f and , X  2:  X  y 2 = LS f, 2 ( x ) + Lap ( GS ( LS f, 2 ( x )) / ) + a  X  GS ( LS 3:  X  y 1 = LS f, 1 ( x ) + Lap (  X  y 2 / ) + a  X   X  y 2 4:  X  y = f ( x ) + Lap (  X  y 1 / ) 5: return  X  y,  X  y 1 ,  X  y 2 Theorem 3.6. Algorithm 1 is (2 , 1 2 e  X  ) -differential pri-vacy. Algorithm 2 is (3 , 1 2 e  X  + 1 2 e 2  X  ) -differential privacy.
The step of replacing the global sensitivity by a high-likelihood bound on the local sensitivity can be repeatedly applied to form more complex higher order algorithms. How-ever, each additional bounding step requires splitting the privacy budget and the combined effects of repeatedly over-estimating higher order sensitivities may diminish utility. For the two alternating statistics we consider, and the datasets we tested on, we found that first order and second order is sufficient.
 Definition 3.7. Y 0 ,Y 1 ,...,Y n is a random variable chain , when the following condition is satisfied: for any i  X  [0 ,n  X  2], Y i is conditionally independent of Y i +2 ,Y i +3 ,...Y Y
From conditional independence, an important property of a random variable chain is the following:
It is easy to see that  X  y,  X  y 1 ,... is actually a random variable chain. We use mean squared error (MSE) as the measure-ment of error. In Algorithm 1 and 2, MSE of  X  y can be written as E [(  X  y  X  f ( x )) 2 ] = V [  X  y ] + ( E [  X  y ]  X  f ( x )) always unbiased (Laplace noise in the last step with mean zero), MSE = V [  X  y ].

Without knowing the true value of the local sensitivities, it is quite hard to compute the MSE. That is to say, we cannot compute the error like we do for the Laplace mechanism, since the noise in the latter is independent of input graph x . But, by exploring properties of the random variable chain, it is possible to utilize the following Lemma as a closed form calculation tool for MSE. In fact, we extend law of total expectation/variance [32].
 Lemma 3.8. Y 0 ,Y 1 ,...,Y n is a random variable chain. Write F
E [ Y 0 ] = G
V [ Y 0 ] = G
Applying Lemma 3.8, one can calculate the MSE of Al-gorithms 1 and 2. Such error measurement, which needs access to the input graph, can serve as an evaluation tool for privacy researchers when working with our algorithms. From the perspective of data owners, the analytic result of MSE can help them to decide between Algorithm 1 and 2, i.e., with fixed privacy parameters, selecting the algorithm with lower error.
Now we apply the idea of bounding the local sensitivity to alternating k -triangle and alternating k -twopath. Let  X  = 1  X  1 / X  . By binomial coefficients, we can rewrite alternating k -triangle T ( x ;  X  ) as Lemma 3.9. Set C 0 iv = C iv  X  x ij and C 0 vj = C vj  X  x Let N ij be all shared partners of node i and j and C max max i&lt;j C ij . The local sensitivity of T is
LS T, 1 ( x ) = max
As C max has global sensitivity 1, LS T, 1 has global sen-sitivity at most 2. So we can construct a first-order local sensitivity bound using LS T, 1 =  X  + 2 C max to compute pri-vate alternating k -triangle.
 For alternating k -twopath U ( x ;  X  ), we can rewrite it as Lemma 3.10. Let N i be the set of neighbors of node i and d max be the maximum degree. Set C 0 iv = C iv  X  x ij C vj = C vj  X  x ij . We have local sensitivity
From Lemma 3.10 above, (8) has global sensitivity 2, since d max will change by at most 1 by adding or removing an edge. (9) has global sensitivity 1 / X  for C max &gt; 3. Therefore, we can construct either a first-order or second-order bound. Note that (7) is the exact local sensitivity of alternating k -twopath, but we cannot bound it in Algorithm 1 as the global sensitivity of (7) is not clear. Instead we use (8). When applying Algorithm 2, (7) is the local sensitivity to be bounded at Line 4, as by that step the higher order (second-order) local sensitivity (9) has already been safely bounded. We compare the resulting error empirically in Section 5.
The parameter estimation step in our workflow takes the private sufficient statistics  X  y from the previous perturbation step and finds the best parameter vector  X  . As stated above, this step is essentially post-processing a differentially private output, so the output  X  is also differentially private. In this section, we discuss different ways of estimating  X  given  X  y .
Current estimation techniques [29, 3] provide a baseline solution for parameter estimation with private statistics. As these procedures essentially only need access to model statis-tics, our sufficient statistics in  X  y take the place of the true model terms. The semantics is now to search for  X  that defines a probability distribution on graphs with expected method depends on the amount of noise added into y 0 and how  X  reacts to those changes in y 0 .

Prior to applying standard estimation, we post-process  X  y to cope with some of the difficulties of the perturbed model statistics. As the output of perturbed  X  y might not be graph-ical (i.e., no graph has statistics equal to  X  y ), standard esti-mation may fail to converge. We propose generating a graph that has the closest statistics to  X  y and use the statistics from that graph to replace  X  y , in order to avoid non-converging situations and to potentially remove noise from  X  y simulta-neously. We use simulated annealing for this purpose and, in practice, we often see big improvements in the accuracy of estimates.
Standard estimation is the direct way of post-processing  X  y , but since we know the distribution of the noise added to  X  y , we can  X  X uess X  the true values and incorporate them into the estimation algorithm. This idea naturally fits into Bayesian inference based post-processing. While based on earlier work [3] on Bayesian inference for non-private esti-mation, our method deals with the extra hidden variable of graph x in our setting. And later we will see, by introducing the unknown x , our method can utilize more information from private statistics, such as the local sensitivity bounds. In particular, we search for  X  given  X  y , represented as the posterior distribution of ERGM parameter  X  : where x is our guess about x 0 , but the fact is that we need to summarize over all possible x to get to the posterior. In (10), p (  X  y | x ) is the privacy distribution , defined by the dif-ferential privacy mechanism applied on sufficient statistics. p ( x |  X  ) is the ERGM distribution , as shown in (1) and q ( x ;  X  ) represents the unnormalized distribution.
The probability distribution (10) is hard to calculate or even sample from directly due to summarization over all graphs and normalizing constant Z  X  . Using the exchange algorithm [23], we introduce extra variables x ,  X  0 and x bypass the difficult terms (10). By carefully choosing the probability distribution of these new random variables, the posterior distribution is now augmented as shown in (12). The key is that the marginal posterior distribution for  X  in (12) is equivalent to (10). Thus, if we are able to sample from the distribution in (12), the marginal posterior distribution for  X  can be obtained by summarizing over all samples.  X  is sampled from proposal distribution p (  X  0 |  X  ), where, for a given  X  , a new  X  0 can be proposed according to p (  X  A common choice is a multivariate normal distribution or a multivariate t distribution, with mean equal to  X  . x,x 0 sampled graphs under the ERGM with parameter  X  and  X  0 .
A MCMC based sampling process for (12) is shown in Al-gorithm 3. In particular, the initial input  X  and x could be any parameters and any graph. In Line 3, we need a sepa-rated MCMC chain to sample x 0  X  p ( x 0 |  X  0 ). In such MCMC algorithms, at each iteration ( T iterations in total), we pro-pose adding or removing edges in the current state of graph, calculate the new model statistics, compare the probability of new state x new to that of old state x old , and with prob-process should be run long enough so that final sample x 0 truly from p ( x 0 |  X  0 ).
 Algorithm 3 ERGM parameter estimation with private model statistics Require:  X  y , initial  X ,x 1: for i in 1 to T do 2: Sample  X  0  X  p (  X  0 |  X  ) 3: Sample x 0  X  p ( x 0 |  X  0 ) 4: Replace  X  with  X  0 and x with x 0 , with probability 5: return samples of  X  .

H in Line 4 is the ratio of accepting the exchange, com-puted by comparing the probability before and after ex-change. That is, we exchange  X  with  X  0 and x with x (12) and calculate the ratio. Then the complex terms are cancelled out and each remaining term is easy to compute.
In practice, Algorithm 3 usually results in low acceptance rates in the exchange step in Line 4 and thus long mixing times for the MCMC process. We now propose to separate that last step, isolating simultaneously updated  X  and x into two different steps, as shown in Algorithm 4, which improves the acceptance rate significantly.
 Algorithm 4 Improved ERGM parameter estimation with private model statistics Require:  X  y , initial  X ,x 1: for i in 1 to T do 2: Sample  X  0  X  p (  X  0 |  X  ) 3: Sample x 0  X  p ( x 0 |  X  0 ) 4: Exchange  X  with  X  0 , with probability min(1 ,H 1 ) 5: Replace x with x 0 , with probability min(1 ,H 2 ) 6: return samples of  X  .
 H 1 and H 2 in Algorithm 4 are defined as follows.

The correctness of Algorithm 4 can be proved briefly in terms of a component-wise Metropolis-Hasting algorithm, with hybrid Gibbs updating steps. In each iteration,  X  0 x (Line 2 and 3) are drawn based on the full conditional distribution, so the updating probability is always 1. In Line 4 and 5, we update  X  and x with Hasting ratios. Although we may end up updating  X  0 and x 0 more times in a iteration, we still get to the detailed balance in MCMC [10]. Figure 1: Perturbation error on alternating k -star on syn-thetic graphs. Left: p = log( n ) /n , varying size of graph n . Right: n = 1000, varying  X  . Figure 2: Perturbation error for alternating k -triangle.
Left: p = log( n ) /n . Right: p = 0 . 1. Trend lines for LS are non-private.

When applying Algorithm 4 to real ERGM models, the key is correctly computing H 1 and H 2 . Everything in H independent of the privacy mechanism used for the model terms. In H 2 , the ratio of privacy distribution p (  X  y | x mechanism dependent. Here, we illustrate the cases for both Laplace mechanism and local sensitivity bounding al-gorithms.
 Example 4.1 (Laplace mechanism) . If Laplace mechanism is applied on a model term f , and  X  y , and GS are private statistic, privacy parameter and global sensitivities respec-tively, p (  X  y | x ) is then: Assume we use a symmetric proposal distribution for  X  , i.e., p (  X  0 |  X  ) = p (  X  |  X  0 ). With Algorithm 4, ratio H be written as (after taking logarithm) Example 4.2 (Local sensitivity bounding) . Assume a sin-gle model term and first-order local sensitivity bounding (multiple model terms and second order can be adjusted accordingly), and privacy parameter and  X  is the input for Algorithm 1. Let a = ln(1 / X  ) / .

In the process of MCMC, for current sampled graph x , we write l 1 as the local sensitivity on x and g 1 as the global sensitivity of local sensitivity. The first-order local sensitiv-ity bounding (Algorithm 1) returns  X  y,  X  y 1 for the observed graph. p (  X  y,  X  y 1 | x ) can be represented as follows by omitting terms that will be cancelled out later in calculating p (  X  y,  X  y sion of local sensitivity  X  y 1 , but also more statistics from the sampled graph in each iteration of MCMC (local sensitiv-ity l 1 ). Recall in the standard estimation, none of them are incorporated in the process. In the next section, we em-pirically show that such extra information can benefit the estimation. As in the example above, assume a symmetric proposal distribution. With Algorithm 4, ratio H 1 is the same as (17). H 2 is: log H 2 = (  X   X   X  0 )  X  f ( x 0 )  X  f ( x ) + pled  X  to represent the marginal distribution on  X  . A straight-forward way to generate a single instance of estimated  X  is to calculate the average of those samples. However, in prac-tice, we found that marginal maximum a posterior (MMAP) could give analysts better estimates instead. Formally, MMAP of  X  is defined as argmax reusing the samples of  X  from Algorithm 4, and performing approximate MMAP estimation by histogram or density es-timation. More sophisticated solutions will require further expanding (12) before MCMC sampling [6, 16].
Our evaluation has two goals. First we assess the per-turbation error of our privacy mechanisms, particularly the Laplace mechanism on alternating k -star and the local sen-sitivity bounding algorithms on alternating k -triangle and k -twopath. Second, we evaluate the ERGM parameter es-timation with private statistics using different approaches proposed in Section 4. All our experiments are run on Linux servers with Intel Xeon CPU and 8GB memory. In the ex-periments, we differ privacy parameters and  X  . Note that, whenever we clarify a value for or  X  , it always means the overall privacy budget of the entire perturbation process.
Our datasets include synthetic and real graphs. Synthetic graphs are generated using a random graph model, G ( n,p ), where parameters n and p control the size of graph and the probability of two nodes connecting, respectively. We iterate from n = 100 to n = 1000 in steps of 100. p is set to log( n ) /n for relatively sparse graphs and then moved to 0 . 1 and higher. Though we only report the sparse case and p = 0 . 1, results for larger p generally agree with the conclusions. Error measurement is root mean square error (RMSE).
 Alternating k -star As described in Section 3.1, we can apply the Laplace mech-Relative RMSE anism (LAP) directly or compute the degree distribution privately first, by isotonic regression (ISO) from [13] and use it as a sufficient statistic for alternating k -star. Figure 1 shows the error of the two methods by varying p and  X  , with different settings of = 1 , 0 . 1, listed in the legend text. As we do not have analytical RMSE for the ISO case, it is cal-culated from 100 independent perturbations. We clearly see LAP significantly outperforms ISO, even when  X  = 10 at both settings (and recall that the global sensitivity is 2  X  ). For the rest of this section, if not stated, we set  X  = 2 as it is the value normally recommended [21] and usually plays a minor part in the workflow.
 Alternating k -triangle The first-order local sensitivity bounding algorithm is ap-plied here while setting to 1 and 0.1 and fixing  X  = 0 . 01. In Figure 2, we use  X  X SB X  to represent Algorithm 1. For comparison purposes, we plot the non-private noisy output resulting from adding Laplace noise based on true local sen-sitivity, marked as  X  X S X  in Figure 2. We find that LSB can add modest error when compared to this non-private base-line, especially when the privacy budget is relatively large. Alternating k -twopath We discussed in Section 3.3 how a first-order or second-order local sensitivity bound can be applied to alternating k -twopath. We present these results in Figure 3, by distin-guishing them as  X 1-LSB X  and  X 2-LSB X . We find that for our test cases with random graphs, 1-LSB is consistantly better than 2-LSB, illustrated by RMSE in the left two subfigures. Referring to Sec. 3.3, recall that in 1-LSB we bound (8) while in 2-LSB we bound eqrefeq:ktwop1 by using bounded (9). If (7) and (9) are not small enough compared to (8), the fact that we split the budget of privacy one more time Table 1: Real networks for ERGM parameter estimation will outweigh the gain. In the right two subfigures of Fig-ure 3, we plot the true local sensitivity and the expected values of private, bounded local sensitivity for both LSB al-gorithms. We see that 1-LSB results in a bound that is close to the true value but that 2-LSB results in a significant over-estimate, especially with a smaller = 0 . 1. Although 1-LSB is superior across our tested networks, it remains possible that 2-LSB could outperform 1-LSB for some input graphs or large and  X  .
 Real graphs For real graphs, we consider several collected networks from the SNAP collection 2 to determine if our alternating statis-tics can be perturbed in a  X  X eaningful X  way, i.e., small rela-tive noise that doesn X  X  destroy utility. Our metric is relative RMSE, which is RMSE divided by the true statistic. As shown in Figure 4, with = 0 . 1, all three alternating statis-tics (with shortened names: astar, atri, atwop) are estimated with low relative error. In particular, error for alternating k -star is between 10  X  3 and 10  X  4 , alternating k -triangle at 10  X  1 and alternating k -twopath at 10  X  2 .
For the evaluation of ERGM parameter estimation, we want to compare the algorithms in Section 4. In practice, the data owner will only perturb each statistic once and then re-lease it to the analysts. As the perturbation is a randomized process, our goal is to understand how good our estimation algorithm is on average . So for each graph and each model description, we perturb the statistics N = 50 times and run the estimation algorithm on each perturbation, finally http://snap.stanford.edu measuring their quality by RMSE with respect to estimates the  X  X rue X  value, calculated from the non-private estimation algorithm from [15] or [3],  X   X  i is  X  from i -th perturbation.
As mentioned in [3], the estimation using the Bayesian technique has general scalability issues, where it becomes very slow for any graphs beyond a few hundred of nodes. Moreover such time cost also varies with the model terms, e.g., alternating k -twopath takes much more time than the other two alternating statistics, as calculation of the accep-tance ratio in MCMC sampling of x  X  p ( x |  X  ) is more com-plicated. Therefore, here we focus on smaller graphs, and this is the common practice for many ERGM works such as [3, 5, 21]. Our test networks 3 include dol phins, les mis, pol books, adj noun and foo tball. Detailed facts are listed in Table 1. We fix = 0 . 5 and  X  = 0 . 01.

We experimented with three models, each of which corre-sponds to one of the alternating statistics, with the purpose of testing estimation by isolating other factors. We include the count of edges as a shared term in all models, as it is very common in ERGM applications. As shown in Table 2, each model contains two terms, with correspondingly two parameters,  X  = (  X  1 , X  2 ). The estimation algorithms will be standard estimation (STD) and Bayesian inference (BINF). In all cases, the privacy budget is distributed evenly in a way such that each generation of noise uses same share of the overall . In Figure 5, each graph is represented with 4 bars, showing  X  1 of STD,  X  1 of BINF,  X  2 of STD,  X  BINF. In M1 and M2, we see a significant improvement of  X  from STD to BINF. Especially in M2, BINF limits all errors to around 5 or smaller where STD can go up much higher. We believe this is because BINF can utilize the ex-tra information presented by the local sensitivity bound as shown in Example 4.2. In M3, compared to the other mod-els, we see that parameters of the model is quite insensitive to the changes due to perturbation, i.e., all graphs show much lower errors even under STD. In such situation, the-oretically, there is not much room left for the improvement from BINF. This is illustrated in our experiment by showing comparable performance from both methods on M3. In gen-eral, we think BINF can improve the accuracy of parameter estimation significantly by leveraging the privacy distribu-tion, while at the same time, the amount of benefit will vary depending on intrinsic properties of the model. http://www-personal.umich.edu/  X mejn/netdata/
Differential privacy [9] has been actively studied in many sub-areas of computer science. Although the original focus was mainly on tabular data, the definition can be adapted to graph data [13] as well as other data models. Most research into differentially private analysis of graphs has focused on releasing graph statistics, e.g., degree sequence [13], trian-gle/star [17, 25], joint degree distribution/assortativity [26, 28] and clustering coefficient [31]. For modeling graphs pri-vately, we are aware only of a private Kronecker graph mod-eling approach under differential privacy [22]. While our work relies on obtaining good private statistics, the ultimate goal is to allow ERGM modeling under differential privacy.
All of these works, including ours, protect relationships, i.e. they support edge-differential privacy. A stronger stan-dard is to protect individuals, where neighbors are defined by changing a single node. Recently, researchers have devel-oped some mechanisms for calculating private graph statis-tics under node differential privacy [18, 2, 4].

Parameter estimation for ERGMs has also evolved from pseudo likelihood estimation (MPLE) [1], to Monte Carlo maximum likelihood (MC-MLE) [11] to recent stochastic ap-proximation [29] and Bayesian inference [3]. These advances have helped ERGMs become central to social network anal-ysis with many successful applications [21].
In this work, we consider the problem of estimating pa-rameters for the exponential random graph model under dif-ferential privacy. Our solution decomposes the process into two steps: releasing private statistics first and running esti-mation second. Our local sensitivity-based algorithms can offer lower error than common baselines. The redesigned Bayesian parameter estimation is flexible and more accurate than standard methods. For future work, improving scala-bility is an important direction as well exploring alternative model terms.
This material is based on work supported by the NSF through awards CNS-1012748 and IIS-0964094. [1] J. Besag. Spatial interaction and the statistical [2] J. Blocki, A. Blum, A. Datta, and O. Sheffet.
 [3] A. Caimo and N. Friel. Bayesian inference for [4] S. Chen and S. Zhou. Recursive mechanism: towards [5] S. J. Cranmer and B. A. Desmarais. Inferential [6] A. Doucet, S. J. Godsill, and C. P. Robert. Marginal [7] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, [8] C. Dwork and J. Lei. Differential privacy and robust [9] C. Dwork, F. McSherry, K. Nissim, and A. Smith. [10] D. Gamerman and H. F. Lopes. Markov chain Monte [11] C. J. Geyer and E. A. Thompson. Constrained monte [12] S. M. Goodreau, J. A. Kitts, and M. Morris. Birds of [13] M. Hay, C. Li, G. Miklau, and D. Jensen. Accurate [14] D. Hunter. Curved exponential family models for [15] D. Hunter and M. Handcock. Inference in curved [16] A. M. Johansen, A. Doucet, and M. Davy. Particle [17] V. Karwa, S. Raskhodnikova, A. Smith, and [18] S. P. Kasiviswanathan, K. Nissim, S. Raskhodnikova, [19] D. Kifer and B.-R. Lin. An axiomatic view of [20] J. Leskovec, D. Chakrabarti, J. Kleinberg, [21] D. Lusher, J. Koskinen, and G. Robins. Exponential [22] D. Mir and R. N. Wright. A differentially private [23] I. Murray, Z. Ghahramani, and D. MacKay. Mcmc for [24] M. Newman. Networks: an introduction . Oxford [25] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth [26] D. Proserpio, S. Goldberg, and F. McSherry.
 [27] G. Robins, T. Snijders, P. Wang, M. Handcock, and [28] A. Sala, X. Zhao, C. Wilson, H. Zheng, and B. Zhao. [29] T. Snijders. Markov chain monte carlo estimation of [30] T. A. Snijders, P. E. Pattison, G. L. Robins, and M. S. [31] Y. Wang, X. Wu, J. Zhu, and Y. Xiang. On learning [32] N. A. Weiss, P. T. Holmes, and M. Hardy. A course in
The proof relies on Lemma 4.4 from [17], and we restate it as follows: Lemma A.1. If M is ( 1 , X  1 ) -differentially private, and Pr[ M ( x )  X  LS f ( x )] &gt; 1  X   X  2 for all x , the following al-gorithm A , which returns a pair of values, is ( 1 + 2 , X  1 + e 1  X  2 ) -differentially private. Proof of Theorem 3.6. In Algorithm 1,  X  y 1 is -differentially private as it is based on Laplace mechanism and post-processing (adding positive offset). Let g 1 = GS ( LS f, 1 ( x )). If the sam-pled Laplace noise in line 2 is z , So, Pr[  X  y 1  X  LS f, 1 ( x )] &gt; 1  X   X / 2. By applying Lemma A.1, Algorithm 1 is ( + , 0 + e  X  2 )-differential privacy, which is (2 , 1 2 e  X  )-differential privacy.

In Algorithm 2, both  X  y 1 and  X  y 2 are -differentially pri-by applying Lemma A.1,  X  y 1 is (2 , 1 2 e  X  )-differential privacy. Furthermore, applying Lemma A.1 one more time, the fi-(3 , 1 2 e  X  + 1 2 e 2  X  )-differential privacy.
