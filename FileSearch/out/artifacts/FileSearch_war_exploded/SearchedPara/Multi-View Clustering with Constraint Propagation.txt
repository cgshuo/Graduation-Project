 Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated con-straints to be transferred across views via the partial map-ping. It uses co-EM to iteratively estimate the propaga-tion within each view based on the current clustering model, transfer the constraints across views, and update the clus-tering model, thereby learning a unified model for all views. We show that this approach significantly improves cluster-ing performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. H.3.3 [ Information Search and Retrieval ]: Clustering; I.2.6 [ Artificial Intelligence ]: Learning algorithms, experimentation constrained clustering, multi-view learning, semi-supervised learning This work was conducted while the first author was at Lockheed Martin Advanced Technology Laboratories and the University of Maryland Baltimore County.

Using multiple different views often has a synergistic ef-fect on learning, improving the performance of the result-ing model beyond learning from a single view. Multi-view learning is especially relevant to applications that simultane-ously collect data from different modalities, with each unique modality providing one or more views of the data. For ex-ample, a textual field report may have associated image and video content, and an Internet web page may contain both text and audio. Each view contains unique complementary information about an object; only in combination do they yield a complete representation of the original object. Con-cepts that are challenging to learn in one view (e.g., iden-tifying images of patrons at an Italian restaurant) may be easier to recognize in another view (e.g., via the associated textual caption), providing an avenue to improve learning. Multi-view learning can share learning progress in a single view to improve learning in the other views via the direct correspondences between views.

Current multi-view algorithms typically assume that there is a complete bipartite mapping between instances in the different views to represent these correspondences, denoting that each object is represented in all views. The predictions of a model in one view are transferred via this mapping to instances in the other views, providing additional labeled data to improve learning. However, what happens if we have only a partial mapping between the views, where only a limited number of objects have multi-view representations?
This problem arises in many industrial and military appli-cations, where data from different modalities are often col-lected, processed, and stored independently by specialized analysts. Consequently, the mapping between instances in the different views is incomplete. Even in situations where the connections between views are recorded, sensor avail-ability and scheduling may result in many isolated instances in the different views. Although it is feasible to identify a partial mapping between the views, the lack of a complete bi-partite mapping presents a challenge to most current multi-view learning methods. Without a complete mapping, these methods will be unable to transfer any information involving an isolated instance to the other views.

To address this problem, we propose a method for multi-view learning with an incomplete mapping in the context of constrained clustering. Constrained clustering [21, 5, 7, 24] is a class of semi-supervised learning methods that clus-ter data subject to a set of hard or soft constraints that Figure 1: An illustration of multi-view constrained clustering between two disjoint data views: text and images. We are given a very limited mapping be-tween the views (solid black lines) and a set of pair-wise constraints in the images view: two must-link constraints (thick solid green lines) and one cannot-link constraint (thick dashed red line). Based on the current clustering, each given constraint is propa-gated to pairs of images that are in close proximity to the given constraint and can be mapped to the text view. These propagated must-link and cannot-link constraints (thin solid green and dashed red lines, respectively) are then directly transferred via the mapping to form constraints between texts and influence the clustering in the next co-EM iteration. specify the relative cluster membership of pairs of instances. These constraints serve as background information for the clustering by specifying instance pairs that belong in either the same cluster (a must-link constraint) or different clus-ters (a cannot-link constraint). Given a set of constraints in each view, our approach transfers these constraints to affect learning in the other views. With a complete map-ping, each constraint has a direct correspondence in the other views, and therefore can be directly transferred be-tween views using current methods. However, with a partial mapping, these constraints may be between instances that do not have equivalences in the other views, presenting a challenge to multi-view learning, especially when the map-ping is very limited.

This paper proposes the first multi-view constrained clus-tering algorithm that considers the use of an incomplete mapping between views. Given an incomplete mapping, our approach propagates the given constraints within each view to pairs of instances that have equivalences in the other views. Since these propagated constraints involve only in-stances with a mapping to the other views, they can be directly transferred to instances in those other views and affect the clustering. The weight of each propagated con-straint is given by its similarity to the original constraint, as measured by a local radial basis weighting function that is based on the current estimate of the clustering. This pro-cess is depicted in Figure 1. Our approach uses a variant of co-EM [17] to iteratively estimate the propagation within each view, transfer the constraints across views, and update the clustering model. Our experiments show that using co-EM with constraint propagation provides an effective mech-anism for multi-view learning under an incomplete mapping between views, yielding significant improvement over several other mechanisms for transferring constraints across views.
Our approach combines constrained clustering with multi-view learning. In this section, we review the related work on both of these topics.
Constrained clustering algorithms [21, 5, 7, 24] learn a clustering model subject to a set of constraints C that spec-ify the relative cluster membership of sets of instances. De-pending on the algorithm, this labeled knowledge may be treated as either hard constraints that cannot be violated, as in COP-Kmeans [21], or soft constraints that can be vi-olated with some penalty, as in SCOP-Kmeans [22], PCK-Means [5], and MPCK-Means [7]. In this paper, we focus on soft constrained clustering, where each constraint specifies the relative cluster membership of pairs of points.
A pairwise constraint  X  x i ,x j ,w, type  X   X  C denotes the relative clustering of instances x i and x j , where the non-negative weight of the constraint is given by w  X  R + 0 (the set of non-negative real numbers) and type  X  { must -link , cannot -link } specifies whether x i and x j belong in either the same cluster (must-link) or different clusters (cannot-link). In soft constrained clustering, w can be viewed as the penalty for violating the constraint. Throughout this pa-per, wherever the weight or type of constraint are obvious from context, we will omit them and indicate a pairwise con-straint as simply  X  x i ,x j  X  or  X  x i ,x j ,w  X  . For convenience, we refer to the sets of all must-link and cannot-link constraints as, respectively, C ml and C cl .

Although our approach can use most current constrained clustering algorithms, we focus on the PCK-Means [5] and MPCK-Means [7] algorithms. PCK-Means performs soft constrained clustering by combining the K-Means objective function with penalties for constraint violations. MPCK-Means builds on PCK-Means to learn the distance metrics for each cluster during the clustering process. In the re-mainder of this section, we provide a brief overview of these methods; further details are available in the original papers.
We first describe the MPCK-Means algorithm, and then show the simplifications that yield PCK-Means. The MPCK-Means algorithm generates a k -partitioning of the data X  X  R d by minimizing the following objective function, which combines the K-Means model with penalties for violating must-link and cannot-link constraints: where  X  i and M x i are respectively the centroid and metric of the cluster to which x i belongs, x 0 x i and x 00 x i are the points with the greatest separation according to the M x i metric, the function 1 ( b ) = 1 if predicate b is true and 0 otherwise, and k x i  X  x j k M = p ( x i  X  x j ) T M ( x i  X  x j ) is the Mahalanobis distance between x i and x j using the metric M . The first term of J MPCK attempts to maximize the log-likelihood of the K-Means clustering, while the second and third terms incorporate the costs of violating constraints in C .
MPCK-Means uses expectation-maximization (EM) to lo-cally minimize J MPCK to generate the clustering. The E-step consists of assigning each point to the cluster that min-imizes J MPCK from the perspective of that data point, given the previous assignments of points to clusters. The M-step consists of two parts: re-estimating the cluster centroids given the E-step cluster assignments, and updating the met-ric matrices { M h } K h =1 to decrease J MPCK . The latter step enables MPCK-Means to learn the metrics for each cluster in combination with learning the constrained clustering model. Learning a Mahalanobis metric has also been considered by Xing et al. [24] and Bar-Hillel et al. [2]. The PCK-Means al-gorithm is a simplified form of this approach that minimizes the same objective function as MPCK-Means, but eliminates the metric learning aspect and assumes an identity distance metric, setting f ml ( x i ,x j ) = 1 and f cl ( x i ,x j
Multi-view learning was originated by Blum et al. in the co-training algorithm [8] for semi-supervised classification. Co-training uses the model for each view to incrementally label the unlabeled data. Labels that are predicted with high confidence are transferred to the corresponding unlabeled in-stances in the other views to improve learning, and the pro-cess iterates until all instances are labeled. Co-training as-sumes independence between the views, and shows decreased performance when this assumption is violated [17].
Nigam and Ghani [17] propose the co-EM algorithm as an iterative multi-view form of expectation-maximization. At each iteration, co-EM estimates the model for a view and uses it to probabilistically label all of the data; these labels are then transferred to train another view during the next iteration. Co-EM repeats this process until the models for all views converge. Unlike co-training, co-EM does not require the views to be independent in order to perform well. The approach we explore in this paper uses a variant of co-EM to iteratively infer constraints in each view, and transfer those constraints to affect learning in the other views.
Clustering with multiple views has previous been explored by Bickel and Scheffer [6], who developed a multi-view EM algorithm that alternates between the views used to learn the model parameters and estimate the cluster assignments. Multi-view clustering has also been studied using canoni-cal correlation analysis to construct low-dimensional embed-dings from multiple views [9], spectral clustering that mini-mizes the disagreement between views [12], cross-modal clus-tering between perceptual channels [11], and information-theoretic frameworks [19, 14, 20].
Our multi-view constrained clustering approach (described in the next section) takes as input multiple views of the data X = { X A ,X B ,... } . Each view V of the data is given by a set of instances X V = { x V 1 ,x V 2 ,...,x V n x fer between the views. We will initially focus on the case of two views, given by X A and X B , and extend our approach to handle an arbitrary number of views in Section 4.3.
Within X , there are pairs of instances that correspond to different views of the same objects. We denote this con-nection between two instances x A u and x B v in different views by a relation r i =  X  x A u ,x B v  X   X  X A  X  X B . The set of rela-tions R A  X  B = { r 1 ,r 2 ,... }  X  X A  X  X B defines a bipartite graph between X A and X B . Most other work on multi-view learning [17, 8, 6] assumes that R A  X  B defines a complete bipartite mapping between the two views. We broaden this assumption and consider the case where R A  X  B provides only a partial mapping between the views, and moreover when there are many more data instances than relations between views (i.e., |R A  X  B | &lt;&lt; | X A | + | X B | ).
We also have a set of pairwise must-link and cannot-link constraints for each view V , given by C V  X  X V  X  X R 0  X { must -link , cannot -link } . Depending on the applica-tion, these constraints may either be manually specified by the user or extracted automatically from labeled data. Note that the constraints describe relationships between instances within a single view, while the mapping R A  X  B defines con-nections between instances in different views.
Our multi-view constrained clustering approach takes as input multiple views of the data X = { X A ,X B ,... } , their associated sets of pairwise constraints C A , C B ,... , and a (po-tentially incomplete) mapping R U  X  V between each pair of different views U and V . Although we focus primarily on the case of two views A and B , we also generalize our ap-proach to multiple views, as described in Section 4.3. The objective of our approach is to determine a k -partitioning of the data for each view that respects both the constraints within each view and the mapping between the views.
Our approach, given as Algorithm 1, iteratively clusters each view, infers new constraints within each view, and transfers those inferred constraints across views via the map-ping. Through this process, progress in learning the model for one view will be rapidly transmitted to other views, mak-ing this approach particularly suited for problems where dif-ferent aspects of the model are easy to learn in one view but difficult to learn in others.
 The base constrained clustering algorithm is given by the CKmeans subfunction, which computes the clustering that maximizes the log-likelihood of the data X given the set of must-and cannot-link constraints C . Our implementation uses either the PCK-Means or MPCK-Means algorithms as the CKmeans subfunction due to their native support for soft constraints and, for MPCK-Means, metric learn-ing. However, our approach can utilize other constrained K-Means clustering algorithms, provided they meet the cri-teria for the CKmeans function listed above.

We fit the clustering model across both views using a vari-ant of the co-EM algorithm [17]. In the E-step, we propa-gate the set of given constraints based on the current clus-tering models to those instances (  X  X A and  X  X B ) with direct mappings to the other views (Step 11, further described in Section 4.1). These propagated constraints can then be di-rectly transferred to the other views via the mapping R A  X  B (Steps 4 X 9, further described in Section 4.2) to influence clustering during the M-step (Step 10). Note that instead of taking the direct union of all of the constraints, we keep only the maximally weighted constraint of each type (must-link and cannot-link) for every pair of instances; this operation is notated by the max S operator in Step 9. Figure 2: The relationship between the E-steps and M-steps in the different views.

Following previous work on co-EM and multi-view clus-tering [17, 6], we iterate the E-step in one view to propagate the constraints followed by the M-step in the other view to transfer those constraints and update the clustering. Each iteration of the co-EM loop (Steps 6 X 13) contains two it-erations of both the E-step and the M-step, one for each view. The relationship between these steps is illustrated in Figure 2. The co-EM process continues until each view has internally converged. We assume convergence has oc-curred when the PCK-Means/MPCK-Means objective func-tion X  X  value differs by less than = 10  X  6 between successive iterations. Like Nigam and Ghani [17], we observed that our co-EM variant converged in very few iterations in prac-tice. The iterative exchange of constraints between the views ensures a consistent clustering that respects both the con-straints within and the mapping between views. The next two sections detail each step of the co-EM process.
In our model, the sets of pairwise constraints are the sole mechanisms for guiding the resulting clustering. We can directly map a constraint  X  x u ,x v  X  between views only if the mapping is defined in R A  X  B for both endpoints x u and x of the constraint. When R A  X  B is incomplete, the number of constraints with such a direct mapping for both endpoints is likely to be small. Consequently, we will be unable to directly map many of the constraints between views; each constraint that we cannot map represents lost information that may have improved the clustering.

Let  X  X V  X  X V be the set of instances for view V that are mapped to another view. Given the initial constraints in C V , we infer new constraints between pairs of instances in  X 
X V based on their local similarity to constraints in C V We define this local similarity metric based on the current clustering model for view V , and propagate a constraint  X  x u ,x V v  X  X  X C V to a pair of points x V i ,x V j  X   X  X V if the pair is sufficiently similar to the original constraint. This process essentially considers these as spatial constraints [15] that af-fect not only the endpoints, but local neighborhoods of the instance space around those endpoints. Any effect on a pair of points in the neighborhood can be realized as a weighted constraint between those instances. Our constraint prop-agation method infers these constraints between instances in  X 
X V with respect to the current clustering model. Since this set of new constraints (which we refer to as propagated constraints ) is between instances with a direct mapping to other views, these constraints can be directly transferred to those other views via the mapping R A  X  B . This approach can also be interpreted as inferring two weighted must-link closure of them with  X  x V u ,x V v  X  to obtain  X  x V i ,x Algorithm 1 Multi-view Constrained Clustering with Con-straint Propagation Input: first view X A and constraints C A , 1: Compute the transitive closure of C A S C B S R A  X  B 2: Augment C A , C B , and R A  X  B with additional constraints 3: Let  X  X A  X  X A be the set of instances from X A involved 4: Define constraint mapping functions f A 7 X  B and f 5: Initialize the sets of propagated constraints P V =  X  for 6: repeat 7: for V  X  X  A,B } do 8: Let U denote the opposite view from V . 9: Define the unified set of constraints, mapped with 10: Update the clustering using constrained K-Means: 11: Estimate the set of propagated constraints: 12: end for 13: until P A and P B have both internally converged Output: the clustering P A and P B .
 Subfunction: ( P, M ) = CKmeans ( X, C ,k ) Function prototype for constrained K-Means.
 Input: data X , must-link and cannot-link constraints C , Output: the clustering P and set of metrics for each
The propagation process occurs with respect to the cur-rent clustering model for view V . Since we use K-Means variants as the base learning algorithm, the learned model is essentially equivalent to a Gaussian mixture model, under particular assumptions of uniform mixture priors and con-ditional distributions based on the set of constraints [7, 4]. Therefore, we can consider that each cluster h is generated by a Gaussian with a covariance matrix  X  h . For base clus-tering algorithms that support metric learning (e.g., MPCK-Means), the cluster covariance is related to the inverse of the cluster metric M h learned as part of the clustering process. Bar-Hillel et al. [2] note that, in practice, metric learning typically constructs the metric modulo a scale factor  X  Although this scale factor does not affect clustering, since only relative distances are required, constraint propagation requires absolute distances. Therefore, we must rescale the learned covariance matrix M  X  1 h by  X  h to match the data.
We compute  X  h based on the empirical covariance  X   X  h of the data P h  X  X V assigned to cluster h , given by adding a small amount of regularization  X  I to ensure that  X   X  h is non-singular for small data samples. Given M  X  1 h and  X   X  h , we compute  X  h as the scale such that the variances of the first principal component of each matrix are identical. We take the eigendecomposition of each matrix to yield diagonal matrices of eigenvalues in  X  M  X  1 To derive the scale factor  X  h , we ensure that both first prin-cipal components have equal variances, which occurs when yielding  X  h =  X  h M  X  1 h as the covariance matrix for cluster h . When the base learning algorithm does not support metric learning, such as PCK-Means, we can instead use  X  h =  X   X  as cluster h  X  X  covariance matrix. The model for cluster h is then given by where x V  X   X  V h 2  X  squared Mahalanobis distance between x V and  X  V h according to the cluster X  X  rescaled metric  X  h  X  1 .

We assume that each constraint should be propagated with respect to the current clustering model, with the shape (i.e., covariance) of the propagation equivalent to the shape of the respective clusters (as given by their covariance matri-ces). Additionally, we assume that the propagation distance should be proportional to the constraint X  X  location in the cluster. Intuitively, a constraint located near the center of a cluster can be propagated a far distance, up to the clus-ter X  X  edges, since being located near the center of the cluster implies that the model has high confidence in the relation-ship depicted by the constraint. Similarly, a constraint lo-cated near the edges of a cluster should only be propagated a short distance, since the relative cluster membership of these points is less certain at the cluster X  X  fringe.
We propagate a given constraint  X  x V u ,x V v  X   X  C V other points x V i ,x V j  X  X V according to a Gaussian radial basis function (RBF) of the distance as  X  x V i ,x V j  X  moves away from  X  x V u ,x V v  X  . Under this construction, the weight of the propagated constraint decreases according to the RBF cen-tered in 2 d V -dimensional space at the original constraint X  X  based on the respective clusters X  covariance matrices.
To form the propagation covariance matrices for each end-point, we scale the covariance matrix associated with end-point x V u by the weight assigned to that endpoint according to the clustering model (Equation 4). This ensures that the amount of propagation falls off with increasing distance from the centroid, in direct relation to the model X  X  confidence in the cluster membership of x V u . The covariance matrix for the constraint propagation function is then given by where c u denotes the cluster of x u and 0 denotes the d V zero matrix. This construction assumes independence be-tween x V u and x V v . While this assumption is likely to be violated in practice, we empirically show that it yields good results. For convenience, we represent the covariance matri-ces associated with each endpoint by  X  x V the results of this process on an example cluster.
Given a constraint  X  x V u ,x V v ,w, type  X  X  X C V and two candi-date points x V i  X  X V and x V j  X  X V , we can now estimate the weight of the propagated constraint  X  x V i ,x V j  X  as where Since the ordering of the instances matters in the propaga-tion, we compute both possible pairings of constraint end-points ( x V u and x V v ) to target endpoints ( x V i and x the maximum value of the propagation in Equation 6 to de-termine the best match. Under this propagation scheme, a constraint propagated to its own endpoints is given a weight of w (since the second term of the RHS of Equa-tion 6 will be 1); the weight of the propagated constraint decreases as the endpoints x V i and x V j move farther from x and x V v . Section 4.4 describes mechanisms for implementing constraint propagation efficiently, taking advantage of the independence assumption between the two endpoints of a constraint and memoization of repeated computations.
The E-step of Algorithm 1 (Step 11) uses Equation 6 to propagate all given constraints within each view to those instances  X  X V with cross-view mappings, thereby inferring the expected value of constraints between those instances given the current clustering. Using this expected set of con-straints, we can then update the current clustering model in the M-step as described in the next section.
Given the expected constraints between instances in  X  X V we transfer those constraints to the other views and then update the clustering model to reflect these new constraints. These steps together constitute the M-step of Algorithm 1.
Any propagated constraint where both endpoints are in  X  X
V can be transferred directly to another view U via the bipartite mapping R V  X  U . We define a mapping function f V 7 X  U : X V  X  X V  X  R + 0  X { must -link,cannot -link }7 X  X X straint c =  X  x V i ,x V j ,w, type  X   X  C V and maps it to constrain Figure 3: Constraint propagation applied to a sin-gle example cluster h , showing the learned covari-ance matrix  X  h (dashed blue ellipse) rescaled to fit the data, two constraints (solid black lines) and the weighting functions centered at each endpoint (dot-ted green ellipses), which decrease in variance as they move farther from the centroid  X  h . instances in X U by:
Using this construction, we can define the mapping func-tions f B 7 X  A and f A 7 X  B in Algorithm 1. We then use these functions f A 7 X  B and f B 7 X  A to map propagated constraints between views in Step 9, transferring constraints inferred in one view to the other related views. These transferred constraints (from view U ) can then be combined with the original constraints in each view V to inform the clustering. Instead of taking the direct union of these constraints, we keep only the maximally weighted constraint between each pair of instances to form the set since each inferred constraint represents an estimate of the minimal strength of the pairwise relationship.

The optimal clustering models for view V can then be computed by clustering the data in each view subject to the constraints in  X  C V (Step 10). The CKmeans subfunction computes the clustering that maximizes the log-likelihood of the data subject to the set of constraints, thereby completing the M-step of Algorithm 1.
Algorithm 1 can be easily extended to support more than two views. Each view X V independently maintains its own sets of given constraints C V , threshold t V , data  X  X V involved in any cross-view relations, current partitioning P V , current cluster metrics M V , and propagated constraints P
V . To handle more than two views, we maintain separate mappings R U  X  V for each pair of views X U and X V and use each mapping to define pairwise mapping functions f U 7 X  V and f V 7 X  U between views. For D views, X (1) ,...,X ( D ) this approach will yield D 2  X  D mapping functions.
To generalize our approach to more than two views, we hold each set of propagated constraints fixed, and iteratively update the clustering (M-step), then recompute the set of propagated constraints (E-step) for one view. The unified sets of constraints for each view V becomes (Step 9) under the convention that f U 7 X  U ( P U ) =  X  . Each iteration of co-EM loops over the E-steps and M-steps for all views, and proceeds until the clustering for each view converges.
The overall computational complexity of Algorithm 1 is limited by the maximum number of EM iterations and the complexity of the CKMeans function, which depends on the chosen clustering algorithm. Besides these aspects, the con-straint propagation step (Step 11) incurs the greatest com-putational cost. To make this step computationally efficient, our implementation relies on the independence assumption inherent in Equation 7 between the two endpoints of the con-straint. To efficiently compute the weight of all propagated constraints, we memoize the value of each endpoint X  X  prop-agation G ( x V i ,x V u ) = exp  X  1 2 ( x V i  X  x V u ) for x V i  X   X  X V and x V u  X   X  X V , where  X  X V is the set of points involved in C V . Through memoization, we reduce the con-straint propagation step to |  X  X V |  X  |  X  X V | Gaussian evalu-ations. Memoization applies similarly to all other views. Each constraint propagation is inherently independent from the others, making this approach suitable for parallel imple-mentation using Hadoop/MapReduce [13].

When the covariance matrix  X  x V further reduce the computational cost through early stop-ping of the Gaussian evaluation once we are certain that the endpoint X  X  propagation weight will be below the given threshold t V . When  X  x V diag (  X  2 1 , X  2 2 ,..., X  2 d V ),
Since a constraint is only propagated when the weight exceeds t V &gt; 0 and the maximum propagation for each Gaussian weight G ( x V i ,x V u )  X  [0 , 1], we only need to evalu-ate W 0  X  x V i ,x V j  X  ,  X  x V u ,x V v  X  when both G ( x G ( x V j ,x V v )  X  t V . Therefore we must ensure that Since all terms in the RHS summation are positive, we can compute them incrementally and stop early once the sum exceeds  X  2 ln t V , since we will never need to evaluate any propagation weight W 0 (  X  ) involving G ( x V i ,x V u ). In our im-plementation, we set G ( x V i ,x V u ) = 0 in any cases where we can guarantee that G ( x V i ,x V u ) &lt; t V .
We evaluated multi-view constrained clustering on a vari-ety of data sets, both synthetic and real, showing that our approach improves multi-view learning under an incomplete mapping as compared to several other methods.
In order to examine the performance of our approach under various data distributions, we use a combination of synthetic and real data in our experiments. We follow the methodology of Nigam and Ghani [17] to create these multi-view data sets by pairing classes together to create  X  X uper-instances X  consisting of one instance from each class in the pair. The two original instances then represent two differ-ent views of the super-instance, and their connection forms a mapping between the views. This methodology can be trivially extended to an arbitrary number of views. These data sets are described below and summarized in Table 1. Four quadrants is a synthetic data set composed of 200 instances drawn from four Gaussians in R 2 space with identity covariance. The Gaussians are centered at the coordinates (  X  3 ,  X  3), one in each of the four quadrants.
Quadrants I and IV belong to the same cluster and quadrants II and III belong to the same cluster. The challenge in this simple data set is to identify these clus-ters automatically, which requires the use of constraints to improve performance beyond random chance. To form the two views, we drew 50 instances from each of the four Gaussians, divided them evenly between views, and created mappings between nearest neighbors that were in the same quadrant but different views.
 Protein includes 116 instances divided among six classes of proteins { c 1 ,c 2 ,...,c 6 } . This data set was previously used by Xing et al. [24]. To create multiple views of this data set, we partition it into two views contain-ing respectively instances from classes { c 1 ,c 2 ,c { c 4 ,c 5 ,c 6 } . We connected instances between the fol-lowing pairs of classes to create the two views: c 1 &amp; c c 2 &amp; c 5 , and c 3 &amp; c 6 . Through this construction, a model learned for clustering { c 1 ,c 2 ,c 3 } in one view can be used to inform the clustering of { c 4 ,c 5 ,c 6 other view. Since the clusters do not contain the same numbers of instances, some instances within each view are isolated in the mapping.
 Letters/Digits uses the letters-IJL and digits-389 data sets previously used by Bilenko et al. [7]. These are subsets of the letters and digits data sets from the UCI machine learning repository [1] containing only the let-ters { I,J,L } and the digits { 3 , 8 , 9 } , respectively. We map instances between views according to the follow-ing pairings: I &amp; 3, J &amp; 8, and L &amp; 9, leaving those instances without a correspondence in the other view isolated in the mapping.
 Rec/Talk is a subset of the 20 Newsgroups data set [18], containing 5% of the instances from the newsgroups { rec.autos , rec.motorcycles } in the rec view, and 5% of the newsgroups { talk.politics.guns , talk.politics.mideast } in the talk view. We process each view independently, removing stop words and representing the data as a bi-nary vector of the 50 most discriminatory words as de-termined by Weka X  X  string-to-wordvector filter [23]. As in the previous data sets, we form the mapping between views by pairing clusters in order.

We create a low-dimensional embedding of each data set using the spectral features [16] in order to improve clus-tering, with the exception of Four Quadrants, for which we use the original features because the dimensionality is already low. For each view V , we compute the pairwise affinity matrix A between the instances x i and x j using a radial basis function of their distance, given by A exp(  X  X | x i  X  x j || 2 / 2  X  2 ). We use  X  = 1 as the rate at which the affinity falls off with increasing distance. From A , we form the normalized Laplacian matrix [10] for the data set, given by L = I  X  D  X  1 2 AD  X  1 2 , where D is the diagonal degree matrix D i,i = P d V j =1 A i,j and I is the identity ma-trix. The eigendecomposition of the normalized Laplacian matrix L = Q X Q T yields the spectral features for the data set in the columns of the eigenvector matrix Q . We keep the 2 nd through d + 1 th eigenvectors (corresponding to the 2 nd through d + 1 th lowest eigenvalues in  X  ) as the fea-tures for clustering; we discard the first eigenvector since it is constant and therefore does not discriminate between the instances. In this paper, we use d = Letters/Digits, and d = 5 for the Rec/Talk data set. Addi-tionally, we standardize all features to have zero mean and unit variance. These spectral features are computed inde-pendently between the different views, further emphasizing that the mapping is the only connection between views.
Within each view, we use the cluster labels on the in-stances to sample a set of pairwise constraints, ensuring equal proportions of must-link and cannot-link constraints. The weight of all constraints w is set to 1. We also sample a portion of the mapping to use for transferring constraints between views. Both the sets of constraints and the mapping between views are resampled each trial of our experiments.
We compare Constraint Propagation against several other potential methods for transferring constraints: Direct Mapping transfers only those constraints that already exist between instances in  X  X V . This approach is equivalent to other methods for multi-view learning that are only capable of transferring labeled information if there is a direct mapping between views.
 Cluster Membership can be used to infer constraints between instances in  X  X V . This approach simply con-siders the relative cluster membership for each pair of instances in  X  X V and infers the appropriate type of con-straint with a weight of 1.
 Single View performs constrained clustering on each of the individual views in isolation and serves as a lower baseline for the experiments.
 For the base constrained clustering algorithm, we use the PCK-Means and MPCK-Means implementations provided in the WekaUT extension 1 to the Weka toolkit [23] with their default values. For PCK-Means, Constraint Propaga-tion uses the full empirical covariance for each cluster; for MPCK-Means, it uses the diagonal weighted Euclidean met-rics learned on a per-cluster basis by MPCK-Means. http://www.cs.utexas.edu/users/ml/risc/code/ methods as the other plots. (Best viewed in color.)
We measure performance using the pairwise F-measure  X  a version of the information-theoretic F-measure adapted to measure the number of same-cluster pairs for clustering [3]. The pairwise F-measure is the harmonic mean of precision and recall, given by
F-measure = 2  X  precision  X  recall where precision = num-pairs-correctly-predicted-in-same-cluster We take the mean of the performance for all views, yielding a single performance measure for each experiment.

In each trial, we consider performance as we vary the num-ber of constraints used for learning and the percentage of instances in each view that are mapped to the other views. Our results are shown in Figure 4, averaged over 100 trials.
As shown in Figure 4, Constraint Propagation clearly per-forms better than the baseline of Single View clustering, and better than Cluster Membership for inferring constraints in all cases, except for when learning with few constraints on Letters/Digits. Constraint Propagation also yields an im-provement over transfer using the Direct Mapping method for each percentage of instances mapped between views, as shown in Figure 5. We omit the Four Quadrants (PCK-Means) results from Figure 5 due to the relatively high per-Figure 5: The performance improvement of con-straint propagation over direct mapping in Figure 4, averaged over the learning curve. The peak whiskers depict the maximum percentage improvement. formance gain of Constraint Propagation, which averages a 21.3% improvement over Direct Mapping with peak gains above 30%. Unlike Direct Mapping, Constraint Propagation Figure 6: The number of propagated constraints as a function of the number of original constraints, aver-aged over all co-EM iterations. The line type (solid, dashed, dotted) depicts the mapping percentage, as defined in the corresponding plot in Figure 4. Figure 7: The precision of the propagated must-link (solid line) and cannot-link (dashed line) con-straints, as measured against the true class labels. is able to transfer those constraints that would otherwise be discarded, increasing the performance of multi-view cluster-ing. The performance of both Constraint Propagation and Direct Mapping improve as the mapping becomes more com-plete between the views, with Constraint Propagation still retaining an advantage over Direct Mapping even with a complete mapping, as shown in all data sets. We hypothesize that in the case of a complete mapping, Constraint Propa-gation behaves similarly to spatial constraints [15], warping the underlying space with the inference of new constraints that improve performance.
 On these data sets, the number of constraints inferred by Constraint Propagation is approximately linear in the num-ber of original constraints, as shown in Figure 6. Clearly, as the mapping between views becomes more complete, Con-straint Propagation is able to infer a larger number of con-straints between those instances in  X  X V .

The improvement in clustering performance is due to the high precision of the propagated constraints. Figure 7 shows the average weighted precision of the propagated constraints for the 100% mapping case, measured against the complete set of pairwise constraints that can be inferred from the true class labels. The proportion that each propagated con-straint contributed to the weighted precision is given by the constraint X  X  inferred weight w . We also measured the preci-sion of propagated constraints for various partial mappings, and the results were comparable to those for the complete mapping. For each data set, the constraint inferred through propagation show a high average precision of 98 X 100%, sig-nifying that very few incorrect constraints are inferred by the propagation method.

Interestingly, the constraint propagation method works slightly better for cannot-link constraints than must-link constraints. This phenomenon can be explained by a count-ing argument that there are many more chances for a cannot-link constraint to be correctly propagated than a must-link constraint. For example, with k clusters where each clus-ter contains n/k instances, each given must-link constraint can be correctly propagated to numMLprop = n/k 2  X  1 other pairs of instances in the same cluster. However, each given cannot-link constraint can be correctly propagated to numCLprop = n 2  X  k n/k 2  X  1 other pairs of instances that belong in different clusters, 2 which is much greater than numMLprop (e.g., for n = 1 , 000 and k = 10, numCLprop = 449 , 999 4 , 949 = numMLprop ). Therefore, a must-link constraint has much less chance of being propagated cor-rectly than a cannot-link constraint.

We found that for high-dimensional data, the curse of di-mensionality causes instances to be so far separated that Constraint Propagation is only able to infer constraints with a very low weight. Consequently, it works best with a low-dimensional embedding of the data, motivating our use of spectral feature reduction. Other approaches could also be used for creating the low-dimensional embedding, such as principal components analysis or manifold learning. Additionally, like other multi-view algorithms, we found Constraint Propagation to be somewhat sensitive to the cut-off thresholds t V , but this problem can be remedied by using cross-validation to choose t V . Too high a thresh-old yields performance identical to Direct Mapping (since no constraints would be inferred), while too low a thresh-old yields the same decreased performance as exhibited by other co-training algorithms. For this reason, we recom-mend setting t V to optimize performance as evaluated by cross-validation over the set of constrained instances.
We ran several additional experiments on data sets with poor mappings and distributions that violated the mixture-of-Gaussians assumption of K-Means clustering; we omit these results due to space limitations. On these data sets, Constraint Propagation decreased performance in some cases, due to inferring constraints that were not justified by the data. This would occur, for example, in clusters with a nested half-moon shape, where Constraint Propagation would incorrectly infer constraints between instances in the oppos-ing cluster. In these cases, clustering using only the directly mapped constraints yielded the best performance.
The number of propagated cannot-link constraints is de-rived by taking the number of possible different constraints 2  X  1, and subtracting off the total number of possible must-link constraints k n/k 2 .
Constraint propagation has the ability to improve multi-view constrained clustering when the mapping between views is incomplete. Besides improving performance, constraint propagation also provides the ability for the user to inter-act with one view to supply constraints, and for those con-straints to be transferred to improve learning in the other views. This is especially beneficial when the interaction is more natural for the user in one view (e.g., images) than the other views (e.g., text or audio) that may require lengthy ex-amination of each instance in order to infer the constraints.
In future work, we will consider other methods for learning with a partial mapping between views, such as label infer-ence, manifold alignment, and transfer learning. This work will improve the ability to use isolated instances that do not have a corresponding multi-view representation to improve learning, and enable multi-view learning methods to be used for a wider variety of applications We would like to thank Kiri Wagstaff, Katherine Guo, Tim Oates, and Tim Finin for their feedback. This work is partly based on the first author X  X  Master X  X  thesis at UMBC. This research was supported in part by a graduate fellowship from the Goddard Earth Sciences and Technology Center at UMBC, NSF ITR grant #0325329, and Lockheed Martin. [1] A. Asuncion and D. Newman. UCI machine learning [2] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. [3] S. Basu. Semi-Supervised Clustering: Probabilistic [4] S. Basu, A. Banerjee, and R. J. Mooney.
 [5] S. Basu, A. Banerjee, and R. J. Mooney. Active [6] S. Bickel and T. Scheffer. Multi-view clustering. In [7] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [8] A. Blum and T. Mitchell. Combining labeled and [9] K. Chaudhuri, S. M. Kakade, K. Livescu, and [10] F. R. K. Chung. Spectral Graph Theory . Number 92 in [11] M. H. Coen. Cross-modal clustering. In Proceedings of [12] V. R. de Sa. Spectral clustering with two views. In [13] J. Dean and S. Ghemawat. MapReduce: simplified [14] Y. Gao, S. Gu, J. Li, and Z. Liao. The multi-view [15] D. Klein, S. D. Kamvar, and C. D. Manning. From [16] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [17] K. Nigam and R. Ghani. Analyzing the effectiveness [18] J. Rennie. 20 Newsgroups data set, sorted by date. [19] K. Sridharan and S. M. Kakade. An information [20] W. Tang, Z. Lu, and I. S. Dhillon. Clustering with [21] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [22] K. L. Wagstaff. Intelligent Clustering with [23] I. H. Witten and E. Frank. Data Mining: Practical [24] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell.
