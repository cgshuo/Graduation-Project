 Typical pseudo-relevance feedback methods assume the top-retrieved documents are relevant and use these pseudo-relevant documents to expand terms. The initial retrieval set can, however, contain a great deal of noise. In this paper, we present a cluster-based resampling method to se lect better pseudo-relevant documents based on the relevance m odel. The main idea is to use document clusters to find dominant documents for the initial retrieval set, and to repeatedly feed the documents to emphasize the core topics of a query. Experimental results on large-scale web TREC collections show si gnificant improvements over the relevance model. For justificati on of the resampling approach, we examine relevance density of feedback documents. A higher relevance density will result in greater retrieval accuracy, ultimately approaching true relevance feedback. The resampling approach shows higher relevance density than the baseline relevance model on all collections, resulting in better retrieval accuracy in pseudo-relevance feedb ack. This result indicates that the proposed method is effective for pseudo-relevance feedback. H.3.3 [ Information Storage &amp; Retrieval ]: Relevance Feedback Algorithms, Experimentation Information retrieval, pseudo-relevance feedback, a cluster-based resampling, dominant documen ts, query expansion Most pseudo-relevance feedback methods (e.g., [15, 23, 22]) assume that a set of top-retrieve d documents is relevant and then learn from the pseudo-relevant doc uments to expand terms or to assign better weights to the original query. This is similar to the process used in relevance feedback, when actual relevant documents are used [26]. But in general, the top-retrieved documents contain noise: when the precision of the top 10 documents (P@10) is 0.5, 5 of them are non-relevant. This is common and even expected in a ll retrieval models. This noise, however, can result in the query representation  X  X rifting X  away from the original query. 
This paper describes a resampling method using clusters to select better documents for pse udo-relevance feedback. Document clusters for the initial retrieval se t can represent aspects of a query on especially large-scale web coll ections, since the initial retrieval results may involve diverse subtopi cs for such collections. Since groups for feedback. By permitting overlapped clusters for the top-retrieved documents and repeatedly feeding dominant documents that appear in multiple highly-ranked clusters, we expect that an expansion query can be represented to emphasize the core topics of a query. 
This is not the first time that clustering has been suggested as an improvement for relevance feedback. In fact, clustering was mentioned in some of the first work related to pseudo-relevance feedback [1]. Previous attempts to use clusters have not improved effectiveness. The work pres ented here is based on a new approach to using the clusters that produces significantly better results. 
Our motivation for using clusters and resampling is as follows: the top-retrieved documents are a query-oriented ordering that does not consider the relationshi p between documents. We view the pseudo-relevance feedback problem of learning expansion terms closely related to a query to be similar to the classification problem of learning an accurate decision boundary, depending on training examples. We approach this problem by repeatedly selecting dominant documents to expand terms toward dominant documents of the initial retrieval set, as in the boosting method for a weak learner that repeatedly selects hard examples to change the decision boundary toward hard examples. The hypothesis behind using overlapped documen t clusters is that a good representative document for a query may have several nearest neighbors with high similarities, pa rticipating in several different clusters. Since it plays a central role in forming clusters, this document may be dominant for this topic. Repeatedly sampling dominant documents can emphasize the topics of a query, rather than randomly resampling documents for feedback. 
We show that resampling feedb ack documents based on clusters contributes to higher relevance density for feedback documents on a variety of TREC collections. The results on large-scale web collections such as the TREC WT10g and GOV2 collections show significant improvements over the baseline relevance model. 
The rest of the paper is organized as follows: Section 2 presents related work. Section 3 descri bes a cluster-based resampling framework. Section 4 shows experi mental results on TREC test collections, results analyses and justification of the results. We will conclude in Section 5. Our approach is related to previous work on pseudo-relevance feedback, resampling approaches, and the cluster hypothesis in information retrieval. 
Relevance feedback (RF) and ps eudo-relevance feedback (PRF) have been shown to be effective ways of improving retrieval accuracy by reformulating an original query using relevant or pseudo-relevance documents from the initial retrieval result. New interest in relevance feedback ha s resulted in the establishment of a relevance feedback track at TREC 2008 [30]. This track will provide a framework for exploring the effects of different factors on relevance feedback, such as initial retrieval, judgment procedure, core reformulation algorithm, and multiple iterations on large scale collection. The motivation of the track shows the current state of research: that relevance feedback is one of the successes of information retrieval ove r the past 30 years, in that it is applied in a wide variety of settings as both explicit and implicit feedback; however there is surprisingly little new basic research [4]. At the RIA workshop [2], there were comparative experiments on the effects of seve ral factors for pseudo-relevance feedback. The report provides the effects of the number of documents, the number and source of terms used, the initial set of documents, and the effects of swapping documents or terms since some factors are mixed up with other effects. Traditional pseudo-relevance feedback algorithms such as Okapi BM25 [22] and Lavrenko and Croft X  X  relevance model [15] are based on the assumption of re levancy for the top-retrieved documents. Research has been conducted to improve traditional PRF by using passages [33] instead of documents, by using a local context analysis method [31], by using a query-regularized estimation method [29], and by usi ng latent concepts [20]. These methods follow the basic assumption that the top-retrieved documents are relevant to a query. 
Recently there has been some work on sampling and resampling method by Sakai et al [25] skips some top-retrieved documents based on a clustering criterion. Th e cluster is generated not by document similarity but by the same set of query terms. The sampling purpose is to select a more varied and novel set of documents for feedback. Their a ssumption is that the top-ranked documents may be too similar or redundant. However, their results did not show signi ficant improvements on NTCIR collections. Our approach of repeatedly using dominant documents is based on a different assumption. A resampling method suggested by Collins-Tompson and Callan [5] uses bootstrap sampling on the top-retrieved documents for the query and variants of the query obtained by leaving a single term out. The assumption behi nd query variants is that one of the query terms is a noise term. From their experimental analysis, the main gain is from the use of query variants, not document resampling. Their results on robustness and precision at 10 documents (P@10) show impr ovements, but the performance in terms of mean average precision (MAP) is lower than the baseline relevance model on TREC collections. Our approach primarily focuses on the effects of resampling the top-retrieved documents. 
On the other hand, many inform ation retrieval techniques have adopted the cluster hypothesis to improve effectiveness. The cluster hypothesis states that closely related documents tend to be relevant to the same request [11]. Re-ranking using clusters [16, 17] based on the vector space model has shown successful results. A cluster-based retrieval model [18] based on language modeling ranks clusters by the likelihood of generating the query. The results show improvements over the query-likelihood retrieval model on TREC collections. A local score regularization method [6] uses a document affinity matrix to adjust initial retrieval scores so that topically related documents receive similar scores. The results on TREC collections show that regularized scores are significantly better than the initia l scores. Our work is closely related to document re-ranking usi ng cluster validation and label propagation [32], document-based language models by the incorporation of cluster inform ation [12], re-ranking method using cluster-based language models w ithin a graph-based framework [14], re-ranking using affinity graph [34], and iterative pseudo-query processing using cluster-bas ed language models [13]. 
There has also been work on term expansion using clustering in the vector space model [3, 2, 33, 19]. At TREC 6, Buckley et al [3] used document clustering on SMART though the results of using clusters did not show improvements over the baseline feedback method. At the RIA workshop to investigate the effects criteria for pseudo-relevance feedback [2], there are comparisons to investigate the effects of sw apping documents and clusters by document clustering and passa ge-level clustering. The experimental setup is too complex to see the individual effects of clusters, since an outside sour ce factor is mixed up with the clustering factor [33]: using outsi de sources for feedback itself affects the performance. Thus the analysis for the comparative experiments is inconclusive. This section describes the rationa le for the method for selective resampling, our resampling proce dure, and a justification based on relevance density. The main issues in pseudo-relevance feedback are how to select relevant documents from the top -retrieved documents, and how to select expansion terms. Here we deal with the problem of selecting better feedback documents. 
The problem in traditional ps eudo-relevance feedback is obtaining a set of expansion terms from the top-retrieved documents that may have low pr ecision. If a method can select better documents from the given sample, it can almost certainly contribute better expansion terms. For pseudo-relevance feedback, the initial retrieval set can be seen as the sample space of query expansion terms from which we estimate the sampling distribution. 
In statistics, resampling (boot strapping) [8] is a method for estimating the precision of sample statistics by sampling randomly with replacement from the original sample, leading to robust estimates. If a method is available for selecting better examples from the original sample space, selective sampling will perform better than random sampling. Boosting [27, 10] is a selective resampling method in mach ine learning. It is an iterative procedure used to adaptively change the distribution of training examples so that the weak learners focus on examples that previous weak learners misclassified. 
To find some direction to change the distribution, we assume that a dominant document for a query is one with good representation of the topics of a query X  X .e. one with several nearest neighbors with high simila rity. In overlapped clusters, a dominant document will appear in multiple highly-ranked clusters. Since a topic can contain several subtopics, the retrieved set can be divided into several subtopi c groups. A document that deals with all subtopics will likely be in all subtopic clusters, so we call that document dominant . From such a dominant document, expansion terms that retrieve doc uments related to all subtopics can be selected. 
Based on the above assumption, we selectively resample documents for feedback using k-nearest neighbors (k-NN) clustering to generate overlappe d clusters from the given top-retrieved documents space. A cluster-based resampling method to get better pseudo-relevant documents is based on the language model [21] and the relevance model [15] frameworks. Relevance models have been shown to be a powerful way to construct a query model from the top-retrieved documents [15, 7]. The essential poi nt of our approach is that a document that appears in multiple highly-ranked clusters will contribute more to the query te rms than other documents. The resampling process proceeds as follows: 
First, documents are retrieved for a given query by the query-likelihood language model [21] with Dirichlet smoothing [35]. 
A statistical language model is a probabilistic distribution over all the possible word sequences for generating a piece of text. [22]. In information retrieval, the language model treats documents themselves as models and a query as strings of text generated from these document models. The popular query-likelihood retrieval model estimat es document language models using the maximum likelihood estim ator. The documents can be ranked by their likelihood of ge nerating or sampling the query from document language models: P ( Q|D ). where q i is the i th query term, m is the number of words in a query Q , and D is a document model. 
Dirichlet smoothing is used to estimate non-zero values for the query likelihood language model as follows. where P ML (w|D) is the maximum likelihood estimate of word w in the document D , Coll is the entire collection, and smoothing parameter. | D | and | Coll | are the lengths of a document D and collection C, respectively. freq(w,D) and freq(w,Coll) denote the frequency of a word w in D and Coll , respectively. The smoothing parameter is learned using training topics on each collection in experiments. 
Next, clusters are generated by k -nearest neighbors ( k-NN) clustering method [9] for the top-retrieved N documents to find dominant documents. (In experiments, N is set to 100.) Note that one document can belong to several clusters. 
In k -NN clustering, each document plays a central role in making its own cluster with its k closest neighbors by similarity. We represent a document by tfidf weighing and cosine normalization. The cosine similarity is used to calculate similarities among the top-retrieved documents. 
Our hypothesis is that a dominant document may have several nearest neighbors with high similar ities, participating in several clusters. On the other hand, a non-relevant document ideally makes a singleton cluster with no nearest neighbors with high similarity, though practically it will have neighbors due to noise such as polysemous or general term s. Document clusters can also reflect the association of terms and documents from similarity calculation. In this work, if a document is a member of several clusters and the clusters are highly related to the query, we assume it to be a dominant document. A cluster-based resampling method is repeatedly feeding such dominant documents based on document clusters. 
After forming the clusters, we rank them by a cluster-based query-likelihood language model described below [18]. The documents in the top-ranked cluste rs are used for feedback. Note that clusters are only used fo r selecting feedback documents. 
A cluster can be treated as a la rge document so that we can use the successful query-likelihood retrieval model. Intuitively, each cluster can be represented by ju st concatenating documents which belong to the cluster. If Clu represents such a cluster, then: where freq(w,Clu) is sum of freq(w, D) for the document D which belongs to the cluster Clu . 
Finally, expansion terms are selected using the relevance model for each document in the top-ranked clusters. Note that the set of feedback documents chosen from th e top-ranked clusters are used to estimate the relevance model with their initial query-likelihoods. 
A relevance model is a query e xpansion approach based on the language modeling framework. The relevance model [15] is a multinomial distribution which estimates the likelihood of words w given a query Q . In the model, the query words q 1 ... q words w in relevant documents are sampled identically and independently from a distribution R . Following that work, we estimate the probability of a word in the distribution R using where R is the set of documents that are pseudo-relevant to the query Q . We assume that P(D) is uniform over the set. 
After this estimation, the most likely e terms from P(w|R) are chosen as an expansion query fo r an original query. The final expanded query is combined with the original query using linear interpolation, weighted by parameter  X  . The combining parameter is learned using training topics on each collection in experiments. 
The original relevance model and traditional pseudo-relevance feedback methods use the initial retrieval set to get expansion terms directly after the first step. The problem is that the top-retrieved documents contain non -relevant documents, which add noise to expansion terms. Our effo rt uses overlapping clusters to find dominant documents for the query. It may still find non-relevant documents, but we will show it finds fewer. The rationale for the proposed method is that resampling documents using clusters is an effective way to find dominant documents for a query from the initial retrieval set. We measure relevance density to justify our assumption that dominant documents are relevant to the query and redundantly appear over the top-ranked clusters. 
The relevance density is defined to be the proportion of the feedback documents that contain relevant documents. Density = (8) 
A higher relevance density implies greater retrieval accuracy, ultimately approaching true relevance feedback. 
If a resampling method is effective, it will produce higher relevance densities for pseudo-relevant documents than a set of top-retrieved documents. To justify the cluster-based resampling method, we will examine the relevance density of feedback documents through experimental analysis. To validate the proposed method, we performed experiments on five TREC collections and compar ed the results with a baseline retrieval model, a baseline f eedback model, and an upper-bound model. We tested the proposed method on homogeneous and heterogeneous test collecti ons: the ROBUST, AP and WSJ collections are smaller and contain newswire articles, whereas GOV2 and WT10G are larger web collections. For all collections, the topic title field is used as the query. A summary of the test collections is shown in Table 1. 
Version 2.3 of the Indri system [28] is used for indexing and retrieval. All collections are stemmed using a Porter stemmer. A standard list of 418 common terms is removed at retrieval time. For each test collection, we divide topics into training topics and test topics, where the training topics are used for parameter estimation and the test topics are used for evaluation. 
In order to find the best parame ter setting we sweep over values for smoothing parameter to construct the language model (  X   X  {500, 750, 1000, 1500, 2000, ..., 5000}), to construct the relevance model for the number of feedback documents (| R |  X  {5, 10, 25, 50, 75, 100}), the num ber of expansion terms ( e  X  {10, 0.2, ..., 0.9}). To train the pr oposed model, we sweep over the corresponds to the number of feedback documents since one cluster can have at most five documents as a member ( k = 5) in our k -nearest neighbors clustering. Th e threshold for clustering is set to 0.25. Expansion terms ar e represented using the following Indri query form: where q 1 ... q m are the original query terms, t 1 ... t combining the original query and the expanded query. 
All comparison methods are optimi zed on the training set using mean average precision (MAP) defined as, where ap ( q ) is average precision for a query in the topic set Q . The best parameters on training for each test collection are used for experimental results with the test topics. We provide two baselines: the language model and the relevance model.  X  Language Model ( LM ): The performance of the baseline Table 1. Training and test collections Collection Description # of docs Train Test GOV2 2004 crawl of .gov domain 25,205,179 701-750 751-800 WT10g TREC web collection 1,692,096 451-500 501-550 ROBUST Robust 2004 collection 528,155 301-450 601-700  X  Relevance Model ( RM ): The performance of the baseline To investigate the performance of the upper-bound of the proposed method, we compare w ith true relevance feedback.  X  True Relevance Feedback ( TrueRF ): The performance using To provide the effectiveness of clusters for the initial retrieval set, we also include a cluster-based reranking method.  X  Reranking using clusters ( Rerank ): The performance of The results for the comparison met hods on five test collections are presented in Table 2. 
The Resampling method significantly outperforms LM on all test collections, whereas RM does not significantly outperform LM on the WT10g collection. For the GOV2 and WT10g heterogeneous web test collections, the Resampling method significantly outperforms RM . The relative improvements over RM are 6.28% and 19.63% on GOV2 and WT10g, respectively. For the ROBUST newswire collection, the Resampling method shows slightly lower performance than RM . For the AP and WSJ newswire collections, the Resampling method shows small, but not significant improvements over RM . 
In the precision at 5 (P@5) evalua tion metric (not shown in the table), the Resampling method shows 14.8%, 24.7%, 3.9%, 20.0%, and 11.9% improvements over LM , whereas RM shows -7.1%, 7.4%, 1.6%, 18.8% and 7.4% improvement on GOV2, WT10g, ROBUST04, AP and WSJ, respectively. 
The Rerank method using clusters shows significant improvements over LM on all test collections. In fact, the Rerank method outperforms RM on WT10g collection. The results indicate that document clustering can help find relevant document groups for the initial retrieval set and provide implicit document context to the query. 
TrueRF shows significant improvements over all methods on test collections. The results provide upper-bound performance on each collection, when we are able to choose better pseudo-relevant documents, approaching to true relevant documents. 
We have also examined the effectiveness as the number of feedback documents and terms va ries. As shown in Figure 1, Resampling achieves better performance over RM for most values. The best parameters selected for feedback on GOV2 are 10 documents and 50 terms for RM , 25 documents and 100 terms for Resampling . For test topics using the best parameters (  X  , e , and  X  ) chosen from training, the Resampling method outperforms RM regardless of the number of feedback documents. In this section we aim to deve lop a deeper understanding of why expansion by the cluster-based resampling method helps. 
For justification of a cluster-b ased resampling approach using overlapping clusters, we have anal yzed the relevance density by dominant documents and the perfo rmance of feedback without redundant documents. We can expect that higher relevance density produces higher performance since more relevant doc uments are used for feedback. 
As shown in Figure 2, the resampling method shows higher density compared to the relevance model for all test collections. (The density for AP and WSJ coll ections is not shown but has the same pattern as the ROBUST collection.) 
When the number of feedback documents is set to 100, we can expect that the resampling method outperforms the relevance model since the resampling method uses more relevant documents for feedback. Table 3. Performance on fixed feedback documents. The number of feedback documents and terms are both set to 100. The superscripts  X  and  X  indicate statistically significant improvements over LM and RM, respectively. We use the paired t -test using significance at p &lt; 0.05. 
GOV2 0.3258 0.3519  X  8.01 0.3764  X  X 
WT10G 0.1861 0.1886 1.34 0.2072 ROBUST 0.2920 0.3262  X  11.71 0.3549  X  X  21.54
AP 0.2077 0.2758  X  32.79 0.2853 
WSJ 0.3258 0.3785  X  16.18 0.4009 Table 2. Performance compar isons using mean average precision for the test topics on test collections. The superscripts  X  ,  X  ,  X  and  X  indicate statistically significant improvements over LM, Rerank, RM and Resampling, respectively. We use the paired t -test with significance at p &lt; 0.05. GOV2 0.3258 0.3406  X  0.3581  X  X  0.3806  X  X  X  0.4315 WT10g 0.1861 0.2044  X  0.1966 0.2352  X  X  X  0.4030 ROBUST 0.2920 0.3206  X  0.3591  X  X  0.3515  X  X  0.5351 AP 0.2077 0.2361  X  0.2803  X  X  0.2906  X  X  0.4253 WSJ 0.3258 0.3611  X  0.3967  X  X  0.4033  X  X  0.5306 
To verify our expectation for density, we compared performance with the number of feedback documents and terms set to 100. The performance of feedback on fixed documents is shown in Table 3. The resa mpling method outperforms the relevance model for all collections. The results show that the density of relevant documents supports the improvements from the resampling approach which extracts better feedback documents from the top-ranked 100 documents. 
From the results of density according to number of feedback documents and effectiveness on a ll collections, we can conclude that the redundant dominant documen ts help the density of the relevant documents. To support the observation of relevance density and performance, we have examined performance by removing redundant documents in feedback. That is, a document is not repeated in the feedback even if it occu rs in multiple clusters. 
We assumed that dominant documents for the initial retrieval set are relevant and redundant docum ents that play a central role in making overlapping clusters. Ta ble 4 shows the performance of Sampling without Redundancy . It outperforms RM , but is worse than Resampling . The results show that redundant documents give positive effects for feedback. 
We have also examined how redundancy affects the number of relevant documents in the feedback sample. If we look at the top 5, 10, 25, 50, 75, a nd 100 documents, we find the following. For the RM approach, the relevance density was 0.6, 0.5, 0.36, 0.3, and 0.25, respectively. For Resampling , however, the densities respectively. To illustrate the level of redundancy, consider one query where the top 10 clusters contained 50 documents, 40 of which were relevant: 37 of those relevant documents appeared in other clusters. One relevant docum ent appeared in nine of the top 10 clusters and another was in seven. Some documents that appear in multiple highly-ranke d clusters and their redundancy contribute to query expansion terms. terms on GOV2 collection (in mean average precision). Table 4. The effect of redundant documents for feedback. LM 0.3258 -0.1861 -Rerank 0.3406 4.54 0.2044 9.83 RM 0.3581 9.91 0.1966 5.64 Resampling 0. 3806 16.82 0.2352 26.38 Sampling without redundancy 0.3745 14.95 0.2193 17.84 We analyze the robustness of the baseline feedback model and the resampling method over the baseline retrieval model. Here, robustness is defined as the num ber of queries whose performance is improved or hurt as the resu lt of applying these methods. 
Figure 3 presents an analysis of the robustness of the baseline feedback model and the re sampling method on GOV2, WT10g and WSJ. The robustness of ROBU ST and AP showed the similar pattern with WSJ. For the hom ogeneous newswire collections such as WSJ, AP and ROBU ST, the relevance model and resampling method showed a sim ilar pattern for robustness. 
The resampling method shows st rong robustness for each test collection. For the GOV2 collection, the resampling method improves 41 queries and hurts 9, whereas the relevance model improves 37 and hurts 13. For the WT10g collection, the resampling method improves 30 and hurts 19, whereas the relevance model improves 32 and hurts 17. Although the relevance model improves the performance of 2 more queries than the resampling method, the improvements obtained by the resampling method are significantly larger. For the ROBUST collection, the resampling me thod improves 63 and hurts 36, whereas the relevance model improves 64 and hurts 35. 
Overall, our resampling method improves the effectiveness for 82%, 61%, 63%, 66% and 70% of the queries for GOV2, WT10g, ROBUST, AP and WSJ, respectively. Resampling the top-ranked documents using clusters is effective for pseudo-relevance feedback. The improvements obtained were consistent across nearly all collections, and for large web collections, such as GOV2 and WT10g, the approach showed substantial gains. The relative improvements on GOV2 collection are 16.82% and 6.28% over LM and RM, respectively. The improvements on the WT10g coll ection are 19.63% and 26.38% over LM and RM, respectively. We showed that the relevance density was higher than the baseline feedback model for all test collections as a justification of why expansion by the cluster-based resampling method helps. E xperimental results also show that overlapping clusters are helpful for identifying dominant documents for a query. 
For future work, we will study how the resampling approach can adopt query variants by c onsidering query characteristics. Additionally, in our experiments we simply represent a cluster by concatenating documents. Using a better representation of a cluster should improve the performance of pseudo-relevance feedback by improving the cluster ranking. This work was supported by the Korea Research Foundation Grant funded by the Korean Government (MOEHRD)" (KRF-2006-611-D00025) and in part by the Center for Intelligent Information Retrieval. Any opini ons, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. [1] Attar, R. and Fraenkel, A. S. 1977. Local Feedback in Full-[2] Buckley, C. and Harman, D. 2004. Reliable information [3] Buckley, C., Mitra, M., Walz, J., and Cardie, C. 1998. Using [4] Buckley, C. and Robertson, S. 2008. Proposal for relevance [5] Collins-Thompson, K., and Calla n, J. 2007. Estimation and [6] Diaz, F. 2005. Regularizing ad hoc retrieval scores. In Proc. [7] Diaz, F., and Metzler, D. 2006. Improving the Estimation of [8] Efron, B. 1979. Bootstrap me thods: Another look at the [9] Fix, E. and Hodges, L. 1951. Discriminatory analysis: [10] Freund, Y. 1990. Boosting a weak learning algorithm by [11] Jardine. N. and Rijsbergen , C.J.V. 1971. The use of [12] O. Kurland and L. Lee, 2004. Corpus structure, language [13] Kurland, O., and Lee, L. 2005. Better than the real thing? [14] Kurland, O., and Lee, L. 2006. Respect my authority! HITS [15] Lavrenko, V. and Croft, W.B. 2001. Relevance-based [16] Lee, K.S., Park, Y.C., a nd Choi, K.S. 2001. Re-ranking [17] Lee, K.S., Kageura, K., a nd Choi, K.S. 2004. Implicit [18] Liu, X., and Croft, W.B. 2004. Cluster-based retrieval using [19] Lynam, T., Buckley, C., Clarke, C., and Cormack, G. 2004. [20] Metzler, D., and Croft, W. B. 2007. Latent Concept [21] Ponte, J.M., and Croft, W. B. 1998. A language modeling [22] Robertson, S.E., Walker, S., Beaulieu, M., Gatford, M., and [23] Rocchio, J.J. 1971. Relevance feedback in information [24] Rosenfeld, R. 2000. Two decad es of statistical language [25] Sakai, T., Manabe, T. and Koyama, M. 2005. Flexible [26] Salton, G., and Buckley, C. 1990. Improving retrieval [27] Schapire, R. 1990. Strength of weak learnability. Journal of [28] Strohman, T., Metzler, D., Tur tle, H., and Croft, W.B. 2005. [29] Tao, T., and Zhai, C. 2006. Regularized estimation of [30] TREC. 20008. Call for participation. [31] Xu, J and Croft, W.B. 1996. Query expansion using local [32] Yang, L., Ji, D., Zhou, G., Nie, Y., and Xiao, G. 2006. [33] Yeung, D.L., Clarke, C.L.A., Cormack, G.V., Lynam, T.R., [34] Zhang, B., Li, H., Liu, Y., Ji, L., Xi, W., Fan, W., Chen, Z., [35] Zhai, C., and Lafferty, J. 2004. A study of smoothing 
