 Rejection is the norm in academic publishing. One of the main reasons for rejections is that the topics of the submitted papers are not relevant to the scope of the journal, even when the papers themselves are excellent. Submissi on to a journal that fits well with the publication may avoid this issue. A system that is able to suggest journals that have pub lished similar articles to the submitted papers may help authors choose where to submit. The Elsevier journal finder, a freely available online service, is one of the most comprehensive journa l recommender systems, covering all scientific domains and more than 2,900 per-reviewed Elsevier journals. The system uses natura l language processing for feature generation, and Okapi BM25 ma tching for the recommendation algorithm. The procedure is to paste text, such as an abstract, and get a list of recommend journals and relevant metadata. The website URL is http://journalfinder.elsevier.com . H.3.3 [ Information Search and Retrieval ]: Clustering; I.2.7 [ Natural Language Processing ]: Text analysis; I.5.3 [ Clustering ]: Similarity measures Algorithms, Measurement, Experimentation, OKAPI BM25. Keywords: Natural language processing, Recommender system, Noun phrase, TF-IDF, Okapi BM25 Finding the right journal to submit a paper is one of the most important steps during the process of paper publishing. For most authors, this job is difficult because many journals have a very wide diversity of topics, and many articles involve several academic disciplines or professional specializations. According to the records from the Scopus database [2], from 1992 to 2002, 12 million peer-reviewed papers have been published. This number has doubled between 2003 and 2013. With the rapid growth of new journals and papers each year, the task to select a correct journal to submit a paper becomes more and more difficult. In this study, we present the Elsevier journal finder, a comprehensive journal recommender system that covers all major scientific domains and more th an 2,900 peer-reviewed Elsevier journals, to help authors easily find relevant journals for their paper. Recommender systems have become quite common in recent years, and are applied in a variety of applications [10][1]. In the field of journal recommendation, there are already some systems that search for similar articles [7]. For example, PubMed [6] offers the function to search similar records from Medline records, but only existing Medline records can be used as queries. eTBLAST [3] accepts full abstracts search for journal recommendation, and Jane [12] al so provides similar functions. However, these systems only cove r the biomedical domain. Some cross domain tools such as Mendeley [9] search for similar articles based on articles that have already been published, but do not recommend journals. The Scopus database [2] is used as the source of journals and papers for the Elsevier journal fi nder. The Scopus database is the largest abstract and citation data base of peer-reviewed literature from scientific journals, books and conference proceedings. It contains more than 55 million records and 5000 publishers, and covers all major scientific dom ains: Agriculture, Chemistry, Economics, Geo-Sciences, Humanitie s and Arts, Life and Health Sciences, Materials Science and Engineering, Mathematics, Physics, and Social Sciences. For our system, we only use the papers that are published after 2008, for the reason that the scope of the journals may change over time, and newer papers reflect the current scope of journals more accurately. Also, the system X  X  response time improves when it includes less sample papers. We also filter out all papers from non-Elsevier journals because our system only recommends Elsevier journals. The remaining 1.98 million paper records from the Scopus database are used as the sample papers set. The Elsevier journal finder uses noun phrases [5] as features for the paper matching and jour nal ranking algorithm. The noun phrases are annotated and normalized by the Elsevier Fingerprint Engine [14]. The Elsevier Finge rprint Engine (EFE) applies a variety of Natural Language Pro cessing (NLP) techniques to mine the input text and generates a ll relevant annotations, including sentence boundaries, tokenization, part-of-speech tags, and phrase chunking. Noun phrases are extracted based on a relatively simple pattern of part-of-speech (POS) tag sequences. We employed a simple noun phrase syntax, sket ched in Backus-Naur-form: In this noun phrase grammar, POS tags are used as terminals ( jj is noun X , nns is  X  X lural noun X , and in is  X  X reposition X ). To improve feature generation however, we made the algorithm to select sub-phrases of full noun phrases, in order to avoid a very sparse vector space containing only very specific noun phrases. The feature set consists of noun phrases in normali zed form, meaning that plural forms translate to singular varian ts, and spelling variations, e.g. British to American, are normalized away. We preprocess the 1.98 million sample papers from the Scopus database to generate normali zed noun phrases as weighted features (vector-dimensions) for each paper published in the target journals. For each query input text submitted, we use the EFE to generate normalized noun phrases as a query vector for the paper matching algorithm. We remove all noun phrases that occur only once and the top 300 noun phrases that occur most frequently (e.g., study, method, data, analys e, paper, conclusion, model, system, etc. ....). These noun phras es are too commonly used to contribute to the ranking algorithm. By testing the accuracy, these optimization parameters give the highest accuracy. The journal recommendation ranking algorithm is divided into two parts. The first part is matching the submitted query to existing papers in the database . For this purpose, we use the Okapi BM25 algorithm [11]. The Okapi BM25 algorithm is widely used in the domain of in formation retrieval. It ranks matching documents according to their relevance to a given search query. Normally, the input is a bag-of-words, and the output is a set of documents with scores and ranks based on the query words appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). In our case, instead of using th e whole text as input for the retrieval function, we use the normalized noun phrase annotations feature selection may account for part of the improved accuracy of our system relative to other recommender systems. The Okapi BM25 can be described as below: Given an input text Q, containing noun phrases , ..., , the BM25 score of a paper D is: where is  X  X  frequency in paper D, |D| is the length of the paper D in noun phrases, and avgdl is the average paper length in noun phrases in the sample paper set. The parameters k allow for adapting the algorithm to different use cases. In our case, we used 1.5 for k 1 , 0.6 for b (experimentally), and measured 68 for avgdl as the average document length. IDF q i is the IDF (inverse document frequency) [12] weight of the noun phrase q i . It is usually computed as: where N is the total number of papers in the sample paper set, and is the number of papers containing . After the first step, we get a ranked paper list with a BM25 score for each paper that has already been published in a journal [5]. text. The second part of the journal recommendation ranking algorithm translates the scores for individua l papers to scores for journals. This step is divided into the following sub-steps: 1. Keep the top 1 million papers with the highest BM25 score from the ranked paper list, and fi nd the journal and the journal X  X  scientific domains of the journal that each paper belongs to. Given outside the top 1 million will contribute to the aggregated score per journal (see below, 3); 2. If the end-user has already selected a domain, then remove all documents that do not belong to this domain. This step is skipped if the end-user did not select a domain for the input text. (See the section of system overview for mo re information about the input from end-users); 3. Compute an average BM25 score per journal by averaging the scores of all papers published in the same journal: Where N J is the number of papers published in journal J, the average to correct for journal size. In Figure 1, we show the system overview of the Elsevier journal finder. When an end-user inputs an abstract, the EFE first generates the normalized noun phrases, which are then used by the paper matching algorithm to fi nd the related papers from the Scopus database, and then these pa pers are used by the journal ranking algorithm to get the recommended journals list. From Figure 2, we can see the input interface of the Elsevier journal finder. The end users can simply input the paper title and abstract, or even just a few keywords, and then select one or more scientific domains that the input te xt belongs to (this step can also be skipped if the end-user is not sure about which domain(s) the input text belongs to). Figure 3 shows the list of recommended Elsevier journals, together with some important metadata of the recommended journals, such as matching score, impact factor, open access, editorial times, acceptance rate, and production times. By clicking each journal title, user can also see the scope and more information about this journal. This information can help the authors to decide to which journal to submit their papers, and may reduce the probability of rejection. To evaluate the accuracy of our system, we applied a strategy similar to leave-one-out cross-va lidation: we randomly selected 10 to 100 (depending on the number of papers published in each journal) already published papers from each Elsevier journal as the input documents, and remove d these input documents from the source database. If the top three or top ten recommended journals contained the journal in which the input paper was published, then this is counted as a correct recommendation, otherwise it is counted as a false recommendation. Table 1 shows the performance of each optimization. By changing the features from c oncepts to noun phrases, the performance is improved by more than 10%. By optimizing the noun phrases (normalize and filter the noun phrases) and the algorithm (tuning the parameters of the paper matching algorithm and the journal ranking algorithm), the performance is further improved by another 5%. The best performance is 42.6% for the top 3, and 64.6% for the top 10. We use normalized noun phrases as the features for our ranking algorithm, and do not use advanced annotations such as concepts as thesaurus-defined entities. A lthough the EFE can generate high quality concept annotations, these are not suitable for this use case. Using concept annotations re sults in a sparse feature set, particularly as a comprehensive, good-coverage thesaurus spanning all disciplines is not r eadily available. Furthermore, considering the diverse nature of the data (texts from multiple science domains), using noun phrase annotations as features is better than using concept annotations. For an extensive discussion of the best feature sets, cf. [4]. The ranking algorithm only works well if there are enough sample papers (at least more than 100) in each journal. However, for some new journals, there are not enough published papers. To solve this problem, we asked the editors to select some papers from other journals that are rele vant to the scope of the new journals, and then used these select ed papers as the sample papers for the ranking algorithm. The performance of the Elsevier journal finder is better than Jane (42% for top 3 and 58% for top 10) [13] and eTBLAST (35% for top 3 and 50% for top 10) [3] that use the same evaluation method of leave-one-out cross-validati on. Besides that, the Elsevier journal finder is the only system that covers all major scientific domains (including the biomedi cal domain), whereas the other two systems only cover the biomedical domain. However, the performance figures of these systems are based on different test document sets. They could be changed if we use the same test document set. This is difficult to do because the leave-one-out cross-validation method needs to change the training document set, which is impossibl e if we do not have the source code of Jane and eTBLAST. Theoretically, the Elsevier journal finder can recommend any journal in the Scopus database. Although the recommended journals are limited to Elsevier journals only, the system can always recommend highly relevant journals to the authors for their papers, since Elsevier has more than 2900 peer-reviewed journals that cover almost all major scientific domains. [1] Bobadilla, J. et al. 2013. Recommender systems survey. [2] Burnham, J.F. 2006. Sc opus database: a review. Biomedical [3] Errami, M. et al. 2007. ETBLAST: A web server to identify [4] Jimeno Yepes, A.J., Plaza, L., Carrillo-de-Albornoz, J., [5] Kang, N. et al. 2011. Comparing and combining chunkers of [6] Kantrowitz, M. et al. 2000. Stemming and its effects on [7] Lu, Z. 2011. PubMed and beyond: A survey of web tools for [8] McEntyre, J. and Lipman, D. 2001. PubMed: Bridging the [9] Reiswig, J. 2010. Mendeley. Journal of the Medical Library [10] Ricci, F. et al. 2011. Introduction to Recommender Systems [11] Robertson, S.E. 1990. On term selection for query [12] Roelleke, T. and Wang, J. 2008. TF-IDF uncovered: a study [13] Schuemie, M.J. and Kors , J.A. 2008. Jane: Suggesting [13] Vestdam, T.V. et al. 2014. Black magic meta data -Get a 
