 Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. For ex-ample, the well-known Ratnaparkhi parser (Ratna-parkhi, 1999) used a finite-state POS-tagger and NP-chunker to reduce the search space for his Maxi-mum Entropy parsing model, and achieved linear observed-time performance. Other recent examples of the utility of finite-state constraints for parsing pipelines include Glaysher and Moldovan (2006), Djordjevic et al. (2007), Hollingshead and Roark (2007), and Roark and Hollingshead (2008). Note that by making use of constraints derived from pre-processing, they are no longer performing full exact inference X  X hese are approximate inference meth-ods, as are the methods presented in this paper. Most of these parsing pipeline papers show empirically that these techniques can improve pipeline efficiency for well-known parsing tasks. In contrast, in Roark and Hollingshead (2008), we derived and applied the finite-state constraints so as to guarantee a reduc-tion in the worst-case complexity of the context-free parsing pipeline from O ( N 3 ) in the length of the string N to O ( N 2 ) by closing chart cells to entries. We demonstrated the application of such constraints to the well-known Charniak parsing pipeline (Char-niak, 2000), which resulted in no accuracy loss when the constraints were applied.

While it is important to demonstrate that these sorts of complexity-reducing chart constraints do not interfere with the operation of high-accuracy, state-of-the-art parsing approaches, existing pruning tech-niques used within such parsers can obscure the im-pact of these constraints on search. For example, us-ing the default search parameterization of the Char-niak parser, the Roark and Hollingshead (2008) re-sults demonstrated no parser speedup using the tech-niques, rather an accuracy improvement, which we attributed to a better use of the amount of search per-mitted by that default parameterization. We only demonstrated efficiency improvements by reducing the amount of search via the Charniak search param-eterization. There we showed a nice speedup of the parser versus the default, while maintaining accu-racy levels. However, internal heuristics of the Char-niak search, such as attention shifting (Blaheta and Charniak, 1999; Hall and Johnson, 2004), can make this accuracy/efficiency tradeoff somewhat difficult to interpret.

Furthermore, one might ask whether O ( N 2 ) com-plexity is as good as can be achieved through the paradigm of using finite-state constraints to close chart cells. What methods of constraint would be required to achieve O ( N log N ) or linear complex-ity? Would such constraints degrade performance, or can the finite-state models be applied with suffi-cient precision to allow for such constraints without significant loss of accuracy?
In this paper, we adopt the same paradigm pur-sued in Roark and Hollingshead (2008), but apply it to an exact inference CYK parser (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965). We demonstrate that imposing constraints sufficient to achieve quadratic complexity in fact yields observed linear parsing time, suggesting that tighter complex-ity bounds are possible. We prove that a differ-ent method of imposing constraints on words be-ginning or ending multi-word constituents can give O ( N log 2 N ) or O ( N ) worst-case complexity, and we empirically evaluate the impact of such an ap-proach.

The rest of the paper is structured as follows. We begin with a summary of the chart cell constraint techniques from Roark and Hollingshead (2008), and some initial empirical trials applying these tech-niques to an exact inference CYK parser. Complex-ity bounding approaches are contrasted (and com-bined) with high precision constraint selection meth-ods from that paper. We then present a new approach to making use of the same sort of finite-state tag-ger output to achieve linear or N log 2 N complexity. This is followed with an empirical validation of the new approach. The basic algorithm from Roark and Hollingshead (2008) is as follows. Let B be the set of words in a string w 1 . . . w k that begin a multi-word constituent, and let E be the set of words in the string that end a multi-word constituent. For chart parsing with, say, the CYK algorithm, cells in the chart represent sub-strings w i . . . w j of the string, and can be indexed with ( i, j ) , the beginning and ending words of the substring. If w i 6 X  B , then we can close any cell ( i, j ) where i &lt; j , i.e., no complete constituents need be stored in that cell. Similarly, if w j 6 X  E , then we can close any cell ( i, j ) where i &lt; j . A dis-criminatively trained finite-state tagger can be used to classify words as being in or out of these sets with relatively high tagging accuracy, around 97% for both sets ( B and E ). The output of the tagger is then used to close cells, thus reducing the work for the chart parser.

An important caveat must be made about these closed cells, related to incomplete constituents. For simplicity of exposition, we will describe incom-plete constituents in terms of factored categories in a Chomsky Normal Form grammar, e.g., the new non-terminal Z : X + W that results when the ternary rule production Z  X  Y X W is factored into the two binary productions Z  X  Y Z : X + W and Z :
X + W  X  X W . A factored category such as Z : X + W should be permitted in cell ( i, j ) if w j  X  E , even if w i 6 X  B , because the category could subsequently combine with an Y category to create a Z constituent that begins at some word w p  X  B . Hence there are three possible conditions for cell ( i, j ) in the chart: 1. w j 6 X  E : closing the cell affects all con-2. w i 6 X  B and w j  X  E : closing the cell affects 3. w i  X  B and w j  X  E : cell is not closed, i.e., it
In Roark and Hollingshead (2008), we proved that, for the CYK algorithm, there is no work neces-sary for case 1 cells, a constant amount of work for case 2 cells, and a linear amount of work for case 3 cells. Therefore, if the number of cells allowed to fall in case 3 is linear, the overall complexity of search is O ( N 2 ) .

The amount of work for each case is related to how the CYK algorithm performs its search. Each cell in the chart ( i, j ) represents a substring w i . . . w j , and building non-terminal categories in that cell involves combining non-terminal categories (via rules in the context-free grammar) found in cells of adjacent substrings w i . . . w m and w m +1 . . . w j The length of substrings can be up to order N (length of the whole string), hence there are O ( N ) midpoint words w m in the standard algorithm, and in the case 3 cells above. This accounts for the lin-ear amount of work for those cells. Case 2 cells have constant work because there is only one pos-sible midpoint, and that is w i , i.e., the first child of any incomplete constituent placed in a case 2 cell must be span 1, since w i 6 X  B . This is a very con-cise recap of the proof, and we refer the reader to our previous paper for more details. Despite referring to the CYK algorithm in the proof, in Roark and Hollingshead (2008) we demonstrated our approach by constraining the Charniak parser (Charniak, 2000), and achieved an improvement in the accuracy/efficiency tradeoff curve. However, as mentioned earlier, the existing complicated system of search heuristics in the Charniak parser makes in-terpretation of the results more difficult. What can be said from the previous results is that constraining parsers in this way can improve performance of even the highest accuracy parsers. Yet those results do not provide much of an indication of how performance is impacted for general context-free inference.
For this paper, we use an exact inference (exhaus-tive search) CYK parser, using a simple probabilis-tic context-free grammar (PCFG) induced from the Penn WSJ Treebank (Marcus et al., 1993). The PCFG is transformed to Chomsky Normal Form through right-factorization, and is smoothed with a Markov (order-2) transform. Thus a production such as Z  X  Y X W V becomes three rules: (1) Z  X  Y Z : X + W ; (2) Z : X + W  X  X Z : W + V ; and (3) Z : W + V  X  W V . Note that only two child categories are encoded within the new factored cate-gories, instead of all of the remaining children as in our previous factorization example. This so-called  X  X arkov X  grammar provides some smoothing of the PCFG; the resulting grammar is also smoothed us-ing lower order Markov grammars.

We trained on sections 2-21 of the treebank, and all results except for the final table are on the devel-opment section (24). The final table is on the test section (23). All results report F-measure labeled bracketing accuracy for all sentences in the section.
To close cells, we use a discriminatively trained finite-state tagger to tag words as being either in B or not, and also (in a separate pass) either in E or not. Note that the reference tags for each word can be derived directly from the treebank, based on the spans of constituents beginning (or ending) at each word. Note also that these reference tags are based on a non-factored grammar.

For example, consider the chart in Figure 1 for the five symbol string  X  abcde  X . Each cell in the chart is labeled with the substring that the cell spans, along with the begin and end indices of the substring, e.g., (3 , 5) spans the third symbol to the fifth symbol: cde . If our tagger output is such that b 6 X  E and d 6 X  E , then four cells will be closed: (1 , 2) , (1 , 4) , (2 , 4) and (3 , 4) . The gray shaded cells in the figure have some midpoints that require no work, because they involve closed children cells. 4.1 High Precision vs Complexity Bounding The chart constraints that are extracted from the finite-state tagger come in the form of set exclu-sions, e.g., d 6 X  E . Rather than selecting constraints from the single, best-scoring tag sequence output by the tagger, we instead rely on the whole distribu-tion over possible tag strings to select constraints. We have two separate tagging tasks, each with two possible tags of each word w i in each string: (1) B or  X  B; and (2) E or  X  E, where  X  X signifies that w i 6 X  X for X  X  X  B, E } . The tagger (Holling-shead et al., 2005) uses log linear models trained with the perceptron algorithm, and derives, via the forward-backward algorithm, the posterior probabil-ity of each of the two tags at each word, so that Pr( B ) + Pr(  X  B ) = 1 . Then, for every word w in the string, the tags B and E are associated with a posterior probability that gives us a score for w i  X  B and w i  X  E . All possible set memberships w i  X  X in the string can be ranked by this score. From this ranking, a decision boundary can be set, such that all word/set pairs w i  X  B or w j  X  E with above-threshold probability are accepted, and all pairs be-low threshold are excluded from the set.

The default decision boundary for this tagging task is 0.5 posterior probability (more likely than not), and tagging performance at that threshold is good (around 97% accuracy, as mentioned previ-ously). However, since this is a pre-processing step, we may want to reduce possible cascading errors by allowing more words into the sets B and E . In other words, we may want more precision in our set exclusion constraints. One method for this is to count the number c of word/set pairs below poste-rior probability of 0.5, then set the threshold so that only kc word/set pairs fall below threshold, where 0 &lt; k  X  1 . Note that the closer the parameter k is to 0, the fewer constraints will be applied to the chart. We refer to the resulting constraints as  X  X igh precision X , since the selected constraints (set exclu-sions) have high precision. This technique was also used in the previous paper.

We also make use of the ranked list of word/set pairs to impose quadratic bounds on context-free parsing. Starting from the top of the list (high-est posterior probability for set inclusion), word/set pairs are selected and the number of open cells (case 3 in Section 2) calculated. When the accumulated number of open cells reaches kN for sentence length N , the decision threshold is set. In such a way, there are only a linear number of open, case 3 cells, hence the parsing has quadratic worst-case complexity.
For both of these methods, the parameter k can vary, allowing for more or less set inclusion. Fig-ure 2 shows parse time versus F-measure parse ac-curacy on the development set for the baseline (un-constrained) exact-inference CYK parser, and for various parameterizations of both the high preci-sion constraints and the quadratic bound constraints. Note that accuracy actually improves with the im-position of these constraints. This is not surpris-ing, since the finite-state tagger deriving the con-straints made use of lexical information that the sim-ple PCFG did not, hence there is complementary in-formation improving the model. The best operating points X  X ast parsing and relatively high accuracy X  are achieved with 90% of the high precision con-straints, and 5 N cells left open. These achieve a roughly 20 times speedup over the baseline uncon-strained parser and achieve between 1.5 and 3 per-cent accuracy gains over the baseline.

We can get a better picture of what is going on by considering the scatter plots in Figure 3, which plot each sentence according to its length versus the pars-ing time for that sentence at three operating points: baseline (unconstrained); high precision at 90%; and quadratic with 5 N open cells. The top plot shows up to 120 words in the sentence, and up to 5 seconds of parsing time. The middle graph zooms in to under 1 second and up to 60 words; and the lowest graph zooms in further to under 0.1 seconds and up to 20 words. It can be seen in each graph that the uncon-strained CYK parsing quickly leaves the graph via a steep cubic curve.
 Three points can be taken away from these plots. First, the high precision constraints are better for the shorter strings than the quadratic bound con-straints (see bottom plot); yet with the longer strings, the quadratic constraints better control parsing time than the high precision constraints (see top plot). Second, the quadratic bound constraints appear to actually result in roughly linear parsing time, not quadratic. Finally, at the  X  X rossover X  point, where quadratic constraints start out-performing the high precision constraints (roughly 40-60 words, see mid-dle plot), there is quite high variance in high preci-sion constraints versus the quadratic bounds: some sentences process more quickly than the quadratic bounds, some quite a bit worse. This illustrates the difference between the two methods of select-ing constraints: the high precision constraints can provide very strong gains, but there is no guarantee for the worst case. In such a way, the high preci-sion constraints are similar to other tagging-derived constraints like POS-tags or chunks. 4.2 Combining Constraints Depending on the length of the string, the quadratic constraints may close more or fewer chart cells than the high precision constraints X  X ore for long strings, fewer for short strings. We can achieve worst-case bounds, along with superior typical case speedups, by combining both methods as follows: first apply the quadratic bounds; then, if there are any high precision constraints that remain unap-plied, add them. Table 1 shows F-measure accuracy and parsing time (in seconds) for four trials on the development set: the baseline CYK with no con-straints; high precision constraints at the 90% level; quadratic bound constraints at the 5 N level; and a combination of the quadratic bound and high preci-sion constraints. We can see that, indeed, the com-bination of the two yield speedups over both inde-pendently, with no significant drop in accuracy from the high precision constraints alone. Further results with worst-case complexity bounds will be com-bined with high precision constraints in this way.
The observed linear parsing time in Figure 3 with the quadratic constraints raises the following ques-tion: can we apply these constraints in a way that guarantees linear complexity? The answer is yes, and this is the subject of the next section. Given the two sets B and E , recall the three cases of chart cells ( i, j ) presented in Section 2: 1) w j 6 X  E (cell completely closed); 2) w j  X  E and w i 6 X  B (cell open only for incomplete constituents); and 3) w i  X  B and w j  X  E (cell open for all constituents). Quadratic worst-case complexity is achieved with these sets by limiting case 3 to hold for only O ( N ) cells X  X ach with linear work X  X nd the remaining O work, hence overall quadratic (Roark and Holling-shead, 2008).

One might ask: why would imposing constraints to achieve a quadratic bound give us linear observed parsing time? One possibility is that the linear num-ber of case 3 cells don X  X  have a linear amount of work, but rather a constant bounded amount of work. If there were a constant bounded number of mid-points, then the amount of work associated with case 3 would be linear. Note that a linear complexity bound would have to guarantee a linear number of case 2 cells as well since there is a constant amount of work associated with case 2 cells.

To provide some intuition as to why the quadratic bound method resulted in linear observed parsing time, consider again the chart structure in Figure 1. The black cells in the chart represent the cells that have been closed when w j 6 X  E (case 1 cells). In our example, w 2 6 X  E caused the cell spanning ab to be closed, and w 4 6 X  E caused the cells span-ning abcd , bcd and cd to be closed. Since there is no work required for these cells, the amount of work required to parse the sentence is reduced. However, the quadratic bound does not include any potential reduced work in the remaining open cells. The gray cells in the chart are cells with a reduced number of possible midpoints, as effected by the closed cells in the chart. For example, categories populating the cell spanning abc in position (1 , 3) can be built in two ways: either by combining entries in cell (1 , 1) with entries in (2 , 3) at midpoint m = 1 ; or by com-bining entries in (1 , 2) and (3 , 3) at midpoint m = 2 . However, cell (1 , 2) is closed, hence there is only one midpoint at which (1 , 3) can be built ( m = 1 ). Thus the amount of work to parse the sentence will be less than the worst-case quadratic bound based on this processing savings in open cells.

While imposition of the quadratic bound may have resulted (fortuitously) in constant bounded work for case 3 cells and a linear number of case 2 cells, there is no guarantee that this will be the case. One method to guarantee that both conditions are met is the following: if | E | X  k for some con-stant k , then both conditions will be met and parsing complexity will be linear. We prove here that con-straining E to contain a constant number of words results in linear complexity.

Lemma 1: If | E | X  k for some k , then the Proof: Recall from Section 2 that for each cell ( i, j ) , there are j  X  i midpoints m that require com-bining entries in cells ( i, m ) and ( m +1 , j ) to create entries in cell ( i, j ) . If m &gt; i , then cell ( i, m empty unless w m  X  E . If cell ( i, m ) is empty, there is no work to be done at that midpoint. If | E | X  k , then there are a maximum of k midpoints for any cell, hence the amount of work is bounded by ck for some constant c . 2
Lemma 2: If | E | X  k for some k , then the num-Proof: For a string of length N , each word w j in the string has at most N cells such that w j is the last word in the substring spanned by that cell, since each such cell must begin with a distinct word w i in the string where i  X  j , of which there are at most N . Therefore, if | E | X  k for some k , then the number of cells ( i, j ) such that w j  X  E would be no more than kN . 2
Theorem: If | E | X  k , then the parsing complex-Proof: As stated earlier, each cell ( i, j ) falls in one of three cases: 1) w j 6 X  E ; 2) w j  X  E and w i 6 X  B ; and 3) w i  X  B and w j  X  E . Case 1 cells are com-pletely closed, there is no work to be done in those cells. By Lemma 2, there are at maximum kN cells that fall in either case 2 or case 3. By Lemma 1, the amount of work for each of these cells is bounded by ck for some constant c . Therefore, the theorem is proved. 2
If | E | X  k for a constant k , the theorem proves the complexity will be O ( N ) . If | E | X  k log N , then parsing complexity will be O ( N log 2 N ) . Fig-ure 4 shows sentence length versus parsing time under three different conditions 1 : baseline (uncon-strained); O ( N log 2 N ) at | E | X  3 log N ; and linear at | E | X  16 . The bottom graph zooms in to demon-strate that the O ( N log 2 N ) constraints can outper-form the linear constraints for shorter strings (see around 20 words). As the length of the string in-creases, though, the performance lines cross, and the linear constraints demonstrate higher efficiency for the longer strings, as expected.

Unlike the method for imposing quadratic bounds, this method only makes use of set E , not B . To select the constraints, we rank the word/ E posterior probabilities, and choose the top k (either constant or scaled with a log N factor); the rest of the words fall outside of the set. In this approach, every word falls in the B set, hence no constraints on words beginning multi-word constituents are im-posed. Figure 5 plots F-measure accuracy versus time to parse the development set for four methods of imposing constraints: the previously plotted high precision and quadratic bound constraints, along with O ( N log 2 N ) and linear bound constraints us-ing methods described in this paper. All meth-ods are employed at various parameterizations, from very lightly constrained to very heavily constrained. The complexity-bound constraints are not combined with the high-precision constraints for this plot. As can be seen from the plot, the linear and O ( N log 2 N ) methods do not, as applied, achieve as favorable of an accuracy/efficiency tradeoff curve as the quadratic bound method. This is not surprising, given that no words are excluded from the set B for these methods, hence far fewer constraints over-all are applied with the new method than with the quadratic bound method.

Of course, the high precision constraints can be applied together with the complexity bound con-straints, as described in Section 4.2. For combining complexity-bound constraints with high-precision constraints, we first chose operating points for both the linear and O ( N log 2 N ) complexity bound meth-ods at the points before accuracy begins to de-grade with over-constraint. For the linear complex-ity method, the operating point is to constrain the set size of E to a maximum of 16 members, i.e., | E | X  16 . For the N log 2 N complexity method, | E | X  3 log N .

Table 2 presents results for these operating points used in conjunction with the 90% high precision constraints. For these methods, this combination is particularly important, since it includes all of the high precision constraints from the set B , which are completely ignored by both of the new methods. We can see from the results in the table that the com-bination brings the new constraint methods to very similar accuracy levels as the quadratic constraints, yet with the guarantee of scaling linearly to longer and longer sentences.

The efficiency benefits of combining constraints, shown in Table 2, are relatively small here because the dataset contains mostly shorter sentences. Space limitations prevent us from including scatter plots similar to those in Figure 3 for the constraint combi-nation trials, which show that the observed parsing time of shorter sentences is typically identical under each constraint set, while the parsing time of longer sentences tends to differ more under each condition and exhibit characteristics of the complexity bounds. Thus by combining high-precision and complexity constraints, we combine typical-case efficiency ben-efits with worst-case complexity bounds.

Note that these speedups are achieved with no additional techniques for speeding up search, i.e., modulo the cell closing mechanism, the CYK pars-ing is exhaustive X  X t explores all possible category combinations from the open cells. Techniques such or setting of probability thresholds on entries in cells X  X hese are all orthogonal to the current ap-proach, and could be applied together with them to achieve additional speedups. However, none of these other techniques provide what the current methods do: a complexity bound that will hold even in the worst case.

To validate the selected operating points on a dif-ferent section, Table 3 presents speed and accuracy results on the test set (WSJ section 23) for the exact-inference CYK parser.

We also conducted similar preliminary trials for parsing the Penn Chinese Treebank (Xue et al., 2004), which contains longer sentences and differ-ent branching characteristics in the induced gram-mar. Results are similar to those shown here, with chart constraints providing both efficiency and ac-curacy gains. We have presented a method for constraining a context-free parsing pipeline that provably achieves linear worst case complexity. Our method achieves comparable observed performance to the quadratic complexity method previously published in Roark and Hollingshead (2008). We were motivated to pursue this method by the observed linear parsing time achieved with the quadratic bound constraints, which suggested that a tighter complexity bound could be achieved without hurting performance.
We have also shown that combining methods for achieving complexity bounds X  X hich are of pri-mary utility for longer strings X  X ith methods for achieving strong observed typical case speedups can be profitable, even for shorter strings. The result-ing combination achieves both typical speedups and worst-case bounds on processing.

The presented methods may not be the only way to achieve these bounds using tagger pre-processing of this sort, though they do have the virtue of very simple constraint selection. More complicated methods that track, in fine detail, how many cells are open versus closed, run the risk of a constraint selection process that is itself quadratic in the length of the string, given that there are a quadratic number of chart cells. Even so, the presented methods criti-cally control midpoints for all cells only via the set E (words that can end a multi-word constituent) and ignore B . More complicated methods for using both sets that also achieve linear complexity (perhaps with a smaller constant), or that achieve O ( N log N ) complexity rather than O ( N log 2 N ) , may exist. This research was supported in part by NSF Grant #IIS-0447214 and DARPA grant #HR0011-08-1-0016. Any opinions, findings, conclusions or recom-mendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF or DARPA.
