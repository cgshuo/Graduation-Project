 We organized a machine translation (MT) task at the Sev-enth NTCIR Workshop. Participating groups were requested to machine translate sentences in patent documents and also search topics for retrieving patent documents across lan-guages. We analyzed the relationship between the accuracy of MT and its effects on the retrieval accuracy.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.7 [ Arti-ficial Intelligence ]: Natural Language Processing Measurement, Performance, Experimentation Cross-lingual information retrieval, Machine translation, Patent information, Evaluation measures, NTCIR
To aid research and development in cross-lingual infor-mation access, we produced a test collection for machine translation (MT) targeting Japanese and English, and orga-nized the Patent Translation Task at the Seventh NTCIR Workshop (NTCIR-7). To evaluate submissions from par-ticipating groups, which are MT results for a test data set, we used intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) [2], which had been proposed as an au-tomatic evaluation measure for MT, and human rating. In the extrinsic evaluation, we investigated the contribution of the MT to Cross-Lingual Patent Retrieval (CLPR). This pa-per focuses mainly on the extrinsic evaluation and explores the relationship between the accuracy of MT, evaluated by BLEUandhumanrating,anditseffectsonCLPR.
Research groups participated in NTCIR-7 were allowed to use any types of MT. However, compared with a knowledge-intensive rule-based MT, statistical MT (SMT), which has
In principle, for the extrinsic evaluation we were able to use all of the 1189 search topics produced in NTCIR-5. How-ever, because the length of a single claim is usually much longer than that of an ordinary sentence, the computation time for the translation can be prohibitive. Therefore, in practice we independently selected a subset of the search topics for the dry run and the formal run. If we use search topics for which the average precision of the monolingual re-trieval is small, the average precision of CLPR methods can be so small that it is difficult to distinguish the contributions of participating groups to CLPR. Therefore, we sorted the 1189 search topics according to the Average Precision (AP) of monolingual retrieval using the standard retrieval system and selected 100 topics (AP  X  0.9) and 124 topics (0.9 &gt; AP  X  0.3) for the dry run and the formal run, respectively.
Thenumberofgroupsparticipatedintheextrinsiceval-uation was 12. All of these groups also participated in the E X  X  intrinsic evaluation, in which the purpose was to ma-chine translate sentences in patent documents from English to Japanese. As a baseline system, the organizers submit-ted a result produced by Moses 1 . Table 1 shows the results for the E X  X  intrinsic evaluation and the extrinsic evalua-tion, which are denoted as  X  X ntrinsic X  and  X  X xtrinsic X , re-spectively. The rows in Table 1, each of which corresponds to the result of a single group, are sorted according to the values for BLEU in  X  X ntrinsic X . For human rating, experts evaluated each translation result based on fluency and ade-quacy, using a five-point rating. The value for  X  X uman X  is the average of adequacy and fluency. However, mainly be-cause of time and budget constraints, human rating was per-formed only for five systems. To calculate MAP values, we used both relevant and partially relevant documents as the correct answers for the top 1000 documents. In Table 1, the row  X  X ono X  shows the results for monolingual retrieval. The best MAP for CLPR obtained by HCRL is 0.3536, which is 74% of that for Mono.
 We also used Recall@N as an evaluation measure for CLPR. We calculated the correlation coefficient ( X  X  X ) between BLEU in the extrinsic evaluation and each CLPR evaluation mea-sure. We found that the value of R for MAP was 0.936 whereas the values of R for Recall@N were below 0.9, irre-spective of the value of N . In other words, we can potentially use BLEU to predict the contribution of MT to CLPR with respect to MAP, without performing retrieval experiments. This is a significant step toward the automatic evaluation of CLPR by means of the evaluation of MT. However, human rating did not correlate with MAP because as in Table 1 ts-bmt outperformed the other groups with respect to human rating, but achieved the lowest MAP.

We used the two-sided paired t -test for statistical testing with respect to MAP. We also analyzed the extent to which the BLEU value should be improved to achieve a statistically significant improvement in MAP value. Figure 1 shows the relationship between the difference in BLEU value and the level of statistical significance of the MAP value. In Figure 1, each bullet point corresponds to a comparison of two groups. The bullet points are classified into three clusters according to the level of statistical significance for MAP. The y-axis denotes the difference between the two groups X  BLEU values. http://www.statmt.org/wmt07/baseline.html
