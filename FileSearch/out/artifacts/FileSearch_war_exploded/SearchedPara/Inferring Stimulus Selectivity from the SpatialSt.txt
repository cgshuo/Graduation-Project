 Stimulus selectivity in neural networks was historically measured directly from input-driven re-sponses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that an-alyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists.
 Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 -8].
 In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. In this section, we describe the network model and the methods we use to analyze its dynamics. Sub-sequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a firing rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N  X   X  , the system  X  X elf averages X  making the description independent of a specific network architecture, see also [11, 12]). Each unit is characterized by an activation variable x i  X  i = 1 , 2 ,...N , and a nonlinear response function r i which relates to x i through r = R 0 +  X  ( x i ) where, Eq. 1 allows us to independently set the maximum firing rate R max and the background rate R 0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4].
 We introduce a recurrent weight matrix with element J ij equivalent to the strength of the synapse from unit j  X  unit i . The individual weights are chosen independently and randomly from a Gaus-sian distribution with mean and variance given by [ J ij ] J = 0 and J 2 ij brackets are ensemble averages [9 -11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Specifically, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit x i is therefore determined by the relation, with the time scale of the network set by the single-neuron time constant  X  r of 10 ms. The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-specific phase factor  X  i , chosen randomly from a uniform distribution between 0 and 2  X  , such that In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied profitably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the fixed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity.
 The instantaneous network state can be described by a point in an N -dimensional space with coor-dinates equal to the firing rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network firing rates given by, where &lt;&gt; denotes a time average. The eigenvalues of this matrix expressed as a fraction of their sum (denoted by  X   X  a in this paper), indicate the distribution of variances across the different orthogonal directions in the activity trajectory.
 Spontaneous activity is a useful indicator of recurrent effects, because it is completely determined by network feedback. We can therefore study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the spatial structure of evoked and spontaneous ac-tivity. In the spontaneous state, there are a number of significant contributors to the total variance. For instance, for g = 1 . 5 , the leading 10% of the components account for 90% of the total variance with an exponential taper for the variance associated with higher components. In addition, projec-tions of network activity onto components with smaller variances fluctuate at progressively higher frequencies, as illustrated in Fig. 1b &amp; d.
 Other models of chaotic networks have shown a regime in which an input generates a non-chaotic network response, even though the network returns to chaotic fluctuations when the external drive is turned off [5, 16]. Although chaotic intrinsic activity can be completely suppressed by the in-put in this network state, its imprint can still be detected in the spatial pattern of the non-chaotic activity. We determine that the perfectly entrained driven state is approximately two-dimensional corresponding to a circular oscillatory orbit, the projections of which are oscillations  X / 2 apart in phase. (The residual variance in the higher dimensions reflects harmonics arising naturally from the nonlinearity in the network model). 2.3 Dimensionality of spontaneous and evoked activity To quantify the dimension of the subspace containing the chaotic trajectory in more detail, we intro-duce the quantity which provides a measure of the effective number of principal components describing a trajectory. For example, if n principal components share the total variance equally, and the remaining N  X  n principal components have zero variance, N eff = n . Although recurrent feedback in the network plays an important role in the structure of driven network responses, the spatial pattern of the activity is not fixed but rather, is shaped by a complex interac-tion between the driving input and intrinsic network dynamics. It is therefore sensitive to both the amplitude and the frequency of this drive. To see this, we examine how the orientation of the approx-imately two-dimensional periodic orbit of driven network activity in the non-chaotic regime depends on input frequency. We use the technique of principal angles described above, to examine how the orientation of the oscillatory orbit changes when the input frequency is varied (angle between  X  n osc1 and  X  n osc2 in Fig. 3c). For comparison purposes, we choose the dominant two-dimensional subspace of the network oscillatory responses to a driving input at 5 Hz as a reference. We then calculate the principal angles between this subspace and the corresponding subspaces evoked by inputs with different frequencies. The result shown in Fig. 3d indicates that the orientation of the orbit for these driven states rotates as the input frequency changes.
 The frequency dependence of the orientation of the evoked response is likely related to the effect seen in Fig. 1b &amp; d in which higher frequency activity is projected onto higher principal components of the spontaneous activity. This causes the orbit of the driven activity to rotate in the direction of higher-order principal components of the spontaneous activity as the input frequency increases. In addition, we find that the larger the stimulus amplitude, the closer the response phases of the neurons are to the random phases of their external inputs (results not shown). We have shown that the response of a network to random-phase input is strongly affected by the spatial structure of spontaneous activity (Fig. 3b). We now ask if the spatial patterns that dominate the spontaneous activity in a network correspond to the spatial input patterns to which the network responds most robustly. In other words, can the spatial structure of an input be designed to maximize its ability to suppress chaos? Rather than using random-phase inputs, we align the inputs to our network along the directions defined by the different principal components of its spontaneous activity. Specifically, the input to neuron i is set to, where I is the amplitude factor and V a i is the i th component of principal component vector a of the spontaneous activity. The index a is ordered so that a = 1 corresponds to the principal component with the largest variance and a = N , the least.
 The signal amplitude when the input is aligned with different leading eigenvectors shows no strong dependence on a , but the noise amplitude exhibits a sharp transition from no chaotic component for small a to partial chaos for larger a (Fig.4b). The critical value of a depends on I , f and g but, in general, inputs aligned with the directions along which the spontaneous network activity has large projections are most effective at inducing transitions to the driven periodic state. The point a = 5 corresponds to a phase transition analogous to that seen in other network models [5, 16]. The noise is therefore more sensitive to the spatial structure of the input compared to the signal. Suppression of spontaneously generated noise in neural networks does not require stimuli so strong that they simply overwhelm fluctuations through saturation. Near the onset of chaos, complete noise suppression can be achieved with relatively low amplitude inputs (compared to the strength of the internal feedback), especially if the input is aligned with the dominant principal components of the spontaneous activity. Many models of selectivity in cortical circuits rely on knowledge of the spatial organization of affer-ent inputs as well as cortical connectivity. However, in many cortical areas, such information is not available. This is analogous to the random character of connectivity in our network which precludes Figure 4: a) An example autocorrelation function. Horizontal lines indicate how we define the signal and noise amplitudes. Parameters used for this figure are I/I 1 / 2 = 0 . 4 , g = 1 . 8 and f = 20 Hz. b) Network selectivity to different spatial patterns of input. Signal and noise amplitudes in the input-evoked response aligned to the leading principal components of the spontaneous activity of the network. The inset shows a larger range on a coarser scale. The results in this figure come from a network simulation with N =1000 , I/I 1 / 2 = 0 . 2 and f = 2 Hz for b. a simple description of the spatial distribution of activity patterns in terms of topographically orga-nized maps. Our analysis shows that even in cortical areas where the underlying connectivity does not exhibit systematic topography, dissecting the spatial patterns of fluctuations in neuronal activity can yield important insight about both intrinsic network dynamics and stimulus selectivity. Analysis of the spatial pattern of network activity reveals that even though the network connectivity network size. This suppression of spatial modes is much stronger than expected, for instance, from a linear network that low-pass filters a spatiotemporal white noise input. Further, this study extends a similar effect demonstrated in the temporal domain elsewhere [5 -8] to show that active spatial patterns exhibit strong nonlinear interaction between external driving inputs and intrinsic dynamics. dynamics.
 Our results show that experimentally accessible spatial patterns of spontaneous activity (e.g. from voltage-or calcium-sensitive optical imaging experiments) can be used to infer the stimulus selec-tivity induced by the network dynamics and to design spatially extended stimuli that evoke strong responses. This is particularly true when selectivity is measured in terms of the ability of a stimulus to entrain the neural dynamics. In general, our results indicate that the analysis of spontaneous activity can provide valuable information about the computational implications of neuronal circuitry. Acknowledgments Research of KR and LFA supported by National Science Foundation grant IBN-0235463 and an NIH Director X  X  Pioneer Award, part of the NIH Roadmap for Medical Research, through grant number 5-DP1-OD114-02. HS was partially supported by grants from the Israel Science Foundation and the McDonnell Foundation. This research was also supported by the Swartz Foundation through the Swartz Centers at Columbia, Princeton and Harvard Universities. References [1] Hubel, D.H. &amp; Wiesel, T.N. (1962) Receptive fields, binocular interaction and functional archi-tecture in the cats visual cortex. J. Physiol. 160, 106-154. [2] Arieli, A., Shoham, D., Hildesheim, R. &amp; Grinvald, A. (1995) Coherent spatiotemporal patterns of ongoing activity revealed by real-time optical imaging coupled with single-unit recording in the cat visual cortex . J. Neurophysiol. 73, 2072-2093. [3] Arieli, A., Sterkin, A., Grinvald, A. &amp; Aertsen, A. (1996) Dynamics of ongoing activity: expla-nation of the large variability in evoked cortical responses. Science 273, 1868-1871. [4] Sompolinsky, H., Crisanti, A. &amp; Sommers, H.J. (1988) Chaos in Random Neural Networks. Phys. Rev. Lett. 61, 259-262. [5] Rajan, K., Abbott, L.F. &amp; Sompolinsky, H. (2010) Stimulus-dependent Suppression of Chaos in Recurrent Neural Networks. Phys. Rev. E., 82: 01193. [6] Rajan, K. (2009) Nonchaotic Responses from Randomly Connected Networks of Model Neurons. Ph.D. Dissertation, Columbia University in the City of New York. [7] Rajan, K., Abbott, L. F., &amp; Sompolinsky, H. (2010) Stimulus-dependent Suppression of Intrinsic Variability in Recurrent Neural Networks. BMC Neuroscience, 11, O17: 11. [8] Rajan, K. (2010) What do Random Matrices Tell us about the Brain? Grace Hopper Celebration of Women in Computing, published by the Anita Borg Institute for Women &amp; Technology and the Association for Computing Machinery. [9] van Vreeswijk, C. &amp; Sompolinsky, H. (1996) Chaos in neuronal networks with balanced excita-tory and inhibitory activity. Science 24, 1724-1726. [10] van Vreeswijk, C. &amp; Sompolinsky, H. (1998) Chaotic balanced state in a model of cortical circuits. Neural Comput. 10, 1321-1371. [11] Shriki, O., Hansel, D. &amp; Sompolinsky, H. (2003) Rate models for conductance-based cortical neuronal networks. Neural Comput. 15, 1809-1841. [12] Wong, K.-F. &amp; Wang, X.-J. (2006) A Recurrent network mechanism of time integration in perceptual decisions. J. Neurosci. 26, 1314-1328. [13] Rajan, K. &amp; Abbott, L.F. (2006) Eigenvalue spectra of random matrices for neural networks. Phys. Rev. Lett. 97, 188104. [14] Broome, B.M., Jayaraman, V. &amp; Laurent, G. (2006) Encoding and decoding of overlapping odor sequences. Neuron 51, 467-482. [15] Ipsen, I.C.F. &amp; Meyer, C.D. (1995) The angle between complementary subspaces. Amer. Math. Monthly 102, 904-911. [16] Bertchinger, N. &amp; Natschl X ger, T. (1995) Real-time computation at the edge of chaos in recur-rent neural networks. Neural Comput. 16, 1413-1436. [17] Molgedey, L., Schuchhardt, J. &amp; Schuster, H.G. (1992) Suppressing chaos in neural networks by noise. Phys. Rev. Lett. 69, 3717-3719.
