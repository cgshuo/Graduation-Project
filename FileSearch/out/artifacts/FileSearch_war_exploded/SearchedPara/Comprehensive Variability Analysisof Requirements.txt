 Variability analysis deals with determining the degree of similarity of different software artifacts, commonly in order to improve the effectiveness and ef development and maintenance through increase of reuse [ 7 ]. Variability analysis is extensively studied in the fi eld of Software Product Line Engineering (SPLE) [ 11 , 21 ], where variability is considered  X  an assumption about how members of a family may differ from each other  X  [ 28 ]. Variability analysis is known as time consuming and error-prone. Thus, various studies have suggested automatizing variability analysis using different software development artifacts. Many of these studies concentrate on analyzing the differences of requirements (e.g., [ 2 , 12 , 15 ]), perceiving reuse of requirements very important since requirements are essential in all development approaches and elicited and speci fi ed early in the software development lifecycle [ 11 ]. These studies frequently apply semantic, syntactic, metric-based, or graph-based similarity measurements and utilize clustering algorithms. The result is presented in a form of variability models, most notably feature diagrams [ 14 ]. Other types of and code [ 3 ]. In contrast, testing artifacts seem to attract less attention in variability analysis [ 10 ]. This may be due to their reliance on other development artifacts (i.e., the requirements that they aim to test) or their description of speci sometimes include particular values or conditions). Moreover, to the best of our knowledge, utilizing different types of development artifacts in order to coherently analyze variability has not been studied. We claim that analyzing artifacts from dif-ferent, but related, development phases may result in more comprehensive variability analysis outcomes that better represent the similarities and differences among software products and may consequentially increase reuse and improve software development and maintenance.
 To this end, we propose in this paper to utilize information stored and managed on requirements and testing artifacts in existing software development tools. These kinds of artifacts are highly related in most development approaches and refer to software behaviors rather than to concrete implementations. Particularly, we explore Application Lifecycle Management (ALM) environments whose aim is to plan, govern, and coordinate the software lifecycle tasks. Although ALM environments are geared towards development of single products, we propose here to utilize them in order to analyze the variability of different software products managed in their repository. Particularly, we introduce a method, named Semantic and Ontological Variability Analysis based on Requirements and Test cases (or SOVA R-TC for short), that extracts software behaviors from requirements and testing artifacts, enables their comparison to other behaviors at different level of abstraction, and identi similar behaviors. Those variants set the ground for comprehensive variability analysis. The rest of the paper is structured as follows: Sect. 2 reviews the background and related work, motivating the need to analyze variability of both requirements and testing artifacts. Section 3 elaborates on the suggested approach, while Sect. 4 presents insights from preliminary analysis of the approach outcomes. Finally, Sect. 5 concludes and provides directions for future research. 2.1 Application Lifecycle Management (ALM) Application Lifecycle Management (ALM) environments [ 15 , 16 ] aim to support the development of software products from their initial planning through retirement. Their main advantages are: (1) maintaining high level of traceability between artifacts produced in different development phases, e.g., requirements and testing artifacts; (2) reporting on the development progress in real time to different stakeholders; and (3) improving stakeholders  X  communication across development tasks, e.g., developers can easily access the complete information about the failure of a test case and its results which led to fi nding defects (bugs). Due to those bene fi implementations of ALM can be found in the industry. Most of them support software requirements de fi nition and management, software change and con agement (SCM), software project planning, quality management (testing), and defect management. The fi rst generation of environments, called ALM 1.0, integrates a few individual tools for helping stakeholders perform their tasks. The next generation, called ALM 2.0, proposes a holistic platform (rather than a collection of tools) for coordinating and managing development activities [ 15 ].
 BigLever [ 6 ]  X  a leading vendor of product line solutions multi -phase as one of three dimensions in their SPLE framework (the other two dimensions are multi -baseline referring to evolution of artifacts over time and multi -product referring to the diversity of products in the same software product line). The multi-phase dimension directly refers to the development lifecycle phases. It concerns consistency and traceability among asset variations in different lifecycle phases. Yet, this dimension addresses the development of single software products.
 (i.e., existing product artifacts are re-engineered into a software product line) or reactively (i.e., one or several products are built before the core assets are developed). In those scenarios the information stored in ALM environments for different software products can be utilized to analyze their variability.
 different methods have been suggested for analyzing variability of different types of software artifacts stored in ALM environments, most notably requirements [ 10 ]. Next we review relevant studies on variability analysis at different development phases, concentrating of requirements engineering and testing artifacts. 2.2 Variability Analysis at Different Development Phases Recently, Bakar et al. [ 4 ] conducted a systematic literature review on feature extraction from requirements expressed in a natural language. The main conclusions of this review is that most studies use Software Requirements Speci but product descriptions, brochures, and user comments are also used due to practical reasons. The outputs of the suggested methods are commonly feature diagrams [ 14 ], clustered requirements, keywords or direct objects. Moreover, the extraction process can be divided into four phases: (1) requirements assessment, (2) terms extraction (using different techniques, such as algebraic models, similarity metrics, and natural language processing tools), (3) features identi fi cation, and (4) feature diagram (or variability model) formation. Most studies automatize the second phase of terms extraction, while the other phases are commonly done manually. A work that addresses the automatization of phases 3 and 4 in addition to that of phase 2 is SOVA (Semantic and Ontological Variability Analysis) which analyzes requirements variability based on ontological and semantic considerations [ 20 , 22 ]. Due to the high relevance of SOVA to this work, we elaborate on it in Sect. 2.3 .
 ability management [ 10 ]. According to [ 10 ], only FAST tion, Speci fi cation and Translation  X  can be considered covering the full lifecycle phases, from requirements engineering to testing. However, this approach concentrates on documentation and representation of variability and not on its analysis. Other development artifacts, e.g., design artifacts [ 1 , 17 ] and code [ 24 ], have also been analyzed to fi nd differences between software products. A few approaches, e.g., [ 1 , 25 ], further propose utilizing several distinct sources of information for analyzing variability. However, these sources commonly belong to the same lifecycle phase (e.g., textual feature descriptions and feature dependencies belonging to the requirements engineering phase; or software architecture and plugin dependencies belonging to the design phase). In our work, we aim to explore how analyzing the variability of artifacts from different lifecycle phases, particularly, requirements engineering and testing, can contribute to understanding the differences between various software products. 2.3 Semantic and Ontological Variability Analysis (SOVA) SOVA aims to analyze variability among different software products based on their and the fi nal states (post-conditions) are identi fi ed. This is done by parsing the requirement text utilizing the Semantic Role Labeling (SRL) technique [ 13 ]. Six roles that have special importance to functionality are used in SOVA: (1) Agent performs?; (2) Action  X  What is performed?; (3) Object  X  On what objects is it performed?; (4) Instrument  X  How is it performed?; (5) Temporal modi performed?; And (6) Adverbial modi fi er  X  In what conditions is it preformed? Based on these roles, the different phrases of the requirements (called vectors) are classi fi ed into initial states, external events, and fi nal states. External events are: (1) Action vectors (i.e., vectors identi fi ed by verbs) whose agents are external and their actions are active , or (2) Action vectors whose agents are internal and their actions are passive . Oppositely, states are: (1) Action vectors whose agents are internal and ac-tions are active , or (2) Action vectors whose agents are external and actions are passive . The decision whether a vector is classi fi ed as an initial state or a done according to the place of the vector with respect to other vectors classi external events. Particularly, initial states are: (1) Action vectors classi appear before the fi rst external events in the requirements, or (2) Non-action vectors (identi fi ed by temporal or adverbial modi fi ers) that appear before the events in the requirements. Conversely, fi nal states are action vectors classi states that appear after the last external event in the requirements.
 As an example, consider Fig. 1 which presents SOVA  X  s parsing outcome for the requirement: When a borrower returns a book copy, the system updates the number of is returning a book copy by a borrower; and the fi nal state is derived from updating the number of available copies of the book.
 events, and fi nal states, SOVA enables comparison of requirements, belonging to different software products, based on different perspectives. In [ 20 ], two perspectives are mainly discussed: structural  X  in which focus is put (through controlling weights) on differences in the initial and fi nal states  X  and functional differences in the external events. The fi nal outcomes of SOVA are feature diagrams organized according to selected perspectives.
 software behaviors [ 22 ], its success heavily depends on the level of details of the requirements and especially on their ability to express the initial state, external event, and fi nal state. From practice, it is known that requirements are not always complete and particularly do not explicitly specify all pre-conditions and post-conditions. Hence, we aim to overcome this limitation by using the corresponding testing artifacts. To motivate the need, consider the following two requirements which may appear in different library management systems to describe book return functionality: 1. When a borrower returns a copy of a book that can be pre-ordered by other bor-2. When a borrower returns a book copy, the system updates the number of available However, they differ in their pre-and post-conditions. While the post-conditions are speci fi ed in the requirements (  X  the system sends a message to the borrower who is waiting for this book  X  for the fi rst requirement and  X  the system updates the number of available copies of the book  X  for the second requirement), the preconditions in this example are not explicitly speci fi ed. Speci fi cally, in the there is a borrower waiting for the returned book is not mentioned. As a result, the the variability analysis will be negatively affected. As we claim next, utilizing test artifacts associated with those requirements may improve variability analysis (in-creasing or decreasing the similarity of the corresponding requirements). Our working hypothesis is that what commonly matters to stakeholders involved in different development lifecycle phases, including requirement engineers and testers, is the expected behavior of the implemented software. This premise is manifested by concepts such as functional requirements or functional testing. Therefore, the suggested approach, called SOVA R-TC, concentrates on functional requirements (hereafter requirements, for short) and the test cases associated to them. SOVA R-TC employs a view of a software product as a set of intended changes in a given application domain. We term such changes software behaviors. The approach uses an ontological model of behaviors based on concepts from Bunge  X  s work [ 8 , 9 ]. This model is described in Sect. 3.1 . We further develop an ALM metamodel that concentrates on requirements and testing artifacts (Sect. 3.2 ) and use it for identifying variants based on the onto-logical model (Sect. 3.3 ). 3.1 The Ontological Model of Bunge Bunge  X  s work [ 8 , 9 ] describes the world as made of things that possess properties . Properties are known via attributes , which are characteristics assigned to things by humans. Software products can be considered things.
 To de fi ne or represent things and compare them, we have to de view from which we wish to conduct the analysis. For example, libraries and car rental agencies may be perceived differently, since the fi rst one deals with borrowing books that are different in terms of structure and functionality from cars, the focus of the second thing. However, these things can be perceived as very similar if we conduct the analysis from the point of view of checking out of items: both libraries and car rental agencies handle checking out of physical items (either books or cars).
 To de fi ne the point of view, we use Bunge  X  s notion of a state variable vector of state variables  X  values at a particular point in time. The abstraction level of and the license number of a car vs. an item identity. An event is a change of a state of a thing and can be external or internal: an external event is a change in the state of a thing as a result of an action of another thing (e.g., borrowing a book by a person or renting a car by a client), whereas an internal event arises due to an internal transformation in the thing (e.g., operations triggered by the library itself or the car rental agency). Corre-spondingly, a state can be stable or unstable: a stable state can be changed only by an external event while an unstable state may be changed by an internal event. Bunge  X  s ontological concepts have been widely adapted to conceptual modeling in the context of systems analysis and design [ 26 , 27 ]. In [ 23 ], Bunge is suggested to de fi ne software behavior as a triplet of an initial state describing the stable state the system is in before the behavior occurs, a sequence of external events that trigger the behavior, and a fi nal state specifying the stable state the system reaches after the behavior terminates. Both initial and fi nal states are described with respect to relevant state variables, allowing for variability analysis in different granularity levels. Formally expressed: De fi nition 1 (Behavior). Given a stable state s &lt;e &gt;, a behavior is a triplet (s 1 ,&lt;e i &gt;, s * ), where s reaches when it is in state s 1 and the sequence of external events &lt;e termed the initial state of the behavior and s *  X  the fi are de fi ned over a set of state variables SV = {x 1 ... assignments to x 1 ... x n . 3.2 An ALM Metamodel In order to use Bunge  X  s concepts as a basis for comprehensively analyzing variability of software products, we turn now to the introduction of a partial ALM metamodel that depicts the characteristics of requirements and testing artifacts, as well as their rela-tions. Exploring IBM  X  s Collaborative Lifecycle Management (CLM) solution is one of the leading ALM environments [ 29 ], and different ISO standards, most notably, ISO/IEC/IEEE 29119-3 on software testing documentation [ 19 ], we drafted the metamodel in Fig. 2 . A requirement exhibits just a textual description (in a natural According to [ 19 ], a test case is a set of  X  preconditions, inputs (including actions, where applicable), and expected results, developed to drive the execution of a test item to meet test objectives, including correct implementation, error identi quality, and other valued information  X  . A test case precondition describes state of the test environment and any special constraints pertaining to the execution of the test case,  X  whereas inputs are the  X  data information used to drive test execution. An expected result is  X  observable predicted behavior of the test item under speci conditions based on its speci fi cation or another source. are de fi ned as actions  X  required to bring the test item into a state where the expected result can be compared to the actual results.  X  This is in-line with the realization of inputs in existing ALM environments as fi elds named actions or test steps. straightforward: the preconditions de fi ne the initial state of the behavior (test case scenario), but also some technical constraints (e.g., regarding the environment); the inputs are the events that trigger (  X  drive  X  ) the behavior ( results partially 2 de fi ne the fi nal states of the behavior.
 case for each one of them. Generally, a single requirement may be validated by several test cases, e.g., a test case that validates the main scenario and test cases for different exceptions. Similarly, a single test case can validate more than one requirement, e.g., a test case that validates an exception common to different behaviors (requirements). Thus, the relations between requirements and test cases are many-to-many. parts of the behavior they represent. For a requirement s and s* is the set of expected results.
 De fi nition 2 (A Requirement). A requirement 3 R is represented by R = (rs the sequence of events triggering that behavior, and rs* is the behavior. De fi nition 3 (A Test Case). A test case TC is represented by TC = (pre, &lt;inp&gt;, rst), an ordered set of inputs, and rst is a set of expected results of the tested behavior. In this paper, we assume that the test case inputs are actually realizations of the requirement external events (describing how the external events are captured by the system) and therefore we concentrate on the state of the system before the behavior occurs (as speci fi ed both in the initial states of the requirements and the preconditions of the test cases) and the state of the system after the behavior occurs (as speci in the fi nal states of the requirements and the expected results of the test cases). Note that these two parts of the behavior (s 1 and s*) represent states and hence can be perceived as de fi ned by possible assignments to state variables. Returning to the examples in Table 1 , the precondition of the fi rst test case refers to two assignments of state variables  X  the book can be pre-ordered (book = can_be_preordered) and a borrower is waiting for the book (borrower = is_waiting_for_the_book). Each of the post-conditions of the two test cases refers to an assignment to a state variable: sending a message (message_to_the_borrower = sent) for the fi rst test case and updating the number of available copies (number_of_available_copies = increased_by_1) for the second test case. Next we formally de fi ne states (e.g., rs pairs describing assignments to state variables.
 De fi nition 4 (States). Let SV = {x (x )={v i1 ,v i2 , ... } is the possible values (domain) of x s = {(x i ,v ij )| x i 2 SV and v ij 2 Dom(x i )}.
 (in the form of requirements and test cases), the extraction of state variables and assignments uses a natural language processing technique. Particularly, using the Semantic Role Labeling (SRL) technique mentioned in Sect. 2.3 , the state variables are extracted from the object parts of the phrases, while the assignments are extracted from the action parts. The examples above follow these rules. 3.3 Behavior Transformations Deduced from Requirements Using Bunge  X  s ontological model and the metamodel introduced above, we aim to explore the relations between a requirement and its associated test cases in order to improve variability analysis. Particularly, SOVA R-TC compares the different parts of behaviors as speci fi ed in the requirements and their associated test cases. level functionality of the system, test cases detail how the functionality should be tested. Therefore, the initial state of a requirement may be different from the precon-may be missing or the preconditions of the test cases may re requirement. Similarly, the expected results of the test cases may differ (at least in the level of details) from the fi nal state of the requirements. We do not refer to these differences as inconsistencies, but as differences in scope or in level of speci Moreover, a single requirement may be associated to different test cases, each of which describes a different scenario to be tested. We thus consider the intersection of these scenarios (which describes the characteristics of the behavior rather than of a particular scenario/test case) and unify this intersection with the requirement in order to enrich the speci fi cation of the initial and fi nal states. This is expressed through the notion of behavior transformation de fi ned next.
 De fi nition 5 (Behavior Transformation). Given a requirement R = (s set of test cases {TC 1 , ... TC n } associated to it, such that TC we de fi ne the behavior transformation bt as (uis, ufs), where uis = s uni fi ed initial state of the behavior and ufs = s* [ i  X  single requirement with two associated test cases. The initial state of behavior does not explicitly appear in the requirement and thus SOVA does not extract it. The generally refers to update of a certain variable (the number of available copies of the book). Using the two test cases associated to this requirement, we learn on two possible scenarios: one in which  X  no borrower pre-ordered the book least one borrower is waiting for the book.  X  In both scenarios it is assumed that book was borrowed  X  and thus we can conclude that this assignment of the state variable (book_status) characterizes the behavior rather than provides technical conditions of speci fi c test cases. As a result the uni fi ed initial state of the behavior transformation is  X  the book was borrowed.  X  For similar reasons, the expected result of notifying the borrower who is waiting for the book about the arrival is considered scenario-speci and hence the uni fi ed fi nal state of the behavior transformation refers only to the state variable  X  number of available copies of the book.  X  Here both requirement and test to the need to update this state variable, while the test cases refer to how this state variable needs to be updated  X  increase by 1. Assuming that test cases are more detailed than requirements and refer to state variables in a higher level of details, we adopt the assignment proposed to a state variable by test cases. In other words, if the same state variable appears both in the requirements and in the test cases with different proposed is  X  the system increases the number of available copies of the book by 1. 3.4 Calculating the Similarity of Requirements Considering Their Associated Test Cases The basis for analyzing variability in SOVA R-TC is identifying variants of similar behaviors. Thus, given a set of software products, each represented by requirements and their associated test cases, we calculate the similarity of behaviors as follows. De fi nition 6 (Behavior Similarity). Given two requirements R ciated transformation behaviors bt 1 = (uis 1 , ufs 1 ) and bt and a semantic similarity sim, the behavior similarity, Sim of the pair-wise semantic similarities of their uni fi ed initial and expressed: Where:  X  w  X  sim x  X  uis similar assignments both in the uni fi ed initial and fi nal states of the compared behaviors. To this end, different semantic similarity measures can be used. Those measures are commonly classi fi ed as corpus-based or knowledge-based [ 18 ]. Corpus-based measures identify the degree of similarity based on information derived from large corpora, while knowledge-based measures use information drawn from semantic networks. Combining corpus-and knowledge-based semantic approaches, the measure suggested by Mihalcea et al. (MSC) [ 18 ] calculates sentence similarity by fi nding the most similar words in the same part of speech class. The derived word similarity scores are weighted with the inverse document frequency scores that belong to the corresponding word.
 the uni fi ed initial and fi nal states. Basically, we could assume w however, we wanted to enable the analysts to fi ne tune the calculation, based on observations they may have, e.g., regarding the accuracy and completeness of the uni fi ed initial states vs. the uni fi ed fi nal states of the compared software products. their associated test cases, consider Fig. 3 . The two requirements, taken from different products, are similar in the sense that they handle borrowing books. However, the similarity value of SOVA is 0.5 and the similarity value of MCS is 0.6, pointing on medium similarity. Calculating the behavior similarity of these requirements in SOVA R-TC, using the same basic semantic metric (MCS), we get a higher value of 0.7 which better depicts the expected conclusion that the behaviors can be considered variants of each other.
 We developed a tool supporting SOVA R-TC and analyzed a set of requirements and their associated test cases. Based on these examples we extracted patterns that worth further discussion and explorations. Note that although we are interested in the dif-ferences between analyzing the variability based only on requirements (the output of SOVA) and additionally utilizing test cases (the output of SOVA R-TC), the patterns are presented and explained based on the relations between the requirements and the test cases in the compared behaviors. Particularly, we refer to the requirement using the notation of rs 1 and rs*, and to the intersection of test cases via the notation of tcs and tcs* = \ i  X  1 :: n rst i . Remember that s 1 =rs 1 [ lyzing the four intersection possibilities between sets, Table 3 summarizes eight pat-terns, their characteristics, and implications on requirements similarity (and consequently on their variability analysis). Those patterns can be used in the future to study the impact of utilizing different types of software artifacts on the comprehen-siveness of the variability analysis of their corresponding software products. Perceiving requirements as essential artifacts in software development, we advocate for utilizing test cases in order to better understand the similarity, and consequently the variability, of software products. Analyzing the similarity of requirements not just from their textual descriptions but also from their associated artifacts (test cases in our research), may improve understanding the requirements context and improve their reuse. This is especially important when the requirements are known to be incomplete or less detailed (e.g., in agile development approaches). The relevant information to do that is already stored and managed in Application Lifecycle Management (ALM) en-vironments that are commonly used in software development. We suggest SOVA R-TC which utilizes ALM environments in order to extract the uni of the behavior transformations and calculate the similarity of requirements considering the associated test cases. Based on this analysis, decisions on SPLE adoption (in extractive and reactive scenarios) can evidentially be taken.
 In the future, we plan to further explore the patterns and evaluate their existence and potential meanings in different case studies. The evaluation will be done in comparison to existing methods and interviewing developers. We also intend to explore combination of patterns and examine similarity and variability of events (inputs) and how they impact requirements reuse, potentially adding patterns. Moreover, we plan to explore the impact of different types of relations between requirements and test cases (e.g., main scenarios vs. exceptions) on requirements similarity and variability.
