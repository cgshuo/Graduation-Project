 Since manually constructing domain-specific sentiment lex-icons is extremely time consuming and it may not even be feasible for domains where linguistic expertise is not avail-able, research on automatic construction of domain-specific sentiment lexicons has become a hot topic in recent years. The main contribution of this paper is the illustration of a novel semi-supervised learning method which exploits both term-to-term and document-to-term relations hidden in a corpus for the construction of domain-specific sentiment lex-icons. More specifically, the proposed two-pass pseudo label-ing method combines shallow linguistic parsing and corpus-base statistical learning to make domain-specific sentiment extraction scalable with respect to the sheer volume of opin-ionated documents archived on the Internet these days. Our experiments show that the proposed method can generate high quality domain-specific sentiment lexicons according to users X  evaluation.
 I.2.6 [ Artificial Intelligence ]: Learning; I.2.7 [ Artificial Intelligence ]: Natural Language Processing ; H.3.3 [ Information Search and Retrieval ]: Text Mining Algorithms, Performance, Experimentation Sentiment Lexicon, Sentiment Analysis, Text Mining, Sta-tistical Learning
Sentiment analysis or opinion mining is a hot research topic given the sheer volume of user-contributed opinionated documents in the era of Web 2.0 [14, 17, 20, 21]. Utilizing sentiment lexicons, also called polarity lexicons or opinion lexicons, to identify the sentiments embedded in opinionated documents and predicting the polarities (i.e., orientations) of these sentiments has been a widely used approach [3, 4, 19]. Nevertheless, manually constructing sentiment lexicons for various domains is extremely labor intensive. More re-cently, researchers have examined various methods such as linguistic-based methods [7, 16], lexical graph-based prop-agation methods [18], and corpus-based statistical learning methods [1, 5] to automatically construct sentiment lexicons. Nevertheless, solely based on linguistic rules to identify sen-timents and predict their polarities may lead to the low re-call problem, i.e., many sentiment words may be missed out due to the extreme flexibility of language usages for most natural languages. On the other hand, supervised machine learning methods suffer from the problem of low autonomy (e.g., large human effort is often involved in labeling train-ing examples) [14, 21]. Semi-supervised sentiment lexicon construction methods seem promising since they strives for optimizing two probably contradictory requirements, that is, high recall and high learning autonomy.
The main intuition behind the proposed computational method for domain-specific sentiment lexicon learning is de-picted in Figure 1. The observable layer represent the re-ality (e.g., labeled or unlabeled opinionated documents and the terms contained within them) that the proposed compu-tational method can inspect to extract the domain-specific sentiments which are captured by the hidden layer. Fig-ure 1 also highlights the  X  X istributional characteristic X  of sentiments in positively and negatively opinionated docu-ments, respectively. Basically, a positively opinionated doc-ument tends to contain more positive sentiments than nega-tive sentiments; this is also the reason why the document is regarded as positive in the first place. The same concept is applied to negatively opinionated documents. When a sheer volume of opinionated documents is readily available (the reality in the era of Web 2.0), an effective lexicon learning mechanism can exploit the distributional characteristics of a term in positively and negatively opinionated documents (the observable layer) to estimate the probability of the term having a specific polarity (the hidden layer).

Moreover, term relationships exist among sentiments since they may be semantically related (e.g.,  X  X ove X  and  X  X ike X ). However, these relationships may be hidden in a corpus. An effective lexicon learning mechanism should also explore term relationships to discover new sentiments based on the known seeding sentiments. In addition, Figure 1 pinpoints the domain-specific nature of sentiments. A sentiment deemed negative in one domain (e.g.,  X  X mall X  hotel room) may well be regarded as positive in another domain (e.g.,  X  X mall X  physical size of a netbook). It is true that not all opin-ionated documents are labeled (e.g., financial news). The proposed sentiment lexicon construction method can exploit the hidden term relationships between seeding sentiments and candidate sentiments of a corpus to discover the initial domain-specific sentiments. These sentiments can then be use to tentatively label (i.e., pseudo labeling) opinionated documents in order to extract more domain-specific senti-ments.
 Figure 1: The Intuition of the Pseudo Labeling Method
The proposed semi-supervised computational method for two-pass domain-specific sentiment lexicon construction can be summarized with the following steps: 1. Extract domain-independent sentiments based on ex-2. Apply the principle of  X  X ontext coherence X  to extract 3. Apply the proposed SMI measure to identify statisti-4. Pseudo label the opinionated documents of a corpus; 5. Apply the proposed statistical learning method SE to
One can use the manual labels such as  X  X trong subjective X  in OpinionFinder [19] or a high sentiment score in Senti-WordNet [4] to extract the domain-independent sentiments. To promote language independency of the proposed method, we do not rely on fragile language specific rules nor sentence dependency parsing method to extract domain-specific sen-timents; instead only the general principle of  X  X ontextual coherence X  is applied. In particular, we assume that senti-ments appearing within the same sentence should have the same polarity unless contrary indicators such as  X  X ot X ,  X  X o X ,  X  X ut X ,  X  X xcept X , etc. appear around the candidate senti-ment. Both the domain-independent sentiments extracted from a generic sentiment lexicon and the contextually in-ferred domain-specific sentiments from a specific review cor-pus are used to construct an initial domain-specific senti-ment lexicon. PMI has been applied to extract sentiments statistically correlated with the seeding sentiments from a Web corpus [20]. To extract candidate sentiments statisti-cally associated with the sentiments captured in the initial domain-specific sentiment lexicon, we extend the PMI mea-sure to develop the sentiment mutual information estimator (SMI). The extended PMI measure has been successfully applied to concept extraction in ontology discovery [12, 9].
Through contextual inference and statistical inference, ini-tial domain-specific sentiments are discovered. These ini-tial domain-specific sentiments and the domain-independent sentiments extract from a generic sentiment lexicon are ap-plied to assign pseudo polarity labels to opinionated doc-uments of a corpus (e.g., online financial reviews). In the field of information retrieval (IR), the distributional char-acteristic of terms in relevant and non-relevant documents have been explored to induce an information seeker X  X  real interests [8, 10, 11]. The basic assumption is that terms occur more often in the set of relevant documents are likely representing the information seeker X  X  need. Accordingly, the measure of Keyword Classifier (KC) has been developed to extract positive, negative, and neutral terms representing the information seeker X  X  positive, negative, or neutral infor-mation needs [8]. The KC measure has been successfully ap-plied to induced the beliefs for adaptive information filtering later on [10, 11]. For our research, we adapt the KC mea-sure to develop a statistical learning mechanism called sen-timent extractor (SE). The formulations of our SE method are shown as follows: polarity ( t ) = where SE ( t ) indicates how likely the term t is a sentiment; the higher the sentiment estimation score, the more likely the term is a sentiment. The term t carries a specific POS such as Adjective; this is one of the ways to prune noisy sentiments. The parameters $ pos =  X  pos  X | D | and $ neg  X  neg  X | D | control the learning rates for positive and neg-ative sentiments, respectively. The set D = D +  X  D  X  rep-resents the corpus of opinionated documents, whereas D + ( D  X  ) is the set of positively (negatively) rated documents; the document label can be generated though pseudo la-beling or directly retrieved from Web 2.0 sites. The term ity that a document is rated positive given that it contains a candidate sentiment term t . It is expressed as the frac-tion of the number of positive documents (i.e., document frequency df ) which contain the term t (i.e., df ( D + t the total number of documents which contain t (i.e., df ( t )). probability that a document is rated negative when it con-tains the term t . In addition, Pr ( Pos ) = | D + the estimated priori probability that a document is posi-probability that a document is negative in the collection.
Basically, Eq. 2 is applied to prune noisy sentiments. In particular, only the candidate terms with the | SE ( t ) | score greater than a threshold  X  will be identified as positive or negative sentiments. The parameters  X  pos ,  X  pos , and  X  are empirically established based on the training corpus of a specific domain. For the experiments reported in this paper, we adopted the following values:  X  pos = 0 . 02%,  X  pos = 0 . 01%, and  X  = 0 . 05. The computational complexity of the proposed Pseudo Labeling method can be character-ized by O ( L s L d | D | ), where L s is the number of seeding sen-timents, and L d is the average document length of a corpus; | D | is the size of the corpus. It has a linear time complexity with respect to the size of the input document collection and the number of seeding sentiments. To predict the polarity of an unseen opinionated document (e.g., an online financial news), the automatically constructed domain-specific senti-ment lexicon is referred to. We apply the unigram language model originally developed in the field of IR to develop a probabilistic measure to estimate the strength of polarity of an opinionated document [15].
Similar to previous studies [2, 16], we retrieved real-world opinionated documents from the Web to build our evalua-tion data set. More specifically, we downloaded online re-views from amazon.com, tripadvisor.com, mldb.com, and re-suters.com using the provided APIs (e.g., amazon.com and tripadvisor.com) or invoking our crawler programs (e.g., for mldb.com and reuters.com). A total of 993 , 324 positive reviews and 358 , 144 negative reviews were used in our ex-periment. For amazon.com and tripadvisor.com, the reviews with user rating 4 and 5 were treated as positive documents, and the reviews with user rating 1 were taken as negative documents. The products reviews of books, automotive gears, computers (PC hardware), digital cameras, music, and health care products extracted from amazon.com rep-resent six different application domains. For movie reviews extracted from mldb.com, a review with user ratings 8 to 10 was considered positive, and a review with user ratings 1 to 3 was considered negative. Nevertheless, user-contributed label was not available for the financial news retrieved from reuters.com.

We examined the intrinsic quality of the domain-specific lexicons generated by the proposed system. More specifi-cally, we applied the P@100 (precision at 100 documents) measure to evaluate the quality of the domain-specific sen-timents generated by the system when compared to the ground truths provided by three human annotators. A sim-ilar approach was adopted to evaluate a topic-specific senti-ment lexicon construction method before [6]; a performance measure similar to P@10 (precision at 10 documents ) adopted in the TREC Blog Track [13] was applied to our experi-ment. 513 positive and 527 negative seeding sentiments were manually extracted from OpinionFinder [19] by three human annotators. These sentiments were considered generic and applicable to different domains. The same annotators also assessed the quality of the sentiment lexicons generated by the proposed method. Each human annotator examined the positive and the negative lists of sentiments generated by the proposed Pseudo Labeling (PL) method individually. If there was a disagreement about the polarity of a system gen-erated sentiment, the third annotator X  X  judgment would be used to determine the final ground truth. The inter-rater agreement of these annotators was  X  = 0 . 81 according to Cohen X  X  Kappa statistic. The high inter-rater agreement of our annotators shows that their judgment is reliable, and our  X  value is close to that obtained in a previous study [7]. Table 1: The Intrinsic Evaluation of Sentiment Lex-icons
The results of the intrinsic assessment of the domain-specific lexicons generated by the PL system are listed in Table 1. The figures represent the average precision for the system generated positive and negative sentiments. The re-sults achieved by using user labeled (UL) opinionated docu-ments for domains such as books, computers, cameras, etc. are listed under the third column; such results can be seen as the upper bound precision of the system generated sen-timent lexicons. For the financial news downloaded from reuters.com, user-contributed polarity labels of documents were not available. As can be seen in Table 1, the pro-posed PL method achieves average precisions (listed under the second column) close to the supper bound performance in various domains. The average P@100 precision achieved by the PL method is 0 . 828. The upper bound of the P@100 precision achieved by using user-labeled documents is 0 . 838. On average, the result produced by the PL method is only 1 . 4% inferior to that produced by using user-contributed la-bels of opinionated document to extract the domain-specific sentiments. However, the distinct advantage of the proposed PL method is that labor-intensive human labeling of opin-ionated documents is not required. Table 2 show the top 20 positive and negative domain-specific sentiments extracted from the finance domain. The domain-specific sentiments were discovered using the proposed PL sentiment lexicon construction method.
The main contribution of this paper is the illustration of Table 2: Top 20 Domain-specific Sentiments (Fi-nance) Discovered by System a scalable semi-supervised method for the automatic con-struction of domain-specific sentiment lexicons across differ-ent application domains. Based on direct human assessment, the intrinsic evaluation of the domain-specific sentiment lex-icons automatically generated by the proposed computa-tional method reveals that it can discover sentiments with good quality; the average P@100 achieved is 0 . 828. Future work will conduct more experiments to examine the effec-tiveness of the proposed pseudo labeling method across more diversified application domains. The utility of the automat-ically constructed domain-specific sentiment lexicons will be examined based on polarity detection tasks at the document and the sentence levels, respectively.
 The work reported in this paper has been funded in part by the Hong Kong RGC X  X  GRF Research Grant (Project: 9041569). [1] G. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and [2] N. Archak, A. Ghose, and P. Ipeirotis. Show me the [3] S. Das and M. Chen. Yahoo! for amazon: Sentiment [4] A. Esuli and F. Sebastiani. Determining the semantic [5] B. He, C. Macdonald, J. He, and I. Ounis. An effective [6] V. Jijkoun, M. de Rijke, and W. Weerkamp.
 [7] H. Kanayama and T. Nasukawa. Fully automatic [8] T. Kindo, H. Yoshida, T. Morimoto, and [9] R.Y.K. Lau, C.L. Lai, and Y. Li. Fuzzy ontology [10] R.Y.K. Lau, P. Bruza, and D. Song. Belief Revision [11] R.Y.K. Lau, P. Bruza, and D. Song. Towards a Belief [12] R.Y.K. Lau, D. Song, Y. Li, C.H. Cheung, and J.X. [13] I. Ounis, C. Macdonald, and I. Soboroff. Overview of [14] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [15] J. Ponte and W. B. Croft. A language modeling [16] G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding [17] G. Qiu, B. Liu, J. Bu, and C. Chen. Opinion word [18] D. Rao and D. Ravichandran. Semi-supervised [19] E. Riloff, T. Wilson, P. Hoffmann, S. Somasundaran, [20] P.D. Turney and M. Littman. Measuring praise and [21] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
