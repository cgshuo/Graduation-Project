 Kernel SVM is prohibitively expensive when dealing with large nonlinear data. While ensembles of linear classifiers have been proposed to address this inefficiency, these meth-ods are time-consuming or lack robustness. We propose an efficient classifier for nonlinear data using a new iterative learning algorithm, which partitions the data into clusters, and then trains a linear SVM for each cluster. These two steps are combined into a graphical model, with the param-eters estimated efficiently using the EM algorithm. Dur-ing training, clustered multi-task learning is used to capture the relatedness among the multiple linear SVMs and avoid overfitting. Experimental results on benchmark datasets show that our method outperforms state-of-the-art methods. During prediction, it also obtains comparable classification performance to kernel SVM, with much higher efficiency. G.3 [ Probability and Statistics ]: Statistical computing; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Nonlinear Classification; Linear SVMs; Multi-Task Learning
Kernel SVM often produces satisfactory classification re-sults on nonlinear data. Unfortunately, the complexity of kernel SVM relies on the number of support vectors. Alter-natively, while linear SVM is extremely efficient, it performs poorly on nonlinear data. Ensembles of linear SVMs can im-prove performance, though these methods either lack robust-ness, such as CSVM [5] which aligns the linear SVM weight vectors with a global weight vector, or are time-consuming, such as SVM-KNN [8] which uses a lazy learning strategy.
In this paper, an efficient classifier for nonlinear data is constructed by using Linear SVMs and Multi-Task Learning (LSVM-MTL). The method uses a divide-and-conquer strat-egy which partitions the data into clusters using a Gaussian mixture model (GMM), and then trains a linear SVM for each cluster. Instead of being treated independently, the two steps are combined into a generative model and alternatively performed in each iteration. To ensure the data points in each cluster are linearly separable, some clusters may have relatively few points, and can overfit. In this work, we con-sider training a linear SVM for a cluster as a single task, and the training of the classifier ensemble as a multi-task learn-ing problem. Clustered multi-task learning is used to exploit the relatedness between tasks, and avoid overfitting. To our knowledge, multi-task learning has not previously been used to train multiple linear SVMs on nonlinear datasets. Ex-perimental results on benchmark datasets demonstrate that the model outperforms state-of-the-art methods. For pre-diction, LSVM-MTL achieves much higher efficiency than kernel SVM, with comparable classification performance.
Methods for learning multiple linear SVMs for nonlinear data can be roughly divided into two categories. In lazy learning methods, such as SVM-KNN [8], the learning pro-cess is postponed until the testing phase, which is therefore expensive. By contrast, eager learning methods construct lo-cal classifiers during the training phase, usually employing a divide-and-conquer strategy. MLSVM [4] and CSVM [5] fall into this category. Other eager learning methods, such as LLSVM [6], use local coordinate coding. These methods ei-ther lack robustness, such as CSVM, or are time-consuming, such as SVM-KNN, MLSVM and LLSVM.

Multi-task learning (MTL) is a method where multiple related tasks are learned simultaneously to improve gener-alization [9]. It has been used in various areas, such as web mining [7].
At a high level, LSVM-MTL is an iterative divide-and-conquer approach which alternates between two steps: par-titioning the data into clusters with a GMM and training a linear SVM in each cluster. Instead of being independent, the two steps promote each other: GMM clustering improves the SVM classification performance for each cluster, and vice versa. This idea is integrated into our LSVM-MTL model as shown in Figure 1(b). The upper part of the model cor-responds to the GMM in Figure 1(a), which is responsible for partitioning the data into clusters. The lower part of the model trains linear SVMs for each cluster.

We introduce the following notation: in Figure 1(b), there are N i.i.d. samples X = { x n } n =1 ; ;N and their corre-sponding labels Y = { y n } n =1 ; ;N . The latent variables Z = { z n } n =1 ; ;N denote the assignments of samples to the K mixtures. The parameters  X  = {  X  j } j =1 ; ;K and  X  = {
 X  j } j =1 ; ;K denote the centroids and covariance matrixes of Gaussian components respectively. W = { w j } j =1 ; ;K represents the weight vectors of the linear SVMs for the K clusters.  X  = {  X  j } j =1 ; ;K are the mixing coefficients of the GMM. Let  X  = {  X , X ,  X  ,W } be the total set of model parameters. The joint distribution over X and Y is: which is obtained by summing the joint distribution of ob-served variables X , Y and latent variables Z over all pos-sible states of Z , with z n taking values in { 1 ,  X   X   X  mixing coefficient  X  j is the prior probability of picking the j th Gaussian component. P ( x n | z n = j, X ,  X ) = N ( x n j, X  j ,  X  j ) is a Gaussian component of the mixture, specify-ing the probability of x n conditioned on the j th component. P ( y n | x n ,w j ) is the posterior probability of the n th sample output by the j th linear SVM. We estimate the parameters by maximum likelihood. The regularized log-likelihood is: where  X ( W ) is a regularization term on the linear SVM weight vectors. This term encodes prior knowledge about the K classifiers, and is defined shortly.

Unlike previous work, our model learns the multiple linear classifiers simultaneously rather than independently. More specifically, if training a linear SVM in a cluster is regarded as a task, training linear SVMs for all clusters corresponds to a multi-task learning problem. To ensure that the samples in each cluster are linearly separable, the samples available for each cluster may be limited, possibly leading to over-fitting. More importantly, since all the clusters are partitioned from the same dataset, they should be latently related. Multi-task learning can be employed to capture the intrinsic re-latedness between tasks and avoid over-fitting in each task. We use clustered multi-task learning [9], which clusters tasks into groups, with tasks in each group having similar weight vectors. This approach is motivated by the desire for the decision boundary to be smooth and have constrained cur-vature, since a decision boundary with arbitrary curvature will likely overfit the data [6]. Hence, tasks in adjacent re-gions on the decision boundary should have similar weight vectors and should be clustered into one group. We illus-trate this issue with a synthetic dataset in the experimen-tal section. In practice, we find this clustering is beneficial for using multiple linear SVMs to nonlinear datasets. Note that LSVM-MTL clusters based on task similarities and the cluster structure is unknown beforehand. Furthermore, note that this clustering of SVM weight vectors is distinct from the GMM data clustering described previously.

In order to incorporate multi-task learning and linear SVMs into the above maximum likelihood estimation framework (2), we recall that training a linear SVM usually leads to the following quadratic optimization problem: where the first term regularizes the SVM weight vector, and the second term measures the total loss. We next describe correspondence between minimization problem in (3) and maximization problem of the log-likelihood function in (2). The first term in (3) corresponds to the second term in (2), since they both regularize the weight vector. Finally, to incorporate multi-task learning,  X ( W ) is formulated as: where the first term on the right-hand side assumes that the total K tasks are clustered into r clusters, with the index set of the c th cluster defined as I c = { j | j  X  cluster c } erage weight vector of the c th cluster is  X  w c = 1 m wh ere there are m c tasks in the c th cluster. The first term measures the within-cluster variance, which requires tasks from the same cluster to have similar weight vectors. The second term improves the generalization performance.
We next establish the relationship between the first term in (2) and the second term in (3) by defining: The posterior probability P ( y n | x n ,w j ) will be equal to 1 if the loss  X  ( w j ; x n ,y n ) is zero, otherwise P ( y be less than 1. To facilitate computation, P ( y n | x n ,w not normalized. When we employ the EM algorithm to maximize (2) in the following section, the value of the log-likelihood function will increase in each iteration, regardless of whether P ( y n | x n ,w j ) is a proper probability measure. We now apply the EM algorithm to the above LSVM-denote the collection of parameters at the t th iteration.
In each E step, the posterior probability of assigning the n th sample to the j th linear SVM is evaluated as: A lgorithm 1 LSVM-MTL Inp ut: Training data { ( x n ,y n ) | n = 1 ,  X   X   X  ,N }  X  R d  X  { X  1 , 1 } and the number of clusters K Output: Parameter  X  = {  X  j , X  j ,  X  j ,w j | j = 1 ,  X   X   X 
Initialise  X  by K-means. repeat until convergence Th is posterior probability is then utilized to derive the fol-lowing lower bound on the log-likelihood function: where  X ( W ( t +1) ) is the regularization term given by (4).  X  ( t ) is the parameter for the current iteration, which directly determines the posterior probability q ( t ) n;j on the right-hand side through Equation (6).

In the M step, the parameter is updated to  X  ( t +1) by maximizing (7). Fortunately, updating the parameters of the GMM and the linear SVMs is decoupled in (7). For updating the GMM-related parameters {  X , X ,  X  } , we follow the steps in [2]. To update the weight vectors W of the linear SVMs, we solve the following optimization problem: max tion (5). With the first term regarded as a weighted loss function, Equation (8) is equivalent to the clustered multi-task learning and is solved by the method of [9]. A sketch of our algorithm is presented in Algorithm 1. K-means is utilized to initialize the mixing coefficients  X  , centroids  X  and covariance matrixes  X . A linear SVM is then trained for each cluster, resulting in the initial weight vectors W . With the log-likelihood function in Equation (2) being increased in each iteration of EM, our algorithm is guaranteed to converge.

During testing, a new sample x is classified by the weighted average of the linear classifiers:  X  The sample is classified as positive if the weighted average is greater than 0, and negative otherwise. Obviously, the prediction complexity is linear in the number of tasks K . Prediction efficiency is particularly critical for large-scale or online applications.
The synthetic dataset in Figure 2 consists of two shifted sine signals, with 1000 points each, and each signal con-sidered a separate class. K-means is used to partition the data into five clusters, denoted with different colors. Linear SVM is then applied to each cluster. This simple baseline is denoted as K-means+SVM, and is also the initial stage of LSVM-MTL. Figure 2(a) shows that the linear SVM does not accurately classify the data points in each cluster. Ap-plying CSVM [5] gives similar results. The lack of improve-ment for CSVM here is because the method is not itera-tive, so its performance is directly determined by the initial K-means clustering. Additionally, CSVM aligns each SVM weight vector with a global weight vector, which is inappro-priate here. In contrast to CSVM, LSVM-MTL is iterative; Figure 2(b) shows the result at the fifth iteration. The linear SVMs correctly classify the data in each cluster, which em-pirically shows that the two steps in LSVM-MTL mutually reinforce each other. When the number of clusters (tasks) is increased to ten, Figure 2(c) illustrates that the ten tasks are clustered into five groups, with each group containing two tasks in adjacent regions. This result occurs because the decision boundary is smooth and tasks in adjacent re-gions on the decision boundary are similar. Note that the decision boundaries in Figure 2(c) correctly classify all the data points, since each linear SVM is only applied to its corresponding cluster. We use six benchmark datasets: IJCNN1, SVMGUIDE1, SKIN segmentation, LETTER recognition, Pendigits and Landsat Satellite. The first two are available at the LibSVM website [3], and the others are taken from the UCI machine learning repository [1].

We compare LSVM-MTL with seven previously-mentioned methods: Linear SVM, Kernel SVM, SVM-KNN, K-means+ SVM, MLSVM, LLSVM, and CSVM. The parameters of all the methods are set as in [5], with most parameters set by cross validation. For methods using K-means clustering, we calculate the average accuracy and the standard deviation on the test set over 10 random repetitions. The results are presented in Table 1. Here, we set the number of clusters K to 14 for K-means+SVM, CSVM and LSVM-MTL. As expected, linear and kernel SVM achieve the worst and best performance, respectively, over all datasets. Nevertheless, kernel SVM can be prohibitively expensive for large-scale datasets. Our proposed LSVM-MTL achieves not only com-parable performance to kernel SVM, but also much higher efficiency for prediction. The reason is that the predic-tion complexity of LSVM-MTL is linear in the number of tasks K , while the complexity of kernel SVM scales with the number of support vectors. For example, with K = 14, the prediction time of LSVM-MTL on the IJCNN1 dataset is 0.62 seconds, whereas the time of kernel SVM is 34.71 seconds, with 7924 support vectors learned. Even though SVM-KNN and LLSVM also perform well in most cases, they are slow due to the nature of lazy learning and local coordinate coding respectively. LLSVM is sometimes slower than kernel SVM [5]. The relatively poor performance of K-means+SVM is likely due to its ignorance of the relatedness among the multiple tasks. MLSVM only yields slightly bet-ter results than K-means+SVM, with increased complexity.
CSVM is state-of-the-art for this field, so we compare it with LSVM-MTL in detail. Figure 3 shows the classifica-tion accuracy of CSVM and LSVM-MTL with the number of clusters ranging from 2 to 20. LSVM-MTL outperforms CSVM on all the datasets except the SKIN dataset. How-ever, the SKIN dataset is simple, so even linear SVM pro-duces satisfactory results. The performance of LSVM-MTL generally improves with the number of clusters. Two factors may account for this improvement. First, when the number of clusters increases, the samples in each cluster become lin-early separable, and the SVM can classify them well. Sec-ond, with more clusters (tasks), multi-task learning is better utilized to transfer knowledge between tasks and avoid over-fitting. The performance of LSVM-MTL generally stabilizes as the number of clusters exceeds a certain threshold.
In this paper, we have proposed the LSVM-MTL model, which clusters the data with a GMM and trains a linear SVM for each cluster. These two steps are combined into a generative model and implemented with an EM algorithm. Furthermore, we consider the training of each linear SVM as a single task and use clustered multi-task learning to cap-ture the relatedness between tasks. Experimental results on benchmark datasets demonstrate that LSVM-MTL outper-forms state-of-the-art methods. In the prediction phase, it also achieves much higher efficiency than kernel SVM with comparable classification performance.
This work is partly supported by NSFC (Grant No. 61379098, 61003115, 61103056) and Baidu research fund. [1] K. Bache and M. Lichman. UCI machine learning [2] C. M. Bishop. Pattern Recognition and Machine [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [4] Z. Fu, A. Robles-Kelly, and J. Zhou. Mixing linear [5] Q. Gu and J. Han. Clustered support vector machines. [6] L. Ladicky and P. Torr. Locally linear support vector [7] O. Wu, R. Hu, X. Mao, and W. Hu. Quality-based [8] H. Zhang, A. C. Berg, M. Maire, and J. Malik. [9] J. Zhou, J. Chen, and J. Ye. Clustered multi-task
