 unlabeled data.
 lish that there exist nonparametric classes of distributio ns, denoted P sample size is too small, but (c) can be estimated if sample de nsity is high enough. sup P semi-supervised learners, denoted b f examples in order to estimate the decision sets, which perfo rm as well as b f grows appropriately relative to n . Specifically, if the error bound for b f learner and compare that to upper bounds on the errors of b f cannot be achieved by any supervised learner. sets are disjoint, and negative otherwise. P with marginal distribution P on the domain X = [0 , 1] d .
 The marginal density p ( x ) = P K component densities { p P 1. where g (1) 2. p k is bounded from above and below, 0 &lt; b  X  p k  X  B . 3. p k is H  X older- X  smooth on C k with H  X older constant K 1 [12, 13]. Let the conditional label density on C point ( X, Y ) is obtained as follows. With probability a p drawn i.i.d according to P m additional unlabeled data U = { X Let D denote the collection of all non-empty sets obtained as inte rsections of { C complements { C c consider a specific target function.
 The collection P a decision set or separation between the component support s ets C illustrated in Figure 2. Formally, for j, k  X  X  1 , . . . , K } , let Then the margin is defined as where h closest point on the grid. Let G denote the kernel and H 2) Decision set estimation  X  Two points x if there exists a sequence of points x k z x need to be evaluated for the test and labeled training points . proof sketch of the lemma and all other results are deferred t o Section 7. Lemma 1. Let  X  X  denote the boundary of D and define the set of boundary points as If |  X  | &gt; C o ( m/ (log m ) 2 )  X  1 /d , where C o = 6 x , x 2  X  supp ( p ) \B and all D  X  X  , with probability &gt; 1  X  1 /m , for large enough m  X  m E ( b vised learner b f sample upper bound holds Then there exists a semi-supervised learner b f compared to  X  needs to grow polynomially (exponentially) in n .
 If (i) the supervised learning task, so that  X  reduces the problem to a hypothesis testing problem for whic h  X  then  X  semi-supervised learning provided m grows exponentially in n . is the conditional density on the k -th component and let E E [ Y | X = x ] and we assume that for k = 1 , . . . , K 1. f k is uniformly bounded, | f k | X  M . 2. f k is H  X older- X  smooth on C k with H  X older constant K 2 . Here 1 penalty term pen ( f  X  ) is proportional to log( P n and also assume that m  X  n 2 d so that ( n/ X  only depend on the fixed parameters of the class P If m rate of error convergence is attained by SSL compared to SL, p rovided m  X  n 2 d . and asymptotic analyses may not capture the complete pictur e. densities are bounded from below and above, define p 7.1 Proof of Lemma 1 G max &lt;  X  , p  X  X  X , with probability at least 1  X  1 /m , Notice that  X  k X i  X  x k X  Proof. From Theorem 1, for all x  X  supp ( p ) \B , b p ( x )  X  p ( x )  X   X  sufficiently large. This implies P m Using the density estimation results, we now show that if |  X  | &gt; 6 pairs of points x we have x 1. connecting x is zero and since the region is at least |  X  | &gt; 6 in Section 3 such that k z the marginal density p ( x ) jumps by at least p x D  X  6 = D begins (in the sequence). Then since D  X  is at least |  X  | &gt; 6 for all sequences connecting x sequence that lie in D \B , D  X  \B , respectively, and k z each decision set is H  X older- X  smooth, we have | p ( z Since z m . Thus, x 2. x 1 , x 2  X  D  X  x 1  X  x 2 : Since D has width at least |  X  | &gt; 6 &gt; 2 sequence(s) contained in D \B connecting x z Since z m . Thus, x 7.2 Proof of Corollary 1 Let  X  event that the test point X and training data X B Now observe that b f discern which labeled points X ing on  X  semi-supervised learner b f knowledge of whether X, X sup P sup P 7.3 Density adaptive Regression results 1) Semi-Supervised Learning Upper Bound: The clairvoyant counterpart of b f b f the regression function is H  X older- X  smooth. Let n Since E [( f ( X )  X  b f tion over n finite constant, the overall error of b f subset of distributions in P bound of n  X  1 /d when  X  &lt; c where c to a decision set can be greater than a constant c
