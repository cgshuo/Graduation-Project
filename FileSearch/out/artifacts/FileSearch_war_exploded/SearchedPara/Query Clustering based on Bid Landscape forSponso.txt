 In sponsored search auctions, the auctioneer operates the marketplace by setting a number of auction parameters such as reserve prices for the task of auction optimization. The auction parameters may be set for each individual keyword, but the optimization problem becomes intractable since the number of keywords is in the millions. To reduce the dimen-sionality and generalize well, one wishes to cluster keywords or queries into meaningful groups, and set parameters at the keyword-cluster level. For auction optimization, key-words shall be deemed as interchangeable commodities with respect to their valuations from advertisers, represented as bid distributions or landscapes. Clustering keywords for auction optimization shall thus be based on their bid dis-tributions. In this paper we present a formalism of cluster-ing probability distributions, and its application to query clustering where each query is represented as a probability density of click-through rate (CTR) weighted bid and dis-tortion is measured by KL divergence. We first derive a k -means variant for clustering Gaussian densities, which have a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalize a single Gaussian and are typically a more realistic para-metric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL di-vergence. The clustering algorithm has been deployed suc-cessfully into production, yielding significant improvement in revenue and clicks over the existing production system. While motivated by the specific setting of query clustering, the proposed clustering method is generally applicable to many real-world applications where an example is better characterized by a distribution than a finite-dimensional fea-ture vector in Euclidean space as in the classical k -means. H.3.3 [ Information Search and Retrieval ]: Clustering. Algorithms, Experimentation, Performance Clustering; Bayesian methods; sponsored search; auction; optimization
In search advertising, advertisers bid on keywords for ad-vertising opportunities alongside algorithmic search results, through a generalized second-price auction (GSP) [6]. The bidder with the highest estimated click-through rate (CTR) weighted cost-per-click (CPC) bid (also known as rank score) wins the auction (impression opportunity). If the served ads are clicked, the advertisers pay the search engine (e.g., Google or Bing) the CTR adjusted next highest CPC bid.
The auctioneer or the search engine operates the mar-ketplace by setting a number of auction parameters, which play an important part in determining the outcome of the auction. An example of an auction parameter is reserve prices; only ads that clear the reserve price participate in the auction [12, 13]. Another example is the exponent to which the CTR estimate is raised in the rank score func-tion [9,11]. Auction optimization is the task of finding pa-rameters to optimize an objective such as maximizing click volume or revenue, while satisfying constraints such as the average number of ad impressions shown. One may seek to set the auction parameters for each individual keyword, but the optimization problem becomes intractable since the number of keywords is in the millions. To reduce the dimen-sionality for a parsimonious model that generalizes well, one wishes to cluster keywords or queries into meaningful groups, and set parameters at the keyword-cluster level.
For the purpose of auction optimization such as reserve s etting, keywords shall be regarded as interchangeable com-modities with respect to their valuations from advertisers, more precisely the estimated CTR weighted CPC bids. The valuation of a keyword is the underlying parameter of a prob-ability distribution referred to as bid landscape, and each advertiser X  X  bid is a sample drawn from the distribution. Clustering keywords for auction optimization shall thus be based on their bid distributions. To see intuitively why rep-resenting a keyword as a bid distribution is more advanta-geous than point estimates for this specific task of auction optimization, let us consider reserve price setting as an ex-ample. It is clear that both mean and variance are relevant to finding optimal reserve. The mean reflects the overall valuation level, while the variance captures the important aspects of competitive landscape including bid density and spread.

The main contribution of this paper is to present a for-malism of clustering probability distributions. We describe a query clustering algorithm where each query is represented as a probability density of CTR-weighted bid and distortion is measured by Kullback-Leibler (KL) divergence. We first derive a k -means variant for clustering Gaussian densities, which have a closed-form KL divergence, and show that the iterative algorithm monotonically decreases the total KL di-vergence. We then develop an algorithm for clustering Gaus-sian mixture densities, which generalize a single Gaussian and are typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mix-ture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence.

The clustering algorithm has been deployed successfully into production at Bing Ads, and has produced keyword clusters for auction optimization. The method yielded a 22% gain in CTR over k -means in offline simulation, and a 5% improvement in revenue and clicks over the existing production system, which is a very significant improvement for a multi-billion dollar marketplace. As a consequence, the reported query clustering method is now serving 100% Bing and Yahoo search advertising traffic.

The paper is organized as follows. In Section 2, we formu-late the problem of auction optimization. We then examine some empirical bid distributions in sponsored search auc-tions in Section 3, to support the parametric assumption that each keyword is represented as a Gaussian mixture density. In Section 4, we discuss a Bayesian perspective of clustering, and in Section 5, we formalize the problem of clustering probability distributions and derive a k -means variant, using Gaussian density as an illustrative example, while the approach remains general. We then in Section 6 generalize the single Gaussian model to Gaussian mixture model (GMM) and derive a variational EM algorithm with the otherwise analytically intractable KL divergence. Em-pirical results with the application of keyword clustering for auction optimization is presented in Section 7. Finally, we conclude the work in Section 8.
The state of the art of auction optimization is to formulate an integer programming (IP) problem using counterfactual auction simulation as input. Before we formulate the auction optimization problem, let us first introduce the following concepts and notations for sponsored search. 1. Ranking. Given a keyword-ad pair, let the estimated 2. Pricing. In a GSP auction, if an ad from bidder (ad-3. Allocation. Ads are allocated, in the descending or-4. Optimization. For auction optimization, one typ-
Auction optimization consists of two steps as follows. 1. A counterfactual auction simulation is performed by
In practice, CTR estimates are typically at more granular levels including, e.g., user signals. We focus our discussion on the keyword-ad level to abstract from the nuances irrele-vant to this work, without loss of generality. The keyword-ad pair is arguably the most predictive feature anyway. The rank score function has been evolving as well, and may con-tains other terms such as relevance of the ad landing page.
In search advertising auction, there are other constraints on ad relevance (minimum relevance), CTR (minimum es-timated CTR), and page layout (maximum numbers of ads in ML and SB), and so on. 2. An integer programming (IP) problem is formulated Formally, let j index m parameter settings, p index k key-word clusters, and x pj be the binary decision variable indi-cating whether to choose parameter setting j for keyword cluster p or not. A representative IP problem to maximize clicks is formulated as follows. Here one maximizes clicks while lower bounding revenue by g . There is an empirical trade-off between these two ob-jectives; whereas a sustaining auction shall yield revenue though clicks rather than CPC price. v p is the number of search result pageviews for keyword cluster p , and g 2 the global upper bound on MLIY. The coefficient matrix [ y pj , w pj , a pj ] k  X  m is obtained from auction simulation. The number of variables is k  X  m , and in practice the number of settings m can easily be several hundred to cover a large number of parameters. It is clear that the optimization is in-tractable at the individual keyword level, where the number of keywords k is typically in the order of several million.
We wish to cluster keywords to reduce the dimensionality of the IP problem for a parsimonious optimal solution that generalizes well. For the purpose of clustering queries for setting parameters such as reserve prices, queries shall be treated as interchangeable commodities given same valua-tion distributions. To make a sound parametric assumption for the bid distribution in an appropriately chosen metric space, we examine the empirical distributions of rank scores for most frequently searched keywords, with several plau-sible metric transformations, as shown in Figure 1 for the example of  X  X ngry birds X , and Figure 2 for the example of  X  X lights san francisco X .
 First of all, we choose the coordinate space of log (bid)  X  CTR, instead of the original or logarithmic space of (bid  X  CTR). The original space of rank score does not exhibit any pattern of a smoothed parametric distribution, typi-cally with a sharp spike focused on a very low rank score range. Bids (in cents) are observed from advertisers, hence a log transformation effectively squashes out large variances likely due to noises, e.g., from outlier bidders. On the other Figure 1: Empirical and fitted distributions of log ( bid )  X  CTR for the keyword  X  X ngry birds X . hand, a log transformation of CTR estimates is not nec-essary, since CTRs are already normalized into the range [0 , 1], typically below 10%. Empirically, we find that us-ing the metric log (bid)  X  CTR works best for maximizing objectives, particularly clicks.

Second and more appealingly, almost all empirical curves show a good fit to the two-component Gaussian mixture sig-nature, as illustrated in Figures 1(a) and 2(a). One hypoth-esis is that there are two reserves, for mainline and sidebar respectively; bidders are aware and place bids reacting to one reserve at a time. This behavior is intuitive since ad-vertisers usually have campaign goals and budgets. It turns out that our hypothesis is strongly supported by the data, as shown in Figures 1(a) and 2(a), the two peaks are dominated by mainline (blue) and sidebar (brown) ads, respectively.
Now that we have made the parametric assumption that each keyword is represented as a two-component Gaussian Figure 2: Empirical and fitted distributions of log ( bid )  X  CTR for the keyword  X  X lights san fran-cisco X . mixture density of log (bid)  X  CTR, this is not only a more realistic assumption than, e.g., a single Gaussian, but also may expose better opportunities to the IP optimization, e.g., the saddle area between two peaks presents more feasible region for seeking optimal mainline reserve.

Further, given the two-reserve bidding mechanism evident from the data, the hidden variable or the membership of Gaussian component for each bid is known (think of the generative process of GMM). In other words, the mixture weights of GMM are known. This observation considerably reduces the complexity of learning GMMs for keywords, that is, the standard iterative EM is no longer needed. Recall that we need to learn one GMM for each of the possibly several million keywords. Specifically, fitting a GMM for each keyword has a closed-form solution as follow, where i indexes bids and z indexes mainline or sidebar. As shown in Figures 1(b) and 2(b), the fitted GMM (the blue solid line) captures the nature of the bid distribution much better than a single Gaussian (the green dotted line), par-ticularly the sharpnesses of the two peaks.
Clustering is an unsupervised learning method widely used for dimensionality reduction. Given a set of examples D = { x i } n i =1 , the goal is to partition them into reasonable clus-ters. In k -means clustering, each example is represented as a feature vector x i  X  R d , and an iterative algorithm finds a lo-cally optimal set of k clusters  X  = {  X  j } k j =1 so as to minimize the distortion measured as the squared Euclidean distance  X   X  = argmin  X  L ( D |  X  ) = argmin  X  P k -means algorithm is the limiting case of EM algorithm for Gaussian mixture models with infinitely small covariances.
The classical clustering methods hold a frequentist per-spective in that data examples are a repeatable random sample from an underlying process with fixed parameters. The clustering task is then essentially inferencing the clus-ter centers  X  as point estimates of means from repeatable observations, which are represented as point estimates as well. Recall that in the k -means case, minimizing quadratic loss L ( D |  X  ) is equivalent to maximizing log likelihood  X  ( D |  X  ) under Gaussian mixture models with infinitely small covari-ances, that is, the inference problem of computing the mode of log likelihood argmax  X   X  ( D |  X  ) or the maximum likelihood estimate (MLE) of cluster centers  X  . With the frequentist view, the classical clustering methods are more natural for clustering statically measurable objects such as documents and images using bag-of-words representation.

In many real-world applications, however, data examples to be clustered are better described probabilistically, and so are the cluster centers or representatives. In predictive modeling such as logistic regression, one wants to estimate the probability of an outcome given an unseen input feature vector p ( y | x ). Clustering is typically applied to input fea-tures to reduce dimensionality for a better model generality. While clusters  X  are learned from historical data, one is most interested in inferencing the cluster membership of a future x , which is bound to change. One motivational application is predicting CTR of search results or ads given a user among other input features [2]. To reduce the high dimensionality of user features (e.g., user ID or IP address), one may clus-ter users based on their click propensities. A frequentist ap-proach would simply cluster point estimates of the Bernoulli success probability p , whose MLE is the sample mean, and hence unable to capture higher-order moments of the distri-bution of p such as variance and skewness. One important aspect of the distribution is the variance, e.g., for the pur-pose of exploring users with few Bernoulli trials formulated as a multi-armed bandit problem [7,14]. The frequentist in this regard would characterize the distribution with a finite number of quantities, whereas clustering feature vectors of m oments in Euclidean space has no clear interpretation.
A Bayesian perspective is more natural in such predictive settings where underlying parameters are unknown and their uncertainty is of interest. For the CTR prediction applica-tion, a user with respect to click propensity is represented as a probability distribution of the Bernoulli success prob-ability p , that is, the posterior beta distribution. For clus-tering distributions, a natural and sound choice of distance function is the KL divergence, for its probabilistic under-pinning and additivity. While KL divergence is asymmetric, centroid-based parametric clustering approaches [1] only re-quire a directed distance function, i.e., from a cluster center to an example.

The significance of introducing a Bayesian perspective to the classical clustering methods is beyond the predictive set-ting as illustrated above. In many scenarios, an object shall be clustered with respect to some underlying parameters of a probability distribution, from which individual observations about the object are drawn. The motivational application in this paper is clustering keywords based on their underlying valuations for sponsored search auction optimization.
The Gaussian distribution is considered the most promi-nent probability distribution given the central limit theorem and its analytical tractability. We use Gaussian density as a representative example to derive a formal clustering algo-rithm. This choice is mathematically convenient and helps to guide intuition, yet it is sufficient to illustrate the un-derlying principle. It is important to emphasize, however, that the formulation of the clustering problem generalizes to other distributions.

Let us assume that the observations x for an object p fol-low a Gaussian distribution. Each object can be represented as a Gaussian density: The goal is to cluster objects with similar distributions to-gether, for which we need a pairwise distance function. A natural choice of distance measure between two distributions denoted by p and q is the KL divergence: KL divergence measures in bits how close a model distribu-tion q is to the true underlying distribution p . Although KL divergence is not symmetric nor does it satisfy the triangle inequality, for centroid-based parametric clustering meth-ods [1], such as the one we present, one only needs a di-rected distance measure from a cluster center p to an object q . KL divergence is additive for independent variable dis-tributions, that is, D KL ( p k q ) = D KL ( p 1 k q 1 ) + D if p ( x, y ) = p 1 ( x ) p 2 ( y ) and q ( x, y ) = q 1 ( x ) q particularly useful for clustering distributions based on KL divergence, since it allows for a clear interpretation of mini-mizing total KL divergence.

We now formulate the optimization problem underlying the task of clustering Gaussian densities, following a simi-lar approach as k -means. Let us begin with the KL diver-gence from a cluster center p  X  N  X  p ,  X  2 p to an example q  X  N  X  q ,  X  2 q . The KL divergence between two Gaussian densities has a closed form.
 Given a set of training examples D = { q  X  N x ;  X  q ,  X  2 indexed by q and observed as x , we wish to learn a set of cluster centers  X  = { p  X  N x ;  X  p ,  X  2 p } indexed by p , by minimizing the loss in KL divergence. where q  X  p denotes that the example q belongs to the clus-ter p .

If the assignment q  X  p,  X  p, q is fixed, the objective func-tion L is convex in  X  p and  X  2 p . This is an unconstrained optimization problem, thus we set the partial derivatives of L w.r.t.  X  p and  X  2 p to zero to derive the update rule for cluster centers.
For fixed cluster centers  X  p and  X  2 p ,  X  p , we simply assign q to its closest cluster center p ( q ) in terms of KL divergence. We now arrive at an iterative algorithm that monotonically decreases the loss in KL divergence as follows. 1. Randomly choose k examples as cluster centers. 2. Repeat until convergence.
The optimal solution to the cluster centers  X  p and  X  2 p in Eqs. (12) and (13), reveals an appealing yet somewhat nontrivial intuition. First, the optimizing center mean  X  an inverse-variance weighted average of example means  X  q Inverse-variance weighted averaging is known to minimize the variance of the sum and is typically used in statistical meta-analysis to combine evidences from independent stud-ies [8]. Intuitively, we weight studies to give preference to the more precise ones with larger samples. This is appro-priate for clustering distributions, since the measurements or sampling of x fo r each example distribution q shall be treated as individual studies, instead of from a single large study, to allow for other differences such as in variance. The latter views examples q as parts of a single sampling, whose mean would be a sample-size weighted average of example means. Second, the optimizing center variance  X  2 p is the harmonic mean of example variances  X  2 q . Harmonic mean is typically used for averaging rate variables, and variance can be interpreted as the rate of imprecision of a study. A more well-known metric is the F 1 -score, defined as the harmonic mean of precision and recall, from information retrieval.
The proposed algorithm for clustering Gaussian densities can be viewed as a special case of the Bregman clustering problem [1], in that KL divergence is the Bregman diver-gence realized from the convex function  X  ( p ) = P j p j in a d -simplex. In [1], Banerjee et al. unify centroid-based clustering approaches into a meta hard clustering algorithm that is applicable to all Bregman divergences including squared Euclidean distance and KL divergence. They also show that there is a bijection between regular exponential fam-ilies and regular Bregman divergences. These findings es-tablish a general theoretical foundation for our work. More specifically, the update step for the centroid mean in the Bregman hard clustering (Algorithm 1 in [1]) has the form:  X  j  X  1  X  j P i  X  j  X  i x i , where i indexes examples, j indexes cluster centers,  X  j = P i  X  j  X  i , and  X  = {  X  i } n i =1 ity measure over X = { x i } n i =1 . We have shown, in Eqs. (12) and (13), that for Gaussian densities with unknown  X  and  X  , the general probability measure  X  i in Bregman hard clus-tering is realized as the inverse variance 1 / X  2 i .
We have formalized the problem of clustering probability distributions with KL divergence, and derived a simple k -means type iterative algorithm for Gaussian densities. In this section, we generalize a single Gaussian to Gaussian mixture model (GMM), which often times appears to be a better parametric assumption for many real-world applica-tions such as speech and image recognition [10].

Let us begin with a simplified yet practically represen-tative case of GMMs with equal-number non-exchangeable components. The Gaussian mixture densities of a cluster center p and an example q are: Here z indexes matched components,  X  z and  X  z are mixture weights, p z and q z are component Gaussian, for the cluster center and the example, respectively.

The KL divergence between two GMMs is no longer ana-lytically tractable, which renders exact EM impossible. One solution is to use variational inference that decreases but not necessarily minimizes the loss in KL divergence, so as to find approximate estimates of cluster center parameters  X  = {  X  pz ,  X  2 pz ,  X  pz ;  X  p, z } . Let us treat the assignment q  X  p,  X  p, q as variational parameters (indicator variables) and cluster centers  X  as model parameters, which is invariant for finding a local minimum. We first minimize a tractable upper bound w.r.t. the variational parameters q  X  p,  X  p, q , and then for fixed variational parameters, minimize the up-per bound w.r.t. the model parameters  X  , alternating until convergence. This procedure is known as variational EM algorithm.

Let us first give an upper bound on the otherwise in-tractable KL divergence.

D GMM KL = Z p ( x ) log p ( x ) The inequality in Eq. (16) follows from the log-sum inequal-P i x i , y = P i y i and x i , y i  X  0, with equality iff stant. When p and q are aligned well in terms of both mix-ture weights and component Gaussian,  X  z p z  X   X  z , D GMM KL approaches zero, so does its upper bound, which tends to be tight. This is particularly useful for hard clus-tering since, in the assignment step, one only seeks the mini-mizing p . At the other extreme, when all components except one vanish, The KL divergence between GMMs D GMM KL de-grades to the one for Gaussian D Gauss KL . This is why the clustering algorithm for GMMs is a generalization of clus-tering Gaussian densities, hence can be used directly for the latter.

The optimization problem defined on the upper bound is
For a fixed assignment q  X  p,  X  p, q , this is a constrained optimization problem. We form the Lagrangian and set its partial derivatives w.r.t.  X  p z ,  X  2 pz and  X 
For cluster assignment with fixed centers  X  pz ,  X  2 pz and  X  pz ,  X  p, z , we assign q to its closest cluster p ( q ) in terms of the upper bound. We now arrive at the variational EM algorithm as follows. The EM recurrence substantiates simple intuitions. In the M-step update for the cluster center mixture weights  X  pz (Eq. (25)), the belonging example mixture weights or pri-ors  X  qz contribute multiplicatively as exp P q  X  p (log  X  while penalized by their matched-component KL divergence D ( p z k q z ), or equivalently, the negative log likelihood [1]. The M-step update for the component Gaussian parameters  X  pz and  X  2 pz (Eqs. (23) and (24)) is very similar to the single Gaussian case (Eqs. (12) and (13)), except at the component level.

As the clustering algorithm (the assignment E-step and the update M-step) iterates, the total KL divergence de-creases and clusters become more homogeneous, hence the upper bound becomes tight. In practice, we find that the clustering algorithm converges sublinearly, and typically con-verges after 20 to 30 iterations.

It is important to note that, when some example variances  X  qz approach zero, the optimization problem becomes ill-conditioned. Since example variances  X  2 qz appear as denom-inators in the update formulae (Eqs. (23), (24) and (25)), the zero-variance examples will dominate hill climbing. One ap-proach to coping with this numerical issue is to smooth the Gaussian parameters by adding an i.i.d. zero-mean Gaussian noise  X   X  N (0 ,  X  2 ) to each observation x , and the resulting Gaussian parameters of example q is N (  X  qz ,  X  2 qz +  X  The smoothing variance  X  2 can be chosen in a data-driven manner, e.g., the first percentile of nonzero variances. In fact, the smoothing variance  X  2 introduces a useful mecha-nism to control whether the clustering shall emphasize more on mean or variance, depending on different applications. A sufficiently large smoothing  X  2 effectively makes the clus-tering based upon the example means. On the other hand, smoothing cluster center variances  X  2 pz is not necessary, since they never appear as denominators in the algorithm.
With the learned GMMs for keywords described in Sec-tion 3, we apply the variational EM algorithm described in Section 6 to cluster keywords into k partitions. We collected auction related data (e.g., bids, CTR estimates, and display positions) over a one-month period, learned k clusters from a smaller set of most frequent keywords (about 1 M ), and performed a final assignment step to infer clusters for all keywords (about 8 M ). The choice of k is made such that clusters will have a mild loss in entropy, while the dimension-ality k  X  m fits well with the IP solver. A good empirical choice is k = 2000  X  4000. The clustering results are vi-sualized in Figure 3. Figure 3(a) shows how clusters are spanning in the 3D space of (  X  ml ,  X  sb ,  X  ml ), where each ball denotes a cluster center, with a volume proportional to  X  It is clear that the algorithm does not partition examples in the Euclidean sense, e.g., more clusters are derived in the low-variance area since those examples have greater impacts on the total loss in KL divergence (Eq. (16)). Figure 3(b) illustrates how keywords are clustered, where each ball rep-resents a keyword GMM and each same-color cloud forms a cluster. The clustering exhibits a meaningful yet non-Euclidean pattern, e.g., low-variance clusters are denser in belonging keywords.

Finally, we evaluate the effectiveness of the proposed GMM clustering algorithm in the context of auction optimization, through both offline simulation and online A/B testing. The IP problem is formulated as: max { clicks } This optimization is to maximize clicks given a 5% more budget in mainline impression yield, while maintaining the same amount of revenue.

In offline experiments, we compare the proposed GMM clustering ( k -GMM) with three benchmarks: 1. The Gaussian clustering ( k -Gauss) that represents each 2. The k -means clustering that represents each keyword 3. A univariate binning approach ( k -bins) that partitions The offline simulation results are summarized in Table 1. The k -GMM clustering outperforms all other methods with respect to lift in clicks over actual log traffic. The ratio  X  X licks /  X  X LIY measures the efficiency of converting ad impression to click by a particular method. With a 5% MLIY budget, k -GMM yields a 5 . 3-fold improvement in click-converting efficiency over the existing approach in pro-duction k -bins, a 22% improvement over k -means primarily due to the Bayesian treatment, and a 48% improvement over k -Gauss in consequence of a sound parametric assumption. Table 1: Auction optimization results with different clustering methods
We have also conducted online A/B testing to compare t he k -GMM clustering algorithm with the current k -bins approach in production. The online experiment ran for a two-week period and accounted for 16 . 2 M search result pageviews. The results are shown in Table 2. The proposed GMM clustering has gained a 5 . 60% revenue lift over the ex-isting k -bins approach, entirely from the gain in clicks 5 . 79% with a slight and favorable drop in CPC price  X  0 . 27%, at an approximately same MLIY level 0 . 80%. As a consequence, the novel query clustering method has been successfully de-ployed to the Bing search engine.

W e have presented a formalism of clustering probability distributions, motivated by real-world applications where observations are drawn from underlying distributions and the goal is to cluster the underlying concepts with uncer-tainty. An appealing Bayesian analog is that the cluster center or representative distribution is the prior p (  X  ) and the example distribution is the posterior p (  X  | D ). We have derived the algorithms for clustering Gaussian densities and GMMs, while the underlying principle generalizes to other distributions such as beta distribution for binomially dis-tributed data, Dirichlet distribution for multinomial data, and gamma distribution for Poisson data. The algorithm has been applied to the important problem of sponsored search auction optimization, and yielded significant improvement in CTR over k -means in offline simulation, and as well as improvement in revenue and clicks over the existing produc-tion system. [1] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. [2] Y. Chen, M. Kapralov, D. Pavlov, and J. F. Canny. [3] Y. Chen and T. W. Yan. Position-normalized click [4] T. M. Cover and J. A. Thomas. Elements of [5] M. N. Do. Fast approximation of Kullback-Leibler [6] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [7] J. C. Gittins. Bandit processes and dynamic allocation [8] G. V. Glass. Primary, secondary, and meta-analysis of [9] T. Graepel, J. Q. Candela, T. Borchert, and [10] J. R. Hershey and P. A. Olsen. Approximating the [11] S. Lahaie and P. Mcafee. Efficient ranking in [12] M. Ostrovsky and M. Schwarz. Reserve prices in [13] F. Pin and P. Key. Stochastic variability in sponsored [14] A. Slivkins. Multi-armed bandits on implicit metric
