 Many collections of data contain items that are inherently similar. For example, archives contain files with incremental changes between releases. Long-range inter-file similarities are not exploited by standard approaches to compression. We investigate compression using similarity from all parts of a collection, collection-based compression (CBC). Input files are delta-encoded by reference to long string matches in a source collection. The expected space requirement of our encoding algorithm is sublinear with the collection size, and the compression time complexity is linear with the input file size. We show that our scheme achieves better compres-sion for large input files than existing differential compres-sion systems, and scales better. Also, we achieve significant compression improvement compared to compressing each file individually using standard utilities: our scheme achieves several times the compression of gzip or 7-zip .Theoverall result is a dramatic improvement on compression available with existing approaches.
 E.4 [ Coding and Information Theory ]: Data Compaction and Compression Algorithms, Experimentation, Performance Compression, data representation, repository storage
Many data collections contain vast quantities of repeated material. Examples include version repositories for software, web site archives, and collections of genomes. It is attractive to compress large collections to reduce the cost of storage and synchronisation, but most compression methods require a representation of the data in memory.

In this paper we explore ways of using an entire collec-tion as a reference against which new material can be com-pressed. We describe this as collection-based compression (CBC), an approach that generalises previous methods based on coding similarities with a previous file or small set of files. A successful CBC system: (1) has the ability to compress a query string or file using information from every location in the collection; (2) has running time growing in proportion to the size of the file being compressed; and (3) operates within fixed or limited memory.

We present cobald , a simple, but effective, CBC scheme using two stages. The first stage uses hash-based string search to find long matching substrings in the collection, and produces a delta encoding of the query file in which the string matches are replaced with references to the col-lection, thus taking advantage of long-range similarities. In the second stage, the delta encoding is compressed using a standard compression tool, thus taking advantage of local similarities. Our scheme differs from previous approaches in two key aspects. It uses a permanent index, consisting of hashes of large substrings, to support the search during delta encoding; and it employs a novel encoding algorithm.
A benefit of our approach is that encoding a new file does not require large parts of the collection to be re-read. Overall compression time is comparable to that with a stan-dard compressor alone, and often is faster. Our experiments demonstrate that our approach achieves a several-fold com-pression improvement compared to standard tools.
Differential encoding , also known as delta encoding, is the process of encoding one string or file in terms of another. The resulting delta represents a sequence of instructions to reconstruct, or decode , a query string from a reference string. (In some literature, the query string is referred to as the  X  X ersion X  or  X  X arget X , and the delta as a  X  X atch X .)
Existing LZ compression utilities build a dictionary of the strings contained in a sliding window from the input file. Gzip uses a 32 kB window, whereas 7-zip achieves better compression using a window up to 4 GB in size. Techniques for exploiting similarity in the data beyond the compression window typically employ hash-based string search, such as the Karp-Rabin string search algorithm [5]. The strings of length n ,or n -grams , in the reference and query strings are hashed, then matching strings are located by finding match-ing hashes. However, as string matches shorter than n will not be found, the delta is no longer optimal. We refer to this string search method as n -gram search .

The rzip program [8] performed a long-range search over a 900 MB window for duplicate 32-byte blocks, which were hashed using rsync . The resulting delta was compressed using bzip2 . Bentley and McIllroy [3] reduced the space requirement of hash-based search by inserting every k th n gram hash of the reference string into the hash table, and discarding the rest; a technique we refer to as a sampled hash index . The search algorithm using a sampled hash in-dex is more complicated. Each n -gram of the query string was hashed and queried in the hash table to identify po-tential matches in the reference string. A matching n -gram found in the index is likely to occur partway through a longer matching string, so the matching n -gram is then extended backwards and forwards using a symbol-by-symbol compar-ison of the strings to find the complete matching substring. Their delta was then compressed using gzip .

While these approaches compress a single file, utilising matching data from another file may be achieved using dif-ferential encoding. Vdelta [4] processes a concatenation of the reference string followed by the query string, construct-ing an index of n -gram hashes in a single pass. By applying compression techniques such as efficient coding of the in-structions in the delta, Hunt et al. [4] developed the concept of differential compression . This has been incorporated into the VCDIFF format (RFC3284) [6] for differentially com-pressed data. Vdelta is not suitable for CBC without mod-ification as each consecutive unmatched n -gram is indexed, which requires more space than the unmatched input sym-bols themselves. The xdelta program [7] constructs a delta using constant memory. As the n -gram hashes are indexed, a new hash replaces a duplicate hash in the hash table. The compression performance may be expected to degrade for larger inputs as a larger proportion of n -grams will have been deleted from the index.

Delta compression techniques have been applied to file system backup and archiving file system snapshots. The  X 1.5-pass X  algorithm of Ajtai et al. [2] also uses hash-based search in constant memory by only indexing a single n -gram reference for each hash. The hsadelta [1] algorithm achieves constant space using a different approach. It uses a sampled hash index in which every n th n -gram is indexed, and varies n with the input size so that the number of index entries is constant. Consequently, we expect gradually worse com-pression with increasing collection size. However, hsadelta includes features that are likely to be useful for CBC. It uses hash sampling with a fixed sampling period in order to re-duce the index size to a fraction of the input. The problem of false negative index queries is addressed by including several data structures intended to quickly answer most queries for hashes which are not in the index. Another feature is that it uses a larger hash space of 2 64  X  1 to obtain a very small probability of hash collisions. This index is sparsely popu-lated even for multi-terabyte collections, so a tree structure is used for the index, rather than a hash array. As hsadelta records duplicate hashes, it is able to locate all instances of a repeated string. It searches for the longest of these string matches by finding sequential references in the hash table.
An alternative approach to CBC is to encode the query file as a delta with respect to a small selection of reference files which are most similar to the query file. The principal challenge of this approach is to find methods to identify the files in a collection which are most similar to the new query file. We do not pursue this approach as we expect similarity to be dispersed over multiple fragments of collection files.
Cobald compresses using a two-stage pipeline with an en-code operation followed by a compress operation. In the encode operation the query string is differentially encoded in terms of the files already in the collection. An index of n -gram hashes is constructed and then used to locate strings in the collection matching substrings of the query string. The constructed index is a subset of the hash list file which is maintained on external storage. The encode operation also reads data from the collection when check-ing for hash collisions and extending string matches. The output of the encode operation is a delta file which is input to both the compress and insert operations. In the insert operation , an encoded delta file is indexed by appending its n -gram hashes to the hash list file. In the compress opera-tion a standard compression utility (we present results using gzip and 7-zip ) is applied to the delta file to produce the final compressed version of the query string.

In order to search a reference collection, cobald  X  X  encode operation uses several of the previously-described modifica-tions of the hash-based n -gram search algorithm. Similarly to hsadelta and the system of Bentley and McIllroy [3], cobald uses a sampled hash index, sampling every n th n -gram. Like vdelta and xdelta ,in cobald we select the first match found, rather than searching for the longest match. By discarding references to duplicate n -grams from the in-dex, the space complexity of cobald is proportional to the unmatched data in the collection files and the query string. The following scenario illustrates the system operation. A collection consisting of several data sets , each containing multiple files, is to be compressed. Initially, the hash list file is empty. The first file is encoded, perhaps identifying some duplicate substrings within itself and achieves a modest re-duction in size. The encoded file is then inserted into the index; a sample (every n th ) of its string hashes is appended to the hash list file. A second file is then encoded, replac-ing strings matched to duplicates found in the first file, or earlier in itself. The encoded second file is then inserted. This process continues for subsequent files: each file is able to refer to previous occurrences of duplicate strings, either in a previously-inserted file, or in itself. Once an input file has been encoded and inserted, it is compressed using gzip or another compression utility.

In order to retrieve a compressed file from the collection, it is decompressed and decoded, which requires retrieving its references to other encoded files in the collection. These referenced files must also be decompressed, but they do not need to be decoded as the delta references are to matching strings in other delta files.

The scenario highlights several desirable aspects of the compression system. First, the entire collection does not need to be read when encoding or inserting a new file. Sec-ond, a file may be decoded without needing to decode other files in the collection (although they will need to be decom-pressed), and consequently decoding may be expected to be fast. Third, as the encoded file is subsequently compressed using a standard compression tool, the first stage can use an inefficient coding method for the match references in the encoded file. where unable to compress the 6 GB file. n =256 n =1024
The delta file begins with an array of the names of all collection files referred to by the COPY commands in the delta. The delta commands follow as a sequence of blocks. A COPY block describes a COPY command to locate the matching string in a collection file and consists of a triple file id, length, offset where file id is the index of the file name array in the header, length is the number of bytes of the matching string and offset is the position in the file of the first byte. An ADD block describes an ADD command and contains a variable-length string of verbatim data to be inserted into the decoded string.

The hash list file contains triples hash, match set, off-set for every k th n -gram in each file in the collection, which are appended as the file is inserted. To ensure complete coverage of the data, we set k = n . The hashes are calcu-lated over substrings of fixed length n , using the Karp-Rabin rolling hash function [5] with the maximum hash being a large prime less than 2 28 . During insertion, hash matches are resolved by comparing the new n -gram with n -grams in the collection files. Matching n -grams are assigned the same match set identifier, and a match set defines a set of refer-ences to identical n -grams in the collection files. In many cases, several match sets are associated with a single hash value, a consequence of hash collisions. Offset is the posi-tion in the collection file of the first byte of the n -gram. This process is modified slightly for the special case of inserting a delta file; to avoid attempting to match a string containing a COPY block, only the hashes from ADD blocks are ap-pended to the hash list file, and the count for the k th n is restarted at the beginning of each block.

Index construction. Before searching the collection, the encode operation constructs an index in memory from the hash list file. We use a separately-chained hash table. The first occurrence of each match set in the hash list file is inserted into the hash table, and subsequent matching n -grams, which have the same match set identifier, are dis-carded. This has the effect of preferring the oldest occur-rences of a duplicate n -gram as file indexes are appended to the hash list file in chronological order.

Encoding algorithm. When a hash match is found, the n -gram references in the index are retrieved from the collec-tion files and compared to the n -gram in the query string. A successful match is extended forwards and backwards to find the complete matching substring. This match reference is then appended to a match list , and the search resumes from the end of the match in the query string. Once the search of the query string is complete, the match list is tra-versed to remove overlapping references, which occur when a reference has been extended backwards to overlap previous references. Then the delta is created by writing the refer-ences from the match list as COPY blocks and inserting ADD blocks in the gaps between matches.
Results are presented using two data collections: web snap-shots and genome . Each collection contains several data sets , each containing multiple files. The data sets in each collection were created sequentially as a revision of the pre-vious data set. Therefore a substantial proportion of the data will be unchanged between data sets, as might be ex-pected from a sequence of archival snap-shots.

The web snap-shots collection is eight web crawls of a media web site ( abc.net.au ) taken one week apart over a two-month period. It contains only the (untruncated) html files; media files such as images have been discarded. It consists of 8 data sets, ranging from 6,450 to 7,850 MB in size, with a total size of 55,714 MB.

The genome data collection comprises releases 36 to 56 of the Drosophila melanogaster genome from the Ensembl project ( ftp.ensembl.org ). It consists of 21 data sets, rang-ing from 1,100 to 1,400 MB in size, with a total size of 26,141 MB. The data consists of nucleotide sequences with anno-tations describing aspects such as protein and RNA expres-sion. Each release is an update of the data from the previous release, incorporating new results from the genomic research community.
We compared cobald to other delta-encoders by encoding and compressing a query file with respect to a collection containing a single reference file. In trials conducted using smaller files ( &lt; 1 GB) (results not shown), hsadelta and vcdiff (an open source implementation of vdelta ) achieved the best compression. The files compressed using cobald ( n = 256) were approximately 15% larger, and using cobald ( n = 1024) and xdelta were approximately 30 to 60% larger than the compression achieved by hsadelta and vcdiff .
For larger files, we found that cobald achieves compa-rable, and often better, compression than the other tools. Some compression results for larger files are presented in Table 1. This compression improvement with input size is Table 2: Total compressed collection file size in megabytes, including the hash list file size and com-pressed collection file sizes, using gzip and 7-zip for the second compression stage in cobald with n = 1024 . The unencoded rows show the compression by gzip and 7-zip without prior encoding by cobald . expected as the other tools contain limits on their index sizes as described in Section 2.

To evaluate cobald for CBC, we compressed the data col-lections and used gzip and 7-zip for the second compression stage. The results presented were obtained using n = 1024. While a smaller value for n will yield better compression, a larger index file is required; as n decreases the additional index storage exceeds the marginal improvement in compres-sion. We found that n = 1024 required the least external storage (the total sizes of the compressed collection files and the index file). The compression results using other values for n will be presented in a full version of this article.
Table 2 shows the total file space used for the compressed web snap-shots and genome collections. We see that our scheme is of considerable benefit. For example, combining cobald with 7-zip reduces the total size of the web snap-shots collection to less than 30% of the archive made when 7-zip is used alone. Compression of the genome collection was more than 15 times that achieved by 7-zip alone.
Compression of individual data sets in the genome collec-tion is shown in Figure 1. We observe that, after the first data set has been inserted into the index, the compression of subsequent data sets is in the range of 0.5% to 2%. Looking at individual data sets, the compression is not uniform. For releases 49 and 56, for instance, the compressed file sizes are significantly larger, most likely because of major updates to the database. Even then, our encoding scheme achieves a gain close to a factor of ten.
Our scheme, cobald , is a practical approach to collection-based compression. It compresses the large collections we have tested six to ten times more than is achieved by com-pressing the collection files individually. Additional files may readily be added to the collection and files are decompressed faster than with 7-zip . Recalling the desirable properties of a CBC system described in Section 1, our scheme avoids the need to read the entire collection when encoding a new file by maintaining the hash list file and constructing an index. Cobald achieves similar compression to mature delta encod-ing systems, and demonstrates some advantage with larger input files. Figure 1: Compressed sizes of each data set in the genome collection using cobald with n = 1024 .
 [1] R. C. Agarwal, K. Gupta, S. Jain, and [2] M. Ajtai, R. Burns, R. Fagin, D. D. E. Long, and [3] J. Bentley and D. McIlroy. Data compression with long [4] J. J. Hunt, K.-P. Vo, and W. F. Tichy. Delta [5] R. M. Karp and M. O. Rabin. Efficient randomized [6] D. G. Korn and K.-P. Vo. Engineering a differencing [7] J. P. MacDonald. File system support for delta [8] A. Tridgell. Efficient Algorithms for Sorting and
