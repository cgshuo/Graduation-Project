 1. Introduction
The establishment of reliable processes with increased ef -ciency and cost reduction is of primary importance in the fermentation industry. Nowadays, the increase of the number of sensors in biotechnology fi eld provides an important amount of heterogenous data. This data with various natures and various levels of precision require the use of automatic method of data fusion. The goal of data fusion is to diagnosis here the correctness of the bioprocess. The role of fermentation diagnosis is to detect the anomalies able to modify the optimal conditions maximizing out of product, in our case biomass. Anomalies can occur in biochemical parameters such as temperature, aeration, pH and dissolved oxygen. Monitoring and diagnosis of bioprocesses has been tackled in three ways: 1. Adaptive state estimators: These approaches are designed to adapt to the time varying characteristics of the process, for example to the changes in growth and metabolite expression rates when the nutrient levels are depleted. The measurements are used in conjunction with the process model and need to be carefully tuned to achieve accurate reconstruction of 2. Arti fi cial intelligence based algorithms: The second approach to 3. Model-based statistical signal processing: In these approaches, more than the task detection but provide information on the localiza-tion of these faults. Basically, the predictions obtained from models (analytical or logical) lead to contradictions with the observations produced by the different sources of information if these are not correct. This reasoning by absurd lets us regroup these con order to localize the faults. The detection of con fl icts is the the diagnostic. The second consists in generating the diagnosis from the set of con fl icts.

In this frame work the theory of belief functions used in this paper is an Arti fi cial Intelligence Method of Diagnosis. The originality of the approach presented in this paper is its capacity to manage imprecise and uncertain information. The fi nal purpose of our approach is, fi rst, to have an automatic method to evaluate the relevance of an information source, instead of using the subjective knowledge of an human expert, and secondly, to improve the result of classi fi cation (by detecting non-relevant sources of information). More precisely, we use the notion and values of distance measures in order to assess the relevance of a source. This approach enables to estimate the relevance without a priori informations on the source: the estimation of the relevance is based only on the intrinsic informations. We use two measures of con fl ict: a measure based on a norm between the mass function of the sources ( R X gis et al., 2004a , 2007 ) and a measure called the
Jousselme distance ( Jousselme et al., 2001 ). We test the evaluation of the relevance with the two measures on a practical application of fermentation bioprocess and the results show that this approach improves the results of classi fi cation.

The paper is organized as follows: Section 2 presents the context and problematic of bioprocess diagnosis, Section 3 presents the notion of relevance; Section 4 provides the basic notion of the theory of belief functions. In Section 5 we make the connection between the relevance and the theory of belief functions and Section 6 presents the experimental results for a bioprocess. 2. Bioprocess diagnosis
There are several kind of bioprocesses. One of the most well known is the batch, but there are also fed-batch and single CSTR (chemostat) bioreactors which also fed with sterile nutrient medium. The difference between them is in the manner of operate without or with external sources of biomass after inoculation.
When we speak of by external source of biomass, we mean the biomass introduced into the bioreactor after the inoculation. After this process of inoculation, the culture is maintained at conditions that are compatible with growth (e.g. at suitable temperatures) and often kept in an agitated state.

Three biological reactions need to be diagnosed in view of control: Growth:
Glucose  X  Salts  X  Vitamins  X  O 2 -Y S ; X Biomass  X  CO 2 Ethanol production: Glucose -Y S ; P Ethanol  X  CO 2  X  2  X  Maintenance:
Glucose  X  O 2 -CO 2  X  H 2 O  X  3  X  where Y represents the yield.

Depending on the organism being cultivated, the fermentation is typically carried out at volume ranging from a few liters up to a few hundred thousand liters, and lasting for a period of several hours or up to a few weeks. Furthermore, there may be additional requirements for amino acids, vitamins, purines, or pyrimidine.
It may be necessary to add precursors according to the metabolites that we want to obtain. Oxygen may or may not be required as the terminal electron acceptor, and the fermentation will need to be carried out at an appropriate pH. The medium components used to satisfy an organism's nutritional requirements are partially in enced by the nature of the fermentation process used developed. The biological system studied in this article is the yeast Saccharomyces cerevisiae . Yeast is one of the smallest eukaryotic systems sequenced and is unparalleled for the level of molecular investigations that have been carried out and the range of possible manipulations. It is an ideal target for a comprehensive study at the system-level. Two directions have been explored: 1. on-line analysis: It does not permit diagnosis in an instanta-neous manner or with certainty regarding the physiological state of the yeast. 2. off-line analysis: It makes possible to soundly characterize the current state, but unfortunately after too late to take into account this information or to adjust the process on the fl by actions of regulators allowing to adjust some critical para-meters such that pH, temperature (addition of basis, heats, cooling).

To remedy these drawbacks, computer scientists in collaboration with micro-biologists have developed tools for supervised control of the bioprocess. They use the totality of information provided by the sensors during a set of sample processes to infer some general rules to which the biological process obeys. These rules can be used to control the next processes. This is exactly the problem we tackle in this paper. To sum up, our application focuses on the evolutive behavior of a bio-reactor (yeast fermentation), that is to say an evolutive biological system whose interaction with the physical world, as described by pH, pressure, temperature, etc ... , generates an observable reaction. This reaction is studied by way of a set of sensors providing a large amount of (generally) numerical data.

Physiological state diagnosis uses the relevant information pro-vided by sensors in a process of classi fi cation. This  X  tion  X  associates classes to physiological states. Faults or abnormal behavior means apparitions of physiological states different from the fermentation goals. Therefore our goal is to avoid the reinforcement of the faults or abnormal behavior in the classi fi cation process.
The data used in this paper was obtained from the biomass production. Saccharomyces cerevisiae was studied under oxidative regime (i.e. no ethanol production). Two different protocols have been applied: a batch procedure that is followed by a continuous procedure. The batch procedure is composed by a sequence of biological stages. This phase can be thought of a start-up proce-dure. Biotechnologists state that the behavior in the batch proce-dure induced in fl uences later phenomena in the continuous phase. So complete knowledge of the batch phase is of great importance for the biotechnologist. The traditional way of getting acquainted with such knowledge is at present carried out through of fl measurements and analysis which most of the time produce results when the batch procedure has ended, thus lacking real time performance. Contrast the proposed methodology allows for real time implementation. This example deals with the batch procedure. The expert chooses among the set of available on-line signals which, according to the expert knowledge contain the most relevant information to diagnose the physiological state: 1. DOT: Partial oxygen pressure in the medium. 2. O 2 : Oxygen percent in the output gas 3. CO 2 : Carbon dioxide percent in the output gas 4. pH. 5. OH-ion consumption: Derived from control action of the pH regulator and the index of re fl ectivity.

The consumption of negative OH ions is evaluated from the control signal of the pH regulator. The actuator is a pump, switch on by an hysteresis relay, that inoculates a basic solution (NaOH).
The re fl ectivity, which is measured by the luminance, seems to follow the biomass density. Nevertheless its calibration is not constant and depends on the run.

The analysis of signals provided by sensors and actors allows to build up a system of physiological state diagnosis. In practice, we have to make a classi fi cation of the biochemical parameters: the aim is for a class (or a group of classes) obtained from classi to correspond to a physiological state. The detection of the physiological state makes it possible to control and optimize the bioprocess. The classi fi cation involves segmenting the biochemical parameters (which are time series) so that a class or a group of consecutive classes correspond to a precise physiological state. In fact, classi fi cation can be made  X  manually  X  by an expert in microbiology (see Fig. 1 ). Our goal is to automize to the analyze given by the expert (see for example R X gis et al., 2004b , 2008a ;
R X gis and Doncescu, 2007 ). As we said above, the microbiologist experts need to better control the physiological states of the micro-organisms but also the biochemical parameters which enable the characterization of the physiological states. Indeed, the microbiologists have knowledge no or a little about the pertinence of the biochemical parameters, and this knowledge is mainly empirical and subjective . Moreover this knowledge tends to deal with no more than fi ve parameters at a time while there are generally at least 12 parameters. A lot of information is then either redundant, unexploited or even erroneous. So it is important to fi nd tools to analyze these parameters in order to con fi rm or cancel the expert knowledge and then add or remove biochemical parameters from the classi fi cation.

In fact, the approach proposed in this article tries to answer this question:
Is it necessary to use all the biochemical parameters to make a diagnosis ?
The answer is not obvious, as one ( Greiner et al., 1997 ) shows that less of the half of the available parameters are actually used in some applications. And even if the expert in microbiology should answer yes to the question, we have seen that they only use some of these signals. That's why it is interesting to have an automatic method to evaluate the relevance of the parameters. This auto-matic method is based on the theory of belief functions and particularly on the notion of distance measures between belief functions distributions. 3. Diagnosis using arti fi cial intelligence-based system poses the available knowledge is limited to a set of observations (possibly uncertain or imprecise) of some variables and the problem of diagnosis is reduced to a problem of decision. The adaptation of a decision system to non-steady states is a funda-mental problem in diagnosis but not directly linked to a system of representation of the uncertainty. In this article we characterize the partial knowledge from some observations which are impre-cise and uncertain. Let us present the notion of relevance of data and some approaches used to manage it. 3.1. Notion of relevance as there is no single de fi nition in the fi elds of computer science.
However, this notion of relevance (but also and especially the notion of irrelevance) is widely used in the fi elds of computer sciences: classi fi cation ( Lazo-Cort X s and Ruiz-Schulcloper, 1995 ;
Blum and Langley, 1997 ), fault detection ( Baluja and Pomerleau, 1997 ), intelligent websearch engines ( Zadeh, 2004 ; Paltoglou et al., 2011 ), and so on. The de fi nition of relevance depends on its use and the framework. For Lazo-Cort X s and Ruiz-Schulcloper (1995) , the notion of relevance is linked to the ability to distinguish several classes. In the fi elds of information science, for Spoerri (2007) , the notion of relevance of a source depends on the number of information retrieval systems which fi nd this source. For Zadeh (2004) , the global relevance of a source depends on the informa-tion given by the other sources. For Paltoglou et al. (2011) , the relevance of a source is based on the relevance of the data (i.e. documents) of this source. Blum and Langley (1997) propose at least 5 different de fi nitions of the relevance and Pichon et al. (2012) modelize the relevance (and also the truthfulness) by using a mass function in the framework of the theory of belief functions. at hand, the nature of which is linked to the objective. This de fi nition is based on the encyclopedic de fi nition ( le Tr X sor de la Langue Fran X aise Informatis X  ; The Merriam-Webster dictionary ). We suggest a fi rst de fi nition of relevance.

De fi nition 1. A source is relevant if:
The fi rst requirement shows that the source does not produce artifacts. The second one is linked to the fact that the objective is to fi nd a correct classi fi cation. The last is based on the assumption that the majority of the sources re fl ects the truth at least to some extent: this last one is linked to the application. For example in the case of a bioprocesses (we will present the experimental results in these fi elds) we have assumed that on the majority of the biochemical sources, because all the sources should re fl same biological situation. 3.2. Methods of evaluation of relevance source. For instance Blum and Langley (1997) propose classifying them into 4 groups: the same time as the classi fi cation. Methods using logical rules ( Blum and Langley, 1997 ) for instance, belong to this class.
Filter approaches. This is not related to signal processing. The notion of fi ltering comes from the fact that these approaches use mathematical tools, independently of classi fi cation. Princi-pal Component Analysis (PCA) for example can be viewed as a fi ltering approach to select relevant features.

Wrapper approaches. These are based on the analysis of the classi fi cation and thus they are made after the classi fi
The method proposed by Dubois and Prade (1992) and as well as our approach belong to this category of approach.

Weighting methods. These are methods using weighting func-tions to provide a degree of relevance for each source. Neural networks can be used for this approach.

Other approaches such as Branch &amp; Bound ( Chen, 2003 ), voting ( Dubois and Prade, 1992 ) or the Felix's method ( Felix, 1994 ) can be viewed as methods which either implicitly or explicitly eva-luate the relevance. We must note that most of these methods give a global evaluation of each source and not a local evaluation.
The notion of dynamical relevance is more adapted to biopro-cesses as the relevance of biochemical parameters can vary according to the evolution of the bioprocess. Thus we propose to use the belief functions theory to evaluate the relevance of biochemical parameters. 4. Theory of belief functions 4.1. Basic notions Belief functions theory can be viewed as a generalization of the
Bayesian theory which take into consideration uncertainty and partial knowledge. It was introduced by Dempster (1968) and was mathematically formalized by Shafer (1976) .

We consider all hypotheses; this set is called the frame of discernment and is denoted by  X  . All the elements of the frame are exclusive and are called singletons . The belief functions theory works on the set of subsets (or assertions) A of  X  . This set of subsets of  X  is denoted by 2  X  . A can be a union of several singletons or one singleton. A mass function m  X  :  X  is then de from 2  X  to [0, 1] with the following properties:  X  m  X  A  X  X  1 m  X   X   X  X  0  X  4  X  The belief and plausibility functions are de fi ned from 2 Bel  X  A  X  X   X  Pl  X  A  X  X   X 
The Dempster rule allows to merge the mass function of two sources (or more)  X  m 1 m 2  X  X  A  X  X 
A ; B ; C A 2  X   X  6  X  K (called Dempster's con fl ict) is de fi ned as K  X   X 
The denominator  X  1 K  X  is here to enable the normalization. K is a measure of a kind of con fl ict between the sources. If K is equal to 1, the fusion is unde fi ned. For several sources, it is possible to compute K iteratively using the con fl ict between two sources then between these two sources and a third source, etc. 4.2. About the con fl ict in the theory of belief functions
Various studies have been done on the notion of con fl ict de fi ned by the theory of belief functions. Several alternatives merging rules ( Yager, 1987 ; Smets, 1988 ; Smets and Kennes, 1994 ; Lefevre et al., 2002 ; Schubert, 2007 ) have been proposed to overcome the pre-supposed erroneous results generated by the Dempster's con fl ict used in the combination from theory of belief functions (see for instance the famous example proposed by Zadeh, 1984 ). However it has been show ( Haenni, 2005 ) that this counter-intuitive example proposed by Zadeh, showing that Dempster's con fl ict K can lead to erroneous conclusion, does not come from erroneous properties of the Dempster rule of combina-tion themselves, but rather from an erroneous use of the theory of belief functions. Moreover, erroneous results can come from the combination of subproblems that ought to be handled indepen-dently ( Schubert, 1993 , 1996 ). On the other hand, the fact that the Dempster's con fl ict of two identical belief functions is not null has been studied and explained ( Liu ; Schubert, 2012 ). Furthermore, the notion of con fl ict itself K has been reevaluated ( Liu; Martin, 2012 ; Destercke and Burger, 2012 ): the Dempster's con fl really a con fl ict measure between the basic belief assignments and can be interpreted qualitatively as an indicator of compatibility between two hypothesis. Nowadays, after the proposition of new formulas for the combination in belief functions theory, one of the trends is to propose and classify several kinds of distance mea-sures between bodies of evidence ( Martin et al., 2008 ; Jousselme and Maupin, 2012 ). A possible distinction between existing mea-sure distance is based on what kind of distances are measured by these measures. There are at least two possibilities ( Schubert, 2011 ): some measures distance measure the degree to which two bodies are different (as the Minkowski family of distance belief functions ( Jousselme and Maupin, 2012 ), based on metrics) and other distances (as Dempster's con fl ict) measure the degree to which they are compatible. Different measures measure different types of distance ( Schubert, 2011 ; Jousselme and Maupin, 2012 ). The choice of a distance measure should be based on the applica-tion and the fusion's aim. 5. Diagnostic of relevance and the theory of belief functions 5.1. Use of the theory of belief functions for relevance
We propose to use distance measures from belief functions theory to evaluate the relevance of the biochemical parameters (which are here the sources of information). One of the fi introduce the con fl ict of belief functions theory to compare sources of information was Schubert (1993 , 1995) . He proposed using the Dempster' s con fl ict between sources in order to regroup the sources which have small differences and thus make clusters of sources of information. The aim of theses clusters is to have a more coherent fusion of sources. Moreover, in belief functions theory it is important ( Li et al., 2006 ) to select the sources before making the fusion. R X gis et al. (2004a , 2007) then Chebbah et al. (2010) and Martin et al. (2008) have independently proposed using distance measure to evaluate the quality of the sources of information. R X gis et al. call this quality relevance whereas Chebbah et al. and Martin et al. call it reliability, but the two notions are quite similar. By computing the distance measure two by two between the sources of information, it is possible to see which sources are in consensus and which sources are not. For a given source, the mean of the distance measure (two by two with the other sources) is computed and if this mean is lower than a threshold  X  A  X  0 ; 1 (a priori,  X   X  0 : 5 but empirically its value can vary) it is considered as relevant, otherwise it is considered as non-pertinent. Thus as we said above, we have made the assump-tion that the majority of the sources are valid providing that all the sources observe the same situation. It is then possible to char-acterize with a certain fl exibility the pertinence of the sources.
This characterization is made for each sample, that is to say in a local way. This local characterization is more signi fi cant than a global characterization which does not take into account the evolution of the experience over time. Thus if a source is estimated as non-relevant, it is weakened ( Shafer, 1976 ) before the fusion of the sources. The approach can be summarized as follows:
For each given time t : 1. Characterization of the relevance 2. Fusion of the information by the Dempster combination of all the parameters with:
We propose to use a binary approach (relevant/not relevant) for the data source by using a reducing weight arbitrarily equal to 0.5 but one can note that there are approaches to evaluate and optimize the degree of quality (or relevance/reliability) for a data source ( Martin et al., 2008 ). An other approach which can be used is the discounting of the data source until it enables to have results fi tting to a prede fi ned level of con fl ict; more details can be found in the work of Schubert (2011) . 5.2. Use of distance measures
As we have said above, the choice of a distance measure depends on the application and the aim of the data fusion. Our approach is based mainly on the de fi nition of the relevance (see
De fi nition 1 in 3.1) particularly for the third point:  X  a source is relevant if it implies a decision which agrees with most of the other sources  X  . This point clearly implies a notion of similarity between data sources and led us to choose measures distances which let us to measure the degree to which two bodies are different. The Dempster' s con fl ict is not well adapted for this application because it does not measure similarity or difference between two belief function distributions. We propose to use two distance measures, both from the Minkowski family of distance belief functions ( Jousselme and Maupin, 2012 ).

The fi rst is based on the usual norm called norm 1 ( R X gis et al., 2004a , 2007 ) (see also Cuzzolin, 2008 ) and is de fi ned as follows: conf 1  X  S i ; S j  X  X  d met  X  S i ; S j  X  X  1 2  X  where m 1 and m 2 are the mass functions for the information sources S i and S j . The factor 1 2 is a factor of normalization.
This distance measure is easy to use, but its main drawback is that it can be used only if the mass function of each of the sources are all distributed on the same focal elements (in practice it is often the case, but one can fi nd actual or simulated cases where the mass functions of the sources do not work on the same elements). and is called the Jousselme distance ( Jousselme et al., 2001 ). This enables us to re fl ect on the speci fi c functions of belief since this distance used the Jaccard coef fi cient j A i \ A j j = j A and A j are two focal elements. This Jaccard coef fi cient takes into account the cardinality of focal elements. A matrix of the Jaccard coef fi cient D is well de fi ned on the set 2  X  , which makes this speci fi c distance belief functions. Jousselme distance is given by with
D  X  A i ; A j  X  X  1if A i  X  A j  X   X   X  where m 1 and m 2 are two mass functions of the sources S
The distance measure between S 1 and S 2 is given by conf 2  X  S 1 ; S 2  X  X  d jous  X  m 1 ; m 2  X  X  13  X  without condition on the distribution of mass functions on the focal elements. It may therefore be used in all real cases where the sources are working or not on the same focal elements. The value of distance Jouselme will be preferred to the distance metric for experimental tests.
 of a source S i (from other sources, more precisely from the belief function distributions of the other sources) is de fi ned as follows:
ConfMean  X  S i  X  X  1 n 1 :  X  where n is the number of sources. 6. Experimentation 6.1. Material: bath fermentation bioprocess
Saccharomyces cerevisiae ) which takes about 20 h and corresponds to 1012 points of measurement of the biochemical parameters. We consider the start of the bioprocess as t  X  0 h (0 h). Recall that we are trying to identify three main physiological states (see Fig. 1 ): 1. State 1: Fermentation (ethanol). It goes up about for 9 h (from 2. State 2: Dioxidation. This state starts at about 9 h and ends at 3. State 3: Oxidation (biomass). It begins at 9 h 46 and ends at
There are 22 biochemical parameters and each of them has 1012 elements. So there is time series with 1012 points for each of them. 6.2. Methods: mass functions for theory of belief functions
R X gis et al. (2004a , 2007 , 2008b) . We use two different kinds of computation for the mass function, one for each kind of biopro-cess. In order to compute the mass function, we use the method proposed by Denoeux (1995) because in this bioprocess it is possible to make a supervised classi fi cation. The method of
Denoeux is based on the use of k nearest neighbors. For this bioprocess, 68 samples were used: 30 samples for the state 1 (called class C 1 ), 7 samples for the state 2 (called class C samples for the state 3 (called class C 3 ). The approach of Denoeux (1995) let us compute the masses for the classes C 1 , C 2 the set  X  by using the following equations: m  X  C
 X  X  m i  X  C i  X  m  X 
 X  X  where K is the following factor of normalization:
K  X   X 
With m  X  C
 X  X  1  X  m  X 
 X  X   X  where d ki ; l represents the metric distance between each element to classify x l and the labeled sample x ki of the class C for each biochemical parameter; e represents the exponential function and  X  0 is a fi xed value between 0 and 1 (see Denoeux, 1995 ). For this application we have chosen  X  0  X  0 : 95. For the experimentation, we have tested several values of k for the k nearest neighbours (from k  X  1to k  X  7).

As we use a method which computes the masses only for singletons (i.e. here each of the 3 exclusive classes) and for the set , the choices of the plausibility or the belief lead here to similar results in the classi fi cation. 6.3. Experimental results
First of all, let us clarify that in this subsection, when we talk about con fl ict of sources, it is in a general way (it is not the
Dempster's con fl ict): it means con fl ict from the difference from distance measure of belief function theory. And before we discuss about the results of classi fi cation, let us analyze the experimental interest of the relevance of the sources of information (which are here biochemical parameters). The relevance should have an experimental sense, otherwise it will not be useful. The following examples are given for k  X  7 (number of k nearest neighbours),  X  0 : 3 and with the measure based on norm 1 (but with the Jousselme distance the results are quite similar). For example in
Fig. 2 , for the variable RQ (the Respiratory Quotient) the singular-ity localized at 5.35 h (which is actually an artifact) is considered as not relevant by the method, and consequently is not taken into account during the classi fi cation. Thus the classi fi cation tends to make fault detection and eliminate those fault detections. More-over, the parameter temperature which is a regulated parameter is relevant from t  X  0h to t  X  8.05 h and then becomes irrelevant before the beginning of state 2 (see Fig. 3 ). When we analyze this parameter, we note that it does not change until t  X  15 h where it is changed by the expert. The relevance comes again from t  X  16.5 h until the death of the micro-organisms and corresponds to the change made by the expert (the time delay of the relevance may correspond to a lag time of the biological system).

We can see that the evaluation of the relevance provides consistent results concerning the regulated parameters. These results are con fi rmed by the experts in microbiology. This con-fi rmation of the relevance by the expert in microbiology is not so surprising: actually the estimation of the relevance of the data sources is a non-model-based approach and it is based on the data from the process. As data of the bioprocess express the biological phenomenon (and also the biochemical actions of the expert) the estimation of relevance is thus biologically consistent.
Secondly, with these fi rst experimental results, we can make an empirical comparison between the Jousselme distance and the distance based on norm 1. We recall that they both belong to the Minkowski family of measures. Let us take some biochemical parameters to compare the value of their con fl icts computed with the Jousselme distance and with the distance based on norm 1: we can see that the value of the average of each con fl ict is similar for the two distances. For example for the measured carbon dioxide (see Fig. 4 ), the two curves (representing the mean of the con with the Jousselme distance and with the distance based on norm 1) seems to be similar; the average of the con fl ict from the distance based on norm 1 seems to be a positive translation of the one from the Jousselme distance. In Fig. 5 , results are similar for the acidity (pH). This is con fi rmed if we compute statistical properties of the average of the con fl ict of all biochemical para-meters at each measured time. In Fig. 6 , minimum, maximum, and average of the con fl ict from the distance based on norm 1 are respectively close to minimum, maximum and average of the con fl ict based on Jousselme distance. In fact the con fl distance based on norm 1 are not positively translated from the ones from the Jousselme distance, but the differences between the derivatives are close to 0: more precisely, the difference between the two derivatives is at the scale of 10 3 whereas the values of con fl icts, whatever the biochemical parameter, are close to 10 1 (see Table 1 and Fig. 7 ). Thus they have a similar trend.
This is con fi rmed by the difference between the standard devia-tion which is also close to 0. Furthermore, the distance between the con fl ict from the con fl ict the Jousselme distance and the con fl ict from the distance based on norm 1 is at the scale of 10 2 (see for example Fig. 8 ). We note that the con fl ict from the distance based on norm 1 is always higher, than the one from the Jousselme distance, whatever the biochemical parameter.
Thus the con fl ict have similar behavior but the con fl ict from the distance based on norm 1 is higher so more restrictive than the one from the Jousselme distance: at each time, con fl ict from distance from norm 1 will have less relevant biochemical parameters (relevant data sources) than the con fl ict from the Jousselme distance. approach presented in this paper provides better results. We note that it is dif fi cult to provide a benchmark and to compare this approach with other methods because, as we have seen above, each method of evaluation of the relevance of data sources depends on the de fi nition of relevance which has been chosen.
However, it is possible to analyze the results of classi fi expert in microbiology give us a normal behaviour, we can compare the results of the classi fi cation with and without estima-tion of relevance. The improvement come from 6 points to 20 points compared to the result of classi fi cation without relevance.
We have tested the two measures (with several empirical values for the threshold), and the results are given in Table 2 . We can see that, whatever the measure, whatever the values of threshold, the results are better with than without the estimation of relevance.
Moreover, results with the two measures are quite similar, with a little advantage (but a priori not really signi fi cant) for the measure of con fl ict based on norm 1. This little difference can be explained by the fact that the measure of con fl ict based on norm 1 is more restrictive than the one based on Jousselme distance. Thus the selected biochemical parameters, considered as relevant, are less numerous than those of the Jousselme distance and therefore give more precise information for the classi fi cation. However, we repeat that regardless of the chosen measure, this approach improves the perform of the classi fi cation.

We note that experimental results show that the threshold in fl uences the results of correct classi fi cation; the study and the optimization of this threshold will be not discussed in this article but will be part of future work. In any case, the taking into account of the relevance improves the results of classi fi cation and this is, from our point of view, an important element of this work. 7. Conclusion
We have proposed the use of belief functions theory for the evaluation of relevance of the sources of data and for the diagnosis by classi fi cation of bioprocesses. The diagnosis is viewed as the best choice of variables to determine the physiological states. The de fi nition of the relevance that we use is based on the distance measures in the framework of belief functions theory. We compare two distance measures, and we show that the results of the classi fi cation are empirically similar. On the one hand, the estima-tions of relevance from this approach of the data source are con fi rmed by the expert in microbiology: this method provides meaningful and consistent information about the relevance of the data sources. On the other hand, whatever the chosen measure, the estimation of relevance improves the percentage of correct classi fi cation. These two results improve the process: relevance carries out using theory of believe function contributes to the optimization of bioprocesses. Further works should concern the analysis of the relevance on time sets (instead of an analysis of a discrete system) and the optimization of the threshold  X  regarding the results of classi fi cation. Indeed, as we said above, an other approach ( Schubert, 2011 ) adjusting the relevance of data source (by using discounting) in order to satisfy a prede fi ned level of con fl ict could also be used in this application. An other alternative method is the use of a weakened factor depending on the value of the average distance measure ( Martin et al., 2008 ); this could let us avoid the use of a direct threshold as  X  . These approaches should be tested in future works.
 Acknowledgments
We would like to thank the anonymous reviewers for their constructive comments which have greatly helped us to improve the paper. We also thank the staff of the INSA laboratory of biotechnology-bioprocesses of Toulouse for their help and advices and particularly to Prof. Jean-Louis Urribelarea. This work is partially supported by Directorate of Youth, Sports and Social Cohesion of Guadeloupe, French Caribbean ( http://www.guade loupe.drjscs.gouv.fr/ ).
 References
