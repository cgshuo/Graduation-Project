 Text reuse is a common phenomenon in a variety of user-generated content. Along with the quick expansion of social media, reuses of local text are occurring much more frequently than ever before. The task of detecting these local reuses serves as an essential step for many applications. It has attracted extensive attention in recent years. However, semantic level similarities have not received consideration in most previous works. In this paper, we introduce a novel method to efficiently detect local reuses at the semantic level for large scale problems. We propose to use continuous vector representations of words to capture the semantic level similarities between short text segments. In order to handle tens of billions of documents, methods based on information geometry and hashing methods are introduced to aggregate and map text segments presented by word embeddings to binary hash codes. Experimental results demonstrate that the proposed methods achieve significantly better performance than state-of-the-art approaches in all six document collections belonging to four different categories. At some recall levels, the precisions of the proposed method are even 10 times higher than previous methods. Moreover, the efficiency of the proposed method is comparable to or better than that of some other hashing methods. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Search and Retrieval; H.3.7 [ Digital Libraries ]: Collection, Systems Issues Local Text Reuse, Word Embedding, Fisher Vector
With the rapid expansion of the World Wide Web, digital information has become much easier to access, modify, and duplicate. Text reuse, which is the practice of using existing content without modification or with few modifications, has become a common phenomenon on the Web. Rather than reusing a whole document, sentences, facts or passages were also often reused and modified, especially in social media [28]. Local text reuse is usually used to refer to this kind of phenomenon. The task of detecting local text reuses is important for many applications such as information flow tracking [20], plagiarism detection [29], partial-duplicate detection [39], and so on.

Along with the increasing requirements, the task has received considerable attention in recent years [28, 39, 34, 38]. Existing works on local text reuse detection have been conducted from different perspectives. However, we argue that most of them worked on the lexical level. The semantic similarities between text segments were not considered. Let us consider the following examples: From these examples, we can observe that although only one word differs between the three sentences, sentence S3 should not be considered as the local text reuses of sentence S1 and sentence S2 . The meaning expressed by S3 is almost opposite to S1 and S2  X  X . Since existing methods are usually based on lexical level similarities, this kind of issue cannot be well addressed by these methods.

Inspired by the success of continuous space word repre-sentations in capturing the semantic similarities in various natural language processing tasks, we propose in this work to incorporate continuous space word representations, Fisher kernel framework, and hashing methods to generate hash codes for short text segments, while preserving semantic level similarities. The generated hash codes can be used to detect local reuses more efficiently and effectively. Due to the ability of continuous space word representations (also called  X  X ord embeddings X ) at capturing syntactic and semantic regularities, we firstly transform words in a text segment into continuous vector representations by looking up tables. These word representations were learned in advance using a feed-forward neural network language model [3], continuous skip-gram model [22], or other continuous space word representation learning methods. Then, the variable sizes of word embedding sets will be aggregated into a fixed length vector, Fisher Vector (FV), based on the Fisher kernel framework [27]. Since FVs are usually high-dimensional and dense, it makes the system less efficient for large-scale applications. Hence, the final step of the proposed framework is to compress FVs to binary hash codes using hashing methods [6, 36, 26]. Through these steps, the proposed method can map text segments into compact binary hash codes, while preserving the similarities at the semantic level. Hence, it can efficiently and effectively achieve the local text reuse detection problem. Through evaluating on six large collection belonging to four different categories, experimental results demonstrate that the proposed framework can achieve better performance than state-of-the-art approaches.

The main contributions of this work are summarized as follows.
Our approach relates to the following three research areas: text reuse detection, learning to hash, and fisher kernel framework. In this section, we briefly describe the related works on these areas.
The task of detecting text reuse has received considerable attentions in recent years. Previous works studied the problem from different aspects such as fingerprint extraction methods with or without linguistic knowledge, hash codes learning methods, different reuse granularity, and so on.
Broder [4] proposed Shingling method, which uses con-tiguous subsequences to represent documents. It does not rely on any linguistic knowledge. If sets of shingles extracted from different documents are appreciably overlap, these documents are considered exceedingly similar, which are usually measured by Jaccard similarity. In order to reduce the complexity of shingling, meta-sketches was proposed to handle the efficiency problem [5]. In order to improve the robustness of shingle-like signatures, Theobald et al. [32] introduced a method, SpotSigs. It provides more semantic pre-selection of shingles for extracting characteristic signa-tures from Web documents. SpotSigs combines stopword antecedents with short chains of adjacent content terms. The aim of it is to filter natural-language text passages out of noisy Web page components. They also proposed several pruning conditions based on the upper bounds of Jaccard similarity.

I-Match [7] is one of the methods using hash codes to represent input document. It filters the input document based on collection statistics and compute a single hash value for the remainder text. If the documents have same hash value, they are considered as duplicates. It hinges on the premise that removal of very infrequent terms and very common terms results good document representations for the near-duplicate detection task. Since I-Match signatures are respect to small modifications, Ko lcz et al. [18] proposed the solution of several I-Match signatures, all derived from randomized versions in the original lexicon.

Local text reuse detection focused on identifing the reused and modified sentences, facts or passages, rather than whole documents. Seo and Croft [28] analyzed the task and defined six categories of text reuses. They proposed a general framework for text reuse detection. Several fingerprinting techniques under the framework were evaluated under the framework. Zhang et al. [39] also studied the partial-duplicate detection problem. They converted the task into two subtasks: sentence level near-duplicate detection and sequence matching. Except for the similarities between documents, the method can simultaneously output the positions where the duplicated parts occur. In order to handle the efficiency problem, they implement their method using three Map-Reduce jobs. Kim et al. [17] proposed to map sentences into points in a high dimensional space and leveraged range searches in this space. They used MD5 hash function to generate hash code for each word. File signature is then created by taking the bitwise-or of all signatures of words that appear in the file.

Different with these existing methods, in this paper, we propose to use aggregated word embeddings to capture the semantic level similarities to reduce the false matches.
Due to the ability of solving similarity search in high dimensional space, hash-based methods have received much more attention in recent years. Extensive works on similarity search have been proposed to find good data-aware hash functions using machine learning techniques.
Hinton and Salakhutdinov [10] proposed to train a multilayer neural network with a small central layer to convert high-dimensional input vectors into low-dimensional codes. They used a two-layer network called a Restricted Boltzmann machine (RBM) to do it. Spectral hashing [36] was defined to seek compact binary codes in order to preserve the semantic similarity between codewords. They defined the criterion for a good code which is related to graph partitioning and used a spectral relaxation to obtain a solution. Norouzi and Fleet [24] introduced a method for learning similarity-preserving hash functions, which is based on latent structural SVM framework. They designed a specific loss function taking Hamming distance and binary quantization into consideration. Self-Taught Hashing (STH) [37] divided the hash codes learning problem into two stages. Firstly, they used unsupervised method, binarised Laplacian Eigenmap, to optimize l -bit binary codes for all documents in the given corpus. The classifiers were trained to predict the l -bit code for unseen documents.

Besides the works on document level, based on the qSign framework [17], Zhang et al. [38] proposed a method to optimize hash codes of words/characters rather than directly use MD5 for sentence level reuse detection. They also proposed to use GPU to accelerate the Hamming distance calculation. Wang et al. [35] proposed to use hashing methods to address tag completion and prediction problem. They used discrete optimization to construct the hash codes for both data examples and tags. The constructed the hash codes for the observed tags are consistent.
Fisher Kernel (FK) introduced by Jaakkola and Haus-sler [15] combines the benefits of generative and discrim-inative approaches for classification through the kernel derived from a generative probability models. It has been successfully applied to many tasks [14, 23, 12, 31, 8].
Jaakkola et al. [14] proposed to use the derived kernel function from HMMs corresponding to the protein family of interest to detect remote protein homologies. In [23], Moreno et al. explored their work in using Fisher kernel methods for audio classification problem. They proposed to use FK to map audio sequences into a fixed length representation and then use discriminative models to classify the data. Hofmann proposed a FK based method to calculate the similarities between documents. Each document is modeled as a memoryless information source. Fisher kernel is derived from the learned multinomial subfamily. Sun et al. [31] introduced a LDA-based Fisher kernel method to achieve text segmentation task. They proposed to use Latent Dirichlet Allocation to compute words semantic distribution and use Fisher kernel to measure semantic similarities.

The most similar work to ours is the recent work by Clinchant and Perronnin [8]. They proposed to use Fisher kernel to aggregate word embeddings to represent documents. They studied the relationship with LSI, pLSA, and LDA. They also evaluated the representation method through clustering and retrieval. Different with them, in this paper, we study the FK framework in representing the short text segments for the local text reuse detection. To overcome the efficiency problem, we also propose to incorporate hashing methods to map Fisher vectors to binary hash codes.
The processing flow of the proposed method is shown in Figure 1. Given a collection of documents, sentences or short text segments are treated as the basic processing units. Through table lookup, all the words in a sentence are transformed to continuous vectors generated by the word embeddings learning step. The dashed arrow between corpus to word embeddings learning represents that in-domain corpus can also be included into the training data. Based on the learned word embeddings, sentences are represented by variable-size sets of word embeddings. Generative model estimation step, which is the generative part in the FK framework, tries to estimate the parameters for the models used to describe the data. Fisher Vector generation step uses the estimated parameters of generative model and bag-of-embedded words to generate FVs for all the sentences. After that, learning to hash methods are used in the hash codes generation step to transfer the dense FVs to hash codes. Semantic level local text reuses can be detected through calculating the hamming distances between these hash codes.

From the framework, we can see that although word embeddings computation and generative model estimation steps are time consuming, they run only once in advance. Meanwhile, the computational requirements of FV gener-ation and hash code generation are limited. Hence, the proposed framework can efficiently achieve the local reuse detection task at the semantic level. In the following of this section, we detail the main steps of the proposed framework.
Representation of words as continuous vectors recently has been shown to benefit performance for a variety of NLP and IR tasks [11, 33, 30]. Similar words tend to be close to each other with the vector representation. Moreover, Mikolov et al. [21] also demonstrated the learned word representations could capture meaningful syntactic and semantic regularities. Hence, in this work, we propose to use word embeddings to capture the semantic level similarities between short text segments.

Fig. 2 shows three architectures used for learning word embeddings. w i represents the i th words in the given words sequence { w 1 ,w 2 , ..., w T } . Fig. 2(a) shows the architecture of the probabilistic neural network language model (NNLM) proposed by Bengio et al. in [3]. It can have either one hidden layer beyond the word features mapping or direct connections from the word features to the output layer. They also proposed to use softmax function for the output layer to guarantee positive probabilities summing to 1. The word vectors and the parameters of that probability function w the current word [21]. can be learned simultaneously. In this work, we only use the learned word vectors.

Fig. 2(b) and Fig. 2(c) show the architectures of the methods proposed by Mikolov in [21]. The architecture of CBOW, which is similar to NNLM, is shown in Fig. 2(b). The main differences are that (i) the non-linear hidden layer is removed; (ii) the words from the future are included; (iii) the training criterion is to correctly classify the current ( w t ) word. The Skip-gram architecture, which is shown in Fig. 2(c), is similar to CBOW. However, instead of predicting the current word based on the history and future words, it tries to maximize classification accuracy of words within a certain range before and after the current word based on only the current word as input.

Besides the methods mentioned above, there are also a large number of works addressing the task of learning distributed word representations [33, 19, 13]. Most of them can also be used in this work. The proposed framework has no limits in using which of the continuous word representation methods.
Base on the learned word embeddings, short text seg-ments can be represented by variable length sets of word embeddings, which can be viewed as Bag-of-Embedded-Words (BoEW) [8]. Semantic level similarities between text segments represented by BoEW can be captured more accurately than previous BoW methods. However, since BoEWs are variable-size sets of word embeddings and most of the index methods are not suitable for this kinds of issues, BoEWs cannot be directly used for large scale problems. Inspired by the success of Fisher Kernel framework in various image processing techniques, we propose to adopt it to aggregate BoEW to a fixed-length vector.

Given a corpus D = { d i , 1  X  i  X | D |} ,where d i is the i th text segment and | D | is the number of text segments in the corpus. The i th text segment d i is composed by a sequence of text words w i = { w ij , 1  X  j  X  N i } ,where N i represents the length of d i . Through table lookup, the i th text segment d i can be represented by ew i = { ew ij , 1  X  j  X  N i } , where ew ij is the word embedding of w ij . According to the framework of Fisher Kernel, text segments are modeled by a probability density function. In this work,we use Gaussian mixture model (GMM) to do it. We assume that the continuous word embeddings ew i in the text segment d are generated by GMM, which is denoted as u  X  in the following.  X  =[  X  1 , X  2 , ...,  X  K ] denotes the vector of K parameters of u  X  .The i th vector component in GMM is characterized by normal distributions with weights  X  i means  X  i and covariance matrices  X  i .Hence,inthiswork,  X  includes {  X  i , X  i ,  X  i , 1  X  i  X  K } . The parameters  X  usually can be estimated through the optimization of Maximum Likelihood (ML) criterion using Expectation Maximization (EM) method. In the following of this section, we follow the notations used in [27].

Based on the learned u  X  , the text segment d i can be characterized using the following score function: where G d i  X  is a vector whose size is only dependent on the number of parameters in  X  and not the number of words in the text segment. The gradient describes the contribution of each individual parameters to the generative process.
According to the theory of information geometry [1], U = { u  X  , X   X   X  } , which is a parametric family of distributions, can be regarded as a Riemanninan manifold M  X  with a local metric given by the Fisher Information Matrix (FIM) F  X   X 
Based on this observation, the following equation can be used to measure the similarity between two text segments d and d j using the the Fisher Kernel [15]: Since F  X  is symmetric and positive definite, F  X  1  X  can be transformed to L  X  L  X  based on the Cholesky decomposition. Hence, K FK ( d i ,d j ) can be rewritten as follows: where Under the assumption that ew ij is sampled independently, in this work, G d i  X  can be rewritten as follows: In [27], G d i  X  is also referred to as Fisher Vector of d dot product between Fisher vectors can be used to calculate the semantic similarities.

Based on the specific probability density function, GMM, we used in this work, FV of d i is respect to the mean  X  and standard deviation  X  of all the mixed Gaussian distributions. Let  X  j ( k ) be the soft assignment of the j th word embedding ew ij in d i to Gaussian k ( u k ): Mathematical derivations lead to: where N i is the number of word embeddings in d i .The division between vectors is as a term-by-term operation. The final gradient vector G d i  X  is is the concatenation of the dimensionality of the word embeddings. The final gradient vector G d i  X  is therefore 2KT-dimensional.
Through the previoussteps, a variable length of text segments can be transferred to a fixed length vector and the semantic similarities between text segments can be preserved. However, Fisher vectors are usually high dimensional and dense. It limits the usages of FVs for large-scale applications, where computational requirement should be studied. In this work, we propose to use hashing methods to address the efficiency problem.

The task of generating hash codes for samples can be formalized as learning a mapping b ( x ), referred to as a hash function, which can project p -dimensional real-valued inputs x  X  R p onto q -dimensional binary codes h  X  X  X { X  1 , 1 } q , while preserving similarities between samples in original spaces and transformed spaces. The mapping b ( x )canbe parameterized by a real-valued vector w as: where sign(  X  ) represents the element-wise sign function, and f ( x ; w ) denotes a real-valued transformation from R p . In this work, Fisher vectors of text segments are the x in mapping function b ( x ; w ). A variety of existing methods have been proposed to achieve this task under this framework using different forms of f and different optimization objectives. Most of the learning to hash methods for dense vectors can be used in this framework. In this work, we evaluated several state-of-the-arts hashing methods, whose performances are shown in the experiment section.
In this section, we firstly describe how we construct the collection. Then we introduce the experiment configurations and baseline methods. Finally, the evaluation results and discussions are given.
We evaluate the proposed method on two different datasets. The first one is used in [38] ( CRD for short in the following). It contains six collections includes: TIPSTER (Volume 1-3), ClueWeb09-T09B, Tweets2011, SogouT 2.0, Baidu Zhidao, Sina Weibo. Table 1 shows the statistics of the six collections. From the statistics, we can see that two languages (English and Chinese) and four categories (news, web page, microblog, and Q&amp;A community ) are included in the dataset. They randomly selected 1 million sentences from each collections as the evaluation dataset and 2,000 sentences as reuse detection queries. All the sentences whose similarities the query in word level are bigger than 0.8 are extracted as golden standards. In this work, we used the same corpus as them to evaluate the proposed framework.
Since the golden standards used in [38] are constructed based on only similarities in word level, semantic level similarities were not taken into consideration. In this work, we manually annotated another dataset, CRD-S .We randomly selected 100 queries from the original 2,000 queries in CRD for each of the collections. We calculated the cosine similarities between them and all the sentences. The sentences whose similarities with the query are higher than 0.7 were extracted as candidates for further processing. Three annotator were asked to determine whether an extracted candidate is a reuse of the query or not. To evaluate the quality of corpus, we validated the agreements of human annotations using Cohen X  X  kappa coefficient. The average  X  among all annotators is 0.637. It indicates that the annotations of the corpus are reliable. These manually annotated pairs construct the golden standards of CRD-S.
Following the parameters used in [17] and [38], in this work, we also set the length of hash code to 32 bit. Words for both English and Chinese collections are used as the basic units for learning vector representations. Since Chinese is written without spaces between words, we use FudanNLP [25] to segment sentences into words. For generating Fisher vectors for text segments represented by word embeddings, we use INRIA X  X  Fisher vector implemen-tation [16]. The number of Gaussian densities in GMM is set to 32. Precision (P), Recall (R), and F1-score ( F 1 )are used as the evaluation metrics.

For comparing the effectiveness, the following state-of-the-art methods were also evaluated on both CRD and CRD-S datasets. For SmH, SpH, and SimH methods, text segments are repre-sented by bag-of-words model. The vocabulary construction is based on the TF  X  IDF scores of words. We filtered stop words and low frequency words and selected 20,000 words to construct the vocabulary.

As we mentioned in the previous section, most of word embeddings and hashing methods can be used in the proposed framework. We select some of state-of-the-art methods for evaluation. Tabel 2 shows the word representations we used in this work. Due to the high computing cost required by C&amp;W and HLBL, we used the publicly available word embeddings provided by Collobert et al. [9] 3 and Turian et al. [33] 4 for English words. The word representations trained by GCNLM are also public available for English. 5 CBOW and Skip-gram represent the word embeddings estimated based on the evaluation data sets. CBOW-CL and Skip-gram-CL represent the word embeddings trained based on ClueWeb09. Since the proposed framework has no limitations about using which word embeddings learning methods and hash codes http://www.cs.toronto.edu/  X  hinton/ http://www.cs.huji.ac.il/  X  yweiss/SpectralHashing/ http://ml.nec-labs.com/senna/ http://metaoptimize.com/projects/wordreprs/ http://www.socher.org/ Table 2: The word embeddings used in this work. The top 8 rows of the table are English ones and the others are Chinese word embeddings.
 generation methods, we evaluate several combinations of them. CBOW+SpH presents the words embeddings learned with CBOW and compressed with spectral hashing. Skip-gram+SimH denotes the combination of Skip-gram and similarity hash [6].
For comparing the performances of different hash code generation methods, we firstly evaluate the proposed method and state-of-the-art methods in CRD corpus. For English collections, we use the word embeddings provided by Collobert et al. [9]. For Chinese datasets, we use CBOW-CL method, which uses ClueWeb09 as the training corpus, to generate word embeddings. Spectral hash and similarity hash were used to compact Fisher vectors of sentences into hash codes. The precision-recall curves graph of all six collections are shown in Figure 3. From the results we can see that, in all cases, both the proposed methods achieve better performance than the other hash code generation approaches. In all three English datasets, the proposed method achieves significantly better results than existing methods. Among the three Chinese datasets, the proposed methods achieve the highest relative improvement in SogouT dataset. We think that the performance loss provided by Chinese word segmentation method may be one of the main reasons. Since the most of the documents in Baidu Zhidao and Weibo are user generated, the performances of CWS toolkit are worse than in SogouT. The error of CWS may impact the generated Fisher vectors and the final results. In most of the cases, the performance of Simhash and Spectral hash are the worse than others. We think that the main reason is that the vector of short text segment is too sparse with BoW presentation. However, Spectral hash and Simhash can preserve the similarities between FVs well. Precision Recall Precision Recall
To detail the performance of our framework using different word embeddings and hashing methods, we select two datasets, SogouT and TISPTER, for evaluating Chinese and English respectively. Figure 4(a) and (b) give the results in TIPSTER with different hashing method to compact FVs. From the results, we can see that the proposed method with different word embeddings and hashing methods can achieve significantly better performance than previous methods in most of the cases. For TIPSTER, the word embeddings C&amp;W generated by [9] obtain the best performance. The training data used by them is also the biggest among other word embeddings used in this work. Comparing the results shown in Figure 4 (a) and (b), we can observe that the performances of Simhash and Spectral hash are similar with each other when the same word embeddings are used. Figure 4(c) gives the results in SogouT. Because of the highly computational requirements, we evaluate GCNLM, CBOW and Skip-gram for SogouT. From the results, we can see that the proposed methods with different word embeddings and hashing method can always achieve better performance than existing methods.

As described in the previous section, in this work, we also manually annotated another dataset CRD-S to evaluate the semantic level duplicates. Table 3 illustrates the results of different methods in CRD-S. The precision and recall of different methods are shown in the table. The first column in all the tables represent the thresholds of different hamming distances. Comparing to the results in CRD, the performances of existing methods based on lexical similarities drop. However, the performances of the proposed methods are even better than it in CRD. We think that it is due to the golden standards used in CRD, which may contain false positive ones. While, testing instances were manually annotated in CRD-S Since the vectors of short text segments represented by BoW model extremely sparse, the performances of Spectral hash and Simhash are much lower than others. Due to space limited, we cannot list all the results for SogouT in Table 3(f). When the threshold of hamming distance is set to 10, the recall and precision are 96.8% and 19.6% respectively. Among all the methods, the proposed methods achieve the best result in all six collections. At the same recall level, the precisions of the proposed method are 2-10 times higher than E-qSign. It also demonstrates that incorporating the word embeddings can bring benefits for calculating the semantic level similarities.
The following hash codes are generated by CBOW-SpH for the sentences illustrated in the Section 1. The numbers in square brackets are the generated hash codes for the sentences in the right. From the examples we can see that the hamming distance between the hashcodes of S1 and S3 is two. Although there is one word difference between S1 and S2, the hash codes of S1 and S2 are equal. Precision Recall
Time (S) Figure 5: The efficiency comparison of different hashing methods.
 It demonstrates that the proposed method can effectively preseve the semantic level similarities.
Due to the requirement of processing huge amounts of data, efficiency is also an important issue. In this work, we compare the running time of the proposed approach with other hashing learning methods. Although the offline stage of the proposed framework requires massive computation cost, the computational complexity of online stage is small or comparable to other hashing methods. Figure 5 shows the efficiency comparison of different hashing methods. We implement the all methods to run on single thread in the same machine, which contains Xeon quad core CPUs (2.53GHz) and 32GB RAM. All the methods take the sentences as inputs. The processing time is calculated from receiving the inputs to generating hash codes. For processing out-of-sample extension of spectral hashing, we propose to use the Nystrom method [2] to do it. Since the computational cost of generating Fisher vector for different word embeddings are same, we only list the results of word embeddings CBOW-CL. From the results, we can observe that the computational complexity of the proposed method is comparable with and state-of-the hashing methods. The efficiency of CBOW-CL-SimH is even better than spectral hash and semantic hash. It demonstrates that the proposed method is applicable for large scale applications.
In this work, we study the semantic level local text reuse detection problem and introduce an novel framework to efficiently achieve the task. To capture the semantic level similarities between short text segment, we propose to use continuous vector representations of words to represent short text segments. For processing tens of billions of documents, we propose to incorporate the Fisher vectors to aggregate the variable size BoEW to represent text segments. Moreover, we use hashing methods to compress the dense and high dimensional FVs to binary hash codes. Through experiments on six different collections in both Chinese and English, we demonstrate that the proposed method can achieve better performance than state-of-the-art approaches in all collections. Besides that, the efficiency of the proposed method is comparable with most of hashing methods.
The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by 973 Program (2010CB327900), National Natural Sci-ence Foundation of China (61003092,61073069), Shanghai Leading Academic Discipline Project (B114) and  X  X hen Guang X  X roject supported by Shanghai Municipal Education Commission and Shanghai Education Development Founda-tion(11CG05). [1] S. Amari and H. Nagaoka. Methods of information [2] Y. Bengio, O. Delalleau, N. Le Roux, J.-F. Paiement, [3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. [4] A. Z. Broder. On the resemblance and containment of [5] A. Z. Broder. Identifying and filtering near-duplicate [6] M. S. Charikar. Similarity estimation techniques from [7] A. Chowdhury, O. Frieder, D. Grossman, and M. C. [8] S. Clinchant and F. Perronnin. Aggregating [9] R. Collobert, J. Weston, L. Bottou, M. Karlen, [10] G. Hinton and R. Salakhutdinov. Reducing the [11] G. Hinton and R. Salakhutdinov. Discovering binary [12] T. Hofmann. Learning the similarity of documents: [13] E. H. Huang, R. Socher, C. D. Manning, and A. Y. [14] T. Jaakkola, M. Diekhans, and D. Haussler. Using the [15] T. Jaakkola, D. Haussler, et al. Exploiting generative [16] H. J  X  egou, F. Perronnin, M. Douze, J. S  X anchez, [17] J. W. Kim, K. S. Candan, and J. Tatemura. Efficient [18] A. Kolcz, A. Chowdhury, and J. Alspector. Improved [19] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. [20] D. Metzler, Y. Bernstein, W. B. Croft, A. Moffat, and [21] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [23] P. J. Moreno and R. Rifkin. Using the fisher kernel [24] M. Norouzi and D. Fleet. Minimal loss hashing for [25] X. Qiu, Q. Zhang, and X. Huang. Fudannlp: A toolkit [26] R. Salakhutdinov and G. Hinton. Semantic hashing. [27] J. S  X  anchez, F. Perronnin, T. Mensink, and J. Verbeek. [28] J. Seo and W. B. Croft. Local text reuse detection. In [29] A. Si, H. V. Leong, and R. W. Lau. Check: a [30] R. Socher, E. H. Huang, J. Pennin, C. D. Manning, [31] Q. Sun, R. Li, D. Luo, and X. Wu. Text segmentation [32] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs: [33] J. Turian, L. Ratinov, and Y. Bengio. Word [34] E. Varol, F. Can, C. Aykanat, and O. Kaya. Codet: [35] Q. Wang, L. Ruan, Z. Zhang, and L. Si. Learning [36] Y. Weiss, A. Torralba, and R. Fergus. Spectral [37] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught [38] Q. Zhang, Y. Wu, Z. Ding, and X. Huang. Learning [39] Q. Zhang, Y. Zhang, H. Yu, and X. Huang. Efficient
