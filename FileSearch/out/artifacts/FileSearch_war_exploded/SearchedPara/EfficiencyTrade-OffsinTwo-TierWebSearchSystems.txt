 Search engines rely on searching multiple partitioned cor-pora to return results to users in a reasonable amount of time. In this paper we analyze the standard two-tier archi-tecture for Web search with the difference that the corpus to be searched for a given query is predicted in advance. We show that any predictor better than random yields time savings, but this decrease in the processing time yields an increase in the infrastructure cost. We provide an analysis and investigate this trade-off in the context of two differ-ent scenarios on real-world data. We demonstrate that in general the decrease in answer time is justified by a small increase in infrastructure cost.
 H.3.3 [ Information Search and Retrieval ]; H.3.4 [ Systems and Software ] Algorithms, Measurement, Performance, Experimentation Tiered indexing, distributed systems, pre-retrieval predic-tion
Due to the size of the Web and the nature of its growth, the collection of documents that make up the Web must be partitioned in order to be searched efficiently. There are natural partitions of the Web, such as by country, by lan-guage, by common interest (such as shopping, finance or news), by quality or popularity. There are constructed par-titions, such as those based on ranking functions or caching strategies. How to select the best partitions is well-studied  X 
Work done while the author was an Intern at Yahoo! Re-search Copyright 2009 ACM 978-1-60558-483-6/09/07 ... $ 5.00. in the distributed retrieval literature. This paper assumes that such decisions have been made by the system, and does not address how to select the most appropriate collections for a query. Instead we provide an analysis of the trade-off between efficiency and infrastructure cost in distributed retrieval systems.
 Traditionally partitioned corpora are searched in serial. The first tier is searched, and if the results are not satisfac-tory the second tier is searched. For many queries, searching the first tier is sufficient if the corpus indexed by the first tier is appropriate for the query. In those cases, the user in-formation need is satisfied more efficiently because the first tier collection is smaller and often more focused. The sec-ond tier is searched only when the results from the first tier are not satisfactory. For the rest of the queries, the system is slower because first searches the first collection, only to find out that has also to search the second collection. A better solution is to search both systems in parallel, and if the results from the first tier are not sufficient, the results from the second tier are already available. But we can do better.

If we knew before searching which queries were best han-dled by the first tier, and which queries were necessarily served also by the second tier, we could avoid evaluating the results from the second tier entirely when the results should be served by the first tier. When the results cannot be served by the first tier with sufficient confidence, we save time by searching both in parallel.

If we had an oracle predictor, the system would perform at optimal efficiency because it would always predict the right collection. There are two ways the predictor can make an error. If the system predicts the first tier erroneously, then the first and second tiers are searched in serial. If the predictor erroneously predicts the second tier, there is an increased load on the second tier because more queries are served by this tier than necessary and thus there is an increased cost in the infrastructure of the second tier. In this case, the efficiency of the system is not harmed because the first and second tiers are searched in parallel, and as soon as the first tier results are available they can be returned if they are sufficient.

In this paper we formalize the discussion by examining the trade-off between efficiency and infrastructure cost in the two-tier architecture described above. The first compo-nent in the architecture classifies the query according to the collection most likely to satisfy the user information need. It then routes the query to either the first tier, or to both tiers, according to the prediction. If the results from the first tier are sufficient they are returned to the user, otherwise, if the prediction was successful, the results from both collections are merged. If not, we search the second tier, and then we merge the results.

We show that there is a trade-off between efficiency and infrastructure cost, which depends on the accuracy of the predictor. We validate the analysis empirically with two search scenarios. The first scenario creates a subset of the top 30% page-ranked pages from a large Web corpus. The second considers the distributed case, in which a local Web crawl of Spanish language documents comprises the first tier, and the second tier consists of a large English-based Web collection.

The rest of the paper is as follows. In Section 2 we discuss previous work related to the sister task of corpus selection, as well as corpus partitioning. In Section 3 we describe the system architecture in detail and analyze the efficiency-cost trade-off of the system. In Section 4 we present the empirical validation of the analysis, which includes the pre-retrieval prediction. Section 5 presents the results for both the centralized and distributed search scenarios. Section 6 presents a discussion of the system in general as well as the simulation, and finally Section 7 presents the conclusions and future directions.
Although there is a component of our system that per-forms collection selection, this paper is about the trade-off between cost and efficiency. We cover the work on collec-tion selection to provide context for the reader. Our system implements a pre-retrieval predictor that uses features de-scribed in recent literature on query performance prediction, which we employ in a machine-learning framework. We also describe work on corpus partitioning as it relates to the par-titioned system we describe.
Collection selection is the task of ranking an entire col-lection of documents according to its appropriateness to a query, and has been widely studied in the literature since the early 1990s when collections became large enough to merit distributed search [4, 5, 6, 8, 9, 20, 26, 28]. We report a brief overview of approaches here.

The generalized GlOSS system [9] ranks collections based on the number of documents in the database containing a term from the query and the total weight of a term over all documents. The similarity between the query and the database is based on the average weight of the query terms in the database, where the weight of a given term was de-termined by the database term weighting scheme.

The CORI system [4] ranks collections as if they are doc-uments, using the Inquery [3] inference network approach to document retrieval. In ranking collections, each collection is represented by its terms, where the term weight is deter-mined by the document frequency and collection frequency statistics. The collections retrieved by the query are clus-tered by their ranking scores, and the top k collections are selected.

Other early approaches indexed the vocabulary of terms contained in the collection, and ranked the collections as pseudo-documents according to a similarity score based on the term frequency in the collection, and document fre-quency [28]. The work of Yuwono and Lee [26] attempts to determine which terms in a collection distinguish it from other collections, and assign a greater weight to more dis-tinctive terms. D X  X ouza etal. [8] inde xthe first n unique terms from each document in the collection.

More recently, Si etal. [20] propose a language-modeling framework for resource selection. Whereas the GlOSS sys-tem assumes rank scores are comparable across databases, the system proposed by Si etal. assumes the scores may not be comparable. In their system, collections are described with query-based sampling where queries are single terms sampled from a background distribution. For each query, a language model is created from the top four documents. They found that a collection is adequately represented by as few as 300 documents. Collections are ranked as in the lan-guage modeling approach to document retrieval, where the  X  X ocument X  being retrieved is the collection representative.
Collection selection can also be framed in terms of pre-dicting how good the results from a given collection for a set of queries will be, without looking at the results in ad-vance. Work in pre-retrieval predictors of query difficulty has had mixed results, and these systems usually have a lower accuracy compared to post-retrieval mechanisms such as the work of Cronen-Townsend etal. [7] and Yom-Tov et al. [25], because the information available to them is much more general and much more sparse. We do not focus on pre-retrieval prediction, rather we take features reported in the literature. Hauff etal. [11] provide an overview and evaluation of these techniques.
In order to evaluate methods for distributed retrieval, dis-tributed environments were simulated with publicly avail-able data. D X  X ouza etal. [8] investigate three methods of constructing partitioned corpora, using the TREC volumes one and two data from the Associated Press [10]. Their partitions represented a random partitioning, a partition-ing based on the chronological ordering of documents, and a partitioning based on the document authorship to simulate a managed environment, where the collection is partitioned based on some common characteristic.

Craswell etal. [5] use the wt2g collection, which is a subset of the wt10g collection described in this paper. The collec-tion is crawled from 956 document servers, and they main-tain this partitioning for their experiments. In this sense their data is more realistic than other simulations on TREC data.

Web search engines commonly partition their collections, but little is published about it. One exception is Risvik X  X  work on how to use a multi-tier system for scalability [18, 19]. They propose a multi-tier system in which tiers act as filters to allow a query to  X  X all through X  to the next tier, based on the number of hits in a given tier, and the rele-vance score of the results for the query from that tier. This differs from the current work in that we propose determin-ing the partition to search before any results are obtained. Another difference is that Risvik etal. propose an algorithm for determining the tiers in the system, whereas we assume the tiers are given. In that sense our work is more flexible because it acts upon a given set of collections, as they exists now, rather than prescribing a partitioning algorithm. Liu etal. [15] showed experimentally on an extract of the Chinese Web that it is possible to reduce the corpus size by 95% and still retain retrieval performance -more than 90% of queries could be answered with pages from the cleansed corpus. They exploit query-independent features such as PageRank [2] and the number of inlinks to classify each page into a potentialretrievaltargetpage (together those pages form the cleansed corpus) or an ordinarypage (removed). In our system we form a second corpus of Web pages with the highest PageRank score, but whereas they use the PageRank in combination with other features to classify pages, we use it as a filter. Similarly, in their system they propose search-ing only the smaller corpus, where as our system searches both the smaller and the larger corpora.

Yom-Tov etal. [24] test various corpus partitioning schemes of the .GOV collection within the TREC Terabyte setting. Each query is submitted to all partitions and the final result list is merged by weighting each individual result list based on a query prediction algorithm (how well does the parti-tion answer the query). The examined partition schemes are based on document clusters, domains, number of inlinks and document title strings. The latter performed best over-all, however random partitioning ranked second. In their system they use the query prediction algorithm to merge results rather than as a pre-processing step in the corpus se-lection. Furthermore, in their system every corpus is always searched in parallel, rather than deciding in advance which corpora to search.
For the purpose of the analysis, we assume there are two corpora, although in practice there could be many. Figure 1 shows the basic architecture of the system. We assume that corpus A is smaller and better than corpus B , or nearer to the user, and it is much faster to search, therefore by default we always search corpus A . We have a predictor that tells us whether a query must also be served by corpus B before any search results are returned. If the predictor predicts corpus B ,then B is searched in parallel with A .Ifthepre-dictor is wrong, and the results from A were sufficient, the system can stop processing B , and no time is lost, although there is an increased load on system B . If the predictor was correct, both results are merged. If the predictor predicts corpus A , then only corpus A is searched. In this case, if the predictor is wrong, then corpus B will have to be searched in serial. Thus the cost of erroneously predicting A affects the query response time, while the cost of erroneously pre-dicting B affects the load on system B , increasing the cost of the infrastructure of B .
 Let t A be the time it takes to process a query with corpus A ,and t B be the time to process a query with corpus B ,and by our assumption, t B &gt;t A . Let f be the fraction of queries that should go to corpus B . As we discussed above, we have two possible prediction errors. The first kind of error is e
FP , which is the fraction of queries that were incorrectly predicted to go to corpus B (that is, false positives, using B as the positive class). These queries are processed by A and B in parallel. Since t A &lt;t B ,assoonas A returns results, we can abandon B , so these queries take time t A to process 1 . However, now B has an extra load fraction of e FP .The second kind of error is e FN , which is the fraction of queries that were incorrectly predicted to go to corpus A , but had to 1 We could take advantage of the results coming from B , but we would trade a small improvement in answer quality by a longer response time.
Figure 1: A parallel architecture for Web search. be processed in serial by corpus B . Thus the time to serve these queries is t A + t B . These are the fraction of queries that are false negatives. In terms of precision and recall of the predictor, e FP corresponds to 1  X  precision and e FN 1  X  recall .

Our analysis is independent of any ranking function, and we make no assumptions about the relevance of the search results, since we do not assume anything about the infor-mation need of the person doing the search. Therefore, we only assess whether the system returns a sufficient number of documents for a given query. Because of this, we have an oracle that tells whether the predictor was correct, af-ter seeing the results of the search, based on the number of documents returned for a given query.

In a purely serial system, all the query load is first sent to subsystem A . A fraction f of the query load is not answered well by subsystem A and is then sent to subsystem B .We assume that the query broker can merge both result sets in negligible time. Thus, the average answer time for the serial architecture is:
Suppose now that in our parallel system we had a pre-dictor that was 100 percent accurate for some subset of queries. 2 Let g be the fraction of queries also served by corpus B , for which our prediction is perfect. We can sub-tract these queries from f , because we can identify them up-front. In this case, the average answer time will be: T P =(1  X  f ) t A + gt B +( f  X  g )( t A + t B )= T S  X  gt In the best possible case, that is, a perfect predictor, we will have g = f . In this case perfect prediction. This is because we are evaluating the corpus prediction on the existence of a sufficient number of documents returned by a given query. Thus, we can predict perfectly for a given single-term query how many documents will be returned, based on the document frequency of the term in the corpus.
Let us consider now the general case when the predictor is not perfect. The average time in this case is: Notice that the results of the false positives obtained by B , e
FP , are not used, thus they do not affect the average answer time. T P can be expressed as: Clearly, T P  X  T S if e FN &lt;f , which can be achieved easily by a predictor that answers always the same, and the degree to which it is reduced is dependent on the accuracy of the predictor, as reflected in the difference with T min .Then, the time improvement of the parallel system is Let us analyze now the extra cost of the overall system. Let C B be the infrastructure cost of the subsystems that serve corpus B . Recall that corpus A is always queried, so there is no additional cost associated with searching corpus A . We have seen that the overall system is faster, as shown in equations 4 and 5, but there is a cost trade-off because subsystem B receives a larger query load. The additional load is e FP . Hence the new cost for system B , C  X  B is
The extra cost also depends on the cost of system A , C A namely Hence, C P , the cost of the parallel solution, is 1+ X  C times the cost of the serial case, C S . Notice that the extra cost depends only on the false positives, so the goal will be to try to minimize both kind of errors.
 Is this trade-off worth it? The trade-off is worth it, if That is, the serial case is either slower or more expensive relatively to the parallel case. This is equivalent to saying that we can have the same answer time in both cases, but the parallel case will be cheaper, or that we can answer more queries with the same cost. We can solve this inequality by relating answer time and cost. For this, we assume a simple model where the cost of a search engine is proportional to the size of the collection (roughly, the infrastructure needed will grow linearly with inde xsize, which is proportional to the collection size). On the other hand, to improve the search time you need to increase the infrastructure. As answer time should be kept approximately at a constant value, we will assume an inverse linear relation. That is, we will consider that to decrease the answer time by half, we need to dupli-cate the infrastructure. Hence, the cost will be proportional to volume divided by average answer time. That is: where we have defined variables for all the ratios involved, being  X  the collection size ratio. From here we get that R
T =  X /R C . By replacing this relation in equation 5 we obtain f (  X  ( f  X  e FN )  X  e FP ) R C +  X  (( f + e FP )( f  X  e FN This gives a lower bound for R C . However, as R C must be positive, the statement above is trivally true if both terms are positive. This implies Hence, the collection A cannot be too small and the false negatives cannot be too large, and that depends on the false positives. A better analysis can be done using the real time/cost relation in the target system and the result should be used to design how to partition the collections and train the predictor sub-system.
We constructed two simulations of the system. The first simulates the case where a subset of the collection is parti-tioned based on the popularity of pages within the system. This partition was created by computing the PageRank [2] of each document in the collection, and selecting the 30% of documents with the highest PageRank scores. We com-this as the centralized system.

The second set of experiments simulates a distributed en-vironment, where the queries are issued by a user of a local search engine, and the system decides whether to serve local results, or to query a broader Web collection.

The focus of the system is on improving the efficiency, without harming effectiveness. To assess this we used the metric P k ( N ), which computes the precision at rank N in a second list, assuming that a result is  X  X elevant X  if it appeared in the top k of a first ranked list. We assess P k ( N ) where k = { 50 , 100 } and N = { 5 , 10 , 20 , 50 , 100 } , over the ranked lists generated by our two corpora. The intuition behind using this metric is that we would like to assess the overlap in the results produced by the experimental system, with the results produced by the baseline system (which searches the full collection). We assume that the retrieval algorithm returns relevant documents when it has access to the full col-lection. If the results lists are very similar ( P k ( N ) is high), particularly at the top of the ranked list, then reducing the size of the collection does not harm the results shown to the user. If the user experience is the same whether the results Figure 2: The distribution of positive examples (lower curve) compared to the Excite logs as a whole. The x-axis is the rank of the query frequency, and the y-axis is the query frequency. were drawn from the smaller collection or the entire collec-tion, then the system is successful when it searches only the smaller collection.

The centralized system was simulated with the WT10G corpus [22], which is a collection of English web pages and 20,000 queries from the Excite query logs [23]. Table 1 shows the statistics of this corpus. The corpus was indexed using mer [14]. The language modeling approach [17] was chosen for retrieval, specifically language modeling with Dirichlet smoothing [27] (  X  = 2500). Corpus A was constructed from the top 30 percent page-ranked pages, and corpus B is the entire wt10g. The proportion of the vocabulary size in cor-pus A to corpus B is 0 . 37.

Figure 2 shows the distribution of queries in the Excite logs, compared to the distribution of queries that should have been sent to corpus A according to the ground truth. In this figure the x-axis is the rank of the query frequency, so the most frequent query is at 1, the 10th most frequent query is at 10, etc. The y-axis shows the frequency of the query in the Excite logs. Both axes are on a log scale. The plot shows that the distribution of queries served by the smaller corpus resembles the larger distribution.
For the distributed system, we used two collections of doc-uments. For the local collection, we used documents from a and a sample of 22000 user queries, both from the month of February, 2006. In addition, for this set of experiments, in 2007. To simulate the Web, the English collection and the Spanish collection were indexed together.

For the purpose of predicting the corpus to search, we es-tablish the ground truth in the following way. We would like the user experience, given results from corpus A ,to be indistinguishable from the user experience given results from corpora A and B . In the absence of such truth data, we approximate it using the metric P 20 (20) to measure the May 2009. overlap between the top 20 results in two ranked lists, since users rarely look further down the ranked list [23]. We set a threshold on P 20 (20). In practice, such a threshold would be set as a parameter by the search engine based on many fac-tors. The threshold for the centralized system was 0 . 60, and the threshold for the distributed system was 0 . 75. We set the threshold higher for the distributed system because the two corpora are in different languages. The lower threshold for the centralized system reflects that the subcollection A is the top 30% PageRanked pages, and users are less likely to be sensitive to variations in the results when mixing corpora A and B . A threshold of 0 . 75 indicates that three-fourths of the results are identical between the first and second lists in the top 20 results, and thus the results from corpus A are sufficiently similar to corpus B .
As shown in Section 3, the cost of the system is dependent on the accuracy of the pre-retrieval prediction algorithm. For the predictor, we learned an SVM with 13 features com-puted over corpora A and B . We used the implementation of  X  = 2 for the centralized system, and  X  = . 005 for the dis-tributed system. We consider the positive examples to be queries that should be processed by corpus B . In the cen-tralized data, the positive examples outnumber the negative examples by a ratio of four to one, so to compensate for this we set a cost on the misclassification of negative examples for that data. In the data for the distributed example, the classes were roughly evenly split, with 46% from the positive class.
Pre-retrieval predictors do not rely on calculations of the top retrieved documents, but instead consider the query and the collection as a whole. The advantage of this type of pre-dictor is that there is no retrieval step necessary to determine if a query is good or bad (given the collection). However, since the prediction is retrieval independent, the influence of the retrieval algorithm is ignored. In addition to the features described in He etal. [12] we included four features that take into account the relationship between query terms: Aver-aged Pointwise Mutual Information (PMI), Maximum PMI, Averaged Chi-Square Statistic (CSS) and Maximum CSS. As the features are calculated over pairs of query terms, their value is 0 in case of a single term query.

In order to determine which features would be better dis-criminators, we analyzed the correlation with the corpus WT10G. Table 2 lists the linear correlation coefficients with respect to average precision of the TREC9 and TREC10 ad hoc tasks as well as the TREC10 entry page (ep) task. Re-trieval was performed with the language modeling approach to information retrieval [17]. Dirichlet smoothing [27] was applied and the best performing parameter setting in terms of mean average precision was used. The queries of the TREC 9 and 10 tasks were taken from query logs of a Web search engine, and therefore the results for TREC 9 and 10 are most likely to be indicative of the results of our system. Although none of the features correlates strongly by itself, together they produce classifiers that are significantly better than random, for both data sets. Feature TREC9 TREC10 TREC10 ep Averaged TF -0.1806 -0.0123 -0.0173 TF Deviation -0.0396 0.2248 -0.0203 Averaged IDF 0.1425 0.3259 0.0266 IDF Deviation 0.0665 0.2430 0.1586 Simplified Clarity 0.0813 0.3064 0.0491 Score Query Scope 0.0845 0.1253 -0.0817 Max. Query Scope 0.2058 0.3225 0.3016 Av. Doc. Length 0.0565 -0.0899 0.1687 Av. PMI 0.2914 0.1037 0.0526 Max. PMI 0.2606 0.3044 0.1394 Av.  X  2 -0.0921 0.0556 -0.0291 Max.  X  2 -0.0685 0.0799 0.0127 Table 2: Linear correlation coefficients of the search independent predictors with respect to the best per-forming retrieval run.
The system can assume that the results post-retrieval can be predicted with 100% accuracy. Such a predictor would have to be very fast, and for this reason, none of the post-retrieval algorithms described in the previous work are suit-able. Furthermore, the post-retrieval algorithms assume some knowledge of the user information need, and assess the results based on their relevance to the query. Our system makes no such assumption.

In practice a search engine would deem the results from a corpus satisfactory if there were enough documents re-turned. For example, a system returning 50,000 documents likely has produced enough results to satisfy the user. Search engines do not publicize their thresholds, and our system is so small by comparison to the Web that we have no way to set such a threshold. Since we are assessing the system in terms of the cost of searching in parallel, with the assump-tion that searching the entire document space gives the op-timal results, we use the ground truth data as an  X  X racle X  that tells when to re-submit the queries to corpus B .Inthe following section we report retrieval results both before and after the oracle decision. The cost analysis is done after the oracle decision. We are free to use the ground truth data for this purpose because we do not assess the post-retrieval evaluation mechanism.
In this section we present the classification results, as well as the infrastructure cost for two search scenarios, using real-world data.
For the centralized system, since the positive examples outnumbered the negative examples a cost on mispredict-ing the positive examples was set at 2 empirically with a held-out set. Table 3 shows the classification results. The better the pre-retrieval predictor, the more efficient the sys-tem is, but any predictor that is better than random will improve the system. The random prediction corresponds to the number of examples in the majority class. In the case of the centralized system, the majority of queries should be sent to both corpora. The results are statistically significant using a t-test at the p&lt;. 01 level, using 10-fold cross vali-dation, where the training set was 90% of the data, and the test set was 10% of the data.
 Table 3: Centralized system classification accuracy for pre-retrieval predictors. The Accuracy is statis-tically significantly better than the random baseline, using a t-test at the p&lt; 0 . 01 level. Table 4: Retrieval results for the centralized system.
For each query, the pre-retrieval classification determined whether the query was served by collection A or collections A and B. In either case, the results from collection A were verified against an oracle, and if they were sufficient, the results from A were presented. Otherwise the results from B were presented. Table 4 shows the final results of the system, in terms of P k ( N ) where k = { 50 , 100 } and N = { 5 , 10 , 20 , 50 , 100 } . The results at the top of the ranked list are indistinguishable for both systems, which is exactly what we would want in a real system. Further down in the ranked list, the results vary to a greater degree from query to query. The results below rank 50 no longer overlap between the two systems.

In this case we have f =0 . 714, e FP =0 . 210 and e FN = 0 . 001. Then, the cost of the centralized system is: This implies that if for example the time to process a query in system B is twice the time to process a query in system A , and because B is a larger system, carries twice the cost, then the system is 29% faster, but at the same time 20% more expensive due to the load on B . Replacing these values in the conditions of equation 6, we have that the trade-off is also true.
The distributed system had approximately ten times the number of documents that the centralized system had (Ta-ble 1). The vocabulary for the distributed system was ap-Table 5: Distributed system classification accuracy for pre-retrieval predictors. The Accuracy is statis-tically significantly better than the random baseline, using a t-test at the p&lt; 0 . 01 level. Table 6: Retrieval results for the distributed system. proximately four times the size of the centralized system. However, in spite of the fact that the distributed system in-cludes documents in two languages, the proportion of the vocabulary of corpus A to corpus B in the distributed sys-tem is only 0.41. The similarity in proportion to the cen-tralized system is somewhat surprising as you would expect a system representing two languages to have roughly twice the vocabulary size. It is important to remember that not all documents and queries in the Spanish collection are in Spanish.

For the distributed system, the positive examples made up nearly 50% of the data, so there was no need to adjust the cost of misclassification. The classification results are given in Table 5. The results are statistically significant using a t-test at the p&lt;. 01 level, using 10-fold cross validation, where the training set was 90% of the data, and the test set was 10% of the data.

The retrieval performance for the distributed system is shown in Table 6. As with the centralized case, the top of the ranked list is indistinguishable between corpus A and corpus B, and the results after rank 50 do not degrade as quickly for the distributed system as for the centralized. This is an artifact of the number of positive examples in the data, how-ever as the queries for the distributed system were sampled randomly from a real-world query log, this case represents a more realistic view of the system. The load on the infras-tructure that handles corpus B is higher, but in general the system performs more efficiently. This is not surprising as the pre-retrieval classifier for the distributed system is more accurate. In this case we have f =0 . 539, e FP =0 . 004 and e FN =0 . 220, so: Using the same example as in the centralized system ( t A t / 2and C B =2 C A ), the distributed system is 15% faster with just 0.5% additional cost. However, a more realistic answer time relation, as we are contacting a remote system, would be t A = t B / 5. In that case the time improvement is only 9%, but still with a negligible cost increase. Replacing these values in the conditions of equation 6, we have that the trade-off is worth it if  X &gt; 0 . 013 (it is 0 . 436) and e 0 . 532, which is also true.
The centralized system and the distributed system have completely different characteristics, that play out in interest-ing ways in terms of how the classification accuracy affects the system performance. For example, in the centralized system, the documents in corpus A resemble the documents in corpus B in terms of their vocabulary and term distribu-tions. In the distributed system the documents in corpus A are mostly in a different language than the documents in corpus B. This affects not only the term distribution, but also the classification of a query as suitable for one corpus over another. For example, features like the inverse doc-ument frequency, the term frequency, the simplified clarity score, PMI and  X  2 are good discriminators if the vocabulary of the query overlaps in one corpus and does not overlap in the other.

In our data, in the distributed system, the positive and negative examples were roughly equally distributed. This made the classification problem simpler in the sense that the system had enough positive examples to learn from, but the prior probability of one class did not dominate the clas-sification. In the centralized system there was a significant imbalance in the classes, and the prior probability of the dominant class (the negative examples) combined with the poor discriminative power of the features, reduced the accu-racy of the classifier.

The bias in the ground truth data in the centralized sys-tem was introduced by the way the corpus was partitioned. Partitioning the corpus according the PageRank ranking, or some other property of the data seems a reasonable thing to do. As the system we are proposing is based on a new query processing strategy, how to partition the corpus effec-tively to improve the performance of the predictor is an open problem. In the distributed case the partition was given by the nature of the Web. The fact that the system performed better on the real-world data than on the simulated data is encouraging.

Our evaluation metric is designed to measure the overlap between the system representing the current state-of-the-art, and the proposed system. The idea is that the classi-fication accuracy affects the final results, but hopefully not so much so that the user experience is degraded. In our data, several assumptions were made that affected the final results. Since we assume that after the results are retrieved from corpus A we can verify their quality with 100% accu-racy, we used the ground truth data as an oracle. In practice such an assessor might be something very simple and fast, such as the results from corpus A are sufficient if there are at least 1000 results. Nevertheless, further research on post-evaluation of results is needed.

The evaluation metric does not tell us whether the results were relevant or not, and thus whether the system as a whole is returning good documents or bad documents, or whether the quality of the results are degraded slightly by the new documents injected into the list by the system. It is possi-ble that corpus A could be constructed to serve queries on specific topics, and thus the documents returned by A that were not returned by B are more relevant.

As stated in Section 3, the efficiency of the system is de-pendent upon the classification accuracy. As the results of the simulation show, even modest achievements in pre-retrieval classification are sufficient to improve the efficiency of the system. The features used in the classification can be calculated off-line, and thus do not affect the performance of the system as a whole. Once the classifier has been learned, the time to classify the query is negligible.
Pre-retrieval query performance predictors have been stud-ied, and have had mixed results, however most predictors have been compared against post-retrieval predictors used for other purposes such as query expansion, or spelling cor-rection. In our system we assume that we have a post-retrieval quality assessor that is 100% accurate, but this assumption may be studied in terms of a relevance-based quality assessment of the results.

How to partition the corpus based on quality is an open problem. In the distributed case, the partition is given as a natural artifact of the problem. Many such partitions exist on the Web, even within one search engine, some of them representing collections in the same language. Neverthe-less, the partitioning problem is orthogonal to the prediction problem, as the latter depends on the partition needed.
Our analysis can naturally be extended to multi-tier col-lections by using the same idea in a cascading fashion. That is, if the query is sent to the first two tiers, a second predic-tor decides if the query needs also to be send to the third tier and so on. As an alternative, if a sufficiently accurate multi-class predictor can be designed, the system can choose the appropriate partition from among many.

Our work is also related to inde xcaching and pruning techniques [1, 16, 21]. In fact, these techniques can be con-sidered a case of tiering the inde xinstead of the document collection. Hence, our results can be modified by ad-hoc caching schemes that retain the most frequent cases of false negatives and false positives, diminishing then the average answer time as well as the extra cost.
