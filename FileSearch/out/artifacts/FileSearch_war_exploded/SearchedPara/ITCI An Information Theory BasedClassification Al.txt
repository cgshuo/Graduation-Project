 Classification algorithms, as an important aspect of data mining, has been widely studied and applied to many fields. However, previous studies often assumed the available data is complete, thus did not take the missing values into account, but in real life, however, incomplete data is ubiquitous[1], for example, in an industrial test, part of the data may be lost because of mechanical or electronic failure; in medical field, doctors may not get all the required data due to lack of equipment or patients X  physical conditi on; in a social survey, some respondents may refuse to provide part of information; for the lack of permission, database query can not get all the data needed et c. Thus, it is of great theoretical and practical importance to study how to classify directly with incomplete data.
The missing(incomplete) data mechanism can be divided into the follow-ing three groups[2]: missing completely at random(MCAR), missing at ran-dom(MAR), not missing at random(NMAR). MCAR occurs when the missing of a variable is independent of itself and any other external influences; the miss-ingness of MAR is independent of the missing variables but traceable from other variables; NMAR happens when patterns of missingness is non-random and de-pends on the missing variables, which is the most common situation in real life.
Currently, the methods to deal with incomplete data in classification falls mainly into three aspects. (1) Deleting the incomplete instances[3], this method is simple, but it will lose the useful information contained in incomplete data. (2) Using statistical and machine learning methods to fill in values most likely to be[4][5][6], such as filling manually, filling with mean or median, regression filling, KNN filling, filling based on neural networks etc. In general, filling man-ually will brings small bias, but it is not feasible given a large dataset with many missing values; filling by mean or median does not fully reflect the data variability and ignores the association between attributes; regression filling as-sumes a regression relationship exists among complete items and missing items, which is often incorrect in practice; KNN filling needs to define a reasonable similarity measure and has a relatively high computational complexity; filling based on neural networks requires desig ning appropriate network architecture for specific missing modes and it is too complex and cumbersome to apply. (3) Training and classifying with incomplete data directly, such as those methods based on EM[7], decision tree[8], fuzzy C-means[9], support vector machines[10], Bayesian networks[11] and the nearest neighbor[12]. Those EM-based methods require that the probability density function and missing attributes must be given, besides, they are often complex to train and converge slowly; the ID3-based approach treats the missing values as a special one different from known ones, which does not fit the real world well and it is difficult to get optimum due to the lack of a global search; fuzzy C-means and support vector machine based methods need assumptions of missing data X  X  distribution, which is often not available in practice, thus the application is limited; Bayesian networks based methods require domain knowledge and dependencies among variables must be known, otherwise, complex network stru cture will be produced, what X  X  more, the network nodes will increase exponentially with the growth of variables, which will result in high maintenance cost; as for nearest neighbor based method, when data X  X  dimension is high, the sample space will still appears to be sparse even the dataset is large and applying the method directly will result in poor performance. Among the methods to classify directly with incomplete data, [13] found that Naive Bayes methods are most insensitive to missingness, but they rely on apri-ori probability density to make classification inferences, which results in a low accurate. [14] proposed a method named RBC, which estimates incomplete data by intervals. In this method, missingness mechanism is not required to meet MAR assumption because all possibilities of the incomplete values are consid-ered. Though it has better classification a ccuracy, the calculation is relatively complicated. [15] proposed the NCC2 method, which has higher classification accuracy, but it requires the missing ness mechanism is declared and assumes each attribute contributes to classification independently, however, when the as-sumption is not met, classification accura cy decreases sharply. Other studies for classification with incomplete data include rough set based methods[16][17], such methods don X  X  require any assumptions of missingness mechanism, but they are inefficient and have poor scalability.

In this paper, an information theory based classification algorithm, ITCI, is proposed for incomplete nominal data. The basic idea of the algorithm is as follows. At first, an initial uncertainty is calculated for each class, then an instance X  X  attributes are inspected one by one to reduce class uncertainty. When all attributes are used, the instance is assigned to the class whose uncertainty is minimum. During the training stage, ITCI estimates the initial uncertainty with the help of the incomplete records, meanwhile, it calculates attributes X  attribution to decrea se uncertainty and for missing attributes it gets expected contribution. In the classification stage, expected contribution is used to estimate the decrease of uncertainty for missing attributes. With these measures, ITCI need not to estimate missing values explicitly, at the same time, it makes full use of the information contained in incomplete instances. Extended experiments show that the accuracy and stability of the proposed method are significantly higher than RBC and NCC2, and the time complexity is comparable low.
The rest of the paper is organized as follows: Section 2 gives the related concepts of information theory, their properties and application in classification. Section 3 gives an information theory based classification algorithm, ITC, for complete data. Sect ion 4 extends the methods pres ented in section 3 to get an algorithm, ITCI, for incomplete data. Sect ion 5 presents the res ults and analysis of experiments; Section 6 is the conclusion. 2.1 Basic Concepts of Information Theory Definition 1. ( self-information ) The self-information of a random event is de-fined as the negative logarithm of the event X  X  probability, namely, if the probability of event x i is p ( x i ) , then its self-information is defined as: Definition 2. ( conditional self-information ) For any events x i and y j in a join set XY , the conditional self-information of x i given y j is defined as: Definition 3. ( mutual information ) For sets X and Y of discrete random events, the information x i acquired given y j is called mutual information, which is defined as: We can get from formula (3) that I ( x i ; y j )= log 1 p ( x
Formula(4) implies that mutual information equals the result of subtracting conditional self-information from self-information, or in another way, mutual information is a measurement of decrease d uncertainty, namely, mutual infor-mation equals the resul t of prior uncertainty log 1 p ( x (1) Reciprocity: (2) When event x i and event y j are mutual independent, the mutual informa-tion is zero, namely, I ( x i ; y j )=0. (3) Mutual information may be positive or negative. When the value is posi-tive, it means the appearance of event y j will certainly contribute to the appear-ance of event x i , on the contrary, it is disadvantageous. (4) The mutual information between two events may not exceed the self-information of either one.
 Definition 4. ( conditional mutual information ) The conditional mutual infor-mation of x i and y j given z k in join set XYZ is defined as: The mutual information of x i and y j z k is defined as:
Formula(10) implies that given the appearance of a pair of events y j z k ,the information x i will get is I ( x i ; y j z k ), which equals the information x i get from the appearance of y j , add the information x i get from z k when y j is known.
The above four definitions are based on single event, similarly, they can be extended to event sets. We leave them out due to the limitation of the space. 2.2 Use Information Theory to Solve Classification Problems In classification problems, an instan ce X  X  feature can be represented by a n-dimensional vector x = { x 1 ,x 2 ,x 3 ,...,x n } . The classification task is to assign a label in label set C = { C 1 ,C 2 ,...,C K } to each instance. Usually, the task in-cludes two stages: classifier X  X  training and testing. Considering the testing stage, for an instance x , we may assign any of the K labels to it and have some degree of uncertainty at the same time. The self-information of classes, I ( c k ), can be used to measure these initial uncertainty. Then attributes are taken into consid-eration one by one, meanwhile, the uncertainty of the classes will change with the adding of attributes. When all attributes are considered, we can get the fi-nal uncertainty of each class, namely I ( c k | x 1 x 2 ,...,x n ), then the instance x is assigned to the class whose uncertainty is minimum. As for the training stage, we estimate self-information and conditional mutual information with the help of training instances and they will be used as arguments of the final classifier. Assume the input space X X  R n is a n-dimensional vector set and the output space is class label set Y = { c 1 ,c 2 ,...,c K } . For each instance, classification algorithms take x  X  X  as input and get y  X  X  as output. The training set is be a random vector defined in input space X , Y be a random variable defined in output space Y . ITC builds a classifier by learning the self-information I ( c k )of each class and the mutual information I ( c k ; x )( k =1 , 2 ,...,K ) between class c k and feature vector x .

Considering the estimation of I ( c k ), we need to get the probability P ( c k )of class c k , which can be estimated by the following formula:
For I ( c k ; x ), when x  X  X  dimension is 1, we can get the value following formula(3) probability estimation. But when the dimension continues to grow, the number of parameters will increase exponentially, which means it is not feasible to estimate all of them efficiently. Let the number of different values for attribute x i is p i , i =1 , 2 ,...,n , the number of possible values of Y is K , then the total count of arguments is K n i =1 p i . Therefore, we take some approximation measures to simplify the estimation described above.
 Denote the mutual information between c k and feature vector x as I ( c k ; x ):
Formula(12) implies that when the feature vector x is known, the decreased uncertainty I ( c k ; x )equalsthesumof I ( c k ; x 1 ,x 2 ,...,x n  X  1 ), which measures the decreased uncertain ty get from the former n  X  1 dimensions, and the con-creased uncertainty get from x n when the former n  X  1 dimensions are given. Using formula(12) recursively, we can get: we can see the arguments also increased e xponentially, here, we simplify it as follows:
Formula (14) implies that when the former i  X  1 dimensions are given, the de-creased uncertainty we get from x i is approximated by the d ecreased uncertainty we get from x i when x i  X  1 is given.

Theoretically, if the mutual information is estimated according to formula(13), the results will be sole no matter in which order the attributes are considered, however, they will differ from each other if estimated following formula(14). In optimal order to make the expectation of I ( c k ; x ) minimum, which will enable thebiasaslowaspossible.Let | x 1 | , | x 2 | , ... , | x n | be the number of different values of x 1 ,x 2 ,...,x n , among which we denote the maximum one as x max ,so the complexity of enumeration and estimation is O ( Kn ! x 2 max n ) and it will be O ( NKn ! x 2 max n ) if we estimate the expectation of I ( c k ; x ) additionally. When the feature vector X  X  dimension is high, it is not hard to see that the calculation is costly, or even impossible, thus we pr oposed a heuristic attribute order. Definition 5. ( expected mutual information ) The expected mutual information between x i ( whose value can take any one of x i 1 ,x i 2 ,...,x ip ) and class c k is defined as: Definition 6. (  X  2 of attribute pair ) Assume x i and x j cantakeanyvaluefrom x then we define the  X  2 value of attribute pair x i and x j as:
In the formula above, p rs denotes the expected joint probability of x ir and x js when they independent of each other, which can be estimated by p rs =
For a set of attributes, A = { x 1 ,x 2 ,...,x n } , we give a heuristic algorithm to find the optimal order S for class c k as follows:
ATT ORDER select the attribute with maximum mutual information as the first one for it can decrease th e uncertainty largely. In the following process, it selects the attribute which has the largest  X  2 value with the last selected one, in choices, so the total approximation error is small.
 Algorithm 1. ATT ORDER
On the basic of ATT ORDER, we can get an optimal attribute order from the training data. Here, an algorithm named ITC is given for complete data as follows. The algorithm is made up of two parts, ITC LEARN, which is used for learning model arguments, and ITC TEST, which is used for applying the learnt model to classify instances with unknown labels.
 Algorithm 2. ITC LEARN For incomplete data, we assume the missing mechanism to be MAR. The missing items can be any of the attributes of X or class label Y .Inthesameway,one or more attributes can be missing from feature vector X in the testing set.
We can get the algorithm, ITCI, based onITCproposedinsection3.Themain improvements include two parts: the estimation of statistic used to calculate model arguments; the estimation of decreased uncertainty of missing attribute. Once the estimations are acquired, I TCI can be trained and tested in the same way as ITC. Algorithm 3. ITC TEST 4.1 Estimations of Statistic It can be seen from formulas (1)(3)and(8) that the key point of estimating model quencies if we use frequency to approximate probability. The main estimation
Let the non-empty values of x i to be x i 1 , x i 2 , ... , x ip and the empty value to the estimation formula is as follows:
Let the non-empty values of x j to be x j 1 , x j 2 , ... , x jq and the empty value to be x jq +1 . Denote the frequency of instances whose value is x ir on attribute x i formula to assign in proportion is: to be the sets get from partitioning G by class label. All the instances whose class label is missing are assigned to T cK +1 . Denote the number of instances in T
The estimation of f ( c k x ir ) is related to T ck and T cK +1 . We treat all of the instances X  class label of T cK +1 as c k and assign a weight as follows:
The frequency estimation g ck ( c k x ir )of T ck can be estimated following formula (17), and the frequency of T cK +1 can be estimated as follows:
The subscripts c k , c K +1 indicate that the frequency is estimated based on the dataset T ck , T cK +1 . Formula (21) implies that we get the frequency of instances whose class label is missing based on the proportion of complete instances. Com-bining(20)(21), we can get the final estimation of g ( c k x ir ) like:
In the same way, the estimation of f ( c k x ir x js ) also contains two parts, we can get g ck ( c k x ir x js ) following formula(18) from dataset T ck and get the weight w k following formula(20), and then get the estimation g cK +1 ( c K +1 x ir x js )from dataset T cK +1 in a similar way as formula(18), but the weight is acquired from complete instances. Finally, g ( c K +1 x ir x js ) can be estimated like: 4.2 The Arguments Estimation for Missing Attributes Due to the existence of missing attribute, the arguments we need to estimate also we use the expected mutual information of all known attribute pairs as the estimations.
 section 4.1 and formula(1)(3)(8). The conditional probabilities p ( x ir | c k )canbe estimated by the frequency of the instances whose value is x ir on attribute x i in class c k . Other conditional probabilities can be estimated in a same way. In order to evaluate the effectiveness o f the algorithm, we did experiments on 12 datasets.The datasets are downloaded from the UC Irvine Machine Learn-ing Repository and their basic information is given in table 1. Notice that all datasets are with missing values, among them, those whose names ended with 5 are acquired by randomly deleting 5% attr ibutes from complete instances, oth-ers are incomplete initially. For continuous attributes, we discrete them in the preprocessing stage. All algorithms are implemented with java and run on a PC with 2.2GHZ cpu, 2GB memory and the operating system is Windows XP.

Table 2 shows the accuracy and standard deviation of the classifiers. In our experiments, the proposed algorithm ITCI and two classical classification algo-rithms dealing with incomplete data named RBC, NCC2 are compared. All the results are got from 10 times 10 fold cross-validation. (1) We can see that the only dataset on which the accuracy of ITCI is lower than RBC is balance 5 and the difference is 2.90%. While on other 11 datasets, the accuracy is significantly hi gher, especially on vehicle 5 which exceeded more than 13.09%. Comparing ITCI and NCC2, we find that ITCI is lower on datasets credit and balance 5, while outstands significantly on the left 10. Especially on vehicle 5, it increased by 10.03%. For dataset balance 5, careful analysis found that the number of the three classes take proportions of 46.08%, 7.84%, 46.08% respectively. As the proportion of class 2 is too small, the initial uncertainty is relatively high, what X  X  more, the number of attributes is too small to decrease uncertainty during the classification process, so instances of class 2 are easily mis-classified and then the final accuracy is aff ected. In fact, attr ibutes of balance 5 are numerical and the class label is dete rmined by the difference of the product of the first two attributes and the product of the last two attributes.While in ITCI, we assumed the attribute is nominal, so it is this inconsistency led to a low classification accuracy. (2) By comparing the standard deviation of ITCI, RBC and NCC2,we find that ITCI is much better except on datasets nursery 5 and balance 5. By An-alyzing dataset nursery 5, we find it also has the problem of imbalance classes, which leads to a large deviation of the initial uncertainty, thus the standard deviation is relatively high. But on the left 10 datasets, the standard deviation is small than or equal to the latter two.
 (3) By comparing the total time consumption(Details are not presented due to space limitation), we find NCC2 has the highest efficiency, RBC has the middle and ITCI has the lowest. But we also notice the total running time of ITCI for 10 times 10 fold cross-validation is 19.547s on nursery 5 which has 12960 instances. On average, an experiment takes only 0.195s, which implies the efficiency is still relatively high. In fact, ITCI can get all the arguments needed by scanning datasets only once, that means the complexity is not high at all.

Combining the above three comparison, we can draw the conclusion that the proposed algorithm, ITCI, is more accurate and stable than RBC and NCC2. Although the efficiency of ITCI is lower than the latter two, the running time and complexity is still relatively low, so it is useful in practice. In the paper, an information theory based classification algorithm for incomplete data, ITCI, was proposed. ITCI treats cl assification as a pr ocess of decreasing uncertainty, it calculates classes X  initial uncertainty at first, then attributes are inspected one by one to decrease the uncer tainty, and then an instance is assigned to the class whose uncertainty is minimum. In the training stage, ITCI weights frequencies by proportion, which makes full use of the information contained in incomplete instances. What X  X  more, ITCI estimates the decr eased uncertainty of missing attributes by expected mutual information. Experiments show that ITCI is more accurate and stable than existing ones and the time complexity is low, thus it is considered to be simple and practical. Our future work is to study classification with incomplete data for continuous attributes.

