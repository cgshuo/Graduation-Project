 In the field of data mining, ensemble methods have been proven to be very ef-fective for not only improving the classification accuracies but also in reducing the bias and variance of the estimated classifier. We choose to demonstrate our multi-resolution based framework using  X  boosting  X  algorithm, which is a stan-dard additive modeling algorithm popular in data mining and machine learning domains. The Boosting meta-algorithm is an efficient, simple, and easy to ma-nipulate additive modeling technique that can use potentially any weak learner available [8]. The most popular variant of boosting, namely the AdaBoost (Adap-tive Boosting) in combination with trees has been described as the  X  X est off-the-shelf classifier in the world X  [3]. In simple terms, boosting algorithms combine weak learning models that are slightly better than random models. Recently, several researchers in other domains like computer vision, medical imaging have started using boosting algorithms extensively for real-time applications. Both classification and regression based boo sting algorithms have been successfully used in a wide variety of applications in the fields of computer vision [12], in-formation retrieval [11], b ioinformatics [9] etc. Insp ite of their great success, boosting algorithms still suffer from a few open-ended issues such as the choice of the parameters for the weak learner. The framework proposed in this paper is more generally termed as  X  Multi-resolution Boosting  X , which can model any ar-bitrary function using the boosting methodology at different resolutions of either the model or the data. Here, we propose a novel boosting model that can take advantage of using the weak learners at multiple resolutions. This method of handling different resolutions and building effective models is similar to wavelet decomposition methods for multi-resolution signal analysis. In this work, we achieve this multi-resolution concept in the context of b oosting algorithms by one of the following two ways:  X  Model-driven multi-resolution: This is achieved by varying the complexity  X  Data-driven multi-resolution: This can be achieved by considering the data The main idea of the proposed framework is: the use of Multi-resolution data (or model) driven fitting in the context of additive modeling using concepts that are similar to wavelet decomposition techniques . The rest of the paper is orga-nized as follows: Section 2 gives some relevant background on various boosting techniques and scale-space kernels. S ection 3 shows the problem formulation in detail and discusses the concepts n ecessary to comprehend our algorithm. Section 4 describes both the model-driven and the data-driven multi-resolution boosting frameworks. Section 5 gives the experimental results of the proposed methods on real-world datasets and Section 6 concludes our discussion. Ensemble learning [4] is one of the most powerful modeling techniques that was found to be effective in a wide variety of a pplications in recen t years. different ensemble techniques have been proposed in the literature and is still a very ac-tive area of research. Boosting is one of the most widely used algorithm that has caught the attention of several researchers working in the areas of pattern recog-nition and machine learning [5]. A main advantage of boosting algorithms is that the weak learner can be a black-box which can deliver only the result in terms of accuracy and can potentially be any weak l earner. This is a very desirable prop-erty of the boosting algorithms that can be applied in several applications for predictive modeling [8,6]. The additive model provides a reasonable flexibility in choosing the optimal weak learners for a desired task. In this paper, we propose a novel multi-resolution framework for choosing optimal weak learners during the iterations in boosting. This approach allows for effective modeling of the dataset at any given resolution [10]. In terms of analyzing (or modeling) a given dataset at different resolutions, our approach closely resembles wavelet decomposition techniques which are effective tools in the field of multi-resolution signal analy-sis [7]. In the model-driven multi-resolution boosting framework, the models are built by increasing the complexity during the boosting process. The data-driven multi-resolution, on the other hand, considers the data at different resolutions decomposition techniques which are effectiv e tools in the field of multi-resolution signal analysis. The main advantages of using this multiple resolution framework in the context of boosting are that they:  X  allow systematic hierarchical modeling of the final target model.  X  provide more flexibility by allowing the user to stop at a reasonable resolution  X  require very few pre-de fined user parameters.  X  avoid the use of strong learners in the beginning stages of modeling and Let us consider N i.i.d. training samples D =( X , Y ) consisting of samples (
X , Y )=( x the case of binary classification problems, we have y i  X  X  X  1 , +1 } and for regres-sion problems, y i takes any arbitrary real value. In other words, the univariate response Y is continuous for regression problems and discrete for classification problems. Now, we will discuss boosting algorithms applied to general classifi-cation problems. We choose to demonstrate the power of scale-space kernels in the context of Logitboost algorithm because of its popularity and its power of demonstrating the additive modeling nature.

Each boosting iteration performs the following three steps: (1) Computes response and weights for every datapoint. (2) Fits a weak learner to the weighted training samples and (3) Computes the error and updates the final model. In this way, the final model obtained by boosting algorithm is a linear combination of several weak learning models.

In the case of classification problems, the penalty function induced by the error estimation is given by: where I denotes an indicator function which returns value 0, when y i = F ( t ) ( x i ) and 1 otherwise. In other words, the penalty term is 1 if the i th sample is mis-classified and 0 if it is correctly classified. Whether it is a classification or a regression problem, the main challenges in the boosting framework are the fol-lowing: (i) The choice of the weak learner and (ii) The complexity of the weak learner. While choosing a weak learner model can be a complicated task in itself, tuning the right complexity for such a weak learner might be even more chal-lenging. The multi-resolution framework proposed in this paper addresses the second issue.

The boosting framework discussed abo ve works for classification problems and can be easily adapted to solve regression problems. In the case of regression problems, the penalty function is given by: where  X  p indicates the L p norm. We will consider p = 2 (namely, the Eu-clidean norm) in this paper. We formulate this multi-resolution boosting using the standard boosting algorithm with exponential L 2 norm loss function and demonstrate empirical results on classification problems. In our previous work [10], we have demonstrated the use of scale-space kernels in the data-driven boosting framework on several regression datasets.
 Algorithm 1. Model-driven Multi-Resolution Boosting We will now describe both the model-driven and data-driven multi-resolution boosting algorithms. To demonstrate a reasonably wide applicability of the multi-resolution framework, we implement our framework using both the ad-aboost and logitboost algorithms. We show the model-driven multi-resolution algorithm using the adaboost framework for classification problems and the data-driven multi-resolution algorithm using the logitboost framework for regression problems. Though, we chose to demonstrate in this setting, the proposed frame-work is generic and can be applied to other additive modeling techniques used for solving classification and regression problems. 4.1 Model-Driven Multi-resolution Boosting In the model driven boosting framework, the complexity of weak learner is mod-ified as the boosting iterations progress. Changing the complexity of the weak model can be done in a very intuitive manner depending on the choice of the weak learner. For example, if decision trees are used as a weak learner, the res-olution can be changed by changing the number of levels in the decision tree that is being considered. The initial boosting iterations use trees with only one level (or decision stumps) and later on the resolution can be increased by in-creasing the tree-depth. One has to note that the complexity of the modeling (or classification boundary) is significantly increased by changing the resolution. Algorithm 1 describes our model-driven m ulti-resolution boosting framework using the adaboost algorithm for a binary classification problem. The weight vector W is initialized to 1 /N (uniform). The main algorithm runs for a prede-fined number (T) of iterations. The procedure Train will obtain weak learner (and the corresponding training error) using the weights W ( t ) .Thenumberof splits ( nsplits ) is a parameter that determines the complexity of the model i.e. the more the number of splits in the weak learner, the more the complexity of the model. It is initialized to one at the beginning. As the iterations progress, the complexity of the weak learner is either retained or incremented depending upon the training error.

For every iteration, the training error of the current model is compared with the error of a slightly complex model (with nsplits + 2 nodes in the tree). If this new model performs well, then the complexity of the current model is increased ( nsplits = nsplits + 2) and the re-weighting of the data points is computed using this new model. The weights are normalized (so that they sum to one) in every iteration. One can see that the algorithm appears to be working in a similar manner to the traditional Adaboost, except for the fact that the choice of the weak learner is made more systematically from simple to complex and is not chosen arbitrarily as done in the standard boosting procedure. In this manner, the algorithm increases the comp lexity of the weak learners chosen and the final weighted combinations of the selected weak learners are used as the final trained model. Hence, the model will have a very simple classification boundary in the initial stages and the boundary becomes more and more complex as the iterations proceed. 4.2 Data-Driven Multi-resolution Boosting In this section, we will describe the data-driven approach where we maintain the same complexity of the weak learner, but change the number of data points to be modeled during each boosting iteration. Algorithm 2 describes our data-driven multi-resolution boosting framework for a regression problem . As mentioned ear-lier, this approach is demonstrated using the logitboost algorithm. The initial model is set to null or to the mean value of the target values. The main program runs for a predefined number (T) of iterations. Initially, res is set to 1 indicating the simplest model possible (which will consider all the data points). The feature values are sorted independently by column-wise and the indices corresponding to each column are stored. As the iterations progress, the resolution considered for fitting the weak learner is retained or doubled depending on the error. In other words, depending on the error obtained at a given iteration, the resolution of the data is either maintained or increased for the next itera tion. For every iteration, the residual r is computed depending on the difference between the target value (
Y ) and the final model ( F ). By equating the first derivative of the loss function to zero, we will set the residual as the data to be modeled during the next iter-ation using another weak regressor. Using the quasi-Newton X  X  method the data to be modeled in the next iteration will be set to  X  ( I +2 rr T )  X  1  X  r .Thebest multivariate Gaussian model will be fitted to this data at a given resolution. Theorem 4.1. During each boosting iteration, the minimum of the loss function is achieved by setting f = r and the Newton X  X  update is chosen by setting f =  X  ( I +2 rr T )  X  1  X  r .
 Proof. We will discuss the derivations for the first derivative and the second derivative and show the Newton updates in the case of the boosting for regression problems. Consider the following exponential loss function: For the Newton X  X  update equation, we need to compute the first and second derivatives with respect to f ( x ) and evaluate them at f ( x )=0. Taking the derivative again, we have Hence, the inverse of the Hessian becomes Finally, the Newton X  X  update is given as follows:
Hence, we plug-in the value  X  ( I +2 rr T )  X  1  X  r as the regression value to be modeled using the weak regressor. Also, we can notice that the minimum of the loss function can also be obtained by equating the first derivative to zero.
In other words, by modeling the residual directly using the weak regressor, the minimum of the loss function can be obtained. End of Proof The details of the procedure bestf it which obtains the best weak model at a given resolution of the data is described in the next section. The main reason for retaining the resolution of the next iteration is that sometimes there might be more than one significant component at that given resolution. One iteration can model only one of these components. In order to model the other component, one has to perform another iteration of obtaining the best weak model at the same resolution. Increasing the resolution for the next iteration might fail to model the component accurately. After ensu ring that there are no more significant components at a given resolution, our algorithm will increase the resolution for the next iteration. Hence, the bes t weak model corresponding to current resolution or next higher resolution is obtained at every iteration and the model with the lowest error is added to the final model.

For every iteration, the best weak mod el is fit to the data based on a single feature value at a given resolution. This is performed using the bestf it function in the algorithm. One way of achieving the multi-resolution in this context is to use scale-space kernel to model a subset of data and handling the data in a multi-resolution fashion. The procedure bestgaussf it (instead of bestf it )performsthis task for a particular value of resolution. Additive modeling with smooth and continuous kernels will result in smooth functions for classifier boundary and regression functions. Gaussian kernels are a simple and a trivial choice for scale-space kernels that are powerful universal approximators. Also, Gaussian kernels allow generative modeling of a target function which is a good choice for many applications like object detection. The basic idea is to slide a Gaussian window across all the datapoints corresponding to each feature at a given resolution. Algorithm 3 contains two loops. The outer loop ensures that the Gaussian fit has to be computed for each feature and the inner loop corresponds to the sliding Gaussian. In other words, depending on the given resolution (indicated by n datapoints), a Gaussian kernel containing n datapoints is moved across all the data points and the location where the minimal residual error is obtained.
The result f is obtained by fitting a Gaussian ke rnel computed using weighted median (  X  ) and standard deviation (  X  ) for the datapoints within this window. Algorithm 2. Data-driven Multi-Resolution Boosting Algorithm 3. bestgaussfit After obtaining the weak learner it must be scaled (scale factor is  X  ) according to the target values. Finally, the error i s computed between the weak learner and the target values. If the error with the new model is improved, the resolution is doubled (change at a logarithmic scale) or in other words, the number of datapoints consid ered to fit a Gaussian is halved. In fact, we can use any other heuristic to change the resolution more efficiently. Experimental results showed that this change of resolution is optimal and also this logarithmic change of resolution has nice theoretical properties as they mimic some of the wavelet decomposition methods.

The multi-resolution aspect of our algorithm can be seen from the fact that the resolution of the data to be modeled is either maintained or increased as the number of iterations increase. In fact, one might interpret this approach as an improvement in the weak learner alone because the algorithm proposed here will obtain improved weak learner at every iteration and hence the overall boosting will have faster convergence. We consid er that the main contribution of this paper is not just at the level of choosing a weak learner but it is at the junction between the choice of weak learner and the iterations in the boosting algorithm. Also, our algorithm obtains the weak models in a more systematic hierarchical manner. Most importantly, the increase in the resolution is monotonically non-decreasing, i.e. the resolution ei ther remains the same or increased. We will now demonstrate our results on some real-world datasets. All experi-ments were run in MATLAB 7.0 and on a pentium IV 2.8 GHz machine. Six dif-ferent real world binary classification datasets were chosen from the UCI machine learning repository [2]. Multi-class classification problems can also be performed using methods similar to [1]. Two different sets of experiments were conducted on these datasets to illustrate the power of multi-resolution boosting. In order to demonstrate the model-driven framework, decision trees at multiple resolu-tions (different number of levels in the decision tree) are considered, and in order to demonstrate the data-driven framework, Gaussian kernels are considered for fitting the data at multiple resolutions. 5.1 Results for Model-Dri ven Multi-resolution Fig. 1 shows the test error results on different datasets during the boosting it-erations. Comparisons are made between the standard Adaboost and the multi-resolution boosting framework. We can see that the error obtained using the multi-resolution boosting procedure is significantly lower compared to the stan-dard procedure. This clearly illustrates the fact that the multi-resolution scheme is less prone to the over-fitting problem. Under this framework, during the initial iterations of boosting, decision stumps (trees with only one level of child nodes) are used. As the iterations proceed, mor e deeper trees (with levels greater than 2) are used for modeling. This way, a hierarchical approach is used for comput-ing the classification boundary from low resolution to high resolution. Using a high-resolution weak models will suffer from the problem of over-fitting. For ex-ample, by using a tree with many levels in the first few iterations in the boosting procedure might obtain a very complicated decision boundary which is prone to the over-fitting problem. Also, it will be expensive to use complex trees from the start of the boosting procedure when it is not required to have a complex decision boundary. 5.2 Results for Data-Driven Multi-resolution We demonstrate the power of data-driven multi-resolution approach using scale-space kernels on binary classification problems. Additive modeling with smooth and continuous kernels will result in smooth functions for classifier boundary and regression functions. Since, obtaining the width of the kernel during the boosting process can be a challenging task, the use of scale-space kernels can resolve the problem by using adaptive step-sizes by a  X  global-to-local  X  fitting process. One cannot predetermine the reduction in the kernel width. In our multi-resolution framework, we choose to reduce it by halves using the concepts of wavelet decom-position methods which were well studied concepts in the context of handling image operations efficiently. We compare the performance of these scale-space kernels with other static and dynamic kernels. Exhaustive kernel is the most expensive one which tries to fit a kernel of various widths during each iteration of boosting. Dynamic kernel (or random kernel) fits a kernel of random width during the boosting process. Static kernels will have static widths that do not change during the boosting process.

Compared to other static kernels of fixed width, the scale-space kernels do not suffer from the generalization problem as clearly illustrated by the results on the test data shown in Table 1. Scale-space kernels consistently perform better than the exhaustive or dynamic kernels. For some datasets, wider static kernels perform better than the scale-space kernels and for other datasets static kernels with lesser width perform better. However, scale-space kernels are competitive with the best possible kernels and can be generically used for any dataset. Overall, the scale-space kernels are less than twice as expensive as the static width kernels. One can also see that the results of the scale-space kernels are fairly robust compared to other kernels. This multi-resolution framework will provide a systematic hierar-chical approach of obtaining the classification boundary in the context of addi-tive modeling. One of the main reasons for using the scale-space framework is for faster convergence of the results by dynamically choosing the weak regressors dur-ing the boosting procedure. Choosing an optimal weak regressor by exploring all possibilities might yield a better result, but it will be computationally inefficient and infeasible for most of the practical problems. For such problems, scale-space kernels will give the users with a great flexibility of adaptive kernel scheme at a very low computational effort (also considering fast convergence). The fact that the scale-space kernels converge much faster than static kernels make them more suitable for additive modeling algorithms. To the best of our knowledge, this is the first attempt to use the concepts of scale-space theory and wavelet decomposition in the context of boosting algorithms for predictive modeling. Recently, additive modeling techniques have received a great attention from several researchers working in a wide variety of applications in science and en-gineering. Choosing opti mal weak learners and setti ng their parameters dur-ing the modeling have been a crucial and challenging task. In this paper, we proposed a novel boosting algorithm that uses multi-resolution framework to obtain the optimal weak learner at every iteration. We demonstrated our re-sults for logitboost based regression problems on real-world datasets. Advan-tages of our method compared to existing methods proposed in the literature is clearly demonstrated. As a continuation of this work, we would like to perform the generalization of the multi-resolution approach for other ensemble learning techniques.

