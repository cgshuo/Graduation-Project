 Abstract Supervised machine learning methods to model word sense often rely on human labelers to provide a single, ground truth label for each word in its context. We examine issues in establishing ground truth word sense labels using a fine-grained sense inventory from WordNet. Our data consist of a sentence corpus of 1,000 sentences: 100 for each of ten moderately polysemous words. Each word was given multiple sense labels X  X r a multilabel X  X rom trained and untrained annota-tors. The multilabels give a nuanced representation of the degree of agreement on instances. A suite of assessment metrics is used to analyze the sets of multilabels, such as comparisons of sense distributions across annotators. Our assessment indicates that the general annotation procedure is reliable, but that words differ regarding how reliably annotators can assign WordNet sense labels, independent of the number of senses. We also investigate the performance of an unsupervised machine learning method to infer ground truth labels from various combinations of labels from the trained and untrained annotators. We find tentative support for the hypothesis that performance depends on the quality of the set of multilabels, independent of the number of labelers or their training.
 Keywords Word sense annotation Multilabel learning Inter-annotator reliability 1 Introduction Most words have multiple meanings. In all natural languages, open class words (word classes whose membership is not fixed and where new words can be coined, borrowed, or derived), and many closed class words (such as prepositions), are more often polysemous than not. Many proposals exist for characterizing word sense in computational linguistics, and there are no widely agreed upon standards for determining the number of senses for any given word. Rather, the representation one chooses for word sense is an abstraction shaped by one X  X  theoretical or application goals. Yet determining the meanings of words in their contexts of use is by definition a prerequisite to Natural Language Processing tasks that depend on representing the meanings of utterances, regardless of the approach to word sense representation. This paper presents the results of a study of manual word sense annotation where sense labels are selected from pre-defined sense inventories, using a relatively fine-grained set of senses for each word, and many annotators per instance. The use of multiple sense labels per word and multiple annotators per instance results in a dataset with many gradations in the association between words in context and possible sense labels. In this paper, we argue that the resulting multiplicity provides a more nuanced representation of word meaning, and should benefit automated word sense disambiguation.

Our investigation of manual annotation of word sense relies on WordNet, a widely used lexical resource (Miller et al. 1993 ). The annotation was performed on the Manually Annotated SubCorpus (MASC), a heterogeneous corpus of present day American English that is a subset of the American National Corpus (ANC) 1 (Ide et al. 2010 ). The ANC includes a broad range of genres and consists of 22 million words to date, nearly two thirds of which are freely distributed as the Open American National Corpus (OANC). MASC is a 500,000 word subset of the OANC including equal portions of nineteen spoken and written genres. It has been manually annotated or validated for fourteen types of annotation, including WordNet senses and semantic frames annotated by the FrameNet project (Ruppenhofer et al. 2006 ). One of the goals of MASC word sense annotation is to support efforts to align the sense distinctions made in WordNet and FrameNet, as well as to facilitate investigation of word sense in general.

MASC annotation follows best practice for creating ground truth corpora, as described in Sect. 3 . However, we assume that this methodology merits re-examination. In particular, we address two questions. The first is how best to assess word sense labels from multiple trained or untrained annotators for moderately polysemous words. Each instance in our data has a multilabel consisting of the set of labels from n annotators. Each multilabel will thus contain as many as n distinct label values (lowest consensus on the instance) or as few as one (highest consensus on the instance). The larger n is, the easier it is to distinguish noisy disagreements, where the contexts might be vague, from systematic ones, such as confusability between a pair of senses. The latter case, for example, could arise if a subset of annotators repeatedly chooses label l i where another subset always chooses l , j = i . To provide the richer assessments that our data merits, we present the use of several metrics to supplement the more usual measures of pairwise agreement and chance-corrected agreement coefficients. Results of all these measures on sets of multilabels from trained annotators indicate that the general annotation procedure is reliable, but that individual words differ regarding how reliably annotators can assign WordNet sense labels.

The second question we address about the use of multilabels for sense representation is what is the relation between the overall quality of a set of multilabels and the ability to determine a single ground truth label for each instance in the data. Supervised machine learning methods to model word sense typically rely on a single, ground truth sense label for each word in its context. We examine the tradeoffs between inferring a single ground truth label using multilabels from fewer trained annotators versus more untrained annotators. Recently, there has been increasing interest within the NLP community in carrying out annotation efforts through crowdsourcing, meaning the collective effort of a group of individuals. Our second question bears on the important issue of whether it is possible to rely on crowdsourcing for word sense annotation, given moderately polysemous words. We build on existing work that deals with other types of data, such as image labeling, that indicates that an expert quality labeling can be learned from a set of noisy multilabels. We apply machine learning to infer ground truth sense labels from sets of multilabels, using various combinations of trained and untrained annotators. Our results suggest that learning performance depends in part on the quality of the set of multilabels. Finally, our consideration of how to assess and how to use multilabels for polysemous word sense annotation has forced us to ask whether it is possible in all cases to assign a single ground truth label. We conclude that one way to capture the differences across instances exhibited by multilabels is to represent ground truth as a probability distribution over the sense labels in a word X  X  inventory.

The next two sections present related work and our data. This is followed by analysis of the multilabels in sections on metrics ( 4 ) and assessments ( 5 ). The machine learning experiments in Sect. 6 are followed by a discussion ( 7 ) and a summary of our results ( 8 ). 2 Related work Word meaning has been variously represented in lexicography, linguistics and computational linguistics. Approaches include detailed sense hierarchies for a given word (as in conventional dictionaries), WordNet X  X  ordered sets of synonyms with definitions, components of a conceptual frame as in FrameNet (Fillmore et al. 2003 ), a decomposition into logical predicates and operators (Dowty 1979 ), a cluster of sentences where a word in all of them has the same meaning [as argued for in (Kilgarriff 1997 )], or some combination of the above. Work by Erk and colleagues builds on the view that a sense can be defined as the contexts it occurs in (Kilgarriff 1997 ), or more specifically, as regions in a vector space model (Erk 2009 ). Vector space models, such as Latent Semantic Analysis (Landauer and Dumais 1977 ), represent a word as an N-dimensional vector of contextual dimensions (e.g., a 2-dimensional matrix of sentences by documents). Words with more similar contexts have similar vector representations, thus similarity of vectors captures semantic similarity. Erk and McCarthy ( 2009 ) rely on WordNet senses for an annotation method they refer to as graded sense assignment, in which annotators score all possible senses for every annotation instance. The MASC annotation task also relies on WordNet senses for sense labels. Because we collected multilabels for the annotations presented here, and a multilabel gives a distribution over the sense labels for a given word, this distribution is analogous to the graded sense assignment in Erk and Mccarthy (2009 ). Since all sentences for a given lemma are annotated at the same time, and the WordNet senses include examples along with definitions (see next section), MASC annotation is also similar to grouping instances by their similarity to the examples.

There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in several Senseval efforts (1998, 2001, 2004, 2007 and 2010; cf. Kilgarriff 1998 ; Pedersen 2002a , b ; Palmer et al. 2007 ; Manandhar et al. 2010 ; Agirre et al. 2010 ), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora (Palmer et al. 2007 ). Differences in inter-annotator agreement and system performance across part-of-speech have been examined for two to three annotators (Palmer et al. 2007 ; Ng et al. 1999 ). Investigations of factors that might affect human and system performance have looked at whether each annotator is allowed to assign multiple senses (Ve  X  ronis 1998 ; Ide et al. 2002 ; Passonneau et al. 2006 ), the number or granularity of senses (Ng et al. 1999 ), merging of related senses (Snow et al. 2007 ), how closely related they are (Chugur et al. 2002 ), sense perplexity (Diab 2004 ), and entropy (Palmer et al. 2007 ; Diab 2004 ). Similarly, there have been studies of how distinguishable senses are for systems (Resnik and Yarowsky 1999 ; Ide 2000 ) or humans (Klein and Murphy 2002 , Ide and Wilks 2006 ). As noted below, we find a tentative part-of-speech effect for the 10 words studied here that is not borne out for the full set of MASC words. We do not find significant correlations of annotator agreement with the number of senses, and only a modest correlation with the number of senses used, depending on the agreement metric. What other studies fail to consider, and that we find here, is that the general annotation annotators to apply the sense inventory reliably, independent of the part-of-speech or number of senses.

Previous work has suggested alternatives to pairwise agreement or the j family of agreement coefficients for assessing human or automated sense annotation. In Erk and McCarthy X  X  graded sense assignment (Erk et al. 2009 ), every sense in a word X  X  inventory is assigned a grade on a 5 point scale. To evaluate human graded sense annotation, and automated word sense disambiguation (WSD) against the human data, they consider metrics such as Spearman X  X  correlation coefficient, precision and recall, and Jensen Shannon Divergence (JSD), a distance metric for two probability distributions. Because individual annotators tend to be biased towards higher or lower ratings on senses, Erk and McCarthy use JSD to abstract away from the absolute values. They explicitly do not interpret the distribution of ratings as a probability distribution over the senses. This is in contrast to a suggestion made by Resnik and Yarowsky in an article on evaluation of automated WSD systems (Resnik and Yarowsky 1999 ). Resnik and Yarowsky propose cross entropy, which is related to JSD, to evaluate WSD systems that output a probability score for each available sense from the inventory. They motivate their proposal in two ways: first, that even when incorrect, systems should get partial credit for assigning a relatively higher probability to the correct sense, and second, that a probabilistic result fits in well with downstream processing that is probabilistic. Our use of JSD and related metrics differs from both Resnik and Yarowsky (1999 ) and Erk et al. (2009 ). We compare annotators X  sense distributions on the assumption that each sense has a likelihood that should be roughly equivalent across annotators.

We collected labels from multiple annotators in part to reveal differences across words with respect to annotator behavior. This has been used previously for coreference phenomena. Poesio and Artstein ( 2005 ) analyzed annotations from 18 annotators doing coreference annotation to detect contexts where annotators disagree because the context is ambiguous or vague. When there is data from many annotators, cases of disagreement can be more confidently identified as instances where no one referent (Poesio and Artstein 2005 ) or no one word sense (as in our data) is significantly more probable than all others, or the converse.

Recent work has examined how to leverage word sense data from multiple untrained annotators, using words with very few senses (Snow et al. 2008 ; Callison-Burch 2009 ). Snow et al. included a word sense disambiguation task among several annotation tasks presented to Amazon Mechanical Turkers in which annotators were required to select one of three senses of the word president for 177 sentences taken from the SemEval Word Sense Disambiguation Lexical Sample task (Pradhan et al. 2007 ). They show that majority voting among three annotators reaches 100 % accuracy in comparison to the SemEval gold standard, after correcting a single apparent disagreement where the expert annotation turned out to be incorrect. Many approaches to learning from crowds apply a probabilistic framework, and model differences in annotator expertise (Yan et al. 2010 ), item difficulty, or both (Whitehill et al. 2000 ). Rayker et al. ( 2010 ) propose a Bayesian framework to estimate the ground truth and learn a classifier. One of their contributions is the extension of the approach from binary to categorical, ordinal and continuous labels. None of this work has combined learning from multilabels with assessments of them. We use the method in Whitehill et al. ( 2000 ), because it models both annotator expertise and instance difficulty, factors that affect the quality of the sets of multilabels used here. To our knowledge, no one has attempted to compare trained annotators with crowdsourcing for word sense annotation. 3 Word sense annotation data: multiple annotators The data described here consists of annotations for ten words for one round from half a dozen trained annotators from the MASC project, plus annotations of three of these words collected from fourteen untrained annotators recruited through Amazon Mechanical Turk (AMT), and from expert annotators. The most common components of best practice to create an annotated resource in NLP are development of annotation guidelines; training the annotators; documenting inter-annotator reliability for a subset to demonstrate that the annotation can be applied consistently, to verify that the annotators are reliable, or both. For the full word sense corpus, trained MASC annotators have participated in eleven annotation rounds, with approximately ten words per round, and 1,000 sentences per word. Each round began with a small sample of 50 to 100 sentences used for training annotators on the labels for new words, and for re-consideration of the word sense labels in case they needed revision. In general, we found the distribution of senses in the pre-annotation samples (which are not included in the MASC releases) to be similar to that for the full annotation data. For most rounds, annotator reliability was assessed using two to four annotators on 100 sentences per word, randomly selected from the 1,000 sentences. 3.1 Availability of the data The MASC corpus and word sense data are available from the MASC downloads page. 2 It includes MASC word sense rounds 2 through 5. The round 2.2 data from MASC annotators investigated here is already available for download as part of the WordNet sense annotations and interannotator agreement data. The annotations from turkers and experts will be included in future MASC releases, along with data from the remaining rounds of word sense annotation. 3.2 MASC data: trained annotators The MASC annotators for the data presented here were six undergraduate students: three from Vassar College majoring in cognitive science or computer science, and three linguistics majors from Columbia University. They were trained using guidelines written by Christiane Fellbaum, based on her experience with previous WordNet annotation efforts. The annotation tool is described below. For each new word, annotators applied the same general procedures, but learned a new set of sense labels. As part of the general procedure, annotators were to become familiar with the full set of WordNet senses for a word prior to any annotation, and to consider the WordNet sense relations (e.g., synonymy, hypernymy) during annotation. It was also part of the general procedure that each sentence exemplifies a single word to be annotated; note that all tokens of that word in a given sentence are annotated. Annotators typically completed all instances for a single word before doing the next word. 3.3 Annotation materials and tools The ten words investigated in this study (round 2.2 of MASC) are fairly frequent, moderately polysemous words, balanced for part-of-speech. The ten words are shown in Table 1 with the total number of occurrences in the OANC and the number of WordNet 3.0 senses. Round 2.2 followed an initial round 2.1 where annotators used a beta version of the annotation tool, and where the sense inventory was reviewed, with no modifications to WordNet. For all subsequent rounds, in order to review and possibly revise the WordNet sense inventory, each annotation round of approximately 10 words began with a pre-annotation sample of 50 sentences per word annotated by 4 annotators. Any revisions to the sense inventory to support MASC annotation were included in subsequent versions of WordNet. After the review of sense labels, the 1,000 sentences per word would be annotated, with a subset of 100 annotated by all 4 annotators for assessing annotator reliability. For each of the ten words in the multiply annotated sample of round 2.2, 100 sentences per word were annotated by five or six trained annotators, depending on the word. 3 The resulting 1,000 sentences came from 578 texts representing eight written genres: fiction, journalism, letters, generic non-fiction, technical reports, government reports, government proceedings and travel guides. Mean sentence length was 27.26 words.

Figure 1 a shows WordNet 3.0 senses for adjectival fair as displayed to trained and untrained annotators. The sense number appears in the first column, followed by the glosses in italics, then sample phrases in plain font. When annotating a word for its sense, an annotator minimally considered this combination of an index (the sense number), an intensional definition (gloss), and examples (extensional definition). The examples for each sense can be considered a sentence cluster, where the annotator is to determine which cluster to assign the new sentence to. The trained annotators were instructed to consider WordNet sense relations, such as synsets, hypernyms, troponyms, and antonyms. An example in the general guidelines discusses two similar senses of the noun center whose immediate hypernyms are area and point , thus further discriminating the senses into a central area versus a center point. When creating the MASC annotation tool, it was decided to aim for a balance between ease of use and richness of information, thus the annotation tool displays only the sense number, gloss and example for each sense. Annotators used the WordNet browser interface to view the remaining WordNet lexical information directly from this resource.

Figure 1 b is a screenshot of the SATANiC annotation tool developed to facilitate centralized management of annotation assignments and data collection. It connects directly to the ANC subversion (SVN) repository, allowing the trained annotators to retrieve new assignments (SVN check out ) and save results (SVN commit ). The top frame displays the current sentence with the sample word in bold face. Annotators can enter free-form comments in the next frame. Below that is a scrollable window showing each WordNet sense number and its associated gloss, followed by a list of examples for the sense. Three additional labels are for uses of the word in a collocation, for sentences where the word is not the desired part-of-speech, or where no WordNet sense applies. Note that the annotation tool did not display the WordNet synsets (sets of synonymous senses). For example, the synset for sense 1 of fair also contains sense 3 of the adjective just . As noted above, however, annotators were encouraged to consult WordNet directly to view WordNet sense relations, including synsets. 3.4 Amazon Mechanical Turk data: untrained annotators Amazon X  X  Mechanical Turk (AMT) is a crowdsourcing marketplace where Human Intelligence Tasks (HITs) can be offered, and where results from a large number of annotators (or turkers) can be obtained quickly. We used AMT to obtain annotations from turkers on the three adjectives. The task was designed to acquire annotations for 150 occurrences of each of the three adjectives: fair , long and quiet .We (a) (b) collected annotations from 14 turkers per word. Of the 150 occurrences, 100 were the same as those done by the trained annotators. 4
Previous work has discussed some of the considerations in using AMT for language data (Callison-Burch and Dredze 2010 ) or word sense annotation (Akkaya et al. 2010 ), such as using a qualification test as a quality filter. We found that using a preliminary annotation round as a qualification test discouraged turkers from signing up for our HITs. As it would have been impractical to include all 150 sentences in a single HIT, we divided the task into 15 HITs of 10 occurrences each. To make the turker annotations parallel to the MASC data, we aimed to have each turker complete all HITs, rather than mix-and-match turkers across HITs. As a result, we had to discard or reject HITs for turkers who did not complete them all. This generated two types of protests: (1) some turkers wanted payment for the partial tasks, despite the fact that our instructions indicated that payment would be conditional on completion of all HITs; (2) rejected HITs result in lower AMT future work. We handled the second case by creating pseudo-tasks for those turkers whose HITs we had rejected, and accepting all the pseudo-HITs. This ensured that these turkers X  ratings would not go down. 3.5 Expert labels We collected expert labels for evaluating the unsupervised learning approach. One of the co-authors assigned labels to two adjectives, fair and long , and worked together with an undergraduate research assistant to assign expert labels to the third ( quiet ). The sets of expert labels were reviewed twice: A first independent pass was followed by a second pass that led to a few corrections (2 X 3 %) after comparison with the MASC annotators X  results, or after comparison between the co-author and the undergraduate. 4 Assessment methods Agreement among annotators is typically measured as the proportion of pairs of agreements that occur overall (pairwise agreement), or by an agreement coefficient that calculates the proportion of observed agreement that is above chance expectation, meaning the agreements that would be expected if annotators applied labels randomly at the same rate as observed. We know a priori that a word X  X  senses are not all equally likely, thus another obvious way to compare annotations is to look at the relative probabilities of each sense for each annotator. This can tell us whether annotators differ markedly with respect to the likelihood of specific senses, or with respect to the distribution of likelihoods over the set of senses. Here we present the formulae for computing pairwise agreement and the a agreement coefficient (Krippendorff 1980 ), along with a python package we refer to as Anveshan (Bhardwaj et al. 2010 ) that implements three probability-based metrics and descriptive statistics for multilabels. 5 4.1 Pairwise agreement Pairwise agreement is the ratio of the observed number of pairwise agreements among annotators to the maximum possible number. It is a descriptive statistic that provides a measure of coverage in that it answers the question, how much of the annotated data is agreed upon .

Computation of pairwise agreement for c annotators on i items from k labels, where n ik B c is the number of annotators who labeled item i as k , is given by:
It sums the number of observed pairs of agreements on labels k for the i instances and divides by the total number of possible pairs of agreements. 4.2 Krippendorff X  X  a Krippendorff X  X  a is an agreement coefficient like p (Scott 1955 ), j (Cohen 1960 ), and related coefficients that factor out chance agreement. 6 Where A o is the observed agreement, and A e is the agreement that would be expected by chance, the general formula for agreement coefficients is given by:
For binary annotation labels, the ratio takes values in [ -1,1], otherwise ( -1,1], where 1 represents perfect agreement, -1 represents perfect disagree-ment, and 0 represents the agreement that would occur if annotators chose labels at the same rate as observed, but randomly. 7 Krippendorff X  X  a evolved from measures of variance, thus casts the above ratio as a difference involving observed and expected disagreement [equivalent to the above agreement ratio (Passonneau 1997 )]. Where D o is the observed disagreement and D e is the expected disagreement, it is given by:
For i items, k labels, and c annotators, where n ik is the number of annotators who assign label k to item i , and d k j k l is the distance between a pair of label values k j and k :
For categorical (nominal) data such as sense labels, the distance function assigns the value 1 if k j = k l , and zero otherwise. All disagreements contribute to the sum D o and all agreements do not. [In other MASC rounds, where an annotator could assign multiple sense labels if they seemed equally fitting, we used a set-based distance metric to compare pairs of values k j k l (Passonneau 2006 )]. Expected disagreement is given by: 4.3 Comparison of pairwise agreement and a The combination of pairwise agreement and a is more informative than either is alone. Because pairwise agreement credits all agreements between any pair of annotators, and a only credits agreements that would not be predicted by a random distribution, pairwise agreement is necessarily C a . However, for a given value of pairwise agreement, a can be high or low. High pairwise agreement and low a correspond to a case where the observed high agreement is relatively close to predicted agreement, given the rate at which label values occur. Consider the two following cases for ten instances, two annotation labels, and two annotators. For case 1, the two annotators agree on label L 1 for five instances, label L 2 for four instances, and disagree only on the tenth instance. For case 2, the annotators agree in assigning L 1 to nine instances, and disagree on the tenth instance. In both cases, they have the same number of agreements, with different rates of L 1 and L 2 . Pairwise agreement for both is 90 %, but a is 0.81 for case 1 versus 0.00 for case 2. Case 1 has a roughly equal probability for the two labels  X  p  X  L 1  X  X  11 occur about half the time, and the two types of agreements combined should occur about half the time. At 90 %, the rate of agreement is thus much higher than expected, and a is correspondingly high. For case 2, because the probability of label L 1 is close to 1, the expected agreement equals the observed agreement, and a is zero.
 4.4 Metrics for sense distributions For a dataset of c annotators who label i items with k values, there are multiple annotations of the data that will give the same values for pairwise agreement and a , and all the more so as c , i or k increase in size. For example, given a low a , the disagreement might be due to a single annotator who deviates strikingly from all others (an outlier); to distinct subsets of annotators who have high agreement within but not across the subsets; or to an overall pattern of disagreement. Here, where we have relatively large values for c and k , there are many additional facts of interest about the annotation data besides what proportion of the pairs of values are the same (pairwise agreement), or what proportion are the same after factoring out those that might arise by chance ( a ). Given that sense distributions tend to be highly skewed, it is revealing to know the overall distribution of senses for each word, the distribution of senses for each annotator, and how similar these distributions are.

To distinguish different sources of disagreement by comparing sense distributions within and across annotators, we use the following metrics: Leverage (Piatetsky-Shapiro 1999 ), Kullback-Leibler Divergence (KLD) (Kullback and Leibler 1951 ) and Jensen-Shannon Divergence (JSD) (Lin 1991 ). Each provides a measure of distance of two probability distributions. Each can be used to compare pairs of annotators X  sense distributions, an annotator with combinations of other annotators, or groups of annotators with each other. Rather than reporting all possible combinations, we present summary results that illustrate that they often, but not always, co-vary. Sometimes one, sometimes another of the metrics is more revealing, as we will see later for Table 5 . We report average Leverage of each annotator with all annotators, average KLD of each annotator with all but that annotator, and the average JSD for all pairs of annotators. 4.4.1 Leverage Leverage is a metric which compares two probability distributions over the same population of individuals k [ Novelty (Lavrac et al. 1999 ) is another term for Leverage]. The Leverage of P and Q is given by: the remaining values of k, P a ( k )&gt;0, P b ( k ) = 0. Thus a low Leverage indicates similar distributions, while a high score indicates the inverse. Leverage is used here to compare an individual annotator a  X  X  distribution of senses ( P a ( k )) to the average distribution of senses P  X  k  X  for all annotators, with values in [0,2]. Where n k a is the number of times annotator a uses sense k , c is the number of annotators, and i is the number of instances: 4.4.2 Kullback X  X eibler divergence Kullback-Leibler divergence (KLD) is a non-symmetric measure of the difference between two probability distributions P and Q , where P is a reference distribution and Q is often an approximation of P . It has values in [0, 1 ), and is given as:
The KLD score grows higher for increasingly large deviations. For a given annotator X  X  distribution as the reference, we use KLD to get its comparison with the average sense distribution of all other annotators. The omission of the reference annotator from the average makes it more apparent whether this annotator differs from all the rest. (Note that if we instead take the reference distribution to be the average for other annotators, KLD becomes very large for annotators who failed to use one or more senses used by other annotators.) We compute a distance measure KLD 0 for each annotator by computing the KLD between each annotator X  X  sense distribution ( P a ) and the average of the remaining annotators ( Q ). Where n k b is the number of times annotator b uses sense k , c is the number of annotators, and i is the number of instances: 4.4.3 Jensen X  X hannon divergence Jensen-Shannon divergence is a modification of KLD known as total divergence to the average . In contrast to KLD, JSD is symmetric. It is given by:
Like KLD, JSD takes on values in [0, 1 ), with lower scores indicating the distributions are more similar. We compute JSD  X  P a i ; P a j  X  , V ( i , j ), where i , j B c and i = j . 5 Assessment of label quality MASC is intended to cover a broad spectrum of genres, and to include accurate annotations for less frequent word senses. In the lexicographic and linguistic literature, it is taken for granted that there will be differences in judgment across language users regarding word sense, but the ramifications of preserving such differences when creating annotated resources have not been explored. Current practice in NLP word sense efforts typically assumes that appropriate annotation guidelines and training can yield a single label for word senses (cf. Hovy et al. 2006 ). In our view, this achieves consensus at the expense of a more realistic view of the fluidity of sense judgments and linguistic usage. For example, sense 1 of fair generally occurs with high consensus, but sometimes competes with sense 2. Of the 100 instances of adjectival fair in our data, there are 69 where at least one annotator selected sense 1 from WordNet. In 44 of these cases (64 %), sense 1 is selected by nearly all five of the MASC annotators who worked on fair . In 6 of the cases (9 %), annotators were split 2 X 3 between senses 1 and 2. (In the remaining 27 % of cases, at least one annotator chose one of the eight other possible senses.) In WordNet, sense 1 is glossed as free from favoritism or self-interest or bias or deception; ... , and one of its synonyms is just (an evaluative sense). Sense 2 is glossed as not excessive or extreme (a scalar sense), and one of its synonyms is reasonable . Whether a circumstance brings up issues of justice versus reasonableness is often a matter of opinion, thus leading to different interpretations, as in this example where the project annotators (A101, A102, etc.), plus one expert (E101), were split evenly between the two senses 8 : 1. And our ideas of what constitutes a fair wage or a fair return on capital are
We believe the cases of near ties between these two senses of fair reflect an inherent open-endedness of meaning, rather than poor annotator performance.
By comparing several metrics for the annotations, we can identify annotators who are outliers, meaning those whose overall sense assignments differ markedly from other annotators. We can also provide a more nuanced assessment of a group of annotators than is given by pairwise agreement or by a alone. We first review the annotations of the ten words to identify outliers. An annotator can be an outlier, for example, due to gaps in training; below we identify one annotator who overuses the label that indicates the word is part of a collocation, due to a misunderstanding of the criteria for collocations. By eliminating outliers, we can arrive at a more accurate representation of annotator reliability.

After eliminating one to two outliers, agreement among the remaining annotators is sufficiently high on some words to indicate that MASC sense annotation can be performed reliably, depending on the word. We get the same finding across sets of four well-trained annotators (distinct subsets of a larger set of ten MASC annotators) on the full set of MASC words from all rounds (Passonneau et al. 2012 ). Here, because the same five or six well-trained annotators were used for all ten words, differences in quality after outliers are eliminated are presumed to result from properties of the sense labels themselves, such as sense similarity or confusability, or from inherent differences in how the annotators interpret the sentences. We do not attempt to distinguish these two cases in the present paper.
The next subsections identify outliers among the trained annotators for each word, assess the labels from the remaining trained annotators, and assess the mechanical turkers. Note that all MASC annotators, including outliers, are used in the machine learning experiments described in Sect. 6 . The learning method estimates annotator quality from the observed distribution of labels, and learns to place less trust in some annotators. 5.1 Outlier identification and reliability: trained annotators Table 2 shows pairwise agreement and a on the ten words, prior to elimination of outliers. There is a weak negative correlation of pairwise agreement with number of senses in the inventory that is not significant, using Spearman X  X  q ( q =-0.64, p &amp; 0.05), but a non-significant correlation for a ( q =-0.47, p &amp; 0.16). 9 However, there is a highly significant negative correlation of pairwise agreement with number of senses used ( q =-0.84, p &amp; 0.002), and similarly for a ( q =-0.72, p &amp; 0.018). Agreement goes down as the number of senses used goes up. Further discussion of pairwise agreement and a is deferred until after outliers are eliminated and these metrics are recomputed.

An outlier is a statistical observation whose value is markedly different from others in a sample. When data fits a known distribution, outliers can be identified by measuring the distance of a data point from metrics that characterize the distribution. For example, given a population that follows the normal distribution, the number of standard deviations from the mean indicates how far from normality a sample observation lies. Given a heterogeneous corpus such as MASC, the distribution of senses often appears to be Zipfian, meaning a few labels occur with very high frequency, a few more occur with moderate frequency, and a long-tailed remainder occur relatively rarely. Given many annotators, the rate that each sense label occurs for a given word can serve as an estimate of the true probability of the word sense, and the rate of each sense label for a given instance can serve as an estimate of the probability distribution over senses for that word in that context. We use Leverage, JSD and KLD 0 to identify outlier annotators.

An outlier is an annotator who uses one or more labels at a rate much higher or lower than other annotators. Outliers can result from differences in the procedures followed by the annotator, or from differences in the way annotators interpret the labels and instances; the metrics alone cannot distinguish these two cases. In this section we illustrate both cases through examples and plots that accompany the metrics to show concretely how outliers can be inferred given extreme values of one or more metrics.

The three metrics of Leverage, JSD and KLD 0 are different measures of the similarity of a given annotator X  X  distribution of senses to other annotators, taken as a group (Leverage, KLD 0 ) or one by one (JSD). Leverage represents how far on average the probability of annotator a  X  X  sense labels k are from the average probability of k across all annotators. JSD( P a ( k ), P b ( k )), a = b indicates the similarity of the two sense distributions for a pair of annotators a and b. JSD for an annotator a is the average JSD for all pairings of a with annotators other than a , and indicates the average similarity of that annotator X  X  sense distributions to those of all other annotators. KLD 0 indicates how different an annotator X  X  sense distribution is from the average of all the others. An annotator with values of Leverage, JSD and KLD 0 that are far above the norm is a clear outlier.

Figure 2 illustrates how annotator A108 is identified as an outlier for long . Table (2a) of the figure shows that A108 has much higher values of all three metrics than the five remaining annotators. The bar chart in Fig. 2 b illustrates for each annotator (x-axis) the frequency of each sense (y-axis), thus depicts the similarity of sense distributions across annotators. Inspection of the sense distributions for A108 in the bar chart, compared with other annotators, shows a far greater proportion of long annotated as part of a collocation ( Other ; a rate of 0.36 compared with 0.09 on average). This pattern also appears in A108 X  X  annotations of other words, but exceptionally so for long . The marked difference in A108 X  X  sense distributions reflects a gap in training regarding the criteria for collocations. It should be noted that A108 joined the effort later than the other annotators, and received training at a different time. The remaining annotators have rather similar values of Leverage, JSD and KLD 0 . After dropping A108, the consistency across the remaining annotators is reflected in an increase in pairwise agreement from 0.81 to 0.89 and an increase in a from 0.67 to 0.80. The latter value is noteworthy in that a C 0.80 is taken to represent excellent annotator reliability by one of the more conservative scales of interpretation (Krippendorff 1980 ).
Figure 3 represents a contrasting case in which there are two annotators of quiet (A108 and A102) we identify as outliers. The remaining annotators fall into two subsets who are consistent within but not across the subsets. A108 and A102 have similarly high Leverage, and A108 also has very high JSD and KLD 0 . The plot in Fig. 3 b shows that A108 again has a far greater than average frequency of collocations and a compensatorily much lower than average rate of sense 2. A102 has a much greater than average rate of sense 2 and a rather lower rate of sense 1. After dropping A108 and A102 for quiet , the remaining annotators are not as consistent with one another as we saw above for long : pairwise agreement increases only from 0.64 to 0.66, and a does not increase. However, for two pairs, agreement is very high: for (A101, A103) pairwise agreement is 0.93 and a is 0.86; for (A105, A107), pairwise agreement is 0.89 and a is 0.81. The main difference between the two pairs is that the latter pair uses sense 1 ( characterized by an absence ... of agitation or activity ) relatively more often (52.5 vs. 41 % on average) and sense 3 ( not showy or obtrusive ) relatively less often (17 vs. 32 % on average).

We briefly summarize the remaining cases. Fair is similar to quiet in that annotator A108 again uses the label Other more often than other annotators. A102 uses sense 1 relatively less often than the average of other annotators (43.4 vs. 54.5 %). For the word say , annotators A101 and A103 have relatively high Leverage (0.400 or above vs. a range of 0.121 X 0.313 for the rest), and KLD 0 for A103 is very high (0.877). For A101 and A103, KLD 0 is high relative to the others (0.391 and 0.432 vs. a range of 0.091 X 0.315), as is JSD (above 0.130 vs. below (a) (a) 0.097). A101 uses sense 1 56 % of the time compared with an average over the remaining annotators of 34 % on sense 1. A103 uses sense 2 56 % of the time compared with an average for the rest of the annotators on sense 2 that is also 34 %.
Table 3 shows pairwise agreement and a after dropping outliers. In this small sample, the results show greater agreement on adjectives than nouns, and on nouns than verbs. While this accords with claims for a part-of-speech effect from prior work (Palmer et al. 2007 ; Ng et al. 1999 ), it is not borne out in the full MASC data (Passonneau et al. 2012 ). Also, the magnitudes of the Spearman correlations of senses used with pairwise agreement ( q =-0.645, p &amp; 0.04) or with a ( q = -0.581, p &amp; 0.08) are now much less strong (see paragraph one of Sect. 5.1 ).
The last three columns show subsets of annotators who have relatively low JSD (more similar sense distributions), and also whose a is relatively higher; for the last row ( say ) with three annotators in the Subset column, JSD for the three pairs is shown. After dropping outliers and finding consistent subsets by means of Leverage, JSD and KLD 0 , the values in column Subset a of Table 3  X  X here they exist X  X r in column a otherwise, range from a low of 0.51 (moderate reliability) to 0.80 (excellent reliability). As described above for quiet , there are often subsets of annotators who are consistent within but not across subsets. 5.2 Untrained annotators As expected, when we turn to the assessment of the 14 turkers, they exhibit lower pairwise agreement and lower a scores than the trained annotators. This is shown in Table 4 , with pairwise agreement in [0.25, 0.48], and a in [0.09, 0.29]. Note that as a group, the turkers use all senses in the inventory, in contrast to the trained annotators. For example, the trained annotators do not use senses 4, 5 and 9 of long compared with the turkers who use them about 4, 3.5 and 5 % of the time, respectively. Some turkers perhaps assume the task is to find examples for all the senses. The turkers exhibit higher pairwise agreement and a on fair than on the other two adjectives, despite the fact that fair has the largest number of senses. For the trained annotators, agreement was higher on long than on fair ; for both sets of annotators, agreement was lowest for quiet .

Turning to the measures based on the sense distributions, the turkers X  annotations exhibit markedly higher values of Leverage, JSD and KLD 0 in comparison to the trained annotators, which is also to be expected. For example, Leverage ranges from 0.047 to 0.553 for the trained annotators on the three adjectives (0.047 X 0.580 for all words), and from 0.217 to 1.433 for the turkers. 10 JSD ranges from 0.0188 to 0.1687 for the trained annotators on the three adjectives (same range for all words; the minimum and maximum JSD are for adjectives), compared with 0.123 X 0.759 for the turkers. KLD 0 ranges from 0.023 to 0.891 on adjectives for the trained annotators (0.023 X 1.345 on all words), and from 0.077 to 2.75 for the turkers.

Despite the very high disagreement among turkers overall, and the large differences in sense distributions, it is possible to identify subsets of turkers who have agreement as good as or better than the trained annotators. For example, for fair there is a subset of five turkers who have relatively good agreement: pairwise agreement = 0.86 and a = 0.74. The machine learning experiments include a condition in which labels from the best subset of turkers are used to learn a true label, in order to compare the trained annotators with a roughly equal number of turkers who agree most with one another. We decided to select five rather than six best turkers because some of the sets of trained annotators were size five, and because adding any sixth turker significantly lowers the quality of the sets of turker multilabels for the three words. The 5 turkers with the highest agreement on long have pairwise agreement =0.74 and a = 0.57. For quiet , the best subset of turkers has lower agreement than for the other two words: pairwise agreement =0.54 and a = 0.392.

Table 5 shows the Leverage, JSD and KLD 0 for the 5 best turkers for the three adjectives. Fair , which has the highest agreement, also has a range of values for the probability distribution metrics that is closer to the trained annotators. Not all the same turkers did all the words, but we see that certain turkers who perform well on fair also perform well on long : T107, T108 and T111. The two annotators least similar to the rest are T111 and T114. From Fig. 4 a, showing the sense distributions for each of the 5 best turkers on fair , we can see that T111 and T114 differ most in having a lower probability for sense 1. For long , T108 and T119 have the most similar values of the three metrics; T104 has particularly high KLD 0 , T107 has particularly high JSD ; and T111 and T107 have the highest Leverage. Figure 4 a illustrates that the five best turkers are similar in using sense 1 most often, followed by sense 2, and that two annotators also use sense 5 quite often. Of the three adjectives, quiet exhibits the least uniformity among the turkers, as shown clearly in Fig. 4 c. As shown in Table 5 , T127 has the highest JSD and KLD 0 ; T119 has the highest Leverage. 5.3 Discussion of variation in annotator reliability For the ten moderately polysemous words investigated here, even after eliminating outliers, there is still a wide range of a values (Table 3 ) across words, from a high of 0.80 to a low of 0.44. As noted in the introduction, we take this as evidence that individual words differ regarding the reliability of their sense inventories. We assume that this has to do with differences across words in their inherent semantic properties and contexts of use, rather than with differences in the way the annotators perform, or differences in the methods to create WordNet sense inventories. Because the same annotators apply the same general procedures for all ten words, it is highly unlikely that the words where the set of annotators has a lower a are due to annotator noise or error. To assume this would be to assume that all the annotators are more careless on the instances for certain words and more careful on the instances for other words, and that the annotators become careless or careful on the same words. Because the sense inventories for all ten words have been carefully reviewed by the annotators and an expert member of the WordNet team (Christiane Fellbaum), the lower agreement values observed in Table 3 are also unlikely to be due to misapplication of the procedures for creating WordNet sense inventories. We believe they result instead from a natural variation regarding the meanings of certain words in context. Some classes of contexts are more objective than others. For example, the adjective long pertains to a physical property that can be measured objectively on easily distinguishable dimensions, such as extent in time versus extent in space. In contrast, the distinct semantic dimensions associated with fair are a matter of judgment (see example 1), such as whether there are rational versus moral grounds at issue.

The word quiet , with a lower a (0.49) than the other adjectives, has meanings which are also a matter of judgment. Figure 5 shows the WordNet senses of quiet (excluding one sense specific to water, and another specific to the sun), and two sentences with labels illustrating a fair amount of disagreement. The labels are from six trained annotators, plus two expert labelers (E101, E102, one of whom is one of the co-authors).

The word spot in sentence 1 refers to a 1996 political advertisement in Slate magazine. We observe a difference in whether the annotators seem to interpret images as referring only to a visual dimension (sense 3) or to an audiovisual dimension (senses 2 and 4), and whether the absence of sound is the result of intentional activity (sense 4). The three senses selected by two or three annotators can be associated with the following interpretations reflecting these differences: the images are not associated with sounds in the sound track, possibly inherently (sense 2); the images are unobtrusive and backgrounded with respect to the message (sense 3); sounds associated with the depicted entities have been muted by the depicted individuals or by the filmmaker (sense 4).
 Sentence 2 is from a 1999 Slate article reporting that gunmen killed the Armenian Prime Minister and other government leaders. It describes the city and country as quiet ; which the annotators interpreted variously as exhibiting no activity (a) (sense 1); being relatively free of noise (sense 2); characterized by citizens behaving in a restrained fashion (sense 3); or where people have intentionally lowered the volume of their activities (sense 4).

In both cases from Fig. 5 , it would be difficult to claim that there is a single correct reading; none of the readings appears to be incorrect. How one interprets each sentence presumably depends in part on the perspective one takes on the production values of political advertisements, or on the nature of claims made by a government. 6 Machine learning from multiple labels or features Our next goal is to determine whether it is possible to learn expert quality labels from sets of multilabels produced by trained or untrained annotators, and whether training or number of annotators has a predictable impact. Since supervised WSD typically learns from corpora where one sense is assigned per word (Navigli 2009 ), we investigate here the potential to learn a single ground truth label per instance. Our experiments use an unsupervised learning method applied to the three adjectives fair , long , and quiet , because they had higher levels of agreement from the trained annotators. 11 We create five sets of multilabels from: (1) the trained annotators; (2) a subset of untrained annotators that represent the most consistent of them; (3) random subsets of an equal number of untrained annotators where we average results over the random subsets; (4) all of the untrained annotators; and (5) the combination of all trained and untrained annotators. Our original hypothesis was that future annotation efforts could benefit from insight into the tradeoffs between using fewer labels from trained annotators versus more labels from untrained annotators for word sense. Ultimately, we find no consistent pattern regarding the number of annotators to use. Instead, we find that learning performance depends at least in part on the quality of a given set of multilabels, as measured by our assessment metrics.

GLAD, the unsupervised method we rely on, is an example of a family of graphical models that have been applied to NLP at least since (Bruce and Wiebe 1999 ), where their application to word sense disambiguation data is illustrated for nearly three dozen words, with an average of 8.5 senses each. GLAD assumes that items vary in difficulty, and that labelers vary in accuracy (Whitehill et al. 2000 ). It treats the true labels, labeler accuracy and instance difficulty as hidden variables to be inferred probabi-listically from the observed multilabels, as illustrated in Fig. 6 . 12 From the distribution of observed labels L ij from i annotators on j instances, it learns the probability of true labels Z i , given inferred annotator accuracies a i and instance difficulties b j : Maximum likelihood estimates of the model parameters are obtained using Expectation X  X aximization.
GLAD outperforms majority voting on several image datasets. The method of generating an integrated labeling quality proposed by Sheng et al. (2008 ) also outperformed majority voting. Although majority voting has been proposed as a way to combine labels from multiple sources, it does not perform well in our case. When we compared the results on the five types of multilabel sources (e.g., trained annotators vs. turkers) for the seven word sense tasks (thirty-five cases), we found that GLAD significantly outperformed majority voting twenty-six times out of the thirty-five on accuracy and F-measure; in the remaining nine cases GLAD results did not differ significantly from a majority vote.

GLAD is designed to learn a binary classification, so we prepared seven learning tasks, using the highest frequency senses for each word: senses 1 and 2 of fair , senses 1 and 2 of long , and senses 1 through 3 of quiet . Column 2 of Table 6 shows the number of positive and negative instances (out of 100) assigned by the expert for each task. We ran five experiments on each learning task, using the different sets of labels from trained or untrained annotators mentioned above. In the first experiment, GLAD was applied to the five or six labels from the trained annotators (MASC), including the outliers. In the second, learning was from the best subset of size 5 from the turkers X  labels (AMT best5); these are the turkers from Table 5 . In the third, learning was from subsets of size 6 from the turkers X  labels: 50 random samples of size 6 were selected for each sense, and the average over the 50 samples is reported (AMT subsets avg ). In the fourth, all the turkers labels were used for learning (AMT all). In the fifth, GLAD was applied to the combination of labels from trained annotators and turkers (COMB). Evaluation used the ground truth labels described in Sect. 3.5 . To evaluate performance, we report recall, precision, and F-measure on the positive class, and accuracy. Table 6 shows GLAD performance for the five experiments. 13 The rows with the highest recall, precision, F measure and accuracy are in boldface.

Experiment 1 X  X alf a dozen trained annotators (MASC) This experiment, which used all the labels from the MASC annotators, addressed whether there is an advantage to a smaller set of labels from trained annotators. In three of the learning tasks, GLAD learned best from the trained annotators: sense 1 of fair , and senses 1 and 3 of quiet . For sense 2 of quiet , MASC labels were competitive with or better than all but COMB. For sense 1 of long , GLAD MASC results were better than AMT subsets avg , about the same as AMT best5, and not as good as AMT. For sense 2of fair , MASC labels yielded the poorest GLAD performance of the 5 sets of multilabels.

Experiment 2 X  X est subset of five turkers (AMT best5) This experiment addressed whether selecting subsets of turkers with agreement levels closer to those of the trained annotators could yield GLAD results equivalent to learning from the same number of labels from trained annotators. The answer was yes for sense 2 of fair , both senses of long , and no otherwise. AMT best5 was never the best, but was close to best on sense 2 of fair .

Experiment 3 X  X verage over random subsets of half a dozen turkers (AMT subsets avg ) This experiment addressed the quality of learning a ground truth label by averaging over fifty iterations of random subsets of six turkers. For sense 2 of fair , the AMT subsets avg multilabels led to better performance than the MASC multilabels and nearly as good as the best (COMB). For both senses of fair , AMT subsets avg was equivalent or almost equivalent to learning from labels from the best subsets of turkers.

Experiment 4 X  X ourteen turkers (AMT all) This experiment addressed whether with untrained annotators, doubling the number of labels always improves results, as reported elsewhere (Snow et al. 2008 , Raykar et al. 2010 ). Learning from all the turkers improved over AMT subsets avg for senses of fair and long . For senses 2 and 3of quiet , performance on AMT all was comparable to AMT subsets avg , but for sense 1 quiet it had lower accuracy (0.66 vs. 0.71), and was zero for recall, precision and f-measure. The low performance here is due to the fact that the negative label was always assigned; the probability of the positive label is so low (0.12) that given the relatively few instances, it becomes harder to estimate its probability using expectation maximization. However, the AMT labels did produce the highest or next highest results for three senses (sense 2 of fair , and both senses of long ). Overall, experiment 4 results were good but not the best on accuracy, and were often poor on F measure.

Experiment 5 X  X ombination of trained annotators and turkers (COMB) This experiment addressed whether combining labels from trained and untrained annotators improves results. Per expectations, the combination of all turkers with trained annotators never degraded results. Results improved over untrained labels alone in only four of the seven cases, and were roughly equivalent in the remaining cases.
Comparison across experiments The comparison of the five cases does not yield consistent results across the seven learning tasks. Learning from trained annotators often yields results closest to an expert X  X  labels, but not always. Learning from many turkers X  labels is as good or better than from fewer trained annotators only half the time. This suggests that the overall quality of the set of multilabels might matter when using less than the maximum set of multilabels (COMB). We next ask whether assessments of the sets of labels for each experiment sheds any light on the pattern of results.
 The assessment metrics presented in Sect. 5 were for all senses per word. Because the GLAD experiments use a modified form of the data in which all labels other than the target sense are treated as Other , we recompute the assessment metrics using this binary representation for each target sense. Table 7 gives the pairwise agreement and a scores across all annotators for a given experiment on a given binary sense label. 14 For a given sense label, such as sense 1 of fair , the new data representation obscures the fact that annotators who did not choose sense 1 might have disagreed with sense 1 in different ways (e.g., sense 2 vs. sense 3), therefore the absolute values of the assessments no longer measure the actual agreement. Howevever, they still show the relative degrees of agreement. Thus the trained annotators (MASC) have a slighly lower pairwise agreement on fair sense 1 (0.82) than the best subset of five turkers (AMT best5: 0.89). On average, the MASC annotators X  sense distributions are often more similar to one another than the AMT best5 annotators X . Average Leverage, JSD and KLD 0 are lower for MASC on four of the seven tasks, lower for AMT best5 for two, and about the same for the remaining task.

For each learning task (e.g., sense 1 of fair ), the experiment label in column 2 is in boldface for the experiment that had the best result, or the experiments that had similarly good results. Values in columns 6 through 8 (the probability-based assessment metrics) are in boldface to indicate which of the five sets of labels had the best (lowest) values for average Leverage, JSD and KLD 0 . Here we see a possible explanation for the difference in performance shown in Table 6 . GLAD performs relatively better in predicting expert labels when the sense distributions across annotators are more similar, which we attribute to a loose association with higher accuracy. In four of the seven learning tasks, the set of multilabels with the lowest average Leverage, JSD and KLD 0 has the highest GLAD accuracy (MASC on sense 1 of fair , AMT best5 on sense 2 of fair ), is tied for the highest GLAD accuracy (AMT best5 on sense 1 of long ), or has the next highest GLAD accuracy (MASC on sense 1 of quiet ; AMT best5 has the most similar sense distributions on all three measures, with scores very close to those for MASC). In a fifth case X  X ense 3of quiet  X  X he probability-based metrics are rather low in all the experiments, and the two that had the highest GLAD performance (MASC, AMT best5) are the only ones that had non-chance values of a . While there are no strong correlations of F-measure or accuracy with any of our metrics, the density in the lower right corner of Fig. 7 shows an association between accuracies above 0.80 and KLD 0 below 0.10. JSD and Leverage exhibit a similar pattern.

The two remaining cases are somewhat anomalous. COMB had the highest performance for sense 2 of quiet , but nothing in the assessment data to distinguish this experiment among the five. All had relatively low JSD and KLD 0 ; three of the experiments had relatively low Lev along with a distribution of a scores similar to sense 3 of quiet . For sense 1 of long , we see no explanation for the unusually good GLAD performance for AMT all and COMB. JSD and KLD 0 are low, while Lev is rather high (0.453 and 0.406). The number of positive instances is high (57), but is also high for sense 1 of fair (52). Comparison of the average values for annotator accuracy and item difficulty produced by GLAD was also unrevealing.

In summary, the results presented here suggest there is no a priori best number of annotators or level of annotator training that consistently yields expert quality labels. On the other hand, it is possible to learn a single label close to expert quality. Further, it appears that crowdsourcing could substitute for trained labelers even on word sense labeling using fine-grained sense inventories. 7 Discussion Regarding our first question of how to assess word sense labels for moderately polysemous words, we have shown the aptness of using Leverage, JSD and KLD to compare distributions of word sense data from multiple annotators. We find that the annotation procedure we followed is reliable, and that it is possible to collect reliable labels from trained annotators for some polysemous words. For other words, the sense labels cannot be applied as reliably. Because the same annotators followed the same procedures for all words, we assume that lower performance on certain words is due to properties of the words and their sense inventories, or their contexts of use, or both. The probability-based metrics help determine whether words with lower agreement (e.g., a below 0.50) nevertheless have subsets of annotators who agree well with one another. In previous work, we speculated that lower interannotator agreement for a given word correlated with an inter-sense similarity measure (Passonneau et al. 2010 ), or with the relative concreteness of the word X  X  senses, or with the specificity of the word X  X  contexts of use (Passonneau et al. 2009 ). To go beyond speculation would require much more data than we have investigated here, so this is an endeavor we leave for future work.

We have also explored whether we can posit criteria to collect sets of multilabels that can be used to infer a true sense label for each instance, based on the number of annotators or their level of training. Such criteria could guide crowdsourcing efforts to create new word sense corpora, or to collect other types of annotation that have multiple categorical values. Results from our suite of metrics indicate that there is a trend for accuracy of the inferred true label to be higher when annotators X  sense distributions are more similar. Apart from this trend, there is no explanation for the observed variation in learning performance; it cannot be accounted for solely by the number of annotators, or whether the annotators were trained.

Due to the increasing cost of creating high quality annotated corpora, NLP researchers are turning to crowdsourcing as a lower cost alternative (e.g., Snow et al. 2008 ; Callison-Burch and Dredze 2010 ). A growing body of work provides methods to estimate a true label, given labels from many annotators (Sorokin and Forsyth 2008 ; Raykar et al. 2009 ). This includes studies of the role of annotator quality in the estimation (Yan et al. 2010 ; Whitehill et al. 2000 ; Sheng et al. 2008 ). Sheng et al. ( 2008 ) investigated the relation between the number of annotators, their quality, and change in learning performance as the number of annotators increases. On the assumption that a set of annotators has a uniform probability of being correct on a binary labeling task, they found that as long as the probability of a true label was &gt;0.5, adding new labelers eventually yielded high performance. Our results suggest that for categorical labels (many values), it will be difficult to determine in advance how many noisy annotators would be sufficient to accurately estimate a ground truth label. We suggest that for the word sense task, it might be possible to monitor quality as noisy labels are collected from untrained annotators, and to continue acquiring new labels until a certain quality threshold is reached for the full set of multilabels. At this point, probabilistic methods for estimating the true label could be applied with greater confidence. Alternatively, the multilabels could be curated to eliminate outliers, and each curated multilabel could serve as a ground truth representation of the probability distribution over the possible sense labels.
For semantic and pragmatic distinctions, it is to be expected that some judgments are more difficult to make than others, and will give rise to less agreement among annotators. Towards understanding such cases, interesting patterns are revealed by collecting labels from multiple annotators that would not be apparent given only two or three annotators. In particular, multilabels make it possible to distinguish between noisy and systematic disagreement. Figure 5 b illustrated two examples where there is no single high probability sense for quiet ; there is no pattern to the disagreement. This contrasts with example 1), where annotators were split evenly between two senses of fair , and where there is a systematic pattern of disagreement between senses 1 and 2 on many instances. For the cases of systematic disagreement between the two senses of fair , while it is difficult to assign a true sense label, it is clear that the true label is not any of the senses other than 1 or 2. We believe that a corpus that documents instances with these two types of disagreement can improve automated word sense disambiguation, as well as increase our understanding of word sense and its relation to sentence contexts.

As discussed above, for assessing reliability, each of the one hundred words in the full MASC word sense corpus will have a subset of one hundred instances annotated by at least four annotators. This combined with the data discussed in this paper will provide many examples of instances where annotators disagreed either because they each assigned a different label (noisy disagreement), or because two senses tied for the highest score, or something in between. Automated word sense disambiguation (WSD) can profit from such data through modifications to the algorithms, or in the way they are evaluated. On the algorithm side, WSD might benefit from an approach that separates instances into two sets, high versus low human consensus, and then applies distinct learning methods for each set. For scoring, it makes sense to assign less of a penalty to incorrect answers on instances where humans have low consensus. Further, systems could be given partial credit for cases where they assign an incorrect label that is nevertheless relatively probable. Over a decade ago, Resnik and Yarowsky proposed the use of cross entropy, which is related to JSD and KLD, to score WSD systems that assign a weight to each possible sense label for every instance (Resnik and Yarowsky 1999 ). In their proposal, systems that fail to assign the highest weight to the correct label would still be rewarded if the weight they assign it is relatively high; systems would receive relatively greater penalties for assigning higher weights to an incorrect probability distribution over possible senses, then a metric like cross entropy is the only appropriate way to assess WSD systems. 8 Conclusion We presented a dataset consisting of word sense multilabels from trained and untrained annotators for moderately polysemous words, with instances taken from a heterogenous corpus. Our assessment of the multilabels demonstrated that word sense annotation based on labels from a relatively fine-grained sense inventory can achieve excellent to moderate reliability, depending on the word. We find the same range of differences in reliability across words for the entire MASC word sense corpus, using different sets of four well-trained annotators (Passonneau et al. 2012 ). Annotation, which has long been an investigative tool in Natural Language Processing, seems to be growing in importance, given the increasing number of venues for annotation projects. 15 This suggests that inexpensive methods to achieve high quality annotation will become increasingly important. To this end, our analysis of sets of multilabels on 1,000 instances (100 for each of 10 words) included a deeper investigation of 300 instances (for three adjectives). Our learning experiments demonstrated that expert quality labels for word sense can be learned from noisy multilabels acquired by crowdsourcing. At the same time, they also showed that many questions remain to be addressed regarding the best tradeoff between the cost of adding new labelers and the ability to arrive at a precise estimation of the true label, assuming there is one. Finally, our data indicates that for distribution over senses might be more appropriate.

The data presented in this paper, which will be included with MASC releases, shows the benefits of multilabels for understanding the distributions of word senses, and comparing distributions across annotators. We believe it can also be used to move towards a representation of word sense in context as a distribution over the available sense labels. At the very least, it can be used to discriminate between instances that yield high agreement across annotators, those associated with a split among annotators (as in example 1), and those where annotators choose many senses. While it is expensive for individual research groups to collect such data, incorporating it as part of a community resource provides researchers an opportunity to investigate in new ways the complex interaction among words, senses, contexts of use, and annotators.
 References
