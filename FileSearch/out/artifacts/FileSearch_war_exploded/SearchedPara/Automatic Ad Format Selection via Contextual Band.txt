 Visual design plays an important role in online display ad-vertising: changing the layout of an online ad can increase or decrease its e ff ectiveness, measured in terms of click-through rate (CTR) or total revenue. The decision of which lay-out to use for an ad involves a trade-o ff : using a layout provides feedback about its e ff ectiveness (exploration), but collecting that feedback requires sacrificing the immediate reward of using a layout we already know is e ff ective (ex-ploitation). To balance exploration with exploitation, we pose automatic layout selection as a contextual bandit prob-lem. There are many bandit algorithms, each generating a policy which must be evaluated. It is impractical to test each policy on live tra ffi c. However, we have found that of-fline replay (a.k.a. exploration scavenging) can be adapted to provide an accurate estimator for the performance of ad layout policies at Linkedin, using only historical data about the e ff ectiveness of layouts. We describe the development of our o ffl ine replayer, and benchmark a number of common bandit algorithms.
 H.1.0 [ Information Systems ]: Models and PrinciplesGen-eral Algorithms, Experimentation, Measurement Online advertising, personalization, recommender systems, layout, o ffl ine evaluation, bandit algorithms, exploration/ exploitation
Online advertising continues to grow rapidly as a funda-mental component of the internet economy and ecosystem. It has been a subject of considerable research in machine learning, data mining, economics, user interfaces, and social sciences in general, among other areas. A widespread form of on-line advertising called display advertising consists of placing text or graphical ads (including a combination of these or related media) in certain reserved slots on pub-lisher web-pages, targeted to specific user segments. Some common slots used for these purposes are the top and right panels of the page, but the location and layouts that could be employed are practically limitless.

Two common forms of payment methods used in display advertising include pay per impression (CPM) or pay per action. Typically actions consists of a click (CPC) or some post-click outcome like purchase, subscription, etc. referred to as conversion (CPA). In a CPM campaign, the advertiser pays when an ad is shown to the specified user segment at the specified location on a page, while for CPC the advertiser pays only if the user clicks on the ad 1 .Publisherstypically sell display advertising using two methods  X  a) Guaranteed delivery where ad impressions are sold in advance and the volume to be delivered by the publisher is guaranteed to the advertiser. Since the publisher guarantees a certain number of impressions in the future using this method, advertis-ers typically pay a premium. Such methods are typically used by brand advertisers. b) Another increasingly popular mechanism for buying/selling advertising is to sell each im-pression through an auction with no guarantees on impres-sion volume to advertisers. When a user requests a page with an ad space, an auction is run among the eligible ads (normally those that satisfy certain targeting constraints). Then, one or more ads, depending on the space and layout other performance variants like conversions can be treated as CPC by methods described in this paper characteristics, are chosen for display, ranked based on some function of expected performance and advertiser bid. Ad-vertisers are free to change their auction bids as they see fit to maximize their return on investment (ROI). Normally the location and layout of the available space (even within the same web-page) influence their bidding decisions. Such a method is typically used by performance advertisers. In this paper, we will only consider the auction method of selling display ads.

An online display advertisement is a combination of text, graphics, and potentially other media (e.g., photographs, videos), placed in a reserved space on a web page. We fo-cus on the pay-per-click model, where payment is received when a user clicks on the advertisement. Each serve of an advertising spot on a web page to a user is referred to as an impression. An auction mechanism determines which ads are selected for an impression, and the price an advertiser will pay if their ad is clicked [9, 10]. A critical component of the auction is an estimate of the probability that an ad will be clicked on a given impression; also known as click-through rate (CTR) prediction [16, 1, 11, 6, 4, 15]. While there is a large literature on methods to predict CTR of ads, the concern of this paper is with optimizing the layout (also referred to as format) of an ad in addition to the choice of a high performing ad for an impression. The same ad content can be displayed in di ff erent ways: e.g., changing the back-ground, border, fonts, the size of images, the degree/type of interactivity, the spacing of elements, the order of text and images, etc. We use the terms layout and format inter-changeably, to refer to any di ff erence in the look and feel of an advertising slot. Figure 1 contains four examples of ad layouts. While there are many di ff erent layouts, not all lay-outs are feasible for a given impression and set of ads: e.g., the layout might be too large or small; or a format might be specific to certain ad content, such as video ads.
The notion that the design of an ad a ff ects ad performance predates online advertising [8]. However, such designs are often manually selected by humans. In this context, [7] pre-sented an approach for modeling click response for di ff erent ad arrangement templates on a web-page. Our main con-tribution is to formalize the layout optimization problem as a contextual bandit problem (Section 2). The policy pro-duced by the contextual bandit defines a distribution over layouts, which can be used to choose best performing layouts for ads on future impressions. We show such algorithmic for-mat selection significantly improves performance relative to manual selection on the LinkedIn self-serve display advertis-ing application. While we make no claim to new algorithms for generating contextual bandit policies, we evaluate the performance of several algorithms on data from Linkedin (Section 4) through o ffl ine policy evaluation. In our experi-ence, it is important to have an accurate o ffl ine estimation method of whether a particular policy will work well in prac-tice, without having to first deploy it. Simply testing many di ff erent policies on live tra ffi c using A/B testing methods is costly. In fact, several bandit policies have tuning pa-rameters that have to be selected carefully to ensure good performance, o ffl ine policy evaluation is an e ff ective mech-anism to do so. We share our experience with adapting an existing technique for policy evaluation, o ffl ine replay [14] (a.k.a. exploration scavenging [12]) to layout optimization (Section 3). Figure 1: Examples of ad layouts. Each arm is re-stricted to a certain size on a page.
We begin by posing ad layout optimization as an instance of a contextual multi-armed bandit problem [13]. Each of the K possible ad layouts is represented by an arm. The set of all arms (often referred as the action space) is denoted as A = { a 1 ,...a K } ,where a i denotes the i th format/arm. Although the action space in our scenario is dynamic since new formats are introduced and old ones retired, we shall not consider this in our preliminary exposition to avoid no-tational clutter. When a page is requested by a user, the ad server is called to fill up all reserved ad slots. Each slot has asizeconstraintandanavailablesetofadformatstochoose from. The ad-server has to choose one of the formats for the slot and the ads that will be shown within the format. We note that for the LinkedIn self-serve system, a given format may display more than one ad. Thus, a format may consist of more than one ad impression. A click on any ad within the format is considered a click on the format itself. Every instance to select an ad format on a page will be referred to as an opportunity .

In this paper, we decouple layout optimization from ad selection. We assume the ad selection algorithm is fixed and only focus on the problem of selecting the best format for a given request. While joint optimization of both ad-format and ad selection is desirable, it is more complex (especially due to tight latency constraints the ad-server has to adhere to), and left as future work.
Amulti-armedbanditproblemisasequentialdecision making process; e.g., the stream of page views that include ad slots is a sequence. Bandit problems involve making a decision in each round; e.g., deciding which layout a  X  A to use for a given opportunity. Once a decision is made an observation is collected, and the corresponding reward com-puted; e.g., measuring whether the user clicked on the ad format, with the reward either being the click itself, or the payment for the click (pay-per-click).

An important extension of bandit problems is the addition of context or side information. The context for opportunity t is encoded as a feature vector x t  X  X .Contextcaninclude anything we know about opportunity t .Contextcaninclude features of a predictive model, as well has hard constraints: e.g., if the format has a 300 x 250 allocated pixel space, we cannot choose arms/layouts that are larger or smaller. Every layout is represented by an arm a  X  A ,butcontextallows us to select a subset of feasible layouts for each opportunity t .

For any opportunity t ,wecanencodewhetherornota user would have clicked on any of the ads displayed using layout a  X  A as r t,a  X  { 0 , 1 } .Thevectorofrewardsforeach layout/arm is denoted r t = { r t, 1 ,..., r t,K } .Fortheoretical exposition, we assume the reward vector is drawn from an (unknown) distribution that is i.i.d. We further denote the expected reward when arm a is displayed for opportunity t as  X  r a .

Each opportunity t of a contextual bandit can be decom-posed into three steps: 1. The world draws a context and reward from an unknown distribution: ( x t , r t )  X  D .Thecontextisrevealedtothe player; the reward is not. 2. The player uses some policy  X  : X  X  A to choose an arm  X  ( x t ), given the revealed context. 3. The world reveals the reward of the choice, r t,  X  ( x 4. (Optional) The policy  X  is revised with the data collected for this opportunity,
Apolicyissimplyafunctionthatmapsthecontextto an arm. Policies can be deterministic or non-deterministic. For instance, a hard-coded manual policy to choose format for di ff erent ad slots by humans is a deterministic policy. A non-deterministic policy maintains a distribution over arms, P (  X  ( x )= a | x ), choosing one of them at runtime by sam-pling from the distribution. Layout optimization seeks to learn a policy which maximizes the average expected reward per opportunity We have defined the reward as a click, so layout optimiza-tion is learning a policy which maximizes the click-through rate. If we redefine the reward as product of bid and click-throughs, the goal becomes maximizing expected revenue.
For the most part, this paper concerns itself with the set-ting where an opportunity consists of steps 1-3. A non-deterministic policy  X  is learned from data, prior to the first opportunity based on historical data, and does not change. The online bandit setting includes step 4, where the player adapts their policy for the next opportunity using data col-lected for the current one. To distinguish the two settings, we refer to an online policy as  X  t ,for t =1 ...T .Theonline bandit problem does not require that the policy be updated after each t ;inSection3.1.2,thepolicyisupdatedperiodi-cally in a batched mode 2 . The di ffi cult part of maximizing Equation 1 is evaluating R (  X  )onagivenpolicy.Considerevaluationstrategiesused in other settings: 1. A/B testing :Tocomputethemarginalexpectation with respect to context distribution, segment the impres-sions or users into B buckets at random, and serve each
In the online bandit setting, the order of the rounds mat-ters, which is not reflected in Equation 1. The sample esti-mator for R (  X  ), Equation 2, is readily adapted to the online setting. bucket with a di ff erent policy and estimate the theoreti-cal expectation with empirical average. Each bucket may need a substantial amount of tra ffi c to generate a test with su ffi cient power In large online systems, there can be a substantial engineering cost in deploying many mod-els. Not all policies are better than the current serving one; exploring the policy space often involves sacrificing immediate expected reward for higher expected future re-ward. Furthermore, optimal performance of a given pol-icy often involves tweaking some constants, running A/B tests for this purpose is often prohibitive. 2. Build a model for D :Computing R (  X  )isstraightfor-ward if we have the probability density function for D .
However, this would generally involve building an accu-rate model of user activity, including when a user will click on an ad. Accurately modeling D would allow for solving computational advertising beyond just layout op-timization. Modeling D is not very practical, since his-torical data does not typically contain observations on all arms show in a given context 3 .

What we do have in historical tracking data is a log of i) past impressions, along with necessary context; ii) what format was used to display for an opportunity, i.e., the arm chosen; iii) whether the displayed format was clicked or not, i.e., the reward. The formats are chosen using a fixed serv-ing policy s : X  X  A ,sothehistoricaldataconsistsofa time-ordered stream { ( x t ,s ( x t ) , r s ( x t ) ) }  X  present.

In the published literature on contextual bandits, using historical data from policy s to evaluate other policies  X  $ = s is referred to as o ffl ine replay [14], or exploration scavenging [12]. O ffl ine replay describes a class of sample estimators where 1 [  X  ]istheindicatorfunction,and w t,a is a normaliza-tion weight, In our ad layout selection problem, the contextual features of an opportunity are the channel (web page) and the layout size. Each channel and size combination has its own set of admissible formats. In other words, a layout can only be recommended for one channel and one layout size. Let c , # t denote the channel and layout size of impression x t t =1 ,...,T .Then, For any other channel c or size # , c $ = c t or # $ = # t a.k.a. the partial-label problem [14]. Let r t = r s ( x t ) 1 [  X  ( x t )= s ( x t )]. By substituting w into Eq. 2, we have  X  R (  X  )= 1 Given a channel c and a layout size # , P (  X  ( x t )= a | c, # )only depends on a .Hence,wecanlet h c, # ,a = P (  X  ( x t )= a | c, # ) and substitute it into  X  R (  X  ), we have  X  R (  X  )= 1 where R c,s is the total reward for policy  X  obtained within channel c and layout size s , M a is the number of matched impressions for arm a .Eq.3requiresestimatingvarious distributions since they are unknown. We let:
As can be seen, the accuracy of the estimate  X  R (  X  )de-pends in turn on how well the various probabilities can be estimated. Since this is done from historical data, this data plays a major role in guaranteeing a good estimate. For ex-ample, if the serving policy s rarely serves some layouts a , this could lead to a high variance estimate. Similarly, if the evaluated policy  X  tends to serve some a very few times the variance could increase although this could be a consequence of a providing very little reward. In an extreme case, if s never serves a ,thereisnowaytoknowtherewardof  X  if  X  always recommends a .Thisistoalargeextentequivalent to the problem that arises in importance sampling when the proposal distribution has near zero probability in areas the distribution of interest has non-negligible mass.
In order to evaluate the replayer estimation framework discussed in the previous section at scale, we implemented it in the Map/Reduce framework. The testing event set is partitioned into several subsets. Each reducer only handles the evaluation for one event subset. The general data flow is shown in Figure 2.

In Figure 2, the historical events (such as opportunities) are stored in the distributed file system (HDFS). The map-pers transform every event record into a ranked event by using a specified event transformer. The ranked event is a unified data structure consisting of event attributes, time stamp and reward. Testing policies are implemented as rec-ommender instances in every reducer. The replayer in the reducers feed every ranked event to each recommender and records their output.

We considered two scenarios for o ffl ine evaluation. One scenario for static recommendation policies, where the mod-els never change during the evaluation, but are kept fixed from the start of the evaluation. In this scenario, all models are built before the evaluation starts. The other scenario is the case of dynamic recommendation policies, where the models are updated based on the user feedback recorded from the events that they recommended. The static scenario omits Step 4 (Section 2.1); the dynamic scenario includes it.
Based on these two scenarios, we consider two di ff erent methods to shu ffl e and partition the ranked events from the mappers.
If the recommendation policies are static and there is no feedback from the test events, the ranked events from the mappers can be randomly assigned to any reducer. The re-player only needs to address the correct data size balance for the reducers. In our implementation, the Hadoop parti-tioner is computed by a hash function taking as argument the time stamp of the event.
If the recommendation policies are dynamic and the re-player needs to provide feedback to these policies, the ranked events from the mappers cannot be randomly shu ffl ed and assigned because the sequential order of the test events af-fects the test policies. For some bandit algorithms, such as the $ -first algorithm, the exploration is done only on the events at the beginning so we cannot break the original or-der of the historical events. To keep the original event order, the time stamp of the ranked event is a part of the key given to the reducers. As a result, Hadoop is able to sort all rank events by their time stamps before sending the reducers.
In a real system, the user feedback cannot in general be re-trieved immediately after each impression. In each reducer, the event sequence is split into a collection of batches. The feedback is accumulated within a batch and only provided to the recommendation models at the end of each batch (see Figure 3). In our implementation the batch size is a free parameter, given by the user.
In this section, we first present the performance of various recommendation policies for ad format selection by using the introduced replayer. Then, we compare the accuracy of the replayer estimation with the live (online) system for some policies. All of the experiments are in the context of the LinkedIn advertising platform using data from certain randomly selected markets. Adequate measures were taken to preserve user privacy while conducting all analyses.
The goal in the ad format selection problem considered in this section is to select appropriate ad formats that max-imize the cumulative pay-o ff : the cumulative CTR (click-through-rate) or alternatively revenue. Other performance quantities are possible but we will use these as they are sim-ple to measure for the purposes of this paper. As discussed before, in real scenarios new formats are added into the on-line system on a regular basis. The recommendation mod-els can better predict the popularity of the new formats as they are displayed. Thus, the trade o ff between exploration and exploitation is a fundamental element in this problem. Therefore, this problem is formalized as a bandit problem, where an arm corresponds to an ad format (or an item in general) and the reward is a click (binary reward) or revenue (continuous value reward), depending on the problem. Figure 4: Comparison of bandit algorithms without historical training data
In this evaluation, we consider two general experimental settings. In the first setting, used for evaluating traditional multi-armed bandit algorithms, policies do not have any his-torical information about the ad formats. This setting is useful to evaluate the e ff ect of new ad formats. However, as most of the ad formats are not new at any given time, we Figure 5: Comparison of bandit algorithm with 7 days of historical training data also employ another experimental setting to look at these policies when we have historical data.

The testing data consists of about 184M sampled impres-sion events collected for a period of several weeks. There are more than 40 channels and each channel has about 2 to 15 layout sizes. The ad formats are not uniformly served in the historical data because some channels receive a large amount of tra ffi c and some a relatively smaller amount. Since some policies are randomized algorithms and sensitive to the or-der of the testing events, we randomly split the entire data sequence into 5 subsequences and run every policy indepen-dently. The results are then averaged appropriately using the number of matched events (Eq. 3). Table 1 summarizes the multi-armed bandit algorithms used in this evaluation. Note that softmax(0) is the uniform random policy. In this evaluation, softmax(0) is regarded as a baseline policy.
Figure 4 shows the revenue per request when no historical data is available at the beginning of the experiment (cold-start setting). Figure 5 shows the revenue per request with the previous week of historical data available for training (warm-start). The x -axis has undergone scaling to remove confidential information. Batch = t indicates that a time of length t is employed to simulate updating the recom-mendation models. We use either one day or 10 minutes, this means that the system will update the recommenda-tion models daily or every then minutes. In addition to this, we also vary di ff erent parameters for the bandit algorithms considered, as indicated in the graph.

As shown in Figure 4, by choosing appropriate parame-ters, $ -softmax, $ -firstsoftmax and $ -greedy have the best performances. For $ -softmax and $ -firstsoftmax, when  X  is large, their strategies are close to $ -greedy. $ -ngreedy is a variant of $ -greedy [3] with $ decreasing as follows: where c and d are parameters, K is the number of formats that can be recommended, and n is the total number of recommendations.

There are many variations of the UCB algorithm. In this experiment, we apply a variant of the UCB1 algorithm, UCB1(  X  ), which can be seen as a generalized UCB1 algo-rithm [2]. Given an ad format request, assume there are K formats that can be recommended, the UCB1(  X  )algorithm recommends the format: where r i is the total revenue from the i -th format, T i the total number of recommendations for the i -th format, and  X  is a parameter controlling the priority of exploration versus exploitation. When  X  is larger there is more explo-ration. In the experiment,  X  is varied from 2 to 0.01. When  X  =2,UCB1(  X  )istheoriginalUCB1algorithm.InFigures 4and5,UCB1(0.01)achieveshigherrevenuethananyother UCB1(  X  ). This means that, for this particular setting, we do not need to spend a lot of events on exploration and should focus more on exploitation. In Figure 4, for  X  X atch = 10 min X , UCB(  X  )with  X  &gt; =0 . 1isevenworsethantheuni-form random policy (softmax(0)). This is because the UCB prefers to explore the format that has a large uncertainty. However, those formats may be underperforming formats (whose CTRs are lower than the average CTR). Therefore, the UCB policy could be worse than the random policy.
In Figures 4 and 5, the results for $ -softmax, $ -firstsoftmax, $ -greedy and UCB1(  X  )aresimilartotheconclusionmen-tioned in [3]. Through parameter tuning the performance of $ -greedy and UCB1(  X  )canapproachtheoptimalone. When the algorithms are not well-tuned they can degrade rapidly. However, the optimal parameter values depend on the dis-tribution of the data set. If the data distribution changes, the tuned parameters may not be as e ff ective anymore. On the other hand, if we already have historical data for pa-rameter tuning, those multi-arm bandit algorithms are in a warm-start setting rather than cold-start setting. The best parameter tuned in one setting may not be the best for the other setting. As shown in Figure 4 (cold-start), $ -greedy with larger $ performs well. But in Figure 5 (warm-start), there are 7 days X  historical training data, so $ -greedy with smaller $ performs better. Note that the best $ -greedy in Figure 5 is better than the best $ -greedy in Figure 4.
For $ -ngreedy, [3] asserts that it should approach the op-timal performance by careful parameter tuning. However, [19] also indicates that the family of $ -decreasing algorithms does not have a clear advantage over $ -greedy. Our evalua-tion results shown in Figures 4 and 5 confirm the point in [19].
The performance of Bernoulli Thompson sampling is close to the best observed performance when the batch size is 10 minutes. However, when we increase the batch size to 1 day, it cannot adjust its Beta distributions in time. As a result, it wastes some impressions on exploration and spends less on exploitation. Considering the cold-start and warm-start settings together, Bernoulli Thompson sampling is overall the best policy in our evaluation for ad format selection.
In order to evaluate the accuracy of the replayer estimates described, we use data from the online LinkedIn ad system, where various policies were implemented, as the source of ground-truth. In the online production systems, the tra ffi c is split into several portions that run di ff erent recommenda-tion policies. For this paper we select the tra ffi c where two of these policies are run. In particular, we selected two ex-treme policies for evaluation: softmax(0) and softmax(15). We calculate daily revenue per request for the online system and let this be our ground-truth performance for these poli-cies. We use the historical data to run the replayer X  X  o ffl ine evaluation approach and estimate the revenue per request for the same policies.

In Figures 6 and 7 we can observe the di ff erence between the estimated value and the actual value for the metric of interest. This shows the accuracy of the replayer estimates. The average reward is computed as the total reward over the number of matched events, as in [14]. The historical data is not uniformly served since the ad recommendation has to be optimized. As a result, the average reward estimate is biased [14]. As shown by these figures, the average re-ward always underestimates the performances of softmax(0) and softmax(15). The normalized reward is the estimate proposed in Section 3. As shown by the two figures, the normalized reward is always closer to the actual value of the revenue per request than the average reward.
This paper presented a study of optimizing real-time, web-page ad layout selection, where the goal is to optimize user response. Optimal ad layout selection was formulated as an instance of the contextual bandit problem. As evaluat-ing a battery of bandit algorithms on an on-line system is costly and impractical, the paper described a method for large scale o ffl ine evaluation. In particular, we have formu-lated a method that can be used even in cases where the serving policy is biased ( i.e., items are not served at ran-dom). This is an important practical problem for which aformalapproachhasnotbeenprovidedindetailinthe past. In addition we provided a practical system design tak-ing advantage of the Hadoop-MapReduce architecture. We provided an experimental comparison among many bandit algorithms in the context of a large system, the LinkedIn Ad platform. Additionally, we compared the proposed of-fline policy evaluation approach with the on-line production system and demonstrate its accuracy.

We did not consider the interaction between ad-format selection and ad selection within a format. A complete solu-tion would require joint optimization that makes the prob-lem considerably di ffi cult since the number of ads in a sys-tem is typically large. Further, ranking at runtime has to be done under strict latency constraints. Thus, evaluating the goodness of all possible format and ad combinations is not feasible, approximate procedures are needed. We plan to pursue such an approach in the future. [1] D. Agarwal, B.-C. Chen, and P. Elango.
 [2] J.-Y. Audibert, R. Munos, and C. Szepesv  X  ari. [3] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time [4] K. Bauman, A. Kornetova, V. Topinskii, and [5] O. Chapelle and L. Li. An empirical evaluation of [6] O. Chapelle, E. Manavoglu, and R. Rosales. Simple [7] H. Cheng, E. Manavoglu, Y. Cui, R. Zhang, and [8] D. S. Diamond. A quantitative approach to magazine [9] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [10] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [11] T. Graepel, J. Q. Candela, T. Borchert, and [12] J. Langford, A. L. Strehl, and J. Wortman.
 [13] J. Langford and T. Zhang. The epoch-greedy [14] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased [15] H. B. McMahan, G. Holt, D. Sculley, M. Young, [16] M. Richardson, E. Dominowska, and R. Ragno.
 [17] R. S. Sutton and A. G. Barto. Reinforcement learning: [18] L. Tran-Thanh, A. C. Chapman, E. M. de Cote, [19] J. Vermorel and M. Mohri. Multi-armed bandit
