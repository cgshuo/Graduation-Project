 Object detection remains one of the core objectives of computer vision, either as an objective per se , for instance for automatic focusing on faces in digital cameras, or as means to get high-level understanding of natural scenes for robotics and image retrieval.
 The standard strategy which has emerged for detecting objects of reasonable complexity such as faces is the so-called  X  X liding-window X  approach. It consists of visiting all locations and scales in the scene to be parsed, and for any such pose, evaluating a two-class predictor which computes if the object of interest is visible there.
 The computational cost of such approaches is controlled traditionally with a cascade, that is a suc-cession of classifiers, each one being evaluated only if the previous ones in the sequence have not already rejected the candidate location. Such an architecture concentrates the computation on diffi-cult parts of the global image to be processed, and reduces tremendously the overall computational effort.
 In its original form, this approach constructs classifiers one after another during training, each one technique suffers from three main practical drawbacks. The first one is the need for a very large number of negative samples, so that enough samples are available to train any one of the classifiers. The second drawback is the necessity to define as many thresholds as there are levels in the cascade. This second step may seem innocuous, but in practice is a serious difficulty, requiring additional validation data. Finally the third drawback is the inability of a standard cascade to properly exploit the trade-off between the different levels. A response marginally below threshold at a certain level is enough to reject a sample, even if classifiers at other levels have strong responses. At a more conceptual level, standard training for cascades does not allow the classifiers to exploit their joint modeling: Each classifier is trained as if it has to do the job alone, without having the opportunity to properly balance its own modeling effort and that of the other classifiers. The novel approach we propose here is a joint learning of the classifiers constituting a cascade. and define the overall response of the cascade as the probability of all the classifiers responding positively under an assumption of independence. Instead of training classifiers successively, we directly minimize a loss taking into account this global response. This noisy-AND model leads to a very simple criterion for a new Boosting procedure, which improves all the classifiers symmetrically on the positive samples, and focuses on improving the classifier with the best response on every negative sample.
 show that this joint cascade learning requires far less negative training examples, and achieves per-formance better than standard cascades without the need for intensive bootstrapping. At the compu-tational level, we propose to optimally permute the order of the classifiers during the evaluation to reduce the overall number of evaluated classifiers, and show that such optimization allows for better error rates at similar computational costs. A number of methods have been proposed over the years to control the computational cost of form of adaptive testing : only candidates which cannot be trivially rejected as not being the object of interest will require heavy computation. In practice the majority of the candidates will be rejected with a very coarse criterion, hence requiring very low computation. 2.1 Reducing object detection computational cost Heisele et al. [1] propose a hierarchy of linear Support Vector Machines, each trained on images of increasing resolution, to weed out background patches, followed by a final computationally intensive polynomial SVM. In [2] and [3], the authors use an hierarchy of respectively two and three Support Vector Machines of increasing complexity. Graf et al. [4] introduced the parallel support vector machine which creates a filtering process by combining layers of parallel SVMs, each trained using the support vectors of classifiers in the previous layer.
 Fleuret and Geman [5] introduce a hierarchy of classifiers dedicated to positive populations with ge-ometrical poses of decreasing randomness. This approach generalizes the cascade to more complex pose spaces, but as for cascades, trains the classifiers separately.
 Recently, a number of scanning alternatives to sliding window have also been introduced. In [6] a branch and bound approach is utilized during scanning, while in [7] a divide and conquer approach is proposed, wherein regions in the image are either accepted or rejected as a whole or split and further processed. Feature-centric approaches is proposed by the authors in [8] and [9]. The most popular approach however, for both its conceptual simplicity and practical efficiency, is the attentional cascade proposed by Viola and Jones [10]. Following this seminal paper, cascades have been used in a variety of problems [11, 12, 13]. 2.2 Improving attentional cascades In recent years approaches have been proposed that address some of the issues we list in the intro-duction. In [14] the authors train a cascade with a global performance criteria and a single set of parameters common to all stages. In [15] the authors address the asymmetric nature of the stage goals via a biased minimax probability machine, while in [16] the authors formulate the stage goals as a constrained optimization problem. In [17] a alternate boosting method dubbed FloatBoost is proposed. It allows for backtracking and removing weak classifiers which no longer contribute. ( x , y n ) , n = 1 ,...,N , training examples.
 Sochman and Matas [18] presented a Boosting algorithm based on sequential probability ratio tests, minimizing the average evaluation time subject to upper bounds on the false negative and false posi-tive rates. A general framework for probabilistic boosting trees (of which cascades are a degenerated case) was proposed in [19]. In all these methods however, a set of free parameters concerning de-tection and false alarm performances must be set during training. As will be seen, our method is capable of postponing any decisions concerning performance goals until after training. The authors in [20] use the output of each stage as an initial weak classifier of the boosting classifier approach only constitutes a backward view of the cascade. No information concerning the future performance of the cascade is available to each stage. In [21] sample traces are utilized to keep track of the performance of the cascade on the training data, and thresholds are picked after the cascade training is finished. This allows for reordering of cascade stages. However besides a validation set, a large number of negative examples must also be bootstrapped not only during the training phase, but also during the post-processing step of threshold and order calibration. Furthermore, different learning targets are used in the learning and calibration phases.
 To our knowledge, very little work has been done on the joint optimization of the cascaded stages. In [22] the authors attempt to jointly optimize a cascade of SVMs. As can be seen, a cascade effectively performs an AND operation over the data, enforcing that a positive example passes all stages; and that a negative example be rejected by at least one stage. In order to simulate this behavior, the authors attempt to minimize the maximum hinge loss over the SVMs for the positive examples, and to minimize the product of the hinge losses for the negative examples. An approximate solution to this formulation is found via cyclic optimization. In [23] the authors present a method similar to ours, jointly optimizing a cascade using the product of the output of individual logistic regression base classifiers. Their method attempts to find the MAP-estimate of the optimal classifier weights ordering of the stages a priori fixed. Our approach can be interpreted as a noisy-AND: The classifiers in the cascade produce stochastic Boolean predictions, conditionally independent given the signal to classify. We define the global response of the cascade as the probability that all these predictions are positive.
 This can be interpreted as if we were first computing from the signal x , for each classifier in the cascade, a probability p k ( x ) , and defining the response of the cascade as the probability that K independent Bernoulli variables of parameters p 1 ( x ) ,...,p K ( x ) would all be equal to 1 . Such a criterion takes naturally into account the confidence of individual classifiers in the final response, and introduces an additional non-linearity in the decision function.
 This approach is related to the noisy-OR proposed in [24] for multi-view object detection. How-ever, their approach aims at decomposing a complex population into a collection of homogeneous populations, while our objective is to speed up the computation for the detection of a homogeneous population. In some sense the noisy-OR they propose and the noisy-AND we use for training are addressing dual objectives. 3.1 Formalization as the probabilistic interpretation of the deterministic output of classifier k .
 predictions, under the assumption that they are conditionally independent, given x In the ideal Boolean case, an example x will be classified as positive if and only if all classifiers classify it as such. Conversely the example will be classified as negative if p k ( x ) = 0 for at least one k . This is consistent with the AND nature of the cascade. Of course due to the product, the final classifier is able to make probabilistic predictions rather than solely hard ones as in [22]. 3.2 Joint Boosting Let denote a training set. In order to train our cascade we consider the maximization of the joint maxi-mum log likelihood of the data: At each round t we sequentially visit each classifier and add a weak learner which locally minimizes J the most. If p t ( x ) denotes the overall response of the cascade after having added t weak learners with examples are negative.
 pushes every classifier in the cascade to maximize the response on positive samples, irrespective of the performance of the overall cascade.
 p ( x n )) , each classifier in the cascade is then passed information regarding the overall performance becomes 0 and the classifier ignores its performance on the specific example. On the other hand, if the cascade is performing poorly, then the term becomes increasingly large and the classifiers put large weights on that example.
 Furthermore, due to the term 1  X  p t k ( x n ) , each classifier puts larger weight on negative examples that it is already performing well on, effectively partitioning the space of negative examples. The weights of the weak-learners can not be computed in a close formed as for AdaBoost and are estimated through a numerical line-search. 3.3 Exponential variant To assess if the asymptotic behavior of the loss  X  which is similar in spirit to the logistic one  X  is critical or not in the performance, we also experimented the minimization of the exponential error of the output.
 This translates to the minimization of the cost function : and leads to the following expression for the sample weights during Boosting: for the positive samples and for the negative ones.
 Such a weighting strongly penalizes outliers in the training set, in a manner similar to Adaboost X  X  exponential loss. 4.1 Implementation Details We comparatively evaluate the proposed cascade framework on two data-sets. In [10] the authors present an initial comparison between their cascade framework and an AdaBoost classifier on the CMU-MIT data-set. They train the monolithic classifier for 200 rounds and compare it against a simple cascade containing ten stages, each with 20 weak learners. As cascade architecture plays an important role in the final performance of the cascade, and in order to avoid any issues in the comparison pertaining to architectural designs, we keep this structure and evaluate both the pro-posed cascade and the Viola and Jones cascade, using this architecture. The monolithic classifier is similarly trained for 200 rounds. During the training, the thresholds for each stage in the Viola and Jones cascade are set to achieve a 99.5% detection rate.
 As pointed out, our approach does not make use of a validation set, nor uses bootstrapping during training. We experimented with bootstrapping a fixed number M of negative examples at fixed intervals, similar to [21] and attained higher performance than the one presented here. However it was found that training, was highly sensitive to the choice of M and that furthermore this choice of M was application specific.
 We tested three versions of our JointCascade approach: JointCascade is the algorithm described in  X  3.2, JointCascade Augmented is the same, but is trained with as many negative examples as the total number used by the Viola and Jones cascade, and JointCascade Exponential uses the same number of negative samples as the basic setting, but uses the exponential version of the loss described in  X  3.3. 4.2 Data-Sets 4.2.1 Pedestrians For pedestrian detection we use the INRIA pedestrian data-set [25], which contains pedestrian im-consists of 1239 images of pedestrians as positive examples, and 12180 negative examples, mined from 1218 pedestrian-free images. Of these we keep 900 images for training (together with their mirror images, for a total of 1800) and 9000 negative examples. The remaining images in the origi-nal training set are put aside to be used as a validation set by the Viola and Jones cascade. As in [25] we utilize a histogram of oriented gradient to describe each image. The reader is referred to this article for implementation details of the descriptor.
 The trained classifiers are then tested on a test set composed of 1126 images of pedestrians and 18120 non-pedestrian images. 4.2.2 Faces For faces, we evaluate against the CMU+MIT data-set of frontal faces. We utilize the Haar-like wavelet features introduced in [10], however, for performance reasons, we sub-sample 2000 of these features at each round to be used for training.
 For training we use the same data-set as that used by Viola and Jones consisting of 4916 images of faces. Of these we use 4000 (plus their mirror images) for training and set apart a further 916 (plus mirror images) for use as the validation set needed by the classical cascade approach. The negative portion of the training set is comprised of 10000 non-face images, mined randomly from non-face containing images.
 In order to test the trained classifiers, we extract the 507 faces in the data-set and scale-normalize to 24x24 images, a further 12700 non-face image patches are extracted from the background of the images in the data-set. We do not perform scale search, nor do we use any form of post-processing. 4.2.3 Bootstrap Images As, during training, the Viola and Jones cascade needs to bootstrap false positive examples after each stage, we randomly mine a data-set of approximately 7000 images from the web. These images have been manually inspected to ensure that they do not contain either faces or pedestrians. These images are used for bootstrapping in both sets of experiments. 4.3 Error rate The evaluation on the face data-set can be seen in Figure 1. The plotted lines represent the ROC curves for the evaluated methods. The proposed methods are able to reach a level of performance on par with the Viola and Jones cascade, without the need for a validation set or bootstrapping. The log-likelihood version of our method, performs slightly better than the exponential error version. The ROC curves for the pedestrian detection task can be seen in Figure 2. The log-likelihood version of our method significantly outperforms the Viola and Jones Cascade. The exponential error version is again slightly worse than the log-likelihood version, however this too outperforms the classical approach. Finally, as can be seen, augmenting the training data for the proposed method, leads to further improvement.
 par or better than the Viola and Jones cascade, while avoiding the need for a validation set or for bootstrapping. This lack of a need for bootstrapping, further means that the training time needed is considerably smaller than in the case of the classical cascade. 4.4 Optimization of the evaluation order As stated, one of the main motivations for using cascades is speed. We compare the average number of stages visited per negative example for the various methods presented.
 Typically in cascade training, the thresholds and orders of the various stages must be determined during training, either by setting them in an ad hoc manner or by using one of the optimization schemes of the many proposed. In our case however, any decision concerning the thresholds as well as the ordering of the stages can be postponed till after training. It is easy to derive for any given for an image patch to be accepted as positive. Subsequently the image patch will be rejected if the product of any subset of strong classifiers has a value smaller than  X  .
 Based on this we use a greedy method to evaluate, using the original training set, the optimal order of classifiers as follows : Originally we chose as the first stage in our cascade, the classifier whose Figure 1: True-positive rate vs. false-positive rate on the face data-set for the methods proposed, AdaBoost and the Viola and Jones type cascade. The JointCascade variants are described in  X  4.1. At any true-positive rate above 95%, all three methods perform better than the standard cascade. This is a particularly good result for the basic JointCascade which does not use bootstrapping during training, which would seem to be critical for such conservative regimes. Figure 2: True-positive rate vs. false-positive rate on the pedestrian data-set for the methods pro-posed, AdaBoost and the Viola and Jones type cascade. All three JointCascade methods outperform the standard cascade, for regions of the false positive rate which are of practical use. Table 2: Average number of classifiers evaluated on a sample, for each method and different true-positive rates, on the two data-sets. As expected, the computational load increases with the accuracy. The JointCascade variants require marginally more operations at a fixed rate on the pedestrian popu-lation, and marginally less on the faces except at very conservative rates. This is an especially good result, given their lower false-positive rates, which should induce more computation on average. reponse is smaller than  X  for the largest number of negative examples. We then iteratively add to the order of the cascade, that classifier which leads to a response smaller than  X  for the most negative examples, when multiplied with the aggregated response of the stages already ordered in the cascade. As stated this ordering of the cascade stages is computed using the training set. We then measure the speed of our ordered cascade on the same test sets as above, as shown on Table 2. As can be seen, in the case of the face dataset, in almost all cases our approach is actually faster during scanning than the classical Viola and Jones approach. When the augmented dataset is used however this speed advantage is lost, there is a thus a trade-off between performance and speed, as is to be expected. The speed of our JointCascade approach on the pedestrian data-set is marginally worst than that of Viola and Jones, which is due to the lower false-positive rates. We have presented a new criterion to train a cascade of classifiers in a joint manner. This approach has a clear probabilistic interpretation as a noisy-AND, and leads to a global decision criterion which avoids thresholding classifiers individually, and can exploit independence in the classifier response amplitudes.
 validation data. It allows to easily fix the final performance without the need for re-training. Finally, we have demonstrated that it reaches state-of-the-art performance on standard data sets, without the need for bootstrapping.
 This approach is very promising as a general framework to build adaptive detection techniques. It could easily be extended to hierarchical approaches instead of simple cascade, hence could be used for latent poses richer than location and scale.
 Finally, the reduction of the computational cost itself could be addressed in a more explicit manner than the optimization of the order presented in  X  4.4. We are investigating a dynamic approach where the same criterion is used to allocate weak learners adaptively among the classifiers. This could be hence providing an incentive for early rejection of more samples in the cascade.
 Acknowledgments We thank the anonymous reviewers for their helpful comments. This work was supported by the European Community X  X  Seventh Framework Programme FP7 -Challenge 2 -Cognitive Systems, Interaction, Robotics -under grant agreement No 247022 -MASH.
