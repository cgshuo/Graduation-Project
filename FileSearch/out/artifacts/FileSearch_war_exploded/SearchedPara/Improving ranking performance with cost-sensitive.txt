 Abstract This paper proposes a novel ranking approach, cost-sensitive ordinal classifi-cation via regression (COCR), which respects the discrete nature of ordinal ranks in real-world data sets. In particular, COCR applies a theoretically sound method for reducing an wise regression. Furthermore, COCR allows us to specify mis-ranking costs to further improve the ranking performance; this ability is exploited by deriving a corresponding cost for a popular ranking criterion, expected reciprocal rank (ERR). The resulting ERR-tuned COCR boosts the benefits of the efficiency of using point-wise regression and the accuracy of top-rank prediction from the ERR criterion. Evaluations on four large-scale benchmark data sets, i.e.,  X  X  X ahoo! Learning to Rank Challenge X  X  and  X  X  X icrosoft Learning to Rank, X  X  verify the significant superiority of COCR over commonly used regression approaches. Keywords List-wise ranking Cost-sensitive Regression Reduction 1 Introduction In web-search engines and recommendation systems, there is a common practical need to learn an effective ranking function for information retrieval. In particular, given a query, the ranking function can be used to order a list of related documents, web pages, or items by relevance and display users the relevant items at the top of the ranking list to the users. In recent years, the learning to rank has drawn much research attention in the information retrieval and machine learning communities (Richardson et al. 2006 ; Liu 2009 ; Lv et al. 2011 ).
 Three important characteristics of the learning problem will be considered in this paper. First, the real-world data sets for ranking are usually huge X  X ontaining millions of doc-uments or web pages. This paper focuses on such a large-scale ranking problem. Second, many of the real-world benchmark data sets for learning to rank are labeled by humans with ordinal ranks  X  X hat is, by using qualitative and discrete judgments, for example, {highly irrelevant, irrelevant, neutral, relevant, highly rele-vant} . We shall focus on learning to rank from such ordinal data sets. Third, the resultant ranking list, with more emphasis on items featuring at the top of the ranking list. Such list-wise evaluation criteria match users X  perception in using the ranking function for information retrieval. We shall study learning to rank under the list-wise evaluation criteria.

To tackle the large-scale ranking problem, many learning-based ranking algorithms are based on a longstanding method in statistics and machine learning: regression . In partic-function for ranking through regression. Theoretical connections between regression and list-wise ranking criteria have been studied by Cossock and Zhang ( 2006 ). The benefit of regression is that there are some standard and mature tools that can efficiently deal with large-scale data sets. Nevertheless, standard regression tools often require some metric assumptions on the real-valued scores (e.g., rank 4 is twice as large as rank 2), while the assumptions do not naturally fit the characteristics of the ordinal ranks. A few other studies, therefore, try to resort to ordinal classification , which is more aligned with the qualitative and discrete nature of ordinal ranks. Some theoretical connections between classification and list-wise ranking criteria have been established by Li et al. ( 2007 ).

In this study, we improve and combine the regression and the classification approaches to tackle the ranking problem. In particular, we connect the problem with cost-sensitive therefore, it can express the list-wise evaluation criteria much better. We study theoretical guarantee that allows the use of cost-sensitive classification to embed a popular list-wise ranking criterion X  expected reciprocal rank (ERR; Chapelle et al. 2009 ). Furthermore, we exploit an existing method to reduce the cost-sensitive ordinal classification problem to a strong theoretical guarantee that respects the qualitative and discrete nature of the ordinal ranks. Finally, we utilize the benefits of the regression tools by using them as soft learners for the batch of binary classification tasks. We name the whole framework cost-sensitive ordinal classification via regression (COCR). The framework not only enables us to use the well-established regression tools without imposing unrealistic metric assumptions on embedding them as costs.

Evaluations on four large-scale and real-world benchmark data sets,  X  X  X ahoo! Learning to Rank Challenge X  X  and  X  X  X icrosoft Learning to Rank, X  X  verify the superiority of COCR over conventional regression approaches. Experimental results show that COCR can per-form better than the simple regression approach using some common costs. The results demonstrate the importance of treating ordinal ranks as discrete rather than as continuous. Moreover, after adding ERR-based costs, COCR can perform even better, thereby dem-onstrating the advantages of connecting the top-ranking problem to cost-sensitive ordinal classification.

While this paper builds upon the reduction method proposed by Lin and Li ( 2012 )as well as the ordinal classification work by Li et al. ( 2007 ), there are three major differences. The reduction to regression instead of binary classification is a key idea that has not been explored in the literature; the focus on the ERR criterion for the top-ranking problem instead of the earlier studies on the discrete costs or the Normalized Discounted Cumu-lative Gain (Lin and Li 2012 ) is a novel contribution on the theoretical side; the thorough and fair comparison on four real-world benchmark data sets is an important contribution on the empirical side.

This paper is organized as follows. We introduce the ranking problem and illustrate related works in Sect. 2 . We formulate the COCR framework in Sect. 3 . Section 4 derives the cost corresponding to the ERR criterion. We present the experimental results on some large-scale data sets and conduct several comparisons in Sect. 5 . We conclude in Sect. 6 . 2 Setup and related work We work on the following ranking problem. For a given query with index q , consider a set document x q , i is encoded as a vector in X R D . For the task, we attempt to order all x q , i according to their relevance to q . In particular, each x q , i is assumed to be associated with Q queries with labeled document-relevance examples: The goal of the ranking problem is to use D to obtain a scoring function (ranker) is close to the ordering by the target value y q , i .
 For simplicity, we use n to denote the abstract pair ( q , i ). Then, the data set D becomes D X f X  x n ; y n  X g N n  X  1 ; where N is the total number of documents. Learning-based approaches for the ranking problem can be classified into the following three categories (Liu 2009 ):  X  Point-wise The point-wise approach aims at directly predicting the score of x .Inother  X  Pair-wise In this category, the ranking problem is transformed into a binary which captures the local comparison nature of ranking. Approaches for pair-wise ranking documents, the number of pairs can be as many as X N  X  q  X  2 ; which makes the pair-wise approach inefficient for large-scale data sets. Representative approaches in this category include RankSVM (Joachims 2002 ), RankBoost (Freund et al. 2003 ), and RankNet (Burges et al. 2005 ). When the target value y belongs to an ordinal set f 0 ; 1 ; ... ; K g ; the ranking problem is called multipartite ranking, which is closely related to ordinal regression, as discussed by Fu  X  rnkranz et al. ( 2009 ).  X  List-wise While point-wise ranking considers scoring each instance x q , i by itself and
This study focuses on improving the point-wise ranking by incorporating structural costs, and introduce it into the reduction process for ordinal ranking. The proposed approach not only inherits the benefit of point-wise ranking in terms of dealing with large-scale data sets, but also possesses the advantage of list-wise ranking that takes the structure of the entire ranking list into account. 3 Cost-sensitive ordinal classification via regression regression (COCR). We first describe how to reduce a ranking problem from cost-sensitive ordinal classification to binary classification based on the work of Lin and Li ( 2012 ). Then, we discuss how the reduction method can be extended to pair with regression algorithms instead of binary classification ones. 3.1 Reduction to binary classification ranking approach and solves the ordinal classification problem. Consider a data set D X  ranker r : X!Y from D such that r ( x ) is close to y 2Y . The task of learning a ranker r is decomposed to K simpler sub-tasks, and each sub-task learns a binary classifier g k : X! question:  X  X  X s x ranked higher than or equal to rank k ? X  X  Each binary classifier g k is learned from the transformed data set: where encodes the desired answer for each x n on the associated question. If all binary classifiers g answer most of the associated questions correctly, it has been theoretically proved by Lin and Li ( 2012 ) that a simple  X  X  X ounting X  X  ranker: can also predict rank y closely.

In addition to reducing from the ordinal classification task to binary classification ones, ticular, each example ( x n , y n ) can be coupled with a cost vector c n whose k th component c [ k ] denotes the penalty for scoring x n as k . The value of c n [ k ] reflects the extent of the difference between y n and k . Thus, it is common to assume that c n [ k ] = 0 when k = y n .In common functions satisfy the requirements and have been widely used in practice:  X  Absolute cost vectors  X  Squared cost vectors For instance, suppose that the highest rank value K = 4. Given an example ( x n , y n ) with y = 3, the absolute cost is (3, 2, 1, 0, 1) and the squared cost is (9, 4, 1, 0, 1). Note that the squared cost charges more than the absolute cost when k is further away from y n . The cost vectors give the learning algorithm some additional information about the preferred ranking criterion and can be used to boost ranking performance if they are chosen or designed carefully.

The reduction method transforms the cost vector c n to the weight of each binary example x n ; b  X  k  X  n to indicate its importance. The weight is defined as Intuitively, when the difference between the k th and the ( k -1) th costs is large, a ranker will attempt to answer the question associated with the k th rank correctly. The theoretical included as an additional piece of information when training g k . Many existing binary classification approaches can take the weights into account by some simple changes in the algorithm or by sampling (Zadrozny et al. 2003 ).

In summary, the reduction method starts from a cost-sensitive data set D X  (equal weights) and leads to the simple weight-less version mentioned earlier in this section. Many existing approaches (Li et al. 2007 ; Mohan et al. 2011 ) also decompose the ordinal classification problem to a batch of binary classification sub-tasks in a weight-less manner and thus implicitly consider only the absolute cost. The reduction method, on the other hand, provides the opportunity to use a broader range of costs in a principled manner. 3.2 Replacing binary classification with regression The reduction method learns a hard ranker r g from X to Y X f 0 ; 1 ; 2 ; ... ; K g ; that is, many different instances x q , i can be mapped to a same rank. While such a ranker carries a strong theoretical guarantee, it results in ties of ordering, and is, therefore, usually not preferred in practice. Next, we discuss how we can obtain a soft ranker from X to R instead.
The basic idea is that we replace g k : X!f 0 ; 1 g with soft binary classifiers h k : X! h  X  x  X  0 : 5 jj represents the confidence of the prediction. Note that the hard ranker r g in the reduction method is composed of a batch of hard binary classifiers g k . To use the detailed confidence information after getting h k , we propose to keep Eq. ( 2 ) unchanged. That is, the soft ranker will be constructed as
Below we show that r h can be a reasonable ranker by using the above equation. The common way to learn the soft binary classifiers h k is to use regression. Traditional least squares regression, when applied to the binary classification problem from x to some binary label b 2f 0 ; 1 g ; can be viewed as learning an estimator of the posterior probability P ( b = 1| x ). Following the same argument, each soft binary classifier h k ( x ) in our pro-posed approach estimates the posterior probability P ( y C k | x ). Let us first assume that each h k is perfectly accurate with regard to the estimation. That is, let P k = P ( y = k | x ), Taking a summation on both sides of the equations, Note that the left-hand-side is the expected rank: In other words, when all soft binary classifiers h k ( x ) perfectly estimate P ( y C k | x ), the soft ranker r h ( x ) can also perfectly estimate the expected rank given x .

Note that ( 6 ) has been similarly derived by Fu  X  rnkranz et al. ( 2009 ) to combine the scoring approach of Frank and Hall ( 2001 ), which is a precursor of the reduction method (Lin and given x .
 Theorem 1 Consider any binary classifiers h k : X! R for k  X  0 ; 1 ; ... ; K. Assume that Then , Proof . Note that Inequality ( 7 ) is based on the Cauchy X  X chwarz inequality, which states that the inner product between two vectors is no more than the length-multiplication of the vectors:
Theorem 1 shows that when soft binary classifiers h k can estimate the posterior probability P ( y C k | x ) correctly, the soft ranker r h will also obtain the expected rank of x closely. According to the theorem, we propose to replace the binary classification algorithm in the reduction method with a base regression algorithm A r . The base ranker r h by using Eq. ( 6 ). Algorithm 1 summarizes the process of the proposed COCR framework. h 4 Costs of the criterion of expected reciprocal rank costs in the COCR framework. The criterion has been used as the major evaluation metric in the Yahoo! Learning to Rank Challenge . 1 4.1 Expected reciprocal rank Expected reciprocal rank (ERR; Chapelle et al. 2009 ) is an evaluation criterion for mul-tiple relevance judgments. Consider a ranker r that defines an ordering: follows: The continued product term is defined to be 1 when i = 1. An intuitive explanation of ERR is where higher values indicate better performance. The function R ( y ) maps the ordinal rank y to a probability term that models whether the user would stop at the associated document x . When y is large (highly relevant), R ( y ) is close to 1; in contrast, when y is small (highly irrelevant), R ( y ) is close to 0. Top-ranked (small i ) documents are associated with a shorter product term, which corresponds to the focus on the top-ranked documents.
As suggested by Chapelle et al. ( 2009 ), ERR reflects users X  search behaviors and can be used to quantify users X  satisfaction. The main difference between ERR and other position-based metrics such as RBP (Moffat and Zobel 2008 ) and NDCG (Ja  X  rvelin and Keka  X  la  X  inen 2002 ) is that the discount term: of ERR depends not only on the position information 1 i ; but also on whether highly relevant instances appear before position i .

Next, we derive an error bound on the ERR criterion. To simplify the derivation, we work on a single query and remove the query index q from the notation. In addition, given that ERR depends only on the permutation p introduced by r , we denote ERR ( r , q )by ERR ( p ). Then, we can permute the index in ( 8 ) with p and get an equivalent definition of ERR as: 4.2 An error bound on ERR Some related studies work on optimizing non-differentiable ranking metrics, such as NDCG (Valizadegan et al. 1999 ) and Average Precision (Yue and Finley 2007 ). Fur-thermore, the NDCG criterion is shown to be bounded by some regression loss functions 2007 ). Inspired by the two studies, we derive a bound for ERR in order to find suitable costs for COCR. Note that Mohan et al. ( 2011 ) make a similar attempt with some different derivation steps and shows that ERR is bounded by a scaled error rate in multi-class classification. Our bound, on the other hand, will reveal that ERR is approximately bounded by some costs in cost-sensitive ordinal classification.
 inverse permutation ~ r ; we define The term F i represents the probability of a user stopping at position i when the documents are ordered by ~ p while having ranks ~ y . The definition simplifies the ERR criterion ( 9 )to where b is a vector with b  X  i  X  1 i and y is a vector with y [ i ] = y i .
 the upper-bound of the difference between ERR ( p ) and the ERR of a perfect ranker. that y q ( i ) is a non-increasing sequence. Then , Proof . From the definition in Eq. ( 10 ), with b  X  p  X  i  X  . By the rearrangement inequality, In addition, q is the ordering constructed by y . Thus, for all i , From ( 11 ) and ( 12 ), Then, by the Cauchy X  X chwarz inequality, 4.3 Optimistic ERR cost Next, we use the bound in Theorem 2 to derive the costs for the ERR criterion. In specific, we attempt to minimize the right-hand side of the bound with respect to r . The term in the bound depends on the total ordering introduced by r and is difficult to calculate in a point-wise manner by COCR. Thus, we minimize the term introduced by r will be close to the ordering q introduced by the prefect ranker p . Thus, If the ranker predicts r ( x i ) = k , Therefore, if we are optimistic about the performance of the rankers, the optimistic ERR (oERR) cost vector can be used to minimize the bound in Theorem 2. In particular, when the optimistic ERR cost is minimized, each is approximately minimized and the right-hand side of the bound in Theorem 2 is small. ERR ( p ) would then be close to the ideal ERR of the perfect ranker. For K = 4, given an (49, 36, 16, 0, 64). We see that, when normalized by the largest component in the cost, the oERR vector penalizes for mis-ranking errors more than the squared cost.

The optimistic assumption allows using a point-wise (cost-sensitive) criterion to approximate a list-wise (ERR) one, and is arguably realistic only when the rankers being considered are strong enough. Otherwise, the oERR cost may not reflect the full picture of the ERR criterion of interest. Nevertheless, as to be demonstrated in the experiments with real-world data sets, using the approximation (oERR) leads to better performance than not using the approximation (say, with the absolute cost only). In other words, the oERR cost captures some properties of the ERR criterion and can hence be an effective choice when integrated within the COCR framework. 5 Experiments We carry out several experiments to verify the following claims: 1. For large-scale, list-wise ranking problems with ordinal ranks, with a same base
We first introduce the data sets and the base regression algorithms used in our exper-iments. Furthermore, we will compare COCR with different costs and discuss the results. 5.1 Data sets Four benchmark, real-world, human labeled, and large-scale data sets are used in our experiments. The statistics of the benchmark data sets are described below.  X  Yahoo! Learning To Rank Challenge Data Sets 2 : In 2010, Yahoo! held the Learning to  X  Microsoft Learning to Rank Data Sets : 3 The data sets were released by Microsoft 5.2 Base regression algorithms Three base regression algorithms are considered in our experiments, including linear regression (Hastie et al. 2003 ), M5 X  0 decision tree (M5P; Wang and Witten 1997 )to Gradient Boosted Regression Trees (GBRT; Friedman 2001 ). In the experiments, we use WEKA (Hall et al. 2009 ) for the linear regression and M5P, and use RT-Rank (Mohan et al. 2011 ) implementation for GBRT.  X  Linear regression is arguably one of the most widely-used algorithm for regression. It  X  M5P is a decision tree algorithm based on an earlier M5 (Quinlan 1992 ) algorithm.  X  GBRT aggregates multiple decision trees with gradient boosting to improve the regression
In the following section, we conduct several comparisons using the above mentioned base regression algorithms under the COCR framework with different costs. 5.3 Comparison using linear regression Table 1 shows the average test ERR of direct regression and three COCR settings for the four data sets when using linear regression as the base algorithm. Bold-faced numbers indicate that the COCR setting significantly outperforms direct regression at the 95 % confidence level using a two-tailed t test. The corresponding p values are also listed in the McRank approach (Li et al. 2007 ), can also achieve a higher ERR over direct regression on some data sets. The results verify that it is important to respect the discrete nature of the ticular, the reduction method within COCR takes the discrete nature into account properly and should thus be preferred over direct regression on the data sets with ordinal ranks.
Table 1 shows that COCR with the oERR cost is not only better than direct regression, but can also further boost the ranking performance over the absolute and the squared costs to reach the best ERR for all data sets, except the smallest data set LTRC2. On larger data sets like MS10K and MS30K, the difference is especially large and significant. We further compare COCR with different costs to COCR with the oERR cost using a two-tailed t test and list the corresponding p values in Table 3 a. The results show that COCR with the oERR cost is definitely the best choice within the three COCR settings on LTRC1, MS10K and MS30K. The results justify the usefulness of the proposed oERR cost over the com-monly-used absolute or square costs.
 Another metric for list-wise ranking is normalized DCG (NDCG; Ja  X  rvelin and NDCG@10 results in Table 2 . Note that higher NDCG values indicate better performance. For NDCG@10, COCR with the squared cost is better than the direct regression on all data sets. In addition, COCR with the squared cost is better than COCR with the absolute cost on MS10K and MS30K, and better than COCR with the oERR cost on LTRC1 and settings. On the other hand, COCR with the oERR cost is weaker in terms of the NDCG criterion. Thus, the flexibility of COCR in plugging in different costs is important. More specifically, the flexibility allows us to obtain better rankers toward the application needs (NDCG or ERR) by tuning the costs appropriately (Table 3 ).

The oERR cost is known to be equivalent to an NDCG-targeted cost derived for discrete ordinal classification (Lin and Li 2012 ). The observation that the oERR cost does not lead to the best NDCG performance for list-wise ranking suggest an interesting future research direction to see if better costs for NDCG can be derived. 5.4 Comparison using M5P The M5P decision tree comes with a parameter M , which stands for the minimum number of instances per leaf when constructing the tree. A smaller M results in a more complex (possibly deeper) tree while a larger M results in a simpler one. Figure 1 shows the results of tuning M when applying M5P in direct regression and COCR with the oERR cost on the LTRC1 data set. The M values of 4, 256, 512, 1,024, and 2,048 are examined. The default value of M in WEKA is 4.

Figure 1 a shows that direct regression with M5P can reach the best test performance on the default value of M = 4. However, as shown in Fig. 1 b, COCR with the oERR cost can overfit when M = 4. Its training ERR is considerably high, but the test ERR is extremely low. The findings suggest a careful selection of the M parameter. We conduct a fair selection scheme using the validation ERR. In particular, we check the models constructed by M = 4, 256, 512, 1,024, and 2,048, pick the model that comes with the highest validation ERR, and report its corresponding test ERR. 4 The results for LTRC1 are listed in Table 4 . The first five rows show the validation results of different algorithms, and the last row shows the test result when using the best model in validation. The results demonstrate that when M is carefully selected, all COCR settings significantly outperform direct regression on LTRC1 and COCR with the oERR achieves the best ERR of the three settings.

With the parameter selection scheme above, Table 5 lists the test ERR on the four data sets. The results in the table further confirm that almost all COCR settings are significantly better than direct regression on all data sets, except COCR with the absolute cost on the smallest LTRC2. Furthermore, COCR with oERR cost achieves the best ERR performance on all data sets. After comparing COCR with the oERR cost to COCR with other costs using a two-tailed t test, as shown in Table 3 b, we verify that the differences are mostly significant, especially on MS30K and MS10K. The results again confirm that the oERR cost is a competitive choice in the COCR settings.

Table 6 shows the test NDCG results. Both COCR with the squared and the oERR costs perform better than direct regression on all data sets. In addition, COCR with the absolute cost performs better than direct regression on all data sets except the smallest LTRC2. The finding echoes the results in Table 2 regarding the benefit of COCR on improving NDCG with a carefully chosen cost. 5.5 Comparison using bagging-M5P One concern about the comparison using M5P is that the COCR framework appears to be combining K decision trees while direct regression only uses a single tree. To understand more about the effect on different number of trees, we couple the bagging algorithm (Breiman 1996 ) with M5P. In particular, we run T rounds of bagging. In each round, a T rounds, the trees are averaged to form the final prediction. Then, bagging-M5P for direct regression generates T decision trees and bagging-M5P for COCR generates TK trees. Figure 2 a compares direct bagging-M5P to COCR-bagging-M5P with the oERR cost under the same T . That is, for the same horizontal value, the corresponding point on the COCR curve uses K times more trees than the point on the direct regression curve. The figure shows that the whole performance curve of COCR is always better than direct regression. On the other hand, Fig. 2 b compares the two algorithms under the same total number of trees. That is, COCR with T rounds of bagging-M5P is compared to direct regression with TK rounds of bagging-M5P. The figure suggests that COCR with the oERR cost continues to perform better than direct regression. The results demonstrate that COCR with the oERR cost is consistently a better choice than direct regression using bagging-M5P, regardless of whether we compare under the same number of bagging rounds or the same number of M5P trees. 5.6 Comparison using GBRT Next, we compare COCR settings with direct regression using GBRT (Friedman 2001 )as the base regression algorithm. We follow the award-winning setting (Mohan et al. 2011 ) for the parameters of GBRT X  X he number of iterations is set to 1000; the depth of every 0.1, 0.05, or 0.02. The setting makes GBRT more time-consuming to train than bagging-M5P, M5P, or linear regression, and thus we can only afford to conduct the experiments on the data sets LTRC1 and LTRC2. Tables 7 and 8 show the ERR results on LTRC1 and LTRC2 respectively. In Table 7 , COCR with any type of costs performs significantly better than direct regression with GBRT in most cases. In Table 8 , when using a larger step size of 0.1 or 0.05, COCR with the squared or the oERR costs performs significantly better than direct regression with GBRT; COCR with the absolute cost is similar to direct regression with GBRT. When using a smaller step size 0.02, however, all four algorithms in Table 8 can reach similar ERR on the small data set. Because COCR with the oERR cost setting always enjoys a similar or better performance than direct regression or COCR with other costs, it can be a useful first-hand choice for a sophisticated base regression algorithm like GBRT. 6 Conclusions We propose a novel COCR framework for ranking. The framework consists of three main components: decomposing the ordinal ranks to binary classification labels to respect the discrete nature of the ranks; allowing different costs to express the desired ranking criterion; using mature regression tools to not only deal with large-scale data sets, but also provide good estimates of the expected rank. In addition to the sound theoretical guarantee of the proposed COCR, a series of empirical results with different base regression algo-rithms demonstrate the effectiveness of COCR. In particular, COCR with the squared cost can usually do perform better than direct regression a commonly used baseline on both the ERR criterion and the NDCG criterion.
 Furthermore, we prove an upper bound of the ERR criterion and derive the optimistic ERR cost from the bound. Experimental results suggest that COCR with the optimistic ERR cost not only outperforms direct regression but often also obtains better ERR than COCR with the absolute or the squared costs. Possible future directions includes coupling the proposed COCR framework with other well-known regression algorithms, deriving costs that correspond to other relevant pair-wise or list-wise ranking criteria, and studying the potential of the proposed framework for ensemble learning.
 References
