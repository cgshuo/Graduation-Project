 Learning semantic lexicons is the task of automatically acquiring words with semantic classes (e.g.  X  X un X  is a WEAPON). It has been proved to be useful for many natural lan-guage processing tasks, including question answering [1,2], information extraction [3] and so on. In recent years, several algorith ms have been developed to automatically build semantic lexicons using supervised or semi-supervised methods [4,5,6,7]. As un-supervised method dispenses with the manually labeled training data, more and more methods have been proposed [8,9,10,11,12,13]. There exist some semantic dictionar-ies (e.g., WordNet [14], HowNet [15]). However most of them don X  X  contain resources from specialized domain.

In this paper we propose a weakly supervised learning method, GMR-Bootstrapping, to learn semantic lexicons. It begins with unlabeled corpus and a few of seed words. Then it automatically generates a lists of words with the same category with seed words. From analyzing procedure of the similar bootstrapping algorithm, Basilisk [10], we found that some of the good patterns are given very low score at the beginning stage by Basilisk. Because Basilisk X  X  scoring functions give low scores to all the patterns with large amount of extractions at the beginning stage. However some of them are good ones, whose extractions almost belong to the same category. Therefore we incorporate Graph Mutual Reinforcement to weight candidate words and extraction patterns, in or-der to partially overcome this problem. Evaluations on MUC4 corpus [16] show that incorporating Graph Mutual Reinforcement to weight the candidate words and extrac-tion patterns enables substantial performance gains in extracting BUILDING, EVENT, HUMAN, LOCATION, TIME and WEAPON lexicons.

Another novelty of GMR-Bootstrapping is that pattern X  X  uncertainty is added into scoring functions to learn multiple categories simultaneously. Normally, if a pattern X  X  extractions belong to several different categories, the pattern X  X  correctness should be low. In order to use this information, we integrate a scoring functions to measure pat-tern X  X  uncertainty. The experimental results show that adding patterns X  uncertainty into scoring functions improves the performance too.

We also evaluate GMR-Bootstrapping method on extracting automobile manufac-ture names and automobile models Chinese corpus (details in Section 4). From the experimental results we also observe that the quality of lexicons of extracted by GMR-Bootstrapping is better than quality of lexicons extracted by Basilisk.

The reminder of the paper is organized as follows: Section 2 discussed the related works. In section 3, we introduce our bootstrapping structure and scoring functions. In section 4, experiments are given to sho w the improvements. Section 5 concludes the paper. Several weakly supervised classifier algorithms have been proposed to learning seman-tic lexicons with a small set of labeled data and a large number of unlabeled data, such as Co-training and Bootstrapping. Co-training [17] alternately learns using two orthog-onal views of data in order to utilize unlabel ed data. This enabl es bootstrapping from a small set of labeled training data via a large set of unlabeled data.

Meta-bootstrapping [18] is a Bootstrapping algorithm that uses a two layer boot-strapping structure to learn a dictionary of extraction patterns and a domain specific se-mantic lexicon. Snowball [19], a system for extracting relations from large collections of plain-text documents, uses standard bootstrapping structure and introduces novel techniques for evaluating the quality of the patterns and tuples generated at each step of the extraction process. The KnowItAll [12] utilizes a set of domain-independent extrac-tion patterns to generate candidate facts. Th en the candidate facts are evaluated by point wise mutual information (PMI) statistics. Hassan et al.(2006) presented an unsupervised method, which does not require seeds or exa mples. Instead, it depends on redundancy in large data sets and graph based mutual rein forcement to acquire extraction patterns.
The algorithm most closely related to our method is Basilisk [10], which is also a bootstrapping algorithm. While meta-bootstrapping trusts individual extraction pat-terns to make unilateral decisions, Basilisk gathers collective evidence from a large set of extraction patterns. We also use the same idea and structure. While there are some differences between GMR-Bootstrapping and Basilisk. Firstly, our method incorporates Graph Mutual Reinforcement to weight candidate words and extraction patterns. An-other difference is that we enhance the GMR-Bootstrapping with pattern X  X  uncertainty to learn multiple categories simultaneously. GMR-Bootstrapping is a weakly supervised learning method. It is used to generate semantic lexicons. The input to GMR-Bootstrapping are few manually selected seed words for each semantic category and an unlabeled text corpus. Figure 1 shows the structure of GMR-Bootstrapping process. In this section, we describe details of GMR-Bootstrapping algorithm.
 3.1 Structure and Algorithm of GMR-Bootstrapping As shown in Figure 1, the GMR-Bootstrapping begins by extracting a number of the ex-traction patterns that can match the seed words. After that candidates for the lexicon are extracted with these patterns. Then a bipartite graph (Figure 2) is built, which represents the matching relation between patterns and candidate words. Finally GMR Scoring is applied to iteratively assign correctness weights of patterns and candidate words. The five best candidate words are added to the lexicon.Then process starts over again. 3.2 Pattern In order to find new lexicon entries, extraction patterns are used to provide contex-tual evidence that a word belongs to which semantic classes. There are two commonly used patterns, Syntactic Pattern and Context Pattern . Both of them are used in our experiments. Syntactic Pattern. Syntactic Pattern is used by many other Bootstrapping methods [20,18,10]. We follow Thelen and Riloff (2002) X  X  methods, which used the AutoSlog system [21], to represent extraction patterns. AutoSlog X  X  extraction patterns represent linguistic expressions that extract a noun phrase X  X  head noun in one of three syntactic roles: subject, direct object, or prepositional phrase object. For example, three patterns that would extract weapon are: [subject]was fired, carry [direct object], wounded by [pp object].
 Context Pattern. Different from the Syntactic Pattern, Context Pattern uses words only. Syntactic roles are not included in it. In our implementations, if X 0 belongs to lexicons, a number of patterns will be generated based on the following templates in MUC4 corpus: X  X  1 represents the word immediately before current word,while X 1 represents the word immediately after current word.

Different from English, Chinese words ar e not delimited by spaces. There is no clue to tell where the word boundaries are. Automobile manufacture names and automobile models names can not be segmented well by most of the state-of-the-art Chinese word segmenters. Therefore, patterns we used in Chinese corpus contain characters before and after the target words. Chinese word segmenter could be omitted through this kind of patterns. The following templates are used in Chinese corpus: where X  X  represents the Chinese character. If a article fragment matches both the part before and after the [ ext ] , the characters between them are extracted as candidate word. 3.3 GMR Scoring GMR Scoring is used to iteratively assign scores of patterns and candidate words. We assume that patterns that match many words from the same category tend to be im-portant. Similarly, words matched by many patterns that belong to same category tend to be correct. A bipartite graph is build to represent the relation between pattern and candidate word. Each pattern or candidate word is represented by a node in the graph. Edges represent matching between patterns and candidates. Then the problem is trans-ferred to hubs (patterns) and authorities (words) problem which can be solved using the Hypertext Induced Topic Selection (HITS) algorithm [22].

Each pattern p in P is associated with a weight sp ( p ) denoting its correctness. Each candidate words w in W has a weight sw ( w ) which express the correctness of the word. The weights are calculated iteratively through equation 1 to equation 5 as follows: where sw ( u ) is initialized to 1 if u  X  Semantic Lexicons andto0if u  X  SemanticLexicons , W ( p ) is the set of words matched i , SW ( i ) and SP ( i ) are the normalization factors defined as: Equation 2 is similar with RlogF , which has been used to score patterns [10], except that we changed the F i in RlogF to Equation 1. RlogF is where F i is the number of category members extracted by pattern i and N i is the total number of nouns extracted by pattern i . From the definition of RlogF , we observe that the patterns which contain a large amount of extractions would be given low scores by RlogF at the beginning stage. Although some of them are good ones. Equation 3 is changed from AvgLog , which has been used to score candidate words [10]. Through those changes the scoring functions of pattern and candidate words are connected and can be iteratively calculated. The scores of good patterns with amount extractions will increase through iterations. The problem of RlogF could be partially overcame. 3.4 Learning Multiple Semantic Categories From and Thelen and Riloff (2002) X  X  analysis and results of Basilisk-MACT+ [10], we observe that learning multiple semantic categories can improve all the categories X  results. We also extend GMR-Bootstrapping to learn multiple semantic categories si-multaneously (GMR-M-Bootstrapping). Norma lly, if extractions of a pattern belong to several different categories, the pattern X  X  correctness should be low.
 We u s e L p to represent labels of pattern p X  X  extractions, H ( L p ) is the entropy of L , which is calculated only in the extractions which have been labeled to a semantic category. For example: pattern p , whose extractions are w 1 ,w 2 , ...w n . We can find its extractions X  labels through semantic lexicons at this stage. L p = l 1 ,l 2 , ..., l n ,where Then the entropy of L p is calculated through Equation 6.
 Through we could define the patterns X  uncertainty, (1  X  H ( L p ) log when L p is uniform to 1 when L p contains one types of labels [23].

Therefore equation 2 and 3 are changed into: which are modified by multiplying patterns X  uncertainty. The experiments and results usingE.q7andE.q8areshowninSection4. To compare the performance of GMR-Bootstrapping with other weakly supervised methods, we design several experiments to evaluate with two corpora. One is the MUC-4 corpus [16], which contains 1700 texts (includes both test and training parts) in terror-ism domain. All the words in the corpus are divided into nine semantic categories [10]: BUILDING, EVENT, HUMAN, LOCATION, ORGANIZATION, TIME, VEHICLE, WEAPON and OTHER. A few of semantic lexicon learners have previously been eval-uated on the this corpus [20,4,18,10], and of these Basilisk achieved the best results. We reimplemented the Basilisk algorithm to compare it with GMR-Bootstrapping. Another one, CRCV (Chinese Review Corpus a bout Vehicle), which was collected by ourselves, contains about 500,000 articles in around 500 automobile domain forums. All the ar-ticles are reviews about vehicle. GMR-Boot strapping and Basilisk are used to learn MANUFACTURE and MODEL categories in this corpus. 4.1 Results in MUC4 Corpus Figure 3 shows results of Repeated Basilisk (R-Basilisk) with different patterns, Con-text Pattern ( CP ) and Syntactic Pattern ( SP ). For each category, 10 most frequent nouns that belong to the category are extracted as seed words. It is the same as Thelen and Riloff (2002) X  X  way. We ran the algorithm with different patterns for 200 iterations , so 1000 words are extracted. The X axis shows the number of words were extracted. The Y axis shows the number of correct ones. From the results we know that the performance of SP is better than CP X  X  in all categories except the time category. The results reflect that syntactic roles are useful to learn semantic lexicons. Experimental results also show that the R-Basilisk X  X  performance is similar with the Basilisks X  results reported by The-len and Riloff (2002) in all categories.

The next experiment we did is to evaluate the impact of the number of seed words on both our method and Basilisk. Table 1 shows the result. The left-hand column represents the number of seed words given and each cell represents the number of correct words learned by different algorithms. Same as the previous experiments, the seed words are also selected by their frequency in MUC-4 Corpus. From the result we can know that performance of GMR-Bootstrapping is better than Basilisk in all the conditions. The improvements are more significant when the number of seed words is small. GMR-Bootstrapping X  X  result is good even when only 2 seed words are given. We can also find that the sum of the number of seed words and correct extractions doesn X  X  have signifi-cant increase in building, time and weapon categories when the number of seed words is more than 5. It shows that both GMR-Bootstrapping and Basilisk have a bottleneck. Even a lot of seed words are given, low frequency words are hard to extract.
Then we evaluated GMR-M-Bootstrapping to learn multiple semantic categories si-multaneously. The results are shown in Table 2, where the column GMR represents the result of GMR-Bootstrapping, GMR-M represents GMR-M-Bootstrapping X  X  results, RI represents the relative improvement of GMR-M-Bootstrapping over GMR-Boot strapping, Basilisk-MACT+ X  X  results [10], which is the previous best result in MUC-4 corpus, are shown in column B-MACT+. Experiments show that GMR-M-Boot strapping X  X  results are better than GMR-Bootstrapping X  X  in all the categories. Those results indicate that our method can improve the results and including pattern X  X  un-certainty into scoring functions can benefit the final results. We know that GMR-M-Bootstrapping X  X  results are better than the previous best results in all the categories and GMR-Bootstrapping X  results are similar with the Basilisk MACT+ X  X .
 4.2 Results in CRCV Corpus Finally we compare the results of GMR-Bootstrapping with R-Basilisk in CRCV Cor-learn MANUFACTURE and MODEL categories. 10 seed words are given for each category. From the results, which are shown in Table 3, we can know that GMR-M-Bootstrapping X  X  performance is better than GMR-Bootstrapping X  X  and GMR-Boot strapping X  X  performance is better than R-Basilisk X  X  in both of the categories. The trend is same as the results in MUC-4 corpus. In this work, we present a novel bootstrapping method, GMR-Bootstrapping, to learn semantic lexicons. Through changing the ca ndidate words and patterns scoring func-tions, we incorporate Graph Mutual Reinforcement to weight the correctness of candi-date word and extraction patterns. The motivation for our approach is provided from graph theory and graph link analysis. We also enhance the GMR-Bootstrapping to learn multiple categories simultaneously by adding patterns X  uncertainty into scoring functions. Another contribution of this work is that we present the number of seed words X  impact on Basilisk and our method. Experimental results show that GMR-Bootstrapping X  X  results are better than previous best algorithm X  X  results in MUC4 cor-pus. Experiments in Chinese corpus also show that GMR-Bootstrapping outperform the state-of-the-art technique Basilisk.

In our future work, we plan to focus on Chinese corpus, try to find more generalized feature and pattern.
 This work was partially supported by Chinese NSF 60673038 and 60503070. The au-thors would like to thank Ellen Riloff for sharing AutoSlog and labeled data with us. Thanks Chaofeng Sha and Xian Qian for their valuable comments and suggestions. Thanks the anonymous reviewers for their valuable comments and suggestions.
