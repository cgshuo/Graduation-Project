 Marek Petrik mpetrik@us.ibm.com IBM T.J. Watson Research Center, Yorktown, NY, USA Large Markov decision processes (MDPs) are common in reinforcement learning and operations research and are often solved by approximate dynamic program-ming (ADP). Many ADP algorithms have been de-veloped and studied, often with impressive empirical performance. However, because many ADP methods must be carefully tuned to work well and offer insuffi-cient theoretical guarantees, it is important to develop new methods that have both good theoretical guaran-tees and empirical performance.
 Approximate linear programming (ALP) X  X n ADP method X  X as been developed with the goal of achieving convergence and good theoretical guaran-tees (de Farias &amp; van Roy, 2003). Approximate bilin-ear programming (ABP) improves on the theoretical properties of ALP at the cost of additional computa-tional complexity (Petrik &amp; Zilberstein, 2009; 2011). Both ALP and ABP provide guarantees that rely on conservative error bounds in terms of the L  X  norm and often under-perform in practice (Petrik &amp; Zilber-stein, 2009). It is, therefore, desirable to develop ADP methods that offer both tighter bounds and better em-pirical performance.
 In this paper, we propose and analyze distribu-tionally robust approximate dynamic programming (DRADP) X  X  new approximate dynamic program-ming method. DRADP improves on approximate lin-ear and bilinear programming both in terms of the-oretical properties and empirical performance. This method builds on approximate linear and bilinear pro-gramming but achieves better solution quality by ex-plicitly optimizing tighter, less conservative, bounds stated in terms of a weighted L 1 norm. In particular, DRADP computes a good solution for a given initial distribution instead of attempting to find a solution that is good for all initial distributions.
 The objective in ADP is to compute a policy  X  with the maximal return  X  (  X  ). Maximizing the return also min-imizes the loss with respect to the optimal policy  X  ?  X  known as the policy loss and defined as  X  (  X  ? )  X   X  (  X  ). There are two main challenges in computing a good policy for a large MDP. First, it is necessary to ef-ficiently evaluate its return; evaluation using simula-tion is time consuming and often impractical. Second, the return of a parameterized policy may be a func-tion that is hard to optimize. DRADP addressed both these issues by maximizing a simple lower bound  X   X  (  X  ) on the return using ideas from robust optimization. This lower bound is easy to optimize and can be com-puted from a small sample of the domain, eliminating the need for extensive simulation.
 Maximizing a lower bound on the return corresponds to minimizing an upper bound  X  (  X  ? )  X   X   X  (  X  ) on the policy loss. The main reason to minimize an upper bound X  X s opposed to a lower bound X  X s that the ap-proximation error can be bounded by the difference  X  (  X  ? )  X   X   X  (  X  ? ) for the optimal policy only, instead of the difference for the set of all policies, as we show formally in Section 4.
 The lower bound on the return in DRADP is based on an approximation of the state occupancy distribu-tions or frequencies (Puterman, 2005). The state oc-cupancy frequency represents the fraction of time that is spent in the state and is in some sense the dual of a value function. Occupancy frequencies have been used, for example, to solve factored MDPs (Dolgov &amp; Dur-fee, 2006) and in dual dynamic programming (Wang, 2007; Wang et al., 2008) (The term  X  X ual dynamic pro-gramming X  also refers to unrelated linear stochastic programming methods). These methods can improve the empirical performance, but proving bounds on the policy loss has proved challenging. We take a differ-ent approach to prove tight bounds on the policy loss. While the existing methods approximate the state oc-cupancy frequencies by a subset , we approximate it by a superset .
 We call the DRADP approach distributionally robust because it uses the robust optimization methodology to represent and simplify the set of occupancy distri-butions (Delage &amp; Ye, 2010). Robust optimization is a recently revived approach for modeling uncertainty in optimization problems (Ben-Tal et al., 2009). It does not attempt to model the uncertainty precisely, but in-stead computes solutions that are immunized against its effects. In distributionally robust optimization, the uncertainty is in probability distributions. DRADP introduces the uncertainty in state occupancy frequen-cies in order to make very large MDPs tractable and uses the robust optimization approach to compute so-lutions that are immune to this uncertainty.
 The remainder of the paper is organized as follows. First, in Section 2, we define the basic framework in-cluding MDPs and value functions. Then, Section 3 introduces the general DRADP method in terms of generic optimization problems. Section 4 analyzes ap-proximation errors involved in DRADP and shows that standard concentration coefficient assumptions on the MDP (Munos, 2007) can be used to derive tighter bounds. To leverage existing mathematical program-ming methods, we show that DRADP can be formu-lated in terms of standard mathematical optimization models in Section 5. Finally, Section 6 presents ex-perimental results on standard benchmark problems. Due to space constraints we omit the proofs in this version; please see the extended version for the full proofs (Petrik, 2012).
 We consider the offline X  X r batch X  X etting in this pa-per, in which all samples are generated in advance of computing the value function. This setting is iden-tical to that of LSPI (Lagoudakis &amp; Parr, 2003) and ALP (de Farias &amp; van Roy, 2003). In this section, we define the basic concepts required for solving Markov decision processes: value functions, and occupancy frequencies. We use the following gen-eral notation throughout the paper. The symbols 0 and 1 denote vectors of all zeros or ones of appropri-ate dimensions respectively; the symbol I denotes an identity matrix of an appropriate dimension. The op-erator [  X  ] + denotes an element-wise non-negative part of a vector. We will often use linear algebra and expectation notations interchangeably; for example: E u [ X ] = u T x , where x is a vector of the values of the random variable X . We also use R X to denote the set of all functions from a finite set X to R ; note that R X is trivially a vector space.
 A Markov Decision Process is a tuple ( S , A ,P,r, X  ). Here, S is a finite set of states, A is a finite set of actions, P : S X A X S 7 X  [0 , 1] is the transition function ( P ( s,a,s 0 ) is the probability of transiting to state s from state s given action a ), and r : S  X  A 7 X  R is a reward function. The initial distribution is:  X  : S 7 X  [0 , 1], such that P s  X  X   X  ( s ) = 1. The set of all state-action pairs is W = S  X  A . For the sake of simplicity, we assume that all actions can be taken in all states. To avoid technicalities that detract from the main ideas of the paper, we assume finite state and action sets but the results apply with additional compactness assumptions to infinite sets. We will use S and W to denote random variables with values in S and W .
 The solution of an MDP is a stationary determinis-tic policy  X  : S  X  A , which determines the action to take in any state; the set of all deterministic poli-cies is denoted by  X  D . A stationary randomized X  X r stochastic X  X olicy  X  : S X A X  [0 , 1] assigns the prob-ability to all actions for every state; the set of all ran-domized policies is denoted by  X  R . Clearly  X  D  X   X  R holds by mapping the chosen action to the appropriate distribution. A randomized policy can be thought of as a vector on W that assigns the appropriate proba-bilities to each state X  X ction pair.
 For any  X   X   X  R , we can define the transition probabil-ity matrix and the reward vector as follows: P  X  ( s,s 0 ) = P  X  ( s,a ). We use P a and r a to represent values for a policy that always takes action a  X  X  . We also define a matrix A and a vector b as follows: A
T = I  X   X P T Values A and b are usually used in approximate linear programming (ALP) (Schweitzer &amp; Seidmann, 1985) and linear program formulations of MDPs (Puterman, 2005). The main objective in solving an MDP is to compute a policy with the maximal return.
 Definition 2.1. The return  X  :  X  R  X  R of  X   X   X  R is defined as:  X  (  X  ) = P  X  n =0  X  T (  X   X  P  X  ) n r  X  . The optimal policy solves  X  ?  X  arg max  X   X   X  R  X  (  X  ) and we use  X  ?  X  (  X  ? ).
 DRADP relies on two main solution concepts: state occupancy frequencies and value functions . State oc-cupancy frequencies X  X r measures X  X ntuitively repre-sent the probability of terminating in each state when the discount factor  X  is interpreted as a probability of remaining in the system (Puterman, 2005). State-action occupancy frequencies are defined for state X  action pairs and represent the joint probability of be-ing in the state and taking the action.
 State occupancy frequency for  X   X   X  R is denoted by d  X   X  R S and is defined as: d  X  = (1  X   X  )  X  State-action occupancy frequency is denoted by u  X   X  R
W (its set-valued equivalent is U (  X  )) and is a product of state X  X ccupancy frequencies and action probabili-ties: Note that U (  X  ) is a set-valued function with the out-put set of cardinality 1. State and state-action occu-pancy frequencies represent valid probability measures over S and W respectively. We use d ? = d  X  ? and u ? = u  X  ? to denote the optimal measures. Finally, we use u |  X   X  R S + for  X   X   X  D to denote a restriction of u to  X  such that u |  X  ( s ) = u ( s, X  ( s )).
 State-action occupancy frequencies are closely related to the set U of dual feasible solutions of the linear pro-gram formulation of an MDP, which is defined as (e.g. Section 6.9 of (Puterman, 2005)): The following well-known proposition characterizes the basic properties of the set U .
 Proposition 2.2 (e.g. Theorem 6.9 in (Puterman, 2005)) . The set of occupancy frequencies satisfies the following properties. (ii) For each  X  u  X  U , define  X  0 ( s,a ) = (iii) 1 T u = 1 for each u  X  X  . (iv) A T u = (1  X   X  )  X   X  for each u  X  X  .
 Part (i), in particular, holds because deterministic policies represent the basic feasible solutions of the dual linear program for an MDP.
 A value function v  X   X  R S of  X   X   X  R maps states to the return obtained when starting in them and is defined by: The set of all possible value functions is denoted by V . It is well known that a policy  X  ? with the value function v ? is optimal if and only if v ?  X  v  X  for every  X   X   X  R . The value function update L  X  for a policy  X  and the Bellman operator L are defined as: L  X  v =  X P  X  v + r  X  and Lv = max  X   X   X  R L  X  v .
 The optimal value function v ? satisfies Lv ? = v ? . The following proposition states the well-known connection between state X  X ction occupancy frequencies and value functions.
 Proposition 2.3 (e.g. Chapter 6 in (Puterman, 2005)) . For each  X   X   X  R :  X  (  X  ) = E  X  [ v  X  ( S )] = E u  X  [ r ( W )] / (1  X   X  ) .
 The value function, computed by a dynamic program-ming algorithm, is typically then used to derive the greedy policy. A greedy policy takes in every state an action that maximizes the expected conditional return. Definition 2.4. A policy  X   X   X  D is greedy with re-spect to a value function v when L  X  v = Lv ; in other words:  X  ( s )  X  arg max for each s  X  X  with ties broken arbitrarily.
 MDP is a very general model. Often, specific prop-erties of the MDP can be used to compute better so-lutions and to derive tighter bounds. One common assumption X  X sed to derive L 2 bounds for API X  X s a smoothness of transition probabilities (Munos, 2003), also known as the concentration coefficient (Munos, 2007); this property can be used to derive tighter DRADP bounds.
 Assumption 1 (Concentration coefficient) . There ex-ists a probability measure  X   X  [0 , 1] S and a con-stant C  X  R + such that for all s,s 0  X  S and all  X   X   X  D the transition probability is bounded as: P ( s, X  ( s ) ,s 0 )  X  C  X   X  ( s 0 ) . In this section, we formalize DRADP and describe it in terms of generic optimization problems. Prac-tical DRADP implementations are sampled versions of the optimization problems described in this section. However, as it is common in ADP literature, we do not explicitly analyze the sampling method used with DRADP in this paper, because the sampling error can simply be added to the error bounds that we derive. The sampling is performed and errors bounded identi-cally to approximate linear programming and approx-imate bilinear programming X  X tate and action sam-ples are used to select a subset of constraints and vari-ables (de Farias &amp; van Roy, 2003; Petrik et al., 2010; Petrik &amp; Zilberstein, 2011).
 The main objective of ADP is to compute a policy  X   X   X  R that maximizes the return  X  (  X  ). Because the MDPs of interest are very large, a common approach is to simplify them by restricting the set of policies that are considered to a smaller set  X   X   X   X  R . For example, policies may be constrained to take the same action in some states; or to be greedy with respect to an approximate value function. Since it is not possible to compute an optimal policy, the common objective is to minimize the policy loss. Policy loss captures the difference in the discounted return when following policy  X  instead of the optimal policy  X  ? .
 Definition 3.1. The expected policy loss of  X   X   X  R is defined as: where k X k 1 , X  represents an  X  -weighted L 1 norm. ADP relies on approximate value functions  X  V  X  X  that are a subset of all value functions. In DRADP, ap-proximate value functions are used simultaneously to both restrict the space of policies and to approximate their returns. We, in addition, define a set of approx-imate occupancy frequencies  X  U (  X  )  X  U (  X  ) that are a superset of the true occupancy frequencies. We call any element in the appropriate approximate sets rep-resentable .
 We consider linear function approximation , in which the values for states are represented as a linear combi-nation of nonlinear basis functions (vectors) . For each s  X  S , we define a vector  X  ( s ) of features with |  X  | being the dimension of the vector. The rows of the basis matrix  X  correspond to  X  ( s ), and the approxi-mation space is generated by the columns of  X . Ap-proximate value functions and policy-dependent state occupancy measures for linear approximations are de-fined for some given feature matrices  X  u and  X  v as:  X  U (  X  ) = n u  X  R A + : Clearly,  X  U (  X  )  X  U (  X  ) from the definition of u  X  . We will assume the following important assumption with-out reference for the remainder of the paper.
 Assumption 2. One of the features in each of  X  u and  X  v is a constant; that is, 1 =  X  u x u and 1 =  X  v x v for some x u and x v .
 The following lemma, which can be derived directly from the definition of  X  U and Proposition 2.2, shows the importance of Assumption 2.
 Lemma 3.2. Suppose that Assumption 2 holds. Then for each  X   X   X  R : u  X   X  U (  X  )  X  1 T u = 1 . Approximate policies  X   X  are most often represented indirectly X  X y assuming policies that are greedy to the approximate value functions. The set G of all such greedy policies is defined by: G = {  X   X   X  D : L  X  v = Lv,v  X   X  V } . Although DRADP applies to other ap-proximate policy sets we will particularly focus on the set  X   X  = G .
 We are now ready to define the basic DRADP formu-lation which is analyzed in the remainder of the paper. Definition 3.3. DRADP computes an approximate policy by solving the following optimization problem: arg max where the function  X   X  :  X  R  X  R is defined by: Note that the solution of (DRADP) is a policy; this policy is not necessarily greedy with respect to the op-timal v in (3.3) unlike in most other ADP approaches. The expression (3.3) can be understood intuitively as follows. The first term,  X  T v , represents the expected return if v is the value function of  X  . The second term which offsets any gains when v 6 = v  X  and is motivated by the primal-dual slack variables in the LP formula-tion of the MDP. Given this interpretation, DRADP simultaneously restricts the set of value functions and upper-approximates the penalty function.
 The following theorem states an important property of Definition 3.3, which is used to derive approximation error bounds.
 Theorem 3.4. For each  X   X   X  R ,  X   X  lower-bounds the true return:  X   X  (  X  )  X   X  (  X  ) . In addition, when  X   X  v are invertible and  X   X   X  D then  X  (  X  ) =  X   X  (  X  ) . We now show that the lower bound  X   X  in (3.3) can be simplified in some cases by ignoring the value functions for any  X   X   X  R ; the formulation (3.3) will neverthe-less be particularly useful in the theoretical analysis because it relates value functions and occupancy fre-quencies. where  X  U 0 (  X  ) is defined equivalently to  X  U (  X  ) with the exception that  X  u =  X  v =  X  for some  X .
 Proposition 3.5. When  X  v =  X  u , then  X   X  (  X  ) =  X   X  0 When  X  v 6 =  X  u , then define  X  U 0 and  X   X  0 using a new representation  X  0 = [ X  v  X  u ] . Then:  X   X  (  X  ) =  X   X  For the remainder of the paper assume that  X  v =  X  u since assuming that they are the same does not reduce the solution quality.
 A potential challenge with DRADP is in representing the set of approximate policies  X   X , because a policy must generalize to all states even when computed from a small sample. Note, that for a fixed value function necessarily the greedy with respect to v . The following representation theorem, however, shows that when the set of representable policies  X   X  is sufficiently rich, then the computed policy will be greedy with respect to a representable value function.
 Theorem 3.6. Suppose that  X   X   X  X  . Then: (i) max  X   X   X   X   X   X  (  X  ) = max  X   X  X   X   X  (  X  ) . (ii)  X   X   X   X  arg max  X   X   X   X   X   X  (  X  ) such that  X   X   X  X  . Note that the assumption  X   X   X  G simply implies that DRADP can select a policy that is greedy with respect to any approximate value function v  X   X  V . This is an implicit assumption in many ADP algorithms, includ-ing ALP and LSPI. We state the assumption explicitly to indicate results that do not hold in case there are additional restrictions on the set of policies that is con-sidered.
 Theorem 3.6 implies that it is only necessary to con-sider policies that are greedy with respect to repre-sentable value functions which is the most common approach in ADP. However, other approaches for rep-resenting policies may have better theoretical or em-pirical properties and should be also studied. This section describes the a priori approximation properties of DRADP solutions; these bounds can be evaluated before a solution is computed. We focus on several types of bounds that not only show the per-formance of the method, but also make it easier to theoretically compare DRADP to existing ADP meth-ods. These bounds show that DRADP has stronger theoretical guarantees than most other ADP methods. The first bound mirrors some simple bounds for ap-proximate policy iteration (API) in terms of the L  X  norm (Munos, 2007): lim sup where  X  k and k are the policy and L  X  approximation error at iteration k .
 Theorem 4.1. Suppose that  X   X   X  G and that  X   X   X  arg max  X   X   X   X   X   X  (  X  ) in (DRADP) . The policy loss  X   X  (  X   X  ) is then bounded as: Theorem 4.1 highlights several advantages of the DRADP bound (4.2) over (4.1): 1) it bounds the ex-pected loss k v ?  X  v  X   X  k 1 , X  instead of the worst-case loss k v ?  X  v  X   X  k  X  , 2) it is smaller by a factor of 1 / (1  X   X  ), 3) it holds in finite time instead of a limit, and 4) its right-hand side is with respect to the best approxima-tion of the optimal value function instead of the worst case approximation over all iteration. In comparison with approximate linear programming bounds, (4.2) bounds the true policy loss and not simply the ap-proximation of v ? (de Farias &amp; van Roy, 2003). The limitation of (4.2), however, is that it relies on an L  X  norm which can be quite conservative. We address this issue in two ways. First, we prove a bound of a different type.
 Theorem 4.2. Suppose that  X   X   X  G and that  X   X   X  arg max  X   X   X   X   X   X  (  X  ) in (DRADP) . Then, the policy loss  X  ?  X   X  (  X   X  ) is bounded as: The bound (4.3), unlike bounds in most ADP algo-rithms, does not contain a factor of 1 / (1  X   X  ) of any power. Although (4.3) does not involve an L  X  norm, it does require that v  X  v ? which may be undesirable. Next, we show bounds that rely purely on weighted norms under additional assumptions on the concen-tration coefficient.
 As mentioned above, Assumption 1 can be used to improve the solutions of DRADP and to derive tighter bounds. Note that this assumption must be known in advance and cannot be gleaned from the samples. To this end, for some fixed C  X  R + and  X   X  R S in Assumption 1, define:  X  U
S (  X  ) = These assumptions imply the following structure of all admissible state frequencies.
 Lemma 4.3. Suppose that Assumption 1 holds with constants C and  X  . Then: d  X  C  X   X  for each d  X   X  U S (  X  ) and  X   X   X  R .
 The following theorem shows a tighter bound on the DRADP policy loss for MDPs that satisfy the smooth-ness assumption.
 Theorem 4.4. Suppose that Assumption 1 holds with constants C and  X  ,  X   X   X  G , and that  X   X   X  is bounded as: The bound in Theorem 4.4 is similar to comparable L p bounds for API (Munos, 2003), except it relies on a weighted L 1 norm instead of the L 2 norm and pre-serves all the advantages of Theorem 4.1. Theorem 4.4 exploits that the set of occupancy frequencies is re-stricted under the smoothness assumption which leads to a tighter lower bound  X   X  S on the return. Finally, DRADP is closely related to robust ABP (Petrik &amp; Zilberstein, 2009; 2011) but provides several significant advantages. First, DRADP does not require transitive feasible (Petrik &amp; Zilberstein, 2011) value functions, which simplifies the use of constraint generation. Second, ABP minimizes L  X  bounds  X  r :  X 
R  X  R on the policy loss, which can be too conserva-tive. In fact, it is easy to show that DRADP solutions can be better than ABP solutions by an arbitrarily large factor. In this section, we describe how to solve the DRADP optimization problem. Since DRADP generalizes ABP (Petrik &amp; Zilberstein, 2009), it is necessarily NP complete to solve in theory, but relatively easy to solve in practice. Note that the NP-completeness is in terms of the number of samples and features and not in the number of states or actions of the MDP. In addition, the NP completeness a is favorable prop-erty when compared to API algorithms, such as LSPI, which may never converge (Lagoudakis &amp; Parr, 2003). To solve DRADPs in practice, we derive bilinear and mixed integer linear program formulations for which many powerful solvers have been developed. These formulations lead to anytime solvers X  X ven approxi-mate solutions result in valid policies X  X nd can there-fore easily trade off solution quality with time com-plexity.
 To derive bilinear formulations of DRADP, we repre-sent the set of policies  X   X  using linear equalities as:  X   X  =  X   X  [0 , 1] W : P a  X  X   X  ( s,a ) = 1 . This set can be defined using matrix notation as B X  = 1 and  X   X  0 , where B : |S| X |W| is defined as: B ( s 0 , ( s 0 ,a )) = 1 when s = s 0 and 0 otherwise. Clearly  X   X   X  X  , which im-plies that the computed policy is greedy with respect to a representable value function from Theorem 3.6 even as sampled. It would be easy to restrict the set  X  by assuming the same action must be taken in a subset of states: one would add constraints  X  ( s,a ) =  X  ( s 0 for some s,s 0  X  X  and all a  X  X  .
 When the set of approximate policies is represented by linear inequalities, the DRADP optimization problem can be formulated as the following separable bilinear program (Horst &amp; Tuy, 1996).
Bilinear programs are a generalization of linear pro-grams and are in general NP hard to solve.
 Theorem 5.1. Suppose that  X   X   X  X  . Then the sets of optimal solutions of (5.1) and (DRADP) are identical and there exists an optimal solution (  X   X ,  X   X  1 ,  X   X  such that: (i)  X   X  is deterministic and greedy with respect to  X   X  (ii)  X   X  T  X   X  2 = 0 .
 Because there are few, if any, industrial solvers for bilinear programs, we reformulate (5.1) as a mixed integer linear program (MILP). Any separable bilin-ear program can be formulated as a MILP (Horst &amp; Tuy, 1996), but such generic formulations are imprac-tical because they lead to large MILPs with weak lin-ear relaxations. Instead, we derive a more compact and structured MILP formulation that exploits the existence of optimal deterministic policies in DRADP (see (i) of Theorem 5.1) and is based on McCormic inequalities on the bilinear terms (Linderoth, 2005). To formulate the MILP, assume a given upper bound  X   X  R on any optimal solution  X  2 ? of (5.1) such that  X   X   X  2 ? ( s,a ) for all s  X  X  and a  X  X  . Then: Theorem 5.2. Suppose that  X   X   X  G and (  X   X ,  X   X  1 ,  X  is an optimal solution of (5.2) . Then, (  X   X ,  X   X  1 ,  X  optimal solution of (5.1) with the same objective value given that  X  &gt; k  X   X  2 k  X  .
 As discussed above, any practical implementation of DRADP must be sample-based. The bilinear program (5.1) is constructed from samples very similarly to ALPs (e.g. Sec 6 of (de Farias &amp; van Roy, 2003)) and identically to ABPs (e.g. Sec 6 of (Petrik &amp; Zil-berstein, 2011)). Briefly, the formulation involves only the rows of A that correspond to transitions of sampled state-action pairs and b entries are estimated from the corresponding rewards. As a result, there is one  X  variable for each feature, and  X  2 and  X  are nonzero only for the sampled rows of A (zeros do not need to be considered). The size of the optimization problem (5.1) is then independent of the number of states and actions of the MDP; it depends only on the number of samples and features. In this section, we experimentally evaluate the empiri-cal performance of DRADP. We present results on the inverted pendulum problem X  X  standard benchmark problem X  X nd a synthetic chain problem. We gather state and action samples in advance and solve MILP (5.2) using IBM CPLEX 12.2. We then compare the results to three related methods which work on of-fline samples: 1) LSPI (Lagoudakis &amp; Parr, 2003), 2) ALP (de Farias &amp; van Roy, 2003), and 3) ABP (Petrik &amp; Zilberstein, 2009). While solving the MILP formula-tion of DRADP is NP hard (in the number of features and samples), this does not mean that the computa-tion takes longer than for other ADP methods; for ex-
Expected Return ample, the computation time of LSPI is unbounded in the worst case (there are no convergence guarantees). In the experiments, we restrict the computation time for all methods to 60s.
 Inverted Pendulum The goal in the inverted pen-dulum benchmark problem is to balance an inverted pole by accelerating a cart in either of two direc-tions (Lagoudakis &amp; Parr, 2003). There are three ac-tions that represent applying the force of u =  X  50 N , u = 0 N , and u = 50 N to the cart with a uniform noise between  X  10 N and 10 N . The angle of the inverted pendulum is governed by a differential equation. We used the standard features for this benchmark problem for all the methods: 9 radial basis functions arranged in a grid over the 2-dimensional state space with cen-ters  X  i and a constant term required by Assumption 2. The problem setting, including the initial distribution is identical to the setting in (Lagoudakis &amp; Parr, 2003). Fig. 1 shows the number of balancing steps (with a 3000-step bound) for each method as a function of the number of training samples averaged over 5 runs. The figure does not show error bars for clarity; the vari-ance was close to 0 for DRADP. The results indicate that DRADP computes a very good solution for even a small number of samples and significantly outperforms LSPI. Note the poor performance of ABP and ALP with the 10 standard features; better results have been obtained with large and different feature spaces (Petrik et al., 2010) but even these do not match DRADP. The solution quality of ABP decreases with more samples, because the bounds become more conservative and the optimization problems become harder to solve.
 Chain Problem Because the solution quality of ADP methods depends on many factors, good results on a single benchmark problem do not necessarily generalize to other domains. We, therefore, compare DRADP to other methods on a large number of ran-domly generated chain problems. This problem con-sists of 30 states s 1 ...s 30 and 2 actions: left and right with 10% chance of moving the opposite way. The features are 10 orthogonal polynomials. The re-wards are 0 except: r ( s 2 ) =  X  50 , r ( s 3 ) = 4 , r ( s  X  50 , r ( s 20 ) = 10. Fig. 2 shows the results of 1000 instantiations with randomly chosen initial distribu-tions and indicates that DRADP significantly outper-forms other methods including API (a simple version of LSPI). This paper proposes and analyzes DRADP X  X  new ADP method. DRADP is based on a mathematical optimization formulation X  X ike ALP X  X ut offers sig-nificantly stronger theoretical guarantees and better empirical performance. The DRADP framework also makes it easy to improve the solution quality by in-corporating additional assumptions on state occupa-tion frequencies, such as the small concentration co-efficient. Given the encouraging theoretical and em-pirical properties of DRADP, we hope it will lead to better methods for solving large MDPs and will help to deepen the understanding of ADP.
 Acknowledgements I thank Dan Iancu and Dhar-mashankar Subramanian for the discussions that in-spired this paper. I also thank the anonymous ICML 2012 and EWRL 2012 reviewers for their detailed com-ments.

