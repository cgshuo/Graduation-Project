 Knowledge discovery in databases (KDD) is the non-trivial process of identifying valid, novel, potentially useful, and understandable patterns. Detecting patterns effec-tively and efficiently is a challenging task since these patterns usually reside in large and the underlying clustering structure may change whereby previously uncovered react quickly to dynamic changes in the data patterns is highly desirable. 
One of the primary data analysis tasks in KDD is cluster analysis. The main goal of a clustering algorithm is to partition a set of data points into groups such that similar There are two main kinds of clustering algorithms: partitioning and hierarchical. Par-clustering algorithms like the OPTICS [2] or Single-Link method [3] compute a rep-resentation of the possible hierarchical clustering structure of the database in the form of a dendrogram or a reachability plot from which clusters at various resolutions can be extracted, as has been shown in [4]. lot of attention in recent years. One approach for scaling up a clustering algorithm is whole database. In data summarization methods such as Data Bubbles [5] and BIRCH represents its elements by a number of sufficient statistics. A modified version of the the interesting patterns. For example, OPTICS [2] was shown to uncover the cluster-ing structure effectively and very efficiently from data bubbles [5]. 
Various dynamic updates of deletions and insertions to very large databases add new challenges to the clustering task by possibly changing the underlying data distri-reapply the data mining algorithms and extract the hidden patterns every time follow-tively slow for fast changing and large databases, especially if an up-to-date clustering structure is required frequently, e.g., in order to detect the changes in the data distri-bution after a small fraction of updates occur and important decisions are based on the summaries of dynamic databases. 
There are two main strategies to address the problem of incremental clustering in a database environment. In the first strategy, a specialized incremental clustering algo-strategy, a data summarization technique is developed and used to compress the data-subsequently applied to the generated data summarizations. several incremental clustering algorithms that do not use the data summarization tech-nique but attempt to directly restructure the clusters to reflect the dynamic changes of the dataset. 
Chen et al. [7] propose the incremental hierarchical clustering algorithm GRIN for numerical datasets, which is based on gravity theory in physics. Ester et al. [8] present a new incremental clustering algorithm called Incremental DBSCAN suitable for mining in a data-warehousing environment. Incremental DBSCAN is based on the DBSCAN algorithm [9] which is a density based clustering algorithm. However, the proposed method does not address the problem of changing point densities over time, which would require adapting the input parameters for Incremental DBSCAN over time. Widyantoro et al. [10] present the agglomerative incremental hierarchical clus-tering (IHC) algorithm that utilizes a restructuring process while preserving homoge-introduce new deterministic and randomized incremental clustering algorithms while trying to minimize the maximum diameters of the clusters. 
Unlike the above algorithms that typically invent yet another  X  X ew X  incremental algorithm for a particular application, the second strategy is more flexible and generic as it allows the application of a broad range of existing standard clustering algorithms clustering algorithm to data summarization typically requires only minor modifica-tions, as has been shown in [5]. It also has the advantage that the data summaries can be used for other data mining tasks such as computing approximate statistics of data tribute ranges of interest. 
Samer et al. [12] use the second approach and propose a scheme to incrementally maintain data summaries of a dynamic database, i.e., they enhance data summariza-database. Furthermore, by using a measure of the compression quality called data bubble), they can identify the data bubbles that still compress their points well follow-ing the insertions and the deletions. 
For the scheme proposed by Samer et al. [12], there are some remaining works needed to solve. First, they use data summarization index as the measure of the com-Using Chebyshev X  X  Inequality needs to know the mean and standard deviation of the data summarization index. But they do not give an analysis about how to get the mean they can not deal with the situations where the number of data points or data bubbles is changed. And they only consider where there is an equal number of insertions and deletions for each update and the number of data bubbles are unchanged. This is be-cause they can not judge which data bubbles are not good if the number of points or incremental update of data bubbles. background related to the problems of incremental data summarization. Then we show how to get the theoretic mean and give some statements about the standard deviation of the data summarization index. Also we propose an algorithm about fully dynamical incremental data bubble maintena nce. In section 3, we perform an exten-data summarization for hierarchical clustering. The conclusions and some future di-rections are presented in section 4. Previously it has been shown that for hierarchical clustering algorithms, the data bub-bles [5] are much more effective than basic clustering features CF =( n, LS, SS ), where special sufficient statistics that are required for effective hierarchical clustering based on data summarizations. Data bubbles have been evaluated in [5], using OPTICS [2], and were shown to reduce the runtime of OPTICS dramatically while still producing high-quality hierarchical clustering structures. A data bubble has been defined as follows: n, extent, nnDist ) where
Although the information in a data bubble is more specialized than the basic suffi-age nearest neighbor distances nnDist ( k,B ) can be easily derived from n , LS , SS . 
The method that has been proposed to construct data bubbles consists of the fol-lowing two steps: 1. Retrieve randomly s points from the database as  X  X eeds X . 2. Scan the database, and assign each point in the database to the closest seed in the set obtained in step 1. 
We assume that we have initially constructed a set of data bubbles that summarize a large database of d -dimensional points following the above description. If the data-base is dynamic, new points are inserted and old points are deleted over time, possibly changing the underlying data distribution. We are interested in the updated clustering structure and hence the underlying data summarization after a set of updates. 
For incrementally updating a set of data bubbles following a batch of updates to the underlying database, the sufficient statistics of affected data bubbles are decremented previously assigned are updated to ( n-1 , LS-p , SS-p 2 ), whereas when inserting a point dated to ( n+1 , LS+p , SS+p 2 ). points well or lost all of their points such that the overall compression quality is poor, order to recover from structural distortions due to changes in the data distribution, we summarization and re-build them quickly, while at the same time maintaining a given compression rate. 
The measure for determining the quality of a data bubble is the number of points it summarizes relative to the total database size. Roughly speaking,  X  X ood X  data bubbles summarize not too many and not too few points. Samer et al. [12] introduce the data summarization index  X  to capture the quality of a data bubble. presses n points is defined as  X  i = n / N . 
Based on Chebyshev's Inequality theorem: 2 1 1 ) | | ( distinguishes three classes of data bubbles according to their compression quality. distribution of the  X  values for all data bubbles in  X  . Given a probability p (where the corresponding k value is computed according to Chebyshev's Inequality), a data bub-ble B with the data summarization index  X  is called: Figure 1 shows the pseudo code for improving the quality of an over-filled data bub-ble [12] while keeping the number of data bubbles unchanged. The quality of B over-filled
DevideOverFilledBubble() { none exists, select the  X  X ood X  data bubble with lowest quality in the  X  X ood X  data bubbles set); closest data bubble(s); by selecting a new seed s 1 for it from the points of B over-B and s 2 ; } 
Although Samer et al. [12] use the data summarization index to judge the quality of data bubbles, they do not give a further analysis about how to get the mean and stan-dard deviation of it when the number of points or data bubbles are changed. In their increase or decrease the number of data bubbles is needed to solve to further improve the compression of a database. They assume that when building data bubbles from initial data bubble distribution to compute the mean and standard deviation of the data applications, with the dynamic update of deletions and insertions to database, the total change of database, we also need to change the number of data bubbles. At this time, useless. So we will give further explore about the mean and standard deviations of the data summarization index. We will show which factors can affect the mean and stan-dard deviation and which can not. 
We illustrate how to compute the mean of data summarization index in theory, and also give some statements about the relationships between standard deviation and some database factors. In the next section, we present the detail experiments support-namic incremental data summarization maintenance algorithm in this section. Samer et al. [12] use experiment to evaluate the mean of data summarization index. In this paper, we give a theorem to show how to compute the mean of data summari-zation index in theory. Theorem 1. Given a database D of N points and a set  X  of s data bubbles that com-bubbles in  X  . . There is  X   X  = 1/s. definition of mean, there is  X   X  =  X  = 1/s.  X  proportional to it. If we know the number of data bubbles in advance, we can compute dramatically changes. 
Next we give analysis and statements about the standard deviation of data summa-1 actual number of points in each data bubble, so it is difficult to give a theoretical value independent to each sampling process. But we can give some statements about which statements. In the next section, we will use experiments to confirm our statements. 
First, we show the relationships between  X   X  and the number of points, the distribu-tion of points, and the dimensions of points. Statement 1: Given a database D ,  X   X  is the standard deviation of the data summariza-distribution of points, and the dimensions of points in the database. not related to the number of points in database. We know that  X  represents the fraction Statement 1 also says that  X   X  is independent to the distribution of points, which means that the clustering structure can not affect  X   X  . In paper [12], although they do not say explicitly, they just use this fact. In their experiments, they do not change the number of points in database, but they change the underlying data distribution and the associ-ated clustering structure when updating database. We can think that where data distri-related to the data distribution. In statement 1, we also argue that  X   X  is independent to IncrementalDataBubble ( ) 
Input: A batch of data update, original data bubble number m , new data bubble number n { tics of the corresponding data bubbles; data bubbles; ber of points; ber of points; closest data bubble(s); boundary for the data bubble quality; 1; } world applications, it is common to change the number of data points and data distri-confirm the arguments in statement 1. 
Although it is hard to give a precise value about  X   X  , we can use experiments to es-timate it. In this section, we first give the statement about how to compute  X   X  , and in the next section, we use experiments to confirm this statement.  X  = 0.546 /s.

Statement 2 claims that  X   X  is also only decided by the number of data bubbles and it is inverse proportional to s . And the correlation coefficient is 0.546. Using theorem 1, the above formula in statement 2 can also be written as  X   X  = 0.546  X   X  . 
Using these statements and theorem, we can give a fully dynamic incremental data bubble maintenance algorithm. This algorithm can deal with the change to the number of points or data bubbles, which can not be dealt with in paper [12]. We can dynami-cally increase or decrease the number of data bubbles or keep it unchanged when the original database is updated. data bubbles according to data deleting and inserting. If need to change the number of data bubbles, we increase or decrease the number of data bubbles to meet the condi-tion of new data bubble number. Next, according to the new data bubble number we compute the new  X   X  and  X   X  based on theorem 1 and statement 2. And we use the new  X  and  X   X  to decide which data bubbles are good or over-filled. The sequence of syn-base with each batch of insertions and deletions. sented in section 2, and show the efficiency of our new method for incremental data values of 10 repetition tests. 
First we give an experiment to illustrate that the standard deviation of data summa-rization index is independent to the number of points. Figure 3 shows the relationship between the number of points and standard deviation. We use a 2-dimensional data-theorem 1, the mean of data summarization index is 0.01. In figure 3, the trend of the standard deviation with the increase of the number of points is nearly a straight line. 
Furthermore, we use the following formula to define the difference between the maximum and minimum standard deviation. And the difference can also give us some difference in figure 3 is about 4.6%. Thus, we can say that the standard deviation of data summarization index is independent to the number of points, which confirms our statement 1. 
Figure 4 gives the relationship between the dimensions of points and standard de-summarization index is 0.01. In figure 4, the trend of the standard deviation with the increase of the dimensions is also nearly a straight line. The maximum and minimum which confirms our statement 1. Because for most real world applications, the dimen-sions of point rarely change, so we can ignore the effect of this factor when incremen-tally building data bubbles. 
Figure 5 gives the relationship between the distribution of points and standard de-2-dimensional database. In this experiment, we generate seven types of data distribu-maximum and minimum standard deviation X  X  difference is about 1.7%. Thus, we can distribution of points, which confirms our statement 1. Also, we confirm paper [12] X  X  assumption, where they use this conclusion but not state explicitly. 
Now we discuss the relationship between the number of data bubbles and standard deviation. Figure 6 gives the line representing the multiplication of standard deviation (  X  ) and the number of data bubbles ( s ). In this experiment, we use 50000 2-dimensioal points which are randomly generated. Figure 6 shows that (bubble  X  Therefore the statement 2 in section 2 is acquired. 
Above experiments confirm the statement 1 and 2 in section 2. Thus, when we maintain data bubbles incrementally, we know which factors will affect the quality of data bubbles. And using these conclusions we can dynamically build data bubbles. effectiveness of the incremental data bubbles by studying their effect on the perform-ance of a clustering algorithm relative to its performance when using completely rebuilt data bubbles. After each batch of update, we summarize each database of the plots of the completely rebuilt and incremental clustering structures. The clusters are extracted from these plots using a modified version of an automatic method devel-oped in [4]. The performance of OPTICS is determined using the F score measure [13] (where F = 2*p*r/(p+r), p is precision and r is recall). 
In table 1, we give the experimental results about the F score comparison between complete rebuilt and incremental update. The number of points in database is in-tional to the number of points, where the compression factor is 100, which means the number of data bubbles is increased from 100 to 1000. Therefore, in this experiment, the numbers of points and data bubbles are all changed, which can not be dealt with in paper [12]. 
We notice from Table 1 that the F score of the clustering algorithm (OPTICS) us-scheme for dynamically maintaining the incremental data bubbles is effective in pre-serving the quality of the clustering algorithm as measured by the F score. dynamic scheme for incrementally maintaining data summarization. We show that the points, and the dimensions of points in the database. It is only related to the number of data bubbles and inverse proportional to it. An extensive experimental evaluation for various cases confirm our statements, and show that the incremental data bubbles provide an efficient data summarization technique for dynamically changing large databases, and is effective in preserving the quality of the clustering algorithm. 
