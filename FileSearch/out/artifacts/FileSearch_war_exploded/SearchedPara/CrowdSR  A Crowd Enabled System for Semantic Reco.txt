 be used to find related tables and learn binary relationships between multiple col-umns[1]. But most of the web tables are lack of header rows[2]. State-of-the-art tech-accuracy[3][4]. We develop a hybrid machine-crowdsourcing framework that lever-ages human intelligence to settle the problem. CrowdSR has the following unique features. (1) CrowdSR presents the worker a small number of representative tuples in the table by clustering based on novel integrative distance. (2) CrowdSR prompts workers with candidate lists of concepts by machine-based algorithms. (3) CrowdSR implements an evaluation mechanism on Answer Credibility to recommend the most related tasks for workers and decide the final answers more accurately. 2.1 CrowdSR Architecture CrowdSR is implemented in JSP with SQL Server database as back-end. Fig.1 depicts the system architecture. 
User Interface is used to interact with users. Basically, DB stores answers collected from the crowd, targeting information and details about each user and task. Task builder receives requests from requesters and build tasks. Crowd Manager constantly receives an updated list of online workers from targeting information in DB. Once a worker logs in, the Task Recommender recommends a list of tasks for him based on 
A web table is composed of columns with various data types. In order to make the 2.4 Evaluation Mechanism Based on Answer Credibility which are the setting of expertise, brilliance test and practical performance. 
When a new worker registers in CrowdSR, he X  X  required to set his expertise and join the brilliance test to evaluate his acquaintance with fields. Then his answer credi-bility is dynamically changed with the practical performance of doing tasks. We es-recommend related tasks for workers and decide the final answers for each task. We plan to demonstrate the CrowdSR system with the following scenario: required to assign several fields for each task. Semantic Annotation: Workers complete tasks via a question-choice game where could check the alteration of his answer credibility based on his performance. Brilliance Test: When a new worker registers in CrowdSR, he X  X  invited to participate credibility on each field by a histogram. 
