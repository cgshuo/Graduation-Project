 In reinforcement learning, an agent interacts with the environment, learning through trial-and-error based on scalar reward signals. Many reinforcement learning algorithms estimate values for states to enable selection of maximally rewarding actions. Obtaining confidence intervals on these estimates has been shown to be useful in practice, including directing exploration [17, 19] and deciding when to exploit learned models of the environment [3]. Moreover, there are several potential applications using confidence estimates, such as teaching interactive agents (using confidence estimates as feed-back), adjusting behaviour in non-stationary environments and controlling behaviour in a parallel multi-task reinforcement learning setting.
 Computing confidence intervals was first studied by Kaelbling for finite-state Markov decision pro-cesses (MDPs) [11]. Since this preliminary work, many model-based algorithms have been proposed for evaluating confidences for discrete-state MDPs. The extension to continuous-state spaces with model-free learning algorithms, however, has yet to be undertaken. In this work we focus on con-structing confidence intervals for online model-free reinforcement learning agents.
 The agent-environment interaction in reinforcement learning does not satisfy classical assumptions typically used for computing confidence intervals, making accurate confidence estimation challeng-ing. In the discrete case, certain simplifying assumptions make classical normal intervals more appropriate; in the continuous setting, we will need a different approach.
 The main contribution of this work is a method to robustly construct confidence intervals for approx-imated value functions in continuous-state reinforcement learning setting. We first describe boot-strapping , a non-parametric approach to estimating confidence intervals from data. We then prove that bootstrapping can be applied to our setting, addressing challenges due to sample dependencies, changing policies and non-stationarity (because of learning). Then, we discuss how to address com-plications in computing confidence intervals for sparse or local linear representations, common in reinforcement learning, such as tile coding, radial basis functions, tree-based representations and sparse distributed memories. Finally, we propose several potential applications of confidence inter-vals in reinforcement learning and conclude with an empirical investigation of the practicality of our confidence estimation algorithm for exploration, tuning the temporal credit parameter and tracking. Kaelbling was the first to employ confidence interval estimation method for exploration in finite-state MDPs [11]. The agent estimates the probability of receiving a reward of 1.0 for a given state-action pair and constructs an upper confidence bound on this estimate using a Bernoulli confidence interval. Exploration is directed by selecting the action with the highest upper confidence bound, which corresponds to actions for which it has high uncertainty or high value estimates [11]. Interval estimation for model-based reinforcement learning with discrete state spaces has been quite extensively studied. Mannor et al. (2004) investigated confidence estimates for the parameters of the learned transition and reward models, assuming Gaussian rewards [5, 16]. The Model Based Interval Estimation Algorithm (MBIE) uses upper confidence bounds on the model transition probabilities to select the model that gives the maximal reward [22]. The Rmax algorithm uses a heuristic notion of confidence (state visitiation counts) to determine when to explore, or exploit the learned model [3]. Both Rmax and MBIE are guaranteed to converge to the optimal policy in polynomially many steps. These guarantees, however, become difficult for continuous state spaces.
 A recently proposed framework, KWIK ( X  X nows What It Knows X ), is a formal framework for algo-rithms that explore efficiently by minimizing the number of times an agent must return the response  X  X  do not know X  [23]. For example, for reinforcement learning domains, KWIK-RMAX biases ex-ploration toward states that the algorithm currently does not  X  X now X  an accurate estimate of the value [23]. KWIK-RMAX provides an uncertainty estimate (not a confidence interval) on a linear model by evaluating if the current feature vector is contained in the span of previously observed feature vectors. Though quite general, the algorithm remains theoretical due to the requirement of a solution to the model.
 Bayesian methods (e.g., GPTD [6]) provide a natural measure of confidence: one can use the poste-rior distribution to form credible intervals for the mean value of a state-action pair. However, if one wants to use non-Gaussian priors and likelihoods, then the Bayesian approach is intractable without appropriate approximations. Although this approach is promising, we are interested in computing classical frequentist confidence intervals for agents, while not restricting the underlying learning algorithm to use a model or particular update mechanism.
 Several papers have demonstrated the empirical benefits of using heuristic confidence estimates to bias exploration [14, 17, 19] and guide data collection in model learning [9, 18]. For example, Nouri et al. [19] discretize the state space with a KD-tree and mark the state as  X  X nown X  after reaching a visitation count threshold.
 model-free, online reinforcement learning value estimates in the continuous-state setting. In this section, we will introduce the reinforcement learning model of sequential decision making and bootstrapping, a family of techniques used to compute confidence intervals for means of depen-dent data from an unknown (likely non-normal) underlying distribution. 3.1 Reinforcement Learning In reinforcement learning, an agent interacts with its environment, receiving observations and se-lecting actions to maximize a scalar reward signal provided by the environment. This interaction is usually modeled by a Markov decision process (MDP). An MDP consists of ( S,A,P,R ) where S is the set of states; A is a finite set of actions; P , the transition function, which describes the probability which returns a scalar value for transitioning from state-action ( s,a ) to state s 0 . The state of the agent X  X  objective is to learn a policy ,  X  : S  X  A , such that R is maximized for all s  X  S . Many reinforcement learning algorithms maintain an state-action value function , Q  X  ( s,a ) , equal to the expected discounted sum of future rewards for a given state-action pair: Q  X  ( s,a ) = E The optimal state-action value function, Q  X  ( s,a ) , is the maximum achievable value given the agent starts in state s and selects action a . The optimal policy,  X   X  , is greedy with respect to the opti-mal value function:  X   X  ( s ) = argmax a  X  A Q  X  ( s,a ) for all s  X  S . During learning the agent must balance selecting actions to achieve high reward (according to  X  Q ( s,a ) ) or selecting actions to gain more information about the environment. This is called the exploration-exploitation trade-off. In many practical applications, the state space is too large to store in a table. In this case, a function approximator is used to estimate the value of a state-action pair. A linear function approximator produces a value prediction using a linear combination of basis units:  X  Q ( s,a ) =  X  T  X  ( s,a ) . We refer the reader to the introductory text [25] for a more detailed discussion on reinforcement learning. 3.2 Bootstrapping a confidence interval for dependent data sample mean), particularly when the underlying distribution is complicated or unknown, samples are dependent and power calculations (e.g. variance) are estimated with limited sample sizes [21]. This estimate can then be used to approximate a 1  X   X  confidence interval around the statistic: an interval for which the probability of seeing the statistic outside of the interval is low (probability  X  ). For example, for potentially dependent data sampled from an unknown distribution P ( X 1 ,X 2 ,... ) , we can use bootstrapping to compute a confidence interval around the mean, T n = n  X  1 P n i =1 x n . The key idea behind bootstrapping is that the data is an appropriate approximation, P n , of the true distribution: resampling from the data represents sampling from P n . Samples are  X  X rawn X  from P n example, can be used to estimate Var P ( T n )  X  Var P n ( T n ) = P ( T  X  n,b  X  T  X  n ) 2 / ( B  X  1) . Bootstrapped intervals have been shown to have a lower coverage error than normal intervals for dependent, non-normal data. A normal interval has a coverage error of O (1 / strapping has a coverage error of O ( n  X  3 / 2 ) [29]. The coverage error represents how quickly the estimated interval converges to the true interval: higher order coverage error indicates faster con-vergence 1 . Though the theoretical conditions for these guarantees are somewhat restrictive [29], bootstrapping has nevertheless proved very useful in practice for more general data [4, 21]. With the bootstrapped samples, a percentile-t (studentized) interval is constructed by tion of size n is the continuous sample quantile: where m is dependent on quantile type, with m =  X  +1 3 common for non-normal distributions. The remaining question is how to bootstrap from the sequence of samples. In the next section, we describe the block bootstrap, applicable to Markov processes, which we will show represents the structure of data for value estimates in reinforcement learning. 3.2.1 Moving Block Bootstrap In the moving block bootstrap method, blocks of consecutive samples are drawn with replacement from a set of overlapping blocks, making the k -th block { x k  X  1+ t : t = 1 ...,l } . The bootstrap resample is the concatenation of n/l blocks chosen randomly with replacement, making a time series of length n ; B of these concatenated resamples are used in the bootstrap estimate. The block bootstrap is appropriate for sequential processes because the blocks implicitly maintain a time-dependent structure. An common heuristic for the block length, l , is n 1 / 3 [8]. The moving block bootstrap was designed for stationary, dependent data; however, our scenario involves nonstationary data. Lahiri [12] proved a coverage error of o ( n  X  1 / 2 ) when applying the moving block bootstrap to nonstationary, dependent data, better than the normal coverage error. Fortunately, the conditions are not restrictive for our scenario, described further in the next section. Note that there are other bootstrapping techniques applicable to sequential, dependent data with lower coverage error, such as the double bootstrap [13], block-block bootstrap [1] and Markov or Sieve bootstrap [28]. In particular, the Markov bootstrap has been shown to have a lower cover-age error for Markov data than the block bootstrap under certain restricted conditions [10]. These techniques, however, have not been shown to be valid for nonstationary data. In this section, we present a theoretically sound approach to constructing confidence intervals for parametrized Q ( s,a ) using bootstrapping for dependent data. We then discuss how to address sparse representations, such as tile coding, which make confidence estimation more complicated. 4.1 Bootstrapped Confidence Intervals for Global Representations The goal is to compute a confidence estimate for Q ( s t ,a t ) on time step t . Assume that we are learning a parametrized value function Q ( s,a ) = f (  X  ,s,a ) , with  X   X  R d and a smooth function f : R d  X  S  X  A  X  R . A common example is a linear value function Q ( s,a ) =  X  T  X  ( s,a ) , with  X  : S  X  A  X  R d . During learning, we have a sequence of changing weights, {  X  1 ,  X  2 ,...,  X  n } up to time step n , corresponding to the random process {  X  1 ,...,  X  n } . If this process were stationary, then we could compute an interval around the mean of the process. In almost all cases, however, the process will be nonstationary with means {  X  1 ,..., X  n } . Instead, our goal is to estimate which represents the variability in the current estimation of the function  X  Q for any given state-action pair, ( s,a )  X  S  X  A . Because Q is parametrized, the sequence of weights, {  X  t } , represents the variability for the uncountably many state-action pairs.
 Assume that the weight vector on time step t + 1 is drawn from the unknown distribution P states and weight vectors. Notice that P a incorporates P and R , using s t ,  X  t (giving the policy  X  ) and R to determine the reward passed to the algorithm to then obtain  X  t +1 . This allows the learn-ing algorithm to select actions using confidence estimates based on the history of the k most recent  X  , without invalidating that the sequence of weights are drawn from P a . In practice, the length of the dependence, k , can be estimated using auto-correlation [2].
 Applying the Moving Block Bootstrap method to a non-stationary sequence of  X   X  X  requires several assumptions on the underlying MDP and the learning algorithm. We require two assumptions on the underlying MDP: a bounded density function and a strong mixing requirement. The assumptions on the algorithm are less strict, only requiring that the algorithm be non-divergent and produce a sequence of { Q t ( s,a ) } that 1) satisfy a smoothness condition (a dependent Cramer condition), 2) have a bounded twelfth moment and 3) satisfy an m -dependence relation where sufficiently sepa-rated Q i ( s,a ) ,Q j ( s,a ) are independent. Based on these assumptions (stated formally in the sup-plement), we can prove that the moving block bootstrap produces an interval with a coverage error of o ( n  X  1 / 2 for the studentized interval on f n ( s,a ) . Theorem 1 Given that Assumption 1-7 are satisfied and there exists constants C 1 ,C 2 &gt; 0 , 0 &lt; produces a one-sided confidence interval that is consistent and has a coverage error of o ( n  X  1 / 2 ) for the studentization of the mean of the process { f (  X  t ,s,a ) } , where Q t ( s,a ) = f (  X  t ,s,a ) . The proof for the above theorem follows Lahiri X  X  proof [12] for the coverage error of the moving block bootstrap for nonstationary data. The general approach for coverage error proofs involve approximating the unknown distribution with an Edgeworth expansion (see [7]), with the coverage error dependent on the order of the expansion, similar to the the idea of a Taylor series expansion. Assuming P a is k -order Markov results in two important practical implications on the learning algorithm: 1) inability to use eligibility traces and 2) restrictions on updates to parameters (such as the learning rate). These potential issues, however, are actually not restrictive. First, the tail of eligibility traces has little effect, particularly for larger k ; the most recent k weights incorporate the most important information for the eligibility traces. Second, the learning rate, for example, cannot be updated based on time. The learning rate, however, can still be adapted based on changes between weight vectors, a more principled approach taken, by the meta-learning algorithm, IDBD [24]. The final algorithm is summarized in the pseudocode below. In practice, a window of data of length w is stored due to memory restrictions; other data selection techniques are possible. Corresponding dependently sampled blocks for the i th resample and T  X  i the mean of the i resample.
 Algorithm 1 GetUpperConfidence ( f (  X  ,s,a ) , {  X  n  X  w ,...  X  n } , X  ) l = block length, B = num bootstrap resamples last w weights and confidence level  X  (= 0.05) 1: Q N  X  X  f (  X  n  X  w ,s,a ) ,...f (  X  n ,s,a ) } 3: M  X  X  w/l c the number of length l blocks to sample with replacement and concatenate 4: for all i = 1 to B do 5: ( Q  X  1 ,Q  X  2 ,...,Q  X  M  X  l )  X  concatMRandomBlocks(Blocks, M) 7: end for 8: sort( { T  X  1 ,...,T  X  B } ) 11: Return 2mean( Q N )  X  T  X   X / 2 4.2 Bootstrapped Confidence Intervals for Sparse Representations We have shown that bootstrapping is a principled approach for computing intervals for global rep-resentations; sparse representations, however, complicate the solution. In an extreme case, for ex-ample, for linear representations, features active on time step t may have never been active before. have never been updated for those features. Consequently, the samples erroneously indicate low variance for Q ( s t ,a t ) .
 We propose that, for sparse linear representations, the samples for the weights can be treated inde-pendently and still produce a reasonable, though currently unproven, bootstrap interval. Notice that for  X  ( i ) the i th feature
P because updates to weights  X  ( i ) , X  ( j ) are independent given the previous states and weights vectors for all i,j  X  { 1 ,...,d } . We could, therefore, estimate upper confidence bounds on the individual an upper confidence bound on Q ( s t ,a t ) . To approximate the variance of  X  ( i ) on time step t , we can use the last w samples of  X  ( i ) where  X  ( i ) changed. Proving coverage error results for sparse representations will require analyzing the covariance be-tween components of  X  over time. The above approach for sparse representations does not capture this covariance; due to sparsity, however, the dependence between many of the samples for  X  ( i ) and  X  ( j ) will likely be weak. We could potentially extend the theoretical results by bounding the covariance between the samples and exploiting independencies. The means for individual weights could likely be estimated separately, therefore, and still enable a valid confidence interval. In future work, a potential extension is to estimate the covariances between the individual weights to improve the interval estimate. high uncertainty. Confidence-based exploration should be comparable to optimistic initialization in domains where exhaustive search is required and find better policies in domains where noisy rewards and noisy dynamics can cause the optimistic initialization to be prematurely decreased and inhibit exploration. Furthermore, confidence-based exploration reduces parameter tuning because the policy does not require knowledge of the reward range, as in softmax and optimistic initialization. Confidence-based exploration could be beneficial in domains where the problem dynamics and re-ward function change over time. In an extreme case, the agent may converge to a near-optimal policy before the goal is teleported to another portion of the space. If the agent continues to act greedily with respect to its action-value estimates without re-exploring, it may act sub-optimally indefinitely. These tracking domains require that the agent  X  X otice X  that its predictions are incorrect and begin searching for a better policy. AN example of a changing reward signals arises in interactive teaching. In this scenario, the a human teaching shapes the agent by providing a drifting reward signal. Even in stationary domains, tracking the optimal policy may be more effective than converging due to the non-stationarity introduced by imperfect function approximation [26].
 Another potential application of confidence estimation is to automate parameter tuning online. For example, many TD-based reinforcement learning algorithms use an eligibility parameter (  X  ) to ad-dress the credit assignment problem. Learning performance can be sensitive to  X  . There has been little work, however, exploring the effects of different decay functions for  X  ; using different  X  values for each state/feature; or for meta-learning  X  . Confidence estimates could be used to increase  X  when the agent is uncertain, reflecting and decrease  X  for confident value estimates [25]. Confidence estimates could also be used to guide the behaviour policy for a parallel multi-task reinforcement learning system. Due to recent theoretical developments [15], several target value functions can be learned in parallel, off-policy, based on a single stream of data from a behaviour policy. The behaviour policy should explore to provide samples that generalize well between the various target policies, speeding overall convergence. For example, if one-sided intervals are main-tained for each target value functions, the behaviour policy could select an action corresponding to the maximal sum of those intervals. Exploration is then biased to highly uncertain areas where more samples are required.
 Finally, confidence estimates could be used to determine when features should be evaluated in a feature construction algorithm. Many feature construction algorithms, such as cascade correlation networks, interleave proposing candidate features and evaluation. In an online reinforcement learn-ing setting, these methods freeze the representation for a fixed window of time to accurately evaluate the candidate [20]. Instead of using a fixed window, a more principled approach is to evaluate the features after the confidence on the weights of the candidate features reached some threshold. In this section, we provide a preliminary experimental investigation into the practicality of confi-dence estimation in continuous-state MDPs. We evaluate a naive implementation of the block boot-strap method for (1) exploration in a noisy reward domain, (2) automatically tuning  X  in the Cartpole domain and (3) tracking a moving goal in a navigation task. In all tests we used the Sarsa(  X  ) learn-ing algorithm with tile coding function approximation (see Sutton and Barto [25]). All experiments were evaluated using RL-Glue [27] and averaged over 30 independent runs. Figure 1: Results showing (a) convergence of various exploration techniques in the navigation task and (b) average cumulative reward of various exploration techniques on the navigation task. 6.1 Exploration To evaluate the effectiveness of confidence-based exploration, we use a simple two-goal continuous navigation task. The small goal yields a reward of 1.0 on every visit. The flashing goal yields a reward selected uniformly from { 100 ,  X  100 , 5 ,  X  5 , 50 } . The reward on all other steps is zero and  X  = 0 . 99 (similar results for -1 per step and  X  = 1 . 0 ). The agent X  X  observation is a continuous ( x,y ) position and actions move the agent { N,S,E,W } perturbed by uniform noise 10% of the time. We present only the first 200 episodes to highlight early learning performance.
 compare our confidence exploration algorithm to three baselines commonly used in continuous state MDPs: (1) -greedy (selecting the highest-value action with probability 1  X  , random otherwise), (2) optimistic initialization (initializing all weights to a high fixed value to encourage exploration) and (3) softmax (choosing actions probabilistically according to their values). We also compare our algorithm to an exploration policy using normal (instead of bootstrapped) intervals to investigate the effectiveness of making simplifying assumptions on the data distribution. We present the results for the best parameter setting for each exploration policy for clarity. Figure 1 summarizes the results. The -greedy policy convergences slowly to the small goal. The optimistic policy slowly converges to the small goal for lower initializations and does not favour either goal for higher initializations. The softmax policy navigates to the small goal on most runs and also convergences slowly. The normal-interval exploration policy does prefer the flashing goal but not as quickly as the bootstrap policy. Finally, the bootstrap-interval exploration policy achieves highest cumulative reward and is the only policy that converges to the flashing goal, despite the large variance in the reward signal. 6.2 Adjusting Lambda To illustrate the effect of adjusting  X  based on confidence intervals, we study the Cartpole problem. We selected Cartpole because the performance of Sarsa is particularly sensitive to  X  in this domain. The objective in Cartpole is to apply forces to a cart on a track to keep a pole from falling over. An episode ends when the pole falls past a given angle or the cart reaches the end of the track. The reward is +1 for each step of the episode. The agent X  X  observations are the cart position and velocity and the poles X  angle and angular velocity. The Cartpole environment is based on Sutton and Barto X  X  [25] pole-balancing task and is available in RL-library [27].
 To adjust the  X  value, we reset  X  on every time step:  X  = normalized( ucb ) where ucb = 0 . 9  X  ucb + 0 . 1  X  getUpperConfidence (  X  ( s,a ) , X , X  ) . The confidence estimates were only used to adjust  X  for clarity: exploration was performed using optimistic initialization. Figure 2 presents the average balancing time on the last episode for various values of  X  . The flat line depicts the average balancing time for Sarsa with  X  tuned via confidence estimates. Setting  X  via confidence estimates achieves performance near the best value of  X  . We also tested adjusting  X  using normal confidence intervals, however, the normal confidence intervals resulted in worse performance then any fixed value of  X  . 6.3 Non-stationary Navigation Task One natural source of non-stationarity is introduced by shaping a robot through successive approx-imations to a goal task (e.g., changing the reward function). We studied the effects of this form of non-stationarity, where the agent learns to go to a goal and then another, better goal becomes available (near the first goal to better guide it to the next goal). In our domain, the agent receives -1 reward per step and +10 at termination in a goal region. After 150 episodes, the goal region is tele-ported to a new location within 50 steps of the previous goal. The agent receives +10 in the new goal and now 0 in the old goal. We used = 0 to enable exploration only with optimistic initialization. We recorded the number of times the agent converged to the new goal with the change after an initial learning period of 150 episodes. The bootstrap-based explorer found the new goal 70% of the time. It did not always find the new goal because the -1 structure biased it to stay with the safe 0 goal. Interestingly, optimistic initialization was unable to find the new goal because of this bias, illustrating that the confidence-based explorer detected the increase in variance and promoted re-exploration automatically. In this work, we investigated constructing confidence intervals on value estimates in the continuous-state reinforcement learning setting. We presented a robust approach to computing confidence es-timates for function approximation using bootstrapping, a nonparametric estimation technique. We proved that our confidence estimate has low coverage error under mild assumptions on the learning algorithm. In particular, we did so even for a changing policy that uses the confidence estimates. We illustrated the usefulness of our estimates for three applications: exploration, tuning  X  and tracking. We are currently exploring several directions for future work. We have begun testing the confidence-based exploration on a mobile robot platform. Despite the results presented in this work, many traditional deterministic, negative cost-to-goal problems (e.g., Mountain Car, Acrobot and Puddle World) are efficiently solved using optimistic exploration. Robotic tasks, however, are often more naturally formulated as continual learning tasks with a sparse reward signal, such as negative reward for bumping into objects, or a positive reward for reaching some goal. We expect confidence based techniques to perform better in these settings where the reward range may be truly unknown (e.g. generated dynamically by a human teacher) and under natural variability in the environment (noisy sensors and imperfect motion control). We have also begun evaluating confidence-interval driven behaviour for large-scale, parallel off-policy learning on the same robot platform.
 There are several potential algorithmic directions, in addition to those mentioned throughout this work. We could potentially improve coverage error by extending other bootstrapping techniques, such as the Markov bootstrap, to non-stationary data. We could also explore the theoretical work on exponential bounds, such as the Azuma-Hoeffding inequality, to obtain different confidence es-timates with low coverage error. Finally, it would be interesting to extend the theoretical results in the paper to sparse representations.
 Acknowledgements: We would like to thank Csaba Szepesv  X  ari, Narasimha Prasad and Daniel Li-zotte for their helpful comments and NSERC, Alberta Innovates and the University of Alberta for funding the research.
