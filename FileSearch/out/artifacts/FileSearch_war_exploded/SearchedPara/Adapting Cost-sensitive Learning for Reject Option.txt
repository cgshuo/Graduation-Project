 Traditional cost-sensitive learning algorithms always deter-ministically predict examples as either positive or negative (in binary setting), to minimize the total misclassification cost. However, in more advanced real-world settings, the algorithms can also have another option to reject examples of high uncertainty. In this paper, we assume that cost-sensitive learning algorithms can reject the examples and obtain their true labels by paying reject cost . We therefore analyse three categories of popular cost-sensitive learning approaches, and provide generic methods to adapt them for reject option.
 I.2.6 [ Artificial Intelligence ]: Learning X  induction Algorithms Cost-sensitive, reject, classification
The existing cost-sensitive learning algorithms can be cat-egorized into three types:
However, most of these existing cost-sensitive learning al-gorithms always make deterministic predictions on test ex-amples, as either positive or negative (in binary setting). In more advanced real-world settings, the algorithms can, instead, have another option to reject examples of high un-certainty, and allow another, presumably better, classifier to predict them.

In this paper, we assume that the learning algorithms can reject the examples and obtain their true labels by paying reject cost . Then, we analyse these three categories of cost-sensitive learning approaches, and provide generic methods to adapt them for reject option. Empirical study verifies that, the adapted algorithms can indeed significantly reduce the total cost of misclassification and reject, compared with the original ones.
In adapting these algorithms for reject option, we mainly consider binary classification, uniform cost matrix and re-ject cost; however, we also briefly discuss the applicabil-ity of these algorithms to multi-class problems, non-uniform (i.e., example-dependent) cost matrix and reject cost. More specifically, in binary setting, we assume that, the costs for bothtruepositiveandtruenegativeare0,andthecostsfor false positive (denoted by  X   X  X  X  ) and false negative (denoted by  X   X  X  X  ) are both known and greater than 0. In addition, we also denote  X   X  the reject cost, with meaningful range
In Sections 2.1 to 2.3, we analyse in detail the previous three categories of cost-sensitive algorithms. In Section 2.4, we summarize and compare their properties for reject option, and provide practical guidance for real-world applications.
Some recent studies have adapted specific learning algo-rithms for reject option, such as [4, 7]. In this paper, how-ever, we tend to provide generic solutions for taking reject option into consideration, thus would not develop more spe-cific algorithm-dependent methods. Instead, we will analyse the property of this category of algorithm-dependent ap-proach, and study its applicability to multi-class problems, as well as to non-uniform cost matrix and reject cost.
Most previous studies in adapting specific learning algo-rithms for reject option only consider binary classification. If multi-class problems are encountered, those solutions are usually not applicable. This is demonstrated in Figures 1.
We suppose the original classes are linear separable (or would be linear separable after being projected into a high dimensional space), and thus the original learning algorithm would only build linear classifiers (such as perception, logis-tic regression, SVM, and so on). We can see from Figure 1(a) that, in binary setting, after regarding the highly uncertain examples as a new class  X  X ejected X , the consequent three classes (i.e.,  X  X ositive X ,  X  X egative X  and  X  X ejected X ) are still linear separable. Therefore, after slight modification, the original (linear) algorithms can be easily adapted for such reject option. However, it is not the case when multi-class problems are encountered.

Figure 1(b) shows a linear separable case for a three-class problem. If the highly uncertain examples are rejected (i.e., regarded as the new class  X  X ejected X ), the consequent four classes (i.e.,  X  X lass 1 X ,  X  X lass 2 X ,  X  X lass 3 X  and  X  X ejected X ) are no longer linearly separable. As a result, the original (linear) algorithm could not be easily adapted to handle this more complex case with reject option. More sophisticated strategies, such as a new non-linear classifier or a new kernel function, need to be applied. This simple example clearly demonstrates the limitation of the algorithm-dependent ap-proaches with reject option in multi-class situations. (a) Binary-class problem Figure 1: Adapting linear classifier for reject option.
We now further discuss this algorithm-dependent approach for non-uniform cost matrix and reject cost. That is, given different examples, the cost matrix and the reject cost are still known but might also be different. In such cases, the algorithm-dependent approach is usually still applicable. More specifically, the algorithm-dependent approach generally han-dles misclassification cost and reject cost by manipulating the loss function. For example, the hinge loss is usually modified when adapting SVM related algorithms for reject option (see [4, 7] for details). Therefore, if the cost matrix and the reject cost change with the specific examples, the loss function can still be easily formalized, and the further optimization can also be easily implemented. This indicates the flexibility of such algorithm-dependent approach with non-uniform cost matrix and reject cost.

To summarize, some previous studies have developed vari-ous algorithm-dependent approaches for reject option. These approaches are usually difficult to be expanded to the generic methods, and difficult to handle multi-class problems. How-ever, they usually can be easily applied when cost-matrix and reject cost are non-uniform.
Thresholding is a generic method to handle cost-sensitive learning problem. It requires the classifier to produce (ac-curate) posterior probabilities, and sets specific threshold(s) accordingly to make predictions on examples.

Specifically, for binary classification, we can set a thresh-old  X   X  and classify an example  X  as
When the reject cost is introduced, we still use the original posterior probability estimation, but set two thresholds to classify examples as positive (1), negative (0) or rejected. The following theorem provides us with a generic way to classify examples according to these thresholds.

Theorem 1. In cost-sensitive learning with reject, given  X  to classify an example  X  as
Proof. Given  X   X  X  X  ,  X   X  X  X  and  X   X  , the classifier would predict an example as rejected only when the reject cost is smaller than the expected misclassification cost for both positive and negative. Thus, for a specific example  X  ,we have
Solving the above two inequalities, we can conclude that,
Similarly, we can prove that, the example  X  is predicted negative (0) when  X  (1  X   X  )  X   X   X 
We now further discuss the applicability of thresholding to multi-class situations. When encountering multi-class problems, thresholding could no longer simply set thresh-olds to make predictions. However, given the (accurate) posterior probabilities, it still could primitively calculate the expected misclassification cost for each class, and make the optimal prediction by comparing them with the given re-ject cost. More specifically, given  X  classes (denoted by {  X  1 , X  2 ,  X  X  X  X  , X   X  } ) and pre-defined misclassification cost ma-trix (each element of the matrix is denoted by  X   X  X  X  where  X , X   X  X  1 , 2 ,  X  X  X  X  , X  } ), the expected misclassification cost to predict example  X  as  X   X  is:  X  [  X  (  X   X   X   X  )] = Among all the classes, the minimum expected misclassifi-cation cost  X   X  X  X  X  [  X  (  X   X   X   X  )] is chosen and compared with the given reject cost  X   X  , the optimal prediction therefore could be naturally made to minimize the total cost.

This simple strategy could also be easily adapted for non-uniform cost matrix and reject cost. Specifically, for each given example, the original uniform  X   X  X  X  and  X   X  would be substituted by the specific example-dependent ones, and the same comparison could be conducted to make the optimal prediction.

To summarize, as a generic method, thresholding with re-ject can be combined with any learning algorithm that pro-duces (accurate) posterior probabilities. Under this condi-tion, thresholding with reject could be flexibly applied to both binary class and multi-class problems, as well as non-uniform cost matrix and reject cost.
Sampling is another generic method for cost-sensitive learn-ing. Roughly, it first modifies the class distribution of train-ing data according to the given cost matrix, and then applies cost-insensitive classifiers (with the threshold being 0.5) on the sampled data. In binary setting, [6] shows that if all negative examples are kept, the number of positive examples should be multiplied by  X   X  X  X  / X   X  X  X  to implement sampling . Compared with thresholding , sampling does not require the learning algorithms to produce accurate probability estima-tion. This indicates that the learning algorithms, as decision tree or naive Bayes, could still work well with sampling . 1
With (possible) inaccurate probability estimation, we can-not set two thresholds to predict examples as in Section 2.2. Instead, we fix the threshold as 0.5, and construct two clas-sifiers according to two different class distributions on the training data, so as to take the reject cost into account. The following theorem tells us how to construct these two clas-sifiers based on two different class distributions, given fixed reject cost (  X   X  ).

Theorem 2. In cost-sensitive learning with reject, if all negative examples are kept, and the number of positive ex- X  should be classified as Similarly, if the number of positive examples is multiplied by
Proof. [6] has proved an equivalent relation between the probability threshold and resampling. Specifically, given a learning algorithm and an original probability threshold  X  based on this algorithm, we can set a new target probability threshold  X   X  via multiplying the number of positive examples
To distinguish all the positive examples from the negative and the rejected examples, we first set  X   X  =  X   X  1 =(  X   X  X  X  1 In essence, sampling can also work with the algorithms that do not at all produce probability estimation (such as, all types of discriminant functions). For the sake of conve-nience, in the rest of the section, we still use the probability representation with threshold 0 . 5 for illustration.
Table 1: Predicting examples with two classifiers.  X   X  ) / X   X  X  X  (see Theorem 1 in Section 2.2), and  X   X  =0 . 5. Thus, we can have the target threshold as 0 . 5 via multiplying the number of positive examples by
Similarly, to distinguish all the negative examples from thepositiveandtherejectedexamples,wethenset  X   X  = =  X   X  / X   X  X  X  (see Theorem 1 in Section 2.2), and  X   X  =0 . 5. Thus, we can have the target threshold as 0 . 5 via multiplying the number of positive examples by
After constructing two classifiers based on Theorem 2, we can accordingly make predictions for the examples. Specifi-cally, we can directly predict the examples as positive, nega-tive or rejected, when the two classifiers are consistent with each other (i.e., two classifiers have the same prediction for the given examples). However, it is still possible that the two classifiers conflict with each other on some examples (i.e., the example being predicted positive by the first clas-sifier and negative by the second one). This situation might occur, as we construct two different classifiers based on two different class distributions (i.e., two different training sets). Here, we simply predict these examples in conflict as re-jected . Table 2.3 illustrates how we make prediction based on the two classifiers.

We now further discuss the applicability of sampling in multi-class situations. We can clearly notice in the previous implementation of sampling with reject that, the sample ra-tio is implicitly determined by the originally threshold  X  However, such original threshold could hardly be derived in multi-class setting (as we have discussed in Section 2.2). Thus, in this case, the sample ratio could not be easily ob-tained, and the adaptation to multi-class could not be easily implemented.

In the case of non-uniform cost matrix and reject cost, sampling also could not be easily applied. Specifically, sam-pling in essence could be regarded as a pre-precess for the learning procedure. When the cost-matrix and reject cost change with the specific examples, the sampling ratio also changes accordingly. Thus no fixed sampling ratio could be used for such pre-processing, and the sampling approach therefore could not be applied.

To summarize, for the binary classification and given uni-form cost matrix and reject cost, sampling with reject can be combined with any learning algorithm, regardless of its capability of producing posterior probabilities. However, it could hardly be adapted for multi-class setting, or non-uniform cost matrix and reject cost.
Based on the previous analysis on the three categories of cost-sensitive learning approaches, Table 2 summarizes Table 2: Summary of the three cost-sensetive learn-ing approaches for reject option the properties for each of them. We therefore can make comparison and conclude when to use them in practice:
The above conclusions summarize the properties of these cost-sensitive learning approaches. More importantly, they provide us a practical guidance to adapt these approaches for reject option in real-world applications.
In this section, we empirically verify the performance of the proposed algorithms with reject option. Specifically, we conduct experiments on 12 UCI data sets [2], including  X  X reast-cancer X , X  X olic X , X  X redit X , X  X iabetes X , X  X eart-h X , X  X ono-sphere X ,  X  X onar X ,  X  X ote X ,  X  X udiology X ,  X  X utos X ,  X  X coli X  and  X  X lass X . We set the misclassification costs for false negative and false positive as 100 and 10 respectively (  X   X  X  X  = 100,  X   X  X  X  = 10) for all the data sets, thus we can obtain the meaningful range of reject cost 0 &lt; X   X  &lt; 9 . 09.
We implement thresholding by CostSensitiveClassifier [12], and implement sampling by Weighting [11] for comparison. We choose decision tree (C4.5) and random forest as the base learners, due to their generally good classification per-formance. On each data set, we make the comparison be-tween thresholding with reject ( X  X hresholding-r X  in short), sampling with reject ( X  X ampling-r X ), thresholding and sam-pling . 10-fold cross validation is repeated 50 times on each data set, and the average total cost is recorded, with reject cost varying from 1 to 9.

Figures 2 shows the comparisons of the four algorithms on a typical UCI data set  X  X eart-h X , with C4.5 and random forest as the base learners. The experimental results on the other data sets are similar thus omitted.

Interesting observations can be discovered from Figure 2. Figure 2: Comparison of total cost on a typical UCI data set  X  X eart-h X , with C4.5 and random forest as base learners.
To summarize, when the reject cost is considered, the adapted cost-sensitive algorithms always outperform the orig-inal ones, especially when the reject cost is relatively small.
In this paper, we assume that the learning algorithms can reject the examples and obtain their true labels by paying reject cost. We therefore analyse three popular categories of cost-sensitive learning approaches, and provide generic methods to adapt them for reject option. Theoretical and empirical studies show that, the adapted algorithms can in-deed significantly outperform the original ones in reducing the total cost of misclassification and reject.
