 Most of the relevance feedback algorithms only use docu-ment terms as feedback (local features) in order to update the query and re-rank the documents to show to the user. This approach is limited by the terms of those documents without any global context. We propose to use statistical topic modeling techniques in relevance feedback to incorpo-rate a better estimate of context by including global infor-mation about the document. This is particularly helpful for difficult queries where learning the context from the inter-actions with the user is crucial. We propose to use the topic mixture information obtained to characterize the documents and learn their topics. Then, we rank documents incorpo-rating positive and negative feedback by fitting a latent dis-tribution for each class of documents online and combining all the features using Bayesian Logistic Regression. We show results using the OHSUMED dataset for 3 different variants and obtain higher performance, up to 12.5% in Mean Aver-age Precision (MAP).
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Relevance Feedback, Document Filtering Algorithms, Experimentation Relevance Feedback,Topic Models, Language Models
Relevance feedback has been studied extensively in Infor-mation Retrieval as a form of incorporating feedback from the user to refine the results retrieved. The authors in [5] concluded that negative feedback is also valuable to improve the ranking. However, the need to capture broader context in difficult queries is still a challenge. The authors in [2] have showed that including global features and using clusters can improve the retrieval performance significantly. Thus, statistical topic modeling provides a robust and automatic method to incorporate context to the user feedback.  X  Main contact.

Previous approaches have used statistical topic models to represent documents according to their latent topic content and use this representation in information retrieval [1, 4]. Authors in [4], use topics as a form of smoothing the Lan-guage Model used in retrieval. However, this approach does not address the incorporation of relevance feedback. Recent work from [1] explores the use of topics as a form to perform the query expansion for relevance feedback. However, this action might make the query noisier because the top topic terms might not contribute to a better discrimination of the relevant documents. In addition, these terms might not be distinctive across different topics.

We propose to include the topic information as feedback using the document topic mixture instead of the document word mixture. We first estimate the topic mixture for each document in the corpus using LDA and save it as meta data. Given an initial query we use a standard retrieval engine, Language Models for this case, to show the first set of documents to the user and obtain relevance judgments. Then, we assume that topic mixtures for feedback docu-ments are observed. We then define two latent Dirichlet dis-tributions: one for relevant documents and another for non-relevant documents. We fit these distributions iteratively, by finding a sufficient statistic and maximizing the likeli-hood of observing this statistic. To score the documents, we use Bayesian Logistic Regression. This function results in a very efficient scoring function, and incorporates the benefits of active learning. Under this model, we incorporate posi-tive and negative feedback, and context based on topics in the interaction without changing the query. We also provide efficient updates of the latent distributions based on topics.
In this section we describe how we incorporate the topic mixture of feedback documents as a global measure in con-trast to query expansion. To achieve this, we estimate the topic mixtures of the documents,  X  i ,for K topics in the cor-pus using LDA off-line. Given a initial ranking, the user provides relevance feedback which is used to fit the latent relevant/non-relevant distributions of topic mixtures. Thus, we assume two Dirichlet distributions: one for the relevant set of documents  X  R , and one for the non-relevant docu-ments  X   X  R . Therefore, we have: P (  X  i |  X  R )  X  Dirichlet (  X  R )=  X ( for  X  R and  X   X  R . Then, we calculate the log-probability of the document being generated by those distributions: Log P (  X  i |  X  )=log X (
We denote these scores as PR i and P  X  R i respectively. To update the latent distributions of the relevant  X  R and non-relevant topics  X   X  R , we use the topic content from the docu-ments labeled by the user as relevant,  X  R , and non-relevant,  X  , after each interaction. The Dirichlet distribution guar-antees a unique maximum when the Maximum Likelihood (ML) is estimated for  X  . Moreover, a sufficient statistics, SS , can be estimated to update this distribution as more observations are available. We can update SS efficiently without keeping previous document feedback. The initial value of the sufficient statistic SS (0) k,R for the relevant topic k and its update from the interaction j is described by: of relevant documents at the j -th interaction. Given SS ( j ) and SS ( j )  X  R , we use the method proposed in [3] to calculate distributions, we use the topic-based Language Model P TW for document i as follows: where  X   X  are the word mixture for the topics obtained from LDA. Thus, the score S TW,i for query Q with terms q is defined as:
To combine the scores from the latent relevant/non-relevant topic mixtures and the topic-based Language Model, we use the Bayesian Logistic Regression approach [5]. Let y i = { +1 ,  X  1 } be the relevant/non-relevant label for document, we have the score function: where d i is the feature vector scores: PR i , P  X  R i , is a parameter vector assumed to be normally distributed and updated in a Bayesian form. Here, the distribution of from the j -th iteration is taken as prior distribution for the next iteration. To approximate the posterior distribution we use the Laplace approximation as discussed in [5].
We test our method using the OHSUMED dataset which consists of 196 , 000 medical abstracts and 3 , 506 relevance la-bels for 63 queries from the Document Filtering Track from TREC 4. As suggested in the track, we assume unobserved labels as non-relevant. We fit the LDA model using K =50 topics, which is the number of topics with highest perfor-mance based on Empirical Likelihood. To test the impact of topic information, we use standard Language Model (LM) with Dirichlet smoothing described in [4] as baseline. This score is used with Bayesian Logistic Regression. We test 3 variants of the model and the baseline: LM as baseline; Table 1: Results of Topic feedback using 50 topics in the OHSUMED dataset Method P@10 MAP DiscGain LM 0 . 3968 0 . 4286 0 . 5660 LM + S TW 0 . 4206 0 . 4557 0 . 6315 LM + S TW + PR 0 . 2968 0 . 3307 0 . 5590 LM + S TW + PR + P  X  R 0.4698 0.5141 0.6580 LM+ S TW ;LM+ S TW + PR ;LM+ S TW + PR + P  X  R .Wecal-culate the initial ranking using LM and asked for feedback until we have at least one relevant and one non-relevant documents. We use 10 feedback documents and estimate precision at 10 (P@10), Mean Average Precision (MAP), and Discounted Gain (DiscGain). There are two relevance level labels available in the dataset, { 1 , 2 } , that are assumed equally, { +1 } for P@10 and MAP. However for DiscGain, we use both labels in the evaluation.

Table1showstheresultsforthevariantstested.Weob-serve that the LM+ S TW performs better than the baseline. This score is similar to the LDA-based retrieval proposed in [4] but the value of the linear combination parameters is fitted based on the feedback as opposed to a corpus-wide parameter. When we incorporate only the score from the rel-evant distribution of topics PR , the performance decreases. However, when the score for the non-relevant distribution P  X  R is incorporated, the performance is the highest. This shows the value of negative feedback reported previously in [5]. We notice that, the combination of PR and P  X  R is equivalent to the log of the likelihood ratio test (probabilis-tic ranking principle) weighted by  X  . This also explains why both scores should be included in the model.

We observe that the combination of the four scores im-proves the general performance by: 11.6 % P@10, 12.8 % MAP, and 4.6 % DiscGain respect topic-based language model (LM+ S TW ). This demonstrates, the power of statistical topic modeling in relevance feedback.
We have presented a method to incorporate statistical topic information in relevance feedback without changing the query. Results show that including the mixture of topics in relevance feedback improves the performance by pruning the search space, and adding context to the query. As fu-ture work, we plan to incorporate a policy to decide when to update the parameters of the relevant and non-relevant topic distribution optimally.
This work is partially f unded by CONAC YT grant 207751 and SAP Gift Support
