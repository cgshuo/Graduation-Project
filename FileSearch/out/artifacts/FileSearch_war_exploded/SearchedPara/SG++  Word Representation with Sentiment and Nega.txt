 Here we propose an advance Skip-gram model to incorporate both word sentiment and negation information. In particular, there is a a softmax layer for the word sentiment polarity upon the Skip-gram model. Then, two paralleled embedding layers are set up in the same embedding space, one for the affirmative context and the other for the negated context, followed by their loss functions. We evaluate our proposed model on the 2013 and 2014 SemEval data sets. The experimental results show that the proposed approach achieves better performance and learns higher dimensional word embedding informatively on the large-scale data.
 Twitter Sentiment Classification; Word Representation; Neural Net-work; Negation
Twitter sentiment classification is to treat Tweets as positive, negative and neutral. This will help the stakeholders make proper decisions, when the public sentiment on topics, such as the newly released products or services, entertainment news and politics, can be obtained.

Nowadays, many approaches in Twitter sentiment classification, utilize a supervised classifier and rely on extensive feature engi-neering [3, 1, 10, 6, 9, 4, 8, 5]. However, there is an open problem for feather engineering is that it costs extensive labour work and specific domain knowledge. Therefore, feature learning is an alter-native way to learn discriminative features automatically from data. The work presented by [12, 16] proves that the features of a sen-tence/document can be learnt through its word embedding. Current approaches of learning word embedding [2, 7, 15] focus on mod-eling the syntactic context. This situation does not take the senti-ment information into account. [14] then proposed a complex neu-ral network to learn sentiment-specific word embedding and lan-guage model simultaneously. However, it is very time-consuming to training a big neural network and the negation is not included in Tang X  X  proposal as well.
 tions to predict both syntactic context and word sentiment. Figure 1 shows there layers, where the first one indicates the training word w , the second one is the projection and the third one includes the surrounding words w t  X  2 ,w t  X  1 ,w t +1 ,w t +2 and the sentiment in-formation s t . Correspondingly, given a sequence of training words w ,w 2 ,...,w T , we modify Equation 1 and Equations 2 and then obtain the goal of SG+ to maximize the log probability where c is the size of the context window, s t is the sentiment label of word w t , and  X  is the hyper-parameter that weights the impact of word sentiment and syntactic context. Similar to p ( w t + j | w t ) , p ( s t | w t ) is defined using the softmax function as well: where v w is the vector representation of w ,  X  s is the parameter vector of sentiment label s , and S is the number of sentiment labels.
The objective of SG++ is to further incorporate negation. Here we propose to learn the affirmative and negated word embedding si-multaneously. Intuitively, affirmative (negated) words are mapped to the affirmative (negated) representations, which can be used to predict the surrounding words and word sentiment in affirmative (negated) context. Figure 2 presents an example... Given a se-quence of training words w 1 ,w 2 ,...,w T where negation detection are already performed, the goal of NSSG+ is to maximize the log probability where sa t ( sn t ) is the sentiment label in affirmative (negated) con-text, and n t is the indicator whether w t is negated or not. Techni-cally, we set two paralleled embedding layers with the same initial tweets, which express sentiment about popular topics in 2013 and 2014. Twitter2014Sarcasm is a small test set of tweets that contain the sarcasm hashtag to determine how sarcasm affects the tweet polarity.

We present the experimental results on SemEval 2013 and 2014 in Table 2, where  X  X G++" is our proposed advanced Skip-gram model with sentiment and negation,  X  X G+" denotes the Skip-gram model with word sentiment and  X  X G" stands for the basic Skip-gram model. We can see that  X  X G++" outperforms  X  X G+" and  X  X G" on all the datasets besides of  X  X witter2014Sarcasm".
In Table 2, we design the experiments to evaluate the influence of sentiment by comparing the performance of  X  X G+" with  X  X G". At the same way, we investigate negation by testing  X  X G++" and  X  X G+" respectively. We can see that  X  X G+" outperforms  X  X G", and  X  X G++" is better than  X  X G+" on the first four data sets. Hence, we conclude that sentiment and negation play very important roles in Twitter sentiment classification. Furthermore, our experiments confirm our motivation and suggest the future work to consider sen-timent and negation together.

The performance of Twitter2014Sarcasm is not stable in Table 2, since our proposed  X  X G++" deals with the normal tweets and the sarcasm ones equally.
Here we present an example to show the intuitive insight of our proposed advanced Skip-gram model  X  X G++". Table 3 shows the neighbors of the target word  X  X ood" under  X  X G++" and  X  X G". We can see that the neighbor words found by  X  X G++" are more reason-able than those by  X  X G", considering the sentiment and negation. In particular, there are some negative words, such as rough , testy and bad in the list of  X  X G".

Note that the way to find the neighbors adopts the utility pro-gram distance . The distance program computes cosine similarity between two words in the embedding space, and returns the top 40 closest words. networks. We observe that our proposed  X  X G++" achieves one of the best results.

In order to better understanding the advantages of  X  X G++", we summarize the characters of all the listed algorithms/models in Ta-ble 5. We draw the conclusions that: (1) the proposed  X  X G++" makes the best result with automatic feature learning; (2) although the quality of the hand-crafted labels is high, the amount is less and the coverage is low; (3) the deep model outperforms the shal-low one, since the deep one can capture the latent connections and comprehensive linguistic phenomenon of words.
In this paper, we propose an advanced Skip-gram model (SG++) to learn better word embedding and negation for Twitter sentiment classification efficiently. Three layers are presented in SG++, namely the syntactic layer, the affirmative layer and the negation one. We also built an SG+ model to show the effectiveness of sentiment without negation. Our experiments on SemEval 2013 and 2014 in-dicate that both sentiment and negation play very important roles in Twitter sentiment classification. Furthermore, we conduct exten-sive performance comparisons with other similar algorithms/models. The proposed SG++ obtains the top 3 results with automatic feature learning.

Based on the analysis in Table 5, the deep neural network is able to achieve better tweet representation than the shallow one. In the future, we will focus on developing deep neural network to learn language models and other possibilities. This is also our ongoing work.
This research is funded by the National High Technology Re-search and Development Program of China (No. 2015AA015801), and the Science and Technology Commission of Shanghai Munic-ipality of China (No.15PJ1401700). [1] L. Barbosa and J. Feng. Robust sentiment detection on [2] R. Collobert, J. Weston, L. Bottou, M. Karlen, [3] A. Go, R. Bhayani, and L. Huang. Twitter sentiment
