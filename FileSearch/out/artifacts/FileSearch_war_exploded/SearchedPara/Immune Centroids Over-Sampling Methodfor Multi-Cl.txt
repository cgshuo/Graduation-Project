 The class imbalance problem typically occurs when there are many more instances belonging to some classes than others in multi-class classification. Recently, reports from both academy and industry indicate that the imbalanced class distribution of a data set has posed a serious difficulty to most classification algorithms which assume a relatively balanced distribution. Furthermore, iden-tifying rare objects is of crucial importance. In many real-world applications, the classification performances on the small classes are the major concerns in determining the property of a classification model.
 tions are designed for binary classification. However, multi-class imbalanced learning problems appear frequently. Identifying the concept for each class in these problems is usually equally important. When multiple classes are present in an application domain, solutions proposed for binary classification problems may not be directly applicable, or may achieve a lower performance than expected. For example, solutions at the data level suffer from the increased search space, and solutions at the algorithm level become more complicated, since they must consider small classes and it is difficult to learn the corresponding concepts for these small classes. Additionally, learning from multiple classes itself implies a difficulty, since the boundaries among the classes may overlap. The overlap would downgrade the learning performance.
 There exist many researches on multi-class imbalance learning. However, most ex-isting researches transfer multi-class imbalance learning into binary using different class decomposition schemes and apply existing binary imbal-ance learning solutions. These decomposition approaches help reuse the existing binary imbalance learning solutions. However, they have their own shortcom-ings, which will be discussed in the next section related work. To overcome these shortcomings, in this paper we present a novel global multi-class imbal-ance learning approach, which does not need to transfer multi-class into binary. This novel approach is based on immune network theory, and utilizes an aiNet model [ 3 ] to generate immune centroids for the clusters of each small class, which have high data density, called global immune centroids over-sampling (denoted as Global-IC). Specifically, our novel approach Global-IC resamples each small class by introducing immune centroids of the clusters of the examples belonging to the small class. Our experimental results show that Global-IC achieves better performance, comparing with existing methods.
 The rest of this paper is organized as follows. We review related work in Section 2. Section 3 presents our proposed over-sampling method Global-IC. Our experimental results and comparisons are shown in Section 4. Finally, we conclude this paper in Section 5. As we said before, most existing solutions for multi-class imbalance classifica-tion problems use different class decomposition schemes to convert a multi-class classifi-cation problem into multiple binary classification problems, and then apply binary imbalance techniques on each binary classification problem. For example, Tan et al. [ 4 ] used both one-vs-all (OVA) [ 2 ] and one-vs-one (OVO) [ 1 ] schemes to break down a multi-class problem to binary problems, and then built rule-based learners to im-prove the coverage of minority class examples. Zhao [ 20 ] used OVA to convert a multi-class problem into multiple binary prob-lems, and then used under-sampling and SMOTE [ 5 ] techniques to overcome the imbalance issues. Liao [ 6 ] investigated a variety of over-sampling and under-sampling techniques with OVA for a weld flaw classification problem. Chen et al. [ 7 ] proposed an approach that used OVA to convert a multi-class classifica-tion problem to binary problems and then applied some advanced resampling methods to rebalance the data of each binary problem. All these methods are based on multi-class decomposition. Multi-class decomposition oversimplifies the original multi-class problem. It is obvious that each individual classifier learned for a binary sub-problem couldnt be trained with the full information of the original data. This can cause classification ambiguity or uncovered data regions with respect to each type of decomposition.
 (called Global-GS) was proposed [ 8 ], which re-weights the instances from each class accord-ing to their ratio without using class decomposition. In order to equi-librate the signi-ficance of the examples for different classes in an imbalanced framework, it resam-ples each class in a consistent manner by considering a fac-tor of N i /N max , where N i the number of the examples of the is the number of the examples for the majority class of the problem. Navarro et al. [ 9 ] presented a preprocessing mechanism based on SMOTE, which iteratively generates new synthetic samples from the least represented class at each step, known as Static-SMOTE. The synthetic examples are obtained by applying the SMOTE algorithm [ 5 ] only over the instances of the minority classes. Wang et al. [ 10 ] developed a study regarding the extension of boosting techniques for imbalance problems with  X  X ulti-minority X  and  X  X ulti-majority X  classes, called AdaBoost.NC. Their approach is based on AdaBoost [ 22 ], combining with neg-ative correlation learning. The initial weights of the examples in this boosting approach are assigned in inverse proportion to the number of instances in the corresponding class. Our novel approach Global-IC is closely related to these methods. However, it introduces a complete new approach, immune centroid gen-eration, to oversample the examples of the small classes for multi-class imbalance learning. Before we introduce our solution Global-IC, we first briefly introduce the basic con-cepts and knowledge of immune systems. After that, we present the details of Global-IC. 3.1 Immune Systems Before discussing our method, we sketch a few aspects of the human adaptive im-mune system. The immune systems guard our bodies against infections due to the attacks of antigens. The surface receptors on B-cells (one kind of lym-phocyte) are able to recognize to specific antigens. The response of a receptor to an antigen can activate its hosting B-cell. Activated B-cell then proliferates and differentiates into memory cells. Memory cells secret antibodies to neutralize the pathogens through complementary pattern matching. During the proliferation of the activated B-cells, a mutation mechanism is employed to create diverse antibodies by altering the gene segments. Some of the mutants may be a better match for the corresponding antigen. In order to be protective, the immune sys-tem must learn to distinguish between our own (self) cells and malefic external (nonself) invaders. This process is called self/nonself discrimination: those cells recognized as self dont promote an immune response. The system is said to be tolerant to them, while those that are not provoke a reaction resulting in their elimination.
 Immune network theory, originally proposed in [ 11 ], hypothesizes a novel viewpoint of lymphocyte activities, natural antibody production, pre-immune repertoire selection, tolerance and self/nonself discrimination, memory and the evolution of an immune system. It was suggested that the immune system is com-posed of a regulated network of cells and molecules that recognize one another. The immune cells can respond either positively or negatively to the recognition signal (antigen or other immune cell or molecule). A positive response would result into cell proliferation, cell activation and antibody secretion, while a neg-ative response would lead to tolerance and suppression.
 Learning in the immune system involves raising the population size and affin-ity of those lymphocytes that have proven themselves valuable by having recog-nized any antigen. Burnet [ 12 ] introduced clonal selection theory by modifying N.K. Jerne X  X  theory. The theory states that in a pre-existing group of lympho-cytes (specifically B cells), a specific antigen only activates (i.e. selection) its counter-specific cell so that a particular cell is induced to multiply (producing its clones) for antibody production. With repeated exposures to the same anti-gen, the immune system produces antibodies of successively greater affinities. A secondary response elicits antibodies with greater affinity than in a primary response. Based on the clonal selection principle, Castro proposed a computa-tional implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. He also defined aiNet (an artificial immune network model) for data analysis [ 3 ]. The aiNet is an edge-weighted graph, not necessarily fully connected, composed of a set of nodes, called antibodies, and sets of node pairs called edges with an assigned number called weight or connection strength, associated with each connected edge. The aiNet clusters serve as internal images (mirrors) responsible for mapping existing clusters in the data set into network clusters. These clusters map those of the original data set. The shape of the spatial distribution of antibodies follows that of the antigenic spatial distribution. 3.2 Immune Centroids Resampling In order to directly handle the imbalance problem of multi-class classification, our Global-IC takes two separate major steps. Its first step resamples the exam-ples of each class. Each class is resampled in a consistent manner by considering a factor of ( N max  X  N i ), where N i is the number of the examples of the and N max is the number of the examples for the majority class of the problem. The second step of Global-IC generates the synthetic examples for the small classes. The synthetic ex-amples are generated based on our proposed immune centroids over-sampling tech-nique (ICOTE), which is the core of our Global-IC. Briefly, the synthetic examples are derived from the immune network and repre-sent the internal images of original small class examples. The details of ICOTE will be discussed later. The pseudo code of our Global-IC algorithm is shown in Algorithm 1. Note that the size of the majority class will not be increased. Algorithm 1. Global-IC (im-mune centroids). The shape of the spatial distribution of the immune cen-troids follows that of the original examples. As illustrated in Fig.1 and Fig. 2, Global-IC introduces the immune centroids and follows the shape of the neigh-boring minority class examples. Global-IC thus not only creates larger and less specific decision regions, but also overcomes small disjunct problem introduced by over-sampling [ 13 ]. After we present the framework (Global-IC) of our solution, here we will discuss the details of the core of Global-IC, ICOTE. ICOTE uses the aiNet model [ 3 ] to generate antibody-derived synthetic examples from the original examples of a class. It includes five major steps as follows: Step 1: Attribute selection In order to reduce computational cost, we first remove the attributes whose values are constant.
 Step 2: Unit-based normalization Then we adjust the values of attributes on different scales to a notionally common scale [0, 1]. Step 3: Immune centroids generation There are three sub-steps to generate immune centroids. First, the selected antibodies Ab are going to proliferate (clone) proportionally to their antigenic affinity. The higher the affinity, the larger the clone size nc is for each selected antibody. Note that the affinity (complementarity level) of the antigen-antibody match is measured by their Euclidean distance, which is inversely proportional to their Euclidean distance. That is, the smaller the distance, the higher the affinity is, and vice-versa. Formally, the Euclidean distance of two vectors is defined as follows. where m is the dimension of each vector.
  X  , which is inversely proportional to the antigenic affinity of its parent antibody Ag . antigen-antibody affinity (clonal suppression) f ij and a high antibody-antibody affinity (network suppression) f ij . Step 4: De-normalization Next, we de-normalize memory antibodies M and make synthetic examples identical to sample distribution. Step 5: Attribute recovery The last step, we put back constant-value attributes that are removed in Step 1.
 rithm 2: In this section, we will investigate the performance of our proposed method Global-IC, and compare it with existing well-known resampling methods. 4.1 Experimental Settings Our experiments are conducted using three base classifiers: kNN [ 15 ], C4.5 [ 16 ] and SVM [ 17 ], respectively. We use these algorithms, since they are available within the KEEL software tool [ 14 ]. In the experiments, the parameter values are set based on the recommendations from the corresponding authors. The specific settings are as follows: 1. Instance based learning kNN [ 15 ]: In this algorithm, we set k=3 and use the 2. C4.5 Decision tree [ 16 ]: For C4.5, we set a confidence level as 0.25, the 3. Support vector machines (SVM) [ 17 ]: We choose Polykernel reference func-Algorithm 2. ICOTE We conduct experiments on 12 datasets from the KEEL dataset repository whose characteristics are summarized in Table 1, namely the number of examples (#Ex.), number of attributes (#Atts.), and the number of examples in each class(separated by comma). The experiments are evaluated in terms of one of the popular metrics, the Area Under the ROC Curve (AUC) [ 18 ]. Our experimental results are obtained based on 10-fold cross-validation. 4.2 Experimental Results In this section, we investigate the performance of different methods on the imbal-anced datasets listed in Table 1.
 sampling X  and  X  X VO+cost-sensitive learning X  outperforms both the direct multi-class decompo-sition schemes OVO and OVA in almost all the cases. So we will investigate our proposed method Global-IC with OVO+over-sampling and OVO+cost-sensitive learning. In order to study the combination of pre-processing and cost-sensitive ap-proaches for multi-class imbalance learning, we combine OVO with four representa-tive methods, namely ROS [ 21 ], SMOTE-ENN [ 21 ], SMOTE [ 5 ], and CS [ 20 ]. In addition, since our Global-IC is closely related to Global-GS[ 8 ], Static-SMOTE [ 9 ], and AdaBoost.NC [ 10 ]. All of them are the directed multi-class imbalance learning methods. It is nature for us to make comparisons among these methods. The average experimental results for each method are shown in Table 2-4 respectively, in term of three different base learners, i.e., KNN, C4.5, and SVM.
 than other seven resampling methods, on all the three base learners. When we used KNN as the base learner, our Global-IC performs the best on nine out of 12 datasets. Its average AUC over the 12 datasets is 85.08, which is much higher than the second highest (76.70) achieved by OVO+SMOTE. When we used C4.5 as the base learner, our Global-IC performs the best on 10 out of 12 datasets. Its average AUC over the 12 datasets is 88.38, which is much higher than the second highest (78.81) achieved by Ada-Boost.NC. When we used SVM as the base learner, our Global-IC also performs the best on 10 out of 12 datasets. Its average AUC over the 12 datasets is 84.57, which is much higher than the second highest (78.88) achieved by Global-CS.
 ond best method among the eight approaches on all the three base learners. From the average results shown in Table 2-4, we can see that the ranks of the perfor-mance of all other methods are varied with the base learner. Besides the average results shown in Table 2-4, we also rank these methods on each dataset with each base learner for further comparison analysis. The average rank of each method with each base learner is shown in Fig.3. From Fig.3, we can see that the aver-age rank of Global-IC is the best under the three base learners. The ranks of the other methods depend on the base learner. The other three directed multi-class imbalance learning methods Global-CS, Static-SMOTE and AdaBoost.NC do not consistently rank higher than the OVO combination methods (i.e., OVO+ROS, OVO+SMOTE-ENN, OVO+SMOTE, and OVO+CS). OVO+CS has a relatively robust rank with the three base learners.
 In this paper we present a novel global multi-class imbalance learning approach Global-IC, which does not need to transfer multi-class into binary. This novel approach is based on immune network theory, and generates immune centroids for the clusters of each small class, which have high data density. It is completely different from renowned resampling methods. Our experimental results showed that Global-IC achieves better performance, comparing with existing methods.
