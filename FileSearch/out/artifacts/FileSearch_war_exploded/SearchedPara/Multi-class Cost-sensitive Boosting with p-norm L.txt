 I.2.6 [ Artificial Intelligence ]: Learning Algorithms cost-sensitive learning, multi-class classification, boosting
Cost-sensitive learning involves classification tasks in pres-ence of varying costs associated with different types of mis-classification, such as false-positives and false-negatives. A large number of practical application domains motivate cost-sensitive learning, as documented in the literature: examples include targeted marketing, medical diagnosis, and fraud de-tection. There has been considerable theoretical as well as empirical research on this topic, both in the machine learn-ing and data mining communities [7, 5, 3, 13, 15, 16, 18].
For pure classification, extensive past research has estab-lished that the family of boosting methods, including Ada-Boost [6] and its many variations, enjoys superior empirical performance and strong theoretical guarantees. For cost-sensitive learning, however, there has not been a compre-hensive study of relative merits of different boosting algo-rithms. Some attempts have been made to extend the Ada-Boost algorithm into cost-sensitive versions, e.g. AdaCost [5] and CSB2 [13], but the aggressive weight updating scheme based on the exponential loss posed difficulties in balancing the contributions of the cost information and boosting X  X  fo-cus on misclassification error. More recently, an effort was made to bridge this gap with the proposal of a cost-sensitive boosting method called GBSE [1], inspired by the framework of gradient boosting, but only a partial theoretical justifica-tion was provided, where the proof of convergence was given for a variant of the proposed method.

In this paper, we propose a class of new cost-sensitive boosting methods by applying the theory of gradient boost-ing to a family of p -norm cost functionals, and investigate their theoretical and empirical properties. The p -norm cost functionals include, as special cases, the linear cost (ex-pected cost) and the squared loss based cost functional. We derive a family of general multi-class cost-sensitive boosting methods, which use binary weak classifiers, and establish some basic properties for this family, including proof of con-vergence and the rate thereof. We also give interpretations for some of the existing algorithms in terms of the proposed family, notably including a generalization of the costing al-gorithm [16], DSE and GBSE [1], and the Average Cost method [8].

We also empirically compare the performance of the pro-posed family of algorithms with representative existing meth-ods of cost-sensitive boosting, including AdaCost [5], CSB2 [13] as well as AdaBoost.M2 [6] with cost-sensitive weight initialization. Our experiments, using multi-class data sets from the UCI repository and KDD cup 99 under multiple cost scenarios, indicate that the proposed methods based on p -norm cost functionals attain excellent performance in terms of cost minimization. More specifically, the general-ized costing algorithm for the linear loss function compares favorably against AdaCost and CSB2, but does not out-perform AdaBoost with cost-sensitive weight initialization. In contrast, the variants with higher order p -norm loss func-tions, including squared loss, strictly out-perform all three of the comparison methods, establishing the advantage of the proposed family of methods. This advantage is observed for different scenarios, including uniformly random cost matri-ces and those that are (randomly) inversely proportional to class frequencies.
We first describe the framework for cost sensitive learning along with related concepts and notation used in this paper.
We consider a general formulation of cost-sensitive (mul-ticlass) classification 1 where a cost function C ( x, y 1 used to specify the cost of predicting that an example x belongs to class y 2 when the correct label is y 1 . Formally, denote by X the input space and by Y the set of classes. Let k = | Y | . We assume that examples ( x, ~ C ) are drawn from a distribution D over X  X  R + k . Here ~ C is the vector of costs C x,y = C ( x, y, y  X  ) where y  X  denotes the label with mini-mum cost and y  X  Y . We note that the above formulation allows C to depend on the individual instances, x , gener-alizing the common formulation in terms of cost matrices, following [15].

Based on a sample S = { ( x, ~ C ) } drawn i.i.d. from D , we wish to find a classifier h : X  X  X  1 , . . . , k } which minimizes the expected cost Without loss of generality we assume that the costs are nor-malized so that  X  x  X  X C x,y  X  = C ( x, y  X  , y  X  ) = 0 . Then the problem is equivalent to the minimization in term of misclassification cost, i.e. where I (  X  ) denotes the indicator function.

Our proposed methods will make use of importance weigh-ted classification , which we review below. In importance-weighted classification, examples of the form ( x, y, w ) are drawn from a distribution D over X  X  Y  X  R + . Given a training set S = { ( x, y, w ) } the goal is to find a classifier h : X  X  Y having minimum expected weighted misclassifi-cation error:
The aforementioned concepts were introduced in terms of functional hypotheses h , i.e. h : X  X  Y. but also apply to stochastic hypotheses, namely hypotheses h : X  X  Y  X  [0 , 1] satisfying the stochastic condition  X  x  X  X In particular a stochastic cost-sensitive learner is expected to minimize Since D is unknown, one could consider methods which, given a training sample, attempt to minimize the sample average  X 
This general formulation allowing costs to depend on individual instances was first proposed by Zadrozny &amp; Elkan [15].
 Such procedures, however, are computationally intractable for many hypothesis classes, one major obstacle being the non-convexity of the objective. To remedy this issue, the learning methods proposed in this paper are based on the minimization of a convex surrogate of the objective. Notice that max y h ( y | x ) = k h ( y | x ) k  X  . Thus which can be approximated by mization of the following convexification of the original ob-jective of Eq. 1: We expect that the larger p is the closer it approximates Eq. 1. The minimization of the convex objective as defined in Eq. 2 is carried out by adopting a boosting-style functional gradient descent approach (see [10, 9]) combined with a stochastic interpretation of ensemble hypotheses. We now elaborate on our methodology.

Given multiple functional hypotheses h t , t = 1 , . . . , T, we define a stochastic ensemble hypothesis H as the conditional distribution resulting from the mixture of the component hypotheses, namely, To solve Eq. 2, an incremental algorithm is used, which, at each round t , updates the current ensemble hypothesis by the convex combination of the previous ensemble hypothesis H t  X  1 and a new hypothesis h t , i.e., by setting where  X   X  [0 , 1] .
 operator. The new hypothesis h t is output by a weak learner so as to maximize  X  X  X  X  L ( H t  X  1 ) , f  X  H t  X  1  X  , where We remark that Mason et al. [10] also used this type of gradi-ent descent formulation. By the Fr  X echet-like differentiability condition of the p -norm cost functional considered So at each iteration t , h t is chosen to minimize where This optimization problem with respect to these particular weights is the basis of the family of methods proposed in this paper. Note, in particular, that the special case in which p = 1 , the weights become w x,y = C x,y , corresponding to various existing methods with cost based weighting, including the costing algorithm [16] and DSE [1].
We now consider implementing the optimization problem of Eq. 4 using binary base learning procedures. While it is also possible to use a multi-class classifier as the base learner, in practice it tends not to work as well as those based on binary classifiers, hence our focus on binary weak learners. When converting multiclass into binary classifica-tion, it is useful to consider the notion of relational hypothe-ses, namely those that are relations over X  X  Y : h : X  X  Y  X  { 0 , 1 } .

A straightforward multiclass method to find a new weak hypothesis consists in using a weak learner minimizing the weighted classification error for the expanded data set and weights (It was shown in [1] that this also minimizes Eq. 4.) We remark that for linear loss ( p = 1 ), the resulting procedure is identical to the DSE Method [1], as the weights reduce to max y 0 C x,y 0  X  C x,y , which are constant over the itera-tions. Notice that with this expanded dataset formulation all the labels are effectively treated as correct labels, albeit to varying degrees due to having different weights, resulting in sub-optimal performance in practice. In subsequent de-velopments, we address this issue in a number of different ways, resulting in various concrete algorithms implementing the weighting scheme of Eq. 4.
Consider again the optimization problem of Eq. 4, and the corresponding weights in Eq. 5. Notice that C x,y  X  = 0 implies w x,y  X  = 0 , and hence the optimization problem of Eq. 4 effectively involves the dataset For stochastic hypotheses, ( x, y  X  ) is indirectly taken into ac-count, since for any such hypothesis f ,  X  x f ( y  X  | x ) = 1  X  P y  X  Y,y 6 = y  X  f ( y | x ) . For relational hypotheses h : X  X  Y  X  { 0 , 1 } , however, the minimization of Eq. 4 can be achieved simply by assigning h ( x, y ) = 0 everywhere. The pseudo-loss [6] is thus introduced as a way to explicitly incorporate h ( x, y  X  ) in the objective:
We can reformulate this minimization problem as a weigh-ted binary classification problem by converting the weighted sample { ( x, y, w x,y ) , x  X  X, y  X  Y } into  X  An input sample S = { ( x, ~ C ) } .  X  A component learner A for importance weighted  X  An integer T specifying the number of iterations to 1. Set S 0 = 2. Initialize H 0 by  X  x  X  X, y  X  Y H 0 ( y | x ) = 1 /k. 3. For t:=1 to T Do 4. End For 5. Return H T .
 Figure 1: Method L p -CSB (Cost Sensitive Boosting with p -norm Loss) where
The component learner is then to find a relational hypoth-esis h that minimizes the weighted error on S 2 , i.e., which is equivalent to minimizing the pseudo loss. The re-sulting procedure, L p -CSB (Cost Sensitive Boosting with p -norm Loss) is our main method and is depicted in Fig-ure 1.
We now discuss several ways to characterize the relation-ship between the pseudo-loss and the original loss For convenience we omit the factor 1 / 2 in the pseudo-loss and let since the minimizer is unaffected. Notice that Hence if for some hypothesis h ,  X  l w ( h ) =  X  then l w So an hypothesis with small pseudo-loss has small original loss as well.

The following proposition states that the pseudo loss can be seen as the Lagrangian associated with the minimization of the original loss subject to the stochasticity constraint.
Proposition 3.1. Consider the optimization problem Define the Lagrangian associated with this problem as For every ( x, y )  X  B where B is as defined in Eq. 6, let and for all x  X  X let  X  x = Proof. Using the fact that  X  x  X  X w x,y  X  = 0 , the Lagrangian can be rewritten as Now from Eq. 7, the pseudo-loss can be expressed as Then Eq. 8 and Eq. 9 are equal if  X  w x,y = w x,y  X   X  x + and  X  x  X   X   X  x =
P
For stochastic hypotheses, the equivalence between pseudo loss and original loss is expressed by the following proposi-tion.
 Proposition 3.2. For any stochastic hypothesis h : X  X  Y  X  X  0 , 1 } , l w ( h ) =  X  l  X  w ( h ) if where B is as defined in Eq. 6.

Proof.  X  An input sample S = { ( x, ~ C ) } .  X  A component learner A for importance weighted  X  An integer T specifying the number of iterations 1. Set S 0 = 2. Initialize H 0 by  X  x  X  X, y  X  Y H 0 ( y | x ) = 1 /k. 3. For t:=1 to T Do 4. End For 5. Return H T .
 Figure 2: Method L p -CSB-PA (Cost Sensitive Boost-ing with p -norm Loss and Pseudo-loss Adjustment)
Here Eq. 10 follows from the fact that h is stochastic and hence 1  X  h ( x, y  X  ) = w side of Eq. 11 equals l w ( h ) , since for all x  X  X w x,y  X  Now setting w x,y =  X  w x,y + P Proposition 3.2 naturally suggests an alternative method, L -CSB-PA (Cost Sensitive Boosting with p-norm Loss and Pseudo-loss Adjustment), which is similar to L p -CSB but where the weights w x,y are replaced by  X  w x,y , and stochastic hypotheses are required. The resulting method is shown in Figure 2
Note that, for the linear loss ( p = 1 ), the weights  X  w are identical to those of the  X  X verage Cost X  method due to Margineantu [8]. Also, when p = 2 , that is under squared loss, the weights become where These weights are remarkably similar ( though different ) to the weighting scheme of the GBSE-t algorithm [1] in the sense that for each sample ( x, y ) both weighting schemes involve relating the cost at ( x, y ) to the average cost incurred by the current hypothesis at x divided by the number of classes.

Notice that the weights  X  w x,y can be negative, which im-plies that the component learner is asked in step 3(d) of Fig-ure 2 to minimize weighted misclassification with positive  X  An input sample S = { ( x, ~ C ) } .  X  A component learner A (  X  S ) that takes a training sample  X 
S = { ( x, y, w x,y ) } , and output a stochastic hypothesis f that attempts to minimize  X  An integer T specifying the number of iterations to be 1. Set S 0 = 2. Initialize H 0 by  X  x  X  X, y  X  Y H 0 ( y | x ) = 1 /k. 3. For t:=1 to T Do 4. End For 5. Return H T .
 Figure 3: Method L p -CSB-A (Abstracted view of Cost Sensitive Boosting with p-norm Loss). and negative weights. This is a perfectly valid optimiza-tion problem, but implementing it using a standard weak learner requires a transformation: converting each example
For simplicity we consider the  X  X bstracted X  version of our methods depicted in Figure 3, which is expressed in terms of the original optimization problem stated in Eq. 4. Denote by F the class of base hypotheses and by H the set of convex combinations of hypotheses in F .

At each round t, the new hypothesis f t returned by the weak learner attempts to minimize w izes convergence in terms of the relative performance of the selected weak hypothesis in completing that task compared to that of the current composite hypothesis H t  X  1 .
Theorem 4.1. Consider the method L p -CSB-A. Assume that at each iteration t the new hypothesis f t returned by the weak learner is such that with  X  t  X  0 . Pick  X  t =  X  rithm converges to the global minimum of the cost L over H .

Proof. Notice that L is Lipschitz differentiable with Lip-schitz constant M, i.e. k X  L ( H 2 )  X  X  X  L ( H 1 ) k X  M k H for all H 1 , H 2 in H . Hence where the inequality is obtained by applying Lemma 3 of [10]. This upperbound can be optimized by setting: We thus have for this choice of  X  where the last inequality follows from the fact that k f  X  H k 2  X  4 , since H and f are defined on [0 , 1] . Combining Eq. 13 and Eq. 3 we obtain
L ( H t )  X  L ( H t  X  1 )  X   X  p Assume that if at a certain round t 0 Since L is lower bounded, this and Eq. 14 imply that the sequence H t converges to an accumulation point H . We conclude by using the proof of Theorem 6 of [10], which shows that any such accumulation point has minimum cost L over H .

The weak learning assumption required by the theorem is reasonable as f t is specifically picked as an attempt to minimize is reasonable to expect the former to outperform the latter. Note that the weak learning assumption is similar to what is asked of the weak learner in MarginBoost. L 1 [10], in the sense that it involves the weighted average of the difference between current composite classifier and weak hypothesis. The next theorem considers approximate minimization of P potheses as the weak learning condition, and provide some convergence rates for the L p -CSB-A procedure.

Theorem 4.2. Assume that at each iteration t of the al-gorithm L p -CSB-A the component learner returns hypothesis f such that we have for H t obtained by L p -CSB-A that Proof. From the proof of Theorem 4.1, we have
L ((1  X   X  ) H +  X f )  X  L ( H )  X  X  X  1 By convexity of L ,  X  X  X  X  L ( H ) , f  X  H  X  X  X  L ( H )  X  L ( f ) . Thus Using Eq. 3 and Eq. 5, notice that the weak learning as-sumption of Eq. 15 is equivalent to assuming that f t +1 is such that for some  X  t +1  X  0 . Here we used the fact that the supre-mum of a linear function over a convex polygon is achieved at one of the vertices. Let L + ( H ) = L ( H )  X  inf f  X  X  Combining Eq. 17, Eq. 18 and Eq. 19 leads to where the last inequality follows from the fact that L + ( H p  X  1 M, since it can easily be show that | L ( a )  X  L ( b ) |  X  p 2 p  X  1 | a  X  b | , for a, b  X  [0 , 1] . An upperbound for L can be obtained by maximizing the right hand side of Eq. 20 with respect to L + ( H t ) . We obtain The remainder of the proof consists in showing that Eq. 16 holds for all t . We do so by induction similarly to the proof of Theorem IV.2 in [17]. For t = 1 Eq. 21 implies L + ( H 2 M + 2  X  1  X  2 M + M 4 &lt; 9 M 3 . Assume that L + ( H t and that L + ( H t )  X  9 M t +2  X  4 M . Thus using Eq. 20, we get
We conducted systematic experiments to compare the per-formance of the proposed methods with a number of existing algorithms: AdaBoost.M2, AdaCost and CSB2, using multi-class data sets from the UCI repository and KDD cup 99. We did not explicitly compare our methods against gener-alized Costing, Average Cost, DSE and GBSE, since either they are contained in the L p -CSB family, or can be inter-preted as related methods. We elected to empirically com-pare the L p -CSB family, and not the L p -CSB-PA family, against the comparison methods, mainly due to the prac-tical issue posed by the fact that the L p -CSB-PA family expects a stochastic weak learner. A representative multi-class boosting method is the Ada-Boost.M2 algorithm, due to Freund and Schapire [6]. Here, we consider for comparison a version of this algorithm with the boosting weights initialized according to the misclassifi-cation costs. In obtaining a weak hypothesis with approx-imately minimum pseudo-loss, we use weighted sampling with the weighting scheme described in Section 3.3.1.
One simple way to make AdaBoost.M2 cost-sensitive is to initialize the boosting weights proportionally to the misclas-sification costs. That this leads to a method with a theoret-ical guarantee on the minimization of misclassification cost can be seen relatively easily from a theorem on the boosting property of a generalization of AdaBoost.M2 for multi-label classification called AdaBoost.MR [12]. Theorem 6 [12] es-sentially gives an upper bound on the so-called ranking loss with respect to the initial distribution w 1 , defined effectively over the expanded data set. Noting that the ranking loss provides an upper bound on the empirical weighted misclas-sification error, we see that initializing w 1 proportionally to the misclassification costs will yield an upper bound on the empirical misclassification cost.

Various authors have noted, however, that AdaBoost tends to forget the cost information embedded in the initial weights, as the boosting procedure progresses, and this has moti-vated them to propose more involved modifications to Ada-Boost for cost-sensitive learning [5, 13]. Here we briefly re-view a couple of representative methods in this family (cost-sensitive modifications of AdaBoost), namely AdaCost [5] and CSB2 [13].

Both of these modifications are based on the generalized version of AdaBoost for confidence-rated boosting, due to Schapire and Singer [12]. In particular for multi-class learn-ing, they can be obtained as modifications of AdaBoost.MR, the multi-class, multi-label version of AdaBoost. The weight update rule of AdaBoost.MR can be stated as follows. where h t ( x i , l ) is a possibly real-valued (confidence rated) prediction made by the t -th weak hypothesis on the instance x i and a label l , and  X  t is a real-valued parameter. This weight is to be defined for each correct label l 1 and wrong label l 0 .

AdaCost modifies the above update rule by introducing a cost-adjustment function  X  , as follows. Here we let  X  t,i denote h t ( x i , l 1 )  X  h t ( x i , l fined as  X  (  X , C ) = (1  X   X C ) , where  X   X  { X  1 , 0 , +1 } and the cost C is assumed to be in the range [0 , 1] . The re-sulting method generalizes the original AdaCost to multi-class problems, with the choice of cost-adjustment function made by Fan et al and used in their experiments. (i.e.  X  + =  X  (+1 , C ) = 1 2 (1  X  C ) and  X   X  =  X  (  X  1 , C ) = 1
The CSB family of cost-sensitive boosting methods [13] are also obtained by modifying the update rule of AdaBoost, but they differ from the AdaCost rule in that the cost adjust-ment is done multiplicatively, rather than on the exponent. Although CSB X  X  are not defined for relational hypotheses, which allow possible ties between competing labels, a rea-sonable generalization for the CSB2 rule is stated below. We use this generalized version of CSB2 in our experiments.
We conducted all of our experiments using randomly gen-erated cost matrices, except on the KDD cup 99 data set, for which we used the cost matrix provided as part of the problem description. The experiments with synthetic cost matrices are further divided into two types, depending on how the cost matrices are generated. We basically follow the cost matrix generation procedure from [3], with modifi-cations employed in [1]. In the first type of cost model, which we call the class frequency cost model , the rare classes tend to be assigned proportionally higher costs than the frequent uniform distribution from the range [0 , 2000  X  P ( y 2 ) / abilities) of classes y 1 and y 2 in the data. The diagonal entries of the cost matrix, C ( y, y ) , are assigned identically zero. (See [4] for the rationale.) This model reflects the sit-uations often faced in real world applications, in which rare classes are more important to classify correctly (e.g. frauds in fraud detection). In the second type of cost model, or the uniform cost model , the cost-matrices are generated by a procedure that assigns a random cost to any misclassifica-tion with a uniform distribution from the range [0 , 1000] . As data sets, we elected to use those data sets from UCI ML data repository [2] that (i) are multi-class data sets; and (ii) have a large enough data size (exceeding approximately 1,000). The second condition was placed in an attempt to make our experiments realistic , from the viewpoint of typical industrial or business applications of cost-sensitive learning.
As the  X  X eak X  learner, we use the weka j48 implementa-tion of C4.5 in all of our experiments [11, 14]. To implement the importance weighted classification required of the weak learner, we do weighted rejection sampling (as done in [16] for example), rather than feeding the weights as input to the weak learner. The number of boosting iterations in each method was set at 100, except for the largest  X  X etter X  data set, for which we did 30 iterations for all methods. For each experiment, 10 runs are run, each of which uses a randomly generated cost matrix according to a cost model, and the average test set cost and standard error are reported. The same 10 cost matrices are used for all the comparison meth-ods, to facilitate a fair comparison. Whenever available, we used the training and test data splits as originally provided in UCI and used it for all methods and cost matrices, and for other datasets, we did a random split and used the same split for all experiments.

We note that in all of our boosting methods, we elected not to optimize the mixture weights  X  , that is we always set  X  = 1 /t where t is the iteration number. We chose not to do so, in part because optimizing  X  has limited impact on the
We only consider CSB2 from the CSB family, since most of the methods in this family appear to have similar perfor-mance [13].
The costs used by Domingos [3] were actually [0 , 2000 / ( k  X  1)] , where k is the number of classes. performance, but also because it may introduce additional noise in performance, which may be undesirable for our goal of comparing competing methods. The results of these experiments are shown in Table 1, Table 2, Table 3, Table 4 and Table 5. The first two tables summarize the results for the class frequency cost model, giving the average test set cost and standard error for each of the 8 data sets, and for each of four methods consid-ered. 4 Table 1 compares the average costs of all comparison methods, including a generalization of the costing algorithm ( L p -CSB with p = 1 ), with L p -CSB with squared loss as the representative of the proposed family of methods. From these results, it appears convincing that the L p -CSB fam-ily of boosting methods out-perform all of the comparison methods we consider. It is interesting to note that the linear case (generalization of costing) does not consistently outper-form AdaBoost. We need the squared loss or higher p -norm, which both correspond to boosting weights that vary over iterations, to do so. Table 2 compares the performance of the L p -CSB family for different values of p. It is seen that for many datasets, the performance continues to improve for higher values of p , which is what would be expected by our motivation to approximate the objective cost function by a convex p-norm functional.

Table 3 and Table 4 give the analogous results for the uni-form cost model, which are also plotted in Figure 4. Note here that for datasets, Pendigits and Segmentation, the two cost models are equivalent, as these are perfectly balanced datasets. Interestingly, while the cost matrices are much less dramatic than in the first case, the general trend in the experimental results remain largely unchanged. It appears from these results that the performance of the proposed fam-ily of methods is robust, in the sense that it works well for highly skewed cost matrices as well as relatively moderate ones.
 Table 5 exhibits the results of our experiments on the KDD cup 99 data  X  the test cost (  X  1000 ), after 10 boost-ing iterations, averaged over 5 runs. 5 They suggest that the relative advantage of the proposed methods over the com-parison methods should persist with realistic cost matrices with mild cost values.
We have proposed a novel family of cost-sensitive boosting methods based on p -norm cost functionals and the gradient boosting framework. Our theoretical development provides a framework to interpret a variety of existing methods of cost-sensitive learning and their variants. In addition, we have provided empirical evidence that our approach can lead to excellent performance in practice, as the proposed family of methods outperforms representative algorithms on bench-mark data sets. A direction we wish to pursue in the near future is to conduct a systematic empirical evaluation of our second proposed family, L p -CSB, using a stochastic weak learner, and characterize its performance.
In these and subsequent tables, the figures that correspond to best performance are shown in bold font.
We note that, unlike in the original problem formulation, we used random splits of a subset of the training data (size 200 thousands) for both training and test. class frequency cost model: the average cost and standard error. standard error.
 uniformly random cost model: the average cost and standard error. Quadratic Loss, 8. 5th-power Loss [1] N. Abe, B. Zadrozny, and J. Langford. An iterative [2] C. L. Blake and C. J. Merz. UCI repository of [3] P. Domingos. MetaCost: A general method for making [4] Charles Elkan. Magical thinking in data mining: [5] W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan. [6] Y. Freund and R. E. Schapire. A decision-theoretic [7] U. Knoll, G. Nakhaeizadeh, and B. Tausend.
 [8] D. Margineantu. Methods for Cost-Sensitive Learning . [9] L. Mason, J. Baxter, P. Barlett, and M. Frean. [10] L. Mason, J. Baxter, P. Bartlett, and M. Frean. [11] J. Quinlan. C4.5: Programs for Machine Learning . [12] R. E. Schapire and Y. Singer. Improved boosting [13] K. M. Ting. A comparative study of cost-sensitive [14] I. H. Witten and E. Frank. Data Mining: Practical [15] B. Zadrozny and C. Elkan. Learning and making [16] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive [17] T. Zhang. Sequential greedy approximation for certain [18] Z. H. Zhou and X. Y. Liu. On multi-class cost
