 Popular commercial search engines usually pr ovide related search queries to users in search result page. Since users prefer usin g short queries [1,2] which may not express their information needs exactly, the recomme nded queries can help them to submit more precise query to search engine and s earch engine can satisfy users X  information needs more easily. Also, when some users do not know how to describe what they exactly desire, query recommendation can guide them to refine their queries in precise terms. We have analyzed large amounts of user query logs(same as the data which about 15% query sessions.So recommending adequate related queries to users is very important for enhancing the user experience. 
Considering a typical search session with query recommendation clicks,a user in-puts a query with ambiguous information needs. Unfortunately, the search results do some recommended query exactly describes what he wants, and he clicks it. After reviewing the new results list, he may get what he wants, or go on changing his query for more information, or even end his attempt. 
In such a session, any step may affect user experience. No matter how many times a user has changed his query, only when the search engine provides useful results, the user may get satisfied.Most research on query recommendation are focused on how to recommend queries which can describe users X  needs, despite whether the search en-gine can return high-quality results for recommended queries. In this case, a user may find some attractive queries by recommendation, but he will not get useful informa-tion after he clicked on it and get another result page. It can be conceivable that this kind of related queries has nothing to do with user experience. 
To improve user experience, we would like to progress the recommendation The key to this problem is developing an automatic method which can judge whether the search results of a query will satisfy the users. Many reasons cause a query doesnot have good results: no related resource on Internet, no related resource indexed by search engine, or useful web pages not shown unfeasible to evaluate the results by the possible reasons.In recent years, some works on user browsing information provide us new perspective for search result evaluation. In our work, we use some features based on user browsing information to help us evaluate search results. The work [3] of Baeza-Yates etc. uses click-through data in user query log clustering similar queries for query recommendation. In [3], term-weight vectors of the clicked URL X  X  for queries are used for aggregating the queries. Query-click bipartite graph is presented in [4] for query recommendation. Wen [5], Zaiane [6] and Cucerzan [7] introduces user X  X  query refinement information in logs for evaluating the similarity of different queries and the most similar queries are used for recommendation. Liu X  X  work [9] considers the relationship between two queries more complicated. In [9], the recommendation method is asymmetrical which means two related queries may have different strength to recommend each other. 
In these works, different methods are tried to recommend related queries. Most ef-forts of these works are trying to evaluate the semantic similarity and recommend the queries may interest users. No related work has considered the search performance of recommended queries, which has great impact on user experience. 
There are some works trying to evaluate search performance of a query from user X  X  perspective. The work [10] investigated the strong relationship between result rele-vance and user satisfaction, but they had not proposed an automatic prediction proc-evaluation method which predicts user search goal success by modeling user behav-ior. They use sequences of search actions to represent users X  search process and esti-mate the probability of being successful with a continuous time Markov model. 
In our work, we consider the search performance of related queries from the users X  point of view while recommending related queries. We propose some search perform-ance features base on behavior features and combine them with other typical features of query recommendation. With these features, we develop a new recommendation method which ensures the recommended queries to show useful results and provide better user experience to users. To the best of our knowledge, our method is the first one which take the factor of user satisfaction into consideration. In our query recommendation method, we need a list of recommendation candidates first. The list could be raw, but it has to contain high-quality related queries as many related works in recommendation candidates generation. 
We collect query log data from a commercial search engine during a time period of stop words are removed. Some thesaurus and different names of same entity are merged in inverted index. 
For any input query, we can segment it into terms and retrieve them from the in-mendation candidates. This section presents the query recommendation re-ranking method with considering sumptions of high quality query recommendation. Section 4.2and 4.3 introduce the user behavior features and some other features we used for recommendation candi-dates re-ranking, and Section 4.4 shows how we use SVM to re-rank the candidates with the features we selected. 4.1 Query Recommendation with Good User Experience Before we discuss recommendation re-ranking, we should firstlydefine what an effec-tive query recommendation is. In other words, we should give a definition of a high-quality query recommendation which leads to good user experience. 
With analysis into user behavior information recorded in query log data, we pro-certain query Q proposed by a user U , a query recommendation R is effective if: 1. R is relevant with Q , and 2. R is better atdescribing U  X  X  potential information needs, and 3. Search results of R can meet U  X  X  potential information needs. The basic demands of the query recommendation we want is described above. In a user experience. When a user U inputs a query Q , U should be interested in something about Q . So it is unhelpful to recommend an irrelevant query R . If R is relevant to Q , query is  X  X indows Live Messenger download X , a query recommendation  X  X SN Messenger download X  may not be a high-quality one. Although it is relevant to input also necessary since a query recommenda tion with poor-performance search results leads to bad user experience, too. In the following work, the feature selection, training annotation, and recommendation evaluation are all based on these. 4.2 User Behavior Features As we present 3 basic criteriain the section before, the features for re-ranking should be indicative of some demands. While extracting candidates from user log as the described ofSection 3, the method prefers to select the queries which related in literal. Now we would like to choose more user behavior features which help us to evaluate whether a query may satisfy users. Note that all the features are for  X  X nput query Q i  X  recommendation candidate Q r  X  pair, although some are only related to Q r . 
After a user submits a query (either by input or by clicking related search) to the result page, he may havesome interaction with the elements on the page, such as clicking some result items or clicking  X  X ext Page X  for more results. These behaviors may contain information of user satisfaction on query Q r . The user behavior features are listed in Table 1. Introduction and analysis of some user behavior features is given after the table. FirstClickRank&amp;LastClickRank. The two features are about users X  clicks on search small. Conversely, if a query performs badly, a user may find the result he desires has However, the values of these features will be larger. 
For experimental analysis, we select a set of queries, and ask 3 annotators to search for well-performance queries and poor-performance queries. The distributions are shown in Fig. 1. It provides us the effectiveness of the features. HintClick&amp;NextPage. These two features are about users X  interaction with search the user should be able to find what he desires in top results. So the user does not need values of the two features should be smaller than the feature values of poor-performance queries.With the same annotated query set, we can also see the effec-tiveness of these features in Fig. 2by the distribution.The feature AdvancedSearch is similar to the two features. We will not go into analysis of it. AllClickNum. This feature is about users X  actions in a whole search session. Its value is the average number of users X  clicks in the session of a specific search query. When the query performs well, the session may end with few actions. But, if the query per-forms badly, a user may spend quite a lot of actions in finding satisfactory results. 
The effectiveness of AllClickNum is shown in Fig. 3. The figure presents that more than 82% of the well-performance search queries X  AllClickNum values are less than 3. In contrast, more than 50% of poor-performance search queries X  AllClickNum values are greater than 2.The feature AllLogRec is similar to AllClickNum. 4.3 Other Features search engine, query log or the Q i -Q r pair. They are shown in Table 2. comparatively popularity of Q r , which is also important to a query recommendation. 4.4 Re-ranking via SVM With the selected features, we can re-rank the candidates and get the query recom-mendation by algorithms based on machine learning. For the recommendation in a web search engine, we always need top N best related queries, so the algorithm should evaluate each candidate query with a continuous function and re-rank them by the evaluation result.SVM algorithm can give each sample a decision function value queries forrecommendation. 5.1 Training Data Annotation  X  X nput query  X  recommendation candidate query X  pairs. The sample set should con-tain both high-quality  X  X nput query  X  recommendation candidate query X  pairs and bad case pairs. 
For constructing the set, we extract top 2,000 hot queries from a commercial search engine X  X  query log. For each query, we extract at most 100 recommendation candi-dates by the method in Section 3, and get about 200,000  X  X nput query  X  recommenda-tion candidate query X  pairs. Among these pairs, we choose some obvious high-quality recommendation pairs as positive cases, and choose the pairs not related or not unable not be annotated. 
Two annotators have worked for the annotation. Only when two annotators have annotation, we get 1,065 positive pairs and 681 negative pairs, which come from 123 input queries. These annotated pairs form a training set for re-ranking algorithm. 5.2 Recommendation Experiment With annotated data, we use SVM algorithm to build a recommendation candidates re-ranking model. We filtered out pornography queries from top 2,000 hot queries and get 1,499 input queries for the following experiment. 
We use the trained model to re-rank the recommendation candidates of 1,499 input queries, and get top 10 candidate queries of each input query for recommendation. Altogether, 14,413 recommended queries are generated for 1,499 input queries. 5.3 Evaluation of Recommendation X  X  Relevance and Search Performance 5.3.1 Evaluation Framework and Indicator In Section 4.1, we propose 3 criteria to judge whether a query recommendation is effective. With the criteria, we can evaluate a query recommendation X  X  effectiveness. The experiment in Section 5.3 shows a framework which we propose for evaluating an  X  X nput query  X  recommended query X  pair with Criterion 1 and 3. 
For evaluating an  X  X nput query  X  recommended query X  pair ( A , B ), we use three factors C B , R A, B , and E B . Table 3 shows the meaning and the range of the factors. 
C B is a basic requirement for an effective recommended query. R A,B is correspond-ers X  information needs, and it is an indicator of Criterion 3. 
For evaluation, we select 29 input queries with different search frequency of the 1,499 input queries. For each input query, we have 10 related queries recommend by our method and 10 queries recommend by the commercial search engine we mentioned before. So we need to annotate C B and R A, B for at most 580  X  X nput  X  X ecommended X  pairs (since there are overlaps between our recommendation and the search engine X  X ). For each of its top 5 search results, and then we can getDCG 5 of B . recommended query X  pair ( A , B ) by the following equation. 5.3.2 Evaluation Results our method and 280 queries recommended by the search engine. The average score of our recommendation is 10.49 while the average score of the search engine X  X  recom-mendation is 9.51. Fig.4shows the distribution of S A, B values of different recommenda-tion.In Fig.4, we can find that the queries recommended by the search engine has a lot look into the values of the factors and find that for any factor the recommended que-ries of the search engine have more value of 0 than the ones recommended by us. For an input query A , our method gives a group of recommended queries B 1 , B calculate the average score of each group and have a comparison. Of 29 input queries we annotated, 22 groups of our method X  X  recommendation have higher scores than the search engine X  X  recommendation. 5.4 Popularity of Recommended Queries As the assumptions of a high-quality query recommendation in Section 4.1, a good recommended query should describe users X  need s and attract users to click. This char-acteristic of recommended queries can be evaluated by analyzing the recommended queries X  click frequency in search engine log. The following work would evaluate the popularity of the query recommendation generated by our method. We collect 30-days click-through log of a popular commercial search engine in China from Nov 2009 to Dec 2009. We extract 30,057 different clicked recommended other queries which we do not recommend is called subset B . 
We count the click frequency for all 30,057 extracted recommended queries, and compare the click frequency of the queries recommended by our method(subset A ) and the ones we do not recommend(subset B ). The average click frequency of subset A is 253.97, while subset B  X  X  average click frequency is 230.22. 
For a more detailed analysis, we put the queries in different buckets by their click each subset are shown in Fig. 5. 
Fig. 5shows that queries in subset A are more concentrated in high-frequency buckets than the ones in subset B . But we can also find that in the two buckets with subset A performs much better than B in the buckets with frequency from 64 to 1,024. We look into the recommended queries with top click frequencies in subset B and find sults. But it doesnot happen in subset A . The contrast shows our method can filter out the queries with poor search performance. 
In summary, theexperiment results show us that the recommendation of our method is more popular than the recommendation of the commercial search engine in average while popular queries with bad search results are not recommended by us. We have presented a new point of view on good query recommendation. We gave some assumptions of high quality query recommendation, and proposed a new query recom-mendation method which can recommend relevant, popular queries with high perform-ance search results. Then we evaluated the query recommendation generated by our method. It is shown that our recommendations perform better than a commercial search engine in different aspects. 
Future study will focus on the performance of our methodon long queries and low search frequency queries. Traditional query recommendation method can also be introduced into our framework. 
