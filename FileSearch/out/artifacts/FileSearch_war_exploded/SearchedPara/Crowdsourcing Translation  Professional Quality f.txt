 In natural language processing research, translations are most often used in statistical machine translation (SMT), where systems are trained using bilingual sentence-aligned parallel corpora. SMT owes its ex-istence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to pro-duce state-of-the-art results for language pairs like Arabic X  X nglish, where there is ample data. How-ever, large bilingual parallel corpora exist for rela-tively few languages pairs.

There are various options for creating new train-ing resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are bet-ter suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolin-gual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, be-cause it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a Tamil-English corpus at $0.36/word. At that rate, translat-ing enough data to build even a small parallel corpus like the LDC X  X  1.5 million word Urdu X  X nglish cor-pus would exceed half a million dollars.

In this paper we examine the idea of creating low cost translations via crowdscouring. We use Ama-zon X  X  Mechanical Turk to hire a large group of non-professional translators, and have them recreate an Urdu X  X nglish evaluation set at a fraction of the cost of professional translators. The original dataset al-ready has professionally-produced reference trans-lations, which allows us to objectively and quantita-tively compare the quality of professional and non-professional translations. Although many of the in-dividual non-expert translators produce low-quality, disfluent translations, we show that it is possible to get high quality translations in aggregate by solicit-ing multiple translations, redundantly editing them, and then selecting the best of the bunch.

To select the best translation, we use a machine-learning-inspired approach that assigns a score to each translation we collect. The scores discrimi-nate acceptable translations from those that are not (and competent translators from those who are not). The scoring is based on a set of informative, intu-itive, and easy-to-compute features. These include country of residence, number of years speaking En-glish, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators, with the weights set using a small set of gold standard data from profes-sional translators. To collect crowdsourced translations, we use Ama-zon X  X  Mechanical Turk (MTurk), an online market-place designed to pay people small sums of money to complete Human Intelligence Tasks (or HITs)  X  tasks that are difficult for computers but easy for people. Example HITs range from labeling images to moderating blog comments to providing feedback on relevance of results for search queries. Anyone with an Amazon account can either submit HITs or work on HITs that were submitted by others. Work-ers are referred to as  X  X urkers X , and designers of HITs as  X  X equesters. X  A Requester specifies the re-ward to be paid for each completed item, sometimes as low as $0.01. Turkers are free to select whichever HITs interest them, and to bypass HITs they find un-interesting or which they deem pay too little.
The advantages of Mechanical Turk include:  X  zero overhead for hiring workers  X  a large, low-cost labor force  X  easy micropayment system  X  short turnaround time, as tasks get completed  X  access to foreign markets with native speakers
One downside is that Amazon does not provide any personal information about Turkers. (Each Turker is identifiable only through an anonymous ID like A23KO2TP7I4KK2.) In particular, no in-formation is available about a worker X  X  educational background, skills, or even native language(s). This makes it difficult to determine if a Turker is qualified to complete a translation task.

Therefore, soliciting translations from anony-mous non-professionals carries a significant risk of poor translation quality. Whereas hiring a profes-sional translator ensures a degree of quality and care, it is not very difficult to find bad translations provided by Turkers. One Urdu headline, profes-sionally translated as Barack Obama: America Will Adopt a New Iran Strategy , was rendered disfluently by a Turker as Barak Obam will do a new policy with Iran . Another translated it with snarky sar-casm: Barak Obama and America weave new evil strategies against Iran . Figure 1 gives more typical translation examples. The translations often reflect non-native English, but are generally done conscien-tiously (in spite of the relatively small payment).
To improve the accuracy of noisy labels from non-experts, most existing quality control mechanisms employ some form of voting, assuming a discrete set of possible labels. This is not the case for trans-lations, where the  X  X abels X  are full sentences. When dealing with such a structured output, the space of possible outputs is diverse and complex. We there-fore need a different approach for quality control. That is precisely the focus of this work: to propose, and evaluate, such quality control mechanisms. In the next section, we discuss reproducing the Urdu-to-English 2009 NIST evaluation set. We then describe a principled approach to discriminate good translations from bad ones, given a set of redundant translations for the same source sentence. 3.1 The Urdu-to-English 2009 NIST We translated the Urdu side of the Urdu X  X nglish test set of the 2009 NIST MT Evaluation Workshop. The set consists of 1,792 Urdu sentences from a vari-ety of news and online sources. The set includes four different reference translations for each source sentence, produced by professional translation agen-cies. NIST contracted the LDC to oversee the trans-lation process and perform quality control.

This particular dataset, with its multiple reference translations, is very useful because we can measure the quality range for professional translators, which gives us an idea of whether or not the crowdsourced translations approach the quality of a professional translator. 3.2 Translation HIT design We solicited English translations for the Urdu sen-tences in the NIST dataset. Amazon has enabled payments in rupees, which has attracted a large de-mographic of workers from India (Ipeirotis, 2010). Although it does not yet have s direct payment in Pakistan X  X  local currency, we found that a large con-tingent of our workers are located in Pakistan.
Our HIT involved showing the worker a sequence of Urdu sentences, and asking them to provide an English translation for each one. The screen also included a brief set of instructions, and a short ques-tionnaire section. The reward was set at $0.10 per translation, or roughly $0.005 per word.

In our first collection effort, we solicited only one translation per Urdu sentence. After confirming that the task is feasible due to the large pool of work-ers willing and able to provide translations, we car-ried out a second collection effort, this time solicit-ing three translations per Urdu sentence (from three distinct translators). The interface was also slightly modified, in the following ways:  X  Instead of asking Turkers to translate a full doc- X  We converted the Urdu sentences into images  X  We collected information about each worker X  X 
The translations from the first pass were of notice-ably low quality, most likely due to Turkers using automatic translation systems. That is why we used images instead of text in our second pass, which yielded significant improvements. That said, we do not discard the translations from the first pass, and we do include them in our experiments. 3.3 Post-editing and Ranking HITs In addition to collecting four translations per source sentence, we also collected post-edited versions of the translations, as well as ranking judgments about their quality.

Figure 2 gives examples of the unedited transla-tions that we collected in the translation pass. These typically contain many simple mistakes like mis-spellings, typos, and awkward word choice. We posted another MTurk task where we asked workers to edit the translations into more fluent and gram-matical sentences. We restrict the task to US-based workers to increase the likelihood that they would be native English speakers.

We also asked US-based Turkers to rank the trans-lations. We presented the translations in groups of four, and the annotator X  X  task was to rank the sen-tences by fluency, from best to worst (allowing ties).
We collected redundant annotations in these two tasks as well. Each translation is edited three times (by three distinct editors). We solicited only one edit per translation from our first pass translation effort. So, in total, we had 10 post-edited translations for each source sentence (plus the four original transla-tions). In the ranking task, we collected judgments from five distinct workers for each translation group. 3.4 Data Collection Cost We paid a reward of $0.10 to translate a sentence, $0.25 to edit a set of ten sentences, and $0.06 to rank a set of four translation groups. Therefore, we had the following costs:  X  Translation cost: $716.80  X  Editing cost: $447.50  X  Ranking cost: $134.40 (If not done redundantly, those values would be $179.20, $44.75, and $26.88, respectively.)
Adding Amazon X  X  10% fee, this brings the grand total to under $1,500, spent to collect 7,000+ transla-tions, 17,000+ edited translations, and 35,000+ rank labels. 1 We also use about 10% of the existing pro-fessional references in most of our experiments (see 4.2 and 4.3). If we estimate the cost at $0.30/word, that would roughly be an additional $1,000. 3.5 MTurk Participation 52 different Turkers took part in the translation task, each translating 138 sentences on average. In the editing task, 320 Turkers participated, averaging 56 sentences each. In the ranking task, 245 Turkers par-ticipated, averaging 9.1 HITs each, or 146 rank la-bels (since each ranking HIT involved judging 16 translations, in groups of four). Our approach to building a translation set from the available data is to select, for each Urdu sen-tence, the one translation that our model believes to be the best out of the available translations. We evaluate various selection techniques by compar-ing the selected Turker translations against existing professionally-produced translations. The more the selected translations resemble the professional trans-lations, the higher the quality. 4.1 Features Used to Select Best Translations Our model selects one of the 14 English options gen-erated by Turkers. For a source sentence s i , our model assigns a score to each sentence in the set of available translations { t i, 1 , ...t i, 14 } . The chosen translation is the highest scoring translation: tr ( s i ) = tr i,j  X  s.t. j  X  = argmax where score ( . ) is the dot product:
Here, ~w is the model X  X  weight vector (tuned as described below in 4.2), and ~ f is a translation X  X  cor-responding feature vector. Each feature is a function computed from the English sentence string, the Urdu sentence string, the workers (translators, editors, and rankers), and/or the rank labels. We use 21 features, categorized into the following three sets. Sentence-level (6 features). Most of the Turk-ers performing our task were native Urdu speakers whose second language was English, and they do not always produce natural-sounding English sentences. Therefore, the first set of features attempt to discrim-inate good English sentences from bad ones.  X  Language model features: each sentence is  X  Sentence length features: a good translation  X  Web n -gram match percentage: we assign a  X  Web n -gram geometric average: we calculate  X  Edit rate to other translations: a bad translation Worker-level (12 features). We add worker -level features that evaluate a translation based on who pro-vided it.  X  Aggregate features: for each sentence-level  X  Language abilities: we ask workers to provide  X  Worker location: two binary features reflect a Ranking (3 features). The third set of features is based on the ranking labels we collected (see 3.3).  X  Average rank: the average of the five rank la- X  Is-Best percentage: how often the translation  X  Is-Better percentage: how often the translation
Other features (not investigated here) could in-clude source-target information, such as translation model scores or the number of source words trans-lated correctly according to a bilingual dictionary. 4.2 Parameter Tuning Once features are computed for the sentences, we must set the model X  X  weight vector ~w . Naturally, the weights should be chosen so that good translations get high scores, and bad translations get low scores. We optimize translation quality against a small sub-set (10%) of reference (professional) translations.
To tune the weight vector, we use the linear search method of Och (2003), which is the basis of Min-imum Error Rate Training (MERT). MERT is an iterative algorithm used to tune parameters of an MT system, which operates by iteratively generating new candidate translations and adjusting the weights to give good translations a high score, then regener-ating new candidates based on the updated weights, etc. In our work, the set of candidate translations is fixed (the 14 English sentences for each source sen-tence), and therefore iterating the procedure is not applicable. We use the Z-MERT software package (Zaidan, 2009) to perform the search. 4.3 The Worker Calibration Feature Since we use a small portion of the reference trans-lations to perform weight tuning, we can also use that data to compute another worker-specific fea-ture. Namely, we can evaluate the competency of each worker by scoring their translations against the reference translations. We then use that feature for every translation given by that worker. The intuition is that workers known to produce good translations are likely to continue to produce good translations, and the opposite is likely true as well. 4.4 Evaluation Strategy To measure the quality of the translations, we make use of the existing professional translations. Since we have four professional translation sets, we can calculate the B LEU score (Papineni et al., 2002) for one professional translator P 1 using the other three P 2 , 3 , 4 as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of profes-sional quality translation. We can see how a trans-lation set T (chosen by our model) compares to this range by calculating T  X  X  B LEU scores against the same four sets of three reference translations. We will evaluate different strategies for selecting such a set T , and see how much each improves on the B LEU score, compared to randomly picking from among the Turker translations.

We also evaluate Turker translation quality by us-ing them as reference sets to score various submis-sions to the NIST MT evaluation. Specifically, we measure the correlation (using Pearson X  X  r ) between B LEU scores of MT systems measured against non-professional translations, and B LEU scores mea-sured against professional translations. Since the main purpose of the NIST dataset was to compare MT systems against each other, this is a more di-rect fitness-for-task measure. We chose the middle 6 systems (in terms of performance) submitted to the NIST evaluation, out of 12, as those systems were fairly close to each other, with less than 2 B LEU points separating them. 2 We establish the performance of professional trans-lators, calculate oracle upper bounds on Turker translation quality, and carry out a set of experiments that demonstrate the effectiveness of our model and that determine which features are most helpful.
Each number reported in this section is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets. Further-more, each of those 4 numbers is itself based on a five-fold cross validation, where 80% of the data is used to compute feature values, and 20% used for evaluation. The 80% portion is used to compute the aggregate worker-level features. For the worker cal-ibration feature, we utilize the references for 10% of the data (which is within the 80% portion). 5.1 Translation Quality: B LEU Scores We first evaluated the reference sets against each other, in order to quantify the concept of  X  X rofes-sional quality X . On average, evaluating one refer-ence set against the other three gives a B LEU score of 42.38 (Figure 3). A Turker set of translations scores 28.13 on average, which highlights the loss in quality when collecting translations from amateurs. To make the gap clearer, the output of a state-of-the-art machine translation system (the syntax-based variant of Joshua; Li et al. (2010)) achieves a score of 26.91, a mere 1.22 worse than the Turkers.
We perform two oracle experiments to determine if there exist high-quality Turker translations in the first place. The first oracle operates on the segment level: for each source segment, choose from the four translations the one that scores highest against the reference sentence. The second oracle operates on the worker level: for each source segment, choose from the four translations the one provided by the worker whose translations (over all sentences) score the highest. The two oracles achieve B LEU scores of 43.75 and 40.64, respectively  X  well within the range of professional translators.

We examined two voting-inspired methods, since taking a majority vote usually works well when deal-ing with MTurk data. The first selects the translation with the minimum average T ER (Snover et al., 2006) against the other three translations, since that would be a  X  X onsensus X  translation. The second method se-lects the translation that received the best average rank, using the rank labels assigned by other Turkers (see 3.3). These approaches achieve B LEU scores of 34.41 and 36.64, respectively.

The main set of experiments evaluated the fea-tures from 4.1 and 4.3. We applied our approach using each of the four feature types: sentence fea-tures, Turker features, rank features, and the cali-bration feature. That yielded B LEU scores ranging from 34.95 to 37.82. With all features combined, we achieve a higher score of 39.06, which is within the range of scores for the professional translators. 5.2 Fitness for a Task: Correlation With We evaluated the selection methods by measuring correlation with the references, in terms of B LEU scores assigned to outputs of MT systems. The re-sults, in Table 1, tell a fairly similar story as eval-uating with B LEU : references and oracles naturally perform very well, and the loss in quality when se-lecting arbitrary Turker translations is largely elimi-nated using our selection strategy.

Interestingly, when using the Joshua output as a reference set, the performance is quite abysmal. Even though its B LEU score is comparable to the Turker translations, it cannot be used to distinguish closely matched MT systems from each other. 3 The oracles indicate that there is usually an accept-able translation from the Turkers for any given sen-tence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations.

The results indicate that, although some features are more useful than others, much of the benefit from combining all the features can be obtained from any one set of features, with the benefit of adding more features being somewhat orthogonal.
Finally, we performed a series of experiments ex-ploring the calibration feature, varying the amount of gold-standard references from 10% all the way up to 80%. As expected, the performance improved as more references were used to calibrate the transla-tors (Figure 5). What X  X  particularly important about this experiment is that it shows the added benefit of the other features: We would have to use 30% X  40% of the references to get the same benefit ob-tained from combining the non-calibration features and only 10% for the calibration feature (dashed line in the Figure; B LEU = 39.06). 6.1 Cost Reduction While the combined cost of our data collection ef-fort ($2,500; see 3.4) is quite low considering the amount of collected data, it would be more attractive if the cost could be reduced further without losing much in translation quality. To that end, we inves-tigated lowering cost along two dimensions: elimi-nating the need for professional translations, and de-creasing the amount of edited translations.
The professional translations are used in our ap-proach for computing the worker calibration feature (subsection 4.3) and for tuning the weights of the other features. We use a relatively small amount for this purpose, but we investigate a different setup whereby no professional translations are used at all. This eliminates the worker calibration feature, but, perhaps more critically, the feature weights must be set in a different fashion, since we cannot optimize B LEU on reference data anymore. Instead, we use the rank labels (from 3.3) as a proxy for B LEU , and set the weights so that better ranked translations re-ceive higher scores.

Note that the rank features will also be excluded in this setup, since they are perfect predictors of rank labels. On the one hand, this means no rank labels need to be collected, other than for a small set used for weight tuning, further reducing the cost of data collection. However, this leads to a significant drop in performance, yielding a B LEU score of 34.86.
Another alternative for cost reduction would be to reduce the number of collected edited translations. To that end, we first investigate completely eliminat-ing the editing phase, and considering only unedited translations. In other words, the selection will be over a group of four English sentences rather than 14 sentences. Completely eliminating the edited translations has an adverse effect, as expected (Fig-ure 4). Another option, rather than eliminating the editing phase altogether, would be to consider the edited translations of only the translation receiving the best rank labels. This would reflect a data col-lection process whereby the editing task is delayed until after the rank labels are collected, with the rank labels used to determine which translations are most promising to post-edit (in addition to using the rank labels for the ranking features). Using this approach enables us to greatly reduce the number of edited translations collected, while maintaining good per-formance, obtaining a B LEU score of 38.67.

It is therefore our recommendation that crowd-sourced translation efforts adhere to the follow-ing pipeline: collect multiple translations for each source sentence, collect rank labels for the transla-tions, and finally collect edited versions of the top ranked translations. Dawid and Skene (1979) investigated filtering annotations using the EM algorithm, estimating annotator-specific error rates in the context of patient medical records. Snow et al. (2008) were among the first to use MTurk to obtain data for several NLP tasks, such as textual entailment and word sense dis-ambiguation. Their approach, based on majority voting, had a component for annotator bias correc-tion. They showed that for such tasks, a few non-expert labels usually suffice.

Whitehill et al. (2009) proposed a probabilistic model to filter labels from non-experts, in the con-text of an image labeling task. Their system genera-tively models image difficulty, as well as noisy, even adversarial, annotators. They apply their method to simulated labels rather than real-life labels.
Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. One such method was to collect reference translations to score MT output. It was only a pilot study (50 sentences in each of several languages), but it showed the pos-sibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch (2010) solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experi-ments, examining how well the collected transla-tions agreed with the professional translations when evaluating three MT systems.

That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), fo-cusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that work-shop were by Ambati and Vogel (2010), focusing on the design of the translation HIT, and by Irvine and Klementiev (2010), who created translation lexicons between English and 42 rare languages.

Resnik et al. (2010) explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source lan-guage paraphrased the corresponding source por-tion. The paraphrased source would then be re-translated to produce a different translation, hope-fully more coherent than the original. We have demonstrated that it is possible to ob-tain high-quality translations from non-professional translators, and that the cost is an order of magni-tude cheaper than professional translation. We be-lieve that crowdsourcing can play a pivotal role in future efforts to create parallel translation datasets. Beyond the cost and scalability, crowdsourcing pro-vides access to languages that currently fall outside the scope of statistical machine translation research. We have begun an ongoing effort to collect transla-tions for several low resource languages, including Tamil, Yoruba, and dialectal Arabic. We plan to:  X  Investigate improvements from system combi- X  Modify our editing step to collect an annotated  X  Calibrate against good Turkers, instead of pro- X  Predict whether it is necessary to solicit another  X  Analyze how much quality matters if our goal This research was supported by the Human Lan-guage Technology Center of Excellence, by gifts from Google and Microsoft, and by the DARPA GALE program under Contract No. HR0011-06-2-0001. The views and findings are the authors X  alone. We would like to thank Ben Bederson, Philip Resnik, and Alain D  X  esilets for organizing work-shops focused on crowdsourcing translation (Bed-erson and Resnik, 2010; D  X  esilets, 2010). We are grateful for the feedback of workshop participants, which helped shape this research.

