 chunks by using an overlapped sliding window. We decom-pose the query into exhaustive combinations (subsets) of query terms and consider them as latent variables over the chunks. We then propose using a Latent Variable Model (LVM) which connects a chunk d and a word w through the latent variables. The dependencies between the latent variables are governed by an ergodic Hidden Markov Model (HMM), where the Viterbi algorithm is applied to optimize parameters involved in the HMM and the underlying LVM.
In our model, the combinations of query terms are seen as the latent variables over the top-ranked documents. The reasons for selecting the query terms as the latent variables are that the word dependence to the query Q is to be esti-mated in the query language modeling framework, and the estimation of co-occurrence of words with query terms is key to query expansion. Eq. 1 shows an intuitive derivation of theory: S j is a latent variable in the set S ( S = { S 1 ,  X  X  X  ,S M } ) and w is the word whose occurrence probability in the ex-panded query model  X  Q to be estimated. The latent vari-able S j is generated from the query Q ,whereeach S j is de-fined as a combination of query terms. For example, given Q = { q 1 ,q 2 } , the set of latent variables can be represented methods the use of all the combinations of query terms ex-pands the observing space.

In Eq. 1, the relationship between the word w and the la-tent variable S j is derived from the relevance feedback doc-uments. As we have introduced in Section 1, the top-ranked documents in pseudo relevance feedback are not necessarily relevant to the query. Thus, we propose using the segmented chunks to connect S j and w , and the relevance of each chunk to the query will also be considered. Then, a new equation is given as below: d is the collection of chunks. P ( S j ) is the prior distri-bution of latent variables, P ( d i | S j ) is the probability of an observed chunk d i given a latent variable S j ,and P ( w | d i ,S j ) is the probability of a word w in a chunk d i given a specific latent variable S j .
 In order to calculate the three probability parameters in Eq. 2, we design a framework based on the Hidden Markov The second method, HMM-II , uses the linear interpolation to combine the original query model.
 The latter method involves manual adjustment of the inter-polation parameter to generate the optimal retrieval perfor-mance.
We evaluate our methods by testing TREC topics (Topics151 X  200, 601 X 700, and 501 X 550)on three large collections (AP88 X  90, ROBUST, and WT10G), respectively. we apply our ap-proach to two scenarios: pseudo-relevance feedback and true relevance feedback . As a comparison, we list the the values of MAP obtained by using HMM-II and HMM-I in Table 1.
 Table 1: Comparison of Optimal MAPs using HMM-II and HMM-I
Table 1 shows the optimal value when using HMM-II. It is found that using HMM-II can only obtain slightly and insignificantly better performances than HMM-I. However, HMM-I integrates original query model neatly in the Hid-den Markov Model in a fully automatic way. Therefore, the HMM-I has demonstrated its robustness and effectiveness from both theoretical and practical perspectives.
In this paper, we present a novel method to build a latent variable model using the HMM for query expansion. This paper tries to address some specific issues on the dependen-cies between query terms, the different degree of relevance of different chunks in the feedback documents to the query, and noise existing within the feedback documents. Our tech-nique incorporates these key issues in a single comprehen-sive framework and apply the HMM to estimating and opti-mizing the structure of the LVM. Our experimental results therefore show that our method always obtains significant improvements in comparison with KL, RM1 and RM2.
