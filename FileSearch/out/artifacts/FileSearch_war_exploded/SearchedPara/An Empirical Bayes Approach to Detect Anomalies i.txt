
We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a categorical variable. In typical data mining applications, the number of cells in such arrays are usually large. Our primary focus is de-tecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problem -performing multiple statistical tests on the same data produce excessive num-ber of false positives. We use an Empirical Bayes method which works by fitting a two component gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive  X  X er component error rate X  procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application in-formation and business intelligence from the daily logs that accompany large-scale spoken dialog systems deployed by AT&amp;T. We illustrate our method on one such system.
Consider a computational model of streaming data where a block of records are simultaneously added to the database at regular time intervals (e.g. daily, hourly etc) [13]. Our focus is on detecting anomalous behaviour by comparing data in the current block to some baseline model based on historic data. However, we are more interested in detecting anomalous patterns rather than detecting unusual records. A powerful way to accomplish this is to monitor statisti-cal measures (e.g., counts, mean, quantiles) computed for combinations of categorical attributes in the database. Con-sidering such combinations gives rise to a multidimensional array at each time interval. Each dimension of such an ar-ray corresponds to the levels of a categorical variable. We note that the array need not necessarily be complete i.e, only a subset of all possible cells might be of interest. A univariate measurement is attached to each cell of such an array. When the univariate cell measures are counts, such arrays are called contingency tables in Statistics. Hence-forth, we also refer to such arrays as -data streams. For instance, consider calls received at a call center and consider the two dimensional array where the first dimension corresponds to the categorical variable  X  X aller intent X (reason for call) and the second dimension corresponds to the  X  X riginating location X  (State where the call originates). A call center manager is often interested in monitoring daily percentages of calls that are attached to the cells of such an array. This is an example of a two di-mensional cross-classified data stream which gets computed from call logs that are added to the database every day.
Some other examples are a) daily sales volume of each item sold at thousands of store locations for a retail enter-prise. Detecting changes in cells might help for instance in efficient inventory management, provide knowledge of an emerging competitive threat. b) Emergency room visits at several hospitals with different symptoms. The anomalies in this case might point to an adverse event like a disease outbreak before it becomes an epidemic.

Apart from the standard reporting tasks of presenting a slew of statistics, it is often crucial to monitor a large num-ber of cells simultaneously for changes that take place rela-tive to expected behavior. A system that can detect anoma-lies by comparison to historical data provides information which might lead to better planning, new business strate-gies and in some cases might even lead to financial benefits to corporations. However, the success of such a system criti-cally depends on having resources to investigate the anoma-lies before taking action. Too many false positives would require additional resources, false negatives would defeat the purpose of building the system. Hence, there is need to have sound statistical methods that could achieve the right balance between false positives and false negatives. This is particularly important when monitoring data classified into a large number of cells due to the well known multiple hy-potheses testing problem.

Methods to detect changes in data streams have a rich literature in database and data mining. The primary focus of several existing techniques is efficient processing of data to compute appropriate statistics (e.g counts,quantiles,etc.), with change detection being done by using crude thresh-olds derived empirically or based on domain knowledge. For instance,[17] describe efficient streaming algorithms in the context of multiple data streams to compute statistics of interest (e.g. pairwise correlations) with change being signalled using pre-specified rules. Non-parametric pro-cedures based on Wilcoxon and Kolmogorov-Smirnov test statistics are proposed in [5] to detect changes in the sta-tistical distribution of univariate data streams. In [16], the authors describe a technique to detect outliers when mon-itoring multiple streams by comparing current data to ex-pected, the latter being computed using linear regression on past data. Our work, though related has important dif-ferences. First, we are dealing with cross-classified data streams which introduce additional nuances. Second, we adjust for multiple testing which is ignored by [16].
Adjusting for margins : When monitoring cells for de-viations, it is prudent to adjust for sharp changes in the marginal statistics. Failure to do so may produce anomalies which are direct consequences of changes in a small num-ber of marginals. For instance, it is not desirable to produce anomalies which indicate a drop in sales volume for a large number of items in a store merely because there was a big drop in the overall sales volume due to bad weather. We accomplish this by adjusting for the marginal effects in our statistical framework.

Multiple testing , also known as the multiple compar-isons problem has a rich literature in Statistics dating back to the 1950s. Broadly speaking, if multiple statistical tests are simultaneously performed on the same data, it tends to produce false positives even if nothing is amiss. This can be very serious in applications. Thus, if a call center manager is monitoring repair calls from different states, he might see false positives on normal days and stop using the system. Much of the early focus in multiple testing was on controlling the family wise error rates (FWER) (proba-bility of at least one false detection). If statistical tests are conducted simultaneously at per comparison error rate (PCER) of (probability of false detection for each individ-ual test), the FWER increases exponentially with . Bon-ferroni type corrections which adjust the PCERs to achieving a FWER of are generally used. However, such corrections may be unnecessarily conservative. This is es-pecially the case in data mining scenarios where is large. An alternate approach have been proposed in [4] which uses shrinkage estimation in a hierarchical Bayesian framework in combination with decision theory. Later, [15] proposed a method based on controlling the False Discovery Rate (FDR)(proportion of falsely detected signals) which is less strict than FWER and generally leads to gain in power com-pared to FWER approaches. In fact, controlling the FDR is better suited to high dimensional problems that arise in data mining applications and has recently received a lot of attention in Statistics, especially in genomics. Empirical and theoretical connections between Bayesian and FDR ap-proaches have been studied in [10][8]. Another approach to tackle the curse of multiple testing is based on random-ization [9] but might be computationally prohibitive in high dimensions. We take a hierarchical Bayesian approach in a decision theoretic framework similar in spirit to [4] but re-place the normal prior with a two component mixture as in [12]. An added advantage of the hierarchical Bayesian ap-proach over FDR is the flexibility it provides to account for additional features that might be present in some situations. For instance, if one of the dimension corresponds to spatial locations, correlations induced due to geographic proxim-ity are expected and could be easily accounted for. For a detailed introduction to hierarchical Bayesian models, we refer the reader to [3].
This research was motivated by the need to build a data mining tool which extracts information out of spoken di-alog systems deployed at call centers. The data mining tool built to accomplish this is called the VoiceTone Daily News (VTDN)[6] and supplements AT&amp;T X  X  call center ser-vice called VoiceTone by automatically extracting critical service information and busin ess intelligence from records of dialogs resulting from a customer calling an automated help desk. The Daily News uses the spoken dialog interac-tion logs to automatically detect interesting and unexpected patterns and presents them in a daily web-based newsletter intended to resemble on-line news sites such as CNN.com or BBC.co.uk. The front page news items are provided with links to precomputed static plots and a drill down capabil-ity, powered by a query engine and equipped with dynamic visualization tools that enables a user to explore relevant data pertaining to news items in great detail. The data min-ing task in this application involves three challenging steps, viz., a) extraction of relevant features from dialogues b) detect changes in these features and c) provide a flexible framework to explore the detected changes. Our focus in this paper is on task b), for complete details on a) and c) we refer the reader to [6].

To end this section, we briefly summarize our contribu-tions below.

We present a framework to detect anomalies in cross-classified data streams with potentially large number of cells. We correct for multiple testing using a hierarchical Bayesian model and suppress redundant alerts caused due to changes in the marginal distributions. We empirically il-lustrate the superiority of our method by comparison to a PCER method and illustrate it on a novel application that arise in speech mining.

The roadmap is as follows -section 2 describes the theo-retical setup for our problem followed by a brief description of the hierarchical Bayesian procedure called hbmix . Sec-tions 3 and 4 describe our data in the context of the VTDN application. Section 5 compare hbmix to a PCER method through simulation followed by an illustration of hbmix on actual data in section 6. We end in section 7 with discussion and scope for future work.
For ease of exposition, we assume the multidimensional array consists of two categorical variables with and lev-els respectively and note that generalization to higher di-mensions is similar. In our discussion, we assume the array is complete. In practice, this is usually not the case but the theory still applies. Let the suffix refer to the and levels of the first and second categorical variables respec-tively at time .Let denote the observed value which is assumed to follow a gaussian distribution. Often, some transformation of the original data might be needed to en-sure this is approximately true. For instance, if we observe counts, a square root transformation is adequate, for propor-tions arc sine ensures approximate normality. In general, the Box-Cox transformation with param-eters and chosen to  X  X tabilize X  variance if it depends on the mean is recommended.

For time interval , we may want to detect anomalies af-ter adjusting for changes in the marginal means. We show the difference between adjusting and not adjusting the mar-gins by using a toy example. Consider a table, the levels of the row factor being A,B and the column factor being a,b respectively. We denote the 4 cell entries corre-sponding to (Aa,Ab,Ba,Bb) by a vector of length 4. Let the expected values be (50,50,50,50) and the observed values be (25,25,75,75). Then the raw changes are (-25,-25,25,25) which are all large. The deviations after adjusting for the changes in the row and columns means are (0,0,0,0) pro-ducing no anomalies. Note that the significant values in the non-adjusted changes can be ascribed to a drop in the first row mean and a rise in the second row mean. Hence, non-adjusted cell changes contain redundant information. In such situations, adjusting for margins is desirable.
However, marginal adjustments are not guaranteed to produce a parsimonious explanation of change in all situ-ations. For instance, consider a second scenario where the observed values are (50,0,50,100). The raw and adjusted changes are (0,-50,0,50) and (25,-25,-25,25) respectively. The raw changes in this case produce two alerts which pinpoint the culprit cells that caused deviations in the row means, the adjusted changes would alert all four cell en-tries. To summarize, adjusting the margins work well when changes in the marginal means can be attributed to some common cause affecting a large proportion of cells associ-ated with the margins. Also, one byproduct is the automatic adjustment of seasonal effects, holiday effects, etc., that af-fect the marginals, commonplace in applications. However, if the marginal drops/spikes could be attributed to a few specific cells and the goal is to find them, the unadjusted version is suitable. In our application, we track changes in the margins separately(using simple process control tech-niques) and run both adjusted and unadjusted versions but are careful in interpreting the results. In fact, the adjusted version detects changes in interactions among the levels of categorical variables which might be the focus of several applications. For instance, in the emergency room exam-ple it is important to distinguish an anthrax attack from the onset of flu season. Since an anthrax attack is expected to be localized initially, it might be easier to identify the few culprit hospitals by adjusting for margins. Also, in higher dimensions one might want to adjust for higher order mar-gins, which is routine in our framework.
 Let denote historical information upto time .
 Deviations at time are detected by comparing the ob-served values  X  X  with the corresponding posterior pre-dictive distributions (expected distribution of data at time based on historic data until ) which in our set up are gaussian with means and variances Strategies to compute the posterior predictive distributions are discussed in section 2.1.

Letting ( denotes the random variable has a univariate normal dis-tribution with mean and variance ), the goal is to test for zero values of  X  X . For marginal adjustment, write overall, row and column effects respectively at which are unknown but plugged-in by their best linear unbiased es-timates) and the problem reduces to testing for zero val-ues of  X  X . More formally, with and want to test multiple hypotheses ( ,
A naive PCER approach generally used in process control[2] is to estimate with and declare the cell an anomaly if The central idea of the hierarchical Bayesian method hbmix is to assume  X  X  are random samples from some distri-bution . The form of may be known but depend on unknown parameters. For instance, [7] assumes to be prior probabilites for the unknown parameters. In [11], a non-parametric approach which assigns a Dirichlet process prior to is advocated but not pursued here due to com-putational complexity. Following [12] and [8], we take a semi-parametric approach which assumes to be a mix-ture i.e. a proportion of cells don X  X  change at time while the remainder are drawn from a normal distribution. We assume a log-logistic prior for centered at the harmonic mean of  X  X  as in [7] and a half-beta prior( ) centered around for ( is the estimated value of at time .) At time , we assume a uniform prior for .

Conditional on the hyperparameters ( , ),  X  X  are independently distributed as a two-component mixture of normals . The joint marginal likelihood of  X  X  are the product of the indi-vidual two-component mixture densities and from Bayes rule the posterior distribution of ( ) is proportional to the joint likelihood times the prior. The posterior distribu-tion of conditional on ( ) is degenerate at with probability and with probability it follows ( denotes density at for a normal distribution with mean and variance .) An Empirical Bayes ap-proach makes inference about  X  X  by using plug-in es-timates of the hyperparameters ( ) which are obtained as follows -compute the mode ( ) by maximizing the posterior of ( ) (for very large values of ,weuse a data squashing technique [14]) and define the estimates as ( ,wherethe smoothing constant is chosen in the interval .At time , . This exponential smoothing allows hyperparameters to evolve smoothly over time. In a fully Bayesian approach, inference is obtained by numerically in-tegrating with respect to the posterior of ( )usingan adaptive Guass Hermite quadrature. Note that the posterior distribution of depends directly on and indirectly on the other  X  X  through the posterior of the hyperparam-eters. Generally, such  X  X orrowing of strength X  makes the posterior means of  X  X  regress or  X  X hrink X  toward each other and automatically builds in penalty for conducting multiple tests.

A natural rule is to declare the cell anomalous when the posterior odds , which yields (after simpli-cation) where ( (log of the variance ratio) and monotonically increasing in both and . Thus, the cell penalty increases monotonically with predictive vari-ance. Also, the overall penalty of the procedure at time depends on the hyperparameters which are estimated from data. In fact, replacing  X  X  by their harmonic mean in (2) gives us a constant which provides a good measure of the global penalty imposed by hbmix at time .However, the loss assigned to false negatives by (2) does not depend on the magnitude of deviation of  X  X  from zero. Motivated by [4] and [12], we use a loss function where , ) is a parameter which represents the cost of a false negative relative to a false positive, de-notes change and denotes no change. With ,we recover (2) and gives us the loss function in [12]. In fact, is a sensible choice for the VTDN appli-cation where missing a more important news item should incur a greater loss. In our application we assume but remark other choices elicitated using domain knowledge are encouraged. Having defined the loss function, the opti-mal action(called the Bayes rule) minimizes the posterior expected loss of . In our setup, we declare a change if sion is a known function of hyperparameters and could be computed either by using plug-in estimates or numerical in-tegration.
Two popular approaches used to capture history are sliding window and exponential smoothing .Inthefor-mer, a window size is fixed a-priori and the distribu-tion at is assumed to depend only on data in the window approaches to maintain summary statistics under this model have been done (see [1] for an overview). In an exponential smoothing model, a decay parameter is used to downweight historic data with the weights dropping expo-nentially in the past.

In principle, any statistical model that could provide an estimate of posterior predictive means and variances could be used to obtain  X  X  and  X  X . Also, to be useful in streaming scenarios, the chosen model should easily adapt to new data.

For the VTDN application illustrated in this paper, we use a sliding window to capture . We assume the cells are uncorrelated and for the cell, ple mean of  X  X  and the posterior predictive variance is .Since is unknown, it is replaced by its estimator , the sample variance of  X  X . In order to adjust for seasonal effects, a separate sliding window is maintained for each season.
We illustrate and evaluate hbmix on a customer care(BCC) application supported by VoiceTone(client X  X  identity not disclosed due to reasons of confidentiality). Be-fore we describe the data, a high level description of the features extracted are given below(see [6] for complete de-tails).

A dialog is a stream of events (in XML) which is di-vided into a sequence of turns. A turn consists of a sys-tem prompt, the user response as recognized by the system, and any records associated with the system X  X  processing of that response. Each turn is mapped to one of a set of call types using BoosTexter -a member of the AdaBoost family of large-margin classifiers. A dialog ends when a goal is achieved by completing a transaction, for instance, or rout-ing the user to an appropriate destination.

The features that are currently extracted include the orig-inating telephone number for the call ( ANI ), the number of turns in a dialog ( NTURNS ), the length of the call ( DURATION ), any final routing destination the call gets routed to ( RD )andthe final actionable call type(FACT) . This is the last call type the classifier obtained in the course of the system X  X  dialog with the user before routing. FACT and RD are primary features tracked by the  X  X aily News X  alert system. The FACT is our closest approximation to the caller X  X  intent. This is of particular interest to VoiceTone X  X  clients (banks, pharmacies, etc.), who want to know what their customers are calling about and how that is changing.
Due to proprietary nature of the data, all dates were translated by a fixed number of days i.e. acutual date date used in the analysis ,where is not revealed. The news page for this application is updated on a daily basis. The system handles approximately care calls per day. Features tracked by hbmix include average call du-ration cross-classified by FACT X STATE (STATE where the calls originate are derived using ANI), RD X STATE, FACT X Hourofday, RD X STATE. The system is flexible enough to accept any new combination of variables to track. We present an analysis that tracks proportions for FACT X STATE.

There are about categories in FACT, states we X  X e interested in. At time , we only include cells that have occured at least once in the historic window of length which, for a window size of days (we choose this by a using predictive loss criteria on initial training data) results in about categories being monitored on average. The system went live last week of January, 2004. We use data ending April, 2004 as our training set to choose an appropri-ate window size and to choose parameters for a simulation experiment discussed later. Finally, we run hbmix on data from May, 2004 through Jan 2005.

Our cell measurements are proportions computed from the block that gets added to the database every day. For the cell, = number of calls in cell/Total number of calls. This multinomial structure induces nega-tive correlations among cells. Under a multinomial model, the negative correlation between any pair of cells is the ge-ometric mean of their odds ratio. This is high only if both odds ratio are large, i.e., if we have several big categories. From the training data we compute the percentile of the distribution of  X  X  for each cell. The top few cells have values which means the correlation is approximately bounded below by . To ensure sym-metry and approximate normality, we compute the score ization meant to preserve the multinomial structure. The top few cells after transformation have percentile val-ues of which gives a lower correla-tion bound of about . Hence, the assumption of cell independence seems reasonable in this case.
Here, our goal is to compare the performance of hbmix with a naive PCER approach for the BCC application. We take a simulation based approach, i.e., we generate data whose statistical properties are close to that of our actual data during the training period, artificially inject anomalies and then score the two methods under consideration.
We compare the methods based on performance at a sin-gle time interval. We simulate streams ( is the num-ber of cells in our stream, we ignore the issue of adjusting for margins since it is not relevant for this experiment) at point and compare the FDR and false negative rates based on several repititions of the experiment. Since the differ-ence between FDR and false negative rate is not symmetric, we tweak the value of so that the false negative rate for PCER matches the one obtained for hbmix with . The tweaking is done using a bisection algorithm due to the monotonic dependence of false negative rate on .Simu-lation details are given below.

Table 1. Results comparing hbmix and PCER with experiment 500 18.4 3.1 4.4 7.4 6.8 1000 19.7 3.5 5.7 9.2 7.4 2000 20.9 3.8 7.7 12.4 8.3 5000 21.9 4.0 13.8 21.4 9.9 The FDR for hbmix is consistently smaller than PCER. Moreover, the difference increases with . Also, the differ-ence is statistically significant indicated by the significant t-statistics(p-values were all close to ) obtained using atwo-samplet-test. For hbmix , we obtained similar re-sults for both the Empirical Bayes and full Bayes methods. Computational time for the full Bayes method is roughly times slower and hence we recommend Empirical Bayes if the main goal is inference on s X .
In this section, we present results of our analyses on cus-tomer care from May,2004 to January, 2005 for the combi-nation FACT X State. We apply hbmix both adjusting and not adjusting for the marginal changes (call them adjusted hbmix and non-adjusted hbmix respectively). In figure 1, the top panel shows time series plots of for both versions of hbmix (horizontal gray line shows the constant threshold of for PCER). As noted earlier, provides an estimate of the penalty built into hbmix at each time interval. The bottom panel shows the number of alerts obtained using the three procedures. The figure provide insights into the working of adjusted and non-adjusted hbmix relative to the PCER method. Large values of correspond to periods when the system is relatively stable producing a few alerts. (e.g., mid June through mid July.) In general, the PCER pro-duces more alerts compared to hbmix .Onafewdays(the ones marked with dotted lines on the bottom panel of figure 1), adjusted hbmix drastically cuts down on the number of alerts relative to non-adjusted hbmix . These are days when a system failure caused a big increase in HANGUP rate trig-gering several related anomalies. The adjusted version al-ways gives smaller number of alerts compared to PCER and it never produces more than a couple of extra alerts com-pared to the unadjusted version. In fact, there are about days where the adjusted version produces one or two alerts when the unadjusted version produce none. These represent subtle changes in interactions. To illustrate the differences between adjusted and unadjusted hbmix , we investigate the alerts obtained on Sept (we had other choices as well but believe this is sufficient to explain our ideas).
Sept 3rd, 2004 : This is an interesting day. Our univariate alert procedures don X  X  point to anything for FACT, we no-tice a couple of spikes in the STATE variable for Maryland (3.2% to 7.4%) and Washington D.C.(.6%to 2.1%). There are 8 alerts common to both versions of hbmix . Interest-ingly, these alerts are spatially clustered, concentrated to states that are geographically close to each other. There is one alert (an increase) that appear only with the unadjusted hbmix , viz., about Indicate(Service Line) in Maryland. One alert indicating increase in Ask(Cancel) in Connecticut is unique to the adjusted version. Figure 2 shows the differ-ence in the Indicate(Service Line) alert in Maryland using the adjusted and non-adjusted hbmix . The broken lines are the appropriate control limits about the historic mean. (For the marginals, the control limits are computed using PCER.) It provides an illustrative example of how the adjusted ver-sion works, the spike in Maryland when adjusted for reduce severity and the alert is dropped. Figure 3 shows an example where adjusted hbmix produce the alert missed by the unad-justed one on Sept . Although marginal changes are well within their respective control limits, drops in Ask(Cancel) number of alerts alerts. and connecticut increase severity of the alert with the ad-justed version.
We proposed a framework for detecting anomalies in massive cross-classified data streams. We described a method to reduce redundancy by adjusting for marginal changes. We solve the multiple testing problem using a hierarchical Bayesian model within a decision theoretic framework and prove the superiority of hbmix to a naive PCER method through simulation. We illustrate hbmix on a new speech mining application.

Ongoing work includes relaxing the gaussian assumption for  X  X  to the one-parameter exponential family.
I thank Divesh Srivastava and Chris Volinsky for useful discussions.

Figure 2. Example on sept where the adjusted version drops an alert caused due to a spike in one of the marginal means. Absence of control lines in one of the plot indicate all points are within the control limits.
Figure 3. Example on sept where adjusted ver-sion detects an alert missed by the unadjusted ver-sion. Absence of control lines in one of the plot indi-cate all points are within the control limits.

