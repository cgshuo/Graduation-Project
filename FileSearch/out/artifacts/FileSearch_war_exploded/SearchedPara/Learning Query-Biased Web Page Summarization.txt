 Query-biased Web page summarization is the summarization of a search engines. In this paper, we propose a learning-based query-biased Web page summarization method. The summarization problem is solved within the typical sentence selection framework. Different from existing Web page summarization methods that use page content or link context alone, both of them are considered as based summarization methods treat summarization as a sentence between extracted sentences and non-extracted sentences of all training documents. The basic assumption of these methods is that sentences from different documents are comparable with sentences higher than non-extracted sentences of each training document. The underlying assumption that sentences within a document are comparable is weaker and more reasonable than the assumption of classification-based scheme. Extensive results using intrinsic evaluation metrics gauge many aspects of the proposed method. H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms, Experimentation, Performance Query-biased Web page summarization, Ranking, Classification, Support vector machines Automatic text summarization is a long-standing research topic typically branches out in several dimensions as follows: This work was performed at Microsoft Research Asia. In this paper, we focus on query-biased Web page su mmarization. The extraction-based scheme is adopted to meet the real time paragraphs. Since the main purpose of query-biased summarizatio n is to assist relevance ranking should also be considered and emp loyed in snippet generation. summarization extract text-spans either from Web pa ge contents the Web page content is not too few, i.e. contains more than three sentences, combining content with context will be b etter than could be extracted in our algorithm. Extraction-based summarization has been cast in the framework of supervised learning for the first time in the semin al work of [22]. It formulates summarization as a statistical classi fication problem. Given a training set of documents with manually sel ected extracts, from the non-extracted sentences of all training do cuments. New Based on several discrete features, a na X ve Bayesia n classifier was This assumption may hold for scientific articles th at contain similar contents. However, for Web documents whose contents this issue, we treat summarization as a ranking pro blem. The goal extracted sentences for each training document. Fo r a new document, all of its sentences are ranked and top r anked ones are selected as the summary. The underlying assumption of this comparable. This weaker assumption is more reasona ble for Web results. query-biased Web page summarization algorithm is di scussed in as well as some discussions. Finally, we conclude in Section 4. Both content and context of Web pages could be used as the source for sentence selection, although most of exi sting Web page summarization algorithms only use one of them. We assume that the Web pages discussed here are HTML documents. F or a Web corresponding HTML document. The extracted sentenc es are information such as location and format information are also  X  X ontext sentences. X  If more than 80% words of two context sentences are identical, the shorter sentence will be removed. the relevance of the summary with the query, while the later indicates the correspondence of the summary with th e original document. A good summary should keep both high rele vance and high fidelity. Based on the belief that the larger the number of q uery terms in a sentence, the more likely that sentence conveys a s ignificant amount of the information need expressed in the que ry, [29] used total number of query terms. properties are inapplicable for context sentences, the  X  X ist X  of the content and context sentences. words are important, [23] proposed a keyword-based clustering method to measure the importance of sentences. Mor e word and its term frequency is larger than a thresh old T . Similar for n &gt; 40. I is 0 for 25&lt;= n &lt;=40 and 1 otherwise. words are separated by not more than 4 insignifican t words within the measure of that sentence. The significance sco re for a cluster (or the sentence that cluster represented) is: cluster boundaries are significant words), and TW is total number of words in the cluster. As the titles of news articles that tend to reveal the major subject pages. Therefore, sentences containing more title words could calculate the title score of a sentence s : words appearing in the sentence s . As analyzed in [18], 33.5% of HTML documents in the TREC HTML documents using the algorithm of [18]. We def ine an where n is the number of extracted title words and i is the number of extracted title words appearing in the sentence s . the same, we deem them as one anchor text with a we ight number of occurrences of AT i being O i . Then, the weight W AT i is defined as: Based on the weights, we define the anchor text-bas ed score of a sentence s as follows: where n i is the number of words in AT i and m words of AT i appearing in s . importance score of sentences, any combination of t hem could be according to the sum of scores. As aforementioned in Section 1, extraction-based su mmarization of documents with manually selected extracts, a cla ssifier could be learned to separate the extracted sentences from th e non-extracted are selected as the summary. foundations and proven empirical successes. the positive and negative training data with maxima l margin [19]. into the following optimization problem: where is a summary sentence or -1 otherwise. resolve this issue of unbalanced training data. Mo re specifically, formula (7) becomes: To summarize a new document, a significance score f ( s ) for each sentence s is calculated: highest scores are selected as the summary. Since a document is summarized by ranking its sente nces and selecting the top ones, it is more natural to treat summarization as discriminates extracted sentences from non-extracte d sentences of extracted sentences higher than non-extracted ones within each training document. community. Several ranking algorithms have been pr oposed in RankNet [5], RankBoost [14] and Ranking SVM [20]. Some has Considering that SVM classifier is used in the clas sification-based summarization, we choose Ranking SVM as the ranking algorithm: where N is the number of training documents, r k is a sentence set containing all extracted summary sentences of docum ent k , and x that : x w s f  X  = ) ( . Both the classification and ranking problems are so lved using SVM Light [19]. Extensive experiments were performed to gauge many aspects of sources and learning algorithms are evaluated respe ctively. We used the queries and relevance judgments of Web Tracks in TREC-2003. The queries are classified into three c ategories: named-page finding (NP), homepage finding (HP) and topic distillation (TD) [10]. Considering that NP and HP queries have few relevant Web pages, we only use TD queries. 10 queries were randomly chosen from the total 50 TD queries. The selected searches, Shipwrecks, Literacy, deafness in childre n, wireless communications, Counterfeit money, mining gold silv er coal. The first five queries and their corresponding document s were used as training data, while other queries and documents we re testing data. In order to evaluate the effectiveness of summariza tion algorithms and randomly selected irrelevant pages excluding th ose top ranked three kinds of pages Relevant, BM25 Irrelevant and Random Irrelevant respectively. We randomly selected ten Relevant Pages, five Random Irrelevant Pages, and five BM25 Irrelev ant Pages for each query. If there are less then ten Relevant Pa ges, all of them selected page, content sentences, context sentences and anchor text phrases were extracted. Two human evaluators were asked to summarize all th e 175 pages evaluators for selection. For each TD query in TRE C-2003, there  X  X ireless communications X  is  X  X nformation on existi ng and them decide the relevance of sentences. The number of sentences for a summary is recommended to be three. However, if there are less or more appropriate sentences, they could be s elected in spite summaries. On the other hand, 44.09% sentences of evaluator 1 X  X  summarization results of two evaluators are relativ ely consistent. Summarization evaluation methods could be classifie d into two the summarization system in itself, while extrinsic evaluation tests the summarization based on how it affects the compl etion of some other task. Considering that we have the human labeled summarie s, intrinsic measures were adopted. Precision, recall and F straightforward measures widely used in intrinsic s ummarization evaluation [26]. For each document, the manually e xtracted three sentences ranked by certain algorithm are con sidered as candidate summary. The precision, recall and F 1 values are computed by comparing the candidate summary with th e reference summary as follows: summary and the reference summary. Since there are two evaluators whose extracted summ aries are not totally consistent, we average the P , R and F 1 using each evaluator X  X  summaries as the reference s ummaries. The averaged F 1 score is used as the evaluation measure in all the following evaluations. Since the title of a Web page can be extracted easi ly from the title field of the corresponding HTML document, most of e xisting all the 175 Web pages, the title sentences of on av erage 25 pages are selected by the evaluators. This shows the imp ortance of title and reference summaries. 
Figure 1. An item of retrieval results list of Goog le for query Each of the five features introduced in Section 2.2 could be used Since we have three kinds of pages: relevant pages, BM25 irrelevant pages and random irrelevant pages, we fi rst evaluate all the three page sets were mixed into one set. The p erformances of five features and their combination are shown in Fi gure 3. When anchor and E-title are clearly better than TO. The combination of five features is clearly better than using any sing le feature. 0.05 0.1 0.15 0.2 0.25 0.3 Figure 2. Performance comparison for different kind s of pages 0.05 0.15 0.25 0.35 Figure 3. Performance comparison of single features and their 0.24 0.25 0.26 0.27 0.28 0.29 combination (Proposed), two combinations of [30] an d [29] were uses query, title, location, and term occurrence in formation. Since features: anchor and E-title. Since the number of non-summary sentences is about five times of C classifier and Ranking SVM, we used the Gaussian ke rnel: parts and three-fold cross-validation was performed using two The results of Ranking SVM and SVM classifier are s hown in chosen. The  X  values for Ranking SVM and SVM classifier are listed in Table 1.
 SVM classifier 3.4638 0.3041 2.3092 Based on the appropriate parameters, both Ranking S VM and testing documents were summarized and evaluated. T he results Ranking SVM outperformed  X  X o Learning X , which shows the effectiveness of learning algorithms. Moreover, Ra nking SVM is better than SVM classifier as expected. 0.05 0.15 0.25 0.35 Figure 5. Performance of different  X  on validation sets using Figure 6. Performance of different  X  on validation sets using 0.27 0.28 0.29 0.3 0.31 0.32 0.33
Figure 7. Performance comparison of learning algori thms summarization results using on only content sentenc es (Content), only context sentences (Context), or both content a nd context sentences (Combined) are evaluated and shown in Fig ure 8. that, out of the 175 pages, there are 47 pages (27% ) that have no context sentences and 90 (51%) pages that have less than 3 very important or even indispensable. To show the effect of different sentence sources on learning algorithms, two classifiers and two ranking algorit hms are trained based on either only content sentences or only cont ext sentences. using only content sentences, only context sentence s, or both content and context sentences are shown in Figure 9 and 10. For both SVM classifiers and Ranking SVMs, using both s entence example, the F 1 value of Ranking SVM using both sentence sources is higher than that using content (context) sentences alone by relatively 6.8% (23.2%). 0.23 0.24 0.25 0.26 0.27 0.28 0.29
Figure 8. Performance comparison using different se ntence 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.31
Figure 9. Performance comparison using different se ntence 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.31 0.32 0.33
Figure 10. Performance comparison using different s entence In this paper, we propose a query biased Web page s ummarization both Web page content and link context are used. U nlike most of summarization as a ranking problem. Our assumption is that Experimental results on 175 documents of 10 queries from TREC summarization over a typical classification-based s ummarization for all extraction-based summarization problems. W e will pursue this direction in the future. [1] Amitay, E. and Paris, C. 2000. Automatically summar ising [2] Amini, M. and Gallinari, P. 2002. The use of unlabe led data [3] Berger, A. L. and Mittal, V. O. 2000. OCELOT: a sys tem for [4] Berger, A. and Mittal, V. O. 2000. Query-relevant [5] Burges, C., Shaked, T., Renshaw, E., Lazier, A., De eds, M., [6] Buyukkokten, O., Garcia-Molina, H., and Paepcke, A. 2001. [7] Carbonell, J. and Goldstein, J. 1998. The use of MM R, [8] Chuang, W. T. and Yang, J. 2000. Extracting sentenc e [9] Crammer, K. and Singer, Y. Pranking with ranking. I n [10] Craswell, N., Hawking, D., Wilkinson, R., and Wu, M . [11] Delort, J., Bouchon-Meunier, B., and Rifqi, M. 2003 . [12] Edmundson, H. P. 1969. New Methods in Automatic [13] Eiron, N. and McCurley, K. S. 2003. Analysis of anc hor text [14] Freund, Y., Iyer, R., Schapire, R. E., and Singer, Y. 2003. [15] Goldstein, J., Kantrowitz, M., Mittal, V., and Carb onell, J. [16] Herbrich, R., Graepel, T., and Obermayer, K. 2000. Large [17] Hirao, T., Isozaki, H., Maeda, E., and Matsumoto, Y . 2002. [18] Hu, Y., Xin, G., Song, R., Hu, G., Shi, S., Cao, Y. , and Li, [19] Joachims, T. Making large-Scale SVM Learning Practi cal, in [20] Joachims, T. 2002. Optimizing search engines using [21] Jones, K. S., Galliers, J. R., and Galliers, J. R. 1996 [22] Kupiec, J., Pedersen, J., and Chen, F. 1995. A trai nable [23] Luhn, P. H. Automatic creation of literature abstra cts. IBM [24] Mani, I. and Bloedorn, E. 1998. Machine learning of generic [25] Mitra, M., Singhal, A., and Buckley, C. Automatic T ext [26] Radev, D. R., Teufel, S., Saggion, H., Lam, W., Bli tzer, J., [27] Robertson, S. E. and Walker, S. 1994. Some simple e ffective [28] Sun, J., Shen, D., Zeng, H., Yang, Q., Lu, Y., and Chen, Z. [29] Tombros, A. and Sanderson, M. 1998. Advantages of q uery [30] White, R.W., Jose, J.M., and Ruthven, I. A task-ori ented 
