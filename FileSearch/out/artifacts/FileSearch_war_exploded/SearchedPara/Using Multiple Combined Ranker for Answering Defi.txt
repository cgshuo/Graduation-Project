 merely answers, rather than documents containing answers. Related work concerning to definitional question answering are mostly concentrated on Patterns Extraction, Ccentroid-based ranking, as well as utilizing Web knowledge as external source. Patterns Extraction has been extensively adopted in information retrieval tasks. These [1] employed TF*IDF to get a set of relevant sentences and built patterns from them. Other approaches employed to extract defi nitional sentences include various pattern matching methods, in which hand-crafted or machine learned rules are generated to find nuggets[2][3][4]. Moreover, some definitional question answering systems adopt a centroid-based ranking method to identify and select definition sentences [2][5]. For centroid vector, which was utilized to rank input sentences using cosine similarity. 
Our multiple combined ranker (MCR) approach for answering definitional novel and effective way. Instead of using Centroid-based or Pattern-based method, we adopt different rankers, which respectively measures candidate sentences X  importance based on AQUAINT corpus, question target expansion, as well as Web knowledge collections. These three rankers act as mutual supplements. 2.1 Basic Ranker consists of two components: the searching procedure and the refining procedure. A search engine (Lucene) retrieves documents in respond to the target query, and ranks them using some algorithm. We make the assumption that the search engine already produced a good result. Consequently, the sentences in these documents are supposed to have a tight relationship with the questi on target. The refining procedure considers other possible factors that might make a candidate sentence become an appropriate answer. The assumption is that the sentences containing words, phrases and Name ones for answering the question. Also, a single sentence could appear in several above two procedures, the Basic Ranker scores the candidate sentences so that the relevant sentences receive higher scores. 2.2 Web Ranker Until recently, Web knowledge bases (Web KBs) are increasingly recognized as a promising way to provide online knowledge, thus we adopt Web KBs as an alternative way for knowledge acquisition and build another ranker called Web Ranker. During this procedure, we calculate the similarity scores between candidate sentence and definitions from different knowledge bases respectively, and merge are ranked using definitions from Web KBs. Firstly we construct a words vector space, which is based on TF*IDF, for all candidate sentences and Web definitions. Each of them is projected into this vector space. Secondly, the similarity of a particular candidate sentence and Web definition are computed based on the cosine of the two vectors. 
Our definitional question answering systems got promising results by employing several external Web knowledge bases during TREC 2003 and TREC 2004. However, characteristic, by way of adopting more complex question types. As a result, we could only find out online definitions for about 65% of the total question targets in TREC2005. Although we still employ Web ranker as one of our strategies to rank the candidate sentences, it is not reliable as before, more details of this approach could be referred to in [6]. 2.3 Related Terms Ranker We construct the Related Terms (RT) Ranker based on the extension of the question targets, for the purpose of obtaining more reliable and target-related information nuggets. At the heart of RT Ranker is the process of identifying and selecting words, These terms were acquired at the end of preliminary processes like word segmentation and stemming. Also, a Relation Degree is defined to weigh the relationship between extracted terms and the question target In previous work, expansion of terms were adopted in automatic query expansion, as well as open-domain question answering [7][8]. Our approach differ from the above in that 1) Making full use of NE extraction technology which is quite helpful in identifying Related Terms. 2) Taking into account of not only the Relation Degree of the terms, but also their weights, which are related with the Basic Ranker score of the sentence named Related Terms) is denoted by T ={ t 1 , t 2 , ..., t n }. The process of Relation Degree computing is defined as below: normalized and ranked, top terms were selected as RT for further processing. Consequently, we rank the candidate sentences based on Relation Degree of RT. Let n w , n p , n e respectively represent the number of words, phrases and Name Relation Degree of them, RT_score(S X ) is introduced to denote the score of this sentence according to the Related Terms Ranker, which is defined as follows: According to experiments and heuristic assumptions, Name Entity should play a relative important role in RT, thus  X  received a slightly higher weight than other two parameters. In our system, they are allotted to 0.3, 0.3 and 0.4 respectively to receive the optimal result. To evaluate the effectiveness of multiple combined ranker, we utilize the data set from TREC 2006 QA track, which contained 75 series as well as answer judgments. Our system official F(  X  =3) scores is 0.223, ranked second in all participated systems. To further compare the effectiveness of our MCR approach, we experimented on the TREC 2005 definition question set using our evaluation system, which can keep the rank when evaluates the top 10 submitted result. document retrieval, which is the foundation of Basic Ranker and Related Terms Ranker. In the second experiment we evaluate effectiveness of sentence selection. The purpose of the third experiment uses the Basic Ranker as a baseline, and Multiple Combined Ranker is compared with the baseline to show its effectiveness. 3.1 Effectiveness of Document Retrieval In this part, we utilize Lucene 2.0 as our search engine and judge the returned documents by Vital and Okay nuggets recall, respectively. We vary the number of returned documents from 1 to 200 to study the effect of document number on nuggets recall. The result is listed in Table 1. 
As shown from Table 1, Vital nugget recall in all TOP200 documents can achieve up to 90.0% recall and Okay nugget reca ll reach 81.9%, which are especially high information nuggets. So we also test nuggets recall on top N (1-100) returned documents, experiment results show that the Vital nuggets recall is higher than Okay nuggets recall in TOPN documents. Because the Vital nugget is more important than Okay one, our solution of document retrieval is successful. We can also see from Table 1, R(V)/N and R(O)/N decrease with the increasing of N, which is in accordance with the Basic Ranker hypothesis. 3.2 Candidate Sentences Selection Evaluation The returned documents always contain some sentences that were not related to the to evaluate the process of candidate sentence selection, we use the same method (MCR) for definitional question answering but with different candidate sentences sets. The first set is all candidate sentences without selection. Although all candidate sentences contain 90.0% Vital nuggets and 81.9% Okay nuggets, the system X  X  F-score is only 0.187. In contrast, the other candidate sentence set is selected by Basic Ranker and some manual constructed rules. More candidate sentences were discarded in the selection process, as shown in Table 2. The Vital nugget recall and Okay nuggets recall decreased 30.3% and 45.1% respectiv ely. However, although both Vital recall and Okay recall decreased obviously, the system performance improved 72.0%. In the same time, we try some different candidate sentence sets in our system. These Table 2. The effect of candidate sentences selection in definitional question answering experiments show that the confidence of qu estion answers is determined according to the Vital/Okay nuggets information with the noise information for definitional question answering. 3.3 Effectiveness of Multiple Combined Ranker For each candidate sentence, three scores are calculated by Basic Ranker, WEB Ranker and Related Terms (RT) Ranker respectively. These scores are then applied to extract the question answers, both respectively and synthetically. In ranking process, weights of the three scores are estimated by our automatic evaluation system. performance of these ranking procedures, briefly named as BASIC, WEB and RT, have been evaluated and are shown in Table 3. As can been seen from this table, the BASIC method in choosing candidate sentences is not only an important element for answering question, but it is also the foundation of WEB Ranker and RT Ranker. The third Ranker (RT) returns the worst F-measure against other two single method though, it shows competitive performance while working together with the BASIC Ranker and WEB Ranker. We can see from Table 3 that adding Related Terms Ranker to BASIC Ranker and WEB Ranker could improve the system performance up to 12% and 6% respectively, and compared with BASIC+WEB, employing Multiple Combined Ranker (MCR) could enhance system performance by 2%. Generally, the combined solution is much better than separated ones. This could be deduced from the fact that the best solution method BASIC + WEB + RT (MCR), whose F-Measure achieved 0.318, outperformed the best single solution to a great extent (about 17% improvement). Compared with other question answering tasks, definitional question answering has more uncertain factors. There are still many divergences even among experts while reliable knowledge related to the target. So we propose a Multiple Combined Ranker (MCR) approach to rank candidate sentences for definitional question answering. To acquire the reliable and related information, external knowledge from online websites and the related words, phrases and entities were extracted. Using these multiple knowledge, the definitional QA system can rank the candidate answers effectively. This research was supported by the National Natural Science Foundation of China under Grant No. 60435020 and No. 60503070 
