 In the field of Music Information Retrieval (MIR), multi-label genre classification is the problem of assigning one or more genre labels to a music piece. In this work, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification. Our goal is to enhance clas-sification performance by combining multiple classifiers. In addition, we also investigate some existing ensemble tech-niques from machine learning. The effectiveness of these techniques is demonstrated through a set of empirical ex-periments and various related issues are discussed. To the best of our knowledge, there has been limited work on apply-ing ensemble techniques to multi-label genre classification in the literature and we consider the results in this work as our initial efforts toward this end. The significance of our work has two folds: (1) proposing a set of ensemble techniques specific to music genre classification and (2) shedding light on further research along this direction.
 H.5.5 [ Sound and Music Computing ]: Methodologies and techniques; H.3 [ Information Storage and Retrieval ]: Information search and retrieval Experimentation, Performance Music Information Retrieval (MIR), genre classification, multi-label classification, ensemble techniques
For the past decade, digital music collections have been growing enormously in volume, due to advances in technolo-gies, such as storage capacity, network transmission, data compression, information retrieval, etc. The rapid rise in music downloading has created a major shift in the music industry away from physical media formats to electronic dis-tributions. Large on-line music providers now offer catalogs that contain millions of songs. At present, these catalogs are commonly accessed through textual meta-data , such as genre , style , mood , artist , etc. While this meta-data is rich and descriptive, it is difficult to maintain and in many cases is not comprehensive, due to the ambiguity and subjectivity that is introduced in the annotation process [13]. Therefore, manual annotation is insufficient and ineffective when facing large volumes of music.

Music Information Retrieval ( MIR ) is an emerging area of interdisciplinary research that is engaged in the design and implementation of algorithmic approaches to manage digital music collections [6]. Various tasks, such as music classification, recommendation, fingerprinting, play-list gen-eration, etc., are studied in MIR. In our work, we study the problem of automatic music genre classification. As a fundamental task of MIR, automatic genre classification has attracted considerable attention for many reasons. For in-stance, music genres have historically been used as cate-gorical descriptions to organize music collections. While no universally accepted definition of genres exists, a genre can be described by the common characteristics shared by its members. These characteristics are related to the instru-mentation, harmonic content, rhythmic structures, etc. [23] However, as McKay [11] indicates, there is some controversy regarding the current state of automatic genre classification with some works proposing that it be abandoned in favor of a more general similarity search problem. It has also been sug-gested that only limited agreement can be achieved among human annotators when classifying music by genres. This imposes an unavoidable ceiling on the performance of auto-matic genre classification. Moreover, the time and expertise needed to manually classify a corpus of music poses serious obstacles to generating quality ground-truth, against which classification performance can be evaluated. To further com-plicate matters, the understanding of existing genres can change with time, which can necessitate the re-annotation of ground truth [11].

Despite these controversies, genre categorization provides a common vocabulary which can be used to discuss musi-cal categories. Additionally, users are already accustomed to browsing music collection by genres, and this approach is proved to be at least reasonably effective [11]. Although one may argue that generating ground truth is a serious issue that needs to be handled, manually labeling growing music collections is a much greater challenge. Therefore, contin-uing efforts in automatic genre classification have much to o ffer. While previous approaches are concerned with learn-ing from a set of music pieces that are each associated with a single label, it has been observed that a music piece could belong to multiple genres. Furthermore, different portions of a music piece might be classified as different genres which stand in contrast to each other [17]. Therefore, it is de-sirable that automatic genre classification be modeled as a multi-label process.

In our recent studies, we have identified a set of multi-label classification algorithms that perform well on a selection of music datasets. We believe that, due to the diversity of the datasets used and the comprehensibility of our experiments, our results are representative and can be further utilized for algorithmic improvements on genre classification. Toward this end, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification. In ad-dition, we also investigate some existing ensemble techniques from machine learning. To the best of our knowledge, our work is the first to apply ensemble techniques to multi-label music genre classification. It is important to note that, it is commonly accepted that music genres may depend on cul-tural extrinsic habits. However, in our work, we assume that there are intrinsic properties of the music that can be used for automatic genre classification.

The paper is organized as follows. Section 2 introduces related works in multi-label classification of music while Sec-tion 3 discusses the multi-label classification algorithms and their evaluation metrics in our experiments. Section 4 presents our proposed ensemble techniques along with existing ones. Experiment setup is discussed in Section 5 and results are presented and analyzed in Section 6. Finally, Section 7 con-cludes our presentation with some future work.
Multi-label classification has been used to categorize mu-sic into different emotions. Wieczorkowska et al. [25] address the problem of multi-label classification of emotions using a modified k -NN algorithm. A set of 875 audio samples, 30 seconds each, manually labeled into 13 classes of emotion are used for their experiment. The modified k -NN algo-rithm aims at taking multiple labels into account. There-fore, the algorithm returns a set of labels for each neighbor of a test instance. In addition, Trohidis et al. [19] evaluate and compare four multi-label classification algorithms for the purpose of detecting emotions in music. Experiments are conducted on a set of 593 samples, from which a 30-second excerpt is extracted and annotated with a set of emotions. It is claimed that the overall predictive performance is high and encourages further investigation of multi-label classifi-cation.

Multi-label classification has also been used for the pur-pose of automatic tag annotation of music. These tags can be any semantically meaningful words and can represent a variety of different concepts, including genre, instrumen-tation, emotions, geographic origins, social conditions, etc. Automatic tag annotation of music learns a relationship be-tween acoustic features and words from a dataset of labeled audio tracks [12]. The resulting trained model can retrieve audio samples based on lists of tags and annotate unlabeled ones. For example, Wang et al. [24] study the problem of combining user-generated tags and music content for artis-tic style classification. They investigate the effectiveness of using tags and audio content separately for clustering and propose a novel language model that makes use of them. Re-sults show that tag features are more effective than music content for artistic style clustering, and the proposed model can marginally improve clustering performance by combin-ing tags and music content.

Despite the volume of previous results on the applica-tion of multi-label techniques, to the best of our knowledge, there has been limited work on the application of them to music genre classification. Lukashevich et al. [10] present a two-dimensional approach for genre classification. The multi-label classification problem is decomposed into multi-ple single-class problems. The novelty of the approach lies in the combination of segment-wise and domain-specific genre classification. The music collection used for testing is com-prised of 430 full-length music pieces from 16 world genres. However, as described by Zhang and Zhou [26], decompos-ing multi-label problems into multiple binary classifications does not consider the correlation between the labels of each instance. Therefore, the expressive power of such an ap-proach can be limited.
Multi-label classification deals with problems where an object may belong to one or multiple classes simultane-ously. These algorithms can be grouped into two categories as proposed in [20]: (1) problem transformation methods , and (2) algorithm adaptation methods . Problem transfor-mation methods transform a multi-label classification prob-lem into one or more single-label classification problems and offer the flexibility of using any single-label base classifier. Moreover, algorithm adaptation methods extend traditional single-label classifiers to handle multiple labels directly. Table 1: Multi-label classifiers along with the asso-c iated base classifier, where applicable.

Based on our recent studies with multi-label genre clas-sification, in this work we select and combine the following algorithms using a set of ensemble techniques: Random k -Labelsets ( RA k EL ), Calibrated Label Ranking ( CLR ), Multi-label k-Nearest Neighbor ( ML-k NN ), Hierarchy of Multi-label Classifiers ( HOMER ), and Instance Based Logistic Regres-sion ( IBLR ). A Decision Tree ( DT ) is used as a base clas-sifier for RA k EL while a Support Vector Machine ( SVM ) is used for CLR and HOMER. Table 1 presents the multi-label classification algorithms derived for our study. The details of them can be found in [21].
In order to evaluate the performance of multi-label classi-fication algorithms, a selection of evaluation metrics are em-ployed. These metrics can be categorized into three groups: example-based, label-based and rank-based. The first two groups are based on a bipartition vectors while the third g roup derives ranking information from a score vector and conducts evaluations accordingly. Furthermore, different classification algorithms perform better under different eval-uation metrics. Therefore, it is desirable that multiple and contrasting metrics are used in any multi-label classification experiment. We consider the following evaluation metrics for each group.
 Example-based: Hamming Loss ( HL ), Accuracy ( Accu. ), Recall, F-Measure ( F 1 ) and Precision ( Prec. ); Label-based: Micro Precision ( MicroP ), Micro F-Measure ( Micro F 1 ), Micro Recall ( MicroR ), Macro Precision ( MacroP ), Macro F-Measure ( Macro F 1 ) and Macro Recall ( MacroR ); Rank-based: Average Precision ( AP ), Coverage ( CO ), Rank-ing Loss ( RL ), and One-Error ( OE ).

For the sake of space and due to the nature of our work, we will not digress into the details of these evaluation metrics. The interested reader is referred to [20, 21].
In this section, we discuss and propose a set of ensemble techniques for improving multi-label music genre classifica-tion.

Despite extensive work in multi-label classification, there exist two major challenges. The first challenge is that in many situations, we can have highly imbalanced datasets, due to the availability of instances for some labels. The sec-ond is related to our limited knowledge regarding the corre-lation among class labels for a given dataset. Surprisingly, most multi-label classification algorithms are designed to fo-cus mainly on the second problem and limited work has been devoted to handling imbalanced datasets [18].

One approach to these problems is an ensemble of multi-label classifiers, which consists of a set of individually trained classifiers whose predictions are combined when classifying new instances. This approach is generally more accurate and achieves greater predictive performance than any of the individual classifiers [4].

Ensembles can be homogeneous, where each individual classifier is constructed using the same algorithm, or hetero-geneous, where each classifier is constructed from a different algorithm [4, 18]. Some multi-label classification algorithms directly use homogeneous or heterogeneous ensemble tech-niques internally to improve overall performance. For exam-ple, Instance Based Logistic Regression [5] uses a combina-tion of Logistic Regression and Nearest Neighbor classifiers.
Tahir et al. [18] present a first study, as claimed, on com-bining the outputs of multi-label classification algorithms. They propose two heterogeneous ensemble techniques and apply them to publicly available datasets using several eval-uation metrics. The results show that these approaches pro-vide significant performance improvements when compared with individual multi-label classifiers.

The goal of our work is to use a heterogeneous ensemble of existing multi-label classification algorithms to improve mu-sic genre classification. In the following, we introduce a set of ensemble techniques that combine multiple classifiers for the purpose of music genre classification. We first introduce some notation. Let D denote a set of music pieces and let L = { l 1 , l 2 , ...l N } be the finite set of labels. Given a training set D s = { ( x 1 , Y 1 ), ( x 2 , Y 2 ),  X   X   X  , ( x m , Y is a single music piece and Y i  X  L is the label set associated with x i . We attempt to design a multi-label classifier that predicts a set of labels for an unseen music piece from a test set D t = { ( x 1 , Y 1 ), ( x 2 , Y 2 ),  X   X   X  , ( x n , Y
An ensemble of multi-label classifiers ( EML ) trains q multi-label classifiers H 1 , H 2 , ...H q . In our work, all classifiers are likely to be unique and be able to generate different multi-label predictions. For an unseen instance x j , classi-fier H k produces two N -dimensional vectors: a score vector P k = [ p fidence of the class label l b assigned by classifier H k correct, and a bipartition vector V j k = [ v j 1 ,k , v j where v j b,k is 1 if the class label l b is predicted by classifier H k and 0 otherwise.

We denote by H eml the classifier obtained after applying an ensemble technique to the q classifiers. We use P j eml [ p vector for the unseen instance y j and use V j eml = [ v j v vector.

There are many techniques to combine the outputs of these q classifiers. In this work, we consider two sets of com-biners based on the output of the classifiers. The first set is based on the bipartition vector while the second set is based on the score vector. These techniques are discussed below. It is important to note that, multi-label classification algo-rithms may only output a score vector. These classifiers can employ a thresholding method to produce bipartitions [7]. In our experiments, we assume that all of the classifiers out-put a score vector and bipartition vector.
The Intersection rule ( EM L I ) calculates the intersection of the bipartition vectors from the q classifiers using v min s ( v j i,s ), for i = 1 , 2,  X   X   X  , N , i.e., the binary values for each label in all vectors are combined using the logical AND operator. The set of output labels common to all classi-fiers result in an ensemble decision. We propose this naive method to emphasize the common labels output by all indi-vidual classifiers.
The Union rule ( EM L U ) calculates the union of the bi-nary vectors from the q classifiers using v j i,eml = max for i = 1 , 2,  X   X   X  , N , i.e., the binary values for each label in all vectors are combined using the logical OR operator. An ensemble decision is constructed by computing, for each la-bel, the union of outputs from the q classifiers. We propose this method to optimistically select a label for the ensemble if it is selected by any of the individual classifiers.
The Majority Vote rule ( EM L MV ) counts how many times a label appears in the q classifiers. It is one of the most fre-quently used ways for combining label outputs [8]. where i = 1 , 2,  X   X   X  , N .
The Minimum rule [8, 18]( EM L Min ) represents a pes-simistic view of the scores from the q classifiers and only considers the smallest score for each class label. We calcu-late the minimum score as shown in the following equation,
The Maximum rule [8, 18] ( EM L Max ) represents an op-timistic view of the scores from the q classifiers and only considers the largest score for each class label. We calculate the maximum score as shown in the following equation,
The Mean rule [8, 18] ( EM L Mean ) considers the scores from the q classifiers for a class label. We calculate the mean score as shown in the following equation,
We propose a Top-k rule (denote as EM L T opk ), which is a combination of the Maximum and Mean rules. It selects the top k largest scores and averages them. We use the following equation to calculate p j i,eml . Constant k is a user-selected parameter. This method represents our desire to have a degree of certainty when calculating the score for H eml . where i = 1 , 2,  X   X   X  , N , function topk (  X  ) picks the largest k elements from a set and function avg (  X  ) averages them.
In addition, we propose a score-based label selection tech-nique wherein we first consider the output score from each classifier and select the top n scores and the corresponding labels. Then, we merge the results from all the classifiers, from which we select the top k labels that appear the most, where n  X  k . If there is a tie, we arbitrarily select one. For each of these k labels, we set the corresponding entry to be  X 1 X  X n the bipartition vector for the final ensemble H eml the other labels, we set the respective entries to be  X 0 X . In this proposed technique, since we first select the top n scores and then select the top k class labels accordingly, we refer to it as EM L C n L k in the following discussions. We note that, this technique produces a bipartition vector and will be compared to the bipartition-based ensemble techniques in our experiments.
We propose a label substitution technique that utilizes taxonomy information of music genres to improve multi-label genre classification. We call this technique Hierar-chical Label Substitution ( HLS ). The technique reduces the number of labels using a substitution method. A set of multi-label classifiers are trained and the resulting output is combined using one of the bipartition-based techniques described above. A set of labels in the ensemble decision are then substituted based on a local genre hierarchy, rep-resented as a taxonomy . For our experiments, we derive our local genre hierarchy based on the taxonomy of music genres the local genre hierarchy.
 Figure 1: Example genre taxonomy used for Hierar-c hical Label Substitution.

Our hierarchy is constructed to only contain music genres in our datasets and consists of top level nodes referred to as  X  X eta genres X  [14], as shown in Figure 1. These should represent main musical genres, such as Classical , Rock , Jazz , etc. The hierarchy is further defined to consist of children nodes, i.e., sub-genres , which can be conceptualized as spe-cific genres derived from a parent genre and become more concrete as the depth increases. For example, from Figure 1, Techno is considered as a sub-genre of Electronica . It is im-portant to emphasize that our taxonomy is by no means authoritative but our substitution technique is applicable to any other genre taxonomy.

We define a depth d for label substitution, where all labels below d are substituted with their parent label at this level. For example, for sub-genre Dance , at d = 2, any occurrence of labels in the resulting classifier output below level d will be replaced with their parent label, e.g., Disco would be replaced with Dance and Techno with Electronica . It is easy to see that, as d decreases, the resulting label set shrinks.
Although our label substitution technique produces a smaller label set, the usefulness lies in its ability to simplify the la-bel space by reducing the number of possible  X  X verlapping X  genres. Moreover it has been shown [1] that there is little consensus between the genre labels used by individuals. Sim-plification to a common set of high-level genres may provide a remedy to this.
In this section we describe the preparation for our experi-ments. We apply the ensemble techniques discussed above to the multi-label classifiers in Table 1 and evaluate their per-formance. The evaluation is conducted using 10-fold cross validation. The Mulan [22] open source library for multi-label learning is used to train and evaluate each of the classi-fiers using default parameters, e.g., the number of neighbors is set to 10 for ML-k NN and IBLR, the SVM is trained with a linear kernel, etc.
There are a variety of benchmark datasets available for multi-label classification in various domains. However, no dataset exists specifically for the task of multi-label music genre classification. Current datasets used in the evaluatio n of genre classification are comprised of music pieces anno-tated with a single genre. Therefore, for the purpose of our experiment, we derive three multi-genre datasets from the Magnatagatune collection [9].

Magnatagatune is a collection of approximately 21,000 music clips, each annotated with a combination of 188 differ-ent tags. The annotations are collected through an on-line game, referred to as  X  X agATune X , developed to collect tags for music and sound clips. Each clip, 29 seconds in length and sampled at 16kHz, is an excerpt of a music piece pub-lished by Magnatune. All of the tags in the dataset have been verified, i.e., a tag is associated with a clip only if it is generated independently by more than two players. More-over, only those tags that are associated with more than 50 clips are included in the collection.

For our experiment, we are only interested in those clips annotated with musical genres. A set of 22 genre tags, de-noted as L , are identified and used to create a subset of the Magnatagatune collection, which is referred to as D s in our following discussions. D s is created by filtering the Mag-natagatune collection to contain only clips annotated with the set of selected genre tags. The following three datasets are further derived from D s and discussed further in Sec-tion 6.4.
 The Random dataset, denoted D Ra , consists of 1000 clips chosen at random from D s . No other selection criteria are used in the creation of the dataset.
 The UniqueArtist dataset, denoted D Ar , consists of 198 clips derived from D s . The dataset is created by randomly select-ing a single clip from each artist in D s . The use of an artist filter is recommended as it ensures that the artists in the test set are not present in the training set during classifica-tion [2].
 The UniqueAlbum dataset, denoted as D Al , consists of 375 clips derived from D s . The dataset is created by randomly selecting a single clip from each album in D s . Similar to the artist filter, the use of an album filter ensures that clips from the same album are not present in the training and testing sets simultaneously.
 Table 2: Multi-genre music datasets and their statis-t ics.

Table 2 displays the datasets and their associated statis-tics. The label cardinality ( LC ) of a dataset D is the average number of labels each instance has in D and is used to indi-cate the number of alternative labels that characterize the instances in a multi-label training dataset [20].
Prior to classification, the music pieces must be parame-terized based on a set of features and their changes over time. However, there is no accepted criteria as which features are best for music classification [3]. Therefore, we use the follow-ing set of features which are commonly used for genre classi-fication: MFCC, ZCR, Spectral Centroid, Rolloff, Spectral work is used for the computation of the features.

Following a general practice in MIR, we consider a bag-of-frames approach based on the aforementioned features [15]. This approach consists of modeling the audio signal as the statistical distribution of audio features computed on indi-vidual, short segments. This process yields a large number of feature vectors. Therefore, the feature vectors are then ag-gregated together using statistical methods. Although more elaborate representations have been proposed in the litera-ture, the simplicity of using a single vector for classification is appealing [12]. Frame-level features in our experiment are compressed into a single set of song-level features by com-puting the mean across the feature vectors [12].

Furthermore, we explore the effects of frame size on multi-label classification. For each &lt; dataset , classifier &gt; pair, we plot the classification performance as we adjust the frame size, f r , represented as the number of samples collected during a certain time period. Each pair is evaluated us-ing f r  X  { 256 , 340 , 512 , 1024 , 2048 , 3200 , 4096 } . In the fol-lowing, we only report results for f r = 340, corresponding to approximately 23 milliseconds. This frame size has been commonly used in the MIR literature for classification tasks. For instance, Tzanetakis and Cook [23] use a frame size of 23 milliseconds for automatic genre classification. We dis-cuss the effects of frame size on classification performance in Section 6.4.
We use the following ensemble parameters in our exper-iments. For hierarchical label substitution (HLS), we set d = 1, i.e., sub-genres are replaced with their respective  X  X eta genre X . In addition, we set k = 3 for EM L T opk , that is, we select the top 3 largest scores and average them. For score-based label selection ( EM L C n L k ), we set n = 3 and k = 2. More specifically, we first select the top 3 scores from each classifier and then select the top 2 class labels accord-ingly. We provide more discussions on these parameters in Section 6.4.
In this section, we present the results from our experi-ments. Each ensemble technique is compared with the indi-vidual multi-label classifiers to determine its performance.
Table 3 shows the comparison of the bipartition-based ensemble techniques with the individual multi-label classi-fiers for D Ra using example-based evaluation metrics; the best result for each metric is shown in bold face. Note that in the Table 3, (  X  ) indicates better performance when the value is smaller while (  X  ) indicates better performance when the value is bigger. When the individual multi-label classifiers are compared with each other, we find that CLR and HOMER demonstrate the best performance for differ-ent example-based metrics. Furthermore, when we compare the classifiers using label-based metrics, presented in Ta-ble 4, we find that CLR performs well for MicroP, MacroP, and Macro F 1 , while IBLR, HOMER, and RA k EL perform well for Micro F 1 , MicroR, and MacroR respectively. How-ever, by using ensemble techniques, significant performance gains have been observed for a majority of example-based and label-based metrics. Specifically, HLS outperforms the individual multi-label classification algorithms and the EML techniques for example-based metrics and also offers an im-provement in classification performance for various label-based metrics. If we exclude HLS from our analysis and compare the remaining ensemble techniques, we find that a selection of them perform better for a majority of the evalua-tion metrics. However, we find that there is little consistency between the techniques making it difficult to select one that performs the best overall.
 Table 5: Comparison of score-based ensembles for D R a using rank-based metrics.

Table 5 presents the comparison of score-based ensem-ble techniques with the individual classifiers for D Ra using rank-based evaluation metrics. First, when the individual classifiers are compared with each other, we find that CLR delivers the best performance for all of the metrics. We find this interesting as CLR does not outperform the other multi-label classifiers for the example-based and label-based metrics. Further investigation is needed into this result. As before, the fusion of multi-label classifiers has improved the overall performance for rank-based metrics. We observe that EML T opk makes an impact on the performance in compari-son to the other EML techniques and multi-label classifiers. Moreover, it offers the best performance for all of the rank-based metrics for D Ra .

We find that the largest performance improvements are of-fered by HLS for bipartition-based ensembles and EML T opk for score-based ensembles, respectively. It should be noted that, the performance gain observed for HLS is predictable as the label space is simplified by reducing the number of possible  X  X verlapping X  genres. This is further discussed in Section 6.4.
Table 6 shows the comparison of the bipartition-based en-sembles with the individual multi-label classifiers for D using example-based evaluation metrics. We find it inter-esting that for this data set, when the individual multi-label classifiers are compared with each other, we find a similar trend to the results reported in Table 3. That is, CLR and HOMER perform well for a selection of evaluation metrics. When we compare the individual classifiers using label-based metrics, presented in Table 7, we find that CLR performs well for MicroP, Micro F 1 , MacroP, and Macro F while HOMER and RA k EL perform well for MicroR, and MacroR, respectively. This is similar to the results reported in Table 4. As before, using the proposed ensemble of multi-label classifiers has improved the overall performance for example-based and label-based metrics, i.e., HLS(EML I ), HLS(EML U ), and HLS(EML MV ) deliver improvements for a selected set of example-based and label-based metrics. Fur-thermore, if we exclude HLS from our analysis and compare the remaining ensembles, we find that for a majority of the evaluation metrics, a selection of the ensembles demonstrate the best performance. However, analogous to D Ra , we find it difficult to select one that performs the best overall. Table 8: Comparison of score-based ensembles for D A r using rank-based metrics.

Table 8 presents the comparison of score-based ensembles with the individual classifiers for D Ar using rank-based eval-uation metrics. When the individual multi-label classifiers are compared with each other, we find that CLR performs the best for all of the metrics. However, the combination of multi-label classifiers offers improvements for different evalu-ation metrics. For example, performance gains are observed for the evaluation metrics AP and OE with an ensemble us-ing the Top-k rule (EML T opk ). We also observe that CLR outperforms all of the proposed ensemble techniques for the evaluation metrics CO and RL. Note the difference in CO for CLR and EML Mean . This will be discussed further in Section 6.4.

We find it interesting that for this dataset the average classification performance of each multi-label classifier and ensemble is lower than the performance on D Ra . These re-sults are expected as classification accuracy can be lower if an artist filter is used since an artist is restricted from ap-pearing in both the training and testing sets [16]. This will be discussed further in Section 6.4.
Finally, Table 9 shows the comparison of the bipartition-based ensembles with the individual multi-label classifiers for D Al using example-based evaluation metrics. It is easy to see that the results for the individual classifiers are sim-ilar to those observed for D Ra and D Ar . Specifically, we find that CLR performs well for HL, F 1 and Prec. while HOMER delivers the best performance for Accu. and Re-call. Moreover, when we compare the individual classifiers using label-based metrics, presented in Table 10, we find that CLR and HOMER perform well for various evaluation metrics. However, using the proposed ensemble techniques improves classification performance. Once again, we observe that HLS outperforms the other ensembles and classifiers for the majority of the evaluation metrics. If we exclude HLS from our analysis, we find that the performance of the re-maining ensembles is similar to those reported on D Ra and D
Ar , i.e., the ensembles demonstrate the best performance for a majority of the evaluation metrics. However, there is little consistency between the techniques making it difficult to select one that performs the best overall.
 Table 11: Comparison of score-based ensembles for D A l using rank-based metrics.

Table 11 presents the comparison of score-based ensembles with the individual classifiers for D Al using rank-based eval-uation metrics. With regard to the individual multi-label classifiers, we observe that CLR performs the best for all of the metrics. This is analogous to the results presented for D
Ra and D Ar . As before, improvements are observed by us-ing the proposed ensemble of multi-label classifiers. Specif-ically, EML Mean demonstrates the best performance of the proposed ensembles while EML T opk follows shortly behind. Furthermore, we observe that CLR outperforms the other proposed ensemble techniques for Coverage. However, we note that the difference is marginal.
The results in our experiments show the merits of combin-ing multi-label classification algorithms for music genre clas-sification. For all three datasets, we observe improvements to classification performance when using heterogeneous en-semble techniques. However, this performance gain is at the expense of inherent computational cost as individual classi-fiers need to be trained and combined. Additionally, some of the proposed ensemble techniques provide no improvement and it would be interesting to further explore them. It is im-portant to note that all of the processing and experiments performed in this work are conducted off-line. Therefore, we are not primarily concerned with the inherent computa-tional cost incurred. In the following, we discuss some other related issues.
 Frame Size As discussed in Section 5, to capture the fine-timescale structures of music, features are typically extracted from short audio frames. We present a brief systematic ex-ploration of the effects of frame size on multi-label genre classification. We examine the classification performance of each classifier and EML as we adjust the frame size, f r , rep-resented as the number of samples collected during a certain time period. Each classifier and ensemble is evaluated using f r  X  { 256 , 340 , 512 , 1024 , 2048 , 3200 , 4096 } , corresponding to approximately 16ms, 21ms, 32ms, 64ms, 128ms, 200ms, and 256ms, respectively.

For the sake of space, we only present the performance of the proposed ensembles along with two individual classifica-tion algorithms for comparison using D Ra . Figure 2 shows the classification performance of the proposed bipartition-based ensembles along with CLR and HOMER using the evaluation metric HL. Moreover, Figure 3 shows the classi-fication performance of the proposed score-based ensembles using the evaluation metric AP. We observe that the pro-
Figure 2: Bipartition-based ensemble performance u sing D Ra for various frame sizes. Figure 3: Score-based ensemble performance using D
R a for various frame sizes. posed ensembles tend to perform well when f r = { 340 , 512 , 1024 , 3200 } . However, we note a decrease in performance when f r = { 2048 , 4096 } . These results are consistently ob-served for the other evaluation metrics and datasets with the exception that classification performance increases for D when f r = 4096. These results lend support to the notion of using a frame size of 23ms, as proposed by various other works. Table 12: Classification performance of E M L T opk for different k .
 Top-k rule : The Top-k rule ( EM L T opk ) selects the top k largest scores and averages them, where k is a user-selected parameter. To determine this value, we perform a series of experiments on D Ra and examine the performance as we adjust k . Table 12 shows the classification performance of EM L T opk where k  X  { 2 , 3 , 4 , 5 } using the evaluation metrics AP, CO, RL, and OE. We observe that EM L T opk demon-strates good achievement on D Ra when k = 3 for the ma-jority of evaluation metrics. We note that when k is set to the number of classifiers in the ensemble, the result will be the same as the one obtained using the Mean rule.
 Score-based Label Selection : Similar to the Top-k rule, we perform a series of experiments on D Ra to determine the values of n and k for EM L C n L k . Recall that in this en-semble, we first select the top n scores and then select the top k class labels accordingly. Table 13 shows the classifica-tion performance of EM L C n L k using the evaluation metrics HLoss, Accu., Recall, F 1 , and Prec. We observe that as both values of n and k increase, classification performance tends to decrease for the evaluation metrics. We can see that EM L C n L k performs best when n = 3 and k = 2. This could be plausibly explained by the label cardinality which is between 2 and 3 for each instance in our datasets, as shown in Table 2. It is important to note that we examine a wide range of values for n and k . However, due to space limitations, we only report a subset of the results. Hierarchical Label Substitution : From the results pre-sented above, we observe that Hierarchical Label Substi-tution (HLS) outperforms the bipartition-based ensemble techniques for a majority of the evaluation metrics. More-over, significant performance gains are observed for all of the dataset when HLS is used with one of the proposed bipartition-based ensemble techniques. For example, there is a 14.51% increase in Prec. for D Ra when compared to CLR. These results are attributed to the simplification of the label space by reducing the number of possible  X  X verlap-ping X  genres. We believe this ensemble produces a common set of high-level genres which is closer to the human experi-ence.
 Table 13: Classification performance of E M L C n L k for different n and k .
 Data Selection We observe, on average, that D Ra achieves the best dataset performance as shown in Tables 3, 4, and 5. Moreover, classification performance on D Ar is lower than D
Ra and D Al . The artist filter limits performance by re-stricting artists from appearing in both the training and testing sets. In addition, D Al employs an album filter to ex-clude music pieces from the same album appearing in both training and testing sets. However, this does not exclude artists from appearing in them. For this reason, we observe that the classification performance on D Al is better than D
Ar but worse than D Ra . We also notice a larger difference in the performance of the multi-label classifiers and ensem-ble techniques between D Ar and D Al . For example, if we examine the evaluation metric CO for CLR and EML Mean we find that the difference between them in D Ar (see Ta-ble 8) is larger than the difference between them in D Al Table 11). This is due to the data selection as there is some possible overlap in D Al , as discussed above.
I n this work, we propose a set of ensemble techniques that not only improve upon individual multi-label classification algorithms, but also overcomes their limitations as afore-mentioned. For all of the datasets, we observe improve-ments to classification performance when using ensemble techniques. To the best of our knowledge, this is the first study that aims to combine heterogeneous multi-label clas-sification algorithms for music genre classification.
To improve the impact of this work, we plan to use more multi-label classification algorithms and investigate other ensemble techniques. From our study, we observe that some of the proposed ensemble techniques provide no improve-ments and it would be interesting to further explore them. A wealth of work exists surrounding the area of ensemble methods and may offer some insight. Moreover, further in-vestigation is needed into the selection of classifier parame-ters. Finally, while we focus on the computational classifi-cation of music genres, it would be interesting to investigate how to combine our conclusions here with human perception of music. We believe that this could further improve music genre prediction. [1] J. J. Aucouturier and F. Pachet. Representing musical [2] J. J. Aucouturier and F. Pachet. Improving timbre [3] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and [4] N. V. Chawla and J. Sylvester. Exploiting diversity in [5] W. Cheng and E. Hullermeier. Combining [6] J. Futrelle and J. S. Downie. Interdisciplinary [7] M. Ioannou, G. Sakkas, G. Tsoumakas, and [8] L. Kuncheva. Combining Pattern Classifiers: Methods [9] E. Law and L. von Ahn. Input-agreement: a new [10] H. Lukashevich, J. Abe X er, C. Dittmar, and [11] C. McKay and I. Fujinaga. Musical genre [12] S. R. Ness, A. Theocharis, G. Tzanetakis, and L. G. [13] F. Pachet. Content management for electronic music [14] F. Pachet and D. Cazaly. A taxonomy of musical [15] F. Pachet and P. Roy. Improving multi-label analysis [16] E. Pampalk, A. Flexer, and G. Widmer.
 [17] C. Sanden, C. R. Befus, and J. Z. Zhang. Perception [18] M. Tahir, J. Kittler, K. Mikolajczyk, and F. Yan. [19] K. Trohidis, G. Tsoumakas, G. Kalliris, and [20] G. Tsoumakas and I. Katakis. Multi-label [21] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining [22] G. Tsoumakas, J. Vilcek, E. Spyromitros, and [23] G. Tzanetakis and P. Cook. Musical genre [24] D. Wang, T. Li, and M. Ogihara. Are tags better than [25] A. Wieczorkowska, P. Synak, and R. Zbigniew.
 [26] M. L. Zhang and Z. H. Zhou. ML-k NN: A lazy
