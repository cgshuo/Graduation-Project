 theoretical results and demonstrate tremendous speedup over the prior state of the art. enables our method to perform so far beyond the pre vious state of the art. kernel conditional density estimation (KCDE): general way. We begin by formulating an inducti ve generalization of the problem class: B represents the base case, in which a tuple of constant arguments X of variable arguments X the innermost lea ve-one-out summations of S single points other than x k -tuples, the base comple xity is O ( n k ) , where n is the size of the dataset. function are X among the nested instances, e.g. if, as in S is O ( n ) , then the overall base comple xity is O ( n 2 ) .
 to O ( n log p ) . This still lea ves an intractable computation for p as small as 4. for all situations that can tolerate minor stochasticity in the approximated output. sample statistics and how tree-based methods can impro ve those statistics. To begin, note that the summation B ( X expectation is tak en over a discrete distrib ution P to estimating from P is the sample variance, from which we can construct the standard condence interv al: j ^ z implied by z we can ensure our tar get relati ve error by requiring that z which rearranges to: with probability 1 , given that ^ suggests an iterati ve sampling procedure in which m starts at a value m procedure is summarized in Algorithm 1, and we state its error guarantee as a theorem. Theor em 1. Given m probability 1 . Algorithm 1 simply increases the sample size until this condition is met. Algorithm 1 Iterati ve Monte Carlo approximation for at summations. an upper bound, in terms of the distrib utional properties of the full set of f which Equation 3 will be satised.
 Theor em 2. Given m probability at least 1 2 Algorithm 1 terminates with m O 2 f Proof. The termination condition is dri ven by ^ 2 First, with probability 1 we have a lower bound on the absolute value of the sample mean: j ^ to infer that ^ ^ the y will jointly hold with probability at least 1 2 , giving the follo wing 1 2 bound: algorithm will terminate with m no lar ger than: is because the sample comple xity depends only on the distrib utional featur es ( 2 to a constant while speedup becomes unbounded as the dataset size goes to innity . Second, the bound has sensible dependence on by stratifying the dataset into low-v ariance regions.
 we need n m characterization of whether speedup will be attained. Algorithm 2 Iterati ve Monte Carlo approximation for nested summations. is to apply the single-stage Monte Carlo algorithm over the terms f invocation to obtain approximations for the arguments G Theor em 3. Given m sample from a distrib ution P CL T on the sample mean ^ For each sampled f been dra wn from a CL T-type normal distrib ution. Because the b G of Theorem 1). Let b G from m con verges in distrib ution to N ( G entries of the covariance off-diagonal elements may be non-zero if the b G as a variance reduction technique).
 Given the asymptotic normality of b G method can be used, with some modication, to sho w that f Thus, asymptotically , f This being the case, uniform sampling of the recursi vely estimated f a distrib ution ~ P over ~ P base case completes the inducti ve proof.
 Note that the variance over ~ P In other words, the variance with recursi ve approximation is the exact variance 2 of the variances 2 ~ Cor ollary 2.1. Given m with probability at least 1 2 Algorithm 2 terminates with m O ~ 2 f approximated arguments do not pass through to or compound in the overall estimator ^ inuence appears in the variance 2 variance ~ 2 Algorithm 3 Iterati ve Monte Carlo approximation for nested summations with stratication. an extremely fast, accurate, and general approximation scheme.
 interest. In the case of generalized summations, the values being sampled are the f Algorithm 3, and we now establish its error guarantee.
 Theor em 4. Given m properties of the stratied sample mean and its variance estimator (see [7]): where j inde xes the strata, ^ fraction of summation terms in stratum j , and q stratied ^ conditions necessary for the error guarantee, this establishes the theorem. The true variance 2 (^ it is sho wn that 2 stratication can only reduce 2 trarily , 2 with highest p While we never kno w the approximate the optimal allocation (this is the optAlloc routine). Cor ollary 2.2. Given m with probability at least 1 2 Algorithm 3 terminates with m O 2 f s Corr elated Sampling . The variance of recursi vely estimated f variance of f inducing positi ve (ne gative) covariance between G sampled points across the estimates of G some cases the expression for f presented here were beneted by correlated sampling on top of stratication. functions used for bandwidth optimization, i.e. S For the O ( n 2 ) S good, and could be impro ved if desired by increasing m we chose to trade a slight increase in error for an increase in speed. at performance is man y orders of magnitude better than that of pre vious methods. generalization to other computational bottlenecks such as linear algebraic operations.
