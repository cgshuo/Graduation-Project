 In this paper we present a novel approach to learning seman-tic models for multiple domains, which we use to categorize Wikipedia pages and to perform domain Word Sense Dis-ambiguation (WSD). In order to learn a semantic model for each domain we first extract relevant terms from the texts in the domain and then use these terms to initialize a ran-dom walk over the WordNet graph. Given an input text, we check the semantic models, choose the appropriate domain for that text and use the best-matching model to perform WSD. Our results show considerable improvements on text categorization and domain WSD tasks.
 I.2.7 [ Natural Language Processing ]: Text analysis Algorithms Word Sense Disambiguation, Text Classification
In everyday life the use of natural language is often con-fined to specific fields of knowledge. Examples include news-paper sections (crime, sports, finance, etc.), blogs and Web sites on specific topics, business documents, etc. Performing a semantic analysis of domain text is thus a crucial Natu-ral Language Processing (NLP) task that can boost appli-cations such as Question Answering, Information Retrieval and Information Extraction.

The first step in determining the content of an input text is to assign it a domain or topic label, a task referred to as Text Classification (TC). Text classification is a widely stud-ied topic in machine learning and NLP, and has attracted the use of a wide range of machine learning algorithms and text processing approaches. More recently the use of encyclope-dic knowledge resources such as Wikipedia for classification purposes has received considerable attention in the scien-tific community. In fact, Wikipedia contains a vast amount of information covering virtually all domains of knowledge. Wikipedia has been used to enrich shallow text represen-tations with concepts in order to improve supervised text classification [16] and concept-based kernels have been de-veloped to perform semantically-informed supervised text categorization [15].

In the last few years attention has also been paid to the semantic analysis of domain texts at the lexical level, a task called Domain Word Sense Disambiguation (WSD). The au-tomated association of meanings with words occurring in a domain context can benefit from domain cues such as topic words. Current approaches either exploit supervised tech-niques [2] or are based on unsupervised models of domains [5] and word sense dominance [10, 9]. However, the former rely on sense-annotated corpora for many domains, whereas the latter do not achieve high performance (unless a very large knowledge resource is available [12]). Although intermediate solutions have recently been proposed [7], sense-tagged data are still needed for accurate domain WSD.

In this paper we present a novel approach to the semantic analysis of domain texts. For each domain of interest we au-tomatically learn a so-called semantic model vector for that domain. To this end we extract relevant terms from selected texts in the domain, determine their meaning and then use this information to initialize a random walk over the Word-Net [3] graph. This yields a semantic model for each domain, represented as a vector of probabilities over WordNet nodes. Given an input text we identify the best-matching model, i.e., domain, for that text. As a result, the corresponding semantic model is used to disambiguate the words in the text, assigning the senses that are best fit to the context of occurrence and the domain of the text. The strength of our work lies in the use of the same semantic model to perform two different tasks, namely Text Classification and Word Sense Disambiguation. In fact, our experiments show that our approach achieves high performance in both tasks.
We first learn a semantic representation for each domain in the form of Semantic Model Vectors. To this end we perform the following steps: 1. Analyse the training documents in each domain and 2. Use these lists of terms to obtain a weighted list 3. For each domain perform a random walk over the Word-4. Reweight synsets according to their prominence across 5. We thus obtain a semantic model for each domain in
Given a domain d , we first extract candidate domain terms from the training document collection for that domain, and weight them according to TFIDF: where TF d ( t ) is the frequency of term t in the document collection for domain d , N d is the number of documents in the domain collection, and n d,t is the number of domain documents in which t occurs.
Given the weighted list of domain terms, we produce a weighted list of synsets in two different ways.

The uniform method is to distribute the TFIDF weight of each word across all synsets that are denoted by the word (i.e., all its senses) uniformly. Let s be a synset, Synonyms ( s ) the set of words (i.e., synonyms) in the synset s ,TFIDF d the TFIDF value of a word w in domain d and Synsets ( w ) the set of synsets (i.e., senses) for the word w . Then, the uniform method assigns the following score to synset s :
UniformScore d ( s )=
We also tried a more sophisticated method based on re-lated words . In this method we use the information about related words to compute the synset relevance to the do-main. We first define a set of related words for each synset s as follows:
RW d ( s )= where R ( s ) is the set of WordNet synsets directly related to s by any lexical or semantic relation, also including s itself, and Synonyms ( s )and GlossW ords ( s )arethesetof content words which denote s and appear in the gloss of s , respectively. Then, we define a score for synset s given by the normalized sum of the domain TFIDF values of the words in the set of related words of s :
Finally, we normalize the domain scores to range between zero and one, and produce a weighted list of synsets for each domain d .
Given the graph-based representation of WordNet, we ap-ply Personalized PageRank to each domain, obtaining a probability distribution over WordNet synsets for each do-main d , the Semantic Model Vector SMV d . For initializ-ing the random walk we set the so-called reset distribution, which defines which synsets are most relevant for the do-main of interest (cf. [1] for further details on Personalized PageRank). We tried two different reset distributions: the weighted list of synsets for each domain, as produced by the uniform and the related words method in Section 2.2.
In the previous step we obtained an SMV for each do-main. Ideally, domain-specific synsets will rank high in the corresponding domain SMV, and low in the others.
However, some synsets are very central in the WordNet graph (i.e., they are linked to synsets which also have a high number of relations) and receive high scores on most domains. Examples include synsets such as individual 1 n or britain 1 n . Thus, we decided to reweight the SMVs obtained in the previous step, trying to promote domain specific synsets and demote general-purpose synsets (i.e., those concepts with similar scores in all domains).

Let SMV d be the SMV vector for domain d  X  Domains, so that SMV d ( s ) is the score of synset s in the domain d . We calculate the entropy for s using the formula:
Senses with high scores in some domains but low scores in other domains will receive a low entropy. In contrast, senses with high (or low) values on all domains will receive high entropy.

After calculating the entropy, we obtain new SMV d vec-tors by dividing each synset score by the synset X  X  entropy:
Given a test document and the SMVs for each domain (either SMV d or SMV d , see Sections 2.3 and 2.4), we com-bine the information coming from the TFIDF from each do-main (cf. Section 2.1) and the SMVs. Given the fact that the synsets with lowest weights might confuse the classification algorithm, we decided to keep the synsets with words having high TFIDF, and assign a zero weight to the rest. For a do-main d , we first select the set top K of K top-ranking words accordingtotheirTFIDF d values. We then keep the synsets in each SMV d that contain at least one word in top K ,zeroing the rest and normalizing, yielding a new vector SMV K d .This vector is very close to SMV d , but synsets with low TFIDF words are assigned a zero weight. Finally we select the best domain d best for document doc as follows: where Synsets ( w ) returns the synsets denoted by w .
In order to perform Word Sense Disambiguation of the words in a test document, we first classify the document as described above. For each content word to be disambiguated in the document we then combine the information in the chosen SMV and the context(s) of the target word. We thus create a large context by joining all the sentences where the word occurs in the document, set the reset distribution with the content words in the context, and run Personalized PageRank, yielding a Personalized Pagerank vector for the word (PPV w ).

We then select the best sense of the word as follows: where w is the word to be disambiguated, Synsets ( w )are the synsets of word w , d is the chosen domain, SMV d is the SMV of domain d and PPV w is the PPV of the word X  X  context.
We first describe our general experimental setup, which we used to perform experiments on text classification (Section 3.3) and WSD (Section 3.4).
We first chose the set of target domains. We selected the 29 domain labels used in featured Wikipedia articles  X  a list of the best articles in Wikipedia, as determined collectively by its editors. 1 The domain labels cover a wide variety of topics, including business, arts, computing, sport, etc.
For each domain, we randomly picked up 30 featured ar-ticles, thus totalizing 30  X  29 = 870 articles. Each article in the dataset has a single correct domain label. Note that we use Wikipedia only to define the training and test dataset (i.e., a fixed set of domains and the corresponding articles), but that our semantic model is solely based on WordNet.
To build our semantic models we extracted the terms from the training set and calculated their TFIDF scores for each domain. For each domain d , we then used the TFIDF do-main scores to weight synsets and built the SMV d and the corresponding model with cross-domain reweighting.
To perform text classification, we prepared a dataset of 290 Wikipedia articles (10 for each of the 29 domains defined for the training set, cf. Section 3.1) that were collaboratively classified as  X  X ood articles X  2 , i.e., of good quality but that are not yet, or are unlikely to reach, featured article quality. Similarly to the training set, the test set is single-labeled.
We experimented with four variants of our SMVs, each obtained by selecting a specific combination of the reset distribution (either the Uniform or the Related Words strategy) and cross-domain reweighting (CDR) (entropy-based reweighting either enabled or disabled).
Our approach has only one parameter, that is, the K con-stant used for selecting the top domain terms in each SMV for text classification. In order to choose the best value of K , we created a tuning dataset by randomly sampling a set of 58  X  X ood X  Wikipedia articles (2 articles per domain), with no overlap with our training and test sets. We then ran our classification algorithm on this tuning set and found the optimal values for each variant of our method (1250 and 750 respectively for Uniform SMVs with and without cross-domain reweighting; similarly, 400 and 1350, respectively, for RelWords SMVs).
As a first baseline we calculated the random baseline , which selects a random domain label. We also constructed a baseline TFIDF-based classification system. Given the test document, we select the doma in that maximizes the sum of the TFIDF scores for the terms in the document:
In order to give a reference for the results of a state-of-the-art machine learning technique, we chose Support Vector Machines (SVM) [6] as the skyline. Given that we are work-ing on many domains, we adopt the one versus all paradigm.
We used a linear kernel, whose C parameter was set to 0 . 5, after using the tuning dataset for optimization (cf. Section 3.3.3). The features were composed of word stems and their normalized TFIDF weights, following the widely used bag-of-words approach.
In Table 1 we report the results of our systems in the clas-sification task. We show recall@k, i.e., the ratio of examples where the correct answer is found among the system X  X  top Table 1: Classification results on Wikipedia (  X  :sig-nif. diff. from RelWords SMV+CDR, p&lt; 0 . 001 ). k domain labels ( k =1 , 2 , 3). The main evaluation measure is recall@1, which is equivalent to recall, precision, F1 and Break-Even Point (BEP) in this dataset.

Three of our best SMV variants surpass the TFIDF base-line by several points for all three values of k , showing that our classification performance is generally good. The results show that CDR is beneficial in all cases.
 We observe that a 67% recall@1 obtained with our Rel-Words+CDR SMV on this dataset is a significant result, considering that we have 29 classes with identical prior prob-abilities (the performance of the random baseline is extremely low, a mere 3.5%).
Note that our method is basically the TFIDF baseline with WordNet information added on top of it. The improve-ment of our systems over the TFIDF baseline could be caused by the use of cutoffs alone (cf. Section 3.3.3). We thus com-pared our systems against a TFIDF baseline with cutoff on the top-ranking K words (the best value for K was se-lected on the tuning dataset). The result for this TFIDF top K baseline is 61.1 recall@1, only one point above the unthresholded baseline, showing that semantic information from WordNet can improve results beyond the keyword level.
In a related experiment, we used the result of the  X  X eight-ing domain synsets X  X tep (cf. Section 2.2) directly. This would shed some light on the usefulness of the Personalized PageR-ank step when constructing the SMVs. The synset weights provided by the related words approach, when used by our TC algorithm without CDR, yields a recall@1 of 60.8 when used directly, well below the 64.2 attained by our SMV with no CDR ( X  X elWords SMV X  row in Table 1). The only dif-ference between the two runs is the use of PPR, beneficial when building SMVs. We attribute such positive difference to the semantic spreading effect of PPR.
For our domain WSD experiment we used the gold stan-dard dataset released by Koeling et al. [8]. This dataset com-prises examples of 41 words retrieved from the Sports and Fi-nance sections of the Reuters corpus, and also from the bal-anced British National Corpus (BNC). The selected words are quite polysemous and thus difficult to disambiguate, with an average polysemy of 6.7 senses, ranging from 2 to 13 senses. We created a pseudo-document for each word con-taining all its contexts, classified this pseudo-document into one of the domains, and proceeded as in Section 2.6. the random baseline  X  which selects a random sense for each word item to disambiguate, and the SemCor Most Frequent Sense (MFS) baseline  X  based on sense occur-rence counts within the largest sense-tagged corpus. As a skyline we applied the test MFS , i.e., we assigned to each Table 2: WSD results on Finance and Sports (  X  : signif. diff. from RelWords SMV + CDR, p&lt; 0 . 001 ). word its most frequent sense as taken from the test set. This skyline is hard to beat, even for supervised systems [8].
We experimented our Uniform and RelWords SMV methods with and without cross-domain reweighting (CDR). Note that no additional parameter tuning is needed for WSD. We compared our systems with UKB [1], a state-of-the-art WSD system based on Personalized PageRank.
We calculated recall on the two domain test sets (see Ta-ble 2). Our RelWords SMV + CDR outperforms all baselines andtheUKBmethodby12pointsonSportsand8points on Finance (relative improvement of 31.5% and 16.6% re-spectively), and achieves state-of-the-art performance. We remark that the same configuration, i.e., RelWords SMV + CDR, attains the best performance in both catego-rization and WSD tasks, which provides an added value to our system. Interestingly, Uniform SMV + CDR also outper-forms UKB  X  by a small margin on Sports and many points on Finance  X  but other variants obtain lower performance.
Text Classification. The state of the art in text classi-fication uses word-based features and sophisticated machine learning algorithms such as Support Vector Machines or lo-gistic regression among others. Much work tries to improve TC by enriching the documents with semantic information, e.g., with limited information from WordNet [14]. In con-trast, our approach uses the rich structure of WordNet, and yields improvement over the text-only baseline using just the semantic model.

Subsequent work focused on Wikipedia [4, 16], which has been used to augment the text representation with semantic features with a larger coverage of named entities. In contrast to the above approaches, in which Machine Learning is com-bined with bags of words, we show that a semantics-only representation improves over a bag-of-words representation.
Domain Word Sense Disambiguation. Domain WSD is often performed in a type-based fashion, i.e., by assigning a single sense per word. Distributionally similar neighbors in raw text can be used as cues to determine the predominant sense of a target word using a semantic similarity measure [8, 9]. Other distributional methods use word-category matrices and synonym occurrence counts (see [11] for a survey).
Knowledge-based approaches  X  which exploit the seman-tic structure of resources such as WordNet  X  take the middle ground between domain-driven [13] and supervised methods [7], and have been shown to perform equally well on domain texts [1, 12]. In this paper we go one step further: we pro-pose an approach with little human intervention which first classifies a word context and then uses the selected SMV for WSD. In contrast to state-of-the-art WSD [12], we do not use the MFS as a backoff strategy, thanks to our effective combination of the classification and disambiguation steps.
In this paper we presented a unified approach for catego-rizing Wikipedia pages and performing Domain Word Sense Disambiguation. Key to our approach is the automatic ac-quisition of semantic models for multiple domains which we then use to perform semantic analysis of input texts.
The crucial fact that we provide a unified approach with remarkably good results in both tasks opens an exciting direction of research, thus enabling deeper understanding and hopefully boosting performance on applications such as Question Answering and Information Retrieval.
 The first author gratefully acknowledges the support of the ERC Starting Grant MultiJEDI No. 259234. [1] E. Agirre, O. L. de Lacalle, and A. Soroa.
 [2] E. Agirre and O. Lopez de Lacalle. Supervised domain [3] C. Fellbaum, editor. WordNet: An Electronic [4] E. Gabrilovich and S. Markovitch. Wikipedia-based [5] A. Gliozzo, C. Strapparava, and I. Dagan.
 [6] T. Joachims. Learning to Classify Text Using Support [7] M. Khapra, A. Kulkarni, S. Sohoney, and [8] R. Koeling, D. McCarthy, and J. Carroll.
 [9] D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. [10] S. Mohammad and G. Hirst. Determining word sense [11] R. Navigli. Word Sense Disambiguation: A survey. [12] S. P. Ponzetto and R. Navigli. Knowledge-rich Word [13] C. Strapparava, A. Gliozzo, and C. Giuliano. Pattern [14] L. Ure  X  na-L  X  opez, M. de Buenaga Rodr  X   X guez, and [15] P. Wang and C. Domeniconi. Building semantic [16] P.Wang,J.Hu,H.-J.Zeng,andZ.Chen.Using
