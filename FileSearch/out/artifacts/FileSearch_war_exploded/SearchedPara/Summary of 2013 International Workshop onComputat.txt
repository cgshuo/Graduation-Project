 H.3.3 [ Information Systems ]: Information Search and Re-trieval Theory, Experimentation Computational Scientometrics; Bibliometrics
The field of Scientometrics is concerned with the analysis of science and scientific research. As science advances, sci-entists around the world continue to produce large numbers of research articles, which provide the technological basis for worldwide collection, sharing, and dissemination of sci-entific discoveries. Research ideas are generally developed based on high quality citations. Understanding how research ideas emerge, evolve, or disappear as a topic, what is a good measure of quality of published works, what are the most promising areas of research, how authors connect and influ-ence each other, who are the experts in a field, what works are similar, and who funds a particular research topic are some of the major foci of the rapidly emerging field of Sci-entometrics.

Digital libraries and other databases that store research articles have become a medium for answering such ques-tions. Citation analysis is used to mine large publication graphs in order to extract patterns in the data (e.g., cita-tions per article) that can help measure the quality of a journal. Scientometrics, on the other hand, is used to mine graphs that link together multiple types of entities: authors, publications, conference venues, journals, institutions, etc., in order to assess the quality of science and answer complex questions such as those listed above. Tools such as maps of science that are built from digital libraries, allow different categories of users to satisfy various needs, e.g., help re-searchers to easily access research results, identify relevant funding opportunities, and find collaborators. Moreover, the recent developments in data mining, machine learning, nat-ural language processing, and information retrieval makes it possible to transform the way we analyze research publica-tions, funded proposals, patents, etc., on a web-wide scale.
The primary goals and objectives of the workshop are to promote both theoretical results and practical applications within digital libraries to better answer questions such as the ones above, and address challenges that are faced by to-day  X  Os researchers as well as well-known technological com-panies such as Microsoft and Google and publishers such as ACM and Elsevier.

The combination of classical bibliometrics and novel text mining provides a synergy unavailable with each approach taken independently. In this workshop, we also focus on bibliometrics analysis by using sophisticated text mining, or natural language processing methods, which will enable us to generate some innovative research topics, i.e., full-text ci-tation analysis.
In this workshop, we have 6 accepted papers, which cover different topics including program committees recommen-dation for academic conferences, collaborators recommen-dation, and citations classification and labeling. The list of papers is as follows. 1. Recommending Program Committee Candidates for Aca-demic Conferences by Shuguang Han, Jiepu Jiang, Daqing He, Zhen Yue.

Academic conference is an important channel for schol-arly communication. Establishing a respectful and well-functional program committee (PC) consisting of capable PC members is one of the most important tasks for confer-ence organizers. However, little research has been done in this direction for automatically identifying PC candidates. In this paper, we investigate whether automatic methods can help organizers in recommending PC candidates. PC mem-ber finding is a complex task, which may be influenced by many factors such as the candidates  X  O interests match with conference topics, the candidates  X  O social closeness with PC chairs, the candidates  X  O authoritativeness, as well as the can-didates  X  O publication history in the conference. To examine the importance of each feature, we build a real dataset con-sists of four different conferences: KDD, SIGIR, JCDL and GIS (2007-2011). By splitting the dataset into the training and testing subsets based on the temporal information, we evaluate the performance of each of the four features sepa-rately. The results show that simply using the publication historical data produces the best recommendations. The other features, such as the pure authority based method does not working because there are a lot of PC members who ac-tually have lower authority values. The social similarity can also produce reasonable good results. More interestingly, recommendations from social closeness feature is similar to the publication history in terms of they produced the similar lists of candidates with similar distributions on authorita-tiveness values (we use Page Rank to simulate the author-itativeness values). In our later experiments, we also show reasonable improvements by applying simple linear combi-nation of different features. 2. Recommending Intra-Institutional Scientific Collabora-tion through Co-authorship Network Visualization by Gus-tavo A. Parada, Hector G. Ceballos, Francisco J. Cantu, Luc  X  Sa Rodr  X  Sguez-Aceves
For improving research productivity, quality and dissemi-nation, we propose the development of a visual recommenda-tion tool summing up scientific collaboration best-practices found in literature. Social Network Analysis is applied to a co-authorship network for generating a Potential Collabora-tion Index (PCI) based on productivity, connectivity, simi-larity and expertise. This work is evaluated by recommend-ing intra-institutional collaboration in a comprehensive uni-versity. The accuracy of PCI is documented, along with suggestions and comments from 27 interviewed researchers. 3. First Author Advantage: Citation Labeling in Research by Graham Cormode, S. Muthukrishnan, Jinyun Yan
Citations among research papers, and the networks they form, are the primary object of study in scientometrics. The act of making a citation reflects the citer  X  Os knowledge of the related literature, and of the work being cited. We aim to gain insight into this process by studying citation keys: user-generated labels to identify a cited work. Our main ob-servation is that the first listed author is disproportionately represented in such labels, implying a strong mental bias towards the first author. 4. Benchmarking Domain-Specific Expert Search Using Workshop Program Committees by Georgeta Bordea, Toine Bogers, Paul Buitelaar
Traditionally, relevance assessments for expert search have been gathered through self-assessment or based on the opin-ions of co-workers. We introduce three benchmark datasets for expert search that use conference workshops for relevance assessment. Our data sets cover entire research domains as opposed to single institutions. In addition, they provide a larger number of topic-person associations and allow a more objective and fine-grained evaluation of expertise than exist-ing data sets do. We present and discuss base-line results for a language modeling and a topic-centric approach to expert search. We find that the topic-centric approach achieves the best results on domain-specific datasets. 5. Verb Selection using Semantic Role Labeling for Cita-tion Classification by Mohammad Abdullatif, Yun Sing Koh, Gillian Dobbie, and Shafiq Alam
Citation classification is the task of assigning a category to a reference or citation. The current sets of categories or classes proposed in the literature vary in size and they are based on the analysis of a small sample of citation sentences. We are developing a process to automatically generate such categories and base them on the analysis of a large corpus of papers. Part of the generation process involves selecting the main verb relevant to the reference being cited in the sentence. In this paper we present our recently developed technique that automatically identifies the relevant verb in a citation sentence. The technique uses heuristic rules, which are dependent on the results of a semantic role labeler. Four test sets were collected, and the common annotations of the test sets annotated by three people were used to assess the accuracy of the rules. Through experimentation we show that the average accuracy achieved using our technique that automatically extracts verbs from citation sentences across the four test sets is reasonable at 75%. 6. Revealing Comparative Advantages in the Backbone of Science by Miguel Guevara and Marcelo Mendoza
Mapping Science across countries is a challenging task in the field of Scientometrics. A number of efforts trying to cope with this task has been discussed in the state of the art, addressing this challenge by processing collections of scien-tific digital libraries and visualizing author-based measures (for instance, the h-index) or document-based measures (for instance, the averaged number of citations per document). A major drawback of these approaches is related to the pres-ence of bias. The bigger the country, the higher the mea-sure value. We explore the use of an econometric index to tackle this limitation, known as the Revealed Comparative Advantage measure (RCA). Using RCA, the diversity and ubiquity of each field of knowledge is mapped across coun-tries. Then, an RCA-based proximity function is explored to visualize citation and h-index ubiquity. Science maps re-lating 27 knowledge areas and 237 countries are introduced using data crawled from Scimago that ranges from 1996 to 2011. Our results show that the proposal is feasible and can be extended to elaborate a global scientific production characterization.
