 We develop and make publicly available an entity search test col-lection based on the DBpedia knowledge base. This includes a large number of queries and corresponding relevance judgments from previous benchmarking campaigns, covering a broad range of information needs, ranging from short keyword queries to natural language questions. Further, we present baseline results for this collection with a set of retrieval models based on language mod-eling and BM25. Finally, we perform an initial analysis to shed light on certain characteristics that make this data set particularly challenging.
 H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Entity retrieval, test collections, semantic search, DBpedia
Many information needs revolve around entities as has been ob-served in different application domains, including question answer-ing [14, 21], enterprise [1], and web [19] search. This is reflected by the recent emergence of a series of benchmarking campaigns fo-cusing on entity retrieval evaluation in various settings. The INEX 2007-2009 Entity Retrieval track [8, 9] studies entity retrieval in Wikipedia. The Linked Data track at INEX 2012 also considers entities from Wikipedia, but articles are enriched with RDF prop-erties from both DBpedia and YAGO2 [22]. The TREC 2009-2011 Entity track [1, 3] defines the related entity finding task: return homepages of entities, of a specified type, that engage in a spec-ified relationship with a given source entity. In 2010, the Seman-tic Search Challenge introduced a platform for evaluating ad-hoc queries, targeting a particular entity, over a diverse collection of Linked Data sources [11]. The 2011 edition of the challenge pre-sented a second task, list search, with more complex queries [4]. Finally, the Question Answering over Linked Data challenge fo-cuses on natural language question-answering over selected RDF datasets, DBpedia and MusicBrainz [14].

Finding new challenges and tasks for entity search was one of the main topics of discussion at the recently held 1st Joint International Workshop on Entity-oriented and Semantic Search (JIWES) [2]. The following action points were identified as important priorities for future research and development: (A1) Getting more representative information needs and favouring (A2) Limiting search to a smaller, fixed set of entity types (as op-(A3) Using test collections that integrate both structured and un-In this paper we address the above issues by proposing an entity search test collection based on DBpedia. We synthesise queries from all these previous benchmarking efforts into a single query set and map known relevant answers to DBpedia. This results in a diverse query set ranging from short keyword queries to natural language questions, thereby addressing (A1). DBpedia has a con-sistent ontology comprising of 320 classes, organised into a 6 lev-els deep hierarchy; cf. (A2). Finally, as DBpedia is extracted from Wikipedia, there is more textual content available for those who wish to combine structured and unstructured information about en-tities, thereby addressing (A3).

On top of all these, there is one more important, yet still open question: To what extent can methods developed for a particu-lar test set be applied to different settings? To help answer this question we evaluate standard document retrieval models (language models and BM25) and some of their fielded extensions. We make the somewhat surprising finding that, albeit frequently used, none of these extensions is able to substantially and significantly out-perform the document-based (single-field) models. Our topic-level analysis reveals that while often a large number of topics is helped, an approximately identical number of topics is negatively impacted at the same time. Developing methods that can realise improve-ments across the whole query set appears to be an open challenge.
Our contributions in this paper are threefold. First, we create and make publicly available a data set for entity retrieval in DBpedia. Second, we evaluate and compare a set of baseline methods on this data set. Third, we perform a topic-level analysis and point out cer-tain characteristics that make this data set particularly challenging.
The remainder of this paper is organised as follows. In Section 2 we introduce our test collection. Next, in Section 3 we present and evaluate baseline methods. This is followed by a topic-level analysis in Section 4. We summarise our findings in Section 5. http://bit.ly/dbpedia-entity
We consider a range of queries from various benchmarking eval-uation campaigns and attempt to answer them using a large knowl-edge base. In our case this knowledge base is DBpedia, as de-scribed in Section 2.1. Further, we describe both queries and rel-evance judgements in Section 2.2. To conclude the description of the test collection, we give an overview of the evaluation metrics we use in Section 2.3 We use DBpedia as our knowledge base, specifically, version 3.7. DBpedia has X  X part from being one of the most comprehensive knowledge bases on the web X  X he advantage of using a consistent ontology to classify many of its entities via a type predicate. The ontology defines 320 classes, organised into a 6 levels deep hierar-chy. This version of DBpedia describes more than 3 . 64 M entities, of which 1 . 83 M are classified in the DBpedia ontology.
We consider queries from the following benchmarking evalua-tion campaigns (presented in temporal order):
In the 2010 edition, Wikipedia pages are not accepted as entity homepages, therefore, those results cannot be mapped to DBpedia with reasonable effort. We did not include REF 2011 queries as the quality of the pools there is found to be unsatisfactory [3]. The selection above covers a broad range of information needs, ranging from short keyword queries to natural language questions. In all cases, we use only the keyword part of the query and ignore any additional markup, type information, or other hints (like ex-ample entities) that may be available as part of the topic definition according to the original task setup. Also, we take relevance to be binary, that is, both relevant and primary for the TREC Entity queries, and fair and excellent for SemSearch queries count as cor-rect. We normalised all URIs to conform with the encoding used by the official DBpedia dump, replaced redirect pages with the URIs they redirect to, and filtered out URIs that are not entity pages (e.g., categories or templates). Table 1 provides an overview.
We use standard IR evaluation metrics: Mean Average Preci-sion (MAP) and Precision at rank 10 (P@10). To check for signifi-cant differences between runs, we use a two-tailed paired t-test and write M / O and N / H to denote significance at the 0.05 and 0.01 levels, respectively.
This section presents our baseline methods (Section 3.2), fol-lowed by and experimental comparison (Section 3.3). We start out by introducing our experimental setup (Section 3.1).
We indexed all entities that have a label (i.e., a  X  X ame X ) but fil-tered out redirect pages. We considered the top 1000 most frequent predicates as fields; this was done to ensure that all fields occur in sufficiently many entity descriptions. Note that this number is two magnitudes larger than what was considered in prior work ( 6 in [17] and 11 at most in [12, 13]). We employ a heuristic to iden-tify title fields; following [16], attributes names ending in  X  X abel X ,  X  X ame, X  or  X  X itle X  are considered to hold title values. For each en-tity, we store a content field, collapsing all its predicates. We kept both relations (i.e., links pointing to other DBpedia pages) and re-solved relations (i.e., replacement of the link with the title of the page it points to) in our index.
We consider two sets of baseline methods. One is based on lan-guage modeling the other is based on BM25. This particular choice Model INEX-XER TREC Entity SemSearch ES SemSearch LS QALD-2 INEX-LD Total MLM-tc .1585 .2345 O .0855 O .1176 .3541 N .2838 N .1738 .1744 .0989 MLM-all .1589 .2273 .0641 .0882 .3010 .2454 .1514 .1581 .1204 .0593 .0857 PRMS .1897 M .2855 .1206 .1706 .3228 .2515 .1857 .2093 .1050 .0693 BM25F-tc .1720 H .2655 H .0848 .0882 .3337 M .2631 M .1718 .2163 .1067 is made because we consider both families of methods state-of-the-art that are frequently applied in the context of various entity search tasks, see, e.g., [5 X 7, 10, 15, 18]. Here, we confine ourselves to a basic approach where a (fielded) document-based representation is built for each entity. This representation makes limited use of entity-specific features, such as type information and related enti-ties; we leave these to future work.

Specifically, we use the following language modeling based meth-ods: LM : the standard query likelihood approach [23]; MLM-tc : the Mixture of Language Models [17], with two fields: title and content. Following [16] we set the title weight to 0 . 2 and the con-tent weight to 0 . 8 ; MLM-all : the Mixture of Language Models [17], where all fields are considered with equal weight; PRMS : the Probabilistic Retrieval Model for Semistructured Data. The differ-ence to MLM-all is that field weights are determined dynamically for each query term [13]. All methods use Dirichlet smoothing with the smoothing parameter set to the average (document or field) rep-resentation length.

We also use BM25 : with standard parameter settings ( k 1 b = 0 . 8 ) [20]; BM25F-tc : the fielded extension of BM25 [20], we consider title and content fields, the title weight is set to 0 . 2 and the content weight to 0 . 8 [16]; BM25F-all : all fields are considered with equal weight. We use the same b value for all fields in the fielded variant BM25F, analogous to [18].
Table 2 reports the results. We observe that the various query sets exhibit different levels of difficulty; this is indeed what we would have liked to achieve by considering different types of informa-tion needs. SemSearch ES queries (that look for particular entities by their name) are the easiest ones, while natural language queries (TREC Entity, QALD-2, and INEX-LD) represent the difficult end of the spectrum. List-type queries (INEX-XER and SemSearch LS) stand halfway in between, both in terms of query formulation (mixture of keyword and natural language) and retrieval difficulty. While a direct comparison of the scores to the official results of these benchmarks is not possible (due to the different collection used and/or that only a subset of the original queries is used here), based on manual inspection of a randomly selected subset, these results appear to be very reasonable.

When looking for significant differences in Table 2, we cannot find many. MLM-tc represents the only case when a significant im-provement is observed on the whole query set; the absolute score difference compared to LM, however, is less then 5 % and most likely it is a consequence of the improvements on a particular sub-set of queries (SemSearch ES). In all other cases, there is either no significant improvement or only a given subset of queries are significantly helped while another subset is significantly hurt.
In this section we perform a topic-level analysis in order to gain some insights into the differences between the various methods (or lack of thereof). Given the space limitations, we focus on (some of) the LM-based approaches; also, according to Table 2 these exhibit more differences than their BM25-based counterparts.

We compare the MLM-all (fielded language models, with equal field weights) to the baseline (single-field) LM method in Figure 1 and to a more sophisticated PRMS method (with query term-specific field weighting) in Figure 2. In both figures the X-axis represents individual query topics, ordered by AP differences (shown on the Y-axis). MLM-all is taken to be the baseline, that is, positive values indicate that the other method outperforms MLM-all and negative values mean the advantage of MLM-all on that particular topic.
First, we observe that a large number of topics is affected, esp. on the easier query subsets (Figures 1(a) X 1(d) and 2(a) X 2(d)). These improvements, however, do not add up; many of the topics that are improved by moving from LM to MLM-all are hurt when a transition from MLM-all to PRMS is made. When looking into the individual topics with little to no performance differences (i.e., the ones  X  X n the middle X  of Figures 1(e) X 1(f) and 2(e) X 2(f)) we find that both methods that are being considered do equally bad on these topics X  X n many cases they fail to find any relevant results.
In this paper we made several contributions to three main topics that were identified as important priorities for future research and development for the field of entity search [2]: (A1) getting more representative information needs and favouring long queries over short ones, (A2) limiting search to a smaller, fixed set of entity types (as opposed to arbitrary types of entities), and (A3) using test collections that integrate both structured and unstructured informa-tion about entities.

We developed and made publicly available a test collection based on DBpedia and synthesised queries from a number of previous benchmarking evaluation efforts, resulting in a set of nearly 500 queries and corresponding relevance judgments. To initiate further research, we provided baseline results and showed some of the lim-itations of existing methods based on language models and BM25. Additionally, we provided topic-level analysis and insights on how the choice of retrieval models is bound to the characteristics of dif-ferent query sub-sets.

The resources developed as part of this study are made avail-able at http://bit.ly/dbpedia-entity . It is our plan to maintain  X  X erified X  experimental results, a list of papers using this test collection, and pointers to additional related resources (e.g., source code) at the same website.
