 Topical classification of user queries is critical for general-purpose web search systems. It is also a challenging task, due to the sparsity of query terms and the lack of labeled queries. On the other hand, search contexts embedded in query sessions and unlabeled queries free on the web have not been fully utilized in most query classification systems. In this work, we leverage these information to improve query classification accuracy.

We first incorporate search contexts into our framework using a Conditional Random Field (CRF) model. Discrimi-native training of CRFs is favored over the traditional max-imum likelihood training because of its robustness to noise. We then adapt self-training with our model to exploit the information in unlabeled queries. By investigating different confidence measurements and model selection strategies, we effectively avoid the error-reinforcing nature of self-training. In extensive experiments on real search logs, we have av-eraged around 20% improvement in classification accuracy over other state-of-the-art baselines.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous; I.5.2 [ Pattern Recognition ]: Design Methodology  X  Classifier design and evaluation Algorithms, Experimentation, Measurement, Performance Query classification, User search context, Unlabeled queries  X  This work was done when Minmin Chen was an intern at Microsoft Research Asia.

Recent years have witnessed the booming of web search and its related services. Commercial web search engines receive hundreds of m illions of queries every day. Under-standing user intents behind queries is essential in develop-ing search engines. Query classification is a task studied for this purpose. In query classification, user queries are clas-sified into one (or more) predefined target categories. Such category information later can be used for refining web page rankings, triggering the most appropriate vertical searches, and finding matched online advertisements, etc..

Challenges associated with query classification include spar-sity of both query features and training data. First, user queries usually contain only several unique terms, thus it is difficult to derive a good feature representation. Previous works focused primarily on tackling this feature sparseness problem, e.g., by augmenting queries with external knowl-edge such as search engine results. Second, despite colossal volumes of unlabeled queries stored in search engine logs, manually labeled queries are very limited and expensive to obtain. Several works [1, 19] introduced semi-supervised learning methods to explore the information hidden in logs. However, most of the systems take a single query as a unit of search engine interaction, without considering search con-texts.

It has been argued forcefully that exploiting search con-texts can improve information retrieval systems [10, 13, 26]. Intuitively, introducing search contexts, such as the neigh-boring queries in the same query session, can help better understand users X  search intents and thus improve classifi-cation accuracy. For instance, when a user issues a query  X  X iant X , it is unclear whether he or she is interested in the San Francisco National League baseball team, the bicycle manufacturer, or something else. However, if we knew that the query  X  X LB X  (Major League Baseball) was issued in the same query session, we could have classified the query as  X  X ports X  rather than  X  X roducts X . Conversely, if the user is-sued queries related to other bicycle brands before the query  X  X iant X , it would have suggested the query was related to  X  X roducts X .

In this paper, we utilize search contexts embedded in query sessions, together with the large number of unlabeled queries available free on the web, to improve query classifica-tion. To achieve this goal, we need to address the following questions: 1) How to model search contexts? 2) How to incorporate the information in unlabeled queries? 3) How ( X - X  indicates that there is no url clicks associated with the query). to integrate these two components? 4) How to scale our algorithm to handle web-scale data?
First, we model search contexts using a Conditional Ran-dom Field (CRF) model. Second, we explore the informa-tion in unlabeled query sessions by combining them with a few manually labeled ones using self-training (bootstrap-ping). Third, to integrate these two components, we inves-tigate: 1) different training methods for CRFs; Discrimina-tive training is favored as it is more robust to labeling noise introduced by self-training. 2) different confidence measure-ments and model selection strategies for self-training to help the CRF model converge faster. We refer to the resulting framework as Adaptive Self-training with Conditional Ran-dom Field (ASCRF) . Forth, we adopt a softmax approxima-tion during the discriminative training of our CRF model, resulting in a formulation which can be efficiently computed and optimized by a slight variant of existing algorithms. As such, our solution can apply to web-scale data.

The rest of this paper is organized as follows. In Section 2, we introduce notations and problem settings. In Section 3, we briefly introduce the CRF model. We compare two commonly used CRF training methods, and apply an ap-proximation to tailor the training algorithm to deal with web-scale data. In Section 4, we detail the ASCRF frame-work. A series of experiments are carried out to demonstrate the effectiveness of our proposed framework in Section 5. In Section 6, we review related works. Finally, we conclude our work in Section 7.
Search contexts are usually embodied as interactions be-tween queries and related user behaviors within a query ses-sion. Various works [7, 24, 17, 14] proposed different meth-ods for identifying the boundaries of query sessions. In this work, we follow a simple yet effective session segmentation rule; that is, two user queries are separated into different ses-sions if they were issued longer than 30 minutes apart [11, 6].

Table 1 shows two query sessions extracted from real log, one labeled and the other one not. They both consist of a sequence of queries and related user behaviors. Due to space limitations, the urls clicked are not listed. We can make two observations: 1) search contexts can help clarify ambiguous queries; The query  X  X ichelin X  in Session 2 can either refer to the tire brand, or the Michelin guide book, or something else. However, considering the search context within the session can help correctly classify the query; 2) unlabeled query sessions can help relief the demand for a large number of labeled data. Intuitively, we can infer the category label of the first query in Session 2 based on its similarity to the queries in Session 1, and obtain the labels for the other queries based on context information. By do-ing so, we can expand the small labeled set with unlabeled queries to improve performance.

Motivated by these observations, we propose to build a semi-supervised context-aware query classification framework that takes advantage of both search contexts and unlabeled query sessions to improve query classification.

Let x = x 1 , x 2 ,  X  X  X  , x T denote a query session, where each observation x t ,t =1 ,  X  X  X  ,T includes a query q t issued by the user, and sometimes related user behaviors, such as clicked urls, url titles and user dwell time. For any query q (1  X  t  X  T ), the observations x 1 ,  X  X  X  , x t  X  1 are referred to as the search context of q t [6].
 sions, where each x ( n ) is a query session as defined, and y is its corresponding labeling sequence. Let U = { x ( m ) } M be a set of unlabeled query sessions. We have, N M . Our goal is to learn a classifier h ( x ; w ) based on both the labeled and unlabeled query sessions, so that given a new query q t , it can correctly classify q t into one of the prede-fined categories, taking into account the search context of q .
Inspired by Cao X  X  work [6], we model search contexts em-beddedineachquerysessionswithalinear-chainCRFmodel. Figure 1: A linear-chain CRF modeling search con-text.

Let x = x 1 , x 2 ,  X  X  X  , x T be a user query session, and y = y 1 ,y 2 ,  X  X  X  ,y T be its corresponding category labeling sequence. As shown in Figure 1, a CRF models the con-ditional distribution of the labeling sequence y ,giventhe observation sequence x , as [18] p ( y | x )= 1 where f = { f k } is a set of predefined feature functions, and Z ( x )= y e w f x ( y ) is a normalization function.
Given a set of trained parameters w  X  , a new query session x is classified as
In this work, we investigate two commonly used training methods for CRF models, i.e., maximum likelihood training vs. margin maximization training.
Given a set of i.i.d training data D = { ( x ( n ) , y ( n ) maximum likelihood training finds the parameters w = {  X  k such that the logarithm of the likelihood, is maximized.

The method has been widely used for CRF training be-cause both of its training and inference can be carried out efficiently using standard dynamic programming algorithms [25], making it scalable to large scale problems. However, as the approach explicitly considers only the probability of the exact training parses, it can easily overfit the training data without proper regularization [22, 9].
Margin maximization training of CRF models is a popular alternative. Given a set of training data D = { ( x ( n ) it finds a hyperplane that not only separates the training data, but also maximizes the score difference between the true label and the others, i.e. [27, 28], where  X  y ( n ) ( y )= I ( y ( n ) t = y t ) denotes the hamming dis-tance between y ( n ) and y .

To simplify the optimization, we replace the  X  X ard X  max margin in (2) with a softmax log exp approximation as proposed in [23, 12]. Let  X  ( w )=max we can now eliminate the slack variables in (2) and rewrite our objective function as follows: SMMCRF: min ( D ; w ,C )= 1 We refer to the resulting model as SoftMax Margin Con-ditional Random Field (SMMCRF). The objective is con-vex, and can be computed efficiently with a slight varia-tion of the standard forward-backward algorithm used in maximum likelihood training [23]. We present an analysis of the generalization performance of SMMCRF as follows. The bound provides a natural way to bridge SMMCRF with semi-supervised learning to utilize unlabeled data to improve query classification.
Given a set of i.i.d. training data D = { ( x , y ) } N , Vapnik [29] showed that with probability 1  X   X  , the expected risk where R emp ( w )= 1 |D| ( x , y )  X  X   X ( y ,h w ( x )) is the empiri-cal risk on the training set, and the VC-confidence  X ( N,d, X  ) is a term depending on the number of training examples N and the VC-dimension d of the hypothesis class H . Proposition 1. Let w  X  be the optimal solution to (3). Then 1 |D| n  X  n ( w  X  ) upper bounds the empirical risk. The proof can be easily generalized from Tsochantaridis X  X  proof for the generalization bound of structured SVM. We refer interested readers to [28].

It is known that the VC-dimension of margin-based clas-sifiers grows inversely with the size of the margin [5]. In other words, the margin control factor C in (3) controls the tradeoff between the training error and the complexity of the classifiers. As C  X  X  X  , the empirical risk goes to zero, at the cost of increasing the complexity of the hypothesis class. Contrarily, as C decreases, the size of the margin in-creases, resulting in a hypothesis class of lower complexity. However, the empirical risk would be increased. In the next section, we will detail how we effectively control C to help avoid the error-reinforcing nature [32] of self-training.
Now we have a model that incorporates search contexts, in this section, we will be focusing on how to use semi-supervised learning to explore the information contained in unlabeled query sessions to further improve query classifica-tion.
The general idea behind semi-supervised learning is to use a large amount of unlabeled data, together with a few labeled data, to build better classifiers. A variety of semi-supervised learning methods have been proposed [30, 4, 2, 16, 20, 15]. However, only a few of them work with graphical models, like CRFs. Self-training [30] is a very commonly used algorithm to wrap complex models for semi-supervised learning. It works as follows: 1. Train a base classifier using the small labeled set; 2. Infer the labels for the unlabeled data; 3. Add confident predictions to the training set; 4. Repeat the process.
 Note that the classifier uses its own predictions to teach itself. The procedure is therefore also called self-teaching or bootstrapping.
The advantage of self-training lies in its simplicity and the ability to combine with complex models. On the other hand, the drawbacks of this approach are: 1) erroneous la-bels are introduced into the training set; and 2) errors are reinforced at each iteration. In other words, wrong predic-tions are strengthened during the self-teaching procedure, especially when combined with complex models. Neverthe-less, the analysis of the generalization performance of the SMMCRF model in Section 3.1.3 provides heuristics on how to best integrate our model with self-training to avoid error-reinforcing.
In this section, we detail the adaptations we made to the self-training algorithm, based on the characteristics of the SMMCRF model. Algorithm 1 listed the details of the overall framework. We call our framework ASCRF, Adaptive Self-training with Conditional Random Field . We start a SMMCRF model trained on the small set of labeled data. A common practice to set the margin controlling factor C in (3) is through cross-validation. However, since manually labeled data is very limited in this case (in our experiments, only a few hundreds of the query sessions are labeled), it is not practical to do so. Structural risk minimization tells us that we get the smallest bound on the test error if we select a class of hypotheses H that minimizes the right hand side of (4). We start with a relatively large C ,sothatwecanenforcezeroempiricalrisk on the training set. As we gradually decrease C ,themargin of our classifier is increased, so that the second term of the bound (4) is decreased. As proved in Proposition 1,  X  ( w ) upper bounds the empirical risk. We stop when the bound exceeds 0.

We then label query sessions in the unlabeled set U with the trained base classifier. That is,  X  x  X  X  ,aprediction  X y = h ( x , w ) is assigned using the trained parameters w . Then, num , the number of most confident predictions ( x ,  X y ) are picked out and added to the labeled set D . The model is then retrained with the augmented labeled data set D X  X  . The process repeats until we can no longer find confident predictions to aid our learning. To further help our algo-rithm avoid stuck with wrong predictions in D ,weremove predictions that our model deems unconfident during this process.

To complete our algorithm, we investigated 1) different confidence measurements for selecting unlabeled queries to expand the training set; 2) different model selection strate-gies (margin control schemes) to balance the capacity of the SMMCRF model to fit the training data, and the complexity of the models.
First, margin maximization tr aining offers us a straight-forward margin-based confidence measurement: It measures the gap between the prediction  X y to any other sequence y in the output space. The larger the gap, the more confident our model is about this prediction. We select the predictions with the maximum c (1) w .

Second, as introduced in section 3, a CRF models the con-ditional distribution of a labeling sequence y given the ob-servation x . Given a trained model w , we can also measure the confidence in a prediction by the conditional probability, Similarly, the larger the conditional probability, the more our model favors the prediction  X y . We select the prediction with the maximum c (2) w
Third, the decision boundary of margin-based classifiers depends on the set of support vectors, i.e., the set of data points with Therefore, our third strategy is to select support vectors, i.e., the predictions that are closest to the decision boundary. Notice here, to avoid wrong predictions, only the predic-tions that are right on the margin or outside of the margin are considered. It is plausible to call c (3) w a confidence mea-surement since high c (3) w does not indicate that our model is confident about the correctness of the prediction. However, to be consistent with the rest of the notations, we still refer to it a confidence measurement.

By definition, the confidence measurement becomes less strict from c (1) to c (3) . With a simple transformation, we can rewrite c (1) as In other words, in order to obtain a high confidence c (1) prediction  X y has to maintain a dominant score even when the other parses y is helped by  X   X y ( y ). In c (2) , the hamming loss  X   X y ( y )) is dropped, resulting in a looser standard. c (3) further relax the require-ment by only constraining the predictions to lie on or outside of the decision boundary. Therefore, we would expect that the predictions selected using confidence measurement c (1) contain the least noise, and the amount of noise increases when using c (2) ,and c (3) is the worst. However, in terms of the usefulness of the new predictions to the convergence of the self-training process, the order is reversed. In [31], Zhang et al. showed that knowing the label of an unlabeled data point, on which the current model has low confidence, has more potential in improving the classification. Intu-itively, adding a prediction ( x ,  X y ) on which our model has already achieved a large margin over the other parses will not add any new constraints to our formulation in (2), since they are already satisfied by the current set of parameters w . Thus, using confidence measurement c (1) leads to slow convergence. In contrast, the unlabeled points lying right on the decision boundary have great influence on the change of
Input:
Parameters:
Output:
Initialize C to a relatively large number; repeat until n  X  n ( w ) &gt; 0; repeat  X  X  X  X  X   X y = h ( x ; w ); ( x ,  X y )from U and add them to the labeled set D ; D if w f x (  X y ) &lt; max y w f x ( y )+ X   X y ( y );  X  num (1  X  p (  X y | x )) /num ; until no more confident predictions can be found ; w . During our empirical study, we found that, as a trade-off between c (1) and c (3) , the confidence measurement c performs the best. It introduces less noise than c (3) ,andit helps the classifier achieve faster convergence than c (1) to retrain our model based on the set of augmented training data D X  X  . As self-training goes on, labeling noise would be introduced to the training set. In this case, instead of forcing the classifier to come up with a set of parameters w that perfectly classify the training data, we should increase the margin so that the mistakenly labeled training data can be ignored. As the margin increases, the second term of the generalization bound (4) can be reduced to ensure general-ization performance. However, blindly enlarging the margin will result in a model with very high bias, increasing both the empirical and expected risk. The tradeoff here is bal-anced through the margin control factor C . Often the case, cross-validation is used to tune hyperparameters. However, it is prohibitive to do in this setting, due to the sparseness of training data, as well as the relatively long running time of the entire self-training process.

Instead, we came up with an updating rule for the penalty factor C that depends on an estimated error rate of the training data. With a set of trained parameter w ,wecan estimate the error rate of the newly selected predictions by where p w (  X y | x ) computes the conditional probability of the true parse being  X y given the observation x under current model.

At each iteration, C is decreased by a factor proportional to the estimated error rate, that is,
To further avoid error-reinforcement, we also drop the in-stances that were added into D during the early stage of the self-training process, but later were found out to be un-confident ones. That is, a prediction ( x ,  X y )isremovedfrom the training set if  X  w ( x ,  X y )= w f x (  X y )  X  max This term can be efficiently computed with a slight variant of the standard Viterbi algorithm [21]. Note that our first confidence measurement c (1) upper bounds this term, that use c (1) directly is that when it becomes less than zero, the prediction ( x ,  X y ) is not necessarily within the margin, which might cause correct predictions being removed.
In this section, we validate our proposed methods through systematic empirical comparisons with previous work on two real datasets.
Both datasets were extracted from real search logs. Queries were segmented into different sessions according to the rule described in section 2.
The first dataset is from previous work [6]. It contains 3,500 query sessions (8,988 queries) extracted from one day X  X  search log of a commercial search engine. Three labelers were invited to label the queries of each session using the target taxonomy of ACM KDD Cup X 05, which is a two-level taxonomy with seven level-1 categories and 67 level-2 cate-gories.
To make our story complete, we also apply our algorithm to a relatively larger second dataset. The set was extracted and labeled by a product team in 2010. It contains 1,727 query sessions (39,565 queries) in total. Instead of the ACM KDD Cup X 05 taxonomy, the queries were labeled using a set of categories more closely related to users X  search intents, such as,  X  X ompare products, services, or activities for use X ,  X  X earn how to perform a task X , and  X  X lan travel X . There are 65 categories in total.
In our experiments, we used only previous queries in the same session as the search contexts since related user be-havior information was missing in Dataset 1. Different from classic sequential learning tasks in which the testing se-quences come in batch, queries come in streaming. Hence, the testing phase for query classification was done differ-ently. Given a sequence of queries q 1 ,q 2 ,...,q t ,wetakethe query q t as the test query, and the queries q 1 ,q 2 ,...,q the context of q t , and find the prediction of the session as in (1). Then the label of query q t is set to  X  y t =  X  y t .Wedothis for each time stamp t .

Given a set of queries q t with true label y t , and prediction  X  y , we report the accuracy of our trained classifier as where | T | is the total number of queries in the testing set. We used the smaller set, Dataset 1, for preliminary study. We first compared different training methods for our CRF model. We then investigate different confidence measure-ments and model selection strategies for our adaptive self-training framework. Finally we validate our framework on Dataset 2.
In the first experiment, we compare the performance of softmax margin maximization training (denoted as SMM-CRF) with maximum likelihood training (denoted as CRF), and maximum likelihood training with regularization w / 2  X  [22] (denoted as CRF-R) on Dataset 1.

Experiment Setup. We did 5-fold cross-validation. That is, the entire dataset was randomly split into five folds. Each time four of them were used as training, and the remaining one as testing. The average testing accuracy is reported. In this experiment, all of the training data are labeled, as opposed to the following experiments, in which only frac-tion of the training data are labeled. For this experiment, the margin control factor C in our loss function (3) and the Gaussian prior  X  2 of the regularized CRF [22] were both tuned on a validation set. Figure 2: Comparison of softmax margin maximiza-tion and maximum likelihood training on dataset 1
In order to test the robustness of the trained models to noise, we added random labeling noise to our training data. The labeling noise goes from 10% to 40% of the data. To deal with the increasing noise, the margin control factor C of SMMCRF is decreased by a constant factor each time, same for the Gaussian prior  X  2 of CRF-R.

Discussion. As shown in Figure 2, the model obtained with maximum likelihood training without regularization is seriously overfitting the training data. At point 0, that is, when the data contains no noise, SMMCRF achieved over 20% improvement over CRF in term of generalization per-formance. CRF-R did relieve the overfitting problem. How-ever, as more and more labeling noise is added into the dataset, we can clearly see the advantage of margin maxi-mization training. While the performance of CRF and CRF-R deteriorates very quickly as more noise is added, SMM-CRF turns out to be much more robust. We can see the per-formance gap between MMCRF and CRF-R/CRF becomes larger and larger as more noise is added to the training data from Figure 2.
Now that we have a model shown to be robust to label-ing noise both theoretically and empirically, in this part, we study the effects of different choices of confidence measure-ments and model selection strategies on the performance of our framework on Dataset 1.

Experiment Setup. As classical semi-supervised learn-ing setting, we used 80% of the data as training, and the remaining 20% as testing. 10% of the training data are ran-domly selected to be labeled, and the others are used as unlabeled data. We did 10 runs of these experiments, each time with a different labeled set. Since the size of the train-ing set is small, we used the level-1 categories of the ACM KDD Cup X 05 taxonomy, which contains seven categories in total, as the output.

Confidence measurements. In this experiment, we study the effects of various confidence measurements on the performance of our framework. Figure 3(a) shows the progress of ASCRF framework with the three confidence measure-ments we introduced in Section 4.2.2. Figure 3(b) shows the percentage of noise introduced into our training set when unlabeled query sessions are extracted and added to the set D using different confidence measurements.

The results are consistent with our discusion in Section 4.2.2. 1) In terms of noise rate (Figure 3(b)), c (1) w includes the least amount of noise into the training set, c (2) w the sec-ond, and c (3) w is the worst. Especially at the very beginning of the process, since our model is trained with a very small labeled set, it is seriously overfitting the training data, and does not generalize well to unseen data at that point. There-fore, the predictions close to the decision boundary under the current model are very likely to be wrong. 2) In terms of model convergence (Figure 3(a)), the few correct predic-tions selected by c (3) w significantly drive up the performance, since they are the data points the original model not sure of. Knowing the labels for these query sessions have great potential in improving the performance. On the other hand, although the predictions selected by c (1) w are of high accu-racy, since they are the data points the current model is very confident about, adding these data points do not help much. 3) In the long run, as the self-training procedure goes on, the wrong predictions selected by c (3) w start to hurt the learning process; while on the other hand, as the size of the margin increases, the highly accurate predictions selected by c w start to help the learning process. c (2) w , as a tradeoff be-tween these two, leads our model to the best generalization performance. In the following experiments, we used c (2) w our confidence measurement. Figure 3: Comparison of different confidence mea-surements: (a) Accuracy of Adaptive Self-training with different confidence measurements (b) Noise rate in D with different confidence measurements.
Model selection. In this experiment, we test different model selection strategies introduced in Section 4.2.3. We proposed to update the margin control factor C in (3) with an estimate of the noise rate in set D .Wecomparethis strategy with 1) a constant factor C ; 2) a constant update, Figure 4: Comparison of different model selection strategies. that is, C  X  C/ X  at each iteration of self-training. All ex-periments were carried out in the same setting, except with different updating rules for C . The confidence measurement c w is used.

For constant update, we tried  X  =1 . 05 and  X  =2. As shown in Figure 4, self-training with constant C is over-whelmed by the noise introduced, thus shows no progress. For constant update, a small  X  leads to slow convergence, while a large  X  results in a model of high bias. As shown in the magenta curve, when  X  = 2, the model can not find con-fident predictions within several iterations, and terminates very quickly. On the other hand, with the proposed updat-ing rule, we achieved over 10% improvement compared to a SMMCRF model trained with only the labeled instances shown as red solid line, and around 20% improvement over a CRF model trained with maximum likelihood training, shown as green dotted line in Figure 4. To make compari-son, we also trained a SMMCRF with all the training data labeled, it achieved 58 . 85% test accuracy (Notice here the number is higher than the one shown in Figure 2 because in this experiment we used only 7 level-1 categories as op-posed to 67 level-2 categories in previous one), while our framework achieved 51 . 38% precesion with only 10% of the training data labeled.
In this experiment, we test our overall framework on Dataset 2. The experiment setting is same as previous, 80% of the data are used as training set, and the remaining 20% as test-ing. We only used 5% of the training data as labeled ones, and the remaining 95% are unlabeled. Same here, we did 10 runs of this experiments.

Figure 5 shows the progress of our framework on Dataset 2, with proposed model selection strategy and confidence measurement c (2) w . We compare our ASCRF framework to a baseline, which trains a SMMCRF with the labeled query sessions. As shown in the figure, in Run 1, we improved over the baseline for around 20% in accuracy; and in Run 2, we nearly doubled the performance.

Table 2 reports the overall experimental results. The re-sults are consistent across the two datasets. We have aver-aged around 10% improvement on Dataset 1, and over 20% on Dataset 2. Our algorithm achieved more significant im-provement and smaller deviation on Dataset 2. We believe this is due to the size of the unlabeled set is larger in Dataset 2 than Dataset 1. Figure 5: Two runs of ASCRF on Dataset 2: 5(a) Run 1; 5(b) Run 2.
 Table 2: Overall performance comparison (Accuracy and Standard Deviation) of ASCRF vs. baseline on Dataset 1 and Dataset 2, with 10 runs each.

The main computational time of our algorithm comes from the training of SMMCRF. As stated in Section 3.1.2, using the softmax approximation, we can compute the objec-tive and gradient for our formulation efficiently with a slight variant of the forward-backward algorithm used in the stan-dard maximum likelihood training. Hence, the computa-tional time is linear on the sequence length, and quadratic on the number of categories. Our algorithm was implemented on top of the Limited Memory Variable Metric (LMVM) solver in the Toolkit for Advanced Optimization (TAO) [3]. The experiments were run on a machine with Intel(R) Xeon(R) CPU 2.67GHz, and 4G memory. It takes us around 2 min-utes to finish the entire adaptive self-training process for Dataset 1, with 7 level-1 categories, and 8,988 queries; and around 4 hours on Dataset 2, with 65 categories, and 39,565 queries. The time cost is acceptable for offline training. Also, the algorithm can run parallel to improve efficiency.
Web users often have very strict requirement on the re-sponse time of online applications. Fortunately, the testing phase of our algorithm can be done very efficiently using the standard Viterbi algorithm. During our experiment, it takes less than 1 second to finish the testing phase for Dataset 2, which contains around 7,000 test queries in total.
Cao et al. X  X  work [6] adopted CRF models to include con-text information in query classification. There are two ma-jor differences between our work and theirs: 1) The previous work was carried out under a supervised learning setting. In order to achieve satisfactory performance, a great number of labeling queries are required. In contrast, we propose to combine a small set of labeled instances with the abundant unlabeled queries free on the web to build a good query clas-sifier, reducing human effort in labeling query sessions. 2) The previous work used the traditional maximum likelihood training for CRF models. However, during our study, we found that discriminative training of CRF models is much more robust to labeling noise. Empirical study also sup-ports our claim, SMMCRFs consistently outperform CRFs trained with maximum likelihood training. It provides us not only with guaranteed generalization performance, but also a straightforward mechanism to adapt the popular semi-supervised learning method, self-training, to explore the in-formation contained in unlabeled queries.

The second class of related work is semi-supervised learn-ing [32, 8], which aims at utilizing the information in un-labeled data to improve supervised learning. This can be achieved in various ways [30, 4, 2, 16, 20, 15]. In 2005, Beitzel et al. [1] proposed a semi-supervised learning frame-work tailored specially for query classification. It uses la-beled data to learn logical relationships between query terms and query categories, and unlabeled data to fully exploit the  X  X yntactic X  dependencies between query terms, so that more queries can be classified. Li et al. [19] used click graphs to increase the amount of training data. Specifically, class memberships of unlabeled queries are inferred from those of labeled ones according to their proximities in a click graph. However, both works ignored the context information con-tained in query sessions.
Accurate topical classification of user queries can bene-fit a variety of web applications. A great amount of works have been proposed to study the problem. However, the issue of lacking training data to obtain a high quality clas-sifier has not been well addressed. Inspired by a previous work on context-aware query classification, we built a gen-eral framework that exploits the large number of unlabeled queries available free on the web, as well as search contexts embedded in query sessions to improve query classification. Extensive experiments on two datasets extracted from real search log demonstrated the efficacy of our framework.
In our experiments, we used only previous queries in the same session as search contexts. In future work, we plan to include related user behaviors, such as clicked urls, user dwell time, to enrich our feature set and investigate the im-pact of them. Li et al. [19] found that with a large amount of training data, classifiers using only query words/phrases as features can work remarkably well. It would be interesting to see if the statement still holds in our case. [1] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, [2] M. Belkin, I. Matveeva, and P. Niyogi. Regularization [3] S. Benson, L. McInnes, J. Mor  X  e, and J. Sarich. TAO [4] A. Blum and T. Mitchell. Combining labeled and [5] C. Burges. A tutorial on support vector machines for [6] H. Cao, D. Hu, D. Shen, D. Jiang, J. Sun, E. Chen, [7] L. Catledge and J. Pitkow. Characterizing browsing [8] O. Chapelle, B. Sch  X  olkopf, A. Zien, et al. [9] M. Chen, C. Y., M. Brent, and A. Tenney.
 [10] B. Croft et al. The role of context and adaptation in [11] H. Cui, J. Wen, J. Nie, and W. Ma. Probabilistic [12] K. Gimpel and N. Smith. Softmax-margin crfs: [13] A. Goker. Context learning in Okapi. Journal of [14] B. Jansen, A. Spink, C. Blakely, and S. Koshman. [15] F. Jiao, S. Wang, C. Lee, R. Greiner, and [16] T. Joachims. Learning to classify text using support [17] R. Jones and K. Klinkner. Beyond the session timeout: [18] J. Lafferty, A. McCallum, and F. Pereira. Conditional [19] X. Li, Y. Wang, and A. Acero. Learning query intent [20] G. Mann and A. McCallum. Simple, robust, scalable [21] N. Seshadri and C. Sundberg. List Viterbi decoding [22] F. Sha and F. Pereira. Shallow parsing with [23] F. Sha and L. Saul. Large margin hidden Markov [24] C. Silverstein, H. Marais, M. Henzinger, and [25] C. Sutton and A. McCallum. An Introduction to [26] S. Talja, H. Keso, and T. Pietil  X  ainen. The production [27] B. Taskar, C. Guestrin, and D. Koller. Max-margin [28] I. Tsochantaridis, T. Hofmann, T. Joachims, and [29] V. Vapnik and V. Vapnik. Statistical learning theory . [30] D. Yarowsky. Unsupervised word sense disambiguation [31] T. Zhang and F. Oles. A probability analysis on the [32] X. Zhu. Semi-supervised learning literature survey.
