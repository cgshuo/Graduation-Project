 We propose a method to rate the credibility of news articles using three clues: (1) commonality of the contents of articles among different news publishers; (2) numerical agreement versus contradiction of numerical values reported in the articles; and (3) objectivity based on subjective speculative phrases and news sources. We tested this method on news stories taken from seven different news sites on the Web. The average agreement between the system-produced  X  X redibility X  and the manual judgments of three human assessors on the 52 sample articles was 69.1%. The limitations of the current approach and future directions are discussed. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information Filtering, Selection Process Keywords: Web Document Credibility and Information Filtering When a user collects information from the Web, the information is not always correct. Many lies are placed on electronic bulletin boards or in blogs by malicious persons. In addition, online news articles may include incorrect information. To select credible information, Web users must filter out wrong information by themselves. The purpose of this study is to propose a method to rate the credibility of the information in news articles on the Web. 
Abdulla et al. [1] manually analyzed the credibility of online news. They found it was mainly measured by three dimensions: trustworthiness, currency and bias. Danielson [2] focused on the credibility of Web sites. In Google News [3], news items are ranked according to the reliability of the news publishers. Rubin et al. [4] proposed a four-dimensional analytical framework for certainty identification at the sentence level.  X  X ertainty X  is somehow related to  X  X redibility X , but was defined from the writer X  X  viewpoint, whereas  X  X redibility X  related to the reader X  X  judgment. We propose a method to rate the credibility of Web documents. We restrict our analysis to news stories on the Web. We defined three metrics to rate the credibility of news articles on the Web. The first two are combined into a  X  X redibility score X  and only the third assesses if an article is considered credible or not. 1. Commonality. The more news publishers delivered articles 2. Numerical Agreement. Numerical expressions such as  X 100 3. Objectivity. The credibility of articles containing subjective We defined the commonality among the contents of articles delivered from different news publishers to rate the credibility. To compute commonality, articles were divided into sentences. The heading of an article was treated as a sentence. We collected news articles published within an x -hour ( x = 2 was used in this paper) period and computed the cosine similarities between sentences in all the articles from different news publishers. The dimensionality of term vectors in sentences was reduced to one third of its original value using LSI. If sentence similarities exceeded a threshold, we defined the sentences as  X  X imilar X . 
If two articles from different news publishers contained similar sentences, we regarded those news articles as having higher commonality. To calculate the degree of commonality, a ratio that expressed the number of articles from different news publishers that contained similar contents was computed. We defined the trustworthiness of an article as the averaged similar content ratios for all sentences in the article. This was defined as: 
Here, n is the number of sentences in an article. S k is the number of news publishers with articles containing a sentence similar to the sentence k in the article. t is the number of all news publishers that published articles within the x -hour period. To rate the credibility, we also focused on the agreement of numerical expressions such as  X 100 passengers X  that appeared in the news articles. The combination of attributes related to numerical values like  X  X assengers X  and values were extracted using the Japanese syntax dependency analyzer CaboCha 1 . Then, the numerical expressions were compared among articles from different news publishers within an x -hour period. When numerical expressions agreed, we added a positive score. If the numerical expressions disagreed, we added a negative score. We defined the numerical agreement as: 
Here, i is the number of agreed numerical expressions, and c is the number of contradictory numerical expressions. We set the optimized weight of contradictions as 2 by changing parameters. 
When the added scores of (1) and (2) exceeded the threshold  X  (= 0.5), we categorized them as candidates for credible articles. For candidates for credible articles, the credibility score was rated using a list of speculative clue phrases and the indication of the news sources in the articles. We defined a list of speculative clue phrases with four grade scores 2 (see Table 1). All the sentences in the article were rated using the score of the speculative clue phrases they contained. The heading was also rated. When an interrogative expression appeared in the heading, the score of the heading was degraded, according to the context. (objective) [expressing-policy][guarantee-with] (somewhat speculative) speculative) [hope][possibility][predict][plan][aim][maybe] 
For the news source, we raised the objectivity score if news sources were given in the article. News sources were extracted using surface-level clues such as  X  X rom X  or  X  X ccording to X  in the sentences. The objectivity score was raised according to the news source types and their frequencies. We categorized news source types into four: (a) news agencies and publishers, (b) government agencies, (c) police and (d) TV/radio. The appearance of news agencies such as  X  X ssociated Press X  raised the objectivity higher than the appearance of  X  X V/radio X . For the experiment, 55,994 news stories on the Web (published from October to November, 2005) were collected every 30 minutes from seven news sites (four newspaper sites, one TV news site, one overseas news agency site, and one evening daily site; all stories were written in Japanese). From the collected articles, 52 articles were selected manually for human assessment. We manually categorized the 52 articles into 8 topic groups as shown in Table 2 and divided them as having high (4, 3) or low (2, 1) credibility scores calculated by our proposed method, then combined them into 26 pairs that were published in the same time period. We then asked three assessors to compare credibilities. The agreement rates between our proposed method and the assessments of the three human assessors are shown in Table 2. Agreement rates were defined in the following expression: Agreement rates = # of agreements / # of article pairs (3) Table 2. Agreement Rates between System and Assessors  X  X lood in China X  showed the lowest agreement rates b ecause the news documents were not published within x hours ( x = 2), and commonality/numerical agreement could not be taken into account. We have proposed a method to rate the credibility of news articles on the Web. In an experiment with three assessors, the average agreement between our proposed method and human assessments was 69.1%. Parameter tuning and using affirmation, such as positive/negative nuances among different news publishers, will be part of future work. 
As the proposed method uses commonality and agreement among news stories published prior to the target article, it successfully rates the credibility of  X  X rdinary X  reliable news as high and identifies unreliable news containing wrong information. However, it tends to rate a  X  X coop X  rather lower when the method is applied to developing news stories. Investigating such currency [1] in the online environment is also future work. Moreover, the proposed method may retrospectively identify reliable  X  X coops, X  identifying the first news that shows high commonality and agreement with the news articles published after it. This could be useful for purposes such as news site rating. [1] Abdulla, R. A., Garrison, B., Salwen, M., Driscoll, P., and Casey, D. [2] Danielson, D. R. Web Credibility. In Encyclopedia of Human [3] Ord, R. Google News Patent Application  X  Full Text [online], 2005 [4] Rubin, V. L., Liddy, E. D., and Kando, N. Certainty Identification in 
