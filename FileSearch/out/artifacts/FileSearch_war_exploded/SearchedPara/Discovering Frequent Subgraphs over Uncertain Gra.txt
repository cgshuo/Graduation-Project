 Frequent subgraph mining has been extensively studied on certain graph data. However, uncertainties are inherently accompanied with graph data in practice, and there is very few work on mining uncertain graph data. This paper inves-tigates frequent subgraph mining on uncertain graphs un-der probabilistic semantics. Specifically, a measure called  X  -frequent probability is introduced to evaluate the degree of recurrence of subgraphs. Given a set of uncertain graphs and two numbers 0 &lt; X , X  &lt; 1, the goal is to quickly find all subgraphs with  X  -frequent probability at least  X  .Due to the NP-hardness of the problem, an approximate mining algorithm is proposed for this problem. Let 0 &lt; X &lt; 1bea parameter. The algorithm guarantees to find any frequent subgraph S with probability at least 1  X   X  2 s ,where s is the number of edges of S . In addition, it is thoroughly discussed how to set  X  to guarantee the overall approximation quality of the algorithm. The extensive experiments on real uncer-tain graph data verify that the algorithm is efficient and that the mining results have very high quality.
 H.2.8 [ Database Applications ]: [data mining] Algorithms, Performance, Theory Uncertain graph, frequent subgraph, probabilistic semantics
Graphs are general data structures for representing com-plicated relationships between objects, which have seen wide applications in bioinformatics, social networks and spatial databases, etc. Large amount of data represented by graphs, also known as graph data , call for intelligent tools to analyze and understand them. Frequent subgraph mining [9] is one of the powerful tools to study the structures of graph data, especially, the recurring substructures. More precisely, the mining task is formulated as follows. Given a set D of graphs and a number 0 &lt; X &lt; 1, find all subgraphs that occur in at least  X   X | D | graphs in D . The percentage of graphs in that contain a subgraph S is called the support of S .
Recent researches [4,13,14] have shown that uncertainties are inherent in graph data, in particular, the structures of graphs are uncertain. In the data uncertainty models used in [4, 13], each edge of a graph is associated with a number indicating the probability of the edge existing in reality, and the existence of edges is mutually independent. Such graph is called an uncertain graph [13]. An uncertain graph es-sentially represents a probability distribution over all of the certain graphs in the forms of which the uncertain graph may actually exist. Each of these certain graphs is called an implicated graph [13].

Zou et al. [13] studied frequent subgraph mining on uncer-tain graph data. A set of uncertain graphs D = { G 1 ,G 2 G n } represents a probability distribution over a family F of certain graph sets. Each set of certain graphs { G 1 ,G 2 ,...,G n } X  X  satisfies that G i  X  D is an implicated graph of G i  X  D for 1  X  i  X  n . For a certain subgraph S the degree of recurrence of S in D is measured by the ex-pected value of the supports of S in all certain graph sets in
F , called the expected support of S .Thus,thefrequent subgraph mining problem on uncertain graph data is defined in [13] as follows. Given a set D of uncertain graphs and a number 0 &lt; X &lt; 1, find all subgraphs with expected support at least  X  .

Motivated by some recent work [2,11] on frequent itemset mining on probabilistic data, this paper investigates frequent subgraph mining on uncertain graph data based on seman-tics different from that adopted by [13]. Again, a set D uncertain graphs represents a probability distribution over a family F of certain graph sets. However, the degree of recurrence of a ce rtain subgraph S in D is measured by the probability that S has support at least  X  across all certain graph sets in F . Such probability is called  X  -frequent prob-ability in this paper. Therefore, the problem to be solved in this paper can be defined as follows. Given a set D of uncertain graphs and two numbers 0 &lt; X , X  &lt; 1, find all subgraphs with  X  -frequent probability at least  X  .
Though the difference between the two semantics for min-ing uncertain data has been argued in [11] from the aspect of mining results, there is a lack of explanation of the differ-ence from the angle of application areas. In short, frequent subgraph mining under expected semantics [13] is more suit-able for investigating structural patterns in uncertain graph data, which behaves more like an exploratory tool; whereas, frequent subgraph mining under probab ilistic semantics in this paper is more suitable for extracting features from un-certain graph data. Supposing a subgraph having support at least  X  can be a feature in a set of certain graphs, then  X  -frequent probability perfectly captures the probability of a subgraph being a feature in a set of uncertain graphs, but expected support does not. On the contrary, expected support characterizes the expected degree of recurrence of a subgraph, but  X  -frequent probability does not.

Although the problem to be solved in this paper follows the same semantics as the frequent itemset mining problems in [2, 11], the algorithms presented in [2, 11] can not be ex-tended to our problem. This is because it can be decided in polynomial time whether an itemset is frequent or not [2,11]; however, it is #P-hard to compute the  X  -frequent probabil-ity of a subgraph as will be proved later in this paper. The problem is also proved to be a NP-hard problem. Hence, instead of discovering all strictly frequent subgraphs, we find an  X  -approximate set of frequent subgraphs including all frequent subgraphs and a fraction of infrequent subgraphs but with  X  -frequent probability at least  X   X   X  ,where0 &lt; X &lt;  X  is a small error tolerance. I n other words, all subgraphs with  X  -frequent probability at least  X  must be output, but all subgraphs with  X  -frequent probability less than  X   X   X  need not to be output.

This paper proposes an approximate mining algorithm to find such an  X  -approximate set of frequent subgraphs. Let 0 &lt; X &lt; 1 be a pre-specified parameter. The algorithm guar-antees to discover any frequent subgraph S with probability at least 1  X   X  2 s ,where s is the number of edges of S .
The algorithm is developed based on the well-known gSpan algorithm [9]. First, all subgraphs are encoded into their minimum DFS codes and are organized into a search tree ac-cording to the lexicographic order of minimum DFS codes [9]. Then, the search tree is traversed in depth-first order to find an  X  -approximate set of frequent subgraphs. In particular, for each visited subgraph S , instead of computing the exact  X  -frequent probability of S , it is determined whether the frequent probability of S is certainly not less than  X   X   X  is probably greaterthanorequalto  X  using an approxima-tion algorithm. The approximation algorithm is very simple and fails with probability at most  X  . If the answer from the approximation algorithm is  X  X es X , then output S and con-tinue the search, otherwise stop searching the descendants of
S since the  X  -frequent probability of any supergraph of S is definitely no greater than  X  .

The theoretical analysis shows that to obtain any frequent subgraph with probability at least 1  X   X ,  X  should be at most 1  X  2  X  (1  X   X ) 1 / max ,where max is the maximum number of edges of frequent subgraphs. In addition, extensive ex-periments were performed on real uncertain graph data to evaluate the practical performance and the approximation quality of the proposed algorithm. The experimental results verify that the algorithm is very efficient and accurate.
The main contributions of this paper are as follows.
The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 introduces the model of uncertain graph data and defines the frequent subgraph min-ing problem. Section 4 formally proves the inherent compu-tational complexity of the problem. Section 5 proposes the mining algorithm and analyzes the performance guarantees of the algorithm. Section 6 points out how to set parameters to ensure the overall approximation quality of the algorithm. Section 7 shows the experimental results on real uncertain graph data. Finally, Section 8 concludes this paper.
A number of algorithms have been proposed for mining frequent subgraphs on certain graph data, which have been surveyed in [1]. All these algorithms can not handle uncer-tainties inherent in graph data. However, some state-of-the-art techniques adopted by the algorithms such as minimum DFS codes [9] for representing subgraphs and the right-most extension technique [9] for extending subgraphs can also be used in our work because knowledge to be discovered in this paper is actually certain subgraphs.

Kimmig and Raedt [6] cast pattern mining problems in the context of logic programming ,particularlyin ProbLog ,a probabilistic Prolog system. Due to the powerful expression capability, ProbLog can represent uncertainties of itemsets, sequences, trees or graphs. Their appealing method is an in-tegration of multi-relational data mining and inductive logic programming . However, due to the data representation in ProbLog, operations on graphs such as subgraph isomor-phism testings are implemented by clause reductions ,which become inefficient on large-sized graphs.

Zou et al. [13] studied frequent subgraph mining on un-certain graph data independently. They proposed expected support to evaluate the significance of subgraphs. In partic-ular, the expected support of a subgraph S is the expected value of the supports of S in all implicated graph databases. It differs from  X  -frequent probability proposed in this paper in the following way. Even if the support of S can be less than  X  in a specific implicated graph database, it still con-tributes to the expected support of S but doesn X  X  contribute to the  X  -frequent probability of S .

Although the problem in this paper follows the same se-mantics as the frequent itemset mining problems in [2, 11], the algorithms proposed in [2,11] can not be extended to our problem. Except for the difference in data type, another im-portant reason is that it can be decided in polynomial time whether an itemset is frequent or not; however, it is #P-hard to compute the  X  -frequent probability of a subgraph.
Let us first extend the model of uncertain graphs recently proposed in [13] to the following one that considers uncer-tainties of both vertices and edges.
 Definition 1. An uncertain graph is a system G =( V, E,  X  ,L V ,L E ,P V ,P E ), where ( V, E ) is an undirected graph, L V : V  X   X  is a function assigning labels to the vertices, L
E : E  X   X  is a function assigning labels to the edges, P V V  X  [0 , 1] is a function assigning existence probability values to the vertices, and P E : E  X  [0 , 1] is a function assigning conditional existence probability values to the edges given their endpoints.

The existence probability, P V ( v ), of a vertex v  X  V is the probability of v existing in practice. The conditional exis-tence probability, P E ( e | u, v ), of an edge e =( u, v probability of e existing between vertices u and v on the con-dition that both u and v exist in practice. Thus, a labeled graph in traditional graph mining [9], which is called certain graph in this paper, is a special uncertain graph with exis-tence probability values of 1 on all vertices and conditional existence probability values of 1 on all edges.

Similar to the statement in [13], an uncertain graph es-sentially represents a set of certain graphs implicated by it, each of which is a possible structure in the form of which the uncertain graph might be present in practice. Definition 2. An uncertain graph G =( V, E,  X  ,L V ,L E , P V ,P E ) implicates a certain graph G =( V ,E ,  X  ,L V ,L denoted by G  X  G ,if V  X  V , E  X  E  X  ( V  X  V ),  X   X   X , L
V = L V | V ,and L E = L E | E ,where E  X  ( V  X  V )isthe set of edges with both endpoints in V ,and L | X denotes the function obtained by restricting function L to domain X .
This paper assumes that the existence probabilities of ver-tices and the conditional existence probabilities of edges of an uncertain graph are mutually independent , respectively. Based on this assumption, the probability of an uncertain graph G =( V, E,  X  ,L V ,L E ,P V ,P E ) implicating a certain graph G =( V ,E ,  X  ,L V ,L E )is
Pr( G  X  G )= where E  X  ( V  X  V ) is the set of edges with both endpoints in V .

Let Imp ( G ) denote the set of all implicated graphs of an uncertain graph G .Itiseasytoverifythat by counting the number of implicated graphs of a fully-connected uncertain graph. Due to arguments similar to the proof of Theorem 1 in [15], we have the following theorem.
Theorem 1. For an uncertain graph G , the function given in Equation 1 defines a probability distribution over Imp
Let us proceed to extend the model of uncertain graph databases proposed in [13] to the following one that takes trivial implicated graphs into account.
 An uncertain graph database is a set of uncertain graphs. It essentially represents a set of implicated graph databases .
Definition 3. An uncertain graph database D = { G 1 ,G 2 , ...,G n } implicates a certain graph database D = { G 1 ,G ...,G m } if m  X  n , and there is an injection  X  : { 1 , 2  X  X  1 , 2 ,...,n } such that G  X  ( i )  X  G i for 1  X  i  X  m
Note that, in Definition 3, we have m  X  n because every uncertain graph in D generally has a non-zero probability to implicate a trivial certain graph  X  , i.e., a certain graph with no vertices and no edges. Since trivial graphs do not contain useful knowledge, they should be eliminated from implicated graph databases. This is the main difference from the model of uncertain graph databases proposed in [13].

This paper assumes that the uncertain graphs in an uncer-tain graph database are mutually independent . Based on this assumption, the probability of an uncertain graph database D = { G 1 ,G 2 ,...,G n } implicating a certain graph database D = { G 1 ,G 2 ,...,G m } is
Pr( D  X  D )= where  X  is an injection from { 1 , 2 ,...,m } to { 1 , 2 ,...,n } such that G  X  ( i )  X  G i for 1  X  i  X  m ,and  X  denotes a trivial graph.

Let Imp ( D ) denote the set of all implicated graph data-bases of an uncertain graph database D . Based on the in-dependence assumption, it is easy to know that | Imp ( D )
G  X  D | Imp ( G ) | . Using the arguments in the proof of The-orem 2 in [15], we can prove the theorem below.

Theorem 2. For an uncertain graph database D ,thefunc-tion given in Equation 2 defines a probability distribution over Imp ( D ) .
Definition 4. A certain graph G =( V, E,  X  ,L V ,L E )is subgraph isomorphic to another certain graph G =( V ,E ,  X  ,L V ,L E ), denoted by G G , if there exists an injection f : V  X  V such that (1) L V ( v )= L V ( f ( v )) for any v  X  V , (2) ( f ( u ) ,f ( v ))  X  E for any ( u, v )  X  E , (3) L E (( u, v )) = L E (( f ( u ) ,f ( v ))) for any ( u, v Injection f is called a subgraph isomorphism from G to G . Probabilistic frequent subgraphs. As in traditional fre-quent subgraph mining, this paper also considers mining connected subgraphs. If not otherwise specified, a subgraph refers to a connected subgraph in the rest of the paper.
Let D be an uncertain graph database, Imp ( D )betheset of all implicated graph databases of D ,and S be a certain subgraph. Because each implicated graph database D  X  Imp ( D ) is actually a certain graph database, the traditional definition of support [7] applies to D , that is, the support of
S in D is Thus, the probability that the support of S is no less than 0 &lt; X &lt; 1 across all implicated graph databases of D is where Pr( D  X  D ) is the probability of D implicating D as given in Equation 2. In the rest of the paper, the probability given in Equation 4 is called the  X  -frequent probability of D .When D and  X  are explicit from the context, Pr( S ; D,  X  can be simply written as Pr( S ). A subgraph S is called (  X ,  X  )-probabilistic frequent if the  X  -frequent probability of S is no less than a user-specified confidence threshold 0 &lt;  X &lt; 1. When  X  and  X  are clear from the context, S can be simply called a frequent subgraph.
 Problem statement. The problem of mining frequent sub-graphs in an uncertain graph database under probabilistic semantics can be formalized as follows.
 Input: an uncertain graph database D , a support threshold Output: all (  X ,  X  )-probabilistic frequent subgraphs in D
This section formally proves the inherent computational complexity of the problem of mining frequent subgraphs in an uncertain graph database. The following proofs use the complexity class #P for enumeration problems [8].
Theorem 3. It is #P-hard to compute the  X  -frequent prob-ability of a subgraph S in an uncertain graph database D .
Proof. We prove the theorem by reducing an arbitrary instance of the #P-complete problem of counting the num-ber of assignments satisfying a monotone k -DNF formula F [8] to an instance of the current problem in polynomial time. Here, let F = C 1  X  C 2  X  X  X  X  X  C m contain m clauses over n variables x 1 ,x 2 ,...,x n .Eachclause C i is in the form of l 1  X  l 2  X  X  X  X  X  l k ,where k is a constant, and each literal is an un-negated variable in { x 1 ,x 2 ,...,x n } . Without loss of generality, we assume that a variable occurs in a clause at most once. The reduction is carried out as follows.
Firstly, construct an uncertain graph database D = { G } , where G is a bipartite uncertain graph. The vertex set of U  X  V ,where U = { c 1 ,c 2 ,...,c m } and V = { v 1 ,v 2 All of the vertices in U are labeled by  X  and have existence probabilities of 1, and all of the vertices in V are labeled by  X  and have existence probabilities of 1 / 2. There is an edge between vertices c i and v j if and only if clause C i contains variable x j . All of the edges of G are labeled by  X  and have conditional existence probabilities of 1.

Secondly, create a certain subgraph S .Thevertexsetof S is { c, v 1 ,v 2 ,...,v n } ,where c is labeled by  X  ,and v 1 v are labeled by  X  . There is an edge labeled by  X  connecting vertex c with each of v 1 ,v 2 ,...,v n .
 Thirdly, let  X  =1.

The correspondence between the number of assignments satisfying F and the  X  -frequent probability, Pr( S ; D,  X  S in D can be established due to the following arguments. (1) A truth assignment  X  1-to-1 corresponds to an impli-(2) A truth assignment  X  satisfies F if and only if S has Subsequently, the number of truth assignments satisfying F is 2 n  X  Pr( S ; D,  X  ). The reduction is completed.
Theorem 4. It is #P-hard to count the number of fre-quent subgraphs in an uncertain graph database.

Proof. An instance of the #P-hard problem of counting the number of frequent subgraphs with support at least  X  in a certain graph database D [10] can be trivially reduced to an instance of the current problem by specifying the input uncertain graph database to be D , the support threshold to be  X  and the confidence threshold  X  =1. Thisisbecause D is a special uncertain graph database, and a subgraph has support at least  X  in D if and only if its  X  -frequent probability in D is 1. Thus, the theorem holds.
Due to the hardness results proved in Section 4, it can not be expected to discover all strictly frequent subgraphs in polynomial time unless P = NP. Instead, an approximate mining algorithm is proposed to find a broader set of sub-graphs including all frequent subgraphs and a fraction of in-frequent subgraphs but with  X  -frequent probability at least  X   X   X  ,where0 &lt; X &lt; X  is an error tolerance. In other words, all subgraphs with  X  -frequent probability at least  X  must be output, but all subgraphs with  X  -frequent probability less than  X   X   X  need not to be output.

It is not meant to discover all subgraphs with  X  -frequent probability at least  X   X   X  ;orotherwise,theproblemwould become even harder. Suppose a subgraph S has  X  -frequent probability at least  X   X   X  . If it is definitely known that the  X  -frequent probability of S is less than  X  ,then S need not to appear in the mining results.
The approximate mining algorithm takes as input an un-certain graph database D , a support threshold  X  , a confi-dence threshold  X  , and an error tolerance  X  . The procedure of the algorithm can be briefly outlined as follows. (S1) First, organize all subgraphs in D into a search tree , (S2) Then, examine the subgraphs in the search tree in
Lemma 1 (Apriori property). For two subgraphs S and S ,if S S ,then Pr( S )  X  Pr( S ) .

Proof. The lemma can be readily proved from Equa-tion 4 and the Apriori property of support [7].

The algorithm terminates while no subgraphs are left un-examined. Next, we clarify the details of each step. Building search tree of subgraphs. Because subgraphs are actually certain graphs, all subgraphs in D can be en-coded into minimum DFS codes , the state-of-the-art canon-ical graph coding scheme developed for frequent subgraph mining [9]. Informally speaking, the minimum DFS code of a subgraph is a sequence that is prior to all other sequen-tial representations of the isomorphic subgraphs according to the lexicographic order of DFS codes. For more details on minimum DFS codes, please refer to [9].
 Then, the search tree of subgraphs can be constructed. The nodes are all subgraphs in D . The root is the trivial subgraph  X  . The parent parent ( S ) of each subgraph S (  X  ) satisfies that the minimum DFS code of parent ( S )isthe longest prefix of that of S .
 Depth-first sear ch on search tree. Note that we do not materialize the search tree in memory but enumerate sub-graphs in the search tree in dep th-first order. Particularly, for each subgraph S , all its children are generated as follows. First, perform right-most extension [9] to S , obtaining a set of subgraphs, each of which contains S and has one more edge than S . Then, for each right-most extended subgraph S , if the minimum DFS code of S is a prefix of that of S , then S is a child of S ,otherwise S is not a child of S . Please refer to [9] for more details on right-most extension. Verifying visited subgraphs. For each subgraph S vis-ited in the search, it need to be verified whether S can be output as a result or not using the following method. First, approximate the  X  -frequent probability, Pr( S ), of S by an in-terval [ p l ,p u ] such that Pr( S )  X  [ p l ,p u ]andthat Then, apply the following rules. (R1) If p l  X   X   X   X  and p u  X   X  , then it is certain that Pr( (R2) If p u &lt; X  ,thenPr( S ) &lt; X  ,thus S is not frequent.
Section 5.2 presents the details of the algorithm for ap-proximating Pr( S )bysuchinterval[ p l ,p u ].
This subsection proposes an algorithm for approximating the  X  -frequent probability, Pr( S ), of a subgraph S by an in-terval [ p l ,p u ] such that | p u  X  p l | X   X  and Pr( S ) 0 &lt; X &lt; 1 be a parameter. The algorithm fails to produce such an interval with probability at most  X  . This subsection first develops a dynamic programming-based method to ex-actly compute Pr( S ) in exponential time and then extends it to produce desired interval [ p l ,p u ] with probability at least 1  X   X  in polynomial time. Let us begin with a concept originally introduced in [13]. We say  X  X  subgraph S occurs in an uncertain graph G  X  X f S is subgraph isomorphic to at least one implicated graph of G . Naturally, the probability of S occurring in G is where Imp ( G ) is the set of all implicated graphs of G ,and Pr( G  X  G ) is the probability of G implicating G as given in Equation 1.

With the concept given above, the  X  -frequent probability of a subgraph S can be exactly computed via dynamic pro-gramming. Suppose the uncertain graphs in D are indexed from 1 to n , i.e., D = { G 1 ,G 2 ,...,G n } .Let T [0 ..n, be a three-dimensional array with subscript ranging from 0 to n in each dimension. Each element T [ i, j, k ]of T stores the probability that, across all implicated graph databases of { G 1 ,G 2 ,...,G k } X  D , (1) the number of certain graphs in the implicated graph (2) subgraph S is subgraph isomorphic to i certain graphs
All elements of T can be computed using the following recursive equation. Basically, T [0 , 0 , 0] = 1, and T [ 0if i + j&gt;k . For other cases, T [ i, j, k ] can be computed by Equation 6, where Pr( G k  X  X  X  ) is the probability of G implicating a trivial certain graph  X  ,andPr( S U G k )is the probability of S occurring in G k as given in Equation 5.
It is apparent that the  X  -frequent probability of S can be computed from T by Supposing that the exact values of Pr( G i  X  X  X  )andPr( S U G )for1  X  i  X  n are given as input, the procedure of the dynamic programming is presented in Figure 2. Obviously, the time complexity of the DP procedure is  X ( n 3 ). Procedure DP 1. create array T [0 ..n, 0 ..n, 0 ..n ] 2. T [0 , 0 , 0]  X  1 3. for k  X  1to n do 4. for j  X  0to k do 5. for i  X  0to k  X  j do 6. compute T [ i, j, k ] using Equation 6 7. compute Pr( S ) using Equation 7 8. return Pr( S ) Figure 2: The dynamic programming procedure DP .

Remark 1. A similar dynamic programming method has previously been adopted by [11] to find frequent items over probabilistic data, in which the counterpart of Pr( S U G G k )  X  T [ i  X  1 ,j,k  X  1] if i&gt; 0and j =0 ,
G ))  X  T [ i, j  X  1 ,k  X  1] if i =0and j&gt; 0 ,
G k )  X  T [ i  X  1 ,j,k  X  1]
G ))  X  T [ i, j  X  1 ,k  X  1] if i&gt; 0and j&gt; 0 .
T [ i, j, k ] ,where 0  X  i + j  X  k and 1  X  k  X  n . in the recursive equation is the probability that an item is contained in an x-tuple. Actually, the probability of an item being contained in an x-tuple can be evaluated in polynomial time. Unfortunately, the complexity result is negative for computing Pr( S U G ) , which is shown in the following important theorem.
 Theorem 5. It is #P-hard to compute the probability, Pr( S U G ) , of a subgraph S occurring in an uncertain graph G .

Proof. The proof is very similar to the proof of Theo-rem 1 in [13], so it is omitted here.
 Although the time complexity of the DP procedure is O ( n 3 ), it is computationally prohibitive to evaluate the re-quired input of the DP procedure.
The dynamic programming-based method is extended to an approximation algorithm that produce an interval [ p l in polynomial time such that | p u  X  p l | X   X  and that Pr( [ p ,p u ] with probability at least 1  X   X  ,where0 &lt; X &lt; 1.
To this end, an algorithm for estimating Pr( S U G )with high accuracy is proposed and then is combined with the dynamic programming procedure DP to get the approxima-tion algorithm for computing [ p l ,p u ]. We clarify the details of each step as follows.
 Estimation of Pr( S U G ) . The fundamental idea of the algorithm for estimating Pr( S U G ) is to transform the problem of computing Pr( S U G ) to the problem of com-puting the probability of a DNF formula F being satisfied by a randomly and independently chosen truth assignment to the variables of F .

Given an uncertain graph G and a subgraph S ,letus construct a DNF formula F first. Let G denote the cer-tain graph obtained by removing uncertainties from G .As a side product of enumerating the children of the parent of S in the search tree, all subgraph isomorphisms from S to G have been obtained. Based on all these subgraph isomor-phisms, all embeddings , M 1 ,M 2 ,...,M m ,of S in G can be obtained. Each embedding M i is a subgraph of G that S is subgraph isomorphic to. The DNF formula F therefore can be constructed as follows.
 Step 1. Assign a distinct variable to each vertex contained Step 2. Assign a distinct variable to each edge contained in Step 3. For each embedding M i ,constructaclause C i by Step 4. Build F = C 1  X  C 2  X  X  X  X  X  C m .

Let Pr( F ) denote the probability of F being satisfied by a randomly and independently chosen truth assignment to the variables of F . It is easy to prove the following lemma. Lemma 2. Pr( S U G )=Pr( F ) .

The Monte-Carlo algorithm proposed in [5] can be applied to estimate Pr( F ) within absolute error  X  with probability at least 1  X   X  for arbitrary 0 &lt; X , X &lt; 1.
 Procedure Estimate 1. for all vertex v  X  V ( M 1 )  X  V ( M 2 )  X  X  X  X  X  V ( M m 2. assign variable x v to v 3. for all edge e  X  E ( M 1 )  X  E ( M 2 )  X  X  X  X  X  E ( M m ) do 4. assign variable x e to e 5. for i  X  1to m do 7. F  X  C 1  X  C 2  X  X  X  X  X  C m 8. estimate Pr( F ) using the Monte-Carlo algorithm [5] 9. return the estimated value of Pr( F ) Figure 3: The algorithm for estimating Pr( S U G ) .
Figure 3 shows the procedure Estimate of the algorithm for estimating Pr( S U G ). Let s be the number of edges of
S .Since | V ( M i ) | X | E ( M i ) | = s for all 1  X  i  X  n DNF formula F can be constructed in O ( ms ) time. Since F consists of m clauses and each clause contains O ( s )vari-ables, the Monte-Carlo algorithm completes in O m 2 s  X  2 time [5]. Thus, the time complexity of the Estimate proce-dure is O m 2 s  X  2 log 2  X  . Wehavethefollowingtheorem.
Theorem 6. For any 0 &lt; X , X &lt; 1 , Pr( S U G ) can be estimated by Pr( S U G ) such that in and m is the number of embeddings of S in the certain graph G obtained by removing uncertainties from G . Approximation algorithm. Based on Theorem 6, an ap-proximation algorithm for computing [ p l ,p r ]canbedevel-oped. Given an uncertain graph database D = { G 1 ,G 2 ,..., G n } , a subgraph S , an error tolerance 0 &lt; X &lt; 1andafail-ure probability tolerance 0 &lt; X &lt; 1, the algorithm works in a very simple and elegant way as follows.
 Step 1. Compute Pr( G i  X  X  X  )byEquation1for1  X  i  X  n . Step 2. Estimate Pr( S U G i )by Pr( S U G i )within Step 3. Call the dynamic programming procedure DP with Step 4. Return [ p l ,p u ]=[ Pr( S )  X   X / 2 , Pr( S )+  X /
Since the value of Pr( G i  X  X  X  ) does not depend on the specific input subgraph S ,thevaluesofPr( G i  X  X  X  ) for all 1  X  i  X  n can be computed just at the beginning of the mining algorithm and are reused in all subsequent calls of this approximation algorithm for all input subgraphs. The procedure Approximate of this approximation algorithm is presented in Figure 4.
 Procedure Approximate 1. for i  X  1to n do 2. Pr( S U G i )  X  the estimated value of Pr( S U G i ) 3. Pr( S )  X  the output of the DP procedure using Pr( G i 4. return [ Pr( S )  X   X / 2 , Pr( S )+  X / 2] Figure 4: The algorithm for approximating Pr( S ) . Thetimecomplexityofthe Approximate procedure is where s is the number of edges of S ,and m is the maximum number of embeddings of S in the uncertain graphs in D .
The expression of the time complexity can be simplified as follows. Let f ( x )=1  X  (1  X  x ) 1 /n be a real function, where 0  X  x  X  1and n  X  N . By Taylor X  X  expansion at x =0, where 0  X   X   X  x . The Lagrange remainder is nonnegative, so f ( x )  X  x/n .At x =  X  ,wehave1  X  (1  X   X  ) 1 /n  X   X /n . Therefore, the time complexity of the Approximate proce-dure can be simplified to
It is obvious that the output [ p l ,p u ]ofthe Approximate procedure satisfies | p r  X  p l | X   X  . The rest of this subsection proves that the Approximate procedure fails with proba-bility at most  X  .

For ease of proof, let DP ( D, i ) denote the output of the dynamic programming procedure DP over uncertain graph database D = { G 1 ,G 2 ,...,G n } with Pr( S U G j )for1 j  X  i and Pr( S U G j )for i +1  X  j  X  n as input, where Pr( S U G j ) is the estimated value of Pr( S U G j )within absolute error  X  2 n with probability at least (1  X   X  ) 1 /n have the following crucial lemma.

Lemma 3. For 1  X  i  X  n , | DP ( D, i  X  1)  X  DP ( D, i ) | X | Pr( S U G i )  X  Pr( S U
Proof. Let DP be the dynamic programming procedure that is the same as DP except that in line 7 the equation for computing the final result is replaced by Also, let DP be the dynamic programming procedure that is the same as DP except that in line 7 the equation for computing the final result is substituted with Furthermore, let D = D \{ G i } , q i =Pr( G i  X  X  X  ), p i = Pr( S U G i ), and p i = Pr( S U G i ). We have
DP ( D, i  X  1) = q i  X  DP ( D ,i  X  1) + p i  X  DP ( D ,i  X  and By simple mathematics, | DP ( D, i  X  1)  X  DP ( D, i ) | Since the outputs of DP and DP are both in [0 , 1], Thus, the lemma holds.

Recall that Pr( S ) is the estimated value of Pr( S )com-puted in line 3 of the Approximate procedure. We have the following theorem.

Theorem 7. With probability at least 1  X   X  , Proof. We observe that Pr( S )= DP ( D, 0) and Pr( S )= DP ( D, n ). Then, By Lemma 3, | DP ( D, i  X  1)  X  DP ( D, i ) | with probability at least (1  X   X  ) 1 /n .Thus, with probability at least (1  X   X  ) 1 /n n =1  X   X  .
Consequently, the Approximate procedure fails with prob-ability at most  X  .
Let the input parameter  X  of the Approximate procedure be set to the same value for all input subgraphs S .This section discusses how to set  X  to guarantee the overall quality of approximation of the mining algorithm.

Note that, in most cases, it is expected to have  X &gt; 0 .  X  . Then, we have the following crucial theorem.

Theorem 8. The probability of a frequent subgraph S be-ing output as a mining result is at least 1  X   X  2 s ,where the number of edges of S .

Proof. Let S 0 ,S 1 ,...,S s be the subgraphs on the path from the root to S in the search tree, where S 0 =  X  , S s and S i  X  1 is the parent of S i for 1  X  i  X  s . Due to the Apriori property of  X  -frequent probability, i.e., Lemma 1, all of S 0 ,S 1 ,...,S s  X  1 are also frequent. Furthermore, since subgraphs are visited in depth-first order, S is output as a mining result if and only if all of S 0 ,S 1 ,...,S s are output as mining results. Mathematically,
Pr( S is output) = Pr( S 0 is output) Particularly, Pr( S 0 is output) = 1, and for 1  X  i  X  s , Due to the fact that Pr( S i )  X   X &gt; X  and to Theorem 7, Subsequently, The theorem thus holds.

Let max be the maximum number of edges of a frequent subgraph, which can either be estimated by sampling ap-proaches [3] or be specified as a constraint on mining re-sults [12]. From Theorem 8, we have the following corollary.
Corollary 1. To ensure that the probability of any fre-quent subgraph being output is at least 1  X   X  ,theparameter  X  should be at most 1  X  2  X  (1  X   X ) 1 / max .

Table 1: Summary of uncertain graph database.
Extensive experiments were performed to evaluate the ef-ficiency of the proposed algorithm and the quality of mining results. The experimental results are shown in this section.
We experimented on a real uncertain graph database that was obtained by integrating the BioGRID database 1 with the STRING database 2 . The real uncertain graph database contains the protein-protein interaction (PPI) networks of six organisms in the BioGRID database. A PPI network is an uncertain graph where vertices represent proteins, edges represent interactions between proteins, the labels of vertices are the COG functional annotations of proteins 3 provided by the STRING database, the existence probabilities of all vertices are 1, and the conditional existence probabilities of edges are provided by the STRING database. The sum-mary of the uncertain graph database is shown in Table 1, where | V | indicates the number of vertices, | E | the number of edges, and avg( P E ) the average value of the conditional existence probabilities of edges.
 The proposed algorithm was implemented in C on Linux. All experiments were performed on an IBM X3950 server with 2.4GHz Xeon E7440 CPU and 8GB of RAM. The op-erating system installed on the server is CentOS.
We first performed experiments to test the execution time of the algorithm with respect to support threshold  X  , confi-dence threshold  X  and parameters  X  and  X  .Theexperimental results are presented and explained as follows.
 Execution time vs.  X  . Figure 5(a) shows the execution time of the algorithm while  X  varies from 0 . 2to1,  X  =0 .  X  =0 . 05 and  X  =0 . 05. It can be seen that the execution time decreases significantly as  X  gets larger. This is because the number of frequent subgraphs reduces quickly as  X  be-comes larger, leading to the decrease in the number of visited subgraphs as shown in Figure 5(b).
 Execution time vs.  X  . Figure 6(a) shows the execution time of the algorithm while  X  varies from 0 . 6to1,  X  =0 .  X  =0 . 05 and  X  =0 . 05. The execution time decreases quickly as  X  increases. The reason is that fewer subgraphs will have  X  -frequent probability beyond  X  as  X  becomes larger, result-ing in less subgraphs to be visited by the algorithm as shown in Figure 6(b). http://thebiogrid.org http://string-db.org http://www.ncbi.nih.gov/COG
Figure 5: Impact of threshold  X  on time efficiency. Figure 6: Impact of threshold  X  on time efficiency. Execution time vs.  X  . Figure 7(a) shows the execution time of the algorithm as  X  varies from 0 . 025 to 0 . 2,  X   X  =0 . 9and  X  =0 . 05. We can see that the execution time decreases substantially as  X  increases. This is due to the following facts. The execution time of the algorithm is dom-inated by the time for computing the  X  -frequent probabili-ties of all visited subgraphs. Although the number of visited subgraphs increases as  X  gets larger as shown in Figure 7(b), the time for computing the  X  -frequent probability of each subgraph is inversel y proportional to  X  2 as analyzed in Sec-tion 5.2.2, thus decreasing the overall time for computing the  X  -frequent probabilities of all visited subgraphs. Figure 7: Impact of parameter  X  on time efficiency. Execution time vs.  X  . Unlike in the previous experiments with respect to  X  ,  X  and  X  , we can not figure out the rela-tionship between the execution time and  X  when running the algorithm only once for each tested value of  X  .Hence,we ran the algorithm 30 times for each tested value of  X  .Fig-ure 8(a) illustrates the box plot of the execution time as varies from 0 . 05 to 0 . 5,  X  =0 . 2,  X  =0 . 9and  X  =0 . execution time statistically decreases as  X  becomes larger. This is because the number of visited subgraphs does not vary significantly as shown in Figure 8(b) but the time for computing the  X  -frequent probability of each subgraph is proportional to log(2 n/ X  ) as analyzed in Section 5.2.2. Figure 8: Impact of parameter  X  on time efficiency.
We also evaluated the quality of approximation of the al-gorithm by testing the precision and recall of mining re-sults. Precision is the proportion of frequent subgraphs in all output subgraphs, and recall is the proportion of output subgraphs in all frequent subgraphs.

Since it is computationally prohibitive to find all strictly frequent subgraphs, we instead use the set of subgraphs ob-tained for  X  =0 . 2,  X  =0 . 9,  X  =0 . 02 and  X  =0 . 001 as all frequent subgraphs with respect to  X  =0 . 2and  X  =0 . 9.
The experimental results with respect to parameters  X  and  X  are presented and analyzed as follows.
 Precision vs.  X  . Figure 9 shows the precision of the min-ing results while  X  varies from 0 . 025 to 0 . 2,  X  =0 . 2, and  X  =0 . 05. We observe that the precision is over 90% for  X   X  0 . 05 and goes down as  X  getslarger. Thisisbecausewith the increasing of  X  , the number of infrequent subgraphs with  X  -frequent probability within [  X   X   X ,  X  ) increases, resulting in more infrequent subgraphs been output as results as il-lustrated in the #OIS row i n Table 2. Moreover, since  X  fixed, the probability of a frequent subgraph being output is also fixed and is at least 1  X   X  2 s as proved in Theorem 8. Thus, the number of output frequent subgraphs is stable and is independent of  X  as shown in the #OFS row in Table 2. The precision is equal to #OFS / (#OFS + #OIS), thus it decreases as  X  becomes larger.
 Recall vs.  X  . By the arguments above, we know that the number of output frequent subgraphs #OFS is stable and is independent of  X  .Moreover,for  X  =0 . 2and  X  =0 . 9, the number of frequent subgraphs is 184. Since the recall of the mining results is equal to #OFS / 184, we have that the recall does not depend on  X  and is over 94%.
 Precision vs.  X  . In this experiment, we ran the algorithm 30 times for each tested value of  X  . Figure 10(a) shows the box plot of the precision of the mining results while  X  varies from 0 . 05 to 0 . 5,  X  =0 . 2,  X  =0 . 05. It can be seen that with respect to threshold  X  . the precision is as high as 90% and does not depend on  X  statistically. This is due to the following reasons. Since fixed, the number of infrequent subgraphs with  X  -frequent probability in [  X   X   X ,  X  ) is also fixed. Although  X  varies, the ratio of the probability of a specific frequent subgraph being output to the probability of a specific infrequent subgraph with  X  -frequent probability in [  X   X   X ,  X  ) being output is a constant. Therefore, the precision of the mining results is stable and is statistically independent of  X  . Figure 10: Impact of  X  on precision and recall.
 Recall vs.  X  . By Theorem 8, the probability of a frequent subgraph being output as a result is at least 1  X   X  2 s ,thusthe recall of the mining results should decrease as  X  increases. However, Figure 10(b) does not show this trend. The reason is that the Monte-Carlo algorithm [5] used in the Estimate procedure for estimating the probability of a DNF formula being satisfied has much higher accuracy in practice than its theoretical lower bound used in the proof of Theorem 8.
In this paper, an approximate mining algorithm has been developed for efficiently and accurately mining frequent sub-graphs over an uncertain graph database under probabilistic semantics. The algorithm guarantees to find any frequent subgraph with a provably high probability by carefully set-ting parameter  X  using a systematic method. The extensive experiments on the real uncertain graph database verify that the algorithm is practically efficient and that the mining re-sults have very high precision and recall.
 The research work in this paper was in part supported by the NSF of China under Grant No. 60773063, the N SFC-RGC of China under Grant No. 60831160525, the Natio nal Grand Fundamental Research 973 Program of China under Grant No. 2006CB303000, and the NSF o f China under Grant No. 60903017. [1] C. C. Aggarwal and H. Wang. Managing and Mining [2] T. Bernecker, H.-P. Kriegel, M. Renz, F. Verhein, and [3] M. A. Hasan and M. J. Zaki. Output space sampling [4] P. Hintsanen and H. Toivonen. Finding reliable [5] R. M. Karp and M. Luby. Monte-Carlo algorithms for [6] A. Kimmig and L. D. Raedt. Local query mining in a [7] M. Kuramochi and G. Karypis. An efficient algorithm [8] L. G. Valiant. The complexity of enumeration and [9] X. Yan and J. Han. gSpan: Graph-based substructure [10] G. Yang. The complexity of mining maximal frequent [11] Q. Zhang, F. Li, and K. Yi. Finding frequent items in [12] F. Zhu, X. Yan, J. Han, and P. S. Yu. gPrune: A [13] Z. Zou, J. Li, H. Gao, and S. Zhang. Frequent [14] Z. Zou, J. Li, H. Gao, and S. Zhang. Finding top-k [15] Z. Zou, J. Li, H. Gao, and S. Zhang. Mining frequent
