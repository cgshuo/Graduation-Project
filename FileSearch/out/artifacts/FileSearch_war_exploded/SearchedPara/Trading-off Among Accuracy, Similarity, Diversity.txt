 Improving recommendation accuracy is the mostly focused target of recommendation systems, while it has been increas-ingly recognized that accuracy is not enough as the only quality criterion. More concepts have been proposed recent-ly to augment the evaluation dimensions, such as similarity, diversity, long-tail, etc. Simultaneously considering multi-ple criteria leads to a multi-task recommendation. In this paper, a graph-based recommendation approach is proposed to effectively and flexibly trade-off among them.
Our approach is considered based a 1st order Markovian graph with transition probabilities between user-item pairs. A  X  X ost flow X  concept is proposed over the graph, so that items with lower costs are stronger recommended to a user. The cost flows are formulated in a recursive dynamic form, whose stability is proved to be guaranteed by appropriately lower-bounding the transition costs. Furthermore, a mixture of transition costs is designed by combining three ingredi-ents related to long-tail, focusing degree and similarity. To evaluate the ingredients, we propose an orthogonal-sparse-orthogonal nonnegative matrix tri-factorization model and an efficient multiplicative algorithm. Empirical experiments on real-world data show promising results of our approach, which could be regarded as a general framework for other affects if transition costs are designed in various ways. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Algorithm, Proof, Experimentation, Performance Collaborative filtering, graph-based cost flow, fixed point stability, contraction mapping, orthogonal-sparse-orthogonal nonnegative matrix tri-factorization
With extensive recommendation system technologies de-veloped over the past decade, there emerge lots of successful efforts in both academia and industry, taking the well-known applications by Amazon and Netflix for example. The most common formulation of a recommendation problem adopt-s the notion of user-to-item ratings, with boolean, integer, or ordinal values. A recommendation system typically tries to predict the values (or orders) of the ratings of unrated items 1 for each user, with a satisfactory ranking [2, 5, 10].
The major existing efforts in the literature focused on im-proving the recommendation accuracy, as exemplified by the Netflix Prize competition [20]. However, it has been increas-ingly recognized that it is not sufficient to have accuracy as the only criterion during measuring the recommendation quality. Recently, more concepts have been considered as al-ternative evaluation dimensions to the recommended items, such as similarity, diversity, novelty, trust, long-tail, etc., to generate results that are not only accurate but also valuable with additional properties [2, 5, 10, 23, 25, 27].
If considering accuracy together with some of these ad-ditional targets, we encounter a multi-task recommendation and need to seek a solution that trades-off among them. Par-ticularly, four evaluation concepts are chosen as our focus:
In different application scenarios, these four evaluation perspectives play different roles. First, accuracy should be the fundamental target in any recommendation systems. Moreover, as listed in Table 1 for example, the remaining three metrics tend be also valuable for recommendation on consumable goods. However, for the non-consumable goods, the similarity measure might be not that important because,
Following existing recommendation studies, here and throughout this paper,  X  X ser X  and  X  X tem X  are not limited to their names, whereas they generally stand for the subjective side and the objective side, respectively. e.g., a person bought a TV set is unlikely to buy another soon even if they are extremely similar. Moreover for food recommendation, accuracy and similarity that indicate the taste of a user become important, whereas diverse/long-tail items without qualified ratings are not preferred. Table 1: Evaluations for recommendations. Symbol  X   X  or  X ? X  indicates an evaluation is or may be not important for the corresponding scenario
From another point of view, we X  X  like to discuss about the  X  X atisfaction X  of a recommendation system. Ideally, a rec-ommendation system should receive satisfactions from not only the user-side but also the platform-side (e.g., Amazon, Netflix). Table 2 lists four types of feelings that involve sat-isfaction. First, highly related to accuracy and similarity, feeling a recommendation matched with users X  need/tastes is important to both user-side and platform-side. Second, the novelty is usually contributed by diversity and long-tail, is the similarity is guaranteed. Third, to widely cover each user X  X  interest, we X  X  better to enhance the similarity and di-versity. Fourth, a platform is satisfactory and attractive to retailers if the item coverage throughout all users is wide, which concerns the diversity and long-tail factors. Table 2: Satisfactions to recommendations. Differ-ent feelings are important on either or both of user-and-platform sides, which should have different key evaluation metrics (as indicated by ) platform-cover platform  X   X 
Focusing on Acc , Sim , Div and Lt , this paper proposes a graph-based recommendation approach that can effective-ly and flexibly trade-off among them. At the beginning, we construct a directed graph with 1st order Markovian, whose transition probabilities are calculated based on rat-ing weights. Given a user, a cost flow concept is proposed on this graph, and items with lower cost are stronger recom-mended. The costs are formulated in a recursive dynamic form. Thereafter, based on fixed point theorem, we prove that the stability of the dynamics is guaranteed by appropri-ately lower-bounding the transition cost of each edge. Ac-cordingly, a mixed version of transition costs is designed by combining three ingredients related to long-tail, focusing de-gree and similarity. In order to evaluate the latter two ingre-dients, an orthogonal-sparse-orthogonal nonnegative matrix tri-factorization (OSO-NMTF) model is proposed, togeth-er with an efficient multiplicative algorithm with the help of gradients on Stiefel manifold. Empirical experiments on real-world data show promising results of our approach. The remainder of this paper is organized as follows. In Section 2, we briefly review related efforts in the litera-ture. Based on a directed graph representation, Section 3 introduces the cost flow formulation, provides the stability analysis, and proposes a mixture construction to the tran-sition costs. The specification and learning algorithm are described in Section 4. After reporting experimental results in Section 5, we finally draw concluding remarks in Section 6.
Among a large number of recommendation techniques that have been developed over the past decade, collaborative fil-tering (CF) techniques represent most widely used and well-performing algorithms. Aiming to directly predict the rat-ings, two representative CF techniques are neighborhood-based CF algorithms and matrix factorization based CF al-gorithms [14, 20, 21]. Moreover, there exist a stream of graph-based recommendation approaches, aiming to propa-gate information throughout a graph and mostly predict the ranking of the ratings [1, 3, 11, 15], taking the well-known PageRank and related algorithms for example.

Besides the differences on implementation algorithms, ex-isting efforts also take different objective functions to be optimized. Most approaches that directly predict the rat-ings [14] adopt the estimation errors (e.g., RMSE, MAE) or further ranking accuracies (e.g., MRR, NDPM). Since it is increasingly recognized that merely caring about accuracy is not enough [23], papers [3, 11] further considered the sim-ilarity between items and users X  tastes. The authors of [1, 10, 17, 22] focused on optimizing diversity, so that not only the redundancy of recommended items to each user is low, but also the coverage of recommended items to all users is large. Moreover, researchers also observed that most of ex-isting recommender systems, especially CF based methods, can not recommend tail products due to the data sparsity issue. On the other, the success of  X  X nfinite-inventory X  re-tailers such as Amazon and Netflix is greatly attributed to a long-tail phenomenon, where tail product availability is able to boost head sales by offering the shopping convenience for both mainstream and niche tastes. Under this motivation, recently several efforts [2, 5, 25, 27, 29] turned to take the long-tail effects into consideration. However, to the best of our knowledge, there is still no effort that considers these cri-teria simultaneously under a unified framework. This paper is thus motivated for such a study.
In this section, we introduce the graph representation of the user-item relation and the cost flow concept. Then, after proposing the flow dynamics, we proceed to analyze its sta-bility and the transition cost composed of multiple effects.
In recommendation scenarios, the user-item relationship is usually represented by an edge-weighted undirected bi-partite graph G ( V,E, X  ) as illustrated in Fig. 1(a), where V represents the set of vertices, E represents the set of edges, and  X  represents the weights on the edges. With V u and V i denoting the sets of user-vertices and item-vertices re-spectively, there is no edge between any two user-vertices or between two item-vertices. Clearly V = V u  X  V i ,andthe number of users is N u = | V u | and of items is N i = | V edge set E has element e ( u, i ) if and only if user u  X  touches item i  X  V i (e.g., rating in user-movie scenario, click in query-). Since the graph is undirected, the edge weights  X  ( u, i )=  X  ( i, u ) are symmetric, and indicate the strength of the relation between the user-item pair ( u, i ). (a) undirected (b) directed (c) cost flow Figure 1: User(circle)-item(square) relation ex-pressed in bipartite graphs. (a) undirected graph: each edge is associated with symmetric weights  X  . (b) directed graph: each edge is associated with asymmetric transition probability. (c) cost flow: each edge is associated with transition probability and cost; given absorbing nodes S = S u  X  S i ,attime t each vertex j has a cost F ( t ) S ( j ) to reach S , with S u = { u 2 } and S i = { i 3 } highlighted for instance
Like many existing works on random walk and Markov chain [18, 22, 24, 27], we convert the undirected G ( V,E, X  ) into directed graph G ( V,E,P ) as illustrated in Fig. 1(b), where P is the set of transition probabilities on the edges. In graph G ( V,E,P ), every undirected edge in G ( V,E, X  )is converted into two directed edges with opposite directions. In P = { p iu ,p ui } ,the p ui denotes the transition probability from user-u to item-i ,and p iu is the transition probability from item-i to user-u , both obtained by normalization on  X  :
In the recommendation scenario, given a single user or auserset S u and its connected item set S i , our target is to output a ranking list on all items based on some objec-tive/criterion and recommend them to S u .

Consider a random flow (walk) on graph G ( V,E,P ), we define S = S u  X  S i as the absorbing/sink nodes, i.e., the flow stops when any node in S is reached. Moreover, we assume that each edge e ( j, ) has a nonnegative one-step transition cost C ( j | ) when a flow passes through. Finally, we have the cost flow graph G ( V,E,P,C ), with an additional one-step transition costs C . An example is illustrated in Fig. 1(c). We define the long-term cost of flowing from a vertex j to be absorbed as F S ( j ), and F S ( )=0for  X   X  S by definition. Also, we require the vector F S to locate in a simplex S , i.e., nonnegative and sum equal to 1. Given G ( V,E,P,C )with appropriately defined C , we can output the ranking of F S given S = S u  X  S i as the recommendation list to user set S where item-i with a smaller F S ( i ) is more preferred.
However, directly evaluating the flow cost F S is quite d-ifficult. Instead, we formulate it into the dynamic recursion F S = f (
Definition 1. (Cost flow dynamics). At iteration t ,the cost to reach the set S = S u  X  S i is defined as F ( t ) [ F
S (1) ,...,F { p iu } X  X  p ui } in the following dynamic way as t increases: where C ( u | i ) describes the one-step non-negative transition cost from item-i to user-u , and vice versa for C ( i | Remember that we start from a simplex initialization F (0) S . First, the above dynamics capture the one-step cost flow in the user-item bipartite graph. Second, at any time t ,it not only guarantees F ( t ) S ( j )=0for  X  j  X  S , but also ensures each F ( t ) S ( j )  X  0and j F ( t ) S ( j ) = 1, i.e., x  X  S  X 
One crucial analysis of the dynamics in Eq. (2) is to check its stability, i.e., whether it can converge to a unique equilib-rium with any random initialization. This part introduces a sufficient condition that guarantees the stability.
Our analysis begins with the Banach Fixed Point Theo-rem as restated in Theorem 1, and Theorem 2 relates the contraction mapping with its Jacobian matrix [12, 19]. Theorem 1. Consider a set D X  R n and a function f : D X  R n ,if(1) D is a closed set; (2) x  X  X  =  X  f ( x )  X  X  ; (3) f (  X  ) is a contraction mapping on D ,then: (1) there is one unique x  X   X  X  s.t. f ( x  X  )= x  X  ; (2) iteration
Theorem 2. Assume D X  R n is convex and f : D X  R n has continuous partial derivatives in D .Denote f ( x ) as the Jacobian matrix. If  X  q&lt; 1 s.t., for some matrix norm || X || ,then f is a contraction mapping. Based on these theorems, we analyze the mapping F ( t +1) f ( with the following elements  X 
In the below, we show that simply lower-bounding all one-ficient to guarantee the flow dynamics X  stability.
Theorem 3. (Lower-bounded transition cost.) If all one-step transition costs are lower-bounded by 1, i.e,: then the Jacobian matrix in Eq. (3) has a bounded infinite-norm || J ||  X   X  q&lt; 1 . Consequently, f (  X  ) is a contraction mapping, and there exists a unique equilibrium F  X  S  X  S ,s.t. iterating Eq. (2) from  X  F (0) S  X  S converges to F  X  S as t
The proof of Theorem 3 is sketched in Appendix A. We proceed to design the detailed one-step transition cost C ( u | i )and C ( i | u ) for each linked user-item pair ( u, i ). It should be noted that, the transition cost is a general con-cept and not limited to the designs presented below, choices alternative to which deserve study interests in future.
Particularly, we consider three factors that encourage sim-ilarity , long tail ,and focusing degree , respectively. All of them are nonnegative costs.
Combining the above three factors, we summarize our fi-nal one-step transition cost C ( u | i )and C ( i | u )asbelow: C ( u | i )=1+  X  sim C sim ( u | i )+  X  foc C foc ( u ) , C ( i | u )=1+  X  sim C sim ( i | u )+  X  lt C lt ( i )+  X  foc which is a linear mixture of the three factors. The constant 1 is added for stability due to Theorem 3, which also serves as a basic cost similar to the  X  X bsorbing Time X  method [27]. Connected Subgraph v.s. Smoothed Full Graph .

In implementation, we find that the iteration by Eq. (2) converges very fast, e.g., almost always within 5 rounds. However, each iteration has a complexity of O ( | E | )with being the number of edges. In order to scale the graph size and improve the efficiency, given the absorbing set S ,we consider to iteratively extract a connected subgraph G S : and then propagate the flow costs by Eq. (2) only through the connected subgraph G S . This is based on a reasonable intuition: a group of users and items they like are connected. The round number is chosen as  X  = 2 and enough to pro-vide desired results based on our experiences. On the other extreme, some works [22] also smooth the graph by adding edges between each pair of users, resulting in a wholly con-nected full graph. We do not follow that way for efficiency.
Up till now, we still have not explained the detailed de-signs on C foc and C sim . First, the computation of C foc Eq. (6) requires clustering both users and items. In the next section, we will propose a model named as OSO-NMTF that clusters both sides simultaneously. Second, one can certain-ly evaluate the user-item similarity based on meta features if available (e.g., text description or genre of users and items). However for applications with no or limited meta informa-tion, the problem is how to measure the user-item similarity C sim given only the graph weights, for which a novel simi-larity measure is proposed based on the OSO-NMTF model.
There are extensive studies under the name of topic mod-els in the literature [26], which show promising performances on finding meaningful latent topics. Intuitively, with each topic representing a cluster, the topic based recommendation or ranking [15, 27] owe its success to the abstracted high-level topic features, which are more robust and meaningful than the raw low-level features. Nevertheless, most of exist-ing works only focus on clustering either one of users or item-s. Aiming to not only implement user-item bi-clustering but also extract user-item similarity simultaneously, we adopt the nonnegative matrix tri-factorization (NMTF) formula-tion [8, 28]. This section introduces the OSO-NMTF model that assigns orthogonal-sparse-orthogonal constraints, and proposes a multiplicative learning algorithm. Thereafter, we construct the similarity cost C sim and focusing degree cost C foc in Eq. (7) based on OSO-NMTF. In our user-item scenario, a nonnegative matrix X  X  + is constructed with x ij =  X  ( i, j ), where  X  ( i, j )is the graph weight in Eq. (1). Given X , NMTF decomposes it into a product of three nonnegative factors W  X  R N u  X  K u Further imposing orthogonality on both W and H leads to the orthogonal NMTF [8, 28], which can be achieved via solving the following optimization: where || X || F denotes the matrix Frobenius norm. Nonnega-tivity plus strict orthogonality on W and H are difficult to be ensured during optimization, because it will leads each row in both W and H to be all zeros except only one ele-ment equals 1, i.e., a typical combinatorial optimization. In consequence, paper [8] proposes a multiplicative algorithm based on an approximate Lagrange, and [28] uses gradients on Stiefel manifold [9] for an approximate updating.
Bi-Clustering Interpretation. From a clustering per-spective, the O-NMTF formulation in Eq. (8) corresponds to the simultaneous clustering of the rows (users) and columns (items) of X [7, 8]. Particularly, there are K u user-clusters and K i item-clusters, W is the cluster indicator matrix for clustering users, H is the cluster indicator matrix for clus-tering items, A is the association weight matrix between user-clusters and item-clusters. It should be noted that, de-spite the approximate orthogonality on W and H ,theycan be still regarded proportional to the soft -clustering posteri-ors. More interpretations and relations to other clustering methods are referred to [7, 13, 16, 21].

The Orthogonal-Sparse-Orthogonal Constraints. As a revised extension to O-NMTF in Eq. (8), we further as-sume that the association matrix A is sparse. Albeit this tiny revision, it brings the following three main benefits: Mathematically, orthogonal-sparse-orthogonal NMTF (OSO-NMTF) is formulated into the following optimization: where  X &gt; 0 is the Lagrange multiplier, and the second term is the classic L 1 -norm for sparsity [16]. To solve the constrained optimization of OSO-NMTF in Eq. (9), one may augment it into an approximate Lagrangian similar to [8], and determine three Lagrange multipliers on W , H , A , respectively. In contrast, the orthogonality con-straint on W and H indicates the Stiefel manifold, the ge-ometry of which was systematically studied in [9]. Employ-ing the gradient on Stiefel manifold similar to [28], we obtain the following multiplicative updates: where operator  X  denotes the Hadamard (element-wise) prod-uct, operator [  X  ] [  X  ] is the Hadamard (element-wise) division. The derivation of this algorithm is sketched in Appendix B. In experiments, this updating converges quickly and effec-tively preserves a rough orthogonality, with low generaliza-tion errors when missing values are predicted.
Once an OSO-NMTF model is learned, the focusing degree cost C foc and the similarity cost C sim in Eq. (7) can be obtained. Remember the bi-clustering properties of OSO-NMTF discussed in Section 4.1, W and H describe the user-cluster and item-cluster posteriors, respectively. Intuitively, a user or item should have a smaller/larger cost C foc if its cluster posteriors are shaper/smoother. More concretely, we employ entropy for the quantification:
C
C where normalize(  X  ) is the row normalization operator such that each row in  X  W and  X  H has a posterior-over-clusters form. Again, paper [27] only focuses on clustering users by Latent Dirichlet Allocation, while our C foc is constructed by clustering both items and users simultaneously.
For the user-item similarity C sim inEq.(7),herewein-troduce a novel user-item similarity measure based on OSO-NMTF. Consider an item i and a user u , they are represented by a column X  X  i  X  R N u + and a row X u  X   X  R N i + , respective-ly. Using the normalized  X  W and  X  H as in Eq. (11), we can explain  X  i =  X  W T X  X  i as the accumulated posteriors over the user-clusters contributed by this item i .Thecaseis similar for  X  u =  X  H T X T u  X  . Given the association A transfer to measure the similarity between  X  i and  X  u : where a symmetric similarity is assumed in our implemen-tations. In case of some applications, one may want to use asymmetric to differentiate the bi-directions. In that case, one possible alternative choice would be: That is, normalizing the association matrix A by row will get the transition probability from user-clusters to item-clusters, and vice versa for normalization by column. Moreover, one may also consider to normalize  X  i and  X  u before product, in order to balance the different weights of users and items.
In experiments, we consider two real world tagging/rating datasets: Movielens100K and Last.fm . Collected by the GroupLens Research Project at the University of Minnesota, the Movielens100K dataset 2 consists of user-to-movie rat-ings (in the range of 1  X  5), together with basic tag infor-mation of both users and movies. Collected from Last.fm online music system, the Last.fm dataset 3 contains user-to-artist tagging (i.e., binary ratings), together with the tagged properties and user social network information.

We preprocess the datasets as follows. For Movielen-s100K , we filter out ratings that are less than 4 (i.e., neutral or negative), and then filter out users who have rated less than 10 items. Moreover, each movie has a genre tag out of 18 possible values. In order to evaluate performance dur-ing testing , for each user, his/her user-tag vector is obtained by accumulating the genre tags of items rated by him/her. For Last.fm , we filter out users who have tagged less than 10 items. Moreover, a user tagged an artist by description phrases. Similarly, we accumulate all tags posted by each user as his/her user-tag vector, and all tags received as the item-tag vector for each item. Thereafter, each tag-vector is normalized so that elements X  sum equals to 1. We denote  X  and  X  i as tag-vectors to a user u andanitem i , respectively.
Several key statistics of the data after preprocessing are summarized in Table 3. In order to further decrease the sparsity in Movielens100K , we randomly pick 20% ratings for training and leave the remaining 80% for testing, while the training v.s. testing ratio for Last.fm is 80% v.s. 20%.
For each user u , we are given a list of truly rated item set  X  u based on the testing data. During recommendation, once an algorithm proposes a ranked list  X  u of all unrated items (in the training data) by each user u ,weuse  X  u ( )to denote the -th most strongly recommended item, and  X  10 u represent the set of top 10 recommended items. Four metrics [6] are chosen for evaluating experimental performances: http://movielens.umn.edu http://ir.ii.uam.es/hetrec2011
In order to investigate how each performance metric changes as the mixing weights (  X  sim , X  lt , X  foc ) in Eq. (7) vary, we tra-verse on grids of the three weights X  configurations within the 3-dimensional simplex, as illustrated in Fig. 2. This enumer-ation is implemented on both Movielens100K and Last.fm datasets. Two representative CF methods are considered in comparison, namely Matrix Factorization (MF) [20] and Nonnegative Matrix Factorization (NMF) [21]. The latent bi-dimensionalities of OSO-NMTF model are set as K u =30 and K v = 50, and the latent dimensionalities of MF and N-MF are both fixed as 30 for a fair comparison.

Once OSO-NMTF has been trained, the three ingredients of transition costs are computed according to Eqs. (5&amp;11&amp;12). Enumerating through hundreds of gridded configurations of (  X  sim , X  lt , X  foc ) in the simplex, we perform our cost flow ap-proach with each configuration. Finally, the performances on Movielens100K and Last.fm are reported in Fig. 3 and Fig. 4, respectively, where each subfigure illustrates one met-ric X  X valuesoverthesimplexsameasinFig.2. Themaxand min values of each simplex are reported in Table 4, compared with the results by MF and NMF.

As shown by Figs. 3&amp;4, each metric varies smoothly with-in the simplex, and the tendencies are similar on the two data. Interestingly, the cost ingredients C lt and C sim play correlated roles in some extent: large weights on them en-courage relatively large MRR (high accuracy), small DIV (low diversity), and large LT (low long-tail ratio). Even more, the  X  X ocusing-degree X  cost C foc brings forward much more diverse and long-tailed results than the  X  X ong-tail X  cost C lt . Roughly, the  X  X est X  values for all metrics are provided with only two cost ingredients combined (i.e., locate near to the simplex edges). Particularly, the best MRR comes from combining C lt and C sim , while all the best values of SIM DIV and LT result from combining C foc and C sim .We may imagine C sim as a bridge that connects C lt and C foc to slither among good environments for different metrics. Figure 2: The simplex Movielens100K Last.fm Figure 3: Performances of our approach on Movielens100K within the simplex of Fig. 2 (best viewed in color) within the simplex of Fig. 2 (best viewed in color)
In Table 4, all the  X  X est X  performances of each metric over the simplex are much better than those by MF and NMF. Moreover, even the X  X orst X  X erformances, except MRR ,are better than or at least comparable to MF and NMF. Specifi-cally, MF and NMF tend to recommend top items with small DIV and large LT . In contrast, in the simplex not only the smallest DIV is higher, but also the largest LT is smaller than MF/NMF. That is, our approach performs robust with much better similarity, diversity and long-tail measures.
Time cost. Once transition costs are calculated, our approach includes no training phase. The average prediction time per user is within 0.2 second on Movielens100K and within 3 seconds on Last.fm , implemented in Python and using an Intel Xeon 2.4G single core CPU.
It is increasingly recognized that accuracy is not enough as the only quality criterion to a recommendation system, and more concepts have been proposed recently as additional evaluation dimensions. Simultaneously considering accura-cy, similarity, diversity, and long-tail, this paper proposes a graph-based recommendation approach that effectively and flexibly trades-off among them. Particularly, a cost flow con-cept is proposed based on a 1st order Markovian graph with transition probabilities between user-item pairs. The costs are further formulated in a recursive dynamic form, whose stability is proved to be guaranteed if the transition costs are appropriately lower-bounded. Furthermore, a mixed version of transition costs is designed by combining three ingredi-ents related to long-tail, focusing degree and similarity. To evaluate the ingredients, we propose an orthogonal-sparse-orthogonal nonnegative matrix tri-factorization model and an efficient multiplicative updating algorithm. Experiments on real-world data show valid performances of our approach.
Consequently, for applications with a specific composi-tion of different objectives, we can easily adjust the mixing weights in Eq. (7) appropriately, hinted by Figs. 3&amp;4. Addi-tionally, although in experiments we only consider the rec-ommendation task to a single user, the extension to a group of users can be obtained by revising the absorbing node set to include those users. More applications and systematic comparisons with other collaborative filtering methods are expected in future. Last but not the least, our approach could be regarded as a general framework with different af-fects if alternative schemes are considered and designed di-rectly in place of Eq. (7) and the transition costs therein. [1] G. Adomavicius and Y. Kwon. Maximizing aggregate [2] C. Anderson. The Long Tail: Why the Future of [3] M. Brand. A random walks perspective on maximizing [4] K. P. Burnham and D. Anderson. Model Selection and [5] O. Celma. Music Recommendation and Discovery in [6] P. Cremonesi, Y. Koren, and R. Turrin. Performance [7] C. Ding, X. He, H. Zha, and H. Simon. On the [8] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [9] A. Edelman, T. Arias, and S. Smith. The geometry of [10] D. M. Fleder and K. Hosanagar. Recommender [11] F. Fouss, A. Pirotte, and M. Saerens. A novel way of [12] D. J. H. Garling. Inequalities: A Journey into Linear [13] E. Gaussier and C. Goutte. Relation between PLSA [14] A. Gunawardana and G. Shani. A survey of accuracy [15] T. H. Haveliwala. Topic-sensitive pagerank. In Proc. [16] P. O. Hoyer. Non-negative matrix factorization with [17] N. Hurley and M. Zhang. Novelty and diversity in [18] J. G. Kemeny and J. L. Snell. Finite Markov Chains . [19] W. A. Kirk and M. A. Khamsi. An Introduction to [20] Y. Koren, R. Bell, and C. Volinsky. Matrix [21] D. D. Lee and H. S. Seung. Learning the parts of [22] H. Ma, M. R. Lyu, and I. King. Diversitying query [23] S. M. McNee, J. Riedl, and J. A. Konstan. Being [24] J. Norris. Markov Chains . Cambridge University [25] Y.-J. Park and A. Tuzhilin. The long tail of [26] M. Steyvers and T. Griffiths. Probabilistic Topic [27] H. Yin, B. Cui, J. Li, J. Yao, and C. Chen.
 [28] J. Yoo and S. Choi. Nonnegative matrix factorization [29] M. Zhang and N. Hurley. Avoiding monotony:
According to Theorem 1, since f (  X  ) in Eq. (2) satisfies f : S  X  S and the simplex S is closed convex, we need to find one sufficient condition such that f (  X  ) is a contraction mapping, which leads to the stability.

Based on Theorem 2, we evaluate the infinite matrix-norm || J ||  X  =max i j | J ij | , i.e., the maximum absolute row sum. Specifically, we derive =  X  X | J ||  X  =max where the first inequality is based on | a  X  b | X | a | + | a, b  X  0. To let || J ||  X   X  q&lt; 1, our target becomes to find the condition that can ensure j g ( t ) ( j ) &gt; 1+ N .
On the other side, if we lower-bound each one-step tran-sition cost C ( u | i )and C ( i | u )by C lb  X  0, we have based on the definition in Eq. (2).

In consequence, a necessary condition to || J ||  X   X  q&lt; 1is letting the lower-bound C lb  X  1, which completes the proof.
For the optimization task in Eq. (9), we denote L = 0 . 5 X  X  WAH T 2 F +  X  i,j A ij , the partial derivatives w.r.t. parameters W , H and A are given as
Due to the orthogonality W T W = I and H T H = I ,we employ the gradients  X  W and  X  H on Stiefel manifold [9]:  X  W =  X  L  X  H =  X  L
The above yields the multiplicative updates in Eq. (10).
