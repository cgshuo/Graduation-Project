 Algorithms Text Mining, Clustering, Knowledge Management, Gap 
Analysis 
In this paper, we focus on analyzing text produced by service and support helpdesks and corresponding knowledge bases. Human helpdesk operation is very labor intensive and therefore expensive. Consequently, automation of helpdesk problem solving represents a key objective for providers of electronic customer services. The first step in automation of a task is to understand it, and one of the first steps in understanding is to segment examples into meaningful categories [1]. The thesis of this paper is that there is much value derived from using our algorithms and methodology to automatically leverage the text within problem tickets to identify areas and specific problems within a knowledge base that need to be improved. Our approach uses the analysis, mining, and summarization of problem tickets text using an automated unsupervised clustering algorithm in concert with a data analyst. Our algorithms then automatically discover those categories of problem tickets which are not well represented within a knowledge base. The goal in this comparison is not to find the best solution to each problem category, but to see which problem categories are least addressed by the knowledge base. 
In this paper, we will illustrate our results and methodology on a text data set containing over 10,000 helpdesk examples obtained from an IBM Global Services support center. A typical text document (known as a problem ticket) from this data set consists of a brief description by the heipdesk agent of the exchanges between an end user and an expert helpdesk advisor, for example: solution set. Finally, in Section 5, we discuss future work and how our approach might be applied in other domains In this section we describe the general process used for creating a classification of the problem ticket information. This classification is a critical step in the process of finding knowledge gaps between the problems and solutions. In the absence of such a classification we would need to do a detailed analysis of individual problem tickets against individual solution documents. Typically this is accomplished by sampling the problem tickets and manually categorizing and counting. Depending on the size of our problem ticket data set, this could be a daunting, expensive and error prone undertaking. This approach usually leads to the most common questions being addressed, but can miss many important problems and sometimes whole categories. Particularly, as more emphasis is placed on leveraging knowledge bases to augment service and support operations more and increasingly complex issues must be addressed within the knowledge base. The approach to manually categorize and count simply does not scale to this problem. We use the k-means text clustering algorithm to obtain an initial clustering of the data. First, a dictionary of terms is generated by finding those words in the text that occur with the highest frequency. A generic stop word list is then subtracted to eliminate common non-informative words such as "and" and "the". What usually remains, in addition to the informative words, is a set of proper names. Some of these, such as "Lotus", may be useful features for clustering. Others, such as "Bob", may be a distraction to the clustering algorithm. Our software provides features for automatically locating proper names (via capitalization) and for manually refining the dictionary to improve clustering performance. A stemming algorithm is employed to combine all variants of a word into a single term (e.g. print, prints, printer, printed, etc.). Each problem ticket is converted into a vector of floating point values by counting in each problem ticket the number of occurrences of each dictionary term. This integer vector is then normalized to have unit Euclidean norm. The k-means algorithm is then executed starting with k random seeds selected from the population of problem tickets. The distance metric employed is the cosine similarity metric. Thus two points are considered identical if the cosine of the angle between them is 1.0 and considered most dissimilar if the cosine of this angle is 0.0. The k-means approach starts by finding the nearest of the k problem ticket seeds to each problem ticket example (using the cosine distance metric). Each problem ticket is then said to "belong" to the cluster corresponding to that nearest seed. For each cluster, a centroid is then calculated, which is the mean of the examples contained in that cluster. In the next iteration, each problem ticket is compared to every centroid and each problem ticket is moved to the cluster of the nearest centroid. The centroids are then recalculated and the process continues until an iteration does not result 
Equation 3: The g-score is the distance of the centrold Note that we use the maximum cosine distance to select the "most similar" document because cosine distance returns a value of 1.0 for identical documents and 0,0 for completely distinct documents. As the g-score increases, therefore, we expect the likelihood of a matching solution document to the cluster to also increase. A low g-score indicates that no matching solution document is present (thus a low g-score indicates a large "knowledge gap"). Clusters with cohesion approaching approaching 1.0 (e.g. those consisting of nearly identical documents) will require solution documents nearer their centroid to achieve the same g-score as clusters with less cohesion. This inverse proportionality of cohesion is important when using the cosine distance metric to avoid the smallest g-score always corresponding to the cluster with the lowest cohesion since, generally, centroids from clusters with low cohesion tend not to match any particular document very exactly. There are some inherent assumptions underlying this approach. The principle assumption is that solution documents contain many of the same terms as the problems that they intend to solve. While there are applications and domains where this may not be the case, we have observed across a broad set of helpdesks and knowledge bases this assumption to hold. We have found that in practice most helpdesk solution documents contain at least a brief summarization of the problem that they propose to solve, and this summary invariably will contain words that are similar to those used in the problem ticket descriptions. In addition, the problem tickets themselves will often contain brief descriptions of what was done to resolve the issue. In many cases, the terms in this solution description match those used in the solution documentation. Therefore, we believe this assumption is valid and will hold for many other domains as well. In order to evaluate the efficacy of our gap metric, we establish a situation where we know that a definitive knowledge gap exists and run our algorithm to determine if it correctly identifies the actual knowledge gap. For comparison, we also evaluate various other related metrics against the same data sets. One of the alternative metrics is the cosine distance metric, which is essentially the same as the g-score with the denominator (cohesion) always equal to 1.0. The second alternative is to use an Euclidean distance metric instead of the cosine distance metric. In the second case the denominator is also set to 1.0. For this evaluation, our test sets are drawn from three distinct collections of test documents. One of these is the well-known Reuters-21578 data set [4] containing 9603 training documents, and the other two are helpdesk problem tickets from two different help centers. For the Reuters data set, we made use of both the existing classification of the data and a new classification generated 
Knowledge Gap Analysis has many potential applications beyond helpdesk problem ticket analysis. Virtually any domain with unstructured problem descriptions and a knowledge base of text solutions will be amenable to this type of analysis. One such example would be to order the results of a keyword search query so that documents which are most unlike any previously seen documents are listed first. 
Another potential application of this approach as an adjunct to web crawlers to search for areas of the web with content that is most unlike anything seen before. 
The authors gratefully acknowledge Mike Lamb for first suggesting the problem which Knowledge Gap analysis solves and Tracy Knoblaueh, our first user of the technology who provided invaluable feedback. [1] Brachman, R. and Anand T. (1996). The Process of Knowledge Discovery in Databases. In Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, 
Advances in Knowledge Discovery and Data Mining, Chapter 2, pages 37-58. AAAI/MIT press. [2] Duda, R. O. and Hart, P. E. (1973). Pattern Classification and Scene Analysis. Wiley. [3] Hartigan, J. A. (1975) ClusteringAigorithms. Wiley. [4] D. D. Lewis (1999). Reuters-21578 text categorization test collection distribution 1.0. http://www.researeh.att.com/~lewis. [5] Rasmussen, E. (1992). Clustering algorithms. In Frakes, W. B. and Baeza-Yates, R., editors, Information Retrieval: Data Structures and Algorithms, pages 419-442. Prentice Hall, 
Englewood Cliffs, New Jersey. [6] Salton, G. and McGilI, M. J. (1983). Introduction to Modern 
Retrieval. McGraw-Hill Book Company. [7] Salton, G. And Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. Information Processing &amp; Management, 4(5):512:523. 
