 Knowledge discovery aims at constructing predictive models from data that are both accurate and comprehensible. Use of prior knowledge in the form of taxonomies over attribute values offers an attractive approach to this problem. 
Several authors have explored the use of taxonomies defined over attribute values to guide learning. Zhang and Honavar developed a Decision Tree [8] and a Naive Bayes [9] learning algorithm that explo it user-supplied feature value taxonomies. Kang et al [2] introduced WTL, Word Taxonomy Learner for automatically deriving taxonomies from data and a Word Taxonomy-guided Naive Bayes (WTNBL-MN) algorithm for document classification. Michalski [7] has proposed a general frame-user-supplied knowledge in the form of attribute value taxonomies to generate rules at higher levels of abstraction, named TRIPPER (Taxonomical RIPPER). We report widely used benchmark data set (the Reuters text classification data set [10]). RIPPER ( Repeated Incremental Pruning to Produce Error Reduction) , was proposed by Cohen [1]. It consists of two main stages: the first stage constructs an initial ruleset using a rule induction algorithm called IREP* [4]; the second stage further optimizes the ruleset initially obtained. These stages are repeated for k times. IREP*[1] is called partitioned in two subsets: a growing set, that usually consists of 2/3 of the examples IREP* uses MDL[5] as a criterion for stopping the process. The rule growth phase: The initial form of a rule is just a head (the class value) and an empty antecedent. At each step, the best condition based on its information gain is added to the antecedent. The stopping criteri on for adding conditions is either obtain-prove the information gain score. specific. Pruning is done accordingly to a scoring metric denoted by v* .
IREP* chooses the candidate literals for pruning based on a score v* which is ap-plied to all the prefixes of the antecedent of the rule on the pruning data: rule. The prefix with the highest v* score becomes the antecedent of the final rule. Before introducing TRIPPER , it is helpful to formally define a taxonomy: Taxonomy: Let S = {v1, v2, ... vn} be a set of feature values. Let T be a directed tree where children(i) denotes the set of nodes that have incoming arrows to the node i. A assigns to a node i of the tree T a subset S X  of S with the following properties: 1. TRIPPER(G) -improvement at rule growth phase: Introducing the taxonomical knowledge at the rule-growth phase is a straightforward process we call feature growth phase. 2. TRIPPER(G+P) -improvement at rule pruning phase: A more general version choose from a whole range of levels of specificity for the feature under consideration. 
The effect on the resulting rule can be observed in the following example: [original rule] -(rate = t) and (bank = t) and (dollar = t) =&gt; is_interest [pruned rule] -(rate = t) and (bank =t) and (any_concept = t) =&gt; is_interest [abstracted rule] -(rate = t) and (bank = t) and (monetary_unit= t) =&gt; is_interest Example 1: Variants of a classification rule for the class  X  X nterest X  Prune-by-abstraction(Rule,PruneData) PrunedRule=PruneRule(Rule,PruneData) Score=v*(PrunedRule,PruneData) PrunePos=GePrunePos(PrunedRule), Level=0 While(improvement) Improvement=false, Increase(Level) For j:=PrunePos to size(Rule) AbstrRule=PrunedRule For i:=j to size(Rule) Literal=Rule(i) AbstrRule:=AbstrRule^Abstract(Literal, Level) If(v*(AbstrRule, PruneData)&gt;Score) Update(Score) WinRule=AbstrRule, Improvement=true Return WinRule Experimental setup: Experiments were performed on the benchmark dataset Reuters perimental setup used in [6], only the ten biggest classes in the dataset were used. As in [6], only the 300 best features were used as inputs to the classifier. The experiments compare RIPPER with TRIPPER (G+P) . The text-specific taxonomies used for our experiments on the Reuters dataset comes from WordNet[3], using only the hy-pernimy relation that stands for  X  X sa X  relation between concepts. Results: Our experiments show that: (a) TRIPPER (G+P) outperforms, or matches RIPPER in terms of break-even point on the Reuters dataset (Table 3-1) in a majority (8 out of 10) of classes; (b) TRIPPER generates more abstract (and often more com-prehensible) rules than RIPPER: Table 3-2 shows some of the abstract literals discov-TRIPPER(G+P) are often more concise than those generated by RIPPER (results not shown) [11]. Trip. 86.3 85.7 82.5 95.1 87.9 71.5 70.4 80.9 58.9 84.5 Ripp. 85.3 83.9 79.3 94 90.6 58.7 65.3 73 68.3 83 Class subject Ab stract literals 
Crude Oil assets, chemical_phenomenon, chemical_element, finan-Money, Foreign 
Exchange 
Trade assembly, assets, calendar_month, change_of_magnitude, 
The usefulness of abstraction is confirmed by the prevalence of abstract literals in that both of the extensions are useful. TRIPPER is a taxonomy-based extension of the popular rule-induction algorithm RIPPER [1]. The key ingredients of TRIPPER are: the use of an augmented set of features based on taxonomies defined over values of the original features (WordNet in the case of text classification) in the growth phase and the replacement of pruning, as an overfitting avoidance method, with the more general method of abstraction guided show that TRIPPER generally outperforms RIPPER on the Reuters text classification task in terms of break-even points, while generating potentially more comprehensible rule sets than RIPPER. It is worth noting that on the Reuters dataset, TRIPPER slightly outperforms WTNBL [2] in terms of break-even points on 7 out of 10 classes. The additional computation cost of TRIPPER is small when compared with RIPPER, consisting in an additional multiplicative factor that represents the height of the largest taxonomy, which in the average case scales logarithmically with the num-ber of feature values. 
