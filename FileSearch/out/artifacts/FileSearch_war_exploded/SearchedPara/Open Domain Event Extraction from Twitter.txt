 Tweets are the most up-to-date and inclusive stream of in-formation and commentary on current events, but they are also fragmented and noisy, motivating the need for systems that can extract, aggregate and categorize important events. Previous work on extracting structured representations of events has focused largely on newswire text; Twitter X  X  unique characteristics present new challenges and opportunities for open-domain event extraction. This paper describes TwiCal  X  the first open-domain event-extraction and categorization system for Twitter. We demonstrate that accurately ex-tracting an open-domain calendar of significant events from Twitter is indeed feasible. In addition, we present a novel approach for discovering important event categories and clas-sifying extracted events based on latent variable models. By leveraging large volumes of unlabeled data, our approach achieves a 14% increase in maximum F1 over a supervised baseline. A continuously updating demonstration of our sys-tem can be viewed at http://statuscalendar.com ;Our NLP tools are available at http://github.com/aritter/ twitter_nlp .
 I.2.7 [ Natural Language Processing ]: Language pars-ing and understanding; H.2.8 [ Database Management ]: Database applications X  data mining Algorithms, Experimentation
Social networking sites such as Facebook and Twitter present the most up-to-date information and buzz about current This work was conducted at the University of Washington
Table 1: Examples of events extracted by TwiCal . events. Yet the number of tweets posted daily has recently exceeded two-hundred million, many of which are either re-dundant [57], or of limited interest, leading to information overload. 1 Clearly, we can benefit from more structured rep-resentations of events that are synthesized from individual tweets.

Previous work in event extraction [21, 1, 54, 18, 43, 11, 7] has focused largely on news articles, as historically this genre of text has been the best source of information on cur-rent events. In the meantime, social networking sites such as Facebook and Twitter have become an important com-plementary source of such information. While status mes-sages contain a wealth of useful information, they are very disorganized motivating the need for automatic extraction, aggregation and categorization. Although there has been much interest in tracking trends or memes in social media [26, 29], little work has addressed the challenges arising from extracting structured representations of events from short or informal texts.

Extracting useful structured representations of events from this disorganized corpus of noisy text is a challenging prob-lem. On the other hand, individual tweets are short and self-contained and are therefore not composed of complex discourse structure as is the case for texts containing nar-ratives. In this paper we demonstrate that open-domain event extraction from Twitter is indeed feasible, for exam-ple our highest-confidence extracted future events are 90% accurate as demonstrated in  X  8.

Twitter has several characteristics which present unique challenges and opportunities for the task of open-domain event extraction.

Challenges: Twitter users frequently mention mundane events in their daily lives (such as what they ate for lunch) which are only of interest to their immediate social network. Incontrast,ifaneventismentionedinnewswiretext,it http://blog.twitter.com/2011/06/ 200-million-tweets-per-day.html is safe to assume it is of general importance. Individual tweets are also very terse, often lacking sufficient context to categorize them into topics of interest (e.g. Sports , Pol-itics , ProductRelease etc...). Further because Twitter users can talk about whatever they choose, it is unclear in advance which set of event types are appropriate. Finally, tweets are written in an informal style causing NLP tools designed for edited texts to perform extremely poorly.
Opportunities: The short and self-contained nature of tweets means they have very simple discourse and pragmatic structure, issues which still challenge state-of-the-art NLP systems. For example in newswire, complex reasoning about relations between events (e.g. before and after ) is often re-quired to accurately relate events to temporal expressions [32, 8]. The volume of Tweets is also much larger than the volume of news articles, so redundancy of information can be exploited more easily.

To address Twitter X  X  noisy style, we follow recent work on NLP in noisy text [46, 31, 19], annotating a corpus of Tweetswithevents,whichisthenusedastrainingdatafor sequence-labeling models to identify event mentions in mil-lions of messages.

Because of the terse, sometimes mundane, but highly re-dundant nature of tweets, we were motivated to focus on extracting an aggregate representation of events which pro-vides additional context for tasks such as event categoriza-tion, and also filters out mundane events by exploiting re-dundancy of information. We propose identifying important events as those whose mentions are strongly associated with references to a unique date as opposed to dates which are evenly distributed across the calendar.

Twitter users discuss a wide variety of topics, making it unclear in advance what set of event types are appropri-ate for categorization. To address the diversity of events discussed on Twitter, we introduce a novel approach to dis-covering important event types and categorizing aggregate events within a new domain.

Supervised or semi-supervised approaches to event catego-rization would require first designing annotation guidelines (including selecting an appropriate set of types to annotate), then annotating a large corpus of events found in Twitter. This approach has several drawbacks, as it is apriori unclear what set of types should be annotated; a large amount of effort would be required to manually annotate a corpus of events while simultaneously refining annotation standards.
We propose an approach to open-domain event catego-rization based on latent variable models that uncovers an appropriate set of types which match the data. The au-tomatically discovered types are subsequently inspected to filter out any which are incoherent and the rest are anno-tated with informative labels; 2 examples of types discovered using our approach are listed in figure 3. The resulting set of types are then applied to categorize hundreds of millions of extracted events without the use of any manually annotated examples. By leveraging large quantities of unlabeled data, our approach results in a 14% improvement in F 1 score over a supervised baseline which uses the same set of types.
This annotation and filtering takes minimal effort. One of the authors spent roughly 30 minutes inspecting and anno-tating the automatically discovered event types.
 Table 2: By training on in-domain data, we obtain a 52% improvement in F 1 score over the Stanford Named Entity Recognizer at segmenting entities in Tweets [46].
TwiCal extracts a 4-tuple representation of events which includes a named entity, event phrase, calendar date, and event type (see Table 1). This representation was chosen to closely match the way important events are typically men-tioned in Twitter.

An overview of the various components of our system for extracting events from Twitter is presented in Figure 1.Givenarawstreamoftweets,oursystemextractsnamed entities in association with event phrases and unambigu-ous dates which are involved in significant events. First the tweets are POS tagged, then named entities and event phrases are extracted, temporal expressions resolved, and the extracted events are categorized into types. Finally we measure the strength of association between each named en-tity and date based on the number of tweets they co-occur in,inordertodeterminewhetheraneventissignificant.
NLP tools, such as named entity segmenters and part of speech taggers which were designed to process edited texts (e.g. news articles) perform very poorly when applied to Twitter text due to its noisy and unique style. To address these issues, we utilize a named entity tagger and part of speech tagger trained on in-domain Twitter data presented in previous work [46]. We also develop an event tagger trained on in-domain annotated data as described in  X  4.
NLP tools, such as named entity segmenters and part of speech taggers which were designed to process edited texts (e.g. news articles) perform very poorly when applied to Twitter text due to its noisy and unique style.

For instance, capitalization is a key feature for named en-tity extraction within news, but this feature is highly un-reliable in tweets; words are often capitalized simply for emphasis, and named entities are often left all lowercase. In addition, tweets contain a higher proportion of out-of-vocabulary words, due to Twitter X  X  140 character limit and thecreativespellingofitsusers.

To address these issues, we utilize a named entity tag-ger trained on in-domain Twitter data presented in previous work [46]. 3
Training on tweets vastly improves performance at seg-menting Named Entities. For example, performance com-pared against the state-of-the-art news-trained Stanford Named Entity Recognizer [17] is presented in Table 2. Our system obtains a 52% increase in F 1 score over the Stanford Tagger at segmenting named entities.
In order to extract event mentions from Twitter X  X  noisy text, we first annotate a corpus of tweets, which is then
Available at http://github.com/aritter/twitter_nlp . work are shaded in grey. used to train sequence models to extract events. While we apply an established approach to sequence-labeling tasks in noisy text [46, 31, 19], this is the first work to extract event-referring phrases in Twitter.

Event phrases can consist of many different parts of speech as illustrated in the following examples:
These phrases provide important context, for example ex-tracting the entity, Steve Jobs and the event phrase died in connection with October 5th, is much more informative than simply extracting Steve Jobs . In addition, event mentions are helpful in upstream tasks such as categorizing events into types, as described in  X  6.

In order to build a tagger for recognizing events, we anno-tated 1,000 tweets (19,484 tokens) with event phrases, fol-lowing annotation guidelines similar to those developed for the Event tags in Timebank [43]. We treat the problem of recognizing event triggers as a sequence labeling task, us-ing Conditional Random Fields for learning and inference [24]. Linear Chain CRFs model dependencies between the predicted labels of adjacent words, which is beneficial for ex-tracting multi-word event phrases. We use contextual, dic-tionary, and orthographic features, and also include features based on our Twitter-tuned POS tagger [46], and dictionar-ies of event terms gathered from WordNet by Sauri et al. [50].

The precision and recall at segmenting event phrases are reported in Table 3. Our classifier, TwiCal-Event , obtains an F-score of 0.64. To demonstrate the need for in-domain training data, we compare against a baseline of training our system on the Timebank corpus.
In addition to extracting events and related named enti-ties, we also need to extract when they occur. In general there are many different ways users can refer to the same calendar date, for example  X  X ext Friday X ,  X  X ugust 12th X ,  X  X omorrow X  or  X  X esterday X  could all refer to the same day, depending on when the tweet was written. To resolve tem-poral expressions we make use of TempEx [33], which takes Table 3: Precision and recall at event phrase ex-traction. All results are reported using 4-fold cross validation over the 1,000 manually annotated tweets (about 19K tokens). We compare against a system which doesn X  X  make use of features generated based on our Twitter trained POS Tagger, in addition to a system trained on the Timebank corpus which uses the same set of features. as input a reference date, some text, and parts of speech (from our Twitter-trained POS tagger) and marks tempo-ral expressions with unambiguous calendar references. Al-though this mostly rule-based system was designed for use on newswire text, we find its precision on Tweets (94% -estimated over as sample of 268 extractions) is sufficiently high to be useful for our purposes. TempEx X  X  high precision on Tweets can be explained by the fact that some tempo-ral expressions are relatively unambiguous. Although there appears to be room for improving the recall of temporal extraction on Twitter by handling noisy temporal expres-sions (for example see Ritter et. al. [46] for a list of over 50 spelling variations on the word  X  X omorrow X ), we leave adapting temporal extraction to Twitter as potential future work.
To categorize the extracted events into types we propose an approach based on latent variable models which infers an appropriate set of event types to match our data, and also classifies events into types by leveraging large amounts of unlabeled data.

Supervised or semi-supervised classification of event cat-egories is problematic for a number of reasons. First, it is a priori unclear which categories are appropriate for Twitter. Secondly, a large amount of manual effort is required to an-notate tweets with event types. Third, the set of important categories (and entities) is likely to shift over time, or within a focused user demographic. Finally many important cat-egories are relatively infrequent, so even a large annotated dataset may contain just a few examples of these categories, making classification difficult.

For these reasons we were motivated to investigate un-Sports 7.45% Party 3.66% TV 3.04% Politics 2.92% Celebrity 2.38% Music 1.96% Movie 1.92% Food 1.87% Concert 1.53% Performance 1.42% Fitness 1.11% Interview 1.01% ProductRelease 0.95% Meeting 0.88% Fashion 0.87% Finance 0.85% School 0.85% AlbumRelease 0.78% Religion 0.71% Figure 2: Complete list of automatically discovered event types with percentage of data covered. Inter-pretable types representing significant events cover roughly half of the data. supervised approaches that will automatically induce event types which match the data. We adopt an approach based on latent variable models inspired by recent work on modeling selectional preferences [47, 39, 22, 52, 48], and unsupervised information extraction [4, 55, 7].

Each event indicator phrase in our data, e , is modeled as a mixture of types. For example the event phrase  X  X heered X  mightappearaspartofeithera PoliticalEvent ,ora SportsEvent . Each type corresponds to a distribution over named entities n involved in specific instances of the type, in addition to a distribution over dates d onwhicheventsofthe type occur. Including calendar dates in our model has the effect of encouraging (though not requiring) events which occur on the same date to be assigned the same type. This is helpful in guiding inference, because distinct references to the same event should also have the same type.

The generative story for our data is based on LinkLDA [15], and is presented as Algorithm 1. This approach has the advantage that information about an event phrase X  X  type distribution is shared across it X  X  mentions, while ambiguity is also naturally preserved. In addition, because the approach is based on generative a probabilistic model, it is straightfor-ward to perform many different probabilistic queries about the data. This is useful for example when categorizing ag-gregate events.

For inference we use collapsed Gibbs Sampling [20] where each hidden variable, z i , is sampled in turn, and parameters are integrated out. Example types are displayed in Figure 3. To estimate the distribution over types for a given event, a sample of the corresponding hidden variables is taken from the Gibbs markov chain after sufficient burn in. Prediction for new data is performed using a streaming approach to inference [56].
To evaluate the ability of our model to classify significant events, we gathered 65 million extracted events of the form Figure 3: Example event types discovered by our model. For each type t , we list the top 5 entities which have highest probability given t ,andthe5 event phrases which assign highest probability to t . Algorithm 1 Generative story for our data involving event types as hidden variables. Bayesian Inference techniques are applied to invert the generative process and infer an appropriate set of types to describe the observed events. listed in Figure 1 (not including the type). We then ran Gibbs Sampling with 100 types for 1,000 iterations of burn-in, keeping the hidden variable assignments found in the last sample. 4
One of the authors manually inspected the resulting types and assigned them labels such as Sports , Politics , Musi-cRelease and so on, based on their distribution over enti-ties, and the event words which assign highest probability to that type. Out of the 100 types, we found 52 to correspond to coherent event types which referred to significant events; the other types were either incoherent, or covered types of events which are not of general interest, for example there was a cluster of phrases such as applied , call , contact , job interview , etc... which correspond to users discussing events related to searching for a job. Such event types which do not correspond to significant events of general interest were simply marked as OTHER . A complete list of labels used to annotate the automatically discovered event types along with the coverage of each type is listed in figure 2. Note that this assignment of labels to types only needs to be done once and produces a labeling for an arbitrarily large number of event instances. Additionally the same set of types can eas-ily be used to classify new event instances using streaming inference techniques [56]. One interesting direction for fu-ture work is automatic labeling and coherence evaluation of automatically discovered event types analogous to recent work on topic models [38, 25].

In order to evaluate the ability of our model to classify aggregate events, we grouped together all (entity,date) pairs which occur 20 or more times the data, then annotated the 500 with highest association (see  X  7) using the event types discovered by our model.

To help demonstrate the benefits of leveraging large quan-tities of unlabeled data for event classification, we com-pare against a supervised Maximum Entropy baseline which makes use of the 500 annotated events using 10-fold cross validation. For features, we treat the set of event phrases
To scale up to larger datasets, we performed inference in parallel on 40 cores using an approximation to the Gibbs Sampling procedure analogous to that presented by New-mann et. al. [37].
After labeling some types were combined resulting in 37 distinct labels.
 Table 4: Precision and recall of event type catego-rization at the point of maximum F 1 score. Figure 4: Precision and recall predicting event types. that co-occur with each (entity, date) pair as a bag-of-words, and also include the associated entity. Because many event categories are infrequent, there are often few or no training examples for a category, leading to low performance.
Figure 4 compares the performance of our unsupervised approach to the supervised baseline, via a precision-recall curve obtained by varying the threshold on the probability of the most likely type. In addition table 4 compares preci-sion and recall at the point of maximum F-score. Our un-supervised approach to event categorization achieves a 14% increase in maximum F 1 score over the supervised baseline.
Figure 5 plots the maximum F 1 score as the amount of training data used by the baseline is varied. It seems likely that with more data, performance will reach that of our ap-proach which does not make use of any annotated events, however our approach both automatically discovers an ap-propriate set of event types and provides an initial classifier with minimal effort, making it useful as a first step in situ-ations where annotated data is not immediately available.
Simply using frequency to determine which events are sig-nificant is insufficient, because many tweets refer to common events in user X  X  daily lives. As an example, users often men-tion what they are eating for lunch, therefore entities such as McDonalds occur relatively frequently in association with references to most calendar days. Important events can be distinguished as those which have strong association with a unique date as opposed to being spread evenly across days on the calendar. To extract significant events of general in-terest from Twitter, we thus need some way to measure the strength of association between an entity and a date.
In order to measure the association strength between an Figure 5: Maximum F 1 score of the supervised base-line as the amount of training data is varied. entity and a specific date, we utilize the G 2 log likelihood ratio statistic. G 2 has been argued to be more appropriate for text analysis tasks than  X  2 [12]. Although Fisher X  X  Ex-act test would produce more accurate p-values [34], given the amount of data with which we are working (sample size greaterthan10 11 ), it proves difficult to compute Fisher X  X  Exact Test Statistic, which results in floating point overflow even when using 64-bit operations. The G 2 test works suffi-ciently well in our setting, however, as computing association between entities and dates produces less sparse contingency tables than when working with pairs of entities (or words).
The G 2 test is based on the likelihood ratio between a model in which the entity is conditioned on the date, and a model of independence between entities and date references. Foragivenentity e and date d this statistic can be computed as follows: Where O e,d is the observed fraction of tweets containing both e and d , O e,  X  d is the observed fraction of tweets con-taining e , but not d , and so on. Similarly E e,d is the expected fractionoftweetscontainingboth e and d assumingamodel of independence.
To estimate the quality of the calendar entries generated using our approach we manually evaluated a sample of the top 100, 500 and 1,000 calendar entries occurring within a 2-week future window of November 3rd.
For evaluation purposes, we gathered roughly the 100 mil-lion most recent tweets on November 3rd 2011 (collected us-ing the Twitter Streaming API 6 , and tracking a broad set of temporal keywords, including X  X oday X , X  X omorrow X , names of weekdays, months, etc.).

We extracted named entities in addition to event phrases, and temporal expressions from the text of each of the 100M https://dev.twitter.com/docs/streaming-api tweets. We then added the extracted triples to the dataset used for inferring event types described in  X  6, and performed 50 iterations of Gibbs sampling for predicting event types on the new data, holding the hidden variables in the origi-nal data constant. This streaming approach to inference is similar to that presented by Yao et al. [56].

We then ranked the extracted events as described in  X  7, and randomly sampled 50 events from the top ranked 100, 500, and 1,000. We annotated the events with 4 separate criteria: 1. Is there a significant event involving the extracted en-2. Is the most frequently extracted event phrase informa-3. Is the event X  X  type correctly classified? 4. Are each of (1-3) correct? That is, does the event Note that if (1) is marked as incorrect for a specific event, subsequent criteria are always marked incorrect.
To demonstrate the importance of natural language pro-cessing and information extraction techniques in extracting informative events, we compare against a simple baseline which does not make use of the Ritter et. al. named en-tity recognizer or our event recognizer; instead, it considers all 1-4 grams in each tweet as candidate calendar entries, relying on the G 2 test to filter out phrases which have low association with each date.
The results of the evaluation are displayed in table 5. The table shows the precision of the systems at different yield levels (number of aggregate events). These are obtained by varying the thresholds in the G 2 statistic. Note that the baseline is only comparable to the third column, i.e., the precision of (entity, date) pairs, since the baseline is not performing event identification and classification. Although in some cases ngrams do correspond to informative calendar entries, the precision of the ngram baseline is extremely low compared with our system.

In many cases the ngrams don X  X  correspond to salient en-tities related to events; they often consist of single words which are difficult to interpret, for example X  X reaking X  X hich is part of the movie  X  X wilight: Breaking Dawn X  released on November 18. Although the word  X  X reaking X  has a strong association with November 18, by itself it is not very infor-mative to present to a user. 7
Our high-confidence calendar entries are surprisingly high quality. If we limit the data to the 100 highest ranked calen-dar entries over a two-week date range in the future, the pre-cision of extracted (entity, date) pairs is quite good (90%) -an 80% increase over the ngram baseline. As expected precision drops as more calendar entries are displayed, but
In addition, we notice that the ngram baseline tends to produce many near-duplicate calendar entries, for exam-ple:  X  X wilight Breaking X ,  X  X reaking Dawn X , and  X  X wilight Breaking Dawn X . While each of these entries was annotated as correct, it would be problematic to show this many entries describing the same event to a user. on Nov 11th. include event phrases or types. remains high enough to display to users (in a ranked list). In addition to being less likely to come from extraction errors, highly ranked entity/date pairs are more likely to relate to popular or important events, and are therefore of greater interest to users.

In addition we present a sample of extracted future events on a calendar in figure 6 in order to give an example of how they might be presented to a user. We present the top 5 entities associated with each date, in addition to the most frequently extracted event phrase, and highest probability event type.
We found 2 main causes for why entity/date pairs were un-informative for display on a calendar, which occur in roughly equal proportion: Segmentation Errors Some extracted X  X ntities X  X r ngrams Weak Association between Entity and Date In some
While we are the first to study open domain event ex-traction within Twitter, there are two key related strands of research: extracting specific types of events from Twitter, and extracting open-domain events from news [43].
Recently there has been much interest in information ex-traction and event identification within Twitter. Benson et al. [5] use distant supervision to train a relation extractor which identifies artists and venues mentioned within tweets of users who list their location as New York City. Sakaki et al. [49] train a classifier to recognize tweets reporting earthquakes in Japan; they demonstrate their system is ca-pable of recognizing almost all earthquakes reported by the Japan Meteorological Agency. Additionally there is recent work on detecting events or tracking topics [29] in Twitter which does not extract structured representations, but has the advantage that it is not limited to a narrow domain. Petrovi  X  c et al. investigate a streaming approach to identi-fying Tweets which are the first to report a breaking news story using Locally Sensitive Hash Functions [40]. Becker et al. [3], Popescu et al. [42, 41] and Lin et al. [28] investigate discovering clusters of related words or tweets which corre-spond to events in progress. In contrast to previous work on Twitter event identification, our approach is independent of event type or domain and is thus more widely applica-ble. Additionally, our work focuses on extracting a calendar of events (including those occurring in the future), extract-ing event-referring expressions and categorizing events into types.

Also relevant is work on identifying events [23, 10, 6], and extracting timelines [30] from news articles. 8 Twitter status messages present both unique challenges and oppor-tunities when compared with news articles. Twitter X  X  noisy text presents serious challenges for NLP tools. On the other hand, it contains a higher proportion of references to present and future dates. Tweets do not require complex reasoning about relations between events in order to place them on a timeline as is typically necessary in long texts contain-ing narratives [51]. Additionally, unlike News, Tweets often discus mundane events which are not of general interest, so it is crucial to exploit redundancy of information to assess whetheraneventissignificant.

Previous work on open-domain information extraction [2, 53, 16] has mostly focused on extracting relations (as op-posed to events) from web corpora and has also extracted relations based on verbs. In contrast, this work extracts events, using tools adapted to Twitter X  X  noisy text, and ex-tracts event phrases which are often adjectives or nouns, for example: Super Bowl Party on Feb 5th .

Finally we note that there has recently been increasing interest in applying NLP techniques to short informal mes-sages such as those found on Twitter. For example, recent work has explored Part of Speech tagging [19], geographical variation in language found on Twitter [13, 14], modeling informal conversations [44, 45, 9], and also applying NLP techniques to help crisis workers with the flood of informa-tion following natural disasters [35, 27, 36].
We have presented a scalable and open-domain approach to extracting and categorizing events from status messages. We evaluated the quality of these events in a manual evalu-ation showing a clear improvement in performance over an ngram baseline
We proposed a novel approach to categorizing events in an open-domain text genre with unknown types. Our ap-proach based on latent variable models first discovers event types which match the data, which are then used to classify aggregate events without any annotated examples. Because this approach is able to leverage large quantities of unlabeled data, it outperforms a supervised baseline by 14%.
A possible avenue for future work is extraction of even richer event representations, while maintaining domain in-dependence. For example: grouping together related enti-ties, classifying entities in relation to their roles in the event, thereby, extracting a frame-based representation of events.
A continuously updating demonstration of our system can be viewed at http://statuscalendar.com ;OurNLPtools are available at http://github.com/aritter/twitter_nlp . http://newstimeline.googlelabs.com/
The authors would like to thank Luke Zettlemoyer and the anonymous reviewers for helpful feedback on a previous draft. This research was supported in part by NSF grant IIS-0803481 and ONR grant N00014-08-1-0431 and carried out at the University of Washington X  X  Turing Center. [1] J.Allan,R.Papka,andV.Lavrenko.On-linenew [2] M. Banko, M. J. Cafarella, S. Soderl, M. Broadhead, [3] H. Becker, M. Naaman, and L. Gravano. Beyond [4] C. Bejan, M. Titsworth, A. Hickl, and S. Harabagiu. [5] E. Benson, A. Haghighi, and R. Barzilay. Event [6] S. Bethard and J. H. Martin. Identification of event [7] N. Chambers and D. Jurafsky. Template-based [8] N. Chambers, S. Wang, and D. Jurafsky. Classifying [9] C. Danescu-Niculescu-Mizil, M. Gamon, and [10] A. Das Sarma, A. Jain, and C. Yu. Dynamic [11] G. Doddington, A. Mitchell, M. Przybocki, [12] T. Dunning. Accurate methods for the statistics of [13] J. Eisenstein, B. O X  X onnor, N. A. Smith, and E. P. [14] J. Eisenstein, N. A. Smith, and E. P. Xing.
 [15] E. Erosheva, S. Fienberg, and J. Lafferty.
 [16] A. Fader, S. Soderland, and O. Etzioni. Identifying [17] J. R. Finkel, T. Grenager, and C. Manning.
 [18] E. Gabrilovich, S. Dumais, and E. Horvitz.
 [19] K. Gimpel, N. Schneider, B. O X  X onnor, D. Das, [20] T. L. Griffiths and M. Steyvers. Finding scientific [21] R. Grishman and B. Sundheim. Message [22] Z. Kozareva and E. Hovy. Learning arguments and [23] G. Kumaran and J. Allan. Text classification and [24] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [25] J. H. Lau, K. Grieser, D. Newman, and T. Baldwin. [26] J. Leskovec, L. Backstrom, and J. Kleinberg. [27] W. Lewis, R. Munro, and S. Vogel. Crisis mt: [28] C. X. Lin, B. Zhao, Q. Mei, and J. Han. PET: a [29] J. Lin, R. Snow, and W. Morgan. Smoothing [30] X. Ling and D. S. Weld. Temporal information [31] X. Liu, S. Zhang, F. Wei, and M. Zhou. Recognizing [32] I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and [33] I. Mani and G. Wilson. Robust temporal processing of [34] R. C. Moore. On log-likelihood-ratios and the [35] R. Munro. Subword and spatiotemporal models for [36] G. Neubig, Y. Matsubayashi, M. Hagiwara, and [37] D. Newman, A. U. Asuncion, P. Smyth, and [38] D. Newman, J. H. Lau, K. Grieser, and T. Baldwin. [39] D.  X  OS  X  eaghdha. Latent variable models of selectional [40] S. Petrovi  X c, M. Osborne, and V. Lavrenko. Streaming [41] A.-M. Popescu and M. Pennacchiotti. Dancing with [42] A.-M. Popescu, M. Pennacchiotti, and D. A. Paranjpe. [43] J.Pustejovsky,P.Hanks,R.Sauri,A.See, [44] A. Ritter, C. Cherry, and B. Dolan. Unsupervised [45] A. Ritter, C. Cherry, and W. B. Dolan. Data-driven [46] A. Ritter, S. Clark, Mausam, and O. Etzioni. Named [47] A. Ritter, Mausam, and O. Etzioni. A latent dirichlet [48] K. Roberts and S. M. Harabagiu. Unsupervised [49] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [50] R. Saur  X  X , R. Knippen, M. Verhagen, and [51] F. Song and R. Cohen. Tense interpretation in the [52] B. Van Durme and D. Gildea. Topic models for [53] D. S. Weld, R. Hoffmann, and F. Wu. Using wikipedia [54] Y. Yang, T. Pierce, and J. Carbonell. A study of [55] L. Yao, A. Haghighi, S. Riedel, and A. McCallum. [56] L. Yao, D. Mimno, and A. McCallum. Efficient [57] F. M. Zanzotto, M. Pennaccchiotti, and
