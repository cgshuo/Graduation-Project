 Shivaram Kalyanakrishnan 1 shivaram@yahoo-inc.com Ambuj Tewari 2 ambuj@cs.utexas.edu Peter Auer 3 auer@unileoben.ac.at Peter Stone 2 pstone@cs.utexas.edu
Yahoo! Labs Bangalore, Bengaluru Karnataka 560071 India We consider the Explore -m problem introduced pre-viously by the authors ( Kalyanakrishnan &amp; Stone , 2010 ). The problem is that of selecting, from among the arms of a stochastic n -armed bandit, a subset of size m of those arms with the highest expected re-wards, based on efficiently sampling the arms. This subset selection problem finds application in a vari-ety of areas, such as simulation, industrial engineering, on-line advertising, and also within certain stochastic optimization techniques. Explore -m generalizes Ex-plore -1, a PAC setting introduced by Even-Dar et al. ( 2006 ) for selecting the (single) best arm. The authors X  previous work introduces the Halving algorithm for Explore -m ( Kalyanakrishnan &amp; Stone , 2010 ). While this algorithm improves upon the sam-ple complexity of a uniform sampling strategy, its sample complexity is identical across different bandit instances X  X herefore not implementing the intuition that in bandit instances where the highest m and the lowest n  X  m means of the arms are separated by a relatively large margin, fewer samples should suffice for reliably identifying the best arms. In other words, while the Halving algorithm is sufficient for achieving a PAC guarantee in the worst case, we are apt to won-der if on  X  X asier X  bandit instances, a sampling algo-rithm can finish any earlier. The question we confront is akin to one addressed by Schuurmans &amp; Greiner ( 1995 ), who show in the context of supervised learn-ing that while preserving a PAC correctness guarantee over an entire class of problems, often it is possible to improve efficiency on a given problem instance by sequentially adapting based on the samples observed. As the first contribution of this paper, we present the LUCB algorithm for Explore -m , which meets the relevant PAC requirement, and enjoys an ex-pected sample complexity bound that increases with a natural measure of problem complexity. The algo-rithm maintains a clear separation between its stop-ping rule and sampling strategy, therein contrasting with previous elimination-based approaches for explo-ration ( Even-Dar et al. , 2006 ; Mnih et al. , 2008 ). In elimination-based algorithms, it becomes difficult to ensure low sample complexity on runs in which erro-neous eliminations occur. By contrast, LUCB deter-mines the entire set of m arms to select (or n  X  m to reject) only at termination. Consequently we can analyze LUCB  X  X  sample complexity separately from its correctness, and guarantee a low expected sam-ple complexity (including runs on which mistakes are made). We believe such a bound is novel even for the single-arm case, and that the confidence bounds-based sampling approach of LUCB  X  X gain novel for a PAC setting X  X s a natural means for achieving the bound. Note that the worst case sample complexity of LUCB can be kept to within a constant factor of Halving  X  X . 1 The second contribution of this paper is indeed a lower bound on the worst case sample complexity of PAC algorithms for Explore -m . This lower bound shows that the worst case sample complexity of the Halving algorithm is optimal up to a constant factor. In Section 2 , we review Explore -m and introduce relevant terminology. In Section 3 , we describe and analyze the LUCB algorithm. Section 4 presents our lower bound, and Section 5 serves as the conclusion. Below we review the Explore -m problem ( Kalyanakrishnan &amp; Stone , 2010 ) and introduce terms related to problem complexity.
 Stochastic multi-armed bandits. We consider an arbitrary instance of an n -armed bandit, n  X  2; let its arms be numbered 1 , 2 , . . . , n . Each sample (or  X  X ull X ) of arm a yields a reward of either 0 or 1, generated ran-domly from a fixed Bernoulli distribution with mean p a  X  [0 , 1]. 2 Indeed each bandit instance is completely determined by the distributions corresponding to its arms. For simplicity of notation we assume an index-ing of the arms such that Naturally the learner is unaware of this indexing. The random variables generating rewards for the arms are mutually independent. Arm a is defined to be (  X , m )-optimal,  X   X   X  (0 , 1),  X  m  X  X  1 , 2 , . . . , n  X  1 } , iff We find it convenient to denote as Arms the set of all arms in our n -armed bandit instance; by T op the m arms with the highest mean rewards; and by Bot the n  X  m arms with the lowest mean rewards.
 We see from ( 1 ) and ( 2 ) that every arm in T op is (  X , m )-optimal. Hence, there are at least m (  X , m )-optimal arms. Let Good be the set of all (  X , m )-optimal arms, and let the set Bad contain all the remaining arms. In general: m  X | Good | X  n and 0  X | Bad | X  ( n  X  m ). E
XPLORE -m problem. A bandit instance may be sampled sequentially in rounds. On each round t  X  { 1 , 2 , . . . } , an algorithm for Explore -m must either (1) select an arm a t to sample, or (2) terminate and return an m -sized subset of Arms . The outcome of each sampling is a pair of the form (arm, reward), where the reward is drawn as an independent sample from the distribution associated with the arm. We refer to the sequence of outcomes up to (and excluding ) round t as the history at round t : during each round t , this history is available to the algorithm. Note that we do not require the m arms returned by the algorithm to be in any particular order.
 For  X   X  (0 , 1), an algorithm A is defined to be (  X , m,  X  )-optimal, iff for every bandit instance : (1) with proba-bility 1, A terminates in a finite number of rounds, and (2) with probability at least 1  X   X  , every arm returned by A is (  X , m )-optimal. Note that Explore -1, for which (  X , 1 ,  X  )-optimal algorithms are to be designed, matches the formulation of Even-Dar et al. ( 2006 ). The sample complexity of algorithm A during a ter-minating run is the total number of pulls it performs before termination. Assume that with probability 1, A indeed terminates in a finite number of rounds. Then, for a given bandit instance B , the worst case sample complexity of A is the maximum sample complexity among its runs on B , and the expected sample com-plexity the average over all its runs on B . The (over-all) worst case sample complexity of an algorithm is its maximum worst case sample complexity across all bandit instances.
 Problem complexity. Before proceeding, we define quantities to describe the complexity of the Explore -m problem for a bandit instance. Intuition suggests that the problem must be easier if the mean rewards of the arms are more separated. We denote by  X  i,j the separation between the means of arms i and j : We find it convenient to use additional shorthand for the separation of arms in T op from arm m + 1, and the separation of arms in Bot from arm m . Observe that  X  m =  X  m +1 = p m  X  p m +1 . Let c denote the mid-point of the means of arms m and m + 1; it is easy to establish a relationship between p a ,  X  a , and c . For  X   X  [0 , 1], we denote the larger of  X  a and  X  as [ X  a  X   X  ], and aggregate over all the arms to derive a complexity measure H  X  . Theorem 6 will show that the expected sample complexity of LUCB1 (an instance of LUCB ) is LUCB1 is its improved expected sample complexity on bandit instances for which H  X / 2 is relatively small (in practice we expect to mostly encounter such in-stances). By contrast, the Halving algorithm uni-formly has a sample complexity of O n  X  2 log m  X  over all bandit instances: therefore it is inefficient except on those instances for which H  X / 2  X  n  X  2 . The algorithm we introduce in this paper for subset selection naturally decomposes into two elements: (1) a stopping rule that maps the set of histories to the set { STOP , CONTINUE } , and (2) a sampling strategy that maps the set of histories to Arms . The algorithm is initialized by sampling each arm once. On every sub-sequent round, the algorithm evaluates the stopping rule. If it must stop, it returns the m -sized subset of arms with the highest empirical means (assume some fixed rule exists for breaking ties). Otherwise it pulls an arm according to the sampling strategy, and con-tinues. We name our algorithm  X  LUCB  X  based on the centrality of lower and upper confidence bounds in its stopping rule and sampling strategy. 3.1. Stopping Rule The stopping rule under LUCB is of an intuitive form. During round t , let u t a denote the number of times arm a has been sampled, and let  X  p t a be the empirical mean of the rewards from arm a . The key element of our stopping rule is a confidence bound  X  ( u t a , t ), which is a positive number interpreted to be a high-probability bound on the deviation of the empirical mean of arm a from its true mean. In particular the lower confidence bound for arm a during round t is is given by  X  p t a +  X  ( u t a , t ).
 During round t , let High t be the set of m arms with the highest empirical averages, and Low t be the set of n  X  m arms with the lowest empirical averages. Among the arms in High t , let h t  X  be the arm with the lowest lower confidence bound; among the arms in Low t , let l be the arm with the highest upper confidence bound: Our stopping rule is to terminate iff If the algorithm does stop, then High t is returned. We show that if  X  ( u, t ) is sufficiently large, then LUCB guarantees a low mistake probability (Section 3.3 pro-vides a concrete choice of  X  ).
 Theorem 1. Let S : Set of histories  X  Arms be an arbitrary sampling strategy, and let  X  : { 1 , 2 , 3 , . . . } X  { 1 , 2 , 3 , . . . } X  (0 ,  X  ) be a function such that Then, if LUCB ( n , m ,  X  ,  X  , S ,  X  ) terminates, the probability that it returns a non-(  X , m ) -optimal arm (an arm in Bad ) is at most  X  .
 Proof. We employ a standard argument using confi-dence bounds. If indeed some arm b in Bad is returned, then LUCB must have terminated during some round t such that b is present in High t (and by implica-tion, some arm i in T op is present in Low t ). Since p  X  p b &gt;  X  , from the termination rule, we infer that  X  p b &gt; p b +  X  ( u has violated its upper or lower confidence bound, re-spectively. Hence, the algorithm X  X  mistake probability is upper-bounded by the probability of the event that there exist t , b , i , u t b , and u t i such that  X  p t egy, note that u t b and u t i have to be between 1 and t  X  1. Applying Hoeffding X  X  inequality along with the union bound and ( 4 ), we obtain the desired result. 3.2. Greedy Sampling Strategy Whereas Theorem 1 applies to every sampling strat-egy, we show that it is economical to use a sampling strategy that is greedy with respect to the stopping rule. In particular during round t , the arms h  X  t and l can be construed as the arms that are most likely to lead to a mistake: naturally it would then be advisable to sample these arms instead of others. Implementing this very intuition, our sampling strategy is as follows: Below we introduce some infrastructure for our anal-ysis. Then, in Lemma 2 , we establish that indeed h t  X  and l t  X  are good candidates for sampling. Subsequently in Section 3.3 , we concretely pick a function,  X  1 , as a confidence bound, and formally bound the expected sample complexity of the resulting algorithm, which we denote LUCB1 .
 tion the set of arms into three sets: Above t , which com-prises arms whose lower confidence bounds fall above c ; Below t , which comprises arms whose upper confidence bounds fall below c ; and M iddle t , the remainder. We expect that by and large, arms in T op will be in Above t or M iddle t , while arms in Bot will be in Below t or M iddle t . Let CROSS t a denote the event that arm a does not obey such an expectation (and let CROSS t denote that some arm has  X  X rossed X ). Now, let us define a  X  X eedy X  arm as one in M iddle t with a confidence bound greater than  X  2 : let N EEDY t a be the event that arm a is needy during round t . Additionally let T ERM t denote the event that during round t , the stopping rule will lead to termination:
The following key lemma shows that if CROSS t does not occur, and LUCB does not terminate dur-ing round t , then either h t  X  or l t  X  is a needy arm. Lemma 2.  X  CROSS t  X  X  T ERM t Proof. In our proof below, we reduce notational clutter by dropping the suffix t in our variables. Additionally we use the shorthand  X  [ a ] for  X  ( u t a , t ). To prove the lemma, we prove the following statements. If neither of h  X  and l  X  is in M iddle , then these arms have to be in Above or Below . We prove ( 5 ) by consid-ering four mutually exclusive cases. Recall that h  X  has the lowest lower confidence bound in High ; l  X  has the highest upper confidence bound in Low ; and  X  p h Similarly, we show ( 6 ) by proving two disjoint cases (for Case 1 we use that  X  p h The proof for ( 7 ) is similar. To complete the proof of the lemma, we prove ( 8 ): The above lemma shows that if no arm has crossed and no arm is needy, then the LUCB algorithm must stop. Next we consider a specific confidence bound,  X  1 , for which we bound the probability of arms crossing or staying needy for long. 3.3. LUCB1 We define Algorithm LUCB1 to be an instance of LUCB that uses the stopping rule from Section 3.1 and the greedy sampling strategy in Section 3.2 , while using the following confidence bound: Note that  X  1 satisfies the requirement on  X  in ( 4 ). In the remainder of this section, we bound the expected sample complexity of LUCB1 (Lemma 5 implies that with probability 1, LUCB1 terminates in a finite num-ber of rounds, and thus, is (  X , m,  X  )-optimal). Lemma 3. Under LUCB1 : P { CROSS t } X   X  k Proof. The proof follows directly from the definition of CROSS t , and with applications of Hoeffding X  X  in-equality and the union bound (over arms, and over the possible number of pulls for each arm).
 For sufficiently large t , we define u  X  1 ( a, t ) as an adequate number of samples of arm a such that  X  ( u  X  1 ( a, t ) , t ) is no greater than [ X  a  X   X  2 ]: The following lemma then shows that the probability that arm a remains needy despite being sampled for more than 4 u  X  1 ( a, t ) rounds is small.
 Lemma 4. Under LUCB1 : Proof. Consider an arm a in Arms . If  X  a  X   X  2 , we implies  X  N EEDY t a . Now, let us consider the case that  X  a &gt;  X  2 , which is less trivial. Without loss of generality, we may assume that a  X  T op . Then, by substituting for  X  1 , and using ( 3 ), we get: The derivation for the last step is in Kalyanakrishnan X  X  Ph.D. thesis ( 2011 , see Appendix B.2). From ( 9 ): We now combine the results of Lemma 3 and Lemma 4 to upper-bound the probability that LUCB does not terminate beyond a certain number of rounds T  X  1 . Lemma 5. Let T  X  1 = l 146 H  X / 2 ln H  X / 2  X  m . For every T  X  T  X  1 , the probability that LUCB1 has not termi-nated after T rounds of sampling is at most 4  X  T 2 . Proof. Let T = T 2 . We define two events, E 1 and E , over the interval { T , T + 1 , . . . , T  X  1 } : We show that if neither E 1 nor E 2 occurs, then LUCB1 must necessarily terminate after at most T rounds. If the algorithm terminates after some t  X  T rounds, there is nothing left to prove. Consider the case that the algorithm has not terminated after T rounds, and neither E 1 nor E 2 occurs. In this case, let # rounds be the number of additional rounds of sampling, up to round T . Applying Lemma 2 , we get: It is seen that T  X  T  X  1 =  X  T &gt; 2+8 P a  X  Arms u  X  1 ( Kalyanakrishnan , 2011 , see Appendix B.3). Thus, if neither E 1 nor E 2 occurs, the total number of rounds for which LUCB1 lasts is at most T + # rounds  X  probability that LUCB1 has not terminated after T rounds can be upper-bounded by P { E 1  X  E 2 } . Apply-ing Lemma 3 and Lemma 4 , we obtain: Lemma 5 directly yields a bound on the expected sam-ple complexity, and a related high-probability bound. Theorem 6. The expected sample complexity of Corollary 7. With probability at least 1  X   X  , LUCB1 Proof. From Lemma 5 , it follows that the ex-pected sample complexity of LUCB1 is at most corollary follows trivially from Lemma 5 .
 To the best of our knowledge, the expected sample complexity bound in Theorem 6 is novel even for the m = 1 case. For Explore -1, Even-Dar et al. ( 2006 , see Remark 9) do provide a high-probability bound that essentially matches our bound in Corollary 7 . However, their elimination algorithm could incur high sample complexity on the  X  -fraction of the runs on which mistakes are made X  X e think it unlikely that elimination algorithms can yield an expected sample complexity bound smaller than  X  H  X / 2 log n  X  X  . We note that it is easy to restrict the worst case (and therefore, the expected) sample complexity of LUCB1 to O n  X  2 log m  X  by running it with  X   X  =  X  2 , and if it does not terminate after O n  X  2 log m  X  rounds, restarting and running Halving instead (again with  X  =  X  2 ). Under such a scheme, the mistake probability is at most  X  ; the expected sample complexity is within a constant factor of LUCB  X  X ; and the worst case sam-ple complexity within a constant factor of Halving  X  X . The remainder of this section is devoted to proving that indeed the worst case sample complexity of Halv-ing is optimal up to a constant factor. Section 4.2.1 in our proof is based on a similar proof provided by Mannor &amp; Tsitsiklis ( 2004 , see Section 3) for m = 1. In Section 4.2.2 we present a novel way to aggregate er-ror terms from different bandit instances, which is key for getting an  X  (log ( m )) dependence in our bound. Theorem 8. For 0 &lt;  X   X  q 1 32 , 0 &lt;  X   X  1 4 , m  X  6 , n  X  2 m , and every (  X , m,  X  ) -optimal algorithm A , there is a bandit instance on which A has a worst case To prove the theorem, we consider two sets of bandit instances ( I m  X  1 and I m ), and an (  X , m,  X  )-optimal al-gorithm A . We show that if A has a low worst case sample complexity on instances in I m  X  1 , it must have a high mistake probability on some instance in I m , thereby contradicting that it is (  X , m,  X  )-optimal. In this section, we drop the convention that arms are in-dexed in non-increasing order of their means. Also, we index the arms 0 , 1 , . . . , n  X  1. 4.1. Bandit Instances For l = m  X  1 , m , let I l be the set of all l -sized subsets of arms, excluding arm 0; that is: For I  X  X  1 , 2 , . . . , n  X  1 } we define by the set of remaining arms, again excluding arm 0. For each I  X  I m  X  1  X  X  m , we associate an n -armed bandit instance B I , in which each arm a yields rewards from a Bernoulli distribution with mean as follows: Observe that every instance in I m  X  1 and I m has ex-actly m (  X , m )-optimal arms. In the former case, arm 0 is among these (  X , m )-optimal arms, while in the latter, it is not. We build on the intuition that it is difficult for an algorithm to recognize this distinction without sampling the arms a sufficiently large number of times. 4.2. Bound To derive our bound, we make the following assump-tion, and then proceed to establish a contradiction. Assumption 9. Assume there is an (  X , m,  X  ) -optimal algorithm A that for each bandit problem B I , I  X  I m  X  1 , has sample complexity of at most C n  X  2 ln m 8  X  We denote by P I the probability distribution that re-sults from the bandit problem B I and possible random-ization of the sampling algorithm A . Also we denote by S A the set of arms that A returns, and by T i the number of times algorithm A samples arm i before terminating. Then, for all I  X  X  m  X  1 , since A is (  X , m,  X  )-optimal. From the bound on the sample complexity, we have for all I  X  X  m  X  1 that 4.2.1. Changing P I to P I  X  X  j } Consider a fixed I  X  I m  X  1 . From ( 11 ) we get that there are at most n 4 arms j  X   X  I with E I [ T j ]  X  C n  X  m  X  n 4  X  ( n  X  m ) 2 arms j  X   X  I with E I [ T j ] &lt; K j denote the sum of rewards received for arm j . Lemma 10. Let I  X  X  m  X  1 and j  X   X  I . Then, Proof. Let K j ( t ) denote the sum of rewards of arm j after t trials of arm j . Kolmogorov X  X  inequality gives P Thus: Lemma 11. Let I  X  X  m  X  1 and j  X   X  I . Let W be some fixed sequence of outcomes (or  X  X un X ) of algorithm A with T j  X  T  X  and K j  X  T j 2  X   X  . Then: Proof. We need to bound the decrease in the proba-bility of observing the outcome sequence W when the mean reward of arm j is changed. Since the mean re-ward of arm j is higher in the bandit problem B I  X  X  j } , the probability of W is decreased the most when in W , only few 1-rewards are received for arm j . Thus, Lemma 12. If ( 12 ) holds for I  X  I m  X  1 and j  X   X  I , then for any set W of sequences of outcomes W , In particular, Proof. In this proof, we use the explicit notation T W j and K W j to denote the number of trials and number of 1-rewards, respectively, of arm j for some fixed sequence of outcomes W . By applying Lemma 11 , Lemma 10 , and ( 12 ) in sequence, we get: The last step follows from the observation that for  X   X  1 4 , m  X  6, and C = 1 18375 , we have 32  X   X  &lt; ln m 8  X  . To obtain ( 13 ), observe from ( 10 ) that P I { S A = I  X  X  0 }} X  1  X   X   X  3 4 .
 4.2.2. Summing over I m  X  1 and I m Now we present the main innovation in our proof for generalizing the lower bound from Explore -1 to Ex-plore -m . We carefully aggregate error terms over bandit instances in I m  X  1 and I m to show that if As-sumption 9 is true, then some bandit instance in I m must exhibit a mistake probability in excess of  X  . First we obtain the following inequality.
 Next we note that for I  X  X  m  X  1 , ( 13 ) holds for at least 2 arms j  X  for these arms yields Hence, in contradiction to Assumption 9 , there exists a bandit instance J  X  X  m with P J { S A 6 = J } &gt;  X  . We have addressed the problem of subset selection, which generalizes the problem of single-arm selection. Under a PAC setting, we have presented the LUCB algorithm and provided a novel expected sample com-plexity bound for subset (and single-arm) selection. We have also given a worst case sample complexity lower bound for subset selection.
 It is interesting to note the similarity between the LUCB and UCB ( Auer et al. , 2002 ) algorithms. Sam-pling greedily with respect to suitable lower and up-per confidence bounds, LUCB achieves the best-known PAC bounds; being greedy with respect to upper con-fidence bounds alone, UCB yields minimal cumulative regret. It would be worthwhile to investigate if there is a deeper connection between these two algorithms and their respective settings.
 We conjecture that the expected sample complexity of (  X , m,  X  )-optimal algorithms for Explore -m is at least  X  H  X / 2 log 1  X  , which would indicate that H  X / 2 is in-deed an appropriate measure of problem complexity. However, at present we are unaware of a way to im-prove the O H  X / 2 log H  X / 2  X  upper bound achieved by LUCB1 , unless H  X / 2 is provided as input to the algorithm. On a closely-related exploration problem, Audibert et al. ( 2010 ) also observe that a tighter up-per bound can be achieved if the problem complexity (a quantity similar to H  X / 2 ) is known beforehand. A major portion of Shivaram Kalyanakrishnan X  X  con-tribution to this paper was made when he was a grad-uate student in the Department of Computer Science, The University of Texas at Austin. The authors are grateful to anonymous reviewers for their comments. Peter Stone is supported in part by NSF (IIS-0917122), ONR (N00014-09-1-0658), and FHWA (DTFH61-07-H-00030).

