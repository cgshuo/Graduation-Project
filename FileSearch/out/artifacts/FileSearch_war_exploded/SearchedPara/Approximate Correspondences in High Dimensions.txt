 When a single data object is described by a set of feature vecto rs, it is often useful to consider the matching or  X  X orrespondence X  between two sets X  element s in order to measure their overall similarity or recover the alignment of their parts. For exam ple, in computer vision, images are often represented as collections of local part descriptions extr acted from regions or patches (e.g., [11, 12]), and many recognition algorithms rely on establishing the co rrespondence between the parts from two images to quantify similarity between objects or locali ze an object within the image [2, 3, 7]. Likewise, in text processing, a document may be represented as a bag of word-feature vectors; for example, Latent Semantic Analysis can be used to recover a  X  X  ord meaning X  subspace on which to project the co-occurrence count vectors for every word [9 ]. The relationship between documents may then be judged in terms of the matching between the sets of local meaning features. The critical challenge, however, is to compute the correspo ndences between the feature sets in an efficient way. The optimal correspondences X  X hose that minim ize the matching cost X  X equire cubic time to compute, which quickly becomes prohibitive for size able sets and makes processing realistic large data sets impractical. Due to the optimal matching X  X  c omplexity, researchers have developed approximation algorithms to compute close solutions for a f raction of the computational cost [4, 8, 1, 7]. However, previous approximations suffer from distor tion factors that increase linearly with the dimension of the features, and they fail to take advantag e of structure in the feature space. In this paper we present a new algorithm for computing an appr oximate partial matching between point sets that can remain accurate even for sets with high-d imensional feature vectors, and benefits from taking advantage of the underlying structure in the fea ture space. The main idea is to derive a hierarchical, data-dependent decomposition of the featur e space that can be used to encode feature sets as multi-resolution histograms with non-uniformly sh aped bins. For two such histograms ( pyra-mids ), the matching cost is efficiently calculated by counting th e number of features that intersect in each bin, and weighting these match counts according to ge ometric estimates of inter-feature dis-tances. Our method allows for partial matchings, which mean s that the input sets can have varying numbers of features in them, and outlier features from the la rger set can be ignored with no penalty to the matching cost. The matching score is computed in time l inear in the number of features per set, and it forms a Mercer kernel suitable for use within exis ting kernel-based algorithms. In this paper we demonstrate how, unlike previous set matchi ng approximations (including our orig-inal pyramid match algorithm [7]), the proposed approach ca n maintain consistent accuracy as the dimension of the features within the sets increases. We also show how the data-dependent hierarchi-cal decomposition of the feature space produces more accura te correspondence fields than a previous approximation that uses a uniform decomposition. Finally, using our matching measure as a kernel in a discriminative classifier, we achieve improved object r ecognition results over a state-of-the-art set kernel on a benchmark data set. Several previous matching approximation methods have also considered a hierarchical decomposi-tion of the feature space to reduce matching complexity, but all suffer from distortion factors that scale linearly with the feature dimension [4, 8, 1, 7]. In thi s work we show how to alleviate this decline in accuracy for high-dimensional data by tuning the hierarchical decomposition according to the particular structure of the data, when such structure exists.
 We build on our pyramid match algorithm [7], a partial matchi ng approximation that also uses histogram intersection to efficiently count matches implic itly formed by the bin structures. However, in contrast to [7], our use of data-dependent, non-uniform b ins and a more precise weighting scheme results in matchings that are consistently accurate for str uctured, high-dimensional data. The idea of partitioning a feature space with vector quantiz ation (VQ) is fairly widely used in prac-tice; in the vision literature in particular, VQ has been use d to establish a vocabulary of prototypical image features, from  X  X extons X  to the  X  X isual words X  of [16] . A variant of the pyramid match ap-plied to spatial features was shown to be effective for match ing quantized features in [10]. More recently, the authors of [13] have shown that a tree-structu red vector quantization (TSVQ [5]) of im-age features provides a scalable means of indexing into a ver y large feature vocabulary. The actual tree structure employed is similar to the one constructed in this work; however, whereas the authors of [13] are interested in matching individual features to on e another to access an inverted file, our approach computes approximate correspondences between sets of features. Note the distinction be-tween the problem we are addressing X  X pproximate matchings b etween sets X  X nd the problem of efficiently identifying approximate or exact nearest neigh bor feature vectors (e.g., via k -d trees): in the former, the goal is a one-to-one correspondence between sets of vectors, whereas in the latter, a single vector is independently matched to a nearby vector. The main contribution of this work is a new very efficient appr oximate bipartite matching method that measures the correspondence-based similarity betwee n unordered, variable-sized sets of vec-tors, and can optionally extract an explicit correspondenc e field. We call our algorithm the vocabulary-guided (VG) pyramid match , since the histogram pyramids are defined by the  X  X ocabu-lary X  or structure of the feature space, and the pyramids are used to count implicit matches. The basic idea is to first partition the given feature space in to a pyramid of non-uniformly shaped re-gions based on the distribution of a provided corpus of featu re vectors. Point sets are then encoded as multi-resolution histograms determined by that pyramid, a nd an efficient intersection-based compu-tation between any two histogram pyramids yields an approxi mate matching score for the original sets. The implicit matching version of our method estimates the inter-feature distances based on their respective distances to the bin centers. To produce an explicit correspondence field between the sets, we use the pyramid construct to divide-and-conque r the optimal matching computation. As our experiments will show, the proposed algorithm in practi ce provides a good approximation to the optimal partial matching, but is orders of magnitude faster to compute.
 Preliminaries: We consider a feature space F of d -dimensional vectors, F  X  &lt; d . The point sets our algorithm matches will come from the input space S , which contains sets of feature vectors drawn from F : S = { X | X = { x vary across instances of sets in S . Throughout the text we will use the terms feature, vector, a nd point interchangeably to refer to the elements within a set. Figure 1: Rather than carve the feature space into uniformly-shaped partitions (lef t), we let the vocabulary A partial matching between two point sets is an assignment th at maps all points in the smaller set to some subset of the points in the larger (or equally-sized) set. Given point sets X and Y , where m = | X | , n = | Y | , and m  X  n , a partial matching M ( X , Y ;  X  ) = { ( x 1 , y  X  pairs each point in X to some unique point in Y according to the permutation of indices specified by  X  = [  X  for 1  X  i  X  m . The cost of a partial matching is the sum of the distances bet ween matched points: C ( M ( X , Y ;  X  )) = P x assignment  X   X  that minimizes this cost:  X   X  = argmin we wish to efficiently approximate. In Section 3.2 we describ e how our algorithm approximates the cost C ( M ( X , Y ;  X   X  )) ; for a small increase in computational cost we can also extra ct explicit correspondences to estimate  X   X  itself. 3.1 Building Vocabulary-Guided Pyramids The first step is to generate the structure of the vocabulary-guided (VG) pyramid to define the bin placement for the multi-resolution histograms used in the m atching. This is a one-time process performed before any matching takes place. We would like the bins in the pyramid to follow the feature distribution and concentrate partitions where the features actually fall. To accomplish this, we perform hierarchical clustering on a sample of represent ative feature vectors from F . We randomly select some example feature vectors from the fea ture type of interest to form the repre-sentative feature corpus, and perform hierarchical k -means clustering with the Euclidean distance to build the pyramid tree. Other hierarchical clustering tech niques, such as agglomerative clustering, are also possible and do not change the operation of the metho d. For this unsupervised clustering process there are two parameters: the number of levels in the tree L , and the branching factor k . The initial corpus of features is clustered into k top-level groups, where group membership is deter-mined by the Voronoi partitioning of the feature corpus acco rding to the k cluster centers. Then the clustering is repeated recursively L  X  1 times on each of these groups, filling out a tree with L total levels containing k i bins (nodes) at level i , where levels are counted from the root ( i = 0 ) to the leaves ( i = L  X  1 ). The bins are irregularly shaped and sized, and their bound aries are determined by the Voronoi cells surrounding the cluster centers. (See F igure 1.) For each bin in the VG pyramid we record its diameter, which we estimate empirically based on the maximal inter-feature distance between any points from the initial feature corpus that were assigned to it.
 Once we have constructed a VG pyramid, we can embed point sets from S as multi-resolution histograms. A point X  X  placement in the pyramid is determine d by comparing it to the appropriate k bin centers at each of the L pyramid levels. The histogram count is incremented for the b in (among the k choices) that the point is nearest to in terms of the same dist ance function used to cluster the initial corpus. We then push the point down the tree and conti nue to increment finer level counts only along the branch (bin center) that is chosen at each leve l. So a point is first assigned to one of the top-level clusters, then it is assigned to one of its children, and so on recursively. This amounts to a total of kL distances that must be computed between a point and the pyram id X  X  bin centers. Given the bin structure of the VG pyramid, a point set X is mapped to its pyramid:  X  ( X ) = [ H 0 ( X ) , . . . , H L  X  1 ( X )] dimensional histogram associated with level i in the pyramid, p  X  Z i for entries in H 0  X  i &lt; L . Each entry in this histogram is a triple  X  p , n, d  X  giving the bin index, the bin count, and the bin X  X  points X  maximal distance to the bin center, respec tively.
 Storing the VG pyramid itself requires space for O ( k L ) d -dimensional feature vectors, i.e., all of the cluster centers. However, each point set X  X  histogram is stored sparsely, meaning only O ( mL ) nonzero bin counts are maintained to encode the entire pyram id for a set with m features. This is an important point: we do not store O ( k L ) counts for every point set; H most m triples having n &gt; 0 . We achieve a sparse implementation as follows: each vector in a set is pushed through the tree as described above. At every level i , we record a  X  p , n, d  X  triple describing the nonzero entry for the current bin. The vector p = [ p of the clusters traversed from the root so far, n  X  Z + denotes the count for the bin (initially 1), and d  X  &lt; denotes the distance computed between the inserted point an d the current bin X  X  center. Upon reaching the leaf level, p is an L -dimensional path-vector indicating which of the k bins were chosen at each level, and every path-vector uniquely identi fies some bin on the pyramid. Initially, an input set with m features yields a total of mL such triples X  X here is one nonzero entry per level per point, and each has n = 1 . Then each of the L lists of entries is sorted by the index vectors ( p in the triple), and they are collapsed to a list of sorted nonz ero entries with unique indices: when two or more entries with the same index are found, they ar e replaced with a single entry with the same index for p , the summed counts for n , and the maximum distance for d . The sorting is done in linear time using integer sorting algorithms. Maintaini ng the maximum distance of any point in a bin to the bin center will allow us to efficiently estimate int er-point distances at the time of matching, as described in Section 3.2. 3.2 Vocabulary-Guided Pyramid Match Given two point sets X  pyramid encodings, we efficiently comp ute the approximate matching score using a simple weighted intersection measure. The VG pyrami d X  X  multi-resolution partitioning of the feature space is used to direct the matching. The basic in tuition is to start collecting groups of matched points from the bottom of the pyramid up, i.e., from w ithin increasingly larger partitions. In this way, we will first consider matching the closest point s (at the leaves), and as we climb to the higher-level clusters in the pyramid we will allow incre asingly further points to be matched. We define the number of new matches within a bin to be a count of the minimum number of poin ts either of the two input sets contributes to that bin, minus the numbe r of matches already counted by any of its child bins. A weighted sum of these counts yields an appro ximate matching score.
 Let n c ( n ij ( X )) denote the element n for the h th child bin of that entry, 1  X  h  X  k . Similarly, let d ij ( X ) matching score via their pyramids  X ( X ) and  X ( Y ) as follows: The outer sum loops over the levels in the pyramids; the secon d sum loops over the bins at a given level, and the innermost sum loops over the children of a give n bin. The first min term reflects the number of matchable points in the current bin, and the sec ond min term tallies the number of matches already counted at finer resolutions (in child bins) . Note that as the leaf nodes have no children, when i = L  X  1 the last sum is zero. All matches are new at the leaves. The mat ching scores are normalized according to the size of the input sets in order to not favor larger sets. The number of new matches calculated for a bin is weighted by w distance estimate: (a) weights based on the diameters of the pyramid X  X  bins, or (b) input-dependent weights based on the maximal distances of the points in the bi n to its center. Option (a) is a con-servative estimate of the actual inter-point distances in t he bin if the corpus of features used to build the pyramid is representative of the feature space; its adva ntages are that it provides a guaranteed Mercer kernel (see below) and eliminates the need to store a d istance d in the entry triples. Option (b) X  X  input-specific weights estimate the distance between any two points in the bin as the sum of the stored maximal to-center distances from either input set: w gives a true upper bound on the furthest any two points could b e from one another, and it has the po-tential to provide tighter estimates of inter-feature dist ances (as we confirm experimentally below); however, we cannot guarantee this weighting will yield a Mer cer kernel.
 Just as we encode the pyramids sparsely, we derive a means to c ompute intersections in Eqn. 1 without ever traversing the entire pyramid tree. Given two s parse lists H been sorted according to the bin indices, we obtain the minim um counts in linear time by moving pointers down the lists and processing only those nonzero en tries that share an index, making the time required to compute a matching between two pyramids O ( mL ) . A key aspect of our method is that we obtain a measure of matching quality between two poin t sets without computing pair-wise distances between their features X  X n O ( m 2 ) savings over sub-optimal greedy matchings. Instead, we exploit the fact that the points X  placement in the pyramid reflects their distance from one another. The only inter-feature distances computed are the kL distances needed to insert a point into the pyramid, and this small one-time cost is amortized every tim e we re-use a histogram to approximate another matching against a different point set.
 We first suggested the idea of using histogram intersection t o count implicit matches in a multi-resolution grid in [7]. However, in [7], bins are constructe d to uniformly partition the space, bin diameters exponentially increase over the levels, and inte rsections are weighted indistinguishably across an entire level. In contrast, here we have developed a pyramid embedding that partitions according to the distribution of features, and weighting sc hemes that allow more precise approxima-tions of the inter-feature costs. As we will show in Section 4 , our VG pyramid match remains accu-rate and efficient even for high-dimensional feature spaces , while the uniform-bin pyramid match is limited in practice to relatively low-dimensional feature s.
 For the increased accuracy our method provides, there are so me complexity trade-offs versus [7], which does not require computing any distances to place the p oints into bins; their uniform shape and size allows points to be placed directly via division by b in size. On the other hand, sorting the bin indices with the VG method has a lower complexity, since t he values only range to k , the branch factor, which is typically much smaller than the aspect rati o that bounds the range in [7]. In addition, as we show in Section 4, in practice the cost of extracting an explicit correspondence field using the uniform-bin pyramid in high dimensions approaches the cubi c cost of the optimal measure, whereas it remains linear with the proposed approach, assuming feat ures are not uniformly distributed. Our approximation can be used to compare sets of vectors in an y case where the presence of low-cost correspondences indicates their similarity (e.g., ne arest-neighbor retrieval). We can also employ the measure as a kernel function for structured inputs. Acco rding to Mercer X  X  theorem, a kernel is p.s.d if and only if it corresponds to an inner product in some feature space [15]. We can re-write Eqn. 1 as: C ( X ( X ) ,  X ( Y )) = P L  X  1 to the weight associated with the parent bin of the j th node at level i . Since the min operation is p.d. [14], and since kernels are closed under summation and s caling by a positive constant [15], we have that the VG pyramid match is a Mercer kernel if w child bin receives a similarity weight that is greater than i ts parent bin, or rather that every child bin has a distance estimate that is less than that of its paren t. Indeed this is the case for weighting option (a), where w hierarchical clustering: the diameter of a subset of points must be less than or equal to the diameter of all those points. We cannot make this guarantee for weighting op tion (b).
 In addition to scalar matching scores, we can optionally ext ract explicit correspondence fields through the pyramid. In this case, the VG pyramid decomposes the required matching computa-tion into a hierarchy of smaller matchings. Upon encounteri ng a bin with a nonzero intersection, the optimal matching is computed between only those feature s from the two sets that fall into that particular bin. All points that are used in that per-bin matc hing are then flagged as matched and may not take part in subsequent matchings at coarser resolution s of the pyramid. In this section, we provide results to empirically demonstr ate our matching X  X  accuracy and efficiency on real data, and we compare it to a pyramid match using a unifo rm partitioning of the feature space. In addition to directly evaluating the matching scor es and correspondence fields, we show that our method leads to improved object recognition perfor mance when used as a kernel within a discriminative classifier. Figure 2: Comparison of optimal and approximate matching rankings on image data. Left: The set rankings Approximate Matching Scores: In these experiments, we extracted local SIFT [11] features from images in the ETH-80 database, producing an unordered set of about m = 256 vectors for every example. In this case, F is the space of SIFT image features. We sampled some features from 300 of the images to build the VG pyramid, and 100 images were used to test the matching. In order to test across varying feature dimensions, we also used some traini ng features to establish a PCA subspace that was used to project features onto varying numbers of bas es. For each feature dimension, we built a VG pyramid with k = 10 and L = 5 , encoded the 100 point sets as pyramids, and computed the pair-wise matching scores with both our method and the op timal least-cost matching. If our measure is approximating the optimal matching well, w e should find the ranking we induce to be highly correlated with the ranking produced by the opti mal matching for the same data. In other words, the images should be sorted similarly by either method. Spearman X  X  rank correlation coefficient R provides a good quantitative measure to evaluate this: R = 1  X  6 P N where D is the difference in rank for the N corresponding ordinal values assigned by the two mea-sures. The left plot in Figure 2 shows the Spearman correlati on scores against the optimal measure for both our method (with both weighting options) and the app roximation in [7] for varying feature dimensions for the 10,000 pair-wise matching scores for the 100 test sets. Due to the randomized elements of the algorithms, for each method we have plotted t he mean and standard deviation of the correlation for 10 runs on the same data.
 While the VG pyramid match remains consistently accurate for high feature dimensions ( R = 0 . 95 with input-specific weights), the accuracy of the uniform bi ns degrades rapidly for dimensions over 10. The ranking quality of the input-specific weighting scheme (blue diamonds) is somewhat stronger than that of the  X  X lobal X  bin diameter weighting sc heme (green squares). The four plots on the right of Figure 2 display the actual ranks computed for both approximations for two of the 26 dimensions summarized in the left plot. The black diagona ls denote the optimal performance, where the approximate rankings would be identical to the opt imal ones; higher Spearman correla-tions have points clustered more tightly along this diagona l. For the low-dimensional features, the methods perform fairly comparably; however, for the full 12 8-D features, the VG pyramid match is far superior (rightmost column). The optimal measure req uires about 1.25 s per match, while our approximation is about 2500 x faster at 5  X  10  X  4 s per match. Computing the pyramid structure from the feature corpus took about three minutes in Matlab; this i s a one-time offline cost. For a pyramid matching to work well, the gradation in bin size s up the pyramid must be such bins. That is, unless all the points in two sets are equidista nt, the bin placement must allow us to match very near points at the finest resolutions, and gradual ly add matches that are more distant at coarser resolutions. In low dimensions, both uniform or d ata-dependent bins can achieve this. In high dimensions, however, uniform bin placement and expo nentially increasing bin diameters fail to capture such a gradation: once any features from diff erent point sets are close enough to Figure 3: Number of new matches formed at each pyramid level for either uniform (dashed red ) or VG (solid Figure 4: Comparison of correspondence field errors (left) and associated co mputation times (right). This match (share bins), the bins are so large that almost all of them match. The matching score is then approximately the number of points weighted by a single bin s ize. In contrast, when we tailor the feature space partitions to the distribution of the data, ev en in high dimensions the match counts increase gradually across levels, thereby yielding more di scriminating implicit matches. Figure 3 confirms this intuition, again using the ETH-80 image data fr om above.
 Approximate Correspondence Fields: For the same image data, we ran the explicit matching variant of our method and compared the induced corresponden ces to those produced by the globally optimal measure. For comparison, we also applied the same va riant to pyramids with uniform bins. We measure the error of an approximate matching  X   X  by the sum of the errors at every link in the field: E ( M ( X , Y ;  X   X  ) , M ( X , Y ;  X   X  )) = P x field error and computation times for the VG and uniform pyram ids. For each approximation, there are two variations tested: in one, an optimal assignment is c omputed for all points in the same bin; for the other, a random assignment is made. The left plot show s the mean error per match for each method, and the right plot shows the corresponding mean time required to compute those matches. The computation times are as we would expect: the optimal mat ching is orders of magnitude more expensive than the approximations. Using the random assign ment variation, both approximations have negligible costs, since they simply choose any combina tion of points within a bin. However, in high dimensions, the time required by the uniform bin pyrami d with the optimal per-bin matching approaches the time required by the optimal matching itself . This occurs for similar reasons as the poorer matching score accuracy exhibited by the uniform bin s, both in the left plot and above in Figure 2; since most or all of the points begin to match at a cer tain level, the pyramid does not help to divide-and-conquer the computation, and for high dimens ions, the optimal matching in its entirety must be computed. In contrast, the expense of the VG pyramid m atching remains steady and low, even for high dimensions, since data-dependent pyramids be tter divide the matching labor into the natural segments in the feature space.
 For similar reasons, the errors are comparable for the optim al per-bin variation with either the VG or uniform bins. The VG bins divide the computation so it can b e done inexpensively, while the uniform bins divide the computation poorly and must compute it expensively, but about as accu-rately. Likewise, the error for the uniform bins when using a per-bin random assignment is very high for any but the lowest dimensions (red line on left plot) , since such a large number of points are being randomly assigned to one another. In contrast, the VG bins actually result in similar errors whether the points in a bin are matched optimally or randomly (blue and pink lines on left plot). This again indicates that tuning the pyramid bins to the data  X  X  distribution achieves a much more suitable breakdown of the computation, even in high dimensi ons.
 Realizing Improvements in Recognition: Finally, we have experimented with the VG pyramid match within a discriminative classifier for an object recog nition task. We trained an SVM with our matching as the kernel to recognize the four categories i n the Caltech-4 benchmark data set. We trained with 200 images per class and tested with all the re maining images. We extracted fea-tures using both the Harris and MSER [12] detectors and the 12 8-D SIFT [11] descriptor. We also generated lower-dimensional ( d = 10 ) features using PCA. To form a Mercer kernel, the weights were set according to each bin diameter A distance between a sample of features from the training set. The table shows our improvements over the uniform-bin pyramid match kernel. The VG pyramid ma tch is more accurate and requires minor additional computation. Our near-perfect performan ce on this data set is comparable to that reached by others in the literature; the real significance of the result is that it distinguishes what can be achieved with a VG pyramid embedding as opposed to the u niform histograms used in [7], particularly for high-dimensional features. In addition, here the optimal matching requires 0.31 s per match, over 500 x the cost of our method.
 Conclusion: We have introduced a linear-time method to compute a matchin g between point sets that takes advantage of the underlying structure in the feat ure space and remains consistently ac-curate and efficient for high-dimensional inputs on real ima ge data. Our results demonstrate the strength of the approximation empirically, compare it dire ctly against an alternative state-of-the-art approximation, and successfully use it as a Mercer kernel fo r an object recognition task. We have commented most on potential applications in vision and text , but in fact it is a generic matching measure that can be applied whenever it is meaningful to comp are sets by their correspondence. Acknowledgments: We thank Ben Kuipers for suggesting the use of Spearman X  X  ran k correlation.
