 Chiang X  X  hierarchical phrase-based (HPB) transla-tion model utilizes synchronous context free gram-mar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a mono-tone fashion. Due to lack of linguistic knowledge, Chiang X  X  HPB model contains only one type of non-terminal symbol X , often making it difficult to se-lect the most appropriate translation rules. 1 What is more, Chiang X  X  HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a glue rule is adopted, it requires all rules above it to be glue rules.

One important research question is therefore how to refine the non-terminal category X using linguis-tically motivated information: Zollmann and Venu-gopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by ei-ther POS analysis or unsupervised word class in-duction. Almaghout et al. (2011) employ CCG-based supertags. Mylonakis and Sima X  X n (2011) use linguistic information of various granularities such as Phrase-Pair , Constituent , Concatenation of Con-stituents , and Partial Constituents , where applica-ble. Inspired by previous work in parsing (Char-niak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that lin-guistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically mo-tivated dependency parsing, and use their POS to refine X . In addition HD-HPB provides flexible re-ordering rules freely mixing translation and reorder-ing (including swap) at any stage in a derivation.
Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang X  X  HPB model while at the same time incorporating head information and flex-ible reordering in a derivation in a natural way. Ex-periments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang X  X  HPB as well as a SAMT-style refined version of HPB. Like Chiang (2005) and Chiang (2007), our HD-HPB translation model adopts a synchronous con-text free grammar, a rewriting system which gen-erates source and target side string pairs simulta-neously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007), given a word sequence f i j from position i to position j , we first find heads and then concatenate the POS tags of these heads as f i j  X  X  non-terminal symbol. Specif-ically, we adopt unlabeled dependency structure to derive heads, which are defined as: Definition 1. For word sequence f i j , word f k ( i  X  k  X  j ) is regarded as a head if it is domi-nated by a word outside of this sequence.

Note that this definition (i) allows for a word se-quence to have one or more heads (largely due to the fact that a word sequence is not necessarily lin-guistically constrained) and (ii) ensures that heads are always the highest heads in the sequence from a dependency structure perspective. For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming , ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal cor-responding to the sequence is thus labeled as NN-AD . It is worth noting that in this paper we only refine non-terminal X on the source side to head-informed ones, while still using X on the target side.
According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify ini-tial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based trans-lation models (Och and Ney, 2004) and Chiang X  X  HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang X  X  HPB model (Chiang, 2007), except that we use head POS-informed non-terminal symbols in the source lan-guage. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 demonstrates the dif-ference between hierarchical rules in Chiang (2007) and HD-HRs defined here.

Similar to Chiang X  X  HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding. To alleviate these problems, we filter our HD-HRs according to the same con-straints as described in Chiang (2007). Moreover, we discard rules that have non-terminals with more than four heads. 2.2 NRRs: Non-terminal Reordering Rules NRRs are translation rules without terminals. Given an initial phrase pair on the source side, there are four possible positional relationships for their target side translations (we use Y as a variable for non-terminals on the source side while all non-terminals on the target side are labeled as X ):
Merging two neighboring non-terminals into a single non-terminal, NRRs enable the translation model to explore a wider search space. During train-ing, we extract four types of NRRs and calculate probabilities for each type. To speed up decoding, we currently (i) only use monotone and swap NRRs and (ii) limit the number of non-terminals in a NRR to 2. 2.3 Features and Decoding Given e for the translation output in the target lan-guage, s and t for strings of terminals and non-terminals on the source and target side, respectively, we use a feature set analogous to the default feature set of Chiang (2007), including:
Our decoder is based on CKY-style chart parsing with beam search and searches for the best deriva-tion bottom-up. For a source span [ i , j ], it applies both types of HD-HRs and NRRs. However, HD-HRs are only applied to generate derivations span-ning no more than K words  X  the initial phrase length limit used in training to extract HD-HRs  X  while NRRs are applied to derivations spanning any length. Unlike in Chiang X  X  HPB model, it is pos-sible for a non-terminal generated by a NRR to be included afterwards by a HD-HR or another NRR. We evaluate the performance of our HD-HPB model and compare it with our implementation of Chiang X  X  HPB model (Chiang, 2007), a source-side SAMT-style refined version of HPB (SAMT-HPB), and the Moses implementation of HPB. For fair compari-son, we adopt the same parameter settings for our HD-HPB and HPB systems, including initial phrase length (as 10) in training, the maximum number of non-terminals (as 2) in translation rules, maximum number of non-terminals plus terminals (as 5) on the source, beam threshold  X  (as 10  X  5 ) (to discard derivations with a score worse than  X  times the best score in the same chart cell), beam size b (as 200) (i.e. each chart cell contains at most b derivations). For Moses HPB, we use  X  X row-diag-final-and X  to obtain symmetric word alignments, 10 for the max-imum phrase length, and the recommended default values for all other parameters.

We train our model on a dataset with  X 1.5M sen-tence pairs from the LDC dataset. 2 We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the 2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respec-tively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser 3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit 4 to obtain (unlabeled) dependency structures.

We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying  X  X row-diag-final-and X  re-finement (Koehn et al., 2003). We use the SRI lan-guage modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data.

For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance differ-ence is statistically significant, we conduct signifi-cance tests following the paired bootstrap approach (Koehn, 2004). In this paper,  X  **  X  and  X  *  X  de-note p -values less than 0.01 and in-between [0.01, 0.05), respectively.

Table 2 lists the rule table sizes. The full rule ta-ble size (including HD-HRs and NRRs) of our HD-HPB model is  X 1.5 times that of Chiang X  X , largely due to refining the non-terminal symbol X in Chi-ang X  X  model into head-informed ones in our model. It is also unsurprising, that the test set-filtered rule table size of our model is only  X 0.7 times that of Chi-ang X  X : this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training data. Furthermore, the rule table size of NRRs is much smaller than that of HD-HRs since a NRR contains only two non-terminals. Table 3 lists the translation performance with BLEU scores. Note that our re-implementation of Chiang X  X  original HPB model performs on a par with Moses HPB. Table 3 shows that our HD-HPB model significantly outperforms Chiang X  X  HPB model with an average improvement of 1.91 in BLEU (and sim-ilar improvements over Moses HPB).

Table 3 shows that the head-driven scheme out-performs a SAMT-style approach (for each test set p &lt; 0 . 01 ), indicating that head information is more effective than (partial) CFG categories. Taking lian-ming zhichi in Figure 1 as an example, HD-HPB labels the span VV , as lianming is dominated by zhichi , effecively ignoring lianming in the transla-tion rule, while the SAMT label is ADVP:AD+VV 5 which is more susceptible to data sparsity. In addi-tion, SAMT resorts to X if a text span fails to satisify pre-defined categories. Examining initial phrases extracted from the SAMT training data shows that 28% of them are labeled as X .

In order to separate out the individual contribu-tions of the novel HD-HRs and NRRs, we carry out an additional experiment (HD-HR+Glue) using HD-HRs with monotonic glue rules only (adjusted to re-fined rule labels, but effectively switching off the ex-tra reordering power of full NRRs). Table 3 shows that on average more than half of the improvement over HPB (Chiang and Moses) comes from the re-fined HD-HRs, the rest from NRRs.

Examining translation rules extracted from the training data shows that there are 72,366 types of non-terminals with respect to 33 types of POS tags. On average each sentence employs 16.6/5.2 HD-HRs/NRRs in our HD-HPB model, compared to 15.9/3.6 hierarchical rules/glue rules in Chiang X  X  model, providing further indication of the impor-tance of NRRs in translation. We present a head-driven hierarchical phrase-based (HD-HPB) translation model, which adopts head in-formation (derived through unlabeled dependency analysis) in the definition of non-terminals to bet-ter differentiate among translation rules. In ad-dition, improved and better integrated reordering rules allow better reordering between consecutive non-terminals through exploration of a larger search space in the derivation. Experimental results on Chinese-English translation across four test sets demonstrate significant improvements of the HD-HPB model over both Chiang X  X  HPB and a source-side SAMT-style refined version of HPB.
 This work was supported by Science Foundation Ire-land (Grant No. 07/CE/I1142) as part of the Cen-tre for Next Generation Localisation (www.cngl.ie) at Dublin City University. It was also partially supported by Project 90920004 under the National Natural Science Foundation of China and Project 2012AA011102 under the  X 863 X  National High-Tech Research and Development of China. We thank the reviewers for their insightful comments.
