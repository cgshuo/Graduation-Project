 The world can be seen as a network of causal-ity where people, organizations, and other kinds of entities causally depend on each other. This network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event. Indeed, after the Great East Japan Earthquake in 2011, few ex-pected that it would lead to an enormous trade deficit in Japan due to a sharp increase in en-ergy imports. For effective decision making that carefully considers any form of future risks and chances, we need a system that helps humans do scenario planning (Schwartz, 1991), which is a decision-making scheme that examines possible future events and assesses their potential chances and risks. Our ultimate goal is to develop a system that supports scenario planning through generat-ing possible future events using big data, which would contain what Donald Rumsfeld called  X  X n-
To this end, we propose a supervised method of extracting such event causality as conduct slash-and-burn agriculture  X  exacerbate desertifi-cation and use its output to generate future sce-narios ( scenarios ), which are chains of causal-ity that have been or might be observed in this world like conduct slash-and-burn agricul-ture  X  exacerbate desertification  X  increase Asian dust (from China)  X  asthma gets worse . Note that, in this paper, A  X  B denotes that A causes B , which means that  X  if A happens, the probability of B in-creases . X  Our notion of causality should be inter-preted probabilistically rather than logically.
Our method extracts event causality based on three assumptions that are embodied as features of our classifier. First, we assume that two nouns (e.g. slash-and-burn agriculture and desertifica-tion ) that take some specific binary semantic rela-tions (e.g. A CAUSES B ) tend to constitute event causality if combined with two predicates (e.g. conduct and exacerbate ). Note that semantic re-lations are not restricted to those directly relevant to causality like A CAUSES B but can be those that might seem irrelevant to causality like A IS AN INGREDIENT FOR B (e.g. plutonium and atomic bomb as in plutonium is stolen  X  atomic bomb is made ). Our underlying intuition is the observation that event causality tends to hold between two en-tities linked by semantic relations which roughly entail that one entity strongly affects the other. Such semantic relations can be expressed by (oth-erwise unintuitive) patterns like A IS AN INGRE -DIENT FOR B . As such, semantic relations like the M
ATERIAL relation can also be useful. (See Sec-tion 3.2.1 for a more intuitive explanation.)
Our second assumption is that there are gram-matical contexts in which event causality is more likely to appear. We implement what we con-sider likely contexts for event causality as con-text features. For example, a likely context of event causality (underlined) would be: CO2 levels rose, so climatic anomalies were observed , while an unlikely context would be: It remains uncertain whether if the recession is bottomed the declining birth rate is halted . Useful context information in-cludes the mood of the sentences (e.g., the uncer-tainty mood expressed by uncertain above), which is represented by lexical features (Section 3.2.2).
The last assumption embodied in our associa-tion features is that each word of the cause phrase must have a strong association (i.e., PMI, for ex-ample) with that of the effect phrase as slash-and-burn agriculture and desertification in the above example, as in Do et al. (2011).

Our method exploits these features on top of our base features such as nouns and predicates. Exper-iments using 600 million web pages (Akamine et al., 2010) show that our method outperforms base-lines based on state-of-the-art methods (Do et al., 2011; Hashimoto et al., 2012) by more than 19% of average precision.

We require that event causality be self-contained , i.e., intelligible as causality without the sentences from which it was extracted. For ex-ample, omit toothbrushing  X  get a cavity is self-contained, but omit toothbrushing  X  get a girl-friend is not since this is not intelligible without a context: He omitted toothbrushing every day and got a girlfriend who was a dental assistant of den-tal clinic he went to for his cavity . This is im-portant since future scenarios, which are gener-ated by chaining event causality as described be-low, must be self-contained, unlike Hashimoto et al. (2012). To make event causality self-contained, we wrote guidelines for manually annotating train-ing/development/test data. Annotators regarded as event causality only phrase pairs that were interpretable as event causality without contexts (i.e., self-contained). From the training data, our method seemed to successfully learn what self-contained event causality is.

Our scenario generation method generates sce-narios by chaining extracted event causality; gen-erating A  X  B  X  C from A  X  B and B  X  C . The chal-lenge is that many acceptable scenarios are over-looked if we require the joint part of the chain ( B above) to be an exact match. To increase the num-ber of acceptable scenarios, our method identifies compatibility w.r.t causality between two phrases by a recently proposed semantic polarity, exci-tation (Hashimoto et al., 2012), which properly relaxes the chaining condition (Section 3.1 de-scribes it). For example, our method can iden-tify the compatibility between sea temperatures are high and sea temperatures rise to chain global warming worsens  X  sea temperatures are high and sea temperatures rise  X  vibrio parahaemolyti-cus 2 fouls (water) . Accordingly, we generated a scenario deforestation continues  X  global warm-ing worsens  X  sea temperatures rise  X  vibrio para-haemolyticus fouls (water) , which is written in no document in our input web corpus that was crawled in 2007, but the vibrio risk due to global warming has actually been observed in the Baltic sea and reported in Baker-Austin et al. (2013). In a sense, we  X  X redicted X  the event sequence re-ported in 2013 by documents written in 2007. Our experiments also show that we generated 50,000 scenarios with 68% precision, which include con-duct terrorist operations  X  terrorist bombing oc-curs  X  cause fatalities and injuries  X  cause eco-nomic losses and the above  X  slash-and-burn agri-culture  X  scenario (Section 5.2). Neither is written in any document in our input corpus.
 In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of explanation. Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf . For event causality extraction , clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Be-sides features similar to those described above, we those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we tar-get event causality about two distinct entities.
To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither in-volves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events have previously been observed, and their method only applies to news domain.

Some of the scenarios we generated are written on no page in our input web corpus. Similarly, Tsuchida et al. (2011) generated semantic knowl-edge like causality that is written in no sentence. However, their method cannot combine more than two pieces of knowledge unlike ours, and their tar-get knowledge consists of nouns, but ours consists of verb phrases, which are more informative.
Tanaka et al. (2013) X  X  web information analy-sis system provides a what-happens-if QA service, which is based on our scenario generation method. This section describes our event causality extrac-tion method. Section 3.1 describes how to extract event causality candidates, and Section 3.2 details our features. Section 3.3 shows how to rank event causality candidates. 3.1 Event Causality Candidate Extraction We extract the event causality between two events represented by two phrases from single sentences tences from 600 million web pages. Each phrase in the event causality must consist of a predicate with an argument position ( template , hereafter) like conduct X and a noun like slash-and-burn agriculture that completes X . We also require the predicate of the cause phrase to syntactically de-pend on the effect phrase in the sentence from which the event causality was extracted; we guar-antee this by verifying the dependencies of the original sentence. In Japanese, since the tempo-ral order between events is usually determined by precedence in a sentence, we require the cause phrase to precede the effect phrase. For context feature extraction, the event causality candidates are accompanied by the original sentences from which they were extracted.
 Excitation We only keep the event causality candidates each phrase of which consists of exci-tation templates , which have been shown to be ef-fective for causality extraction (Hashimoto et al., 2012) and other semantic NLP tasks (Oh et al., 2013; Varga et al., 2013; Kloetzer et al., 2013a). Excitation is a semantic property of templates that classifies them into excitatory , inhibitory , and neu-tral . Excitatory templates such as cause X entail that the function, effect, purpose or role of their ar-gument X  X  referent is activated, enhanced, or man-ifested, while inhibitory templates such as lower X entail that it is deactivated or suppressed. Neu-tral ones like proportional to X belong to neither of them. We collectively call both excitatory and inhibitory templates excitation templates. We ac-quired 43,697 excitation templates by Hashimoto et al. X  X  method and the manual annotation of exci-tation filter to all 272,025,401 event causality can-didates from the web and 132,528,706 remained.
After applying additional filters (see Section A in the supplementary notes) including those based on a stop-word list and a causal connective list to remove unlikely event causality candidates that are not removed by the above filter, we finally ac-quired 2,451,254 event causality candidates. 3.2 Features for Event Causality Classifier 3.2.1 Semantic Relation Features We hypothesize that two nouns with some particu-lar semantic relations are more likely to constitute event causality. Below we describe the semantic relations that we believe are likely to constitute event causality.

C AUSATION is the causal relation between two entities and is expressed by binary patterns like A
CAUSES B . Deforestation and global warming might complete the A and B slots. We manually collected 748 binary patterns for this relation. (See Section B in the supplementary notes for examples of our binary patterns.)
M ATERIAL is the relation between a material and a product made of it (e.g. plutonium and atomic bomb ) and can be expressed by A IS MADE OF B . Its relation to event causality might seem unclear, but a material can be seen as a  X  X ause X  of a product. Indeed materials can participate in event causality with the help of such template pairs as A is stolen  X  B is made as in plutonium is stolen  X  atomic bomb is made . We manually col-lected 187 binary patterns for this relation. SARY FOR B , which can be filled with verbal apti-tude and ability to think . Noun pairs with this rela-tion can constitute event causality when combined with template pairs like improve A  X  cultivate B . We collected 257 patterns for this relation.
U SE is the relation between means (or instru-ments) and the purpose for using them. A IS USED FOR B is a pattern of the relation, which can be filled with e-mailer and exchanges of e-mail mes-sages . Note that means can be seen as  X  X ausing X  or  X  X ealizing X  the purpose of using the means in this relation, and actually event causality can be obtained by incorporating noun pairs of this rela-tion into template pairs like activate A  X  conduct B . 2,178 patterns were collected for this relation.
P REVENTION is the relation expressed by pat-terns like A PREVENTS B , which can be filled with toothbrushing and periodontal disease . This rela-tion is, so to speak,  X  X egative C AUSATION  X  since the entity denoted by the noun completing the A slot makes the entity denoted by the B noun NOT realized. Such noun pairs mean event causality by substituting them into template pairs like omit A  X  get B . The number of patterns is 490.

The experiments in Section 5.1.1 show that not C
AUSATION  X ) but the other relations are also ef-fective for event causality extraction.

In addition, we invented the E XCITATION rela-tion that is expressed by binary patterns made of excitatory and inhibitory templates (Section 3.1). For instance, we make binary patterns A RISES B and A LOWERS B from excitatory template rise X and inhibitory template lower X respectively. The E
XCITATION relation roughly means that A acti-vates B (excitatory) or suppresses it (inhibitory). We simply add an additional argument position to each template in the 43,697 excitation templates to make binary patterns. We restricted the argument positions (represented by Japanese postpositions) of the A slot to either ha (topic marker), ga (nomi-native), or de (instrumental) and those of the B slot to either ha , ga , de , wo (accusative), or ni (dative), and obtained 55,881 patterns.

Moreover, for broader coverage, we acquired binary patterns that entail or are entailed by one of the patterns of the above six semantic relations. Those patterns were acquired from our web cor-pus by Kloetzer et al. (2013b) X  X  method, which ac-quired 185 million entailment pairs with 80% pre-cision from our web corpus and was used for con-tradiction acquisition (Kloetzer et al., 2013a). We acquired 335,837 patterns by this method. They are class-dependent patterns , which have seman-tic class restrictions on arguments. The semantic classes were obtained from our web corpus based on Kazama and Torisawa (2008). See De Saeger et al. (2009), De Saeger et al. (2011) and Kloet-zer et al. (2013a) for more on our patterns. They collectively constitute the E NTAILMENT relation.
Table 1 shows our semantic relation features. To use them, we first make a database that records which noun pairs co-occur with each binary pat-tern. Then we check a noun pair (the nouns of the cause and effect phrases) for each event causality candidate, and give the candidate all the patterns in the database that co-occur with the noun pair. 3.2.2 Context Features We believe that contexts exist where event causal-ity candidates are more likely to appear, as de-scribed in Section 1. We developed features that capture the characteristics of likely contexts for Japanese event causality (See Section C in the sup-plementary notes). In a nutshell, they represent a connective ( C1 and C2 in Section C), the distance between the elements of event causality candidate ( C3 and C4 ), words in context ( C5 to C8 ), the ex-istence of adnominal modifier ( 9 to C10 ), and the existence of additional arguments of cause and ef-fect predicates ( C13 to C20 ), among others. 3.2.3 Association Features These features measure the association strength between slash-and-burn agriculture and deser-tification in conduct slash-and-burn agricul-ture  X  exacerbate desertification for instance and consist of CEA-, Wikipedia-, definition-, and web-based features. CEA-based features are based on the Cause Effect Association (CEA) measure of Do et al. (2011). It consists of association measures like PMI between arguments (nouns), between arguments and predicates, and between predicates (Table 2). Do et al. used it (along with discourse relations) to extract event causality. Wikipedia-based features are the co-occurrence counts and the PMI values between cause and ef-fect nouns calculated using Wikipedia (as of 2013-Sep-19). We also checked whether an Wikipedia article whose title is a cause (effect) noun con-tains its effect (cause) noun, as detailed in Section D.1 in the supplementary notes. Definition-based features , as detailed in Section D.2 in the sup-plementary notes, resemble the Wikipedia-based features except that the information source is the definition sentences automatically acquired from our 600 million web pages using the method of Hashimoto et al. (2011). Web-based features provide association measures between nouns us-ing various window sizes in the 600 million web pages. See Section D.3 for detail. Web-based as-sociation measures were obtained from the same database as AC4 in Table 2. 3.2.4 Base Features Base features represent the basic properties of event causality like nouns, templates, and their ex-citation polarities (See Section E in the supple-mentary notes). For B3 and B4 , 500 semantic classes were obtained from our web corpus using the method of Kazama and Torisawa (2008). 3.3 Event Causality Scoring each event causality candidate into causality and non-causality. An event causality candidate is given a causality score CScore , which is the SVM score (distance from the hyperplane) that is nor-malized to [0 , 1] by the sigmoid function 1 Each event causality candidate may be given mul-tiple original sentences, since a phrase pair can ap-pear in multiple sentences, in which case it is given more than one SVM score. For such candidates, we give the largest score and keep only one origi-Original sentences are also used for scenario gen-eration, as described below. Our future scenario generation method creates scenarios by chaining event causalities. A naive approach chains two phrase pairs by exact match-ing. However, this approach would overlook many acceptable scenarios as discussed in Section 1. For example, global warming worsens  X  sea tempera-tures are high and sea temperatures rise  X  vibrio parahaemolyticus fouls (water) can be chained to constitute an acceptable scenario, but the joint part is not the same string. Note that the two phrases are not simply paraphrases; temperatures may be rising but remain cold, or they may be decreasing even though they remain high.

What characterizes two phrases that can be the joint part of acceptable scenarios? Although we have no definite answer yet, we name it the causal-compatibility of two phrases and provide its pre-liminary characterization based on the excitation polarity. Remember that excitatory templates like cause X entail that X  X  X  function or effect is acti-vated, but inhibitory templates like lower X entail that it is suppressed (Section 3.1). Two phrases are causally-compatible if they mention the same entity (typically described by a noun) that is pred-icated by the templates of the same excitation po-larity . Indeed, both X rise and X are high are ex-citatory and hence sea temperatures are high and
Scenarios ( sc s) generated by chaining causally-compatible phrase pairs are scored by Score ( sc ) , which embodies our assumption that an acceptable scenario consists of plausible event causality pairs: where CAU S ( sc ) is a set of event causality pairs that constitutes sc and cs is a member of CAU S ( sc ) . CScore ( cs ) , which is cs  X  X  score, was described in Section 3.3.

Our method optionally applies the following two filters to scenarios for better precision: An original sentence filter removes a scenario if two event causality pairs that are chained in it are ex-tracted from original sentences between which no word overlap exists other than words constituting causality pairs. In this case, the two event causal-ity pairs tend to be about different topics and con-stitute an incoherent scenario. A common argu-ment filter removes a scenario if a joint part con-sists of two templates that share no argument in our h argument, template i database, which is com-piled from the syntactic dependency data between arguments and templates extracted from our web corpus. Such a scenario tends to be incoherent too. 5.1 Event Causality Extraction Next we describe our experiments on event causal-ity extraction and show (a) that most of our fea-tures are effective and (b) that our method outper-forms the baselines based on state-of-the-art meth-ods (Do et al., 2011; Hashimoto et al., 2012). Our method achieved 70% precision at 13% recall; we can extract about 69,700 event causality pairs with 70% precision, as described below.

For the test data , we randomly sampled 23,650 examples of h event causality candidate, origi-nal sentence i among which 3,645 were positive from 2,451,254 event causality candidates ex-tracted from our web corpus (Section 3.1). For the development data , we identically collected 11,711 examples among which 1,898 were posi-tive. These datasets were annotated by three anno-tators (not the authors), who annotated the event causality candidates without looking at the origi-nal sentences. The final label was determined by majority vote. The training data were created by the annotators through our preliminary experi-ments and consists of 112,110 among which 9,657
Table 4: Ablation tests on semantic relations. were positive. The Kappa (Fleiss, 1971) of their judgments was 0.67 (substantial agreement (Lan-dis and Koch, 1977)). These three datasets have no overlap in terms of phrase pairs. About nine man-months were required to prepare the data.
Our evaluation is based on average precision ; 9 we believe that it is important to rank the plausible event causality candidates higher. 5.1.1 Ablation Tests We evaluated the features of our method by ab-lation tests. Table 3 shows the results of remov-ing the semantic relation, the context, and the as-sociation features from our method. All the fea-ture types are effective and contribute to the per-formance gain that was about 5% higher than the Base features only . Proposed achieved 70% pre-cision at 13% recall. We then estimated that, with the precision rate, we can extract 69,700 event causality pairs from the 2,451,254 event causality candidates, among which the estimated number of positive examples is 377,794.

Next we examined whether the semantic rela-tions that do not seem directly relevant to causality like M ATERIAL are effective. Table 4 shows that the performance degraded (46.27  X  45.86) when we only used the C AUSATION binary patterns and their entailing and entailed patterns compared to Proposed . Even when adding the P REVENTION ( X  X egative C AUSATION  X ) patterns and their entail-ing and entailed patterns, the performance was still slightly worse than Proposed . The performance was even worse when using no semantic relation ( X  X one X  in Table 4). Consequently we conclude that not only semantic relations directly relevant Table 5: Ablation tests on association features. Table 6: Average precision of our proposed meth-ods and baselines using CEA. to causality like C AUSATION but also those that seem to lack direct relevance to causality like M A -TERIAL are somewhat effective.

Finally, Table 5 shows the performance drop by removing the Wikipedia-, definition-, web-, and CEA-based features. The CEA-based fea-tures were the most effective, while the Wikipedia-based ones slightly degraded the performance. 5.1.2 Comparison to Baseline Methods We compared our method and two baselines based on Do et al. (2011): CEA uns is an unsupervised method that uses CEA to rank event causality can-didates, and CEA sup is a supervised method us-ing SVM and the CEA features, whose ranking is based on the SVM scores. The baselines are not complete implementations of Do et al. X  X  method which uses discourse relations identified based on Lin et al. (2010) and exploits them with CEA within an ILP framework. Nonetheless, we believe that this comparison is informative since CEA can be seen as the main component; they achieved a F1 of 41.7% for extracting causal event relations, but with only CEA they still achieved 38.6%.
Table 6 shows the average precision of the com-pared methods. Proposed is our proposed method. Proposed-CEA is Proposed without the CEA-features and shows their contribution. Proposed is the best and the CEA features slightly contribute to the performance, as Proposed-CEA indicates. We observed that CEA sup and CEA uns performed poorly and tended to favor event causality candi-dates whose phrase pairs were highly relevant to each other but described the contrasts of events rather than event causality (e.g. build a slow mus-cle and build a fast muscle ) probably because their Figure 1: Precision-recall curves of proposed methods and baselines using CEA.
 Table 7: Average precision of our proposed method and baselines using Cs . main components are PMI values. Figure 1 shows their precision-recall curves.

Next we compared our method with the base-lines based on Hashimoto et al. (2012). They de-veloped an automatic excitation template acqui-sition method that assigns each template an ex-citation value in range [  X  1 , 1] that is positive if the template is excitatory and negative if it is in-hibitory. They ranked event causality candidates by Cs ( p 1 , p 2 ) = | s 1 |  X  | s 2 | , where p 1 and p the two phrases of event causality candidates, and | s 1 | and | s 2 | are the absolute excitation values of p  X  X  and p 2  X  X  templates. The baselines are as fol-lows: Cs uns is an unsupervised method that uses Cs for ranking, and Cs sup is a supervised method using SVM with Cs as the only feature that uses SVM scores for ranking. Note that some event causality candidates were not given excitation val-ues for their templates, since some templates were acquired by manual annotation without Hashimoto et al. X  X  method. To favor the baselines for fairness, the event causality candidates of the development and test data were restricted to those with excita-tion values. Since Cs sup performed slightly better when using all of the training data in our prelimi-nary experiments, we used all of it.

Table 7 shows the average precision of the com-pared methods. Proposed is our method. Its av-erage precision is different from that in Table 6 due to the difference in test data described above. Cs uns and Cs sup did not perform well. Many Figure 2: Precision-recall curves of proposed methods and baselines using Cs . phrase pairs described two events that often hap-pen in parallel but are not event causality (e.g. re-duce the intake of energy and increase the energy consumption ) in the highly ranked event causality candidates of Cs uns and Cs sup . Figure 2 shows their precision-recall curves.

Hashimoto et al. (2012) extracted 500,000 event causalities with about 70% precision. However, as described in Section 1, our event causality crite-ria are different; since they regarded phrase pairs that were not self-contained as event causality (their annotators checked the original sentences of phrase pairs to see if they were event causality), their judgments tended to be more lenient than ours, which explains the performance difference.
In preliminary experiments, since our proposed method X  X  performance degraded when Cs was in-corporated, we did not use it in our method. 5.2 Future Scenario Generation To show that our future scenario generation meth-ods can generate many acceptable scenarios with reasonable precision, we experimentally com-pared four methods: Proposed , our scenario generation method without the two filters, Pro-posed+Orig , our method with the original sen-tence filter, Proposed+Orig+Comm , our method with the original sentence and common argument filters, and Exact , a method that chains event causality by exact matching.
 Beginning events As the beginning event of a scenario, we extracted nouns that describe social problems ( social problem nouns , e.g. deforesta-tion ) from Wikipedia to focus our evaluation on the ability to generate scenarios about them, which is a realistic use-case of scenario generation. We extracted 557 social problem nouns and used the cause phrases of the event causality candidates that Table 8: Number of scenario samples and their precision (%) in parentheses. consisted of one of the social problem nouns as the scenario X  X  beginning event.
 Event causality We applied our event causality extraction method to 2,451,254 candidates (Sec-tion 3.1) and culled the top 1,200,000 phrase pairs from them (See Section F in the supplementary notes for examples). Some phrase pairs have the same noun pairs and the same template polar-ity pairs (e.g. omit toothbrushing  X  get a cavity and neglect toothbrushing  X  have a cavity , where omit X and neglect X are inhibitory and get X and have X are excitatory). We removed such phrase pairs except those with the highest CScore , and 960,561 phrase pairs remained, from which we generated two-or three-step scenarios that con-sisted of two or three phrase pairs.
 Evaluation samples The numbers of two-and three-step scenarios generated by Proposed were 217,836 and 5,288,352, while those of Exact were 22,910 and 72,746. We sampled 2,000 from Pro-posed  X  X  two-and three-step scenarios and 1,000 from those of Exact . We applied the filters to the sampled scenarios of Proposed , and the results were regarded as the sample scenarios of Pro-posed+Orig and Proposed+Orig+Comm . Table 8 shows the number and precision of the samples. Note that, for the diversity of the sampled scenar-ios, our sampling proceeded as follows: (i) Ran-domly sample a beginning event phrase from the generated scenarios. (ii) Randomly sample an ef-fect phrase for the beginning event phrase from the scenarios. (iii) Regarding the effect phrase as a cause phrase, randomly sample an effect phrase for it, and repeat (iii) up to the specified number of steps (2 or 3). The samples were annotated by three annotators (not the authors), who were in-structed to regard a sample as acceptable if each event causality that constitutes it is plausible and the sample as a whole constitutes a single coherent story. Final judgment was made by majority vote. Fleiss X  kappa of their judgments was 0.53 (moder-ate agreement), which is lower than the kappa for the causality judgment. This is probably because Table 9: Estimated number of acceptable scenar-ios with a 70% precision rate. scenario judgment requires careful consideration about various possible futures for which individ-ual annotators tend to draw different conclusions. Result 1 Table 9 shows the estimated number of acceptable scenarios generated with 70% pre-cision. The estimated number is calculated as the product of the recall at 70% precision and the number of acceptable scenarios in all the gener-ated scenarios, which is estimated by the anno-tated samples. Figures 3 and 4 show the precision-scenario curves for the two-and three-step sce-narios, which illustrate how many acceptable sce-narios can be generated with what precision. The curve is drawn in the same way as the precision-recall curve except that the X-axis indicates the estimated number of acceptable scenarios. At 70% precision, all of the proposed methods out-performed Exact in the two-step setting, and Pro-posed+Orig+Comm outperformed Exact in the three-step setting.
 Result 2 To evaluate the top-ranked scenarios of Proposed+Orig+Comm in the three-step set-ting with more samples, the annotators labeled 500 samples from the top 50,000 of its output. 341 (68.20%) were acceptable, and the estimated num-ber of acceptable scenarios at a precision rate of 70% and 80% are 26,700 and 5,200 (See Section H in the supplementary notes). The  X  terrorist oper-ations  X  scenario and the  X  slash-and-burn agricul-ture  X  scenario in Section 1 were ranked 16,386th and 21,968th. Next we examined how many of the top 50,000 scenarios were acceptable and non-trivial , i.e., found in no page in our input web cor-pus, using the 341 acceptable samples. A scenario was regarded as non-trivial if its nouns co-occur in no page of the corpus. 22 among the 341 samples were non-trivial. Accordingly, we estimate that non-trivial scenarios from the top 50,000. (See Section G in the supplementary notes for exam-ples of the generated scenarios.) Discussion Scenario deforestation contin-ues  X  global warming worsens  X  sea temperatures rise  X  vibrio parahaemolyticus fouls (water) was generated by Proposed+Orig+Comm . It is written in no page in our input web corpus, a paper Baker-Austin et al. (2013) that observed the emerging vibrio risk in the Baltic sea due to global warming. In a sense, we  X  X redicted X  an event observed in 2013 from documents written in 2007, although the scenario was ranked as low as 240,738th. We proposed a supervised method for event causality extraction that exploits semantic rela-tion, context, and association features. We also proposed methods for our new task, future sce-nario generation. The methods chain event causal-ity by causal-compatibility. We generated non-trivial scenarios with reasonable precision, and  X  X redicted X  future events from web documents. Increasing their rank is future work.
