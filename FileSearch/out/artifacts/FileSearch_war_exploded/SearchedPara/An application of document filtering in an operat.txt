 1. Introduction
In a previous paper ( Lehner &amp; Wilson, 2008 ) a Bayesian approach to monitoring open news sources for disease outbreaks was tested. This approach detected geographic regions with unusual levels of disease-related reporting by counting the number of articles referencing selected keyword queries and then combining the results across the different queries with a Bayesian network.

Based in part on the research described in Lehner and Wilson, and concerns about Avian Influenza, ational Argus Watch Center was established to monitor disease activity worldwide. 2007), the Argus Watch Center employed approximately 25 analysts who monitored disease-related activities by reviewing the contents of over 6500 news sources, in 16 different languages, on a daily basis.
As often happens with academic and prototype studies, the concept of operations assumed by Lehner and Wilson in their original study was not entirely appropriate for the real world application for which it was intended. The system described by
Lehner and Wilson assumed that there would be just a few analysts allocated to disease monitoring and that the purpose of the system was to automatically alert analysts to regions where there were unusual substantial levels of disease-related reporting. By contrast, the Argus Watch Center is focused on early detection. With 25 analysts, each of whom reads one or more local language in their geographic region of coverage, there is no need to automatically alert analysts to substantial levels of reporting. Rather these analysts seek to find the earliest reports of an outbreak, long before there is substantial reporting.

The problem these analysts must deal with is handling the volume of news sources in their area of coverage. With 25 analysts and more than 6500 news sources, each analyst on each day must cover the material in approximately 260 news sources. Furthermore, since many governments endeavor to hide or minimize international recognition of outbreaks, the rel-evant information is often buried within an article or intentionally obfuscated with cover stories. Consequently analysts can-not simply skim headlines to find the relevant material. Relevant material is sometimes embedded in articles that do not directly discuss disease outbreaks.

Given the extensive volume of material that each analyst must cover, the methodology described in Lehner and Wilson was adapted to automatically assess each article in each news source so as to automatically provide each analyst with a daily  X  X  X ecommended reading list. X  In information retrieval research ( Manning, Raghavan, &amp; Schutze, 2008 ; Voorhes &amp; Harman, 2005) this type of automated article assessment is referred to as  X  X  X ocument filtering X  and there are a variety of algorithms pandemic, the decision was made to quickly adapt existing prototype capabilities to meet this need. The Argus Document
Ranking System (ADRS) that we consequently developed is a Bayesian-based approach to document filtering that is currently in daily use by Argus analysts. At the time of the initial writing of this paper, ADRS automatically generates the  X  X  X ecom-mended reading list X  for analysts for 23 geographic regions in 11 different languages.

This paper serves several purposes. First, we describe the algorithms used in ADRS. Second, we describe the statistical procedures used to estimate Recall and Precision in an operational environment. This procedure uses a stratified sampling scheme to estimate the proportion of relevant documents in the population as well as to estimate performance statistics.
Our approach to combining the filtering algorithm with stratified sampling to evaluate and improve the algorithm is an ap-proach to analyzing text that may generalize to other domains and other filtering algorithms. Third, we report on the opera-tional performance of ADRS for the 23 models initially fielded. Finally, we note some practical lessons learned that we believe are relevant to any operational document filtering system. 2. Overview of ADRS
Briefly described, the information infrastructure in the Argus Watch Center is composed of two functional subsystems: a text processing subsystem and the ADRS document filtering system.

The text processing subsystem, called isis-MITAP ( Damianos, Zarrella, &amp; Hirschman, 2004 ), provides a document archiv-ing and retrieval infrastructure upon which ADRS is built. Specifically, each day the text processing subsystem extracts all documents from selected on-line news sources, creates a keyword index for each document extracted, and stores the doc-ument for later retrieval. Documents are indexed and stored in the local language. Once indexed and archived, users can per-form a Boolean keyword query in the local language to retrieve selected documents.

As detailed below, ADRS provides analysts with an ability to develop, test, refine and put into production  X  X  X odels X  to fil-ter news articles in their geographic area of coverage. An ADRS model is composed of a set of queries and the parameters for each query that are needed to apply a Bayesian filtering algorithm. The parameter associated with a query corresponds to an estimate of how much more likely it is that the query will be satisfied by a document of interest to the analyst than by a document that wouldn X  X  be of interest. In operation, the ADRS model calculates a Relevance Score for each article based on the parameters of the queries satisfied by the article. ADRS also estimates the overall level of disease-related reporting in a region.

A summary of development steps for an ADRS model follows (details are provided below): 1. The analyst specifies news sources to cover and those are entered into ADRS (The analyst can select any subset of news sources that are archived by the text processing subsystem, but all news sources must be in the same language). 2. The analyst proposes Boolean queries in the local language that are entered into ADRS (The analyst can enter any queries that he or she desires, but many of the queries will reflect the same concepts, such as  X  X isease names X  or  X  X ublic panic, X  that are found in nearly all of the models). 3. ADRS presents to the user a list of articles, selected by stratified random sampling, which the analyst must sequentially are presented. Analysts are allowed to stop whenever they want. Analysts typically judge around 50 X 100 articles before stopping. The analysts X  judgments are entirely pragmatic. The analysts were instructed to judge an article as  X  X  X elevant X  only if they would want to read that article, and others like it, as part of their normal duties in outbreak detection and monitoring. 5 They received no other instructions. 4. Based on the analyst X  X  judgments, the parameter values for each query are updated by ADRS. 5. The analyst is provided feedback on the estimated Recall and Precision achieved by the model both before and after the parameters are updated, as well as on the behavior of individual queries. 6. Based on the performance feedback the analyst modifies the queries. 7. Steps 3 X 6 are repeated until the analyst is satisfied with the ADRS model X  X  performance.
 The Appendix A shows several screen shots from this process.

For the 23 models discussed in this paper, Steps 1 and 2 were completed in a few days. The remainder of the process typ-in changes or additions to the queries. During the first two iterations considerable effort is devoted to finding misbehaving ing those queries. The last two or three iterations were done to confirm that model performance had stabilized, did not result in any changes to the queries and typically involved feedback on around 150 articles in total.

All of these steps could have been performed by an analyst with content and language expertise working with ADRS di-rectly. However, for reasons discussed below, we found that it was more effective if this process was facilitated by a modeler with expertise in building ADRS models. The modeler reviewed the performance statistics in Step 5 and made suggestions to the analyst on how to improve the model. It remained the analyst X  X  responsibility to develop and modify queries and to pro-vide feedback on the relevance of selected articles. 3. ADRS algorithms for model development and document filtering
This section describes the ADRS algorithms for calculating Relevance Scores, how an ADRS model is developed for each analyst and the statistical procedures used to estimate ADRS model performance. These algorithms are described in text illustrated with examples. The algorithms are sufficiently straightforward that a formal mathematical description is unnecessary. 3.1. Calculating Relevance Scores
ADRS employs a Na X ve Bayes model, 6 with assumed prior odds at 0.01, to calculate the Relevance Score for each article. The functional form of the algorithm is identical to other Na X ve Bayes text categorization algorithms (see Sebastiani, 2002 for re-view), but varies from those approaches in how evidence items are specified and parameters are updated.
Specifically, ADRS maintains two likelihood ratios for each query (The procedure for deriving the likelihood ratios is de-not satisfy and the likelihood ratios for each query.

To illustrate consider the queries and likelihood ratios in Table 1 . Assume that a news article satisfies the queries  X  X  X os-stant 0.01. In this case
The Relevance Score is calculated by converting the Posterior Likelihood Ratio into a probability using a standard calcu-lation. Specifically,
Article Relevance Score
The Relevance Score of 0.826 may be interpreted as estimating that there is an 82.6% chance that the analyst will consider this article  X  X  X elevant X . Or put another way, around 83% of the time that the analyst reviews an article with an 83% Relevance
Score, the analyst will indeed decide that the article was worth reviewing; and around 17% of the time the analyst will decide after reviewing the article that it was not worth reading.
 If another article satisfies only  X  X  X anic X , then the Relevance Score is analyst will feel that he or she wasted her time reading this article. 3.2. Deriving and updating the likelihood ratios
The likelihood ratios are updated through analyst judgments on the relevance of randomly selected articles. The process for doing this is straightforward, but involves several steps.
 Each likelihood ratio is derived from Relative Frequency  X  X ata X .

Initially the relative frequencies for each query are set up using assumed values as shown in Table 2 . The likelihood ratios in Table 2 were derived from the relative frequencies as follows.

Likelihood ratio for Relevant Articles
Likelihood ratio for Irrelevant Articles
These relative frequencies, and by consequence the likelihood ratios, are updated by analyst judgments on the relevance of randomly sampled articles. Random sampling is stratified by the number of queries satisfied. Table 3 shows the sampling rate for the 23 models discussed in this paper.

That is to say, when providing analysts with articles to judge, ADRS randomly presents 1% of the available articles that three queries.

To illustrate the process of updating ADRS model parameters, assume that an analyst has just created a new model with the four queries in Table 1 above. The initially assumed relative frequencies are shown in Table 4 .

Now assume an analyst has provided relevance judgments on three articles that were randomly selected using the sam-pling rates in Table 3 (In practice, analysts typically provided feedback on 200 or more articles. As noted above, the analysts
The first article satisfied the query  X  X  X anic X  and did not satisfy the other queries. The analyst judged that this article is  X  X  X rrelevant. X  Because the article was judged  X  X  X rrelevant X , the Relative Frequency for Irrelevant Articles for all the queries is updated. For the queries that the article satisfies, the inverse of the sampling rate is added to both the numerator and denominator.
In this case, since the sampling rate for single query articles is 0.02, the numerator and denominator are both increased by rate.

Using the inverse of the sampling ratio ensures that all articles in the population, including those not sampled, are equally represented in the Precision and Recall estimates (described below). For example the sampling rate for articles that satisfy So analyst feedback on the first article changes the relative frequencies to those shown in Table 5 .

The second article satisfied the queries  X  X  X osquito X  and  X  X  X lood X  and was judged  X  X  X elevant X  by the analyst. Because the article was relevant, the Relative Frequency for Relevant Articles is updated for all the queries using the same procedure de-scribed above. Note that in this case the sampling rate is 0.05, so 20 rather than 50 is added to the appropriate numerators and denominators. This result is in Table 6 .

Finally the third article was judged  X  X  X rrelevant X  and satisfied the queries  X  X  X pidemic X  and  X  X  X anic. X  Table 7 shows the resulting relative frequencies.

Using the procedure for calculating the Posterior Likelihood Ratio described above, this results in the updated likelihood ratios shown in Table 8 .
 article is Irrelevant than if it is Relevant. Such queries are usually deleted from a model.

Readers familiar with Bayesian statistics will recognize this procedure as a simple adaptation of the standard procedure for updating a binomial distribution ( Gelman et al., 1995 ).

Finally, it is worth noting here that the rate at which this system learns parameter values depends on the quality of the not satisfy the intended topic of the query), then many more Irrelevant Articles will be sampled that satisfy 3 or more que-feedback process that they judge to be relevant. Mathematically this implies that the Relative Frequency for Relevant Articles to identify and repair individual queries that behave very badly. 3.3. Total Relevance Scores
Although this is not relevant to document filtering, we do note as an interesting aside that analysts are also provided a histogram of the sum of the Relevance Scores for all articles retrieved in the previous period observed. This histogram pro-vides a quick graphic depicting the level of disease-related reporting over time. Fig. 1 shows the Total Relevance Score gra-phic for a model covering English language news sources in India in 2003. These are the same news sources and period used for the test described in Lehner and Wilson.

The three periods of increased disease-related reporting correspond respectively to the SARS outbreak, an outbreak of a  X  X  X ystery virus X  that was never definitively diagnosed and a severe Dengue outbreak. We note that this particular model did not include keywords for  X  X  X ARS X ,  X  X  X engue X  or any other direct disease name.
As noted earlier, the original concept of operation for disease monitoring assumed Lehner and Wilson was that there would be just a few analysts monitoring outbreaks worldwide. In this operational concept the histograms that monitor the quantity of disease-related reporting would have been used to automatically alert analysts of new outbreaks. 3.4. Estimating performance statistics
Each morning in the Argus operating environment, ADRS presents the Argus analysts with the ADRS-calculated priori-are ordered by the relevance scores. Generally analysts will only read the articles with the highest Relevance Scores. For the cles. This corresponds to standard metrics for information retrieval performance: Recall Proportion of Relevant Articles that receive a high Relevance Score Precision Proportion of articles receiving high Relevance Scores that are relevant
We estimate Recall and Precision as follows. 1. Randomly select articles from the population using the stratified sampling scheme described above. 2. Have an analyst judge whether each of the sampled articles are  X  X  X elevant X ,  X  X  X ossibly Relevant X  or  X  X  X rrelevant X . 3. Calculate the estimated proportions of articles in the population that are relevant or irrelevant. 4. Calculate an estimate of the distribution of Relevance Scores for the population of Irrelevant Articles. 5. Calculate an estimate of the distribution of Relevance Scores for the population of Relevant Articles. 6. Calculate an estimate of Recall and Precision from the estimates in 3 X 5.
 To illustrate this process consider a case where an analyst evaluated 48 articles, of which 10 were judged as  X  X  X elevant X . Assume the results in Table 9 .
 In words, there were 30 articles that satisfied 0 queries. All of these articles were judged  X  X  X rrelevant X  and all received a
Relevance Score less than 0.5. There were three articles that satisfied exactly one query all of which had a Relevance Score fied two queries, etc.

Using the inverse of the sampling rate as a weight (e.g. 30 articles at 0.01 sample rate represents the equivalent of 3000 articles), we summarize the results in Table 9 as shown in Table 10 .
 From Table 10 we estimate
Approximately 3% of all the articles in the population are relevant and the daily automatically generated  X  X  X ecommended reading list X  will on average contain slightly less than 50% of those Relevant Articles. This particular model is expected to miss a lot of relevant material. In addition, one third of the articles (33%) in the  X  X  X ecommended reading list X  are expected to be irrelevant.

An examination of Table 9 indicates that the low estimated Recall score in this hypothetical example is due to just one is removed then Precision stays the same, but Recall increases from 0.47 to 0.98.
 Although this example is hypothetical, the results in Table 9 are typical for an ADRS model still in early development.
Often there are one or two articles which the analyst judges as  X  X  X elevant X , but for which there are no keyword queries or for which the keywords provided are not sufficiently discriminating. This implies that the existing queries missed a relevant  X  X  X elevant X , and to then identify one or more additional queries for that topic.

We note that the volatility of the performance estimate in the early stages of model development is not an inherent char-acteristic of our approach. We could have substantially eliminated this volatility by increasing the sampling rate of articles that satisfy 0 or 1 queries. By doing so the impact of any one article is dramatically reduced. But this would have also sub-stantially increased the number of articles presented to analysts that they would judge  X  X  X rrelevant. X  We chose a low sam-pling rate to minimize the burden on the analysts.

The reader may have noticed that our procedures for estimating Recall and Precision completely ignore the articles in the  X  X  X ossibly Relevant X  category. We did this simply because  X  X  X ossibly Relevant X  means that there is no  X  X  X round truth X  as to whether or not an article is of interest. Lacking this ground truth, we could think of no defendable approach to including ative to the population of articles that analysts can definitively say are, or are not, of interest. 4. Operational performance of ADRS
This section describes the performance of ADRS when applied to the topic area of disease surveillance in open news sources. Please note that we make no claim that ADRS is superior other document filtering algorithms or that these results extend to other types of sources and topic areas. This section simply describes the performance of ADRS in this application and topic area.

As noted above, at the time of this initial writing a  X  X ypical X  analyst was responsible for covering the material in approx-imately 260 news sources. If we conservatively estimate that a typical news source contains 50 articles, then a typical analyst is responsible for covering the material in approximate 13,000 news articles  X  each day. In operation, analysts are presented from day to day, depending on whether an outbreak event is being reported. If there is reporting on a topic that generally receives a high Relevance Score, then an analyst may see dozens of articles with a high Relevance Score. Nearly all of these will be variations of the same story being repeated in different news sources. When nothing of interest is happening, an ana-lyst will see few, if any, articles with high Relevance Scores. 4.1. Recall and Precision
For the 23 models described in this paper model development and model performance evaluation co-occurred. As noted above, in Step 3 of the model development process analysts judge the relevance of articles selected by stratified random sam-pling. These analyst judgments can then be used to both evaluate the model and to update model parameters. After each round of analyst judgments tables such as Tables 9 and 10 , along with Precision, Recall and various additional estimates, are generated both before and after the parameters are updated. Pre-update estimates are estimates of how the model would perform in operation if the parameters remained unchanged. The post-update estimates reflect the expected improvement that should result from the parameter changes. The post-update estimates are of course biased since performance is esti-mated for the same articles that were used to update the model parameters. For the 23 models described herein model iter-ation continued until estimated Recall appeared to stabilize at 0.9 or better and estimated Precision is acceptable (at least 0.3).

Table 11 shows the estimated performance scores that were recorded from the last round of judgments for each of the 23 regions. This table uses Relevance Score = 0.1 as the threshold. There are two models (Africa English language and Russia
Region 1) that seem to violate the  X  X tabilize at 0.9 Recall X  described above. However, both models had previous data indicat-ing they were performing well so we simply recorded the results of the final iteration and put the models into production.
Based on the reported results in Table 11 and other considerations discussed below we informally estimate that for these 23 models in particular, and the topic area of outbreak surveillance in general, ADRS achieves approximately 90% Recall and 50% Precision. We say  X  X nformally X  because, as elaborated below, there are some considerations that weaken the strength of the statistical evidence, but there are some nonstatistical considerations that reinforce these estimates.
Regarding the statistical procedures, we first note that these models were developed for an operational watch center looking for indications of near term threats such as an Avian Influenza outbreak. There was pressure to field these models quickly. As soon as the models appeared satisfactory, we rushed them into production. Many of the models shown in Table 11 would have benefited from some additional work. More importantly, for reasons described below, the accuracy of our performance estimates would have benefited from more data points.

In retrospect we believe the pressure to field these models quickly potentially led to a statistical bias toward overestimat-ing performance. Specifically, the fact that we continued iterative model refinement until model performance appeared sta-ble and satisfactory is a de facto optimal stopping criterion that introduces the possibility that we simply stopped  X  X  X hen we these 0-query articles is relevant. Assume further that 10% of the remaining articles, which all satisfy 1 or more queries, are satisfy 0-queries. In this scenario true Recall is at most 53%. Now assume that our procedure to estimate Recall is based on analyst judgments on 150 sampled articles of which  X  satisfy 0 queries. In this scenario, the probability of sampling 1 or true Recall is at most 53%. Other hypothetical scenarios can be constructed in which the potential overestimation problem is either mitigated or worse.

Offsetting this concern is the fact that we obtained generally consistent results across 23 models in 11 different lan-guages. This seems unlikely to occur if true Recall is low, but admittedly could occur if true Recall is not quite as good as estimated (e.g. 75% rather than 90%). In addition we did follow-up with many of the analysts to determine how well their
ADRS models were working. We received some complaints that the models had missed relevant information, but when we examined the basis for those complaints we discovered that in all cases the missing information was found in sources not covered by their ADRS models (e.g. Blogs, NGO reports, etc.). None of the follow-ups led to model changes. Third, all of the
ADRS models covered the same topic area  X  disease surveillance. As analysts discovered new things of interest in one geo-graphic area that discovery was generally communicated to the other analysts. Consequently, many of the models reflect a mature, comprehensive knowledge of indicators to monitor. There was relatively little room for discovery of new  X  X ajor X  (i.e. frequently reported) topics that would invalidate a high Recall estimate.

On balance, we believe that our overall estimates of 90% Recall and 50% Precision across the different models is accurate, but this conclusion is not warranted by the statistical evidence alone. 4.2. Calibration
Another metric commonly used to measure the accuracy of probability estimates is calibration  X  which measures the ex-tent to which probability estimates of happen around percent of the time (see Lichtenstein, Fischhoff, &amp; Phillips, 1982 ). other hand 80% of the articles receiving a score of 0.3 are relevant, then the 0.3 estimates are clearly not calibrated.
Table 12 shows the estimated proportion of Relevant Articles for ten distinct probability bands. It is clear from column 2 that ADRS is not well calibrated. Of particular note is the estimate that 70% of the articles receiving a Relevance Score be-tween 0.1 and 0.2 were Relevant, and 57% of the articles in the 0.2 X 0.3 range are Relevant. Mitigating this result somewhat is the fact that the sample sizes for the mid range bands (0.1 X 0.9) are relatively small, so we would expect the estimated proportions to be less accurate and unstable. Low Relevance Scores (less than 0.1) are clearly calibrated. High Relevance
Scores (more than 0.9) are moderately calibrated with 78% as the estimated proportion of Relevant Articles. We suspect that the calibration of high Relevance Score articles cannot be improved upon since, as we noted above, analysts often rate arti-cles about endemic diseases as  X  X  X rrelevant. X 
These results naturally lead one to wonder how the ADRS filtering approach could do well on Recall and Precision, yet do poorly on Calibration. The answer is that Relevance Scores were typically near the extremes  X  near 0.0 or 1.0. The estimated bands between 0.1 and 0.9. Consequently any calculation of Precision and Recall will be dominated by the observed Rele-vance Scores near 0.0 or 1.0; and therefore will be largely insensitive to where the threshold is set. Poor calibration in the middle range (Relevance Scores between 0.1 and 0.9) will have a small impact on overall Recall and Precision. 5. Anecdotal lessons learned
Below we note some practical lessons learned that we believe are applicable to other applications of information retrieval technology. 5.1. Need for modeling expert
ADRS was designed for analysts to develop their own filtering models. This, we discovered, was often impractical. Analyst time was limited. They were busy. While they were willing to devote a few hours to model development, they generally ex-pected to see a quick return on the time they invested. Without this obvious return on investment, they became hostile.
In order to quickly develop a high Recall X  X recision model it was important that the model developer be able to quickly identify and resolve problems. By themselves analyst had neither the knowledge nor patience needed to resolve problems.
Rather they tended to assume that any performance problems they encountered were inherently systemic and quickly ceased model development.

All 23 of the models described in this paper were developed with the aid of a  X  X  X odeler X . In most cases the modeler X  X  role was limited to interpreting the performance statistics and to providing guidance to the analysts on which queries to delete or add. In most cases the modeler could not read the local language queries or articles; so all advice was based on the estimated performance statistics. It was still the analyst X  X  responsibility to propose, enter and/or revise queries; and provide feedback on the ADRS-selected articles. There is a dedicated modeler in the current operational environment. 5.2. Keeping things simple
Initially ADRS was based on the operational concept and algorithms described in Lehner and Wilson (2008) , which in-volved complex Boolean queries and a complex Bayesian network to integrate the returns of those queries. As work pro-ceeded and performance difficulties were uncovered, we discovered that performance improved dramatically when we simplified things. Some important simplifications are described below. 5.2.1. Na X ve vs. complex models
The model described in Lehner and Wilson was a complex Bayesian network with many non-independent nodes. For example, in that model many of the evidence nodes were conditionally dependent on an  X  X  X nformation suppression X  node which indicated whether a government was actively trying to hide an outbreak. This reflects the reality that news reporting on an outbreak will be severely suppressed if reporters are threatened with prison. Nevertheless, informal testing of a Na X ve
Bayes version of that model, which ignored information suppression entirely, revealed no performance decrease despite the obvious lack of realism. Moving to a simple Na X ve Bayes model structure made it possible to reduce development time to a few hours. 5.2.2. Simple vs. complex queries
The queries in Lehner and Wilson typically involved 20 or more keywords arrayed in a complex Boolean structure. Each current operational models are comprised of a single keyword. Complex Boolean queries (many Ands and Ors) were more problematic than helpful. Complex queries often performed poorly and we typically found that the performance problems were caused by a single keyword in the query. Of course the problematic keyword could only be identified by testing each keyword independently. In general, we just found it far more effective to devote time and effort to finding simple, somewhat discriminating keywords than to carefully develop, test and refine complex queries. 5.2.3. Ignoring geography
All of the queries in Lehner and Wilson included references to local geography (e.g. one or more city names). This was done to ensure that articles reporting on diseases in other regions and countries were not retrieved. As it turns out, fil-tering on geography was counterproductive. Local newspapers rarely report on disease outbreaks in other parts of the world unless they are of some interest to the local population. As a consequence, analysts would sometimes mark such articles as  X  X  X elevant. X  Our informal observations suggest that filtering on geography increases Precision, but reduces
Recall. 5.3. Embed empirical evaluation in the development process
In practice, things never seem to work out as they should in theory or as they do in the laboratory. Individuals often use this fact to argue that research is irrelevant to their work; and that they should base their practices on what works in the experience. However, there is growing evidence indicating that expert practitioners are sometimes poor discrim-inators of what does and does not work in practice and that scientific rigor is needed to determine what really works (e.g., Pfeffer &amp; Sutton, 2006 ). In this effort we incorporated an empirical evaluation process into the development cycle to try to bring some rigor to our development decisions. This as it turned out had two significant benefits. First, the empirical feedback moved us away from our initial approach as articulated in Lehner and Wilson and toward solutions that were both counterintuitive and counter to our previous research result. Second, our ability to claim that we would have an objective measure of performance made it much easier to convince the customer to invest in what was from their perspective an  X  X dvanced X  approach to document filtering and to convince the analysts to spend some time to  X  X  X ry it out X .
 Overall we attribute the success of ADRS to our ability to quickly and repeatedly develop, test, refine and retest models.
And this ability to rapidly iterate depended on model simplicity. Complex models were simply not suitable for rapid test, refine, and retest iterations. Rapid iteration also required the assistance of someone devoted to model development. Analysts had the tools needed to develop models on their own, but not the knowledge. 6. Discussion
Overall the results reported here are encouraging for this application and for the applicability of information retrie-val algorithms in general. The performance of ADRS seems comparable to the reported performance of many document filtering algorithms that have undergone rigorous testing as reported in the Text Retrieval Conference (TREC) series ( Voorhes &amp; Harman, 2005 ). On the other hand, other instances of Bayesian classifiers for document filtering appear to report slightly better performance (e.g. Genkin et al., 2006 ). In our opinion these results should not be interpreted as providing evidence that ADRS algorithms are either a superior or weaker algorithm than other document filtering algorithms. The test problems are not easily compared to the application described here. For example, in TREC the algorithms were tested against many diverse topics; with no opportunity to tailor the algorithms to a specific topic.
ADRS in contrast was iteratively tailored to address a specific problem. On the other hand, ADRS performance is con-sistent across multiple languages. We view the results reported here as indicative of the utility of information retrieval technology in general.

The general purpose nature of the TREC-tested algorithms is important here for another reason. Given the success and popularity of the ARGUS program (e.g. Kaplan &amp; Whitelaw, 2006 ), there is considerable interest in expanding the range of topics that ARGUS addresses ( Turner, 2006 ). Yet information retrieval research indicates that the performance of document will likely be a need to test and incorporate information retrieval approaches into the ARGUS environment that go well be-yond ADRS. We hope this paper stimulates an interest in pursuing such applications.

Finally, we note that much of the interest in ARGUS is associated with the ability to detect spikes in topic related report-ing, such as shown in the Total Relevance Score display in Fig. 1 above. Any document filtering algorithm could generate such displays. We therefore believe that dynamic monitoring and analysis of trends in document collections is an important emerging application of information retrieval technology.
 Appendix A A.1. Example screen shots
The attached screen shots are from the version of the document filtering system described in this paper. These should give the reader a feel for the operation of the system.
 Note that the reference to Rapid Argus Modeling for Biosurveillance Operations (RAMBO) is the same as what we called Argus Document Ranking System (ADRS) in the main body of this paper.
 The first step in building a new filter requires an analyst to designate the language of the source articles.
The second major step is to designate the news sources the analyst wishes to monitor. Third the users enter an initial set of queries The analyst can view and modify the queries at any time. A typical model has about 100 queries.
 paper, and asks the analyst to rate each paper as to whether or not it was worth reading.

After rating the randomly selected articles, the analyst is provided feedback on the performance of individual queries as well as the overall performance of the filter. In the display below, for example, Recall (HR row in bottom left) is estimated to be 100% for any threshold between 0.1 and 0.5, while Precision varies from 0.333 if the threshold is set at 0.1 and 0.6 if the threshold is set at 0.5. Usually a dedicated modeler reviews these results with the analyst to determine if performance is acceptable and to identify specific queries that need to be revised.

Once performance is deemed acceptable, then the analysts can set up when he or she wants to run the model and receive their daily recommended readings.
And finally the daily reading list is simply a prioritized list of articles which the analyst typically reads in the native language or in English if a translation engine available for that language.
 References
