 In order to enable scalable querying of graph databases, in-telligen t selection of subgraphs to index is essen tial. An impro ved index can reduce response times for graph queries signi X can tly. For a given subgraph query , graph candidates that may contain the subgraph are retriev ed using the graph index and subgraph isomorphism tests are performed to prune out unsatis X ed graphs. However, since the space of all possible subgraphs of the whole set of graphs is prohibitiv ely large, feature selection is required to identify a good subset of subgraph features for indexing. Thus, one of the key is-sues is: given the set of all possible subgraphs of the graph set, which subset of features is the optimal such that the algorithm retriev es the smallest set of candidate graphs and reduces the number of subgraph isomorphism tests? We in-troduce a graph searc h metho d for subgraph queries based on subgraph frequencies. Then, we prop ose several novel feature selection criteria, Max-Precision, Max-Irredundan t-Information, and Max-Information-Min-Redundancy , based on mutual information . Finally we show theoretically and empirically that our prop osed metho ds retriev e a smaller candidate set than previous metho ds. For example, using the same number of features, our metho d impro ve the pre-cision for the query candidate set by 4%-13% in comparison to previous metho ds [25, 26]. As a result the response time of subgraph queries also is impro ved corresp ondingly . H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| Query formulation,R etrieval models ; I.5.2 [ Pattern Recognition ]: Design Metho dology| Fea-ture evaluation and selection Algorithms, Design, Experimen tation Cop yright 2009 ACM 978 X 1 X 60558 X 512 X 3/09/11 ... $ 5.00. Graph mining, feature selection, index pruning, graph searc h
Graphs have been used to represen t structured data for a long time. Recen tly, more and more structured data, such as chemical molecule structures [20, 22], DNA and protein structures [10], social networks [5], social and citation net-works [14], and XML documen ts [29], are identi X ed and studied. Large number of such databases are available on the Web. Data mining and searc h metho ds for structured data are needed for users to quickly identify a small subset of relev ant data for further analysis and experimen ts.
End-users pose graph queries to a graph database and seek to retriev e its support , i.e., all graphs of which the queried graph is a subgraph (Figure 1). Graph query answ ering has been addressed while determining chemical structure simi-larity [17]. The crux of the problem lies in the complexit y of subgraph isomorphism. However, since subgraph isomor-phism is NP-complete [26], it is prohibitiv ely expensiv e to scan all graphs in real time. When the number of graphs are large, to enable fast querying, instead of isomorphism tests on the  X  X , we need to index the graphs o X ine to allow for fast graph retriev al. Indexing all possible subgraphs of graphs is impossible because of their sheer number. Thus, intelligen t indexing techniques and index pruning techniques for graph databases are essen tial to enable scalable querying. Graph database querying is decomp osed into three stages [26]: 1) graph mining, 2) graph indexing, and 3) graph querying. First, from the set of graphs in the database certain sub-graph are extracted and selected. Then each selected sub-graph is converted into a canonical string, and each graph is mapp ed into a linear space of subgraphs. Second, a graph index is built using the canonical strings of subgraphs. Fi-nally , for a given subgraph query , all the indexed subgraphs of the query are determined, and the index is looked up with these subgraphs to obtain a candidate set of graphs contain-ing all the indexed subgraphs. Subgraph isomorphism tests are performed on the candidate set to  X nd all graphs that contain the query graph. This candidate set must be small for the graph query if the graph retriev al is in real-time. To keep the candidate set small as well as keep the index size reasonable, we need to select subgraphs judiciously for in-dexing. We address the issue of  X nding the set of subgraphs to index such that the candidate set of subgraphs on which the subgraph isomorphism is computed is the smallest.
All previous approac hes [13, 11, 15, 25, 26] try to  X nd an appropriate way to disco ver a set of subgraphs that contains as much information as possible to achieve a high precision of query answ ers. However, no previous work prop oses any criterion to measure the information contained in the set of subgraphs. Most of them only assume frequent subgraphs are more informativ e than infrequent subgraphs [13, 11, 15, 25], where a frequent subgraph is a subgraph that occurs more frequen tly in a graph database than a threshold value. Some of them  X nd frequen t subgraphs indep enden tly and ignore the redundancy between subgraphs [13, 11, 15]. For example, for a set of graphs D , all subgraphs G 0 i of a fre-quen t subgraph G 0 are also frequen t. If G 0 occurs every time when G 0 i occurs, and does not occur indep enden tly very of-ten, then G 0 i is redundan t. Selecting G 0 i after having selected G 0 cannot increase the information contained in the feature set as expected. In other words, two informativ e features with high redundancy contain less over-all information than two informativ e features with low redundancy . Some pre-vious works prop ose heuristics to avoid selecting redundan t features to some exten t [25, 6, 26]. However, all these meth-ods only remo ve the redundancy partially . We prop ose novel metho ds that remo ve the redundancy as much as possible while indexing such that while looking up the index, fewer false positiv es are selected as candidates on which the full graph isomorphism has to be performed.

All previous works select subgraphs indep enden tly or se-quen tially , but none of them prop ose any criteria to mea-sure the information contained in a set of subgraphs. Meth-ods that select subgraphs indep enden tly ignore the redun-dancy between subgraph features. Sequen tial subgraph se-lection metho ds  X nd frequen t subgraphs from the smallest to the largest size and avoid selecting a subgraph when its supergraph has been selected. However, they do not con-sider highly correlated subgraphs that are not a supergraph-subgraph pair. Previous e X orts [25, 6] focus on two criteria, i.e., informative subgraph principle and/or irredundant sub-graph principle , although they have di X eren t heuristics to estimate information and redundancy . However, they are not shown theoretically to have an optimal or near optimal performance; we do so for our metho ds. This is the major contribution in this paper.

We prop ose a subgraph selection criterion based on mutual information called Max-Irredundan t-Information (MII), which is an appro ximation of Max-Precision, which can measure the over-all information content of a subgraph set and at-tempts to maximize the precision of retriev ed subgraphs over which subgraph isomorphism has to be computed. However, computing the information content of all possible subset of selected subgraphs is expensiv e. Thus, we prop ose a metho d Max-Information-Min-Redundancy (MImR), which appro x-imates the MII metho d, and combined with a greedy feature selection algorithm is much more computationally e X cien t. Our approac h is di X eren t from previous work in that: 1) we optimize the evaluation criterion of precision directly , 2) we use a probabilistic model based on mutual information to appro ximately optimize precision, and 3) we combine the in-formativ e and irredundan t principles naturally using a prob-abilistic model and  X nd an appro ximation metho d to  X nd the near-optimal subgraph feature set to index. The pro-posed metho ds are expected to outp erform previous ones, because we prove theoretically that they can achieve the op-timal or near-optimal precision. Furthermore, Yan, Yu, and Figure 1: Subgraph query (a) and its supp ort (b, c) Han [26] only consider the occurrence of subgraphs in graphs as binary features for searc h. We utilize the subgraph fre-quencies (i.e., the number of times each subgraph occurs) in each graph to prune out more graphs that do not contain the query graph.

Our major contributions are as follows:
The rest of this paper is organized as follows. In Section 2, we review related works. In Section 3, we describ e our graph searc h process. In Section 4, we  X rst prop ose an algorithm for mining indep enden t frequen t subgraphs. Then we pro-pose three criteria to further reduce the set of selected inde-penden t, frequen t subgraphs. Finally ,we describ e a greedy algorithm for subgraph selection. Section 5 presen ts exper-imen tal results validating our algorithms. Conclusions and future work are discussed in Section 6.
Prior work on querying graphs fall into three categories: graph pattern mining, indexing, and searc h. Work on graph mining extract features from graphs to map them into a linear space for indexing. Three types of features can be used, paths only [18], trees only [28], and subgraphs [26]. Most previous approac hes index subgraphs, because paths and trees are special subgraphs that lose much structural information due to their limited represen tativ e capabilities. After the subgraph features are extracted, canonic al labeling or an isomorphism test is used to determine if two graphs are isomorphic to each other. A canonical label is a unique string corresp onding to a graph, such that there is a one-to-one mapping function between graphs and canonical labeling strings. Both canonical labeling and isomorphism tests are NP-complete [26, 13]. Canonical strings of subgraphs based on canonical labeling are used for graph indexing and searc h.
Since there are too many subgraphs to index, feature se-lection is required. A naive idea is to select frequen t sub-graphs only [8]. There are several algorithms for mining frequen t subgraphs, AGM [11], FSG [13], gSpan [26], FFSM [10], Gaston [15], and others [2, 1]. Worlein, et al., [24] pro-vide a quan titativ e comparison of the subgraph miners. Fis-cher and Meinl provide an overview of graph-based molec-ular data mining [9]. Indexing the full set of frequen t sub-graphs results in very large indices, because many of the frequen t subgraphs are redundan t. Previous works usually use the following approac hes to reduce the set of indexed subgraphs: 1) mining closed frequen t subgraphs, where only each subgraph that is not 100% correlated with any of its supergraphs are selected [25], 2) mining for each frequen t subgraph that has correlations with all of its supergraphs lower than a threshold [6], 3) sequen tially selecting sub-graphs from the smallest size to the largest size based on subgraphs' frequency and discrimination [26], and 4) using paths comp osed of small basic structures, such as cycles, crosses, and chains, instead of vertices and edges [12].
For chemical structure searc h, previous metho ds fall into four categories: 1) full structur e search : searc h the exactly matc hed structure as the query graph, 2) substructur e search :  X nd structures that contain the query graphs [19, 26], 3) full structur e similarity search : retriev e structures that are sim-ilar to the query graph [17], and 4) substructur e similarity search :  X nd the structures that contain a substructure that is similar to the query graph [27].

Previous works on index pruning for IR usually prune postings of irrelev ant terms in each documen t [4, 3, 7]. Cri-teria in information theory are applied to measure the in-formation of each term in each documen t. However, most previous works focus on selecting informativ e terms with-out considering redundancy [4, 3]. Moura, et al., consider local information of phrases to keep consisten t postings of correlated terms, instead of global information for feature selection [7]. Peng, et al., prop ose feature selection focusing on supervised learning [16]. The generic goal is to  X nd the optimal subset from the set of feature candidates so that selected features are 1) correlated to the class distribution, and 2) uncorrelated to each other. We extend the idea of feature selection to graph searc h (see in Section 4.2).
In this section, we introduce preliminary notations, and then give an overview of how a subgraph query is processed. Table 1 lists the notations used throughout the paper. We consider only conne cted labeled undir ected (sub)gr aphs . Relev ant notations are given as follows:
De X nition 1. Labeled Undirected Graph: A labeled undirected graph is a 5-tuple, G = f V; E; L V ; L E ; l g , where V is a set of vertices. Each v 2 V is an unique ID rep-resen ting a vertex, E  X  V  X  V is a set of edges where e = ( u; v ) 2 E; u 2 V; v 2 V , L V is a set of vertex la-bels, L E is a set of edge labels, and l : V [ E ! L V [ L function assigning labels to vertices and edges on the graph.
De X nition 2. Connected Graph: A path p ( v 1 ; v n +1 ) = ( e 1 ; e 2 ; :::; e n ) ; e 1 = ( v 1 ; v 2 ) ; e 2 = ( v 2 ; v e 2 E G ; i = 1 ; 2 ; :::; n; on a graph G is a sequence of edges connecting two vertices v 1 2 V G ; v n +1 2 V G . A graph G is connected i X  8 u; v 2 V G , a path p ( u; v ) always exists. The size of a graph Size ( G ) is the number of edges in G . De X nition 3. Subgraph and Connected Subgraph: A subgraph G 0 of a graph G is a graph, i.e., G 0  X  G , where V G 0  X  V G , and E G 0  X  E G where 8 e = ( v 1 ; v 2 ) 2 E two vertices v 1 ; v 2 2 V G 0 . G is the supergraph of G we say G contains G 0 . A subgraph G 0 of a graph G is a connected subgraph if and only if it is a connected graph.
Note that if we change the IDs of vertices on a graph, the graph still keeps the same structure. Thus, a graph isomorphism test is required to identify whether two graphs are isomorphic (i.e., the same) to each other [26]. Another metho d to achieve the function of graph isomorphism tests is to use canonical labels of graphs [26]. Usually if there is a metho d to sequen tialize all isomorphic graphs of the same graph into di X eren t strings, then the minim um or maxim um string is the canonical labeling. Two graphs are isomorphic to each other, if and only if their strings of canonical labeling are the same. Thus, strings of canonical labeling of subgraph features can be used to index graphs for fast searc h. We provide two de X nitions below:
De X nition 4. Graph Isomorphism and Subgraph Iso-morphism: A graph isomorphism between two graphs G and G 0 , is a bijectiv e function f : V G ! V G 0 that maps each vertex v 2 V G to a vertex v 0 2 V G 0 , i.e., v 0 = f ( v ), E
G ; ( f ( u ) ; f ( v )) 2 E G 0 and l G ( u; v )) = l G 0 f is a bijectiv e function, a bijectiv e function f 0 : V also exists. A subgraph isomorphism between two graphs G 0 and G is the graph isomorphism between two graphs G 0 and G 00 , where G 00 is a subgraph of G .
 De X nition 5. Canonical labeling: A canonical labeling CL ( G ) is a unique string to represen t a graph G , where given two graphs G and G 0 , G is isomorphic to G 0 i X  CL ( G ) = CL ( G 0 ).

As mentioned before, both isomorphism tests and canon-ical labeling can be used to determine if two graphs are isomorphic. The canonical labelings of selected subgraph features are indexed for graph searc h.
In this section, we  X rst provide some de X nitions, and then introduce an algorithm to answ er subgraph queries.
De X nition 6. Supp ort, Supp ort Graph, and Sub-graph Query: Given a data set D of graphs G , the support of subgraph G 0 , D G 0 , is the set of all graphs G in D that contain G 0 , i.e., D G 0 = f G j G 2 D; G 0  X  G g . Each graph in D
G 0 is a support graph of G 0 . j D G j is the number of sup-port graphs in D G . A subgraph query G q seeks to  X nd the supp ort of G q , D G q . Algorithm 1 Graph Searc h of Subgraph Query Algorithm: GSSQ ( G q , S , Index D ): Input: Query Subgraph G q , indexed subgraph set S , and index of the graph set D , Index D .
 Output: Supp ort of G q , D G q . 1. if G q is indexed,  X nd D G q using Index D ; return D G 2. D G q = f;g ;
 X nd all subgraphs of G q , G 0 q 2 S with F G 0  X  G q ; 3. if no G 0 q is found, D G q = D ; 4. else for all G 0 q do 5. Find D G 0 6. for all G 2 D G q do 7. if subgraphIsomorphism( G q , G )==false, remo ve G ; 8. return D G q ;
Like words are indexed to supp ort documen t searc h, sub-graphs are indexed to supp ort graph searc h. Note that sub-graphs may overlap with each other. We de X ne subgraph frequency as follows: De X nition 7. Embedding and Subgraph Frequency: An embedding of a subgraph G 0 in a graph G , i.e., Emb G 0  X  G is an instance of G 0  X  G . The frequency of a subgraph G in a graph G , i.e., F G 0  X  G , is the number of embeddings of G 0 in G . Embeddings may overlap.

Algorithm 1 shows how a subgraph query can be an-swered. First, if the query graph G q is indexed, its supp ort is returned directly . Otherwise, the algorithm identi X es all G i.e., all indexed subgraphs of G q , with their corresp onding frequency F G 0 where G satis X es F G 0 subgraph isomorphism tests on each G to identify whether it contains G q . For each selected subgraph G 0 q  X  G q , each supp ort graph G of G q must also be a supp ort graph of each graph query G 0 q .

Note that besides using subgraph frequencies, we also can use binary features to represen t if a subgraph occurs in a graph or not. Many previous works use binary features [25, 26]. In this case, F G 0 means that if for each subgraph G 0 q occurring in G q , G occurs in G , then G is a candidate of the supp ort of the query G q . This is because each occurrence of G 0 q in G occurs in each supp ort graph of G q , G . Thus, the intersec-tion of the supp ort of each G 0 q with F G 0 contain all supp ort graphs of G q . As mentioned before, how to extract and select subgraphs for indexing and querying is the key issue. A subgraph G 0 is frequent if the size of its supp ort j D G 0 j X  F min , the minim um threshold of subgraph frequency .
In this section, we describ e the algorithm to disco ver in-dependent frequen t subgraphs from a set of graphs. Then we introduce three feature selection criteria, MP, MII, and MImR, and  X nally , we prop ose a greedy algorithm to se-lect irredundan t and informativ e subgraph features sequen-tially from the disco vered indep enden t frequen t subgraphs for graph indexing and searc h.
 Algorithm 2 Indep enden t Frequen t Subgraph Mining Algorithm: IFGM ( D , F min , F max , Corr max ): Input: Set of graphs D , minimal and maximal threshold of frequency F min and F maximal threshold of correlation Corr max .
 Output: Set of Indep enden t Frequen t Subgraphs F G , each subgraph has a list of supp ort graphs with corresp onding frequencies. 1. Initialization : F G = f;g , and
 X nd frequen t vertex set F V = f v j F min  X  F v  X  F max g . 2. for all v 2 F V do 3. Find the set of all one-edge extensions of v , L ; 4. searc hSubgraph ( v , path , L ); 5. return F G ; Subpro cedure: searc hSubgraph ( G , T , L ): Input: A graph G , its type T 2f path; tree; cyclic g , and its extension set L .
 Output: F G . 1. Dep G = f alse ; 2. for all l 2 L do 3. G 0 = G + l and  X nd T 0 ; 4. Find the frequency of G 0 in D , F G 0 ; 5. if Corr ( G; G 0 )  X  Corr max , then Dep G = true ; 6. if Dep G == f alse , then put G into F G ; 7. for all l 2 L 0 do 9. Find the set of all one-edge extensions of G 0 , L 0 ; 10. searc hSubgraph( G 0 , T 0 , L 0 );
Di X eren t from previous works [15, 25, 26] using binary fea-tures of subgraphs, our prop osed indep enden t frequen t sub-graph mining algorithm also disco vers subgraph frequencies and uses them to measure the correlation of subgraphs. Very frequen t subgraphs are like stop words. They are not very informativ e because a large number of graphs will contain them and they do not reduce the candidate set of subgraphs on which subgraph isomorphism has to be performed signi X -cantly. We also consider very infrequen t to be not very useful because we assume that queries containing these infrequen t subgraphs will be infrequen t. If we have a real query log and these infrequen t subgraphs appear in frequen t queries, this assumption can be easily relaxed without much bear-ing on the rest of the treatise. So, we de X ne a lower bound and an upper bound of frequencies for frequen t subgraph mining, and we remo ve subgraphs that have any highly cor-related supergraphs. Thus, rather than only identifying the supp ort graphs of each subgraph, we also  X nd the subgraph frequencies on each supp ort graph to compute correlation of two subgraphs G 0 i ; G 0 where G 0 i  X  G 0 , by using the correla-tion of two random variables G 0 i and G 0 , i.e., where Cov ( G 0 i ; G 0 ) is the covariance of G 0 i and G is the standard deviation of G 0 . The random variable G 0 (similar to the random variable G 0 i ) is a variable represen t-ing which graph G the subgraph G 0 occurs in. Its proba-bility distribution p ( G 0 = G ) represen ts the likeliho od of G 0 occurring in G . It is estimated using p ( G 0 = G ) = F [25, 6], which only uses the number of supp ort graphs, we use the correlation utilizing subgraph frequencies on sup-port graphs. For example, if two subgraphs G 0 i  X  G 0 always appear on the same graphs G , previous metho ds only select G . However, if F G 0 embeddings on G than the embeddings of G 0 i in G via the embeddings of G 0 , G 0 i is still useful to index in addition with G 0 and thus, our algorithm indexes G 0 i .
The indep enden t frequen t subgraphs disco vered by algo-rithm 2 can be used for graph indexing. However, the cor-relation in Equation (1) is used only to pre- X lter highly cor-related subgraphs. Partial redundancies between subgraphs still exist. Thus, we prop ose a feature selection approac h for pruning the index further . Consider a matrix of sub-graph frequencies in all graphs. Each subgraph feature G 0 has a list of supp ort graphs G with a frequency F G 0  X  G corresp ondingly a graph has a list of subgraphs G 0 with F
G 0  X  G . Then we can have the joint probabilit y distribu-tion P ( G ; F ), where G is a random variable with outcomes of all the graphs G 2 D , F is a random variable with out-comes of all the subgraph features G 0 2 F G . This joint distribution is computed using p ( G; G 0 ) = F G 0  X  G =Z , where Z is a constan t to normalize all subgraph frequencies into probabilities.
 Max-Precision
Our goal is to select a set of features using which the algorithm can optimize the precision of the candidate graphs among all the retriev ed candidates for all queries. Thus, given the possible user generated graph query set Q , we can  X nd the supp ort graphs of each query G q 2 Q , where each supp ort graph is considered as relevant to G q . Since the possible user generated graph query set is hard to obtain without user logs, we use a pseudo graph query set Q for feature selection that is generated randomly from the set of all the disco vered subgraphs F G . Thus, the Max-Precision (MP) problem to select the optimal subgraph set S opt is de X ned as follows: = 1 j Q j X where S = f G 0 1 ; G 0 2 ; :::; G 0 m g , D G 0 port graphs of G 0 q where 8 G; F G 0 D Then p ( G q  X  G j8 G 0 q ) is the conditional probabilit y that a graph G contains G q given F G 0 that G 0 q 2 G q , G 0 q 2 S . The last term in Equation (2) uses the geometric mean to appro ximate the arithmetic mean.
However, even when we have the possible user generated graph query set Q with a probabilit y distribution of each query ,  X nding the optimal subgraph set S that maximizes P rec ( S ) is computationally expensiv e, since for each possi-ble subset of subgraphs we have to compute P rec ( S ). Even greedy algorithms are expensiv e (shown in Section 4.3 and 5.3). We desire a more time-e X cien t algorithm. We show below how to use an appro ximation metho d to select the set of subgraphs to index.
 Max-Irredundan t-Information
As mentioned in the introduction, we need to combine the informativ e and irredundan t principles together. In order to do so, we prop ose a mutual-information-based strategy as follows. The mutual information (MI) M I ( X ; Y ) is a quan-tity to measure the dependency among two or more random variables [21, 23]. For the case of two random variables, we have Obviously , when random variables X and Y are indep en-dent, I ( X ; Y ) = 0. In our case, the pair of outcomes are two di X eren t subgraphs G and G 0 , where G 0  X  G . Mathe-matically , We use PMI to measure the dependence of a subgraph fea-ture G 0 q and a query graph G q in the set of retriev ed graphs G , or dependence of a pair of subgraphs, G 0 i and G 0 j call this scheme the Max-Irredudan t-Information (MII) that has the following form, S opt = arg max = X Theorem 1. MII is equivalent to MP with geometric mean. Proof. For MP using the geometric mean, which is the same as MII. Thus, they are equiv alent.
Both MP and MII are computationally expensiv e, because for each possible subset of features we have to compute IrreduI nf o ( S ). MP and MII both combine the informativ e and irredundan t principles naturally , where selected sub-graphs should be informativ e (have a high pruning power, i.e., j D j = j D G 0 should be irredundan t of each other. Thus, we decomp ose this problem into two subproblems: Max-Information and Min-R edundancy , in order to reduce the time complexit y. Max-Information If all G 0 2 S are indep enden t of each other, then we have IrreduI nf o ( S ) is the sum of each subgraph's pointwise con-tribution. We prop ose a pointwise-pruning-p ower-based cri-terion to measure the information contained by each sub-graph as follows: where we use p ( G 0 q ) to represen t p ( G 0 q ; F G 0 port graph G has at least F G 0 F is to  X nd a subset of subgraphs S with m subgraphs G 0 i that maximizes the sum of information scores of each G 0 i , called Max-Information (MI), is de X ned as follows: Min-Redundancy
Using PMI, we can de X ne the dependence of a pair of subgraphs G 0 i and G 0 j . Two subgraphs G 0 i and G 0 j irrelevant if they are negativ ely dependen t or indep enden t. Thus, we de X ne redundanc e of two subgraphs as follows: If two subgraphs are irredundan t, i.e., have a low redun-dancy score, together they are more informativ e than two redundan t subgraphs. Thus, another goal of subgraph selec-tion is to  X nd a subset of subgraphs S with m subgraphs G that minimizes the redundancy of the selected subgraphs, i.e., the sum of mutual information of each pair G 0 i and G called Min-Redundance, de X ned as follows: Max-Information-Min-Redundancy
We need to obtain Max-Information and Min-Redundancy in the selected subgraph set, but considering all the selected subgraphs together as in MP or MII is computationally ex-pensiv e. Thus, we prop ose a global criterion that combines the two constrain ts, Max-Information and Min-Redundancy , and is signi X can tly more e X cien t computationally , called Max-Information-Min-Redundancy (MImR), as follows: In practice, usually normal feature selection algorithms us-ing  X rst-or der incremen tal searc h can be used to  X nd the near-optimal subgraph set. Supp ose we have selected k  X  1 subgraphs and want to select the next subgraph. Then the local optimal feature G 0 k is selected to maximize the follow-ing function: Now we show that for the  X rst-order incremen tal searc h, MImR is an appro ximation to MII. First we de X ne pointwise entropy as P H ( x ) =  X  log p ( x ) and joint pointwise entropy as P H ( x; y ) =  X  log p ( x; y ). It is easy to verify that We de X ne pointwise total correlation P C ( S ) as follows: and P C ( S; Q ) as follows: Then by subtracting (9) from (10) and substituting the dif-ference into (8) we have Thus, MII is equiv alent to simultaneously maximizing the  X rst term and minimizing the second term at the left hand side of Equation (11).
 It is easy to show that the  X rst term, is maximized only if all the variables in f S; Q g are the most dependen t. Thus, if all m  X  1 subgraphs in S have been selected, the m th subgraph that is the most dependen t on Q should be selected for MII, because it can maximize P C ( S; Q ). Note that this is the same as the Max-Info strategy . The sec-ond term P C ( S ) &gt; 0 if subgraphs are positiv ely dependen t, &lt; 0 if negativ ely dependen t, and = 0 if all the subgraphs are indep enden t. Thus, if all m  X  1 subgraphs in S have been selected, the m th subgraph that is the most pairwise nega-tively dependen t on each selected subgraph in S should be selected for MII, which can minimize P C ( S ). This is same as the Min-Redu strategy . Thus, MImR is a combination of Max-Info and Min-Redu and an appro ximation to MII. Algorithm 3 Irredundan t Informativ e Subgraph Selection Algorithm: IIGS ( F G , m ): Input: Candidate set of subgraphs F G , and Num ber of features to select m .
 Output: Set of Irredundan t Informativ e Subgraphs S . 1. Initialization : S = f;g , k = 1. 2. while k  X  m , do 3. scan all G 0 2 F G and 4. move G opt from F G to S ; 5. k + +; 7. return IIF G ;
It is easy to shown that  X nding the optimal solution for the MP or MII problem is as expensiv e as: O ( n ! subgraphs in G q and avg j s q j is the average size of s
We use forward selection in this work (Algorithm 3). For-ward selection is a greedy algorithm using  X rst-or der incre-mental searc h, i.e., every time only the best subgraph from the rest of the candidate subgraph set is added to the se-lected subgraph set. Initially , the algorithm  X nds the most informativ e subgraph. Then, every time when a new sub-graph is added, the rest of the subgraphs in the candidate set are evaluated. The one that maximizes Equation (7) is selected. The algorithm repeats this until m subgraphs are selected.

For MP and MII, computational complexit y of the  X rst-order incremen tal selection is For MImR, the computational complexit y of the  X rst-order incremen tal selection involves three parts, for pre-computing information scores of all features, for pre-computing pairwise dependence scores of all feature pairs, and for subgraph selection, which is much faster than that of MP or MII. Because avg ( j s q j ) can be viewed as a constan t compared with other numbers, forward subgraph selection based on MImR is quadratic, while for MP or MII it is quartic. Another advantage of the  X rst-or der incremen tal searc h is that we only need to run the algorithm once to select m subgraphs, and we know what are the best k  X  m subgraphs without re-running the algorithm every time when the number of selected subgraphs is changed.
In this section, we evaluate our prop osed approac hes and compare the experimen tal results with two previous meth-ods. Our results show that:
We use the same real data set and testing query set as those used by Yan, et al., [26]. It is a NCI/NIH HIV/AIDS antiviral screen data set that contains 43,905 chemical struc-tures. The experimen tal subset contains 10,000 chemical structures randomly selected from the whole set and the query set contains 6000 randomly generated queries, i.e., we only use chemical structures for experimen ts, our ap-proac h is applicable to any structures that can be repre-sented by graphs, such as DNA sequences or XML  X les.
We evaluate average precisions of returned graphs for all queries using di X eren t subgraph selection metho ds. When we compute the average precision of returned structures, we only coun t queries with non-empt y supp orts in the data set. In our experimen t, we evaluate six metho ds. First, we evaluate three metho ds without considering subgraph fre-quencies, including two previous metho ds, CloseG [25] and GIndex [26], and our prop osed metho d, MImR . They use binary features that only consider the occurrence of sub-graphs in graphs. Each subgraph feature takes a binary value of 1 or 0 to represen t the occurrence of a subgraph in a graph or not, respectiv ely. Then we evaluate three metho ds considering subgraph frequencies. We extend the CloseG metho d to CloseG.F , the GIndex metho d to GIndex.F , and prop ose a MImR.F , which is an extension of our MImR metho d, respectiv ely. They use numerical features of sub-graph frequencies in graphs, i.e., frequency features . CloseG and CloseG.F [25] just select frequent closed subgraphs in-dependen tly that have D G  X  F min = f 1000 ; 500 ; 200 ; 100 g ( m = f 460 ; 1795 ; 9846 ; 50625 g respectiv ely) without consid-ering redundancy . GIndex and GIndex.F [26] select sub-graphs from the candidate subgraph set where D G  X  100. It scans subgraphs from the smallest to the largest avoiding to select redundan t supergraphs of selected subgraphs. A discriminativ e score is de X ned and computed for each sub-graph candidate to measure the redundancy . If its score f 667 ; 1779 ; 9855 ; 50625 g respectiv ely) then the subgraph is selected. MImR.F is our prop osed metho d in Section 4.2 and MImR is similar but uses binary features. Five vari-ations on the number of selected subgraphs are evaluated, m = f 460 ; 667 ; 1779 ; 9855 ; 50625 g . MP and MII are only evaluated using the data set with 100 structures because they are forbiddingly expensiv e in practice. We randomly sample 30,000 subgraphs as the training query set with the same distributions as that of the testing query set, and then use it for subgraph selection in MImR and MImR.F .
We show precision of returned results for the testing query set in Figure 2. Because each subgraph selection metho d can select di X eren t numbers of subgraphs for indexing by adjusting parameters, we show the curves of precision versus di X eren t values of the selected subgraph number m and the index size in Figure 2. We also presen t the precision curve versus the query graph size j G q j to illustrate the e X ect of di X eren t query sizes in Figure 2. To evaluate the precision versus the index size, we  X rst index the canonical string of each selected subgraph. If a subgraph has a frequency larger than one, we then index the frequency . Thus, using numerical features has a larger index size than using binary features given the same number of selected subgraphs.
In Figure 2 (a), we can observ e that given the same num-ber of selected subgraphs, GIndex impro ves the average pre-cision compared with CloseG . Our prop osed approac h MImR can impro ve the precision by about 4%-13%. This illus-trates that our probabilistic model for subgraph selection works better than the previous metho d prop osed by Yan, et al., [26]. We also can see that CloseG.F , GIndex.F , and MImR.F can impro ve the average precision compared with CloseG , GIndex , and MImR by using subgraph frequencies as features (Figure 2 (a)). This demonstrates that using subgraph frequencies for subgraph queries can impro ve the precision by about 4%-12%. In Figure 2 (b), we can see that using subgraph frequencies as features will increase the index size by about 6%-30%, since more information of frequen-cies are indexed. We can see that given the same index size, CloseG.F , GIndex.F , and MImR.F also have higher preci-sion than CloseG , GIndex , and MImR , even though not as much as that in the cases in Figure 2 (a). In Figure 2 (c), the curves are of the precisions vs. query sizes for the cases in Table 2. We can observ e that CloseG and CloseG.F have higher precision for small queries, while GIndex and GIn-dex.F have higher precision for large queries. MImR and MImR.F are more balanced than either and always have precisions above their precisions. The p-values in Table 2 of 1-sided T-tests show that the impro vemen t is signi X can t with a con X dence level of at least 99.9%.
In this section, we show the overall response time for sub-graph queries using di X eren t subgraph selection metho ds. The searc h process to answ er subgraph queries works as fol-lows. Every time when a subgraph query is entered, the algorithm  X rst generated the canonical string of the query graph and check if this query graph is indexed. If the query is indexed, the supp ort of the query is retriev ed without veri X cation using subgraph isomorphism. If the query is not indexed, its subgraphs are enumerated, and among those subgraphs, indexed ones are use to scan the index and  X nd candidate sets for each subgraph and compute the intersec-tion for all candidate sets. Finally , subgraph isomorphism Figure 4: E X ect of max enumerated subgraph size on response time for query size of 20 &amp; MImR.F tests are performed on all retriev ed candidates to prune out unsatis X ed graphs.

Thus, response time of a subgraph query involves 1) enu-meration time to identify if the query graph is indexed or not, and enumerate the subgraphs of the query if the query is not indexed (note that enumeration time also includes the time of  X nding canonical strings of subgraphs), 2) search time to retriev e candidates from the graph index, and 3) veri X c ation time to prune out unsatis X ed graphs that are not super-graphs of the query from the candidates using subgraph isomorphism tests. Enumeration time  X rst de-pends on how many queries are indexed and then depends on query sizes. Usually large queries have long enumeration time. Searc h time includes time to retriev e candidate sets, and time to compute the set intersection. If the query is indexed, then no set intersection is computed. Moreo ver, if the query is indexed, veri X cation time is zero. Otherwise, it depends on the number of returned candidates. Thus, if the precision of returned candidates is higher, then veri X cation time is shorter.

We illustrate average response times for the cases in Ta-ble 2, in Figure 3. We set the max enumerated subgraph size of queries as 8. From Figure 3, we can observ e that the average over-all response time of subgraph queries using MImR is always the best in comparison with the other two metho ds, GIndex and CloseG, for di X eren t query sizes, irre-spectiv e of whether binary features or frequency features are used in the index and the searc h process. However, the im-provemen t of response time is signi X can t for small queries, while for large queries, it is not signi X can t, because the enu-meration time dominates the over-all response time. We also can observ e that the searc h time is always a small part of the over-all response time, while enumeration time and veri X cation time change much for di X eren t cases and a X ect the over-all response time signi X can tly. Usually for small queries, the major part of response time is the veri X cation time, while for large queries, the major part is enumeration time. This is because small queries usually have large sup-ports containing more supergraphs so that as a result the candidate set is also very large to verify, although each sub-graph isomorphism test for small queries is not expensiv e. In comparison, large queries usually have small candidates to verify, but the enumeration time is expensiv e for large queries, because it increases exponen tially when the query size increases.

Similar to precision curves in Figure 2 (c), we can ob-serve that 1) for GIndex and GIndex.F, the veri X cation time is long for small queries but short for large queries, 2) for CloseG and CloseG.F, the veri X cation time is short for small queries but long for large queries, 3) for MImR and MImR.F, the veri X cation time is short for all queries. Moreo ver, using frequency features can achieve a shorter veri X cation time than using binary features for all the cases. This is con-sisten t with the precision curves in Figure 2 (c). Although using frequency features can achieve a shorter veri X cation time than using binary features, it requires a longer enu-meration time because the exact occurrence number of each subgraph is needed to be identi X ed for frequency features. Thus, the over-all response time is shorter using binary fea-tures than using frequency features for large queries, because the enumeration time dominates the response time for large queries. However, the over-all response time is shorter us-ing frequency features than binary features for small queries, because the veri X cation time contributes more than the enu-meration time to the response time.

Because the subgraph enumeration process uses Algorithm 3, we can set the maxim um subgraph size to enumerate. If we set a smaller maxim um subgraph size, we can expect a shorter enumeration time but a longer veri X cation time, be-cause fewer subgraphs are used to retriev e candidates. We adjust the maxim um subgraph size and evaluate MImR.F with the query size of 20, and show the results in Figure 4. We can observ e that for queries with graph size of 20, if we use MImR.F, the optimal maxim um subgraph size for subgraph enumeration is 5. Below that, veri X cation time increases signi X can tly, while above that, enumeration time increases much. Thus, we can achieve better response times than those in Figure 3, if we tune and  X nd the best maxi-mum subgraph size for subgraph enumeration.
To compare the time complexit y for our prop osed meth-ods, MP.F , MII.F , and MImR.F , we select a data set with 100 chemical structures. We use a testing set of 100 queries ing set of 500 queries with the same distribution. We use the forward feature selection for the three metho ds. We show the time complexit y and the average precision of query an-swers in Figure 5. The results demonstrate that MImR.F is signi X can tly more computationally e X cien t than MP.F and MII.F . MImR.F achieves only a slightly worse precision in comparison to MP.F or MII.F . Note that MP.F and MII.F may not achieve the optimal performance since we use for-ward feature selection. To  X nd the better solution using a global metho d of feature selection, we expect more signif-icant computational costs. We can observ e from Figure 5 (b) that the precision increases while more subgraphs are selected. After a certain number of subgraphs have been selected, the precision reaches the highest value, i.e., adding more subgraphs cannot increase the information contained in the subgraph set for the testing query set. Thus, if the user query set is known, we can  X nd this point and stop adding useless subgraphs to the index after this point. A better subgraph selection metho d can achieve the highest precision with a smaller number of subgraphs. Note that there is no over X tting problem for the task of subgraph queries. This is because in the subgraph query problem, given that a query graph is a subgraph of a graph, all the subgraph features of this query graph are always subgraphs of the same graph. Adding more features can always prune out more (at least zero) unsatis X ed candidates and impro ve (at least main tain) the precision of subgraph queries.
We prop osed a novel probabilistic model to determine a near-optimal set of subgraphs to index in order to answ er a given set of queries given a constrain t on the maxim um number of subgraphs that can be indexed (typically due to space and performance limitations). We consider subgraph frequencies in graphs to impro ve the precision of our metho d further. We introduce several criteria for subgraph selec-tion, including Max-Precision (MP), a metho d that directly optimizes the precision of query answ ers, Max-Irredundan t-Information (MII) and Max-Information-Min-Redundancy (MImR) that are based on a probabilistic model using mu-tual information. We show theoretically that MImR and MII are appro ximations of MP. We also prop ose a greedy feature selection algorithm using MImR that works well in practice. Experimen ts show that our prop osed approac hes perform signi X can tly better than previous metho ds. Future work will use real queries from user logs to select features and evaluate precision.
We acknowledge the partial supp ort of NSF Gran t 0535656 and 0845487.
