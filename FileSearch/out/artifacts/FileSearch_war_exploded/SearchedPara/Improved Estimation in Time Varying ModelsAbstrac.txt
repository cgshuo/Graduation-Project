 School of Computer Science, McGill University, Montreal, Canada Locally adapted parameterizations can produce flexible representations from relatively rigid components; locally weighted regression serves as a canonical example of this approach. Such models reduce bias but increase variance, due to reduced effective sample sizes used for each estima-tion. We tackle this problem using a natural machine learn-ing idea: using a transformed (more restricted or simpler) space in which to find local parameterizations.
 A common approach to improving model efficacy in ma-chine learning is to first transform the data into an alternate representation prior to model estimation, ideally in a way that amplifies useful information while attenuating noise. Algorithms exemplifying this approach include: PCA, ICA (Hyv  X  arinen &amp; Oja, 2000), nonlinear-dimension reduction, e.g. (Tenenbaum et al., 2000), and dimension reduction for regression (Fukumizu et al., 2004; Cook &amp; Forzani, 2009). Another line of work considers transformations of the model used to describe the data, either by reducing the number of degrees of freedom, or by seeking a model form amenable to more powerful estimation procedures. Exam-ples of the first approach include DiscLDA (Lacoste-Julien et al., 2008) and supervised dimensionality reduction us-ing Bayesian mixture models (Sajama &amp; Orlitsky, 2005), which seek useful linear reductions of the parameters of a generative model. The second approach includes the appli-cation of spectral methods to learning transformed repre-sentations of HMMs (Siddiqi et al., 2010) and PSRs (Boots &amp; Gordon, 2011).
 In this paper, we provide a different lens through which to view model transformations. In Sec. 2, we present a general formulation of the problem of estimating useful transformations of model parameters, which encompasses several of the previously mentioned methods for both data and model transformation. Our problem depends on the si-multaneous estimation of a transformation of the parameter space of a model and of the parameters within the trans-formed space. We formulate the problem primarily for use with multiply parameterized models (such as locally weighted linear regression or mixture models), which dis-tinguishes it from the spectral methods for HMM and PSR learning, which seek single transformed parameterizations of a given model. We illustrate our problem formulation in the context of familiar models (locally weighted regression and Gaussian mixtures) in Sec. 3. In Sec. 4 we present a novel algorithm for modeling time varying sparse network structures underlying sequential observations. In Sec. 5 and 6, we use synthetic data and data drawn from real-world BCI EEG experiments to showcase our algorithm. The problem investigated in this paper arises as a general-ization of the following optimization: where the loss ` measures the  X  X oodness X  of fit of the model f to the data X = { ( x 1 ,y 1 ) ,..., ( x m ,y m ) a set of parameterizations B = {  X  1 ,..., X  m 0 } of f , and an optimal set of parameterizations B  X  is sought.
 The idea of using multiple model parameterizations is not often explored in machine learning. As motivation for this view, we begin by expressing standard linear regression in the form of (1). In this case, f measures the residuals pro-duced by a parameter vector: For a set of parameter vectors  X  i , ` is proportional to the log-likelihood of observing the residuals assuming they are normally distributed with variance  X  2 : We usually think of the loss in this case as having m 0 = 1 . However, note that considering m 0 &gt; 1 does not modify the solution, as loss is measured equally over all ( x j ,y which implies that  X  i =  X  j ,  X   X  i , X  j  X  B  X  (i.e., there still is, in effect, one optimal parameter vector).
 Using this view, we can transform standard linear regres-sion into kernel weighted linear regression as follows: ` ( f,X,B ) = where the kernel weighting function k ( x,x 0 ) measures similarity between locations in the input space, and each  X  consists of two components:  X  i = (  X  w ization component  X  x input space and the coefficient component  X  w with a set of regression coefficients.
 Introducing the kernel k allows the  X  i in (3) to have local rather than global effect, which leads to different parame-terizations at each location in observation space. However, no constraint linking different elements in a parameter set. Allowing multiple local parameterizations of f is useful for increasing the power of simple models; estimation of time varying covariance matrices in financial modeling and es-timation of time varying auto-regressions in econometrics are two well-studied examples of this idea.
 While locally weighted regression is typically thought of as a  X  X on-parametric X  method, in the context of our work it is more fruitfully viewed as an approach based on mul-tiple parameterization, in which the implied infinite set can be queried  X  X azily X  for specific parameter locations  X  rather than computed monolithically.
 To illustrate a problem in the form of (1) in which the el-consider the following optimization: ` ( f,X,B ) =  X  log in which 0  X   X   X  ability of observing x given a Gaussian distribution with to estimating a Gaussian mixture model for the data X = { x 1 ,...,x m } . Interdependence among the  X  i  X  B  X  is in-duced by the negative log-likelihood loss, together with a constraint on the set of mixture weights: P Note that in the last two examples, the estimation of B  X  may be subject to high variance. To tackle this problem, and to exploit possible structure in the parameterizations, we introduce a  X  X enerating X  function g , which takes inputs  X   X   X  R p (with p chosen a priori) and transforms them into outputs  X  . This function can be used to express both reg-ularities and restrictions in the space of parameterization. For instance, in the case of a time varying model, the opti-mal, temporally local parameterizations of f may lie on a low-dimensional manifold embedded in the full parameter space of f . The structure of such a manifold could be of interest, and restricting the estimation could significantly reduce variance in the resulting parameter estimates with only a small increase in bias.
 We can now rephrase (1) as an optimization problem in-volving g . Given dimension p , a model f , a loss ` , and a set of inputs X , our optimization becomes: in which  X  B = {  X   X  1 ,...,  X   X  m 0 } is a set of inputs to f | g denotes the restriction of parameterizations of f to the output space of g .
 If we define g (  X   X  )  X   X   X  , then (5) exactly reproduces (1). If we allow g to take an arbitrarily complex form, then we similarly recover the optimization in (1), as we can define g (  X  (5) arise when g is more carefully chosen. The next section illustrates some useful problems that arise from different definitions of g , f , and ` . As a first example, consider performing a locally weighted regression analogous to that in (3), but with the local pa-rameterizations of f restricted to a linear subspace. Let g (  X   X  ) = A  X   X  w , where A is the matrix of the parameters of g . We can re-write (5) as follows: arg min in which we now split each  X   X  i into a localization sub-component  X   X  x one views x T spanned by the columns of A , followed by a linear regres-sion in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002). However, minor modifications, like regularizing the  X   X  i s via  X  P As a second example, we restate the mixture of Gaussians model under the constraint that the means {  X   X  the parameterizations {  X  1 ,... X  m 0 } lie within a linear sub-space of the observation space, i.e.  X  i = ( g (  X   X   X  with g defined as for (6). The resulting optimization can be written as follows: arg min Performing the optimization in (7) was shown to be useful for classification tasks in (Sajama &amp; Orlitsky, 2005). We can similarly generate optimization problems in the form of (5) whose solutions correspond to PCA and sparse coding, which are left out due to space constraints. In this section, we use our new problem formulation to de-rive a novel algorithm for estimating time varying network structure, using a time-dependent sparse combination of learned basis structures. Through an analogy between our algorithm and sparse coding (Olshausen &amp; Field, 1996), we then extend our algorithm to learning of task-driven ba-sis structures, guided by the work in (Mairal et al., 2011). We begin by reviewing existing work on network structure estimation, before describing the new algorithms. 4.1. Sparse Network Structure Estimation In recent years, much effort has gone into developing effec-tive methods for estimating sparsely structured Gaussian graphical models. A Gaussian graphical model (GGM) explains a set of m n -dimensional observations X = { x 1 ,...,x m } ,x i  X  R n using a set of n vertices (each cor-responding to one dimension) and a set of edges, each de-scribing the strength of the relationship between its incident vertices. A GGM implies a covariance  X  and is equivalent to modeling X with a normal distribution N ( ~ 0 ,  X ) . Typi-cally, prior to estimating a GGM, the observations are stan-dardized to have mean 0 .
 Many existing methods addressing GGMs focus on es-timating their structure, i.e. the pattern of zero/non-zero edges. These methods typically work with the precision relationship: in which  X  ij indicates the partial correlation between the i dimensions, and  X   X  ij is the entry in the i th row and j and GGM structure leads to efficient methods for GGM structure estimation, as partial correlations can be directly estimated by  X  X elf-regression X .
 The use of self-regression for network structure estimation is based on the following results (Lauritzen, 1996): in which x i t uncorrelated with x i Hence,  X   X  ij can be efficiently estimated for any given linear regression of the response variables { x i the covariates { x \ i including all dimensions except i ;  X  ij can then be com-puted as well.
 Most existing methods for GGM structure estimation as-ture estimation determines the pattern of zero/non-zero en-tries (Friedman et al., 2008; Song et al., 2009b; Kolar &amp; Xing, 2011)). The sparsity assumption can be incorpo-rated into the self-regression process by using sparsifying regression techniques, such as the well known Lasso (Tib-shirani, 1996). Self-regression methods using sparsity have been shown to produce consistent estimates of structure in 2006; Wainwright et al., 2007; Kolar &amp; Xing, 2011). A recent line of work focuses on extending methods for net-work structure estimation for the case when structures vary over time (Ahmed &amp; Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar &amp; Xing, 2011). We focus on the KELLER algorithm from (Song et al., 2009a), as our algorithm can be seen as its natural generalization using the problem formulation in (5). The KELLER algo-rithm is predicated on two assumptions: sparsity in the time varying network structure, and smoothness in the changes of these structures over time. This second assumption distinguishes KELLER from methods such as (Ahmed &amp; Xing, 2009) and (Kolar et al., 2009), which assume abrupt changes in the network structure.
 To estimate the structure of a network at time t , given a sequence of T observations X = { x 1 ,..,x T | x i  X  R n } KELLER performs a set of n independent ` 1 -regularized locally weighted regressions, with the i th regression esti-mating the values  X   X  ij ,  X  j 6 = i as described above. By using locally weighted regression, these values are specifically adapted to the predominant network structure affecting the observation at time t . For time t , these regressions can be written compactly as follows: A  X  t = arg min in which k ( t,t 0 ) computes a kernel weight measuring tem-poral proximity, diagonal entries of A are fixed at 0, || A || is the entry-wise matrix 1-norm (i.e. P controls the ` 1 regularization, which determines the spar-sity of A . After estimating A  X  according to (12), KELLER performs a simple procedure to make the implied struc-ture estimate coherent with the assumption of an undirected ferring an edge between any pair of vertices ( i,j ) such that A ij 6 = 0 or A ji 6 = 0 . An estimation similar to (12) is used in (Song et al., 2009b), without the additional symmetriza-tion, for networks with directed edges.
 As used in (12), the weighting kernel makes the estimate of
A  X  t at time t effectively independent from observations at times remote from t . This can lead to high variance, and ignores potential structure in the way in which the network structure changes over time. We will now state our algo-rithm, which addresses these problems. 4.2. Estimating Network Structures as Combinations We reformulate the optimization in (12) similarly to the way in which we generalized locally weighted regression from (3) to the form (6). At each time t , the optimal A is estimated as a linear combination of a set of k basis ma-estimation procedure revolves around the following opti-mization:  X   X  t = arg min term, and  X  controls the strength of regularization. Given  X   X  , we estimate A  X  involves a fixed set of basis matrices  X  A , but what we really want is to jointly optimize the loss in (13) over all times 1  X  t  X  T , with respect to both the  X   X  t  X   X  B = {  X   X  1 ,... and the A i  X   X  A . By doing so, information extracted from the entire sequence is allowed to affect the estimation of each A  X  which helps mitigate problems with high variance.
 The desired joint optimization over  X  B and  X  A is easy to ex-press in the terms of (5). Let g (  X   X  ) = P k A i  X   X  A and || A i || 1  X  c . The constraint on the entry-wise tion. Next, we define f ( x,g (  X   X  )) = || x  X  g (  X   X  ) we define ` ( f | g,X,  X  B ) as: ` ( f | g,X,  X  B ) = Finally, we express the full joint optimization as follows:  X  A in which we changed the entry-wise 1-norm constraint on ularization term. Intuitively, our method produces a set of basis network structures, i.e.  X  A  X  , with which the temporally local network structures can be effectively approximated. The joint optimization in (15) is closely analogous to the following sparse coding objective: in which B = {  X  1 ,... X  m |  X  i  X  R k } ,  X  controls the tradeoff between reconstruction accuracy and representational spar-sity, and the columns of A are constrained to unit norm. We can emphasize this by introducing the concept of time varying pseudo-dictionaries D t  X  R n  X  k , in which the i column of D t is A i x t . Using pseudo-dictionaries, we can rewrite (15) as follows:  X  A in which we dropped the sparsifying penalty on A i  X   X  A  X  for notational brevity. From (17), it can be seen that the inner optimization over  X  B in (15) can be addressed as a set of sparse coding problems. For our purposes, we set the regularization term  X   X  r (  X   X  t ) to: which corresponds to elastic-net regularization (Zou &amp; Hastie, 2005). We use this form to meet the assumptions required for the task-driven dictionary learning described in (Mairal et al., 2011), used in the further extension of our algorithm.
 The analogy between our method and sparse coding leads naturally to a method for effecting the joint optimization in (15). As in sparse coding, we can jointly optimize over  X  and  X  B using an EM-like block coordinate descent process that alternates between optimizing  X  B while holding  X  A fixed and optimizing  X  A while holding  X  B fixed (each of these is a convex problem). When optimizing  X  B with  X  A held fixed, we compute the optimal  X   X  t for each t via elastic-net regres-sions solved with the publicly available, highly optimized glmnet package (Friedman et al., 2009). When optimiz-ing  X  A with  X  B held fixed, given current estimates of each basis A i  X   X  A , we compute the partial gradients of the ob-jective in (17) w.r.t. the entries of each pseudo-dictionary D , and then backpropagate these partial gradients through the pseudo-dictionary formation process to get partial gra-dients w.r.t. each entry of each basis structure A i . We sym-metrize the partial gradient of (17) w.r.t. each A i by setting to maintain the zero-diagonal constraint on A i  X   X  A the next subsection we refer to these (unsupervised) par-tial gradients as  X  A i ` u . Using the computed gradients, we then take a single gradient descent step to update each A The full joint optimization process iterates between updat-ing the  X   X  i  X   X  B via the regression in (13) and perform-ing a single gradient descent update of the entries in each A i  X   X  A . We dynamically select the step size for gradient descent updates in each iteration by line search and iter-ate until convergence. We perform the iterative optimiza-tion using subsampled batches of the available observa-tions, which yields a stochastic gradient descent approach to jointly optimizing (15)/(17). 4.3. Supervised Basis Structure Learning We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of basis network struc-tures. We consider the task of minimizing differentiable supervised loss functions that can be written as: where  X   X  R k , y t is the target output at time t , and the  X   X   X   X  B were produced to minimize (17). This includes any differentiable linear function of the  X   X  t  X   X  B . In this paper, we focus on classification tasks and thus use the binomial deviance loss of logistic regression, i.e. ` s (  X  &gt; The crux of task-driven dictionary learning is converting the readily available gradients of ` s w.r.t. the structure codes  X   X  t into gradients w.r.t. the pseudo-dictionaries with which they were computed to minimize (17), as gradi-ents w.r.t. the D t easily produce gradients w.r.t. the A Unfortunately, the optimization producing the  X   X  t makes the conversion  X   X  (2011) show that if elastic-net regularization is used to pro-instance supervised loss ` s w.r.t. D t can be computed as follows:  X  where  X  denotes the indices of non-zero entries in the sparse  X   X  t ,  X  C indicates the complementary set of in-dices, and  X  X   X  is the ` 2 regularization weight from (18). Once gradients of ` s w.r.t. each D t (i.e.  X  D been computed for each time t , they can be backprop-agated through the pseudo-dictionary formation process and summed across time points to get gradients w.r.t. each A Given unsupervised gradients  X  A i ` u , computed as de-scribed at the end of Sec. 4.2, and supervised gradients  X 
A i ` s , we define the final gradients for stochastic descent optimization of the combined unsupervised/supervised ob-jective as follows: where  X  is a mixing parameter controlling the tradeoff be-tween supervised and unsupervised learning. As before, we enforce symmetry and zero-diagonal constraints prior to using the joint gradients for basis updates.
 This section presents tests based on simulated observation sequences which show the ability of our algorithm to re-cover recurring elements of time varying network struc-tures. We generated each observation sequence by draw-ing the observation x t at time t from a normal distribu-tion N (0 ,  X  t ) , in which  X  t was a convex combination of four covariance matrix bases:  X  t = P 4 P jectories for the  X  i in Fig. 1). We generated each  X  i by symmetrically remov-ing two thirds of the off-diagonal entries (the ones with the smallest magnitude) from a random covariance matrix with eigenvalues uniformly distributed in (0 , 1) , and then rescal-ing diagonal entries to ensure positive definiteness. An ex-ample of the sparse basis structures used in our tests can be seen in the right panel of Fig. 2. The inputs ranged from 10-dimensional to 40-dimensional. For each tested dimen-sionality, we generated 25 sequences of 5000 observations, with the first 3000 reserved for training and the last 2000 reserved for testing; each sequence was based on different basis matrices  X  i and different  X  i averaged over the 25 sequences.
 Methods based on (12) are much better suited for this task than methods expecting abrupt  X  X hange point X  structure. Hence, we tested three methods for estimating time varying network structure in our sequences: locally weighted ` 1 regularized self-regression (as described in (12)), the same self-regression followed by projection of the inferred struc-tures onto the principal components of structures estimated for each time point in the training set, and our iterative ap-proach to learning task-driven basis structures.
 The self-regression-based method used in our tests can be considered equivalent to KELLER (Song et al., 2009a). Us-ing the principal components of the set of A  X  this method is itself novel, and can be seen as an approx-imation to our method. When executing our method, we initialized the set  X  A using these principal structures. In our tests, we used six principal structures with the PCA-based method and learned six basis structures with our algorithm. We measured test performance for a classification task in which the class of each x t was set as follows: y t = 1 if  X  estimated a similarity score between the sets of estimated structures and the true precision matrices underlying each sequence, as explained below.
 Classification was performed using the parameterization produced by each method for a given x t (i.e. a matrix A  X  for the self-regression method, the same matrix projected onto a set of principal structures for the PCA method, and the inferred vector  X   X  t for our algorithm) as input features to a regularized logistic regression classifier, with the tar-get class determined by y t . Fig. 3 presents the results. The basis structures learned by our method, and the codes they induce, offer an informative representation of regularities in time varying sparse network structure.
 We measured similarity between learned bases and the true precision matrices using a form of pairwise matrix corre-lation. First we set the diagonal entries of each matrix to zero, then their off-diagonal entries to zero mean and unit norm, and finally  X  X ectorize X  each matrix and compute the subject during a set of test trials, given a labeled set of training trials. In each trial, a cue is given to the subject indicating a motor action, after which the subject visual-izes that action for several seconds. Cortical activity during each trial was measured by a set of 60 electrodes placed on the scalp, taking measurements at 250Hz. Data collected from these electrodes was the subject of our analysis. We used left hand and right hand trials from this dataset for the subjects l1b, k3b, and k6b. Several trials from each sub-ject were discarded due to significant artifacts, as measured by deviation from a Gaussian model of the mean behavior of the joint set of trials for a subject. We also applied a to analysis, where D was a diagonal matrix containing the eigenvalues of the data and the columns of V were the cor-responding eigenvectors. We set kernel widths and regular-ization weights for the optimization in (14) uniformly for all subjects and trials, following a brief manual search. We learned a set of 20 sparse basis structures for each sub-ject using our algorithm in an unsupervised fashion (i.e.  X  =1 ). Afterwards, we performed 20 rounds of random-ized cross-validation in which we split the trials for each subject 4/1 into training/test sets. We trained three classi-fiers in each round of cross-validation: a classifier built on the  X   X  t inferred by our algorithm after a period of supervised basis updates (i.e.  X  =0 . 75 ) using the training set, a clas-sifier built on the output of a set of 20 RCSP filters (Lotte &amp; Guan, 2011), and a classifier built on the combination of both feature sets. The regularization parameter for RCSP was selected to maximize expected performance across all subjects.
 We built our classifier by considering the  X   X  t and class la-bels for each time point in each training trial as inferred feature/label pairs for training an 2 -regularized logistic re-gression classifier. Given the encoding of a particular trial in terms of a set of  X   X  t , an overall output for the trial was RCSP 0.100 (0.056) 0.363 (0.118) 0.103 (0.048) ADAPT 0.041 (0.053) 0.330 (0.098) 0.056 (0.033) JOINT 0.052 (0.052) 0.253 (0.080) 0.063 (0.039) computed by accumulating (i.e. summing) the output of the learned single time-point classifier over the first three post-cue seconds of the trial. After this evidence accumulation phase, the classification for each trial was determined by the sign of its overall output. RCSP filters were trained as described in (Lotte &amp; Guan, 2011), after which the squared responses of these filters to the observations were used as input features to an 2 -regularized logistic regression clas-sifier, trained as above We also trained an analogous classi-fier using the combined features produced by our algorithm and the RCSP filters at each time point. Classification re-sults for each subject are shown in Table 1, and a visual representation of the evidence accumulation process based on our features is shown in Fig. 4. Classifiers constructed in this fashion have the advantage of being amenable to  X  X arly exit X , in the spirit of drift-diffusion decision making.
These results show that our approach produces informative features in a real-world scenario, with the results for the combined features suggesting that our features supplement, rather than replace, the commonly used RCSP features.
We introduced a problem formulation in the context of mul-tiply parameterized models. Using this formulation, we developed a novel algorithm for learning representations of sparse structure in time varying networks with recurring structural motifs. We used tests on synthetic data to show that our algorithm behaves as desired under suitable con-ditions, while an application to BCI EEG data showed the potential value of our algorithm in real world conditions.
We plan to investigate more in-depth the performance of our approach by applying it to other types of tasks, such as analysis of time varying weather and traffic patterns. Our algorithm is also readily extensible to the estimation of time varying structure in Dynamic Bayesian Networks. We also plan to look at alternative parameter transformation meth-ods, beyond the linear transforms considered in this paper. dot product between the resulting vectors. This measure ranges from  X  1 to 1 , with larger magnitudes indicating greater similarity. For each sequence and each method, we magnitude of our correlation score, among the set of bases produced by that method. We then averaged best match scores for each method over both true bases and sequences, to get a final score for each dimensionality. Fig. 3 shows the similarity scores achieved by the PCA-based method and our method, with the bases produced by our method consistently displaying greater similarity to the true bases than those produced by PCA alone. Fig. 2 shows a typical example of a best match produced by our method during these tests; as can be seen, the learned basis is qualitatively very similar to the true basis. We applied our algorithm to the analysis of EEG data from a Brain Computer Interface (abbr. BCI) motor imagery ex-periment available as task 3a from BCI competition III (Schl  X  ogl et al., 2005; Blankertz et al., 2006). In this task, training trials. In each trial, a cue is given to the subject indicating a motor action, after which the subject visual-izes that action for several seconds. Cortical activity during each trial was measured by a set of 60 electrodes placed on the scalp, taking measurements at 250Hz. Data collected from these electrodes was the subject of our analysis. We used left hand and right hand trials from this dataset for the subjects l1b, k3b, and k6b. Several trials from each sub-ject were discarded due to significant artifacts, as measured by deviation from a Gaussian model of the mean behavior of the joint set of trials for a subject. We also applied a to analysis, where D was a diagonal matrix containing the ADAPT 0.041 (0.053) 0.330 (0.098) 0.056 (0.033) JOINT 0.052 (0.052) 0.253 (0.080) 0.063 (0.039) eigenvalues of the data and the columns of V were the cor-responding eigenvectors. We set kernel widths and regular-ization weights for the optimization in (15) uniformly for all subjects and trials, following a brief manual search. We learned a set of 20 sparse basis structures for each sub-ject using our algorithm in an unsupervised fashion (i.e.  X  = 1 ). Afterwards, we performed 20 rounds of random-ized cross-validation in which we split the trials for each subject 4/1 into training/test sets. We trained three classi-fiers in each round of cross-validation: a classifier built on the  X   X  t inferred by our algorithm after a period of supervised basis updates (i.e.  X  = 0 . 75 ) using the training set, a clas-sifier built on the output of a set of 20 RCSP filters (Lotte &amp; Guan, 2011), and a classifier built on the combination of both feature sets. The regularization parameter for RCSP was selected to maximize expected performance across all subjects.
 We built our classifier by considering the  X   X  t and class la-bels for each time point in each training trial as inferred feature/label pairs for training an ` 2 -regularized logistic re-gression classifier. Given the encoding of a particular trial in terms of a set of  X   X  t , an overall output for the trial was computed by accumulating (i.e. summing) the output of the learned single time-point classifier over the first three post-cue seconds of the trial. After this evidence accumulation phase, the classification for each trial was determined by the sign of its overall output. RCSP filters were trained as described in (Lotte &amp; Guan, 2011), after which the squared responses of these filters to the observations were used as input features to an ` 2 -regularized logistic regression clas-sifier, trained as for our algorithm. We also trained an analogous classifier using the combined features produced by our algorithm and the RCSP filters at each time point. Classification results for each subject are shown in Table 1, and a visual representation of the evidence accumulation process based on our features is shown in Fig. 4. Classi-fiers constructed in this fashion have the advantage of be-ing amenable to  X  X arly exit X , in the spirit of drift-diffusion decision making.
 These results show that our approach produces informative features in a real-world scenario, with the results for the combined features suggesting that our features supplement, rather than replace, the commonly used RCSP features. We introduced a problem formulation in the context of mul-tiply parameterized models. Using this formulation, we developed a novel algorithm for learning representations of sparse structure in time varying networks with recurring structural motifs. We used tests on synthetic data to show that our algorithm behaves as desired under suitable con-ditions, while an application to BCI EEG data showed the potential value of our algorithm in real world conditions. We plan to apply our approach to other types of tasks, such as analysis of time varying weather and traffic patterns, in addition to investigating alternative parameter transforma-tion methods, beyond the linear transforms considered in this paper. Our algorithm is readily extensible to the esti-mation of time varying structure in Dynamic Bayesian Net-works.
 Acknowledgements : Funded by NSERC and ONR.

