 Jean Honorio jhonorio@csail.mit.edu CSAIL, MIT, Cambridge, MA 02139, USA Tommi Jaakkola tommi@csail.mit.edu CSAIL, MIT, Cambridge, MA 02139, USA Classification is arguably one of the most well-studied problems in machine learning. This includes the pro-posal of novel classification algorithms, and the study of generalization bounds (and sample complexity). In-tuitively speaking, generalization bounds provide the rate at which the expected risk of the best classifier (chosen from some family) for a finite sample, ap-proaches the expected risk of the best classifier (from the same family) for an infinite sample.
 Less attention has been given to the approximation of the optimal Bayes error rate, which requires nonpara-metric methods. Some of the notable exceptions are the study of asymptotic universal Bayes consistency for weighted-average plug-in classifiers (Stone, 1977) (a class that contains for instance k -nearest neighbors) as well as for Parzen windows (Fralick &amp; Scott, 1971). It is well known that without any further assump-tion on the probability distributions, no rate-of-convergence results can be obtained (Antos et al., 1999). Given this, several authors have considered regularity conditions in the form of Lipschitz conti-nuity. The Lipschitz continuity assumption upper-bounds the change of a function with respect to a given parameter. Under the assumption of Lipschitz poste-rior probability (i.e. P ( y = 1 | x ) Lipschitz with re-spect to x ), (Drakopoulos, 1995; Nock &amp; Sebban, 2001; Gy  X orfi, 1981; Kulkarni &amp; Posner, 1995) provide gener-alization bounds for k -nearest neighbors while (Kohler &amp; Krzy  X zak, 2006) provides generalization bounds av-erage plug-in classifiers. However the generalization bounds in (Drakopoulos, 1995; Nock &amp; Sebban, 2001; Kulkarni &amp; Posner, 1995) do not imply Bayes consis-tency since the analysis considers  X  X wice Bayes error X . Our first goal is the approximation of Bayes error rate between two unknown distributions, from a given training set. More specifically, we are interested on two-sided exponential concentration bounds. As a byproduct, we obtain a classifier that is Bayes consis-tent with provable finite-sample rates. Note that while show generalization bounds for Bayes consistent clas-sifiers, they do not provide a recipe for producing the desired bounds. The main reason is that the bounds are one-sided and in expected risk . While in prac-tice resampling (Jackknife, bootstrapping and cross-validation) is used to assess classifier performance in finite samples, it is unclear how resampling can pro-duce the bounds we are interested on. In this paper we prove that, given N samples, with probability at least 1  X   X  , we can approximate the Bayes error with a the best of our knowledge, this is the first exponential inequality for Bayes error rate estimation.
 We assume Lipschitz continuity as the regularity con-dition for the probability distributions. This is a rea-sonable assumption since universal rate-of-convergence results are not possible (Antos et al., 1999).
 Our second goal is the approximation of Shannon en-tropy of an unknown distribution, from a given train-ing set. Again, we are interested on two-sided expo-nential concentration bounds. Our main motivation is to provide generalization bounds for learning the struc-ture of trees and Bayesian networks from continuous variables. This is due to the fact that a concentration bound for the entropy makes available a concentration bound for related information theoretic measures, such as mutual information and conditional entropy.
 Several asymptotic consistency results are available for Shannon entropy estimation. Techniques such as ker-nel density estimation (Ahmad &amp; Lin, 1976; Egger-mont &amp; LaRiccia, 1999; Paninski &amp; Yajima, 2008) and spacings (Van Es, 1992; Tsybakov &amp; Van der Meulen, 1996; Wang et al., 2005) have been previously pro-posed. The use of k -nearest neighbors was proposed by (P  X erez-Cruz, 2008; P  X oczos &amp; Schneider, 2012) for estimating Shannon entropy and other related infor-mation theoretic measures. Convex risk minimization for estimating divergences was analyzed in (Nguyen et al., 2010). The use of nearest-neighbor graphs was timation with finite-sample rates for Lipschitz distri-butions. We refer the interested reader to the survey articles (Beirlant et al., 1997; Paninski, 2003) for more discussion.
 Very recently, a kernel estimator of the Shannon en-tropy with exponential concentration bounds was pro-posed by (Liu et al., 2012). The authors focused on distributions of the H  X older class (which is a subset of the Lipschitz class studied here). Moreover, the esti-mator proposed by (Liu et al., 2012) requires positive-ness of the density function, differentiability, vanishing first-order derivative in the boundaries, bounded third-order derivative 1 , and prior knowledge of lower/upper bounds of the density function. In contrast, our re-sults also apply to probability distributions with re-gions of zero-probability, nonsmooth density functions (discontinuous derivative), arbitrary behavior in the boundaries, and we do not require prior knowledge of lower/upper bounds. Additionally, our results apply to both bounded and unbounded variables, unlike (Liu et al., 2012).
 As expected, given that our results are relatively more general, our finite-sample rate for the entropy is slower than the rate of (Liu et al., 2012). More specifically, given N samples, with probability at least 1  X   X  , we approximate the Shannon entropy with a finite-sample et al., 2012) provides a rate of O ( N  X  1 / 2 , log (1 / X  )). In this paper, we propose the same framework for ap-proximating both the Bayes error rate and the Shan-non entropy. Our method is based on splitting the variable domain into bins. Then, we compute empiri-cal probabilities for each bin. Finally, we produce an empirical estimate of the statistical measure (Bayes er-ror and entropy). Interestingly, our method has prov-able two-sided exponential concentration bounds. For clarity of exposition, we present our proofs for the one-dimensional case. Given that the extension to sev-eral dimensions is trivial, we defer this topic until Sec-tion 5. In this paper, we assume Lipschitz continuity as the regularity condition for the probability distributions. Our Lipschitz assumption upper-bounds the rate of change of the density function. A kernel estimator of the Shannon entropy for the H  X older class was pro-posed by (Liu et al., 2012). The relationship between the H  X older and the Lipschitz class might not seem im-mediately obvious. In this section, we show that the H  X older class is a subset of the Lipschitz class. Next, we present our Lipschitz continuity assumption. Definition 1. A probability distribution P = p (  X  ) is called K -Lipschitz continuous, if its probability den-sity function p ( x ) is Lipschitz continuous with constant K  X  0 , that is: or equivalently for a continuous function p ( x ) : In Table 1 we provide some few examples of very well known parametric distributions that are Lipschitz con-tinuous. This includes the Gaussian, Laplace, Cauchy and Gumbel distributions. Note that by properties of Lipschitz continuity, any mixture of Lipschitz distri-butions is also Lipschitz.
 Next, we show that the H  X older class analyzed in (Liu et al., 2012) is a subset of the Lipschitz class. Theorem 2. Let f ( x ) be a function that belongs to the second-order Hold  X er class with domain x  X  [0; 1] and vanishing derivatives in the boundaries. That is, there is constant L  X  0 such that: and either  X  X   X  X  (0) = 0 or  X  X   X  X  (1) = 0 . Then, the function f ( x ) is Lipschitz continuous with constant K = 3 L . Proof. Assume that  X  X   X  X  (0) = 0 holds. By setting x = 0 in eq.(3) and by the previous assumption, we have | f ( u )  X  f (0) |  X  Lu 2 . This is a Lipschitz condition at x = 0 since  X  1  X  u  X  1 and therefore Lu 2  X  L | u | . Next, we focus at x &gt; 0. By the reverse triangle in-equality in eq.(3) and by setting u =  X  x , we have:  X  X  ( x ) u  X | f ( x + u )  X  f ( x ) | + Lu  X  X  ( x ) x  X | f (0)  X  f ( x ) | + Lx Therefore,  X  X   X  X  ( x )  X  2 Lx for x &gt; 0. Recall that x  X  1 and  X  1  X  u  X  1. By the reverse triangle inequality in eq.(3), we have:
A similar proof can be done for  X  X   X  X  (1) = 0 instead. In this section, we present our two-sided exponential concentration bound for Bayes error. We show results for both bounded and unbounded random variables. 3.1. Bounded Domain In this paper, we approximate statistical measures by splitting the variable domain into bins. To this end, we first provide a general concentration bound for the probability that a variable falls inside a specific bin. Proposition 3. Let x be a continuous random vari-able with domain D . Let d be a subset of D . Let P = p (  X  ) be a probability distribution with probabil-ity density function p ( x ) . Let the true probability  X  p d = P P [ x  X  d ] = R x  X  d p ( x ) . Given N i.i.d. sam-ples x 1 ,...,x N from P . Let the empirical probability b p probability statement holds: Proof. Let z n  X  1[ x n  X  d ]. We have: Note that z n  X  { 0 , 1 } and ing X  X  inequality, we prove our claim.
 Next, we provide bounds for the Bayes error rate inside a specific bin. These bounds depend only on the em-pirical probabilities, the Lipschitz constant K as well as the bin size.
 Lemma 4. Let x be a continuous random variable with compact domain D . Let d be a compact sub-set of D . Let P = p (  X  ) and Q = q (  X  ) be two K -Lipschitz continuous distributions. Let the true prob-abilities  X  p d = P P [ x  X  d ] = R x  X  d p ( x ) and  X  q d ] = R x  X  d q ( x ) . The true Bayes error rate on d , given lows: min (  X  p d ,  X  q d )  X  where | d | is the size of d .
 Proof. Given that p ( x ) is Lipschitz continuous with and that the maximum change in x is | d | . We have For finding a lower bound, note that: By replacing Z , we prove that the lower bound holds. For finding an upper bound, note that given any two R x  X  d p ( x ) =  X  p d . Similarly, B d ( P , Q )  X   X  q prove that the upper bound holds.
 Armed with the previous results, we present our first main contribution. That is, we show our two-sided exponential concentration bound for the Bayes error rate of a bounded variable.
 Theorem 5. Let x be a continuous random variable with compact domain D . Let P = p (  X  ) and Q = q (  X  ) be two K -Lipschitz continuous distributions. Given N i.i.d. samples x 1 ,...,x N from P and y 1 ,...,y N from Q . We divide D into T = N 1 / 4 compact nonoverlap-ping equally-sized subsets d 1 ,...,d T . Let the empirical probabilities b q ical Bayes error rate b B ( P , Q ) = P t min ( is bounded as follows with probability at least 1  X   X  : b and |D| is the size of D .
 Proof. Let the true probabilities  X  p t = P P [ x  X  d R x  X  d t p ( x ) and  X  q t = P Q [ x  X  d t ] = By Proposition 3 and the union bound, we have P [(  X  t ) | By solving for  X  , we have  X  = q 1 2 N log 4 T  X  . Let B t ( P , Q ) = R x  X  d B ( P , Q ) = P t B t ( P , Q ). By Lemma 4 and since | b p t  X   X  p t | X   X  and | b q t  X   X  q t | X   X  , we have: min ( By summing the latter expression for all t and by as-suming | d t | = |D| T , we have: b B ( P , Q )  X  T X   X  Finally, we replace  X  and set T = N 1 / 4 . 3.2. Unbounded Domain In order to extend our previous result from bounded variables to unbounded variables, we assume a very general concentration inequality. That is, we assume P [ x /  X  D  X  ]  X   X  . Such tail bounds are ubiquitous in the machine learning and statistics literature. In Ta-ble 2 we provide some few examples: distributions with finite variance, finite m -th moment (both by Chebyshev X  X  inequality), and sub-Gaussian distribu-tions. When several samples are available, we can use the union bound. That is, given N i.i.d. samples x ,...,x N , we have P [(  X  n ) x n /  X  X   X /N ]  X   X  . Next, we present our two-sided exponential concentra-tion bound for the Bayes error rate of an unbounded variable. The result is given for general tail bounds. For specific distributions, we can plug-in the size of the subdomain | D  X  | given in Table 2.
 Theorem 6. Let x be a continuous random variable with domain R . Assume that with high probability, x belongs to a compact set D  X  , that is P P [ x /  X  D R x/  X  X   X  p ( x )  X   X  and P Q [ x /  X  D  X  ] = Under the same conditions of Theorem 5, the true Bayes error rate B ( P , Q ) = R x  X  bounded as follows with probability at least 1  X   X  : b where b B ( P , Q ) ,  X  (1) N X  and  X  (2) NK D orem 5.
 Proof. Note that B ( P , Q ) = R x  X  X  R x/  X  X   X  min ( p ( x ) ,q ( x )). Theorem 5 provides bounds for the first term. It suffices to bound the second term. For finding a lower bound, note that both p ( x )  X  0 and q ( x )  X  0, therefore R x/  X  X  For finding an upper bound, note that given any two p ( x ). Therefore R x/  X  X   X  . The same bound is obtained from a similar argu-ment with q ( x ). In this section, we present our two-sided exponential concentration bound for Shannon entropy, for both bounded and unbounded random variables. 4.1. Bounded Domain First, we provide three general inequalities that will be useful for our purposes. In this paper, we approximate statistical measures by splitting the variable domain into bins. For our specific analysis, we need to bound the change in Shannon entropy at the bin level, as well as at each point. In what follows we present an inequality for bounding the entropy change at the bin level. That is, we consider  X  X nterval X  probabilities  X  p = R x  X  d p ( x )  X  [0; 1].
 Proposition 7. For 0  X  we have: Proof. Note that the bound holds trivially for  X  = 0. Next, we focus at  X  &gt; 0.
 Without loss of generality, assume 0  X   X  p  X  (The same holds for 0  X  | b p log where we used log z  X  z  X  1 for z =  X  p Let f ( r ) = r log e r . So far we proved that | | b p log in r  X  [0; 1]. Therefore, for all r such that 0  X  r  X   X   X  1  X  f ( r )  X  f (  X  ), which proves our claim.
 Next, we present an inequality for bounding the en-tropy change at each point. Recall that continuous distributions allow for point densities greater than one. That is, p ( x )  X  [0; +  X  ).
 Proposition 8. For p  X  0 and  X   X  0 : Proof. For proving the first inequality, by reasonably assuming p  X   X  :  X  ( p  X   X  ) log ( p  X   X  ) + p log p =  X  log ( p  X   X  ) + p log For proving the second inequality:  X  ( p +  X  ) log ( p +  X  ) + p log p =  X  log 1 where  X  p log (1 +  X  p )  X  X  X   X  follows from log (1 + z )  X  z for z =  X  p .
 In what follows we extend the log sum inequality from discrete variables to continuous variables. In this case, straightforward application of the Jensen X  X  in-equality is not possible. Therefore, we perform a reparametrization of the probability distribution. Proposition 9 (Log integral inequality) . Given a nonnegative function p ( x ) and a positive function q ( x ) , both with domain d . We have: Proof. Let  X  ( z ) be the Dirac delta function and r ( x ) = have: R where h ( z ) = R x  X  d q ( x )  X  ( z  X  r ( x )). sity function. We can write: Finally, we have: By replacing the E G [ z ], we prove our claim. Next, we provide bounds for the Shannon entropy in-side a specific bin. These bounds depend only on the empirical probability, the Lipschitz constant K as well as the bin and domain size.
 Lemma 10. Let x be a continuous random variable with compact domain D . Let d be a compact subset of D . Let P = p (  X  ) be a K -Lipschitz continuous dis-R H d ( P ) =  X  R x  X  d p ( x ) log p ( x ) is bounded as follows: is the size of d .
 Proof. Given that p ( x ) is Lipschitz continuous with and that the maximum change in x is | d | . We have For finding a lower bound, by Proposition 8 we have: H d ( P ) =  X 
Z =
Z  X 
Z =  X   X  p d log In this derivation, in order to lower-bound eq.(10), we that R x  X  X  p ( x ) = 1. By replacing Z , we prove that the lower bound holds.
 For finding an upper bound, we apply the log inte-gral inequality (Proposition 9) for the given p ( x ) and a constructed q ( x ) = 1. Thus, R x  X  d q ( x ) = | d | . Armed with the previous results, we present our sec-ond main contribution. That is, we show our two-sided exponential concentration bound for the Shannon en-tropy of a bounded variable.
 Theorem 11. Let x be a continuous random variable with compact domain D . Let P = p (  X  ) be a K -Lipschitz continuous distribution. Given N i.i.d. samples x ,...,x N from P . We divide D into T = N 1 / 4 com-pact nonoverlapping equally-sized subsets d 1 ,...,d T Let the empirical probabilities
P b H ( P ) =  X  P t lows with probability at least 1  X   X  : b
H ( P )  X   X  (1) N X  D  X   X  (2) NK D  X H ( P )  X  b H ( P ) +  X  log 4( e |D| ) 4 + 3 log N  X  2 log ( 1 4 log N + log 2  X  ) ,  X  the size of D .
 Proof. Let the true probabilities  X  p t = P P [ x  X  d R x  X  d t p ( x ). By Proposition 3 and the union bound, we have P [(  X  t ) | for  X  , we have  X  = q 1 2 N log 2 T  X  .
 Let H t ( P ) =  X  R x  X  d P t H t ( P ). By Lemma 10, Proposition 7, by assuming | d | X  1, and since |  X  b  X   X  log e By summing the latter expression for all t and by as-suming | d t | = |D| T , we have: b H ( P )  X  T X  log eT Finally, we replace  X  and set T = N 1 / 4 . 4.2. Unbounded Domain In order to extend our previous result from bounded variables to unbounded variables, we assume a very general concentration inequality as in Section 3.2. That is, we assume P [ x /  X  X   X  ]  X   X  .
 Next, we present our two-sided exponential concentra-tion bound for the Shannon entropy of an unbounded variable. The result is given for general tail bounds. For specific distributions, we can plug-in the size of the subdomain | D  X  | given in Table 2.
 Note that a trapezoidal distribution is inside our Lips-chitz class. For maximizing the entropy we can reduce the density of the flat region to an infinitesimal value, thus increasing the support. This explains the assump-tion of bounded variance in the following theorem. Theorem 12. Let x be a continuous random vari-able with domain R , zero mean and bounded variance, high probability, x belongs to a compact set D  X  , that is
P P [ x /  X  D  X  ] = R x/  X  X   X  p ( x )  X   X  . Under the same conditions of Theorem 11, the true Shannon entropy H ( P ) =  X  R probability at least 1  X   X  : b where b H ( P ) ,  X  (1) N X  D Theorem 11,  X  (3) K X  = min(0 , 1 2  X  (1  X  log (2 K X  ))) and  X  Proof. Note that H ( P ) =  X  R x  X  X  R x/  X  X   X  p ( x ) log p ( x ). Theorem 11 provides bounds for the first term. It suffices to bound the second term. For finding a lower bound, we construct a worst-case scenario in which the density concentrates in a small region. In particular, consider a  X  X alf-triangular X  dis-tribution Q with density q ( x ) = K 0 x for 0 &lt; K 0  X  K and x  X  [0; L ] * D  X  . That is, the distribution is in-side our Lipschitz class and the density vanishes at q (0) = 0. (The choice of x = 0 as an extreme of the interval is only for clarity of exposition.) By integra-therefore L = p 2  X  0 /K 0 . The entropy is given by:  X  R Since the latter function is nonincreasing with respect to  X  0 and K 0 , we prove that the lower bound holds. For finding an upper bound, we follow a variational calculus argument. Consider a distribution Q with zero mean and known variance, that is E Q [ x ] = 0 and E Q [ x 2 ]  X   X  . We are interested on finding the maximum  X  X alf-entropy X , that is, the entropy in the domain x  X  [0; +  X  ) = D  X  . (The choice of x = 0 as an extreme of the interval and the zero-mean as-sumption are only for clarity of presentation.) As-the nonnegativity of x 2 q ( x ), the  X  X alf-variance X  fulfills R By variational calculus and Lagrange multipliers, the solution of the above problem is q  X  ( x ) = q 2  X  0 3  X  X  0 The  X  X alf-entropy X  of Q  X  is given by: The function h (  X  0 , X  0 ) is increasing with respect to  X  therefore h (  X  0 , X  0 )  X  h (  X , X  0 ). The function h (  X , X  concave with respect to  X  0 . In order to obtain an upper bound, we make  X  X   X  X  0 (  X , X  0 ) = 0 and obtain  X   X  = 2 e which produces the maximum value h (  X   X  , X  0 ) = 3  X  0 2 . By putting everything together, we prove that the upper bound holds. Extension to several dimensions. Our results easily extend to V -dimensional data. Assume, for clarity of exposition that each of the V variables be-long to the same domain D 0 . That is x  X  D where D = D 0  X  X  X  X  X D 0 = D 0 V . The size of the domain D is |D ponential dependence of the number of samples N with respect to the dimensionality. That is N  X  X  ( |D 0 | 4 V ). For bounded variables where |D 0 |  X  1 this is not an issue. For bounded variables where |D 0 | &gt; 1 and unbounded variables, we have an exponential depen-dence. However this is expected for a nonparametric method. Note that the results in (Liu et al., 2012) only apply to two variables in the unit square ( D = [0; 1] 2 ). The effect of the domain size and the extension to higher dimensions are not immediately obvious.
 In our proofs, for a compact subset d of D , we used the fact that R x  X  d 1 = | d | which is an ` 1 measure. Therefore, we extend Lipschitz continuity for several dimensions with respect to the ` 1 norm. That is (  X  x 1 , x 2 ) | p ( x 1 )  X  p ( x 2 ) | X  K k x 1  X  x 2 k All theorems follow from these assumptions with-out any modification. Theorem 12 requires a minor change. More specifically, the lower bound produces a term  X  V instead of  X  . Finally, note that independent variables maximize the entropy, therefore a factor of V is needed in the upper bound.
 Implications. To the best of our knowledge, we present the first exponential concentration bounds for Bayes error rate estimation. As a byproduct, we ob-tain a classifier that is Bayes consistent with provable finite-sample rates.
 Regarding the Shannon entropy approximation, we ex-tended the class of distributions with provable finite-sample rates from the H  X older class (Liu et al., 2012) to the Lipschitz class. In contrast to (Liu et al., 2012), our results also apply to probability distributions with regions of zero-probability, nonsmooth density func-tions, arbitrary behavior in the boundaries, and we do not require prior knowledge of lower/upper bounds neither boundedness of the variable.
 The entropy approximation provides provable perfor-mance guarantees for learning the structure of trees and Bayesian networks from continuous variables. First, we show a generalization bound for trees (Chow &amp; Liu, 1968).
 Theorem 13. Let N be the number of samples and V the number of continuous variables. Let T be a tree distribution, that is T is a collection of V  X  1 edges that form a tree. Define I ( y,z ) = H ( y ) + H ( z )  X  X  ( y,z ) as the mutual information. Let b L ( T ) = 1 expected log-likelihood. Let b T = arg max T b L ( T ) be the empirical maximizer, and T  X  = arg max T L ( T ) the ground truth model. Under the same conditions of Theorem 11, with probability at least 1  X   X  : Proof. We need to approximate entropies for all nodes and pairs. That is, we need |H ( x v )  X  b H ( x v ) |  X   X  for every pair ( v,w ). The number of approximations is V + V 2  X  ( V + 1) 2 . A minor change in Theorem 11 is required in the union bound (2( V + 1) 2 T events instead of 2 T events) in order to obtain  X  . Note that a bound in entropy approximation implies a bound for mutual information. That is, we have |I ( x v ,x w )  X  b Next, we show a generalization bound for structure learning of Bayesian networks from continuous vari-ables. This bound complements the results for discrete variables (Friedman &amp; Yakhini, 1997; H  X offgen, 1993). In the following theorem we consider maximum like-lihood (De Campos, 2006) among models with a pre-scribed maximum number of parents k .
 Theorem 14. Let N be the number of samples and V the number of continuous variables. Let  X  be a Bayesian network where  X  v  X  X  1 ,...,V } is the parent log-likelihood. Let b  X  = arg max  X  b L ( X ) be the empiri-cal maximizer, and  X   X  = arg max  X  L ( X ) the ground truth model. Under the same conditions of Theorem 11, with probability at least 1  X   X  : Proof. For a maximum number of parents k , we need to approximate entropies from up to k + 1 variables. That is, we need |H ( x S )  X  b H ( x S ) |  X   X  for every set S  X  { 1 ,...,V } such that 1  X  |S|  X  k + 1. The num-nor change in Theorem 11 is required in the union bound (2 V k +2 T events instead of 2 T events) in or-der to obtain  X  . Note that a bound in entropy ap-proximation implies a bound for conditional entropy. ery v and  X  such that |  X  |  X  k . Therefore, we have |L ( X )  X  b L ( X ) |  X  2  X  for every Bayesian network  X . Algorithmic complexity. For V -dimensional data, instead of building an O (2 V ) matrix with all possi-ble bins, we can perform the following. First, we as-sign the proper bin to each of the N samples and store these bin-assignments in a O ( N ) array. Sec-ond, we sort the samples with respect to their bin-assignments, in O ( N log N )-time. Finally, since sam-ples in the same bin are consecutive, we can produce the empirical probabilities in O ( N )-time. Thus, our method is O ( N log N )-time and O ( N )-space. Tighter bounds. The Bayes error rate results can centration inequality for the ` 1 deviation of empirical distributions (Weissman et al., 2003). On the other hand, if we assume a minimum density  X  by Chernoff bounds we can obtain O (1 / X ,N  X  1 / 2 log N, log (1 / X  )). Concluding Remarks. There are several ways of extending this research. Bayes error rate approxi-mation for Lipschitz distributions by using k -nearest neighbors or a more general class of weighted-average plug-in classifiers (Stone, 1977) needs to be analyzed. The extension of kernel methods for Shannon entropy approximation from the H  X older class (Liu et al., 2012) to the Lipschitz class needs to be analyzed. It would be interesting to extend our method to a broader class of probability distributions. Finally, while our method uses equally-sized bins and follows a frequen-tist approach, more adaptive methods and Bayesian approaches should be analyzed.
 Ahmad, I. and Lin, P. A nonparametric estimation of the entropy for absolutely continuous distributions. IEEE Transactions on Information Theory , 1976. Antos, A., Devroye, L., and Gy  X orfi, L. Lower bounds for Bayes error estimation. PAMI , 1999.
 Beirlant, J., Dudewicz, E., Gy  X orfi, L., and
Van der Meulen, E. Nonparametric entropy estima-tion: An overview. International Journal of Mathe-matical and Statistical Sciences , 1997.
 Chow, C. and Liu, C. Approximating discrete prob-ability distributions with dependence trees. IEEE Transactions on Information Theory , 1968.
 De Campos, L. A scoring function for learning
Bayesian networks based on mutual information and conditional independence tests. JMLR , 2006.
 Drakopoulos, J. Bounds on the classification error of the nearest neighbor rule. ICML , 1995.
 Eggermont, P. and LaRiccia, V. Best asymptotic nor-mality of the kernel density entropy estimator for smooth densities. IEEE Transactions on Informa-tion Theory , 1999.
 Fralick, S. and Scott, R. Nonparametric Bayes-risk es-timation. IEEE Transactions on Information The-ory , 1971.
 Friedman, N. and Yakhini, Z. On the sample complex-ity of learning Bayesian networks. UAI , 1997.
 Gy  X orfi, L. The rate of convergence of k n -NN regres-sion estimates and classification rules. IEEE Trans-actions on Information Theory , 1981.
 H  X offgen, K. Learning and robust learning of product distributions. COLT , 1993.
 Kohler, M. and Krzy  X zak, A. Rate of convergence of lo-cal averaging plug-in classication rules under margin condition. ISIT , 2006.
 Kulkarni, S. and Posner, S. Rates of convergence of nearest neighbor estimation under arbitrary sam-pling. IEEE Transactions on Information Theory , 1995.
 Liu, H., Lafferty, J., and Wasserman, L. Exponen-tial concentration for mutual information estimation with application to forests. NIPS , 2012.
 Nguyen, X., Wainwright, M., and Jordan, M. Estimat-ing divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory , 2010.
 Nock, R. and Sebban, M. An improved bound on the finite-sample risk of the nearest neighbor rule. Pat-tern Recognition Letters , 2001.
 of R  X enyi entropy and mutual information based on generalized nearest-neighbor graphs. NIPS , 2010. Paninski, L. Estimation of entropy and mutual infor-mation. Neural Computation , 2003.
 Paninski, L. and Yajima, M. Undersmoothed kernel entropy estimators. IEEE Transactions on Infor-mation Theory , 2008.
 P  X erez-Cruz, F. Estimation of information theoretic measures for continuous random variables. NIPS , 2008.
 P  X oczos, B. and Schneider, J. Nonparametric esti-mation of conditional information and divergences. AISTATS , 2012.
 Stone, C. Consistent nonparametric regression. The Annals of Statistics , 1977.
 Tsybakov, A. and Van der Meulen, E. Root-n con-sistent estimators of entropy for densities with un-bounded support. Scandinavian Journal of Statis-tics , 1996.
 Van Es, B. Estimating functionals related to a density by a class of statistics based on spacings. Scandina-vian Journal of Statistics , 1992.
 Wang, Q., Kulkarni, S., and Verd  X u, S. Divergence es-timation of continuous distributions based on data-dependent partitions. IEEE Transactions on Infor-mation Theory , 2005.
 Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and Weinberger, M. Inequalities for the ` 1 deviation of the empirical distribution. Technical Report HPL-
