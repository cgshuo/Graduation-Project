 The Nystr X m method is an efficient approach to enabling large-scale kernel methods. The Nystr X m method generates a fast approximation to any large-scale symmetric positive semidefinete (SPSD) matrix using only a few columns of the SPSD matrix. However, since the Nystr X m approximation is low-rank, when the spectrum of the SPSD matrix decays slowly, the Nystr X m approximation is of low accuracy. In this paper, we propose a variant of the Nystr X m method called the modified Nystr X m by spectral shifting (SS-Nystr X m). The SS-Nystr X m method works well no matter whether the spectrum of SPSD matrix decays fast or slow. We prove that our SS-Nystr X m has a much stronger error bound than the standard and modified Nystr X m methods, and that SS-Nystr X m can be even more accurate than the truncated SVD of the same scale in some cases. We also devise an algorithm such that the SS-Nystr X m approximation can be computed nearly as efficient as the modified Nystr X m approximation. Finally, our SS-Nystr X m method demonstrates significant improvements over the standard and modified Nystr X m methods on several real-world datasets. G.1.0 [ Numerical Analysis ]: General X  Numerical algorithms ; G.1.3 [ Numerical Analysis ]: Numerical Linear Algebra X  Sparse, structured, and very large systems Kernel approximation; the Nystr X m method; large-scale machine learning Corresponding Author
With the advent of the big-data era, how to efficiently learn from big-data has become a major concern and a hot topic of machine learning research. When data are large, many expensive matrix op-erations, e.g., matrix inverse and eigenvalue decomposition, which cost O ( m 3 ) time and O ( m 2 ) space for an m  X  m matrix, become computational prohibitive. Such expensive matrix operations are indispensable for many classical machine learning methods like kernel methods [22, 23], so these machine learning methods are infeasible when facing big-data problems. One possible approach to making matrix computation and kernel methods scalable is to use randomized matrix approximations to reduce the time and space costs [18], among which the most famous one is perhaps the Nystr X m method [5, 10, 14, 15, 20, 26, 30, 31].

The Nystr X m method approximates an arbitrary symmetric pos-itive semidefinite (SPSD) kernel matrix using a small subset of its columns, and the method reduces the time complexity of many matrix operations from O ( m 3 ) or O ( m 2 k ) to O ( mc complexity from O ( m 2 ) to O ( mc ) , where k is the target rank, c is the number of selected columns, and it holds in general that k &lt; c m . In this way, time and space costs are only linearly in m , so many kernel methods can be efficiently solved even when m is large.

Williams &amp; Seeger [30] used the Nystr X m method to speedup matrix inverse such that the inference of large-scale Gaussian process regression can be efficiently performed. Later on, the Nystr X m method has been applied to spectral clustering [7, 17], kernel SVMs [33], kernel PCA and manifold learning [26, 32, 33], kernel ridge regression [3], determinantal point processes [1], etc.
However, although the Nystr X m method is usually effective and efficient, its approximation quality can be very low in some cases. to the best rank-k approximation) of the Nystr X m approximation grows with the matrix size m at least linearly. Thus the approxi-mation can be rather rough when m is large, unless a large number of columns are selected to construct the Nystr X m approximation, which will violate the intention of using matrix approximations. To improve the approximation quality without sampling a large amount of columns, some other fast matrix approximation models have been proposed. Particularly, Wang &amp; Zhang [28] developed the modified Nystr X m method to generate a low-rank approximation in a similar way to the standard Nystr X m method. The modified Nystr X m method is much more accurate than the standard Nystr X m method in that it only samples an acceptable amount of columns and its relative error does not grow with matrix size m . In addition, the modified Nystr X m method only requires the original matrix to be symmetric, which is milder than SPSD required by the standard Nystr X m method.

The standard/modified Nystr X m methods generate low-rank ap-proximations to kernel matrices, and their approximation errors cannot be better than the rank c truncated SVD, where c is the number of columns selected by the Nystr X m methods. When the spectrum of a kernel matrix decays slowly (that is, the c + 1 to m largest eigenvalues are not small enough), the low-rank approximations constructed by either the truncated SVD or the standard/modified Nystr X m methods are far from the original kernel matrix. Cortes et al. [3] showed that the accuracy of kernel approximations affects the accuracy of learning algorithms. Therefore, when the spectrum of the kernel matrix decays slowly, the standard/modified Nystr X m methods cannot generate effective approximations to be used in learning algorithms.

To make the approximation still effective even when the spec-trum of the original kernel matrix decays slowly, we propose in this paper a new method called the modified Nystr X m by spectral shifting (SS-Nystr X m) . Unlike the standard/modified Nystr X m methods which approximate the kernel matrix K  X  R m  X  m by a low-rank factorization K  X  CUC T , our SS-Nystr X m approximates K by K  X   X  C  X  U  X  C T +  X  I m , where C , U ,  X  U  X  R c  X  c , and  X   X  0 . When the spectrum of K decays slowly, the term  X  I m significantly improves the approximation accuracy. We show that SS-Nystr X m method has a provably tighter bound than the standard/modified Nystr X m methods. In sum, this paper offers the following contributions:
The kernel approximation models proposed in the very recent work [15, 24] are also variants of the Nystr X m method and reported to achieve higher approximation accuracy. It is also straightforward to improve the ensemble Nystr X m method [15] and the memory efficient kernel approximation method [24] using the spectral shifting method proposed in this paper.

The remainder of this paper is organized as follows. In Section 2 we define the notation that will be used in this paper. In Section 3 we formally introduce the standard Nystr X m method and the modified Nystr X m method. In Section 4 we formulate our SS-Nystr X m method and show that our SS-Nystr X m can speedup several kernel methods in the same way as the standard Nystr X m method does. In Section 5 we theoretically show the superiority of SS-Nystr X m over the standard/modified Nystr X m methods. In Section 6 we devise an efficient algorithm for computing SS-Nystr X m. In Section 7 we empirically evaluate the SS-Nystr X m method and the proposed efficient algorithm. The proof of the theorems are deferred to the appendix.
For a matrix A = [ a ij ] , we let a j be its j -th column, and We also let I m be the m  X  m identity matrix and let 1 m be the size-m all-one vector.

Letting  X  = rank( A ) , we write the condensed singular value decomposition (SVD) of A as A = U A  X  A V T A , where the ( i,i ) -th entry of  X  A  X  R  X   X   X  is the i -th largest singular value of A , columns of U A and V A , respectively, and  X  A ,k be the k  X  k top left block of  X  A . Then the matrix A k = U A ,k  X  A ,k V  X  X losest X  rank-k approximation to A . If A is a squared matrix, we let  X  i ( A ) be the i -th largest eigenvalue. If A is SPSD, then the eigenvalue decomposition and SVD are equivalent.

For an m  X  n matrix, the full SVD costs time O (min { m 2 and the rank k truncated SVD costs time O ( mnk ) . Although mul-tiplying an m  X  n matrix by an n  X  p matrix runs in O ( mnp ) flops, the constant in the big-O notation is tremendously smaller than that of SVD, and matrix multiplication can be highly efficiently performed in parallel computing facilities. So we instead denote the time complexity of matrix multiplication by T multiply ( mnp ) , which is far less than O ( mnp ) in practice [12, 28].
Given an m  X  m SPSD matrix K , we let J ( J  X  [ m ] , { 1 , 2 ,...,m } and |J| = c ) be an index set computed by some column selection algorithm. Then we let C  X  R m  X  c columns of K indexed by J , and let W  X  R c  X  c be the rows of C indexed by J . The standard Nystr X m method [30] approximates K by where W  X  is the Moore-Penrose inverse of W . The modified Nystr X m method [28, 29] is defined by The only difference between the standard and the modified Nys-tr X m methods is their intersection matrices: U nys = W the standard Nystr X m method and U mod = C  X  K ( C  X  ) modified Nystr X m method.

When rank( K ) = rank( U nys ) = rank( U mod ) , that is, when K is low-rank, both of the standard/modified Nystr X m approximations are exact [29]. The modified Nystr X m method is in general more accurate than the standard Nystr X m method due to k K  X   X  K c k F  X  k K  X   X  K nys c k F . With the selected columns at hand, it costs time O ( c 3 ) to compute the standard Nystr X m approximation and O ( mc 2 ) + T multiply ( m 2 c ) to compute the modified Nystr X m approximation.

The quality of the Nystr X m approximations is largely determined by whether the selected columns are informative, so a better column selection algorithm makes the Nystr X m approximations more accurate. In the previous work much attention has been paid to the relative-error column selection algorithms [2, 4, 6, 10, 11], among which the uniform sampling [10] and adaptive sampling [4] are the most widely used ones. The following lemma is the strongest theoretical result for the Nystr X m methods [28].
L EMMA 1. Given an m  X  m symmetric matrix K and a target rank k , by selecting c = O ( k  X  2 ) columns of K to form C  X  R m  X  c using the adaptive sampling based algorithm of [28], the following inequality holds: where K k denotes the top k truncated SVD approximation of K. The algorithm takes time O ( mc 2 + mk 3  X  2 / 3 ) + T multiply and space O ( mc ) in computing C and U of the modified Nystr X m approximation.

Lemma 1 indicates that by selecting c = O ( k  X  2 ) columns of K , the modified Nystr X m approximation achieves comparable accuracy as the rank k truncated SVD. When the spectrum of K decays fast, the approximation generated by the truncated SVD is highly accurate, and so is the modified Nystr X m approximation. Otherwise, if the bottom m  X  k singular values (i.e. eigenvalues) of K are large, then k K  X  K k k F is large, and so is k K  X 
This work is closely related to the matrix ridge approximation (MRA) [34], which improves approximation accuracy by pre-serving the eigenvalues both large and small. When the bottom eigenvalues of K are large, MRA is much more accurate than the truncated SVD and the Nystr X m method [27, 34]. However, MRA is solved by iterative algorithms and is thus not pass-efficient and memory-efficient, it is thus limited to medium-scale data problems. Inspired by MRA, we propose a kernel approximation model which inherits the efficiency of the Nystr X m method and is effective when the bottom eigenvalues are large.
In the first subsection we formulate and justify our SS-Nystr X m method. In the second subsection we discuss how to apply SS-Nystr X m to speedup Gaussian process regression, kernel SVM, and kernel ridge regression problems.
Given a target rank k (  X  c m ), the SS-Nystr X m approxima-tion of K is defined as Here  X   X  0 is called the spectral shifting term and the rank c modified Nystr X m approximation of  X  K = K  X   X  I Notice that since  X  K is symmetric but possibly not SPSD, the c  X  c intersection matrix  X  U is also in general indefinite. Later we will that is why we call our method the modified Nystr X m by spectrum shifting .

Now we consider how to choose  X  . It follows from the definition directly that the approximation error is K  X   X  K ss c =  X  Lemma 1 indicates that by selecting sufficiently many columns of  X  K to construct  X  C and  X  U , it holds in expectation that
E K  X   X  K ss c = E  X  K  X   X  C  X  U  X  C T  X  (1 + )  X  K  X  Apparently, for fixed k , the smaller the error k  X  K  X   X  tighter error bound the SS-Nystr X m has; if k  X  K  X   X  K k K k k F , then SS-Nystr X m has a better error bound than the modified Nystr X m. Therefore, to make the error bound as strong as possible, we formulate the following optimization problem to compute  X  : However, since  X  K is in general indefinite, it needs all of the eigenvalues of K to solve the problem exactly. Since computing the full eigenvalue decomposition is expensive, we attempt to relax the problem. Considering that we seek to minimize the upper bound of k  X  K  X   X  K k k 2  X  , leading to the solution If we choose  X  = 0 , then SS-Nystr X m degenerates to the modified Nystr X m method. The following theorem indicates that the SS-Nystr X m with any  X   X  (0 , X  opt ] has a stronger relative-error bound than the modified Nystr X m method.
 T HEOREM 2. Give an m  X  m SPSD matrix K , we let  X  K = K  X   X  I m and  X  opt be defined in (3). Then for any  X   X  (0 , X  following inequality holds:
We give an example in Figure 1 to illustrate why SS-Nystr X m  X  X ail X  of the eigenvalues becomes thinner after the spectral shifting. Specifically, k K  X  K k k 2 F = 0 . 52 and k  X  K  X   X  K When the same number of columns are selected to construct the SS-Nystr X m or the modified Nystr X m approximations, SS-Nystr X m has much tighter error bound because k  X  K  X   X  K k k 2 F than k K  X  K k k 2 F .
We discuss in this section how to speed up matrix inverse and eigenvalue decomposition using the Nystr X m methods.Many kernel methods will become scalable if the matrix inverse and eigenvalue decomposition can be efficiently solved. Let K  X  R m  X  m be the kernel matrix, and let the SS-Nystr X m approximation of K be defined by We show that when K is replaced by  X  K ss c , the aforementioned linear system and eigenvalue decomposition can be efficiently solved. by setting  X  = 0 .

We first show how to approximately compute b = ( K +  X  I m )  X  1 y . Let  X  U = Z X Z T be the condensed eigenvalue decomposition of the intersection matrix of SS-Nystr X m, where Z  X  R c  X   X  ,  X   X  R  X   X   X  , and  X  = rank(  X  U )  X  c . We expand Figure 1: Toy data: 100  X  100 SPSD matrix whose the t -th eigenvalue is 1 . 05  X  t . We set m = 100 , k = 30 , and thus  X  0 . 064 . We plot the eigenvalues of K in Figure 1(a) and K  X   X  opt I 100 in Figure 1(b). (  X 
K c +  X  I m )  X  1 by the Sherman-Morrison-Woodbury formula and obtain where  X  =  X  +  X  . In this way the linear system (4) can be computed in only O ( mc 2 ) time and O ( mc ) space.

Now we show how to approximately compute the eigenvalue decomposition of K . We let  X  C = U  X  C  X   X  C V  X  C be the condensed SVD of  X  C . Suppose  X  = rank(  X  C ) , we let and we write the eigenvalue decomposition of S as S = U S Now we can write the eigenvalue decomposition of  X  K ss c Here U  X   X  R m  X  ( m  X   X  ) is a column orthogonal matrix orthogonal to ( U C U S ) . We provide theoretical analysis for the SS-Nystr X m method in Theorem 3, which shows that SS-Nystr X m has a much tighter error bound than the modified Nystr X m method. We also demonstrate in Example 1 that in some cases the SS-Nystr X m method can be better than any other low-rank matrix approximation methods.
 T HEOREM 3. Suppose there is a column selection algorithm A col such that for any m  X  m symmetric matrix S and target rank k ( m ), by selecting c  X  C ( m,k, ) columns of S using algorithm A col , the modified Nystr X m method attains the error bound Then for any m  X  m SPSD matrix K , we compute  X  opt according to (3) and compute  X  K = K  X   X  opt I m . By using A col C ( m,k, ) columns of  X  K , the SS-Nystr X m defined in (1) attains the error bound K  X   X  K ss c 2 F  X  (1 + ) K  X  K k 2 F  X 
If the columns of  X  K are selected by the adaptive sampling based algorithm of [28], which satisfies the assumption in Theorem 3 and is the best practical algorithm for the modified Nystr X m method, then the error bound incurred by SS-Nystr X m is given in the following corollary.

C OROLLARY 4. Suppose we are given an SPSD matrix K . By sampling c = O ( k  X  2 ) columns of  X  K using the adaptive sampling based algorithm of [28], SS-Nystr X m attains the following error bound: E K  X   X  K ss c 2 F  X  (1 + ) k K  X  K k k 2 F  X 
Recall from Lemma 1 that the best known error bound of the modified Nystr X m method is where c = O ( k  X  2 ) columns are selected from K . When the bottom eigenvalues  X  k +1 ( K ) ,  X  X  X  , X  m ( K ) are large, we can see from Lemma 1 and Corollary 4 that the error bound of SS-Nystr X m is much better than that of the modified Nystr X m method. Here we give an example to demonstrate the superiority of SS-Nystr X m over the the standard/modified Nystr X m methods and even the truncated SVD of the same scale.

E XAMPLE 1. Let K be an m  X  m SPSD matrix such that  X  ( K )  X   X  X  X   X   X  k ( K ) &gt;  X  =  X  k +1 ( K ) =  X  X  X  =  X  m By sampling c = O ( k ) columns by the adaptive sampling based algorithm of [28], we have that and that In this example SS-Nystr X m is far better than the other approxima-tion methods if we set  X  a large constant.
Notice that computing the spectral shifting term  X  in (1) accord-ing to (3) requires the truncated SVD which costs time O ( m and space O ( m 2 ) . This can be accelerated by computing the top-k singular values approximately using random projection techniques [2, 12]. We depict the whole algorithm for computing SS-Nystr X m using random projections in Algorithm 1. The performance of the approximation is analyzed in the following theorem. Algorithm 1 The Modified Nystr X m by Spectral Shifting. 1: Input: an m  X  m SPSD matrix K , a target rank k , the 2: // compute  X  opt approximately 3:  X   X  X  X  m  X  l standard Gaussian matrix; 4: Q  X  X  X  the l orthonormal basis of Y = K X   X  R m  X  l ; 5: s  X  X  X  sum of the top k singular values of A = Q T K  X  R 6:  X   X  = 1 m  X  k tr( K )  X  s  X   X  opt ; 7: // spectral shifting 8:  X  K  X  K  X   X   X  I m  X  R m  X  m ; 9: // compute the modified Nystr X m approximation for  X  K 10:  X  C  X  X  X  c columns of  X  K selected by some column sampling 11:  X  U  X  X  X   X  C  X   X  K (  X  C  X  ) T  X  R c  X  c ; 12: return the SS-Nystr X m approximation  X  K ss c =  X  C  X 
T HEOREM 5. Let  X  opt be defined in (3) and  X   X  , k , l , m be defined in Algorithm 1. The following inequality holds in expectation: where the expectation is taken w.r.t. the Gaussian random matrix  X  in Algorithm 1. Lines 2 X 6 in Algorithm 1 compute O ( ml 2 ) + T multiply ( m 2 l ) and space O ( ml ) .

By using Algorithm 1 to compute  X  opt approximately, it costs only O ( ml 2 ) + T multiply ( m 2 l ) more time to compute the SS-Nystr X m approximation than the modified Nystr X m approximation. Our experiments show that a small l (say, l = 4 k ) is sufficient for obtaining a highly accurate approximation to  X  opt whether the spectrum of K decays fast or slow. Since it costs O ( mc 2 ) + T multiply ( m 2 c ) time to compute the modified Nystr X m complexity for computing SS-Nystr X m is the same as computing the modified Nystr X m.
We conduct experiments on several real-world datasets to evalu-ate the method and algorithm proposed in this paper. In Section 7.1 we describe the setup of the experiments. In Section 7.2 we evaluate the fast approximation of  X  opt proposed in Section 6. In Section 7.3 we compare SS-Nystr X m with the standard/modified Nystr X m methods on several middle-size datasets. In Section 7.4 we compare the methods on a large-scale dataset where the kernel matrix does not fit in RAM.
We perform experiments on several datasets released by UCI [8] and Statlog [19]. We obtain the data collected on the LIBSVM website 1 where the data are scaled to [0,1]. We summarize the datasets in Table 1.

For each dataset, we generate a radial basis function (RBF) kernel K  X  defined by and a sparse RBF kernel K  X , X ,C [9] defined by http://www.csie.ntu.edu.tw/  X  cjlin/libsvmtools/datasets/ Here x 1 ,  X  X  X  , x m are the data instances,  X  &gt; 0 is the scaling parameter, C &gt; 0 is the cutting-off point, d is the number of scaling parameter  X  is, the faster the spectrum of the kernel decays. For the sparse RBF kernel, following [10], we fix  X  = d ( d + 1) / 2 e and C = 3  X  .

We implement all the algorithms in MATLAB and run the algorithms on a workstation with Intel Xeon 2.40GHz CPUs, 24GB RAM, and 64bit Windows Server 2008 system. To compare the running time, we set MATLAB in single thread mode by the command  X  maxNumCompThreads(1)  X .
We evaluate the accuracy of the approximation to  X  opt (Lines 2 X 6 in Algorithm 1) proposed in Theorem 5. We generate RBF kernel matrices of the listed datasets, where we set the scaling parameter  X  = 0 . 1 or  X  = 1 . We use the error ratio |  X  opt  X   X   X  | / X  the approximation performance. We repeat the experiments 20 times and plot the average error ratio versus l/k in Figure 2. Here  X   X  , l , and k are defined in Theorem 5. We can see from Figure 2 that the approximation to  X  opt is of very high quality: when l = 4 k , the error ratios are less than 0 . 03 in all cases. So we set l = 4 k in all of the subsequent kernel approximation experiments in order to obtain a low over-sampling rate with a high accuracy at the same time.
We evaluate the kernel approximation accuracy of SS-Nystr X m mainly in comparison with the standard/modified Nystr X m meth-ods. In this paper our attention is mainly focused on kernel matrices whose spectrum decay slowly, so we set the scaling parameter  X  a small value. Otherwise if  X  is large, the bottom eigenvalues will be very small, and consequently the spectral shifting parameter  X  will be so small that there is no significant difference between SS-Nystr X m and the modified Nystr X m. Specifically, we set k = 50 and  X  = 0 . 2 for the dense RBF kernels and  X  = 2 for the sparse RBF kernels. We use Algorithm 1 to compute the SS-Nystr X m approximation, in which we set l = 4 k . For each of the standard/modified/SS Nystr X m methods, we use two algorithms to select columns: the uniform sampling algorithm [10] and the adaptive sampling algorithm [28].

We report the approximation accuracy and running time of each algorithm for each method. The approximation accuracy is evaluated by where  X  K is the approximation generated by each method. Every time when we do column sampling, we run each sampling al-gorithm 10 times and report the minimal approximation error of the 10 repeats since the error bound of each method is actually guaranteed with expectations and we can get a quite accurate approximation within 10 repeats according to [28]. We report the average elapsed time of the 10 repeat rather than the total elapsed time because the 10 repeats can be done in parallel on 10 machines. We depict the approximation errors and the average elapsed time of the dense RBF kernels in Figures 3 and Figure 4 and those of the sparse RBF kernels are in Figure 5 and Figure 6. In the figures, we use c m as the X -axis because the compared methods have the same RAM cost when c and m are fixed.

The results clearly show that our SS-Nystr X m works significantly better than the standard/modified Nystr X m methods when the spectrum of the kernel matrix decays slowly. As for the running parameter  X  = 0 . 1 , and the right corresponds to  X  = 1 . kernels. time, our SS-Nystr X m is a little slower than the modified Nystr X m because SS-Nystr X m needs to compute  X  opt approximately by randomized SVD, which costs time O ( mk 2 ) + T multiply ( m we set l = 4 k ). Since it costs time O ( mc 2 ) + T multiply compute the modified Nystr X m approximation, so our SS-Nystr X m should be only constant times slower than the modified Nystr X m; this is verified by experiments. Finally, we compare SS-Nystr X m with the standard/modified Nystr X m methods on a large-scale dataset. We use the MNIST RBF kernels. [16] dataset which has 60 , 000 instance. We generate an RBF kernel with the scaling parameter  X  = 5 . The kernel matrix of MNIST has size of 60 , 000  X  60 , 000 which exceed the RAM of our workstation. We partition the kernel matrix to 30 blocks of size 60 , 000  X  2 , 000 and store them in the disk; at each time at most one block is loaded into the RAM. We only use uniform sampling to construct the approximations because other sampling methods are much more expensive, and we set k = c/ 3 for SS-Nystr X m. The standard Nystr X m method goes one pass through the data, the modified Nystr X m method goes two passes, and SS-Nystr X m (Algorithm 1) goes four passes. We report the approximation error (the minimum of 10 repeats) in Figure 7. We can see that the error Figure 7: The kernel approximation error incurred by the standard Nystr X m, modified Nystr X m, and SS-Nystr X m on the MNIST dataset. incurred by SS-Nystr X m is lower than that of the standard/modified Nystr X m methods. This set of experiments demonstrates that our proposed SS-Nystr X m method is still feasible for large-scale data that do not fit in RAM.
The Nystr X m method is an important kernel approximation method for enabling large-scale machine learning algorithms. In this paper we have proposed the SS-Nystr X m method which is a variant of the Nystr X m method and can speedup many kernel meth-ods in the same way as the standard/modified Nystr X m methods. We have shown that SS-Nystr X m has a much stronger error bound than the standard/modified Nystr X m methods. Especially, when the bottom eigenvalues of a kernel matrix are not sufficiently small, the approximation accuracy of the standard/modified Nystr X m method or even the truncated SVD is unsatisfactory, while our SS-Nystr X m can still generate approximations of high accuracy. We have also devised an algorithm for computing SS-Nystr X m efficiently. Finally, the experiments have further demonstrated the effectiveness and efficiency of our SS-Nystr X m method.
Shusen Wang is supported by Microsoft Research Asia Fel-lowship 2013 and the Scholarship Award for Excellent Doctoral Student granted by Chinese Ministry of Education. Hui Qian is supported by the National Natural Science Foundation of China (No. 61272303) and the National Program on Key Basic Research Project of China (973 Program, No. 2010CB327903). Zhihua Zhang is supported by the National Natural Science Foundation of China (No. 61070239).

We prove the three theorems of this paper respectively in the following subsections.

P ROOF . Since (2) is in convex and  X  opt is the minimizer of (2), then for any  X   X  (0 , X  opt ] , it holds that
X Then the theorem follows by the inequality (2) that any  X  in the given interval can result in a smaller error.
 P ROOF . The error incurred by SS-Nystr X m is Here the inequality follows from the property of the column selection algorithm A col . The i -th largest eigenvalue of  X  ( K )  X   X  opt , so the m eigenvalues of  X  K 2 are all in the set eigenvalues of  X  K 2 must be less than or equal to the sum of any m  X  k of the eigenvalues, thus we have by which the theorem follows.
 P ROOF . Let  X  K = Q ( Q T K ) k , where Q is defined in Line 4 in Algorithm 1. It was shown in [2] that where the expectation is taken w.r.t. the random Gaussian matrix  X  .

It follows from Lemma 6 that where  X  K and  X   X  K contain the singular values in a descending order. Since  X  K has a rank at most k , the k + 1 to n entries of  X 
K are zero. We split  X  K and  X   X  K into vectors of length k and m  X  k : and thus Since k x k 2  X k x k 1  X  Then it follows from (3) and Line 6 in Algorithm 1 that
The following lemma is used to prove the theorem. The lemma is easy to prove, so here we do not show the detailed proof.
L EMMA 6. Let A and B be square matrices and  X  A and  X  B contain the singular values in a descending order. Then we have that
