 Web page classification is one of the essential techniques for Web mining. Specifically, classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web. However, constructing a classi-fier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a "homepage" classifier, one needs to collect a sample of homepages (positive exam-ples) and a sample of non-homepages (negative examples). arduous work and special caution to avoid biasing them. We introduce in this paper the Positive Example Based Learn-ing (PEBL) framework for Web page classification which eliminates the need for manually collecting negative train-ing examples in pre-processing. We present an algorithm called Mapping-Convergence (M-C} that achieves classifica-tion accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM, the M-C algorithm performs as well as traditional SVM. H.m [Information Systems]: Miscellaneous; 1.5.2 [Computing and evaluation; 1.7.m [Computing Methodologies]: Doc-ument and Text Processing--Miscellaneous; 1.2.6 [Computing Methodologies]: Artificial Intelligence---Learning Algorithms, Performance, Experimentation permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. Mapping-Convergence (M-C) algorithm, labeled data, unla-beled data, SVM (Support Vector Machine) 
Automatic categorization or classification t of Web pages have been studied extensively since the Internet has become a huge database of information, in terms of both volume and variance. Given the fact that Web pages are based on loosely structured text, many variances of statistical text learning algorithms have been applied to Web page classification. 
Previous approaches for multi-class classification prob-lems [2, 15] define mutually exclusive classes a priori, train each class from training examples, and choose one best match-ing class for each testing data. However, mutual-exclusion between classes is not a very realistic assumption because of the fact that a single page usually falls into several cate-gories. Also, the pre-defined classes usually do not fit end-users' search purposes because it is hard for a pre-defined set of classes to satisfy the diverse and changing interests of users. For example, a company wants to find "XML ex-pert" pages from the Internet. They may start by searching for the keywords "XML" or "expert" on any search engine, then trying to refine the search results repeatedly until they collect a fair amount of "XML expert" pages. However, if they specify a search class or domain information such as "resume" or "personal homepage', the refining process could be eliminated by applying a query of "XML" upon the classes of resume or personal homepage. 
Researchers have realized these problems, and proposed the classifications of user-interesting classes such as "calls for paper", "personal homepage" [9]. This involves binary clas-sification techniques that distinguish Web pages of a user-interesting class from all others. This binary classifier is an essential component for Web mining because identifying Web pages of a particular class from the Interuet is the first step of mining interesting data from the Web. A binary classifier is a basic component of building a domain specific engine [12] as well as a multiclass classification system [16, 1]. When binary classifiers are considered independently 1 Classification is distinguished from clustering in terms that classification requires a learning phase (training phase) be-fore actual classification (testing phase). 
Constructing a binary classifier for Web pages requires la-
We present here the Positive Example Based Learning 
Our goal is to achieve classification accuracy from positive 
In this paper, we focus on the second challenge, achieving 
Our experiments (Section 5) fall into two different do-
One might argue that using a sample of universal set it-
The new contributions of our PEBL framework are the 
Pre-processing for classifier construction requires col-lecting only positive e.Ynrnples, which speeds up the en-tire process of constructing classifiers and also opens a way to support example-based query on the Inter-net. Figua'e 1 shows the difference between a typical learning framework and the PEBL framework for Web page classification. Once a sample of the universal set is collected in PEBL, the sample is reused as unlabeled data for every class, therefore users would not need to resample the universal set each time they construct a new classifier.  X  PEBL achieves accuracy as high as that of a typical framework: without loss of efficiency in testing. PEBL runs the M-C algorithm in training phase to construct an accurate SVM from positive and unlabeled data. 
Once the SVM is constructed, classification perfor-mance in the testing phase will be equivalent to that of a typical SVM in terms of both accuracy and efficiency.  X  Although we address Web page classification in this paper because of its high demand for this application, our PEBL framework can be easily applied to other classification problems within different domains, such as diagnosis databases, text databases, or electronic commercial databases. 
Note that this paper concentrates on the classification al-
The remainder of the paper is organized as follows: Sec-every class. algorithm and provide justification of why it works. Section 5 reports the result of a systematic experimental compari-son using two classification domains: the Internet and CS department sites. Section 6 outlines several important is-sues to consider regarding learning algorithm, and the PEBL framework. Finally, Section 7 reviews and concludes our dis-cussion of the PEBL framework. issues that need discussion. tion? Unlabeled data contains information about the joint distribution over features other than the class label. Clus-tering techniques utilize the features of unlabeled data to identify natural clusters of the data. However, class labels do not always correspond to the natural clustering of data. 
When unlabeled data are used with a sample of labeled data, it increases classification accuracy in certain problem settings. This is called semi-supervised learning. The EM algorithm is a representative algorithm which can be used for either semi-supervised learning or unsupervised learning [6]. However, the result depends on the critical assumption that the data sets are generated using the same paramet-ric model used in classification. Kamal Nigam inserted two parameters into EM (to relax the generative assumptions): one for controlling the contributions of labeled data and un-labeled data, and the other for controlling the quantity of mixture components corresponding to one class [17]. An-other semi-supervised learning occurs when it is combined with SVMs, to form transductive SVM [11]. With careful parameter settings, both of those works show good results within certain environments, such as environments with an extremely low amount of labeled data. When the number of labeled data grows or the generative assumptions are vi-olated, semi-supervised learning schemes suffer significant degradation of classification accuracy. classification is termed learning from positive and unlabeled data. In 1998, F. Denis defined the PAC learning model for positive and unlabeled examples, and showed that k-
DNF (Disjunctive Normal Form) is learnable from positive and unlabeled examples [7]. Since then, some experimental attempts to learn using positive and unlabeled data have been tried using k-DNF or decision trees [13, 5]. However, those methods are not very useful for Web page classifica-tion problems because; (1) k-DNF or decision trees are not very tolerant with high dimensionality and sparse instance space, which are the properties of Web page classification, (2) their algorithms require knowledge of the proportion of positive instances within the universal set, which is not avail-able in many classification problems, and (3) they perform poorer than traditional learning schemes given sufficient la-beled data. The M-C (Mapping-Convergence) algorithm in-troduced in this paper relaxes the above three limitations in 
Web page classification from positive and unlabeled exam-ples. tinguish one class of data from the rest of the feature space given only positive data set [19, 14]. One-class SVMs draw the class boundary of the positive data set in the feature space. However, due to lack of the information about neg-ative data distribution, they require much larger amount of positive training data to induce accurate boundary, and their performance is dependent on the user parameters in-dicating how strictly the boundary should fit around the data. From our experiments of the one-class SVM using the 
LIBSVM 2, the one-class SVM performs initially very poor with the standard parameter setting, and even with careful parameter setting, the performance is much worse than that of our M-C algorithm because the one-class SVM does not utilize the distribution of unlabeled data. proaches. The first stage of the M-C algorithm (called the mapping stage) is based on I-DNF, which was previously proven learnable from positive and unlabeled data [7]. The Figure 2: A graphical representation of a linear 
SVM in a two-dimensional case. (i.e. Only two fea-tures are considered.) M is the distance from the separator to the support vectors in feature space. second stage of the M-C algorithm (called the convergence stage) uses SVM (Support Vector Machine) technology [3], especially the marginal property of SVMs. As a binary classification algorithm, SVM gains increasing popularity because it has shown outstanding performance in many domains of classification problems [8, 10, 21]. Espe-cially it tolerates the problem of high dimensions and sparse instance spaces. There has been a recent surge of interest in SVM classifiers in the learning community. SVM provides several salient properties that other learn-ing algorithms do not have, such as maximization of margin and nonlinear transformation of the input space to the fea-ture space using kernel methods [3]. To illustrate, consider its simplest form, a linear SVM. A linear SVM is a hyper-plane that separates a set of positive data from a set of negative data with raa~imurn margin in the feature space. 
The margin (M) indicates the distance from the hyperplaue (class boundary) to the nearest of the positive and negative data in the feature space. Figure 2 shows an example of a simple two-dimensional problem that is linearly separable. 
Each feature corresponds to one dimension in the feature space. The distance from the hyperplane to a data point is determined by the strength of each feature of the data. 
For instance, consider a resume page classifier. If a page has many strong features related to the concept of "resume" (e.g. words "resume" or "objective" in headings), the page would belong to positive (resume class) in the feature space, and the location of the data point should be far from the class boundary on the positive side. Likewise, another page not having any resume related features but having many non-resume related features should be located far from the class boundary on the negative side. In cases where the points are not linearly separable, the 
SVM has a parameter, C (the penalty imposed on training data that fall on the wrong side of the decision boundary). 
The SVM computes the hyperplane that maximizes the dis-tances to support vectors for a given parameter setting. For problems that are not linearly separable, advanced kernel methods can be used to transform a non-linear input space to a linear feature space. We used the linear kernel of SVM in our experiments because of it's speed and relatively high 
The main thrust of this paper is how to achieve classifi-cation accuracy (from positive and unlabeled data) as high as that from labeled (positive and unbiased negative) data. The M-C algorithm achieves this goal. What can we learn from positive and unlabeled data? We can identify strong positive features from positive and unla-beled data by checking the frequency of those features within positive mad unlabeled training data. For instance, a feature that occurs in 90% of positive data but only in 10% of unla-beled data would be a strong positive feature. Suppose we build a list of every positive feature that occurs in the pos-itive training data more often than in the unlabeled data. 
By using this list of the positive features, we can filter out every possibly positive data point from the unlabeled data set, which leaves only strongly negative data -We call these strong negatives. For instance, we say a strong negative is a data point not having any of the positive features in the list. (In this case, the list is considered I-DNF.) In this way, we can extract strong negatives from the unlabeled data. This is what the mapping stage of the M-C algorithm accom-plishes. However, using the list, we can only identify strong negatives that are located far from the class boundary. In other words, although I-DNF is potentially learnable from positive and unlabeled data, its resulting quality of learning is not good enough. What can we do with the strong negatives to construct an accurate class boundary? We were given samples of positive mad unlabeled data, and now we have strong negatives ex-tracted from the unlabeled data through the mapping stage. 
If we construct a SVM from the positives and only the strong negatives, the class boundary would be fax from accurate due to the insufficient negative training data. We use the marginal property of SVMs to refine the inaccurate bound-ary into the accurate one. This process is the convergence stage of the M-C algorithm. In this section, we explain the details of the M-C algorithm. 
The distance from the hyperplane (the class boundary) to a negative data point in the feature space is proportional to its relative strength. For instance, consider a resume classi-tier. Assume that there are two negative data points (non-resume pages) in the feature space: one is "how to write a resume" page and the other is "how to write an article" page. In the feature space, the article writing page is consid-ered to be more distant from the resume class because the resume writing page has more features related to resumes (e.g. the word "resume" in text) though it is not an actual resume page. The following definition quantizes the level of the negative strength into n discrete levels. (This is concep-tual division of the strength. In real cases, the levels of the strength may be continuous.) strongest negative, Ml (neg), is farthest from POS (the pos-itive) in the feature space of universal set, U. A map of negatives, Mi(neg), is farther than Mi+i(neg) from POS. The map of weakest negative, Mn(neg), is nearest to POS. 
Figure 3 visualizes the strength of negative. points, everything that excludes positives in the universal set. be a set subsuming POS. Namely, for i=0 to n-1 where U = So(pos). Note that S~(pos) = algorithm consists of two stages -(1) the mapping stage and (2) the convergence stage. from U using 1-DNF learning without great concern for the quality of the mapping. The remaining data points, ex-cluding the strongest negative (M1 (neg)) from U, would be 
Si (pos) which subsumes the positive as defined previously. k-DNF has been proven learnable from positive and unla-beled data in previous work [7], which supports the feasi-bility of the mapping stage execution when not overly con-cerned with the quality of the mapping. A condition for the mapping stage is that Ml (neg) excludes true positive. The quality of the mapping is not critical to the performance of the M-C algorithm if this condition is not violated. We discuss this more in Section 4.3. gregate mapped negatives (Mi (ne9)) as close as possible to the unbiased negatives (NEG). We illustrate this through the following example. pages" in a university site. POS is a given sample of faculty (positive) pages. U is a sample of the university site (an unbiased sample of the universal set). NEG is a set of the other pages in the university site excluding faculty pages (unbiased negative pages). POS and U are assumed to be given, and NEG is initially set to null because negative examples are not given. We assume that a university site has the following four different levels (n=4) of the negative strength. 
M4(neg): the weakest negative pages considered most similar to the faculty pages (e.g. staff pages) 
M3(ne9): the secondly weakest negative pages which are less similar to the faculty pages (e.g. student pages) 
M2(neg): the negative pages not very similar to the faculty pages (e.g. project or course pages) 
M1 (ne9): the strongest negative pages considered most dis-parate from the faculty pages (e.g. information or facility pages) 
Next, consider how we fill the NEG with unbiased neg-ative data extracted from U. Let's say the I-DNF identi-ties only the strongest negative, Ml(neg) (e.g. information or facility pages). We save the M1 (neg) into NEG, and the rest of U will be Si(pos). (Sl(pos) = U~=2Mk(neg) U 
POS.) We train a SVM with the NEG (currently contain-ing Ml(neg)) and the given POS. The SVM generates a hyperplane between NEG and POS, which keeps a maxi-mal margin between them in the feature space. (See Fig-ure 4.(a).) When, using the SVM, we test Sl(pos), the 
SVM divides the Sl(pos) into M~(neg) (e.g. project or course pages) and  X (pos). (S2(pos) = POS U M4(neg) U 
M3(neg).) We now accumulate the M2(neg) into NEG, and then we re-train the SVM with the NEG (currently con-raining M1 (neg) t9 M2 (neg) ) and the given POS. The SVM generates another hyperplane between NEG and POS also keeping maximal margin between them in the feature space. (See Figure 4.(b).) When we test S2(pos) using the SVM, the SVM divides the S2(pos) into M~(ne9) (e.g. student pages) and Ss(pos). (S3(pos) = POSU M4(neg).) We accu-mulate the Ma(neg) into NEG again, and re-train the SVM with the NEG (currently containing U~=lMk (ne9)) and the given POS. We iterate these processes until the Mi(neg) be. comes empty set. The SVM constructed at the end of the process will be close to the SVM constructed from positive and unbiased negative data because NEG will converge into the unbiased negative data in the universal set, U. (U was assume to be an unbiased sample of the universal set.) Fig-ure 5 shows the outline of the M-C algorithm, and Figure 6 shows the corresponding data flow of each part of the algo-rithm. 
Example 1 may discretize too much the boundary of each level of the negative strength. However, in real cases with continuous levels, as you see in Figure 4, the marginal prop-erty of SVMs urges the hyperplane to converge into the real boundary of the two classes (positive and negative classes) in the feature space. Our experiments in Section 5 show that the M-C algorithm with positive and unlabeled data actu-ally performs as well as the traditional SVM with labeled data. 
The goal of the mapping stage is to subsume positive with-out great concern for the quality of the mapping as we dis-cussed in Section 4. We do this by building a disjunction list of positive features, which is equivalent to 1-DNF. Af-ter we construct the 1-DNF of positive features, we map the subsuming positive (Sl(pos)) and the strongest negative (Mi(ne9)) by filtering the universal set (U) through the I-In this section, we provide empirical evidence that our 
Experiment I. (The Internet) The first universal set in Bxperiment ~. (University computer science department) 
Input: 
Output: 
Algorithm: 
Rationale: 
Analysis: 
Figure 5: Mapping-Convergence (M-C) algorithm Figure 6: Data flow diagram of the Mapping-Convergence (M-C) algorithm We extracted features from different parts of a page -URL, title, headings, link, anchor-text, normal text, and meta tags. Each feature is a predicate indicating whether each term or special character appears in each part. (e.g. ',~' in UI:tL, a word 'homepage' in title) We did not use stemming or a stoplist because it could hurt performance in Web page classification. For example, a common stopword, 'T' or "my", is a good indicator of a student homepage. 
For SVM implementation, we used SVM light 4. As we dis-cussed in Section 3, we used linear kernel method because it is simple and efficient. We discuss more about the usage of advanced kernel methods in Section 6. For the param-eter, C (the penalty imposed on training data that fall on the wrong side of the decision boundary), we used the de-fault parameter, [ave./x * x] -1 of the SVM light. We didn't rigorously try to find out the optimal C, because the de-fault setting showed good performance in all cases. In many other learning algorithms, finding best parameters is usually critical to the performance. It is necessary for them to per-form cross-validation to determine many problem-specific parameters, which is a time consuming and laborious man-ual process. Without it, they perform extremely poorly (sometimes, poorer than random). The strong mathemat-ical foundation of SVM makes it possible to run the M-C algorithm fully automatically without human interruption to determine best parameter setting for each iteration or each specific problem. We used the same parameter, C, for all our experiments, so the whole process can be done auto-matically and generally (not dependent on specific problem). 
Result reports are based on precision-recall breakeven point (P-R), a standard measure for binary classification. Accu-racy is not a good performance metric because very high accuracy can be achieved by always predicting the negative class. Precision and recall are defined as: The precision-reeaU breakeven point (P-R) is defined as the precision and recall value at which the two are equal. We adjusted the decision threshold b of the SVM at the end of each experiment to find P-R. 
We first show the performance comparison between PEBL and traditional SVM (trained from manually labded data) on the six classes of the two universal sets -the Internet and CS department sites. We first constructed a SVM from positive (POS) and unlabeled data (U) using PEBL. On the other hand, we manually classified the unlabeled data (U) to extract unbiased negatives from them, and then we built a traditional SVM from POS and those unbiased negatives. We tested the same testing documents using those two SVMs -PEBL and TSVM (Traditional SVM). Table 1 shows the P-R (precision-recall breakeven points) of each SVM, and it also shows the number of iterations to converge in the case of the PEBL. In most cases, PEBL without negative training data performs almost as well as the traditional SVM with manually labeled training data. For example, when we 4http://svmlight .joachims.org (positive and unbiased negative) data. 
Table I: Precision-recall breakeven points (P-R) showing performance of PEBL (Positive Exam-ple Based Framework) and TSVM (the Traditional 
SVM trained from manually labeled data) in the two universal sets (U). The number of iterations to the convergence in PEBL is shown in parentheses. 
CS Department project 86.67 86.06 (12) and those 800 negatives gives 0.64 P-R. Likewise, at the sev-enth iteration, the number of induced negatives is almost the same as the number of real unbiased negatives, and also the 
P-K at the point is as high as the P-R of the traditional SVM (TSVM). The performance (P-R: precision-recall breakeven point) of M-C is converging rapidly into that of TSVM in all our experiments. The P-R convergence graphs in Figure 8 show one more line (P-R of UN), which is the P-R when using the sample of universal set (U) as a substitute for negative training data. 
As we discussed at the end of Section 1, they obviously show the performance decrement when using U as a substitute for negative training data, because a small number of false positive training data affects significantly the set of support vectors which is critical to classification accuracy. manually classify 109 resume pages and 2388 unbiased non-resume pages to train a SVM in a traditional way, it gives 96.2% P-R (precision-recall breakeven point). When we use 
PEBL with only the 109 resume pages without non-resume pages, it gives also 96.2% P-R. Figure 7 and 8 show the details of convergence (of the induced negative training data and corresponding P-R) at each iteration in the experiment of the universal set, the In-ternet and CS department sites respectively. For instance, consider the graphs of the first column (personal homepage class) in Figure 7. The number of induced negatives at the first iteration is around 250 (shown on the first row of the graph), and the P-R of the SVM trained from positive and those 250 negatives is 0.60 (shown on the second row of the graph). At the second iteration, the number of induced negatives is around 800, and the SVM trained from positive 
In this section, we discuss several important issues to con-sider regarding learning algorithm and PEBL framework. 1. Possible extension of PEBL methodology  X o non-SVM learning method? 
Other supervised learning methods such as probabilistic (e.g., naive bayes) or mistak~driven learning methods (e.g., per-ceptron, winnow) do not maximize the margin. As we dis-cussed in Section 3, SVM maximizes the margin, which en-sures that the initial class boundary converges into the real boundary of the two (positive and negative) class. Other learning methods do not guarantee the convergence of the class boundary, and even if they converge the boundary oc-casionally, the rate of convergence would be slower. 2. Choice of kernel func X ions. 
SVMs provide nonlinear transformation of input space to [1] E. L. Allwein, R. E. Schapire, and Y. Singer. 
Reducing multiclass to binary: a unifying approach for margin classifiers. Journal of Machine Learning 
Research, 1(2000):113-141, 2000. [2] H. Chen, C. Schuffels, and R. Orwig. Interuet categorization and search: a machine learning approach. Journal of Visual Communications and 
Image Representation, 7(1):88-102, 1996. [3] C. Cortes and V. Vapnik. Support vector networks. 
Machine Learning, (20):273-297, 1995. [4] M. Craven, D. Dipasquo, and D. Freitag. Learning to extract symbolic knowledge from the world wide web. 
In AAAL 1998. [5] F. DeComite, F. Denis, and R. Gilleron. Positive and unlabeled examples help learning. In Algorithmic 
Learning Theory (ALT), 1999. [6] A. P. Dempster, N. M. Laird, and D. B. Rubin. 
Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 
Series B, 39:1-38, 1977. [7] F. Denis. Pac learning from positive statistical queries. In ALT, 1998. [8] S. Dumais and H. Chen. Hierarchical classification of web content. In SIGIR, 2000. [9] E. J. Glover, G. W. Flake, and S. Lawrence. Improving category specific web search by learning query modifications. In Symposium on Applications and the Internet (SAINT ~001), 2001. machines. In ECML, 1998. classification using support vector machines. In ICML, 1999. Building a new niche search engine. In Conference on 2000. positive and unlabeled examples. In ALT, 2000. document classification. Journal of Machine Learning Research, 2(Dec):139-154, 2001. categorization for ir system. Technical report, Stanford University, htt p: //citeseer .nj.nec.com/164846.html, 1998. coupled neural networks or support vector machines. In ICANN, 2001. unlabeled documents using era. Machine Learning, 39(2/3):103-134, 2000. hypertext categorization method using links and incrementally available class information. In SIGIR, 2000. generation for optimizating one-class classifiers. 2(Dec):155-173, 2001. characteristics of web documents for classification. In 
Mining and Knowledge Discovery, 2000. categorization methods. In SIGIR, 1999. semi-structured documents. In KDD, 2000. 
