 Logistic regression is one core predictive modeling technique that has been used extensively in health and biomedical problems. Recently a lot of research has been focusing on enforcing sparsity on the learned model to enhance its effec-tiveness and interpretability, which results in sparse logistic regression model. However, no matter the original or sparse tor form. This limits the applicability of logistic regression in the problems when the data cannot be naturally repre-sented vectors (e.g., functional magnetic resonance imaging and electroencephalography signals). To handle the cases when the data are in the form of multi-dimensional arrays, we propose MulSLR : Multilinear Sparse Logistic Regression. MulSLR can be viewed as a high order extension of sparse lo-gistic regression. Instead of solving one classification vector as in conventional logistic regression, we solve for K classi-fication vectors in MulSLR ( K is the number of modes in the data). We propose a block proximal descent approach to solve the problem and prove its convergence. The con-vergence rate of the proposed algorithm is also analyzed. Finally we validate the efficiency and effectiveness of Mul-SLR on predicting the onset risk of patients with Alzheimer X  X  disease and heart failure.
 J.3 [ Life and Medical Sciences ]: Health; G.3 [ Probability and Statistics ]: Correlation and Regression Analysis Logistic Regression; Multilinear; Proximal Gradient; Health-care
Clinical risk prediction, such as predicting the onset [27] or hospitalization [19] risk of patients with chronic diseases, is an important problem in health informatics. Accurate risk prediction can greatly help reduce the unnecessary costs in hospitals as well as provide the right service at point-of-care.
There has been quite a few existing works in both data mining and health informatics domains on clinical risk pre-diction. For example, Sun et al . [27] developed a LASSO type of method for identifying important risk factors for pre-dicting the onset risk of heart failure patients. Xiang et al . [30] proposed a multi-source learning approach for predict-ing the risk measured by cognitive score of patients with Alzheimer X  X  disease. Miravitlles et al . [18] analyzed the fac-tors associated with increased risk of exacerbation and hos-pital admission for patients with Chronic Obstructive Pul-monary Disease (COPD). In most of those works, logistic regression is at the heart of the predictive modeling process. Because of the large number of potentially related factors in different scenarios, a sparsity constraint is usually added on the learned model coefficients. The resultant model is referred to as sparse logistic regression, which can do both prediction and feature selection simultaneously. Depending on the different sparsity structures the model wants to ex-plore, we can construct different sparsity-induced regulariza-tion terms. By adding them to the objective of conventional logistic regression we can get different types of sparse logistic regression models [13][17][26].

Until now most of the existing sparse logistic regression type of approaches assume their inputs are a set of data vectors. This means that we need to have a vector based representation for each patient if we want to adopt logistic regression type of methods to evaluate their risk. However, many patient medical data are not naturally in vector form. For example, X-Ray images are two-dimensional; Electroen-cephalography (EEG) is two dimensional if you stack all sig-nals captured from different poles; functional Magnetic Res-onance Imaging (fMRI) is three-dimensional. Even in pa-tient Electronic Health Records (EHRs), there could be mul-tiple diagnosis/symptoms accompanied with several drugs patient with a diagnosis by drug co-occurrence matrix if we want to consider the correlation between diagnosis and drugs when predicting the patient risk. We can also represent the patients with high order tensors if we want to consider more than two factors that are inter-correlated with each other. one straightforward way is to stretch those matrices and tensors into vectors as people did in image processing, but this will lose the correlation information among different di-mensions. Moreover, after stretching the dimensionality of the data objects will become very high, which will make traditional logistic regression inefficient.

Based on the above considerations, many researches on extending traditional vector based approaches to two (ma-trix based) or high order (tensor based) approaches have been becoming more and more popular. For example, two-dimensional Principal Component Analysis (PCA) [32] and Linear Discriminant Analysis [33] have been found to be more effective on face recognition task compared to tradi-tional vector based PCA and LDA. Cai et al . [4] also extend Support Vector Machine to multidimensional data and got better results on document classification. Recently, Huang and Wang [9] developed a matrix variate logistic regression model and applied it in electroencephalography data anal-ysis. Tan et al. [28] further extended logistic regression to tensor inputs and achieved good performance in a video classification task.

In this paper, we propose MulSLR , a multi-linear sparse logistic regression method that can directly take matrices or tensors as inputs and perform prediction. Because of the added L1 sparsity regularization terms, we developed a Block Proximal Gradient (BPG) method to solve the prob-lem iteratively. We theoretically prove the convergence of the proposed algorithm and analyze the convergence rate based on the Kurdyka X  X ojasiewicz inequality [3]. Finally we validate the effectiveness of our algorithm on both syn-thetic and real world data sets.
 It is worthwhile to highlight the following aspects of Mul-SLR .
The rest of this paper is organized as follows. Section 2 reviews some related works. The details along with the convergence analysis of MulSLR is introduced in Section 3. Section 4 presents the experimental results, followed by the conclusions in Section 5.
Logistic regression [8] is a statistical classification method that has widely been used in many application areas, such as computer vision [23], information retrieval [6] and health informatics [18][30]. Suppose we have a training data matrix X = [ x 1 , x 2 ,  X  X  X  , x n ]  X  R d  X  n , where x i  X  R d is the i -th training data vector. Associated with each x logistic regression is to train a linear classification function data in class 0 by minimizing the following logistic loss where w  X  R d is the classification vector and b is the bias. They can be learned with gradient descent type of approaches.
In many real world applications, the data vectors { x i } are usually sparse and high-dimensional (e.g., each patient could be a tens of thousands dimensional vector with bag-of-feature representation [27]). To enhance the interpretabil-ity of the model in these scenarios, we can add a sparsity regularization term on w and minimize the following ` regularized logistic loss ` ( w ,b ) = 1 off the prediction accuracy and model sparsity. The resul-tant model is usually referred to as sparse logistic regression model [12]. Compared with the conventional logistic regres-sion model obtained by minimizing J org , the w obtained by minimizing J sp is sparse. In this way, we can not only get a predictor, but also know what are the feature dimensions that are important to the prediction, and these are the fea-tures with nonzero classification coefficients. Zou and Hastie [34] pointed out that there are some limitations if we only adopt 1 norm regularization, and they proposed a regular-ization term called elastic net , which is a mixture of 1 and 2 norm regularizers.

Sparse logistic regression has widely been used in health informatics because the applications in this field usually not only want a good prediction performance but also need the reason why. Basically what are the key factors that will af-fect the prediction performance. For example, sparse logistic regression has been used in the prediction of Leukemia [15], people also designed different regularization terms [13][17][26] to a enforce more complex sparsity patterns on the learned model. However, all these works require a vector based rep-tration on the difference of traditional vector based logistic regression and multilinear logistic regression when working on multi-dimensional data. Those traditional approaches need to stretch the data into an ultra-high dimensional vec-tor first before they can be applied. This may suffer from the curse of dimensionality.
We introduce the details of MulSLR in this section. First we will formally define the problem.
Without the loss of generality, we assume each observation response is y i  X  X  0 , 1 } , then MulSLR assumes Figure 1: Traditional vector based logistic regression and multilinear logistic regression work on multi-dimensional data. where  X  k is the mode-k product, and w k  X  R d k  X  1 is the prediction coefficients on the k -th dimension. Then Let W = { w 1 , w 2 ,  X  X  X  , w K } be the set of prediction coeffi-cient vectors. The loss we want to minimize is ` ( W ,b ) = 1 where for notational convince, we denote The loss function we considered in this paper is Logistic Loss : We adopt the following elastic net regularization term [34] Then the optimization problem we want to solve is We adopt a Block Coordinate Descent (BCD) procedure to solve the problem. Starting from some initialization ( W (0) ( w = arg min where Algorithm 1 Block Coordinate Descent Procedure Require: Data set {X i ,y i } n i =1 , Regularization parameters 1: Initialization : ( W (0) ,b (0) ), t = 0 2: while Not Converge do 3: for k = 1 : K do 4: Update ( w k ( t ) ,b ( t,k ) ) by solving problem (11) 5: t = t + 1 6: end for 7: end while
The proximal descent procedure for updating ( w i ( t ) ,b is w arg min + where  X  w k ` ( W ,b ) = 1 = 1 =  X  1 and e w ( t ) is an extrapolated point defined as The optimal solution to problem (12) can be obtained as shrinkage operator defined as
Similarly we can update b ( t,k ) as b and e b ( t,k ) is the extrapolated point defined as The optimal solution of problem (18) can be obtained by simple gradient descent as
Putting Eq.(14) and Eq.(19) together, we have the follow-ing theorem
Theorem 3.1. The partial gradient  X  ( w k ,b ) ` ( W ,b ) is Lip-schitz continuous with constant
Proof. See Appendix I.  X  k ( t ) in Eq.(16) and Eq.(21) can be set as the Lipschitz con-stant according to Theorem 3.1. For notational convenience, we first introduce Then  X  k ( t ) can be set as  X  ( t ) can be set as 2 1 +
Algorithm 2 summarized the whole algorithmic flow of our algorithm. At each iteration the most time consuming part is evaluating the gradient, which takes O ( n Q time, that is linear with respect to data set size and data dimension.
 Algorithm 2 Block Proximal Gradient Descent for Multi-linear Sparse Logistic Regression Require: Data set {X i ,y i } n i =1 , Regularization parameters 1: Initialization : ( W (0) ,b (0) ), t = 1 2: while Not Converge do 3: for k = 1 : K do 4: Compute  X  k ( t ) , X  k ( t ) with Eq.(24) and Eq.(25) 5: Compute e w k ( t ) , w k ( t ) with Eq.(15) and Eq.(16) 6: Update e b ( t,k ) ,b ( t,k ) by and Eq.(20) and Eq.(21) 7: end for 9: Reupdate w k ( t ) and b ( t,k ) using Eq.(16) and Eq.(21), 10: end if 11: t = t + 1 12: end while
Theorem 3.2. Let W ( t ) be the sequence generated by Al-some point (  X  W ,  X  b ) .
 Proof. See Appendix II.
 With Theorem 3.2, it is not hard to see that (  X  also a stationary point. This is because when W ( t )  X   X  b ( t,k )  X   X  b , according to Eq.(24),  X  k ( t )  X   X   X  k  X  w k = (26) arg min +  X   X  To establish the convergence rate estimation result, we first introduce the following K-L inequality.

Definition 3.3. (Kurdyka-Lojasiewicz (K-L) Inequal-such that subdifferential of f at  X  [24], and
The K-L inequality was first introduced by Lojasiewicz [14] on real analytic functions, for which Eq.(27) is bounded extended this property to the functions on the o -minimal structure, and recenttly Bolte et al. [3] further extended it to non smooth sub analytic functions, and loss function J ( W ,b ) is one function of such type. Then it satisfies the K-L inequality. According to [31], we have the following theorem stating the convergence rate of our BPG method.
Theorem 3.4. Let (  X  W ,  X  b ) be a stationary point of the we have the following convergence rate.
Proof. The proof can easily be derived from the proof of Theorem 2.9 in [31], thus we neglect the details here.
In this section we will present the results of a set of exper-iments we conducted to test the effectiveness and efficiency of our method, including both synthetic examples and real world examples.
We constructed some synthetic data sets to investigate two types of questions: 1. Whether MulSLR can effectively discover the latent 2. What is the scalability behavior of MulSLR when it is Figure 2: Two samples from the synthetic data set we generated. (a) is from class 1, (b) is from class 0. The intensities of the pixels indicates the values of the corresponding entries, where dark means small values and bright means large values.

In order to answer question 1, we constructed a data set matrix with size 100  X  100, where the elements in the data matrices are generated independently from N (0 , 1), i.e., uni-variate Gaussian distribution with zero mean and unit vari-ance. The upper-left 20  X  20 block was different for the data matrices in class 1 and class 0 in the following sense. We generate two vectors w 1  X  R 20 and w 2  X  R 20 whose elements are generated independently from uniform distri-bution between 0 and 1. For any data matrix X from class 20  X  20 block of X . For any data matrix Y from class 2, we block of Y . Therefore there is a special correlation structure on the two dimensions of those data matrices. Basically the bilinear combination on their upper-left 20  X  20 blocks. We provide two sample data matrices on Fig.2, one from each class. From the figure we cannot judge whether there is any differences between them. We generated 1000 samples for each class and thus the entire data set has 2000 samples.
In our implementation, we initialize w 1  X  R 100 , w 2  X  R as uniform vectors, and we iterate the MulSLR until a cer-tain termination condition is satisfied. Such termination condition could be either a maximum number of iteration steps or the absolute difference of objective function value between two consecutive steps is less than a certain toler-ance value. For those free parameters we set  X  1 =  X  2 = 0 . 01,  X  1 =  X  2 = 0 . 0001,  X   X  = 0 . 99. We set the maximum number of iterations to be 100. We randomly select 80% of the data for training (the data in class 1 and 0 are evenly sampled), and the rest 20% data for testing. The objective function value convergence plot is shown in Fig.3 (a), from which we can see that with the iterations going on, the objective function value decreases very fast during the first 30 steps, and decreases slowly from 30 to 60 steps, and becomes al-most stable from then on. We also evaluated the prediction performance on the testing data set in terms of Area Un-der the receiver operating characteristic Curve (AUC) value using w 1 and w 2 obtained from each iteration. The pre-diction AUC versus number of iterations plot is illustrated in Fig.3 (b), which shows that the prediction performance also increases very fast during the first 30 steps, and slowly increase to 1 from step 30 to step 60.

As illustrations, we also plotted the matrix of W = w 1 w after 100 iterations with MulSLR in Fig.4 (b). This is inter-esting because MulSLR makes decisions with a linear func-tion, and in two dimension case the weight, or importance of the ( i,j )-th entry is W ij (as w &gt; 1 Xw 2 = vec ( W ) where vec is a function vectorizing a matrix). From the we can clearly observe a block structure on the upper left corner. This complies with the latent data structure and explains the reason why we can achieve a 1 AUC. For com-parison purpose, we also plotted the matrix of w 1 w &gt; 100 iterations with MulSLR with  X  1 =  X  2 = 0 in Fig.4 (a), in which case the sparsity ( ` 1) regularizations on w w 2 do not take effect. We can observe that in this case the matrix is dense, which is because w 1 and w 2 are dense vec-tors. We also checked the predicted AUC value this dense w 1 and w 2 can get, which is only 0.6525. This validates the superiority of sparse multilinear logistic regression over plain multilinear logistic regression in this case.
To answer the second question, we conducted two sets of experiments on Mac OS 10.7 with 2.2GHz CPU and 12GB main memory. In the first set of experiments, we randomly generated a set of 100  X  100 data matrices, and we record the Figure 3: Convergence plots on the synthetic data. (a) shows how the objective function value with re-spect to the number of iterations when training with 80% of the data. (b) shows how the corresponding testing AUC goes with number of iterations on the rest 20% data. From the figure we can see that in this case MulSLR converges in about 60 iterations averaged running time per iteration for MulSLR with respect to different data set size. The result is shown in Fig.5(a), which shows a clear linear trend between the running time and data scale. In the second set of experiments, we fixed the data set size to be 100, but varying the data dimensionality from 50  X  50 to 2000  X  2000. The result is provided in Fig.5(b), which shows that the trend of that curve is slightly quadratic (as the entire data dimensionality is the square of the horizontal axis values). This is in accordance with our complexity analysis of Algorithm 2.
Functional magnetic resonance imaging or functional MRI (fMRI) is a functional neuroimaging procedure using MRI technology that measures brain activity by detecting associ-ated changes in blood flow 1 . fMRI is an effective approach to investigate alterations in brain function related to the ear-liest symptoms of Alzheimer X  X  disease, possibly before devel-opment of significant irreversible structural damage.
The raw fMRI scans used in our experiments were col-cognitive function scores (semantic, episodic, executive and spatial -ranges between -2.8258 and 2.5123) were also ac-quired at the same time using a cognitive function test. http://en.wikipedia.org/wiki/Functional_magnetic_ resonance_imaging (a) No ` 1 regularizations (b) With ` 1 regularizations Figure 4: The matrix of  X  w 1  X  w &gt; 2 , where  X  w 1 ,  X  w converged solution over 100 iterations. (a) shows the result with  X  1 =  X  2 = 0 , i.e., no ` 1 regularizations. (b) shows the result of MulSLR with  X  1 =  X  2 = 0 . 01 . There are three types of MRI scans that were collected from the subjects: (1) FA, the fractional anisotropy MRI gives information about the shape of the diffusion tensor at each voxel, which reflects the differences between an isotropic dif-fusion and a linear diffusion; (2) FLAIR, Fluid attenuated inversion recovery is a pulse sequence used in MRI, which uncovers the white matter hyperintensity of the brain; (3) GRAY, gray MRI images revealing the gray matter of the brain. In the raw scans, each voxel has a value from 0 to 1, where 1 indicates that the structural integrity of the axon tracts at that location is perfect, while 0 implies either there are no axon tracts or they are shot (not working). The raw scans are preprocessed (including normalization, denoising and alignment) and then restructured to 3D tensors with a size of 134  X  102  X  134. Fig.6 demonstrate a sample image for each of thee three types of scans. Another information we have for this data set is associated with each sample we have a label, which could be either normal , Mild Cognitive Impairment (MCI) or demented .

Because this is a three-class problem and logistic regres-diction tasks with one-versus-rest strategy, i.e., normal vs. MCI and demented , MCI vs. normal and demented , de-mented vs. normal and MCI . For MulSLR , we set the ` term regularization parameters  X  1 =  X  2 =  X  3 and tune it from the grid { 10  X  3 , 10  X  2 , 10  X  1 , 1 , 10 , 10 2 , 10 cross validation. The ` 2 term regularization parameters are set to  X  1 =  X  2 =  X  3 = 10  X  4 . For comparison purpose, we also implemented the following baseline algorithms: We use LIBLINEAR [7] for the implementation of LR and SLR, and LIBSVM [5] for the implementation of SVM. Note that in order to test those vector based approaches, we need to stretch those fMRI tensors into very long vectors (with dimensionality 1,831,512).Table 1 summarized the average and standard deviation over 5-fold cross validation in terms of Areas Under the receiver operating characteristics Curve (AUC) values. The data we used are the FLAIR images. From the table we can observe that:
Congestive heart failure (CHF), which refers to a condi-tion where the heart cannot pump enough blood to meet the body X  X  needs, is a major chronic illness in the U.S., af-fecting more than five million patients. It is estimated CHF costs the nation an estimated $32 billion each year 2 . Effec-tive prediction of the onset risk of potential CHF patients would help identify the patient at risk in time. Thus the de-cision makers can provide the proper treatment to the right patients. this can also help save unnecessary costs.
Electronic Health Records (EHR) are systematic collec-tion of patient health information including diagnosis, med-ication, lab, procedure, demographics, etc. It has now been becoming one of the major information source for conduct-http://www.cdc.gov/dhdsp/data_statistics/fact_ sheets/docs/fs_heart_failure.pdf our experiments is from a real world EMR data warehouse including the longitudinal EHR of 319,650 patients over 4 years. On this data set, we identified 1,000 CHF case pa-tients, i. e., the patients who are confirmed with CHF with match according to patient demographics, comorbidities and primary care physicians similar as in [29] identifying 2,000 control patients. We use the medication orders of those pa-tients within two years from their operational criteria date their CHF confirmation date. For control patients that date is just the date of their last records in the database). On each medication order we use the corresponding pharmacy class and the primary diagnosis in terms of Hierarchical Condition Category (HCC) codes [20] for the medication prescription. In total there are 92 unique pharmacy classes and 195 dis-tinct HCC codes. Therefore each patient can be represented as a 92  X  195 matrix, where the ( i,j )-th entry indicates the frequency that the i -th drug was prescribed during the two years with the j -th diagnosis code as primary diagnosis.
The parameters for MulSLR are set in the same manner as the experiments in last subsection. For comparison pur-pose, we also implemented NN, SVM, LR, SLR, MLR and reported the averaged AUC value over 5-fold cross valida-tion along with their standard deviations on Fig.7. From the figure we can clearly observe that multilinear methods still work better than vector based methods (in order to imple-ment vector based methods, we still need to stretch the pa-tient matrices into vectors). MulSLR performs better than MLR, which is the regular ridged bilinear logistic regres-sion, because the patient matrices formed are typically very sparse. However, in this case the vector based SLR works structures of the patient matrices get lost when stretching them into vectors.

One interesting thing to check is the product of w which is the classification vector on the medication side, and w diag , which is the classification vector on the diagnosis side. Just like what we examined on the toy data. In this way we Figure 5: Averaged running time per iteration of MulSLR . (a) shows the running time per iteration vs. data set size plot, where the data dimension-ality is fixed to 100. (b) shows the running time per iteration vs. one-side data dimensionality plot, where the data set size is fixed to 500. can find some strongly correlated medications and diagno-sis that could be highly predictive for CHF onset risk. To achieve this goal, we use all the data to train the w med w diag and plot their outer-product as in Fig.8 (where we just show a submatrix due to space limitation), where warm color indicate high values and cold color indicates small val-ues. From the image we can observe that: those references we cited. We also provide the detailed de-scription of all HCC codes at https://www.dropbox.com/s/ 6e1qbjf1ce7yi6x/hcc_codes.pdf .
 Figure 7: Prediction performance for different methods on the CHF onset prediction task in terms of averaged AUC value with 5-fold cross validation along with their standard deviations.
We propose a multilinear sparse logistic regression method called MulSLR in this paper, which can directly take data matrices or tensors as inputs and do prediction on that. Mul-SLR is formulated as an optimization problem and we pro-pose an effective BCD strategy to solve it. We proved the convergence and analyzed the convergence rate theoretically. Finally we validate the effectiveness and efficiency of Mul-SLR on both synthetic and real world data sets. We demon-strate that MulSLR can not only achieve good performance, but also discover interesting predictive patterns. Let 6 1 n  X  h 1 + exp y i f ( W 6 1 n 6  X  This completes the proof.
 For notational convenience, we define Figure 8: A sub matrix from the matrix of w med w . Warm color indicates large value, and cold color For similar reason, the column of diagnosis COPD (HCC108) is warm. With Lemma 2.3 in [2], we have Then Therefore
As ` ( W ,b ) is lower bounded, when T  X  X  X  , we have pletes the proof.
