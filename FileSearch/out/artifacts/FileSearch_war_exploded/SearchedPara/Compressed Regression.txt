 is, increasingly, a concern whenever large amounts of confid ential data are manipulated within an organization. It is often important to allow researchers to analyze data without compromising the show that sparse regression for high dimensional data can be carried out directly on a compressed form of the data, in a manner that can be shown to guard privacy in an information theoretic sense. The approach we develop here compresses the data by a random l inear or affine transformation, reducing the number of data records exponentially, while pr eserving the number of original input variables. These compressed data can then be made available for statistical analyses; we focus on the problem of sparse linear regression for high dimensiona l data. Informally, our theory ensures that the relevant predictors can be learned from the compres sed data as well as they could be from the original uncompressed data. Moreover, the actual predi ctions based on new examples are as accurate as they would be had the original data been made avai lable. However, the original data are not recoverable from the compressed data, and the compre ssed data effectively reveal no more information than would be revealed by a completely new sampl e. At the same time, the inference algorithms run faster and require fewer resources than the m uch larger uncompressed data would by a random linear transformation X 7 X  e X  X  8 X , where 8 is a random m  X  n matrix with m n . It is also natural to consider a random affine transformatio n X 7 X  e X  X  8 X + 1 , where 1 is a random m  X  p matrix. Such transformations have been called  X  X atrix mask ing X  in the privacy literature [6]. The entries of 8 and 1 are taken to be independent Gaussian random variables, but other distributions are possible. We think of e X as  X  X ublic, X  while 8 and 1 are private and only needed at the time of compression. However, even with 1 = 0 and 8 known, recovering X from e X requires solving a highly under-determined linear system a nd comes with information theoretic privacy guarantees, as we demonstrate.
 In standard regression, a response variable Y = X  X  +  X  R n is associated with the input variables, where i are independent, mean zero additive noise variables. In com pressed regression, we assume e Y  X  8 Y = 8 X  X  + 8 = e X  X  + e . Note that under compression, e i , i  X  { 1 ,..., m } , in the transformed noise e = 8 are no longer independent. In the sparse setting, the parame ter  X   X  R p is sparse, with a relatively small number s = k  X  k 0 of nonzero coefficients in  X  . The method we We omit details and technical assumptions in the following t heorems for clarity. Our first result variables is identified asymptotically.
 Sparsistence (Theorem 3.3): If the number of compressed examples m satisfies C 1 s 2 log nps  X  m  X   X  C 2 n / log n , and the regularization parameter  X  m satisfies  X  m  X  0 and m  X  2 m / log p  X  P supp ( e  X  m ) = supp ( X )  X  1 as m  X  X  X  , where supp ( X ) ={ j : j 6= 0 } .
 Our second result shows that the lasso is persistent under compression. Roughly speaking, per-sistence [10] means that the procedure predicts well, as mea sured by the predictive risk R ( X ) = E Y  X   X  T X Persistence (Theorem 4.1): Given a sequence of sets of estimators B n arg min uncompressed data with respect to B n n  X  X  X  , in case L n , m = o ( m / log ( np ) ) 1 / 4 .
 Our third result analyzes the privacy properties of compres sed regression. We evaluate privacy in information theoretic terms by bounding the average mutual information I ( e X ; X )/ np per matrix entry in the original data matrix X , which can be viewed as a communication rate. Bounding this mutual information is intimately connected with the proble m of computing the channel capacity of certain multiple-antenna wireless communication systems [13].
 Information Resistence (Propositions 5.1 and 5.2): The rate at which information about X is revealed by the compressed data e X satisfies r n supremum is over distributions on the original data X .
 As summarized by these results, compressed regression is a p ractical procedure for sparse learning in high dimensional data that has provably good properties. Connections with related literature are Section 3 X 5. Simulations for sparsistence and persistence of the compressed lasso are presented in Section 6. The proofs are included in the full version of the p aper, available at http://arxiv. org/abs/0706.0534 . sensing, and privacy, to place our work in context.
 Sparse Regression. An estimator that has received much attention in the recent l iterature is the assumptions. Sparsistency proofs for high dimensional pro blems have appeared recently in [20] and [19]. The results and method of analysis of Wainwright [1 9], where X comes from a Gaussian Gaussian Ensemble result, and compare our results to it in Se ctions 3, 6.Given that under com-their result in Section 4.
 Compressed Sensing. Compressed regression has close connections to, and draws m otivation from sensing. While compressed sensing of X allows a sparse X to be reconstructed from a small number of view of privacy, approximately reconstructing X , which compressed sensing shows is possible [5, 11]. They focus on certain hypothesis testing problems u nder sparse random measurements, and where y  X  R m , x  X  R n and 8 is a known random measurement matrix. The problem is to selec t between the hypotheses e H i : y = 8( s i + ) . The proofs use concentration properties of random projection, which underlie the celebrated Johnson-Linden strauss lemma. The compressed regression problem we introduce can be considered as a more challenging statistical inference task, where the problem is to select from an exponentially large set of linea r models, each with a certain set of relevant variables with unknown parameters, or to predict a s well as the best linear model in some class.
 Lindenstrauss lemma, and argue heuristically that data min ing procedures that exploit correlations guarantees, although an information-theoretic quantifica tion of privacy was proposed in [1]. We all distributions on X , and identify this with the problem of bounding the Shannon c apacity of a area of cryptographic approaches to privacy from the theore tical computer science community, for instance [9, 7]; however, this line of work is quite differen t from our approach. In the standard setting, X is a n  X  p matrix, Y = X  X  + is a vector of noisy observations under the regularization parameter  X  =  X ( Y , L ) , the solutions of these two problems coincide. In compressed regression we project each column X j  X  R n of X to a subspace of m dimensions, using an m  X  n random projection matrix 8 . Let e X = 8 X be the compressed design matrix, and let e Y = 8 Y be the compressed response. Thus, the transformed noise e is no longer i.i.d.. The being the set of optimal solutions: ( a ) ( e P 2 ) min conditions for the stronger property of sign consistency:  X  exists with E sgn ( b  X  n ) = sgn ( X   X  ) : =  X  b  X   X   X  n such that sgn ( b  X ) = sgn ( X   X  ) . Clearly, if a set of estimators is sign consistent then it is s parsistent.
 All recent work establishing results on sparsity recovery a ssumes some form of incoherence condi-matrix X . We will impose the following incoherence condition; relat ed conditions are used by [18] in a deterministic setting. Let k A k Definition 3.2. ( S -Incoherence) Let X be an n  X  p matrix and let S  X  { 1 ,..., p } be nonempty. We say that X is S-incoherent in case Theorem 3.3. Suppose that, before compression, Y = X  X   X  + , where each column of X is supp ( X   X  ) , and define s =| S | and  X  m = min i where e Y = 8 Y , e X = 8 X , and e = 8 , where 8 i j  X  N ( 0 , 1 Then the compressed lasso is sparsistent: P supp ( e  X  m ) = supp ( X )  X  1 as m  X  X  X  . predicts well. We review the arguments in [10] first; we then a dapt it to the compressed case. Uncompressed Persistence. Consider a new pair ( X , Y ) and suppose we want to predict Y from X . the following way: define Q = ( Y , X 1 ,..., X p ) and  X  = (  X  1 , X  1 ,..., X  p ) T , then vectors and the training error is arg min constants M and s , where Z = Q j Q k  X  E ( Q j Q k ) , where Q j , Q k denote elements of Q . Following arguments in [10], it can be shown that under Assum ption 1 and given a sequence of sets lasso estimators b  X  n = arg min Compressed Persistence. For the compressed case, again we want to predict ( X , Y ) , but now the estimator b  X  n (  X  1 , X  1 ,..., X  p ) T as before and we replace b R n with Given compressed sample size m n , let B n We define the compressed oracle predictor  X  lasso estimator b  X  n Theorem 4.1. Under Assumption 1 , we further assume that there exists a constant M 1 &gt; 0 such that E ( Q 2 j ) &lt; M 1 ,  X  j , where Q j denotes the j t h element of Q . For any sequence B n log 2 ( np n )  X  m n  X  n , where B n o is persistent: R ( b  X  n b oracle risks on such a subsequence for a fixed n . Next we derive bounds on the rate at which the compressed data e X reveal information about the uncompressed data X . Our general approach is to consider the mapping X 7 X  8 X + 1 as a noisy communication channel, where the channel is characterized by multiplicative noise 8 and additive noise 1 . Since the number of symbols in X is np we normalize by this effective block length to define the information rate r n the capacity of this channel. A privacy guarantee is given in terms of bounds on the rate r n could be obtained from an independent sample.
 The underlying channel is equivalent to the multiple antenn a model for wireless communication [13], where there are n transmitter and m receiver antennas in a Raleigh flat-fading environment. The propagation coefficients between pairs of transmitter a nd receiver antennas are modeled by the signals, the problem studied in [13]. Formally, the channel is modeled as Z = 8 X +  X 1 , where  X  &gt; 0, 1 i j  X  N ( 0 , 1 ) , 8 i j  X  N ( 0 , 1 / n ) and 1 n constraint.
 Theorem 5.1. Suppose that E [ X 2 j ]  X  P and the compressed data are formed by Z = 8 X +  X 1 , This result is implicitly contained in [13]. When 1 = 0, or equivalently  X  = 0, which is the r m  X   X  . We thus derive a separate bound for this case; however, the r esulting asymptotic order of the information rate is the same.
 Theorem 5.2. Suppose that E [ X 2 j ]  X  P and the compressed data are formed by Z = 8 X , where 8 is m  X  n with independent entries 8 i j  X  N ( 0 , 1 / n ) . Then the information rate r n r Under our sparsistency lower bound on m , the above upper bounds are r n note that these bounds may not be the best possible since they are obtained assuming knowledge of the compression matrix 8 , when in fact the privacy protocol requires that 8 and 1 are not public. Figures 1 X 2 here; additional plots are included in the full v ersion.
 Sparsistency. Here we run simulations to compare the compressed lasso with the uncompressed matrices for both X and 8 , and reproduce the experimental conditions of [19]. A desig n parameter is the compression factor f = n m , which indicates how much the original data are compressed. The results show that when the compression factor f is large enough, the thresholding behaviors as specified in (8) and (9) for the uncompressed lasso carry ov er to the compressed lasso, when requirement that we have in Theorem 3.3 in case X is deterministic. In more detail, we consider the Gaussian ensemble for the projection matrix 8 , where 8 i is  X  N ( 0 , X  2 ) , where  X  2 = 1. We consider Gaussian ensembles for the design matrix X with both diagonal and Toeplitz covariance. In the Toeplitz case, the covariance is given by T ( X ) i we use  X  = 0 . 1. [19] shows that when X comes from a Gaussian ensemble under these conditions, there exist fixed constants  X  then the lasso identifies true variables with probability ap proaching one. Conversely, if ing simulations, we carry out the lasso using procedure lars ( Y , X ) that implements the LARS lars ( Y , X ) such that Y = X  X   X  + , and for the compressed case we run lars (8 Y ,8 X ) such show that the behavior under compression is close to the unco mpressed case.
 Persistence. Here we solve the following ` 1 -constrained optimization problem e  X  = arg min over 200 trials; for each trial, we randomly draw X n lines mark  X  = 1. For 6 = I ,  X  u =  X  for the uncompressed lasso in (8) and in (9). Figure 2: Risk versus compressed dimension. We fix n = 9000 and p = 128, and set s ( p ) = 3 and L trials, and each vertical bar shows one standard deviation. For each trial, we randomly draw X n with i.i.d. row vectors x i  X  N ( 0 , T ( 0 . 1 )) , and Y = X  X   X  + . pressed sample size m , we take the ball B n The compressed lasso estimator b  X  n rem 4.1. The simulations confirm this behavior. This work was supported in part by NSF grant CCF-0625879.

