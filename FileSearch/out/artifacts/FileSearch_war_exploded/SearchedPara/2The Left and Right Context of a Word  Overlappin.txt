 Most ideograph-based Asian languages consist of thousands of characters, making it impractical to create keyboards along the same style as alphabetic languages. In response, most modern systems come with built-in tools called input methods (IMs) for transforming multiple keystrokes into single ideographs. IMs are often categorized into  X  X adical-based X  or  X  X honetic-based X  methods. With radical-based IMs, users con-struct characters by typing the composing radicals or strokes. Alternatively, phonetic-based IMs rely on phonetic transcriptions of ideographs, where users create characters by typing in the approximate spellings of their syllables. In the case of homographs or homophones, users are given a choice, and the proper character is selected and entered. While various types of IM can be used with a keyboard, this work specifically examines the context of predictive Chinese phonetic input method (CPIM). CPIM not only facil-itates word prediction and word or phrase completion, but also disambiguates homo-phones of syllables into characters. To date, most natural language processing (NLP) research on Chinese IMs has focused on these predictive phonetic-based approaches, since Pinyin input is one of the most popular methods for Chinese typing, and homo-phone disambiguation, which can be regarded as a simplified version of speech recog-nition, is a major problem in Pinyin input. There are usually two steps in a CPIM: 1. Syllable Word Segmentation (SWS) -segment the syllable sequence into syllable words, and 2. character word selection -select the most likely character words for each syllable word. In this article we focus on the SWS problem in Step 1. This arti-cle attempts to balance the tradeoff between the cost of computing resource and the performance of homophone disambiguation based on an algorithmic study on short syl-lable sequence segmentation. With minimal context it is often difficult for a system to determine the most appropriate boundaries. Markovian-based Pinyin input methods usually apply n -grams and dynamic programming to resolve ambiguities from both syllable and word sides [Chen and Lee 2000; Gao et al. 2002; Li et al. 2009; Wang et al. 2006; Xiao et al. 2007]. We shall demonstrate that it is inefficient for mobile devices with limited resources [Liu and Wang 2002; Wu and Zheng 2003], and there are better alternatives.

One may argue that since a character-based model is the-state-of-the-art of Chi-nese word segmentation, the most common way of CPIM is not to process syllables as words but to process them as characters. However, most previous works of Pinyin-to-character conversion prefer a word-based model. For example, Chen and Lee [2000] and Gao et al. [2002] applied a word-based tri-gram language model (with maximum matching word segmentation initially) for all possible word strings that match typed Pinyin to select the word string with the highest language model probability, because Yang et al. [1998] suggested that bypassing the issue of word boundaries did not yield good Pinyin-to-character conversion results. Since Gao et al. [2006] further elaborated that they assumed a unique mapping from word string to Pinyin string to make the decision problem depend solely upon probability of words, we may see their works as SWS. Similarly, Liu and Wang [2002] used a unigram model with fewest words seg-mentation to implement their CPIM. Moreover, Wen et al. [2008] conducted an SWS specific work and showed that better SWS yield better CPIM. As a matter of fact, even sequential labeling models that were usually applied in fashion of characters, such as linear-chain Conditional Random Fields or Maximum Entropy Markov Models, were modified to use word-based features [Li et al. 2009; Xiao et al. 2007] to be tractable for CPIM in practice, which can be seen as a joint schema of SWS and character word selection that essentially does SWS in situ. Most studies on Chinese phonetic input method (CPIM) assume that the input is a complete sentence, which would provide sufficient context for language models to op-timize their conversion performance. However, a phenomenon of short syllable input without delimiters is prevalent on mobile phones. Different keyboard layouts and/or computing power lead to different approaches. Some older systems such as Dasher [Ward et al. 2000] can just suggest character by character. Recent platforms may support so-called phrasal text entries that consider different depths of context [Liu and Wang 2007]. T9-alike methods utilize a context of only one word in European languages or of one character in Asian languages [Mackenzie and Soukoreff 2002]. Figure 1 gives examples of common T9 Pinyin usage. In Figure 1(a), typing a Pinyin syllable  X  X hi X  gets a candidate list of frequent characters. Choosing the first candidate  X  (this/that) brings a candidate list of succeeding characters as Figure 1(b) demon-strates. With a software QWERTY keyboard, larger memory, and faster CPU, some operating systems manage to extend context to multi-character words. Figure 2 lists some typical cases. For a disyllabic input without explicit boundary marks, a system attempts to recognize word boundaries as in Figure 2(a). When the third syllable is pushed in, this system either keeps the candidate list unchanged as Figure 2(b) shows, or generates another candidate list as Figure 2(c) does. These cases indicate that, for the Pinyin syllables  X  X hi-shi-wei, X  there is a bi-syllabic word hypothesis  X  X hi-shi X  on the left. For the Pinyin syllables  X  X hi-shi-gu X ; however, the preferred word boundary on the left becomes monosyllabic as  X  X hi. X  Another interesting case here is Figure 2(d) with the Pinyin syllable  X  X ang-shi-gu, X  where the candidate list comes from  X  X ang-shi X  on the left. Similar boundary ambiguities of word hypotheses sur-rounding the syllable  X  X hi X  can be found on phrasal Pinyin input methods with longer context. Figure 3(a) presents the same behavior of Figure 2(a) and Figure 2(b), whereas Figure 3(b) matches Figure 2(c).

Also, for desktop input methods, Pinyin users in China prefer to input short chunks that comprise relatively fewer words than complete sentences have in order to obtain conversion result as soon as possible. This preference reflects on G (first chosen word accuracy), which is emphasized by major Pinyin input method manufacturers since 2006 1,2,3,4,5 and becomes one of China media X  X  favorite eval-uation metrics of Pinyin input methods 6,7 . Table I gives examples extracted from a recent product comparison chart of popular Pinyin input methods in China Overlapping ambiguity on Chinese word segmentation (CWS) has been widely studied [Li et al. 2001, 2003; Liang 1987; Qiao et al. 2008; Sun et al. 1998]. It is reported that more than 90% of overlapping ambiguity of words can be resolved in a context-free way [Li et al. 2001; Qiao et al. 2008; Sun et al. 1998]. According to Li et al. [2003], 47.98% of overlapping words have the same results suggested by forward maximum matching and backward maximum matching. On syllable string segmentation, how-ever, similar phenomenon does not occur. As Zheng [1999] mentioned, there are just about 400 toneless monosyllables and about 1,300 tonal monosyllables, but they repre-sent pronunciations for at least 6,700 Chinese characters. On the average, 17 Chinese characters share one toneless syllable and five Chinese characters share one tonal syl-lable. This situation implies that syllable string segmentation involves more ambiguity than word segmentation, and the boundary determination is even harder. For example, consider a Chinese character string  X  #  X  (knowledge is...), which is easy to segment into two words, namely  X  # (knowledge) and  X  (is). By contrast,  X  X hi-shi-wei X  as the toneless Pinyin syllable string of  X  #  X  , is not that easy to find an unique choice for SWS such as  X  X hi-shi/wei X , because the same syllable string can represent another Chi-nese character string  X  9  X  (someone X  X  guard), which is segmented into and 9  X  (guard) that suggest corresponding SWS as  X  X hi/shi-wei X . 1.3 illustrates the increased ambiguities from a character string  X  )  X   X  to its tonal syllable string  X  X e4-guo2-you3-qi4-ye4 X  and toneless syllable string  X  X e-guo-you-qi-ye X . The first row of the character string in 1.3 can be segmented into either /
 X  (enterprise) or  X  ) (each country) /  X  (has) /  X  (enterprise), where the direc-tion of arrows indicate that ) .(country) is overlapped. The second, third, and fourth rows that are grouped in a cyan plate represent that the character string X  X  tonal sylla-ble string 4  X  X e4-guo2-you3-qi4-ye4 X  introduces an additional ambiguity because of the homophone  X   X  (being angry) of  X  X ou3-qi4 X , while the following plate draws a lattice of the toneless syllable string  X  X e-guo-you-qi-ye X  involving more homophones, such as  X   X  (especially),  X   X  (paint), and  X   X  (oil and gas),  X   X 
Since the study is about short syllable strings, we focus on syllable strings con-sisting of three to six syllables overlapping on their syllable words. For ambiguity on syllable words, formal definitions of ambiguities in word segmentation [Liang 1987] are adopted as the following.  X  A syllable string  X  X YZ X  is an overlap ambiguity string (OAS) if it can be segmented into two syllable words either as  X  X Y/Z X  or  X  X /YZ X , depending on context.  X  A syllable string  X  X Y X  is a combination ambiguity string (CAS) if X, Y, and XY are syllable words.  X  X yllable word X  means a syllable substring that has one or more corresponding Chinese words according to certain word segmentation standard. For example, as mentioned above,  X  X hi-shi-wei X  is an OAS involving four toneless syllable words  X  X hi-shi X ,  X  X hi-wei X ,  X  X hi X , and  X  X ei X  for  X  # , 9  X  ,  X  , and  X  , respectively.

Speaking of ambiguity string, two additional definitions involved. One is called longest OAS (LOAS). The LOAS is an OAS that is not a substring of any other OAS in a given chunk. For example, both  X  [  X  (anytime) and  X  [ (anytime) are OASs, but only  X  [  X  is a LOAS. The LOAS was introduced for word segmentation study in sentence level [Sun et al. 1998], which is not feasible in this study of short syllable strings.

Another additional definition of ambiguity string is about pseudo ambiguity (PA) vs. true ambiguity (TA) [Sun et al. 1998]. The PA indicates that, despite the multiple segmentation possibilities (according to certain dictionary), there is only one way to segment the given string in reality (of certain corpus). For example, a given string = u  X  (city government) can be segmented into either = (city) / or = u (city policy) /  X  (the seat of government) since = u , registered in the given dictionary, but the latter segmentation is not found in the given corpus. On the contrary, the TA means that the string can be segmented in more than one way in practice. For example, both , B (from one X  X  childhood) / (from) / B X  (elementary school) are usually easily recognized from a given corpus. In this study, the appearance of a true overlap ambiguity string (TOAS) is one of the criteria for choosing corpus, but corpora have no TOAS are still useful as open test data, since PA strings of a small corpus may actually be unseen TA strings. We first describe the following important observation on the context of words: when two syllable words overlap (or compete for a boundary) in a short syllable string, their relative positions (left or right) play a crucial role in determining which one should be selected. For example,  X  X un-shi X  as / (military) is a high frequency disyllable whose left and right strengths are quite different. It is very strong when competing with polysyllables on its right, as shown by the tri-syllable  X  X un-shi-jie, X  which is usu-ally segmented into  X  X un-shi/jie X , or / (military) /  X  (area). However, it becomes relatively weak when competing with other polysyllables on its left, as shown by the tri-syllable  X  X u-jun-shi, X  which is usually segmented into  X  X u-jun/shi, X  or 4 (is). Table II provides more examples.

Examples mentioned above clearly demonstrate that a polysyllable X  X  frequency is not necessarily representative of its strength in segmentation, and some of them can lead to complicated relationships, such as the Chinese character strings the row of  X  X i-hui X , namely  X  (get) /  X  X  (chance) and  X   X  (free radicals) / another overlapping syllables of  X  X ou-ji X , while the last three rows of  X  X u-qiu X ,  X  X iu-yi X ,  X  X i-shi X  may form a chain as  X  X u-qui-yi-shi X . Since Table II lists Chinese character words based on tonal Pinyin syllables, one may imagine that the situation of toneless Pinyin syllables can be even more complicated. Therefore, we propose a simple division of a word context into its left context and right context. Furthermore, we design a double ranking strategy for each word to reduce the number of errors in syllable word segmentation of Step 1: for each polysyllable w , we assign an integer as its left rank and another as its right rank. When a polysyllable u overlaps with another polysyllable v to its right, we compare the right rank of u to the left rank of v in order to determine whether the segmentation should follow that of u or v based on the following rules: If the right rank of u is bigger than the left rank of v , then u is selected. Conversely, if the right rank of u is smaller than the left rank of v , then v is selected; and if the right rank of u is equal to the left rank of v , the one with the higher frequency is selected. The left and right ranks of a polysyllable can be considered as the relative strength of the polysyllable in each direction. In some cases, the left and right ranks can differ substantially.

The remainder of this article is organized as follows. Section 2 provides the prelimi-naries of problem formulation. In Section 3, we describe the proposed algorithms. The experiment results are detailed in Section 4. We then summarize our conclusions in Section 5. In DRAP, we assign left and right ranks to each syllable to help us perform syllable word segmentation in Chinese. For the initial assignment, we consider only pairs of syllables overlapping in a single phoneme. Ranks obtained this way will be applied to segment syllables in general short texts. We have conducted closed tests of toneless Pinyin with ranks 10, 20, 40, and 80. We then decide to limit the total number of ranks to 20 for further experiments (on open tests on toneless Pinyin and closed tests on tonal Pinyin), since the 20-rank version can be implemented quite efficiently and does not seem to affect the performance. We now formally define the DRAP for Chinese short syllable word segmentation. For each syllable t , consider the following competition graph G each vertex in L . V / denotes a polysyllable that ends in the syllable t ; each vertex in R .

V / denotes a polysyllable that begins with t ; and each arc e a vertex r in L . V /  X  R . V / to a vertex s with weight w times syllable word r is selected over s in a desirable segmentation. Assign a left rank left rank ( v / to each vertex v in R . V / and a right rank right rank ( u L .

V / . A syllable X  X  left and right ranks are both positive integers no larger than a pre-defined limit rank limit . After rank assignment, we determine the relation between any two connected vertices u and v , where u is in L . V / I. If right rank ( u ) &gt; left rank ( v / , then u is selected.
 II. If right rank ( u ) &lt; left rank ( v / , then v is selected.
 III. If right rank ( u / = left rank ( v / , then the polysyllable with the higher frequency is
Next, let E  X  be the set of arcs e u , v in which u is selected. Our objective is to maximize the total score 6 { w . e u , / | e u ,  X  E  X  } , which represents the number of times this rank assignment correctly chooses segmented syllables. On the other hand, for each arc e Hence, an equivalent objective of DRAP is to minimize the total penalty | e in Figure 5(a) contains four vertices, a , b , c , and d , which are Pinyin polysyllables that begin or end with the Pinyin monosyllable  X  X hi. X  The number on each edge repre-sents its weight. Figure 5(b) shows the optimal solution if we assign right rank ( a 2, right rank ( b / = 4, left rank ( c / = 1, and left rank ( d edges in Figure 5(b) is 38, which is the score of the double rank assignment. A problem closely related to the DRAP is the following Feedback Arc Set Problem (FASP). Given a bipartite graph G = ( V 1 x V 2 , E / with arcs directed between V V , the FASP is to delete a set of arcs E 1 with minimum total weight such that the remaining graph is acyclic. If one ignores the rank-limit and assigns only distinct ranks, then the FASP can be reduced to the DRAP as follows. Given a bipartite graph, regarded as a competition graph with L . V / = V 1 , R . V assigning distinct ranks to vertices in V 1 and V 2 to minimize the total penalty. Let E there is another set E  X  X  with smaller weight whose deletion would also make the graph acyclic, then perform a topological sort on the graph G = obtained would serve as the rank assignment for the DRAP. So E to the DRAP better than E 1 .

FASP is NP-hard [Guo et al. 2007]. Hence, DRAP is also NP-hard. There are several good approximation algorithms for FASP [Even et al. 1998; Gupta 2008]. Generally, they only focus on the un-weighted case. We propose an algorithm in Section 3 for the weighted DRAP with prescribed rank-limit. In this section, we describe our double ranking algorithm (DRA) for DRAP in Chinese syllable word segmentation. As the size of graph G is too large, DRA first reduces the vertex set by pre-assigning ranks to low-frequency syllables. Then, it employs a genetic algorithm to assign ranks to the remaining syllables. A 10-rank DRA is denoted by DRA 10 , a 20-rank one is DRA 20 , and so on. The dataset used in the experiment is described next. The number of vertices (syllables) in a competition graph can sometimes be more than 10,000, which is too large to handle. To reduce the problem size, we set a syllable X  X  left and right ranks as its frequency if the frequency is less than or equal to rank limit /2. This rank pre-assignment step reduces the size of the original problem by approxi-mately 70%. For the remaining high frequency syllables, we use a genetic algorithm to determine their ranks. This pre-assignment can effectively reduce the problem size and still maintain the quality of our solution. Figure 6 explains why we pre-assign ranks for low-frequency syllables. In Figure 6, the number on each vertex represents its frequency, and the number on each edge represents its weight.

Figure 6(a) shows a competition graph with low-frequency vertices. Note that a ver-tex X  X  frequency is equal to the aggregated weight of all its outgoing arcs, and a low-frequency vertex usually has a small number of such arcs. Hence, for a low-frequency vertex, the weight of each of its outgoing arcs and the vertex X  X  frequency are usually similar. In this situation, it is reasonable to assume that a vertex X  X  frequency can rep-resent its strength in syllable word segmentation. For example, in Figure 6(a), if we set the right rank of a and the left rank of b as each vertex X  X  frequency, we obtain an optimal solution in this case (with the number of segmentation errors other hand, Figure 6(b) shows a competition graph that contains high-frequency ver-tices. Since a high frequency vertex usually has many outgoing arcs that could share its frequency, in most cases, the vertex X  X  frequency cannot represent its true strength in syllable word segmentation. For example, in Figure 6(b), if we set each vertex X  X  left and right ranks based on the vertex X  X  frequency we might overestimate the right rank of a and underestimate the left ranks of b , c , d , and e . As a result, the solution would generate a rank assignment with 150 segmentation errors compared with an optimal solution, which only has 100 such errors. These two examples indicate that, in syllable word segmentation, a vertex X  X  strength and its frequency are likely to be similar if its frequency is low. Our rank pre-assignment method uses the lower part of the rank-ing (1 to rank limit /2) for low frequency vertices, and full ranking (1 to rank limit ) for high-frequency vertices whose ranks will be assigned by genetic algorithm later. Therefore, by pre-assigning ranks for low frequency vertices, our method can reduce the problem size; at the same time, by maintaining the full ranking for high frequency vertices, we can maintain the flexibility of solution candidates. In this section we describe the genetic algorithm (GA) for assigning ranks to high frequency syllables. The encoding part of our GA transforms each non-assigned vertex, whose frequency is higher than rank limit /2, into a single bit of a chromosome after rank pre-assignment. Each bit represents the left (resp. right) rank of a vertex in R (resp. L . V // ranges from 1  X  rank limit .

A key to the success of GA is the creation of the initial population. Generating a set of initial populations with quality and diversity is most important for our GA. In our initial population, there are two groups of chromosomes. One group is generated by randomly assigning each vertex X  X  rank from 1  X  rank limit. The aim here is to ensure diversity; whereas the objective of the second group (as described below) is to maintain the quality of the population. Since we use the results of frequency-based method (FBM) as the baseline to evaluate the feasibility of our solution, a set of popu-lations with the quality of FBM would be an appropriate reference for us to generate the initial population. However, FBM assigns each syllable a unique rank (namely, its frequency), which violates the ranking constraint of our problem. Moreover, since we have already pre-assigned ranks to low frequency syllables to represent their fre-quency (1  X  rank limit /2), it is a little challenging to build a set of initial populations with the quality of FBM under the ranking constraint. To resolve this issue, we adopt the following method.

I. Sort the chromosomes according to each vertex X  X  frequency in non-decreasing
II. For each chromosome, randomly select rank limit /2 disjoint intervals in the III. For each chromosome, assign all bits in the first interval with value ( rank limit /2
Since our problem uses syllable frequency as the tie-breaker for two syllables X  re-lations if the syllables have identical ranks, and all syllable ranks in a chromosome are higher than the ranks of pre-assigned syllables, this method can generate a set of chromosomes with similar quality as FBM. Figure 7 shows the steps of the proposed method.

Figure 7(a) shows a chromosome sorted in non-decreasing order of each syllable X  X  frequency, and Figure 7(b) shows a chromosome in which each vertex X  X  rank is also its frequency. Although this chromosome has the same quality as FBM, its ranking could exceed rank limit . Therefore, we need to use another method to generate chromosomes with the quality of FBM. In Figure 7(c), for each chromosome, we randomly generate rank limit /2 disjoint intervals and assign the same rank value to all syllables in an in-terval. As long as the ranks of all the syllables in a chromosome are in non-decreasing order, all the syllables and the pre-assigned syllables will yield a double rank assign-ment result with the quality of FBM. Therefore, we can apply this method to generate as many different chromosomes with the baseline quality as the initial population of our GA.

Crossover has a significant effect on the final result of the GA. Since the topology of our problem is a graph, traditional crossover approaches (one-point crossover, two-point crossover, and uniform crossover) in the GA may not be suitable. Therefore, we adopted a new crossover method called CrossNet, proposed by Stonedahl et al. [2008]. CrossNet provides an effective crossover method for graph-like problems and it helped us generate better double rank assignment results.

The fitness function of our GA calculates the number of syllable word segmentation errors of a chromosome. A chromosome X  X  fitness represents the quality of its double rank assignment and guides the reproduction process of our GA. We use the tourna-ment selection scheme, which randomly picks two chromosomes and selects the one with lower fitness as well as fewer errors in the reproduction process. To compare with our method, we consider the frequency-based method (FBM) and the conditional random fields (CRF) [Lafferty et al. 2001] approach. The latter is a state-of-the-art technique on modern CWS [Zhao et al. 2010]. We conducted two sets of experiments: toneless and tonal. For each syllable, we apply DRA to determine the effect of our double ranking strategy compared with that of FBM and CRF. In this study, we use the Xinhua Agency part of Chinese Gigaword Third Edition (Gigaword in short, hereafter) [Graff 2007] as training set and closed test set, and segment it automatically into approximately 279,500,000 words in simplified Chinese, by Peking University X  X  segmenter (PKU segmenter) [Duan et al. 2003]. We then apply the HowNet dictionary to convert Gigaword into a Pinyin corpus. Two Pinyin polysyl-lables are said to be in conflict if they overlap. Since Gigaword is already segmented, a Pinyin polysyllable is deemed selected if it conforms to the word boundary of the corpus. Thus, for any two overlapping Pinyin polysyllables u and v , we can calculate the number of times u is selected over v and vice versa in word segmentation. Based on the Pinyin corpus, we select a Pinyin syllable t , and build a competition graph G E / . There are 390 types of toneless and 1,198 types of tonal monosyllables converted from Gigaword, and they consequently produce overlapping polysyllables in 116,681 types without tone and 28,533 types with tone, respectively.
 The corpora for independent (open) test are from the Third International CWS Bakeoff of the Special Interest Group of the Association for Computational Linguis-tics (SIGHAN) [Levow 2006]. For a study on Pinyin syllables, four corpora in simpli-fied Chinese are chosen: the training set and test set from Microsoft Research (de-noted by  X  X SR-training X  and  X  X SR-test X , respectively), and the training set and test set from Peking University (denoted by  X  X KU-training X  and  X  X KU-test X , respectively). The Pinyin conversion gets 391, 197, 197, and 199 types of toneless monosyllables in MSR-training, MSR-test, PKU-training, and PKU-test, respectively. For the coverage of monosyllable types, MSR-test, PKU-training, and PKU-test are not large enough. Even worse, TOAS of syllables are not found in these four corpora, which is the main reason for using Gigaword as the training corpus instead of SIGHAN corpora.
Consequently, issues about segmentation standards arise. PKU-training and PKU-test may share a certain level of consistency with Gigaword that was segmented by PKU segmenter, but MSR-training and MSR-test may not. However, although segmen-tation criterions in MSR and PKU corpora are different, these differences are mostly in CAS rather than in OAS, which means that experiments of overlapping syllables will not be much affected. For example, once some standard treats area) as a whole segment instead of two segments as / (military) and corresponding tri-syllable  X  X un-shi-jie X  may disagree with previous outcome of compe-tition via training set for its substring  X  X un-shi X  to  X  : and this situation could be a test for robustness with the problem definition remain-ing intact. For a similar concern, although we are aware of the existence of Tagged Chinese Gigaword Version 2.0 [Huang 2009], it could introduce a more complicated relationship between word segmentation standards, since it is based on heterogeneous CKIP and ICTCLAS tagging systems [Huang 2008]. Nevertheless, according to Wen et al. [2008], errors in SWS caused by CAS mostly will not have any influence on the final CPIM results.

The choice of using large and (semi-)automatic segmented corpora instead of small and manually segmented ones is a common compromise between the ideal and the re-ality of CPIM studies. Most previous works of Pinyin-to-character conversion involved in-house corpora. A series of language model studies involving CPIM is based on large, balanced, yet not publicly available corpora as training set and an independent open test set that is from different sources of the training set [Gao et al. 2002, 2006], similar to this article, to ensure the experiment is pragmatic. In fact, while CPIM studies are usually not evaluated with SIGHAN X  X  or other manually segmented corpora in terms of precision and recall, there is simply no way to make a comparative study of CPIM for the time being. Instead of splitting a relative small and manually segmented corpus into a training set, development (held-out) set, and test set for common parameter tun-ing scheme such as cross-validation or held-out estimation, CPIM researchers tend to investigate performance in a pragmatic way that datasets comprise texts in different domains, styles, and time.
 For each toneless syllable in our corpus, we generate a competition graph and solve DRAP on the graph. For each such syllable, we test FBM, CRF, and DRA and count the number of syllable word segmentation errors as Penalty for each method. Com-mon evaluation metrics of CWS, such as word-based precision, recall, and their har-monic average F 1 -score, do not fit SWS, because overlapping syllables do not have unique choices of segmentations, as mentioned in Section 1.2, to be the gold standard for precision/recall/ F 1 -score calculations.

A series of experiments are conducted by linear-chain CRF since it appears to be very effective on sequential labeling problems including CWS. The character-based tagging scheme [Xue 2003; Zhao et al. 2010] is adopted in CRF as a monosyllable-based one for short syllable word segmentation. Configurations of tag set and feature set are similar to works for SIGHAN Bakeoffs [Low et al. 2005; Peng et al. 2004; Tsai et al. 2005; Tseng et al. 2005; Xue and Shen 2003; Zhang et al. 2006; Zhao et al. 2010]. Table III and Table IV provide the feature templates in the format of CRF++ samples of annotated training data for each configuration, respectively. Specifically, the configuration CRF 6 listed in Table IV is the state-of-the-art of CWS [Zhao et al. 2010], therefore its CRF parameter of Gaussian prior (c = 100 in the usage of CRF is applied. In the interest of brevity and clarity, we do not draw huge tables or charts of preliminary experiments for hyper-parameter (i.e., the Gaussian prior) tuning or for the context window size of feature templates and additional features. The preliminary experiments use five-fold cross-validation to tune parameters or to select features. Like related works, context window sizes larger than three monosyllables do not help much, especially when the tag set applies more monosyllable-position types [Zhao et al. 2010]. Therefore feature templates in Table III exclude tri-syllabic compounds and do not exceed the position -2 or 2. Additional features such as accessor variety substrings [Feng et al. 2005; Zhao and Kit 2011] or word type indicators [Tseng et al. 2005] are not employed since interactions of their combinations can be complicated and may be be-yond the scope of this work, which has no intentions to elaborate upon the feature engi-neering for CRF. Although there X  X  always a certain chance that sophisticated features make CRF invincible, the price those features paid could still be a weakness when this work would like to highlight applications on resource limited devices. Hence CRF ex-periments in this work are not for competitions but for relative benchmarks. It is worth noting that specific word type indicators such as those informing CRF where the over-lapping occurred had been considered, but in our experience, their contributions to character-based segmentation models would be negative sometimes. That X  X  also one of the reasons why this work categorizes CRF experiments by tag set, to make the rela-tive benchmarks purely based on monosyllable for modeling overlapping ambiguities.
The syllable sequence  X  X ai-fen-zhi-wu-shi-wei X  could be segmented as  X  X ai-fen-zhi-wu-shi/wei X  (  X  I  X  (fifty percent) /  X  (is)), or  X  X ai-fen-zhi-wu/shi-wei X  ( (five percent) / @  X  (seen as)). Instead of tagging every occurrence of these conflicting patterns in the whole corpus, only the more frequent one (the former), is annotated to be a training sample as Table IV illustrated. This is a necessary procedure of feature selection, because the training corpus is too large to compute for CRF pragmatically. In Table V, we list the experiment results for the top 10 most frequent toneless Pinyin monosyllables. Table V details the following results: penalty generated by FBM as baseline; penalty generated by DRA 10 , DRA 20 , DRA 40 generated by each CRF configurations (denoted by  X  X RF X  with the number of tag type of corresponding tag set indexed the same way as in Table IV). Boldface indicates the best case of each row in Penalty, and bold-italic style represents the best performance that CRF control group can reach. The results show that FBM and CRF penalties by 106.3% and 43.7% than DRA 20 , respectively.

We test syllable ranks generated from our corpus on open (independent) test corpora to assess the feasibility of DRA. Since Table V shows that performances of DRAs are not sensitive to ranks, DRA 20 is selected for the rest of experiments. For comparison, we also apply FBM using each syllable X  X  frequency in our training corpus on the same test corpora.

Since the test corpora contain the correct segmentation of each unsegmented Pinyin syllable, we could compare DRA with other methodologies such as FBM by segment-ing the unsegmented Pinyin syllable in the corpora. Figure 8 illustrates how we use the generated double ranks in our corpus for syllable word segmentation in the test corpora. In Figure 8(a), we show the correct segmentation of an unsegmented Pinyin mented into a -b and c -d . As the segmentation point is located between b and c , we only consider the cases where two Pinyin polysyllables overlap b or c . Figure 8(b) shows the case where two Pinyin polysyllables, a -b and b -c -d , overlap b . In this case, the DRA compares the right rank of a -b and the left rank of b -c -d to determine which syllable is selected. In contrast, FBM compared the frequency of a -b and the frequency of b -c -d to determine which syllable is selected. After a syllable has been selected, we can assess whether the segmentation is correct. For example, in Figure 8(b), the upper segmenta-tion that selects the syllable a -b and matches the correct segmentation is correct; and the lower segmentation that selects the syllable b -c -d and mismatches the correct seg-mentation is wrong. Therefore, we could count Penalty for DRA and FBM. Figure 8(c) shows another case where two Pinyin polysyllables, a -b -c and c -d , overlap c . Similarly, we count penalties for DRA and FBM. Then, we test models that are trained from Gigaword in the previous experiments by FBM, CRF, and DRA on the four simplified Chinese corpora from SIGHAN X  X  3rd CWS Bakeoff. The results are listed in Table VI.
The experiment results show that, in the four corpora, DRA could reduce the penal-ties of FBM by 206.1%. Since the corpus we used to derive each syllable X  X  double ranks (i.e., Gigaword) is totally independent of the four test corpora, the experiment results clearly demonstrate the robustness of the double ranking strategy.

To see the effects of DRA on tonal syllables, we conduct similar experiments. We still used Gigaword as our test corpus with tonal syllables. The experiment results for the top 10 most frequent tonal Pinyin monosyllables are listed in Table VII.

The results show that DRA is still effective on the tonal Pinyin syllable word seg-mentations according to penalty. Note that in Table VII, the row  X  X e5 X  of  X  (an expletive), every method conduct 0 Penalty. This is because  X  X e5 X  of  X  can only be used as an expletive and is always segmented into an single Pinyin word in Chinese, there is no cycle in the competition graph formed by  X  X e5 X  of  X  . CRF 4 , CRF 3 , and CRF 2 , however, suffer for monosyllable-based modeling that may synthesize syllables into segmentations that are unseen in the training data. This fact implies that lower penalty indicates higher performance of SWS, and subsequently implies better user experiences of CPIM.

One may be curious why DRA does not perform better than CRF on the independent tests shown in Table VI. This phenomenon actually indicates one of the main differ-ences between DRA and CRF, that is, polysyllable (word) based matching method vs. monosyllable (character) based discriminative model, where the former does not solve out-of-vocabulary (OOV) problem directly while the latter usually concatenates un-seen compounds that happen to be unknown words conveniently. DRA, as the proposed method of this work, is not designed to resolve overlapping ambiguities and recognize unknown words simultaneously. To make a clearer picture of overlapping ambiguity resolutions on comparisons between DRA and CRF, Table VIII lists in-vocabulary (IV) penalties on the independent tests, and it turns out that DRA is a lot better.
The OOV rates in terms of overlapping polysyllable pairs according to the train-ing data Gigaword version 3 of MSR-training, MSR-test, PKU-training, and PKU-test are 26.37%, 23.64%, 28.47%, and 29.21%, respectively. For CRF models, advantages and disadvantages both come from monosyllable concatenations that can be either un-known polysyllables luckily or artificial ones unfortunately, so statistics and examples are provided in Table IX.

For instance, the existent Pinyin segmentation  X  X e / fu-ze-ren X  is likely to be  X  segmentation standard is applied, since  X  X e X  for  X  { (of) X  is almost always a bound morpheme. Our training data of Gigaword v3 have been filtered to consist of segmen-tations have overlapping ambiguities only, which means each segmentation comprise exactly two syllable words. However, linear-chain CRF models sometimes tend to syn-thesize consecutive high frequency unigrams and/or bigrams of monosyllables into a single syllable word, no matter that word is an artificial one or not according to the training data.

Besides the OOV issue, we further speculate that the reason DRA outperforms CRF is due to the differentiation of the left and right context. Although linear-chain CRF is able to learn context via expanding the window size of feature templates and increas-ing the variety of prediction label, the weights still come from undirected relationship of context, that is, frequency. For the example mentioned in Section 1.3, a linear-chain CRF may need longer chunks to get better results, while short chunks with a directed graph of left and right context is good enough for DRA. On overlapping short syllable word segmentation as this work defined, CRF has no choice but memorize unigram and bigram mechanically. Table X lists some high-Penalty cases that DRA predicted better than CRF.
 For example, the polysyllable  X  X ou-qi-shi X  can be either  X  X ou-qi / shi X  for  X  cially) / 4 (is) X  or  X  X ou / qi-shi X  for  X   X  (have) /  X  + (inspiration) X  that DRA ranked right-hand side strength of  X  X ou-qi X  higher than left-hand side strength of  X  X i-shi X  while CRF chose  X  X ou / qi-shi X  as the segmentation because  X  X i-shi X  can be a high fre-quency segment for  X   X  " (actually) X  if the left and right contexts were not evaluated.
The fact that DRA performs better than CRF on in-vocabulary SWS of short strings does not imply DRA would perform worse than CRF on longer strings. For example, a recent CPIM evaluation showed that context length and performance do not necessar-ily have positive correlations [Jiang et al. 2011]. Since this study attempts to strike a balance between the cost of computing resource and the benefit of SWS performance, space requirements could be one of the evaluation criterion. However, space requirement may vary from system to system, depending on implementation. Especially for monosyllable-based CRF and polysyllable-based FBM and DRA, the scales of input units are quite different. To make a fair comparison, we calculate the sizes of raw files for each model, without compressions, as the conceptual benchmark of space requirement. For toneless Pinyin, the file size of FBM is about 4.97MB. DRA consumes 5.41MB for identical syllables within FBM and additional small space for double ranks. CRF models as the control groups in the experiment require about 24.2MB  X  35MB, depending on the label combinations of character, tag set and feature template. Zhao et al. [2010] reasons that the space requirement of CRF using L-BFGS algorithm is in the same scale with the time complexity of a single CRF training iteration, which is shown by Cohn et al. [2005] as O of label combinations. For tonal Pinyin, while file sizes of FBM and DRA model both increase slightly to approximately 5.92MB and 6.53MB, respectively. However, the file sizes of CRF model inflate dramatically to 100MB  X  154MB. These facts, as listed in Table XI, indicate that DRA effectively outperforms FBM with similar scale of space complexity while maintains competitive performance to CRF in a much more efficient way.

While CRF-based SWS spends O . n 2 / for Viterbi algorithm, where n stands for the number of monosyllables (characters) of a given input string, DRA and FBM based SWS need only O . n / roughly for a proper greedy algorithm (e.g., forward maximum matching) scanning syllable words from the input string in situations similar to Figure 8. Based on the observation on error cases of CRF and the efficiency on space require-ment of DRA, this work further suggests that DRA can use spare storage to keep track of error cases according to the training data, to memorize high-penalty errors intelli-gently rather than cram up all combinations of monosyllable n -grams. By balancing the trade-off between space and performance, one may decide how many cases are suf-ficient to load in resource limited devices. One of the most intuitive ways to do so is, first, sorting error cases proportionally by penalty-byte rate, and then record preferred segmentations one by one until the limitation of space or the expectation of reduced penalty is reached. Figure 9 and Figure 10 illustrate the trend of accumulated penalty reduction and space requirement, respectively, on the top 10,000 error cases. Accord-ing to these two charts, one may get a highly fitted exemplar set, which can potentially reduce penalty almost to zero in the closed test, by recalling the top 10,000 preferred segmentations as errata that only spend extra 140 Kbytes to store! To provide a more informative analysis, Table XII lists top-10 error cases of DRA penalty-byte rate. Clearly, most errors are caused by high double ranks and relatively high frequencies of competing syllables while strong preferences for one of segmenta-tions on overlapping ambiguities are still there.

Although the cost of intelligent memorization is relatively low, its performance for the open (independent) test on IV still concerns us. Hence Figure 11 shows the utilization of top 10,000 errata, which demonstrates trends similar to the closed test set, while Table XIII lists the percentages of improvements for Penalty statistics in Table VIII. Both of them suggest that intelligent memorization is effective and stable. In this article, we propose a double ranking strategy for overlapping syllable word segmentation in short texts. The experiment results show that the strategy can reduce penalty by 90.2% segmentation of toneless overlapping Pinyin polysyllables using FBM. In addition, the results of experiments on independent corpora and the segmentation of tonal syllables further demonstrate the feasibility and robustness of the double ranking strategy.

As we mentioned in the abstract, there are usually two stages in a CPIM: 1) segment the syllable sequence into syllable words, and 2) select the most likely character words for each syllable word. Being able to do the SWS task in (1) well, the character word selection task in (2) only needs to deal with homophones with the same delimiters, which would make this two-stage approach much better and simpler than the alter-native, namely, intermingle segmentation with word selection. By the way, the task in (2) is very similar to multi-stage part-of-speech (POS) tagging or word sense disam-biguation. In fact, the literature of Chinese POS tagging concluded that although  X  X ll-at-once X  models may work a little better than multi-stage ones, the costs of the former are always much higher than those of the later [Zhang and Sun 2011]. Sometimes, a well-designed multi-stage system can be even more accurate than a joint model system since the joint model usually faces a large and complex search space that makes fine-tuning more difficult or even intractable [Sun 2011]. This is also why we do not process OOV and prefer to have separate stages for unknown word detection and named entity recognition.

We believe a similar strategy could also be adopted to disambiguate conflicting lin-guistic patterns effectively. Linguistic patterns are important features in natural lan-guage processing. In machine learning algorithms, it is customary to train a specific weight for each feature. Given a test sentence, the features X  weights are aggregated to find an optimal combination. However, in some cases, the text could be short and in-complete, and therefore not amenable to full-fledged analysis. As Sproat and Emerson [2003] pointed out, the handling of short strings with minimal context, such as queries submitted to a search engine, has only been studied indirectly. When two patterns in a short text overlap, disambiguation based on one fixed weight for each feature does not necessarily yield the best result, in which case the double ranking strategy could be considered.

There are many other extensions that the double ranking strategy can be considered, which will be the topics for future research.

