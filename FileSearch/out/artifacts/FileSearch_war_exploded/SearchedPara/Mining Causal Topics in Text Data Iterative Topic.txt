 Many applications require analyzing textual topics in conjunction with external time series variables such as stock prices. We de-velop a novel general text mining framework for discovering such causal topics from text. Our framework naturally combines any given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time series data. We iteratively refine topics, increasing the correlation of discovered topics with the time series. Time series data provides feedback at each iteration by imposing prior distribu-tions on parameters. Experimental results show that the proposed framework is effective.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  text analysis ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic processing ; H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Causal Topic Mining, Iterative Topic Mining, Time Series
Probabilistic topic models [4, 8] have proven very useful for min-ing text data in a range of areas including opinion analysis [11, 17], text information retrieval [19], image retrieval [9], natural language processing [6], and social network analysis [12].
 Most existing topic modeling techniques focus on text alone. However, text topics often occur in conjunction with other vari-ables through time. Such data calls for integrated analysis of text and non-text time series data. The causal relationships between the two may be of particular interest. For example, news about compa-nies can affect stock prices, sales numbers, etc. Understanding the impact of, for example, news coverage or customer reviews, is of great practical importance.

While there are many variants of topic models [2, 3, 18], no existing model incorporates jointly text and associated  X  X xternal X  time series variables to identify causal topics. A semantically co-herent topic is "causal" if it has strong, possibly lagged, associa-tions with a non-textual time series variable. This allows for two-way relationships: topics may affect the time series and/or vice versa. Our method can be tailored to the specific application and help an analyst quickly identify a small set of possibly causal topics for further analysis. 1
A basic approach to identifying causal topics is to: (1) find top-ics with topic modeling techniques then (2) identify causal topics using correlations and causality tests. This approach, however, ig-nores the possibly relevant information contained in the time series. Candidate topic sets are the same for every time series.
We instead propose a novel general text mining framework: Iter-ative Topic Modeling with Time Series Feedback (ITMTF). ITMTF naturally combines probabilistic topic modeling with time series causal analysis to uncover topics that are both coherent seman-tically and correlated with time series data. ITMTF can accom-modate any topic modeling technique and any causality measure. This generality enables users to easily adapt different topic mod-els and causality measures as needed. ITMTF iteratively refines a topic model, gradually increasing the correlation of discovered topics with the time series data through a feedback mechanism. In each iteration, the time series data informs a prior distribution of parameters that feeds back into the topic model. Thus, the discov-ered topics are dynamically adapted to fit the patterns of different time series data.

We evaluate ITMTF on a news data set with multiple stock price time series, including stock prices from the Iowa Electronic Mar-kets and those of two large US companies (American Airlines and Apple). The results show that ITMTF can effectively discover causal topics from text data and the iterative process improves the quality of the discovered causal topics.
There are two basic topic models: Probabilistic Latent Seman-tic Analysis (PLSA) [8] and Latent Dirichlet Analysis (LDA) [4]. Both focus on word co-occurrences. Recent advanced techniques analyze the dynamics of topics on a time line [2, 18]. However,
O ur definition allows using  X  X orrelation X  and  X  X ause X  inter-changeably as convenient. they do not conduct integrated analyses of topics and externa l vari-ables; the topic analysis is separate from the external time series.
There are some efforts to incorporate external knowledge in mod-eling. Supervised LDA [3] models topics with better prediction power than simple LDA by incorporating a reference value (e.g. movie review articles with movie ratings) in the modeling process. Labeled LDA [16] associates categorical labels and even text la-bels for topics. Another way of incorporating external knowledge is to use conjugate prior probabilities in the topic modeling process [13]. Topic Sentiment Mixture (TSM) models positive and nega-tive sentiment topics using seed sentiment words such as  X  X ood X  or  X  X ad. X  While these methods show that topic mining can be guided by external variables, none achieves our objective of capturing the correlation structure between text topics and external time series variables. Moreover, while these models are specialized for super-vision with specific external data, our general approach can flexi-bly combine any reasonable topic model with any causal analysis method.

Research on stock prediction using financial news content also relates to our work [14]. Such research typically identifies the most predictive words and labels news according to its effect on stock prices on a specific day using a supervised regression or a classi-fication problem setup. In contrast, we search for general causal topics with unsupervised methods.

Granger testing [7] is popular for testing causality in economics using lead/lag relationships across multiple time series. Recent ev-idence shows that Granger tests can be used in an opinion mining context: predicting stock price movements with a sentiment curve [5]. However, Granger testing has not been used directly in text mining and topic analysis.

A demo system based on our framework is presented [10] with a very brief description about the system components and sample results. Here, we describe the general problem and framework in detail and evaluate the algorithms rigorously.
Consider time series data x 1 , ..., x n , with time stamps t t , and a collection of time stamped documents from the same pe-of causal topics T 1 , ..., T k with associated time lags L A causal topic T i with time lag L i is a topic that is semantically coherent and has a strong correlation with the time series data with time lag L i . Note that L i can be positive or negative, corresponding to topics that might cause, or be caused by, time series data.
We have two criteria to optimize: topic coherence and topic cor-relation. We want to retain the generality of the topic modeling framework while extending it to allow the time series variable to influence topic formation so we can optimize both criteria over a more flexible topic space.
Potential  X  X ausal X  relationships between times series are identi-fied through contemporaneous and/or lagged correlation measures (e.g., Granger tests). The correlation lag structure suggests direc-tional causality. If current observations in time series A correlate with later observations in B, A is said to  X  X ause X  B.

A simple and very common measure uses Pearson correlations, contemporaneously or with leads and lags. Correlations range from -1 to +1 with the sign indicating the direction of correlation and can be used as  X  X mpact X  measures here. A correlation X  X  significance depends on its value and the number of observations.

Granger tests are more structured measures of causality, measur-ing statistical significance at different time lags using auto regres-sion to identify causal relationships. Let y t and x t be two time series. To see if x t  X  X ranger causes X  y t with maximum p time lag, run the following regression: Then, use F-tests to evaluate the significance of the lagged x terms. The coefficients of lagged x terms estimate the impact of x on y . We average the x term coefficients, P p i =1 b i | p | , as an impact value. Input: time series data X = x 1 , ..., x n with time stamp t t , and a a collection of text documents with time stamps from the same period, D = {( d 1 , t d 1 ), ..., ( d m , t dm )}, topic modeling method M, causality measure C, a parameter tn (how many topics to model) Output: k potentially causal topics (k  X  tn ): ( T 1 , L 1
Topic modeling method M identifies topics. The causality mea-sure C gives significance measures (e.g. p-value) and impact ori-entation. Figure 1 illustrates our iterative algorithm. It works as follows: 1. Apply M to D to generate tn topics T 1 , .., T tn 2. Use C to find topics with significance values sig ( C, X, T ) &gt; 3. For each candidate topic in CT , apply C to find the most 4. Define a prior on the topic model parameters using signifi-5. Apply M to D using the prior obtained in step 4 (this injects 6. Repeat 2-5 until satisfying stopping criteria (e.g. reach topic Figure 1: Overview of iterative topic modeling algorithm
I TMTF considers the non-textual time series data in the text min-ing process to find topics that are more highly correlated to non-textual data than general modeling systems. Moreover, ITMTF iteratively improves modeling results by considering inter actions between the text and time series data at both topic and word levels. After identifying causal topics, ITMTF shifts to word level corre-lations between the text and external time series data. It also im-proves the topic modeling process by splitting positively and neg-atively impacting terms into different topics. Because generating and testing all the word time series is inefficient, ITMTF focuses only on the words with the highest frequencies in the most highly correlated topics discovered in each iteration.

The ideal set of causal topics should have tight relationships with the external time series and high topic quality. Traditional topic modeling algorithms form topics based on word coherences in the text data, while causality tests can filter out non-causal topics. Fo-cusing exclusively on one criterion sacrifices the other. Our itera-tive process is a greedy approximate solution to the two-item max-imization problem. It takes turns optimizing each criterion. The prior formation based on causality attempts to optimize causality while a topic modeling optimizes coherence of topics.
From topic modeling results, we generate a topic curve over time by computing each topic X  X  coverage on each time unit (e.g., one day). Consider a weighted coverage count. Specifically, compute the coverage of a topic in each document, p ( topic j | Document This is simply the parameter  X  ( d ) j estimated in the modeling pro-cess. Estimate the coverage of topic j at t i , tc j i as the sum of  X  over all the documents with t i time stamp. Call the list of tc all the time stamps the topic stream T S j : This creates a topic stream time series that, combined with the non-textual time series data, lends itself to standard time series causality measures C and testing.

Selecting lag values is important. One possibility is to use the most significant lag within a given maximum. For example, if we want to find causal topics within 5 days, we can choose the lag within 5 days with the highest significance. If we want to focus on yesterday X  X  effect, we can choose a fixed lag of 1. The specific choice depends on the specific aims of an application.
Based on topic causality scores, we choose a subset of promis-ing topics with the highest causality scores and further analyze the words within each topic to provide feedback for the next iteration by generating topic priors. Specifically, for each significant topic, we check whether the top words in the topic are also significantly correlated with the external time series. For each word, we gener-ate a word count stream W S w by counting frequencies in the input document collection for each day: where c ( w, d ) is the count of word w in document d . Then we measure correlations and significance between word streams and the external non-textual time series. This identifies words that are significantly correlated and their impact values.

Intuitively, we want to emphasize significant topics and words in our next topic modeling iteration to focus in more promising topic space. To do this, we generate topic priors for significant words in significant topics. A topic prior is a Dirichlet distribution that fa-vors topics assigning high probabilities to the identified significant words in significant topics. We assign prior word probabilities in proportion to the significance value of the words. This prior helps  X  X teer X  the topic modeling process to form/discover topics similar to the prior topics [13]. In the next topic modeling iteration, the dis-covered topics are likely to be close to the prior, which is based on the feedback from the time series variable through causality scores.
In addition to keeping significant topics and words, we also want to improve topic quality. A  X  X ood X  correlated topic has a consistent impact relative to the external time series. We want relatively con-sistent topics, those containing words that have mostly  X  X ositive X  or mostly  X  X egative X  impacts on the external time series. Therefore, if one topic has both positive and negative impact words, we separate the positive and negative impact words into two topics in the prior for the next topic modeling iteration. If one of the word impact groups is much smaller than the other (e.g. the number of positive impact words &lt; 0.1 * the number of negative impact words), we keep only one topic and set the probability of words in the smaller group zero. This  X  X ells X  the topic model not to include such words in the refinement of the topic.
 Table 1: Example of topic and word correlation analysis re-sult (left) and prior generated (right) (Sig: significance, Prob: probability)
Suppose, among N total topics, we identify 2 significantly cor re-lated topics with the external time series (left of Table 1). We check correlations of the top words in these two topics. Suppose 4 and 10 words were significant for the two topics respectively. Because there are both positive and negative word groups with similar sizes, we would split the first topic into two topics and assign word prob-abilities based on significance values. For the second topic, only one word has a different impact orientation from the others making the negative group much smaller than the positive group. There-fore, instead of making a separate negative word group topic, we exclude it from the positive word group topic by assigning it zero weight. Right side of Table 1 shows the example prior generated.
Another challenge is selecting a cutoff for  X  X op X  words in the correlation list. The simplest solution is to set a fixed cutoff, say k , and use the top k words. However, rank alone does not determine the importance of words. Importance is determined by word prob-abilities as well. For example, suppose the top three words in Topic 1, A , B , and C have probabilities 0.3, 0.25 and 0.2, respectively, and the top three words in Topic 2, D , E , and F have probabilities 0.002, 0.001 and 0.0001. In this case, while A , B and C are very important in Topic 1, D , E , and F combined only represent a small part of Topic 2. Hence, Topic 2 may require more words to be con-sidered. We address this by using a cumulative probability mass cutoff, probM . We use all words whose accumulated probabilities are within a cutoff.

Formally, for each topic T j = ( w 1 ,  X  ( j ) w 1 ) , ..., ( w is the number of words in the input data set vocabulary), when items are sorted by  X  ( j ) w i , we can add the top ranked word to the top word list T W without violating the constraint P w  X  T W  X  ( j ) where T W = ( w 1 , ..., w m ) . That is, P w  X  T W  X  ( j ) probM . With top word T W = ( w 1 , ..., w m ) and significance value of each word sig ( C, X, w ) , the topic prior  X   X  w puted by the following formula: where  X  is a significance cutoff (e.g. 95%).
U sing the new prior, we remodel topics. New topics will be guided by priors, which depend on correlations with the external data. High probability words in the prior have a greater impact in the modeling results and words with zero probability are not in-cluded in the topic. By repeating the process of topic modeling, correlation analysis, and prior generation, the resulting topics are likely more highly correlated with the external time-series.
The strength of the prior in each iteration is set by a parameter  X  in the modeling process [13]. With  X  = 0 , modeling would not consider the prior information at all (making it the same as independent modeling). With a very high  X  , words in the prior are very likely to appear in the topic modeling results. We study this parameter X  X  influence in our experiments.

While we observe correlations between non-textual series and both word streams and topic streams, we do not compute correla-tions for all word streams. Word level analysis would give us finer grain signals. However, generating all the word frequency time series and testing correlations would be very inefficient. By nar-rowing down to significant topics first, we can prune the number of words to test. This increases efficiency and effectiveness.
We evaluate the proposed algorithms on the New York Times data set 2 with multiple stock time series data.

In one experiment, we examine the 2000 U.S. Presidential elec-tion campaign. The input text data comes from New York Times articles from May through October of 2000. We filter them for key words  X  X ush X  and  X  X ore, X  and use paragraphs mentioning one or both words. The idea is to find specific topics which caused sup-port for Bush or Gore to change and not simply election related topics. As a non-textual time series input, we use prices from the Iowa Electronic Markets (IEM) 3 2000 Presidential Winner-Takes-All Market [1]. In this on-line futures market, prices forecast the probabilities of candidates winning 4 the election. We follow stan-dard practice in the field and use the  X  X ormalized X  price of one candidate as a forecast probability of the election outcome: (Gore price)/(Gore price + Bush price).

In another experiment, we use stock prices of American Airlines and Apple and the same New York Times text data set with a longer time period, but without keyword filtering, to examine the influence of having different time series variables for supervision.
While the framework is general, comparing different topic mod-els is not the focus of our paper. So, we only used one topic model: PLSA implemented based on the Lemur information re-trieval toolkit. 5 For correlation measures, we use both contem-poraneous Pearson correlation coefficients and Granger tests. For Granger tests, we use the R statistical package 6 implementation. Granger tests require stationary time series as inputs. To make the input series stationary, we smooth with a moving average filter with window size 3 (average with adjacent values) and use first differ-ences ( x t )  X  ( x t  X  1 ) of each series. We test causality with up to 5 day lags and pick the lag which shows highest significance.
We report two measure the quality for mined topics: causality confidence and topic purity. For causality confidence, we use the h ttp://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2008T19 http://tippie.uiowa.edu/iem/  X  X inning X  as defined by the IEM is taking the majority of the two-party popular vote http://www.lemurproject.org/ http://www.r-project.org/ significance value (i.e. p-value) of the Granger test between the text stream and the external variable. For topic purity, we use the impact orientation distribution of significant words. If all the sig-nificant words in one topic have the same orientation, it has 100% purity. If significant words are evenly split by positive and negative impact, it has 0% purity. We calculate the entropy of significant word distributions and normalize it to the [0 , 100] range. Thus, the Purity of topic T is defined as:
We report average causality confidence and purity for topics with more than 95% significance. Thus, when there are more significant topics, this measure may be penalized. However, because measur-ing general utility of significant topics is meaningful from a user perspective, we do not adjust this measure.
The first iteration of our method is based on topic modeling with-out guidance from the time series and, thus, is a natural baseline. Comparing iterations and final results to this shows the benefit of iterative topic mining.
We test two parameters for effects on performance. The first is the number of topics modeled ( tn ). A large number of topics may help identify more specific and more coherent (higher purity) top-ics. However, topics that are too specific result in data sparseness that reduces the power of significance testing. A small number of topics gives the opposite effects: topics are likely to have higher statistical significance, but would have lower purity. Also, because many meaningful topics may be merged into a single topic, topics may be too coarse to interpret easily. The second parameter is the strength of the prior (  X  ). A stronger topic prior would guarantee prior information is reflected in the next topic modeling iteration. However, if the initial topic modeling (which uses random initiation without a prior) ends up at a poor local optimum, a strong prior may keep the process there, resulting in poor topics. Strong priors may also exacerbate spurious correlation resulting from noise in the first round. In contrast, weaker priors allow a less restricted iteration of topic modeling, reducing these negative effects. However, positive signals would also have weak impact. Because prior research gives no guidance for selecting these parameters, we examine how they affect the performance of our algorithm. 2000 Presidential Election: Table 2 shows sample results from the 2000 U.S. Presidential election. It shows the top three words of sig-nificant causal topics mined (Pearson correlation, tn =30,  X  =50, 5th iteration). The result reveals several important issues from the cam-paigns, e.g. tax cuts, abortion, gun control and energy. Such topics are also cited in political science literature [15] and Wikipedia important election issues. This shows that our iterative topic min-ing process can converge to issues expected to affect the election. Stock Time Series, AAMRQ vs. AAPL: To study how different time series affect the topics discovered, we compare the topics dis-covered from the same text data set using two different time series. h ttp://en.wikipedia.org/wiki/United_ States_presidential_election,_2000#General_ election_campaign Table 2: Significant topic list of 2000 Presidential Election ( Each line is a topic with top three probability words.) We use New York Times articles from July 2000 through Decem-b er 2001 as the text input. We use American Airlines (AAMRQ) and Apple (APPL) stock prices as external time series. American Airlines X  stock (travel industry) dropped significantly in September 2001 because of the 9/11 terrorist attack, while Apple stock (IT in-dustry) was less affected. We start with the same modeled topic list at the first iteration. Thus, any differences in modeled topics are from feedback of the external time series.
 Table 3: Significant topic list of two different external time se-ries: AAMRQ and AAPL (Each line is a topic. Top three prob-ability words are displayed.)
Table 3 shows the top three words of significant topics mined us -ing the two different external time series after three rounds ( tn =30 and  X  =1000). Topics associated with American airlines include clearly relevant words such as  X  X irlines airport air X  and  X  X nited trade terrorism. X  One topic is clearly about the terrorist attack. Topics associated with Apple differ dramatically. Relevant topics,  X  X omputer technology software X  and  X  X nternet com web X , reference Apple X  X  IT industry.

This example also shows a limitation of our algorithm. In ad-dition to clearly relevant topics, there appear other general topics (e.g., sports). This task presents more challenges than the 2000 U.S. Presidential election example because of the diversity in text data and long time period. While we use text articles which are related to candidates for the Presidential election case, we use all articles in the time period for this example. Greater topic diversity can lead to more spurious correlation independent of real causality. Moreover, our analysis is over 18 months and the algorithm mea-sures correlation over the entire time period. Therefore, if an event is only locally correlated, it may not be selected in the final output topic list. How to measure and deliver local correlation feedback remains for future work.

Despite these difficulties, our algorithm shows how different time series inputs select different topics relevant to themselves using the same text data and initially modeled topics. Thus, our algorithm can effectively guide topic modeling. Pre-filtering relevant articles and shorter time periods may yield better results. Moreover, while some topics seem unrelated at first blush, they may reveal unex-pected, but meaningful, relationships.
We ask two questions: 1) Is our joint mining algorithm more effective than simply combining an existing topic model with a time series analysis method in a straightforward way? 2) Is the feedback mechanism in the proposed framework beneficial? To answer both, we study how results change between the baseline method (with no feedback) and ITMTF through multiple iterations. Figure 2: Causality confidence (left) and purity (right) with dif-ferent parameter settings over iteration (Presidential election data, Granger tests)
Figure 2(a) shows performance evaluation results with differ-ent  X  s using Granger tests. Average causality confidence increases over iterations regardless of the strength of feedback,  X  . The per-formance improvement is particularly notable between the first it-eration (baseline with no feedback) and the second iteration (with one feedback round). Clearly, feedback is beneficial. After the second iteration, performance shows slower improvement. Later rounds appear to fine tune topics resulting from the first round.
The average purity graph shows mixed results. For small  X  values (  X  =10, 50, 100), iterations do not always improve purity. With higher  X  values (  X  =500, 1000), average purity improved from the first iteration to the second. Furthermore, for the highest  X  value (  X  =1000), it showed a steady increase. Weak  X  s would al-low topic modeling more room to explore variations regardless of prior. Therefore, the purity improvement may not be guaranteed in each iteration. Thus, high  X  values lead to higher increases in pu-rity. Further, reported purity is the averaged purity value of all the significant topics found. The number of significant topics increases dramatically between the first and second iteration. Thus, the drop in average purity may not be a negative sign. Still, higher  X  s ensure purity improvement.

Figure 2(b) shows performance evaluation results with different topic numbers ( tn ) using Granger tests. Initially, small topic num-bers ( tn =10, 20) had higher confidence levels than large topic num-bers ( tn =30, 40). Intuitively, the statistical signal is stronger with small topic numbers, while sparseness associated with large topic numbers reduces statistical strength. However, with more itera-tions, the relative order changes. Thus, the feedback process helps overcome sparseness problems. Significantly correlated topics and words are kept by iterations of priors and topic modeling iterations add coherence to them. Thus, in the end, the number of topics has little effect on average confidence.

In general, modeled topics with larger tn show higher purity than with small tn . As expected, each topic is specific, and the chance of combining multiple real topics in one is smaller. Therefore, topics likely have better purity. 8
To show that the improvement is not an artifact of noise in topic modeling, we test significance of the performance improvement be-tween the first and second iteration. We execute 20 separate tri-als with random initiation and applied ITMTF ( tn =30,  X  =1000). Paired t-test between the first and second iterations showed &gt;99% significance for each measure (t-value: 3.87 for average confidence, 14.34 for average purity). Thus, feedback significantly improves average correlation and topic quality over simple topic modeling. Beyond the first feedback round, causality improvements are rela-tively small. Thus, in practice, one round may be sufficient.
Overall high  X  values clearly improve topic quality. Results on tn values are less clear. Next, we describe an approach and experi-ment results for finding the optimal tn .
In practice, selecting the appropriate number of topics ( tn ) presents challenges. We propose a  X  X ariable topic number X  approach. Our algorithm naturally splits topics by word impacts in each iteration. Therefore, we can start with an small tn , let the algorithm split top-ics in each prior and use the increased number of topics in the next iteration. We can also add some buffer topics in some or all rounds to give topic modeling room to explore more topics. For example, 7 out of 10 initial topics may be deemed causal in a round and 5 out of 7 may be split. The next prior will include 12 topics. Adding 5 more buffer topics would result in 17 topics for the next iteration.
Topics tend to have high causality with small tn . Likely, many will be retained as causal from the beginning. With iteration, topics are split. While the number of topics rises, the proportion of causal topic will likely fall. We suggest stopping when the number of causal topics starts to fall relative to the previous iteration, which means topic splitting hurts more than iterative modeling improves causal topic identification. When we actually apply this starting with tn = 10 , the number of significant causal topics starts to decrease after 30 total topics, so we set tn =30.

We test this variable topic number algorithm against fixed topic numbers in our topic number analysis. TNVar in Figure 2(b) shows average confidence and purity compared to the fixed tn methods. In both average confidence and purity, the variable topic number approach performs well, proving its efficacy.
Here, we present a novel general text mining framework: It-erative Topic Modeling with Time Series Feedback (ITMTF) for causal topic mining. ITMTF uses text data and a non-text exter-nal time series as inputs and iteratively models topics related to changes in the external time series. Experimental results show that ITMTF finds topics that are both more pure and more highly cor-related with the external time series than typical topic modeling, especially with a strong feedback loop.

The general problem of mining causal topics opens new direc-tions for future research, both theoretical and applied. ITMTF can be generalized using any topic modeling techniques and causal-ity/correlation measures desired. Our examples illustrate one of many ways to implement the framework, which can certainly be implemented in other ways. In future work, we hope to extend ITMTF X  X  ability to identify locally correlated events. In addition, the proposed alternating optimization strategy between coherence and causality is only a heuristic without theoretical guarantees of convergence. While, empirically, this strategy works well on tested data sets, an obvious and interesting extension would be to inte-grate topic models with time series data more tightly using a single unified objective function for optimization.
W e performed the same series of tests using Pearson correlation coefficients. In general, the results are similar, but not reported here because of space limitations.
 This material is based upon work supported in part by the National Science Foundation under Grant Number CNS-1027965 and by an HP Innovation Research Award.
