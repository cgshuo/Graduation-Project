 Polic y Gradient Reinforcement Learning (PGRL) attempts to nd a polic y that maximizes the av-Since it is possible to handle the parameters controlling the randomness of the polic y, the PGRL, rather than the value-based RL, can nd the appropriate stochastic polic y and has succeeded in sev-learning plateau where the optimization process falls into a stagnant state, as was observ ed for a very simple Mark ov Decision Process (MDP) with only two states [8]. In this paper , we propose a new PGRL algorithm, a gener alized Natur al Actor -Critic (gN AC) algorithm, based on the natural gradient [9].
 Because  X natural gradient X  learning is the steepest gradient method in a Riemannian space and the direction of the natural gradient is dened on that metric, it is an important issue how to design the Riemannian metric. In the frame work of PGRL, the stochastic policies are represented as para-metric probability distrib utions. Thus the Fisher Information Matrices (FIMs) with respect to the polic y parameter induce appropriate Riemannian metrics. Kakade [8] used an average FIM for the polic y over the states and proposed a natur al policy gradient (NPG) learning. Kakade' s FIM has been widely adopted and various algorithms for the NPG learning have been developed by man y the learning plateaus than the NPG. This natural gradient is on the FIM of the state-action joint dients where the NSG learning outperformed NPG learning, especially with lar ge numbers of states in the MDP . Ho we ver, no algorithm for estimating the NSG has been proposed, probably because development of a tractable algorithm for NSG would be of great importance, and this is the one of the primary goals of this paper .
 Meanwhile, it would be very dif cult to select an appropriate FIM because it would be dependent on Natural Gradient (gNG) and deri ved an efcient approach to estimate the gNG by applying the a gN AC algorithm with an instrumental variable, where a polic y parameter is updated by a gNG the performance of the proposed algorithm, numerical experiments are sho wn in Section 5, where the proposed algorithm can estimate the gNG efciently and outperformed the NAC algorithm [1]. We briey revie w the polic y gradient and natural gradient learning as gradient ascent methods for RL and also present the moti vation of the gN AC approach. 2.1 Policy Gradient Reinf orcement Lear ning PGRL is modeled on a discrete-time Mark ov Decision Process (MDP) [15 , 16]. It is dened by the Also, p : S AS ! [0 ; 1] is a state transition probability function of a state s , an action a , a learning agent at each time step. The action probability function : ASR d ! [0 ; 1] uses a , s , and a polic y parameter 2R d to dene the decision-making rule of the learning agent, which is is controlled by tuning . Here, we mak e two assumptions in the MDP .
 Assumption 1 The policy is always dif fer entiable with respect to and is non-r edundant for the Assumption 2 The Mark ov chain M( ) , fS ; A ; p; ; g is always ergodic (irr educible and aperi-odic).
 s j S = s; M( )) , 8 s 2 S . This distrib ution satises the balance equation: d ( s +1 ) = P The goal of PGRL is to nd the polic y parameter that maximizes the average of the immediate rewards, the aver age rewar d , is referred to as the Polic y Gradient (PG), is Therefore, the average reward ( ) will be increased by updating the polic y parameter as := + r ( ) , where := denotes the right-to-left substitution and is a suf ciently small learning rate. This frame work is called the PGRL [4].
 It is noted that the ordinary PGRL methods omit the dif ferences in sensiti vities and the correla-tions between the elements of , as dened by the probability distrib utions of the MDP , while most probability distrib utions expressed in the MDP have some form of a manifold structure instead of gradient method will be dif ferent from the steepest directions on these manifolds. Therefore, the optimization process sometimes falls into a stagnant state, commonly called a plateau [8, 12]. 2.2 Natural Gradients for PGRL To avoid the plateau problem, the concept of the natural gradient was proposed by Amari [9], which is a gradient method on a Riemannian space. The parameter space being a Riemannian space implies that the parameter 2 R d is on the manifold with the Riemannian metric G ( ) 2 R d d (a semi-positi ve denite matrix), instead of being on a Euclidean manifold of an arbitrarily parameterized polic y, and the squared length of a small incremental vector connecting to + is given by k k 2 G ( ) is given by which is called the natural gradient (NG). Accordingly , to (locally) maximize ( ) , is incremen-tally updated with The direction of the NG is dened using a Riemannian metric. Thus, an appropriate choice of the Riemannian metric for the task is required. With RL, two kinds of Fisher Information Matrices (FIMs) F ( ) have been proposed as the Riemannian metric matrices G ( ) : 2 the metric matrix with the notation r a b , ( r a ) b , as on this FIM, e r the FIM of the state-action joint distrib ution for RL, where F s ( ) , P e r Some algorithms for the NPG learning, such as NAC [1] and NTD [10 , 11], can be successfully implemented using modications of the actor -critic frame works based on the LSTD Q ( ) [18 ] and TD( ) [16 ]. In contrast, no tractable algorithm for the NSG learning has been proposed to date. Ho we ver, it has been suggested that the NSG learning it better than the NPG learning due to the three dif ferences [12 ]: (a) The NSG learning appropriately benets from the concepts of Amari' s trib ution that the average reward depends on. (b) F s;a ( ) is an analogy to the Hessian matrix of the average reward. (c) Numerical experiments sho w a strong tendenc y to avoid entrapment in a algorithm for NSG is important, and this is one of the goals of our work. F NSG learning compared to NPG learning, since the mixing time depends on the multiple (not neces-and to estimate the gradient with nite samples. The ranking of the performances of the NPG and NSG learning will be dependent on the RL task properties. Thus, we consider a mixture of NPG and NSG as a generalized NG (gNG) and propose the approach of ' gener alized Natur al Actor -Critic ' First we explain the denition and properties of the generalized Natural Gradient (gNG). Then we introduce the estimating functions to build up a foundation for an efcient estimation of the gNG. 3.1 Denition of gNG for RL In order to dene an interpolation between NPG and NSG with a parameter 2 [0 ; 1] , we consider a linear interpolation from the FIM of (2) for the NPG to the FIM of (3) for the NSG, written as Then the natural gradient of the interpolated FIM is Ob viously , gNG( =0 ) and gNG( =1 ) are equi valent to the NPG and the NSG, respecti vely . When so that : 1 ! 0 is inversely proportional to t : 1 ! 1 . The term 'gener alized' of gNG( ) reects the generalization as the time steps on the joint distrib ution that the NG follo ws. 3.2 Estimating Function of gNG( ) tion, which pro vides well-established results for parameter estimation [14 ].
 Such a function g 2R d for an estimator ! 2R d (and a variable x ) is called an estimating function when it satises these conditions for all : Pr oposition 1 The d -dimensional (random) function is an estimating function for gNG( ), suc h that the unique solution of E [ g 0 respect to ! is equal to the gNG( ).
 Pr oof: From (1) and (4) , the equation holds. Thus, ! is equal to the gNG( ) from (5) . The remaining conditions from (7) and (8) , which the estimating function must satisfy , also obviously hold (under Assumption 1). In order to estimate gNG( ) by using the estimating function (9) with nite T samples on M( ) , the simultaneous equation estimate of gNG( ), so that b ! = ! holds in the limit as T !1 .
 variables r ln f d ( s ) ( a j s ; ) g . 3.3 Auxiliary Function of Estimating Function Although we made a simple algorithm implementing the gN AC approach with the M-estimator of the estimating function in (9) , the performance of the estimation of gNG( ) may be unacceptable for real RL applications, since the variance of the estimates of gNG( ) tends to become too lar ge. For that reason, we extend the estimating function using (9) by embedding an auxiliary function to create space for impro vement in (9) .
 Lemma 1 The d -dimensional (random) function is an estimating function for gNG( ), wher e ( s; s +1 ) is called the auxiliary function for (9) : respectively .
 Pr oof: See supplementary material.
 Let G denote the class of such functions g with various auxiliary functions . An optimal aux-iliary function, which leads to minimizing the variance of the gNG estimate b ! , is dened by the optimality criterion of the estimating functions [22 ]. An estimating function g det j g Lemma 2 Let us appr oximate (or assume) If the policy is non-de gener ate for the task (so the dimension of , d, is equal to P jSj ! denotes the gNG( ), then the `near' optimal auxiliary function in the `near' optimal estimating function g couplets. The value of r ln f d ( s ) ( a j s ; ) g &gt; ! has P jSj has at most P jSj and ! is independent of the choice of due to Lemma 1, we kno w that ! ? = ! holds. Therefore, for is minimized for det j ^ g j = 0 due to (15) .
 From Lemma 2, the near optimal auxiliary function can be regarded as minimiz-ing the mean squared residuals to zero between R ( s; a ) and the estimator b R ( s; a ; ! ) , is interpreted as a near minimization of the Euclidean distance between r and its approximator b R ( s; a ; b ! ) , so that works to reduce the distance of the regressand r and the subspace of the space at the point b ! = ! . Lemma 2 leads directly to Corollary 1.
 Cor ollary 1 Let b at = 0 , then b aver age rewar d, respectively .
 Pr oof: For all s , ! , and , the follo wing equation holds, Therefore, the follo wing equation, which is the same as the denition of the value function b with the average reward c We now propose a useful instrumental variable for the gNG( ) estimation and then deri ve a gN AC algorithm along with an algorithm for r ln d ( s ) estimation. 4.1 Bias from Estimation of r ln d ( s ) compute r ln ( a j s ; ) since we have parameterized the polic y, we cannot compute the Logarithm probabilities and the reward function are kno wn. Thus, we use the LSD estimate from the algo-rithm, LS LSD [13 ]. These LSD estimates b r ln d ( s ) usually have some estimation errors with ( s ) is an d -dimensional random variable satisfying E f ( s ) j s g = 0 .
 In such cases, the estimate of gNG( ) from the estimating function (9) or (10) would be biased, estimating function (10), since the instrumental variable can be replaced with any function I that satises their conditions 5 for any s , , and ! [22 ] and mak es the solution ! become the gNG( ): 4.2 Instrumental variables of near optimal estimating function for gNG( ) We use a linear function to introduce the auxiliary function (dened in (11) ), where 2 R jSj +1 and ( s ) 2 R jSj are the model parameter and the regressor (feature vector linearly independent. Accordingly , the whole model parameter of the estimating function is now [ ! We propose the follo wing instrumental variable Because this instrumental variable I ? has the desirable property as sho wn in Theorem 1, the esti-mating function g ? Theor em 1 To estimate gNG( ), let I ? ( s; a ) be used for the estimating function as and ! and be the solutions, so ! is equal to the gNG( ), ! = e r ~ iary function with is the near optimal auxiliary function provided in Lemma 2, ( s; s +1 ; ) = ( s; s +1 ) , even if the LSD estimates include (zer o mean) random noises.
 Pr oof Sk etch: (i) The condition (18) for the instrumental variable is satised due to Assump-satises E [ g ? and  X  ( s; s +1 ; )= ( s; s +1 )  X  hold, then E [ g ? from (14) and its expectation over M( ) becomes equal to 0 . This means that (20) also satises the condition (16) . From (i), (ii), and (iii), this theorem is pro ven.
 The optimal instrumental variable I ( s; a ) with respect to the variance minimization is deri ved not adress I here. Note that the proposed I ? ( s; a ) of (19) can be computed analytically . 4.3 A Generalized Natural Actor -Critic Algorithm with LS LSD We can straightforw ardly deri ve a generalized Natural Actor -Critic algorithm, gN AC( ), by solv-ing the estimating function g ? Ho we ver, since in the mode parameter is not required in updating the polic y parameter , to re-duce the computational cost, we compute only ! by using the results of the block matrices, The abo ve algorithm table sho ws an instance of the gN AC( ) algorithm with LS LSD( ) [13 ] with the for getting parameter for the statistics, the learning rate of the polic y , and the the denitions ( s t 1 ; s t ) , [ ( s t 1 ) ( s t )] and ~ ( s t 1 ; s t ) , [ ( s t 1 ; s t ) &gt; ; 1] &gt; . Note that the LSD estimate is not used at all in the proposed gN AC( = 0 ). In addition, note that gN AC( = 0 ) is equi valent to a non-episodic NAC algorithm modied to optimize the average results of Corollary 1. Figure 1: Averages and standard deviations over 50 independent episodes: (A) The angles between MDP , (B) The learning performances (average rewards) for the various (N)PGRL algorithms with the auxiliary functions on the 30 -states MDP . randomly synthesized MDPs with jSj = f 5 ; 30 g states and jAj = 2 actions. Starting with the per -formance baseline of the existing PG methods, we used Konda' s actor -critic algorithm [23 ]. This algorithm uses the baseline function in which the state value estimates are estimated by LSTD(0) [25 ], while the original version did not use any baseline function. Note that gN AC( = 0 ) can be regarded as the NAC proposed by [1], which serv es as the baseline for the current state-of-the-art PGRL algorithm. We initialized the setting of the MDP in each episode so the set of the actions was b and s n b U ( jSj ; b ) , temporarily with the Gaussian distrib ution N( =0 ; 2 =1) , normalized so that max ( )=1 and min ( )= 1 ; R ( s; a; s +1 ):=2( R ( s; a; s +1 ) min ( )) = (max ( ) min ( )) 1 . The sho ws the angles between the true gNG( ) and the gNG( ) estimates with and without the auxiliary function ( s; s +1 ; ) at := 0 (x ed polic y), := 1 , := 0 . The estimation without the auxil-iary function was implemented by solving the estimating function of (9) . We can conrm that the estimate using g ? a much more efcient estimator than without the auxiliary function. Figure 1 (B) sho ws the com-parison results in terms of the learning performances, where the learning rates for the gN ACs and Konda' s actor -critic were set as := 3 10 4 and Konda := 60 . The other hyper parameters := 1 and := 0 were the same in each of the algorithms. We thus conrmed that our gN AC( &gt; 0 ) algorithm outperformed the current state-of-the-art NAC algorithm (gN AC( =0 )). In this paper , we proposed a generalized NG (gNG) learning algorithm that combines two Fisher information matrices for RL. The theory of the estimating function pro vided insight to pro ve some important theoretical results from which our proposed gN AC algorithm was deri ved. Numerical ex-periments sho wed that the gN AC algorithm can estimate gNGs efciently and that it can outperform function for the gNG, we dened an auxiliary function on the criterion of the near optimality of the estimating function, by minimizing the distance between the immediate reward as the regressand deeply understand the properties and efcac y of our proposed gN AC algorithm.
