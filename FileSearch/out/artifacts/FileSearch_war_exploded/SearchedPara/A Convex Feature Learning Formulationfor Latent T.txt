 Pratik Jawanpuria pratik.j@cse.iitb.ac.in J. Saketha Nath saketh@cse.iitb.ac.in Dept. of CSE, IIT Bombay, Mumbai, INDIA.
 The paradigm of Multi-task learning (MTL) in-volves learning several prediction tasks simultane-ously (Caruana, 1997). In contrast to single task learn-ing, here the idea is to synergize the related tasks by appropriate sharing of information within them. Fol-lowing Evgeniou &amp; Pontil (2004); Jacob et al. (2008); Jalali et al. (2010), tasks are said to be related if the corresponding task parameters are close to each other. The focus of this paper is in the multi-task learning setting where some relevant features could be shared across few related tasks. Such situations arise in sev-eral real world applications (Tropp, 2006; Jalali et al., 2010). Existing works in this setting (Turlach et al., 2005; Zhang &amp; Huang, 2008; Negahban &amp; Wainwright, 2009; Jalali et al., 2010) employ a ` 1 /`  X  -norm based regularizer that promotes sparsity among features and low variance among the parameters of all the given tasks in the shared feature space. Success of such methods depends on the extent to which the given tasks are related and the extent to which the fea-tures are shared among the tasks. In fact, Negahban &amp; Wainwright (2009) show that ` 1 /`  X  regularization could actually perform worse than simple element-wise ` regularization when the extent to which the features are shared is less than a threshold or when the task parameters are not all close-by. Alternatively, Chen et al. (2010) assume that the relations between the tasks are known and propose employing a regularizer that penalizes deviations in weight vectors for highly correlated tasks. However, in real-world applications such task relations are not known apriori and need to be discovered.
 The main contribution of this work is a convex for-mulation that simultaneously discovers groups of re-lated tasks having close-by task parameters, as well as the feature space shared within each group. Here, the search space for the groups of related tasks is taken to be the power-set of the given tasks. Following the set-up of Multiple Kernel Learning (MKL) (Bach et al., 2004), the feature space in each group is taken to be that induced by a conic combination of a given set of base kernels. In the special case where the base kernels are chosen to be linear kernels formed by individual in-put features, this amounts to feature selection. Note that Widmer et al. (2010) also attempt a search among all possible groups of tasks; however the run-time of their algorithm is exponential in the number of tasks. Moreover, the shared feature space in each group is assumed to be the input space. The pro-posed formulation employs a graph-based regularizer that encodes an important structure among the groups of tasks: if there is no feature space under which a group of tasks has close-by task parameters, then there does not exist such a feature space for any of its su-persets. Note that this specialty when appropriately exploited by an algorithm may avoid search in poten-tially large portions of the search space which are any-way not fruitful. In Section 3, an active set algorithm is presented that exploits this specialty and optimally solves the proposed formulation in a time polynomial in the number of groups of related tasks discovered. Note that this number is typically very small compared to the size of the power-set of the given tasks. Simula-tions on benchmark datasets show that the proposed methodology achieves good generalization and outper-forms state-of-the-art multi-task learning techniques in some cases.
 The rest of the paper is organized as follows. Section 2 formalizes the notation and the problem set-up. The details of the proposed formulation and the algorithm for solving it are described in Sections 3 and 4 respec-tively. Experimental results are discussed in Section 5. We conclude by summarizing the work and the key contributions. Consider a set T of learning tasks, T in number. The training data for the t th task is denoted by: D t = { ( x ti ,y ti ) , i = 1 ,...,m } X  t = 1 ,...,T , where ( x represents the i th input/output pair of the t th task. For the sake of notational simplicity, we assume that the number of training examples is the same for all the tasks. The task predictors are assumed to be affine: F ( x ) =  X  f t , X  ( x )  X  X  X  b t , t = 1 ,...,T , where f t is the weight vector of the t th task,  X  (  X  ) is the feature map and b t is the bias term. Recall that our aim is to discover groups of related tasks from the power-set of T (henceforth denoted by V ). To this end, we further assume that f t = P w  X  X  subsets of T containing task t and f tw is the weight vector indicating the influence of group/subset w on task t . As we shall detail in the subsequent section, we employ a sparse regularizer that forces many f tw to be zero and hence enables selection of promising groups of related tasks.
 In addition to discovering groups of related tasks, the proposed formulation also learns the correspond-ing shared feature spaces induced by conic combi-nations of base kernels. To this end, let k 1 ,...,k be the given base kernels. Let  X  j (  X  ) denote the fea-ture map induced by the j th kernel k j , j = 1 ,...,n . Hence,  X  ( x ) = (  X  1 ( x ) ,..., X  n ( x )). Let f j tw the projection of f tw onto the  X  j space. In other words, f tw = ( f 1 tw ,...,f n tw ). With this notation, the prediction function for the task t can be rewritten as F t ( x ) =  X  f t , X  ( x )  X   X  b t = P w  X  X  0  X  t  X  w , then the feature space corresponding to the j th kernel is absent in the shared feature space of the group w . Hence learning the the task predictors or equivalently the weight vectors f j tw and the bias terms b t amounts to simultaneous discovery of latent task structure as well as the corresponding shared feature spaces. In the subsequent section a novel convex for-mulation for learning the optimal task predictors in the current set-up is presented. This section presents the key contribution of the pa-per  X  a convex feature learning formulation for latent task structure discovery. Following the well-establish methodology of regularized risk minimization (Vapnik, 1998), we consider the following problem: where  X ( f 1 ,...,f T ) 2 is the regularizer, ` (  X  ,  X  ) is a suit-able convex loss function (like the hinge loss) and C is the regularization parameter. In multi-task learn-ing, it is common to choose a regularizer based on some prior knowledge about the relationship among the given tasks. For example, when all the tasks are in-dependent,  X ( f 1 ,...,f T ) 2 can be taken as P T t =1 k f leading to a factorization of the problem into problems involving individual tasks. In cases where it is known that all the given tasks have close-by weight vectors (i.e., all tasks are related), the following regularizer may be employed (Evgeniou &amp; Pontil, 2004): where f t = h 0 + h t  X  t = 1 ,...,T and  X  is the parame-ter that controls the trade-off between regularizing the mean weight vector h 0 and the variance in the weight vectors of the tasks.
 In the following text, we present a novel regular-izer suitable for the current problem. We begin by writing down a basic term in the proposed regu-larizer,  X  j w , which induces close-by feature weights in the group w wrt. the feature space j :  X  j w = This term is motivated from (2). Note that,  X  j w 0  X  f j tw = 0  X  t  X  w i.e., the shared feature space of the group w does not involve the j th kernel/feature space.
 Now, the terms  X  j w , j = 1 ,...,n are combined using a p -norm expression, leading to: k  X  w k p =
P  X  w , j = 1 ,...,n, and p  X  (1 , 2). Such a p -norm pro-motes sparsity in the selection of the kernel induced feature spaces (i.e., forces many  X  j w = 0). With the interpretation of  X  j w noted above, essentially this en-ables feature learning within the w th group of tasks. Also, k  X  w k p = 0  X   X  j w = 0  X  j = 1 ,...,n  X  f j tw = 0  X  t  X  w, j = 1 ,...,n i.e., in case the node w does not contain related tasks (under any feature space in-duced by combinations of the given base kernels), then it does not influence any of the task predictors F t . With this interpretation, one naive way of obtain-ing few promising groups of related tasks (that share a feature space) is by employing a ` q , q  X  (1 , 2) norm over the terms k  X  w k p , w  X  V :  X ( f 1 ,...,f T ) =
P regularizer is that it renders the formulation (1) infea-sible for real-world applications as the resultant opti-mization problem cannot be, in general, solved in a time polynomial in the number of tasks.
 One key idea in the paper is to employ a graph-based regularizer, that alleviates this problem by exploiting a special structure among the groups of tasks. Note that the groups of tasks can be represented as nodes of a directed acyclic graph with the partial order  X  , rep-resenting the  X  X ubset of X  relation. It can verified that  X  X  ,  X  X  is a lattice. The topmost node of the lattice represents a dummy node  X  the group with no tasks in it, the second level nodes represents groups with sin-gle task and so on. The bottommost node represents the group consisting of all the T tasks. As discussed earlier, we would like to encode into our regularizer the following structure among the groups of tasks: if there is no feature space under which a group of tasks has similar task parameters, then there does not exist such a feature space for any of its supersets. In the context of the present lattice, this is same as saying: if a node w is not selected, then the entire sub-lattice D ( w ), which consists of all the descendants of w (in-cluding w itself), need not be selected. In the follow-ing, a regularizer that reflects this special structure is presented.
 Motivated by the graph-based regularizers employed in Zhao et al. (2009); Bach (2008), we propose the following novel regularizer for the problem at hand: where q  X  (1 , 2) , p  X  (1 , 2) and d v is a parameter that enables encoding prior knowledge regarding the task-relatedness in the group/node v . For e.g. one may have the prior knowledge that there is no task which is not related to the others. In this case one may choose d v = 0 for all the nodes in the second level of the lat-tice. Note that the proposed regularizer (3) may also be viewed as a ` 1 ,` q ,` p mixed-norm regularizer. The ` -norm over the nodes ( v  X  X  ) of the lattice promotes sparsity, and hence we have P w  X  D ( v ) k  X  w k q p for most v  X  V i.e., few groups of related tasks are 0  X  t  X  w,  X  j  X  1 ,...,n,  X  w  X  D ( v ). In other words if a group/node is not selected (by the 1-norm), then none of its descendants are selected by the for-mulation  X  which is exactly the special structure we wanted to encode. The q -norm brings in additional sparsity among the descendants of the groups that are selected by the 1-norm. As we detail later, the key advantage with this regularizer is that it renders the proposed formulation (1), solvable in reasonable time. In the following, a specialized partial dual of (1) with the proposed regularizer (3) is presented. This gives further insights into the working of the proposed for-mulation and motivates an efficient active set algo-rithm for solving it. In order to keep notations sim-ple, the dual is presented for the case where each of the given tasks is a binary classification problem and the loss function ` ( F t ( x ) ,y ) is the hinge loss: max (0 , 1  X  yF t ( x )). However, it is easy to extend the derivations to other learning settings and convex loss functions as well.
 Theorem 1. In the case where the given tasks are all of binary classification and the hinge loss is employed as the loss function, the dual of (1) with the regularizer defined in (3) is given by 1 where  X  |V| = z  X  R |V| | z  X  0 , 1 &gt; z = 1 denotes the simplex of dimension |V| and H is a convex function with H (  X  ) equal to the optimal value of the following optimization problem: where y t denotes the vector with entries as y ,  X  = [  X  1 ...  X  T ] &gt; , 1 and 0 denote vec- X  sents the set of ancestors for node w (including to describe the matrix K j w  X  R mT  X  mT is by writing it as a block matrix of size T  X  T with the ( t 1 ,t 2 ) th block as the matrix K j w ( t 1 R  X   X   X  The dual (4) provides interesting insights into the for-mulation. To this end, let us begin with an interpre-tation for the K j w matrices. From their definition, it is easy to see that K j w can also be viewed as the gram matrix of training examples from all the tasks with an appropriately defined kernel function k j w . As  X   X  0, dominates and the kernel function k j w reflects great similarity between examples of tasks in w (and vice-versa). Also, the examples from tasks not belonging to w have low similarity with those in w . Hence, the kernel k j w captures the similarity between the tasks in the group w under the j th feature space.
 Now lets focus on the problem (5). In the special case  X  p =  X  q , this problem is same as the ` formulation (Kloft et al., 2009) with  X  q =  X  q  X  q  X  1 with base kernels as  X  k j w = (  X  w (  X  )) 1  X  q k j  X  j = 1 ,...,n . Hence the problem (5) realizes a sparse combination of these kernels. With the interpretation provided above, this essentially amounts to a sparse selection of groups of related tasks. Hence the prob-lem of latent task structure discovery essentially is posed as an MKL problem (with appropriately de-fined kernels k j w ). However, unlike `  X  q -MKL that per-forms a  X  X lat X  kernel selection, here the kernels are weighted by a monotonic function of  X  w (  X  ) giving rise to a structured selection among the kernels. To see this, let us shift our focus to the dual problem (4). Because of the simplex constraints (i.e., ` 1 regular-ization), most of the  X  v will either be near zero at optimality. From the definition of  X  w (  X  ), we obtain:  X  v = 0  X   X  w (  X  ) = 0  X  w  X  D ( v ). Also,  X  w (  X  ) = 0 implies the entire set of kernels k 1 w ,...,k n w are not se-lected i.e., the group of tasks in w are not related in any feature space under consideration. To summa-rize, few groups of related tasks that share a feature space are selected and if a group of tasks is unrelated (  X  v = 0), then all its supersets are unrelated (because,  X  w (  X  ) = 0  X  w  X  D ( v )). Figure 1 provides an illus-tration of the dual problem for the case of three tasks ( T = 3) and three base kernels ( n = 3). The proposed formulation is equivalent to performing a structured selection among n  X |V| kernels arranged on a lattice. At each node, there are n kernels that represent the relatedness of tasks in that group. The subsequent sec-tion, presents an efficient active-set algorithm for solv-ing the proposed formulation that exploits the special structure in the solution described above. Following a common practice for solving large-scale convex sparsity problems (Lee et al., 2007; Bach, 2008), we propose solving the dual (4) using an ac-tive set algorithm.
 The basic idea of the active set algorithm is as follows: the formulation is solved iteratively using improved guesses for the active set, which is defined as the set of w for which  X  w 6 = 0 at optimality. At each iteration the problem restricted to the variables in the active set is solved using an appropriate solver. In order to save computational cost, the size of the initial active set is usually taken to be minimal. After solving the problem with the variables restricted to the current active set, a sufficiency condition for optimality of the solution is verified. In case the solution is optimal, the algorithm terminates. In case it is not, the active set is updated and the restricted problem with the new active set is solved. This process is repeated until optimality is reached. Any prior knowledge related to the structure of the optimal solution may also be incorporated in building the active set at each iteration. In case of problems like (4) with sparse solutions, the hope is that one may not solve the problem with all the variables,  X  , which are exponential in T in number.
 In order to formalize the active-set algorithm, we need i) an initial guess for the active set and a procedure for building/improving the active set after each iteration, ii) a sufficiency condition for verifying optimality of the current solution. Ideally, the complexity of verification of the condition should depend on the active set size rather than the problem size iii) an efficient algorithm for solving the formulation restricted to active set. We begin with the first. Let W represent the active set. The initial guess as well as the methodology for the identification of the promising nodes is motivated by the special structure in the solution of (4): if a node w is not selected in the optimal solution, i.e.,  X  w = 0, none of the descendants of w are selected (since  X  v (  X  ) = 0  X  v  X  D ( w )). Equivalently, it can be stated that a node w can be selected only if all its ancestors are selected i.e., only if  X  v 6 = 0  X  v  X  A ( w ). Due to this observation, W is always maintained to be equal to its hull, where hull ( W ) is defined to be the set of all the ancestors of the nodes in W . Accord-ingly, the initial guess for W is taken to be the second level nodes i.e., the singleton task groups. Also, in the subsequent iterations, only those nodes which have all their parents in W are considered as potential candi-dates for entry inside W .
 Towards the second requirement, we present the fol-lowing theorem 2 : Theorem 2. For a given active set W such that W = hull ( W ) , let the optimal solution of (4) re-stricted to W be ( X   X ,  X   X  ) . Let  X   X  be the value of the ( X   X ,  X   X  ) is an optimal solution of (4) with duality gap if: where sources ( W ) is the set of nodes in W with no parent in W and W c denotes the set of all the nodes present in the lattice V but not in W .
 As mentioned earlier, the above sufficiency condi-tion is useful only if it can be verified in polynomial time in |W| . Firstly, size of sources ( W c ) is upper-bounded by T |W| . The denominator in the summa-vided d v is decomposable as a product. In the simula-tions we use, d v = 1 . 5 | v | . Because of the block struc-ture of the matrices K j w , the sum over descendants in (6) can be computed in O ( T 2 m 2 ).
 In the following, we present an efficient algorithm of solving (4) restricted to W . Note that (4) has a simple constraint set, which is a simplex and the gradient  X  H (  X  ) can be computed using the Dan-skin X  X  theorem (Bertsekas, 1999): the i th compo-nent of this sub-gradient is given by (  X  H (  X  ))  X 
P an optimal solution of (5) with the given  X  . Hence one can employ projected gradient-descent algorithm or any of its variants for solving (4. Here we employ the mirror-descent algorithm (Ben-Tal &amp; Nemirovski, 2001) for solving (4) 3 . Note that the gradient com-putation  X  H (  X  ) requires solving (5) with the given  X  . Also, since the constraint set in (5) is similar to that in an SVM, the  X   X  will be sparse at optimality. Hence we use a sequential minimal optimization (SMO) algo-rithm (Platt, 1999) for solving (5). Algorithm 1 sum-marizes the proposed active-set method. The com-putational complexity of the active set algorithm is as follows: let the final size of the active set be W . Hence, (4) is solved a maximum of W times. Each run of mirror-descent algorithm takes O ( log ( W )) it-erations (Ben-Tal &amp; Nemirovski, 2001) while in each iteration the dominant computation is that of SMO for solving (5). A conservative complexity estimate for Algorithm 1 Active Set Algorithm the SMO algorithm is O (( Tm ) 3 ( Wn ) 2 ). The comput-ing cost for kernel matrices is O ( n ( Tm ) 2 ), while that of verifying the sufficiency condition is O (( Tm ) 2 TW ). Thus the overall complexity is: O ( n 2 W 2 m 3 T 3 ). We end this section by presenting a variant of the proposed methodology. The motivation for this vari-ant comes from a closer-look at the complexity of the active-set algorithm. Since the active-set always sat-isfies the condition W = hull ( W ) and since the com-plexity depends on the active-set size W ; in practice one cannot realize situations where the group selected is way down the lattice. In other words, it is rare that a group with large number of tasks is selected. However, as shown in simulations, there might exist applications where weight vectors are extremely close-by for all or most of the tasks. Hence realizing a group containing most of the tasks may be beneficial. One simple modi-fication of the proposed methodology for selecting such large groups is: invert the lattice of groups of tasks i.e., revert the parent-child relations, and employ exactly the same formulation (the descendants become the an-cestors and vice-versa). It is easy to see that in this case groups involving large number of tasks may be selected; whereas selecting groups involving few tasks is now improbable. Though this modification is simple and interesting, the natural motivation for employing the graph-based regularizer is absent in this case. The graph-based regularizer needs to be motivated purely from a computational perspective in this case. In this section we present our empirical studies on the following benchmark multi-task classification and re-gression datasets: Sarcos A multiple-output regression dataset used in Zhang &amp; Yeung (2009). The aim is to predict in-verse dynamics corresponding to the seven degrees-of-freedom of a robot arm. The number of tasks is 7 and there are 21 real valued input features. Follow-ing Zhang &amp; Yeung (2009), we sampled 2000 random examples from each task.
 Parkinson A multi-task regression dataset 4 . The aim is to predict Parkinson X  X  disease symptom score for pa-tients at different times using 19 bio-medical features. The dataset has 5,875 observations for 42 patients. The symptom score prediction problem for each pa-tient is considered as a regression task. Thus there are 42 regression tasks with number of instances for each task ranging from 101 to 168.
 Yale A face recognition dataset from Yale face base 5 . It contains 165 images of 15 subjects. Following the experimental setup in Zhang &amp; Schneider (2010), each task is defined as the binary classification problem of classifying two subjects. Thus there are 28 tasks and the number of features is 30.
 Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang &amp; Schneider (2010). It contains examples collected from various landmine fields. Each example is represented as a 9-dimensional real valued feature vector. Each task is a binary classification problem with the goal being to predict landmines (positive class) or clutter (negative class). Following Xue et al. (2007); Zhang &amp; Schneider (2010), we jointly learn 19 tasks from the landmine fields numbered 1  X  10 and 16  X  24 in the data set. Number of instances in each task varies from 445 to 690. The dataset is highly biased against the positive class.
 MHC-I A multi-task classification dataset used in Ja-cob et al. (2008). It contains binding affinities of var-ious peptides with different MHC-I molecules. Each task here is a binary classification problem. We per-form experiments on the same 10 tasks reported in Ja-cob et al. (2008). Total number of instances in the 10 tasks is 1200 and the input space consists of 180 bi-nary features. The number of instances per task varies from 59 to 197 and the the dataset is biased against the positive class.
 Letter A multi-task classification dataset used in Ji &amp; Ye (2009). It consists of handwritten letters from different writers. Each task is a binary classifica-tion problem of distinguishing between pairs of letters. There are 9 such binary classification tasks and we ran-domly sampled 300 data points per task for our simu-lations. The input features used are the 8  X  16 = 128 binary pixels.
 Each dataset was further randomly split into train-ing and test sets. In Landmine, MHC-I and Letter datasets, random 20%-80% train-test splits were con-sidered. In the case of Sarcos dataset 15 random sam-ples per task were used for training and the rest for testing. For Parkinson dataset, 5 random examples per task were used in training and the rest for testing. We compare the following multi-task learning tech-niques in terms of generalization ability: MTL Classical multi-task learning algorithm by Ev-geniou &amp; Pontil (2004). Assumes that all tasks are related and have close-by weight vectors. No feature learning is performed.
 CMTL The clustered multi-task learning formulation proposed in Jacob et al. (2008). Finds clusters of tasks having similar weight vectors. No feature learning is performed. 6 DMTL The multi-task feature learning formulation in Jalali et al. (2010). Performs feature selection to discover features shared across all the tasks as well as task-specific features. Also, induces close-by weight vectors for the tasks in the shared feature space. 7 MTFL The proposed multi-task feature learning for-mulation. The base kernels were taken as linear kernels with individual input features. In addition, the linear kernel using all input features was also employed as a base kernel. Thus if the input space dimensionality is n , then we generate n + 1 linear kernels. We did not employ non-linear kernels for the sake of being fair and comparable to DMTL. The parameters p,q were both fixed at 1 . 5, promoting sparsity in selecting groups of related tasks as well as in selecting the kernel induced feature spaces. Since the base kernels include individ-ual input-feature based linear kernels, this amounts to feature selection. 8 STL A baseline approach in which the tasks are learned independently using SVM.
 Note that all of the above multi-task learning tech-niques rely on the same notion of task-relatedness: weight vectors of related tasks are close. Hence a com-parison among them is indeed meaningful.
 The free parameters in all the methods were tuned using nested 3-fold cross validation procedure. The details of the parameter ranges are as follows: in case of MTFL , MTL and STL , the regularization param-eter C was chosen from the set { 10  X  3 , 10  X  2 ,..., 10 MTFL and MTL have an additional parameter  X  , which was chosen from the set: { 10  X  3 , 10  X  2 ,..., 10 } . CMTL has 4 parameters and we considered 3 val-ues for each leading to 3 4 = 81 combinations (Zhang &amp; Schneider, 2010). DMTL has 2 parameters and we considered 7 values of each leading to 49 combinations. Results of the simulations are summarized in Table 1. In case of regression datasets we report the explained variance, whereas for classification datasets we report AUC (Area Under Curve). In both cases, higher the value reported, the better the algorithm. Also, we report both the mean and standard deviation in the values over 10 random train-test splits. The numbers in the brackets indicate the run-times in minutes with the tuned parameters on a Xeon machine with 16GB RAM. The best result in each dataset is highlighted. In case the best result is with the proposed method ( MTFL ) and its improvement over state-of-the-art is statistically significant, then we additionally mark it with a  X * X . In case the best result is with an exist-ing method and its improvement over the proposed method ( MTFL ) is statistically significant, then we again mark it with a  X * X . Statistical significance test is performed using the paired t-test at 90% confidence. The proposed method outperformed state-of-the-art in both the regression datasets and achieved significant improvement in case of the Yale dataset. Note that in case of Sarcos dataset, the baseline STL performs bet-ter than MTL showing that there may be some tasks that are not related to some others and the task struc-ture is non-trivial. Hence discovering the latent task structure is indeed important in this case. The ex-cellent performance of the proposed method indicates that the task structure is well discovered by it. According to the results, the proposed methodology does not seem to improve over state-of-the-art in case of the MHC-I and Letter datasets. A closer look at the datasets and the predictors achieved with state-of-the-art showed that the weight vectors are extremely close-by in these datasets. This motivated us to try the inverted lattice trick described towards the end of sec-tion 4. With this modified methodology we achieved an improved average AUC of 72 . 77% and 61 . 12% re-spectively on MHC-I and Letter datasets.
 We end this section with a discussion on the run-time of the proposed method. Note that none of the exist-ing methods attempt an optimal search over the ex-ponentially large space of groups of tasks. Hence, as expected, the run-time of the proposed method is on the higher-side. Though this is the case, it is inter-esting to note that the extremely large search space (2 42 ) in case of the Parkinson dataset is searched in a reasonable time of 23min. Moreover, in most datasets the proposed method achieves better generalization. In real-world applications it is important to discover groups of related tasks that share a feature space. The main contribution of the work is a convex formulation for solving this problem. Using a novel graph based regularizer, the search in the exponentially large space of groups of tasks is rendered feasible. Experimental results illustrate the efficacy of the proposed approach. We thank Ganesh Ramakrishnan for insightful discus-sions on this paper.

