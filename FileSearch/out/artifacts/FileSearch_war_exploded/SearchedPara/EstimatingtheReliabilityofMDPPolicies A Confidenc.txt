 NLP researchers frequently have to deal with issues of data sparsity . Whether the task is machine transla-tion or named-entity recognition, the amount of data one has to train or test with can greatly impact the re-liability and rob ustness of one' s models, results and conclusions.

One research area that is particularly sensiti ve to the data sparsity issue is machine learning, speci-cally in using Reinforcement Learning (RL) to learn the optimal action for a dialogue system to mak e given any user state. Typically this involv es learn-ing from pre viously collected data or interacting in real-time with real users or user simulators. One of the biggest adv antages to this machine learning ap-proach is that it can be used to generate optimal poli-cies for every possible state. Ho we ver, this method requires a thorough exploration of the state-space to mak e reliable conclusions on what the best actions are. States that are infrequently visited in the train-ing set could be assigned sub-optimal actions, and therefore the resulting dialogue manager may not pro vide the best interaction for the user .
In this work, we present an approach for esti-mating the reliability of a polic y deri ved from col-lected training data. The key idea is to tak e into ac-count the uncertainty in the model parameters (MDP transition probabilities), and use that information to numerically construct a condence interv al for the expected cumulati ve reward for the learned polic y. This condence interv al approach allo ws us to: (1) better assess the reliability of the expected cumula-tive reward for a given polic y, and (2) perform a re-ned comparison between policies deri ved from dif-ferent MDP models.

We apply the proposed approach to our pre vious work (Tetreault and Litman, 2006) in using RL to impro ve a spok en dialogue tutoring system. In that work, a dataset of 100 dialogues was used to de-velop a methodology for selecting which user state features should be included in the MDP state-space. But are 100 dialogues enough to generate reliable policies? In this paper we apply our condence in-terv al approach to the same dataset in an effort to in-vestigate how reliable our pre vious conclusions are, given the amount of available training data.
In the follo wing section, we discuss the prior work and its data sparsity issue. In section 3, we describe in detail our condence interv al methodol-ogy . In section 4, we sho w how this methodology works by applying it to the prior work. In sections 5 and 6, we present our conclusions and future work. Past research into using RL to impro ve spok en di-alogue systems has commonly used Mark ov Deci-sion Processes (MDP' s) (Sutton and Barto, 1998) to model a dialogue (such as (Le vin and Pieraccini, 1997) and (Singh et al., 1999)).

A MDP is dened by a set of states { s a set of actions { a probabilities which reect the dynamics of the en-vironment { p ( s time t in state s transition to state s Additionally , an expected reward r ( s ned for each transition. Once these model parame-ters are kno wn, a simple dynamic programming ap-proach can be used to learn the optimal control pol-icy  X   X  , i.e. the set of actions the model should tak e at each state, to maximize its expected cumulati ve reward.

The dialog control problem can be naturally cast in this formalism: the states { s correspond to the dialog states (or an abstraction thereof), the actions { a particular actions the dialog manager might tak e, and the rewards r ( s a particular dialog performance metric. Once the MDP structure has been dened, the model param-eters { p ( s pus of dialogs (either real or simulated), and, based on them, the polic y which maximizes the expected cumulati ve reward is computed.

While most work in this area has focused on de-veloping the best polic y (such as (W alk er, 2000), (Henderson et al., 2005)), there has been relati vely little work done with respect to selecting the best features to include in the MDP state-space. For in-stance, Singh et al. (1999) sho wed that dialogue length was a useful state feature and Frampton and Lemon (2005) sho wed that the user' s last dialogue act was also useful. In our pre vious work, we com-pare the worth of several features. In addition, Paek and Chick ering' s (2005) work sho wed how a state-space can be reduced by only selecting features that are rele vant to maximizing the reward function.
The moti vation for this line of research is that if one can properly select the most informati ve fea-tures, one develops better policies, and thus a bet-ter dialogue system. In the follo wing sections we summarize our past data, approach, results, and is-sue with polic y reliability . 2.1 MDP Structur e For this study , we used an annotated corpus of human-computer spok en dialogue tutoring sessions. The x ed-polic y corpus contains data collected from 20 students interacting with the system for ve prob-lems (for a total of 100 dialogues of roughly 50 turns each). The corpus was annotated with 5 state fea-tures (Table 1). It should be noted that two of the features, Certainty and Frustration, were manually annotated while the other three were done automat-ically . All features are binary except for Certainty which has three values.

For the action set { a type of question the system could ask the student given the pre vious state. There are a total of four possible actions: ask a short answer question (one that requires a simple one word response), a com-ple x answer question (one that requires a longer , deeper response), ask both a simple and comple x question in the same turn, or do not ask a question at all (gi ve a hint). The reward function r was the learning gain of each student based on a pair of tests before and after the entire session of 5 dialogues. The 20 students were split into two groups (high and low learners) based on their learning gain, so 10 students and their respecti ve ve dialogues were given a positi ve reward of +100, while the remain-der were assigned a negati ve reward of -100. The rewards were assigned in the nal dialogue state, a common approach when applying RL in spok en di-alogue systems. 2.2 Appr oach and Results To investigate the usefulness of dif ferent features, we took the follo wing approach. We started with two baseline MDPs. The rst model (Baseline 1) used only the Correctness feature in the state-space. The second model (Baseline 2) included both the Correctness and Certainty features. Ne xt we con-structed 3 new models by adding each of the remain-ing three features (Frustration, Percent Correct and Concept Repetition) to the Baseline 2 model.
We dened three metrics to compare the policies deri ved from these MDPs: (1) Dif f's: the number of states whose polic y dif fers from the Baseline 2 pol-icy, (2) Percent Polic y change (P.C.): the weighted amount of change between the two policies (100% indicates total change), and (3) Expected Cumula-tive Re ward (or ECR) which is the average reward one would expect in that MDP when in the state-space.

The intuition is that if a new feature were rele-vant, the corresponding model would lead to a dif-ferent polic y and a better expected cumulati ve re-ward (when compared to the baseline models). Con-versely , if the features were not useful, one would expect that the new policies would look similar (specically , the Dif f's count and % Polic y Change would be low) or produce similar expected cumula-tive rewards to the original baseline polic y. The results of this analysis are sho wn in Table 2 1 The Dif f's and Polic y Change metrics are undened for the two baselines since we only use these two metrics to compare the other three features to Base-line 2. All three metrics sho w that the best feature to add to the Baseline 2 model is Concept Repetition since it results in the most change over the Baseline 2 polic y, and also the expected reward is the highest as well. For the remainder of this paper , when we refer to Concept Repetition, Frustration, or Percent Correctness, we are referring to the model that in-cludes that feature as well as the Baseline 2 features Correctness and Certainty .
 2.3 Pr oblem with Reliability Ho we ver, the approach discussed abo ve assumes that given the size of the data set, the ECR and poli-cies are reliable. If the MDP model were very frag-ile, that is the polic y and expected cumulati ve reward were very sensiti ve to the quality of the transition probability estimates, then the metrics could reveal quite dif ferent rankings. Pre viously , we used a qual-itati ve approach of tracking how the worth of each state (V-value) changed over time. The V-values indicate how much reward one would expect from starting in that state to get to a nal state. We hy-pothesized that if the V-values stabilized as data in-creased, then the learned polic y would be more reli-able.

So is this V-value methodology adequate for as-sessing if there is enough data to determine a sta-ble polic y, and also for assessing if one model is better than another? Since our approach for state-space selection is based on comparing a new pol-icy with a baseline polic y, having a stable polic y is extremely important since instability could lead to dif ferent conclusions. For example, in one compar -ison, a new polic y could dif fer with the baseline in 8 out of 10 states. But if the MDP were unstable, adding just a little more data could result in a dif fer -ence of only 4 out of 10 states. Is there an approach that can cate gorize whether given a certain data size, that the expected cumulati ve reward (and thus the polic y) is reliable? In the next section we present a new methodology for numerically constructing con-dence interv als for these value function estimates. Then, in the follo wing section, we ree valuate our prior work with this methodology and discuss the results. 3.1 Policy Ev aluation with Confidence The starting point for the proposed methodology is the observ ation that for each state s tion a ties { p ( s distrib utions that are estimated from the transition counts in the training data: where n is the number of states in the model, and c ( s i , s j , a k ) is the number of times the system was in state s s in the training data.

It is important to note that these parameters are just estimates. The reliability of these estimates clearly depends on the amount of training data, more specically on the transition counts c ( s instance, consider a model with 3 states and 2 ac-tions. Say the model was in state s a transitioned back to state s tioned to state s we have:
Additionally , let' s say the same model was in state s tion, it transitioned 300 times to state s to state s
While both sets of transition parameters have the same value, the second set of estimates is more reli-able. The central idea of the proposed approach is to model this uncertainty in the system parameters, and use it to numerically construct condence interv als for the value of the optimal polic y.

Formally , each set of transition probabilities trib ution, estimated from data 2 . The uncertainty of multinomial estimates are commonly modeled by means of a Dirichlet distrib ution. The Dirichlet dis-trib ution is characterized by a set of parameters  X   X  , ...,  X  counts { c ( s lik elihood of the set of multinomial transition pa-rameters { p ( s
P ( { p ( s i | s j , a k ) } i =1 ..n | D ) = Note that the maximum lik elihood estimates for the formula abo ve correspond to the frequenc y count formula we have already described:  X  p
To capture the uncertainty in the model parame-ters, we therefore simply need to store the counts of the observ ed transitions c ( s this model of uncertainty , we can numerically con-struct a condence interv al for the value of the opti-mal polic y  X   X  . Instead of computing the value of the polic y based on the maximum lik elihood transition estimates  X  T erate a lar ge number of transition matrices  X  T ...  X  T corresponding to the counts observ ed in the train-ing data (in the experiments reported in this paper , we used m = 1000 ). We then compute the value of the optimal polic y  X   X  in each of these models { V the 95% condence interv al for the value function based on the resulting value estimates: the bounds for the condence interv al are set at the lowest and highest 2.5 percentile of the resulting distrib ution of the values for the optimal polic y { V
The algorithm is outlined belo w: 1. compute transition counts from the training set: 2. compute maximum lik elihood estimates for 3. use dynamic programming to compute the op-4. sample m transition matrices {  X  T 5. evaluate the optimal polic y  X   X  in each of these 6. numerically build the 95% condence interv al
To summarize, the central idea is to tak e into ac-count the reliability of the transition probability esti-mates and construct a condence interv al for the ex-pected cumulati ve reward for the learned polic y. In the standard approach, we would compute an esti-mate for the expected cumulati ve reward, by simply using the transition probabilities deri ved from the training set. Note that these transition probabilities are simply estimates which are more or less accu-rate, depending on how much data is available. The proposed methodology does not fully trust these es-timates, and asks the question: given that the real world (i.e. real transition probabilities) might actu-ally be a bit dif ferent than we think it is, how well can we expect the learned polic y to perform? Note that the condence interv al we construct, and there-fore the conclusions we dra w, are with respect to the polic y learned from the current estimates, i.e. from the current training set. If more data becomes avail-able, a dif ferent optimal polic y might emer ge, about which we cannot say much. 3.2 Related Work Given the stochastic nature of the models, con-dence interv als are often used to estimate the reli-ability of results in machine learning experiments, e.g. (Ri vals and Personnaz, 2002), (Schapire, 2002) and (Dumais et al., 1998). In this work we use a condence interv al methodology in the conte xt of MDPs. The idea of modeling the uncertainty of the transition probability estimates using Dirichlet models also appears in (Jaulmes et al., 2005). In that work, the authors used the uncertainty in model parameters to develop acti ve learning strate gies for partially observ able MDPs, a topic not pre viously addressed in the literature. In our work we rely on the same model of uncertainty for the transition ma-trix, but use it to deri ve condence interv als for the expected cumulati ve reward for the learned optimal polic y, in an effort to assess the reliability of this polic y. Our pre vious results indicated that Concept Repe-tition was the best feature to add to the Baseline 2 state-space model, but also that Percent Correctness and Frustration (when added to Baseline 2) offered an impro vement over the Baseline MDP' s. Ho w-ever, these conclusions were based on a very quali-tati ve approach for determining if a polic y is reliable or not. In the follo wing subsection, we apply our ap-proach of condence interv als to empirically deter -mine if given this data set of 100 dialogues, whether the estimates of the ECR are reliable, and whether the original rankings and conclusions hold up under this rened analysis. In subsection 4.2, we pro vide a methodology for pinpointing when one model is better than another . 4.1 Quantitati ve Analysis of ECR Reliability For our rst investigation, we look at the condence interv als of each MDP' s ECR over the entire data set of 20 students (later in this section we sho w plots for the condence interv als as data increases). Table 3 sho ws the upper and lower bounds for the ECR orig-inally reported in Table 2. The rst column sho ws the original, estimated ECR of the MDP and the last column is the width of the bound (the dif ference be-tween the upper and lower bound).

So what conclusions can we mak e about the reli-ability of the ECR, and hence of the learned policies for the dif ferent MDP' s, given this amount of train-ing data? The condence interv al for the ECR for the Baseline 1 model ranges from 0.21 to 23.73. Re-call that the nal states are capped at +100 and -100, and are thus the maximum and minimum bounds that one can see in this experiment. These bounds tell us that, if we tak e into account the uncertainty in the model estimates (gi ven the small training set size), with probability 0.95 the actual true ECR for this polic y will be greater than 0.21 and smaller than 23.73. The width of this condence interv al is 23.52.
For the Baseline 2 model, the bounds are much wider: from -5.31 to 60.48, for a total width of 65.79. While the ECR estimate is 31.92 (which is seemingly lar ger than 6.15 for the Baseline 1 model), the wide condence interv al tells us that this estimate is not very reliable. It is possible that the polic y deri ved from this model with this amount of data could perform poorly , and even get a negati ve reward. From the dialogue system designer' s stand-point, a model lik e this is best avoided.

Of the remaining three models  X  Concept Repeti-tion, Frustration, and Percent Correctness, the rst one exhibits a tighter condence interv al, indicat-ing that the estimated expected cumulati ve reward (42.56) is fairly reliable: with 95% probability of being between 28.37 and 59.29. The ECR for the other two models (Frustration and Percent Correct-ness) again sho ws a wide condence interv al once we tak e into account the uncertainty in the model parameters.

These results shed more light on the shortcom-ings of the ECR metric used to evaluate the models in prior work. This estimate does not tak e into ac-count the uncertainty of the model parameters. For example, a model can have an optimal polic y with a very high ECR value, but have very wide con-dence bounds reaching even into negati ve rewards. On the other hand, another model can have a rela-tively lower ECR but if its bounds are tighter (and the lower bound is not negati ve), one can kno w that that polic y is less affected by poor parameter esti-mates stemming from data sparsity issues. Using the condence interv als associated with the ECR gives a much more rened, quantitati ve estimate of the reli-ability of the reward, and hence of the polic y deri ved from that data.

An extension of this result is that condence in-terv als can also allo w us to mak e rened judgments about the comparati ve utility of dif ferent features, the original moti vation of our prior study . Basi-cally , a model (M1) is better than another (M2) if M1' s lower bound is greater than the upper bound of M2. That is, one kno ws that 95% of the time, the worst case situation of M1 (the lower bound) will always yield a higher reward than the best case of M2. In our data, this happens only once, with Con-cept Repetition being empirically better than Base-line 1, since the lower bound of Concept Repetition is 28.37 and the upper bound of Baseline 1 is 23.73. Given this situation, Concept Repetition is a useful feature which, when included in the model, leads to a better polic y than simply using Correctness. We cannot dra w any conclusions about the other fea-tures, since their bounds are generally quite wide. Given this amount of training data, we cannot say whether Percent Correctness and Frustration are bet-ter features than the Baseline MDP' s. Although their ECR' s are higher , there is too much uncertainty to denitely conclude the y are better . 4.2 Pinpointing Model Cr oss-o ver The pre vious analysis focused on a quantitati ve method of (1) determining the reliability of the MDP ECR estimate and polic y, as well as (2) assessing whether one model is better than another . In this section, we present an extension to the second con-trib ution by answering the question: given that one model is more reliable than another , is it possible to determine at which point one model' s estimates become more reliable than another model' s? In our case, we want to kno w at what point Concept Rep-etition becomes more reliable than Baseline 1. To do this, we investigate how the condence interv al changes as the amount of training data increases in-stead of looking at the reliability estimate at only one particular data size.

We incrementally increase the amount of train-ing data (adding the data from one new student at a time), and calculate the corresponding optimal pol-icy and condence interv al for the expected cumula-tive reward for that polic y. Figure 1 sho ws the con-dence interv al plots as data is added to the MDP for the Baseline 1 and Concept Repetition MDP' s. For reference, Baseline 2, Percent Correctness and Frustration plots did not exhibit the same con verg-ing beha vior as these two, which is not surprising given how wide the nal bounds are. For each plot, the bold lines represent the upper and lower bounds, and the dotted line represents the calculated ECR.
Analyzing the two MDP' s, we nd that the con-dence interv als for Baseline 1 and Concept Repeti-tion con verge as more data is added, which is an ex-pected trend. One useful result from observing the change in condence interv als is that one can de-termine the point in one which one model becomes empirically better than another . Superimposing the upper and lower bounds (Figure 2) reveals that after we include the data from the rst 13 students, the lower bound of Concept Repetition crosses over the upper bound of Baseline 1.

Observing this beha vior is especially useful for performing model switching. In automatic model switching, a dialogue manager runs in real time and as it collects data, it can switch from using a sim-ple dialogue model to a comple x model. Condence interv als can be used to determine when to switch from one model to the next by checking if a comple x model' s bounds cross over the bounds of the current model. Basically , the dialogue manager switches when it can be sure that the more comple x model' s ECR is not only higher , but statistically signicantly so. Figure 2: Baseline 1 and Concept Repetition Bounds Past work in using MDP' s to impro ve spok en dia-logue systems have usually glossed over the issue of whether or not there was enough training data to de-velop reliable policies. In this work, we present a numerical method for building condence interv als for the expected cumulati ve reward for a learned pol-icy. The proposed approach allo ws one to (1) better assess the reliability of the expected cumulati ve re-ward for a given polic y, and (2) perform a rened comparison between policies deri ved from dif ferent MDP models.

We applied this methodology to a prior experi-ment where the objecti ve was to select the best fea-tures to include in the MDP state-space. Our results sho w that policies constructed from the Baseline 1 and Concept Repetition models are more reliable, given the amount of data available for training. The Concept Repetition model (which is composed of the Concept Repetition, Certainty and Correctness features) was especially useful, as it led to a polic y that outperformed the Baseline 1 model, even when we tak e into account the uncertainty in the model estimates caused by data sparsity . In contrast, for the Baseline 2, Percent Correctness, and Frustration models, the estimates for the expected cumulati ve reward are much less reliable, and no conclusion can be reliably dra wn about the usefulness of these fea-tures. In addition, we sho wed that our condence interv al approach has applications in another MDP problem: model switching. As an extension of this work, we are currently inves-tigating in more detail what mak es some MDP' s reli-able or unreliable for a certain data size (such as the case where Baseline 2 does not con verge but a more complicated model does, such as Concept Repeti-tion). Our initial ndings indicate that, as more data becomes available the bounds tighten for most pa-rameters in the transition matrix. Ho we ver, for some of the parameters the bounds can remain wide, and that is enough to keep the condence interv al for the expected cumulati ve reward from con verging. We would lik e to thank Jef f Schneider , Dre w Bag-nell, Pam Jordan, as well as the ITSPOKE and Pitt NLP groups, and the Dialog on Dialogs group for their help and comments. Finally , we would lik e to thank the four anon ymous revie wers for their com-ments on the initial version of this paper . Support for this research was pro vided by NSF grants #0325054 and #0328431.

