 Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborat-ing Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which di-rectly profile both users and products, and neighborhood models, which analyze similarities betw een products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby build-ing a more accurate combined model. Further accuracy improve-ments are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their perfor-mance at a top-K recommendation task.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms collaborative filtering, recommender systems
Modern consumers are inundated with choices. Electronic retail-ers and content providers offer a huge selection of products, with unprecedented opportun ities to meet a variety of special needs and tastes. Matching consumers with most appropriate products is not trivial, yet it is a key in enhancing user satisfaction and loyalty. This emphasizes the prominence of recommender systems , which pro-vide personalized recommendations for products that suit a user X  X  taste [1]. Internet leaders like Amazon, Google, Netflix, TiVo and Yahoo are increasingly adopting such recommenders.

Recommender systems are often based on Collaborative Filter-ing (CF) [10], which relies only on past user behavior X  X .g., their previous transactions or product ratings X  X nd does not require the creation of explicit profiles. Notably, CF techniques require no do-main knowledge and avoid the need for extensive data collection. In addition, relying directly on user behavior allows uncovering complex and unexpected patterns that would be difficult or impos-sible to profile using known data attributes. As a consequence, CF attracted much of attention in the past decade, resulting in signif-icant progress and being adopted by some successful commercial systems, including Amazon [15], TiVo and Netflix.

In order to establish recommendations, CF systems need to com-pare fundamentally different objects: items against users. There are two primary approaches to facilitate such a comparison, which con-stitute the two main disciplines of CF: the neighborhood approach and latent factor models .

Neighborhood methods are centered on computing the relation-ships between items or, alternatively, between users. An item-oriented approach evaluates the preference of a user to an item based on ratings of similar items by the same user. In a sense, these methods transform users to the item space by viewing them as baskets of rated items. This way, we no longer need to compare users to items, but rather directly relate items to items.
Latent factor models, such as Singular Value Decomposition (SVD), comprise an alternative approach by transforming both items and users to the same latent factor space, thus making them directly comparable. The latent space tries to explain ratings by characteriz-ing both products and users on factors automatically inferred from user feedback. For example, when the products are movies, fac-tors might measure obvious dimensions such as comedy vs. drama, amount of action, or orientation to children; less well defined di-mensions such as depth of character development or  X  X uirkiness X ; or completely uninterpretable dimensions.

The CF field has enjoyed a surge of interest since October 2006, when the Netflix Prize competition [5] commenced. Netflix re-leased a dataset containing 100 million movie ratings and chal-lenged the research community to develop algorithms that could beat the accuracy of its recommendation system, Cinematch. A lesson that we learnt through this competition i s that the neighbor-hood and latent factor approaches address quite different levels of structure in the data, so none of them is optimal on its own [3].
Neighborhood models are most effective at detecting very lo-calized relationships. They rely on a few significant neighborhood-relations, often ignoring the vast majority of ratings by a user. Con-sequently, these methods are unable to capture the totality of weak signals encompassed in all of a user X  X  ratings. Latent factor models are generally effective at estimating overall structure that relates si-multaneously to most or all items. However, these models are poor at detecting strong associations among a small set of closely related items, precisely where neighborhood models do best.

In this work we suggest a combined model that improves predic-tion accuracy by capitalizing on the advantages of both neighbor-hood and latent factor approaches. To our best knowledge, this is the first time that a single model has integrated the two approaches. In fact, some past works (e.g., [2, 4]) recognized the utility of com-bining those approaches. However, they suggested post-processing the factorization results, rather than a unified model where neigh-borhood and factor information are considered symmetrically.
Another lesson learnt from the Netflix Prize competition is the importance of integrating different forms of user input into the models [3]. Recommender systems rely on different types of in-put. Most convenient is the high quality explicit feedback ,which includes explicit input by users regarding their interest in products. For example, Netflix collects star ratings for movies and TiVo users indicate their preferences for TV shows by hitting thumbs-up/down buttons. However, explicit feedback is not always available. Thus, recommenders can infer user preferences from the more abundant implicit feedback , which indirectly reflect opinion through observ-ing user behavior [16]. Types of implicit feedback include purchase history, browsing history, search patterns, or even mouse move-ments. For example, a user that purchased many books by the same author probably likes that author. Our main focus is on cases where explicit feedback is available. Nonetheless, we recognize the im-portance of implicit feedback, which can illuminate users that did not provide enough explicit feedback. Hence, our models integrate explicit and implicit feedback.

The structure of the rest of the paper is as follows. We start with preliminaries and related work in Sec. 2. Then, we describe a new, more accurate neighborhood model in Sec. 3. The new model is based on an optimization framework that allows smooth integration with latent factor models, and also inclusion of implicit user feedback. Section 4 revisits SVD-based latent factor models while introducing useful extensions. These extensions include a factor model that allows explaining the reasoning behind recom-mendations. Such explainability is important for practical systems [11, 23] and known to be problematic with latent factor models. The methods introduced in Sec. 3-4 are linked together in Sec. 5, through a model that integrates neighborhood and factor mod-els within a single framework. Relevant experimental results are brought within each section. In a ddition, we suggest a new method-ology to evaluate effectiveness of the models, as described in Sec. 6, with encouraging results.
We reserve special indexing letters for distinguishing users from items: for users u, v , and for items i, j . A rating r ui preference by user u of item i , where high values mean stronger preference. For example, values can be integers ranging from 1 (star) indicating no interest to 5 (stars) indicating a strong interest. We distinguish predicted ratings from known ones, by using the no-tation  X  r ui for the predicted value of r ui .The ( u, i ) r ui is known are stored in the set K = { ( u, i ) | r ui is known Usually the vast majority of ratings are unknown. For example, in the Netflix data 99% of the possible ratings are missing. In order to combat overfitting the sparse rating data, models are regularized so estimates are shrunk towards baseline defaults. Regularization is controlled by constants which are denoted as:  X  1 , X  2 values of these constants are determined by cross validation. As they grow, regularization becomes heavier.
Typical CF data exhibit large user and item effects  X  i.e., system-atic tendencies for some users to give higher ratings than others, and for some items to receive higher ratings than others. It is cus-tomary to adjust the data by accounting for these effects, which we encapsulate within the baseline estimates . Denote by  X  the overall average rating. A baseline estimate for an unknown rating denoted by b ui and accounts for the user and item effects: The parameters b u and b i indicate the observed deviations of user u and item i , respectively, from the average. For example, suppose that we want a baseline estimate for the rating of the movie Titanic by user Joe. Now, say that the average rating over all movies, 3.7 stars. Furthermore, Titanic is better than an average movie, so it tends to be rated 0.5 stars above the average. On the other hand, Joe is a critical user, who tends to rate 0.3 stars lower than the average. Thus, the baseline estimate for Titanic X  X  rating by Joe would be 3.9 stars by calculating 3 . 7  X  0 . 3+0 . 5 . In order to estimate one can solve the least squares problem: Here, the first term ( u,i )  X  X  ( r ui  X   X  + b u + b i ) 2 b  X  X  and b i  X  X  that fit the given ratings. The regularizing term  X   X  ( u b 2 u + i b 2 i )  X  avoids overfitting by penalizing the magni-tudes of the parameters.
The most common approach to CF is based on neighborhood models. Its original form, which was shared by virtually all earlier CF systems, is user-oriented; see [12] for a good analysis. Such user-oriented methods estimate unknown ratings based on recorded ratings of like minded users. Later, an analogous item-oriented approach [15, 21] became popular. In those methods, a rating is estimated using known ratings made by the same user on similar items. Better scalability and improved accuracy make the item-oriented approach more favorable in many cases [2, 21, 22]. In addition, item-oriented methods are more amenable to explaining the reasoning behind predictions. This is because users are famil-iar with items previously preferred by them, but do not know those allegedly like minded users. Thus, our focus is on item-oriented approaches, but parallel techniques can be developed in a user-oriented fashion, by switching the roles of users and items.
Central to most item-oriented approaches is a similarity measure between items. Frequently, it is based on the Pearson correlation coefficient,  X  ij , which measures the tendency of users to rate items i and j similarly. Since many ratings are unknown, it is expected that some items share only a handful of common raters. Computa-tion of the correlation coefficient is based only on the common user support. Accordingly, similarities based on a greater user support are more reliable. An appropriate similarity measure, denoted by s , would be a shrunk correlation coefficient: The variable n ij denotes the number of users that rated both j . A typical value for  X  2 is 100. Notice that the literature suggests additional alternatives for a similarity measure [21, 22].
Our goal is to predict r ui  X  the unobserved rating by user item i . Using the similarity measure, we identify the k items rated by u , which are most similar to i . This set of k neighbors is denoted by
S k ( i ; u ) . The predicted value of r ui is taken as a weighted av-erage of the ratings of neighboring items, while adjusting for user and item effects through the baseline estimates:
Neighborhood-based methods of this form became very popu-lar because they are intuitive and relatively simple to implement. However, in a recent work [2], we raised a few concerns about such neighborhood schemes. Most notably, these methods are not justi-fied by a formal model. We also questioned the suitability of a simi-larity measure that isolates the relations between two items, without analyzing the interactions within the full set of neighbors. In addi-tion, the fact that interpolation weights in (3) sum to one forces the method to fully rely on the neighbors even in cases where neigh-borhood information is absent (i.e., user u did not rate items similar to i ), and it would be preferable to rely on baseline estimates.
This led us to propose a more accurate neighborhood model, which overcomes these difficulties. Given a set of neighbors we need to compute interpolation weights {  X  u ij | j  X  S k ( i ; u ) } enable the best prediction rule of the form: Derivation of the interpolation weights can be done efficiently by estimating all inner products between item ratings; for a full de-scription refer to [2].
Latent factor models comprise an alternative approach to Collab-orative Filtering with the more holistic goal to uncover latent fea-tures that explain observed ratings; examples include pLSA [13], neural networks [18], and Latent Dirichlet Allocation [7]. We will focus on models that are induced by Singular Value Decomposi-tion (SVD) on the user-item ratings matrix. Recently, SVD mod-els have gained popularity, thanks to their attractive accuracy and scalability. A typical model associates each user u with a user-factors vector p u  X  R f , and each item i with an item-factors vector q  X  R f . The prediction is done by taking an inner product, i.e.,  X  r ui = b ui + p T u q i . The more involved part is parameter estimation.
In information retrieval it is well established to harness SVD for identifying latent semantic factors [8]. However, applying SVD in the CF domain raises difficulties due to the high portion of missing ratings. Conventional SVD is undefined when knowledge about the matrix is incomplete. Moreover, carelessly addressing only the relatively few known entries is highly prone to overfitting. Earlier works [14, 20] relied on imputation to fill in missing ratings and make the rating matrix dense. However, imputation can be very expensive as it significantly increases the amount of data. In ad-dition, the data may be considerably distorted due to inaccurate imputation. Hence, more recent works [4, 6, 9, 17, 18, 22] sug-gested modeling directly only the observed ratings, while avoiding overfitting through an adequate regularized model, such as: min A simple gradient descent technique was applied successfully to solving (5).

Paterek [17] suggested the related NSVD model, which avoids explicitly parameterizing each user, but rather models users based on the items that they rated. This way, each item i is associated with two factor vectors q i and x i . The representation of a user is through the sum: j  X  R( u ) x j / | R( u ) | ,so r ui is predicted items rated by user u . Later in this work, we adapt Paterek X  X  idea with some extensions.
We evaluated our algorithms on the Netflix data of more than 100 million movie ratings performed by anonymous Netflix cus-tomers [5]. We are not aware of any publicly available CF dataset that is close to the scope and quality of this dataset. To maintain compatibility with re sults published by others, we adopted some standards that were set by Netflix, as follows. First, quality of the results is usually measured by their root mean squared error we report results on a test set provided by Netflix (also known as the Quiz set), which contains over 1.4 million recent ratings. Net-flix compiled another 1.4 million recent ratings into a validation set, known as the Probe set, which we employ in Section 6. The two sets contain many more ratings by users that do not rate much and are harder to predict. In a way, they represent real requirements from a CF system, which needs to predict new ratings from older ones, and to equally address all users, not only the heavy raters.
The Netflix data is part of the ongoing Netflix Prize competition, where the benchmark is Netflix X  X  proprietary system, Cinematch, which achieved a RMSE of 0.9514 on the test set. The grand prize will be awarded to a team that manages to drive this RMSE below 0.8563 (10% improvement). Results reported in this work lower the RMSE on the test set to levels around 0.887, which is better than previously published results on this dataset.
As stated earlier, an important goal of this work is devising mod-els that allow integration of explicit and implicit user feedback. For a dataset such as the Netflix data, the most natural choice for im-plicit feedback would be movie rental history, which tells us about user preferences wit hout requiring them to explicitly provide their ratings. However, such data is not available to us. Nonetheless, a less obvious kind of implicit data does exist within the Netflix dataset. The dataset does not only tell us the rating values, but also which movies users rate, regardless of how they rated these movies. In other words, a user implicitly tells us about her preferences by choosing to voice her opinion and vote a (high or low) rating. This reduces the ratings matrix into a binary matrix, where  X 1 X  stands for  X  X ated X , and  X 0 X  for  X  X ot rated X . Admittedly, this binary data is not as vast and independent as other sources of implicit feed-back could be. Nonetheless, we have found that incorporating this kind of implicit data  X  which inherently exist in every rating based recommender system  X  significantly improves prediction accuracy. Some prior techniques, such as Conditional RBMs [18], also capi-talized on the same binary view of the data.

The models that we suggest are not limited to a certain kind of implicit data. To keep generality, each user u is associated with two sets of items, one is denoted by R( u ) , and contains all the items for which ratings by u are available. The other one, denoted by contains all items for which u provided an implicit preference.
In this section we introduce a new neighborhood model, which allows an efficient global optimization scheme. The model offers improved accuracy and is able to integrate implicit user feedback. We will gradually construct the various components of the model, through an ongoing refinement of our formulations.

Previous models were centered around user-specific interpola-item i to the items in a user-specific neighborhood S k ( i ; u ) order to facilitate global optimi zation, we would like to abandon such user-specific weights in favor of global weights independent of a specific user. The weight from j to i is denoted by w will be learnt from the data through optimization. An initial sketch of the model describes each rating r ui by the equation: For now, (6) does not look very different from (4), besides our claim that the w ij  X  X  are not user specific. Another difference, which will be discussed shortly, is that here we sum over all items rated by unlike (4) that sums over members of S k ( i ; u ) .

Let us consider the interpretation of the weights. Usually the weights in a neighborhood model represent interpolation coeffi-cients relating unknown ratings to existing ones. Here, it is useful to adopt a different viewpoint, where weights represent offsets to baseline estimates. Now, the residuals, r uj  X  b uj , are viewed as the coefficients multiplying those offsets. For two related items j , we expect w ij to be high. Thus, whenever a user u rated than expected ( r uj  X  b uj is high), we would like to increase our es-timate for u  X  X  rating of i by adding ( r uj  X  b uj ) w ij estimate. Likewise, our estimate will not deviate much from the baseline by an item j that u rated just as expected ( r uj around zero), or by an item j that is not known to be predictive on i ( w ij is close to zero). This viewpoint suggests several enhance-ments to (6). First, we can use implicit feedback, which provide an alternative way to learn user preferences. To this end, we add another set of weights, and rewrite (6) as: Much like the w ij  X  X , the c ij  X  X  are offsets added to baseline esti-mates. For two items i and j , an implicit preference by u us to modify our estimate of r ui by c ij , which is expected to be high if j is predictive on i . 1
Viewing the weights as global offsets, rather than as user-specific interpolation coefficients, emphasizes the influence of missing rat-ings. In other words, a user X  X  opinion is formed not only by what he rated, but also by what he did not rate. For example, suppose that a movie ratings dataset shows that users that rate  X  X ord of the Rings 3 X  high also gave high ratings to  X  X ord of the Rings 1 X 2 X . This will establish high weights from  X  X ord of the Rings 1 X 2 X  to  X  X ord of the Rings 3 X . Now, if a user did not rate  X  X ord of the Rings 1 X 2 X  at all, his predicted rating for  X  X ord of the Rings 3 X  will be penalized, as some necessary weights cannot be added to the sum.
For prior models ((3),(4)) that interpolated r ui  X  b ui from b uj | j  X  S k ( i ; u ) } , it was necessary to maintain compatibility be-tween the b ui values and the b uj values. However, here we do not use interpolation, so we can decouple the definitions of and b uj . Accordingly, a more general prediction rule would be:  X  r represents predictions of r ui by other methods such as a latent fac-tor model. We elaborate more on this in Section 5. For now, we suggest the following rule that was found to work well:  X  r ui =  X  + b u + b i + Importantly, the b uj  X  X  remain constants, which are derived as ex-plained in Sec. 2.1. However, the b u  X  X  and b i  X  X  become parameters, which are optimized much like the w ij  X  X  and c ij  X  X .
In many cases it would be reasonable to attach significance weights to implicit feedback. This requires a modification to our formula which, for simplicity, will not be considered here.
A characteristic of the current scheme is that it encourages greater deviations from baseline estimates for users that provided many ratings (high | R( u ) | ) or plenty of implicit feedback (high In general, this is a good practice for recommender systems. We would like to take more risk with well modeled users that provided much input. For such users we are willing to predict quirkier and less common recommendations. On the other hand, we are less cer-tain about the modeling of users th at provided onl y a little i nput, in which case we would like to stay with safe estimates close to the baseline values. However, our experience shows that the current model somewhat overemphasizes the dichotomy between heavy raters and those that rarely rate. Better results were obtained when we moderated this behavior, replacing the prediction rule with:
Complexity of the model can be reduced by pruning parameters corresponding to unlikely item-item relations. Let us denote by S k ( i ) the set of k items most similar i , as determined by the simi-and N k ( i ; u ) def =N( u )  X  S k ( i ) . 2 Now, when predicting ing to (9), it is expected that the most influential weights will be associated with items similar to i . Hence, we replace (9) with: When k =  X  , rule (10) coincides with (9). However, for other values of k it offers the potential to significantly reduce the number of variables involved.

This is our final prediction rule, which allows fast online predic-tion. More computational work is needed at a pre-processing stage where parameters are estimated. A major design goal of the new neighborhood model was f acilitating an efficient global optimiza-tion procedure, which prior neighborhood models lacked. Thus, model parameters are learnt by solving the regularized least squares problem associated with (10): min
An optimal solution of this convex problem can be obtained by
Notational clarification: With other neighborhood models it was beneficial to use S k ( i ; u ) , which denotes the k items most similar to i among those rated by u . Hence, if u rated at least k items, we will always have | S k ( i ; u ) | = k , regardless of how similar those items are to i .However, | R k ( i ; u ) | is typically smaller than some of those items most similar to i were not rated by u least square solvers, which are part of standard linear algebra pack-ages. However, we have found that the following simple gradient descent solver works much faster. Let us denote the prediction er-ror, r ui  X   X  r ui ,by e ui . We loop through all known ratings in a given training case r ui , we modify the parameters by moving in the opposite direction of the gradient, yielding:
The meta-parameters  X  (step size) and  X  4 are determined by cross-validation. We used  X  =0 . 005 and  X  4 =0 . 002 for the Netflix data. A typical number of iterations throughout the train-ing data is 15. Another important parameter is k , which controls the neighborhood size. Our experience shows that increasing ways benefits the accuracy of the results on the test set. Hence, the choice of k should reflect a tradeoff between prediction accuracy and computational cost.

Experimental results on the Netflix data with the new neighbor-hood model are presented in Fig. 1. We studied the model under different values of parameter k . The pink curve shows that accuracy monotonically improves with rising k values, as root mean squared error (RMSE) falls from 0.9139 for k = 250 to 0.9002 for k =  X  (Notice that since the Netflix data contains 17,770 movies, is equivalent to k = 17,770, where all item-item relations are ex-plored.) We repeated the experiments without using the implicit feedback, that is, dropping the c ij parameters from our model. The results depicted by the yellow curve show a significant decline in estimation accuracy, which widens as k grows. This demonstrates the value of incorporating implicit feedback into the model.
For comparison we provide the results of two previous neigh-borhood models. First is a correlation-based neighborhood model (following (3)), which is the most popular CF method in the litera-ture. We denote this model as CorNgbr. Second is a newer model [2] that follows (4), which will be denoted as WgtNgbr. For both these two models, we tried to pick optimal parameters and neigh-borhood sizes, which were 20 for CorNgbr, and 50 for WgtNgbr. The results are depicted by the green and cyan lines. Notice that the k value (the x -axis) is irrelevant to these models, as their differ-ent notion of neighborhood makes neighborhood sizes incompati-ble. It is clear that the popular CorNgbr method is noticeably less accurate than the other neighborhood models, though its 0.9406 RMSE is still better than the publis hed Netflix X  X  Cinematch RMSE of 0.9514. On the opposite side, our new model delivers more ac-curate results even when compared with WgtNgbr, as long as the value of k is at least 500.

Finally, let us consider running time. Previous neighborhood models require very light pre-processing, though, WgtNgbr [2] re-quires solving a small system of equations for each provided pre-diction. The new model does involve pre-processing where param-eters are estimated. However, online prediction is immediate by following rule (10). Pre-processing time grows with the value of Typical running times per iteration on the Netflix data, as measured on a single processor 3.4GHz Pentium 4 PC, are shown in Fig. 2.
As mentioned in Sec. 2.3, a popular approach to latent factor models is induced by an SVD-like lower rank decomposition of the Figure 1: Comparison of neighborhood-based models. We measure the accuracy of the new model with and without im-plicit feedback. Accuracy is measured by RMSE on the Netflix test set, so lower values indicate better performance. RMSE is shown as a function of varying values of k , which dictates the neighborhood size. For reference, we present the accuracy of two prior models as two horizontal lines: the green line rep-resents a popular method using Pearson correlations, and the cyan line represents a more recent neighborhood model. Figure 2: Running times (minutes) per iteration of the neigh-borhood model, as a function of the parameter k . ratings matrix. Each user u is associated with a user-factors vector p u  X  R f , and each item i with an item-factors vector q i Prediction is done by the rule: Parameters are estimated by minimizing the associated squared er-ror function (5). Funk [9] popularized gradient descent optimiza-tion, which was successfully practiced by many others [17, 18, 22]. Henceforth, we will dub this basic model  X  X VD X . We would like to extend the model by considering also implicit information. Follow-ing Paterek [17] and our work in the previous section, we suggest the following prediction rule: Here, each item i is associated with three factor vectors . On the other hand, instead of providing an explicit parame-terization for users, we represent users through the items that they prefer. Thus, the previous user factor p i was replaced by the sum new model, which will be henceforth named  X  X symmetric-SVD X , offers several benefits: 1. Fewer parameters. Typically the number of users is much 2. New users. Since Asymmetric-SVD does not parameterize 3. Explainability. Users expect a system to give a reason for 4. Efficient integration of implicit feedback. Prediction accu-
As usual, we learn the values of involved parameters by mini-mizing the regularized squared error function associated with (13): +  X  5 We employ a simple gradient descent scheme to solve the system. On the Netflix data we used 30 iterations, with step size of and  X  5 =0 . 04 .

An important question is whether we need to give up some pre-dictive accuracy in order to enjoy those aforementioned benefits of Asymmetric-SVD. We evaluated this on the Netflix data. As shown in Table 1, prediction quality of Asymmetric-SVD is actu-ally slightly better than SVD. The improvement is likely thanks to accounting for implicit feedback. This means that one can enjoy the benefits that Asymmetric-SVD offers, without sacrificing pre-diction accuracy. As mentioned earlier, we do not really have much independent implicit feedback for the Netflix dataset. Thus, we ex-pect that for real life systems with access to better types of implicit feedback (such as rental/purchase history), the new Asymmetric-SVD model would lead to even further improvements. Neverthe-less, this still has to be demonstrated experimentally.
In fact, as far as integration of implicit feedback is concerned, we could get more accurate results by a more direct modification of (12), leading to the following model: Now, a user u is modeled as p u + | N( u ) |  X  1 2 j  X  N( a free user-factors vector, p u , much like in (12), which is learnt from the given explicit ratings. This vector is complemented by the sum | N( u ) |  X  1 2 j  X  N( u ) y j , which represents the perspective of implicit feedback. We dub this model  X  X VD++ X . Similar mod-els were discussed recently [3, 19]. Model parameters are learnt by minimizing the associated squared error function through gradient descent. SVD++ does not offer the previously mentioned bene-fits of having less parameters, conveniently handling new users and readily explainable results. This is because we do abstract each user with a factors vector. However, as Table 1 indicates, SVD++ is clearly advantageous in terms of prediction accuracy. Actually, to our best knowledge, its results are more accurate than all pre-viously published methods on the Netflix data. Nonetheless, in the next section we will describe an integrated model, which offers fur-ther accuracy gains.
The new neighborhood model of Sec. 3 is based on a formal model, whose parameters are learnt by solving a least squares prob-lem. An advantage of this approach is allowing easy integration with other methods that are based on similarly structured global cost functions. As explained in Sec. 1, latent factor models and neighborhood models nicely complement each other. Accordingly, in this section we will integrate the neighborhood model with our most accurate factor model  X  SVD++. A combined model will sum Table 1: Comparison of SVD-based models: prediction accu-racy is measured by RMSE on the Netflix test set for varying number of factors ( f ). Asymmetric-SVD offers practical ad-vantages over the known SVD model, while slightly improving accuracy. Best accuracy is achieved by SVD++, which directly incorporates implicit feedback into the SVD model. the predictions of (10) and (15), thereby allowing neighborhood and factor models to enrich each other, as follows:  X  r + | R k ( i ; u ) |  X  1 2
In a sense, rule (16) provides a 3-tier model for recommenda-tions. The first tier,  X  + b u + b i , describes general properties of the item and the user, without accounting for any involved interactions. For example, this tier could argue that  X  X he Sixth Sense X  movie is known to be good, and that the rating scale of our user, Joe, tends to be just on average. The next tier, q T i p u + | N( u ) |  X  provides the interaction between the user profile and the item pro-file. In our example, it may find that  X  X he Sixth Sense X  and Joe are rated high on the Psychological Thrillers scale. The final  X  X eigh-borhood tier X  contributes fine grained adjustments that are hard to profile, such as the fact that Joe rated low the related movie  X  X igns X .
Model parameters are determined by minimizing the associated regularized squared error function through gradient descent. Recall that e ui def = r ui  X   X  r ui . We loop over all known ratings in a given training case r ui , we modify the parameters by moving in the opposite direction of the gradient, yielding:
When evaluating the method on the Netflix data, we used the fol-lowing values for the meta parameters:  X  1 =  X  2 =0 . 007 , X  0 . 001 , X  6 =0 . 005 , X  7 =  X  8 =0 . 015 . It is beneficial to de-crease step sizes (the  X   X  X ) by a factor of 0.9 after each iteration. The neighborhood size, k , was set to 300. Unlike the pure neighbor-hood model (10), here there is no benefit in increasing k neighbors covers more global information, which the latent factors already capture adequately. The iterative process runs for around Table 2: Performance of the integrated model. Prediction ac-curacy is improved by combining the complementing neighbor-hood and latent factor models. Increasing the number of fac-tors contributes to accuracy, but also adds to running time. 30 iterations till convergence. Table 2 summarizes the performance over the Netflix dataset for different number of factors. Once again, we report running times on a Pentium 4 PC for processing the 100 million ratings Netflix data. B y coupling neighborhood and latent factor models together, and recovering signal from implicit feed-back, accuracy of results is improved beyond other methods. Recall that unlike SVD++, both the neighborhood model and Asymmetric-SVD allow a direct explanation of their recommen-dations, and do not require re-training the model for handling new users. Hence, when explainability is preferred over accuracy, one can follow very similar steps to integrate Asymmetric-SVD with the neighborhood model, thereby improving accuracy of the indi-vidual models while still mainta ining the ability to reason about recommendations to end users.
So far, we have followed a common practice with the Netflix dataset to evaluate prediction accuracy by the RMSE measure. Achiev-able RMSE values on the Netflix test data lie in a quite narrow range. A simple prediction rule, which estimates r ui as the mean rating of movie i , will result in RMSE=1.053. Notice that this rule represents a sensible  X  X est sellers list X  approach, where the same recommendation applies to all users. By applying personalization, more accurate predictions are obtained. This way, Netflix Cine-match system could achieve a RMSE of 0.9514. In this paper, we suggested methods that lower the RMSE to 0.8870. In fact, by blending several solutions, we could reach a RMSE of 0.8645. Nonetheless, none of the 3,400 teams actively involved in the Net-flix Prize competition could reach, as of 20 months into the com-petition, lower RMSE levels, despite the big incentive of winning a $1M Grand Prize. Thus, the range of attainable RMSEs is seem-ingly compressed, with less than 20% gap between a naive non-personalized approach and the best known CF results. Successful improvements of recommendation quality depend on achieving the elusive goal of enhancing users X  satisfaction. Thus, a crucial ques-tion is: what effect on user experience should we expect by low-ering the RMSE by, say, 10%? For example, is it possible, that a solution with a slightly better RMSE will lead to completely dif-ferent and better recommendations? This is central to justifying research on accuracy improvements in recommender systems. We would like to shed some light on the issue, by examining the effect of lowered RMSE on a practical situation.
 A common case facing recommender systems is providing  X  X op K recommendations X . That is, the system needs to suggest the top K products to a user. For example, recommending the user a few specific movies which are supposed to be most appealing to him. We would like to investigate the effect of lowering the RMSE on the quality of top K recommendations. Somewhat surprisingly, the Netflix dataset can be used to evaluate this.

Recall that in addition to the test set, Netflix also provided a val-idation set for which the true ratings are published. We used all 5-star ratings from the validation set as a proxy for movies that interest users. 3 Our goal is to find the relative place of these  X  X nter-esting movies X  within the total order of movies sorted by predicted ratings for a specific user. To this end, for each such movie 5-stars by user u , we select 1000 additional random movies and predict the ratings by u for i and for the other 1000 movies. Fi-nally, we order the 1001 movies based on their predicted rating, in a decreasing order. This simulates a situation where the sys-tem needs to recommend movies out of 1001 available ones. Thus, those movies with the highest predictions will be recommended to user u . Notice that the 1000 movies are random, some of which may be of interest to user u , but most of them are probably of no interest to u . Hence, the best hoped result is that i (for which we know u gave the highest rating of 5) will precede the rest 1000 random movies, thereby improving the appeal of a top-K recom-mender. There are 1001 different possible ranks for i , ranging from the best case where none (0%) of the random movies appears be-fore i , to the worst case where all (100%) of the random movies appear before i in the sorted order. Overall, the validation set con-tains 384,573 5-star ratings. For each of them (separately) we draw 1000 random movies, predict associated ratings, and derive a rank-ing between 0% to 100%. Then, the distribution of the 384,573 ranks is analyzed. (Remark: since the number 1000 is arbitrary, re-ported results are in percentiles ( 0% X 100%), rather th an in absolute ranks (0 X 1000).) We used this methodology to evaluate five different methods. The first method is the aforementioned non-personalized prediction rule, which employs movie means to yield RMSE=1.053. Hence-forth, it will be denoted as MovieAvg. The second method is a correlation-based neighborhood model, which is the most popular approach in the CF literature. As mentioned in Sec. 3, it achieves a RMSE of 0.9406 on the test set, and was named CorNgbr. The third method is the improved neighborhood approach of [2], which we named WgtNgbr and could achieve RMSE= 0.9107 on the test set. Fourth is the SVD latent factor model, with 100 factors thereby achieving RMSE=0.9025 as reported in Table 1. Finally, we con-sider our most accurate method, the integrated model, with 100 factors, achieving RMSE=0.8870 as shown in Table 2.

Figure 3(top) plots the cumulative distribution of the computed percentile ranks for the five met hods over the 384,573 evaluated cases. Clearly, all methods would outperform a random/constant prediction rule, which would have resulted in a straight line con-necting the bottom-left and top-right corners. Also, the figure ex-hibits an ordering of the methods by their strength. In order to achieve a better understanding, let us zoom in on the head of the x -axis, which represents top-K recommendations. After all, in or-der to get into the top-K recommendations, a product should be ranked before almost all others. For example, if 600 products are considered, and three of them will be suggested to the user, only those ranked 0.5% or lower are relevant. In a sense, there is no dif-ference between placing a desired 5-star movie at the top 5%, top 20% or top 80%, as none of them is good enough to be presented to the user. Accordingly, Fig. 3(bottom), plots the cumulative ranks distribution between 0% and 2% (top 20 ranked items out of 1000).
As the figure shows, there are very significant differences among the methods. For example, the integrated method has a probabil-ity of 0.067 to place a 5-star movie before all other 1000 movies (rank=0%). This is more than three times better than the chance of the MovieAvg method to achieve the same. In addition, it is 2.8 times better than the chance of the popular CorNgbr to achieve the same. The other two methods, WgtNbr and SVD, have a probabil-ity of around 0.043 to achieve the same. The practical interpretation is that if about 0.1% of the items are selected to be suggested to
In this study, the validation set is excluded from the training data. the user, the integrated method has a significantly higher chance to pick a specified 5-star rated movie. Similarly, the integrated method has a probability of 0.157 to place the 5-star movie be-fore at least 99.8% of the random movies (rank 0.2%). For com-parison, MovieAvg and CorNgbr have much slimmer chances of achieving the same: 0.050 and 0.065, respectively. The remaining two methods, WgtNgbr and SVD, lie between with pr obabilitie s of 0.107 and 0.115, respectively. Thus, if one movie out of 500 is to be suggested, its proba bility of being a specific 5-st ars rated one becomes noticeably higher with the integrated model.

We are encouraged, even somewhat surprised, by the results. It is evident that small improvements in RMSE translate into signifi-cant improvements in quality of the top K products. In fact, based on RMSE differences, we did not expect the integrated model to deliver such an emphasized improvement in the test. Similarly, we did not expect the very weak performance of the popular correla-tion based neighborhood scheme, which could not improve much upon a non-personalized scheme. Figure 3: Comparing the performance of five methods on a top-K recommendation task, where a few products need to be suggested to a user. Values on the x -axis stand for the per-centile ranking of a 5-star rated movie; lower values repre-sent more successful recommendations. We experiment with 384,573 cases and show the cumulative distribution of the re-sults. The lower plot concentrates on the more relevant region, pertaining to low x -axis values. The plot shows that the in-tegrated method has the highest probability of obtaining low values on the x -axis. On the other hand, the non-personalized MovieAvg method and the popular correlation-based neighbor-hood method (CorNgbr) achieve the lowest probabilities.
This work proposed improvements to two of the most popular approaches to Collaborative Filtering. First, we suggested a new neighborhood based model, which unlike previous neighborhood methods, is based on formally optimizing a global cost function. This leads to improved prediction accuracy, while maintaining mer-its of the neighborhood approach such as explainab ility of predic-tions and ability to handle new user s without re-training the model. Second, we introduced extensions to SVD-based latent factor mod-els that allow improved accuracy by integrating implicit feedback into the model. One of the models also provides advantages that are usually regarded as belonging to neighborhood models, namely, an ability to explain recommendations and to handle new users seam-lessly. In addition, the new neighborhood model enables us to de-rive, for the first time, an integrated model that combines the neigh-borhood and the latent factor models. This is helpful for improving system performance, as the neighborhood and latent factor models address the data at different levels and complement each other.
Quality of a recommender system is expressed through multi-ple dimensions including: accuracy, diversity, ability to surprise with unexpected recommendations, explainability, appropriate top-K recommendations, and computational efficiency. Some of those criteria are relatively easy to measure, such as accuracy and effi-ciency that were addressed in this work. Some other aspects are more elusive and harder to quantify. We suggested a novel ap-proach for measuring the success of a top-K recommender, which is central to most systems where a few products should be suggested to each user. It is notable that evaluating top-K recommenders sig-nificantly sharpens the differences between the methods, beyond what a traditional accuracy measure could show.

A major insight beyond this work is that improved recommen-dation quality depends on successfully addressing different aspects of the data. A prime example is using implicit user feedback to ex-tend models X  quality, which our me thods facilitate. Evaluation of this was based on a very limited form of implicit feedback, which was available within the Netflix dataset. This was enough to show a marked improvement, but further experimentation is needed with better sources of implicit feedback, such as purchase/rental history. Other aspects of the data that may be integrated to improve pre-diction quality are content information like attributes of users or products, or dates associated with the ratings, which may help to explain shifts in user preferences.
 The author thanks Suhrid Balakrishnan, Robert Bell and Stephen North for their most helpful remarks. [1] G. Adomavicius and A. Tuzhilin,  X  X owards the Next [2] R. Bell and Y. Koren,  X  X calable Collaborative Filtering with [3] R. Bell and Y. Koren,  X  X essons from the Netflix Prize [4] R. M. Bell, Y. Koren and C. Volinsky,  X  X odeling [5] J. Bennet and S. Lanning,  X  X he Netflix Prize X , KDD Cup [6] J. Canny,  X  X ollaborative Filtering with Privacy via Factor [7] D. Blei, A. Ng, and M. Jordan,  X  X atent Dirichlet Allocation X , [8] S. Deerwester, S. Dumais, G. W. Furnas, T. K. Landauer and [9] S. Funk,  X  X etflix Update: Try This At Home X , [10] D. Goldberg, D. Nichols, B. M. Oki and D. Terry,  X  X sing [11] J. L. Herlocker, J. A. Konstan and J. Riedl, ,  X  X xplaining [12] J. L. Herlocker, J. A. Konstan, A. Borchers and John Riedl, [13] T. Hofmann,  X  X atent Semantic Models for Collaborative [14] D. Kim and B. Yum,  X  X ollaborative Filtering Based on [15] G. Linden, B. Smith and J. York,  X  X mazon.com [16] D.W. Oard and J. Kim,  X  X mplicit Feedback for [17] A. Paterek,  X  X mproving Regularized Singular Value [18] R. Salakhutdinov, A. Mnih and G. Hinton,  X  X estricted [19] R. Salakhutdinov and A. Mnih,  X  X robabilistic Matrix [20] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, [21] B. Sarwar, G. Karypis, J. Konstan and J. Riedl,  X  X tem-based [22] G. Takacs, I. Pilaszy, B. Nemeth and D. Tikk,  X  X ajor [23] N. Tintarev and J. Masthoff,  X  X  Survey of Explanations in
