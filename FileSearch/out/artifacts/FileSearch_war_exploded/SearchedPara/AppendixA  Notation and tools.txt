 In this appendix, we present the proof of Theorems 1 and 2 . We begin by introducing some more notation and auxiliary results.
 a domain X and X a random variable with values in sup { E max f  X  X  If T is a bounded linear operator on H its operator where m or n are explained in the context. and y = ( y 1 ,...,y m )  X  R m .
 A multisample is a vector Z = ( z 1 ,..., z T ) com-posed of samples. We also write Z = ( X , Y ) with X = ( x 1 ,..., x T ).
 For members of R K we use the greek letters  X  or  X  . Depending on context the inner product and eu-In the sequel we denote with C  X  the set  X   X  R K : k  X  k 1  X   X  , abbreviate C for the  X  1 -e ,...,e K . Unless otherwise specified the summation from 1 to T , and k will run from 1 to K . A.1. Covariances specified by The definition implies the inequality
X For a multisample X  X  H mT we will consider two quantities defined in terms of the empirical covari-ances.

S 1 ( X ) = S  X  ( X ) = x S smaller than S 1 ( X ).
 A.2. Concentration inequalities replacing the k -th coordinate of x with y . That is ity (ii) is given in ( Maurer , 2006 ).
 Theorem 3. Let F : X n  X  R and define A and B by
A 2 = sup
B 2 = sup X . Then for any s &gt; 0 A.3. Rademacher and Gaussian averages We will use the term Rademacher variables for any set of independent random variables, uniformly dis-Rademacher variables. A set of random variables is called orthogaussian if the members are indepen-and reserve the letter  X  for standard normal vari-always be independent Rademacher variables and  X  sian.
 For A  X  R n we define the Rademacher and Gaussian Mendelson , 2002 ) as and x = ( x 1 ,...,x n )  X  X n we write The empirical Rademacher and Gaussian complexities here in two portions for convenience in the sequel. Theorem 4. Let F be a real-valued function class on define Then E x  X   X  [ X  ( x )]  X  E x  X   X  R ( F ( x )) . Proof. For any realization  X  =  X  1 ,..., X  m of the Rademacher variables because of the symmetry of the measure  X   X  x the triangle inequality gives the result.
 for all f  X  F E x  X   X  [ f ( x )]  X  Under the conditions of this result, changing one of pendent version x  X   X  that for all f  X  F E x  X   X  [ f ( x )]  X  To bound Rademacher averages the following result Zhang , 2005 ; Ledoux &amp; Talagrand , 1991 ) Lemma 7. Let A  X  R n , and let  X  1 ,..., X  n {  X  Sometimes it is more convenient to work with gaus-grand , 1991 ) Lemma 8. For A  X  R k we have R ( A )  X  p  X / 2 G ( A ) .
 1962 ), ( Ledoux &amp; Talagrand , 1991 )). Theorem 9. Let  X  and  X  be mean zero, separable Gaussian processes indexed by a common set S , such that Then B.1. Multitask learning ing uniform bound on the estimation error. D  X  D K and all  X   X  C T  X  that T  X  L X  Fix X  X  H mT and for  X  = (  X  1 ,..., X  T )  X  R K T define the random variable for all t , then Proof. (i) We observe that E F  X  = E sup  X  sup  X  = =  X  (ii) For any configuration  X  of the Rademacher vari-F and any  X   X   X  { X  1 , 1 } to replace  X  sj we have Using the inequality ( 6 ) we then obtain P D The conclusion now follows from part (ii) of Theorem 3 .
 Proposition 12. We have for every fixed Z = ( X , Y )  X  ( H  X  R ) mT we have E general result being a consequence of rescaling. By tion  X  we have E their maxima at the extreme points, we have E sup Now for any  X   X  0 we have, since F  X   X  0, E max  X  c +  X  + X  X  c +  X  + X  X  c +  X  + (2 K ) T  X  c +  X  + Here the first inequality follows from the fact that probabilities never exceed 1 and a union bound. The second inequality follows from Lemma 11 , part (i), lows from a well known estimate on Gaussian random variables. Setting  X  = obtain with some easy simplifying estimates E max Theorem 10 now follows from Corollary 6 .
  X  ployed. The denominator in the exponent of Lemma 11 -(ii) then obtains another factor of tioned in Remark 5 following the statement of Theo-rem 1 .
 Another modification leads to a bound for the method constraint k De k k  X  1 is replaced by k D k 2  X  To explain the modification we set  X  = 1. Part (i) of Lemma 11 is easily verified. The union bound over a union bound over the 2 TK extreme points of the  X  task) and obtain the bound P t  X   X  ( x t ) TKS  X   X  ( X ), leading to Proceeding as above we obtain the excess risk bound to replace the bound in Theorem 1 . The factor in the second term seems quite weak, but it must be borne in mind that the constraint k D k 2  X  weaker than k De k k  X  1, and allows for a smaller approximation error. If we retain k De k k  X  1 and  X 
K in the second term disappears and by comparison to Theorem 1 there is only and additional ln T and k  X  ( 1 ) is possible for the modified method. B.2. Learning to learn  X  , which governs the generation of a training sample in the environment E . On a given training sample empirical risk The algorithm A D , essentially being the Lasso, has means that we only really need to estimate the ex-pected empirical risk E z  X   X  tasks. On the other hand the minimization problem ( 1 ) can be written as min not too large this should be similar to E z  X   X  In the sequel we make this precise.
 Lemma 13. For v  X  H with k v k  X  1 and x  X  H m let F be the random variable Then (i) E F  X  inequality, Theorem 3 (i).
 x  X  H m we have E max r m  X   X  ( x ) for  X   X  0 E max k F k from Lemma 13 (i) and a union bound, the third in-a well known approximation. The conclusion follows from substitution of  X  = Proposition 15. Let S E := E least 1  X   X  in the multisample Z  X   X  T E Proof. Following our strategy we write (abbreviating  X  =  X  E ) and proceed by bounding each of the two terms in turn.
 For any fixed dictionary D and any measure  X  on Z we have  X   X   X   X   X  2 L X  This gives the bound
E Replacing  X  by  X   X  , taking the expectation as  X   X  E right hand side of ( 10 ).
 We proceed to bound the second term. From Corollary 6 and Lemma 8 we get that with probability at least 1  X   X  in Z  X  (  X  E ) T where  X  t is an orthogaussian sequence. Define two Gaussian processes  X  and  X  indexed by D K as and where the  X  ijk are also orthogaussian. Then for D =  X   X  L 2  X   X  = = E ( X  D So by Slepian X  X  Lemma = E sup = =  X   X   X  in the draw of the multi sample Z  X   X  T sion.
 in the definition of R opt , so that R four terms, By definition of  X  R we have for every  X  that E The term ( 18 ) above is therefore non-positive. By Hoeffding X  X  inequality the term ( 17 ) is less than p ity at least 1  X   X / 2 that R
