 Online evaluation methods for information retrieval use implicit sig-nals such as clicks from users to infer preferences between rankers. A highly sensitive way of inferring these preferences is through in-terleaved comparisons. Recently, interleaved comparisons methods that allow for simultaneous evaluation of more than two rankers have been introduced. These so-called multileaving methods are even more sensitive than their interleaving counterparts. Probabilis-tic interleaving X  X hose main selling point is the potential for reuse of historical data X  X as no multileaving counterpart yet. We propose probabilistic multileave and empirically show that it is highly sen-sitive and unbiased. An important implication of this result is that historical interactions with multileaved comparisons can be reused, allowing for ranker comparisons that need much less user interaction data. Furthermore, we show that our method, as opposed to earlier sensitive multileaving methods, scales well when the number of rankers increases.
 H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Information retrieval; Evaluation; Interleaved comparisons; Multi-leaved comparisons
Search engines are constantly being improved upon. Engineers may propose changes to a ranker and such changes need be evalu-ated. Evaluation should not only compare alternative rankers against the current best ranker, but also against each other as these compar-isons are important for guiding ranker development efforts. Pair-wise preferences between rankers can be obtained in several ways. Traditional offline metrics based on relevance assessments can be computed [2] in a TREC-style evaluation setting [20].

Besides being expensive and time consuming, assessors X  judg-ments do not necessarily agree with actual users X  preferences [ 16 ]. In contrast, online metrics infer preferences directly from user in-teractions [ 12 ]. Interleaved comparison methods do so while be-ing one to two orders of magnitude more sensitive than A/B test-ing [ 1 , 9 , 10 , 15 ]. An interleaving method presents a user with a ranked result list that contains documents from two rankers and estimates user preferences by interpreting interactions with the inter-leaved list. Many variations of interleaving methods exist. Balanced interleave (BI) [ 11 ] randomly selects a ranker to start with and then, alternatingly, documents that have not been picked are picked from the two input rankings. BI can result in a preference for a ranker irrespective of where a user clicks. This bias is corrected in team draft interleave (TDI) [ 15 ]. More recent methods include document constraints (DC) [ 4 ] and optimized interleave (OI) [ 14 ]. Recently, interleaving outcomes were made to agree with A/B tests [19].
Interleaving methods have an important disadvantage: they can only evaluate two rankers at a time. When the number of rankers that have to be compared increases, this quickly becomes infeasible. Schuth et al. [18] have proposed multileaved comparison methods as a way of directly inferring preferences among a set of rankers. Instead of requiring rankers to be compared pairwise, their method allows for a single comparison of multiple rankers at once. Mul-tileaved comparison methods produce a list that contains results from multiple rankers and use clicks on this list to infer a ranking over rankers. Schuth et al. [18] propose extensions of TDI and OI to team draft multileave (TDM) and optimized multileave (OM), respectively. Both learn preferences much faster than interleaving methods. Each has its own merit: TDM learns slightly slower but results in more accurate preferences while OM initially learns faster and scales better when the number of compared rankers goes up.
The interleaving and multileaving methods listed so far can only infer preferences among rankers based on interaction data that is generated using those very same rankers, thus preventing general-izations to new and unseen rankers. Probabilistic interleave (PI), however, is a recent interleaving method that can reuse historical interaction data collected using other rankers than the ranker being evaluated to infer preferences between rankers [ 6 , 7 ]. This allows for comparisons that are more efficient in terms of how many user impressions are required for reliable comparisons.

We propose an extension of PI to probabilistic multileave (PM): a multileaving method that is able to reuse historical interaction data. An evaluation method is sensitive if it quickly detects differences in the quality of rankings. That is, if rankers are of different quality, the evaluation method should detect those differences with the least number of comparisons. An evaluation method is unbiased if all the rankers are equal in expectation when clicks are random. That is, the method should evaluate rankers fairly and only on the basis of their actual performance. We show experimentally: (1) that PM is at least as sensitive as TDM (which in turn is more sensitive than OM); (2) that PM is unbiased; and (3) that PM scales well when the number rankers that are compared increases. An important implication of our results is that historical interactions with multileaved comparisons Algorithm 1 Creating a multileaved list in PM. 1: Input : rankers R , size k . 2: L  X  [] // initialize interleaved list 3: while True do 4: R 0  X  X  5: while |R 0 | &gt; 0 do 6: R j  X  draw from uniform distribution over R 0 7: R 0  X  X  0 \{R j } 8: d  X  draw from R j with P ( d |R j ) // see (1) 9: L.append ( d ) 10: if | L | = k then 11: return L 12: for R j  X  X  do 13: R j .remove ( d ) // renormalize P ( d |R j ) can be reused, allowing for ranker comparisons that need much less user interaction data. This, in turn, means that users are exposed less often to inferior rankers and that more rankers can be compared with the same number of user interactions.
In this section we derive probabilistic multileave (PM), a multi-leaving version of probabilistic interleaving (PI). Extending PI to multiple rankers is non-trivial due to three challenges. First, the interleaving method, which combines multiple rankings to one list, must be extended to accommodate more than two rankers in a prob-abilistic way; see Section 2.1. Secondly, we need to represent the outcomes of a multileaved comparison such that it allows for more than two rankers. Lastly, we need to extend the marginalization over all assignments 1 to the multileave setting without increasing the number of possible assignments beyond what we can reasonably handle.
Probabilistic interleaving [ 6 ] constructs interleaved lists in a way similar to TDI. For each rank in the interleaved list a coin is flipped to decide which ranker assigns the next document. Instead of picking the top document from that ranking, like in TDI, the document is drawn from a softmax distribution. After a document has been chosen it is removed from all rankings it occurs in and all softmax distributions are renormalized. An extension of this approach to the setting with r rankers can be found in Algorithm 1. As in TDM, and unlike PI, we choose to introduce the notion of rounds on line 5: in each round all rankers are chosen at random to contribute a document. This way we increase the change of the multileaved list to include documents from the top of each ranker that is to be compared while ensuring non-zero probability for all possible multileavings.
 Documents d are drawn from ranker R j with probability where r j ( d ) denotes the rank of the document in ranker R documents are removed from R j , as is done on line 13, this changes the distribution. The softmax decay is controlled by  X  which we set to 3 , following PI [6].
Once the multileaved list has been created, shown to a user, and once the user has clicked on zero or more documents, these clicks can be interpreted as preferences for rankers.
Following TDM and PI, documents in a multileaved list are as-signed to a rankers X  teams in order to distribute credit from clicks. Algorithm 2 Inferring preferences in PM. 1: Input : multileaved list L , rankers R , clicks C . 2: A  X  X } // assignment tree keeps track of outcome and probabilities 3: A 0  X  X  X  o  X  [0 ,..., 0] ,p  X  0  X  X  // init assignment tree, | o | = |R| 4: for d  X  L do 5: A  X  A 0 ,A 0  X  X } // next layer in assignment tree 6: for R j  X  X  do 7: p j  X  P ( d |R j ) // see (1) 8: R j .remove ( d ) // renormalize P ( d |R j ) 9: for  X  o,p  X  X  X  A do 10: for R j  X  X  do 12: continue // sample, skip branch 13: p 0  X  p + log( p j 2 ) // log probability of assigning d to R 14: o 0  X  o // copy outcome vector from parent 15: if d  X  C then 16: o 0 j  X  o 0 j + 1 // click on d , increment outcome for R 17: A 0  X  A 0  X  X  X  o 0 ,p 0  X  X  // append to next layer 18: o  X  [0 ,..., 0] // init outcome, | o | = |R| 19: for  X  o 0 ,p 0  X  X  X  A 0 do 20: o  X  o + o 0  X  e p 0 // aggregate outcome vector o weighted with e 21: return o
Hofmann et al. [6] proposes an unbiased estimator of the ex-pected outcome of a comparison outcome over many such query impressions as Here, Q denotes a given set of queries, with clicks c signments a q . In the PI implementation, o ( c,a )  X  { X  1 , 0 , 1 } is used to denote the outcome of a comparison, which we change to o ( c,a )  X  N to denote the outcome (credit) for a single ranker R This outcome simply counts how many clicked documents were assigned to this ranker, as in TDI or TDM. Changing the outcome notation leads us to also compute the expected outcome per ranker R The PI algorithm then proposes a marginalization over all possible assignments. The intuition behind why this is a good idea for both PI and PM is that through a single interaction with users, inferences can be performed as if the R rankers were multileaved many more times. This allows for highly sensitive comparisons.

We follow the marginalization of PI closely, and marginalize over all possible assignments as
E [ o j ( C,A )]  X  1 | Q | X In this equation, P ( a q | l q ,q ) denotes the probability of an assign-ment a q occurring given the interleaved list L q for query q . We borrow this probability directly from the PI implementation.
A major difference with PI, however, is that we can no longer consider all possible assignments A as there are generally too many of them, namely |R| | L | , which, even with a small amount of rankers, is prohibitively large. Instead, we limit the number of assignments by taking an, in expectation, random and uniform sample from them. We denote the sample as  X  A , and control the size n  X  X  of assignments in a tree of assignments not. This sampling happens on line 11 in Algorithm 2 which lists our method for inferring preferences in PM. The algorithm builds a tree of assignments by considering assigning each document d  X  L (line 4) to all possible rankers R j  X  R (line 10) with the probability mentioned before (line 11). The algorithm keeps track of the outcome for each ranker for each assignment branch (line 16). It also tracks the probability of each such assignment (line 13). Once the outcomes o j have been computed for all assignments a q  X  A 0 , we aggregate them weighted with their assigned probabilities on line 20 into a single outcome. From this outcome we construct the preference matrix  X  P ij just as is done in TDM.

Note that in our implementation, just as in PI, we use log proba-bilities and sums to avoid buffer underflow. Also following PI, as an optmization, we only actually compute assignments and proba-bilities up to the lowest clicks.
Our implementation of probabilistic multileaving, PM, is evalu-ated experimentally for sensitivity, bias, and scalability using the experimental setup detailed below.

The methodology used to evaluate the different interleaving and multileaving methods is borrowed from [ 18 ]. Several features (i.e., BM25, LMIR.JM, Sitemap, PageRank, HITS, and TF.IDF) from the NP2003 and NP2004 dataset [ 13 ] are used as different rankers. These rankers can then be evaluated against each other using the aforementioned interleaving and multileaving methods. Evaluation is done by simulating users interacting with the interleaving and multileaving methods. In this setup, described by Hofmann [5] , clicks are produced based on a cascade click model (CCM) [ 3 ] with four instantiations.

Relevance R R = 0 R = 1 R = 0 R = 1 perfect 0.0 1.0 0.0 0.0 navigational 0.05 0.95 0.2 0.9 informational 0.4 0.9 0.1 0.5 random 0.5 0.5 0.0 0.0
Queries are sampled from training data, clicks are generated based on relevance labels for each query. We use 1000 queries per run. Results are averaged over 25 repetitions over 5 folds for 2 datasets. We sample |R| = 5 features as rankers for each run.

Ground truth NDCG [ 8 ] is computed for each ranker on held out test data. Then, just as in [ 18 ], a matrix P is defined using the difference in expected NDCG between the separate rankers: Any interleaving or multileaving method tries to estimate this matrix P . Interleaving methods do this by iterating over all pairs of rankers i,j  X  R , while multileaving methods compare all rankers R at once. To measure performance, we follow [ 18 ] and use a binary error metric to evaluate the correctness of this estimated matrix After every simulated query this error can be computed for each interleaving and multileaving method. 2 We test significance with a two tailed student-t test.
 To evaluate the sensitivity of PM, it is compared to TDM and PI. A multileaving method should be more sensitive than an interleaving
All code is open source and available at https://bitbucket. org/ilps/lerot [17].
E
E
E Figur e 1: The binary error E bin is plotted against the number of queries on which the error was evaluated for interleaving and multileaving methods. Clicks are generated by a perfect , navigational , and informational instantiations of CCM [3]. method as it can compare more than two rankers at the same time. For this evaluation, we use the perfect , navigational , and informa-tional instantiations. It is hypothesized that multileaving will need fewer impressions to converge than interleaving.

To assess the bias of all of the interleaving and multileaving methods mentioned above, we observe the error under a random clicking user. The error, in this case, is not based on ground truth NDCG matrix P ij as in (5) but instead on a ground truth P for all pairs of rankers i,j .

Lastly, we look into scaling up the number of rankers. The experiments listed above are all run using |R| = 5 rankers; we also investigate what happens when |R| = 20 for the navigational click model and with the sample size of PM fixed to n = 10 4 . In Figure 1 we see that for impressions from all three click models, TDM performs similar to PM with a large number of samples n , although TDM performs slightly (but not significantly) better in the long run for the (unrealistic) perfect click model. The multileaving methods both perform much better than PI, as expected since the latter can only compare two rankers at a time. 3
The binary error for both probabilistic multileaving quickly goes down to almost zero. Performance after 500 queries for the same experiments is found in Table 1 where we also perform statistical significance testing. In the table we see that PM always has a
The bump for PI in the first few query impressions, that is mostly visible under informational feedback, is due to the fact that it takes a while for PI to have preferences for all pairs of rankers. Table 1: Mean E bin scores after 500 impressions for PM and SPM compared to baselines PI (first symbol) and TDM (second symbol). The symbol N means statistically better with p &lt; 0 . 01 and M for p &lt; 0 . 05 , whereas H and O are their inverses. PI 0.085 (0.08) 0.137 (0.11) 0.363 (0.15) TDM 0.037 (0.06) 0.038 (0.05) 0.099 (0.09)
E Figure 2: The error is plotted against the number of queries. The error was evaluated by comparing to a ground truth of no preferences (i.e., P ij = 0 . 5 for all i,j ). Clicks are generated by a random instantiation of CCM [3]. lower error than PI and when there are enough samples ( n  X  100 ) statistically significantly so. However, when the number of samples is too small, PM is outperformed significantly by TDM. When the number of samples increases sufficiently, PM is on par with TDM in terms of sensitivity. Interestingly, when noise increases, performance of PM decreases less compared to TDM.
 In terms of bias, we see in Figure 2 that PM is on par with TDM. Both methods only need about 100 impressions from a random user to conclude that no preferences between rankers can be inferred. Again naturally, PI needs much more query impressions to draw the same conclusion because it needs to compare all pairs of rankers. PM is as unbiased as TDM, irrespective of the number of samples.
Lastly, we investigate what happens when the number of rankers |R| that are being compared increases from the five rankers used until now. We test this with |R| = 20 and find that after 500 navigational query impressions for PI, E bin = 0 . 56 , for TDM E bin = 0 . 15 , and for PM( n = 10 4 ) we find E bin = 0 . 13 . The advantage of multileaving over interleaving is clearly shown by these numbers. But moreover, PM clearly outperforms TDM when the number of rankers increases. We confirm a finding from Schuth et al. [18] who showed this to be an inherent disadvantage of TDM as it needs to represent al rankers with teams in the multileaving. PM, because it marginalizes over all possible team assignments, does not have this drawback and still performs well when the number of rankers goes up.
We have introduced a new method for online ranker evaluation called probabilistic multileave (PM). PM extends probabilistic inter-leave (PI) such that it can compare more than two rankers at once, while keeping PI X  X  characteristic of being able to reuse historical interaction data. We empirically compared PM to PI as well as an earlier multileaving method called team draft multileave (TDM). The new method infers preferences between rankers by marginaliz-ing over a sample of possible team assignments. We use a sample of controlled size to keep the computation tractable and show ex-perimentally that given a large enough sample, our method is both as sensitive and as unbiased as TDM and more so than PI. That is, PM is capable of quickly finding meaningful differences between rankers and it does not infer preferences where it should not.
An important implication of this work is that historical interac-tions with multileaved comparisons can be reused, allowing for ranker comparisons that need much less user interaction data. Fur-thermore, we show that our method, as opposed to earlier sensitive multileaving methods, scales well when the number of rankers in-creases.

