 Long queries form a difficult, but increasingly important seg-ment for web search engines. Query reduction, a technique for dropping unnecessary query terms from long queries, im-proves performance of ad-hoc retrieval on TREC collections. Also, it has great potential for improving long web queries (upto 25% improvement in NDCG@5). However, query re-duction on the web is hampered by the lack of accurate query performance predictors and the constraints imposed by search engine architectures and ranking algorithms.
In this paper, we present query reduction techniques for long web queries that leverage effective and efficient query performance predictors. We propose three learning formu-lations that combine these predictors to perform automatic query reduction. These formulations enable trading off aver-age improvements for the number of queries impacted, and enable easy integration into the search engine X  X  architecture for rank-time query reduction. Experiments on a large col-lection of long queries issued to a commercial search engine show that the proposed techniques significantly outperform baselines, with more than 12% improvement in NDCG@5 in the impacted set of queries. Extension to the formu-lations such as result interleaving further improves results. We find that the proposed techniques deliver consistent re-trieval gains where it matters most: poorly performing long web queries.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory Query reformulation, Learning to Rank, Combining Searches
Long queries form a sizable fraction of the queries that are submitted to web search engines. Queries of length five words or more have increased at a year over year rate of 10%, while single word queries dropped 3% [1]. Unfortunately, web search engines perform poorly on longer queries when compared to shorter ones [4].

Several past works have focused on improving long query performance [3, 15, 14, 16, 13, 7] (see Section 2). They can be broadly classified into query re-weighting and query reduction approaches. In query re-weighting, the original query terms are assigned different weights before the query is submitted to the search engine. In query reduction, in-stead of the original query, a reduced version of it (obtained by removing one or more terms from the original query) is selected and submitted to the search engine.

Both approaches are motivated by the observation that long queries usually contain superfluous terms, which if down-weighted or completely removed, result in improved perfor-mance. For example, consider the query easter egg hunts in northeast columbus parks and recreation centers , which performs moderately well on most popular web search en-gines. If we remove (or down-weight in some fashion) the terms and recreation centers , we can observe a perceptible improvement in the quality of results.

While query term re-weighting and reduction techniques have shown significant improvements in performance on TREC data [15, 13], their utility in the web environment is not well understood. Further, query reduction/re-weighting on the web poses unique challenges due to the architecture of web search engines, and the extremely low latencies toler-ated. Query re-weighting requires the use of a ranking algo-rithm that permits the online assignment of weights to query terms  X  a difficult thing to implement given that most web retrieval algorithms use a learning to rank framework that relies on boolean match as well as several additional query-dependent and query-independent features. Query reduc-tion requires analyzing a potentially exponential number of reduced versions of the original query, and relies heavily on query quality prediction measures like Clarity that are ex-pensive to compute on the web.

In this paper, we focus on query reduction, and develop techniques that are especially suited for the web. We utilize an efficient, rank-time query quality prediction technique [2], and consider only reduced versions of the original query that are obtained by dropping a single term at a time. As we will show in Section 3.2, dropping just a single (and correct!) http://trec.nist.gov term from the original long query can result in a 26% im-provement in NDCG@5. After formally defining the query reduction problem in Section 3.1, we propose three differ-ent formulations of the problem in Section 3.3. We demon-strate their utility (Section 5) through experiments on a set of approximately 6400 long queries sampled from the query logs of a major web search engine. Additionally, we develop simple thresholding and interleaving extensions to these for-mulations that enable balancing the trade-off between the queries impacted and the improvements attained.

The main contributions of this paper are (1) a rank-time query reduction technique that is well suited for incorpora-tion in a web search engine and works reliably in improving hard long queries (2) formal treatment of the query reduc-tion technique that allows control over when and how to do query reduction, and (3) the first large scale evaluation of query reduction for web queries. Analysis of the exper-imental results show that query reduction achieves consis-tent gains whenever there is high potential for improvement (Section 6). More importantly, we find that query reduction mostly benefits poorly performing queries. In other words, it provides benefits where they are most needed.
Retrieval effectiveness for long queries is often lower than retrieval effectiveness for shorter keyword queries. Kumaran et al. [12] showed that shorter queries extracted from longer user generated queries are more effective for ad-hoc retrieval. Using click positions as surrogates for effectivenessm Ben-dersky and Croft [4] show that long query effectiveness is lower than short keyword queries for web queries. Several works have focused on improving the retrieval performance of long queries [3], [15], [14], [16],[13], and [7]. They can be broadly categorized as automatic term weighting and query reduction approaches.

Weighting -Bendersky and Croft [3] use a key con-cepts classifier to identify important query terms in long queries, and to assign weights to query terms resulting in improved retrieval effectiveness. Also, Bendersky et al. [5] successfully extend this term weighting approach to con-cepts and build a weighted dependence model that performs well for web queries. Lease et al. [15] use Regression Rank, a regression framework to directly learn weights on query terms using query dependent secondary features. An ex-tension to this approach achieves significant improvements over Markov Random Field dependency models on TREC collections [14].

However, as mentioned earlier it is difficult to directly in-corporate term weights in existing web search engine rank-ing algorithms. In contrast, our goal is develop an approach that can be seamlessly incorporated into existing web search engines architectures without requiring modifications to the underlying search algorithms.

Reduction -Kumaran and Carvalho [13] develop an au-tomatic method for reducing long TREC description queries. Using content-based query quality predictors such as Clarity and Mutual Information Gain, they convert the query reduc-tion task into a problem of ranking (reduced) queries based on their predicted effectiveness. Their results on TREC Ro-bust 2004 show the viability of automatic query reduction. In this work, we investigate the extension of this approach to web search engines.

Lee et al. [16] use statistical and linguistic features of query terms to greedily select query terms from the origi-nal long query to achieve the effect of query reduction. Ex-periments on NTCIR collections demonstrate that this ap-proach, and its extension which considers pairs of terms [17] improves long queries X  effectiveness.

Chen et al. [7] use personal query history to select short queries related to the original long query, cluster the short queries based on similarity of their contexts, and select rep-resentatives from each cluster as a substitution for the orig-inal long query. Unfortunately, this approach requires rank-time processing of texts of retrieved documents, extracting noun phrases and other features from sentences that con-tain the query words. These heavy rank-time computations make this approach infeasible for use in web search engines.
In summary, the term weighting, query reduction, and substitution approaches show good potential for improv-ing long query effectiveness. However, term weighting ap-proaches cannot be readily incorporated into web search en-gines, while the features used for the various query reduction approaches are not suitable for efficiency reasons. Further-more, there has been no large scale experiments on actual long web queries that demonstrate the utility of automatic query reduction. In this paper, we investigate the utility of query reduction for long web queries, and develop formula-tions that are well-suited for Web search engines.
Retrieval effectiveness is typically lower for longer queries than for shorter keyword queries, partly because users often utilize extraneous terms in long queries [12]. Query reduc-tion -the technique of automatically identifying and remov-ing extraneous terms from long queries -has proved to be an effective technique for improving performance on long queries [13]. In this section, we present a formal description the query reduction problem.
Let f : P  X  D  X  R denote a ranking function that scores documents (D) with respect to a query P , represented as a set of query terms. Also, let T f ( P ) denote a target measure of the effectiveness of the ranking produced by f for the query P .
 Given an arbitrary query Q = { q 1 ,  X  X  X  , q n } , we use P to denote the power set of Q , i.e., the set containing all subsets of terms from query Q (including the original query Q ). Then, the query reduction problem is to find a reduced version P  X  that achieves the highest value for the target measure as shown in Equation 1. Note that this problem statement allows the original query Q to be selected as well.
Obviously, the target measures cannot be completely spec-ified for inferences over all possible queries, and hence we need to estimate T f ( P ). The query reduction task is then expressed as: Query performance predictors, such as Clarity [8] or Query Scope [9], can be used to obtain estimates of the target mea-sure, c T f in order to select a reduced version P  X  of the original query Q .
Efficiency is a key challenge for query reduction. Because the number of possible reduced queries in P Q is exponen-tially large, enumerating and evaluating all possibilities is not feasible. This challenge is even more important for web search engines, where response times are in the order of mil-liseconds.

To address this issue, we propose a simpler version of the query reduction problem. In particular, instead of consider-ing all possible reduced versions, we only consider those that differ from the original query Q by only one term. That is, instead of using the entire power-set P Q , we use a restricted version P Q 1 = { P | P  X  P Q  X | P |  X  | Q | X  1 } . Thus, if the original query had n query words, we only need to consider the n reduced versions and original query Q .

Despite the obvious limitation of ignoring a large number of potentially better reduced versions, this simple approach can yield dramatic performance improvements. On a large collection of more than 6400 Web queries (see Section 4 for details), we find that an oracle that chooses between an orig-inal long query and its reduced versions achieves more than 10 points gain in NDCG@5.

However, in order to achieve this gain, we need to reli-ably identify reduced versions whose performances are bet-ter than that of the corresponding original queries. To illus-trate the potential impact of this technique, we analyze the distribution of maximum and average gains for reduced ver-sions using Figures 1(a), (b), and (c). Figures 1(a) and (b) show the distribution of gains when compared to the orig-inal query. Figure 1(c) shows distribution of gains (as box plots) for original queries with different NDCG@5 values.
On average, the reduced versions X  effectiveness are worse compared to the original query X  X  effectiveness, as shown by the negative gains dominant in Figure 1(a). Also, the maxi-mum gains, the gains that can be achieved if we always iden-tify the best reduced version, are mostly positive as shown in Figure 1(b). However, for some queries the maximum gains are negative i.e., choosing any reduced version will re-sult in decreased performance. Finally, Figure 1(c) shows that if the original query has poor performance, then it is more likely for some reduced version to be better than the original query. Conversely we are unlikely to find reduced versions of well-performing queries that provide substantial performance gains.

Based on these observations, we develop learning tech-niques that can reliably improve the performance of hard long web queries through query reduction.
We use three formulations for choosing between the orig-inal query and its reduced versions: 1) Independent perfor-mance prediction, 2) Difference prediction, and 3) Rank-ing queries. All three formulations use the same perfor-mance predictors to generate features but differ in their tar-get learning functions. For the remainder of this paper, we assume that the same ranking algorithm, f , is used to re-trieve results for both the original query and its reduced versions and hence drop it from our notations.

Let Q be the set of training queries and let T ( Q ) be the effectiveness of their retrieved results.
Given an original long query and its reduced versions, we predict the performance of each query independently. Then, we select the query that has the highest predicted perfor-mance. Thus, the query selection problem is transformed into a query performance prediction task: Given a query, and the retrieved results, the task is to predict the effective-ness of the retrieved results.

Formally, given a set of functions h : P Q  X  R , we learn a non-linear regressor h  X  that minimizes the mean squared error as given by:
For a given test query Q t , we select the query P  X  with the largest predicted performance, i.e.:
While the Independent formulation is relatively simple, it does not encode the relationship between the original query and its reduced versions. Furthermore, based on the ob-servations from Figure 1(c), it may be more important to predict the difference in performance between the original query and its reduced versions, than to accurately predict the effectiveness of the individual queries.
In the Difference formulation, we predict the difference in performance between each reduced version and its original query, and then select the query that has the highest positive difference. If there is no reduced version with a predicted positive difference, then we choose the original query.
Let D ( Q, P ) = T ( P )  X  T ( Q ), denote the target measure difference between a reduced version P and its original query Q . Given a set of functions h d : Q  X  Q  X  R , we learn a least-squared-errors regressor h  X  d given by: h  X  d = arg min
For a given test query, Q t , we choose a reduced represen-tation P  X  as:
In this formulation, the goal is to rank the original query and its reduced versions in order to select the top ranking query. The ranking model is learned by training on pairwise preferences between the queries.

For each reduced version P  X  P Q 1 , P is preferred over Q if T ( P )  X  T ( Q ). The pairwise preferences induce a par-tial ordering and the query at the top of the ordering is se-lected. This formulation fully encodes dependencies between the original query, and all the reduced versions. Kumaran and Carvalho [13] successfully use this learning to rank ap-proach to select only amongst reduced versions of the query on TREC collections. In this work, we also include the orig-inal query in addition to the reduced versions.

Let  X  denote the error function for incorrect pairwise or-dering defined as follows:  X  ( Q, P ) =
We want to learn a function h  X  r from the set of ranking functions h r : Q  X  R such that it minimizes the overall ranking errors, i.e.,:
For a given test query, Q t , we choose a reduced represen-tation P  X  as:
As an extension to these formulations, we also learn a threshold on the assigned scores in order to control the number of queries for which reduced versions are selected. In Independent , a reduced version is selected if and only if, there exists a reduced version whose predicted perfor-mance is greater than that of the original query by a spec-ified threshold. For Difference , the positive difference has to exceed a threshold in order to choose a reduced version. Finally, for Ranking , the predicted performance of the top-ranking reduced version must exceed the original query X  X  predicted performance by the specified threshold.

For all three formulations, we also learn the best threshold values by selecting the values that achieve the best improve-ments over the training set of queries.
Accurate prediction of performance of the original query and reduced versions is critical to the success of the query reduction formulations we have described in this Section. We leverage a rank-time technique for query performance prediction [2]. The key idea behind this technique is to uti-lize retrieval scores, and the features that a ranker uses for ranking documents to estimate the quality of the results. Ta-ble 1 lists the two broad types of features used for ranking documents as well as predicting query quality  X  those that characterize the prior probability of query effectiveness, and those that characterize the quality of the retrieved results. In Independent the features are used as is, while in Differ-ence and Ranking the difference in feature values between the original and the reduced versions are used.

Query Features : We use several query-specific features to provide a richer representation of the query. Lexical fea-tures flagging the presence of URL, stop words, and numbers as well as location features that denote the presence of town, city, or state names are part of the feature set. Addition-ally, we use query length as a feature. Query length has a strong negative correlation with performance i.e., retrieval effectiveness degrades as query length increases.

Query-Document Features : The retrieval scores as-signed by the ranking algorithm are indicative of the rel-evance of individual documents, and aggregates of these scores are useful predictors of the relevance of the retrieved results. Web search engines often combine multiple rank-time features to score documents. These rank-time features provide different types of evidence for the relevance of the documents. Some useful features include query-independent features similar to page-rank, query-log based features such as variations of click-through counts, and term match-based features such as BM25F.

Using these query and document-related features, we train a regressor to directly predict the performance (NDCG@5) of queries.
We conduct experiments to evaluate query reduction and the utility of the different learning formulations. We use LambdaRank [6] as our web ranking algorithm. LambdaRank has been shown to be an effective learning to rank technique for the web and can handle a large num-ber of features. We target NDCG@5 [10] as the metric to maximize. We use the top 5 results to create the query-document features, and select the top 100 features that are most correlated with NDCG@5 in the training data. We evaluate the query reduction approach and the utility of the different formulations on a large collection of Web queries. To create the query reduction collection, we first obtain a frequency-weighted random sample 2 of more than 6400 long queries issued to a major Web search engine. For each query in this collection, we also create reduced versions by drop-ping a single word each time. We use the LambdaRank retrieval algorithm to rank documents for both the original
The frequency-weighted sampling ensures the chosen queries are a representative mix of the types of long queries. long query and all its reduced versions. Then, we obtain rel-evance judgments for both the results for the original query and the reduced versions from a large pool of independent annotators. The reduced versions results are also judged with respect to the original long query, and not with respect to the reduced one. We refer to this collection as Long Web henceforth.

Learning Algorithms. For Independent and Difference formulation, the goal of learning is to find real-valued func-tions that predicts the target metric, and the differences in the target metric, respectively. For both formulations, we use non-linear regression with the Random Forests [18] al-gorithm 3 , to predict performance of queries and performance differences respectively. For the Ranking formulation, we use RankSVM [11] 4 to learn pair-wise preferences. For all three problem formulations we perform five-fold cross validation for training and evaluating the learning models.
For each formulation, we report results for two types of ex-periments. In Query Replacement experiments, if a reduced version is selected, it is used to replace the original query. In Results Interleaving if a reduced version is selected, the results of the selected reduced version and the original query are interleaved. The interleave order depended on the sign of the difference between their predicted NDCG@5.
Table 2 shows the performance of the different problem formulations (top half) along with the thresholding exten-sion (bottom half) for the Query Replacement task. We compare the formulations using two measures: 1) Overall NDCG@5, which is the macro-averaged NDCG@5 over all queries, and 2) Subset NDCG gain, the average improve-ments on the subset for which reduced versions were chosen.
With no thresholding, Difference achieves the best overall gain. Ranking  X  X  overall gain is lower but the subset gain is substantially higher. While, Difference and Ranking both achieve small but significant overall gains, Independent is actually worse than the original. We believe that this per-formance difference is due to two reasons. First, Differ-ence and Ranking encode the relationship between the orig-inal query and its reduced versions, whereas Independent
We used the randomForest package available from R with default parameters
We used the SVMLight implementation with default pa-rameters and a linear kernel does not capture such relationships. Second, the Indepen-dent formulation appears to solve a harder learning problem. The regression in Independent attempts to minimize mean-squared errors of the predicted and actual NDCG@5 values. The accuracy of the predicted values matter only in terms of the ordering they induce, the difference between the ab-solute values are not important. In fact, we find that the root-mean squared errors for Independent  X  X  regression was 10% worse compared to that of Difference  X  X  regression, even though both regressions are learned using the same learning algorithm, the same set of base features, and same training data.

With thresholding however, Independent improves dra-matically and its overall performance is comparable to Dif-ference . On the other hand, Difference and Ranking do not achieve any additional improvements to the overall measures due to thresholding. Independent benefits from threshold-ing as it selects fewer reduced versions which are more likely to yield improvements. Despite the similar average perfor-mance (over the entire set of queries), the three formulations provide different types of improvements. First, Independent and Ranking select reduced versions for less than 10% of the queries, whereas Difference selects reduced versions for more than 27% of the queries. The number of selected reduced versions depends upon the accuracy of the predicted scores, and the thresholds learnt during training.
 Figure 2: Number of queries affected versus im-provements in subset gains.

To better understand the relationship between the num-ber of queries affected versus the average improvement in performance, we explore the behavior of the different for-mulations at various thresholds. In general, we expect in-creasing thresholds to cause fewer reduced versions to be chosen (lower recall), but to also increase the likelihood of over the baseline ( p &lt; 0 . 05 on a two-tailed paired t-test). improving over the original query (higher precision). Fig-ure 2 shows this precision-recall trade-off in terms of the gains achieved on the subset of affected queries against the percentage of queries affected. As expected for all three for-mulations, the average performance on the subset is drasti-cally high for a small percentage of queries, and performance decreases as more queries are affected. Independent and Dif-ference achieve more than 10 points absolute improvement over the original queries on a subset of 5% of the queries. Compared to Independent and Difference , Ranking achieves smaller gains but retains its performance over a large frac-tion of queries. This suggests that Ranking is able to choose reduced versions effectively for more number of queries but it may not always choose the best reduced version.

Feature Importance. The contribution of the different feature groups for Difference are shown in Table 3. Most of the gains come from the query-document features. Adding the query features provides small improvements. As ex-pected features that characterize the result set directly are more useful than the query-features . Also, adding estimates of original query X  X  effectiveness further improves performance. This is because reduced versions are more likely to improve poorly performing original queries and adding estimates helps Difference to encode this relationship.
 Table 3: Feature importance for Difference : Orig  X  Overal Gain 38.19 38.52 38.63 38.73
Subset Gain 0 1.30 1.61 2.05
To reduce the risk involved in choosing between queries, we conduct interleaving experiments. In all three formula-tions, if a reduced version is selected, we interleave its re-sults with the results of the original query. Furthermore, we decide the order of interleaving based on the predicted per-formance. If the original query X  X  predicted performance was higher than that of the reduced version then interleaving begins with the original query, and vice-versa otherwise 5
Table 4 shows the gains achieved by the interleaving re-sults. Difference achieves the best overall gains, whereas
Because interleaving combines results from the original query and the top-ranked reduced version, it can yield gains even in cases where the top-ranked reduced version X  X  pre-dicted performance is lower than that of the original query. Table 4: Results Interleaving at the best thresholds for the three formulations. For the NDCG Gain row, bold face indicates the highest value, and * indicates statistically significant improvement over original NDCG@5 ( p &lt; 0 . 05 on a paired t-test). Independent achieves the best subset gains. Difference and Ranking both have a positive impact on a large number of queries, 31% and 25% respectively, whereas Independent provides positive gains for only 4%. We hypothesize that one of the reasons that Difference and Ranking achieve higher performance compared to Independent is because Differ-ence and Ranking achieve better ranking of reduced ver-sions compared to Independent , thus allowing more queries to benefit from interleaving.
 Figures 3(a), (b), and (c), show Results Interleaving and Query Replacement performance at different thresholds for all formulations. For all formulations, Results Interleav-ing performs better compared to Query Replacement at all thresholds. Since interleaving ensures that at least some of the original query X  X  results are mixed in with the chosen reduced versions results, we reduce the risk of hurting per-formance in case of erroneous choices. However, interleaving also benefits from the fusion of results from the original long query, and the reduced version in the case of good choices. Although not shown here, an analysis of the gains obtained by Query Replacement, and Results Interleaving for Inde-pendent formulation shows that there are fewer queries with large positive gains for Results Interleaving compared to Query Replacement, but there are also fewer negative gains. For example, nearly 10% of positive gains in Query Replace-ment are above 25 points, whereas only 5% of positive gains in Results Interleaving are above 25 points. On the other hand, nearly 20% of the negative gains in Query Replace-ment are below 20 points, whereas less than 5% of the neg-ative gains in Results Interleaving are below 20 points.
We further analyze the results to better understand the distribution of gains and the nature of the improvements.
Query reduction results in dramatic gains on some subset of queries but also incurs losses on some queries. For Query Replacement using Independent formulation nearly 60% of this impacted queries were improved. Further, for 20% of these queries, the gain was more than 50 points in absolute NDCG. This indicates the dramatic improvements attain-able through query reduction. For the remaining 40%, a large proportion of their losses are small, which explains the overall improvements. We observe similar trends for Differ-ence and Ranking .

Potential versus Achieved Gains. All three formula-tions provide improvements when there is a large potential for improvement. Figure 4 shows the distribution of the gains achieved by Independent in relation to the best gains that an oracle can achieve. In most cases when the potential for improvement is large, Independent formulation achieves larger improvements. Also, When the potential is greater than 0.8, Independent always results in some positive im-provement. We believe that the large gains achieved by the formulations are primarily due to two factors.
 Figure 4: Oracle versus Achieved Gains: Boxplot show-
First, the formulations are able to detect large differences in NDCG@5 more reliably. The distribution of absolute prediction errors for the regression used by Independent is shown in Figure 5(a). The histogram shows the frequencies of absolute difference between the predicted NDCG@5 and the acutal NDCG@5 values for all queries (including reduced versions). Most prediction errors are smaller, and very few errors are larger than 50 points (less than 3%). This suggests that smaller differences in NDCG@5 are harder to capture given the range of prediction errors, while larger differences can be captured more reliably.

Second, for queries with large gains, the problem of choos-ing a reduced version becomes easier. Figure 5(b) shows the distribution of number of reduced versions that are better than the original query. The number of better reduced ver-sions correlates with the maximum achievable gain (shown in the x-axis). For queries with high potential for improve-ment (i.e., queries with high maximum gain), the number of better reduced versions is higher, which makes the problem of finding better reduced versions easier. (a) NDCG@5 Prediction Errors: Frequency histogram of (b) Boxplot showing distribution of reduced versions that Figure 5: Prediction accuracy and difficulty of choosing
Improving Poorly Performing Queries. As illus-trated in Figure 1, poorly performing original queries of-ten have large potential for improvements. Since all for-mulations deliver improvements when there is large poten-tial, most gains are achieved for poorly performing original queries. Figure 6(a) shows the histograms for the number of queries that achieve positive gains against the effectiveness of the original query. Clearly, most of the gains are achieved for queries whose original NDCG@5 low. Nearly 75% of the queries that benefit from Independent are queries with NDCG@5  X  40. Further, the magnitude of the gains for the poorly performing queries are higher than for well perform-ing queries as shown in Figure 6(b). Thus, unlike traditional techniques such as pseudo-relevance feedback, query reduc-tion delivers improvements where it matters most. (a) Number of queries at each effectiveness level which achieved positive gains. (b) Boxplot showing magnitude of achieved gains for original queries of different effectiveness levels. Figure 6: Performance of Independent for original
As the average length of queries users submit to search en-gines increases, long query retrieval becomes an increasingly important problem. In this paper we have addressed some of the key challenges in adapting query reduction for long web queries  X  a particularly difficult task due to the inherent constraints imposed by modern search engine architectures and operational requirements.

We presented three learning formulations that naturally provide different trade-offs in terms of number of queries affected versus the overall average gains achieved by query reduction. Such flexibility is valuable when designing large scale systems, where memory and processing limitations may significantly impact when and whether a query should be al-tered.

We also provided the first comprehensive evaluation on a large collection of real long web queries. Our experi-ments showed that directly predicting performance differ-ences generally outperforms independent performance pre-dictions. Also, performance of the proposed formulations can be improved even further by interleaving results from the original and reduced versions. A careful analysis of the results clearly showed that, unlike traditional techniques such as pseudo-relevance feedback, our query reduction tech-niques achieved most NDCG gains on difficult long queries.
The na  X   X ve approximation to the full scale (exponential) query reduction problem substantially improves efficiency (exponential to linear), while still providing significant ef-fectiveness gains. However, even evaluating a linear num-ber of additional queries can be burdensome for search en-gines. Also, despite improved average performance, we find that there is high variance in performance. As part of fu-ture work, we aim to further improve efficiency and reduce variance using a two-staged approach that first predicts the effectiveness of the original long query to decide when to evaluate the reduced versions.
This work was supported in part by the Center for In-telligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed here are the au-thors X  and do not necessarily reflect those of the sponsor. The authors also thank the anonymous reviewers for their helpful comments and suggestions.
