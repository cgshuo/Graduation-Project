 The common practice of testing a sequence of text classifiers learned on a growing training set, and stopping when a tar-get value of estimated effectiveness is first met, introduces a sequential testing bias. In settings where the effectiveness of a text classifier must be certified (perhaps to a court of law), this bias may be unacceptable. The choice of when to stop training is made even more complex when, as is common, the annotation of training and test data must be paid for from a common budget: each new labeled training example is a lost test example. Drawing on ideas from statistical power analysis, we present a framework for joint minimization of training and test annotation that maintains the statistical validity of effectiveness estimates, and yields a natural defi-nition of an optimal allocation of annotations to training and test data. We identify the development of allocation poli-cies that can approximate this optimum as a central ques-tion for research. We then develop simulation-based power analysis methods for van Rijsbergen X  X  F-measure, and in-corporate them in four baseline allocation policies which we study empirically. In support of our studies, we develop a new analytic approximation of confidence intervals for the F-measure that is of independent interest.
 H.3.4 [ Information Storage and Retrieval ]: Systems and software X  performance evaluation .
 E-discovery; supervised learning; text categorization; evalu-ation University of Maryland.

As text classifiers trained by machine learning have be-come common in practical settings, their effectiveness has become of interest not just to experimentalists, but to practi-tioners, decision makers, and, increasingly, the courts. This process has been accelerated in e-discovery (electronic dis-covery, i.e. finding digital evidence in legal matters) [20]. The growth of content that organizations must review, to select documents for delivery to opposing parties in civil law-suits, or to government regulators, has increased interest in information retrieval in general, and supervised learning in particular [12, 20, 21]. The need to provide statistically valid estimates of classifier effectiveness is not, however, unique to e-discovery, but can be found in other areas where classifier results are subject to controversy, such as text classification in evidence-based medicine [4] and microarray classification in cancer diagnosis [24].

A classifier can be tested on data that is randomly sam-pled from the population of interest, labeled, and not used in training (a  X  X eld out X  certification test set). Combined with an appropriate estimator, this approach produces effective-ness estimates that are valid and unbiased. 1 When classifiers are produced by supervised learning, a common approach is to create a succession of classifiers from training sets of in-creasing size, stopping this process when the effectiveness estimate on the test set reaches some desired level. 2 Unfor-tunately, using the certification test set to decide when to stop growing the training set introduces a sequential testing bias [30]. The reason is that effectiveness estimated on the test set fluctuates above and below true effectiveness over the sequence of classifiers, and the stopping rule is more likely to terminate training at an overestimate than at an underestimate.

We begin in Section 2 by reviewing past work on classifier evaluation and test set minimization. The contributions of the paper then are 1) a framework that avoids sequential bias in certifying classifiers, while allowing the practitioner to use arbitrary learning methods and termination policies (Section 3); 2) a method, based on ideas from statistical power analysis, for estimating the probability that a classi-
T hroughout this paper, we use bias in the statistical sense, to mean that the expected value of an estimator is not equal to the population value being estimated.
Several products for e-discovery and data mining make this approach (too) easy by displaying a graph showing how ef-fectiveness estimated on the test set is changing as additional training data is labeled. fier will pass a certification test on an unseen test set of a s pecified size (Section 4); 3) experimental results on three baseline termination policies, exhibiting both the potential annotation savings possible and the challenges in develop-ing good policies (Sections 5 and 6); and 4) a new analytic approximation for confidence intervals on van Rijsbergen X  X  F-measure that was used in our experiments and is of inde-pendent interest (Appendix A). Section 7 summarizes our contributions and points to future work.
In this section we review techniques for estimating classi-fier effectiveness and choosing test set sizes. We also discuss methods for minimizing the use of labeled data in evaluating classifiers.
In typical text classification evaluations, a classifier is built and applied to a test set. Its predictions on the test set are compared to known labels, and some measure of classifier effectiveness is computed. If the test set is a random sample from the population of documents to be classified, effective-ness on the test set is a point estimate of effectiveness on the population [18, Ch. VII].

Point estimates have uncertainty resulting from the finite random test set. We can make this uncertainty explicit by producing an estimate in the form of an interval. A confi-dence interval [  X  1 ,  X  2 ] on a parameter x consists of lower and upper values for the parameter [18, Ch. VIII]. Any method for producing confidence intervals from a random sample has an associated confidence level , traditionally written 1  X   X  . The confidence level is the probability that, if many random samples of the same size were drawn in the same way from the population, and a confidence interval calculated from each, we would expect at least 1  X   X  of those intervals to contain the true value for the parameter.

A practitioner often must do more than produce an es-timate of effectiveness. They must decide whether there is sufficient evidence that the actual effectiveness, x , exceeds some minimum acceptable value,  X  . Such evidence can be provided by a one-tailed hypothesis test [18, Ch. IX]. We pose the null hypothesis that x =  X  (our classifier is poor) and the alternative hypothesis x &gt;  X  (our classifier is good enough). 3 The practitioner hopes to see evidence supporting the rejection of the null hypothesis.

The goals of both estimating effectiveness and becoming confident it exceeds a minimum value can be combined by estimating a lower one-sided confidence interval (LOSCI), i.e. a confidence interval whose upper limit is the logical maximum of the quantity being estimated (typically 1 . 0 for effectiveness measures) [15, 3]. Finding a 1  X   X  LOSCI [  X , 1 . 0] implies that for all  X  &lt;  X  a one-tailed hypothesis test would reject the null hypothesis x =  X  at the  X  significance level.
Many text classifiers are themselves produced from la-beled examples by applying supervised learning. Reusing the training data for testing such a classifier would produce an optimistic estimate of effectiveness, but using a held out
I ntuitively, the null hypothesis is x  X   X  , but for test pur-poses the null hypothesis is the single point value most dif-ficult to reject. (separate) sample makes unbiased estimates possible, given an appropriate estimator [27]. For a fixed labeling budget, one must then choose how to allocate labeled examples be-tween training and testing.

A common policy is to specify a fixed test set propor-tion , with typical splits ranging from 50% training/50% test, to 80% training/20% test. An alternate policy used in e-discovery is fixed test set size , which allocates a specified number of labeled examples for testing, and whatever is left for training. In a third policy, fixed training set size , the classifier is first built, then any remaining documents are dedicated to testing. All these strategies are valid when used to choose the size of both training and test sets prior to using the test set. However, if examples are incremen-tally allocated to the training or test set based on test set effectiveness estimates, all three policies produce biased es-timates of effectiveness [30].

The cost of held out test sets has led to many alternative approaches to evaluation. Training set bounds are based on analyzing the range of classifiers allowed by a particu-lar learning algorithm, and how the algorithm searches that space [14]. Except when training sets are very small, how-ever, even a small held out test set provides tighter con-fidence intervals than do training set bounds based on all labeled data [13, 14].

Cross-validation is a resampling method that partitions the labeled data into subsets, using each in turn as the test set for a classifier trained on the union of the remaining sub-sets. Intervals produced by cross-validation reflect a biased variance estimate and are not valid frequentist confidence intervals [1]. Other resampling methods, such as bootstrap-ping, jackknifing, repeated random sampling, and repeated independent design and test, similarly produce biased esti-mates of variance [6, 19, 31]. Further, these methods do not evaluate the actual classifier deployed (typically one trained on all labeled data). In addition, resampling methods make it difficult to compare classifiers produced by machine learn-ing with those produced in other fashions (e.g. manual rule writing).
A single application of a fixed size held out test set, while avoiding the biases discussed above, poses two challenges in our setting. First, the practitioner needs to be confident, without using the held out test set, that the classifier has the desired effectiveness. For this purpose, the bias of cross-validation and related methods (Section 2.2), when small, can be tolerated.

The second and deeper problem is that, to avoid sequen-tial bias, the size of the certification test set must be fixed before it is used. Similar problems arise in designing sur-veys, clinical trials, and so on. How to determine test set size in these situations is the subject of power analysis in statistics [5]. The power , 1  X   X  , of a statistical experiment is the probability that the null hypothesis will be rejected when it is in fact false. All things being equal, the larger the test set, the greater the power.
 A critical notion in power analysis is the effect size [5]. In our context, the effect size is the amount by which the classifier X  X  effectiveness exceeds the target effectiveness. If true effectiveness greatly exceeds the target, this can be demonstrated with a relatively small certification test set. Conversely, if true effectiveness is barely above the target, a large certification test set will be needed to show this.
Formulae for determining sample sizes given the effect size, desired significance level, and desired power are avail-able for many standard statistics [5]. Unfortunately, van Rijsbergen X  X  F-measure is not one of them, so in this paper we use computationally intensive simulation methods to find the sample size with a given power.
The research reviewed in the previous section presents a dilemma. Many strategies have been proposed to allocate training and test data in a way that reduces the total number of annotations. However, the strategies that produce valid, unbiased effectiveness estimates (training set bounds and their variants) do not save many annotations. Conversely, the strategies that substantially reduce annotations (cross-validation, sequential allocation) lead to biased estimates.
However, there is more room for maneuver here than it first appears. In Section 3.1 we present a new framework for supervised learning which allows for aggressive minimization of annotations while preserving unbiased classifier evalua-tion. We then introduce in Section 3.2 the notion of policies for the allocation of annotations in this framework, and how to evaluate such policies.
We propose that classifiers requiring certification be de-veloped as follows:
Step 1 Examples are selected, annotated, and used in training
Step 2 The practitioner chooses the number of examples to be
Step 3 The classifier is applied to the certification test set, The goal in our framework is to produce a classifier that has a specified probability of passing the certification test, while minimizing the total cost of annotating training and test data.

This framework is subtly but importantly different from common practice in e-discovery and related applications of
A common consideration in experiment design is what con-stitutes a meaningful effect size. As a practical matter, one may not care whether drug A is shown to be better than drug B unless drug A is quite a bit better. E-discovery is different, in that missing an agreed effectiveness target by any amount may require expensive remediation. In such a setting, effect size is of interest only indirectly, through its impact on sample size.
If the population is logically infinite, then there is no dis-tinction between drawing from the original population and drawing from the unannotated examples. Effectiveness es-timates are conventional estimates of generalization to that infinite population. We assume a logically infinite popula-tion in this paper, but return to the case of a finite popula-tion in our discussion of future work. supervised learning. First, the test set is used only once (Step 3), after the practitioner has committed to a final clas-sifier. This frees the practitioner in Step 1 to use any tech-niques they like (manual classifier creation, online learning, active learning, resampling methods, sequential stopping, etc.) for training, for (non-official) effectiveness estimation, and for deciding when to end training. Any biases in these techniques may affect how often the classifier passes certi-fication (Section 6), and may affect the annotation budget used, but cannot affect the validity of the certification test. From the standpoint of certification, classifier creation oc-curs within a black box, and actions taken  X  X ithin the box X  are irrelevant to certification.

Second, our framework postpones the choice of certifica-tion test set size until immediately before certification. This reflects a key insight of power analysis, which is that the op-timal test set size depends on effect size. In the supervised learning setting, this translates to the observation that we cannot optimally choose test set size without having some estimate (even if biased) of the effectiveness of the classifier to be evaluated.

Third, our framework focuses on minimizing the joint cost of training and test data annotation. This perspective is unusual in e-discovery, and in most applications of machine learning. Yet it appears the right one for e-discovery at least, where both types of annotation may require an attorney billing at US $400-$800/hour or more.
Our framework allows wide latitude for minimizing anno-tation, but does not itself tell a practitioner how to allocate annotations to training vs. testing. Further, in exchange for ensuring the certification test is valid, our framework ex-poses the practitioner to the risk that certification will fail, possibly requiring expensive remediation. We therefore pro-pose that the central objects of study within our framework are allocation policies that provide guidance to the practi-tioner on allocating annotations. The notion of a policy is taken from reinforcement learning [11] where it refers to a rule that maps states to actions in a sequential decision set-ting while attempting to achieve or maximize some goal.
In our framework, the state of the system is the training data observed so far (and the classifiers trained from it). The actions available to the policy are to continue training or, alternatively, stop and choose a certification test set size. The two goals against which we evaluate a policy are: 1. Achieving a power level specified by the practitioner. 2. Minimizing the cost of annotation. How does the av-
Two observations provide guidance for policy design. The first is the observation from power analysis that larger ef-fect sizes allow smaller test set sizes. In our framework this means that the greater the amount by which the ef-fectiveness of a classifier exceeds the certification target, the smaller the certification test set that will be necessary to d emonstrate that fact.

The second is the observation that learning curves (the plot of classifier effectiveness vs. amount of training data) climb steeply at first and then level off [22, 30]. Thus each new training example provides less of an increase in effec-tiveness than the previous one.
 This leads to a natural criterion for terminating training. After the j th allocation of a group of training examples, the practitioner should estimate the effectiveness of a clas-sifier trained on the r j training examples seen so far. They should then do a power analysis to compute the size, s j , of the test set that will, with desired power (probability of success) 1  X   X  , show that the classifier has met the target ef-fectiveness. The sum r j + s j = c j would be the total budget for annotation if training were stopped at that time.
The leveling off of the learning curve means that while this total budget decreases at first, it inevitably reaches a minimum from which it starts to increase. If the policy is operating under a fixed budget, b , it might choose to stop training the first time that c j falls under that budget, or might continue training in hopes of further declines in total budget. There are three possible outcomes that the policy may experience: Whether policies should actually be allowed to terminate in a fixed budget setting without attempting certification is a design choice. In a practical setting one probably wants the policy to take its best shot.

Figure 1 shows an idealized view of how the total anno-tation budget necessary to meet an effectiveness target  X  (with significance level 1  X   X  and power 1  X   X  ) varies with the number of training examples labeled. Early in training the true classifier effectiveness is below the target value, so no amount of test data would let the classifier pass certifi-cation at the desired significance level.

Later (assuming the classification task is tractable given the selected threshold), the true classifier effectiveness hits the threshold effectiveness  X  , but barely, so that a large test set, and thus a large combined annotation budget would be necessary. At this point each additional training example re-duces the necessary test set size by more than one example. The minimum budget occurs after more training, when the classifier X  X  actual effectiveness is well over the target value, and new training examples stop paying for themselves in test set size reduction. Eventually, a limit on classifier ef-fectiveness may be reached, after which additional training data gives no benefit, and the cost c i increases linearly with r .
Deciding when to stop training is more challenging than the idealized Figure 1 suggests. First, there is no standard formula to go from an effect size for F 1 to a test set size that
Total Annotation Budget Figure 1: An idealized depiction of annotation budget mini-mization. The increasing curve plots the unknown true value of F 1 (right y-axis) against number of training examples (x-axis). The certification test can first be passed with high confidence when F 1 &gt;  X  . Additional training data improves F 1 and allows smaller test set sizes, decreasing total annota-tion budget (left y-axis). Eventually effectiveness reaches a maximum and total annotation cost increases linearly with training set size. achieves a given power. We therefore in Section 4.2 develop a simulation method for this purpose. Second, a policy does not get to observe the true effectiveness of any classifier pro-duced during training. We propose in Section 4.4 the use of cross-validation to estimate that effectiveness.

Third, a policy does not have access to the entire curve of annotation budgets for different training set sizes. Instead, it observes only the portion of the curve for training set sizes produced so far, and must make a decision online [2]. Further, it does not observe a smooth budget curve based on monotonically increasing true effectiveness (Figure 1). Instead, it sees a scattering of budgets derived from noisy and possibly biased estimates of a true effectiveness that may itself not increase monotonically. After labeling each increment of training data, the policy must decide, based on that noisy history, whether it should take the budget it can achieve at this point, or press on in hopes of doing better. Finally, the very process of making such a decision is itself susceptible to a sequential testing bias.

Figure 2 illustrates the challenges a policy faces. We show a single run made on one RCV1-v2 topic (Section 5.1), in-creasing training sets by 20 at each iteration. The method described in Section 4 is used to estimate the required certi-fication test set size after each iteration. Early on, estimated effectiveness is below the target effectiveness, so no certifi-cation test set size allows meeting the significance level and power requirements. Later, the estimated effectiveness does exceed the target, enabling the estimation of a test set size and thus a combined budget. To create Figure 2 we draw a single certification test set of the specified size, and check whether the classifier would have passed (green plus sign) or failed (red dot) the certification test with that test set.
In Section 6 we study some baseline policies that have partial success at choosing a stopping point. Note, however, that inadequacy in policies affects only the probability of passing certification and the annotation cost to do so. Our
Total Annotation Budget F igure 2: Annotations are added to the box with increments of 20 documents. A green plus sign (red dot) indicates a suc-cess (failure) in predicting that the threshold will be reached. Cyan triangles are situations in which the threshold is un-achievable no matter what the certification budget is, since the point estimate on F 1 is itself below the threshold. (The striations in the data are the result of the computational shortcut of using nested sets in cross-validation.) framework ensures that the validity of certification is unaf-fected.
Our framework requires a policy to make two decisions: when to stop training, and how big a certification test set to label. In this section we address the second of these de-cisions.

We assume throughout that we have two classes, relevant and nonrelevant, and a binary classifier with two outputs, predicted relevant (which we call  X  X etrieved X ) or predicted nonrelevant ( X  X nretrieved X ). The inputs (Section 2) to the algorithm are the current training set, the effectiveness tar-get  X  , the confidence level 1  X   X  , and the desired test power 1  X   X  . Our algorithm works as follows:
Step 1 For a randomly-sampled training set of r annotations,
Step 2 Treat M as if it were a sample from an infinite pop-
Step 3 Call Algorithm 2 (Section 4.2) with inputs P ( P|M ), Algorithm 1 E stimateT hetaStar input: A distribution P ( P ) over infinite population confu-1: initialize N  X  1000,  X   X  X } 2: for i  X  [1 , N ] do 3: Draw an infinite population confusion matrix P from 4: Simulate a random sample S of size s from P . 5:  X   X   X   X   X   X  ( S ) 6: return  X   X   X  Quantile ( X  ,  X  )
Step 3.1 Call Algorithm 1 (Section 4.1) with inputs P ( P |M ),
Step 3.2 If and only if  X   X   X   X  then the test set size is at or
Section 6 tests some baseline stopping policies which make use of this algorithm.
Our first algorithm (Algorithm 1) takes as inputs P ( P ) a distribution over infinite population confusion matrices, along with the definition of an effectiveness measure ( F this study), a required significance level 1  X   X  and power 1  X   X  , and a test set size s . The algorithm estimates the minimum effectiveness,  X   X  , that would fall inside at least a fraction 1  X   X  of the 1  X   X  LOSCIs [  X , 1] produced from samples of size s from populations drawn from P ( P ). This  X   X  is equal to the maximum target effectiveness  X  for which a one-tailed hypothesis test with null hypothesis x =  X  (and alternative hypothesis x &gt;  X  ) would have power 1  X   X  on a sample of size s .

We find  X   X  by Monte Carlo simulation. Algorithm 1 sim-ulates simple random samples from the infinite population confusion matrix P and calculates a 1  X   X  LOSCI [  X , 1] for each. It uses the  X  quantile of the  X  values as  X   X  .
Algorithm 1 finds the maximum target effectiveness for which we can achieve a specified power, given a known cer-tification test set size. What we actually wish to know, how-ever, is the minimum certification test set size which yields a specified power, given a known target. Using Algorithm 1 as a subroutine, our Algorithm 2 solves the latter problem by a binary search of possible test set sizes.

Algorithm 2 takes the same inputs as Algorithm 1 except that a known test set size s is replaced by a known target effectiveness  X  . Algorithm 2 first checks that  X  F 1 , the point estimate on F 1 , is greater than the target effectiveness  X  . If not, no certification test set size can give the required confidence level and power.

If we do have  X  F 1 &gt;  X  , we first find an upper bound by doubling candidate sizes until the first size u is found where Algorithm 2 E stimate a Certification Test-Set Size input: c onfusion matrix M , threshold  X  , output: certification test-set size 1: if F 1 ( M ) &lt;  X  then 2: return +  X  3: else 4: initialize  X   X  0 . 01  X   X  , Upper bound u  X  1 5: while EstimateT hetaStar ( M , u,  X ,  X  ) &lt;  X  do 6: u  X  2  X  u 7: for Size s  X  [ u/ 2 , u ] selected with binary search do 8: if  X   X  EstimT hetaStar ( M , s,  X ,  X  )  X   X  +  X  then 9: return s Algorithm 1 returns  X   X  s uch that  X   X  &gt;  X  . This corresponds to a test set size that would give power greater than 1  X   X  .
We then do a binary search of test set sizes in the interval [ u/ 2 , u ]. Algorithm 1 is invoked on each candidate test set size. The search stops when a certification test set size is found with  X   X  such that  X  +  X   X   X   X   X   X  . The certification test set size at that point is returned.

There is a potential sequential sampling bias in this binary search. However, since Algorithm 1 does not directly use labeled data, this bias can be made arbitrarily small. We can simply increase the number of Monte Carlo simulations N in Algorithm 1, particularly for candidate test set sizes giving  X   X  close to the target  X  .
Algorithm 2 determines the certification test set size given a known distribution P ( P ) over classifier behaviors on an infinite population of interest. In practice, what we typically have is a finite confusion matrix for the classifier on some sample from the population. To use Algorithm 2 we must use this sample to derive a distribution on infinite population confusion matrices.
 We do this by deriving posterior beta distributions from Jeffreys priors upon the retrieved and unretrieved segments of the population as determined by the classifier [29]. The posterior distributions capture how likely various classifier behaviors are, given the behavior observed on the sample.
Consider the retrieved segment. Let the number of true positives in the sample be n 11 , and the number of false pos-itives be n 10 . Then the beta posterior R  X   X  Beta(0 . 5 + n 11 , 0 . 5 + n 10 ) is taken as a posterior upon the population proportion relevant in the retrieved segment. A similar beta posterior is derived on the unretrieved segment.

Finally, we assume that the proportion of the population retrieved by the classifier is known exactly. 6 The two beta posteriors, along with the proportion of the population re-trieved, completely specify a posterior distribution P ( P ) on infinite population confusion matrices.

We validate this method by randomly sampling n = 100 contingency tables empirically observed on our experimen-tal data (Section 5). Treating these as infinite populations, we draw m = 100 samples from each, and estimate the certification set size required for thresholds of  X  of f =
I n our experiments we assume this value is equal to the proportion retrieved in the sample. If a more precise value is needed, the classifier can be applied to all unlabeled data. Table 1: Validation of Algorithm 2 with 1  X   X  = 0 . 93.
Mean(  X 
F  X   X 
F ) F igure 3: Performance predicted by cross-validation versus actual performance of classifier on a subset of the (remain-ing) population of a size estimated by Algorithm 2. and 1  X   X  = 0 . 93). Then, we observe the proportion of times that  X  exceeds  X  on a sample of this size drawn from the notional true population. We expect this proportion to be 1  X   X  (subject to random variability, and preferring to be slightly above than slightly below). Table 1 shows the results of our validation; our inference method is accurate at predicting the sample size required to pass certification.
Section 4.3 provides a method for deriving a posterior dis-tribution over classifier behaviors (infinite population con-fusion matrices) from a sample. A held-out sample would be ideal, but expensive. An attractive alternative is to em-ploy n -fold cross-validation (Section 2.2), since this thriftily allows all annotations to be used both for training and for estimation.

Cross-validation produces a summary confusion matrix from across the n folds. Our approach is to use this as the sample required in Section 4.3. Since the cross-validated classifiers are trained on ( n  X  1) /n as much training data as the classifier whose effectiveness we wish to certify, cross-validation will generally understate true classifier effective-ness. Figure 3 plots the difference between a point estimate of classifier effectiveness as estimated by cross-validation, and true performance of the classifier as measured on a very large labeled set. For small training sets, the gap is substan-tial, with the maximum understatement of performance be-ing around 0 . 08 for F 1 . However, as the training set size in-creases, the understatement of performance declines quickly, until by around 3 , 000 training documents, cross-validation gives an unbiased average estimate of performance (although individual estimates may still be high or low).
Ou r experiments use an SVM classifier trained and tested on subsets of the RCV1-v2 test collection, for which we wish to certify a desired level of effectiveness. Our experimental data is the RCV1-v2 test collection of Reuters newswire stories [17]. The collection contains 804 , 414 documents, and their assignments to 103 topic categories. To aid replicability, we used the RCV1-v2 token files dis-tributed as On-Line Appendix 12 [17]. The token files are converted to SVM perf format using a modified version of the prep_rcv1 program developed by Bottou. 7 The feature value of each word in a document is a TF x IDF weight [23]. We set the TF (term frequency) component to zero if a word does not appear in a document, and to one plus the natural log of the number of occurrences of the word in the document if it does. We set the IDF (inverse document frequency) component to the natural log of 804 , 414 divided by the number of documents the word occurs in. A total of 288 , 062 distinct stemmed words occur in the token files, so feature vectors nominally are of length 288 , 062, but with only an average of  X  77 . 5 nonzero values per vector.
Binary topic assignments are taken from the Appendix 8 transaction files [17]. These specify 2 , 606 , 875 assignments of topics to the 804 , 414 documents (a mean of 3 . 24 cate-gories per document). We use only the 29 topics with 25 , 000 or more positive examples among the 804 , 414. This allows exploring a range of annotation budgets, while ensuring a reasonable number of positive examples will occur in ran-dom samples. The range of prevalences we use, from 3.1% to 47.4%, captures typical values encountered in e-discovery.
Since our focus is evaluation, we wished to use an exist-ing, widely accepted classification approach. Support vector machines (SVMs) with linear kernels give good effectiveness across a range of text classification tasks [7, 32], so we take that approach to training a classifier. However, most SVM implementations attempt to produce a classifier minimizing error rate, rather than maximizing F 1 . We therefore use the SVM perf (Version 3.00) package [9, 8, 10], which can optimize for F 1 (among other effectiveness measures). For training, we run svm_perf_learn with flags  X  -c 1000 -l 1 -w 3  X , and for applying the classifier to a certification test set we use svm_perf_classify without flags.
We use F 1 , the balanced harmonic mean of recall and precision [28], as our effectiveness measure for binary classi-fiers. The F 1 measure is defined as (2  X  tp ) / (2  X  tp + fp + fn ), where tp is the number of true positives on the test set, fp is the number of false positives, and fn is the number of false negatives.

We use a confidence level of 1  X   X  = 0 . 95 for one-sided confidence intervals and so equivalently a significance level of  X  = 0 . 05 for the corresponding one-tailed hypothesis test. We assume the user desires a test power of 1  X   X  = 0 . 93 when choosing the certification test size. 8 leon.bottou.org/_media/projec ts/sgd-2.0.tar.gz
A power level of 0.80 or lower is common in experimental science, but seems too low in an applied setting. A power
Total Annotation Budget F igure 4: Policies that seek to minimize the total annota-tion budget. The baseline stops when the cost falls within budget. The oracles stop at the lowest possible annotation budget, with the  X  X olor-blind X  oracle being subject to a pos-sible failure (i.e.  X  &lt;  X  ). The  X  X ait w  X  policy waits until observing w stopping points that are strictly below the total budget ( b = 10 , 000 annotations).

In practice  X  , the threshold on effectiveness, would be cho-sen based on the needs of a particular application. For ex-perimental purposes we set  X  for each topic to be 0.9 times the maximum value of F 1 observed on a single training run for that topic on training set sizes up to 10 , 000. This pro-vides a target that is usually, but not always, reachable for arbitrary training sets of size 10 , 000.

We use the certification test set to produce an estimate of the classifier X  X  F 1 in the form of a 1  X   X  = 95% lower one-sided confidence interval [  X , 1 . 0]. To do this, we first produce a point estimate,  X  F 1 , which is simply the value of F 1 on the certification test set. The value of  X  is then found by assuming a normal distribution on  X  F 1 , and analytically deriving Var(  X  F 1 ) using the method of propagation of error (see Appendix A).

We define the requirement for passing the certification test on a particular run to be rejecting the null hypothesis that F 1 =  X  (and thus implicitly rejecting F 1 &lt;  X  as well). This corresponds to  X  for the topic falling within the interval [  X , 1 . 0] generated on that run.

More accurate, but computationally expensive, methods are known for producing confidence intervals on F 1 [30, 29]. Our goal here, however, is to study policies for passing cer-tification tests. Certification tests in practice will often be computationally simple (perhaps using even less accurate es-timates than we use here). In any case, we believe policy design will be relatively insensitive to test design, though this is an area for further study.
We study the behavior of three simple policies to illus-trate the issues arising in designing an efficient and reliable stopping policy under our framework. of 0.95 seems natural, but we use 0.93 here to avoid the c onfusion of too many 0.95 X  X  in our exposition.
I n the first policy, the developer has a fixed total annota-tion budget b . The policy stops and proceeds to certification when the total cost first falls within this budget; that is, when the training cost r plus the predicted required certifi-cation test set size s first sums to no more than the budget b . For this experiment, we set b = 10 , 000. In 29.48% of cases, this budget is never achieved; that is, there is no point for which r + s  X  b . (That is not necessarily to say that there is no way of splitting the budget between training and certifi-cation that would pass certification; but if there is one, our policy has failed to find it.)
Among the 70.52% of cases where the required certifica-tion test set size does fall within budget (and recall that we stop and proceed to certification as soon as it does), the success rate is 79.46%, versus our specified power of 93%. Why does the power not reach our specified level? The rea-son is sequential testing bias [30], now pushed into process management. Our cross-validation estimates of classifier ef-fectiveness are scattered above and below the true effective-ness, due both to sampling variance and the fact that inside the policy we use cross-validation rather than directly eval-uating a classifier trained from the entire training set. The first estimate that falls within budget is more likely to be an overestimate of classifier effectiveness than an underesti-mate.
The second policy is to stop at the lowest total cost pro-jected by the inference method. In terms of Figure 4, this means picking the lowest dot in the figure, whether green (success) or red (failure). (We refer to this as a X  X lind X  X racle because it ignores whether the certification itself is a failure or a success.) This (pseudo-)policy is not implementable in practice, as the classifier developer is not able to go back-wards in time (retrospectively shrink the training allocation to the point that gives the lowest total budget). Instead this pseudo-policy sets a form of lower bound on achievable cost.
Compared to opening the box the first time the total cost falls below 10 , 000 annotations, and ignoring runs for which our algorithm fails to find a split of annotations less than this maximum budget (29.48% out of the 580 runs), the blind oracle policy saves on average 43.21% of the total annotation budget (training and certification included). However, the blind oracle fails the certification step 27 . 14% of the time, against the nominal failure rate of  X  = 7% X  X  higher failure rate even than the 20 . 54% failure rate of stopping at the first within-budget estimate. Again, we are suffering from a sequential sampling bias, where by picking a minimum value, we are much more likely to pick an estimate that is randomly optimistic rather than randomly pessimistic.
If we instead choose the (even less realistic) sighted ora-cle policy, which always picks the lowest green dot (lowest total cost that passes the certification), then the saving over the stop-when-cost-falls-within-budget policy is on average 38.08% (ignoring the failed runs). In principle, the maxi-mum achievable average saving falls somewhere in between the blind and sighted oracles, since our choice of  X  allows for a 7% failure rate. Naturally, the saving achievable for particular topics varies, depending on rate at which classi-fier effectiveness improves (and, of course, the nominal fixed budget against which we are comparing).

Savings in Annotation (%) F igure 5: Savings in annotations if we defer opening the box w times, as opposed to opening at the first time the cost falls within budget.

Success Rate (%) F igure 6: Success rate over 580 runs when the wait-a-while family of policies decide to stop.
The third simple policy, or rather family of policies, builds upon the policy of stopping when a total budget is first achievable (Section 6.1). Instead of stopping immediately, however, this policy waits until a certain number w of fur-ther stopping points that are below total budget are ob-served, where w is a parameter chosen by the user. Note that this policy involves an additional risk X  X t may be that there are not w additional stopping points below budget, in which case the policy fails to proceed to certification at all.
Figure 5 shows the average saving in annotations from adopting the wait-a-while policy, compared to stopping the first time the total cost is at or under the budget of 10 , 000. On average, it is optimal to wait for 25 to 50 iteration cy-cles (of 20 documents added to training for each iteration), though again there will be considerable variability between
W e limit our training sets to 10,000 for computational rea-sons; we therefore do not treat cases that exceed this as failures.
Cannot Open (%) F igure 7: Failure to open the box over 580 runs because the threshold  X  is not reachable, or because the policy has missed the last training set for which the box could have been opened. different topics. Figure 6 shows the success rate of the wait-a-while policy. As we move further from the first time we hit the budget, the sequential sampling bias fades, until even-tually we are more or less hitting nominal success rate of 93%. However, these achievements come at a cost, as Fig-ure 7 shows: the longer we wait, the more likely we are to have missed the chance of achieving our desired budget altogether. (In fact, the increased savings with additional waiting observed in Figure 5 are likely exaggerated by the exclusion of the more difficult runs and topics by this cen-soring effect.)
Failure to achieve required classifier effectiveness is usu-ally handled by training the learner on additional annota-tions and then re-testing the resulting model. This approach unfortunately introduces statistical bias. In this paper we proposed an algorithm that, for a confidence level, indicates if and when a classifier built on a growing training set can achieve a threshold, and how many testing documents will be required to certify the trained model. This method can be integrated as a subroutine for policies that aim to solve a more complex problem X  X inimizing the total annotation budget spread over training and certification sets. We pro-vided a framework in which such policies can be studied and evaluated. Our experiments suggest that there is a room for saving about 40% in annotation budget without compromis-ing the success rate of reaching the targeted effectiveness.
Our study examined only simple random sampling for pro-ducing both training and test sets. Simple random sampling is widely used in e-discovery (where its simplicity is reassur-ing in an adversarial context), and in other applications of supervised learning. However, other approaches, in partic-ular active learning for training [16] and stratified random sampling [26] for testing, are of interest for their potential to further reduce the annotation budget.

We have also confined ourselves here to the traditional generalization framework for studying classifier effectiveness. In e-discovery and some other applied settings, we are ac-tually dealing with a finite population of examples, so that each example annotated is an example to which the classi-fier need not be applied. This finite population annotation framework provides additional opportunities for minimizing annotations.
 [1] Y. Bengio and Y. Grandvalet. No unbiased estimator [2] A. Borodin and R. El-Yaniv. Online Computation and [3] T. T. Cai. One-sided confidence intervals in discrete [4] A. M. Cohen, W. R. Hersh, K. Peterson, and P.-Y. [5] J. Cohen. Statistical Power Analysis for the [6] A. Isaksson, M. Wallman, H. G  X  oransson, and M. G. [7] T. Joachims. Text categorization with support vector [8] T. Joachims. A support vector method for multivariate [9] T. Joachims. Training linear svms in linear time. In [10] T. Joachims and C.-N. J. Yu. Sparse kernel svms via [11] L. P. Kaelbling, M. L. Littman, and A. W. Moore. [12] A. Kershaw and J. Howie. eDiscovery institute survey [13] J. Langford. Combining train set and test set bounds. [14] J. Langford. Tutorial on practical prediction theory [15] E. L. Lehmann and J. P. Romano. Testing Statistical [16] D. D. Lewis and W. A. Gale. A sequential algorithm [17] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [18] A. M. Mood, F. A. Graybill, and D. C. Boes.
 [19] C. Nadeau and Y. Bengio. Inference for the [20] D. W. Oard, J. R. Baron, B. Hedin, D. D. Lewis, and [21] N. M. Pace and L. Zakaras. Where the money goes: [22] F. Provost, D. Jensen, and T. Oates. Efficient [23] G. Salton and C. Buckley. Term-weighting approaches [24] R. Simon, M. D. Radmacher, K. Dobbin, and L. M. [25] J. R. Taylor. Introduction to error analysis . University [26] S. K. Thompson. Sampling . Wiley, 2nd edition, 2002. [27] G. T. Toussaint. Bibliography on estimation of [28] C. J. van Rijsbergen. Information Retrieval . [29] W. Webber. Approximate recall confidence intervals. [30] W. Webber, M. Bagdouri, D. D. Lewis, and D. W. [31] U. Wickenberg-Bolin, H. G  X  oransson, M. Frykn  X  as, [32] Y. Yang and X. Liu. A re-examination of text
The contingency table in Figure 8 describes the population values of the classifier. Then F 1 is: W e draw a simple random sample of size n 1 from the seg-ment of classified relevant documents, and observe r 1 of them to be relevant. Our estimate of R 1 is therefore: a nd similarly for  X  R 0 . Hence, our estimate for F 1 is: Wh at we now require is an expression for d Var(  X  F 1 ). If: then the theory of propagation of error states [25]:
Var( X ) =  X  X  Let X =  X  F 1 , A = b R 1 , and B = b R 0 . Because the classified-relevant and classified-irrelevant strata are independently sampled, Cov AB = 0. It can be shown that: and that:
Plugging Equation (2) and Equation (3) into Equation (1), we find: d
Var(  X  F 1 ) = 4 where: ( ignoring the finite-population adjustment of (1  X  n 1 /N and similarly for d Var( b R 0 ). Equation 4 gives our expression for the sampling variance of F 1 . If we assume that sample F 1 score is approximately normally distributed with equal variance, then a 1  X   X  two-sided confidence interval on F is: where z a is the a  X  X h quantile of the cumulative normal dis-tribution (for instance, z 0 . 975  X  1 . 96 for a 95% confidence interval), and n is the sample size.
 F igure 8: Contingency table of true and false positives and negatives, defined by intersection of the sets classified as relevant ( C ) and actually relevant ( R ).
