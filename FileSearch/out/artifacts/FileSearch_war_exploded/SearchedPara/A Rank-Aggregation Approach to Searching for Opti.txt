 To improve the precision at the very top ranks of a docu-ment list presented in response to a query, researchers sug-gested to exploit information induced from clustering of doc-uments highly ranked by some initial search. We propose a novel model for ranking such ( query-specific )clustersby the presumed percentage of relevant documents that they contain. The model is based on (i) proposing a palette of  X  X itness X  cluster properties that purportedly correlate with this percentage, (ii) devising concrete quantitative mea-sures for these properties, and (iii) ordering the clusters via aggregation of ranki ngs induced by thes e individual mea-sures. Empirical evaluation shows that our model is con-sistently more effective than previously suggested methods in detecting clusters containing a high relevant-document percentage. Furthermore, the precision-at-top-ranks perfor-mance of this model transcends that of standard document-based retrieval, and competes with that of a state-of-the-art document-based retrieval approach.

Users of search engines expect to see the documents per-taining to their queries at the very top ranks of the retrieved results. To address this challenge, some researchers pro-posed to utilize information captured by clusters of the doc-uments most highly ranked by some initial search (a.k.a. query-specific clusters ) [28, 34, 11, 22, 31, 30, 24, 18, 26, 25].
Table 1 illustrates one of the most significant (potential) merits in employing query-specific clustering. The first two rows depict the precision of the top-5 retrieved documents (p@5) performance obtained over three TREC benchmarks [33] by a standard language model approach [27] and by a state-of-the-art retrieval approach, namely, the relevance Table 1: The average percentage of relevant docu-ments in an optimal cluster of 5 documents in com-parison to the p@5 performance obtained by a stan-dard language model (LM) approach and a relevance model (RM3) [1]. model (RM3) [20, 1]. Now, suppose that we use some clus-tering algorithm (for each query) upon the 50 highest-ranked documents (henceforth referred to as the  X  X nitial list X ) by the standard language model approach to produce clusters of 5 documents; the bottom-most row in Table 1 then shows the (average) percentage of relevant documents in the optimal cluster  X  the cluster that contains the highest such per-centage. (Further technical details regarding the retrieval and clustering methods are elaborated in Section 4.) The message rising from Table 1 is clear: if we were able to au-tomatically detect for each query its optimal cluster(s) ,and then position their constituent documents at the top of the returned results, then the resultant retrieval performance would have been substantially better than that of current (state-of-the-art) document-based retrieval approaches.
The conclusion just drawn has long been echoed by re-searchers who employed various clustering techniques to top-retrieved documents [11, 31, 14, 25] following van Rijsber-gen X  X  cluster hypothesis [32]. Nevertheless, there are rela-tively few reports on concrete approaches for automatically detecting optimal (query-specific) clusters [34, 24, 18, 26, 25]. Furthermore, most of these approaches are based on comparing a cluster representation with that of the query, and this turns out to have only limited success [34, 24, 26].
In this paper we propose a general framework for tackling the problem of (automatically) identifying optimal query-specific clusters. The intuitive basic principle underlying our proposal is to
Technically, this evidence-aggregation principle boils down to rank aggregation of rankings induced over the clusters by the individual  X  X itness X  properties. In particular, here we propose four such qualitative properties that we hypothesize to be connected to the percentage of relevant documents in query-specific clusters. All fou r properties are derived from insights  X  most of which are only indirectly related to the task of optimal-cluster detection  X  on individual (and arbi-trary sets of) relevant documents that have been gained in prior work on (i) relevance feedback [29], (ii) cluster-based retrieval [24, 18], and (iii) re-ranking the initial document list using graph-based approaches [17, 7, 18]. We then sug-gest concrete estimation models for quantifying the extent to which a cluster exhibits each of these qualitative proper-ties, and utilize the aggregatio n of the induced estimates for ranking the clusters at hand.

Through an array of experiments conducted over TREC corpora we show that our approach is more effective in de-tecting clusters containing a high relevant-document per-centage than previous (state-of-the-art) methods [24, 18]. Furthermore, posting the constituent documents of the clus-ter most highly ranked by our approach at the top of the retrieved document list yields precision-at-top-ranks perfor-mance that is substantially better than that of standard document-based retrieval; the performance also competes with that of a state-of-the-art document-based retrieval ap-proach, namely, the aforementioned relevance model (RM3) [20, 1].
Let q , d and D denote a query, a document and a corpus of documents respectively. We use D N init (or simply D init denote the set of N most highly-ranked documents by some initial search performed in response to q . We assume that some clustering algorithm was run on D init and produced asetof document clusters C l ( D init )= { c 1 ,  X  X  X  ,c M we wish to rank by the (presumed) percentage of relevant documents that they contain 1 .

Our ranking framework utilizes statistical language mod-els [27, 5]. We use p x (  X  ) to denote a (smoothed) unigram language model induced from x (a query, a document, or a set of documents); language-model induction details are described in Section 4.1. We also make use of Kronecker X  X  delta function: for a predicate s ,  X  [ s ]=1if s is true, and 0 otherwise.

Our cluster-ranking methods utilize information about the relative positioning of documents in ranked lists. For exam-ple, we will be interested in how cluster X  X  constituent docu-ments are ranked when a search is performed upon the entire corpus using a language model induced from the cluster (de-tails below). To that end, we set the following definitions.
Let S X  X  be a set of documents and p x (  X  ) be some (uni-gram) language model. We use the standard language model KL-retrieval approach [19] to rank the documents in S with respect to p x (  X  ) by scoring each document d  X  X  with
We discuss the computational aspect of employing such online clustering in Section 4. where the summation is over all terms w in the vocabu-lary. We also define the schematic function L p x (  X  ) ||S { 1 ,..., |S|} X  X  to provide a strict ranking of S that is consistent with the scores of its documents. (Score ties are broken by document IDs.) For any subset R X  X  , we quan-tify the relative position of R  X  X  documents in the ranking induced by L p x (  X  ) ||S (  X  )over S using the widely used non-interpolated average precision measure with cutoff  X  [33]: For example, if R is the set of relevant documents for query q ,then AP p q (  X  ) ||S ( R ; 1000) is the (non-interpolated) aver-age precision at 1000  X  TREC X  X  widely-used measure  X  obtained by the standard KL-retrieval approach.

As mentioned above, the underlying principle of our pro-posed framework for detecting clusters that contain a high percentage of relevant documents is integration of witness-properties via rank aggregation. Informally, the framework is based on (1) identifying a set of qualitative properties of a cluster (2) committing to concrete cluster-ranking functions for quan-(3) committing to a principle for aggregating the rankings In what follows, we describe our cluster-ranking model in terms of these three components.
We begin with qualitatively describing the four character-istic properties of clusters in C l ( D init ) that we argue to be connected with the percentage of relevant documents that the clusters contain. The qualitative description of each property is then paired with a concrete ranking function that quantitatively measures fulfillment of the property by acluster.
 Query faithfulness ( QF ). Analogously to the case of rank-ing documents in response to a query, one might expect that the more a cluster (as a whole unit) is  X  X imilar X  to the query the higher are the chances that it contains a high percentage of relevant documents. There is evidence, however, that esti-mating (different notions) of cluster-query similarity by the  X  X atch X  between a cluster and a query representation (e.g., their induced language models) is not effective for detecting the desired clusters [34, 24]. Stratifying this estimator, re-cent work on (re-)ranking the initial list D init for obtaining high precision at top ranks using graph-based approaches [17, 7] suggests that a document is likely to be relevant if it is highly similar both to the query and to many documents in D init that are highly similar to the query.

Following this observation we assume that a cluster con-taining a high percentage of relevant documents groups doc-uments that exhibit high similarity to the query 2 .Thishy-pothesis leads us to take a document-mediated approach to assessing the  X  X aithfulness X  (i.e., similarity) of the cluster to the query. Specifically, we score cluster c by the relative posi-tioning of its constituent documents in the ranking induced on the initial list D init by the document-query (language-model) similarity, that is, the smoothing parameter  X  , which we set here and after to +1 , ensures that the score of a cluster is greater than zero. Self faithfulness ( SF ). Work on relevance feedback showed that inducing a  X  X uery model X  from a set of relevant docu-ments and using this model for ranking all documents in the corpus yields highly effective retrieval performance [29]. Now, suppose that cluster c contains only relevant docu-ments; then, we should expect that ranking the corpus based on a query model induced from c will result in relevant doc-uments being positioned high in the ranked list. While at first view this prediction is not very operational, when com-bined with our assumption on the content of c it implies that, in particular, the documents forming c will be posi-tioned high in the ranked list. We therefore post as our second witness property the  X  X aithfulness X  of the ranking in-duced on the corpus by the cluster model to the cluster X  X  constituent documents. To quantify the degree of cluster X  X   X  X elf faithfulness X , each cluster in C l ( D init )isscoredby thought of as reflecting the  X  X orpus-context X  of the query by the virtue of the way it is created. Indeed, this premise is taken by many effective pseudo-feedback-based approaches for automatic query expansion [2, 35]. Furthermore, some recent work has shown that clusters in C l ( D init )thatare similar to many documents in D init  X  and hence can be thought of as reflecting the corpus-context of the query  X  tend to contain a high percentage of relevant documents [18]. Here, we propose to measure the extent to which a cluster exhibits the query X  X  corpus-context by the (relative) situation of the documents from the initial list D init in the ranking induced by the cluster model over the entire corpus , that is, thought of as  X  X eflecting X  different query-related aspects of the corpus that are manifested in the initial list D init 34, 11, 22, 24, 18]. Consequently, clusters that are asso-ciated with many such aspects (while still being relatively focused by the virtue of being  X  X lusters X ) potentially contain a high percentage of relevant documents. This hypothesis leads us to employ the  X  X aithfulness X  of the cluster to the corpus-specific query-related aspects (as manifested in its peer clusters) as yet another witness property of the clusters we look for. To quantitatively assess the  X  X eer-faithfulness X 
We focus on clustering techniques that do not have any explicit knowledge of the query in hand. of cluster c , we score it by the positioning of c  X  X  constituent documents in all rankings induced over the entire corpus by the cluster models of c  X  X  peer clusters C l ( D init ) \{
While we hypothesize that clusters containing a high per-centage of relevant documents are likely to exhibit the four witness properties presented above, obviously,  X  X ikely X  does not mean  X  X lways X . Our suggestion is thus to use these four properties as a set of judges, and compute (via rank-aggregation) a  X  X onsensus X  cluster-ranking aiming at im-proving (on average) over each of the individual cluster-rankings induced by the four properties. Note that, There are a number of reasonable  X  X ggregation functions X  from [0 , 1+  X  ]  X  to [0 , 1+  X  ] that assign a score to a  X  X on-junctive X  combination of  X  fuzzy-scored properties (e.g., see Zimmermann X  X  textbook [41]). To promote clusters that ex-hibit a larger number of properties to high extent, here we adopt the algebraic product aggregation function, and score cluster c with respect to the set of properties  X  by Specifically, the AllProp algorithm that will be of our focus here, uses all four properties for scoring cluster c ,thatis, In Section 4.3 we compare the performance of this aggrega-tion approach to that of some alternatives  X  regular sum and Borda X  X  method.
Clustering the top-retrieved results of a search performed in response to a query has long been proposed as means for improving information delivery [28], especially via the design of cluster-based results interfaces [11, 23, 22, 30]. Rank-ing (hard) clusters in such interactive retrieval systems, was done, for example, based on the highest query-similarity ex-hibited by any of the clusters X  constituent documents [22]. However, this method, which is well-defined only for hard clustering, always ranks first the single cluster that contains the document most similar to the query [30]. In contrast, our QF property leverages information about the query-similarity of all the cluster X  X  constituent documents; further-more, our framework is not committed to any specific type of clustering.

Most previous work on ranking (various types of) clusters has focused on comparing a cluster representation with that of the query [12, 4, 34, 16, 24, 26]. We demonstrate the mer-its of our AllProp algorithm with respect to this approach in Section 4.3.
Some work on re-ranking an initially retrieved list uti-lizes language models of query-specific clusters to smooth document language models [24, 14] so as to improve the document-query similarity estimate. In a related vein, graph-based approaches for re-ranking [7, 17, 18] utilize inter-document similarities information. These approaches can potentially be used to improve our QF estimate, which is based on the document-query  X  X atch X .

A recently suggested decision procedure for employing ei-ther cluster-based or document-based retrieval in response to a query [25] is based on the following observation: clus-ters containing a high-relevant document percentage exhibit (high) query-similarity that does not deviate much from that of the cluster X  X  constituent documents [25]. Cluster central-ity [18, 15], as defined by textual similarity to (many) other (central) clusters or documents, its constituent documents X  centrality [15], and QF [15], were also shown to be poten-tially connected to the percentage of relevant documents in a cluster. We compare our AllProp algorithm with some of these approaches for ranking clusters in Section 4.3. How-ever, integrating these cluster properties with the ones we proposed here remains a challenge for future work.
Unigram language models are the means for producing document rankings in our framework. We now present our methods for inducing a query, a document, and a cluster (unigram) language models  X  p q (  X  ), p d (  X  ), and p spectively.

Let p MLE x ( w ) be the maximum likelihood estimate (MLE) of term w with respect to the text (or text collection) x .The original query q is used in Eq. 3 to rank documents in the initial list. We represent q by p MLE q (  X  ). Documents, on the other hand, are text objects that are being ranked (and not  X  X anked with X ) in our framework. Therefore, we use p (with smoothing parameter  X  ) induced from document d [40] for d  X  X  representation.

To represent cluster c , we adopt an approach previously proposed for pseudo-feedback-based query expansion [39]. Specifically, we assume that each of c  X  X  constituent docu-ments d  X  c is generated by a mixture of the cluster topic model p c (  X  )  X  which we want to estimate  X  with the corpus model. Then, the log-likelihood of the documents in c is
X where tf( w  X  d i ) denotes the number of occurrences of w in d [39].

We find p c (  X  ) by setting  X  to a constant and then apply-ing the EM algorithm [39]. In addition, following common practice in work on query expansion in the language model framework [1, 8], we clip the cluster language model by set-ting p c (  X  )tozeroforallbut  X  terms with the highest p c (Normalization is then performed to yield a valid probabil-ity distribution.) We use p Mix [  X  ;  X  ] c (  X  )todenotetheresul-tant cluster language model. Observe that p Mix [0; ALL (where  X  X LL X  means no term clipping) is the MLE of the big document that results from concatenating c  X  X  constituent documents; this big document served as a basis for cluster representation in some past work [16, 24, 18].
We conducted our experiments on three TREC datasets: These data sets were also used in some previous work on detecting optimal clusters with which we compare our ap-proach [18]. We applied basic tokenization and Porter stem-ming via the Lemur toolkit (www.lemurproject.org), which we also used for producing document rankings. We used the titles of TREC topics as queries.

We define D init , the initially retrieved list upon which clus-referred to as the initial ranking .Thus, D init is composed of the 50 highest-ranked documents by a standard language-model approach 3 .

To produce the set of clusters C l ( D init ), we use a sim-ple nearest-neighbor approach that is known to yield (some) clusters containing a high percentage of relevant documents [14, 18, 25] 4 . In fact, the optimal clusters for which the relevant-document percentage is reported in Table 1 (Sec-tion 1) are such nearest-neighbor clusters. Specifically, for each d  X  X  init , we define a cluster that contains d and its k  X  1 nearest-neighbors in D init as measured by KL ument IDs.) In all the experiments to follow we set k to either 5 or 10.
Given a cluster-ranking method, we measure for each query the percentage of relevant documents in the cluster most highly ranked by the method. We use  X  X @k X  to denote this percentage for cluster of size k , because it is exactly the pre-cision at top k documents that is obtained if the constituent documents of the cluster are positioned at the top of the returned document list. (All th e presented percentages are averages over a given set of queries.) We use the Wilcoxon two sided test at a confidence level of 95% to determine sta-tistically significant differences of p@k performance.
Our goal in the evaluation to follow is to focus on the un-derlying principles of our methods. Therefore, rather than engage in excessive parameter tuning, we fix the following parameter values while realizing that the performance at-tained by our methods is not necessarily the optimal one they can achieve :(i)  X  , the document-cutoff used by our property-quantification approach (see Section 2) is set to 5000, (ii)  X  , the document language model smoothing pa-
To produce an initial ranking of a  X  X easonable X  quality, we set the value of the document-language-model smoothing parameter  X  so as to optimize MAP@1000. Such practice also facilitates the comparison to some previous work on optimal-cluster detection that uses the same approach to create an initial list of 50 documents [18].
Although the clusters are overlapping, we only care about the cluster containing the highest relevant-document per-centage as we discuss later. rameter, is set to 2000 following some previous recommen-dations [40] 5 , and (iii)  X  , the number of terms used for clus-ter representation (see Section 4.1), is set to 50. The single free parameter tuned for our methods is  X  ,thesmoothing parameter that controls cluster representation (see Eq. 8, Section 4.1); we set  X  toavaluein { 0 , 0 . 1 ,..., 0 . 9 optimize p@k for clusters of size k .
 A note on efficiency. We pose our cluster-ranking algo-rithms as means for obtaining high precision at the very top ranks of the retrieved document list. The users of a sys-tem employing our methods should not be concerned with the fact that clustering, or any other computation enhanc-ing the information delivery for that matter, is utilized, as long as the incurred computational overhead is not signif-icant. This is, in fact, the case with our methods. After creating the initial list D init by standard retrieval, its clus-tering can be performed very quickly [38]. (Note that our framework is not committed to any specific clustering al-gorithm; also, recall that the initial list contains only 50 documents.) Furthermore, creating a cluster model takes only a few iterations of the EM algorithm [39]. Finally, using the (no more than 50) cluster models for quantify-ing the properties can be implemented as a single run of a single expanded query that contains the terms used for representing all the clusters. (The total number of unique terms used in the clusters X  models is relatively small since the clusters are query-specific and we are using only  X  =50 terms to represent each cluster.) Similar efficiency consid-erations for using several (query) models were described in some recent reports on predicting query performance and on robust query expansion [36, 3]. We now present the results of our empirical evaluation. First, we compare our AllProp algorithm with a standard document-based retrieval approach, as well as with previously-proposed (state-of-the-art) approaches for detecting optimal clusters. Then, we present a comparison of AllProp with a state-of-the-art pseudo-feedback-based retrieval method. Finally, we perform some in-depth analysis of the marginal contribution of our four cluster properties and the rank ag-gregation method that we employ to the overall effectiveness of AllProp. Table 2 we compare the performance of our AllProp algo-rithm with that of the initial ranking from which D init was derived. Clearly, the perform ance attained by AllProp is substantially better than that of the initial ranking. These results attest to the effectiveness of AllProp in detecting clusters of documents from D init that contain a high per-centage of relevant documents.

Furthermore, AllProp is superior to document-based re-trieval performed over the entire corpus using KL at-top-ranks performance. Case in point, the p@5 perfor-
The only exception is when evaluating the QF property, where we use the same value of  X  that was used for creating the initial list D init . mance numbers of such a p@5-optimized baseline are 56 . 0, 51 . 2, and 46 . 5forWSJ,TREC8andAP,respectively;the p@10 performance numbers of a p@10-optimized baseline are 49 . 4, 46 . 4, and 43 . 9, respectively.
 Table 2: Performance comparison between the All-Prop algorithm and the initial ranking. The best result in each column is boldfaced;  X * X  marks statis-tically significant difference with the initial ranking. detection. A common approach to ranking (various types of) clusters in response to a query is to measure the simi-larity of a cluster (as a whole unit) to the query [12, 4, 34, 16, 24, 26]. (We denote this approach by  X  X QS X .) Specif-ically, CQS was used in the language model framework to rank (hard) query-specific clusters [24]. To compare CQS with our AllProp algorithm, we follow [24] and treat a clus-ter c as the  X  X ig document X  that results from concatenat-ing c  X  X  constituent documents 6 ;wethenrankclustersby KL
In addition, we compare the AllProp algorithm with a recently proposed approach for detecting optimal clusters [18]. The approach is based on defining a one-way bipartite digraph wherein the initial-list ( D init ) documents and the query-specific clusters C l ( D init ) form the two sides of the graph. The edges are from each document d to each of its  X  nearest-neighbor clusters; the nearest-neighbors of d are de-termined by exp serves as a weight function for the edges [18]. (Cluster c is represented as at the above, by the  X  X ig document X  that re-sults from concatenating its constituent documents.) Then, Kleinberg X  X  HITS algorithm [13] is run over the graph, and the clusters are ordered by their induced authority scores; indeed, clusters with high authority scores are shown to con-tain a high percentage of relevant documents [18]. Following [18], we set  X  to values in { 2 , 4 , 9 , 19 , 29 , 39 , 49 timize p@k performance for clusters of size k ;  X  = 2000.
Since the reference comparisons just described utilize a concatenation-based representation for clusters (henceforth denoted with  X  X  X ), we also test a version of the AllProp al-gorithm that uses the same cluster representation approach. (We use p MLE c (  X  ) in Eq. 4-6.) The performance comparison of the methods is presented in Table 3. (We also present for reference the performance of AllProp with the original mixture-model-based representation, denoted with  X  X  X .)
Table 3 shows that both implementations of AllProp post performance that is better to a statistically significant de-gree in all relevant comparisons than that of the CQS ap-proach. (Since CQS ranks query-specific clusters based only on cluster-query similarity, the performance is not very ef-fective [24, 18].) We can also see that both implementations of AllProp outperform the HITS method in most relevant comparisons. These results attest to the effectiveness of the
The order of concatenation has no effect since we only de-fine unigram language models. Table 3: Performance comparison of methods for detecting optimal clusters;  X  X  X  and  X  X  X  respectively indicate whether concatenation-based or (our orig-inally proposed) mixture-model-based representa-tion was used for clusters. The best result in each column is boldfaced; statistically significant differ-ences with the CQS [24] and HITS [18] methods are marked with  X  X  X  and  X  X  X , respectively.
 AllProp algorithm as a cluster-ranking approach that can effectively accommodate different cluster representations. plore the performance of our AllProp algorithm, we now compare it with that of a state-of-the-art pseudo-feedback-based approach, namely, the relevance model [20, 21].
The relevance model RM1 is based on a mixture of the language models of documents in D init [21]. Specifically, if w is a term in the vocabulary, { q i } is the set of query terms, and p JM [  X  ] d (  X  ) is a Jelinek-Mercer smoothed document lan-guage model with smoothing parameter  X  [40], then RM1 is defined by p RM1 is often clipped by setting p RM 1 ( w ;  X  ) to 0 for all but the  X  terms with the highest p RM 1 ( w ;  X  )tobeginwith;fur-ther normalization is then performed to yield a valid prob-ability distribution  X  denoted  X  p RM 1 (  X  ;  X ,  X  ) [1, 8]. For ad-ditional performance enhancement, RM1 is anchored to the original query [1, 8] to yield the RM3 model: p RM 3 ( w ;  X ,  X ,  X  ) def =  X p MLE q ( w )+(1  X   X  )  X  p RM  X  is a free interpolation parameter. The documents in the corpus are then ranked by KL  X  the KL divergence of their language models from RM3. (  X  is set to 2000 as at the above.)
We (independently) optimize the p@5 and p@10 perfor-mance of RM3 by choosing the values of its free parame-ters from the following sets: (i)  X   X  X  0 , 0 . 1 , 0 . 3 ,..., 0 . 9 (ii)  X   X  X  25 , 50 , 75 , 100 , 500 , 1000 , 5000 ,ALL } where  X  ALL  X  stands for all terms in the corpus (i.e., no clipping), and presented in Table 4.
 Table 4 shows that our AllProp algorithm outperforms RM3 on WSJ and TREC8, and underperforms it on AP. (None of the performance differences are statistically sig-nificant.) 7 However, RM3 posts more statistical significant improvements over the initial ranking than AllProp. All in Similar performance patterns are observed if we use RM3 for re-ranking D init , rather than for ranking the entire corpus. Specifically, the optimized p@5 of such a model is 58 . 8, 53 . 6, and 51 . 1 for WSJ, TREC8, and AP, respectively; the optimized p@10 is 53 . 0, 48 . 4, and 48 . 3, respectively. Table 4: Comparison of our AllProp algorithm with a relevance model (RM3) [1, 8]. Boldface: best per-formance in a column;  X * X : statistically significant difference with the initial ranking. all, these results are gratifying, because the performance of AllProp was optimized with respect to a single free param-eter (  X  ) , while that of RM3 was optimized with respect to three (  X  ,  X  and  X  ).
 Perhaps not less important is the following observation. Recall that AllProp looks for relevant documents (via cluster ranking) in an initially retrieved list D init , while pseudo-feedback-based methods such as RM3 use documents from D init for defining a (query) model using which the entire corpus is (re-)ranked. Thus, in principle, AllProp can be employed as a  X  X ilter X  on D init , providing pseudo-feedback-based approaches with a  X  X elevance-focused X  subset of D init upon which they can operate. In fact, the potential merits of such integration were demonstrated in some previous work on implementing relevance-feedback on top of documents selected from manually chosen query-specific clusters [30]. Exploring this direction is thus a promising venue for future research on pseudo-feedback retrieval.
Heretofore we have focused on the AllProp algorithm, whichranksclustersbasedontheextenttowhichtheyex-hibit all four  X  X aithfulness X  properties that were defined in Section 2. Having read this far, the reader may rightfully wonder whether all these properties in fact contribute to the overall performance of the AllProp algorithm. Here we show that the answer to this question is affirmative, and even to a quite surprising extent.

Table 5 shows the resultant performance of using each of our four properties individually, as well as using all their possible combinations (based on Eq. 7). When using each of the properties by itself, PF turns out to be the most ef-fective such property. In fact, it is the only single property that consistently yields performance superior to that of the initial ranking (from which D init was derived) in all rele-vant comparisons. This finding attests to the importance of measuring the (targeted by PF) extent to which a cluster exhibits different  X  X spects X  in the initial list, as manifested in its clustering.
 Note that the other three properties, namely, QF, SF, and ILF, appear to be rather noisy when used in isolation (as their performance is often below that of the initial ranking), and thus these results as if suggest that using these proper-ties is likely to harm the overall ability to detect the desired clusters. Table 5, however, shows that this is very much not the case. Indeed, perhaps the most interesting observation that can be made based on Table 5 is that in all six relevant comparisons (3 corpora  X  2 evaluation measures) the perfor-mance mostly improves from using any subset of properties X to any of its proper extensions X  X  X P} ,where P is a single property in { QF, SF, ILF, P F } . More specifically, Table 5: The performance of ranking clusters by single cluster-properties, as well as by their combi-nations (using Eq. 7). Recall that AllProp stands for QF  X  SF  X  ILF  X  PF. Best result in each column is boldfaced; statistically significant differences with the initial ranking are marked with  X * X .
In light of the demonstrated effectiveness of aggregating estimates of the different witnessing properties of the clus-ters, we now turn to study some alternative rank-aggregation
The fact that the list-faithfulness property, ILF, is the worst performing (used alone) property can be attributed to the fact that for many queries, many of the documents in D init are not relevant; hence, imposing faithfulness to these documents (without using other properties) yields degraded performance. approaches. Recall from Section 2 that our original rank-aggregation method for property integration uses the alge-braic product of the per-property scores of clusters. (See Eq. 7.) We first explore an intuitive alternative that uses sum [10] instead of product: where  X  = { QF, SF, ILF, P F } . This aggregation proce-dure has a more  X  X isjunctive X  nature  X  S core  X  (  X  ; S core  X  (  X  ;  X  ) are ranking-wise equivalents of the arithmetic and geometric means of the individual scores, respectively.
Now, suppose that we are to believe that the only seman-tics of our ranking functions is ordinal, that is, either the ab-solute scores have no meaning, or this meaning significantly varies among the four ranking functions. If so, then some-times the absolute scores are better ignored, and we should resort to one of the ordinal rank aggregation methods [6, 10]. For instance, one of the most popular such methods is Borda X  X  method [37], which in our context, scores cluster c by: where  X  = { QF, SF, ILF, P F } ; then, the clusters are ranked in descending order of their Borda scores. Similarly to product-based or sum-based score aggregations,  X  X ositional X  methods such as Borda X  X  are appealing because they are computation-ally easy 9 . In fact, note that Borda X  X  method boils down to sum-based score aggregation of certain, inter-comparable be-tween the individual properties, yet basically ad hoc scores of the clusters. Hence, if the scores provided by different cluster-ranking properties have similar quantitative nature  X  as is the case with our average-precision-based measures of the four  X  X aithfulness X  criteria  X  then apriori Borda X  X  method is less appealing.
 Table 6: Comparison of rank-aggregation methods: product (  X  , as in our AllProp algorithm), sum (  X  ), and Borda X  X  method. Boldface marks the best per-formance in each column, and  X * X  marks statistically significant difference with the initial ranking.

Table 6 provides a performance comparison between the aforementioned three rank-aggregation methods. Carefully looking into the numbers, it appears that both the product-based method (  X  ) employed by our AllProp algorithm and Borda X  X  method are superior to the sum-based method (  X  ) in most of the relevant comparisons. (While  X  is superior to Borda for clusters of size 5, the reverse is true for clus-ters of size 10.) Having said that, we believe that the more
Depriving from positional methods very quickly brings us to NP-hard methods such as Kemeny optimal aggregation, footfule optimal aggregation, etc. [9]. important observation is that all three rank-aggregation ap-proaches yield performance that transcends (in most rele-vant comparisons) that of the initial ranking that was used to create D init . Thus, it seems that using cluster-properties of a somewhat  X  X omplementary X  nature, as is the case for our properties (see at the above), yields satisfactory perfor-mance with different rank-aggregation approaches.
We presented a novel framework to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. The frame work is based on proposing cluster properties that presu mably correlate with this per-centage, and integrating their estimates via rank aggrega-tion. We showed that our approach outperforms previous methods for detecting clusters containing a high relevant-document percentage, and also produces document-ranking with substantially higher precision-at-top-ranks performance than that of standard document-based retrieval.

We intend to integrate additional cluster properties in our framework [25, 18, 15] and to study alternative rank-aggregation methods. Another future direction is to intro-duce diversity in the resultant list of documents, which is im-portant for handling ambiguous queries. Using documents from different presumably-optimal clusters can potentially be effective in such cases.
 Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by a gift from Google. Any opinions, findings and conclusions or recommendations expressed in this material are the au-thors X  and do not necessarily reflect those of the sponsoring institutions.
