 ENDIF-Dipartimento di Ingegneria, Universit X  di Ferrara, Ferrara, Italy 1. Introduction
Machine Learning has seen the development of the field of Statistical Relational Learning, where and uncertainty. These techniques have been successfully applied in social networks analysis, entity recognition, collective classification and information extraction, to name a few.

Similarly, in the field of Logic Programming, a large number of works have started to appear that combine logic and probability. Among these, many share a common approach to defining the seman-tics of the proposed languages: the distribution semantics [32]. It underlies for example Probabilistic Logic Programs [2], Probabilistic Horn Abduction (PHA) [22], PRISM [32], Independent Choice Logic (ICL) [23], pD [8], Logic Programs with Annotated Disjunctions (LPADs) [41], ProbLog [5] and CP-logic [39]. The approach is particularly appealing for its intuitiveness and because efficient inference algorithms have started to appear [5,15,20,27,29]. Most of these techniques use Binary Decision Di-agrams (BDD) for inference: explanations for the query are found and the probability of the query is computed by building a BDD.
 In this paper we present the EMBLEM system for  X  X M over Bdds for probabilistic Logic programs Efficient Mining X  that learns parameters of probabilistic logic programs under the distribution semantics by using an Expectation Maximization (EM) algorithm. The system exploits the fact that the transla-tion of these programs into graphical models generates models with hidden variables and therefore an EM approach is necessary. Its main characteristic is that it computes the values of expectations using BDDs. EMBLEM is developed for the language of LPADs and tested on the IMDB [21], Cora [34] and UW-CSE [34] datasets and compared with RIB [31], LeProbLog [5], Alchemy [24] and CEM, an implementation of EM based on the cplint interpreter [27].

The paper is organized as follows. Section 2 presents Probabilistic Logic Programming, concentrating on LPADs. Section 3 describes EMBLEM together with an example of its execution. Section 4 discusses related work. In Section 5 the results of the experiments performed are presented. Finally Section 6 concludes the paper. 2. Probabilistic logic programming
Many languages have been proposed that integrate l ogic programming with probability theory. One of the most interesting approaches to the integration is the distribution semantics [32], which was in-troduced for the PRISM language but is shared by many other languages. A program in one of these languages defines a probability distribution over normal logic programs called worlds . This distribu-tion is then extended to queries and the probability of a query is obtained by marginalizing the joint distribution of the query and the programs.

The distribution semantics has been defined both for programs that do not contain function symbols, and thus have a finite set of worlds W , and for programs that contain them, that have an infinite set of worlds. We review here the first case for the sake of simplicity. The probability of a query Q given a world w is P ( Q | w )=1 if w | = Q and 0 otherwise, where | = is truth in the well-founded model [38]. Thus the probability of a query Q is given by The languages following the distribution semantics differ in the way they define the distribution over logic programs. Each language allows probabilistic choices among atoms in clauses: Probabilistic Logic Programs, PHA, ICL, PRISM, and ProbLog allow probability distributions over facts, while LPADs allow probability distributions over the heads of disjunctive clauses. All these languages have the same expressive power: there are transformations with linear complexity that can convert each one into the others [4,40]. In this paper we will use LPADs for their general syntax.

In LPADs the alternatives are encoded in the head of clauses in the form of a disjunction in which each atom is annotated with a probability. Each grounding of an annotated disjunctive clause represents a probabilistic choi ce between a number of ground normal cla uses. By choosing a head atom for each grounding of each clause we get a world. The probability of the world is given by the product of the annotations of the atoms selected.

Formally a Logic Program with Annotated Disjunctions [41] consists of a finite set of annotated  X   X  the body and is indicated with body ( C i ) . Note that if n i =1 and  X  i 1 =1 the clause corresponds to a We denote by ground ( T ) the grounding of an LPAD T .
 { 1 corresponds to a random variable X ij and an atomic choice ( C i , X  j ,k ) to an assignment X ij = k .A set of atomic choices  X  is consistent if ( C,  X , i )  X   X , ( C,  X , j )  X   X   X  i = j , i.e., only one head is selected for a ground clause. A composite choice  X  is a consistent set of atomic choices. The probability P (  X  ) of a composite choice  X  is the product of the probabilities of the individual atomic choices, i.e. P (  X  )=
A selection  X  is a composite choice that, for each clause C i  X  j in ground ( T ) , contains an atomic is called a world of T . Since selections are composite choices we can assign a probability to worlds: P ( w
We consider only sound LPADs, in which every possible world has a total well-founded model. We write w  X  | = Q to mean that the query Q is true in the total well-founded model of the program w  X  . The probability of a query Q according to an LPAD T is given by wherewedefine E ( Q )= {  X   X  X  T ,w  X  | = Q } the set of selections corresponding to worlds where the query is true.

Sometimes a simplification of this semantics can be used to reduce the computational cost of an-swering queries. In this simplified semantics random variables are directly associated to clauses in the single random variable X i . In this way the number of random variables may be significantly reduced the experiments in Section 5 we use this simplification to contain the computational costs. Example 1. The following LPAD T encodes a very simple model of the development of an epidemic or pandemic: that an epidemic or a pandemic arises. We are uncertain about whether the climate is cold but we know for sure that David and Robert have the flu.

Clause C 1 has two groundings, C 1  X  1 with  X  1 = { X/david } and C 1  X  2 with  X  2 = { X/robert } so there are two random variables X 11 and X 12 .

T has 18 instances, the query epidemic is true in 5 of them and its probability is P ( epidemic )= 0 .
In the simplified semantics C 1 is associated to a single random variable X 1 . In this case T has 6
The worlds in which a query is true can be represented using a Multivalued Decision Diagram (MDD) [36]. An MDD represents a function f ( X ) taking Boolean values on a set of multivalued vari-ables X by means of a rooted graph that has one level for each variable. Each node is associated to the variable of its level and has one child for each possible value of the variable. The leaves store either 0 or 1. Given values for all the variables X , we can compute the value of f ( X ) by traversing the graph starting from the root and returning the value associated to the leaf that is reached. An MDD can be equation X ij = k . If we represent with the MDD the function f ( X )=  X   X  E ( Q ) ( C then the MDD will have a path to a 1-leaf for each possible world where Q is true. MDDs can be built by combining simpler MDDs using Boolean operators. While building MDDs simplification operations can be applied that delete or merge nodes. Merging is performed when the diagram contains two iden-tical sub-diagrams, while deletion is performed when all arcs from a node point to the same node. In this way a reduced MDD is obtained, that often has a much smaller number of nodes with respect to a Multivalued Decision Tree (MDT), i.e., an MDD in which every node has a single parent and all the children belong to the level immediately below.
 For example, the reduced MDD corresponding to the query epidemic from Example 1 is shown in Fig. 1(a). The labels on the edges represent the values of the variable associated to the node.
It is often unfeasible to find all the instances where the query is true so inference algorithms find instead explanations for the query, i.e. composite choices such that the query is true in all the worlds whose selections are a superset of them. Explanations however, differently from possible worlds, are not necessarily mutually exclusive with respect to each other, so the probability of the query can not be computed by a summation as in Formula 2. The explanations have first to be made disjoint so that a summation can be computed. Since MDDs split paths on the basis of the values of a variable, the branches are mutually disjoint so a dynamic programming algorithm can be applied for computing the probability.
 Most packages for the manipulation of decision diagrams are however restricted to work on Binary Decision Diagrams (BDD), i.e., decision diagrams where all the variables are Boolean. These packages offer Boolean operators between BDDs and apply simplification rules to the result of operations in order to reduce as much as possible the size of the BDD, obtaining a reduced BDD. Usually reduced BDDs have a much smaller number of nodes than the equivalent Binary Decision Tree (BDT).

A node n in a BDD has two children: the 1-child, also indicated with child 1 ( n ) , and the 0-child, also indicated with child 0 ( n ) . When drawing BDDs, rather than using edge labels, the 0-branch, the one going to the 0-child, is distinguished from the 1-branch by drawing it with a dashed line.
To work on MDDs with a BDD package we must represent multivalued variables by means of binary variables. Various options are possible, we found that the following, proposed in [4], gives the best performance. For a multi-valued variable X ij , corresponding to ground clause C i  X  j ,having n i values, 1 is shown in Fig. 1(b). BDDs obtained in this way can be used as well for computing the probability of queries by associating to each Boolean variable X ijk a parameter  X  ik that represents P ( X ijk =1) .The parameters are obtained from those of multivalued variables in this way: up to k = n i  X  1 . 3. EMBLEM
EMBLEM applies the algorithms for performing EM over BDDs proposed in [12 X 14,37] to the prob-lem of learning the parameters of an LPAD. EMBLEM takes as input a number of goals that represent the examples. For each goal it generates the BDD encoding its explanations. The typical input for EM-BLEM will be a set of interpretations, i.e., sets of ground facts, each describing a portion of the domain the facts for these predicates will then form the queries for which the BDDs are built. The predicates can be treated as closed-world or open-world. In the first case the body of clauses is resolved only with bound on SLD-derivations to avoid going into infinite loops, as proposed by [10]. Given a program con-we obtain the BDD in Fig. 1(b) that represents the query epidemic .

Then EMBLEM enters the EM cycle, in which the steps of expectation and maximization are repeated until the log-likelihood of the examples reaches a local maximum.

Let us now present the formulas for the expectation and maximization phases for the case of a single example Q :  X  Maximization: computes  X  ik for all rules C i and k =1 ,...,n i  X  1 . If we have more than one example the contributions of each example simply sum up when computing E [ where P ( X ijk =1 |  X  )=1 if ( C i , X  j ,k )  X   X  for k =1 ,...,n i  X  1 and 0 otherwise.
Since there is a one to one correspondence between the possible worlds where Q is true and the paths to a 1 leaf in a BDT, node associated to a variable X ijk ,then  X  ( d )=1  X   X  ik .
 Now consider a BDT in which only the merge rule is applied, fusing together identical sub-diagrams. For example, by applying only the merge rule in Example 1 the diagram in Fig. 2 is obtained. The resulting diagram, that we call Complete Binary Decision Diagram (CBDD), is such that every path contains a node for every level.
 For a CBDD, P ( X ijk = x, Q ) can be further expanded as where ( X ijk = x )  X   X  means that  X  contains an x -edge from a node associated to X ijk . We can then write x -child.
 where  X  ikx is  X  ik if x=1 and (1  X   X  ik ) if x=0, and is the forward probability [14], the probability mass of the paths from the root to n ,while set of paths from n to the 1 leaf. If root is the root of a tree for a query Q then B ( root )= P ( Q ) . through the x -edge of node n . We indicate with e x ( n ) such an expression. Thus For the case of a BDD, i.e., a diagram obtained by applying also the deletion rule, Formula 3 is no longer valid since also paths where there is no node associated to X ijk can contribute to P ( X ijk = x, Q ) .In fact, it is necessary to consider also the deleted paths: suppose that a node n associated to variable Y level lower than variable X ijk . The nodes associated to variable X ijk have been deleted from the paths from n to child 0 ( n ) . One can imagine that the current BDD has been obtained from a BDD having a node m associated to variable X ijk that is a descendant of n along the 0-branch and whose outgoing edges both point to child 0 ( n ) . The original BDD can be reobtained by applying a deletion operation that merges the two paths passing through m . The probability mass of the two paths that were merged
Formally, let Del x ( X ) be the set of nodes n such that the level of X is below that of n and is above Del 1 ( X 121 )= { n 1 } , Del 0 ( X 121 )= {} , Del 1 ( X 221 )= {} , Del 0 ( X 221 )= { n 2 } .Then Algorithm 1 Procedure EMBLEM Having shown how to compute the expected counts, we now describe EMBLEM in detail.
 EMBLEM X  X  main procedure, shown in Algorithm 1, consists of a cycle in which the procedures E X -PECTATION and M AXIMIZATION are repeatedly called. Procedure E XPECTATION returns the log likeli-hood of the data that is used in the stopping criterion: EMBLEM stops when the difference between the log likelihood of the current and the previous iteration drops below a threshold or when this difference is below a fraction  X  of the current log likelihood.

Procedure E XPECTATION , shown in Algorithm 2, takes as input a list of BDDs, one for each example, and G ET B ACKWARD that compute the forward, the backward probability of nodes and  X  x ( i, k ) for non-deleted paths only. Then it updates  X  x ( i, k ) to take into account deleted paths. Procedure M AXIMIZATION (Algorithm 3) computes the parameters values for the next EM iteration.
Procedure G ET F ORWARD , shown in Algorithm 4, computes the value of the forward probabilities. It traverses the diagram one level at a time starting from the root level. For each level it considers each node n and computes its contribution to the forward probabilities of its children. Then the forward probabilities of its children, stored in table F , are updated.

Function G ET B ACKWARD , shown in Algorithm 5, computes the backward probability of nodes by traversing recursively the tree from the root to the leaves. When the calls of G ET B ACKWARD for both WARD , rather than being included in G ET O UTSIDE E XPE as in [14].
 that do not have a descendant in level l minus those for nodes in upper levels that have a descendant in accumulator T for level l means that n is an ancestor for nodes in this level. When the x -branch from n Algorithm 2 Procedure expectation Algorithm 3 Procedure maximization node on the path anymore.

Let us see an example of execution. Suppose you have the program of Example 1 and you have the single example epidemic . The BDD of Fig. 1(b) (also shown in Fig. 3) is built and passed to E XPECTA -TION in the form of a pointer to its root node n 1 . After initializing the  X  counters to 0, G ET F ORWARD is called with argument n 1 .The F table for n 1 is set to 1 since this is the root. F is computed for the are shown in Fig. 3. Algorithm 4 Procedure GetForward: Computation of the forward probability Algorithm 5 Procedure GetBackward: Computation of the backward probability, updating of  X  and of  X  Then G ET B ACKWARD is called on n 1 .The function calls G ET B ACKWARD ( n 2 ) that in turn calls G ET -B ACKWARD (0) . The latter call returns 0 because it is a terminal node. Then G ET B ACKWARD ( n 2 ) calls G
ET B ACKWARD ( n 3 ) that in turn calls G ET B ACKWARD (1) and G ET B ACKWARD (0) , returning respec-tively 1 and 0. Then G ET B ACKWARD ( n 3 ) computes e 0 ( n 3 ) and e 1 ( n 3 ) in the following way: where B ( n ) and F ( n ) are respectively the backward and forward probabilities of node n .Nowthe counters for clause C 2 are updated: while we do not show the update of  X  since its value for the level of the leaves is not used afterwards. G ET B ACKWARD ( n 3 ) now returns the backward probability of n 3 B ( n 3 )=1  X  0 . 7+0  X  0 . 3=0 . 7 . G
ET B ACKWARD ( n 2 ) can proceed to compute e e 1 ( n 2 )=0 . The 0-child is a leaf so we do not show the update of  X  .
 computes e 0 ( n 1 ) and e 1 ( n 1 ) as and updates the  X  counters as Finally  X  is updated: G contribution of deleted nodes by cycling over the BDD levels and updating T . Initially T is set to 0, then for variable X 111 T is updated to T =  X  ( X 111 )=0 which implies no modification of  X  0 (1 , 1) and  X  1 (1 , 1) .Forvariable X 121 T is updated to T =0+  X  ( X 121 )=0 . 42 and the  X  table is modified as the expected counts for the two rules can be computed: 4. Related work
Our work has close connection with various other works. [13,14] proposed an EM algorithm for learn-ing the parameters of Boolean random variables given observations of a Boolean function over them, represented by a BDD. EMBLEM is an application of that algorithm to probabilistic logic programs. In-dependently also [37] proposed an EM algorithm which computes expectations over decision diagrams. The algorithm learns parameters for the CPT-L language, a simple probabilistic logic language for de-scribing sequences of relational states, that is less expressive than LPADs. [12] applies the algorithm of [13,14] to the problem of computing the probabilistic parameters of abductive explanations. [11] re-cently presented the C O P R EM algorithm that performs EM for the ProbLog language. We differ from this work in the construction of BDDs: while they build a BDD for an interpretation that represents the want to obtain good predictions, and we build BDDs starting from atoms for the target predicate. More-over, while we compute the contributions of deleted paths with the  X  table, C O P R EM treats missing nodes as if they were there and updates the counts accordingly.

Other approaches for learning probabilistic logic programs can be classified into three categories: those that employ constraint techniques, those that use EM and those that adopt gradient descent. In the certain constraints and then applying mixed integer linear programming to identify a subset of the clauses that form a solution.

Among the approaches that use EM, [1,18,19] first proposed to use the EM algorithm to induce pa-rameters of ground LPADs and the Structural EM algorithm to induce ground LPAD structures. Their EM algorithm however works on the underlying Bayesian network.

RIB [31] performs parameter learning using the information bottleneck approach, which is an exten-sion of EM targeted especially towards hidden variables. However, it works best when interpretations have the same Herbrand base, which is not always the case.
 The PRISM system [32,33] is one of the first learning algorithms based on EM. It exploits Logic Programming techniques for computing expectations but imposes restrictions on the language.
In [16] the authors use EM to learn the structure of first-order rules with associated probabilistic uncertainty parameters. Their approach involves generating the underlying graphical model using a Knowledge-Based Model Construction approach. EM is then applied on the graphical model.
Among the works that use a gradient descent technique, LeProbLog [9,10] starts from a set of queries annotated with a probab ility and from a ProbLog program. It tries to find the values of the parameters of the program that minimize the mean squared error of the probabilities of the queries. LeProbLog uses the Binary Decision Diagrams that represent the queries to compute the gradient.

Alchemy [24] is a state of the art Statistical Relational Learning system that offers various tools for inference, weight learning and structure learning of Markov Logic Networks (MLNs). [17] discusses how to perform weight learning by applying gradient descent of the conditional likelihood of queries for target predicates. MLNs significantly differ from the languages under the distribution semantics since they extend first-order logic by attaching weights to logical formulas, reflecting  X  X ow strong X  they are. MLNs allow the use of logical formulas without syntactic restrictions, but do not allow to exploit logic programming techniques. 5. Experiments EMBLEM has been tested over three real world datasets: IMDB 1 [21], UW-CSE 2 [34] and Cora 3 [34].
We implemented EMBLEM in Yap Prolog 4 and we compared it with RIB [31]; CEM, an implemen-tation of EM based on the cplint inference library [27,30]; LeProblog [9,10] and Alchemy [24]. All experiments were performed on Linux machines with an Intel Core 2 Duo E6550 (2333 MHz) processor and4GBofRAM.

To compare our results with LeProbLog we exploited the translation of LPADs into ProbLog proposed in [4], in which a disjunctive clause with k head atoms and vector of variables X is modeled with k ProbLog clauses and k  X  1 probabilistic facts with variables X .

To compare our results with Alchemy we exploited the translation between LPADs and MLN used in [31] and inspired by the translation between ProbLog and MLNs proposed in [10]. An MLN clause is translated into an LPAD clause in which the head atoms of the LPAD clause are the null atom plus the positive literals of the MLN clause while the body atoms are the negative literals.

For the probabilistic logic programming systems (EMBLEM, RIB, CEM and LeProbLog) we con-sider various options. The first consists in choosing between associating a distinct random variable to expressing whether the clause is used or not. The latter case makes the problem easier, as stated previ-ously. The second option is concerned with putting a limit on the depth of derivations as done in [10], thus eliminating explanations associated to derivations exceeding the depth limit. This is necessary for the number of restarts for EM based algorithms.

All experiments for probabilistic logic programming systems have been performed using open-world predicates, meaning that, when resolving a literal, both facts in the database and rules are used to prove it.

All datasets are partitioned into five mega-examples, so a five-fold cross-validation approach has been adopted in the experiments: of the five mega-examples, a single example is retained for testing, and the remaining four are used as training data. The datasets are described in Table 1 in terms of target predicates, number of different constants, number of different predicates and number of tuples (ground atoms) in the interpretations.
As part of the test, we drew a Precision-Recall curve and a Receiver Operating Characteristics curve, and computed the Area Under the Curve (AUCPR and AUCROC respectively) using the methods re-ported in [3,7].

IMDB regards movies, actors, directors and movie genres. Each mega-example contains all the in-formation regarding four movies. We defined 4 different LPADs, two for predicting the target predicate sameperson / 2 , and two for predicting samemovie / 2 . We had one positive example for each fact that is true in the data, while we sampled from the complete set of false facts three times the number of true instances in order to generate negative examples.

For predicting sameperson / 2 we used the same LPAD of [31]: sameperson(X,Y):p:-movie(M,X),movie(M,Y). sameperson(X,Y):p:-actor(X),actor(Y),workedunder(X,Z), sameperson(X,Y):p:-gender(X,Z),gender(Y,Z). sameperson(X,Y):p:-director(X),director(Y),genre(X,Z), where p is a tunable parameter. We ran EMBLEM on it with the following settings: no depth bound, random variables associated to instantiations of the clauses and a number of restarts chosen to match the execution time of EMBLEM with that of the fastest other algorithm.
 Tables 2 and 3 show respectively the AUCPR and AUCROC averaged over the five folds for EM-BLEM, RIB, LeProbLog, CEM and Alchemy. Results for the two programs are shown respectively in the IMDB-SP and IMDBu-SP rows (where u stands for unseen). Table 4 shows the learning times in hours.

The queries that LeProbLog takes as input are obtained by annotating with 1.0 each positive example for sameperson / 2 and with 0.0 each negative example. We ran LeProbLog for a maximum of 100 iterations or until the difference in Mean Squared Error (MSE) between two iterations got smaller than
For Alchemy we used the preconditioned rescaled conjugate gradient discriminative algorithm [17] and we specified sameperson / 2 as the only non-evidence predicate.

A second LPAD has been created to evaluate the performance of the algorithms when some atoms are unseen: sameperson_pos(X,Y):p:-movie(M,X),movie(M,Y). sameperson_pos(X,Y):p:-actor(X),actor(Y), sameperson_pos(X,Y):p:-director(X),director(Y),genre(X,Z), sameperson_neg(X,Y):p:-movie(M,X),movie(M,Y). sameperson_neg(X,Y):p:-actor(X),actor(Y), sameperson_neg(X,Y):p:-director(X),director(Y),genre(X,Z), sameperson(X,Y):p:-\+ sameperson_pos(X,Y),sameperson_neg(X,Y). sameperson(X,Y):p:-\+sameperson_pos(X,Y),\+sameperson_neg(X,Y). sameperson(X,Y):p:-sameperson_pos(X,Y),sameperson_neg(X,Y). sameperson(X,Y):p:-sameperson_pos(X,Y),\+ sameperson_neg(X,Y).
 The sameperson_pos/2 and sameperson_neg/2 predicates are unseen in the data. Alchemy was run with the  X  withEM option that turns on EM learning. The other parameters for Alchemy and for the other algorithms are set as before.

For predicting samemovie / 2 we used the LPAD: samemovie(X,Y):p:-movie(X,M),movie(Y,M),actor(M). samemovie(X,Y):p:-movie(X,M),movie(Y,M),director(M). samemovie(X,Y):p:-movie(X,A),movie(Y,B),actor(A), director(B), samemovie(X,Y):p:-movie(X,A),movie(Y,B),director(A),director(B), To test the behaviour when unseen predicates are present, we transformed the program for samemovie / 2 as we did for sameperson / 2 , thus introducing the unseen predicates samemovie _ pos / 2 and samemovie _ neg / 2 . We ran EMBLEM on them with no depth bound, one variable for each instanti-ation of a rule and one random restart. As regards LeProbLog and Alchemy, we ran them with the same settings as IMDB-SP and IMDBu-SP, by replacing sameperson with samemovie .

Tables 2 and 3 show respectively the AUCPR and AUCROC averaged over the five folds. Results for the two LPADs are shown respectively in the IMDB-SM and IMDBu-SM rows. RIB in this case obtained a memory error (indicated with  X  X e X ), due to the exhaustion of the available stack space during the execution of the algorithm.

The Cora database contains citations to computer science research papers. For each citation we know are referring to the same paper, by predicting the predicate samebib(cit1,cit2) . The database contains facts for the predicates sameauthor(aut1,aut2) , sametitle(tit1,tit2) , samevenue(ven1,ven2) , haswordtitle(title,word) haswordauthor(author, word) and haswordvenue(venue,word) .

From the MLN proposed in [35] 5 we obtained two LPADs. The first contains 559 rules and differs from the direct translation of the MLN because rules involving words are instantiated with the differ-ent constants, only positive literals for the hasword predicates are used and transitive rules are not included: samebib(B,C):p:-author(B,D),author(C,E),sameauthor(D,E). samebib(B,C):p:-title(B,D),title(C,E),sametitle(D,E). samebib(B,C):p:-venue(B,D),venue(C,E),samevenue(D,E). samevenue(B,C):p:-haswordvenue(B,word_06), ... sametitle(B,C):p:-haswordtitle(B,word_10), .... sameauthor(B,C):p:-haswordauthor(B,word_a), .....
 The dots stand for the rules for all the possible words. The four predicates samebib / 2 , samevenue / 2 , sametitle / 2 and sameauthor / 2 have been set as target predicates and we used as negative examples those contained in the Alchemy dataset. We ran EMBLEM on this LPAD with no depth bound, a single variable for each instantiation of a rule and a number of restarts chosen to match the execution time of EMBLEM with that of the fastest other algorithm.

The second LPAD adds to the previous one four transitive rules: samebib(A,B):p :-samebib(A,C), samebib(C,B). sameauthor(A,B):p :-sameauthor(A,C), sameauthor(C,B). sametitle(A,B):p :-sametitle(A,C), sametitle(C,B). samevenue(A,B):p :-samevenue(A,C), samevenue(C,B). for a total of 563 rules. In this case we had to run EMBLEM with a depth bound equal to two and a sin-gle variable for each non-ground rule; the number of restarts was one. As for LeProbLog, we separately learned the four predicates because learning the whole theory at once would give a lack of memory error. We annotated with 1.0 each positive example for samebib / 2 , sameauthor / 2 , sametitle / 2 , samevenue / 2 and with 0.0 the negative examples for the same predicates. We ran it for a maximum of 100 iterations or until the difference in MSE between two iterations got smaller than 10  X  5 . For Alchemy we used the preconditioned rescaled conjugate gradient discriminative training algorithm and we speci-fied the four predicates as the non-evidence predicates. Tables 2 and 3 show respectively, in the Cora and CoraT (Cora transitive) rows, the average AUCPR and AUCROC. On CoraT, CEM and Alchemy gave a memory error, for memory exhaustion and a segmentation fault (during the use of learnwts command) respectively, while RIB was not applicable because it was not possible to split the input examples into smaller independent interpretations as required by RIB.

The UW-CSE dataset contains information about the computer science department of the University of Washington. It contains 22 different predicates, such as yearsInProgram/2 , advisedBy/2 , taughtBy/3 and so on. The predicates are typed, where possible types are person, course, publication, etc. Each mega-example contains facts for a particular area of the CS department: artificial intelligence, graphics, programming languages, systems and theory. The goal here is to predict the advisedby/2 predicate, namely the fact that a person is advised by another person: this was our target predicate.
The theory used was obtained from the MLN of [34]. 6 It contains 86 rules, such as for instance: advisedby(S, P) :p :-courselevel(C,level_500),taughtby(C,P,Q), tempadvisedby(S, P) :p :-courselevel(C,level_500), professor(P) :p :-courselevel(C,level_500),taughtby(C,P,Q). We ran EMBLEM on it with a single variable for each non-ground rule, a depth bound of two and one random restart. The negative examples have been generated by considering all couple of persons (a,b) where a and b appear in an advisedby/2 fact in the data and by adding a negative example advisedby(a,b) if it is not in the data.

The annotated queries that LeProbLog takes as input have been created by annotating with 1.0 each positive example for advisedby / 2 and with 0.0 each negative example. We ran LeProbLog for a maxi-mum of 100 iterations or until the difference in MSE between two iterations got smaller than 10  X  5 and we used a single variabl e for each non-ground rule. For Alchemy, we used the p reconditioned rescaled conjugate gradient discriminative training algorithm to learn weights, by specifying advisedby/2 as the only non-evidence predicate. RIB was non applicable to this dataset because it does not allow to have variables for non-ground rules. Tables 2 and 3 show respectively the AUCPR and AUCROC averaged over the five departments for all the algorithms.

Tables 5 and 6 show the p-value of a paired two-tailed t-test at the 5% significance level of the dif-ference respectively in AUCPR and AUCROC between EMBLEM and RIB/LeProbLog/CEM/Alchemy (significant differences in bold).

From the results we can observe that over IMDB EMBLEM has comparable performances with CEM for IMDB-SP, with similar execution time. On IMDBu-SP it has better performances than all other systems (see AUCPR), with a learning time equal to the fastest other algorithm. On IMDB-SM it reaches the highest area value in less time (only one restart is needed). On IMDBu-SM it still reaches the highest area with one restart but with a longer execution time.

Over Cora it has comparable performances with the best other system CEM but in significant lower time and over CoraT is one of the few systems to be able to complete learning, with better performances in terms of area (especially AUCPR) and time.
 Over UW-CSE it has better performances with respect to all the algorithms.

A difference in the learning times between EMBLEM and the other systems, in favour of the latter, can be found with the IMDBu-SM and UW-CSE datasets, in which EMBLEM takes a few hours, but it must be noted that in both cases there is also a significant gap in the area values: EMBLEM on IMDBu-SM reaches the highest possible area and on UW-CSE obtains a significantly higher AUCPR with respect to the other algorithms.

Among the probabilistic-logic systems, the closest to EMBLEM are RIB and LeProblog. RIB is, on one hand, based on an efficient algorithm, shown to be superior to EM for learning parameters of Bayesian networks with hidden variables [6] because it can avoid some local maxima, but on the other hand, it requires a different  X  X ormat X  for input examples with respect to EMBLEM, which makes it un-applicable on one dataset, and is less performing in the presence of partially hidden variables. LeProblog is the only system able to complete learning for all datasets as EMBLEM, with good performances in almost all cases, but it takes longer execution times (except for IMDBu-SM and UW-CSE).
Looking at the overall results, EMBLEM achieves higher or equal AUCPR and AUCROC with re-spect to all other systems, except on IMDBu-SP where LeProbLog achieves a non-statistically signifi-cant higher AUCROC. In the other cases the differences between EMBLEM and the other systems are statistically significant in 22 out of 43 cases. 6. Conclusions
We have proposed a technique which applies an EM algorithm to BDDs for learning the parameters of Logic Programs with Annotated Disjunctions. The problem we have faced is, given an LPAD for a domain, efficiently learning parameters for the disjunctive heads of the LPAD clauses. The resulting algorithm  X  EMBLEM  X  returns the parameters that best describe the data and can be applied to all lan-guages that are based on the distribution semantics. It exploits the BDDs that are built during inference to efficiently compute the expectation for hidden variables.

We executed the algorithm over the real datasets IMDB, UW-CSE and Cora, and evaluated its perfor-mances  X  together with those of four other probabilistic systems  X  through the AUCPR and AUCROC. These results show that EMBLEM uses less memory than RIB, CEM and Alchemy, allowing it to solve larger problems, as one can see from Table 2 where, for some datasets, not all the mentioned algorithms are able to terminate. Moreover its speed allows to perform a high number of restarts making it escape local maxima and achieve higher AUCPR and AUCROC.

EMBLEM is available in the cplint package in the source tree of Yap Prolog and information on its use can be found at http://sites.google.com/a/unife.it/ml/emblem.

In the future we plan to extend EMBLEM for learning the structure of LPADs by combining the standard Expectation Maximization algorithm, which optimizes parameters, with structure search for model selection.
 References
