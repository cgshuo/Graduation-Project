 The rapid advance in handheld communication devices and the appearance of smartphones has allowed users to con-nect to the Internet and surf on the WWW while they are moving around the city or traveling. Location based ser-vices have been developed to deliver content that is adjusted to the current user location. Social networks have also re-sponded to the challenge of users who can access the Internet from any place in the city, and location based social-network s like Foursquare have become very popular in a short period of time. The popularity of these applications is linked to the significant advantages they offer: users can exploit live location-based information to take dynamic decisions on is-sues like transportation, identification of places of interest or even on the opportunity to meet a friend or an associate in nearby locations. A side effect of sharing location-based information is that it exposes the user to substantial privacy related threats. Revealing the user X  X  location carelessly can prove to be embarrassing, harmful professionally, or even dangerous.
 Research in the data management field has put significant effort on anonymization techniques that obfuscate spatial information in order to hide the identity of the user or her exact location. Privacy guaranties and anonymization al-gorithms become increasingly sophisticated offering better and more efficient protection in data publishing and data exchange. Still, it is not clear yet what are the greatest dangers to user privacy and which are the most realistic pri-vacy breaching scenarios. The aim of the paper is to provide a brief survey of the attack scenarios, the privacy guaranties and the data transformations employed to protect user pri-vacy in real time . The paper focuses mostly on providing an overview of the privacy models that are investigated in literature and less on the algorithms and their scaling capa-bilities. The models and the attack scenarios are classified and compared, in order to provide an overview of the cases that are covered by existing research. The recent explosive growth in the usage of smart phones and the increased availability of wireless and GSM connec-tion has allowed users to connect to the Internet, use web services or other types of custom services, send and receive data from any place and at any time. Moreover, the de-velopments in positioning technologies like GPS, wireless
Data owner or publisher positioning, GSM etc has allowed for tracing users X  loca-tion, storing it, processing and even easily rendering it on a map. Collecting and processing location based informa-tion has significant benefits to users who can get informa-tion adjusted to their current location, to service providers who can better understand user behavior and offer them customized location aware services(usually termed as Loca-tion Based Services ( LBS )) and finally to researchers and public authorities who can exploit user movement data in order to plan better the future development of services and infrastructure.
 The monitoring, processing and storing of users X  locations in an unprecedented scale has naturally attracted attention to privacy related issues. Location information can reveal sensitive information about users, like health related issues, commercial practices, sexual preferences and can cause em-barrassment, financial damage or even expose users to phys-ical dangers. In the data management and knowledge dis-covery field the focus of research is on privacy preservation in the publication and exchange of data. Numerous privacy preserving techniques have been proposed that allow pub-lishing and exchanging data without breaching the privacy of the users. In the last few years many works have pro-posed methods that protect the privacy of users in scenarios where location based information is involved. The major-ity of works has focused on protecting the privacy of users when they are communicating with LBS providers that are not trusted. These techniques provide protection to the user in real time against location disclosure (they hide the exact location of the user from the untrusted entities in the com-munication scenario), against identity disclosure (they hide the identity of the user from adversaries that already know other identifying information, like the exact location) and against disclosure of sensitive information (they stop the adversary from inferring that a user visited a certain place or made use of a sensitive service).
 Despite the concern about user privacy both in research and in practice, a vast number of users choose to share their lo-cation with LBS providers and friends in the rapidly grow-ing field of location based social networks. At the same time high quality maps and semantic information on them is freely available in the Internet (e.g., Google Maps). The lo-cations of hospitals, schools, companies is usually marked on the map and even the location of unexpected events becomes very soon available on-line. All these freely available data do not only allow malicious adversaries to collect information about user movement in the city, but also the semantically rich map background allows them to infer her work place, her home address, even information not directly associated with location like religious and sexual preferences. Possibly dangerous attacks are quite easy to perform. To raise aware-ness over the privacy problems of sharing location informa-tion in LBSN s, PleaseRobMe ( http://pleaserobme.com/ ) demonstrated how it can easily infer that someone is out of home and his apartment is empty. The attack was possible by combining data from Foursquare ( http://4sq.com )and Twitter ( http://twitter.com ).
 There are several research works which show that people do not put great value on their privacy, and in general they are willing to share their location data [20; 3; 7; 14]. On the other hand, privacy is something that is valuable when missed. Since sharing location data on this scale is some-thing relatively new, the effects on user privacy are not fully understood yet. Adversaries might appear in the future that will be able to collect data from past years. Even now, ma-licious parties might be collecting data that can be used in thefuture.Thescaleofthethreatonuserprivacyissome-thing that will be understood in long run, but given that user data are made available now, it might be too late. The aim of the paper is to provide an overview on the privacy breaching scenarios that have been studied in data man-agement and knowledge discovery research literature. The paper explores what are the usual assumptions about the adversaries background knowledge, what are the communi-cation architectures that make an attack scenario possible and what information is the target of the attack in each case. The privacy guaranties of the different anonymization meth-ods are categorized and the basic data transformations and the information loss they introduce are explored. The focus in this work is to present a broad picture of the base as-sumptions,the dangers and the privacy guaranties that have been devised to address them, and less emphasis is given to algorithmic issues. In this way, it is aimed to provide the background for assessing whether the privacy problems studied in data management literature are the same with those that are or will be encountered in practice. Privacy protection has attracted significant interest in many research areas. This survey focuses on privacy preserving data dissemination ( PPDD ) in scenarios involving location based data. A basic scenario is depicted in Figure 1. The data owner, Nikos, wants to share some of his data with Maria, without revealing sensitive information. In the case of Figure 1 Nikos wants to inform Maria about his location in an abstract manner, without revealing his exact position. In such scenarios there is a common interest to both the data owner and the data recipient to share information. For ex-ample, the data owner might want to receive a service from the data recipient or the data owner might be someone who publishes data to promote a common cause, e.g. research. Good surveys of the rapidly growing PPDD research litera-ture can be found in [15; 24; 42].
 The predominant paradigm in PPDD when dealing with lo-cation data is that of private data sharing in the invocation of a location based service ( LBS ). Content delivered by lo-cation based services is adjusted to the location of the user that invokes them. For example, a user might transmit his location and ask an LBS provider for the nearest gas sta-tion. To protect user privacy, anonymization methods will transform the user request in such a way that certain in-formation will be hidden from the provider. At the same time, the information that remains in the request should be adequate for the provider to deliver his services and should not introduce significant overheads to any of the two parties. There are many scenarios where the privacy of a user who invokes an LBS is at risk. Scenarios are differentiated based on several factors: a) on the definition of privacy, i.e., what information the user wants to hide, b) on the communica-tion architecture, e.g. an intermediate trusted server might exist between Nikos and Maria, c) on the attack model, i.e., the background knowledge of the attacker, and the infer-ence capabilities she has and d) on the data transformation, i.e, in what way are the original data transformed in order to provide the desired privacy guaranty. In Sections 4-6 I present several works that deal with real-time PPDD in the invocation of LBS .
 Another important scenario of PPDD that involves spatial data is that of the off-line publishing of data collections [64; 39; 53; 22; 70; 5; 4; 52]. The difference from the previous scenario is that the data publisher does not want to share a single location or limited information describing his request, but a large collection of data that describe movement of users over significant time periods. In such scenarios, the an-onymization procedure takes into account the whole dataset and can perform more complicated transformations, but at the same time it has to face attackers who can have sig-nificant background knowledge about user movement. Since the focus of these works are in the off-line publications, they are out of the scope of the paper and they are not presented here.
 There is significant work on privacy protection that focuses on the data access control. In this context, policy models that enable users to easily define who and when can ac-cess their data, and models for the propagation of access rights are investigated. Several characteristic approaches for designing and enforcing access policies specific to loca-tion based data appear in [1; 36; 37; 54; 57]. Such methods are again outside the scope of our work and are not pre-sented in the paper. We only present some methods based on cryptography since they do transform the data and in some cases they are employed for ensuring anonymity in the communication with LBS providers. As location based services become more and more popular, users are allowing possibly unknown service providers to col-lect data about their movement. When a user invokes an LBS she usually sends her exact position to the provider, so that she can receive a response that is adjusted to her location. In this usage scenario a malicious provider can collect accurate movement data about users. Following the collection of the data, the provider can process them and ex-tract potentially sensitive information, e.g. the user X  X  home address, work address, sexual and religious preferences etc. This is a technically feasible scenario that can have signif-icant negative consequences to the user. To address such threats research in PPDD has proposed several anonymiza-tion methods, which guarantee that the LBS provider or any other recipient of the user X  X  location-based request re-ceives an anonymized message, where sensitive information is hidden.
 This section briefly describes some of the basic concepts that appear in the anonymization methods for the real-time com-munication with LBS providers. I present the communica-tion architectures that are most frequently adopted, the ba-sic transformations that are used to anonymize the data and finally the attack models and the privacy guaranties that are offered by the different anonymization methods. As in the case of relational data, anonymization techniques for location data assume that each record is partitioned to quasi-identifiers and to sensitive values . Quasi-identifiers are the possible background knowledge of the adversary. Adversaries can associate the quasi-identifiers with the true identity of a person, thus they can use them to identify the person X  X  record in the published dataset. The associa-tion can be done through external public or private cata-logs where the quasi-identifiers appear together with direct identifiers, e.g., voters X  catalogs. Quasi-identifiers are not considered usually as harmful to the person they describe, and they can be safely disclosed. On the other hand, sensi-tive values are unknown to the attacker and they can cause harm to an individual if they are associated with her or him. In most attack scenarios there is a clear separation between quasi-identifiers and sensitive values, i.e., a value is either a quasi-identifier or sensitive, it can not be both. Still, there are several variations where sensitive values act as quasi-identifiers.
 Privacy guaranties that protect against identity disclose, like k -anonymity [63], make a record indistinguishable from a group of records with respect to their quasi-identifiers. For example, they replace the quasi-identifiers of all records in each group with a common value, e.g, all different salaries are replaced with the average salary of the group. We term these groups with indistinguishable quasi-identifiers as equiv-alence classes . Creating equivalence classes is also the basic idea in privacy guaranties against attribute disclosure, like l -diversity [48]. The difference from the case of protection against identity is that the anonymization procedure im-poses restrictions on the statistical distribution of sensitive values inside each equivalence class. For example, it lim-its the percentage of records that might contain a sensitive value.
 Finally, in the case of location data the space of which users move is termed as map throughout the paper. The PPDD literature adopts several different communica-tion architectures for the provision of LBS .Thethreebasic ones are depicted in Figure 2: a)The users send their re-quests to a trusted server, termed anonymizer , who anonymizes them and then forwards them to the LBS provider, b) the users communicate directly with the untrusted LBS provider and the anonymization happens in the side of the client and c) the users communicate not only with an untrusted LBS provider but also with other users. The basic assumptions in each scenario are briefly presented in the following. In the trusted server scenario which is depicted in the left part of Figure 2, the users do not send their requests di-rectly to the LBS provider, but they communicate through a trusted server, who acts as an anonymizer . The anonymizer receives the original service request and anonymizes it, be-fore forwarding it to the LBS provider. The anonymization procedure involves removing direct identifiers from the re-quest, and then applying some data transformation to the quasi-identifiers that are contained in the message. A bene-fit of this architecture is that the truster server does not have to anonymize requests independently, but it can instead per-form a bulk anonymization on several requests. The bulk an-onymization of requests allows the anonymizer to take into account the information of multiple users, thus it can group the requests into equivalence classes which have some collec-tive common property. For example, a user request can be grouped with other requests that share the same or similar quasi-identifiers in order to provide k -anonymity. The cre-ation of equivalence classes is very difficult without the ex-istence of an anonymizer, thus protection against identity is rarely provided without it. An exception to this rule is Prive which employs a peer-to-peer architecture for creating equiv-alence classes [30]. Protection against location and attribute disclose can be provided in the absence of an anonymizer, but without using equivalence classes. The communication through an anonymizer is the most commonly adopted archi-tecture in literature [69; 31; 51; 19; 59; 46; 34; 8]. A slightly alternative architecture appears in [21; 27] where the trusted server does not communicate itself with the LBS provider, but anonymizes the request and returns it to the client who handles all communication with the LBS provider. The au-thors assume that given the increase of the computational power in handheld devices it might be possible for the ano-nymization to fully take part in the client side. Another common communication scenario where the user does not trust any server, is depicted in the middle part of Figure 2. In this case, the user is responsible for obfuscating the message he sends to the server in order to protect his pri-vacy. The lack of an anonymizer imposes certain constraints on the type of the anonymization that be offered; the an-onymization procedure cannot use information from other users to make the user indistinguishable from other users. Privacy guaranties like k -anonymity or l -diversity (with re-spect to the sensitive values issued by other similar users) cannot be provided.
 Most works that adopt this communication architectures adopt secure protocols that rely on Private Information Re-trieval [17; 43] for the communication between the user and the LBS provider. The first proposal that used PIR to re-move the anonymizer from the communication architecture was [29; 41], and was quickly followed by several other works relying again on PIR [56; 40] and on other cryptographic protocols [55; 58]. In these works, the content of the query is completely hidden from the LBS provider though cryp-tographic means, providing stricter guaranties than most other approaches. On the other hand, the architecture does not allow for hiding the identity of the user or the fact that he has sent a request.
 Cryptographic methods are not the only way to to protect the privacy of users in a communication that does not in-clude an anonymizer. In [71] the user sends a fake location to the LBS provider, which lies close to his actual location, in order to get his nearest neighbors. In [49; 62] the user replaces in the request his real location with a region that contains it. In all cases the protocols guarantee that the exact user location is hidden from the LBS provider. A special case of the untrusted server scenario, is the sce-nario where each user wants to also communicate with other users. There are two versions of this scenario in the litera-ture: a) one where the user does not trust the LBS provider, nor the other users and b) and one where users trust each other but do not trust the LBS provider. The most com-mon application case that adopts the former version of this scenario is that of proximity based services where a service depends on nearby users [49]. For example, a proximity based service might inform a user that some of his friends are at nearby locations. Users in this setting want to com-municate with their friends or be alerted when they are in their neighborhood, without revealing their exact locations. In [55] the authors propose a cryptographic protocol for per-forming the proximity test and in [62; 49] the users send a obfuscated version of their location, so they can get approx-imate answers about to the proximity test.
 In [30] the user trusts all other users and communicates with them in a structured peer-to-peer network. The adversary in this scenario is the LBS provider and the users collabo-rate in order to protect themselves against identity disclo-sure. Users need a trust certification to participate in the network, but they are all considered trustworthy. They are organized in clusters, and each cluster has a leader. Leaders are organized recursively to other clusters with new leaders. The network architecture allows each user to acquire infor-mation from nerby users in order to anonymize his request before sending it to the LBS provider. It is one of the few example where users can achieve protection against identity and creation of equivalence classes without the need of an anonymizer.
 A drastically different architecture for real time communica-tion between users appears in [26]. The paper focuses on pri-vacy preservation in Mobile Ad-hoc Networks (MANETs). Figure 3: Spatial cloak with arbitrary and grid based cloak-ing regions MANETs are self-organized networks of mobile users who communicate in order to exchange information. In this set-ting there is no LBS provider or anonymizer and queries are answered by other members of the network. Every user is considered untrusted and each user is responsible for anonymiz-ing his own messages. In this architecture it is very hard to create equivalence classes and provide protection against identity disclosure. The core idea in PPDD is to provide data where certain information has been obfuscated. Depending on how the data have been transformed certain properties can be hid-den in order to protect the user privacy and at the same time other data properties are preserved in order to keep data useful. This section describes the most common data transformations that are employed in the various anonym-ization methods. One of the most popular and intuitive data transformations is spatial cloaking [35], where the exact location of a user is replaced by a broader region termed cloaking region (CR) which almost always contains it. For example, an LBS user walking in the center of Athens, might replace her exact lo-cation as reported by the GPS or the wireless positioning functionality of her handheld device, with the region that covers her nearby building blocks and streets or even with a predefined region, e.g.,  X  X thens center X . The cloaking re-gion CR ( p )ofapoint p iscreatedinsuchawaythatitvali-dates a certain privacy predicate PP , i.e., PP ( CR )= true . For example, PP might require that the number of users who exist in CR is over k , thus guaranteeing k anonymity. The various PP s are discussed in Section 6.
 Spatial cloaking is an adjusted form of the generalization technique used for relational [44], transactional [65] and other types of data. In privacy preserving techniques for publishing relational data it is common to generalize numer-ical data to numeric ranges, e.g., 8 to [5  X  10] or categorical data according to some predefined hierarchy, e.g.  X  X kimmed milk X  to  X  X ilk X . In a similar way, the exact location of users, expressed usually by two coordinates (or three if they are timestamped), is transformed to an arbitrary region that meets the privacy requirements of the data publisher or to predefined regions with known characteristics.
 One approach in creating the CR is to use a predefined grid for the map where the users move. The actual location of the user is replaced by a grid cell or by many cells in order to validate the selected PP . Several approaches use a hier-archical organization of grids cells [51; 35]. Partitioning the map to predefined cells is a popular method since it is com-putationally less expensive than creating arbitrary regions and many works adopt it [49; 62; 21; 33; 35; 19; 51; 46; 8]. Creating arbitrary regions can provide better utility to the anonymized data, since the algorithm can enlarge CR only as much as needed to validate PP and it is not constrained by the grid granularity. The downside is that it is computa-tionally more expensive and it is more prone to minimality attacks [67] as shown in [31; 38]. It is adopted by numer-ous anonymization methods that address both identity and location disclosure [25; 10; 18; 31; 34; 38; 27; 69; 59; 26]. Note that the CR might be a set of disjoint regions and it might even not contain the original location [23]. A number of approaches instead of cloaking the user loca-tion, transform all the data space to a new space, usually with cryptographic methods. The LBS provider works on this transformed space, but is not able to interpret user data. Most of the methods in this area rely on Private Information Retrieval (PIR) protocols [17; 43], which allow the client to retrieve data from a database without the database server learning what information was retrieved. The data in the LBS provider are encrypted and both sides exchange en-crypted messages. At the end of the communication the client is able to decrypt the message of the server which contains the requested information. The idea was first in-troduced in [29] and independently in [41] and was followed by several other works [56; 40]. In another cryptographic approach [62], the authors propose a protocol that allows private proximity testing between users of a mobile social network. The proposed solution is based on sharing of cryp-tographic keys between friends. Another cryptographic so-lution that allows encrypted communication between LBS users is proposed in [58].
 The basic advantage of the cryptography based methods is the strong privacy guaranties they provide. Unfortunately, this comes at a significant computational cost that does not make them ideal solutions in the case of LBS with a very large number of users.
 Finally, in order to combine the merits of the strong privacy guaranties provided by PIR based methods with the low computational cost of the protocols that send an obfuscated version of the users X  location to the LBS server, the authors of [28] propose a hybrid method that combines both PIR and spatial cloaking. In this approach, a broad CR is created for obfuscating the user location and it is sent to the LBS provider. After receiving the CR , the user and the LBS provider engage into a cryptographic protocol that allows retrieving refined results, without the server being able to learn the user location with more accuracy than that of the CR . A simple and intuitive approach that significantly differs both from cloaking and transformation based techniques was recently proposed in [71]. In the setting of [71] the user com-municates directly with the LBS server and sends a request reporting an anchor , i.e., a location that is different from his actual location, but lies in a close distance. The basic idea is that the LBS will provide a set of answers that will fit the position of the anchor. The correct answers for the actual location of the user, will be a subset of the solution provided for the anchor, thus the user will be able to filter them in the client side.
 A dummy based approach is also followed in [16]. The dif-ference here is that the user does not send one location to the LBS provider, but instead he sends l different locations, including his real one. The rest of the locations are dummy locations. In most application scenarios in PPDD the user location is coupled with a timestamp. Obfuscating time, might be an option for the off-line scenarios, but it is not an easy op-tion for real time anonymization methods since in the latter case the timestamp can be inferred by the time the adver-sary received the request. In approaches that protect only against location disclose and do not require creating equiv-alence classes, as discussed in Section 4.2, time could be re-ported accurately. When the creation of equivalence classes is necessary the only way to create them is to delay some requests, until the anonymize has accumulated enough re-quests to create an equivalence class that satisfies the PP . This approach is followed by numerous works in this area [27; 34; 8]. Usually there is a limit on how long a request can be delayed and if it is not anonymized in time, the request is rejected. In [18] where the anonymizer and the adver-sary monitor users continuously, users are allowed to issue queries only if they are already grouped in an anonymous equivalence class. Privacy, even when restricted to the LBS case, has many different meanings and it can be breached in various scenar-ios. Research literature has proposed a variety of privacy guaranties, adjusted to different attack scenarios. The at-tack scenarios vary depending on the background knowledge of the attacker, on the role of the attacker in the communi-cation architecture, on the goals of the attacker and finally on what the user perceives as important sensitive informa-tion. We can divide the various attack modes and privacy guaranties in two main classes: a) attacks against the iden-tity, i.e, the attacker is interested in learning which user issued a request and b) attacks against content, i.e., the at-tacker is interested in associating a request (from a user he already knows) to a certain property, e.g., exact location. Attacks against content focus mainly on the current loca-tion of the user. There are several anonymization methods that provide protection both against identity and content disclosure. Finally, an important class of attack models are those where the attacker can monitor user movement and perform a continuous attack. In the following, some of the most characteristic attack scenarios and privacy guaranties are presented. Scenarios of attacks against user identity usually assume the architecture on Scenario A from Figure 2, where an inter-mediate trusted server handles the anonymization. In these scenarios the adversary is usually the LBS provider. Pro-tection against identity disclosure is provided by anonym-ization methods that guarantee k -anonymity for users that send requests to an LBS server. In the basic scenario for k -anonymity in relational data, an adversary knows a) that a user exists in a dataset and b) the quasi-identifiers that are related with each user, e.g., age, zip code etc.. In the context of LBS almost all anonymization methods consider the location of the user as a quasi-identifier. The task of the anonymizer is to make each user indistinguishable to the LBS provider from at least other k  X  1 users, whose lo-cations are usually close (depending on the information loss metric). Because k -anonymity requires knowing the quasi-identifiers of at least k -users,itisveryhardtoprovideitin scenarios where the trusted server does not exist. A good survey for k -anonymization techniques in LBS appears in [32].
 In the simplest scenario, the adversary wants to know the identity of a user who issued a service request. His back-ground knowledge (at worst case) consists of a) the knowl-edge that a user issued a request and b) the location of all users. The data that are available to the adversary are only 1 request per user that contains her location. An intuitive but naive solution based on spatial cloaking is depicted in theleftpartofFigure4. Assumethatuser A issues a re-quest and the anonymizer wants to make her indistinguish-able from other k  X  1 users. The anonymizer can issue a k -nearest neighbor ( k NN) query to retrieve the k  X  1clos-est users to A and then replace their precise location with the MBR of all their locations ( CR 1 in Figure 4). This technique is followed by Center Cloak [38].
 Casper [19; 51] is grid based approach which provides k -anonymity in the same attack scenario and follows a similar technique. Casper creates a pyramid, quad-tree like struc-ture that partitions the entire map in disjoint cells. The cells of each layer are grouped to larger cells in higher levels of the hierarchy. When a user issues a request, the anonymizer first checks the smallest cell that contains the user location and sets the CR to the cell area. If there are k total users in the cell, it replaces the user location with CR and forwards the request to the anonymizer. Else, it starts augmenting the CR by adding neighbor cells until the k -user constraint is satisfied.
 A similar approach is followed by the bottom-up algorithm of PrivacyGrid [8]. The bottom-up algorithm starts again by setting the CR as the smallest cell that contains the user location and expands it until the privacy criteria are met. In the expansion phase the neighbor grid cells that contain most other users are added first to the CR . Priva-cyGrid proposes also a top-down algorithm that starts from the maximum CR , which is defined based on user prefer-ences about service utility. If the maximum CR satisfies the privacy criteria then the algorithm proceeds by remov-ing a row or a column of cells from CR until it reaches a CR which is not privacy preserving any more. At each step the row or column with the smaller object count is removed. PrivacyGrid provides both k -anonymity and l -diversity. In another approach Interval Cloak [35] creates again a hier-archical grid on the user space and generates the CR not by unifying neighbor cells, but instead it climbs up in grid hierarchy and replaces the user location with higher level cells.
 A significant problem of the aforementioned approaches is that they are susceptible to minimality attacks [67]. The cause of the problem is that they start with the user lo-cation as the center of CR andthentheyexpanditina deterministic way, trying to minimize information loss. As a result, a different characteristic CR can be created for each request. Consider the naive anonymization shown in the left part of Figure 4. The anonymization algorithm pro-vides 3-anonymity by replacing the user location with the bounding MBR of its 2 nearest neighbors B and C , cre-ating CR 1 . In the right part of Figure 4, the CR sthat are created to protect users B and C ( CR 2 and CR 3 re-spectively) are depicted. The CR in each case is different. This fact allows the adversary to infer that the request with CR 1 comes from user A . This problem is studied exten-sively for the first time in [30; 38] and independently in [18]. The solution proposed in [30; 38] and extended in [31] is to add the requirement for reciprocity in the creation of a CR for k -anonymity. A CR that contains k users satisfies reci-procity only if the same CR is generated for everyone of the k users. If the same CR is generated for every user of the same equivalence class, then the attacker cannot infer which user is the source of a request with probability over than 1 /k . The Hilbert Cloak ,proposedin[38],usesaHilbertcurveto transform the two dimensional space of user locations to a one dimensional space. The Hilbert Cloak transformation preserves the spatial locality; points that are close in the two dimensional space are usually close in the one dimen-sional space. Following the transformation of the 2-D space, the algorithm places the users to buckets of size k .Users are placed to buckets according to their Hilbert value, i.e., the k users with the smallest values are placed to the first buckets, the next k users to the second etc. The CR of any user is the MBR that encloses all users in his bucket. In [31] an improvement in terms of efficiency of Hilbert Cloak is proposed, the Greedy Hilbert Partitioning (GH) .TheGH indexes the data with an R-tree and then limits the Hilbert transformation to the subtree that contains the user whose request needs to be anonymized and at least k  X  1other users. The algorithm that offers the best quality CR for reciprocal spatial k -anonymity comes again from [31] and it is based on the partitioning algorithm of the R*-tree [9]. The difference is that instead of using the classic partition-ing heuristic of the R*-tree, the authors use a heuristic that takes into account the size of the CR s that are going to be created in each partitioning.
 Prive [30] provides k -anonymity with the reciprocity re-quirement, but works under a very different communication architecture. In Prive there is no anonymizer and users communicate directly with the LBS provider. Each user is responsible for protecting her own identity by using spatial cloaking. A user creates the CR for her position by taking into account the locations of other nearby users. A CR al-ways contains at least k users. The CR for every user of the same equivalence class is the same, and this is achieved by a similar approach with the one of [38]. Every user is assigned a Hilbert value, which is stored in a distributed B-tree like index among the users. When a user needs to cloak her location, she issues a k -request query to the index and receives the CR that satisfies k -anonymity and reciprocity. The previous methods provide k -anonymity under the as-sumption that the adversary knows only the current location of the user. Still, in various application scenarios the user must communicate with the LBS provider multiple times, under the same id. Even if the id is a pseudo-identifier in-serted by the anonymizer, the adversary gains significant additional knowledge: the previous anonymized locations of the user. An adversary who knows the CR sofprevioususer requests, can intersect the users that lie inside each CR and limit the candidates for a user id to less than k .Tocounter such attacks each user has to be grouped in all his requests with the same k  X  1 other users. In [18] an anonymization method that addresses the problem of adversaries who con-tinuously monitor the movement of a user is addressed. The work of [34] applies to a similar setting, but here the ad-versary is also able to extract frequent movement patterns from historical data.
 In [10; 68] the authors also propose anonymization methods that provide k -anonymity in application scenarios with con-tinuous queries, but there is a significant difference from the works of [18; 34]: the adversaries here cannot intersect the CR s of all requests to find the common users. Adversaries in this setting can only be aware of frequent patterns in their historical information. The proposed anonymization meth-ods obfuscate the position of a user, by creating a CR that takes into account previously traveled trajectories. The location of a user can act as quasi-identifier, as it usu-ally assumed in the attacks against identity, but it can also be a sensitive value. Unlike the approaches in relational privacy preservation, where most methods avoid transform-ing the sensitive value, in the LBS setting transforming the location is the most common method for protecting user privacy. Again most anonymization methods rely on spatial cloaking. The location of users is often protected by privacy guaranties that are based on l -diversity. l -diversity in the spatial context is achieved in a different way than in the re-lational context. Instead of creating equivalence classes of users with l different well represented sensitive values, the sensitive value, i.e., the user location, is generalized to a CR that contains l well presented values. For example if a user has visited a hospital and the hospital is a sensitive value, instead of grouping him with other users, the anonymiza-tion method creates a CR which contains the hospital and Figure 5: MANET forwarding with spatially cloaked users. several other locations.
 A beneficial consequence of avoiding to create equivalence classes is that the anonymization procedure for protection against location disclosure is cheaper than providing protec-tion against identity disclosure. Instead of requiring track-ing and processing other dynamic information (e.g., the lo-cations of other users) the anonymization algorithm only needs to know the map and its properties. The simplest case of protection against location disclosure is to guarantee that the user location cannot be traced with a granularity less than a threshold, i.e., to put a direct con-straint on the size of CR . This is the approach of Casper [51] for providing location privacy (complementary to the k -anonymity). The same type of location protection is pro-posed by SpaceTwist [71]. The approach of SpaceTwist is suitable for privacy preservation in LBS services that pro-vide answers to k NNqueries. The user instead of sending a request with his own position, sends the request with the location of a predefined point on the map termed anchor that lies close to him. The LBS provider starts sending in-crementally the nearest neighbors of the anchor to the user. The user filters these results and when he has the k answers he needs, he signals to the LBS to stop the transmission of results. An approach based on the creation of dummy loca-tions is proposed in [16]. In this scenario the user sends a request directly to the LBS provider and inserts in his re-quest l locations, where one is his real location and the rest l  X  1 are dummies. The dummy locations are created is a way that reflects real movement. Dummy generation algorithm remembers the previous locations of dummies and creates the new one in their proximity. Moreover, the density of users in each neighborhood is taken into account. Protection against exact location disclosure, but in a very different communication scenario, is studied in [26]. The paper considers privacy issues in Mobile Ad Hoc Networks, where there is no anonymizer or LBS provider. In this sce-nario, depicted in Figure 5, the network is comprised only by users. The requests and the information pass from one user to another. A significant class of routing protocols in MANETs forwards the requests according to the location of each user. For example, user A in Figure 5 who wants to send a message to user C , will send first his message to B , who will then forward it to C . In location-aided routing protocols a user must be aware of the location of other users, in order to forward a request to the most suitable one. The work of [26] assumes that each user might be a potential ad-versary, and the sensitive information is location itself. The authors propose adjusting the location-aided routing proto-col to spatially cloaked user locations. This way each user can protect her exact location and still allow for the rout-ing algorithms to work. The privacy guaranties that are supported by the proposed routing protocol are guaranties against location disclosure; depending on how the CR is cre-ated, it can provide protection against exact location disclo-sure or a bound to the probability of associating a sensitive feature with a user.
 The most effective privacy preserving solutions, i.e., those that provide the stronger protection, are based on crypto-graphic methods, like PIR [17; 43]. For example, [56] guar-anties strong location privacy in the evaluation of k NNqueries; the adversary does not learn anything about the location of auser. Themethodproposedin[29]providesthesame strong location privacy but only for single k NNqueries. In [41; 40] another method for evaluating k NNqueries is pro-posed. A k NNquery is answered through multiple requests. Each single request does not reveal anything about the user location but the adversary could exploit the cardinality of the requests per k NNquery to reveal location information. An additional good property of PIR based methods is that they protect against correlation attacks. Adversaries who can continuously monitor a user, cannot intersect different CR s to find the common users, since no location information is revealed to them at any time. Putting a minimum size constraint to the CR is an easy and practical privacy protection measure but it is not necessarily meaningful in every case. For example a CR at the city center might contain many different points of interest, thus effectively obscuring the location and the possible activity of a user. On the other hand, the same CR in a large university campus might not really hide anything; the adversary does not know where in the campus the user is, but he knows she is at the university. To address these types of problems many methods perform some type of semantic preprocessing to identify spatial features on the map where the users move. A feature is an area or a point that represents an entity with spatial extent, e.g., a hospital. Using this preprocessing they can define variations of l -diversity that are based on the number (or spatial extent) of features that appear in the CR .
 In [23] the movement space is modeled as a connected graph where nodes stand for spatial features (e.g., hospitals, schools etc) and edges denote that one location is a direct neighbor of another. The CR in this case is not a continuous area in space, but a set of locations that do not necessarily consti-tute a single connected component. As a result, the provided privacyguarantyisatypeof l -diversity where the actual po-sition of a user might be in any of the semantic locations in CR .In PrivacyGrid [8] the authors provide l -diversity com-bined with k -anonymity. The l -diversity is provided in terms of features of the map as in [23], and the CR is created so that it contains at least l different features. The same guar-anty appears as Weak Location Privacy in [69]. The CR here is the MBR of the l locations chosen to obfuscate the position of a user.
 In [69] another form of location l -diversity, the Strong Lo-cation Privacy (SLP) , is presented, that protects against adversaries who have as background knowledge the associ-ation between a query request and a spatial feature. Such adversaries know the probability distribution function ( pdf ) of any query over all map features. For example, an adver-sary might know that queries for nearby gas stations come more frequently from users that are in the street than from users who are in a hospital. The SLP of [69] guarantees that even if the adversary knows the pdf of a query, he will not be able to associate a query request with any feature with over a probability threshold. In [21] the authors adopt a similar privacy guaranty. The spatial features are catego-rized as sensitive and non sensitive. The privacy of a user is endangered only if she is in a sensitive location. In this case her location is cloaked with a CR , in such a way that the attacker cannot associate her with any sensitive feature that lies inside the CR with probability over 1 /l .Anovelty of the approach is that the CR s are created off-line and only for sensitive regions . Figure 6 depicts two CR screatedin the PROBE frameworks to mask the locations of users who are in the two sensitive features of the map, the hospital and the temple. The CR s are depicted as gray boxes in the map. If a user issues a request from any location inside a CR , then her location is replaced with the CR in the an-onymization. Even if the user is not in the hospital or the temple, but she is simply inside the CR , her location will be obfuscated. This technique protects the user location from reverse engineering attacks. The attack model assumes that the adversary has the pdf of users in any point of the map. For example, the adversary might know that the probabil-ity of a request coming from the river in Figure 6 is 0 and that the respective probability for the hospital is 0 . 05. The probability of a user whose location has been obfuscated to be at a specific feature is calculated based on the pdf ,the area of the feature and the total area of the CR . A special case of location privacy arises in the usage of prox-imity services. In the delivery of proximity services users not only communicate with the LBS provider but they also com-municate with each other, as depicted in the last scenario of Figure 2. In these scenarios the users want to hide their exact location both from the LBS provider and from other users, without the help of an anonymizer. At the same time they want to know if they are in the proximity of some other users. A user A is in the proximity of user B if their distance is smaller than a threshold d . In the solution proposed in A
BCR Figure 8: Reachable areas in successive CR s with maximum velocity v [49; 50] the users create a CR based only on their prefer-ences for their location and they send it to the LBS server. The LBS server can calculate the minimum and maximum distance between the users as shown in Figure 7. If the proximity threshold is greater than the maximum distance then the two users are certainly in proximity so they can get a definite answer. They get also a negative answer if the minimum distance is greater than the proximity threshold. In the in-between cases, the users engage direct communi-cation through an secure-two-party computation protocol that allows them to verify whether they are in proximity or not, without revealing their actual locations. In a different approach, FriendLocator [66] and VincinityLocator [62] al-low users to perform proximity tests through an encrypted protocol. The map is partitioned in grid cells and the user creates a CR as a collection of neighboring cells. The cell ids are then encrypted and sent to the LBS provider, who can decide on the proximity based on the encrypted values. An approach based on a distance-preserving mapping is pro-posed in [60]. A weakness of this solution is that it is possible for the attacker to easily guess the mapping function [47]. A realistic but significantly more powerful adversary is con-sidered in [27]. As already mentioned in the methods for protecting against identity disclosure in Section 6.1, there are several practical scenarios where the user has to com-municate multiple times with the adversary under the same pseudonym. This allows the adversary to intersect the equiv-alence classes that correspond to the different CR s and nar-row down the potential real users that might have issued a request. The authors of [27] observe that in real sce-narios the adversary can easily have additional background knowledge about the speed limits of different transportation means. For example, if the adversary infers that a user is on a car he can be certain that he will not be moving with more that 200km/hour in a city environment. The knowl-edge about the maximum velocity of a user can be exploited by an adversary that monitors continuously the requests of a user. If the adversary knows the cloaking regions CR 1 and CR 2 of the positions where the same user issues two re-quests, he can calculate the maximum distance a user might have traveled from every point of CR 1 and prune parts of CR 2 using the maximum velocity, as shown in Figure 8. The adversary can also do the reverse pruning: he can prune the parts of CR 1 that are so far away from any point of CR 2 that the user could not have been there if he issued a re-quest from CR 2 in a later time. The paper proposes an anonymization method that creates CR sinsuchawaythat an adversary, who can continuously monitor user requests and has the maximum user velocities as background infor-mation, cannot pinpoint the exact user location with accu-racy greater than the intended CR . The paper provides a more powerful guaranty against attackers who additionally have semantic information about the map and know the sen-sitive spatial features on it, e.g., hospitals. In this case the anonymization algorithm guaranties that the adversary will not be able to associate a user with a sensitive feature with probability more than a threshold. The probability that as-sociates a user with a sensitive feature is calculated based on the area of the feature that lies in a CR to the total area of the CR . Apart from the identity and the location of the user, it is also the rest of the query content that might be sensitive. An anonymization method that protects the query content from adversaries that might use location as a quasi-identifier is presented in [59]. In this setting the attacker can keep the history of previously issued queries, and based on this infor-mation he can create the pdf of each query (different service invocation) to each user. In other words he has the proba-bility that associates each different service request with each user. This background knowledge can be used to decide that not all users in an equivalence class are associated with dif-ferent requests with equal probability. Equivalence classes are created again by spatial cloaking. The proposed method guarantees k -anonymity and t -closeness [45]. A CR has al-ways more than k users to provide k -anonymity but also the distribution of requests from users in CR does not differ by the global pdf more than a certain threshold. For example assume a threshold t =5%andaservice s with a probability p = 30%. According to the t -closeness guaranty of [59], the probability p with which the adversary is able to associate any user from a single CR with a request of service s lies inside the bounds p  X  t  X  p  X  p + t , i.e, 25%  X  p  X  30%. Finally, in [46] the authors consider l -diversity for the query content, and location only acts as a quasi-identifier. The cre-ation of equivalence classes is performed by spatial cloaking. The authors define query entropy qe as qe =  X  p i log p i where the p i is the probability that a query q is issued. The l diversityisachievedbyboundingthe qe of each query in an equivalence class with log l . Location Based Social Networks ( LBSN ) become increas-ingly popular and a growing number of users participates by sharing their location information. Users seem to ignore pri-vacy related dangers and simply post their location to wide audiences with little regulation. In LBSN s, like Foursquare ( http://4sq.com ), users alert their friends when they are in certainlocationby X  X hecking-in X  X ndtheyevenmakethis information available to wider audiences by having their alerts being forwarded to Twitter ( http://twitter.com ). The potential dangers are high; for example, PleaseRobMe ( http://pleaserobme.com/ ) demonstrated that it is possi-ble to use this information to infer when users are out of their apartments. There have also been a series of stud-ies which show that users do not put great value on their location privacy [20; 3; 7; 14].
 On the other hand, privacy is not an obvious quality and people are mostly aware of it when it is missed. For exam-ple, when [20] examined the value that users put to privacy by nationality, it noticed that Greek citizens valued their privacy many times more that the rest of nationalities par-ticipating in the survey. The paper attributes this deviation to the fact the privacy related issues have been widely dis-cussed in Greece in the period before the survey, following an eavesdropping scandal. Moreover, [13] supports that the participation of young adult users in Facebook by making their their data available, does not signify that they do not care for their privacy, since they actively adjust their pri-vacy settings. In [6] the authors support that the users X  desire to protect their privacy is not always consistent with their behavior. Often they are not well aware of the public nature of the network and of their audience [61; 12]. [11] argues that electronic publication has disrupted the bound-aries between public and private and that users X  control over data has been undermined. The paper documents four ba-sic characteristics of the electronic publication of personal data, which disrupt the users X  efforts to control their privacy in social networks: These properties make the communication and the dissem-ination of personal information in the Internet drastically different from the non-digital and especially the oral com-munication. Publishing information in such an environment has implications on users X  privacy that are not easily foreseen by a large part of LBSN users. There are several initiatives to help users enforce more effective control on their data. For example, following a regulatory approach, the French government wants to give to users the  X  X ight to be forgot-ten X  [2]. Practically, this is translated to the right of having personal information removed from the WWW after some time, in order to limit the persistence of data. Still, it is not easy to see how such a measure can be enforced from a technical point of view.
 An important challenge for PPDD is to provide techniques that will help users wield better control over their data. Un-til now privacy in LBSN has only been protected by allow-ing users to set an access policy for their data. The policy based paradigm is a poor tool for controlling information dissemination. Data can be either accessible or inaccessi-ble by a certain user or group. On the other hand, in real life the same information is disseminated with varying de-grees of accuracy depending on the audience. In a similar way, PPDD can help to enrich privacy policies in LBSN , by offering varying degrees of access. Following techniques like spatial cloaking, information can be accessed in different degrees of accuracy depending on the audience. Moreover, regulatory approaches could specify more refined rules us-ing privacy guaranties from PPDD about how information about minors or other user groups should appear in social networks. The focus of this survey is on the privacy preserving data dis-semination techniques that have been developed especially for spatial data. These techniques preserve users X  privacy by transforming the data that describe them. The transfor-mation procedure, termed anonymization , is performed to prohibit malicious adversaries from using background infor-mation, to identify people in the published data. The survey presents a broad range of techniques focusing on the models that were adopted at each case.
 The majority of works in PPDD for location data studies the problem of the real time communication between users and LBS providers. In Sections 4-6, a classification of the methods was presented based on the communication archi-tectures, on the data transformation that is used by the ano-nymization algorithm and on the attack and privacy model that is adopted. Basic properties of the different privacy preservation methods were highlighted. We saw that archi-tectures without an anonymizer cannot easily support ano-nymization methods that require the creation of equivalence classes and that spatial cloaking achieves location based pro-tection not by grouping records, but by grouping features on a map. The case of most powerful attackers, who can moni-tor continuously the requests of a user was explored and key ideas for addressing such attacks were presented. There are several technical challenges that arise in PPDD for location data. The case of continuous attacks, which is a realistic scenario for many applications, has only been partially studied. Moreover, most works focus on devising strong privacy guaranties and the data utility is studied only through a few artificial metrics. Investigating in more depth how to efficiently preserve data utility remains an important issue in PPDD . Apart from the technical challenges, there is a important question about how the existing research an-swers the problems of privacy protection in the location based social networks , which are becoming increasingly popular. Users publish more and more information about their lo-cation and movement, without often being aware of how exposed they are. There are already several studies in social sciences that investigate user behavior and privacy dangers in the social networks [20; 3; 7; 14; 61]. A challenge for the PPDD research is to bridge the gap between the low level anonymization techniques, which provide strict but limited guaranties (e.g., identity protection through k-anonymity, protection of sensitive information using l-diversity etc.) and the higher level user requirements on privacy like audience regulation, limitations to data persistence etc. [1] Geopriv (http://datatracker.ietf.org/wg/geopriv/charter/). [2] http://www.gouvernement.fr/gouvernement/charte-[3] Boombox report on location-based social networks, [4] O. Abul, F. Bonchi, and M. Nanni. Never walk [5] O. Abul, F. Bonchi, and M. Nanni. Anonymization of [6] A. Acquisti and R. Gross. Imagined Communities: [7] S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, [8] B. Bamba, L. Liu, P. Pesti, and T. Wang. Support-[9] N. Beckmann, H.-P. Kriegel, R. Schneider, and [10] C. Bettini, X. S. Wang, and S. Jajodia. Protecting pri-[11] D. Boyd. Social network sites: Public, private, or what? [12] D. Boyd and N. Ellison. Social network sites: Defini-[13] D. Boyd and E. Hargittai. Facebook privacy settings: [14] A. J. B. Brush, J. Krumm, and J. Scott. Exploring [15] B.-C. Chen, D. Kifer, K. LeFevre, and A. Machanava-[16] E.-A. Cho, C.-J. Moon, H.-S. Im, and D.-K. Baik. An [17] B. Chor, O. Goldreich, E. Kushilevitz, and M. Sudan. [18] C.-Y. Chow and M. F. Mokbel. Enabling private con-[19] C.-Y. Chow, M. F. Mokbel, and W. G. Aref. Casper*: [20] D. Cvrcek, M. Kumpost, V. Matyas, and G. Danezis. A [21] M. L. Damiani, E. Bertino, and C. Silvestri. The [22] J. Domingo-Ferrer, M. Sramka, and R. Trujillo-Rasua. [23] M. Duckham and L. Kulik. A formal model of obfusca-[24] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu. [25] B. Gedik and L. Liu. Location privacy in mobile sys-[26] G. Ghinita, M. Azarmi, and E. Bertino. Privacy-aware [27] G. Ghinita, M. L. Damiani, C. Silvestri, and E. Bertino. [28] G. Ghinita, P. Kalnis, M. Kantarcioglu, and E. Bertino. [29] G. Ghinita, P. Kalnis, A. Khoshgozaran, C. Shahabi, [30] G. Ghinita, P. Kalnis, and S. Skiadopoulos. Prive: [31] G. Ghinita, K. Zhao, D. Papadias, and P. Kalnis. A [32] A. Gkoulalas-Divanis, P. Kalnis, and V. S. Verykios. [33] A. Gkoulalas-Divanis and V. S. Verykios. A free terrain [34] A. Gkoulalas-Divanis, V. S. Verykios, and M. F. Mok-[35] M. Gruteser and D. Grunwald. Anonymous usage of [36] U. Hengartner and P. Steenkiste. Implementing access [37] U. Hengartner and P. Steenkiste. Access control to peo-[38] P. Kalnis, G. Ghinita, K. Mouratidis, and D. Papadias. [39] E. Kaplan, T. B. Pedersen, E. Savas, and Y. Saygin. [40] A. Khoshgozaran, C. Shahabi, and H. Shirani-Mehr. [41] A. Khoshgozaran, H. Shirani-Mehr, and C. Shahabi. [42] J. Krumm. A survey of computational location pri-[43] E. Kushilevitz and R. Ostrovsky. Replication is not [44] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. [45] N. Li, T. Li, and S. Venkatasubramanian. t-Closeness: [46] F. Liu, K. A. Hua, and Y. Cai. Query l-diversity in [47] K. Liu, C. Giannella, and H. Kargupta. An attacker X  X  [48] A. Machanavajjhala, J. Gehrke, D. Kifer, and [49] S. Mascetti, C. Bettini, D. Freni, X. S. Wang, and [50] S. Mascetti, D. Freni, C. Bettini, X. Wang, and S. Ja-[51] M. F. Mokbel, C.-Y. Chow, and W. G. Aref. The [52] A. Monreale, G. L. Andrienko, N. V. Andrienko, [53] A. Monreale, R. Trasarti, C. Renso, D. Pedreschi, and [54] G. Myles, A. Friday, and N. Davies. Preserving privacy [55] A. Narayanan, N. Thiagarajan, M. Lakhani, M. Ham-[56] S. Papadopoulos, S. Bakiras, and D. Papadias. Nearest [57] N. Poolsappasit and I. Ray. Towards achieving person-[58] K. P. N. Puttaswamy and B. Y. Zhao. Preserving pri-[59] D. Riboni, L. Pareschi, C. Bettini, and S. Jajodia. Pre-[60] P. Ruppel, G. Treu, A. Kupper, and C. Linnhoff-[61] N. M. Sadeh, J. I. Hong, L. F. Cranor, I. Fette, P. G. [62] L. Siksnys, J. R. Thomsen, S. Saltenis, and M. L. Yiu. [63] L. Sweeney. k -Anonymity: A Model for Protecting Pri-[64] M. Terrovitis and N. Mamoulis. Privacy Preservation [65] M. Terrovitis, N. Mamoulis, and P. Kalnis. Privacy-[66] L.  X  Sik X  snys, J. R. Thomsen, S.  X  Saltenis, M. L. Yiu, [67] R. C.-W. Wong, A. W.-C. Fu, K. Wang, and J. Pei. [68] T. Xu and Y. Cai. Exploring historical location data [69] M. Xue, P. Kalnis, and H. K. Pung. Location diversity: [70] R. Yarovoy, F. Bonchi, L. V. S. Lakshmanan, and W. H. [71] M. L. Yiu, C. S. Jensen, X. Huang, and H. Lu.
