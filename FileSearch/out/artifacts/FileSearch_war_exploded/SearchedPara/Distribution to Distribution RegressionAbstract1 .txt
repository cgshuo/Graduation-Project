 Junier B. Oliva joliva@cs.cmu.edu Barnab  X as P  X oczos bapoczos@cs.cmu.edu Jeff Schneider jeff.schneider@cs.cmu.edu 5000 Forbes Avenue Pittsburgh PA 15213 USA In standard regression analysis, one is concerned with inferring a mapping of a real valued vector of covariates (features) X  X  R d to a real valued vector response Y  X  R k . While such a model encompasses many real-world problems, the restriction of finite dimensions on input and output domains excludes the regression of more complex objects. For example, in functional analysis one considers regression where the input domain are functions (Ferraty &amp; Vieu, 2006).
 Probability distributions are another infinite dimen-sional domain of interest for regression. Recently, (Poczos et al., 2012) considered regressing a mapping of probability distributions to a real valued response. Instead, in this paper we study distribution to distri-bution regression , where both the input covariate and the output response are probability distributions. Fur-thermore, we take a nonparametric approach, making as few and as weak assumptions as possible on the nature of input/output distributions and the mapping between them.
 This framework is quite general and applicable to many real-world problems. For instance, consider the case where one collects samples at evenly spaced times t ,...,t M , then given the probability distribution at time t i it is natural to try to predict the distribution at time t i +1 . Furthermore, there are many more domain-specific uses of distribution to distribution regression. For example, in business one may consider the map-ping of the distribution of some weather features to the distribution of some shipping route features. Also, in finance one may be interested in the mapping of the distribution of one sector X  X  prices to the distribution of prices for another sector.
 Our main contributions are as follows. First, we de-velop a nonparametric estimator for distribution to distribution regression. Second, with weak assump-tions on the nature of input/output distributions and the measure that input distributions are sampled from, we derive an upperbound on the rate of convergence for the L 2 risk. Lastly, we show that if the measure that input distributions are sampled from is a doubling measure, then the rate of convergence for the L 2 risk is polynomial.
 Let I be a class of input distributions on  X  k  X  R that have a density with respect to the Lebesgue mea-sure. Similarly, let O be a class of output distributions on  X  l  X  R l that have a density with respect to the Lebesgue measure. We regress a functional f : I 7 X  X  . Consider a set { ( P 1 ,Q 1 ) ,..., ( P M ,Q M ) } where P I , Q i  X  O , and Q i = f ( P i ). Under the usual regression setting one would observe pairs of vari-ables directly, however, we shall not since typically the true distribution of a real-world sample is un-known. Instead, we will consider the case where one only indirectly observes input/output distribu-tions through i.i.d. samples ( X i , Y i ) from P i and Q i respectively. That is, one has a dataset of in-put/output samples D = { ( X 1 , Y 1 ) ,..., ( X M , Y M where X i = { X i 1 ,...,X in i } with each X ij iid  X  P i Y Furthermore, each input distribution is taken to be sampled i.i.d. from a measure P on I : P 1 ,...,P M iid  X  P .
 tion, we shall use Q i = f ( P i ) and q i = f ( p i ), P p interchangeably depending on whether speaking of a distribution or its density. Using nonparametric den-sity estimation, one may estimate the input/output pdfs as {  X  p i } M i =1  X  X   X  q i } M i =1 where  X  p i X i and  X  q i is estimated from Y i . We shall denote the distributions correspond to pdfs {  X  p i } M i =1  X  X   X  q {  X 
P  X  Q ( A ) = R A  X  q i ( x )d x .
 Often, nonparametric estimators for the real-valued re-gression setting take the form of some linear smoother. That is, to get an estimate at a new input query point x 0  X  R d , the function g ( x 0 ) is estimated as  X  g ( x 0 ) = P i Y i W ( X i ,x 0 ) where W ( X i ,x 0 weights depending on input observation X i  X  R d , and Y i  X  R is the (typically noisy) output observation of g ( X i ). Here, instead of a query point x 0 , we have a query distribution P 0  X  X  ; however, again we consider the case when we are only given P 0 indirectly through struct an estimate of f ( p 0 ), the pdf corresponding to the output distribution for P 0 , we will apply a linear smoother using estimates of pdfs obtained using the observed input/output samples. That is our estimate for the pdf of f ( P 0 ) will have the form: where  X  P 0 is the estimator of P 0 estimated from X 0 . We will use orthogonal series estimators for output densities q i , kernel smoothers for weights W (  X  P and kernel density estimators for input densities p Clearly, other regression methods, and density estima-tors may be used; we chose these methods primarily for ease of analysis. Distribution to distribution regression is related to the aforementioned functional analysis. However, the objects this model works over X  X istributions and their densities X  X re inferred through datasets of sam-ples drawn from the objects, with varying finite sizes. In functional analysis, the objects X  X unctions X  X re in-ferred through datasets of ( X,Y ) pairs that are often taken to be arbitrarily dense in the domain of the ob-jects. For a comprehensive background in functional Silverman, 2002).
 A common approach to working with distributions in ML tasks is to embed the distributions in a Hilbert space, then using kernels and kernel machines, solve a learning problem. The most straight forward of these methods is to fit a parametric model to distribu-tions for estimating inner products (Jebara et al., 2004; Jaakkola et al., 1999; Moreno et al., 2003). Kernels have also been developed using nonparametric meth-ods over distributions. For example, since distribu-tions are observed only through finite sets, set ker-nels may be applied (Smola et al., 2007). Moreover, the representer theorem was recently generalized for the space of probability distributions (Muandet et al., 2012). Futhermore, kernels based nonparametric esti-mators of divergences have also been explored (P  X oczos et al., 2012a;b).
 Recently, a non-Hilbert space approach was taken for regression with distribution covariates and real-valued response (Poczos et al., 2012), where an upper-bound was provided in hopes of better understanding the ef-fect of sample sizes and number of input/output pairs on estimation risk. In this paper, we aim to provide a similar understanding to an even richer model, which spans a wider range of problems. As previously mentioned, we do not directly observe q = f ( p i ), instead we are only given a finite sample drawn from q i . In order to provide a linearly smoothed estimate of f ( p ) for unseen p as (1), we must first make an estimate of q i . We will consider the case where the estimate  X  q i is made using an orthogonal series estima-tor (see e.g. Tsybakov (2008)).
 Suppose that  X  l  X  R l , the domain of output densi-thonormal basis for L 2 ( X ). Then, the tensor product of {  X  i } i  X  Z serves as an orthonormal basis for L 2 ( X  that is, serves as an orthonormal basis (so we have  X   X , X   X  Z Let Q  X  X  , then We make an anisotropic Sobolev ellipsoid type as-sumption about the projection coefficients a ( Q ) = { a
 X (  X , X ,  X  A ) = See (Ingster &amp; Stepanova, 2011; Laurent, 1996) for other analysis with this type of assumption. The as-sumption in (3) will control the tail-behavior of pro-jection coefficients and allow us to effectively estimate Q  X  X  using a finite number of projection coefficients on the empirical distribution of a sample.
 Given a sample Y i = { Y i 1 ,...,Y im i } where Y ij iid O and Q i = f ( P i ), let b Q i be the empirical distribution of Y i ; i.e. b Q i ( Y = Y ij ) = 1 m be: Choosing t i optimally can be shown to lead to E [ k  X  q q  X  (Nussbaum, 1983). As previously mentioned, the estimator of q = f ( p 0 ) that we X  X l use to estimate the output query distribution when given a sample from a new input query distribu-tion P 0 is (1). Let (4) have coefficients  X   X  A t , (1) may be written as: mator can be interpreted as smoothing the projection density estimates at each output density or, equiva-lently, as building a new projection density estimate using smoothed projection coefficients from each out-put density.
 With (7), (2) we can upperbound the L 2 loss of  X  f (  X  p k  X  f (  X  p 0 )  X  f ( p 0 ) k 2 =  X  X where (8) follows from orthonormality. Thus,
E [ k  X  f (  X  p 0 )  X  f ( p 0 ) k 2 ]  X  X Thus, we may upperbound the absolute risk for each projection coefficient in (9), and control (10) using akin to the distribution covariate/ real output prob-lem studied in (Poczos et al., 2012). In fact, us-ing a trivial rewrite: a  X  ( b Q i ) = a  X  ( Q i ) +  X   X   X  = a  X  ( b Q i )  X  a  X  ( Q i ) where E [  X  in (Poczos et al., 2012) sample sizes will now play a role in the nature of the  X  X oise X   X  ( i )  X  .
 smoothing. That is, W (  X  P i ,  X  P 0 ) = where D is a metric and K is a kernel function satis-fying assumption A2 below. We will analyze the L 2 risk of the estimator  X  f (  X  p 0 ing the L 1 metric for D and kernel density estimation estimator is used to estimate input densities: where b i &gt; 0 is a bandwidth parameter, B is an ap-propriate kernel function (see e.g. Tsybakov (2008)), and k X k is the Euclidean norm. Furthermore, sup-pose that D (  X  P 0 ,  X  P i ) = k  X  p 0  X   X  p i k 1 R |  X  p 0 ( x )  X   X  p i ( x ) | d x . 6.1. Assumptions We shall assume A1 through A6 :  X  ( A1 ) L  X  H  X older continuous functional . The un- X  ( A2 ) Asymmetric boxed and Lipschitz kernel. The  X  ( A3 ) Class of input/output distributions. The  X  ( A4 ) Bounded basis . Assume max  X   X  Z k  X   X  k  X  &lt;  X  ( A5 ) Lower bound on sample sizes . Assume that  X  ( A6 ) Relation between n and h . Assume that 6.2. Lemmas Before deriving upper bounds, we state several Lem-mas. For proofs, please refer to the Appendix sec-tion. Let B D ( P,h )  X  { P 0  X  I : D ( P,P 0 )  X  h } ,  X  P ( h )  X P ( B D ( P,h )), where P is a fixed distribution. Let  X  ( n,M )  X  1 eM E h 1  X  Lemma 2 P P M i =1  X  K i = 0  X  P P M i =1  X  K i  X  K  X   X  ( n,M ).
 6.3. Upper bound We look to analyze the L2 risk of our estimator (9),(10). As previously mentioned, (10) can be upper-bounded using (3).
 guments M,n,m emphasize the dependence on the respective sample size bounds. We look to find R ( M,n,m ) s.t.  X   X   X  Z l R  X  ( M,n,m )  X  R ( M,n,m ) . Hence, using (9) and (10): E [ k  X  f (  X  p 0 )  X  f ( p 0 order to derive a bound for R ( M,n,m ), we will use some similar arguments in (Poczos et al., 2012). For analysis purposes let  X  a  X   X   X   X   X   X   X   X   X   X   X  the true input distribution { P i } M i =0 instead of the es-pointwise risk as: For our bounds, we shall take b i = n  X  1 2+ k the asymp-totically optimal bandwidths (up to constants) for ker-nel density estimation under MSE loss. 6.3.1. Bound on Eq. 16 Let  X  X  a  X  = |  X  a  X   X   X  a  X  | , we look to upperbound E Define the following six events E 0 ,E 1 ,E 2  X  E 0 , as: E 0 = { P i K i = 0 } , E 1 = { 0 &lt; P i K i  X  K } , E 2 = { K &lt; P i K i } , { 0 &lt; P i  X  K i  X  K } , and  X  E 2 = { K &lt; P i  X  K E If 0 &lt; P i K i , then clearly  X   X  |  X  a  X  | = P i a  X   X  max . Thus, E with (18) following from Lemma 2.
 Furthermore, E  X  X  a  X  I E 1 ( I  X  E  X  E = 2  X  max P (0 &lt; X by Lemma 1; likewise E  X  X  a  X  I  X  E by Lemma 2.
 Lemma 4 E  X  X  a  X  I  X  E (for C 1 &gt; 0 specified in proof, see Appendix). Hence, combining: E [ X  X  a  X  ]  X  C 1 n  X 
E 6.3.2. Bound on Eq. 17 Note that E |  X  a  X   X  a  X  ( f ( P 0 )) | = = E  X  E + E + E a  X  ( f ( P 0 )) I { P We bound the three terms in (19), (20), and (21). To bound (19), let I = I { P since by A2 supp( K )  X  [0 ,R ] so LD ( P i ,P 0 )  X  K i L ( hR )  X  K i . To bound (20), first we bound E [ |  X  and Lemma 5 1 E [ |  X  ( i )  X  | ]  X  I =  X  I + I . To bound (20): second last line by Lemma 5, and last by Lemma 1. Lastly, using Lemma 1, a bound on (21): Combining the above we have that E |  X  a  X   X  a  X  ( f ( P 6.3.3. Projection Coefficient Regression Synthesizing,  X   X   X  Z l R  X  ( M,n,m )  X  R ( M,n,m ), where: R ( M,n,m )  X  6.3.4. Convergence Rate for Distribution to =  X  Furthermore, note that if we have a bound  X   X   X  A , c  X  |  X  i | then (2 c + 1) l  X  | A t | , by a simple count-ing argument. Let  X  = argmin i  X  2  X  i i . We have  X   X  A  X  Thus, asymptotically | A t | = O ( t  X   X  1 ) where  X   X  1 P h , by (14) we have: major result, a bound on the L 2 risk.
 Theorem 7 As a corollary, if  X  i =  X  &gt; 0, then clearly: 6.3.5. Doubling Dimension Clearly, the bounds on R ( M,n,m ) and E [ k  X  f (  X  p 0 )  X  f ( p 0 ) k 2 ] depend on the quantity E ( X  P ( rh/ 2))  X  1 . It can be shown that with-out further assumptions the quantity can be relatively large, leading to slow rates. However, here we will focus on the case when we may control the effective dimension of the support of P .
 Following (Kpotufe, 2011), we look to control the ef-fective dimension through the doubling dimension. We say that P is a doubling measure, with effective di-= E Clearly, 1 M =  X ( 1  X  mM ), as M,m  X   X  . To further simplify, assume that n =  X ( m ). Then, R ( M,n,m )  X 
R ( M,n )  X  C 1 We will analyze R ( M,n ) depending on the dominating term, and choosing the bandwidth h optimally. Fur-thermore, in order to assure that the ( M + 1) e  X  1 2 n term in (22) does not dominate we slightly extend as-h optimally leads to R ( M,n ) = O ( M  X   X   X  + d ). it makes sense that the rate be driven by it.
 Lemma 9 2 ( Case 2 ) If n  X  n tuitive since n is slow growing in this case.
 Lastly, it can be shown that if one choose h optimally then ( nMh d )  X  1 / 2 cannot dominate.
 Lemma 10 2 If one chooses h optimally it can not be Hence we have that in any case, if P is a doubling mea-sure then we have a polynomial rate on R ( M,n,m ). Hence by (23), we have our second major result: Theorem 11 If P is a doubling dimension, then the rate of convergence for E [ k  X  f (  X  p 0 )  X  f ( p 0 ) k mial in M,n,m . 6.3.6. Different Distances, Estimators We note that a very similar analysis may be employed when using the L 2 metric as the distance D , and using cal note, in this case the L 2 distance among the es-timated input distribution and the estimated query distribution is just the ` 2 distance of their projection coefficients. Due to space constraints, further details are omitted. In order to assess the empirical performance of dis-tribution to distribution estimation, we performed experiments on both synthetic and real data. In both cases, the dataset we are operating on is of the form { ( X 1 , Y 1 ) ,..., ( X M , Y M ) } , where ( X a pair of samples drawn from input/output distribu-densities p i ,q i , were estimated using projection series estimators with the cosine basis:  X  0 ( x )  X  1 ,  X  j ( x ) =  X  2 cos( j X x ) , j  X  1. The kernel used for the regression weights was the triangle kernel: (1  X  X  x | ) + . Parameters were selected by cross validating log likelihoods. 7.1. Synthetic Dataset To better understand the effects of sample size quan-tities n , m and M , we generated synthetic datasets with varying sizes and tested the effectiveness of our estimator. The input/output distribution were cre-ated as follows: First we draw  X  1 , X  2  X  Unif[0 , 1] and  X  1 , X  2  X  Unif[ . 05 ,. 1], then pdfs are p ( x ) = g ( x ;  X  1 , X  1 ) + 1 2 g ( x ;  X  2 , X  2 ), q ( x ) = 1 2 g ( x ; 1  X   X  2 , X  2 ) where g is the truncated normal pdf on [0 , 1]: g ( x ;  X , X  ) = 1  X   X  ( x  X   X   X  ) / ( X ( 1  X   X   X   X  and  X  being the standard normal pdf and cdf (see Figures 2(a) and 2(b)).
 A grid was populated evaluating our estimator by generating M pairs of ( p i ,q i ) input/output densi-ties as described above. Then from each ( p i ,q i points are drawn: |X i | =  X  , and |Y i | =  X  . That is, m i = n i =  X  . M and  X  were chosen to be in figuration of M, X  the L 2-risk is reported as the aver-age L 2-loss calculated for a separate test set of 5000 input/output sample pairs of  X  points (Figure 3). Not surprisingly the fastest direction to decrease the L 2 risk is by increasing M ,  X  simultaneously. It is also in-teresting to note that holding either M or  X  fixed and increasing the other decreases the risk, but eventually levels off. This is exactly predicted by our theory, since once either size M or  X  get much bigger than the other size, the smaller size drives the rate. Furthermore, one may see in Figures 2(c) and 2(d), that even for smaller sample sizes, the estimator still produces useful esti-mates. 7.2. Cell Dataset Next, we used a dataset of a time-series of images of frames were used, containing from 53 to 149 cells each. In each time-frame 49 image nuclear features were ex-tracted from each cell. All features were rescaled to lie within [0 , 1] (see Figure 4). At each time-frame, we look to regress the distribution of one of the nuclear features (e.g. short-axis length) when given the dis-tribution of another nuclear feature in the time-frame (e.g. long-axis length). That is, we use { ( X i , Y i ) } where ( X i , Y i ) are samples of the input/output feature at the i th time-frame.
 In addition to a distribution to distribution estimator (DDE), one may use a conditional distribution based estimator (CDE). That is, estimate the output distri-bution as follows: estimate the conditional distribution of a cell X  X  output feature given the input feature, then when given a frame, estimate the output distribution as the average of the conditional distribution given the input feature values for each cell in the time-frame. Note that this CDE requires much more knowledge and special conditions than the DDE since a one to one mapping between input/output samples must ex-ist 3 , and one needs to know the mapping.
 Since now we are estimating over a real-world dataset, no longer do we know the true density for samples. In-stead, we compare the cross-validated log-likelihoods (CVLL) (using a holdout input/output sample pair) of the CDE to the DDE. We regress the mapping of the distribution of cell long-axis length to the distri-bution of cell short-axis length, where CDE yields a CVLL 6657 . 09 and DDE yields 6714 . 95 (see Figure 5). We note that although the CDE uses much more information, the DDE yields a better likelihood for estimated output distributions for unseen input distri-butions. Also, the DDE is able to capture change in distributions than CDE, which stays much more sta-tionary. It may be of scientific interest to consider conditions under which one expects DDE to outper-form CDE. In conclusion, we have provided an estimator for per-forming regression when both covariates and responses are distributions; also, upper bounds were derived for the risk of the estimator. No parametric assumptions were made on the input/output distribution, nor on the measure from which input distributions are drawn from in the estimator or upper bound results. Further-more, if an assumption is made on the doubling dimen-sion of the measure of input distributions P , then we show that the L 2 risk of the estimated output den-sity converges at a polynomial rate. In future work we will derive lower bounds for the risk. Furthermore, we will test the performance of the estimator on other real-world datasets. Buck, T.E., Rao, A., Coelho, L.P., Fuhrman, M.H.,
Jarvik, J.W., Berget, P.B., and Murphy, R.F. Cell cycle dependence of protein subcellular location in-ferred from static, asynchronous images. In En-gineering in Medicine and Biology Society, 2009.
EMBC 2009. Annual International Conference of the IEEE , pp. 1016 X 1019. IEEE, 2009.
 Ferraty, F. and Vieu, P. Nonparametric functional data analysis: theory and practice . Springer, 2006. Ingster, Y. and Stepanova, N. Estimation and detec-tion of functions from anisotropic sobolev classes. Electronic Journal of Statistics , 5:484 X 506, 2011. Jaakkola, T.S., Haussler, D., et al. Exploiting genera-tive models in discriminative classifiers. Advances in neural information processing systems , pp. 487 X 493, 1999.
 Jebara, T., Kondor, R., and Howard, A. Probability product kernels. The Journal of Machine Learning Research , 5:819 X 844, 2004.
 Kpotufe, S. k-nn regression adapts to local intrinsic dimension. arXiv preprint arXiv:1110.4300 , 2011. Laurent, B. Efficient estimation of integral functionals of a density. The Annals of Statistics , 24(2):659 X 681, 1996.
 Moreno, P.J., Ho, P., and Vasconcelos, N. A kullback-leibler divergence based kernel for svm classification in multimedia applications. Advances in Neural In-formation Processing Systems , 16:1385 X 1393, 2003. Muandet, K., Sch  X olkopf, B., Fukumizu, K., and Din-uzzo, F. Learning from distributions via support measure machines. arXiv preprint arXiv:1202.6504 , 2012.
 Nussbaum, M. On optimal filtering of a function of many variables in white gaussian noise. Problemy Peredachi Informatsii , 19(2):23 X 29, 1983.
 Poczos, B., Rinaldo, A., Singh, A., and Wasserman, L. Distribution-Free Distribution Regression. AIS-TATS 2013, arXiv preprint arXiv:1302.0082 , 2012. P  X oczos, B., Xiong, L., and Schneider, J. Nonpara-metric divergence estimation with applications to machine learning on distributions. arXiv preprint arXiv:1202.3758 , 2012a.
 P  X oczos, B., Xiong, L., Sutherland, D.J., and Schnei-der, J. Nonparametric kernel estimators for im-age classification. In Computer Vision and Pat-tern Recognition (CVPR), 2012 IEEE Conference on , pp. 2989 X 2996. IEEE, 2012b.
 Ramsay, J.O. and Silverman, B.W. Applied functional data analysis: methods and case studies , volume 77. Springer New York:, 2002.
 Rigollet, P. and Vert, R. Optimal rates for plug-in estimators of density level sets. Bernoulli , 15(4): 1154 X 1178, 2009.
 Smola, A., Gretton, A., Song, L., and Sch  X olkopf, B. A hilbert space embedding for distributions. In Algo-rithmic Learning Theory , pp. 13 X 31. Springer, 2007. Tsybakov, Alexandre B. Introduction to nonparamet-
