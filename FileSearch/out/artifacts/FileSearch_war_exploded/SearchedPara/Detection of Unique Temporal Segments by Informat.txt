 The central challenge in temporal data analysis is to obtain knowl-edge about its underlying dynamics. In this paper, we address the observation of noisy, stochastic processes and attempt to detect temporal segments that are related to inconsistencies and irregu-larities in its dynamics. Many conventional anomaly detection ap-proaches detect anomalies based on the distance between patterns, and often provide only limited intuition about the generative pro-cess of the anomalies. Meanwhile, model-based approaches have difficulty in identifying a small, clustered set of anomalies.
We propose Information-theoretic Meta-clustering (ITMC), a for-malization of model-based clustering principled by the theory of lossy data compression. ITMC identifies a  X  X nique X  cluster whose distribution diverges significantly from the entire dataset. Further-more, ITMC employs a regularization term derived from the pref-erence for high compression rate, which is critical to the precision of detection.

For empirical evaluation, we apply ITMC to two temporal anomaly detection tasks. Datasets are taken from generative processes in-volving heterogeneous and inconsistent dynamics. A comparison to baseline methods shows that the proposed algorithm detects seg-ments from irregular states with significantly high precision and recall.
 H.2.8 [ Database Management ]: Database Applications -Data min-ing; H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval -Clustering Algorithms, Design, Experimentation temporal data mining, information theoretic meta-clustering
Detecting anomalies and unique patterns is a fundamental unsu-pervised learning problem. With increasing interest towards tem-poral data mining in recent years, a significant amount of effort has been devoted to the topic of temporal anomaly detection. One of the motivations of the task is to obtain useful domain knowl-edge from the anomalies. However, what information is useful, therefore the definition of anomalies, varies depending on the ob-served process and the associated tasks. In this paper, we focus on the observation of noisy, stochastic processes and attempt to detect  X  X nique X  temporal patterns that are related to inconsistencies and irregularities in the underlying dynamics or behavioral models.
Identifying  X  X nique X  patterns can be instrumental in the analy-sis of complex and unstable processes. For example, cellular pro-cesses are observed as expression levels of its components, such as genes and proteins. Components regulate each others X  expres-sions as well as induce and repress regulatory processes. It is an important challenge in bioinformatics to learn from the monitored expression levels, the regulatory relations, or pathways, between components. During many cellular processes, invoked as a reac-tion to some permutation such as DNA damage, the cell is outside of its normal state, i.e., there are induced and repressed pathways by activated components. Identifying temporal segments related to the activated states are useful for inferring regulatory pathways of a particular process.

Many conventional methods for temporal anomaly detection de-fine anomalies using a distance function between individual seg-ments [11, 18]. However, their outputs are not easily interpreted into intuitions about the generative process. Additionally, distance functions are relatively sensitive to noise and fluctuations.
In model-based approaches, on the other hand, the generative process of the data can be described by estimated parameters within the framework of modeling assumption. Assumptions or prior knowl-edge about the process is integrated into learning explicitly through a generative model, e.g., Hidden Markov model. Despite its ben-efits, very few model-based approaches have been proposed in the context of temporal anomaly detection. It is our motivation to de-velop a model-based approach that can detect a set of unique seg-ments and provide statistical property and intuition about the gen-erative process behind anomalous observations.

It is problematic in model-based clustering to address substan-tially small clusters consisting of anomalous objects. There usually exists a trivial solution where such anomalies are not identified as an independent cluster. As a very na X ve example, we consider 2 -component Gaussian clustering. When applied to a dataset sampled from one Gaussian component, it partitions the samples symmetri-cally, which is trivial but a locally optimal solution. The same so-lution is still locally optimal when a small cluster of samples from another  X  X nomalous X  component is added to the dataset, especially if the added samples significantly overlap with the first component. Generally, clustering that includes a cluster of anomalies forms a weak attractor that is difficult to find by the random initialization.
In our previous works [1, 2], we have shown that clustering based on principles of lossy data compression is effective for detecting clustered outliers for non-temporal data. In this paper, we gen-eralize its formalization as Information Theoretic Meta-clustering (ITMC), which is an intuitive formalization of model-based cluster-ing based on the formalization of lossy data compression. Its prin-ciple is to preserve the relevance of the compressed expression to the observed data in reference to the generative model. We present an incremental implementation of ITMC which identifies a cluster of  X  X nique X  segments, by locally maximizing its divergence from the distribution of the majority of the dataset. Furthermore, ITMC integrates a regularization term for emphasizing a higher compres-sion rate, which results in high precision for detecting anomalous segments.

The rest of this paper is organized as follows. Section 2 dis-cusses related work on temporal clustering and anomaly detection. Section 3 reviews general notations and basic concepts from in-formation theory. Section 4 presents Information Theoretic Meta-clustering and its incremental formalization. Section 5 describes the implementation of the algorithm and discusses its convergence and complexity. Section 6 describes the empirical results on datasets from an artificial model and a gene regulatory model from the bioin-formatics literature. Our conclusion is given in Section 7.
Unsupervised learning, e.g., clustering and anomaly detection, of temporal data has many applications including data streams [13], trajectory and sensory data [19] and biological data [5]. Many of the studies are distance-based, i.e., define infrequent patterns [7] or patterns with the largest distance from others [11, 18] as anoma-lies. Common distance functions include correlation [4], Euclid and Hamming distance [11]. The distance-based approaches can benefit from fast, database-friendly algorithms [18], but its output does not intuitively describe the process involved with the anomaly.
In some domains, e.g., economics [10] and gene expression pro-files [16], model-based approaches are preferred for its explicit integration of the temporal dependence [5]. Generative models such as (Vector) Auto-Regressive (AR) model and Hidden Markov Model (HMM) are widely used [16, 10].
Regarding the general notations, the variables are denoted by capital letters, values by small letters and sets by calligraphic let-ters, e.g., X , x and X , respectively. The cardinality of a set X is written as |X| and a temporal sequence of values from t = 1 through T as [ x ( t )] T t =1 . The expectation over X is written as  X  X  X 
The Kullback-Leibler (KL) divergence is a widely used measure of difference between two probability distributions. We denote KL divergence between two distributions p ( x ) and q ( x ) over X , x de-noting the value of X , as
Mutual information quantifies the amount of information shared between two probabilistic variables. The mutual information be-tween variables X and Y is defined as follows.

In lossy data compression, a variable X is compressed using a set of discrete values U and by assigning each value of X to a  X  X ode X  u  X  U . Denoting the coded value of each x by a variable U and the probability function of the assignment by p ( u | x ) , the  X  X ate X , or average number of bits, for the compressed expression U is lower-bound by I ( U ; X ) .

The disruption of the original value x by the compression is quantified by the distortion function d ( x,u ) , defined accordingly to the problem setting. There is an essential trade-off between the rate and the average distortion  X  d ( x,u )  X  X,U , and the fundamen-tal problem of lossy data compression is to achieve the minimal rate under the constraint of a tolerable average distortion. This is a variational problem written as a minimization using a Lagrange multiplier  X  ,
In the context of probabilistic clustering, U can be seen as a hidden variable of X and p ( u | x ) , subsequently, is the conditional probability of a cluster u given x .
Let x denote a k -featured value and y i = [ x i ( t )] T ral sequence of observed values over t = 1 ,...,T . Given a set of sequences Y 0 = { y i } n i =1 , we employ Y = Y 0 or its sampled segments of length L as the input dataset.

We denote by  X  a parametric space associated with the gener-ative model of the dataset, i.e., the likelihood function p ( y ;  X  ) is defined using a parameter instance  X   X   X  .

In the following section, we formalize an information-theo-retically principled clustering of temporal segments Y with regards to an as-sumed generative model.
Consider compressing a set of segments Y = { y i } n a code set U m = { u } m u =1 . Let variable U represent the value of u for each y . Given a parametric space  X  associated with a generative model y = f ( t ;  X  ) :  X   X   X  , each u can be linked to a parameter instance  X  u  X   X  through p (  X  u | y ) . For example, a set of n temporal sequences can be expressed by n discrete values taken from U m , each representing a set of linear regression coefficients  X  u  X   X  . Provided the residuals are non-zero, U provides a  X  X ossy X  compression of Y .

Let us consider a trivial case where m = |Y| , i.e., each code u  X  U m bijectly corresponds to a segment. While the compressed expression U yields the least distortion, i.e., the residual, it also produces a high-rate, trivial solution that is equivalent of clustering n objects into n clusters.

We attempt to generate a non-trivial expression C by compress-ing U using a smaller set of codes C = { c i } k i =1 ( k &lt; n ). Each code c i is associated with a parameter  X  i  X   X  . Intuitively, each c  X  C represents a meta-cluster, to which subclusters u  X  U is as-signed. Fig.1 illustrates an example of the corresponding instances of Y , U , C , and  X  .
The rate of compression is lower-bound by I ( C ; U ) and the compression is characterized by the following function where d ( c,u ) denotes the distortion function and  X  is the Lan-grange multiplier from (1), representing the trade-off between the rate and average distortion.

To adapt this formalization for the task at hand, we define d ( c,u ) as the inversed KL divergence between p ( y | c ) and p ( y | u ) . The expected distortion is written as Intuitively, I ( Y ; C )  X  I ( Y ; U ) quantifies the distinction between respective segments of Y that is preserved in the compression from U to C . Thus, the rationale for such a definition of d ( c,u ) is to induce C to be more informative about Y by minimizing  X  d ( c,u )  X  .
Since I ( Y ; U ) is determined by Y and  X  , it is independent of p ( c | u ) . Subsequently, minimization of F and G = F  X   X I ( Y ; U ) with regards to p ( c | u ) are equivalent variational problems. From (2) and (3), G is written as and Information-theoretic Meta-clustering is formalized as (4) evaluates the cost of expressing the compressed data against the information preserved through meta-clustering, a process of dual compression. The intuition behind meta-clustering is described as follows. Each observation is mapped to some instance in the parametric space  X  as a result of the modelling assumption, gen-erating a trivial compressed expression U . Note that this mapping is implicit as (4) does not require p ( y | u ) or  X  u . Then, two com-pressed expressions U and C are naturally compared based on the amount of information they convey about Y . Sine C cannot convey more information than U , there always exists a non-trivial solution of (5) for a positive value of  X  .

We conclude this section with a reference to Information Bottle-neck (IB) clustering [17], which is a supervised clustering based on the principle of the lossy data compression. IB is formalized as compression of the observed variable X into cluster variable T , given the joint distribution p ( X,Z ) with another relevant variable Z . Assuming a Markov chain condition Z  X  X  X  T , the princi-ple of IB is to partition X in a way that preserves the distinction of Z .

The process of meta-clustering, which compresses Y into U and then further into C , can be described by a Markov chain Y  X  U  X  C . In the analogy of IB clustering, ITMC employs the ob-servation Y as the relevant variable for compressing U to generate C . Thus, ITMC can also be seen as an IB formalization of unsu-pervised model-based clustering. While obtaining the input joint distribution p ( x,z ) is problematic for practical IB applications, its counterpart in ITMC, i.e., p ( y,u ) , is not a required input.
Although multiple clusters of unique segments may exist in a dataset, attempting to find all such clusters at once could be im-practical. Here, we design an incremental approach that detects one unique cluster at a time.

The principle of the incremental approach is to minimize the contribution to the cost function G from an individual cluster c which we denote by G i .

G i = I ( c i ; U )  X  I ( c i ; Y ) (6)
The second term in (6), i.e., the contribution of c i to I ( C ; Y ) , can be regarded as the  X  X niqueness X  of cluster c i . When the major-ity of the dataset is well described by a global parameter estimate  X  , the  X  X niqueness X  is measured by the divergence between p ( y | c and p ( y ;  X  ) . In contrast to the notion of outliers, which consid-ers the likelihood of an individual given the global parameter, the  X  X niqueness X  considers the divergence between the global distribu-tion and the distribution of anomalies. Thus, it can also be seen as a generalization of the definition for a cluster of anomalies.
For the sake of simplicity, we assume that the assignment of u is deterministic, i.e., p ( c i | u )  X  X  0 , 1 } . Given this constraint, G rewritten as  X  = 1 is derived from the variational condition at the cluster bound-ary. The derivation is omitted for brevity.

The probability functions relevant to G i are p ( c i | u ) , p ( c p ( y | c i ;  X  i ) . G i is monotonically decreased through iterative up-dates of these probability functions. A localized implementation of meta-clustering is described as follows.
 Given Y , a set of n segments, and a generative model  X  , define U = { u } n u =1 and compute the global parameter  X  . Randomly initialize a subset Y i  X  Y . Then iterate the following steps until G i converges. 1. Update p ( c i | u ) , p ( c i ) and  X  i as follows 2. Remove y = arg min y  X  X  Given  X  i , y  X  X  can be assigned to a cluster based on p ( y | c not necessarily by a  X  X ard X  assignment.
 Algorithm 1 shows the pseudo code of the procedure.

P ROPOSITION 1. Algorithm 1 monotonically reduces, and con-verges at a local optimum of, G i .

P ROOF . Through lines 9-11 of Algorithm 1, the first term of (7) monotonically decreases while the second term is fixed. Then, at line 12, updating  X  i by (10) reduces the second term while the first term is fixed. Thus, it suffices to show that G i is lower-bounded.
The first term of (7) is positive as 0 &lt; p ( c i ) &lt; 1 , and the second term is non-negative and finite, since it is a KL divergence and p ( y ;  X  ) &gt; 0 over Y . (7) indicates another important property of ITMC. When the ini-tial cluster consists of non-unique segments, i.e., segments well de-scribed by the global parameter  X  , the second term of (7) is un-affected by the update throughout lines 9-12. Meanwhile, the first term always decreases through lines 9-11. Here, the first term func-tions as a regularization term, to reduce such a  X  X on-unique X  seg-ment cluster to a very small size. Small clusters can be rejected us-Algorithm 1 Localized Information Theoretic Meta-clustering Al-gorithm 1: INPUT: segment set Y = { y i } n i =1 , code set U = { u } 2: OUTPUT: { p ( c i | u ) } u  X  X  , p ( c i ) and  X  i 3: METHOD: 4: Compute  X  = arg max 5: Select a medoid y 0  X  X  randomly 6: Compute  X  0 = arg max 7: Initialize Y i = arg max 8: repeat 9: Select y min = arg min 10: Y i  X  X  i \ y min 11: Update { p ( c i | u ) } and p ( c i ) by (8) and (9) 12: Compute  X  i by (10) and update { p ( y | c i ;  X  i ) } 13: Compute G i from { p ( c i | u ) } , p ( c i ) and { p ( y | c 14: until G i converges 15: return { p ( c i | u ) } , p ( c i ) and  X  i ing a threshold c th , which can be determined empirically as shown in Section 6, or arbitrarily to a sufficient number for consistent pa-rameter estimation.

With regards to the efficiency of the algorithm, incremental ITMC requires one complete scan of the dataset for computing  X  . During an iteration, updating p ( c i | u ) and p ( c i ) requires a constant time. The time complexity of updating  X  i depends on the model. It is es-sentially quadratic to the number of variables p when dependencies between all variables are assumed. It is usually linear to the length of segments L . The general complexity of an iteration is therefore O ( p 2 L ) . For an incrementally updated model, it is independent of |Y| or |Y i | . From line 10, the number of iterations is limited by the initial cluster size.
This section describes the experimental settings including datasets, baseline methods and the measures for comparison, and then dis-cuss the results.
For the first experiment, datasets are generated using a univariate polynomial model of the form where N (0 , X  ) denotes a normal distribution with a standard devi-ation  X  , and a = ( a 1 ,...,a p ) the weight coefficients. Two sets of time series are combined to make the test dataset Y = Y 1 + Y 2 , where Y 1 and Y 2 are randomly generated with a common  X  and coefficients a 1 and a 2 , respectively. Each time series is of length l .

Among numerous combinations of parameters tested, we show the results for a typical setting: p = 2 ,  X  = 1 . 0 , a 1 a = (0 . 3 , 0 . 4 ,  X  0 . 4) , l = 12 , |Y 1 | = 800 and |Y| = 40 . Fig.2 il-lustrates a sample dataset. The majority of the segments, Y a linearly increasing trend, while the minority, Y 2 , exhibits a de-scending curve.
Datasets for the second experiment are taken from multivariate time series with causal relationship between variables. Inferring dependence and causality from such observations is a topic in var-ious fields of study [3, 5, 9, 10].

For our experiment, we adopt a model of a cellular process regu-lated by genetic and proteomic components. DNA repair system of E.coli involves over 30 known components with mutual regulations [14, 8, 12]. Fig.3 summarizes its components and their relations. Previous analyses were aimed to estimate a parametric model that fits the expression time series of the SOS genes and to infer the regulations from its parameters. This was a difficult task even for a small network of six components, in part to the large number of possible regulations. Typically, an estimated model includes an ex-cessive number of regulations, most of which presumably are false positive inferences [12].

Our motivation for this experiment is to investigate whether the irregular state of the process can be identified from unique seg-ments and whether intuitions about the state can be obtained from the generative model fitted to such segments. Due to the exponen-tial complexity of the network inference problem, a partial infer-ence is often addressed as a feasible and practical task [8, 12]. For our experiment, we adopt a model consisting of four key compo-nents: an SOS protein, a repressor protein and two SOS genes, which we denote by P S , P R , G 1 and G 2 , respectively. The path-way between components can be written as a set of rules. We de-note the pathways during the normal state by R = { P 2 a G G ,P S  X  P S ,P R  X  P S ,G 1  X  G 1 ,G 2  X  G 2 } , where  X  denotes a positive regulation and a , a negative regulation. R in-cludes self-induction of each component and repression of G G 2 by P R . The pathways during the activated state R  X  = { P P ,P R a G 1 ,P R a G 2 ,G 1 a P S ,G 2 a P S ,P S  X  P S ,P P ,G 1  X  G 1 ,G 2  X  G 2 } , includes three induced negative regu-lations: from the SOS protein to the repressor and from SOS genes to the SOS protein.

From these sets of pathways, we generate influence matrices as-sociated with a vector auto-regressive (VAR) model that describes the cellular dynamics.
 An auto-regressive (AR) process is written in the form of a linear Gaussian model [15], where ` denotes the extent of time delay, ( a 0 ,...,a ` ) is a vector of linear coefficients.

VAR generalizes the AR model for multiple variables as where A i denotes a p  X  p coefficient matrix, associated with the conditional probability function p ( x ( t ) | x ( t  X  i )) . N ( X ) denotes a set of Gaussian noise variables whose deviations are  X  = (  X  respectively.
 We model the DNA repair system as a VAR process, using a Gaussian noise with standard deviations  X  0 and sets of parameters for the normal state of the cell  X  = ( a 0 , A ) , and for the activated state  X   X  = ( a  X  0 , A  X  ) .

We first generate a series of states, either normal or activated, of length T . In each series, the activated state begins at randomly chosen time point and continue for l time steps, with l randomly varying from 3 to 9.

Then, a series of p expression, y = [ x ( t )] T t =1 , is generated ac-cording to (12), with either  X  and  X   X  depending on the state of each time point t . The expression vector for initial l time points are ran-domly generated. Using above procedure, we generate a set of time series Y 0  X  X  y i } n i =1 .

Since the transition of states occur within a time series, we di-vide the time series into segments of length m by sliding the time window of width m , i.e.,
Generally, it is difficult to consider all relevant expressions for the analysis of a cellular process as the numbers of genes and pro-teins are extremely large, and the functions of the most are un-known. For the next experiment, we consider a case in which one component, P S , is hidden. This dataset, consisting of segments with  X  p = p  X  1 expressions, is denoted by  X  Y . We refer to Y and as VAR datasets.

The parameters of the generative process described in this sec-tion is summarized below.

The ITMC algorithm performs a likelihood maximization (10) based on a generative model and a given set of segments. The like-lihood function used for polynomial datasets is
For VAR datasets, given  X  = ( A , a 0 ,  X ) , the likelihood is Let d j = x j  X  ability function in (14) is rewritten as and p ( y ;  X  ) = T Q
Maximizing these likelihood functions is a quadratic regression and a multiple linear regression problem, respectively.

The threshold c th for rejecting non-unique clusters is determined by the following procedure. We perform 20 preparatory runs on sample datasets and record the sizes of converged clusters. We di-vide the set of sizes into two subsets by k -means algorithm, and compute the average  X  c and the standard deviation  X  c for the sub-set closer to zero. We set c th =  X  c + 2  X  c which is approxi-mately the 98 percentile of N (  X  c , X  c ) . With polynomial datasets, (  X  c , X  c ) = (15 . 2 , 3 . 27) , thus c th = 21 . 7 .

For evaluation of ITMC, we generate 20 datasets randomly using the generative model. On each dataset, we perform 20 runs with random initialization. The cluster with smallest G i is output as a unique cluster. The size of the initial cluster is 200 for polynomial datasets and 800 for VAR datasets.
The notion of  X  X iscord X  has been proposed in the subsequence analysis of the temporal data [11]. The k th discord is defined as the segment with the k th largest distance from its nearest neighbor The total number of discords  X  D is a required parameter.
Hierarchical clustering is one of the standard clustering methods and used frequently in time series analysis [13]. We used the fol-lowing options for linkage: single, complete and average. We em-ployed the following functions as distance for the above distance-based methods: correlation, Euclidean and squared Euclidean dis-tance.
A basic definition of an outlier is an object for which one can reject the hypothesis that it comes from the same distribution as the majority of the dataset, with a confidence level  X  . Given a segment set Y , a set of outliers is defined as { y }  X  Y : p ( y ;  X  ) &lt; p where p  X  denote the threshold corresponding to  X  and  X  , the global parameter of the dataset estimated by likelihood maximization.
As a baseline, model-based clustering method, we define a prob-abilistic clustering based on the joint likelihood maximization. Us-ing likelihood functions (13) and (14), the following EM-like steps are iterated until the joint likelihood of all segments converges. a) Assign sequences to k partitions based on the likelihood given the
The notion of self-match is omitted here for brevity as each seg-ment of polynomial datasets, of which the result is reported, is in-dependently generated and not a subsequence. parameters of each partition. b) Compute the parameters of the generative model from segments in each partition. This method can be seen as an adaptation of the k -means clustering [6]. k = 2 , 4 , 6 , 8 , 10 , 12 , 16 , 20 is used as the number of clusters. As with ITMC, the iteration was repeated 20 times for each dataset with random initialization.
The goal of both experiments are to identify the set of minor-ity segments among the dataset. Precision and recall are standard evaluation measures for detection tasks. From the true labels of segments and the segments identified by ITMC as positive sam-ples, true positive TP , false positive FP and true negative TN are counted. The precision Pr and the recall Re is computed as
There is an essential trade-off, or a negative correlation, between the precision and the recall. As an overall measure, the F -measure,
The baseline methods X  precision and recall critically depend on its parameters, e.g., the confidence level or the number of clusters. To compare them in reference to ITMC, we define ErP and EpR as follows. EpR is the best achieved recall at an equal or better pre-cision than the precision of the referenced output. Similarly, ErP is the best precision at an equal or better recall than the referenced value.

ErP / EpR is computed in the following steps. First, ITMC is applied to the dataset and the reference precision/recall values are computed from its output. Then, the baseline method, the outlier detection for example, is applied to the same dataset using a set of different parameters, i.e., confidence levels and precision/recall are computed for all outputs. We then select a set of results with an equal or better recall than the reference values. ErP is the pre-cision of the result with the highest precision in that set. EpR is computed similarly, but referencing the precision/recall in a re-versed order. ErP / EpR is zero if equal or better recall/precision were not achieved.

Note that ErP and EpR should be considered strictly as a base-line for ITMC. Selecting critical parameters based on the output or selecting the highest precision/recall, each method is assessed for the best possible case and not the actual performance in unsuper-vised settings.

We apply the discord discovery with number of discords  X  D 2 , 4 , 6 ,..., 100 for polynomial datasets and  X  D = 2 , 4 , 8 , 16 , 32 , 64 and 100 , 110 ,..., 400 for VAR datasets and compute precision/recall for all results. For the hierarchical clustering, we compute the precision/recall for every subtrees of the dendrogram. For out-lier detection, we change the threshold s.t. the number of outliers  X 
OD changes from 2 , 4 , 6 ,... to 100 for polynomial datasets and 2 , 4 , 8 , 16 , 32 , 64 and 100 , 110 , ..., 400 for VAR datasets. For the model-based clustering, we compute precision and recall of all clusters for every k .
Table 1 shows the precision and the recall of ITMC and the cor-responding ErP / EpR of the baseline methods averaged over 20 polynomial datasets. The outlier detection was omitted because its EpR / ErP were zero in all runs.

ITMC consistently yielded highly balanced precision/recall that were significantly higher than all baseline ErP / EpR . During the convergence of unique clusters, ITMC improved the precision without ceding too much in recall, as observed by the F-measure il-lustrated in Fig.4. It is interesting that the convergence of the clus-ter corresponds roughly with that of F-measure, because the pre-cision/recall are obviously not computed during the iteration. The averaged size of the unique clusters was 38.3 with a deviation of 3.93, and non-unique clusters were rejected with a significant mar-gin. Precision/recall were consistent among iterations indicating that the effect of initialization is insubstantial. A typical iteration took less than 20 min. on Intel CPU of 2.66GHz.
 f-value Figure 4: F-measure during convergence of unique clusters
The distance-based approaches frequently produced high preci-sion with subtrees or discords of smaller sizes, but did not achieve considerable recall. As their EpR/ErP indicate, the trade-off for high precision or recall was steep. The discord discovery based on correlation showed relatively high recall as indicated by its EpR . The difficulty presumably is attributed to the level of noise as these methods are adequate for detecting minority segments with smaller noise.

Outlier detection also produced considerably low recall. Addi-tionally, it achieved a low precision even with higher confidence levels, resulting in zero ErP / EpR . This indicates that the distri-bution of the minority segments Y 2 overlaps significantly with the majority Y 1 .

With the model-based maximum likelihood clustering, high re-call was frequently achieved with small k , resulting in relatively better ErP than the distance-based approaches. High precision were occasionally achieved with larger k . However, minority seg-ments were divided over many clusters in those cases, resulting in low recall and EpR .
Table 2 shows the precision and the recall of ITMC and base-line ErP / EpR averaged over 20 VAR datasets. As the distance-based methods did not obtain non-zero ErP / EpR their results were omitted. It is understandably difficult to account for the tem-poral dependence of the auto-regressive process using a distance function.

The precision and recall of ITMC were consistently balanced and, as discussed in the later paragraph, sufficient for providing an estimate of the generative process. Both were closer to the base-line than in the previous experiment, conceivably due to the larger number of parameters allowing the model to fit the segments of the normal state.

The outlier detection produced high precision at small  X  but gen-erally low recall, suggesting a partial overlap between segments from the normal and the activated state. The precision and recall of maximum-likelihood clustering showed similar trend to the pre-vious experiment, i.e., larger k occasionally resulted in higher re-call, while the performance on precision was generally weak. It achieved slightly higher ErP as the recall of ITMC decreased.
One way to exploit the detected segments for the understanding of its dynamics is to reproduce a graphical structure from the esti-mated coefficient matrix. As denoted in (12), each coefficient is as-sociated with a conditional probability function p ( x q ( t ) | x regarding the expressions of the q th and r th components. Thus, it represents an edge from r to q . We select coefficients averaging over 0.1 in absolute value in 20 estimated models of unique seg-ments. Then, a set of corresponding edges are combined into a graph. Similar approaches are used in the inference of biological regulatory networks [12].

Fig.5 illustrates the graphical structure reproduced from the es-timated model of unique segments. For comparison, we also esti-mated the model of the normal state from all segments and repro-duced a corresponding set of edges. We identified edges that were specific to the unique segments: G 1 a P S , G 2 a P S , P and G 2  X  G 1 . The first three edges are pathways induced in the activated states, while the last edge is a false inference.
Table 3 shows the precision/recall and the baseline ErP / EpR for datasets with a hidden component. In comparison to the previ-ous experiment, ITMC maintains the precision to a marginal de-crease and improves in comparison to the baseline. The recall, meanwhile, is substantially lower and closer to the baseline. This may be attributed to the reduced homogeneity of  X  X nique X  seg-ments, resulting from the incomplete observation.

The importance of precision and recall is relative to the addressed task. Here, ITMC X  X  performance is arguably acceptable for the pur-pose of inferring the generative process.

In this paper, we addressed the task of detecting a set of unique segments from temporal observations. We formulated the task as an optimization problem, i.e., a maximization of the divergence among clusters with regularization, by adapting the information-theoretic principle for lossy compression. We designed an incre-mental algorithm and performed theoretical and empirical analyses of its convergence. Our theoretical claims were supported by the empirical evaluations using common stochastic generative models. Detected clusters correspond well with segments from the irregular state of the model, and the regularization term effectively marginal-ized false positive clusters. The latter was critical to achieving a precise detection of small, clustered anomalies by a model-based approach.

For practical purposes, the model-based assessment of anoma-lies can provide intuitions about the dynamics of its generative pro-cess and achieves robustness against noise, as was experimentally shown.

Since model selection is the primary step, its dependence to the output is an important issue of the methodology. The proposed method distinguishes clusters that do not fit the model of the ma-jority. Therefore, the  X  X niqueness X  of the cluster is relative to how well the majority of the data is represented by its generative model. The strategy of model selection for the majority can naturally trans-late to the definition of unique clusters. [1] S. Ando. Clustering needles in a haystack: An information [2] S. Ando and E. Suzuki. An information theoretic approach to [3] A. Arnold, Y. Liu, and N. Abe. Temporal causal modeling [4] A. Bagnall and G. Janacek. Clustering time series with [5] Z. Bar-Joseph. Analyzing time series gene expression data. [6] I. V. Cadez, S. Gaffney, and P. Smyth. A general probabilistic [7] X. Y. Chen and Y. Y. Zhan. Multi-scale anomaly detection [8] D.-Y. Cho, K.-H. Cho, and B.-T. Zhang. Identification of [9] T. Chu and C. Glymour. Search for additive nonlinear time [10] M. Corduas and D. Piccolo. Time series clustering and [11] E. Keogh, J. Lin, and A. Fu. HOT SAX: Efficiently finding [12] S. Kimura, K. Sonoda, S. Yamane, H. Maeda, [13] P. P. Rodrigues, J. ao Gama, and J. ao Pedro Pedroso;. [14] M. Ronen, R. Rosenberg, B. I. Shraiman, and U. Alon. [15] S. Roweis and Z. Ghahramani. A unifying review of linear [16] A. Schliep, A. Schonhuth, and C. Steinhoff. Using hidden [17] N. Tishby, F. C. Pereira, and W. Bialek. The Information [18] D. Yankov, E. Keogh, and U. Rebbapragada. Disk aware [19] J. Yin and M. M. Gaber. Clustering distributed time series in This work was supported by Japanese Society for Promotion of Science (JSPS) Grant-in-Aid for Young Scientists and the Strategic International Cooperative Program funded by Japan Science and Technology Agency (JST).
