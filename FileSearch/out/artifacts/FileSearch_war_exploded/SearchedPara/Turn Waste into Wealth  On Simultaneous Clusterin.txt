 Dirty data commonly exist. Simply discarding a large num-ber of inaccurate points (as noises) could greatly affect clus-tering results. We argue that dirty data can be repaired and utilized as strong supports in clustering. To this end, we study a novel problem of clustering and repairing over dirty data at the same time. Referring to the minimum change principle in data repairing, the objective is to find a minimum modification of inaccurate points such that the large amount of dirty data can enhance the clustering. We show that the problem can be formulated as an integer lin-ear programming ( ilp ) problem. Efficient approximation is then devised by a linear programming ( lp ) relaxation. In particular, we illustrate that an optimal solution of the problem can be directly obtained without calling a solver .A quadratic time approximation algorithm is developed based on the aforesaid lp solution. We further advance the algo-rithm to linear time cost, where a trade-off between effec-tiveness and efficiency is enabled. Empirical results demon-strate that both the clustering and cleaning accuracies can be improved by our approach of repairing and utilizing the dirty data in clustering.
 I.5.3 [ Clustering ]: Algorithms Data repairing; data cleaning
Density-based clustering can successfully identify noises (see a survey in [16]). However, rather than a small propor-tion of noise points, real data are often dirty with a large number of inaccurate points [10]. For instance, a (very) large portion of GPS data are inaccurate, especially in the indoor environment with weak signals. According to our ex-periments (in Section 6), 139 out of 818 (about 17%) GPS  X  readings are inaccurate. Similar examples of inaccurate data include RFID sensor readings [4] or multimedia data [11].
The large amount of noise points are simply discarded, if we directly apply the existing density-based clustering ap-proaches, e.g., the well-known dbscan [7]. With too much information loss, clustering results could be dramatically af-fected (see examples below).

Instead of discarding dirty data, we argue that the large amount of dirty data can be repaired and utilized as strong supports in clustering. To the best of our knowledge, this is the first study on performing data cleaning and clustering at the same time.

A natural idea is to first clean the dirty data before clus-tering. Existing constraint-based cleaning techniques can be applied, e.g., repairing with functional dependencies ( fd (see [8] for a survey). According to our empirical study (in Figure 11), the clustering accuracy can be improved by ap-plying first the fd -based repairing, when the fd constraints are available in the considered dataset. However, for some other datasets with simple schema, such as GPS data, no such ( fd ) integrity constraints could be declared (and thus the constraint-based repairing is not applicable).
Besides repairing mentored by integrity constraints, we propose to repair the dirty data under the guidance of den-sity information, inspired by the successful identification of noisy data in density-based clustering. The idea is to simul-taneously repair dirty data w.r.t. the density of data dur-ing the clustering process, rather than separately repairing (ahead) w.r.t. integrity constraints. By interchangeably tak-ing the advantages of density-based clustering and repairing, both the clustering and repairing tasks benefit (with accuracy improvement as shown in the experiments in Section 6).
Following the minimum change principle in data repairing [24], i.e., the change made in repairing is expected to be as small as possible, the objective of simultaneous repairing and clustering is to find a minimum repair of data such that all the data can be clustered (utilized). The rationale of minimum change is that systems or humans always try to minimize mistakes in practice. That is, dirty values, such as inaccurate GPS readings or typos in text, are often not very far from true values. Referring to this objective, we formalize the problem as an optimization problem, namely Density-based Optimal Repairing and Clustering ( dorc ). Example 1. We illustrate a motivation example of GPS data in Figure 1. Consider GPS readings in two nearby buildings, denoted by blue and white points in Figure 1(a), respectively. C1 denotes more precise points (high-density, in a building with good GPS signal), whereas C2, for a Figure 1: Clustering with repairing over dirty data building with weaker GPS signal, contains inaccurate points (low-density, that drop far away from the building and need concentrating repairing). The density-based clustering such as dbscan either returns a cluster C1 (with high density requirements) or C1 (with low density parameters). White points corresponding to the second physical building can-not form a separate cluster (either directly ignored as noisy data in C1 or merged with blue ones in C1 ). The mas-sive points identified as noises by dbscan in Figure 1(a) are strong evidence to concentrate C2. (See more example results in Figure 6 in Section 6.1)
In this study, we propose to repair the inaccurate data points during the clustering process. For example, as shown in Figure 1(b), an arrow ( a  X  b ) denotes that a point is repaired from location a to location b . Since points are con-centrated (with higher density) after repairing, two clusters are successfully formed in Figure 1(c).

The problem of clustering with repairing, however, is non-trivial. Simply repairing noise points to the closest clusters is not sufficient, e.g., repairing all the noise points to C1 in Figure 1 does not help in identifying the second cluster C2. Indeed, it should be considered that dirty points may possibly form clusters with repairing (i.e., C2).
It is notable that our proposed dorc techniques are com-plementary to the existing constraint-based repairing (when constraints are available). As illustrated in experiments (Figure 12 in Section 6.3), by combining our dorc approach with the existing fd -based repairing, both the clustering and repairing accuracies are further improved. Compared to ex-isting constraint-based repairing, the major advantages of dorc are in two aspects: (1) it does not require any exter-nal knowledge of integrity constraints or rules; (2) instead, it explores the density information embedded inside the data, which is not considered in the preliminary constraint-based methods. In this sense, our formulation favors errors with significant distortion on density, while minor errors without altering density might not be handled.

Our major contributions in this paper are summarized as: (1) We formalize the dorc problem of simultaneous clus-tering and repairing. In particular, no additional parameters are introduced for dorc besides the density and distance re-quirements  X  and  X  for clustering. (2) We formulate dorc as an ilp problem (in Section 3), investigate its lp relaxation and, most importantly, show that an optimal solution to the lp problem can be directly obtained without calling a solver . (3)Wedevisea quadratic time approximation algorithm qdorc upon the lp solution (in Section 4). (4) We advance the algorithm to linear time complexity (in Section 5). The more efficient ldorc supports a trade-off between effectiveness and efficiency, via a group distance threshold  X  .
 (5) We report an extensive experimental study on both real and synthetic datasets (in Section 6). The results demon-strate that both the clustering and repairing accuracies are improved by our proposed dorc approaches.
 Table 1 lists the frequently used notations in this paper. Proofs of all lemmas and propositions can be found in [1]. Clustering. Consider a set of data points P .Let  X  : P X  P X  R + 0 be a distance function, satisfying nonnegativity  X  ( p i , p j )  X  0, identity of indiscernibles  X  ( p i , p p ,symmetry  X  ( p i , p j )=  X  ( p j , p i ), where p i ,p j
Two points p i ,p j  X  X  are said to be in  X  -neighborhood, if  X  ( p i ,p j )  X   X  .Wedenote C ( p i )= { p j  X  X |  X  ( p i ,p j the set of  X  -neighbors of p i , where p i  X  C ( p i )aswell. Definition 1 (Core points, Border points, Noise points) . Given a distance threshold  X  (Eps) and a density threshold  X  (called MinPts), a point p i with | C ( p i ) | X   X  is considered as a core point .A border point has | C ( p i ) | &lt; X  but is in neighborhood of some core point. All the other points, which are neither core points nor border points (in  X  -neighborhood of some core point), are noise points .
 Repairing. A repair over a set of points is a mapping  X  : P X  X  .Wedenote  X  ( p i ) the location of point p i after re-pairing. The  X  -neighbors of  X  ( p i ) after repairing is { p j  X  X |  X  (  X  ( p i ) , X  ( p j ))  X   X  }
Following the minimum change principle in data cleaning that we prefer a repair close to the input [24], the repairing cost  X (  X  ) is defined as where w ( p i , X  ( p i )) is the cost of repairing a point p new location  X  ( p i ). For instance, a count cost [15], with w ( points that are modified as the repairing cost. Alternatively, the distance of point locations before and after repairing could also be considered [3], with w ( p i , X  ( p i )) =  X  DORC Problem. As mentioned in the introduction, by simply relaxing parameters in dbscan , the diffusion of nearby clusters may force them to combined, e.g., in Figure 1(a), C2 (white points) is either ignored as noises by C1 or merged as C1 X  (by relaxing the density parameter). We propose to utilize the dirty (noise) points for clustering by repairing. That is, the noise points are repaired and thus clustered as either core points or border points. In other words, for each repaired  X  ( p i ), either itself or one of its  X  -neighbors has a neighborhood size greater than MinPts  X  . Since a cluster is uniquely determined by its core points [7], the repairing pro-cess with identification of core points (in the repair results) outputs the clustering results as well.
 Problem 1. Given a set of data points P , a distance thresh-old  X  and a density threshold  X  ,the Density-based Optimal Repairing and Clustering ( dorc ) problem is to find a repair  X  (i.e., a mapping  X  : P X  X  )suchthat (1) the repairing cost  X (  X  ) is minimized, and (2) for each repaired  X  ( p i ) ,either | C  X  ( p i ) | X   X  or | C  X  ( p j ) | X   X  for some p j with  X  (  X  ( p i ) , X  ( p
It is worth noting that multiple points may be repaired to the same  X  X hysical X  location, having  X  ( p i )=  X  ( p j ). Example 2. Consider a clustering density requirement  X  = 3. As shown in Figure 2(a), point p 1 ,whose | C ( p 1 ) |{ p 1 , p 2 , p 4 }| = 3, is a core point. Points p 2 and p 4 neighborhood of p 1 are border points. Point p 3 ,notin  X  neighborhood of any point, is considered as a noise point.
Figure 2(b) illustrates a possible repair  X  , where p 3 is moved to the location of p 2 ,i.e.,  X  ( p 3 )= p 2 .Point  X  ( p 2 )= p 2 remains unchanged (and similarly for p 1 ,p 4 There are two points in the location of p 2 after repairing (denoted by red and black concentric circles). We have C ( p 2 )= C  X  ( p 3 )= { p 1 , p 2 , p 3 } .Thatis,points p p upgrade to core points by repairing.
In this section, we illustrate how to formulate the dorc problem as an ilp problem, and thus existing ilp solvers can be directly applied (a built-in advantage).

Consider variable x ij , 0  X  x ij  X  1. Let x ij = 1 denote that point p i is repaired to location p j after repairing, i.e.,  X  ( p i )= p j ;otherwise, x ij = 0. Obviously, a point can only be repaired to one location, having The weight w ij for x ij is defined as the corresponding cost of repairing p i to p j , w ij = w ( p i ,p j ).

After repairing, there may exist multiple points p i be-ing repaired to the location of a point p j . The new  X  -neighborhood count of location p j is c = |{ p i  X  X |  X  (  X  ( p i ) ,p j )  X   X  }| = where h jk = 1 denotes that locations p j and p k are in  X  neighborhood; otherwise, h jk =0. Thatis, c j counts the total number of points located in p j and all of its  X  -neighbors p , after repairing.

Let y j = 1 denote that location p j has  X  -neighbor count no less than  X  , i.e., core location; otherwise, y j =0. Itfollows where n = |P| is the total number of points. It specifies that y =1iff c j  X   X  ;and y j =0iff c j &lt; X  .
 The repairing should ensure eliminating all noise points. In other words, a point is either a core point or a border point (which is a neighbor of a core point). More precisely, for any location p j with at least one point retained after repairing, i.e., x kj = 1 for some k , it is required that either this point or one of its neighbors belongs to core points (with  X  -neighborhood size no less than  X  ). We have In other words, for any j with x kj = 1 for some k , it requires either y j = 1 or some other y i =1suchthat p i is in  X  -neighborhood with p j ( h ij =1).

Given the constraints in formulas (2), (4) and (5), the dorc problem is formulated as the following ilp problem. subject to
Existing ilp solvers can be directly applied to compute the optimal solutions. It returns not only a repair x ij but also asetofcorepoints  X  ( p i )= p j with y j = 1 after repairing. Proposition 1. The optimal solution x ilp , y ilp of ilp forms where  X  ilp ( p i )= p j iff x ilp ij =1 , 1  X  i  X  n, 1  X  j  X  n Example 3 (Example 2 continued) . Consider again the ex-ample of 4 points in Figure 2 with clustering density require-ment  X  = 3. We show how the repair  X  ,with  X  ( p 3 )= p 2 in Figure 2(b), corresponds to the feasible solution x to the ilp , where x 11 = x 22 = x 44 =1and x 32 =1.

For the location of p 2 ,wehave c 2 = 2 + 1 = 3 by formula (3). It follows y 2 = 1 according to formula (4). In other words, all the points in the location of p 2 after repairing are core points, i.e., p 2 and p 3 as indicated in Example 2.
For the location of p 3 , since there is no point retained after repairing, having 4 k =1 x k 3 = 0, formula (5) is satis-fied. Therefore, the solution corresponding to the repair satisfies all the constraints in formula (6) and is a feasible solution of ilp .
 We can show that the dorc problem is always solvable. Proposition 2. For  X &lt;n , a feasible solution to the ilp problem always exists.
 Proof. By simply repairing all the points to a single location say p 1 ,wehave x i 1 =1and y 1 = 1, i.e., all the points become core points locating in p 1 after repairing.
Another advantage of modeling dorc as ilp is the possi-ble lp relaxation for efficiently computing near optimal so-lutions. In this section, we first indicate that an optimal solution can be directly derived without calling a solver. An approximation algorithm is then built upon the lp solution.
The ilp problem can be relaxed as a lp problem by chang-ing the integer constraints in formula (6) to 0  X  x ij  X  1 y  X  1. We show below that an optimal solution to the lp problem can be directly obtained without calling a solver. Lemma 3. There always exists an optimal solution x lp , y lp for the lp problem, where x lp ii =1 ,i =1 ,...,n (7) x lp ij =0 ,i = j, i =1 ,...,n,j =1 ,...,n
As mentioned, a cluster is uniquely determined by its core point [7]. The clustering and repairing process is thus to round the lp solution, i.e., to determine x ij  X  X  0 , 1 } ,y { 0 , 1 } according to the aforesaid lp solution ( x lp , y lp ).
Before introducing the algorithm, let us first rephrase core, border and noise points in Definition 1 in terms of neighborhood h .
 Definition 2 (Core points, Border points, Noise points) . For a set of data points P and the corresponding neigh-borhood h ,corepoints C ( h ) , border points B ( h ) , and noise points N ( h ) w.r.t. h are N ( h )= P\ ( C ( h )  X  X  ( h )) .
 Algorithm 1 ( qdorc ) presents an approximation to dorc . We consider all the noise points N ( h )w.r.t. h , denoted by in Line 1. Note that y lp j in the lp solution can be interpreted
Figure 3: Example of repairing with LP solution as the probability of a point p j being core point in clustering. In each step, we consider a point p j  X  X  with the largest in Line 4. In order to become a core point, point p j needs at least (1  X  y j )  X  additional  X  -neighbors. We heuristically consider the noise point p i with the minimum w ij (in Line 7) and conduct the repairing of p i to p j , i.e., assign x ij there are no sufficient noise points retained for repairing, i.e., the remaining noise points cannot form at least  X  neighbors of the point p j , we repair all the remaining noise points to their closest non-noise points in Line 13.
 Algorithm 1: qdorc ( P , h , X , X  )
Data :Asetofdatapoints P with neighborhood h ,
Result : A set of core locations with y j =1in y and
N := N ( h )w.r.t. h ; 2 x , y := x lp , y lp the lp solution w.r.t. h in Lemma 3; 3 while N =  X  do // noise points exist 4 let p j  X  X  with the maximum y j and y j &lt; 1; 5 if |N|  X  (1  X  y j )  X  then 7 let p i  X  X  with the minimum w ij ; 8 x ii := 0 ,x ij := 1 and N := N\{ p i } ; 9 until (1  X  y j )  X  times ; 10 y j := 1 and N := N\ ( { p j } X  X  p k | h jk =1 } ); 11 else // no sufficient noises retain 12 for each p i  X  X  do 13 let p k  X  X \N with the minimum w ik ; 14 x ii := 0 ,x ik := 1 and N := N\{ p i } ;
The returned result x corresponds to a repair  X  qdorc such that  X  qdorc ( p i )= p j for x ij =1 , 1  X  i  X  n, 1  X  j  X  n Example 4. Consider an example of 4 points in Figure 3(a) with clustering density requirement  X  =3. Eachedge, Point p 3 is identified as a noise point, referring to Definition 2 w.r.t. neighborhoods, having N = { p 3 } . According to the formulas in Lemma 3, we derive an lp solution, with y =1 , y 2 =2 / 3 , y 3 =1 / 3 , y 4 =2 / 3.

Line 3 in Algorithm 1 selects a point p 2 with the maximum y =2 / 3 &lt; 1. To make p 2 a core point, i.e., y 2 =1,weneed at least one additional noise point, to repair to the location of p 2 . Line 7 selects p 3  X  X  , and repair it to the location of p by setting x 32 =1.

Since no noise point retains in N , the algorithm termi-nates and returns a solution with x 11 = x 22 = x 44 =1 ,x 1 ,y 1 = y 2 = 1 (all the others are equal to 0). It corresponds to a repair  X  with  X  ( p 3 )= p 2 (and 3 other unchanged points) and a set of core points {  X  ( p 1 ) , X  ( p 3 ) , X  ( p 3 ) core locations of p 1 , p 2 ).
 Proposition 4. Algorithm 1 ( qdorc ) runs in O( n 2 ) time, returns a feasible solution to the ilp problem, where n =
It is worth noting that capturing the  X  -neighborhood h for qdorc is costly (in O( n 2 ) time). Following the same line of performing efficient density-based clustering [23], we present an algorithm for efficiently estimating the neighborhood by partitioning data points into groups, and perform dorc over the estimate neighborhood (in linear time). Let  X  be a distance threshold in grouping, 0  X   X   X   X  2 . Each group consists of a leader data point p and a set of follower data points, denoted by follower ( p ). Each follower should have distance to its leader no greater than the group distance threshold  X  ,i.e.,  X  ( p , p i )  X   X ,  X  p i  X  follower
Lines 2-8 in Algorithm 2 present the grouping procedure of generating leaders and followers. Let L denote the set of leaders. Each point p  X  X  is either assigned as a follower of some existing leader p l  X  X  (in Line 8 if  X  ( p l , p ) created as a new leader (in Line 5).

We introduce approximate  X  -neighborhoods, h L for es-timating h . As presented in Line 11 in Algorithm 2, for two leaders p k , p l  X  X  ,weassign h L kl := 1 if  X  ( p k  X   X  2  X  rather than  X  ( p k , p l )  X   X  for h kl := 1. For any p we use the neighborhood of leaders to approximate that of ber of neighbors w.r.t. h L ij for any follower p j  X  follower it is equivalent to  X  X ount X  the corresponding leaders. Lemma 5. The neighbor count w.r.t. h L ij of a point p j has where p i  X  follower ( p k ) , p j  X  follower ( p l ) ,and number of leaders.

When calling qdorc ( P , h L , X , X  )inLine12inAlgorithm 2 ldorc , the noise points N = N ( h L )inLine1andthe lp solution x , y in Line 2 in Algorithm 1 qdorc can be com-puted by counting the aforesaid leaders (in O( mn )time).
For heuristically choosing repair candidates w.r.t. weight w in Lines 7 and 13 of Algorithm 1 qdorc , we employ again the leaders. Similar to approximating the neighborhood of followers by that of leaders, we replace Line 7 of Algorithm 1bychoosinga p i  X  follower ( p k ) such that the weight w of leaders is minimized, where p j  X  follower ( p l ) ,k = Example 5. Consider the example of 4 points in Figure 5(a). Suppose that the points are processed in an order of p 1 , p 2 , p 3 , p 4 during grouping. Two groups are generated with leaders p 1 and p 3 . The followers are follower ( p Algorithm 2: ldorc ( P , X , X , X  )
Data :Datapointset P , group distance threshold  X  ,
Result : A set of core locations with y j =1in y and
L :=  X  ; 2 for each point p  X  X  do 3 find the first leader p l  X  X  s.t.  X  ( p l , p )  X   X  ; 4 if p l does not exist then 5 L = L { p } ; // create a new leader 6 follower ( p ):= { p } ; 8 follower ( p l ):= follower ( p l ) { p } ; 9 for eachleaderpairp k , p l  X  X  do 10 if  X  ( p k , p l )  X   X   X  2  X  then 11 h L kl := 1 ; // for neighborhood estimation 12 return qdorc ( P , h L , X , X  ) { p 1 , p 2 } and follower ( p 3 )= { p 3 , p 4 } . Suppose that  X   X  2  X  . The estimate neighborhoods are h L 12 = h L 21 = h h 43 = 1 (all the others are equal to 0). Algorithm 2 then calls qdorc ( P , h L , X , X  ) in Line 12 to process the repairing.
According to Lemma 5, the number of neighbors w.r.t. h
L of a follower, say p 2 , can be calculated by counting the sizes (number of followers) of all leaders with distance to p (leader of p 2 )in  X   X  2  X  , i.e., n i =1 h L i 2 = | follower The same neighbor count is calculated for the other points.
Given a clustering density threshold  X  = 4 and the afore-said neighbor counts, all the 4 points are identified as noise points w.r.t. h L , i.e., N ( h L )= { p 1 , p 2 , p 3 , p Definition 2. The lp solution has y 1 = y 2 = y 3 = y 4 =2
Suppose that p 2 is first considered in Line 4 (in Algorithm 1) when calling qdorc ( P , h L , X , X  ). Referring to y 2 =2 it has to repair at least two noise points in order to make itself a core point. As introduced in the paragraph before Example 5, we consider a point (say p 4 )in follower ( p 3 repair, since p 3 (is the only leader that) has the minimum weight w 31 to the leader p 1 of p 2 . As illustrated in Figure 5(b), p 4 is repaired to the location of p 2 ,having x 42  X  ( p 4 )= p 2 . The algorithm carries on by repairing p 3 to the location of p 2 ,  X  ( p 3 )= p 2 ,inordertomake y 2 =1. Since p upgrades to a core point, its neighbor p 1 is removed from as well. We have N =  X  and the algorithm terminates. Proposition 6. Algorithm 2 ( ldorc ) runs in O( mn ) time, where n = |P| and m = |L| . h L contains false negatives, i.e., when a h L ij = 0 indicates that the neighborhood does not exist between p i and p j (the result is negative), but it is in fact present ( h ij Nevertheless, h L willneverleadtofalsepositives,where h ij = 1 but h ij =0.
 Lemma 7. For the neighborhood estimation for h ,italways
In other words, the neighborhood h L is a subset of h .The returned result w.r.t. h L must also be a feasible solution of ilp w.r.t. h . Correctness of Algorithm 2 is guaranteed.
Indeed, N ( h L ) considered for repairing in ldorc is a su-perset of N ( h ) for the original qdorc .
 Lemma 8. For the noise points w.r.t. the neighborhood, we have N ( h L )  X  X  ( h ) .

We show that the number of leaders m is bounded by a constant w.r.t.  X  , given a finite domain of data instances (i.e., with a bounded maximum distance of two points). Proposition 9. The number of leaders m = |L| is bounded by m &lt; (  X  max  X  +1) 2 ,where  X  max is the maximum distance between two points, and  X  is the group distance threshold.
Combining Propositions 6 and 9, Algorithm 2 ( ldorc ) runs in O((  X  max  X  +1) 2 n ) time, i.e., a linear time algorithm inthenumberofdatapoints n . When given a finite space of data instances, the maximum distance of two points  X  max is a constant. However, for an infinite space,  X  max could be arbitrarily large.
 Indeed, the parameter  X  of group distance threshold provides a trade-off between efficiency and effectiveness.
First, the larger (closer to  X  max ) the group distance thresh-old  X  , the smaller the bound of m (number of leaders) is, i.e., lower algorithm time cost. However, for an extremely large  X  , e.g., the largest  X  =  X  2 , no neighborhood between leaders exist given  X   X  2  X  = 0. That is, only the neighborhoods be-tween the points in the same group can be identified in h Such a weak estimation leads to inaccurate computation of y in the lp solution, and thus the corresponding repairing might not be reliable (i.e., lower repairing and clustering accuracy as illustrated in Figure 10 of experiments).
On the other hand, for a small  X  ,e.g.,  X  =0,itleads to single size groups, where each point corresponds to the leader of a group but without any other follower. In this sense, ldorc is exactly qdorc without grouping. Since the lp solution w.r.t. accurate neighborhood is utilized, the al-gorithm effectiveness is high in this case. However, the cor-responding time cost increases as grouping takes no effect.
Experimental evaluation answers the following questions: (1) By utilizing dirty data, can it form more accurate clus-ters? In Figure 6, we illustrate an example of artificial data points on how the dirty points affect the clustering and how the simultaneous clustering and repairing work. Figures 7, 8 and 11 compare quantitatively the clustering accuracy of our dorc to the existing dbscan (directly discarding all noisy data)onbothsyntheticandrealdatasets. (2) By simultaneous repairing and clustering, in practice is the repairing accuracy improved compared with the existing data repairing approaches? Figures 9 and 12 compare our proposed dorc with the existing data repairing methods over real data sets. (3) How do the approaches scale? In Figure 10, we show that it is possible to trade effectiveness for efficiency in ldorc via the group distance threshold  X  . Figure 13 re-ports the scalability over a large-scale data set with up to 400k data points.

Our programs are implemented in Java and all experi-ments were performed on a PC with Intel(R) Core(TM) i7-2600 3.40GHz CPU and 8 GB memory.
 Clustering Accuracy. To evaluate the clustering accuracy, we employ the purity measure [18]. It evaluates the most frequent class label of data points in each cluster, that is, counting the maximum number of data points in each cluster i corresponding to a class j . The higher the measure is, the better the clustering accuracy is. Repairing Accuracy. The repair accuracy evaluates how close the repaired result  X  ( p i ) is compared to the true loca-tion truth ( p i ). We employ the repair error measure, root-mean-square error (RMS) [13], The lower the repair error is, the more accurate the repair is (closer to the ground truth). Example Results. To study the exact ilp and approximate dorc solutions, we draw a small synthetic dataset as shown in Figure 6. For the clean data in Figure 6(a), there are two classes, C1 and C2, with 164 data points. We introduce up to 34 artificial dirty points (by moving points in particular areas to random locations in a certain radius). Clustering approaches are performed over the data with dirty points.
Figure 6(b) presents the clustering results by dbscan over the dirty data (without repairing). Three clusters present and a number of 17 black points are identified as noises, i.e., not belonging to any cluster. Figure 6(c) reports the results by our proposed dorc of simultaneous repairing and clustering over the same dirty dataset. As illustrated in Figure 6(c), dorc can successfully repair the dirty points and return two clusters similar to the two classes in ground truth in Figure 6(a). These results verify our intuition that a large number of dirty points may greatly affect the clus-tering results (Figures 6(a) vs. 6(b)), while the clustering with repairing can address the variance introduced by dirty data (Figures 6(a) vs. 6(c)). The results illustrate that our proposal is not limited to splitting cluster (in Figure 1), but may also return merged clusters that are erroneously split by dbscan in Figure 6(b).
 Quantitive Results. Figure 7 delivers the accuracies of clus-tering and repairing under various dirty rates over the syn-thetic dataset. A dirty rate 0.21 denotes that 21% points are modified as dirty data. Besides dbscan , the results of an-other density-based clustering, optics [2], are also reported.
As shown in Figure 7(a), it is not surprising that the clus-tering accuracy of all approaches drops with the increase of dirty rate. Most importantly, our dorc approaches can sig-nificantly improve the accuracy of clustering compared to dbscan , especially when the dirty rate is large. Moreover, the clustering purity of the approximation algorithm qdorc is comparable to that of the exact method ilp (we fail to obtain ilp results in dirty rates greater than 0.18 owing to the extremely high time costs).

Figure 7(b) reports repairing error. The exact method ilp has lower repairing error. The result verifies the rationale of considering the minimum cost repairs, following the min-imum change principle in data repairing [24]. Nevertheless, the approximate qdorc approach is comparable with ilp , especially when the dirty rate is large. The reason behind is that with an extremely large amount of dirty data, the repairing (even the exact one) might not be precise.
For the real dataset, we collect 1 818 GPS reading points in three nearby buildings, which correspond to three classes. Owing to the weak signal inside buildings, a large amount of GPS readings are inaccurate (139 points outside the build-ings). We manually alter these (originally embedded rather than randomly introduced) dirty data points, i.e., label the true building for each inaccurate point as the ground truth. Clustering approaches are conducted over the dirty dataset, where the truth class of each dirty point is labeled. We use Euclidean distance as the distance function  X  on GPS points.
Note that the GPS data are continuously collected in a time period. Filtering techniques can be applied to clean the noisy data in such a time-space correlated time-series [14], e.g., by the widely used Median Filter ( mf ) [22]. The main idea of mf is to go through the GPS readings one by one in the time-series, repairing each point with the me-
Since existing datasets for evaluating clustering are not la-beled for dirty data, we need to collect and manually label the ground truth of dirty data points (with a great manual effort). dian of (temporally) neighboring points. Therefore, instead of simultaneous repairing and clustering, mf + dbscan first applies mf to clean the GPS data, and then performs the existing dbscan clustering over the mf pre-processed data.
Figures 8 and 9 present the clustering and repairing ac-curacy results with various dirty rates, density thresholds and distance thresholds  X  (parameters  X  and  X  are inherited from the density-based clustering dbscan ,see[7]foradis-cussion on determining such parameters). The results are generally similar to that on the synthetic data in Figure 7. In addition, by applying mf , the clustering accuracy of db-scan is slightly improved. It verifies the motivation of this study, i.e., utilizing the dirty data can enhance clustering.
With various clustering requirements of  X  and  X  in Figures 8 and 9, our proposed qdorc always shows aclear improve-ment. In particular, the clustering accuracy improvement by qdorc is more significant than that of mf+dbscan in Figures 8(b) and 8(c). The result is not surprising given the significantly lower repairing error of qdorc compared to mf in the corresponding Figures 9(b) and 9(c).

Figure 10 verifies the analysis at the end of Section 5 that in ldorc provides a trade-off in efficiency and effectiveness. As shown, a large  X  = 1 2  X  shows high efficiency (low time cost) in Figure 10(c), while its clustering accuracy in Figure 10(a) is low and the repairing error in Figure 10(b) is high. On the other hand, a small  X  leads to high time cost but lower repairing error and better clustering purity. When  X  = 0, the result of ldorc is exact the same as that of qdorc without grouping. The corresponding time cost (of  X  = 0) may be a bit higher since ldorc has extra cost on grouping.
Restaurant 2 is a collection of 864 restaurant records that contains 112 duplicates and is widely used for record match-ing [21]. Each group of duplicates can be interpreted as a http://www.cs.utexas.edu/users/ml/riddle/data.html class. A record includes four attributes, name, address, city and type. Edit distance [19] is employed as the distance function  X  . Since the data are originally clean, following the same line of evaluating data repairing performance [3], we introduce dirty values by randomly replacing values (could be any data instead of one particular area) in the data set, with various dirty rates. Different from filtering numerical values on GPS data set, integrity constraints (such as fd name,address  X  city) in databases are employed to clean the dirty data [3]. It is the reason why we employ this data set , where both the existing clustering and repairing techniques can be applied (as rational baselines).
 The results in Figures 11 and 12 are generally similar to Figures 8 and 9 over GPS data. Figure 11 shows that by applying fd -based repairing first, the clustering accuracy is improved by fd + dbscan . Our proposed qdorc achieves significantly higher clustering and repairing accuracies. Sim-ilar results on various  X  and  X  are also observed.
Most importantly, by combining our qdorc with the ex-isting fd constraint-based repairing, i.e., fd + qdorc ,both the clustering purity and repairing error are further im-proved compared to the fd (+ dbscan ) approach. The re-sults demonstrate that the proposed dorc is complementary to the existing constraint-based repairing , by directly apply-ing qdorc over the data repaired by fd .
The experiment on Foursquare dataset focuses on scala-bility over large data sizes, up to 400k check-in data points. Since this large scale data set is not pre-labeled, we mainly observe the time cost.

First, as shown in Figure 13(a), while the number of noise points increases as the data size, the number of leaders keeps low. It verifies the result in Proposition 9 that the number of leaders m is bounded. Consequently, the corresponding time cost of ldorc increases linearly, in Figure 13(b). The result verifies the linear time complexity of the ldorc algorithm in Proposition 6. Finally, Figure 13(c) reports a result similar to Figure 10(c) over GPS data that with the increase of  X  the efficiency is improved.
Finally, in order to show that the proposed approach im-proves the clustering accuracy on real data, we report ex-periments on two labeled publicly available benchmark data, Iris and Ecoli, from UCI 3 . Moreover, a state-of-the-art eval-uation measure, normalized mutual information (NMI) [20], is employed for clustering validation. As shown in Figure 14, similar results are generally observed, i.e., our qdorc still http://archive.ics.uci.edu/ml/datasets.html Figure 14: NMI clustering accuracy on real datasets shows much higher (NMI) accuracy (compared to dbscan and optics ). Clustering. Density-based clustering has proved to be use-ful in various applications (see [16] for a survey). Given a distance threshold Eps  X  and a minimum requirement of neighborhood MinPts  X  , dbscan [7] determines the density region of core points as well as the corresponding border points. Efficient implementation [23] and incremental com-putation [6] of dbscan has been devised. In this study, we also follow the settings of Eps  X  and MinPts  X  . optics [2] produces a cluster-ordering of data points with various Eps levels, and maintains a sorting of core points. In contrast, we rank non-core points in this paper, according to their likelihood of being core points after repairing. denclue [11] generalizes the notation of density clusters by introducing the concept of influence functions. The in-fluence function models the influence of a point to its neigh-bors, e.g., by square wave functions or Gaussian functions. f dbscan [17] considers the extension over fuzzy/uncertain data points. Rather than pruning outliers, Gupta and Ghosh [9] identify dense regions of subsets of points. Xiong et al. [25] study clustering anomaly, where clusters are formed by a (very) small portion of points in a large data set. Again, all these studies focus on identifying noise/non-noise points, while the large amount of dirty data (identified as noises) are still not employed to form clusters. In contrast, our proposal considers cleaning and clustering with dirty data.
While outlier detection (see [12] for a survey) identifies dirty points, data repairing further modifies the points for correction. Indeed, our proposal incorporates density-based dbscan (that also identifies noises and could be regarded as an outlier detection method). In this sense, data repairing and outlier detection are complementary. Repairing. Besides the minimum modification model [24], which is also adopted in our study, a deletion model [5] is often considered. The deletion model finds the minimum re-moval of dirty data. In this sense, dbscan is also a deletion-based cleaning technique that removes dirty data. As dis-cussed, simply ignoring the large amount of dirty data as noises by the existing clustering approaches is not rational and may affect greatly the clustering results.

In addition to the widely used fd constraints, other types of data quality rules are employed (see [8] for a review). Again, as discussed in the introduction, our study considers only the density information embedded in data rather than the external knowledge of constraints or rules. Most impor-tantly, following the same line of combining with fd -based repairing (in Section 6.3), our proposal is complementary to these state-of-the-art techniques, whenever extra informa-tion such as master data or constraint rules are available.
Preliminary density-based clustering can successfully iden-tify noisy data but without cleaning them. On the other side, existing constraint-based repairing relies on external constraint knowledge without utilizing the density informa-tion embedded inside the data. In this paper, inspired by the aforesaid victory and defeat, we study a novel problem of clustering and repairing dirty data at the same time. To the best of our knowledge, this is the first study on enhanc-ing clustering by repairing and utilizing dirty data. With the happy marriage of clustering and repairing advantages, both the clustering and repairing accuracies are significantly improved as presented in the experimental evaluation.
To tackle the dorc problem of simultaneous clustering and repairing, our major technique contributions include: (1) the formulation of dorc as an ilp problem; (2) an opti-mal solution to the corresponding lp relaxation that can be directly obtained without calling lp solvers; (3) a quadratic time approximation algorithm devised upon the lp solution; and (4) a linear time improvement via grouping data points. In particular, the linear time algorithm provides a trade-off between effectiveness and efficiency.
 This work is supported in part by the Tsinghua University Initiative Scientific Research Program; China NSFC under Grants 61202008, 91218302 and 61370055; National Grand Fundamental Research 973 Program of China under Grant 2012-CB316200; Huawei Innovation Research Program. [1] Full Version. [2] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and [3] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A [4] H. Chen, W.-S. Ku, H. Wang, and M.-T. Sun.
 [5] J. Chomicki and J. Marcinkowski. Minimal-change [6] M. Ester, H.-P. Kriegel, J. Sander, M. Wimmer, and [7] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [8] W. Fan. Dependencies revisited for improving data [9] G. Gupta and J. Ghosh. Robust one-class clustering [10] M. A. Hern  X andez and S. J. Stolfo. Real-world data is [11] A. Hinneburg and D. A. Keim. An efficient approach [12] V. J. Hodge and J. Austin. A survey of outlier [13] S. R. Jeffery, M. N. Garofalakis, and M. J. Franklin. [14] K. H. Ji and T. A. Herring. A method for detecting [15] S. Kolahi and L. V. S. Lakshmanan. On [16] H.-P.Kriegel,P.Kr  X  oger, J. Sander, and A. Zimek. [17] H.-P. Kriegel and M. Pfeifle. Density-based clustering [18] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [19] G. Navarro. A guided tour to approximate string [20] X. V. Nguyen, J. Epps, and J. Bailey. Information [21] P. D. Ravikumar and W. W. Cohen. A hierarchical [22] J. W. Tukey. Exploratory data analysis. 1977. [23] P. Viswanath and V. Suresh Babu. Rough-dbscan: A [24] J. Wijsen. Database repairing using updates. ACM [25] Y. Xiong, Y. Zhu, P. S. Yu, and J. Pei. Towards
