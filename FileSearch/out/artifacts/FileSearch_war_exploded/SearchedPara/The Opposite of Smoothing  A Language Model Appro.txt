 Exploiting information induced from ( query-specific )clus-tering of top-retrieved documents has long been proposed as means for improving precision at the very top ranks of the returned results. We present a novel language model approach to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. While most previous cluster ranking approaches focus on the clus-ter as a whole, our model also exploits information induced from documents associated with the cluster. Our model substantially outperforms previous approaches for identify-ing clusters containing a high relevant-document percentage. Furthermore, using the model to produce document rank-ing yields precision-at-top-ranks performance that is con-sistently better than that of the initial ranking upon which clustering is performed; the per formance also favorably com-pares with that of a state-of-the-art pseudo-feedback re-trieval method.

Clustering the results returned by a search engine in re-sponse to a query has long been proposed as means for improving retrieval effectiveness [35, 43, 14, 27, 29, 21]. Much of the motivation for employing clustering of search results (a.k.a. query-specific clustering ) comes from van Ri-jsbergen X  X  cluster hypothesis [40], which states that  X  X losely associated documents tend to be relevant to the same re-quests X . Indeed, researchers showed that applying various clustering techniques to the documents most highly ranked by some initial search produces some clusters that contain a very high percentage of relevant documents [14, 38, 17, 30]. Moreover, positioning these clusters X  constituent doc-uments at the very top ranks of t he returned results yields precision-at-top-ranks performance that is substantially bet-ter than that of (state-of-the-art) document-based retrieval approaches [14, 38, 17]. However, automatically identifying these clusters is a very hard task [43, 29, 21, 31, 30].
We present a novel language-model-based [34, 8] approach to ranking query-specific clusters by the presumed percent-age of relevant documents that they contain. The key in-sight that guides the derivation of our cluster-ranking model is that documents that are strongly associated with a clus-ter can serve as proxies for ranking it. Case in point, since documents can be considered as more  X  X ocused X  units than clusters, they can serve, for example, as mediators for esti-mating the cluster-query similarity. Thus, while most pre-vious approaches to ranking various types of clusters [15, 7, 41, 43, 19, 29, 31] focus on the cluster as a  X  X hole X  unit, our model integrates cluster-induced information with that induced from its proxy (associated) documents. Hence, we conceptually take the opposite approach to that of cluster-based smoothing of document language models that has re-cently been used for document ranking [2, 19, 29, 42].
Our model integrates two types of information induced from clusters and their proxy documents. The first is the es-timated similarity to the query. The second is the centrality of an element (document or cluster) with respect to its refer-ence set (documents in the initially-retrieved list or clusters of these documents); centrality is defined in terms of textual similarity to (many) other (central) elements in the reference set [20]. Using either (or both) type(s) of information just described  X  induced from a cluster as a whole and/or from its proxy documents  X  yields se veral novel cluster-ranking criteria that are integrated in our model. We study the relative contribution of each of the criteria to the overall effectiveness of our approach.

Empirical evaluation shows that our cluster-ranking model consistently outperforms pre viously proposed methods in identifying clusters that contain a high percentage of rel-evant documents. Furthermore, positioning the constituent documents of the cluster most highly ranked by our model at the top of the returned document list yields precision-at-top-ranks performance that is substantially better than that of the initial document ranking upon which clustering was performed. The resultant performance also favorably com-petes with that of a state-of-the-art pseudo-feedback-based document retrieval method. assume that the following have been fixed: a query q ,a corpus of documents D , and an initial list of N documents init  X  X  (henceforth D init ) that are the highest ranked by some search performed in response to q . We assume that D init is clustered into a set of ( query-specific )docu-ment clusters C l ( D init )= { c 1 ,  X  X  X  ,c M } by some clustering algorithm 1 . Our goal is to rank the clusters in C l ( D the presumed percentage of relevant documents that they contain. In what follows we use the term  X  X luster X  to refer either to the set of documents it is composed of, or to a (language) model induced from it. We use p y ( x )todenote the language-model-based similarity between y (a document or a cluster) and x (a query or a cluster); we describe our language-model induction method in Section 4.1.
Similarly to the language model approach to ranking doc-uments [34, 8], and in deference to the recent growing in-terest in automatically labeling document clusters and topic models [11, 39, 32], we state the problem of ranking clusters as follows: estimate the probability p ( c | q )thatcluster c can be labeled (i.e., its content can be described) by the terms in q . Our hypothesis is that the higher this probability is, the higher is the percentage of documents pertaining to q that c contains.

Since q is fixed, we use the rank equivalence to rank the clusters in C l ( D init ). Thus, c is ranked by com-bining the probability p ( q | c )that q is  X  X enerated X  2 for c with c  X  X  prior probability ( p ( c )) of  X  X enerating X  any la-bel. Indeed, most prior work on ranking various types of clusters [15, 7, 43, 41, 19, 29] implicitly uses uniform distri-bution for p ( c ), and estimates p ( q | c ) (in spirit) by comparing arepresentationof c as a whole unit with that of q .
Here, we suggest to incorporate a document-mediated ap-proach to estimating the probability p ( q | c ) of generating the label q for cluster c . Since documents can be considered as more coherent units than clusters, they might help to gener-ate more  X  X nformative/focused X  labels than those generated by using representations of clusters as whole units. Such an approach is conceptually the opposite of smoothing adoc-ument representation (e.g., language model) with that of a cluster [2, 19, 29, 42]. In what follows we use p ( q | d )tode-note the probability that q is generated as a label describing document d  X  X  content (cf. the language modeling approach to ranking documents [34, 8]). Also, we assume that p ( d ) X  the prior probability that document d generates any label  X  is a probability distribution over the documents in the corpus D .

We let all (and only) documents in the corpus D to serve as proxies for label generation for any cluster in C l ( D
Clustering the documents most highly ranked by a search performed in response to a query is often termed query-specific clustering [43]. We do not assume, however, that the clustering algorithm has knowledge of the query in hand.
While the term  X  X enerate X  is convenient, we do not assume that clusters or documents literally generate labels, nor do we assume an underlying generative theory as in Lavernko and Croft [25] and Lavrenko [23], inter alia .
 Consequently, we assume that p ( d | c ), the probability that d is chosen as a proxy for c for label generation, is a probability distribution defined over the documents in the corpus D .
Then, we can write using some probability algebra
We use  X p ( q | c )+(1  X   X  ) p ( q | d i )(where  X  is a free parame-ter) as an estimate for p ( q | c, d i ) [37, 19] in Equation 1, and by applying probability algebra we get the following scoring principle 3 for clusters Equation 2 scores c by a mixture of (i) the probability that q is directly generated from c combined with c  X  X  prior probabil-ity of generating any label, and (ii) the (average) probability that q is generated by documents that are both  X  X trongly as-sociated X  with c (as measured by p ( c | d i )) and that have a high prior probability p ( d i ) of generating labels.
We next derive specific ranking algorithms from Equation 2 by making some assumptions and estimation choices.
We first make the assumption, which underlies (in spirit) most pseudo-feedback-based retrieval models [4, 44, 26], that the probability of generating q directly from d i ( p ( q quite small for documents d i that are not in the initially retrieved list D init ; hence, these documents have relatively little effect on the summation in Equation 2. Furthermore, if the clusters in C l ( D init ) are produced by a  X  X easonable X  clustering algorithm, then p ( c | d i )  X  the cluster-document  X  X ssociation strength X   X  might be assumed to be signifi-cantly higher for documents from D init that are in c than for documents from D init that are not in c .Consequently,we truncate the summation in Equation 2 by allowing only c  X  X  constituent documents to serve as it proxies for generating q . Such truncation does not only alleviate the computational cost of estimating Equation 2, but can also yield improved effectiveness as we show in Section 4.3. In addition, we follow common practice in the language model framework [8], specifically, in work on utilizing cluster-based language models for document retrieval [29, 19], and use language-model estimates for conditional probabilities to produce our primary ranking principle: S core ( c ) def =  X p ( c ) p c ( q )+(1  X   X  ) Note that using p d ( c )for p ( c | d ) means that we use the proba-bility of generating the  X  X abel X  c (i.e., some term-based rep-resentation of c )fromdocument d as a surrogate for the document-cluster association strength.

The remaining task is to estimate the document and clus-ter priors  X  p ( d )and p ( c ), respectively.
The shift in notation and terminology from  X  p ( c | q ) rank  X  X core of c  X  echoes the transition from using (model) proba-bilities to estimates of such probabilities. ( q )
P c p d i ( c ) Cent ( d i )
P
P ( q )+(1  X   X  )
Following common practice in work on language-model-based retrieval we can use uniform distribution for the docu-ment prior p ( d ) [8], and similarly assume a uniform distribu-tion for the cluster prior p ( c ). Such practice would have been a natural choice if the clusters we want to rank were pro-duced in a query-independent fashion. However, we would like to exploit the fact that the clusters in C l ( D init composed of documents in the initially retrieved list D init Case in point, since D init was retrieved in response to q , documents in D init that are considered as  X  X eflecting X  D content might be good candidates for generating the label q [20]; a similar argument can be made for clusters in C l ( that  X  X eflect X  its content. Therefore, instead of using  X  X rue X  prior distributions, we use biases that represent the central-ity [20] of documents with respect to D init and the centrality of clusters with respect to C l ( D init ). 4
Specifically, we adopt a recently suggested approach to inducing document centrality [20] that is based on measur-ing the similarity of a document (in D init )toothercentral documents in D init . To quantify this recursive centrality definition, we compute PageRank X  X  [3] stationary distribu-tion over a graph wherein vertices represent documents in D init and edge-weights represent inter-document language-model-based similarities [20]. We then set p ( d ) def = Cent ( d ) for d  X  X  init and 0 otherwise, where Cent ( d )is d  X  X  Page-Rank score; hence, p ( d ) is a probability distribution over the entire corpus D .
 Analogously, we set p ( c ) def = Cent ( c )for c  X  C l ( where Cent ( c )is c  X  X  PageRank score as computed over a graph wherein vertices are clusters in C l ( D init )andedge-weights represent language-model-based inter-cluster simi-larities. ( p ( c ) is thereby a probability distribution over the given set of clusters C l ( D init ).)
The construction method of the document and cluster graphs follows an approach for constructing document-solely graphs [20], and is elaborated in Appendix A.

Using the document and cluster induced biases, we can now fully instantiate Equation 3 to derive ClustRanker ,
The biases are not  X  X rue X  prior distributions, because of the virtue by which D init was created, that is, in response to the query. However, we take care that the biases form valid probability distributions as we show later. our primary cluster-ranking algorithm:
The ClustRanker algorithm ranks cluster c by integrating several criteria; specifically, (i) ClustCent  X  c  X  X  central-ity ( Cent ( c )), (ii) ClustQueryGen  X  the possibility to generate the label q directly from c as measured by p c ( q ), (iii) DocCent  X  the centrality of c  X  X  constituent documents ( Cent ( d )), and (iv) DocQueryGen  X  the possibility to generate q by c  X  X  constituent documents as measured by p ( q ). (Note that the latter two are combined with the cluster-document association strength p d ( c )).

To study the effectiveness of each of these criteria (and some of their combinations) for ranking clusters, we apply the following manipulations to the ClustRanker algorithm: (i) setting  X  to 1 (0) to have only the cluster (documents) generate q , (ii) using uniform distribution for Cent ( c )(over C l ( D init )) and/or for Cent ( d )(over D init ) hence assuming that all clusters in C l ( D init ) and/or documents in D central to the same extent (we assume that the number of clusters in C l ( D init ) is the same as the number of docu-ments in D init , as is the case for the clustering method that we employ in Section 4), and (iv) setting p c ( q )( p d ( q )) to the same constant value thereby assuming that all clusters in C l ( D init )(documentsin D init ) have the same probabil-ity of directly generating q . For instance, setting  X  to 0 and p d ( q ) to some constant, we rank c by DocCent  X  the weighted-average of the centrality values of its constituent documents: sultant cluster-ranking methods that we explore. ( X   X   X  indi-cates that a method utilizes two criteria.)
Query-specific clusters are often used to visualize the re-sults of search so as to help users to quickly detect the rel-evant documents [5, 14, 28, 27, 33, 36]. Leuski [27], for example, orders (hard) clusters in an interactive retrieval system by the highest query-similarity exhibited by any of their constituent documents. The cluster that contains the document most similar to the query is always ranked first, as is the case in [36]. Hence, such a ranking approach cannot be naturally employed with overlapping clusters, which are used in our experiments in Section 4 following previous work on cluster-based retrieval [13, 19, 21, 31, 30]. In contrast, our framework is not committed to any specific clustering technique, and the ClustRanker algorithm leverages infor-mation from all constituent documents of a cluster.
Some work uses information from query-specific clusters to smooth language models of documents in the initial list so as to improve the document-query similarity estimate [29, 17]. In a related vein, graph-based approaches for re-ranking the initial list utilize inter-document similarity information [20, 21, 9]. These approaches can potentially help to improve the performance of our ClustRanker algorithm, as they pro-vide a higher-quality document ranking to begin with.
Ranking (both query-specific and query-independent )clus-ters in response to a query has t raditionally been based on comparing a cluster representation with that of the query [15, 7, 43, 19, 29, 31, 30]. The ClustQueryGen criterion, which was used in prior work on ranking (hard) query-specific clusters in the language model framework [29], is a language-model manifestation of this ranking approach; we compare its effectiveness with that of the other ranking methods from Table 1 in Section 4.3.

Some previous cluster-based document-ranking models [19, 17] can be viewed as the conceptual  X  X pposite X  of our Clus-tRanker algorithm as they use clusters as proxies for ranking documents. However, these models use only query-similarity information while ClustRanker integrates such information with centrality information. In fact, we show in Section 4.3 that centrality information is more effective than query-similarity (generation) information for ranking query-specific clusters, and that their integration yields better performance than that of using each alone.

Recently, researchers have identified some properties of query-specific clusters that contain a high percentage of rel-evant documents [31, 18]; among which are the cluster-query similarity (ClustQueryGen) [31], the query similarity of the cluster X  X  constituent documents (DocQueryGen) [31, 18], and the differences between the two [31]. These properties were utilized for automatically deciding whether to employ cluster-based or document-based retrieval in response to a query [31], and for ranking query-specific clusters [18]. The latter approach [18] relies on rankings induced by clusters X  models over the entire corpus, in contrast to our approach that focuses on the  X  X ontext X  within the initially retrieved list. However, our centrality-based criteria can potentially be incorporated in this cluste r-ranking framework [18].
Some recent work on ranking q uery-specific clusters re-sembles ours in that it utilizes cluster-centrality information [21]; in contrast to our approach, centrality is induced based on cluster-document similarities. We further discuss this approach and compare it to ours in Section 4.3.
We next evaluate the effectiveness (or lack thereof) of our cluster-ranking methods in detecting query-specific clusters that contain a high percentage of relevant documents.
For language model induction, we treat documents and queries as term sequences. While there are several possible approaches to represent clusters [27, 31], our focus here is on the underlying principles of our ranking framework. There-fore, we adopt an approach commonly used in previous work on cluster-based retrieval [19, 29, 21], and represent a clus-ter by the term sequence that results from concatenating its constituent documents. (The order of concatenation has no effect since we only define unigram language models that assume term independence.)
We use p Dir [  X  ] x (  X  ) to denote the unigram Dirichlet-smoothed language model induced from term sequence x (  X  is the smoothing parameter) [46]. To avoid underflow issues when assigning language-model probabilities to long texts (as is the case for p d ( c )) [24, 19, 20], we adopt the following mea-sure [19, 20, 21]: where x and y are term sequences, and D is the Kullback-Leibler (KL) divergence. This estimate was mathematically shown to compensate for length issues [22, 20] and empiri-cally demonstrated to be effective in settings wherein long texts are assigned with language-model probabilities [19, 20, 21].

Although the estimate just described does not constitute a probability distribution  X  as is the case for unigram lan-guage models  X  some previous work demonstrates the mer-its in using it as is without further normalization [20, 21].
We conducted our experiments on the following TREC corpora: These data sets were used in some previous work on rank-ing query-specific clusters [21] with which we compare our methods. We used the titles of TREC topics for queries. We applied tokenization and Porter stemming via the Lemur toolkit (www.lemurproject.org), which was also used for lan-guage model induction.

We set D init , the list upon which clustering is performed, to be the 50 highest-ranked documents by an initial rank-ing induced over the entire corpus using p Dir [  X  ] d ( q )  X  i.e., a standard language-model approach 5 .

To produce the set C l ( D init ) of query-specific clusters, we use a simple nearest-neighbor clustering approach that is known to yield (some) clusters that contain a high per-centage of relevant documents [17, 30], and, more generally, was shown to be effective for cluster-based retrieval [13, 19, 17, 31, 30]. Specifically, given d  X  X  init we define a cluster that contains d and the k  X  1documents d i  X  X  init ( d i = d ) that yield the highest language-model similarity p d i ( d ). (We breaktiesbydocumentIDs.)
We posed our cluster-ranking methods as means for in-creasing precision at the very top ranks of the returned doc-ument list. Thus, we evaluate a cluster-ranking method by the percentage of relevant documents in the most highly ranked cluster. Specifically, we use p@k to denote the per-centage of relevant documents in a cluster of size k (either
To create an initial ranking of a reasonable  X  X uality X , we set the smoothing parameter (  X  ) to a value that results in optimized MAP (calculated at a 1000 cutoff) performance. This also facilitates the comparison with some previous work on cluster-ranking [21], which employs the same approach for creating an initial list of 50 documents to be clustered. 5 or 10), because it is exactly the precision of the top k documents that is obtained if the cluster X  X  ( k )constituent documents are positioned at the top ranks of the results 6
We optimize p@k performance for clusters of size k by selecting the value of  X  , the interpolation parameter in the ClustRanker algorithm, from { 0 , 0 . 1 ,..., 1 } , and the values of the (two) parameters controlling the graph-construction methods (for inducing the document and cluster biases) from previously-suggested ranges [20]. (See Appendix A for fur-ther details on graph construction.) The value of  X  ,the document language model smoothing parameter, is set to 2000 following previous recommendations [46], except for estimating p d ( q ) where we use the value chosen for creating D init so as to maintain consistency with the initial ranking.
Finally, we note that the computational overhead of our approach on top of the initial search is not significant. Case in point, clustering of top-retrieved documents (50 in our case) can be performed quickly [45] (our framework is not committed to a specific clustering approach), and comput-ing PageRank scores over a graph of 50 documents (clusters) to induce document (cluster) centrality takes only a few it-erations of the Power method [12].
In what follows, we determine statistically significant dif-ferences of p@k performance by using Wilcoxon X  X  two-sided test at a confidence level of 95%.
 tion we are interested in is the effectiveness (or lack thereof) of the ClustRanker algorithm in comparison to that of the initial document ranking from which D init is derived. Recall that ClustRanker ranks clusters of documents from D init , and is evaluated by the percentage of relevant documents in the cluster most highly ranked. As we can see in Table 2, ClustRanker posts performance that is (substantially) better than that of the initial ranking in all relevant comparisons (corpus  X  evaluation measure).

ClustRanker also tends to outperform a document-based language model approach, which ranks all documents in the corpus by p d ( q ) with the smoothing parameter  X  set to optimize precision at top ranks. Case in point, the p@5 performance of such a p@5-optimized document-based re-trieval approach is 46 . 5, 51 . 2, and 56 . 0 for AP, TREC8 and WSJ, respectively. The p@10 performance of such a p@10-optimized approach is 43 . 9, 46 . 4, and 49 . 4, respectively. ClustRanker algorithm looks for relevant documents in D init by exploiting clustering information. Pseudo-feedback-based query expansion approaches, on the other hand, define a query model based on D init and use it for (re-)ranking the entire corpus [4, 44]. To contrast the two paradigms, we use the relevance model RM3 [25, 1, 10]  X  a state-of-the-art pseudo-feedback-based query-expansion approach. We set
An alternative previously-proposed evaluation approach is basedonconvertingarankingover all clusters in C l ( D init toarankingover all documents in D init [27, 29, 21, 31]. However, the  X  X uality X  of the resultant document ranking heavily depends on the induced intra-cluster document or-dering [27, 21] and on the overlap between clusters [21], and hence does not enable a  X  X lean X  evaluation of the percentage of relevant documents within clusters.
 Table 2: Comparison of ClustRanker with the ini-tial document ranking. Boldface: best result in a column;  X * X  marks statistically significant differences with the initial ranking. the values of the (three) free parameters of RM3 so as to independently optimize p@5 and p@10 performance. (See Appendix B for more details.) Table 3: Comparison of ClustRanker with a rele-vance model (RM3) [25, 1]. Boldface: best result in a column.

We can see in Table 3 that ClustRanker outperforms RM3 on AP and TREC8 and underperforms it on WSJ. (The performance differences, however, are not statistically sig-nificant.) These results are gratifying: we have shown that a method (ClustRanker) based on retrieving a cluster in its entirety can outperform both s tandard document-based re-trieval (see the above), and can favorably compete with a state-of-the-art pseudo-feedback-based retrieval approach. performance of the various cluster-ranking criteria (meth-ods) that ClustRanker integrates so as to study their rel-ative contribution to its overall effectiveness. (Refer back to Table 1 for specification of the different methods.) The performance numbers are presented in Table 4.

Our first observation based on Table 4 is that using either of the two types of information (i.e., centrality and query-similarity (generation)), or both, for the cluster X  X  constituent documents, yields in most relevant comparisons (corpora  X  evaluation metric) superior performance to that of us-ing the same type(s) of information for the cluster as a whole. (Compare DocCent with ClustCent, DocQueryGen with ClustQueryGen, and DocCent  X  DocQueryGen with ClustCent  X  ClustQueryGen.) Nevertheless, using informa-tion induced both from the cluster as a whole and from its constituent documents yields performance that in many of the relevant comparisons transcends that of using informa-tion induced from either  X  e.g., compare ClustCent and DocCent with ClustCent  X  DocCent. These findings attest to the importance of integrating information induced from the cluster with that induced from its proxy documents  X  the idea behind our framework.

In comparing the two types of information used for rank-ing, that is, centrality and query-similarity (generation), we see that the former yields in most relevant comparisons su-perior performance to that of the latter. (Compare Clust-Cent with ClustQueryGen, DocCent with DocQueryGen, and ClustCent  X  DocCent with ClustQueryGen  X  DocQuery-Gen.) Nevertheless, combining both types of information
AP TREC8 WSJ 38 . 8  X  39 . 6  X  40 . 6  X  44 . 0  X  37 . 0  X  48 . 8 52 . 0 48 . 8 55 . 6 50 . 6 50 . 6  X  54 . 8 49 . 0 56 . 0 51 . 2 48 . 8  X  54 . 8 49 . 8 56 . 0 51 . 4 50 . 6  X  57 . 6 50 . 6 56 . 0 51 . 2 can improve performance over that of using each alone, as is the case for DocCent  X  DocQueryGen with respect to DocCent and DocQueryGen.

It is not a surprise, then, that the ClustRanker algorithm, which integrates centrality information and query-similarity (generation) information that are induced from both the cluster as a whole and from its constituent documents, is (in most relevant comparisons) the most effective cluster-ranking algorithm among those presented in Table 4. to ranking (various types of) c lusters compare a cluster rep-resentation with that of the query [15, 7, 19, 29, 31]. Specifi-cally, in the language model framework, (hard) query-specific clusters were ranked by the probability assigned by their in-duced language models to the query [29, 31]. Note that this is exactly the ClustQueryGen criterion for ranking clusters.
An additional reference comparison that we consider, which yields state-of-the-art performance in detecting clusters that contain a high relevant-document percentage, is a recently-proposed (bipartite-)graph-based approach for ranking query-specific clusters [21]. Specifically, documents in D init vertices on one side, and clusters in C l ( D init ) are vertices on the other side; an edge connects document d with the  X  clusters c i that yield the highest language-model similarity p ( d ), which also serves as a weight-function for the edges. Then, Kleinberg X  X  HITS (hubs and authorities) algorithm [16] is run on the graph, and clusters are ranked by their in-duced authority values. It was shown that the cluster with the highest authority value tends to contain a high percent-age of relevant documents [21]. For implementation, we fol-low the details described in [21]; specifically, we choose the value of  X  from { 2 , 4 , 9 , 19 , 29 , 39 , 49 } so as to optimize p@k performance for clusters of size k .

Table 5 presents the comparison of ClustRanker with the reference comparisons just described (ClustQueryGen and HITS). We can see that ClustRanker outperforms both ref-erence comparisons in all cases. Moreover, many of the per-formance differences are also statistically significant.
The HITS-based algorithm [21] utilizes cluster-centrality information as induced over a cluster-document graph. Our ClustRanker algorithm, on the other hand, integrates cen-trality information (induced over document-solely and cluster-Table 5: Comparison of ClustRanker with the Clust-QueryGen [29] and HITS [21] methods for ranking clusters. Boldface: best performance in a column;  X  X  X  and  X  X  X  mark statistically significant differences with ClustQueryGen and HITS, respectively. solely graphs) with query-similarity (generation) informa-tion. In Table 6 we contrast the resultant performance of using the different notions of centrality utilized by the two algorithms. (We present the performance of our centrality-solely-based methods ClustCent, DocCent, and ClustCent  X  DocCent and of the HITS approach [21].)
We can see in Table 6 that all our centrality-solely-based approaches outperform the HITS-based method [21] in all relevant comparisons. These results attest to the effective utilization of (a specific type of) centrality information by our framework. We hasten to point out, however, that ClustCent and DocCent incorporate two free parameters and ClustCent  X  DocCent incorporates three, while the HITS-based approach incorporates one free parameter.
 Further analysis. The derivation of the ClustRanker algo-rithm is based on truncating the summation in Equation 2 Table 6: Comparison of our centrality-solely ap-proaches for ranking clusters with the HITS-based method [21]. Boldface: best performance in a col-umn;  X  X  X  marks statistically significant differences with the HITS method. Table 7: Performance numbers of ClustRanker when either all documents in D init serve as proxies for clus-ter c (denoted d  X  X  init ), or when only c  X  X  constituent documents serve as its proxies, as in the original im-plementation (denoted d  X  c ). Boldface: best result in a column;  X * X  marks statistically significant differ-ences with the initial ranking. (Section 2) so as to allow only c  X  X  constituent documents to serve as its proxies. We now study a variant of ClustRanker wherein all documents in the initial list D init can serve as c  X  X  proxies:
As can be seen in Table 7, this variant (represented by the row labeled  X  d  X  X  init  X ) posts performance that is almost al-ways better than that of the initial document ranking from which D init is derived. However, the performance is also consistently worse than that of the original implementation of ClustRanker (represented by the row labeled  X  d  X  c  X ) t h a t lets only c  X  X  constituent documents to serve as its proxies; furthermore, the suggested vari ant never posts statistically significant improvements over the initial ranking as opposed to the original implementation of ClustRanker. (The perfor-mance differences between the two variants of ClustRanker, however, are not statistically significant.) Thus, as is men-tioned in Section 2, using only the cluster X  X  constituent doc-uments as its proxies is not only computationally convenient, but also yields performance improvements.
We presented a novel language model approach to ranking query-specific clusters by the presumed percentage of rele-vant documents that they contain. Our model integrates information induced from the cluster as a whole unit with in-formation induced from documents that are associated with the cluster. We demonstrated the superiority of our model to previous cluster-ranking methods in detecting clusters that contain a high percentage of relevant documents. Further-more, we showed that posting the constituent documents of the cluster most-highly ranked by our model at the top of the returned results yields precision-at-top-ranks perfor-mance that is superior to that of the initial document rank-ing upon which clustering is performed; the performance also favorably competes with that of a state-of-the-art pseudo-feedback-based document-retrieval approach.
 Acknowledgments The author thanks the anonymous re-viewers for their helpful comments. The author also thanks Lillian Lee for discussions that led to ideas presented in this paper; specifically, the cluster-centrality induction method is a fruit of joint work with Lillian Lee. This paper is based upon work done in part while the author was at Cornell Uni-versity and upon work supported in part by the National Science Foundati on under gra nt no. IIS-0329064 and by a gift from Google. Any opinions, findings and conclusions or recommendations expressed in this material are the au-thor X  X  and do not necessarily reflect those of the sponsoring institutions.
We briefly describe a previously proposed graph-based ap-proach for inducing document centrality [20], which we use for inducing document and cluster centrality.
 Let S (either D init  X  the initial list of documents, or C l ( D init )  X  the set of their clusters) be a set of items, and G =( S, S  X  S ) be the complete directed graph defined over S .

The weight w t ( s 1  X  s 2 )oftheedge s 1  X  s 2 ( s 1 ,s 2 is defined as where N bhd ( s 1 ;  X  )isthesetof  X  items s  X  S  X  X  s 1 } that yield the highest p s ( s 1 ). (Ties are broken by item ID.)
We use the PageRank approach [3] to smooth the edge-weight function: w t [  X  ] ( s 1  X  s 2 )=  X   X  1 |  X  is a free parameter.

Thus, G with the edge-weight function w t [  X  ] constitutes an ergodic Markov chain, for which a stationary distribution exists. We set Cent ( s ), the centrality value of s ,tothe stationary probability of  X  X isiting X  s .

Following previous work [20], the values of  X  and  X  are cho-respectively, so as to optimize the p@k performance of a given algorithm for clusters of size k .Weusethe same pa-rameter setting for the document-graph ( S = D init )andfor the cluster-graph ( S = C l ( D init )), and therefore inducing document and cluster centrality in any of our methods is based on two free parameters.

To estimate the standard relevance model RM1, which was shown to yield better performance than that of the RM2 relevance model [26], we follow Lavrenko and Croft [26]. Let w denote a term in the vocabulary, { q i } be the set of query terms, and p JM [  X  ] d (  X  ) denote a Jelinek-Mercer smoothed doc-ument language model with smoothing parameter  X  [46]. RM1 is then defined by p
In practice, RM1 is clipped by setting p RM 1 ( w ;  X  )to0for all but the  X  terms with the highest p RM 1 ( w ;  X  )tobegin with [6, 10]; further normalization is performed to yield a probability distribution, which we denote by  X  p RM 1 (  X  To improve performance, RM1 is anchored to the original query [1, 10] to yield the RM3 model: p RM 3 ( w ;  X ,  X ,  X  ) def =  X p MLE q ( w )+(1  X   X  )  X  p RM p q ( w ) is the maximum likelihood estimate of term w with respect to q . Documents in the corpus are then ranked by the KL divergence D
The free-parameter values are chosen from the following ranges to independently optimize p@5 and p@10 perfor-1000 , 5000 ,ALL } ,where X  ALL  X  stands for using all terms in the corpus (i.e., no clipping), and  X   X  X  0 , 0 . 1 , 0 . 2 ,..., 0 . 9 is set to 2000, as in our cluster-based algorithms, following previous recommendations [46].
