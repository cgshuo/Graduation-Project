 The k nearest neighbour ( k NN) algorithm [1,2,6] applies a simple and intuitive rule to make classification decisions: instances close in the input space are likely to belong to the same class. Typically a k NN classifier classifies a query in-stance to the class that appears most frequently among its k nearest neighbours, where k is a parameter tuning the classificat ion performance. In contrast to the maximum-generality bias of most concept learning systems [22], k NN adopts the maximum-specificity bias for classificat ion and does not formu late a generalised conceptual model from the training instances at the training stage.

In many applications, training instan ces for a class form several clusters in the training instance space. With most co ncept learning systems (e.g. the deci-sion tree), the model for classification usually is disjunction of several component subconcepts, where each disjunctive subcon cept describes a cluster of training in-stances called a disjunct [9,23]. Small disjuncts refer to clusters of a small number of instances. The class imbalance problem often presents itself as a small disjunct problem, where the positive minority class comprises small disjuncts [10]. In our discussions we call the minority class the positive class and the majority class the negative class.

Re-sampling and cost-sensitive learning are common strategies to combat class imbalance [22]. Our experiments s how that however, unlike decision trees, re-sampling and cost-sensitive learning do not significantly improve the perfor-mance of k NN for imbalanced classification. This may be partly explained by that k NN makes classification decision by examin ing the local neighbourhood of query instances while re-sampling and cost-sensitive learning are global strategies. Al-though re-sampling can achieve overall even class distribution in the training instance space, it may not have significant effect on the local neighbourhood of every instance. As a result, given a quer y instance, its neighbourhood is likely overwhelmed by instances from the majority class, and as a result the instance is more likely to be classified to the majority class.

To improve the performance of k NN for imbalanced classification, we propose a positive-biased nearest neighbour (PNN) algorithm to prudently formulate positive subconcepts from small disjuncts of positive training instances, so as to increase the sensitivity of k NN to the positive class while not introducing too many false positives. Given a query instance and parameter k , if positive instances are scarce in the local neighbourhood of the query instance, we enlarge the neighbourhood for classification decision. Moreover we estimate the prob-ability that a query instance belongs to the positive class based on comparing the positive frequency in its local neighbourhood with the overall positive fre-quency in the training instance space; intuitively without any prior knowledge of class prior, any query instance has a 50% probability of being positive, and query instances falling into regions with higher positive frequency than the over-all positive frequency in the training space are more likely, i.e., with &gt; 50% probability, to be positive.

Our experiments show that the simple y et effective decision bias of PNN leads to more accurate decision for the minori ty class. PNN often improves ENN [12] and CCW-k NN [13], two recent imbalanced classifiers based on k NN, while sig-nificantly reducing their computation cost. PNN also outperforms re-sampling and cost-sensitive learning strategies, namely SMOTE [5] and MetaCost [7], for imbalanced classification. Our work high lights that learning g eneralised concepts for disjuncts for the rare class is an effective approach to improving the perfor-mance of the nearest neighbour algorithm for imbalanced classification. The nearest neighbour algorithm has b een advocated for imbalanced classifi-cation [9,19,20]. However the standard k NN algorithm experiences difficulty in the presence of imbalanced class distr ibution. Recently ENN [12] and CCW-k NN were proposed to improve k NN for imbalanced classification. However both ENN and CCW-k NN require a training stage either to find exemplar training samples to enlarge the decision boundaries for the positive class, or to learn the class weight for each training sample by mixture modelling and Bayesian network learning. The computation cost can be substantial with both approaches. In this paper we focus on improving the classification strategy of k NN. We apply a simple yet powerful strategy to estimate the positive posterior probability from the class distribution in the neighbourhood of query instances. Our experiments show that our new classification strategy, further improves the classification per-formance of ENN significantly, and show comparable results with CCW-k NN (better but not statistically significant).

There have been research efforts trying to improve the overall classification accuracy of k NN, such as intelligent instance selection and construction [8] and new classification scheme rather than the standard majority vote from k near-est neibhours [21]. There have also been research efforts trying to improve the classification efficiency [1,2,24] of k NN. Various strategies have been proposed to avoid exhaustive search of all training instances in the input space while ensuring accurate classification. But these approaches do not consider the classification strategy for class imbalance.

Several studies [9,19] in literature have tried to choose a more appropriate induction bias for learning algorithms to deal with the imbalanced class distribu-tion problem, but these work are focused o n how to make the gen erality-oriented induction bias of classification systems like the decision tree more specific so as to improve their performance for the rare class.

Re-sampling and cost-sensitive learning are commonly used strategies in lit-erature to combat imbalanced class distribution for classification. No consistent conclusions have been drawn from existin g studies [22] on the effectiveness of re-sampling techniques on imbalanced classification. Assigning higher cost to false negatives than to false positives can make a classification model more sensitive to the rare class. However the specific cost information is not always available in most applications. Our experiments show that such strategies can improve the performance of the C4.5 decision tree for imbalanced learning but do not work on k NN. This may be partly explained by the maximum-specificity induction bias of k NN  X  classification decision is made by examining the local neighbourhood of query instances, and therefore the global re-sampling and cost-adjustment strategies may not have pronounced effect in the local neighbourhood under examination. The ideal neighbourhood for classifying a query instance should be local to the query instance as well as general enough to form generalised concept for classi-fication. A too large neighbourhood may over-generalise the subconcept for the positive class and introduce false positives whereas a too small neighbourhood may form a very restrictive subconcept a nd miss positive subconcepts. Our main idea to adjust the neighbourhood for making classification decisions is based on the concept of Positive Nearest Neighbour (PNN) region of query instances. An m -PNN region of a query instance t is such that it contains m positive nearest neighbours of t .The m -PNN region for query instances means a varying number of nearest neighbours for making classification decisions  X  a small region for a subspace densely populated with positives, and a large region otherwise. 3.1 Positive Nearest Neighbours With standard k NN classification algorithm, the k -nearest neighbour ( k -NN) region for a query instance t may not contain any positive instances, espe-cially when positive instances are scar ce; the majority vote rule will compute P ( C + | t ) &lt;&lt; 0 . 5 and thus classify t to the negative class. Given query instance t and parameter k , to increase classification sensitivity to the positive class, we adjust the neighbourhood for classifying t so that it contains k/ 2 positive near-est neighbours of t  X  X amelythe k/ 2 -PNN region of t . The total number of neighbours r in the k/ 2 -PNN region of t may be different from k : if the local region of t is densely populated with positive training instances, r is likely to be smaller than k .Otherwise r is likely to be larger than k .

Fig. 1 shows that for k = 5 (and therefore k/ 2 =3)andagivenquery instance t (denoted as *), the different cases for the 3-positive nearest neighbour (3-PNN) region, in comparison to the corresponding 5-nearest neighbour (5-NN) region. The diagrams are in one dimension so that it is easier to explain the distance between instances and size o f different regions. Depending on the distribution of positive instances in the neighbourhood of t , the total number of neighbours r in the 3-PNN region of the query instance may be larger or smaller than k (= 5).  X  Fig. 1(a) shows the most commonly occurring situation where positive in- X  Fig. 1(b) and Fig. 1(c) show the case when the k/ 2 -PNN region of t  X  Fig. 1(d) shows the case when the k -NN region is the same as the k/ 2 -3.2 Estimating the Positive Posterior Probability The region centered at a positive training instance likely forms a positive sub-concept that is a component for the overa ll disjunctive positive concept. Ideally these positive subconcepts collectively should expand the decision boundary for the positive class. Roughly speaking the error rate for a positive subconcept region is the frequency of negative training instances in the region. However, the observed negative frequency is not a ccurate description of its error rate in independent tests. We estimate the false positive error rate by re-adjusting the observed negative frequency using pessimistic estimate [17,25]. The errors in a region follow the binomial distribution B ( N,q ), where N is the total number of training instances in a region, and q is the true false positive error rate in the region. For a given confidence level c , the false positive error rate for a region can be estimated from the observed negative frequency f in the region as follows [25]: where z is the z -score corresponding to a given confidence level, where for c = 10% z =1 . 28.

The false positive rate estimated from Equation 1 is always higher than the observed false positive frequency. A higher confidence level means the estimated false positive error rate closer to the observed negative frequency in the training instance space. If the estimated false positive rate for a region is less than that estimated from the global negative class frequency in the training instance space, the region can form a positive subconcept .

For a query instance t and a given value of k , when making classification decision for t ,the k/ 2 -PNN and the region S ( t,r )for t are evaluated to decide if it can form a positive subconcept, where r is the total number of nearest neighbours for t in the k/ 2 -PNN region. 1 If the k/ 2 region of t forms a is likely negative, or P ( C + | t ) &lt;P ( C  X  | t ).

An important remaining question is how to estimate the positive posterior probability P ( C + | t ). We compute P ( C + | t ) based on the distribution of positives in the neighbourhood of t . Recall that we always have k/ 2 positives in the k/ 2 -PNN region of t . Specifically we compute the positive posterior probability for t according to whether r&gt;k  X  that is whether S ( t,r ) is a larger region than S ( t,k ).  X  If r&gt;k , the neighbourhood of t lacks positive instances, and we need to  X  If r  X  k , S ( t,r ) is a smaller neighbourhood of t and is densely populated with 3.3 The Algorithm We now present our PNN algorithm as shown in Algorithm 1, and for a given k it is denoted k PNN. In the algorithm first error rate threshold  X  is computed using Equation 1 for confidence level c , number of training instances | T | and the prior negative class frequency (line 1). Lines 3 X 11 compute S ( t,r ), the k/ 2 -PNN neighbourhood of t ,where r = p + n . Lines 14 and 15 describe that when r&gt;k the minimal probability &gt; 0 . 5. Otherwise when r&gt;k but S ( t,r ) is not a positive subconcept, or when r&lt;k , P ( C + | t ) is computed based on the positive frequency in the region S ( t,r ).

Line 4 involves a process sorting training instances by distance to the query instance. The sorting algorithm has a complexity of O ( n log n ), where n is the size of the training set. The loop from Line 4 to Line 11 repeats at most n times. Therefore, the time complexity of Algorithm 1 is O ( n log n ). Note the k PNN does not have a separate training stage searching the training space to compute exemplars as in ENN [12] or to learn the class weight for each training sample as in CCW-k NN [13]. So obviously k PNN can save the significant amount of training computation cost involved in both approaches.
 Example 1. Given k =5(andso k/ 2 = 3), we use the examples in Fig. 1 to explain the classification process of PNN. In the figure,  X * X  denotes the query instance, and the size for the 3-PNN n eighbourhood is represented as r =8, 4, 5 respectively. Let p and n denote the number of positive and negative instances in the k -NN neighbourhood of query instance  X * X  S (  X  , 5). Let p and n denote the number of positive and negative instances in the k/ 2 -PNN neighbourhood of  X * X , S (  X  ,r )( r =8,4,5).  X  r&gt;k . Fig. 1(a) gives a scenario where r = 8. In the 3-PNN region of S (  X  , 8),  X  r&lt;k . In Fig. 1(b) and Figure 1(c), r = 4 whereas k = 5. In Fig. 1(b), In Algorithm 1 The k PNN classification algorithm  X  r = k . An example for the last case is shown in Fig. 1(d), where r =5.The We conducted 10-fold cross validation experiments to evaluate the performance of PNN in comparison to ENN, CCW-k NN. We also compare PNN against the SMOTE re-sampling [5] and MetaCost [7] cost-sensitive learning strategies for imbalanced classification. All classifiers were developed based on the WEKA data mining toolkit [25]. With all k NN-based classifiers k =3. The confidence levels for ENN and PNN are set to 10% and 20% respectively. Twelve real-world datasets were used to evaluate the performance of cl assifiers in our experiments, from highly imbalanced (the minority frequency of 4.35%) to moderately imbalanced (the mi-nority frequency of 30.00%). The datasets are summarised in Table 1, ordered in decreasing level of imbalance. The Oil da taset was provided by Robert Holte [11]. Datasets CM1, KC1 and PC1( http://mdp.ivv.nasa.gov/index.html )have been widely used in software engineering r esearch to predict software defects [14]. The other datasets were compiled from the UCI Machine Learning Repository [3] by choosing one class as the positive and combining the remaining classes as the negative. 4.1 Performance Evaluation Using AUC and Convex Hull Analysis We use both Receiver Operating Characte ristic (ROC) curve [18] and Convex Hull analysis to evaluate the performance of classification algorithms. Area Un-der the ROC Curve (AUC) measures the overall classification performance [4], and a perfect classifier has an AUC of 1.0. All results reported next were ob-tained from 10-fold cross validation experiments and two-tailed t-tests at 95% confidence level were used to test the statistical difference between results. The ROC convex hull method provides visual performance analysis of classification algorithms at different levels of sensitivity [15,16].

Table 2 shows the AUC results for all models. CCW-3NN uses the multi-plicative inverse strategy (additive inverse shows similar result). Compared with the remaining models, 3PNN has the highest average AUC of 0.841. 3PNN significantly outperforms 3ENN ( p&lt; 0 . 05) and shows statistically compara-ble ( p&gt; 0 . 05) result with CCW-3NN, despite a higher average AUC. 3PNN, 3ENN and CCW-3NN significantly outperform all of 3NN, 3NNSmt+, 3NN-Meta, C4.5Smt+ and C4.5Meta, This result confirms that our positive con-cept generalization strateg y is very effective for improving the performance of k NN for imbalanced classification, and furthermore the strategy is more effective than re-sampling and cost-sensitive learning strategies. Note also that SMOTE resampling and MetaCost demonstrate improvement on C4.5 (C4.5Smt+ and C4.5Meta vs. C4.5) but they do not demon strate significant improvement on 3NN (3NNSmt+ and 3NNMeta vs. 3NN).
 The New-thyroid dataset has a relatively high level of imbalance of 13.95%. From Table 2, 3PNN and 3ENN have an AUC result of 0.988 and 0.99, while 3NNSmt+ also has a competitive AUC result of 0.972. But as shown in Fig. 2(a), the ROC curves of the three models show very different trends. Notably more points of 3PNN and 3ENN (note their overlapping points on ROC curves) lie on the convex hull at low FP rates ( &lt; 10%). On the other hand more points of 3NNSmt+ lie on the convex hull at high FP rates ( &gt; 50%). It is desirable in many applications to achieve accurate prediction at low false positive rate and so 3PNN and 3ENN are obviously good choices for this purpose. German has a moderate imbalance level of 30%. ROC curves of the four models demonstrate similar trends on German, as shown in Fig. 2(b). Still at low FP rates, more points from 3PNN and 3ENN lie on the ROC convex hull, which again shows that 3PNN and 3ENN are strong models. The convex hull analysis has confirmed again that the positive concept generalisation s trategy is very effect ive for imbalanced classification. 4.2 The Impact of Confidence Level As discussed in Section 3.2, the confidence level affects the decision in PNN of whether to generalise to a positive subconcept. We applied 3PNN to two highly imbalanced datasets (Oil and Glass) and two moderately imbalanced datasets (KC1 and German) with confidence level from 1% to 50%. The AUC results are shown in Fig. 3. For the two datasets with high imbalance (Oil 4.38% and Glass 7.94%) AUC is positively correlated with confidence level. For example on Oil when the confidence level increases from 1% to 50% the AUC decreases from 0.847 to 0.833. However for the two datasets with moderate imbalance (KC1 15.46% and German 30.00%) AUC is inversely correlated with confidence level. On German when confidence level increases from 1% to 50% AUC increases moderately.

The opposite behaviour of AUC in relation to confidence level may be explained by that on highly imbalanced data, to predict more positive instances, it is desir-able to tolerate more negative samples in the training instance space in forming positive subconcepts, which is achieved by setting a low confidence level. Such an aggressive strategy increases the sensitivity of PNN to the positive class. On less imbalanced datasets where there are relat ively sufficient positive instances, a high confidence level is desired to ensure a low error level in positive subconcepts. With the standard k NN classification strategy, the class of a query instance is decided by the majority class among its k nearest neighbours. In the presence of class imbalance, a query instance is often classified to the majority class and as a result many minority class instances are misclassified. Our experiments show that existing popular re-sampling and cost-sensitive learning strategies to combat imbalance can not produce significant improvement on the performance of the k NN algorithm.

We have proposed a Positive-biased Nearest Neighbour (PNN) algorithm to combat imbalanced class distribution at t he classification stage. Generalised pos-itive subconcepts are formulated to improve k NN induction for imbalanced clas-sification, based on adjusting the positive posterior probability estimation via comparing the positive frequency in the local region of a query instance to the overall positive frequency in the training instance space. The size of local regions of query instances (the setting of k ) and confidence level for forming generalised positive subconcepts are parameters for the PNN algorithm. Generally setting these parameters of PNN for optimal perf ormance in different applications re-quires empirical experiments.
 Extensive experiments on real-world imbalanced datasets have shown that PNN significantly improves the performance of k NN for imbalanced classifi-cation. PNN also outperforms popular re-sampling and cost-sensitive learning strategies for the class imbalance prob lem. Compared with recent improvements to the k NN algorithm for imbalanced classification, PNN significantly reduces computation while often impro ving classification accuracy.

Our study highlights that adjusting the induction bias of classification algo-rithms in general and maximum-specificity induction algorithms in particular is a cost-effective strategy to improve classi fication performance for the imbalanced class distribution problem.

