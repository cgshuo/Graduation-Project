 Media Websites often solicit users X  comments on content items such as videos, news stories, blog posts, etc. Com-menting activity increases user engagement with the sites, by both comment writers and readers, and so sites are look-ing for ways to increase the volume of comments. This work develops a recommender system aiming to present users with items  X  news stories, in our case  X  on which they are likely to comment. We combine items X  content with a collaborative-filtering approach (utilizing users X  co-commenting patterns) in a latent factor modeling framework. Building upon pre-vious work, we focus on a continuous, real-time approach to address the problem above.

After an initial training period during which commenting activity of users is observed, the system is tested at each subsequent comment submission event by predicting which story is being commented on by a given user at a given time.
Our results show that we are able to overcome the site X  X  inherent presentation bias and outperform a strong baseline as users X  commenting history grows.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Collaborative filtering, user comments, personalization
Media Web sites are fiercely competing over online users X  attention and engagement. A common feature of many present-day media sites is their attempt to enrich user en-gagement by soliciting and distributing user generated con-tent (UGC), and in particular users X  comments on content. Users are empowered to write comments, are able to rate and share comments, and often may also reply to others X  comments, thereby generating discussion threads. Vibrant Figure 1: Demonstration of a potential application. The application acknowledges a comment submis-sion, and recommends the user other stories to com-ment on. discussions around engaging stories may encompass thou-sands of comments.

This work focuses on news sites, and aims to present to each commenting user a personalized list of stories that may engage the user to the point of commenting on them. Fig-ure 1 depicts such an experience. This is an extreme form of content recommendation, focused on recommending deeply engaging stories that will cause users to actively generate content rather than to merely passively consume it.
In a recently published paper [2], the above problem was tackled in a standard train-test learning framework. The commenting history of users was observed on a training set of stories, and the task was to rank  X  for each user  X  all remaining test stories in descending order of the probabil-ity of the user commenting on them. Using a combination of story vocabulary and collaborative filtering (CF) signals that utilize co-commenting patterns of users, a latent-factor model was shown to have good predictive signal.

This work follows up and extends upon [2]. Our contri-butions are the following.

First, we extend the traditional (batch) train-test analy-sis of [2] with a continuous, dynamic approach that enables personalized recommendation of stories in real time. Af-ter an initial training period, the system is tested at each subsequent comment submission event, by predicting which story is being commented on by a given user at a given time. This temporal consideration allows us to manage the avail-able stories in all stages of their (typically brief) life cycle.
Second, we demonstrate the strong effect of a media site X  X  structure and the ephemeral nature of the news articles on the commenting patterns of its users. Media sites often serve thousands of stories, that constantly vary, of which only a few dozens are prominently featured on the site at any given time (e.g. on its main page). The remaining stories are far less discoverable and are only reachable through deliberate effort-consuming search and browse operations. Therefore, at any moment, only a small subset of stories is heavily clicked, read, and eventually commented on. This form of presentation bias obscures the ideal commenting preferences of users, who end up commenting on the  X  X etter of what X  X  featured X  rather than on the  X  X est of what X  X  available X . As our evaluation is post-factum, we reveal this bias by demon-strating a non-personalized recommendation baseline that achieves strong results. The baseline follows aggregate com-menting trends and predicts, for each commenting instance, stories that have been heavily commented on recently, i.e. the stories that are (presumably) currently featured.
Third, our results show that we are able to overcome the site X  X  inherent presentation bias and outperform the strong baseline above as users X  commenting history grows. As our system learns more about its users, the precision of its pre-dictions improves and it better predicts how users X  comment-ing activity will deviate from the overall commenting trends created by the presentation bias.

Finally, as a contribution to evaluation methodology, we develop a framework for prediction that simulates an on-line scenario using off-line data.

A thorough review of prior art on comments analysis and the algorithmic techniques used in this paper appears in [2]. Also related to the current work is a paper attempting to predict the popularity of news articles according to content characteristics such as source, language and context [1].
The goal of this work is to build a dynamic (real-time) personalized story recommendation system for commenting. The system aims to recommend to a user, who is currently active on the site, stories that she is likely to comment on. The notion of the current time is therefore critical -what-ever happened in the past is known, as opposed to whatever happens next. This is significant, as comments X  dynamics are constantly changing. Stories that are highly trending now may be far less relevant in an hour. Time will play a major role in our experiments, which simulate real-time recommendation using only historical off-line data.
In this work we used data that was extracted by a crawler from the Newsvine website 1 during the period between May 18, 2011 till September 27, 2011 (same as in [2]). The data includes users ( u i ), stories ( s j ), and comments ( c ments are represented by tuples (user, story, creation time), i.e. c k = c u k ,c s k ,c t k . Stories are represented by their title, content, and editorial tags (when available). For simplicity, we filtered out users that wrote only one comment during the whole period, together with their corresponding com-ments. After filtering, our data encompasses 68 , 649 stories with 1 , 036 , 657 comments, written by 61 , 663 users. www.newsvine.com Figure 2: Flow of information between the controller and the predictor in a single step of the simulation.
In order to better illustrate the way we exploit off-line data for simulating an on-line scenario, let us assume the exis-tence of a  X  X ontroller X  object that is aware of all data in the system, and manages the simulation. Let { c 1 ,c 2 ,...,c be the sequence of all comments in the data, sorted by their creation time, and let w be the number of training com-ments. The controller provides the predictor with the se-quence C train = { c 1 , c 2 ,..., c w } for training, together with the story data for the corresponding stories c s j w j =1 tent, titles, tags). The simulation phase iterates over the re-maining comments, C test = { c w + 1 , c w + 2 ,..., c N } simulating on a comment c k , let us define the available sto-ries ( AS ( k )) as the set of all stories seen so far ( c controller provides the predictor with the user id and the current time c u k ,c t k , and the predictor outputs a ranked list of AS ( k ). The controller then computes the prediction performance by calculating the rank ( r ( k )) of the true com-mented story ( c s k ), defined as the number of available stories ranked higher than c s k , which were not commented by c u the entire data. Finally, the controller reveals c s k ,withits available data, for possible usage in subsequent predictions. Figure 2 illustrates a single simulation step. All computed ranks are stored for final evaluation of the predictor.
Given the ranks computed for the comment set C test ,the predictor performance are evaluated using two metrics,
According to the problem setting above, when given a user id and time, we know this user is logged into the system, and available for commenting. It is reasonable, therefore, to inspect recent trends of comments in order to predict the story that the user is about to comment on.
The baseline predictor we applied is based on recent com-menting trends, which we denote as the Trending Score (TS). For each available story s in time t , the current rate of its last comments is computed by Figure 3: (a) Recall@K results for several values of , using the baseline predictor. (b) Recall@K results for =15 , for users with different history lengths. where t s ( ) is the creation time of the th last comment of s . Interpolation is used for computing TS for stories with less than comments. The normalized version of the trending score, 0  X  NTS  X  1 is defined by where D is a fixed large enough comment rate that reflects  X  X ld X  stories (e.g. 3 days per comment). In each prediction step (for any user), the baseline predictor ranks the available stories according to a descending order of their NTS values.
Figure 3(a) presents recall@K results for several values of . Asexpected,forlowvaluesof the results are worse, as every comment written by some user has a strong effect on the story NTS . For higher values of , recall results are better and more stable. While results for = { 10 , 15 , 20 nearly coincide, the value of = 15 exhibited the best re-sults, and we adopt it for the rest of this paper. Due to the strong presentation bias of the site, the baseline predic-tor is quite accurate. It ranks the true commented story in the top 3 places ( Recall @3) in more than 25% of cases, providing a robust baseline that seems difficult to beat by personalization. Trying to learn some more about the users, we analyzed the recall results for commenters with different amounts of commenting history. In Figure 3(b) we show the recall results for the baseline predictor, for users with differ-ent history lengths (note the same user may contribute to all graphs if she commented 20 or more times in our data). We see that  X  X eavy commenters X  are harder to predict using the trending score baseline. Yet, when trying to develop pre-diction based on personalization, longer history becomes an asset. The following section describes our recommendation scheme, that models deviations from the NTS score based on prior actions of users.
We adopt a latent factor method similar to the one pre-sented in [2]. Our system learns a latent factor of dimension d for each user, tag and co-commenter, where as tags we consider both editorial tags and words from the story X  X  title and content in their base format (after stemming). We de-viate from [2] in that we do not expect our model to explain all user-story links, but rather to explain the deviations from the dynamic trending score. That is, there is no need to explain the fact that users comment on trending stories, which is to be expected. However, when a user comments on a non-trending story, our model should have evidence of a strong link between this user and the story. Moreover, if a user does not comment on a trending story, we would expect our algorithm to show a negative link between this user and the story, as the user was likely exposed to this story while commenting, but decided to comment on a different story.
Each person in the system is represented by two latent factor vectors, corresponding to the two roles each person plays: u i  X  R d as a user, and m i  X  R d as a co-commenter. Each story s i is represented by a combination of its com-menters and content. Denoting by t i  X  R d a latent factor vector representing a content element (tag) t i , the story rep-resentation for story s i at time t is defined by s i ( t )= 1 | where T i is the set of tags of s i ,and M i ( t )isthesetofall commenters of s i until time t . Note that the story represen-tation is enriched as more users comment on it. We define a user-story score at time t as the sum of the story X  X  nor-malized trending score and the inner product between the user X  X  latent factor vector and the story X  X  representation: The left additive in Eq. 4 is responsible for the temporal dy-namics. Old or non-popular stories will receive a low score even when they match a user well. This part is constantly changing as time passes. The right additive is the personal-ization part. It may have both positive and negative values, in different magnitudes, and represents the personalized de-viation from the trending score. A large positive value re-flects a good match between a user and a story, and therefore even if the story is not very trending, the user is likely to comment on it. Conversely, a negative value represents a bad match between a user and story, so even if the story is trending, this user is unlikely to comment on it.
In the training stage we train all latent factor vectors in order to minimize a cost function. The cost function for minimization is an aggregation of a loss function, computed per all user-story scores over the training period: For each commented story ( c s k ), we also sample one nega-tive story s  X  ( c u k ) that indicates a bad match for the user ( c did not comment on it during the whole training period). This loss function, presented in [3], tries to generate a gap between the r u i ,s j ,t values of the positive and negative in-dications by maximizing their distance. Sampling was done proportionally to g ( NTS ), where g is a Gaussian function with mean value of 1, and std  X  . When sampling stories with high NTS values, we can more comfortably assume these stories were visible to the user prior to commenting, yet the user decided to not comment on them. We train our model by employing stochastic gradient descent with early stopping. Each iteration enumerates all training comments, updating both the trending score for all stories and the var-ious latent factor vectors. baseline X  X , for users with different history lengths.
Every user-story score changes during simulation because of two reasons. First, the trending score is constantly chang-ing. Second, when comments are added to a story, its repre-sentation changes, also affecting the user-story score. How-ever, the learning process described so far does not change the latent factor vectors of the user. Such an update is ad-vocated since as time passes, more user actions are observed and can be exploited for prediction, in particular in cases where user preferences change over time. We therefore re-train the system periodically, as described in Section 5.3.
In the following, we set the training set size to w =0 . 5 the latent space dimension d = 10, the standard deviation for sampling  X  =0 . 02, and learning rate  X  =0 . 1.
Figure 4(a) shows the mrr as a function of the number of past comments of a story. We compare 3 modes of the algorithm -using only tags for story representation, only commenters, and using both. We observe that tags alone im-prove upon the baseline predictor. However, co-commenting data is even more effective, and results are best when com-bining the two. Figure 4(b) presents the relative improve-ment (in %) of each latent factor model compared to the baseline predictor. We see that the additional signals signifi-cantly improve over the non-personalized baseline, especially for stories that have yet to accumulate many comments.
Figure 4(c) shows recall @ K results for users with differ-ent history lengths. The plots present the percentage im-provement of our algorithm (tags + commenters) compared to the baseline results. We see that the baseline predictor deteriorates for users with longer history (see section 3.2). However, our algorithm is able to overcome this deteriora-tion, and achieves greater improvement for these users.
Finally we show how performance changes when applying the continuous learning procedure. Here we initially trained on 20% of our data, and re-trained after 60 days (shorter time periods did not show noticeable improvement in per-formance). Figure 5 presents mrr results per each 4 days of simulation for the three methods: TS (baseline), tags + commenters, and tags+commenters with additinal learning after 60 days. The latter method coincides with the sec-ond one for the first simulation period, and outperforms it afterwards once additional learning is performed. Figure 5: mrr per each 4 days of simulation. Note the effect of the additional learning on day 60.
This work presented a dynamic, real-time latent factor modeling approach for prediction of stories that users are likely to comment on. In addition to story content and user co-commenting patterns, we tap the temporal dynamics of commenting activity that emerge from the presentation bias of media sites. Our method models the personalized devia-tion of each user from the aggregate commenting behavior, and exhibits good results in a simulation of the real-time scenario that uses off-line data. We examined how person-alization improves recommendation for users with varying lengths of commenting history and demonstrated how it can be further improved by re-training of the model over time. We plan to further tune the personalization factor in an on-line experiment of the proposed user experience. [1] R. Bandari, S. Asur, and B. A. Huberman. The pulse of [2] E. Shmueli, A. Kagian, Y. Koren, and R. Lempel. Care [3] J. Weston, S. Bengio, and N. Usunier. Large scale
