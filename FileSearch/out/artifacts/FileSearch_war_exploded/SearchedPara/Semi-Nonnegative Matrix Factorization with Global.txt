 Collaborative Filtering , considered by many researchers as the most important technique for information filtering, has been extensively studied by both academic and industrial communities. One of the most popular approaches to col-laborative filtering recommendation algorithms is based on low-dimensional factor models. The assumption behind such models is that a user X  X  preferences can be modeled by lin-early combining item factor vectors using user-specific coe f-ficients. In this paper, aiming at several aspects ignored by previous work, we propose a semi-nonnegative matrix fac-torization method with global statistical consistency. Th e major contribution of our work is twofold: (1) We endow a new understanding on the generation or latent compositions of the user-item rating matrix. Under the new interpreta-tion, our work can be formulated as the semi-nonnegative matrix factorization problem. (2) Moreover, we propose a novel method of imposing the consistency between the statis -tics given by the predicted values and the statistics given b y the data. We further develop an optimization algorithm to determine the model complexity automatically. The com-plexity of our method is linear with the number of the ob-served ratings, hence it is scalable to very large datasets. Finally, comparing with other state-of-the-art methods, t he experimental analysis on the EachMovie dataset illustrate s the effectiveness of our approach.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; G.1.6 [ Numerical Analysis ]: Optimization Algorithms, Experimentation Recommender Systems, Collaborative Filtering, Matrix Fac -torization, Optimization
Recommender Systems attempt to suggest items (movies, books, music, news, Web pages, images, etc.) that are likely to interest the users. Typically, recommender systems are based on Collaborative Filtering , which refers to the tech-nique for the task of predicting preferences of users by col-lecting taste information from many other users. The un-derlying assumption of collaborative filtering is that the a c-tive user will prefer those items which the similar users pre -fer [16]. Due to the potential commercial values and the great research challenges, Collaborative Filtering techniques have drawn much attention in both information retrieval [16 , 25, 35, 36] and machine learning [19, 21, 23, 24, 37] com-munities. Collaborative filtering algorithms suggesting p er-sonalized recommendations greatly increase the likelihoo ds of customers making the purchases online. Hence, the de-veloped recommendation applications have been widely de-ployed in several large and famous commercial Web sites, such as Amazon 1 , Ebay 2 , Netflix 3 , Apple 4 , etc.
A number of algorithms have been proposed to improve both the recommendation quality and the scalability prob-lems. These collaborative filtering algorithms can be divid ed into two main categories: neighborhood-based and model-based approaches [2, 25]. Different methods are based on different assumptions. The neighborhood-based recommen-dation algorithms are based on the assumption that those who agreed in the past tend to agree again in the future. They usually fall into two classes: user-based approaches [ 2, 6] and item-based approaches [5, 25]. To predict a rating for an item from a user, user-based methods find other similar users and leverage their ratings to the item for prediction, while item-based methods use the ratings to other similar items from the user instead [4]. Despite their success in the industry, neighborhood-based methods suffer from both the data sparsity and the scalability problems. In addition to the neighborhood-based approach, the model-based ap-proaches use the observed user-item ratings to train a pre-defined model. Algorithms in this category include Bayesian model [35], aspect model [9], etc.

Recently, due to its efficiency in handling very large dataset s, low-dimensional factor models have become one of the most popular approaches in the model-based collaborative filter -ing algorithms. The premise behind a low-dimensional fac-http://www.amazon.com http://www.half.ebay.com http://www.netflix.com http://www.apple.com tor model is that there is only a small number of factors in-fluencing the preferences, and that a user X  X  preference vect or is determined by how each factor applies to that user [21]. Most recently, some assumptions are developed to enhance the factor models. For examples, in [31], a matrix factoriza -tion method is proposed to constrain the norms of U and V instead of their dimensionality; a probabilistic linear mo del with Gaussian observation noise is proposed in [24]; and Gaussian-Wishart priors are placed on the user and item hyperparameters in [23]. These models achieve promising prediction results.

Although these methods can effectively predict missing values, several disadvantages are unveiled, which will pot en-tially decrease the prediction accuracy. First, in low-ran k factor-based approaches, both item factor vectors and user -specific coefficients are understood as latent factors which have no physical meanings, and hence uninterpretable. More -over, the lack of interpretability will result in the improp er modeling of the latent factors. For example, these latent factors in [23, 24] are set to be in the Euclidean space, while they are nonnegative in [34]. Second, due to the sparsity of the user-item rating matrix (the density of available rat -ings in commercial recommender systems is often less than 1% [25]), many matrix factorization methods fail to provide accurate recommendations. In the sparse user-item rating matrix, the ratings for training the user features are rare, hence the learned user features and the coefficients cannot accurately reflect the taste of users, which will result in th e bad prediction accuracy.

In this paper, aiming at providing solutions for the issues analyzed above, we propose a Semi-Nonnegative Matrix Fac-torization with Global Statistical Consistency (SNGSC) ap -proach for collaborative filtering. First, we endow a new un-derstanding on the latent compositions of the ratings, whic h is based on the following assumptions: (1) there are totally a number of d types of items; (2) on each type of items, every user has a confidence value indicating the taste of this user on the type; (3) each item also has a quality value on each type. Based on these assumptions, we formulate the collab-orative filtering algorithm as the Semi-Nonnegative Matrix Factorization problem, and propose an optimization formu-lation with sensitive analysis. Second, based on the observ a-tion that the statistics of the predicted ratings are not con -sistent with the statistics of the training data, we propose to impose the consistency between them. This considera-tion generates very good performance when the dataset is spare. Furthermore, we develop an algorithm to determine the model complexity automatically. The complexity of our method is linear with the number of the observed ratings, which can be applied to very large datasets. Finally, com-paring with other state-of-the-art methods, the experimen tal analysis on the EachMovie dataset shows the effectiveness of our approach.

The rest of this paper is organized as follows. We interpret the physical meaning to latent factors in Section 2.2, condu ct the sensitivity analysis in Section 2.3, and formulate the o p-timization problem in Section 2.4. In Section 2.5, we propos e a method that determines the dimensionality automatically . In Section 3, we present an approach of imposing the consis-tency between the statistics given by predicted values and the statistics given by the observed data. The experimental results on EachMovie dataset are shown in Section 4. The related work is introduced in Section 5. Finally, we draw the conclusions in Section 6.
Without loss of generality, in this paper, we use the movie recommender systems as the example. In a collaborative prediction movie recommendation system, the inputs to the system are user ratings on the movies the users have already seen. Prediction of user preferences on the movies they have not yet seen are then based on patterns in the partially ob-served rating matrix X  X  R n  X  m + , where n is the number of users, and m is the number of movies. The value X ij in-dicates the score of item j rated by user i . This approach contrasts with feature-based approach where predictions a re made based on features of the movies (e.g. genre, year, ac-tors, external reviews) and the users (e.g. age, gender, ex-plicitly specified preferences, social trust networks [17, 18]). Users  X  X ollaborate X  by sharing their ratings instead of rel y-ing on external information [21].

Table 1 and Table 2 are the toy examples on the problem we study. As illustrated in Table 1, each user (from u 1 to u rated some items (from i 1 to i 8 ) on a 5-point integer scale to express the extent of favor of each item. The problem we study in this paper is how to predict the missing values of the user-item matrix effectively and efficiently. Usually, as introduced in Section 1, the density of available ratings in commercial recommender systems ( X ) is often less than 1% [25].
The n  X  m matrix X contains the ratings of users on items. X is generated by the users who rate the movies according to their overall feeling about the movies that the y have seen. By anatomizing their overall feeling, we give a detailed analysis on the rating process as follows.
Each user has a different taste on different type of genre, actors, or something else. But with the only given rating matrix, the information for genre or actors is unknown, so we assume there are d different unknown types of objects, which are named as latent types. We further assume that user i has confidence U ik ( U ik  X  R + ) on k -th type, and U is also the taste of user i in ranking objects of type k ; on the other hand, on k -th type, each item j has a  X  X rue X  quality value V jk ( V jk  X  R ). So to user i , item j should be rated by user i as U ik  X  V jk . As a result, on k -th type, if both the quality of object j and the taste of user i are high, then user i will rate object j with a high score.
 These d latent types may have cross-effects on each other. For example, War type movies may also belong to classic Hollywood sub-category. Considering the cross-effects, we assume a symmetric non-negative matrix  X  d  X  d , in which  X  kl =  X  lk denotes the cross-effect between type k and l , and  X  kk =  X  k . Ideally, we hope that the d latent types are independent, and their significance can be ordered, i.e. , nonnegative significance values  X  1  X   X  2  X  . . .  X   X  d can be assigned to the d latent types.

Consequently, on type k , user i rates item j with a score where the quality V jl of item j on type l is transferred to quality V jl  X   X  kl by  X  kl . Note that, if  X  d  X  d is diagonal, then it becomes different unknown types, we obtain that where U k is the vector consisting of U ik , V k is the vec-tor consisting of V jk , and U = ( U 1 , U 2 , . . . , U d ( V 1 , V 2 , . . . , V d ). We consider factorizations of the form X  X  U  X  V T , where U  X  R n  X  d + ,  X   X  R d  X  d + , and V  X  R Remark . According to the physical meaning of U and V , U is nonnegative while V should be unrestricted. For example, a movie may be very bad so that everyone dislikes it, and hence the quality of this movie can be scored as  X  1. The confidence is the ability of a user to rate a movie, and so should not be negative. To explain it further, if the confidence of a user is also set as  X  1, then the product of -1 and -1 will be 1, which means that a user with low confidence rates a bad movie with a high score, which is not true in reality. On the contrary, the setting U i  X  R avoids such unreasonable cases, leading to the advantage of the interpretability of U . We find U ,  X , and V so that P = U  X  V T approximates X well. But it is not preferable that small changes (due to computing errors or error propagated from observation errors in X ) in these three matrices result in a big change in their product. Since the derivatives with respect to the variables U ,  X , and V mean the change rate, we examine the square sum of the corresponding derivatives. Let the notation || X || F denote the Frobenius norm.
 Similarly we have
Considering both the approximation X  X  U  X  V T and the sensitivity analysis, a factorization problem can be cast a s an optimization problem. where  X  is a hyperparameter that controls the balance be-tween the approximation and the sensitivity, and OI denote the set of observed index pairs. Let U i  X  X  and V i  X  X  be the columns of U and V respectively. Without loss of generality, we set || U k || F = 1 , || V for 1  X  k  X  d . As a result, || U || 2 F = d, || V || 2 F purpose of simplifying the solution, we further assume that  X  d  X  d is diagonal, i.e.,  X  d  X  d = diag(  X  1 ,  X  2 , . . . ,  X  quently, and In order to simplify the notation, we denote U  X  as U , then  X  disappears, and the conditions  X  1  X   X  2  X  . . .  X   X  d can be changed to || U 1 || F  X  || U 2 || F  X  . . .  X  || U d || F above simplification, Eq. (4) can be reformulated as follows .
Given an n  X  m nonnegative matrix X , solve
In order to obtain the most informative latent features and find the dimension d , we fit the incomplete matrix X step by step in such a way that when U k and V k are learned, U j ( j  X  k  X  1) and V j ( j  X  k  X  1) are fixed, and we only learn U k and V k based on the residual R . R is defined as on OI , and R = 0 on others for convenience. The process continues until there is no useful information retained in R . When the process stops, the dimension can be determined. So we only focus on the following problem: Note that the elements in R may be negative. If we ignore the variant  X  k , the Lagrangian of the above problem is where Y  X  R n + , and  X  k  X  R + . Let the i -th element of U the j -th element of V k , and the i -th element of Y be U V kj and Y i respectively. In order to solve this problem, take derivative on J with respect to U ki and V j . We have If U k is given, then minimizing the quadratic function in Eq. (7), we obtain that where  X  k is a parameter such that || V k || F = 1.
If V k is given, considering the constraints that U k  X  0 and || U where Y i is the minimum positive number such that i.e., and and  X  k is the minimum positive number such that
We name our algorithm as Semi-Nonnegative Matrix Fac-torization with Global Statistical Consistency (SNGSC). I n Algorithm 1, we summarize a learning algorithm by employ-ing Eq. (10) and Eq. (11). The criterion that no useful infor-mation can be mined in R is specified in our experiments as: the difference between the mean residual 1 | OI | in the current dimension d and that in the previous dimen-sion is smaller than 0.0005.
 From the algorithm, we can see the time complexity of SNGSC is linear on the number of ratings, i.e., O( | OI | ), be-cause we only need to calculate the multiplications when the ratings values are not missing. Moreover, with the proper physical meaning in U and V , our algorithm is expected to achieve more accurate results.
 Algorithm 1 : SNGSC Learning Algorithm Input : Incomplete matrix X  X  0
Output : d , { U k } d k =1 , and { V k } d k =1 10: V kj = 11: end for 12: for i = 1 TO n do 14: end for 15: until Converge 16: k = k + 1 17: until No useful information can be mined in R 18: d = k  X  1 Figure 1: An illustration showing the problem of SNGSC and SVD without controlling the global statistics. The means predicted by models are far away from the true means. Until now, we only constrain the expression in Eq. (5) by fitting its values on the user-item pairs with the training data. However, we observe that this partial constraint cannot make the values global statistics such as the first moment and the second moment. The previous low-dimensional factor models share this problem because no action is taken on controlling the global statistics. For example, the mean of ratings in Each-Movie Data is 0.607357 (after scaling to the interval [0,1]) , but the mean given by SVD and SNGSC is far away from the true mean. In Figure 1, we demonstrate this problem.
Based on the above observation, we propose to impose the consistency on SNGSC between the predicted statistics and those given in the data samples. Ideally we should consider moments of all orders and the data priors, but considering the computation cost and the model complexity, we only in-clude the first moment  X  X  X  X he mean of ratings in this paper. The predicted values are given by the predicted mean by the model is where  X  U k and  X  V k are the vector means of U k and V spectively. Let  X  be the parameter balancing the tradeoff of fitting the data and fitting the mean of ratings. Then we should optimize When  X  = 0, no global information is included; when  X  = +  X  , all the predicted values  X  X such that the first moment is perfectly fitted. The best  X  should be in the middle of these two extreme cases. In our experiments, we set  X  = ences. An ordinary calculus can result in similar equations as Eq. (10) and Eq. (11).
In this section, we conduct several experiments to com-pare the recommendation quality of our approach with other state-of-the-art collaborative filtering methods. Our exp eri-ments are intended to address the following questions: 1. How does our approach compare with the published 2. How does the model parameter  X  (the global consis-3. How do the non-negative constraints affect the accu-4. What is the performance comparison on users with
We evaluate our algorithms on the EachMovie dataset 5 , which is commonly used in previous work [19, 21, 37]. The EachMovie dataset contains 74,424 users, 1,648 movies, and 2,811,718 ratings in the scale of zero to five. We map the ratings 0,1,2,3,4 and 5 to the interval [0 , 1] using the linear function t ( x ) = x/ 5.

As to the training data, we employ three settings: 80%, 50% and 20% for training, where 80% means we randomly select 80% ratings as training data to predict the remaining 20% ratings. Selecting 80% as training data is the standard evaluation setting which is widely employed in the previous work. However, in this paper, we are also interested in the settings to include 50% and 20% as training data, since these two settings can be used to examine how well the algorithms are under the sparse data settings. The reported results in all of the experiments in this paper are the average of ten runs of the algorithms on the ten random partitions of the dataset. We use the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) metrics to measure the prediction quality of our proposed approach in comparison with other collaborative filtering methods. MAE is defined as: where r i,j denotes the rating user i gave to item j , b r notes the rating user i gave to item j as predicted by our ap-proach, and N denotes the number of tested ratings. RMSE is defined as: http://www.research.digital.com/SRC/EachMovie/. It is retired by Hewlett-Packard (HP).
We compare our SNGSC approach with other four ap-proaches. 1. User Mean : This is a baseline method which predicts 2. Item Mean : This is a baseline method which predicts 3. MMMF [21, 31]: This method constrains the norms 4. PMF [24]: This method proposes a probabilistic frame-The prediction accuracies evaluated by Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are shown in Table 3. In SNGSC, the parameter  X  is set to be 0.000004, and the parameter  X  is set to be the number of observed ratings. The dimensions for SNGSC are automatically determined at each of the ten runs, and they are between 25 and 30. In order to compare other al-gorithms fairly, we set the dimensions of MMMF and PMF to 30.

From Table 3, we can observe that our algorithm con-sistently performs better than the other methods in all the settings. When we use a sparse dataset (20% as training data), we find that our method generates much better per-formance than MMMF and PMF. However, MMMF and PMF do not address the problem of sparsity, hence they even perform worse than the Item Mean method when us-ing 20% as training data. This demonstrates the advantage of our algorithm in handling the sparsity problem.
In Figure 2 and Figure 3, we also plot the percentages of performance increase of our algorithm against other four methods in terms of RMSE and MAE on the EachMovie dataset, respectively. From these figures, we observe an in-teresting phenomenon: as the sparsity of the data increases , the percentages of performance increase against MMMF and PMF keep increasing. This observation again proves the ad-vantage of our algorithm. On the other hand, we can also notice that as the sparsity increases, although our method still can generates much better recommendation qualities than User Mean and Item Mean methods, the percentages of performance increase against these two methods keep drop-ping. This observation is reasonable because our random testing data generation method does not change the distri-bution of the ratings. Hence, the User Mean and Item Mean algorithms should be relatively stable against the sparsit y problem.

In order to show the usefulness of each key part of SNGSC, we also evaluate our algorithm on its various degraded cases as follows: 1. SNGSC-1: It is the SNGSC algorithm without the 2. SNGSC-2: It is the SNGSC algorithm without the 3. SNGSC-3: It is the SNGSC algorithm with nonnega-
The results on the EachMovie dataset are reported in Ta-ble 4. From the results, we observe that our Semi-Nonnegativ e setting is the best among all these variants, which empiri-cally demonstrates the need of introducing SNMF.

However, the global consistency achieves only a little accu -racy improvement in this experimental setting (See SNGSC-1 and SNGSC). This phenomenon may be caused by the set-ting that majority (80%) of data is chosen as training data. In the extreme case that the rating data is very sparse and each user only rates one movie, then the latent features U and V do not have much meanings, but we can at least pre-dict all the missing ratings as the mean of training data. We believe that the sparser the training data, the better the global consistency approach. To demonstrate the ef-fectiveness of the global consistency approach, we run both SNGSC-1 and SNGSC in a different setting: 20% of the data are chosen for training and 80% for testing. The results are shown in Table 5. From the results, we can see SNGSC
Figure 2: Performance Increase on RMSE (EachMovie) with the global consistency significantly outperforms the o ne without the global consistency (SNGSC-1). In such a set-ting, it is not surprising to see that the difference between SNGSC and SNGSC-2 is small, because the latent feature is not very meaningful and hence the sign setting is not so important; therefor, the global consistency dominates the results.
Recommender systems have been developed to automate the recommendation process [10]. Examples of research pro-totypes of recommender systems are PHOAKS [32], Syskills and Webert [20], Fab [1] and GroupLens [13, 26]. These systems recommend various types of Web resources, online news, movies, among others, to potentially interested par-ties [10]. They are becoming part of the standard e-business technology that can enhance e-commerce sales by convert-ing browsers to buyers, increasing cross-selling, and buil ding customer loyalty [27].

As stated in [10], one of the most commonly-used and suc-cessful recommendation approaches is the collaborative fil -tering approach [7, 22, 28]. In the field of collaborative filt er-ing, two types of methods are widely studied: neighborhood-based approaches and model-based approaches.

The neighborhood-based approaches are well studied and successfully applied to lots of commercial recommender sys -tems [14, 22]. The most analyzed examples of neighborhood-based collaborative filtering include user-based approach es [2, 6, 11] and item-based approaches [5, 14, 25]. User-based ap-proaches predict the ratings of active users based on the ratings of similar users found, and item-based approaches predict the ratings of active users based on the computed information of items similar to those chosen by the active user. User-based and item-based approaches often use the PCC algorithm [22] and the VSS algorithm [2] as the similar-ity computation methods. PCC-based collaborative filter-ing can generally achieve higher performance than the other popular algorithm VSS, since it considers the differences of user rating style. Another set of related work considers how to employ the user-based and item-based approaches together [16]. Ma et al. [16] presented a method to em-ploy both user information and item information to firstly fill some missing values before making predictions for activ e users. As mentioned in Section 1, despite the success in the industry, almost most of the neighborhood-based methods suffer from the data sparsity and scalability problems. Figure 3: Performance Increase on MAE (EachMovie)
In contrast to the neighborhood-based approaches, the model-based approaches to collaborative filtering use the observed user-item ratings to train a compact model that explains the given data, so that ratings could be predicted via the model instead of directly manipulating the origi-nal rating database as the neighborhood-based approaches do [15]. Algorithms in this category include the aspect mod-els [8, 9, 29] and the latent factor model [3]. [12] presented an algorithm for collaborative filtering based on hierarchi -cal clustering, which tried to balance both robustness and accuracy of predictions, especially when few data were avai l-able. [8] proposed an algorithm based on a generalization of probabilistic latent semantic analysis to continuous-val ued response variables.

Recently, due to the efficiency in dealing with large datasets , several low-dimensional matrix approximation methods [21 , 23, 24, 30] have been proposed for collaborative filtering. These methods focus on fitting a factor model to the data, and use it in order to make further predictions.

Low-rank matrix approximations based on minimizing the sum-squared errors can be easily solved using Singular Valu e Decomposition (SVD), and a simple and efficient Expec-tation Maximization (EM) algorithm for solving weighted low-rank approximation is proposed in [30]. In [31], Sre-bro et al. proposed a matrix factorization method to con-strain the norms of U and V instead of their dimensional-ity. Salakhutdinov et al. presented a probabilistic linear model with Gaussian observation noise in [24]. In [23], the Gaussian-Wishart priors are placed on the user and item hyperparameters. Although low-dimensional methods are proved to be very effective and efficient, these methods still suffer several disadvantages that are unveiled. In the SVD method, as well as other well-known methods such as the weighted low-rank approximation method [30], Probabilis-tic Principal Component Analysis (PPCA) [33], Probabilis-tic Matrix Factorization (PMF) [24] and Constrained Prob-abilistic Matrix Factorization [24], the latent features a re uninterpretable, and there is no range constraint bound on the latent features vectors. The lack of interpretability r e-sults in the improper modeling of the latent factors, hence downgrades the recommendation accuracy. In [34], a non-negative constraint is imposed on both user-specific featur es U and item-specific features V (Nonnegative Matrix Fac-torization), but this work is also unable to interpret the physical meanings of the latent factors. Furthermore, the low-rank approximation methods also suffer from the data sparsity problem. Hence, in this paper, we propose a novel both U and V (a modified version of NMF with global consistency). matrix factorization method to solve the analyzed problems and remedy the aforementioned deficiencies.
We demonstrate a Semi-Nonnegative Matrix Factorization method with Global Statistical Consistency for collabora-tive filtering, in which the user-specific latent feature U includes the meaning of the confidence of user i on the k -th latent type of the item, and the item-specific latent feature V jk includes the meaning of the quality of the item j on the k -th latent type of the item. This work has showed that the latent features with physical meanings can achieve not only the model interpretability but also the prediction acc u-racy. Moreover, we propose a novel method that imposes the consistency between the statistics of training data and the statistics of the predicted ratings. The experimental anal y-sis shows that our method outperforms other state-of-the-a rt algorithms.

For the global consistency, we only take the first step, i.e., we only make our models consistent with the first moment currently. By doing so we have already achieved promising results. In order to capitalize on these achievements, furt her study is needed on the following problems: 1. We would enforce the consistency with the second mo-ment globally in the models without increasing the complex-ity of our models. 2. There is prior information that all values in the matrix P ping. Without taking any action, prediction by will run outside of the range of valid rating values. For this , one choice is to map the values to the interval [0 , 1] by some nonlinear functions like logistic function. But in our sett ing, such a mapping does not match our intuition X  X he prediction on the user-item pair ( i, j ) results from a linear combina-tion of the products of i  X  X  authority on a latent type and j  X  X  quality. For such a consideration, how can we put a con-straint that 0  X  the latent features dimension by dimension. [1] M. Balabanovi  X c and Y. Shoham. Fab: content-based, [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [3] J. Canny. Collaborative filtering with privacy via [4] B. Cao, J.-T. Sun, J. Wu, Q. Yang, and Z. Chen. [5] M. Deshpande and G. Karypis. Item-based top-n [6] J. L. Herlocker, J. A. Konstan, A. Borchers, and [7] W. Hill, L. Stead, M. Rosenstein, and G. Furnas. [8] T. Hofmann. Collaborative filtering via gaussian [9] T. Hofmann. Latent semantic models for collaborative [10] Z. Huang, H. Chen, and D. Zeng. Applying associative [11] R. Jin, J. Y. Chai, and L. Si. An automatic weighting [12] A. Kohrs and B. Merialdo. Clustering for collaborative [13] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, [14] G. Linden, B. Smith, and J. York. Amazon.com [15] N. N. Liu and Q. Yang. Eigenrank: a ranking-oriented [16] H. Ma, I. King, and M. R. Lyu. Effective missing data [17] H. Ma, I. King, and M. R. Lyu. Learning to [18] H. Ma, H. Yang, M. R. Lyu, and I. King. SoRec: [19] B. Marlin. Modeling user rating profiles for [20] M. Pazzani and D. Billsus. Learning and revising user [21] J. D. M. Rennie and N. Srebro. Fast maximum margin [22] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [23] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [24] R. Salakhutdinov and A. Mnih. Probabilistic matrix [25] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [26] B. M. Sarwar, J. A. Konstan, A. Borchers, [27] J. B. Schafer, J. A. Konstan, and J. Riedl.
 [28] U. Shardanand and P. Maes. Social information [29] L. Si and R. Jin. Flexible mixture model for [30] N. Srebro and T. Jaakkola. Weighted low-rank [31] N. Srebro, J. D. M. Rennie, and T. Jaakkola. [32] L. Terveen, W. Hill, B. Amento, D. McDonald, and [33] M. E. Tipping and C. M. Bishop. Probabilistic [34] S. Zhang, W. Wang, J. Ford, and F. Makedon.
 [35] Y. Zhang and J. Koren. Efficient bayesian hierarchical [36] D. Zhou, S. Zhu, K. Yu, X. Song, B. L. Tseng, H. Zha, [37] S. Zhu, K. Yu, and Y. Gong. Predictive matrix-variate
