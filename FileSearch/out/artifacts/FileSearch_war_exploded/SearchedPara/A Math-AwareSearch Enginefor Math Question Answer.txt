 We propose a math-aware search engine that is capable of handling both textual keywords as well as mathematical ex-pressions. Our math feature extraction and representation framework captures the semantics of math expressions via a Finite State Machine model. We adapt the passive aggres-sive online learning binary classifier as the ranking model. We benchmarked our approach against three classical infor-mation retrieval (IR) strategies on math documents crawled from Math Over ow , a well-known online math question an-swering system. Experimental results show that our pro-posed approach can perform better than other methods by more than 9%.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance math document retrieval, math-aware search engine, learn-ing to rank
Math question answering (Q&amp;A) systems deal with math-ematical questions and answers embedded in math docu-ments. A math document can contain both textual data and math expressions.Major online Math Q&amp;A systems such as Math Over ow are constantly monitored and updated by their online user community. Therefore, an up-to-date and effective math-aware search system is essential for users to find the latest math documents. This system must sup-port searching based on both keywords and math expres-sions. Although there are many conventional keyword-based search engines available for searching textual documents, to the best of our knowledge, there exist no publicly-available math-aware search engine for math Q&amp;A systems.
Wolfram research hosts a public computation knowledge search engine called Wolfram Alpha 1 , which can parse sim-ple math queries and expressions. It is not a search engine per se, but more of a computational engine Mathematica 2 . For example, Alpha will compute and return the numerical result of a math expression like e sin 20 , in addition to pro-viding its alternate form and continuous fraction represen-tation. For math related search queries like  X  X iemann X , Al-pha returns a brief biographical summary of Bernhard Rie-mann the Mathematician, including his full name, date of birth/death, etc. What it will not do is return a list of rel-evant documents like conventional Web search engines. Al-though this system does not require huge amount of storage for operation, it is not really an all-purpose search engine since it returns largely the computational results and/or knowledge from a few selected common topics instead of Web pages crawled from the entire Internet. For instance, if we search for  X  X phone5 X , Alpha will respond with the mes-sage,  X  X evelopment of this topic is under investigation X  even though  X  X phone5 X  is a very popular term 3 . On the other hand, Alpha understands search queries of stock symbols and also geographical entities such as  X  X os Angeles X .
Another approach to tackle the problem is to extend text search systems for math search. In the preprocessing phase, math expressions are simply converted to textual data, af-ter which classical information retrieval (IR) techniques are applied for retrieval, as was previously reported in Math-dex [18] and MathWebSearch [12, 14]. Developing math search systems in this way is fast and simple, but their per-formance depends heavily on the fidelity of the conversion from symbolic math expressions to textual representations.
To overcome the above limitations, we propose a novel approach for math-aware search, which comprises two steps: (1) extract meaningful math index terms based on the se-mantically rich MathML format; (2) apply the learning-to-rank approach to improve the accuracy of the ranking pro-cess. A question posted by user \Alejandro Erickson" from the Math Overflow system is shown below. The question belongs to the probability category. http ://www.wolframalpha.com http://www.wolfram.com/mathematica as of August 2012, prior to the actual product launch date.
To provide a search facility for questions like this, one can ignore math expressions and simply deploy a conventional keyword-based search engine. However, users could also be interested in finding math documents containing a particular math expression. In this case, conventional search engines fail miserably. For one, it cannot handle mathematical ex-pressions in the query. Second, it does not know that a+b is semantically equivalent to x+y. These are just two of the many limitations of the simple approach. We shall discuss more in the following section.
The key contributions of this paper are summarized below. Figure 1: Math-Aware Search Engine Framework.

Figure 1 shows the proposed math-aware search engine framework. The idea is to extract and index text and math terms independently in two separate inverted indexes. A user query is first matched against both the math and text inverted indexes to retrieve an initial set of unranked can-didates based on the intersection of matching documents in both the math and text query terms. A continuously learn-ing ranking model assign scores to each document in this candidate set. Figure 2 shows the graphical user interface of the search engine, in which the user-friendly graphical equation editor CODECOGS allows math expressions to be entered effortlessly. The CODECOGS 4 editor converts user entered math expressions automatically into our preferred
E X storage format. There exists several math-aware retrieval systems such as MathFind [19], ActiveMath [15], Wolfram Formula Search 5 , Wikipedia Formula Search 6 , Whelp [1], Mathdex [18], and MathWebSearch [12, 14]The vast majority of these systems are developed based mainly on conventional text retrieval techniques, which treats math expressions as textual data.
Since text retrieval based math search systems follow more or less the same principles, we will only review a few repre-sentative math search systems in this section. In the Math-Find system [19], Munavalli and Miner encoded math ex-pressions into a sequence of textual data called math frag-ments. In this approach, math expressions are preprocessed and converted into the corresponding MathML format [2]. Similarly, Nguyen et al. [21] extracted math features from MathML and used Formal Concept Analysis as the formula search mechanism. To index math expressions and sup-port sub-expression search, Miner and Munavalli [18] de-fined math n -grams, where each math token is considered a 1 -gram. For instance,  X  X +b X  has three 1-grams, i.e.,  X  X  X ,  X  X  X , and  X + X . To search for math expressions, a query is decom-posed into n -grams. The indexes are then searched for each n -gram in the query. Similarly, Libbrecht and Melis [15] pro-posed the ActiveMath Search Tool, which was built on top of Apache Lucene 7 . In the ActiveMath Search Tool, math data are represented in the OMDoc [13] format. They are subsequently converted into tokens and indexed using the Lucene text search engine.

Wolfram Formula Search 8 is another math search system that was developed by Wolfram Research. Wolfram Formula Search allows users to search for formulas from its formula compendium of math functions via the Web. Its database contains more than three hundred thousand formulas classi-fied into 14 categories. The formula search engine provides a friendly interface for users to explore the vast collection of formulas. However, although Wolfram Formula Search engine provides semantic search, it only supports search queries based on predefined constants, operations, and func-tion names. For example, it understands common constants like e ,  X  , etc., and operations including sum, product, etc., appearing anywhere in a formula.

For the past few years, many learning-to-rank algorithms have been proposed to improve the ranking of search re-sults. Some adapted binary classification algorithms such as Perceptron and its variants [4]. In [9], Gao et al. ap-plied the Perceptron algorithm to learn the ranking func-tion. Although the Perceptron algorithm is elegant and fast, it performs poorly on datasets that are not linearly separable. To overcome this limitation, Crammer et al. [7] proposed the Passive Aggressive (PA) algorithm, which uses large margin optimizations. Improved versions of PA in-clude Confidence-Weighted (CW) Linear Classification [8] and Passive-Aggressive Mahalanobis (PAM) [20]. In the http ://www.codecogs.com/latex/about.php http://functions.wolfram.com/formulasearch http://shinh.org/wfs/ http://lucene.apache.org http://functions.wolfram.com/formulasearch realm of batch learning, the SVM algorithm [6, 26] has also been applied to the ranking problem. However, for dynamic systems like math Q&amp;A systems, where math questions are posted every minute, an online learning algorithm is more desirable as it can incrementally update itself whenever new data is added. Therefore, in this research, we extend and apply the PA algorithm to learn the document ranking func-tion.
Math documents can be significantly more complex than textual documents; it can involve math expressions that are sequence sensitive in two dimensions, e.g., matrices, frac-tions, nested forms. Conventional text retrieval methods, which typically throw away the linear sequence information in text, are thus extremely poorly suited to process math documents. Therefore, new retrieval methods must be de-vised to effectively handle math documents. We introduce our math document format and feature extraction approach in this section.
Mathematical markup languages play an important role in the development of math search systems. It affects the storage and techniques for the processing of math expres-sions. There are several common mathematical markup lan-guages: L A T E X, ASCIIMath 9 , OMDoc [13], OpenMath [5] and MathML [2]. The content-oriented L A T E X markup lan-guage has been commonly used by many researchers, es-pecially mathematicians, for document processing. ASCI-IMath markup is a simpler variant of the L A T E X markup language [25], which is often used as a format for specify-ing math expression queries for math-aware search systems. Math expressions represented using ASCIIMath markup are easy to store as it mainly uses ASCII characters to represent the math expressions. However, the disadvantage of ASCI-IMath is that its data representation is not well-structured, thereby requiring additional processing time to parse and extract math expressions. OMDoc and OpenMath are two other less popular standards for the semantic-oriented rep-resentation of math or expressions. htt p://www1.chapman.edu/  X jipsen/mathml/asciimath.html
Among the various format, the MathML markup of W3C is the most popular, and is currently used by many math search engines for the presentation and storage of math ex-pressions [15, 17, 19]. There are two variants of MathML: MathML content markup and MathML presentation markup. Among the two markup representations, MathML content markup is commonly used for the processing of math ex-pressions as it contains richer semantic content than the MathML presentation markup.
 Despite the advantages of MathML, our system uses the
E X markup as the storage and user interface language for entering queries. This is because L A T E X markup has a sig-nificantly smaller storage footprint than MathML. More im-portantly, L A T E X markup has a large user base and is more intuitive for users to enter arbitrary math expressions on-line. In fact, open source Javascript display engines like MathJax 10 and ASCIIMath can parse L A T E X markups and render the corresponding graphical math expressions on the web browser.

Moreover, during the feature extraction step, we trans-form our L A T E X queries and documents into MathML, which are easier for machines to analyze. Listing 1 shows an ex-ample question in L A T E X format, comprising the title and content of the example question from Section 1.
 Listing 1: An Example Question in L A T E Xformat. For mulate edge length problem as convex I wa nt to use convex optimization to describe a http ://www.mathjax.org
Due to the existence of both semantic and structural in-formation, the preprocessing step for math expressions is more complex than that of text documents. For the pur-pose of math search, math features should be representative enough to reflect the underlying characteristics of each math expression. We thus extract math features as follows:
To convert math expressions from L A T E X, we use the Snug-gleTeX library 11 . We first convert math expressions from
E X to the representation MathML format, then we use cascading stylesheets to map the representation MathML to content MathML. Listing 2 shows an example of the content MathML for the math expression ( x + y ) 2 .

Once the expression is converted into content MathML, we use XML tree traversal to extract the math features. In this paper, we only use two kinds of features, namely single and combination features. Single features are used to express constant numbers, variable names, function names, etc. On the other hand, combination features refer to combinations of math operators and operands.

Listing 2: MathML Content Markup of ( x + y ) 2 . &lt; app ly &gt; % apply operator &lt; /ap ply &gt;
Tak e the sub-expression x + y in Listing 2 as an example, based on the content MathML data, we have four single features, namely ci , plus , cix , and ciy , where ci stands for a variable and cix stands for a variable named x . We also have one combination feature pluscixciy , where plus stands for the operator + and pluscixciy stands for operator + applied to two operands x and y .
In this section, we define two types of index terms, namely text and math. These two types of index terms collectively form our math-aware search engine.

Definition 1 (Text Index Term). Let n be the num-ber of index terms in the system. Denote each text index term by t i , and the set T = { t 1 , . . . , t n } contains all text index terms.

Definition 2 (Math Index Term). Let m be the num-ber of math index terms in the system. Each math index term m j is an element of the math feature set. Let M = { m 1 , . . . , m m } be the set of all math index terms. htt p://www2.ph.ed.ac.uk/snuggletex
Let D = { d 1 , d 2 , . . . , d k } and Q = { q 1 , q 2 , . . . , q collection of k math documents and a set of l user queries, respectively. The math-aware IR problem aims to determine a ranking function score : D  X  Q 7 X  R , which defines an or-dering among documents based on the queries via a frame-work F . This framework considers all math documents and queries. For example, in text IR, the framework F is typi-cally the vector space of document and query points, with the ranking function determined by cosine similarity.
In this section, we formally define the query and rank-ing models for math Q&amp;A search based on the vector space model. The proposed approach is an extension of the vector space model for conventional text IR.
In this section, we formally define math documents and queries in the math-ware search engine, where the data con-sists of both text and math expressions. The search engine should index both types of data simultaneously.

Definition 3 (Math Predicate). A math predicate is de ned as p i if the math index term m i appears in a docu-ment d j . In other words, d j satis es p i if d j contains the math index term m i .

Definition 4 (Math Specification). A math speci-cation P = p 1  X  p 2  X  . . .  X  p s speci es a subset of documents D P  X  D , where d i  X  D P , if d i satis es P .

Definition 5 (Math Document). A math document d j  X  D is determined by a vector d j = ( d 1 j , . . . d nj math speci cation P j , where d ij  X  0 . If the text index term t appears in document d j , then d ij &gt; 0 ; otherwise, d Thus P j is the corresponding math speci cation satis ed by document d j .

Definition 6 (Math Query). A query in a math ques-tion answering system, q = q t | q m , is comprised of two parts: a keyword query q t  X  T and a math expression query q m  X  M . The keyword query q t is a list of keywords and the math expression query q m is a list of math expressions.
For the math query q = q t | q m , suppose that q t = { t t } and q m = { m 1 , . . . , m j } corresponds to math specifica-tion P , the unranked result q ( D ) denotes the subset of doc-uments that satisfies P , and those that contain all the terms in q t as follows: where the selection function  X  t i ( D ) = { d j | t i  X  d
Now, we define the ranking of the result of q . We start by presenting a generic representation of the conventional ranking function for keyword queries. We then extend it into the math document ranking function.

Given a query q and a document d  X  D , a conventional ranking function f (  X  ) takes as argument statistics from q , d , and computes a score of d with respect to q as follows: For exa mple, tf  X  idf weighting [16, 23] is a well-known rank-ing model. Among its variants, the pivoted normalization formula is one of the best performing vector space models, and is widely used in many text search systems. Specifically, it is defined as score( q , d ) = where l en ( d ) is the length of document d , | X | denotes the set cardinality function, tf ( t, d ) is the term frequency of term t in document d , df ( t, D ) is the document frequency of term t in the document collection D , and s is a parameter.
We note that the scoring function depends on the number of documents. Therefore, we revise this scoring function for ranking math document search results as follows: score( q , d ) =  X 
In this section, we describe the Ranking Passive Aggres-sive algorithm in details based on the Passive Aggressive algorithm [7]. Let ( x t , y t ) t =1 ,T be a sequence of examples, Crammer et al. proposed the PA algorithm, which learns the weight w of a linear prediction function f ( x ) = w  X  x based on the following optimization problem: where  X  is the Hinge loss function.

We adapt the Passive Aggressive algorithm to learn-to-rank by training on pairs of instances. A correct classifica-tion corresponds to ranking a pair so that the more relevant instance is scored higher and vice versa.

We know that math and text features are extracted from different data types and therefore should be treated differ-ently. We can apply the cascade ranking model [24], where one type of data is used to rank documents at each stage of a multi-stage ranking procedure. However, in practice, this approach may not be practical because math questions do not always contain math expressions. Therefore, we pro-pose another approach that combines math and text data as follows. Let a t  X  R k and b t  X  R l be the math and text vec-tors representing the document d t  X  R n , respectively, where k + l = n . The simplest way is to concatenate the math and text data as follows: where  X   X  [0 , 1] weighs the relative importance of the text and math data.

Remark  X  = 0 means that we ignore math expressions in the math document. If  X  = 1, we only rank the search result based on the math expressions and ignore the text data. Normally,  X  will be assigned values between 0 and 1. If we treat math and text data as equals, then  X  = 0 . 5. If  X  &lt; 0 . 5, we consider text data to be more important than math data, and vice versa.

Let the feature score vector of a document d t  X  R n and query q be defined as follows: The goal is to learn the weight vector w of the following scoring function.
Suppose that R and N denote relevant and non-relevant feature score vectors. Then the feature score vectors of rele-vant and non-relevant documents to the query q are defined as  X  R t and  X  N t , respectively. Let (  X  R t ,  X  N t ) t =1 ,T of training examples. We should have w  X   X  R t  X  w  X   X  N t cause the score of the relevant document to the query should be greater than that of the non-relevant document. For the Ranking Perceptron [9], the weight vector w t +1 is learnt on each round t as follows: where  X  is a constant learning rate. Similar to the Percep-tron algorithm, the Ranking Perceptron algorithm is very sensitive to the learning rate, and also works poorly on non-linearly separable data.

To overcome this problem, we introduce a new Hinge loss function, which penalizes the difference of the two scores if the score of the non-relevant document is greater than that of the rel-evant document, with respect to the query. We then derive a soft margin optimization problem by introducing a slack variable  X  as follows: where C is an aggressiveness parameter.

Proposition 1. The optimization problem (3) has the closed-form solution as follows: where the learning rate  X  t has the form Algori thm 1 summarizes the Ranking Passive Aggressive al-gorithm.
 Algori thm 1 Ranking Passive Aggressive Algorithm Input: C = p ositive aggressiveness parameter Output: None Process:
Initialize w 1  X  0 ; for t = 1 , 2 , . . . do end for
Let  X  R t and  X  N t b e the feature score vectors of the relevant and non-relevant documents for query q , respectively. Let w t be the weight vector of the Ranking Passive Aggressive algorithm on round t . We say that the proposed algorithm makes a mistake if w t  X   X  R t &lt; w t  X   X  N t . The total number of mistakes made by the proposed algorithm is bounded as shown in Preposition 2. The proofs of Prepositions 1 and 2 are given in the Appendix.

Proposition 2. Let (  X  R t ,  X  N t ) t =1 ,T be a sequence of ex-amples, where  X  R t  X  R n and  X  N t  X  R n for all t . Then for any weight vector u  X  R n and its loss  X   X  t =  X  ((  X  R t the number of mistakes made by the Ranking Passive Ag-gressive algorithm is bounded by
In math question answering system, math questions typi-cally contains additional metadata such as user rating, num-ber of views, number of answers, etc. The metadata can be very useful for improving ranking results, faceted search, and personalization [3]. The question is how to combine the document content and its metadata to answer a user X  X  in-formation need. To address this, we have to first choose a suitable set of metadata. For instance, user may prefer to see the most popular and/or answered questions. Moreover, he could also be looking for questions that have already been answered by other users. In each of these two scenarios, the number of views and answers are useful information for re-ranking the search results.

To retrieve documents based on user ratings, Zhang et al. [27] proposed a simple IR technique, which considers thumbs-up and thumbs-down as a term in the document. They came up with a scoring function, which is the ratio of the number of thumbs-up ( n + ) to the number of thumbs-down ( n  X  ). However, in practice systems like Math Over-flow only store the difference ( n +  X  n  X  ) between these two user ratings. Therefore, the above technique cannot be di-rectly applied. In order to overcome this limitation and consider other kinds of metadata, we propose another IR approach, which is based on the tf  X  idf ranking model.
Since users want to retrieve  X  X ood X  math questions, e.g., questions with thumbs-up, questions with many views, ques-tions with several answers, questions with highly-rated an-swers, and questions with high reputation, we can embed this need into a virtual query v comprising metadata. The scoring function of this query is defined as follows: where m t is the metadata of document d t .

Let the scoring function between document d t and query q be defined as in Equation (2). The overall scoring function is then defined as the linear combination of the query and virtual-query scoring functions, score ( d t , q ) and score ( d as score ( d t , q , v ) =  X   X  score ( d t , q )+(1  X   X  )  X  score ( d where  X   X  [0 , 1] determines the relative importance of con-tent and metadata for document d t . The advantage of this approach is that we are able to determine the metadata score beforehand and update it whenever users update the meta-data.
The major obstacle to evaluating math search is that we do not have a standard benchmark dataset like for other more common IR tasks. Also, it is hard to compare our pro-posed approach with other systems such as MathFind and Wolfram Function Search because they are either unavail-able or inaccessible.

Therefore, we build our own math search dataset by crawl-ing and downloading 31,288 math questions and answers (or math documents for short) from the Math Overflow online question answering system. Each math document can have one or more of the 20 categorical label. We parsed and con-verted the math documents from HTML into the Text RE-trieval Conference (TREC) 12 XML format. We also manu-ally generated 30 queries in the TREC format. Each query has a set of relevant and non-relevant documents deterim-ined manually. Each document is identified by a numerical document ID. All math documents are indexed using standard inverted index techniques with stop-word removal based on the En-glish stop-word list provided by Terrier Information Re-trieval Platform [22]. The total number of index terms is 29,745.

To learn the ranking function, all math documents and queries are processed and converted into tf  X  idf [16] vec-tors. The feature score vectors are calculated based on these documents and queries. The feature score vectors (consist-ing of more than 1000 vectors) are used to train the Ranking Perceptron and Ranking PA algorithms.

After training with cross validation, we obtained an opti-mal weight vector w , which is used to determine the scores of all retrieved math documents. Since other math-aware search systems only focus on math feature extraction and ap-ply classical IR techniques, we only compared our approach against standard retrieval methods such as BM25 [11], InL2, and tf  X  idf , where InL2 stands for  X  X nverse Document Fre-quency with Laplace after-effect and normalization 2 X  [10]. While BM25 and tf  X  idf are established IR techniques, the InL2 model was only proposed recently. For the tf  X  idf ranking model, we applied the straightforward approach de-scribe in Section 4.2.1.
In this section, we evaluate the proposed math feature extraction approach by comparing the retrieval precision on the math expressions with and without math feature extrac-tion. The procedure is as follows: http ://trec.nist.gov/ Table 1: Textual Math and Math Feature Proper-ties.
 # Uniq ue Terms 295 9 6060 7145 Avg. DocLen 40.2 5 13.4 6 65.5 5 Avg. PLLen 25.5 3 8.63 28.0 1
Avg. QT (ms) 62.2 0 53.8 0 78.8 0 Figure 3: FSM for extracting Textual Math Fea-tures.

A finite state machine (FSM) as shown in Figure 3 is used to extract textual math features, where whitespace stands for invisible characters, end-of-stream characters, and de-limiters such as space, tab, comma, etc. Operator refers to common math operators such as plus, minus, power, etc. Literal includes characters from the set  X  X  X  to  X  X  X  and  X  X  X  to  X  X  X . Digit refers to numbers  X 0 X  to  X 9 X . The FSM accepts a math expression as a sequence of characters, on which it extracts math operators, math variables, function names, and constant numbers. Specifically, at state 2, it skips a whitespace. At state 3, it extracts the operator. Upon ter-mination at state 4, it returns a variable, a function name, or a constant number.
 The properties of all three feature sets are summarize in Table 1. We counted the number of unique terms and cal-culated the average document length (DocLen) and average posting list length (PLLen), where the document length is determined based on the number of index terms in the doc-ument (i.e., math expressions). While the number of unique terms represents the size of the lexicon , the average posting list length indicates the number of documents associated with a term in the inverted index. These two major factors impact the performance of a search system in terms of query time (QT) and accuracy.

Note that the total number of unique terms in the math feature set is greater than that of the textual math feature set. However, the average document length and posting list length of the math feature set are smaller than those of the textual math feature set. This implies that the math feature set is less uniform and therefore more discriminative than the textual math feature set. Figure 4: Performance Comparison on the Math Ex-pression Dataset.

To verify the above claim, we carried out a retrieval ex-periment and compared the IR performance on both feature sets. Figure 4 shows the precision at different recall values on the three feature sets. In this experiment, the popular ranking method BM25 was applied. We see that BM25 does not work well on the textual math dataset. Its max preci-sion on this set is 6% at 40% recall. On the other hand, its precision on the math feature set and n-gram set is sig-nificantly larger. Even in the worst case, the math feature set X  X  precision is improved by more than 53%. This result confirms that math feature extraction is essential for an ef-fective math-aware search engine, compared to the simplistic textual math set. Besides, the math feature set gave bet-ter precision than n-gram for all levels of recall greater than 10%, which makes it the practical feature choice.
We trained both the Ranking Perceptron and Ranking PA algorithms on the feature score vectors. We then compared their performance by using the standard cumulative error rate, which is the ratio of mistakes made by the online learn-ing algorithm over the total number of examples observed to-date. In this experiment, the performance of the Ranking PA algorithm and the Ranking Perceptron algorithm [9] is evaluated on the Math Overflow dataset. Figure 5: Cumulative Error Rate for Math Over ow Datatset.

Figure 5 shows the cumulative error rates of both the Table 2: Metadata Properties (  X  using Equation (4)).
 Min -3 0 14 -4 2.92 15 Max 93 121 216 06 86 27.2 277 Avg 5.904 9 1.91 46 498. 2342 5.83 95 14.8 662 Rank ing Perceptron and Ranking PA algorithms on the Math Overflow dataset. Compared to the Ranking Perceptron al-gorithm, the proposed Ranking PA algorithm consistently achieved a lower cumulative error rate than the Ranking Perceptron algorithm. This shows that the proposed algo-rithm is effective in improving the online ranking function learning. Hence, in the ranking process, if the weight of the Ranking PA is utilized, the retrieval precision will be improved compared to that of the Ranking Perceptron al-gorithm. More experimental results shall confirm this claim in the next section.
For the Ranking PA algorithm, we used cross validation to choose the optimal parameter C . Then, we used the op-timal solution/weight to rank retrieved documents based on the scoring function (5) of Section 4.2.3. We also evaluated the Ranking Perceptron algorithm. In this experiment, we used both text and math feature sets, where the text features were extracted using an English tokenizer with stop-word re-moval and the math features were extracted as described in Section 3.2. Moreover, the metadata of each question is ex-tracted to calculate the score. We determined the optimal trade-off parameter  X  in Equation (5) to be around 0 . 958. This means that content largely rules over metadata. The properties of the metadata of all questions are shown in Ta-ble 2.

Figure 6 shows the performance comparison between our proposed approach and other methods. The retrieval preci-sion of each method is compared at recall levels of 0%, 10%, and 20%. The experimental results show that the Ranking PA algorithm consistently outperformed other methods. Its precision is at least 9% better than other methods at 0% and 10% recall levels. In the worst case, the improvement is more than 7% at 20% recall. We did not further evaluate higher recall levels because the precision have fallen to way less than 50% to be of practical significance. Figure 6: Performance Comparison on the Math Over ow Dataset.

Compared with other methods, the straightforward ap-proach tf  X  idf is slightly better. The improvement is nearly 1%. Hence, term weighting method alone is not good enough to rank highly structured data such as math expressions. In this case, the learning-to-rank algorithm should be applied to improve the performance of the search system. In this experiment, the Ranking PA has been shown to be an el-egant solution for this problem. If we take into account the metadata, the performance is just marginally improved. This improvement is nearly 5% at 20% recall but declines gradually with decreasing recall levels.
In this paper, we proposed a new approach to math-aware search engine, which consists of two major steps: feature ex-traction and learning-to-rank. In feature extraction, we pro-posed a new method for math expression feature extraction based on the content MathML format. To learn the score function, we formulated and derived an online optimization problem. We then derive a closed-form solution, leading to our Ranking PA algorithm. In addition, we derived the mis-take bound of the Ranking PA algorithm.

Moreover, we crawled the Math Overflow question answer-ing system and generated a new dataset for math document retrieval. Our Math Overflow dataset is a big dataset, which has more than 30,000 math documents. It is very use-ful for math document retrieval evaluation in general and can be downloaded from project.mosuma.net. We evaluated our proposed ranking approach and benchmarked it against other baseline techniques on the Math Overflow dataset. The experimental results show that the proposed approach performs better than the runner-up by more than 9%. Al-though we evaluated our proposed approach on the math question answering dataset, it is possible to apply it to other kinds of math document retrieval.
 This research was supported in part by Singapore Ministry of Education X  X  Academic Research Fund Tier 2 grant ARC 9/12 (MOE2011-T2-2-056). [1] A. Andrea, G. Ferruccio, C. C. Sacerdoti, T. Enrico, [2] R. Ausbrooks, S. Buswell, S. Dalmas, S. Devitt, [3] P. N. Bennett, K. El-Arini, T. Joachims, and K. M. [4] H. Block. The perceptron: A model for brain [5] S. Buswell, O. Caprotti, D. P. Carlisle, M. C. Dewar, [6] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. [7] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [8] M. Dredze, K. Crammer, and F. Pereira.
 [9] J. Gao, H. Qi, X. Xia, and J.-Y. Nie. Linear [10] R. Guill  X en. Gir with language modeling and dfr using [11] K. S. Jones. Index term weighting. Information [12] A. Kohlhase and M. Kohlhase. Reexamining the mkm [13] M. Kohlhase and I. Sucan. A search engine for [14] M. Kohlhase and I. A. S  X  yucan. A search engine for [15] P. Libbrecht and E. Melis. Methods to access and [16] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [17] B. R. Miller and A. Youssef. Augmenting presentation [18] R. Miner and R. Munavalli. An approach to [19] R. Munavalli and R. Miner. Mathfind: a math-aware [20] T. T. Nguyen, K. Chang, and S. C. Hui.
 [21] T. T. Nguyen, S. C. Hui, and K. Chang. A [22] I. Ounis, G. Amati, V. Plachouras, B. He, [23] A. Singhal. Modern information retrieval: a brief [24] L. Wang, J. Lin, and D. Metzler. A cascade ranking [25] A. S. Youssef. Roles of math search in mathematics. [26] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [27] D. Zhang, R. Mao, H. Li, and J. Mao. How to count Here, we prove Proposition 1 as follows:
Proof. We define the Lagrangian of the optimization problem as follows: where  X   X  0 and  X   X  0 are the Lagrangian multipliers.
Setting the partial derivatives of L with respect to the weight w to zero, we have, Sett ing the partial derivatives of L with respect to weight  X  to zero, we have, Sub stituting (7) and (8) into (6), we have, Set ting the partial derivatives of L with respect to weight  X  to zero, we have, We ha ve  X  +  X  = C and  X   X  0. Therefore, we can conclude that 0  X   X   X  C , which concludes the proof.
 Next, we prove Proposition 2 as follows: Proof. We have We also have where  X  t =  X  ((  X  R t ,  X  N t ); w t ).
 Based on (10) and (11), we can conclude that have, If the Ranking PA algorithm makes a mistake on round t , we have  X  t  X  1. We also have  X  t = min Therefore,  X  t  X  t  X  min Let M be th e number of mistakes made by the Ranking PA algorithm, we have, Su bstituting (14) into (13), we have, min which concludes the proof.
