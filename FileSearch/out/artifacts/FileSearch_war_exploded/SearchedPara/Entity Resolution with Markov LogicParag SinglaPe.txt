
Entity resolution is the problem of determining which records in a database refer to the same entities, and is a crucial and expensive step in the data mining process. In-terest in it has grown rapidly in recent years, and many ap-proaches have been proposed. However, they tend to ad-dress only isolated aspects of the problem, and are often ad hoc . This paper proposes a well-founded, integrated solution to the entity resolution problem based on Markov logic. Markov logic combines first-order logic and proba-bilistic graphical models by attaching weights to first-order formulas, and viewing them as templates for features of Markov networks. We show how a number of previous ap-proaches can be formulated and seamlessly combined in Markov logic, and how the resulting learning and inference problems can be solved efficiently. Experiments on two ci-tation databases show the utility of this approach, and eval-uate the contribution of the different components.
Data cleaning and preparation is the first stage in the data mining process, and in most cases it is by far the most ex-pensive. Data from relevant sources must be collected, in-tegrated, scrubbed and pre-processed in a variety of ways before accurate models can be mined from it. When data from multiple databases is merged into a single database, many duplicate records often result. These are records that, while not syntactically identical, represent the same real-world entity. Correctly merging these records and the infor-mation they represent is an essential step in producing data of sufficient quality for mining. This problem is known by the name of entity resolution, record linkage, object identifi-cation, de-duplication, merge/purge, data association, iden-tity uncertainty, reference reconciliation, and others. In re-cent years it has received growing attention in the data min-ing community, with a related workshop at KDD-2003 [27] and a related task as part of the 2003 KDD Cup [16]. The entity resolution problem was first identified by Newcombe et al. [31], and given a statistical formulation by Fellegi and Sunter [14]. Most current approaches are variants of the Fellegi-Sunter model, in which entity resolu-tion is viewed as a classification problem: given a vector of similarity scores between the attributes of two entities, clas-sify it as  X  X atch X  or  X  X on-match. X  A separate match deci-sion is made for each candidate pair, followed by transitive closure to eliminate inconsistencies. Typically, a logistic re-gression model is used [1]. One line of research has focused on scaling entity resolution to large databases by avoiding the quadratic number of comparisons between all pairs of entities (e.g., [20, 30, 26, 7]). Another has focused on the use of active learning techniques to minimize the need for labeled data (e.g., [44, 38, 4]). Several authors have de-vised, compared and learned similarity measures for use in entity resolution (e.g., [6, 45, 3]). A number of alternate formulations have also been proposed (e.g., [5]). Entity res-olution has been applied in a wide variety of domains (e.g., [33, 10]) and to different types of data, including text (e.g., [25]) and images (e.g., [21]). Winkler [46] surveys research in traditional record linkage.

Most recently, several authors have pointed out that match decisions should not be made independently for each candidate pair. While the Fellegi-Sunter model treats all pairs of candidate matches as i.i.d. (independent and iden-tically distributed), this is clearly not the case, since each entity appears in multiple candidate matches. While this in-terdependency complicates learning and inference, it also offers the opportunity to improve entity resolution, by tak-ing into account information that was previously ignored. For example, Singla and Domingos [42], Dong et al. [12] and Culotta and McCallum [9] allow the resolution of en-tities of one type to be helped by resolution of entities of related types (e.g., if two papers are the same, their authors are the same, which in turn is evidence that other pairs of papers by the same authors should be matched, etc.). Mc-Callum and Wellner [28] incorporate the transitive closure step into the statistical model. Pasula et al. [34] incorporate parsing of entities from citation lists into a citation matching model. Bhattacharya and Getoor [2] use coauthorship rela-tions to help match authors in citation databases. Milch et al [29] propose a language for reasoning about entity resolu-tion. Shen et al. [40] exploit various types of constraints to improve matching accuracy. Davis et al. [10] use inductive logic programming techniques to discover relational rules for entity resolution, which they then combine using a naive Bayes classifier.

In this paper we propose a simple and mathematically sound formulation of the entity resolution problem that in-corporates these non-i.i.d. approaches, and can be viewed as a generalization of the Fellegi-Sunter model. It takes ad-vantage of the recent progress in statistical relational learn-ing (a.k.a. multi-relational data mining), which provides rich representations and efficient inference and learning al-gorithms for non-i.i.d. data [15, 13]. In particular, we use Markov logic, which combines first-order logic and Markov random fields [36], with weighted satisfiability testing for efficient inference and a voted perceptron algorithm for dis-criminative learning [41]. Our formulation makes it practi-cal to combine many different components into a compre-hensive solution to the entity resolution problem. We illus-trate this in this paper by combining a few salient ones, and applying the resulting system to a large citation database.
We begin by briefly reviewing the necessary background on Markov networks, first-order logic and Markov logic. We then describe our proposed approach to entity resolu-tion, report on our experiments, and outline directions for future work.
A Markov network (also known as Markov random field ) is a model for the joint distribution of a set of variables X =( X 1 ,X 2 ,...,X n )  X  X  [35]. It is composed of an undirected graph G and a set of potential functions  X  k .The graph has a node for each variable, and the model has a po-tential function for each clique in the graph. A potential function is a non-negative real-valued function of the state of the corresponding clique. The joint distribution repre-sented by a Markov network is given by where x { k } is the state of the k th clique (i.e., the state of the variables that appear in that clique). Z , known as the partition function , is given by Z = x  X  X  k  X  k ( x { k } Markov networks are often conveniently represented as log-linear models , with each clique potential replaced by an ex-ponentiated weighted sum of features of the state, leading to A feature may be any real-valued function of the state. This paper will focus on binary features, f j ( x )  X  X  0 , 1 } the most direct translation from the potential-function form (Equation 1), there is one feature corresponding to each possible state x { k } of each clique, with its weight being log  X  k ( x { k } ) . This representation is exponential in the size of the cliques. However, we are free to specify a much smaller number of features (e.g., logical functions of the state of the clique), allowing for a more compact represen-tation than the potential-function form, particularly when large cliques are present. Markov logic takes advantage of this.

Maximum a posteriori (MAP) inference in Markov net-works involves finding the most likely state of a set of query (output) variables given the state of a set of evidence (in-put) variables, and is NP-hard [37]. Conditional inference involves computing the distribution of the query variables given the evidence, and is #P-complete [37]. The most widely used approximate solution to this problem is Markov chain Monte Carlo (MCMC) [18], and in particular Gibbs sampling, which proceeds by sampling each non-evidence variable in turn given its Markov blanket (i.e., its neighbors in the graph), and counting the fraction of samples that each variable is in each state.

Maximum-likelihood or MAP estimates of Markov net-work weights cannot be computed in closed form, but, because the log-likelihood is a concave function of the weights, they can be found efficiently using standard gradient-based or quasi-Newton optimization methods [32]. Another alternative is iterative scaling [11]. Features can also be learned from data, for example by greedily con-structing conjunctions of atomic features [11].
A first-order knowledge base (KB) is a set of sentences or formulas in first-order logic [17]. Formulas are con-structed using four types of symbols: constants, variables, functions, and predicates. Constant symbols represent ob-jects in the domain of interest (e.g., people: Anna , Bob , Chris , etc.). Variable symbols range over the objects in the domain. Function symbols (e.g., MotherOf ) repre-sent mappings from tuples of objects to objects. Predicate symbols represent relations among objects in the domain (e.g., Friends ) or attributes of objects (e.g., Smokes ). A term is any expression representing an object in the do-main. It can be a constant, a variable, or a function ap-plied to a tuple of terms. For example, Anna , x , and GreatestCommonDivisor ( x , y ) are terms. An atomic for-mula or atom is a predicate symbol applied to a tuple of terms (e.g., Friends ( x , MotherOf ( Anna )) ). A ground term is a term containing no variables. A ground atom or ground predicate is an atomic formula all of whose ar-guments are ground terms. Formulas are recursively con-structed from atomic formulas using logical connectives and quantifiers. A positive literal is an atomic formula; a negative literal is a negated atomic formula. A KB in clausal form is a conjunction of clauses , a clause being a disjunction of literals. Every KB can be converted to clausal form. A possible world or Herbrand interpretation assigns a truth value to each possible ground atom. In finite do-mains, first-order KBs can be propositionalized by replac-ing each universally (existentially) quantified formula with a conjunction (disjunction) of all its groundings.
A central (and NP-complete) problem in logic is that of determining if a KB (usually in clausal form) is satisfiable , i.e., if there is an assignment of truth values to ground atoms that makes the KB true. One approach to this problem is stochastic local search, exemplified by the WalkSAT solver [39]. Beginning with a random truth assignment, Walk-SAT repeatedly flips the truth value of either (a) an atom that maximizes the number of satisfied clauses, or (b) a ran-dom atom in an unsatisfied clause. WalkSAT is highly effi-cient, and is able to solve hard instances of satisfiability with hundreds of thousands of variables in minutes. Many first-order problems (e.g., planning, software verification) can be solved efficiently by propositionalizing them and applying a satisfiability solver. The weighted satisfiability problem is a variant of satisfiability where each clause has an associated weight, and the goal is to maximize the sum of the weights of satisfied clauses. MaxWalkSAT is a direct extension of WalkSAT to this problem [22].
A first-order KB can be seen as a set of hard constraints on the set of possible worlds: if a world violates even one formula, it has zero probability. The basic idea in Markov logic is to soften these constraints: when a world violates one formula in the KB it is less probable, but not impossible. The fewer formulas a world violates, the more probable it is. Each formula has an associated weight that reflects how strong a constraint it is: the higher the weight, the greater the difference in log probability between a world that sat-isfies the formula and one that does not, other things being equal.
 Definition 1 [36] A Markov logic network (MLN) L is a setofpairs ( F i ,w i ) , where F i is a formula in first-order logic and w i is a real number. Together with a finite set of constants C = { c 1 ,c 2 ,...,c | C | } , it defines a Markov network M L,C (Equations 1 and 2) as follows: 1. M L,C contains one binary node for each possible 2. M L,C contains one feature for each possible ground-
Thus there is an edge between two nodes of M L,C iff the corresponding ground predicates appear together in at least one grounding of one formula in L . An MLN can be viewed as a template for constructing Markov networks. From Definition 1 and Equations 1 and 2, the probability distribution over possible worlds x specified by the ground Markov network M L,C is given by where F is the number formulas in the MLN and n i ( x ) is the number of true groundings of F i in x .Asformula weights increase, an MLN increasingly resembles a purely logical KB, becoming equivalent to one in the limit of all infinite weights.

In this paper we will focus on MLNs whose formulas are function-free clauses and assume domain closure, en-suring that the Markov networks generated are finite [36]. In this case, the groundings of a formula are formed sim-ply by replacing its variables with constants in all pos-sible ways. For example, if C = { Anna , Bob } ,the formula  X  x Smokes ( x )  X  Cancer ( x ) in the MLN L yields the features Smokes ( Anna )  X  Cancer ( Anna ) and Smokes ( Bob )  X  Cancer ( Bob ) in the ground Markov network M L,C (or  X  Smokes ( Anna )  X  Cancer ( Anna ) and  X 
Smokes ( Bob )  X  Cancer ( Bob ) in clausal form). See Rich-ardson and Domingos (2006, Table 2) for details.
MAP inference in Markov logic can be carried out effi-ciently using a weighted satisfiability solver like MaxWalk-SAT [22]. This is because the exponent in Equation 3 is the sum of the weights of the satisfied ground clauses, and thus P (
X = x ) can be maximized by maximizing this sum. To condition on evidence, we first replace the truth values of the evidence atoms into Equation 3 and simplify (false lit-erals disappear, and true literals cause the clauses they ap-pear in to disappear). Conditional probabilities can be com-puted by Gibbs sampling over the minimal ground network needed to answer the query; see Richardson and Domingos [36] for details.

Given a set of formulas, their weights can be learned either generatively (maximizing the joint likelihood of all predicates) or discriminatively (maximizing the conditional likelihood of the query predicates given the evidence ones). In this paper we use discriminative learning, as proposed by Singla and Domingos [41]. The training data is a rela-tional database (i.e., a set of positive ground literals with the closed world assumption, by which all atoms not in the database are assumed false). 1 Let x be the vector of truth values of the evidence atoms, and y the truth values of the query atoms. For simplicity, we assume that all non-evidence atoms are query atoms, which is appropriate for the application in this paper. We learn weights by gradi-ent descent (or, more precisely, ascent) on the conditional log-likelihood of y given x . From Equation 3, the partial derivative of the conditional log-likelihood with respect to the weight of the i th clause is where n i ( x, y ) is the number of true groundings of the i th clause in the data, and E w [ n i ( x, y )] is the clause X  X  ex-pected number of true groundings, averaged over all pos-sible worlds, weighted by their probabilities according to the current weights. Thus a clause X  X  weight should increase when its actual count is greater than its predicted one, and decrease when it is lower. Although computing the ex-pected counts E w [ n i ( x, y )] is intractable, they can be ap-proximated by the counts n i ( x, y  X  w ) in the MAP state (i.e., the most likely state of y given x ). This is a good ap-proximation if most of the probability mass of P w ( y | x concentrated around y  X  w ( x ) , and is the essence of the voted perceptron algorithm [8], which initializes all weights to zero, performs T steps of gradient descent, and returns the weights averaged over all iterations ( w i = T t =1 w i,t While it was originally developed for the special case of hidden Markov models, and used the Viterbi algorithm to find the MAP state, Singla and Domingos [41] generalized it to MLNs by replacing Viterbi with MaxWalkSAT.
It is also possible to learn the structure of MLNs using inductive logic programming techniques [23]. Learning can start from an empty network, or from an initial knowledge base.

Markov logic affords us the expressiveness of first-order logic while avoiding its brittleness, and makes it easy to in-corporate partial, imperfect, and even contradictory knowl-edge into the data mining process. MaxWalkSAT inference and voted perceptron learning are efficient enough to be practical for domains of realistic size. These algorithms are publicly available in the Alchemy system, which we use in our experiments [24].
Most systems for inference in first-order logic make the unique names assumption : different constants refer to dif-ferent objects in the domain. This assumption can be re-moved by introducing the equality predicate ( Equals ( x, y or x = y for short) and its axioms [17]: Reflexivity:  X  xx = x .
 Symmetry:  X  x, y x = y  X  y = x .
 Transitivity:  X  x, y, z x = y  X  y = z  X  x = z . Predicate equivalence: For each binary predicate R : Adding the formulas above with infinite weight to an MLN allows it to handle non-unique names. We can also add the reverse of the last one with finite weight: Reverse predicate equivalence: For each binary predicate
The meaning of this formula is most easily understood by noting that it is equivalent to the two clauses:
As statements in first-order logic, these clauses are false, because different groundings of the same predicate do not generally represent the same tuples of objects. However, when added to an MLN with a finite weight, they cap-ture an important statistical regularity: if two objects are in the same relation to the same object, this is evidence that they may be the same object . Some relations provide stronger evidence than others, and this is captured by as-signing (or learning) different weights to the formula for different R . For example, when de-duplicating citations, two papers having the same title is stronger evidence that they are the same paper than them having the same author, which in turn is stronger evidence than them having the same venue. However, even the latter is quite useful: two papers appearing in the same venue are much more likely to be the same than two papers about which nothing is known.
Remarkably, the equality axioms and reverse predicate equivalence are all that is needed to perform entity resolu-tion in Markov logic . As we will see, despite its simplicity, this formulation incorporates the essential features of some of the most sophisticated entity resolution approaches to date, including the  X  X ollective inference X  approaches of Mc-Callum and Wellner [28] and Singla and Domingos [42]. A number of other approaches can be incorporated by adding the corresponding formulas. (We emphasize that our goal is not to reproduce every detail of these approaches, but rather to capture their essential features in a simple, con-sistent form.) Further, entity resolution and data mining can be seamlessly combined, and aid each other, by performing structure learning on top of the entity resolution MLN.
We now describe our proposed approach in de-tail. For simplicity, we assume that the database to be deduplicated contains only binary relations. This entails no loss of generality, because an n -ary rela-tion can always be re-expressed as n binary relations. For example, if a citation database contains ground atoms of the form Paper ( title , author , venue ) , they can be replaced by atoms of the form HasTitle ( paper , title ) , HasAuthor ( paper , author ) and HasVenue ( paper , venue ) . Each real-world entity (e.g., each paper, author or venue) is represented by one or more strings appearing as arguments of ground atoms in the database. For example, different atoms could contain the strings ICDM -2006 , Sixth ICDM and IEEE ICDM  X  06 , all of which represent the same conference. We assume that the predicates in the database (representing relations in the real world) are typed ; for example, the first argument of the predicate HasAuthor ( paper , author ) is of type Paper , and the second is of type Author . The goal of entity resolution is, for each pair of constants of the same type ( x 1 ,x 2 ) , to determine whether they represent the same entity: is x 1 = x 2 ? Thus the query predicate for inference is equality; the evidence predicates are the (binarized) relations in the database, and other relations that can be deterministically derived from the database (see below). The model we use for entity resolution is in the form of an MLN, with formulas and weights that may be hand-coded and/or learned. The most likely truth assignment to the query atoms given the evidence is computed using MaxWalkSAT. Conditional probabilities of query atoms given the evidence are calculated using Gibbs sampling.
The model includes a unit clause for each query pred-icate. (A unit clause is a clause containing only one lit-eral.) The weight of a unit clause captures (roughly speak-ing) the marginal distribution of the corresponding predi-cate, leaving non-unit clauses to capture the dependencies between predicates. Since most groundings of most pred-icates are usually false (e.g., most pairs of author strings do not represent the same author), it is clearest to use unit clauses consisting of negative literals, with positive weights. Since predicate arguments are typed, there is a unit clause for equality between entities of each type (e.g., paper1 = paper2 , author1 = author2 , etc.). From a discriminative point of view, the weight of a unit clause represents the threshold above which evidence must accu-mulate for a candidate pair of the corresponding type to be declared a match.
We assume that each field in a database to be dedu-plicated is a string composed of one or more words (or, more generally, tokens), and define the predicate HasWord ( field , word ) which is true iff field contains word . Applied to this predicate, reverse predicate equiva-lence states that or, in other words, fields that have a word in common are more likely to be the same. When inserted into Equation 3, and assuming for the moment no other clauses, this clause produces a logistic regression for the field match predicate x 1 = x 2 as a function of the number of words n the two fields have in common: P ( x 1 = x 2 | n )=1 / (1 + e  X  wn where w is the weight of the formula. Effectively, then, reverse predicate equivalence applied to HasWord () imple-ments a simple similarity measure between fields. This measure can be made adaptive, in the style of Bilenko and Mooney [3], by learning a different weight for each ground-ing of reverse predicate equivalence with a different word. (Groundings where y 1 = y 2 are always satisfied, and there-fore their weights cancel out in the numerator and denomi-nator of Equation 3, and are irrelevant. Reflexivity ensures the proper treatment of groundings where y 1 = y 2 .)
It can also be useful to add the  X  X egative X  version of re-verse predicate equivalence:  X  x 1 ,x 2 ,y 1 ,y 2  X  HasWord ( x 1 ,y 1 )  X  HasWord ( x  X  x 1 ,x 2 ,y 1 ,y 2 HasWord ( x 1 ,y 1 )  X  X  HasWord ( x 2  X  x 1 ,x 2 ,y 1 ,y 2  X  HasWord ( x 1 ,y 1 )  X  X  HasWord ( x
Like other word-based similarity measures, this ap-proach has the disadvantage that it treats misspellings, vari-ant spellings and abbreviations of a word as completely different words. Since these are often a significant is-sue in entity resolution, it would be desirable to account for them. One way to do this efficiently is to compare word strings by the engrams they contain [19]. This can be done in our framework by defining the predicate HasEngram ( word , engram ) , which is true iff engram is a substring of word . (This predicate can be computed on the fly from its arguments, or pre-computed for all rele-vant word-engram pairs and given as evidence.) Applying reverse predicate equivalence to this predicate results in a logistic regression model for the equality of two words as a function of the number of engrams they have in common. Combined with the logistic regression for field equality, this produces a two-level similarity measure, comparing fields as sets of words, which are in turn compared as strings. Co-hen et al. [6] found such hybrid measures to outperform pure word-based and pure string-based ones for entity reso-lution. The maximum length of engrams to consider is a pa-rameter of the problem. Linear-interpolation smoothing is obtained by defining different predicates for engrams of dif-ferent length, with the corresponding weights for the corre-sponding versions of reverse predicate equivalence. (These can also be learned, using weight priors to avoid overfit-ting.) It is also possible to incorporate string edit distances like Levenshtein and Needleman-Wunsch [3] into an MLN. This involves stating formulas analogous to the recurrence relations used to compute these distances, with (negative) weights corresponding to the costs of insertion, deletion, etc. Pursuing this approach is an item for future work.
The Fellegi-Sunter model uses naive Bayes to predict whether two records are the same, with field comparisons as the predictors [14]. If the predictors are the field match predicates, Fellegi-Sunter is a special case of reverse predi-cate equality, with R as the relation between a field and the record it appears in (e.g., HasAuthor ( paper , author ) ). If the predictors are field similarities, measured by the num-ber of words present in both, either and none of the fields, Fellegi-Sunter is implemented by clauses of the form  X  x 1 ,x 2 ,y 1 ,y 2  X  HasWord ( x 1 ,y 1 )  X  HasWord ( x  X  x 1 ,x 2 ,y 1 ,y 2 HasWord ( x 1 ,y 1 )  X  X  HasWord ( x  X  x 1 ,x 2 ,y 1 ,y 2  X  HasWord ( x 1 ,y 1 )  X  X  HasWord ( x for each field-record relation R . The last rule corresponds to the class priors implemented as a unit clause.
The combination of Fellegi-Sunter with transitivity pro-duces McCallum and Wellner X  X  [28] conditional random field (CRF) model, with field matches or field similarities as the features. (A logistic regression model is a CRF where all the query variables are independent; transitive closure ren-ders them dependent. Discriminatively-trained MLNs can be viewed as relational extensions of CRFs.)
Reverse predicate equivalence applied to the relations in the database yields the CRF model of Singla and Domingos [42], with the field similarity measure described above in-stead of TF-IDF. It is also very similar to the CRF model of Culotta and McCallum [9]. The model of Dong et al. [12] is also of this type, but more ad hoc . All of these models have the property that they allow entities of multiple types to be resolved simultaneously, with inference about one pair of entities triggering inferences about related pairs of entities (e.g., if two papers are the same, their authors are the same; and vice-versa, albeit with lower weight).

We can also use coauthorship relations for entity resolu-tion, in the vein of Bhattacharya and Getoor [2], by defining the Coauthor ( x 1 ,x 2 ) predicate using the formula with infinite weight, and applying reverse predicate equiva-lence to it. (We can also explicitly state that coauthorship is reflexive and symmetric, but this is not necessary.) Notice that this approach increases the likelihood that two authors are the same even if they are coauthors of a third author on different papers. While this is still potentially useful, a presumably stronger regularity is captured by the clause This formula is related to reverse predicate equivalence, but (with suitably high weight) represents the non-linear in-crease in evidence that can occur when both the papers and the coauthors are the same.

So far, we have seen that an MLN with a small number of hand-coded formulas representing properties of equal-ity is sufficient to perform state-of-the-art entity resolution. However, one of the key features of the entity resolution problem is that a wide variety of knowledge, much of it domain-dependent, can (and needs to) be brought to bear on matching decisions. This knowledge can be incorporated into our approach by expressing it in first-order logic. For example, the constraints listed by Shen et al. [40] can all be incorporated into an MLN in this way. Weighted satisfiabil-ity then performs the role of relaxation labeling in Shen at al. It is also possible to incorporate formulas learned inde-pendently using ILP techniques, as in Davis et al. [10], or to refine the hand-coded formulas and construct additional ones using MLN structure learning [23].
In practice, even for fairly small databases there will be a large number of equality atoms to infer (e.g., 1000 con-stants of one type yield a million equality atoms). However, the vast majority of these will usually be false (i.e., non-matches). Scalability is typically achieved by performing inference only over plausible candidate pairs, identified us-ing a cheap similarity measure (e.g., TF-IDF computed us-ing a reverse index from words to fields). This approach can easily be incorporated into our framework simply by adding all the non-plausible matches to the evidence as false atoms. Most clauses involving these atoms will always be satisfied, and thus there is no need to ground them. We select plau-sible candidates using McCallum et al. X  X  canopy approach [26], with TF-IDF cosine as the similarity measure, but any other approach could be used. While some of the appar-ent non-matches might be incorrect, this is a necessary and very reasonable approximation. Notice that reverse predi-cate equivalence might be able to correct some of these er-rors, by indirectly inferring that two very different strings in fact represent the same object. We have developed a version of MaxWalkSAT and MCMC that lazily grounds predicates and clauses, effectively allowing predicates that are initially assumed false to be revisited, without incurring the compu-tational cost of completely grounding the network [43]. In-corporating this into our entity resolution system is an item for future work.
We used two publicly available citation databases in our experiments: Cora and BibServ. 6.1.1 Cora The hand-labeled Cora dataset is provided by McCallum 2 and has previously been used by Bilenko and Mooney [3] and others. This dataset is a collection of 1295 different ci-tations to computer science research papers from the Cora Computer Science Research Paper Engine. The original dataset contains only unsegmented citation strings. Bilenko and Mooney [3] segmented each citation into fields (au-thor, venue, title, publisher, year, etc.) using an informa-tion extraction system. We used this processed version of Cora. We further cleaned it up by correcting some labels. This cleaned version contains references to 132 different re-search papers. We used only the three most informative fields: first author, title and venue (with venue including conferences, journals, workshops, etc.). We compared the performance of the algorithms for the task of de-duplicating citations, authors and venues. For training and testing pur-poses, we hand-labeled the field pairs. The labeled data contains references to 50 authors and 103 venues. After forming canopies, the total number of match decisions was 61,177. 6.1.2 BibServ BibServ.org is a publicly available repository of about half a million pre-segmented citations. It is the result of merg-ing citation databases donated by its users, CiteSeer, and DBLP. We experimented on a subset of 10,000 records ex-tracted randomly from the user-donated subset of BibServ, which contains 21,805 citations. As in Cora, we used the first author, title and venue fields. In order to focus only on hard decisions, we discarded all the canopies of size less than 10. This left us with a total of 15,954 decisions. Since we lacked labeled data for BibServ, we used the parameters learned on Cora (with appropriate modifications) to perform inference on BibServ. Word stemming was used to identify the variations of the same underlying root word both in Cora and BibServ. We compared the following models in our experiments.
NB. This is the naive Bayes model as described in Sec-MLN(B). This is the basic MLN model closest to naive
MLN(B+C). This is obtained by adding reverse predicate
MLN(B+T). This is obtained by adding transitive clo-
MLN(B+C+T). This model has both reverse predicate
MLN(B+C+T+S). This model is obtained by adding rules
MLN(B+N+C+T). This model has a two-level learn-
MLN(G+C+T). This model is similar to MLN(B+C+T)
In the Cora domain, we performed five-fold cross valida-tion. In the BibServ domain, because of the absence of la-beled data, the naive Bayes model could not be learned. The weights of the inverse predicate equivalence rules for each word were fixed in proportion to the IDF of the word. This was used as a baseline for all the enhanced models. The weights of other rules were determined from the weights learned on Cora for the corresponding model.
For each model, we measured the conditional log-likelihood (CLL) and area under the precision-recall curve (AUC) for the match predicates. The advantage of the CLL is that it directly measures the quality of the probability es-timates produced. The advantage of the AUC is that it is in-sensitive to the large number of true negatives (i.e., ground atoms that are false and predicted to be false). The CLL of a set of predicates is the average over all their groundings of the ground atom X  X  log-probability given the evidence. The precision-recall curve for match predicates is computed by varying the threshold CLL above which a ground atom is predicted to be true. We computed the standard deviations of the AUCs using the method of Richardson and Domin-gos [36]. 6.4.1 Cora Table 1 shows the CLL and AUC for the various models on the Cora dataset. For the case of AUC in venues, there is monotonic increase in the performance as we add various collective inference features to the model, with the best-performing model being MLN(B+C+T). This trend shows how adding each feature in turn enhances the performance, giving the largest improvement when all the features are added to the model. For the case of CLL in venues, the performance tends to fluctuate as we add various collec-tive inference features, but the best-performing model is still MLN(B+C+T). For the case of citations, the results are similar, although the improvements as we add features are not as consistent as in case of AUC in venues. For AUC in authors, we have similar results, although the perfor-mance gain is much smaller compared to venues and cita-tions. There is more fluctuation in the case of CLL but the collective models are still the best.
 MLN(B+C+T+S) helps improve the performance of MLN(B+C+T) on citations and authors (both CLL and AUC) but does not help on venues.

MLN(B+N+C+T) improves performance on authors but not on citations and venues. This shows that while stem-ming can be a good way of identifying word duplicates in most cases, the engram model is quite helpful when deal-ing with fields such as authors, where initials are used for the complete word and can not be discovered by stemming alone.

MLN(G+C+T) (the model with global word rules) per-forms well for citations but less so for authors and venues. In general, per-word rule models seem to be particularly helpful when there are few words from which to infer the relationship between two entities. System CLL AUC CLL AUC CLL AUC
NB  X  0.637  X  0.010 0.913  X  0.000  X  0.133  X  0.021 0.986  X  0.000  X  0.747  X  0.017 0.738  X  0.002
MLN(B)  X  0.643  X  0.010 0.915  X  0.000  X  0.131  X  0.022 0.987  X  0.000  X  0.760  X  0.017 0.736  X  0.002
MLN(B+C)  X  0.809  X  0.012 0.891  X  0.000  X  0.386  X  0.064 0.968  X  0.000  X  1.163  X  0.034 0.741  X  0.001
MLN(B+T)  X  0.369  X  0.003 0.949  X  0.000  X  0.213  X  0.036 0.994  X  0.000  X  1.036  X  0.029 0.745  X  0.002
MLN(B+C+T)  X  0.597  X  0.007 0.964  X  0.000  X  0.171  X  0.043 0.984  X  0.000  X  0.704  X  0.023 0.828  X  0.002
MLN(B+C+T+S)  X  0.503  X  0.006 0.988  X  0.000  X  0.100  X  0.033 0.992  X  0.000  X  0.874  X  0.027 0.807  X  0.002
MLN(B+N+C+T)  X  0.879  X  0.008 0.952  X  0.000  X  0.096  X  0.032 0.992  X  0.000  X  0.781  X  0.023 0.817  X  0.002
MLN(G+C+T)  X  0.394  X  0.004 0.973  X  0.000  X  0.263  X  0.053 0.980  X  0.000  X  1.196  X  0.031 0.743  X  0.002 6.4.2 BibServ Since we did not have labeled data for BibServ, we hand-labeled 300 pairs each for citations and venues, randomly selected from the set where the MAP prediction for at least one of the algorithms was different from the others. For au-thors, we labeled all the potential match decisions, as there were only 240 of them. The results reported below are over these hand-labeled pairs.

Table 2 shows the CLL and AUCs for the various algo-rithms on the BibServ dataset. As in the case of Cora, for ci-tations and authors, the best-performing models are the ones involving collective inference features. MLN(B+N+C+T) gives the best performance for both CLL and AUC in au-thors, for the reasons cited before. It is also the best-performing model for AUC in citations, closely followed by other collective models. MLN(B+C) gives the best per-formance for CLL in citations.

Whereas MLN(G+C+T) performs quite poorly on cita-tions and authors, it in fact gives the best performance on CLL in venues. On AUC in venues, MLN(G+C+T) and MLN(B) are the two best performers. Overall, even though BibServ and Cora have quite different characteristics, ap-plying the parameters learned on Cora to BibServ still gives good results. Collective inference is clearly useful, except for AUC on venues, where the results are inconclusive.
Directions for future work include:  X  Using the EM algorithm applied to MLNs to learn  X  Allowing context-dependent match decisions (e.g., J.  X  Specializing reverse predicate equivalence for one-to- X  Explicitly representing entities (and not just references  X  Integrating further components and aspects of the en- X  Applying our approach to other entity resolution do- X  Extending our approach to schema and ontology  X  Extending our approach to other types of data be- X  Generalizing across database sizes by learning weights  X  Combining entity resolution and data mining in one
This paper proposes a unifying framework for entity res-olution. We show how a small number of axioms in Markov logic capture the essential features of many different ap-proaches to this problem, in particular non-i.i.d. ones, as well as the original Fellegi-Sunter model. Experiments on two citation databases evaluate the contributions of these System CLL AUC CLL AUC CLL AUC
MLN(B)  X  0.008  X  0.003 0.997  X  0.001  X  0.586  X  0.114 0.910  X  0.013  X  0.806  X  0.121 0.908  X  0.011
MLN(B+C)  X  0.001  X  0.000 0.999  X  0.000  X  0.544  X  0.113 0.887  X  0.007  X  1.166  X  0.151 0.876  X  0.012
MLN(B+T)  X  0.006  X  0.003 0.993  X  0.003  X  0.600  X  0.116 0.909  X  0.013  X  0.827  X  0.123 0.898  X  0.010
MLN(B+C+T)  X  0.006  X  0.004 0.998  X  0.000  X  0.473  X  0.105 0.928  X  0.009  X  1.146  X  0.149 0.876  X  0.012
MLN(B+C+T+S)  X  0.006  X  0.004 0.970  X  0.020  X  0.486  X  0.107 0.926  X  0.010  X  1.133  X  0.148 0.876  X  0.012
MLN(B+N+C+T)  X  0.018  X  0.005 1.000  X  0.000  X  0.363  X  0.091 0.940  X  0.008  X  0.936  X  0.133 0.897  X  0.012
MLN(G+C+T)  X  0.735  X  0.101 0.491  X  0.000  X  4.679  X  0.256 0.432  X  0.001  X  0.716  X  0.112 0.906  X  0.012 approaches, and illustrate how Markov logic enables us to easily build a sophisticated entity resolution system. This research was partly funded by DARPA contract NBCHD030010/02-000225, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313. The views and conclusions contained in this docu-ment are those of the authors and should not be interpreted as necessarily representing the official policies, either ex-pressed or implied, of DARPA, NSF, ONR, or the United States Government.

