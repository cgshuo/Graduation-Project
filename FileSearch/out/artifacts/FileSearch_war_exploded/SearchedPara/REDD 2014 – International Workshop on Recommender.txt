 Evaluation is a cardinal issue in recommender systems; as in any algorithmic research and development in the community. Yet, in the field of recommender systems, there still exists considerable disparity in evaluation methods, metrics and experimental designs, as well as a significa nt mismatch between evaluation methods in the lab and what constitutes an effective recommendation for real ha ve been defined, a clear evaluation protocol should be specified and experiments conducted by different authors. This would enable any contribution to the same problem to be incremental and add up on top of previous work, rather than grow sideways. The REDD 2014 workshop seeks to provide an informal forum to tackle such issues and to move towards better understood and shared evaluation methodologies, allowing one to leverage the efforts and the workforce of the academic community towards meaningful and relevant directions in real-world developments. H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms, Measurement, Performance, Experimentation, Standardization, Theory Utility, evaluation, methodology, metrics, recommender systems Thoughtful evaluation of recommender systems is a key cha l-lenge, as it guides algorithmic research and development [3,9]. In the community there is, however, considerable disparity in evaluation methods, metrics and experimental designs, as well as a significant mismatch between evaluation methods in the lab and what constitutes an effective recommendation for real users and businesses [5,6,9,11,12,13,15]. On the one hand, REDD 2014 places a specific focus on the identification and measurement of different recommendation quality dimensions that go beyond the monolithic concept of simply matching user preferences. Novelty and diversity, for instance, have been recognized as key perspectives of the utility of recommendations for users in real-world scenarios, with a direct Considering the business perspectiv e, performance metrics related recommendation funnel should also be used . Additionally, from internal algorithms at the core of the system. On the other hand, once a relevant quality dimension has been enables different authors to build on top of other researchers X  previous works . Even when measuring recommendation accuracy, experimental design questions for which there are not always precise and consensual answers. Therefore, there remains room for further methodological development and convergence, which motivates this workshop. REDD 2014 gathered researchers and practitioners interested in better understanding the unmet needs in the field in terms of evaluation methodologies and experimental practices. The workshop provided an informal setting for exchanging and discussing ideas as well as sharing experiences and viewpoints. REDD sought to identify and better understand the current gaps in recommender system evaluation methodologies, help lay consolidation and convergence of experimental methods and best practices. Specific questions raised and addressed by the workshop include, among others, the following:  X  What are the unmet needs and challenges for evaluation in the 
Recommender Systems field? Where do we stand? What changes would we like to see? How could we speed up progress?  X  What relevant recommendation utility and quality dimensions should be considered? How can they be captured and measured? How should evaluation methods be designed to effectively evaluate such dimensions?  X  How can metrics become more clearly and/or formally related to the task, contexts, and goals for which a recommender application is deployed?  X  How should IR metrics be applied to recommendation tasks? 
What aspects require adjustment or further clarification? What further methodologies should we draw from other disciplines (e.g., HCI, Machine Learning, etc.)?  X  What biases and noise should experimental design typically watch for?  X  Can we predict the success of a recommendation algorithm with offline experiments? What offline metrics correlate better and under which conditions?  X  What are the outreach and limitations of offline evaluation? 
How can online and offline experiments complement each other?  X  What type of public datasets and benchmarks would we want to have available, and how can they be built?  X  How can the recommendation effect be traced on business outcomes?  X  How should the academic evaluation methodologies improve their relevance and usefulness for industrial settings?  X  How can we promote reproducibility of recommender systems methods?  X  How do we envision the evaluation of recommender systems in the future? The accepted papers and the discussions held at the workshop addressed, among others, the following topics:  X  Evaluation methodology  X  Experimental design  X  Open evaluation platforms and infrastructures  X  Recommendation quality dimensions: accuracy, novelty, diversity, unexpectedness, serendipity, coverage, risk, robustness, usability, explanations, persuasiveness, etc.  X  Evaluating for efficiency and scalability  X  Definition and assessment of evaluation metrics  X  Matching metrics to tasks, needs, and goals  X  Business-oriented evaluation  X  Offline and online evaluation  X  Datasets and benchmarks The workshop opened with a keynote talk, followed by the presentation of accepted papers and open discussions. The accepted papers and a summary of discussions are available in the workshop proceedings, which can be reached from the workshop we bsite at http://ir.ii.uam.es/redd 201 4. [1] Adamopoulos, P. and Tuzhilin, A. On unexpectedness in [2] Adomavicius, G. and Kwon, Y. Improving aggregate [3] Breese, J. S., Heckerman, D., and Kadie, C. Empirical anal y-[4] Celma, O. and Herrera, P. A New Approach to Evaluating [5] Cremonesi, P., Garzotto, F., Negro, S., Papadopoulos, A. V., [6] Cremonesi, P., Koren, Y., and Turrin, R. Performance of [7] Fleder, D. M. and Hosanagar , K . Blockbuster Culture X  X  Ne xt [8] Ge, M. , Delgado-Battenfeld, C. and Jannach, D. Beyond [9] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. [10] Lathia, N., Hailes , S. , Capra , L. , and Amatriain , X . Temporal [11] McNee, S. M. , Riedl, J. , and Konstan, J. A. Being Accurate [12] Shani, G. and Gunawardana, A. Evaluating Recommendation [13] Steck, H. Training and Testing of Recommender Systems on [14] Vargas , S. and Castells, P . Rank and Relevance in Novelty [15] Voorhees, E. M. and Harman, D. K. TREC: Experiment and 
