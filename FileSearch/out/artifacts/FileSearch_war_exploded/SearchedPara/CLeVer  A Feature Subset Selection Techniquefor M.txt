 Feature subset selection (FSS) is one of the techniques to pre-process the data before we perform any data mining tasks, e.g., classification or clustering. FSS is to identify a subset of original features from a given dataset while removing irrelevant and/or redundant features [1]. The objectives of FSS are to improve the prediction performance of the predictors, to provide faster and more cost-effective predictors, and to provide a better understanding of the underlying process that generated the data [2].
 the subsequent processes. Hence, only the data generated from those features need to be collected ignoring all the other features. This makes FSS different from feature extraction , where the correspondence information to the original features are in general not maintained, so all the original variables are required to be measured.
 made sequentially through time where i indexes the measurements made at each time point t [3]. It is called a univariate time series when n is equal to 1, and a multivariate time series (MTS) when n is equal to, or greater than 2. An MTS item is naturally represented in an m  X  n matrix, where m is the number of observations and n is the number of variables , e.g., sensors. However, the state of the art FSS techniques, such as Recursive Feature elimination (RFE) [2], require each item to be represented in one row. Consequently, to utilize these techniques on MTS datasets, each MTS item needs to be first transformed into one row or column vector. However, since each of variables is considered separately during this vectorization , the correlation information among the features might be lost in the previous FSS method for MTS datasets [4].
 tivariate time series (MTS) 1 named CL e V er (descriptive C ommon principal component L oading based V ariable subset selection method). CL e V er utilizes the property of the principal components and common principal components to retain the correlation information among the variables. CL e V er is a novel variable subset selection method for multivariate time series (MTS) based on common principal component analysis (CPCA) [5, 6]. Figure 1 illustrates the entire process of CL e V er , which involves three phases: (1) prin-cipal components (PCs) computation per MTS item, (2) descriptive common principal components (DCPCs) computation per label 2 and their concatenation, and (3) variable subset selection using K -means clustering on DCPC loadings of variables. Each of these phases is described in the subsequent sections. Table 1 lists the notations used in the remainder of this paper, if not specified otherwise. 2.1 PC and DCPC Computations The first and second phases (except the concatenation) of CL e V er are incor-porated into Algorithm 1. It obtains both PCs and then DCPCs consecutively. The required input to Algorithm 1 is a set of MTS items with the same label. are adequate for the purpose of representing each MTS item, are taken into consideration. Algorithm 1 takes in the threshold (  X  ) to determine p .Thatis, for each input MTS item, p is determined to be the minimum value such that the ratio of the variances explained by its first p PCs to the total variance exceeds the provided threshold  X  for the first time (Lines 3  X  10). Since the MTS items can have different values for p , p is finally determined as their maximum value (Line 11).
 them be denoted as L i ( i =1 ,...,N ). Then, the DCPCs that agree most closely with all N sets of p PCs are successively defined by the eigenvectors of the matrix rows of V T are eigenvectors of H and the first p of them define pDCPC sfor N MTS items. This computation of DCPC is captured by Lines 16  X  17.
 Algorithm 1. ComputeDCP C : PC and DCPC Computations 2.2 Variable Subset Selection CL e V er utilizes a clustering method to group the similar variables together and finally to select the least redundant variables. First, the DCPCs per label are computed by Algorithm 1 and are concatenated column-wise if the MTS dataset has more than one label. Subsequently, K -means clustering is performed on the columns of the concatenated DCPC loadings, each column of which holds the one-to-one correspondence to the original variables. Then, the column vectors with the similar pattern of contributions to each of the DCPCs will be clustered together.
 tives of clusters. Once the clustering is done, one column vector closest to the centroid vector of each cluster is chosen as the representative of that cluster. The other columns within each cluster therefore can be eliminated. Finally, the corresponding original variable to the selected column is identified, which will form the selected subset of variables with the least redundant and possibly the most related information for the given K . For details, please refer to [6]. We evaluate the effectiveness of CL e V er in terms of classification performance and processing time. We conducted several experiments on three real-world datasets: HumanGait, BCAR, and BCI MPI. After obtaining a subset of vari-ables using CL e V er , we performed classification using Support Vector Machine (SVM) with linear kernel and leave-one-out cross validation for BCAR and 10 fold stratified cross validation for the other two datasets. Subsequently, we com-pared the performance of CL e V er with those of Recursive Feature Elimination (RFE) [2], Fisher Criterion (FC), Exhaustive Search Selection (ESS), and using all the available variables (ALL). The algorithm of CL e V er for the experiments is implemented in Matlab TM . SVM classification is completed with LIBSVM [7]. Classification Performance. For the MTS dataset to be fed into SVM, each MTS item is vectorized using the upper triangle of its correlation matrix for CL e V er , Exhaustive Search Selection (ESS), and using all the variables (ALL). For RFE and FC, we vectorized each MTS item as in [4]. That is, each variable is represented as the autoregressive (AR) fit coefficients of order 3 using the forward backward linear prediction [8].
 dataset. The X axis is the number of selected subset of variables, i.e., the number of clusters K , and the Y axis is the classification accuracy. It shows that a subset of 27 variables selected by CL e V er out of 66 performs the same as the one using all the variables, which is 99.4% accuracy. The 27 variables selected by CL e V er are from only 16 markers (marked with a filled circle in Figure 2(b)) out of 22, which would mean that the values generated by the remaining 6 markers does not contribute much to the identification of the person. The performances by RFE and FC is much worse than the ones using CL e V er . Even when using all the variables, the classification accuracy is around 55%. Figure 2(c) illustrates CL e V er consistently outperforms RFE and FC when the number of selected features is more than 4. The 7 variables selected by CL e V er produce about 100% classification accuracy, which is even better than using all the 11 variables which is represented as a horizontal solid line. This implies that CL e V er never eliminates useful information in its variable selection process. Figure 2(d) repre-sents the performance comparison using the BCI MPI dataset 3 . It depicts that when the number of selected variables is less than 10, RFE performs better than CL e V er and FC technique. When the number of selected variables is greater than 10, however, CL e V er performs far better than RFE. Using the 17 variables selected by CL e V er , the classification accuracy is 72.85%, which is very close to the performance of MIC 17 , i,e., the known 17 motor imagery channels, whose accuracy is 73.65%.
 Processing Time. The processing time for CL e V er includes the time to per-form Algorithm 1 and the average time to perform the clustering and obtain the variable subsets while varying K from 2 to the number of all variables for each dataset. The processing time for RFE and FC includes the time to obtain 3 autoregressive fit coefficients and perform the feature subset selection. Overall, CL e V er takes up to 2 orders of magnitude less time than RFE, while performing better than RFE up to about a factor of two. The detailed results are provided in [6].

