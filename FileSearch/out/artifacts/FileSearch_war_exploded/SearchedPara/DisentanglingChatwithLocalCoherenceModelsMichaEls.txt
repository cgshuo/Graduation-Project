 One property of a well-written document is coher-ence , the way each sentence ts into its context X  sen-tences should be interpretable in light of what has come before, and in turn make it possible to inter-pret what comes after. Models of coherence have primarily been used for text-based generation tasks: ordering units of text for multidocument summariza-tion or inserting new text into an existing article. In general, the corpora used consist of informative writing, and the tasks used for evaluation consider different ways of reordering the same set of textual units. But the theoretical concept of coherence goes beyond both this domain and this task setting X  and so should coherence models.

This paper evaluates a variety of local coher-ence models on the task of chat disentanglement or  X threading X : separating a transcript of a multiparty interaction into independent conversations 1 . Such simultaneous conversations occur in internet chat rooms, and on shared voice channels such as push-to-talk radio. In these situations, a single, correctly disentangled, conversational thread will be coherent, since the speakers involved understand the normal rules of discourse, but the transcript as a whole will not be. Thus, a good model of coherence should be able to disentangle sentences as well as order them.
There are several differences between disentan-glement and the newswire sentence-ordering tasks typically used to evaluate coherence models. Inter-net chat comes from a different domain, one where topics vary widely and no reliable syntactic annota-tions are available. The disentanglement task mea-sures different capabilities of a model, since it com-pares documents that are not permuted versions of one another. Finally, full disentanglement requires a large-scale search, which is computationally dif-cult. We move toward disentanglement in stages, carrying out a series of experiments to measure the contribution of each of these factors.

As an intermediary between newswire and inter-pus. S W B D contains recorded telephone conversa-tions with known topics and hand-annotated parse trees; this allows us to control for the performance of our parser and other informational resources. To compare the two algorithmic settings, we use S W B D for ordering experiments, and also articially entan-gle pairs of telephone dialogues to create synthetic transcripts which we can disentangle. Finally, we present results on actual internet chat corpora.
On synthetic S W B D transcripts, local coherence models improve performance considerably over our baseline model, Elsner and Charniak (2008b). On internet chat, we continue to do better on a con-strained disentanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrat-ing prior information, our improvements on S W B D could transfer fully to IRC chat. There is extensive previous work on coherence mod-els for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome.

In addition to text ordering, local coherence mod-els have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from or-dering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility.

The task of disentanglement or  X threading X  for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # L I N U X corpus; the best published re-sults on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion tech-nique to incorporate context beyond a single sen-tence. Unigram overlaps are used to model coher-ence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (La-pata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available imple-
Adams (2008) also created and released a disen-tanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measur-ing similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisen-stein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difcult to nd useful topics for IRC chat.

A few studies have attempted to disentangle con-versational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party prob-lem, the task of attending to a specic speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some inuence on what the listener per-ceives, but only for extremely salient cues such as the listener's name (Moray, 1959), so cocktail party research does not typically use lexical models. In this section, we briey describe the models we in-tend to evaluate. Most of them are drawn from pre-vious work; one, the topical entity grid, is a novel extension of the entity grid. For the experiments be-low, we train the models on S W B D , sometimes aug-mented with a larger set of automatically parsed con-versations from the F I S H E R corpus. Since the two for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue because we do not have enough disentangled train-ing data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some prin-ciples of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), ob-ject (O), other (X) and not present (-). In each new utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For in-stance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total, the grid predicts its role in sentence 3 accord-ing to the conditional P ( j S; O ; sal =4) .
As in previous work, we treat each noun in a doc-ument as denoting a single entity, rather than using a coreference technique to attempt to resolve them. In our development experiments, we noticed that coreferent nouns often occur farther apart in conver-sation than in newswire, since they are frequently referred to by pronouns and deictics in the interim. Therefore, we extend the history to six previous ut-terances. For robustness with this long history, we model the conditional probabilities using multilabel logistic regression rather than maximum likelihood. This requires the assumption of a linear model, but makes the estimator less vulnerable to overtting due to sparsity, increasing performance by about 2% in development experiments. 3.2 Topical entity grid This model is a variant of the generative entity grid, intended to take into account topical informa-tion. To create the topical entity grid, we learn a set of topic-to-word distributions for our corpus us-ing LDA (Blei et al., 2001) 3 with 200 latent top-ics. This model embeds our vocabulary in a low-dimensional space: we represent each word w as the vector of topic probabilities p ( t i j w ) . We ex-perimented with several ways to measure relation-ships between words in this space, starting with the standard cosine. However, the cosine can depend on small variations in probability (for instance, if w has most of its mass in dimension 1, then it is sensitive to the exact weight of v for topic 1, even if this es-sentially never happens).

To control for this tendency, we instead use the magnitude of the dimension of greatest similarity:
To model coherence, we generalize the binary his-tory features of the standard entity grid, which de-tect, for example, whether entity e is the subject of the previous sentence. In the topical entity grid, we instead compute a real-valued feature which sums up the similarity between entity e and the subject(s) of the previous sentence.
 These features can detect a transition like:  X The House voted yesterday. The Senate will consider the bill today. X . If  X House X  and  X Senate X  have a high similarity, then the feature will have a high value, predicting that  X Senate X  is a good subject for the cur-rent sentence. As in the previous section, we learn the conditional probabilities with logistic regression; we train in parallel by splitting the data and averag-ing (Mann et al., 2009). The topics are trained on F
I S H E R , and on N A N C for news. 3.3 IBM-1 The IBM translation model was rst considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to gen-erate the content words of the next sentence by trans-lating them from the words of the previous sentence, plus a null word; thus, it will learn alignments be-tween pairs of words that tend to occur in adjacent sentences. We learn parameters on the F I S H E R pus, and on N A N C for news. 3.4 Pronouns The use of a generative pronoun resolver for co-herence modeling originates in Elsner and Char-niak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsuper-vised model which they also make publicly available (Charniak and Elsner, 2009) 4 . They model each pro-noun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, sig-naling that the text is less coherent because the pro-noun is hard to interpret correctly.

We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F I S H E R 3.5 Discourse-newness Building on work from summarization (Nenkova and McKeown, 2003) and coreference resolution (Poesio et al., 2005), Elsner and Charniak (2008a) use a model which recognizes discourse-new versus old NPs as a coherence model. For instance, the model can learn that  X President Barack Obama X  is a more likely rst reference than  X Obama X . Follow-ing their work, we score discourse-newness with a maximum-entropy classier using syntactic features counting different types of NP modiers, and we use NP head identity as a proxy for coreference. 3.6 Chat-specic features Most disentanglement models use non-linguistic in-formation alongside lexical features; in fact, times-tamps and speaker identities are usually better cues than words are. We capture three essential non-linguistic features using simple generative models.
The rst feature is the time gap between one utter-ance and the next within the same thread. Consistent short gaps are a sign of normal turn-taking behavior; long pauses do occur, but much more rarely (Aoki et al., 2003). We round all time gaps to the nearest sec-ond and model the distribution of time gaps using a histogram, choosing bucket sizes adaptively so that each bucket contains at least four datapoints.
The second feature is speaker identity; conver-sations usually involve a small subset of the to-tal number of speakers, and a few core speakers make most of the utterances. We model the distri-bution of speakers in each conversation using a Chi-nese Restaurant Process (CRP) (Aldous, 1985) (tun-ing the dispersion to maximize development pe-formance). The CRP's  X rich-get-richer X  dynamics capture our intuitions, favoring conversations domi-nated by a few vociferous speakers.

Finally, we model name mentioning . Speakers in IRC chat often use their addressee's names to co-ordinate the chat (O'Neill and Martin, 2003), and this is a powerful source of information (Elsner and Charniak, 2008b). Our model classies each utter-ance into either the start or continuation of a conver-sational turn, by checking if the previous utterance had the same speaker. Given this status, it computes probabilities for three outcomes: no name mention, a mention of someone who has previously spoken in the conversation, or a mention of someone else. (The third option is extremely rare; this accounts for most of the model's predictive power). We learn these probabilities from IRC training data. 3.7 Model combination To combine these different models, we adopt the log-linear framework of Soricut and Marcu (2006). Here, each model P i is assigned a weight i , and the combined score P ( d ) is proportional to:
The weights can be learned discriminatively, maximizing the probability of d relative to a task-specic contrast set. For ordering experiments, the contrast set is a single random permutation of d ; we explain the training regime for disentanglement be-low, in subsection 4.1. To measure the differences in performance caused by moving from news to a conversational domain, we rst compare our models on an ordering task, discrimination (Barzilay and Lapata, 2005; Karama-nis et al., 2004). In this task, we take an original document and randomly permute its sentences, cre-ating an articial incoherent document. We then test to see if our model prefers the coherent original.
For S W B D , rather than compare permutations of the individual utterances, we permute conversa-tional turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 X  3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we also show results for the same mod-els on W S J , using the train-test split from Elsner and Charniak (2008a); the test set is sections 14-24, to-talling 1004 documents.

Purandare and Litman (2008) carry out similar ex-periments on distinguishing permuted S W B D doc-uments, using lexical and WordNet features in a model similar to Lapata (2003). Their accuracy for this task (which they call  X switch-hard X ) is roughly 68%. EGrid 76.4 z 86.0 Topical EGrid 71.8 z 70.9 z IBM-1 77.2 z 84.9 y Pronouns 69.6 z 71.7 z Disc-new 72.3 z 55.0 z Combined 81.9 88.4 -EGrid 81.0 87.5 -Topical EGrid 82.2 90.5 -IBM-1 79.0 z 88.9 -Pronouns 81.3 88.5 -Disc-new 82.2 88.4
In Table 1, we show the results for individual models, for the combined model, and ablation re-sults for mixtures without each component. W S J is more difcult than S W B D overall because, on av-erage, news articles are shorter than S W B D con-versations. Short documents are harder, because permuting disrupts them less. The best S W B D re-sult is 91%; the best W S J result is 82% (both for mixtures without the topical entity grid). The W S J result is state-of-the-art for the dataset, improving slightly on Elsner and Charniak (2008a) at 81%. We test results for signicance using the non-parametric Mann-Whitney U test.

Controlling for the fact that discrimination is eas-ier on S W B D , most of the individual models perform similarly in both corpora. The strongest models in both cases are the entity grid and IBM-1 (at about 77% for news, 85% for dialogue). Pronouns and the topical entity grid are weaker. The major outlier is the discourse-new model, whose performance drops from 72% for news to only 55%, just above chance, for conversation.

The model combination results show that all the models are quite closely correlated, since leaving out any single model does not degrade the combi-nation very much (only one of the ablations is sig-nicantly worse than the combination). The most critical in news is IBM-1 (decreasing performance by 3% when removed); in conversation, it is the entity grid (decreasing by about 1%). The topical entity grid actually has a (nonsignicant) negative impact on combined performance, implying that its predictive power in this setting comes mainly from information that other models also capture, but that it is noisier and less reliable. In each domain, the combined models outperform the best single model, showing the information provided by the weaker models is not completely redundant.

Overall, these results suggest that most previ-ously proposed local coherence models are domain-general; they work on conversation as well as news. The exception is the discourse-newness model, which benets most from the specic con-ventions of a written style. Full names with titles (like  X President Barack Obama X ) are more common in news, while conversation tends to involve fewer completely unfamiliar entities and more cases of bridging reference, in which grounding information is given implicitly (Nissim, 2006). Due to its poor performance, we omit the discourse-newness model in our remaining experiments. We now turn to the task of disentanglement, test-ing whether models that are good at ordering also do well in this new setting. We would like to hold the domain constant, but we do not have any disen-tanglement data recorded from naturally occurring speech, so we create synthetic instances by merging pairs of S W B D dialogues. Doing so creates an arti-cial transcript in which two pairs of people appear to be talking simultaneously over a shared channel.
The situation is somewhat contrived in that each pair of speakers converses only with each other, never breaking into the other pair's dialogue and rarely using devices like name mentioning to make it clear who they are addressing. Since this makes speaker identity a perfect cue for disentanglement, we do not use it in this section. The only chat-specic model we use is time .

Because we are not using speaker information, we remove all utterances which do not contain a noun before constructing synthetic transcripts X  these are mostly backchannels like  X Yeah X . Such utterances cannot be correctly assigned by our coherence mod-els, which deal with content; we suspect most of them could be dealt with by associating them with the nearest utterance from the same speaker.
Once the backchannels are stripped, we can cre-ate a synthetic transcript. For each dialogue, we rst simulate timestamps by sampling the number of sec-onds between each utterance and the next from a dis-cretized Gaussian: b N (0 ; 2 : 5) c . The interleaving of the conversations is dictated by the timestamps. We truncate the longer conversation at the length of the shorter; this ensures a baseline score of 50% for the degenerate model that assigns all utterances to the same conversation.

We create synthetic instances of two types X  those where the two entangled conversations had differ-ent topical prompts and those where they were the same. (Each dialogue in S W B D focuses on a prese-lected topic, such as shing or movies .) We entangle dialogues from our ordering development set to use for mixture training and validation; for testing, we use 100 instances of each type, constructed from di-alogues in our test set.

When disentangling, we treat each thread as inde-pendent of the others. In other words, the probability of the entire transcript is the product of the probabil-ities of the component threads. Our objective is to nd the set of threads maximizing this. As a com-parison, we use the model of Elsner and Charniak (2008b) as a baseline. To make their implementa-tion comparable to ours, in this section we constrain it to nd only two threads. 5.1 Disentangling a single utterance Our rst disentanglement task is to correctly assign a single utterance, given the true structure of the rest of the transcript. For each utterance, we compare two versions of the transcript, the original, and a version where it is swapped into the other thread. Our accuracy measures how often our models prefer the original. Unlike full-scale disentanglement, this task does not require a computationally demanding search, so it is possible to run experiments quickly. We also use it to train our mixture models for disen-tanglement, by construct a training example for each utterance i in our training transcripts. Since the El-sner and Charniak (2008b) model maximizes a cor-relation clustering objective which sums up indepen-dent edge weights, we can also use it to disentangle a single sentence efciently.

Our results are shown in Table 2. Again, re-sults for individual models are above the line, then EGrid 80.2 72.9 76.6 Topical EGrid 81.7 73.3 77.5 IBM-1 70.4 66.7 68.5 Pronouns 53.1 50.1 51.6 Time 58.5 57.4 57.9 Combined 86.8 79.6 83.2 -EGrid 86.0 79.1 82.6 -Topical EGrid 85.2 78.7 81.9 -IBM-1 86.2 78.7 82.4 -Pronouns 86.8 79.4 83.1 -Time 84.5 76.7 80.6 E+C `08 78.2 73.5 75.8 our combined model, and nally ablation results for mixtures omitting a single model. The results show that, for a pair of dialogues that differ in topic, our best model can assign a single sentence with 87% accuracy. For the same topic, the accuracy is 80%. In each case, these results improve on (Elsner and Charniak, 2008b), which scores 78% and 74%.
Changing to this new task has a substantial im-pact on performance. The topical model, which per-formed poorly for ordering, is actually stronger than the entity grid in this setting. IBM-1 underperforms either grid model (69% to 77%); on ordering, it was nearly as good (85% to 86%).

Despite their ordering performance of 72%, pro-nouns are essentially useless for this task, at 52%. This decline is due partly to domain, and partly to task setting. Although S W B D contains more pronominals than W S J , many of them are rst and second-person pronouns or deictics, which our model does not attempt to resolve. Since the disen-tanglement task involves moving only a single sen-tence, if moving this sentence does not sever a re-solvable pronoun from its antecedent, the model will be unable to make a good decision.

As before, the ablation results show that all the models are quite correlated, since removing any sin-gle model from the mixture causes only a small de-crease in performance. The largest drop (83% to 81%) is caused by removing time; though time is a weak model on its own, it is completely orthogo-nal to the other models, since unlike them, it does not depend on the words in the sentences.

Comparing results between  X different topic X  and  X same topic X  instances shows that  X same topic X  is harder X  by about 7% for the combined model. The IBM model has a relatively small gap of 3.7%, and in the ablation results, removing it causes a larger drop in performance for  X same X  than  X different X ; this suggests it is somewhat more robust to similar-ity in topic than entity grids.

Disentanglement accuracy is hard to predict given ordering performance; the two tasks plainly make different demands on models. One difference is that the models which use longer histories (the two entity grids) remain strong, while the models considering only one or two previous sentences (IBM and pro-nouns) do not do as well. Since the changes being considered here affect only a single sentence, while permutation affects the entire transcript, more his-tory may help by making the model more sensitive to small changes. 5.2 Disentangling an entire transcript We now turn to the task of disentangling an entire transcript at once. This is a practical task, motivated by applications such as search and information re-trieval. However, it is more difcult than assign-ing only a single utterance, because decisions are interrelated X  an error on one utterance may cause a cascade of poor decisions further down. It is also computationally harder.

We use tabu search (Glover and Laguna, 1997) to nd a good solution. The search repeatedly nds and moves the utterance which would most improve the model score if swapped from one thread to the other. Unlike greedy search, tabu search is constrained not to repeat a solution that it has recently visited; this forces it to keep exploring when it reaches a local maximum. We run 500 iterations of tabu search (usually nding the rst local maximum after about 100) and return the best solution found.

We measure performance with one-to-one over-lap, which maps the two clusters to the two gold dialogues, then measures percent correct 5 . Our re-sults (Table 3) show that, for transcripts with dif-ferent topics, our disentanglement has 68% over-EGrid 60.3 57.1 58.7 Topical EGrid 62.3 56.8 59.6 IBM-1 56.5 55.2 55.9 Pronouns 54.5 54.4 54.4 Time 55.4 53.8 54.6 Combined 67.9 59.8 63.9 E+C `08 59.1 57.4 58.3 lap with truth, extracting about two thirds of the structure correctly; this is substantially better than Elsner and Charniak (2008b), which scores 59%. Where the entangled conversations have the same topic, performance is lower, about 60%, but still bet-ter than the comparison model with 57%. Since cor-relations with the previous section are fairly reliable, and the disentanglement procedure is computation-ally intensive, we omit ablation experiments.
As we expect, full disentanglement is more dif-cult than single-sentence disentanglement (com-bined scores drop by about 20%), but the single-sentence task is a good predictor of relative perfor-mance. Entity grid models do best, the IBM model remains useful, but less so than for discrimination, and pronouns are very weak. The IBM model per-forms similarly under both metrics (56% and 57%), while other models perform worse on l oc 3 . This supports our suggestion that IBM's decline in per-formance from ordering is indeed due to its using a single sentence history; it is still capable of getting local structures right, but misses global ones. In this section, we move from synthetic data to real multiparty discourse recorded from internet chat rooms. We use two datasets: the # L I N U X corpus (Elsner and Charniak, 2008b), and three larger cor-2008). We use the 1000-line  X development X  sec-tion of # L I N U X for tuning our mixture models and the 800-line  X test X  section for development experi-ments. We reserve the Adams (2008) corpora for testing; together, they consist of 19581 lines of chat, with each section containing 500 to 1000 lines.
Chat-specic 74.0 +EGrid 79.3 +Topical EGrid 76.8 +IBM-1 76.3 +Pronouns 73.9 +EGrid/Topic/IBM-1 78.3 E+C `08b 76.4
In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typi-cal for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag  X lol X ,  X haha X  and  X yes X  as UH (rather than NN, NNP and JJ).

In this section, we use all three of our chat-specic models (sec. 2.0.6; time , speaker and men-tion ) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentan-gling a single sentence, given the correct structure of the rest of the transcript. We average perfor-mance on each transcript over the different annota-tions, then average the transcripts, weighing them by length to give each utterance equal weight.
Table 4 gives results on our development corpus, #
L I N U X . Our best result, for the chat-specic fea-tures plus entity grid, is 79%, improving on the com-parison model, Elsner and Charniak (2008b), which gets 76%. (Although the table only presents an av-erage over all annotations of the dataset, this model is also more accurate for each individual annota-tor than the comparison model.) We then ran the same model, chat-specic features plus entity grid, on the test corpora from Adams (2008). These re-sults (Table 5) are also better than Elsner and Char-niak (2008b), at an average of 93% over 89%.
As pointed out in Elsner and Charniak (2008b), the chat-specic features are quite powerful in this domain, and it is hard to improve over them. Elsner and Charniak (2008b), which has simple lexical fea-tures, mostly based on unigram overlap, increases +EGrid 92.3 96.6 91.1 E+C `08b 89.0 90.2 88.4 performance over baseline by 2%. Both IBM and the topical entity grid achieve similar gains. The en-tity grid does better, increasing performance to 79%. Pronouns, as before for S W B D , are useless.
We believe that the entity grid's good perfor-mance here is due mostly to two factors: its use of a long history, and its lack of lexicalization. The grid looks at the previous six sentences, which dif-ferentiates it from the IBM model and from Elsner and Charniak (2008b), which treats each pair of sen-tences independently. Using this long history helps to distinguish important nouns from unimportant ones better than frequency alone. We suspect that our lexicalized models, IBM and the topical entity grid, are hampered by poor parameter settings, since their parameters were learned on F I S H E R rather than IRC chat. In particular, we believe this explains why the topical entity grid, which slightly outperformed the entity grid on S W B D , is much worse here. 6.2 Full disentanglement Running our tabu search algorithm on the full disen-tanglement task yields disappointing results. Accu-previous work, but also worse than simple baselines like creating one thread for each speaker. The model nds far too many threads X  it detects over 300, when the true number is about 81 (averaging over annota-tions). This appears to be related to biases in our chat-specic models as well as in the entity grid; the time model (which generates gaps between adja-cent sentences) and the speaker model (which uses a CRP) both assign probability 1 to single-utterance conversations. The entity grid also has a bias toward short conversations, because unseen entities are em-pirically more likely to occur toward the beginning of a conversation than in the middle.

A major weakness in our model is that we aim only to maximize coherence of the individual con-versations, with no prior on the likely length or num-ber of conversations that will appear in the tran-script. This allows the model to create far too many conversations. Integrating a prior into our frame-work is not straightforward because we currently train our mixture to maximize single-utterance dis-entanglement performance, and the prior is not use-ful for this task.

We experimented with xing parts of the tran-script to the solution obtained by Elsner and Char-niak (2008b), then using tabu search to ll in the gaps. This constrains the number of conversations and their approximate positions. With this structure in place, we were able to obtain scores comparable to Elsner and Charniak (2008b), but not improve-ments. It appears that our performance increase on single-sentence disentanglement does not transfer to this task because of cascading errors and the neces-sity of using external constraints. We demonstrate that several popular models of lo-cal coherence transfer well to the conversational do-main, suggesting that they do indeed capture coher-ence in general rather than specic conventions of newswire text. However, their performance across tasks is not as stable; in particular, models which use less history information are worse for disentan-glement.

Our results study suggest that while sophisticated coherence models can potentially contribute to dis-entanglement, they would benet greatly from im-proved low-level resources for internet chat. Bet-ter parsing, or at least NP chunking, would help for models like the entity grid which rely on syntactic role information. Larger training sets, or some kind of transfer learning, could improve the learning of topics and other lexical parameters. In particular, our results on S W B D data conrm the conjecture of (Adams, 2008) that LDA topic modeling is in prin-ciple a useful tool for disentanglement X  we believe a topic-based model could also work on IRC chat, but would require a better set of extracted topics. With better parameters for these models and the integra-tion of a prior, we believe that our good performance on S W B D and single-utterance disentanglement for IRC can be extended to full-scale disentanglement of IRC.
 We are extremely grateful to Regina Barzilay, Mark Johnson, Rebecca Mason, Ben Swanson and Neal Fox for their comments, to Craig Martell for the NPS chat datasets and to three anonymous review-ers. This work was funded by a Google Fellowship for Natural Language Processing.

