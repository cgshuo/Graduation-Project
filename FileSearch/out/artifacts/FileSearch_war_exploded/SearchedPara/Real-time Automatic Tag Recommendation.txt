 Tags are user-generated labels for entities. Existing rese arch on tag recommendation either focuses on improving its ac-curacy or on automating the process, while ignoring the ef-ficiency issue. We propose a highly-automated novel frame-work for real-time tag recommendation. The tagged training documents are treated as triplets of (words, docs, tags), an d represented in two bipartite graphs, which are partitioned into clusters by Spectral Recursive Embedding (SRE). Tags in each topical cluster are ranked by our novel ranking al-gorithm. A two-way Poisson Mixture Model (PMM) is pro-posed to model the document distribution into mixture com-ponents within each cluster and aggregate words into word clusters simultaneously. A new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks. Ex-periments on large-scale tagging datasets of scientific doc u-ments (CiteULike) and web pages (del.icio.us) indicate tha t our framework is capable of making tag recommendation ef-ficiently and effectively. The average tagging time for testi ng a document is around 1 second, with over 88% test docu-ments correctly labeled with the top nine tags we suggested. I.5.3 [ Pattern Recognition ]: Clustering X  algorithms ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance tagging system, mixture model, graph partitioning Copyright 2008 ACM 978-1-60558-164-4/08/07 ... $ 5.00.
Tagging usually refers to the action of associating a rel-evant keyword or phrase with an entity (e.g. document, image, or video). With the recent proliferation of Web 2.0 applications such as Del.icio.us 1 and Flickr 2 , tagging ser-vices have become popular and have drawn much attention in both academia and industry. Existing research on tagging services includes improving the quality of searching and re c-ommendation in the tag space [2], analyzing the usage pat-terns of tagging systems [11], and automating the process of tag assignment [4].

Here, we address the problem of automatic tag recom-mendation for document search engines and digital librarie s, which bears some similarities to that of query recommenda-tion [1]. However, our problem space is arguably larger, because relevant candidate tags may not even appear in the document, while candidate queries are most likely bounded in the document term space in keyword-based search.
While automatic tag recommendation is an actively pur-sued research topic, to the best of our knowledge, we are the first to study in depth the problem of automatic and real-time tag recommendation, and propose a solution with promising performance when evaluated on two real-world tagging datasets, i.e., CiteULike 3 for scientific documents and del.icio.us for web pages.

Specifically, we advocate a two-state framework. First, the relationship among documents, tags, and words are rep-resented in two bipartite graphs. During the offline learning stage, we use the Lanczos algorithm for symmetric low rank approximation for the weighted adjacency matrix for the bi-partite graphs, and Spectral Recursive Embedding (SRE) to symmetrically partition the graphs into multi-class clust ers. We propose a novel node ranking algorithm to rank nodes (tags) within each cluster, and then apply a Poisson mixture model to learn the document distributions for each class.
During the online recommendation stage, given a docu-ment vector, its posterior probabilities of classes are firs t calculated. Then based on the joint probabilities of the tag s and the document, tags are recommended for this document based on their within-cluster ranking. The efficiency of the Poisson mixture model lets our model make recommenda-tions in linear-time in practice. As an example, using reaso n-http://del.icio.us/ http://www.flickr.com/ http://www.citeulike.org/ able resources, the average tagging time for a test document is only 1.1 seconds.

It should be noted that tag suggestion is still a compli-cated problem that can be addresses in many aspects. e.g., examine the tag growth and reuse by user study [8]. In this paper, we address this issue from a machine learning per-spective by analyzing the content of tags. We believe this approach could be a useful component that can be com-bined with other powerful tools to boost the performance of real-world tagging systems.

The rest of the paper is organized as follows. We briefly review the related work in Section 2. Section 3 introduces bipartite graphs, the Lanczos algorithm, and the graph par-titioning algorithm as well as the node ranking method. Sec-tion 4 presents the mixture model for document classifica-tion and a online tag recommendation algorithm. Section 5 presents the experimental results on two data sets. Section 6 concludes our work.
Bipartite Graph Partitioning: A bipartite graph con-sists of two disjoint sets of vertices X and Y such that no edge has both end points in the same set. The general graph partitioning problem is NP hard so that for bipartite graphs , partitioning is optimized by minimizing a global function. Many clustering algorithms have been proposed to partition bipartite graphs. The Min-Max Cut algorithm minimizes between-cluster sum of weights and maximizes the within-cluster sum of weights [6]. The spectral clustering simulta -neously clusters rows and columns of adjacency matrix of the graph [5]. However, as pointed out in [10], spectral cluster -ing may fail in certain cases where two bipartite graphs are merged to be one tripartite graph due to the heterogeneous nature of the vertices. To address this problem, the algo-rithm, Consistent Bipartite Graph Co-partitioning (CBGC) , is proposed [10]. CBGC applies semi-definite programming (SDP) to deal with star-structured high order heterogeneou s data by representing them as several bipartite graphs, and optimizes a global function to find the best cut. However, CBGC only deals with binary clustering problems and is thus not suitable for multi-clustering tasks.

Low Rank Matrix Approximation: Low rank matrix approximation is the problem of approximating a m  X  n ma-trix A by another rank k matrix, where k is smaller than m and n . Traditional methods like Singular Value Decom-position (SVD) can be used to find such matrix but the computation time is usually too long ( O ( min { mn 2 , nm
Recently, near-optimal low rank matrix approximation meth-ods have become increasing popularity. If we denote A k as an optimal rank k approximation of matrix A , the goal is to find a near-optimal matrix A  X  k that minimizes the error  X  :
CUR [7] decomposition is one such algorithm that approx-imates A by A = CUR , where C is a matrix consisting of a small number of columns of A , R is a matrix consisting of a small number of rows of A , and U is an appropriately-defined low-dimensional encoding matrix. Thus, a CUR matrix de-composition provides a dimensionally-reduced low-rank ap -proximation to the original data matrix A that is expressed in terms of a small number of actual columns and rows of the original matrix. Both linear and constant time CUR algo-rithms have been proposed to efficiently approximate large sparse matrices. However, since the rows and columns are picked randomly, CUR can not guarantee the symmetry of a matrix, which makes it not suitable for bipartite graphs since the weight matrices are always symmetric. We define a graph G = ( V, E, W ) as a set of vertices V and their corresponding edges E , with W denoting the weight of edges. e.g., w ij denotes the weight of the edge between vertices i and j .
 A graph G is bipartite if it contains two vertex classes X and Y such that V = X  X  Y and X  X  Y =  X  , each edge e ij  X  E has one endpoint ( i ) in X and the other endpoint ( j ) in Y . In practice, X and Y usually refer to different types of objects and E represents the relationship between them. In the context of document representation, X represents a set of documents while Y represents a set of terms, and w denotes the number of times term j appears in document i . Note that the weighted adjacency matrix W for a bipartite graph is always symmetric.For example, Figure 1 depicts an undirected bipartite graph with 4 documents and 5 terms. Figure 1: A bipartite graph of X (documents) and Y (terms).
Normalization is usually performed first for the weight matrix W to eliminate the bias. The most straightforward way to normalize W is row normalization, which does not take into account the symmetry of W . However, to consider the symmetry of W , we propose to use normalized graph Laplacian to approximate W . The normalized Laplacian L ( W ) is defined as: where d i is the out degree of vertex i , i.e., d i = P w V . We can then define a diagonal matrix D where D ii = d Therefore, the normalized Laplacian can be represented as
For large-scale datasets such as the Web corpora and im-age collections, their feature space usually consists of mi l-lions of vectors of very high dimensions (e.g., x = 10 6 , y = 10 7 ). Therefore, it is often desirable to find a low rank ma-trix  X  W to approximate L ( W ) in order to lower the com-putation cost, to extract correlations, and remove noise. Traditional matrix decomposition methods, e.g., Singular Value Decomposition (SVD) and eigenvalue decomposition (when the matrix is symmetric), require superlinear time fo r matrix-vector multiplication so they usually do not scale t o real-world applications.

For symmetric low rank approximation, we use the Lanc-zos algorithm [12] which iteratively finds the eigenvalues a nd eigenvector of square matrices. Given an n  X  n sparse sym-metric matrix A with eigenvalues: the Lanczos algorithm computes a k  X  k symmetric tridi-agonal matrix T , whose eigenvalues approximate the eigen-values of A , and the eigenvectors of T can be used as the approximations of A  X  X  eigenvectors, with k much smaller than n . In other words, T satisfies: where kk F denotes the Frobenius norm, with  X  as a con-trolled variable. For example, to capture 95% variances of A ,  X  is set to 0.05. For multi-clustering on bipartite graphs, we apply the Spectral Recursive Embedding (SRE) algorithm [16]. SRE essentially constructs partitions by minimizing a normali zed sum of edge weights between unmatched pairs of vertices, i.e., min  X ( A,B ) Ncut ( A, B ), where A and B are matched pairs in one partition with A c and B c being the other. The normalized variant of edge cut Ncut ( A, B ) is defined as: Ncut ( A, B ) = cut ( A, B ) where
The rationale of Ncut is not only to find a partition with a small edge cut, but also partitions that are as dense as possible. This is useful for our application of tagging doc-uments, where the documents in each partition are ideally focused on one specific topic . As a result, the denser a par-tition is, the better that relevant documents and tags are grouped together.
We define two new metrics N-Precision and N-Recall for node ranking. N-Precision of a node i is the weighted sum of its edges that connect to the nodes within the same cluster, divided by the total sum of edge weights in that cluster. Denote the cluster label of i as C ( i ),
For the unweighted graph, the above equation equals to the number of edges associated with node i in cluster C ( i ), divided by the total number of edges in cluster C ( i ). Gen-erally, N-precision measures the importance of a node to the cluster, in comparison with other nodes. In the context of text documents, the cluster is a topic set of documents and the weight of the word nodes shows the frequency of the words appearing in that topic. With the cluster deter-mined, the denominator of equation (7) is constant, so that the more weight the node has, the more important it is.
In contrast, N-recall is used to quantify the posterior prob -ability of a node i to a given cluster and is the inverse frac-tion of i  X  X  edge associated with its cluster
It is evident that N-Recall is always no less than 1. The larger N-Recall is, the more probable that a word is associ-ated with a specific topic.
 Given np i and nr i , we can estimate the ranking of i :
Depicted in Figure 2, our ranking function is a smoothed surrogate that is proportional to both node precision and recall, guaranteed to be in the range of (0 , 1).
Potential applications of the aforementioned bipartite gr aph node ranking methodology include interpreting the documen t-author relationship. i.e., determine the social relations (e.g.,  X  X ub X  and  X  X uthority X ) of authors in the same research topic, and finding the most representative documents in the topic. In what follows, we apply this framework to tag rec-ommendation by ranking nodes that represent tags in each cluster.
A typical document of concern here consists of a set of words and several tags annotated by users. The relationship among documents, words, and tags can then be represented by two bipartite graphs as shown in Figure 3.
 The weighted graph can be written as where A and B denote the inter-relationship matrices be-tween tags and docs, docs and words, respectively. Figure 3: Two bipartite graphs of documents, words and tags.

Given the matrix representation, a straightforward ap-proach to recommend tags is to consider the similarity (e.g. , cosine similarity) between the query document and training documents by their word features, then suggest the top-ranked tags from most similar documents. This approach is usually referred to as collaborative filtering [3]. Never -theless, this approach is not efficient for real-world scenar -ios. To take the advantage of the proposed node ranking algorithm, we propose a Poisson mixture model that can ef-ficiently determine the membership of a sample as well as clustering words with similar meanings.

Before presenting the mixture model, we first summarize our framework of tag recommendation in Algorithm 1. Algorithm 1 Online Tag Recommendation 1: Input ( D , S, T ), K, M, L 2: Represent the weighted adjacency matrix W as in eq. (10) 3: Normalize W using the normalized Laplacian 4: Compute a low rank approximation matrix using the Lanczos: 5: Partition  X  W into K clusters using SRE [16], 6: Assign labels to each document D j , j  X  X  1 , ...m } 8: Build a Poisson mixture model for (  X  B, C ( D )) with M com-9: For each test document Y , calculate its posterior probabilities 10: Recommend tags based on the rank of tags, i.e., the joint
Intuitively, this two-stage framework can be interpreted as an unsupervised-supervised learning procedure. During the offline learning stage, nodes are partitioned into cluste rs using an unsupervised learning method, cluster labels are assigned to document nodes as their  X  X lass labels X , and tag nodes are given ranks in each cluster. A mixture model is then built based on the distribution of document and word nodes. In the online recommendation stage, a document is classified into predefined clusters acquired in the first stag e by naive Bayes so that tags can be recommended in the de-scending orders of their ranks. To avoid confusion, we will refer to the clusters determined by the partitioning algo-rithm in the first stage as classes in the next section.
We propose to use Poisson mixture models to estimate the distribution of document vectors, because they fit the data better than standard Poissons by producing better estimate s of the data variance, and are relatively easy for parameter estimation. Although it takes time to fit the training data, it is efficient to predict the class label of new documents once the model is built. Because of the numerical stability of this statistical approach, the results are usually relia ble. Since only probabilistic estimation is involved, it is capa ble for real-time process.

Nevertheless, traditional unsupervised learning approac hes of mixture models [9] are not always capable of dealing with document classification. Considering the sparseness and high-dimensionality of the document-word matrix where most entries are zeros and ones, the model may fail to pre-dict the true feature distribution (i.e. the probability ma ss function) of different components. As a result, word cluster -ing is a necessary step before estimating the components in the model. In what follows, we utilize the two-way Poisson mixture model [14] in order to simultaneously cluster word features and classify documents. Figure 4: An example of two mixtures of the Poisson distribution in two clusters.(Top) The histograms of mixture components. (Bottom) Mixture model clas-sification results. (a) Three-component mixtures. (b) Two-component mixtures.

Given a document D = { D 1 , ..., D p } , where p is the di-mension, the distribution of the document vector in each class can be estimated by using a parametric mixture model. Let the class label be C = { 1 , 2 , ..., K } , then P ( D = d | C = k ) = where  X  m is the prior probability of component m , with P m =1  X  m = 1. I ( F ( m ) = k ) is an indicator function, i.e., whether component m belongs to class k , and  X  denotes the probability mass function (pmf) of a Poisson distribution,
In this way, each class is a mixture model with a multi-variate distribution having variables that follow a Poisso n distribution. Figure 4 shows the histogram of two mixtures which can be regarded as the pmfs of two Poisson mixtures.
Our assumption is that within each class, words in dif-ferent documents have equal Poisson parameters, while for documents in different classes, words may follow different Poisson distributions. For simplicity, we also assume that all classes have the same number of word clusters. De-note l = { 1 , , , , L } to be the word clusters, words in the same word cluster m will have the same parameters, i.e.,  X  notes the cluster label of word i in class k . Therefore, Equa-tion (11) can be simplified as follows (with L  X  p ): P ( D = d | C = k )  X 
With the classes determined, we apply EM algorithm to estimate the Poisson parameters  X   X  l,m , l  X  X  1 , ..., L } , m  X  { 1 , ..., M } , the priors of mixture components  X  m , and the word cluster index c ( k, j )  X  X  1 , ..., L } , k  X  X  1 , ..., K } , j  X  { 1 , ..., p } .

The E-step estimates the posterior probability p i,m : The M-step uses p i,m to maximize the objective function = max and update the parameters where | d ( i, j ) | denotes the number of j  X  X  in component l .
Once  X   X  ( t +1) m is fixed, the word cluster index c ( t +1) can be found by doing linear search over all components:
Normally, the class label C ( d t ) of a new document d t determined by  X  C ( x ) = arg max k P ( C = k | D = d t ). However in our case, we determine the mixed membership of a doc-ument by calculating its posterior probabilities to classe s, with P K k =1 P ( C = k | D = d t ) = 1. Applying equation (12) and the Bayes rule, = where P ( C = k ) are the prior probabilities for class k and are set uniform. Finally, the probability for each tag T i { 1 , ..., n } to be associated with the sample is R ( T i , d t ) = P ( T = T i | D = d t ) = Rank T i  X  P ( C = x | D = d
By ranking the tags in descending order of their probabil-ities, the top ranked tags are selected for recommendation.
We evaluate our proposal (PMM) by conducting two sets of experiments in different application contexts: recommen d-ing tags for scientific documents (CiteULike) and web pages (del.icio.us).

Parameters are tuned before the online step takes place, i.e., the number of clusters K , the number of components M , and the number of word clusters L . Due to the space limitation, we only present the best results here.
For comparison, the Vector Similarity (VS) approach is used as a baseline, which calculates the cosine similarity b e-tween a query Q and each training document D i , Sim ( Q, D  X  j  X  X  word in sample i . The top t tags from s most simi-lar documents are then considered. In our experiment, we set both t and s to be 3, resulting in 9 recommendations for each query document. To improve performance, we augment the vector similarity approach by applying information-ga in (VS+IG) to select roughly 5% of the total features.
Meanwhile, we also compare with a recent developed method named SimFusion [15] which leveraged unified relationship matrix to iteratively calculate the similarity between ob-jects. Details are omitted here.
In addition to the standard Kendall  X  rank correlation metric [13] that measures the degree of correspondence be-tween two ranked lists, we also propose the following metric s to measure the effectiveness of our algorithm.
In our experiments, we return top 9 tags for evaluation.
For evaluation on scientific documents, we acquired the tagging dataset from CiteULike for over two years from November 15, 2004 to February 13, 2007. We mapped the dataset to papers that are indexed in CiteSeer 4 to extract the metadata. Each entry of the CiteULike record contains four fields: user name, tag, key (the paper ID in CiteSeer), and creation date. Overall, there are 32,242 entries, with 9,623 distinct papers and 6,527 distinct tags (tag vocabu-lary). The average number of tags per paper was 3.35. The 5 most tagged papers are listed in Table 1 respectively.
We report the results of 50% training and 50% test data here due to space limitation 5 . The optimal number of clus-ters K is 30, number of component M is 40, and the number of word cluster L is 30.

Table 1 also lists the top user tags for each of the top 10 papers, as well as the top 9 tags recommended by our al-gorithm. The bold fonts indicate an overlap. Generally, at least one correct recommendation is made for each paper, and the first tag recommended always matches one of the user tags. In addition, although some recommended tags do not match the user tags literally, most of them are seman-tically relevant. e.g.,  X  X ww X  is relevant to  X  X eb X ;  X  X om-munities X  is often consisted in  X  X ocial networks X ;  X  X age X  and  X  X ank X  together have the same meaning as  X  X agerank X . In the best scenario, 7 of 9 recommended tags match with the user tags for the paper  X  X  Tutorial on Learning With Bayesian Networks X , which has a Kendall  X  rank of 0.78.
The comprehensive performance on CiteULike data set is depicted in Figure 6. On average, the Kendall  X  rank is 0.24 for PMM, indicating a positive correspondence be-tween the two rankings. The top-9 tag performance is shown in (a), where our algorithm makes 67.2% correct recommen-dation for the top-most( top-1 ) tag, for which SimFusion and VS+IG are around 58.2% and 43.3%. As the number of tags increases, the top-k performance gradually improves. The accuracy of top 9 tags for PMM reaches 93 . 1%, indicating that at least 1 of 9 tags recommended by our algorithm is also annotated by the users of over 4,000 test papers. Fig-ure 6 (b) shows the accuracy at different ranks. It is clear that the top-most tags achieve the best accuracy (67.2%) for PMM and the performance decreases as the rank of the tags decreases. Finally, Figure 6 (c) shows the precision-recal l graph. Our method is clearly the winner. With the number of tags increases from 1 to 9, tag-precision drops from 67.2% to 43.5%, while tag-recall goes up from 10.2% to 55.6%.
On average, the histogram of the number of correctly-labeled tags implies that 3.72 tags are correctly recommend ed.
Using the tagging data set from del.icio.us, we subscribed to 20 popular tags, each of which is treated as a topic. For these topics, we retrieved 22,656 URLs from March 3rd, 2007 to April 25, 2007. For each URL, we crawled del.icio.us to obtain the most popular tags with their frequencies. We also harvested the HTML content of each URL. We ended up with 215,088 tags, of which 28,457 are distinct (tag vo-cabulary), averaging 9.5 tags per URL. The total size of the dataset is slightly over 2GB. http://citeseer.ist.psu.edu/oai.html
We also tested our algorithm on different splitting ratios, with similar results observed.

We sorted the data chronically and used the first half for training, the rest for testing 5 . By filtering out stop words and using mutual information to select the most informa-tive words, we form a feature space with 83,205 words, con-taining 11,300 samples. The sparseness of the training set (number of zeros vs. dimensionality) is 92%.
 Figure 7 shows the results for 11,356 test URLs, with a Kendall  X  value of 0.13 for PMM. Comparing with CiteU-Like, the performance degrades for all metrics. Again, our algorithm outperform the others significantly. The accurac y of top-1 tag is around 48%, and degrades to 22% for the 9th tag. However, the top 9 recommended tags together are still able to overlap with 88.5% of the user-annotated tags.
We give two explanations for the degraded performance on the web page tag recommendation task. First, we notice that our algorithm usually fails when the content of a specifi c URL contains little of the necessary information, i.e., wor ds in our case. As an example, for the topics  X  X hotography X  and  X  X ravel X , many pages only contain images and short descriptions, making it hard for our model to determine the proper components for a test sample.

Second, unlike structured scientific documents with con-trolled vocabularies, the heterogeneous nature of web page s not only results in varied length (word count) of the html pages, but also the distribution of the tag vocabulary. In fact, for PMM, the tag/doc ratio for the CiteULike data is 0.68 (6,527 unique tags vs. 9,623 papers), compared with 1.26 (28,457 unique tags vs. 22,656 URLs) for del.icio.us. A previous study [11] has shown that the tag vocabulary usually does not converge for a specific user, reflecting a continual growth of interests. Thus, we believe that a large tag vocabulary could possibly compromise the recommenda-tion performance for unstructured web pages. On average, 2.91 correct tags are recommended for each test sample.
Figure 5 depicts the user tags as well as our recommended tags that co-occurred with  X  X jax X . It can be observed that the co-occurred tags in our model are consistent with those annotated by the users.
To show that our model is capable of making real-time tagging for large volumes of documents, we evaluate our model in terms of the average tagging time for query doc-uments. Different proportions of training documents (from 10% to 90 %) are tested.
 Figure 8 and Table 2 present the performance of CiteU-Like and del.icio.us data respectively 6 . Our approach ex-hibits stable performance on both data sets with very small variance. On average, only 1.08 seconds is needed for one test document on CiteULike and 1.23 seconds for del.icio.us . On the other hand, the average tagging time for SimFusion and VS+IG is 6.4 and 16 seconds respectively, expected to grow exponentially with the increase of the features.
The efficiency of our model can be explained by its linear calculation. With the model determined, calculating the membership of a new document within each class is readily automated, requiring only multiplication in equation (17) of the M components and L word clusters. Recall that M = O ( K ) and L  X  p with the process taking linear time to complete. Moreover, by taking the log over both sides of equation (17), the multiplication can be replaced by additi on operations, making it even more efficient.
The experiment was performed on a 3.0GHZ sever. Paper Name Tags Top User Tags Our Tags (Larry Page et al.) networks, socialnetworks, ir rank, www, ir
Hypertextual Web Search Engine 94 web, engine, www, www , page, rank, Tags with bold font match one of the user-annotated tags. tags with  X  X jax X , which is suggested 1,325 times. Figure 8: Average tagging time of CiteULike data.
In this paper, we proposed a learning framework for tag recommendation for scientific and web documents. We pro-posed a Poisson mixture model for efficient document classi-fication. We also proposed a novel node ranking method as well as several new metrics for evaluating the performance o f our framework. The proposed framework demonstrates its potential in evaluations on two real-world tagging data set s, indicating its capability of handling large-scale data set s in real-time. Being runtime efficient, our proposed method can recommend tags in one second on average.

Future work would be to examine the performance of our algorithm for less popular tags/documents. A user study could be done by assigning relevance scores to each tag (e.g. 0-3), and applying Normalized Discounted Cumulative Gain (NDCG) to measure the performance. Of course it would be interesting to apply this framework to other tagging rec-ommendations.
