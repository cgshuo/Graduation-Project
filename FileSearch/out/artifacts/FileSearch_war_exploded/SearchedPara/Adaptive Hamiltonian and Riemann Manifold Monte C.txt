 Ziyu Wang ziyuw@cs.ubc.ca Shakir Mohamed shakirm@cs.ubc.ca Nando de Freitas nando@cs.ubc.ca Hamiltonian Monte Carlo (HMC) (Duane et al., 1987) is widely-known as a powerful and efficient sampling algorithm, having been demonstrated to outperform many existing MCMC algorithms, espe-cially in problems with high-dimensional, continuous, and correlated distributions (Chen et al., 2001; Neal, 2010). Despite this flexibility, HMC has not been widely adopted in practice, due principally to the sen-sitivity and difficulty of tuning its hyperparameters. Tuning HMC has been reported by many experts to be more difficult than tuning other MCMC methods (Ishwaran, 1999; Neal, 2010). In this paper we aim to remove this obstacle in the use of HMC by providing an automated method for determining these tunable parameters, paving the way for a more widespread application of HMC in statistics and machine learning. There are few existing works dealing with the automated tuning of HMC. Two notable approaches are: the No U-turn sampler (NUTS) (Hoffman &amp; Gelman, 2011), is an adaptive algorithm that aims to find the best parameter settings by tracking the sample path and preventing HMC from retracing its steps in this path; and Riemann manifold HMC (RMHMC) (Girolami &amp; Calderhead, 2011), which provides adaptations using Riemannian geometry. In this paper, we follow the approach of adapting Markov chains in order to improve the convergence of both HMC and RMHMC. Our adaptive strategy is based on Bayesian optimization; see for example Brochu et al. (2009) and Snoek et al. (2012) for a comprehensive introduction to Bayesian optimization. Bayesian optimization has been proposed previously for the adaptation of general MCMC samplers by Ma-hendran et al. (2012) and Hamze et al. (2013). To guarantee convergence, these works were limited to a finite adaptation of the Markov chain. This finite adaptation can result in the sampler being trapped in suboptimal parameter settings, leading to inefficient sampling, and we address this limitation in this paper. We describe Hamiltonian-based Monte Carlo samplers (sect. 2), and then make the following contributions:  X  We present an algorithm for adaptive HMC in  X  Importantly, we prove that the adaptive MCMC  X  We provide a comprehensive set of experiments  X  We use a version of the expected squared jumping Hamiltonian (or Hybrid) Monte Carlo (Duane et al., 1987; Neal, 2010), has become established as a pow-erful, general purpose Markov chain Monte Carlo (MCMC) algorithm for sampling from general, contin-uous distributions. Its efficiency is due to the fact that it makes use of gradient information from the target density to allow for an ergodic Markov chain capable of large transitions that are accepted with high prob-ability. This efficiency and flexibility is demonstrated by the wide range of applications to which HMC has been applied, including: Bayesian generalized linear models (Ishwaran, 1999), Bayesian neural networks (Neal &amp; Zhang, 2006), Gaussian process regression and classification (Rasmussen &amp; Williams, 2006), exponen-tial family PCA and factor analysis (Mohamed et al., 2008), and restricted Boltzmann machines (Ranzato &amp; Hinton, 2010), amongst others.
 For HMC, we are required to specify a potential energy function, which is the log of the joint distribution we wish to sample from, U ( x ) =  X  log p ( x ) and a kinetic energy function, most typically, K ( p ) = p T M  X  1 p / 2, with momentum vector p and a positive definite mass matrix M . For standard HMC, the mass matrix is set to the identity. We defer the technical details of HMC to existing work (Neal, 2010), and present only the algorithm here (Alg. 1).
 HMC requires the selection of two free parameters: a step-size and a number leapfrog steps L . The ac-cepted guidance is to choose the step-size to ensure that the sampler X  X  rejection rate is between 25%-35%. It is also preferable to have a large L , since this reduces the random walk behavior of the sampler (Neal, 2010), but too large an L results in unnecessary computa-tion. In this paper, we consider a slight variation of the HMC algorithm: instead of performing L leapfrog steps at each iteration, we perform a random num-ber of leapfrog steps, chosen from the discrete uniform distribution over { 1 ,  X  X  X  ,L } , i.e. L r  X  U (1 ,L ) steps. This approach amounts to using a mixture of L differ-ent HMC transition kernels, thus preserving detailed balance (Andrieu et al., 2003).
 HMC is known to be highly sensitive to the choice of and L . We demonstrate HMC X  X  sensitivity to these pa-rameters by sampling from a bivariate Gaussian with correlation coefficient 0.99. We consider three settings behavior of the sampler as well as the autocorrelation plot in figure 1. While the first setting exhibits good behavior and low auto-correlation, small changes to these settings results in poor mixing and high auto-correlation, as seen in the other graphs. Theoreti-Algorithm 1 Hamiltonian Monte Carlo Algorithm cal results concerning the optimal acceptance rate for HMC exist, having been described by Beskos et al. (2010) and Neal (2010), with both concluding a rate around 0 . 65 as optimal. Such results, however, would not help in choosing the best sampler out of the three in Figure 1, since all three samplers in this demonstra-tion have an acceptance rate around 0 . 7, leaving little guidance for finding the most efficient sampler. To address the problem of choosing these parame-ters, we will introduce a method for automatically and adaptively tuning the parameters of HMC, reducing the need for time-consuming, expert tuning. An ex-isting approach for automatic tuning of HMC was in-troduced by Hoffman &amp; Gelman (2011), referred to as the No U-turn sampler (NUTS). NUTS allows for au-tomatic tuning of both HMC X  X  parameters by tuning the stepsize during the burn-in phase, after which it is fixed and the number of leapfrog steps is ad-justed thereafter for every iteration. is chosen using a stochastic approximation method referred to as dual averaging, and L is chosen for every sample using a recursive algorithm in which the number of leapfrog steps is allowed to increase until the proposal trajec-tory taken by the sampler begins to move back towards its initial point, thus preventing U-turns and allowing for the good mixing of the chain.
 Riemann manifold HMC (RMHMC) (Girolami &amp; Calderhead, 2011) is a sampling method derived from HMC, and provides an adaptation mechanism by ex-ploiting the Riemannian geometry of the parameter space. Rather than adapting and L , RMHMC ac-counts for the local structure of the joint density by adapting the mass matrix M used in HMC. Since RMHMC automatically adapts its mass matrix, the stepsize is usually fixed and the number of leapfrog steps L , which is a single scalar, can be chosen using the rejection rate. While the sensitivity to these pa-rameters is greatly reduced, they must still be set and there is no general guidance on how these parameters should be chosen, making it desirable to have a fully automatic method for RMHMC as well. In order to adapt the MCMC parameters L and for HMC, we need to (i) define an objective function and (ii) choose a suitable optimization method.
 As pointed out in Pasarica &amp; Gelman (2010), a natu-ral objective function for adaptation is the asymptotic efficiency of an MCMC sampler, (1 + 2 P  X  k =1  X  k ) where  X  k is the auto-correlation of the sampler with lag k . Despite its appeal, this measure is problematic because the higher order auto-correlations are hard to estimate. To circumvent this problem, Pasarica &amp; Gel-man (2010) introduced an objective measure called the expected squared jumping distance (ESJD): where  X  = ( L, ) denotes the set of parameters for HMC. Maximizing the above objective is equivalent to minimizing the first-order auto-correlation  X  1 . In prac-tice, the above intractable expectation with respect to the Markov chain is approximated by an empirical es-timator, as outlined in Pasarica &amp; Gelman (2010). The ESJD measure is efficient in situations where the higher order auto-correlations increase monotonically with respect to  X  1 . However, it is not suitable for tuning HMC samplers since by increasing the number of leapfrog steps one can almost always generate better samples. What we need is a measure that also takes computing time into consideration. With this goal in mind, we introduce the following objective function: This function simply normalizes the ESJD by the num-ber of leapfrog steps L , thus taking both statistical efficiency and computation into consideration. Most of our experiments will use this measure as we have found it to work very well in practice. Many works in the adaptive MCMC literature have considered match-ing empirical and theoretical acceptance rates in order to adapt MCMC samplers; see for example Andrieu &amp; Robert (2001) or Vihola (2010). We have found this strategy to perform poorly in the case of HMC, where samplers with the same acceptance rate can exhibit different mixing behavior (figure 1). When discussing Bayesian neural networks in our experiments (section 5.4), we will introduce an alternative objective func-tion based on predictive performance. Such a measure does however only apply in predictive domains and is, consequently, less general than the normalized ESJD objective.
 Now that we are armed with an objective function, we need to address the issue of optimization. Since the objective is only available point-wise (that is, it can be evaluated but its exact form is intractable), re-searchers typically use stochastic approximation. We use Bayesian optimization to optimize the objective. A discussion contrasting these two alternatives is pre-sented in Hamze et al. (2013).
 Bayesian optimization is an efficient gradient-free opti-mization tool well suited for expensive black box func-tions. Our objective function (normalized ESJD) is of this nature. As mentioned earlier, normalized ESJD involves an intractable expectation that can be approx-imated by sample averages, where the samples are pro-duced by running HMC for a few iterations. Each set of HMC samples for a specific set of hyper-parameters  X   X   X  results in a noisy evaluation of the normalized ESJD: r (  X  ) = f (  X  ) +  X  , where we assume that the measurement noise is Gaussian  X   X  X  (0 , X  2  X  ). Following the standard Bayesian optimization method-ology, we set  X  to be a box constraint such that for some interval boundaries b l  X  b u and b L l  X  b L u . The parameter L is discrete. The parameter is continu-ous, but since it is one-dimensional, we can discretize it using a very fine grid.
 Since the true objective function is unknown, we specify a zero-mean Gaussian prior over it: function, which forms the covariance matrix: and k = [ k (  X  ,  X  1 ) ... k (  X  ,  X  i )] T . In this Algorithm 2 Adaptive HMC. work, we adopt a Gaussian ARD covariance func-tion with k (  X  i ,  X  j ) = exp(  X  1 2  X  T i  X   X  1  X  j ) where  X  is a positive definite matrix. We set  X  = diag [  X  ( b u  X  b l )] 2 ;  X  ( b L u  X  b L l ) 2 , where  X  = 0 . 2. Given noisy evaluations of the objective function { r k } i k =1 where evaluated at points {  X  k } i k =1 , we form the dataset D i = {  X  k } i k =1 , { r k } i k =1 . Using Bayes rule, we arrive at the posterior predictive distribution over the unknown objective function: For more details on Gaussian processes, please refer to Rasmussen &amp; Williams (2006).
 The Gaussian process simply provides a surrogate model for the true objective. The surrogate can be used to efficiently search for the maximum of the ob-jective function. In particular, it enables us to con-struct an acquisition function u (  X  ) that tells us which parameters  X  to try next. The acquisition function uses the Gaussian process posterior mean to predict regions of potentially higher objective values (exploita-tion). It also uses the posterior variance to detect re-gions of high uncertainty (exploration). Moreover, it effectively trades-off exploration and exploitation. Dif-ferent acquisition functions have been proposed in the literature (Mo X ckus, 1982; Srinivas et al., 2010; Hoff-man et al., 2011). We adopt a variant of the Upper Confidence Bound (UCB) (Srinivas et al., 2010), mod-ified to suit our application: As in standard UCB, we set  X  i +1 =  X  and  X  is set to 0 . 1. The parameter p i ensures that the diminishing adaptation condition for adaptive MCMC (Roberts &amp; Rosenthal, 2007) is satisfied. Specifically, we set p i = (max { i  X  k + 1 , 1 } )  X  0 . 5 some k  X  N + . As p i goes to 0, the probability of Bayesian optimization adapting  X  vanishes as shown in Algorithm 2.
 It could be argued that this acquisition function could lead to premature exploitation, which may prevent Bayesian optimization from locating the true optimum of the objective function. There is some truth to this argument. Our goal when adapting the Markov chain, however, is less about finding the absolute best hyper-parameters but more about finding sufficiently good hyper-parameters given finite computational re-sources. Given enough time, we could slow the an-nealing schedule thus allowing Bayesian optimization to explore the hyper-parameter space fully. However, under time constraints we must use faster annealing schedules. As p i decreases, it becomes increasingly dif-ficult for Bayesian optimization to propose new hyper-parameters for HMC. Consequently, the sampler ends up using the same set of hyper-parameters for many iterations. With this in mind, we argue, it is more reasonable to exploit known good hyper-parameters rather than exploring for better ones. This intuition matches our experience when conducting experiments. The acquisition function also includes a scalar scale-invariance parameter s , such that  X  i (  X  ,s ) = k
T ( K +  X  2 tomatically so as to rescale the rewards to the same range each time we encounter a new maximal reward. Gaussian processes require the inversion of the covari-ance matrix and, hence, have complexity O ( i 3 ), where i is the number of iterations. Fortunately, thanks to our annealing schedule, the number of unique points in our Gaussian process grows sub-linearly with the num-ber of iterations. This slow growth makes it possible to adopt kernel specification techniques, as proposed by Engel (2005), to drastically reduce the computational cost without suffering any loss in accuracy.
 Even our adaptive strategy has hyperparameters that must be set. Of all these, the length scale of the kernel is the most important. In all our experiments, we set  X  = 4, k = 100, m = B k , where B is the number of burn-in samples. In our experience, our algorithm is highly robust to these settings and we use the same parameter settings for all of our experiments , with the exception of  X .  X  is easy to set, since one can choose the bound to be large enough to contain all reason-able and L , while allowing the adaptive algorithm enough time to explore. Alternatively, one could gauge the hardness of the sampling problem at hand and set more reasonable bounds. In general, harder sampling problems require a smaller and a larger L . We follow this second strategy throughout our experiments and found that most sensible bounds led to performance similar to the ones reported. The proof of ergodicity of the adaptive HMC algorithm capitalizes on existing results for Langevin diffusions and adaptive MCMC on compact state spaces. The method of proof is based on the standard Lyapunov stability functions, also known as drift or potential functions.
 We assume that our target distribution is compactly supported on M . In practice, for target distributions that are not compactly supported, we could set M large enough to contain most of the mass of our tar-get distribution. The sampler is restricted to M by following this standard approach of rejecting all pro-posals that fall outside M .
 Let { P  X  }  X   X   X  be a collection of Markov chain kernels, each admitting  X  as the stationary distribution. That is, for each value of  X  = ( ,L ), we have one such kernel. Moreover, let P n  X  denote the n -step Markov kernel. Our proof requires the following classical definitions: Definition 1. ( Small set ) A subset of the state space C  X  X is small if there exists n 0  X  N + ,  X  &gt; 0 and a probability measure  X  ( . ) such that P n 0 ( x,  X  )  X   X  X  (  X  )  X  x  X  C .
 Definition 2. ( Drift condition ) A Markov chain satisfies the drift condition if for a small set C , there exist constants 0 &lt;  X  &lt; 1 and b &lt;  X  , and a function V : X  X  [1 ,  X  ] such that  X  x  X  X  Having defined the necessary concepts, we now move on to show the ergodicity of our adapted approach. Proposition 3. Suppose that P  X  , when restricted to a compact set M , admits the stationary distribution  X  for all  X   X   X  . If  X  is continuous, positive and bounded on M , and |  X  | is finite, then the adaptive HMC sam-pler is ergodic.
 Proof. To show that adaptive HMC converges on a compact set, we first show that M is a small set. The transition kernel of the random time HMC algo-rithm can be written as P  X  ( x,. ) = P L l =1 1 L Q where Q l, ( x,. ) is the transition kernel of an HMC sampler that takes l leapfrog steps with parameter . In particular Q 1 , ( x,. ) is the transition kernel of Metropolis adjusted Langevin algorithm (MALA). As  X  is bounded, and the proposal distribution of MALA is positive every where, we have that Q 1 , is  X  Leb irreducible. By a slight modification of Theorem 2-2 in Roberts &amp; Tweedie (1996), for Markov chains de-fined by MALA, and any compact set C with posi-tive Lebesgue measure (i.e.  X  Leb ( C ) &gt; 0) there exists  X  &gt; 0 and a probability measure  X  (  X  ) such that  X  x  X  C Q 1 , ( x,. )  X   X  X  ( . ) . Hence, M is a small set since for any compact set C where  X  Leb ( C ) &gt; 0. The drift condition is trivially satisfied by each HMC sampler when we choose C to be M , and V to be such that V ( x ) = 1 for all x .
 Having proved these conditions, we can now appeal to Theorem 15.0.1 of Meyn &amp; Tweedie (1993) to conclude 0 &lt;  X   X  &lt; 1. Since V ( X ) = 1  X  x , we have  X  x  X  X  and  X   X   X   X  we have We have shown that the kernels { P  X  ( x,  X  ) }  X   X   X  are si-multaneously uniformly ergodic. Also, the adaptive HMC sampler has diminishing adaptation by design. By Theorem 5 of Roberts &amp; Rosenthal (2007), these two conditions imply the claim of our proposition. In general two sets of conditions together guarantee ergodicity of an adaptive MCMC algorithm (Roberts &amp; Rosenthal, 2007; Atchad  X e &amp; Fort, 2010). First, the adaptation has to diminish eventually. The second set of conditions is usually placed on the underlying MCMC samplers. In Roberts &amp; Rosenthal (2007), the samplers are required to be either simultaneously uni-formly or geometrically ergodic. Without restricting the state space to be compact, it is unlikely that HMC is uniformly ergodic. Also, to the best of our knowl-edge, no theoretical results exist on the geometric er-godicity of HMC when the state space is not com-pact. However, Roberts &amp; Stramer (2002) showed that Langevin diffusion, which is closely related to HMC, is geometrically ergodic. Thus one potential challenge would be to prove or disprove geometric er-godicity of HMC in general state spaces. Atchad  X e &amp; Fort (2010) weakened the conditions required, still re-quiring diminishing adaptation, but the requirements on the underlying MCMC samplers were reduced to sub-geometric ergodicity. Although these conditions are weaker, it remains hard to check whether HMC satisfies them. We show the performance of our adaptive algorithm on four widely-used models. We evaluate the perfor-mance of the samplers using the effective sample size (ESS) using: ESS = R (1 + 2 P k  X  k ), where R is the number of posterior samples, and P k  X  k is the sum of K monotone sample auto-correlations computed using the monotone sequence estimator (Girolami &amp; Calder-head, 2011). We adopt the total number of leapfrog steps used in producing the set of samples as a proxy for computational demand, since the computation is dominated by the gradient evaluation required for each leapfrog step. An efficient sampler will result in the highest ESS for the least computation, and we will thus report the effective sample size per leapfrog step used (ESS/L), similarly to Hoffman &amp; Gelman (2011), since this takes into account computational require-ments. We compute the ESS/L over all dimensions of the target distribution and report the minimum, me-dian and maximum ESS obtained. While we report all three summary statistics, we focus on the mini-mum ESS/L as the most useful measure, since this allows us to evaluate the efficiency of the most con-fined coordinate, and is more indicative of ESS jointly over all coordinates rather than, as computed, over ev-ery coordinate independently (Neal, 2010; Girolami &amp; Calderhead, 2011).
 We compare our adaptive HMC to NUTS, and ex-tend our approach and compare an adaptive version of RMHMC to the standard RMHMC. For NUTS, we tuned the free parameters of its dual averaging algorithm to obtain the best performance, and for RMHMC we use the experimental protocol and code used by Girolami &amp; Calderhead (2011). We do this for all experiments in this section. Code to reproduce these results will be available online 1 . 5.1. Bayesian Logistic Regression We consider a data set X consisting of N observations and D features or covariates, and a binary label y . Using regression coefficients  X  and bias  X  0 the joint distribution for the logistic regression model is: log p ( X , y ,  X  , X  0 )  X  log p ( y | X ,  X  , X  0 )+log p (  X  , X  =  X  X where y i  X  { X  1 , 1 } , and  X  2 is the prior variance of the regression coefficients. We present results on five data sets from the UCI repository. The data sets have varying characteristics with features D ranging from 2 to 24, and the number of observations N from 250 to 1000. For each data set, we generate 5000 samples after a burnin phase of 1000 samples, and repeat this process 10 times using differing starting points. The top row of figure 2 compares the performance of our adaptive HMC (AHMC) to NUTS, while the bottom row compares our adaptive RMHMC (ARMHMC) to RMHMC. For this experiment, for AHMC, we set  X  such that  X  [0 . 01 , 0 . 2] and L  X  X  0 ,  X  X  X  , 100 } , and for ARMHMC, we use  X  [0 . 1 , 1] and L  X  X  1 ,  X  X  X  , 12 } . The columns of figure 2 show box plots of the minimum, median and maximum ESS/L values ob-tained. We see that the adaptive methods (AHMC and ARMHMC) exhibit good performance. For the minimum ESS/L, AHMC has better (higher) values that NUTS for all the data sets, and this behavior is consistent across most other data sets for the other summary statistics. Thus AHMC typically provides better performance and a higher effective number of samples per unit of computation used than NUTS. We also see that the ARMHMC can improve RMHMC and provide better ESS/L on what is already a highly effi-cient sampler. 5.2. Log-Gaussian Cox Point Process We model a data set Y = { y ij } that consists of counts at locations ( i,j ) ,i,j = 1 ...,d in a regular spatial grid using a log-Gaussian Cox point process (LGC) (Chris-tensen et al., 2005; Girolami &amp; Calderhead, 2011). Ob-servations y ij are Poisson distributed and condition-ally independent given a latent intensity process  X  = {  X  ij } with means s X  ij = s exp( x ij ), where s = 1 d 2 . The rates X = { x ij } are obtained from a Gaussian process with mean function m ( x ij ) =  X  1 and covari-ance function  X ( x ij ,x i 0 j 0 ) =  X  2 exp (  X   X  ( i,i 0 probability log p ( y , x |  X , X , X  ) is proportional to:
X We generate samples jointly for x , X , X , X  using a grid of size d = 64, using a synthetic data set obtained by drawing from the generative process for this model. We generate 5000 samples after a burnin of 1000 samples. For this model, we use L  X  { 1 ,  X  X  X  , 500 } ,  X  [0 . 001 , 0 . 1] for AHMC, and use L  X  { 1 ,  X  X  X  , 60 } ,  X  [0 . 01 , 1] for ARMHMC. We compare the perfor-mance of the adaptive method we presented in terms of ESS per leapfrog step in figure 3. We compare AHMC versus NUTS and ARMHMC versus RMHMC, showing the minimum, median and maximum ESS per leapfrog step obtained for 10 chains with dis-persed starting points. We see that almost all points lie below the diagonal line, which indicates that the AHMC and ARMHMC have better ESS/L compared to NUTS and RMHMC, respectively. Thus even for high-dimensional models with strong correlations our adaptive method allows for automatic tuning of the sampler and consequently the ability to obtain higher quality samples than with competing methods.
 We examine the quality of the posterior distribution obtained for AHMC and NUTS in figure 4, by visualiz-ing the latent field and its variance, and comparing to the true data (which is known for this data set). The top row shows the true latent fields. The true data observations are shown in top right corner and we see that there are few data points in this region, and thus we expect to have a high variance in this region. The average of the samples obtained using AHMC shows that we can accurately obtain samples from the latent field x , and that the samples have a variance matching our expectations. While NUTS is able to also produce good samples of the latent field, the variance of the field is not well captured (bottom right image). 5.3. Stochastic Volatility We consider a stochastic volatility model described by Kim et al. (1998) and Girolami &amp; Calderhead (2011), in which we consider observations y t , regularly spaced in time for t = 1 ,...,T . Each y t is specified using a latent variable x t , which represents the log-volatility following auto-regressive AR(1) dynamics. The model is specified as: |  X  | &lt; 1 to ensure stationarity of the log-volatility, and the standard deviation  X  &gt; 0, whose priors we set to  X  +1 2  X  Beta (20 , 1 . 5) and  X  2  X  inv- X  2 (10 , 0 . 05), re-spectively. The parameters to be sampled by HMC are thus  X  = { x , X , X , X  2 } , and the joint probability is: p ( y ,  X  )= We make use of the transformations  X  = exp(  X  ) and  X  = tanh(  X  ) to ensure that we sample using uncon-strained variables; the use of this transformation re-quires the addition of the Jacobian of the transforma-tion of variables. We generate samples jointly using AHMC, using training data with T = 2000. For our experiments, we use a burnin period of 10 , 000 samples and thereafter generate 20 , 000 posterior samples. We restrict our box constraint such that L  X  X  1 ,  X  X  X  , 300 } ,  X  [10  X  4 , 10  X  2 ]. We show the results comparing ESS for the two methods in table 1. These results again show higher values for ESS per leapfrog step, demon-strating that a better performing sampler can be ob-tained using AHMC  X  further demonstrating the ad-vantages of AHMC methods for sampling from com-plex hierarchical models. 5.4. Bayesian Neural Networks We demonstrate the application of our adaptive ap-proach using Bayesian neural networks (BNNs) to show that AHMC allows for more effective sampling of posterior parameters even when compared to sam-plers finely tuned by an expert. We make use of the Dexter data set from the NIPS 2003 feature selection challenge, which is a subset of the well-known Reuters text categorization benchmark. The winning entries submitted by Neal &amp; Zhang (2006) used a number of feature selection techniques followed by a combination of Bayesian Neural Networks and Dirichlet diffusion trees. The entry that used only BNNs was placed sec-ond and achieved highly competitive results (Guyon et al., 2005).
 The BNN model consists of 295 input features and 2 hidden layers with 20 and 8 hidden units respec-tively. The input features are selected from the full set of features through univariate feature selection. The weights and bias as well as a few other parameters of this particular network adds up to form a 6097 dimen-sional state space for the HMC sampler.
 For this model, we use cross-validation to construct the reward signal. We divide the data into n sets, and train n BNNs each on n  X  1 sets and test them on the remaining set like in the case of normal cross-validation. The cross-validation error is then used to calculate the reward. To take computation into ac-count, we always evaluate the reward over the same number of leapfrog steps, i.e. for each evaluation of the reward we use a different number of samples and a different number of leapfrog steps for each sample, but the product of the two remains constant.
 We compare the results in table 2, where the perfor-mance measure is the prediction error on a test set (unknown to us) and was obtained after submission to the competition system. The improved results ob-tained using the AHMC strategy are clear from the ta-ble, also demonstrating that good adaptation can be preferable to the introduction of more sophisticated models.
 We described the use of the expected squared jumping distance, which is a general objective suitable for mod-els used both for inferential and predictive tasks, e.g., generalised regression. We also presented predictive measures as an alternative when prediction is the key task, e.g. neural networks, and where sufficient data is available for cross-validation. Several other objectives, such as the mean update distance, cross-validation er-ror and the cumulative auto-correlation, are also suit-able, and their use depends on the particular modelling problem. In many machine learning tasks, researchers design MCMC algorithms to estimate model param-eters and, subsequently, evaluate these models using cross-validation, such as the competition task in sec-tion 5.4. In this paper, we demonstrate the use of pre-dictive losses, such as cross-validation error, to guide the adaptation. Moreover, researchers often modify their samplers so as to reduce test set error. Hence, it is natural to use predictive performance on such pre-dictive tasks to improve the exploration of the poste-rior distribution. This approach, although never re-ported before to the best of our knowledge, simply makes the tuning process followed by many researchers explicit.
 We addressed the widely-experienced difficulty in tun-ing Hamiltonian-based Monte Carlo samplers by de-veloping algorithms for infinite adaptation of these Markov chains using Bayesian optimization. The adaptive Hamiltonian Monte Carlo and adaptive Rie-mann manifold HMC we developed automate the process of finding the best parameters that control the performance of the sampler, removing the need for time-consuming and expert-driven tuning of these samplers. Our experiments show conclusively that over a wide range of models and data sets, the use of adaptive algorithms makes it easy to obtain more effi-cient samplers, in some cases precluding the need for more complex approaches. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we hope that this paper removes a key obstacle towards the more widespread use of these samplers in practice.
 References
