 Search engines became a de facto place to start information acquisition on the Web. Though due to web spam phe-nomenon, search results are not always as good as desired. Moreover, spam evolves that makes the problem of provid-ing high quality search even more challenging. Over the last decade research on adversarial information retrieval has gained a lot of interest both from academia and industry. In this paper we present a systematic review of web spam de-tection techniques with the focus on algorithms and under-lying principles. We categorize all existing algorithms into three categories based on the type of information they use: content-based methods, link-based methods, and methods based on non-traditional data such as user behaviour, clicks, HTTP sessions. In turn, we perform a subcategorization of link-based category into five groups based on ideas and prin-ciples used: labels propagation, link pruning and reweight-ing, labels refinement, graph regularization, and feature-based. We also define the concept of web spam numerically and provide a brief survey on various spam forms. Finally, we summarize the observations and underlying principles applied for web spam detection.
 web spam detection, content spam, link spam, cloaking, collusion, link farm, pagerank, random walk, classification, clustering, web search, user behaviour, graph regularization, labels propagation Spam pervades any information system, be it e-mail or web, social, blog or reviews platform. The concept of web spam or spamdexing was first introduced in 1996 [31] and soon was recognized as one of the key challenges for search en-gine industry [57]. Recently [50; 97], all major search en-gine companies have identified adversarial information re-trieval [41] as a top priority because of multiple negative e  X  ects caused by spam and appearance of new challenges in this area of research. First, spam deteriorates the quality of search results and deprives legitimate websites of revenue that they might earn in the absence of spam. Second, it weakens trust of a user in a search engine provider which is especially tangible issue due to zero cost of switching from one search provider to another. Third, spam websites serve as means of malware and adult content dissemination and fishing attacks. For instance, [39] ranked 100 million pages using PageRank algorithm [86] and found that 11 out of top 20 results were pornographic websites, that achieved high ranking due to content and web link manipulation. Last, it forces a search engine company to waste a significant amount of computational and storage resources. In 2005 the total worldwide financial losses caused by spam were estimated at $50 billion [63], in 2009 the same value was estimated already at $130 billion [64]. Among new challenges which are emerging constantly one can highlight a rapid growth of the Web and its heterogeneity, simplification of content creation tools (e.g., free web wikis, blogging platforms, etc.) and decrease in website maintenance cost (e.g., domain reg-istration, hosting, etc.), evolution of spam itself and hence appearance of new web spam strains that cannot be cap-tured by previously successful methods.
 Web spam phenomenon mainly takes place due to the fol-lowing fact. The fraction of web page referrals that come from search engines is significant and, moreover, users tend to examine only top ranked results. Thus, [98] showed that for 85% of the queries only the first result page is requested and only the first three to five links are clicked [65]. There-fore, inclusion in the first SERP 1 has a clear economic in-centive due to an increase in website tra c. To achieve this goal website owners attempt to manipulate search engine rankings. This manipulation can take various forms such as the addition of a surrogate content on a page, excessive and undeserved link creation, cloaking, click fraud, and tag spam. We define these concepts in Section 2, following the work [54; 82; 13; 57]. Generally speaking, web spam man-ifests itself as a web content generated deliberately for the purpose of triggering unjustifiably favourable relevance or importance of some web page or pages [54]. It is worth mentioning that the necessity of dealing with the malicious content in a corpus is a key distinctive feature of adversarial information retrieval [41] in comparison with the traditional information retrieval, where algorithms operate on a clean benchmark data set or in an intranet of a corporation. According to various studies [85; 25; 12] the amount of web spam varies from 6 to 22 percent, which demonstrates the scope of the problem and suggests that solutions that re-quire manual intervention will not scale. Specifically, [7] shows that 6% of English language web pages were classi-fied as spam, [25] reports 22% of spam on a host level and [12] estimates it as 16.5%. Another group of researchers study not only the cumulative amount of spam but its dis-Search Engine Result Page.
 tribution among countries and top level domains [85]. They report 13.8% of spam in the English speaking internet, 9% in Japanese, 22% in German, and 25% in French. They also show that 70% of pages in the *.biz domain and 20% in the *.com domain are spam.
 This survey has two goals: first, it aims to draw a clear roadmap of algorithms, principles and ideas used for web spam detection; second, it aims to build awareness and stim-ulate further research in the area of adversarial information retrieval. To the best of our knowledge there is no com-prehensive but concise web spam mining survey with the focus on algorithms yet. This work complements existing surveys [54; 13; 82; 57] and a book [23] on the topic of web spam. [54] presents a web spam taxonomy and provides a broad coverage of various web spam forms and definitions. In [13] issues specific to web archives are considered. [82; 57] enumerate spam forms and discuss challenges caused by spam phenomenon for web search engines. [23] is a recent book, which provides the broadest coverage of web spam detection research available so far. We think that paral-lel research on email spam fighting [19; 95; 113] and spam on social websites [58] might also be relevant. [33] presents a general framework for adversarial classification and ap-proaches the problem from game-theoretical perspective. The paper is organized as follows. In Section 2 we pro-vide a brief overview of web spam forms following [54; 82; 13; 57]. Then we turn to content-based mining methods in Section 3.1. In Section 3.2 we provide a careful cover-age of link-based spam detection algorithms. In Section 3.3 we consider approaches to fight against spam using click-through and user behaviour data, and by performing real-time HTTP sessions analysis. Finally, we summarize key principles underlying web spam mining algorithms in Sec-tion 4 and conclude in Section 5. The content spam is probably the first and most widespread form of web spam. It is so widespread because of the fact that search engines use information retrieval models based on a page content to rank web pages, such as a vector space model [96], BM25 [94], or statistical language models [114]. Thus, spammers analyze the weaknesses of these models and exploit them. For instance, if we consider TFIDF scoring where q is a query, p is a page, and t is a term, then spam-mers can try to boost TF of terms appearing on a page 2 . There are multiple facets based on which we can categorize content spam.
 Taking a document structure into account there are 5 sub-types of content spamming. we assume that IDF of a term cannot be manipulated by a spammer due to an enormous size of the Web With the advent of link-based ranking algorithms [86; 68] content spam phenomenon was partially overcome. How-ever, spam is constantly evolving and soon afterwards spam-mers started constructing link farms [53; 3], groups of highly interconnected spam pages, with the aim to boost ranking of one or a small number of target pages in a farm. This form of spam is referred to as link spam. There are two major categories of link spam: outgoing link spam and incoming link spam. This is the easiest and cheapest method of link spam be-cause, first, a spammer have a direct access to his pages and therefore can add any items to them, and second, they can easily copy the entire web catalogue such as DMOZ 5 or Yahoo! Directory 6 and therefore quickly create a large set a visible caption for a hyperlink it is worth noting that the example also demonstrates idea of keyword repetition, because the word  X  X aptops X  appears three times in a tokenized URL representation dmoz.org http://dir.yahoo.com/ of authoritative links, accumulating relevance score. Out-going link spamming targets mostly HITS (Section 3.2.1 ) algorithm [68] with the aim to get high hub score. In this case spammers try to raise a PageRank (Section 3.2.1 score of a page (often referred to as a target page) or sim-ply boost a number of incoming links. One can identify the following strategies depending on an access to pages. now link farm maintenance is in dozens of times cheaper than before and the price is constantly decreasing some researchers consider it as a separate form of spam-ming technique Cloaking is the way to provide di  X  erent versions of a page to crawlers and users based on information contained in a re-quest. If used with good motivation, it can even help search engine companies because in this case they don X  X  need to parse a page in order to separate the core content from a noisy one (advertisements, navigational elements, rich GUI elements). However, if exploited by spammers, cloaking takes an abusive form. In this case adversary site owners serve di  X  erent copies of a page to a crawler and a user with the goal to deceive the former [28; 108; 110; 75]. For exam-ple, a surrogate page can be served to the crawler to manipu-late ranking, while users are served with a user-oriented ver-sion of a page. To distinguish users from crawlers spammers analyze a user-agent field of HTTP request and keep track of IP addresses used by search engine crawlers. The other strategy is to redirect users to malicious pages by executing JavaScript activated by page onLoad() event or timer. It is worth mentioning that JavaScript redirection spam is the most widespread and di cult to detect by crawlers, since mostly crawlers are script-agnostic [29]. Since search engines use click stream data as an implicit feedback to tune ranking functions, spammers are eager to generate fraudulent clicks with the intention to bias those functions towards their websites. To achieve this goal spam-mers submit queries to a search engine and then click on links pointing to their target pages [92; 37]. To hide anoma-lous behaviour they deploy click scripts on multiple ma-chines or even in large botnets [34; 88]. The other incentive of spammers to generate fraudulent clicks comes from online advertising [60]. In this case, in reverse, spammers click on ads of competitors in order to decrease their budgets, make them zero, and place the ads on the same spot. All the algorithms proposed to date can be roughly cate-gorized into three groups. The first one consists of tech-niques which analyze content features, such as word counts or language models [44; 85; 100; 81; 101; 89; 40], and con-tent duplication [42; 43; 103]. Another group of algorithms utilizes link-based information such as neighbour graph con-nectivity [25; 47; 49; 48; 119; 118; 116], performs link-based trust and distrust propagation [86; 55; 71; 99; 112; 12; 52; 26; 66; 11; 5; 8], link pruning [16; 84; 73; 93; 74; 35; 109; 32; 111; 115; 6], graph-based label smoothing [121; 1; 30], and study statistical anomalies [4; 7; 38; 9]. Finally, the last group includes algorithms that exploit click stream data [92; 37; 60] and user behaviour data [77; 78], query popularity information [28; 10], and HTTP sessions information [107]. Seminal line of work on content-based anti-spam algorithms has been done by Fetterly et al. [43; 44; 42; 85]. In [44] they propose that web spam pages can be identified through statistical analysis. Since spam pages are usually automat-ically generated, using phrase stitching and weaving tech-niques [54] and aren X  X  intended for human visitors, they exhibit anomalous properties. Researchers found that the URLs of spam pages have exceptional number of dots, dashes, digits and length. They report that 80 of the 100 longest discovered host names refer to adult websites, while 11 re-fer to financial-credit-related websites. They also show that pages themselves have a duplicating nature  X  most spam pages that reside on the same host have very low word count variance. Another interesting observation is that the spam pages X  content changes very rapidly 9 .Specifically,theystud-ied average amount of week-to-week changes of all the web pages on a given host and found that the most volatile spam hosts can be detected with 97.2% based only on this feature. All the proposed features can be found in the paper [44]. In their other work [42; 43] they studied content duplication and found that the largest clusters with a duplicate con-tent are spam. To find such clusters and duplicate content they apply shingling [22] method based on Rabin finger-prints [91; 21]. Specifically, they first fingerprint each of n words on a page using a primitive polynomial P A ,sec-ond they fingerprint each token from the first step with a di  X  erent primitive polynomial P B using prefix deletion and extension transformations, third they apply m di  X  erent fin-gerprinting functions to each string from the second stage and retain the smallest of the n resulting values for each of the m fingerprinting functions. Finally, the document is represented as a bag of m fingerprints and clustering is per-formed by taking the transitive closure of the near-duplicate relationship 10 . They also mined the list of popular phrases by sorting ( i, s, d ) triplets 11 lexicographically and taking suf-ficiently long runs of triples with matching i and s values. Based on this study they conclude that starting from the 36 th position one can observe phrases that are evidence of machine-generated content. These phrases can be used as an additional input, parallel to common spam words, for a  X  X ag of word X -based spam classifier.
 In [85] they continue their analysis and provide a handful of other content-based features. Finally, all these features are blended in a classification model within C4.5, boosting, and bagging frameworks. They report 86 . 2% true positive and 97 . 8% true negative rates for a boosting of ten C4.5 trees. Recent work [40] describes a thorough study on how various features and machine learning models contribute to the quality of a web spam detection algorithm. The au-thors achieved superior classification results using state-of-the-art learning models, LogitBoost and RandomForest, and it can even change completely on every request. two documents are near-duplicates if their shingles agree in two out of six of the non-overlapping runs of fourteen shingles [42] s is the i th shingle of document d only cheap-to-compute content features. They also showed that computationally demanding and global features, for in-stance PageRank, yield only negligible additional increase in quality. Therefore, the authors claim that more careful and appropriate choice of a machine learning model is very important.
 Another group introduces features based on HTML page structure to detect script-generated spam pages [103]. The underlying idea, that spam pages are machine-generated, is similar to the work discussed above [42; 43]. However, here authors make a non-traditional preprocessing step by removing all the content and keeping only layout of a page . Thus, they study page duplication by analyzing its layout and not content. They apply fingerprinting techniques [91; 21] with the subsequent clustering to find groups of struc-turally near-duplicate spam pages.
 There is a line of work dedicated to language modelling for spam detection. [81] presents an approach of spam detection in blogs by comparing the language models [59] for blog com-ments and pages, linked from these comments. The under-lying idea is that these models are likely to be substantially di  X  erent for a blog and a spam page due to random nature of spam comments. They use KL-divergence as a measure of discrepancy between two language models (probability distributions)  X  1 ,  X  2 : The beneficial trait of this method is that it doesn X  X  require any training data. [101; 89] extend the analysis of linguistic features for web spam detection by considering lexical validity, lexical and content diversity, syntactical diversity and entropy, emotive-ness, usage of passive and active voices, and various other NLP features.
 Finally, [10] proposes a number of features based on occur-rence of keywords on a page that are either of high advertis-ing value or highly spammed. Authors investigate discrim-inative power of the following features: Online Commercial Intention (OCI) value assigned to a URL by Microsoft Ad-Center 12 ,Yahoo! Mindsetclassificationofapageaseither commercial or non-commercial 13 , Google AdWords popular keywords 14 , and number of Google AdSense ads on a page 15 They report an increase in accuracy by 3% over the [24] which doesn X  X  use these features. Similar ideas were applied for cloaking detection. In [28] search engine query logs and online advertising click-through logs are analyzed with re-spect to query popularity and monetizability. Definition of query popularity is straightforward, query monetizability is defined as a revenue generated by all ads that are served in response to the query 16 . Authors use top 5000 queries out of each query category, request top 200 links for each query four times by providing various agent-fields to imitate requests by a user (u) and a crawler (c), and then apply a cloaking test (3), which is a modified version of a test pro-adlab.msn.com/OCI/oci.aspx mindset.research.yahoo.com adwords.google.com/select/keywordtoolexternal google.com/adsense overlap between these two categories is reported to be 17% posed in the earlier work on cloaking detection [108; 110]: where is a normalized term frequency di  X  erence for two copies of a page represented as a set of terms, may be with repetitions. They report 0.75 and 0.985 precision at high recall values for popular and monetizable queries correspondingly, which suggests that the proposed technique is useful for cloaking detection. However, the methods described in [28; 108; 110] might have relatively high false positive rate, since legiti-mate dynamically generated pages generally contain di  X  er-ent terms and links on each access, too. To overcome this shortcoming an improved method was proposed [75], which is based on the analysis of structural (tags) properties of the page. The idea is to compare multisets of tags and not words and links of pages to compute the cloaking score. All link-based spam detection algorithms can be subdivided into five groups. The first group exploits the topological relationship (distance, co-citation, similarity) between the web pages and a set of pages for which labels are known [86; 55; 71; 99; 112; 12; 52; 26; 66; 11; 5; 8]. The second group of algorithms focuses on identification of suspicious nodes and links and their subsequent downweighting [16; 84; 73; 93; 74; 35; 109; 32; 111; 115; 6]. The third one works by extracting link-based features for each node and use various machine learning algorithms to perform spam detection [4; 7; 38; 9]. The fourth group of link-based algorithms uses the idea of labels refinement based on web graph topology, when preliminary labels predicted by the base algorithm are modified using propagation through the hyperlink graph or a stacked classifier [25; 47; 49; 48]. Finally, there is a group of algorithms which is based on graph regularization tech-niques for web spam detection [121; 1; 30]. one can also analyze host level web graph [83] studies the robustness of PageRank and HITS algo-rithms with respect to small perturbations. Specifically, they analyzed how severely the ranks will change if a small portion of the Web is modified (removed). They report that PageRank is stable to small perturbations of a graph, while HITS is quite sensitive. [18] conducts a comprehensive anal-ysis of PageRank properties and how link farms can a  X  ect the ranking. They prove that for any link farm and any set of target pages the sum of PageRank scores over the target pages is at least a linear function of the number of pages in a link farm. [53] studies optimality properties of link farms. They de-rive the boosting factor for one-target spam farm and prove that this farm is optimal if all the boosting pages in a farm have no links between them, they point to the target page and target page links back to a subset of them. They also discuss motivations of spammers to collude (form alliances), and study optimality of web spam rings and spam quasi-cliques. There is also a relevant work which analyzes prop-erties of personalized PageRank [72] and a survey on e cient PageRank computation [14]. The key idea behind algorithms from this group is to con-sider a subset of pages on the web with known labels and then compute labels of other nodes by various propagation rules. One of the first algorithms from this category is TrustRank [55], which propagates trust from a small seed set of good pages via a personalized PageRank. The algorithm rely on the principle of approximate isolation of a good set  X  good pages point mostly to good pages. To select a seed set of reputable pages they suggest using an inverse PageRank, which operates on a graph with all edges reversed. Having computed inverse PageRank score for all pages on the Web, they take top-K pages and let human annotator to judge on reputation of these pages. Then they construct a person-alization vector where components corresponding only to reputable judged pages are non-zero. Finally, personalized PageRank is computed. TrustRank shows better properties than PageRank for web spam demotion.
 The follow up work on trust propagation is Anti-TrustRank [71]. Opposite to TrustRank they consider distrust propagation from a set of known spam pages on an inverted graph. A seed set is selected among pages with high PageRank values. They found that their approach outperforms TrustRank on the task of finding spam pages with high precision and is able to capture spam pages with higher PageRank values than TrustRank. There is also an algorithm, called Bad-Rank [99], which proposes the idea to compute badness of a page using inverse PageRank computation. One can say that the relation between PageRank and TrustRank is the same as between BadRank and Anti-TrustRank. [112] further researches the the idea of propagation by an-alyzing how trust and distrust propagation strategies can work together. First, they challenge the way trust is prop-agated in TrustRank algorithm  X  each child 18 gets an equal strategies: They also analyze various partial trust aggregation strate-gies, whereas TrustRank simply sums up trust values from each parent. Specifically, they consider maximum share strat-egy, when the maximum value sent by parents is used; and maximum parent strategy, when propagation is performed to guarantee that a child score wouldn X  X  exceed maximum of parents scores. Finally, they propose to use a linear com-bination of trust and distrust values: where  X  , 2 (0 , 1). According to their experiments, combi-nation of both propagation strategies result in better spam demotion (80% of spam sites disappear from the top ten buckets in comparison with the TrustRank and PageRank), maximum share with logarithmic splitting is the best way to compute trust and distrust values. The idea of trust and distrust propagation in the context of reputation systems was studied in [51].
 Two algorithms [12; 52] utilize PageRank decomposition property (Section 3.2.1 )toestimatetheamountofunde-served PageRank coming from suspicious nodes. In [12] the SpamRank algorithm is proposed; it finds supporters for a page using Monte Carlo simulations [46], assigns a penalty score for each page by analyzing whether personal-ized PageRank score PPR ( ~ j ) i is distributed with the bias towards suspicious nodes, and finally computes SpamRank for each page as a PPR with the personalization vector ini-tialized with penalty scores. The essence of the algorithm is in the penalty scores assignment. Authors partition all supporters for a page by their PageRank scores using bin-ning with exponentially increasing width, compute the cor-relation between the index of a bin and the logarithm of a count in the bin, and then assign penalty to supporters by summing up correlation scores of pages which they support. The insight behind the proposed approach is that PageR-ank follows power law distribution [87]. The concept of spam mass was introduced in [52]. Spam mass measures the amount of PageRank that comes from spam pages. Similar to TrustRank it needs the core of known good pages to esti-mate the amount of PageRank coming from good pages. The algorithm works in two stages. First, it computes PageR-ank ~ X  and TrustRank ~ X  0 vectors and estimates the amount of spam mass for each page using the formula ~ m = ~ X  ~ X  0 Second, the threshold decision, which depends on the value of spam mass, is made. It is worth noting that the algorithm can e  X  ectively utilize knowledge about bad pages. in this paragraph we will refer to out-neighbours of a page as children and in-neighbours as parents Credibility-based link analysis is described in [26]. In this work the authors define the concept of k-Scoped Credibility for each page, propose several methods of its estimation, and show how it can be used for web spam detection. Specifi-cally, they first define the concept of BadPath , a k-hop ran-dom walk starting from a current page and ending at a spam page, and then compute the tuned k-Scoped Credibility score as C ( p )= where k is a parameter specifying the length of a random walk, ( p ) is a credibility penalty factor that is needed to deal with only partial knowledge of all spam pages on the Web 19 ,and P path l ( p ) = can be used to downweight or prune low credible links before link-based ranking or to change the personalization vector in PPR, TrustRank, or Anti-TrustRank.
 In [66] the concept of anchor is defined, as a subset of pages with known labels, and various anchor-based proxim-ity measures on graphs are studied. They discuss personal-ized PageRank; harmonic rank, which is defined via random walk on a modified graph with an added source and a sink such that all anchor vertices are connected to a source and all vertices are connected to a sink with probability c ;non-conserving rank, which is a generalization of personalized PageRank satisfying the equation They report that non-conserving rank is the best for trust propagation, while harmonic rank better suits for distrust propagation.
 Spam detection algorithm utilizing pages similarity is pro-posed in [11], where similarity-based top-K lists are used to compute a spam score for a new page. Authors consider co-citation, CompanionRank, SimRank [61], and kNN-SVD projections as methods to compute similarity between pages. First, for a page to be classified a top-K result list is re-trieved using some similarity measure. Second, using the retrieved pages the following four values are computed: frac-tion of the number of labeled spam pages in the list (SR), anumberoflabeledspampagesdividedbyanumberofla-beled good pages in the list (SON), sum of the similarity values of labeled spam pages divided by the total similarity value of pages retrieved (SVR), and the sum of the simi-larity values of labeled spam pages divided by the sum of the similarity values of labeled good pages (SVONV). Third, threshold-based rule is used to make a decision. According to their experiments, similarity-based spam detection (SVR, SVONV) performs better at high levels of recall, while Anti-TrustRank [71] and combined Trust-Distrust [112] algorithms show higher precision at low recall levels.
 The seminal line of work was done by Baeza-Yates et al. [5; 8; 9; 7]. In [5], inspired by the PageRank representation (Equation 8), they propose the concept of functional rank , which is a generalization of PageRank via various damping functions. They consider ranking based on a general formula several strategies to define are proposed. and prove the theorem that any damping function such that the sum of dampings is 1 yields a well-defined normalized functional ranking. They study exponential (PageRank), linear, quadratic hyperbolic (TotalRank), general hyperbolic (HyperRank) damping functions, and propose e cient meth-ods of rank computation. In [8] they research the appli-cation of general damping functions for web spam detec-tion and propose truncated PageRank algorithm, which uses truncated exponential model. The key underlying observa-tion behind the algorithm is that spam pages have a large number of distinct supporters at short distances, while this number is lower than expected at higher distances. There-fore, they suggest using damping function that ignore the direct contribution of the first few levels of in-links In this work they also propose a probabilistic counting al-gorithm to e ciently estimate number of supporters for a page. Link-based feature analysis and classification mod-els using link-based and content-based features are studied in [7; 9] correspondingly. Algorithms belonging to this category tend to find unreli-able links and demote them. The seminal work [16] raises problems in HITS [68] algorithm, such as domination of mu-tually reinforcing relationships and neighbour graph topic drift, and proposed methods of their solution by augment-ing a link analysis with a content analysis. They propose to assign each edge an authority weight of 1 k if there are k pages from one site link to a single page on another site, and assign a hub weight of 1 l if a single page from the first site is pointing to l pages on the other site. To combat against topic drift they suggest using query expansion, by taking top-K frequent words from each initially retrieved page; and candidate page set pruning, by taking page relevance as a factor in HITS computation. The same problems are stud-ied in [84], where a projection-based method is proposed to compute authority scores. They modify eigenvector part of HITS algorithm in the following way. Instead of computing a principal eigenvector of A T A ,theycomputealleigenvec-tors of the matrix and then take the eigenvector with the biggest projection on the root set (set of pages originally re-trieved by keyword search engine, as in HITS), finally they report authority scores as the corresponding components of this eigenvector.
 Another group introduces the concept of tightly-knit com-munity (TKC) and proposes SALSA algorithm [73], which performs two random walks to estimate authority and hub scores for pages in a subgraph initially retrieved by keyword-based search. It is worth noting that the original and an in-verted subgraphs are considered to get two di  X  erent scores. An extension of this work [93] considers clustering structure on pages and their linkage patterns to downweight bad links. The key trick is to count number of clusters pointing to a page instead of number of individual nodes. In this case authority of a page is defined as follows: page p i . The approach acts like popularity ranking methods discussed in [20; 27]. [74] studies  X  X mall-in-large-out X  problem of HITS and pro-poses to reweight incoming and outgoing links for pages from the root set in the following way. If there is a page whose in-degree is among the three smallest ones and whose out-degree is among the three largest ones, then set the weight 4 for all in-links of all root pages, otherwise set to 1. Run one iteration of HITS algorithm without normalization. Then if there exists a root page whose authority value is among the three smallest ones and whose hub value is among the three largest ones, set the weight 4 for all in-links of all root pages, and then run the HITS algorithm again. [35] introduces the concept of  X  X eponistic X  links  X  links that present for reasons rather than merit, for instance, naviga-tional links on a website or links between pages in a link farm. They apply C4.5 algorithm to recognize neponis-tic links using 75 di  X  erent binary features such as IsSim-ilarHeaders, IsSimilarHost, is number of shared in-links is greater than a threshold. Then they suggest pruning or downweighting neponistic links. In their other work [109] they continue studying links in densely connected link farms. The algorithm operates in three stages. First, it selects a set of bad seed pages guiding by the definition that a page is bad if intersection of its incoming and outgoing neighbours is greater than a threshold. Second, it expands the set of bad pages following the idea that a page is bad if it points to a lot of bad pages from the seed set. Finally, links between ex-panded set of bad pages are removed or downweighted and any link-based ranking algorithm [86; 68] can be applied. Similar ideas are studied on a host level in [32]. In [111] the concept of a complete hyperlink is proposed, a hyperlink coupled with the associated anchor text, which is used to identify pages with suspiciously similar linkage patterns. Rationale behind their approach is that pages that have high complete hyperlink overlap are more likely to be machine-generated pages from a link farm or pages with duplicating content. The algorithm works as follows. First, it builds a base set of documents, as in HITS, and generates a page-hyperlink matrix using complete hyperlinks, where A ij =1,ifapage p i contains complete-hyperlink chl j .Then it finds bipartite components with the size greater than a threshold in the corresponding graph, where parts are pages and links, and downweight complete hyperlinks from large components. Finally, a HITS-like algorithm is applied on a reweighted adjacency matrix. [115] notices that PageRank score of pages that achieved high ranks by link-spamming techniques correlates with the damping factor c . Using this observation authors identify suspicious nodes, whose correlation is higher than a thresh-old, and downweight outgoing links for them with some function proportional to correlation. They also prove that spammers can amplify PageRank score by at most 1 c and experimentally show that even two-node collusion can yield a big PageRank amplification. The follow-up work [6] per-forms more general analysis of di  X  erent collusion topologies, where they show that due to the power law distribution of PageRank [87], the increase in PageRank is negligible for top-ranked pages. The work is similar to [53; 3]. Algorithms from this category represent pages as feature vectors and perform standard classification or clustering anal-ysis. [4] studies link-based features to perform website cate-gorization based on their functionality. Their assumption is that sites sharing similar structural patterns, such as average page level or number of outlinks per leaf page, share simi-lar roles on the Web. For example, web directories mostly consist of pages with high ratio of outlinks to inlinks, form a tree-like structure, and the number of outlinks increases with the depth of a page; while spam sites have specific topologies aimed to optimize PageRank boost and demon-strate high content duplication. Overall, each website is represented as a vector of 16 connectivity features and a clustering is performed using cosine as a similarity measure. Authors report that they managed to identify 183 web spam rings forming 31 cluster in a dataset of 1100 sites. Numerous link-based features, derived using PageRank, Trust-Rank, and truncated PageRank computation are studied in [7]. Mixture of content-based and link-based features is used to combat against web spam in [38; 9], spam in blogs [69; 76]. The idea of labels refinement has been studied in machine learning literature for general classification problems for a long time. In this section we present algorithms that ap-ply this idea for web spam detection. In [25] a few web graph-based refinement strategies are proposed. The algo-rithm in [25] works in two stages. First, labels are assigned using a spam detection algorithm discussed in [7]. Then, at the second stage labels are refined in one of three ways. One strategy is to perform Web graph clustering [67] and then refine labels guided by the following rules. If the ma-jority of pages in a cluster is predicted to be spam, they denote all pages in the cluster as spam. Formally, they as-sume that predictions of the base algorithm are in [0 , 1], then they compute the average value over the cluster and compare it against a threshold. The same procedure is done for non-spam prediction. The other strategy of label refine-ment, which is based on propagation with random walks, is proposed in [120]. The key part is to initialize the person-alization vector ~ r in PPR by normalizing the predictions of the base algorithm: r p = s ( p ) P tion of the base algorithm and r p is the component of the vector ~ r corresponding to page p . Finally, the third strategy is to use stacked graphical learning [70]. The idea is to ex-tend the original feature representation of an object with a new feature which is an average prediction for related pages in the graph and run a machine learning algorithm again. They report 3% increase over the baseline after two rounds of stacked learning.
 A few other relabelling strategies are proposed in [47; 49; 48]. [47] suggests constructing an absolutely new feature space by utilizing predictions from the first stage: the label by the base classifier, the percentage of incoming links coming from spam and percentage outgoing links pointing to spam, etc. Overall, they consider seven new features and report in-crease in performance over the base classifier. [48] proposes to use feature re-extraction strategy using clustering, propa-gation, and neighbour-graph analysis. Self-training was ap-plied in [49] so as to reduce the size of the training dataset requiring for web spam detection. Algorithms in this group perform trunsductive inference and utilize Web graph to smooth predicted labels. According to experimental results, graph regularization algorithms for web spam detection can be considered as the state-of-the-art at the time of writing. The work [121] builds a discrete analogue of classification regularization theory [102; 104] by defining discrete operators of gradient, divergence and Laplacian on directed graphs and proposes the following al-gorithm. First, they compute an inverse weighted PageRank with transition probabilities defined as a ij = w ji In ( p they build the graph Laplacian where  X  is a user-specified parameter in [0 , 1], A is a transi-tion matrix, and  X  is a diagonal matrix with the PageRank score 20 over the diagonal. Then, they solve the following matrix equation where ~ y is a vector consisting of { 1 , 0 , 1 } such that y page is normal, y i = 1ifitisspam,and y i = 0 if the label for a page is unknown. Finally, the classification decision is made based on the sign of the corresponding component of vector ~' . It is worth noting that the algorithm requires strongly connected graphs.
 Another algorithm that follows regularization theory is de-scribed in [1]. There are two principles behind it. First, it addresses the fact that hyperlinks are not placed at ran-dom implies some degree of similarity between the linking pages [36; 27] This, in turn, motivates to add a regularizer to the objective function to smooth predictions. Second, it uses the principle of approximate isolation of good pages that argues for asymmetric regularizer. The final objective function has the following form:  X  ( ~ w , ~ z )= 1 l where L ( a, b ) is a standard loss function, ~ w is a vector of coe cients, ~ x i and y i are feature representation and a true label correspondingly, z i is a bias term, a ij is a weight of the link ( i, j ) 2 E ,and ( a, b )=max[0 ,b a ] 2 is the reg-ularization function. The authors provide two methods to find the solution of the optimization problem using conju-gate gradient and alternating optimization. They also study the issue of weights setting for a host graph and report that the logarithm of the number of links yields the best results. Finally, according to the experimental study, the algorithm has good scalability properties.
 Some interesting idea to extract web spam URLs from SEO forums is proposed in [30]. The key observation is that on SEO forums spammers share links to their websites to find partners for building global link farms. Researchers pro-pose to solve a URL classification problem using features extracted from SEO forum, from Web graph, and from a website, and regularizing it with four terms derived from link graph and user-URL graph. The first term is defined as follows. Authors categorize actions on a SEO forum in three groups: post URL in a root of a thread (weight 3), post computed at the first stage.
 URL in reply (weight 2), view URL in previous posts (weight 1). Then they build a user-URL bipartite graph where an edge weight is the sum of all weights associated with the corresponding actions. After that they compute SimRank for all pairs of URLs and introduce the regularization term via Laplacian of the similarity matrix. The second regular-ization term is defined analogously, but now simply via a Laplacian of a standard transition matrix (see eq. 5). Third and fourth asymmetric terms, defined via the Web graph transition matrix and its diagonal row or column aggregated matrices, are introduced to take into account the principle of approximate isolation of good pages. Finally, the sound quadratic problem is solved. Authors report that even le-gitimately looked spam websites can be e  X  ectively detected using this method and hence the approach complements the existing methods. In this section we discuss algorithms that use non-traditional features and ideas to combat against web spam. The problem of unsupervised web spam detection is studied in [119; 118; 116]. Authors propose the concept of spamicity and develop an online client-side algorithm for web spam de-tection. The key distinctive feature of their solution is that it doesn X  X  need training data. At the core of their approach is a (  X  , k ) -page farm model introduced in [117], that allows to compute the theoretical bound on the PageRank score that can be achieved using the optimal page farm with a given number of pages and links between them. The proposed al-gorithm works as follows. For a given page it greedily select pages from the k-neighbourhood, that contribute most to the PageRank score, following the definition PRContrib ( v, p )= and at each iteration it computes the link-spamicity score as the ratio of the observed PageRank contribution from selected pages over the optimal PageRank contribution. The algorithm uses monotonicity property to limit number of supporters that needs to be considered. Finally, it marks the page as suspicious if the entire k-neighbourhood of this page is processed and link-spamicity score is still greater than a threshold. Analogous optimality conditions were proposed for a page content. In this case term-spamicity score of a page is defined as the ratio of the TFIDF score achieved by the observed content of a page over the TFIDF score that could be achieved by an  X  X ptimal X  page that has the same number of words. [77] proposes to incorporate user browsing data for web spam detection. The idea is to build a browsing graph G = ( V , E , T , ) where nodes V are pages and edges E are transi-tions between pages, T corresponds to a staying time and denotes the random jump probability, and then compute an importance for each page using PageRank-like algorithm. The distinctive feature of the proposed solution is that it considers continuous -time Markov process as an underly-ing model because user browsing data include time informa-tion. Formally, the algorithm works as follows. First, using staying times Z 1 ,...,Z m i for a page i it finds the diagonal element q ii of the matrix Q = P 0 ( t )asasolutionofthe optimization problem: Non-diagonal elements of the matrix Q are estimated as where additional N + 1 pseudo-vertex is added to the graph to model the teleportation e  X  ect such that all last pages in the corresponding browsing sessions are connected to it via edges with the weights equal to the number of clicks on the last page, and pseudo-vertex is connected to the first page in each session with the weight equal to normalized frequency of visits for this page; j is a random jump probability. At the second step, the stationary distribution is computed using the matrix Q , which is proven to correspond to the stationary distribution for the continuous-time Markov pro-cess. Similar idea is studied in [90]. The authors define a hyperlink-click graph, which is a union of the standard Web graph and a query-page click-log-based, and apply random walks to perform pages ranking.
 User behaviour data is also used in [78]. There are two observations behind their approach. First, spammers aim to get high ranking in SERP and therefore the major part of the tra c to spam website is due to visits from a search engine site. Second, users can easily recognize a spam site and therefore should leave it quite fast. Based on these observations authors propose 3 new features: ratio of visits from a search site, number of clicks and page views on a site per visit. Methods in this subsection can be naturally partitioned into 2 groups: client-side and server-side. The methods from the first group use very limited information, usually doesn X  X  re-quire learning, and less accurate. The latter group represen-tative methods, in reverse, are more precise since they can incorporate additional real-time information.
 Lightweight client-side web spam detection method is pro-posed in [107]. Instead of analyzing content-based and link-based features for a page, researchers focused on HTTP ses-sion information and achieved competitive results. They represent each page and a session, associated with it, as a vector of features such as IP-address or words from a request header in a  X  X ag-of-words X  model, and perform a classifica-tion using various machine learning algorithms. The same group introduced the way of large dataset creation for web spam detection by extracting URLs from email spam mes-sages [106]. Though not absolutely clean, the dataset con-tains about 350000 web spam pages.
 Similarly, HTTP sessions are analyzed, among others, in the context of malicious redirection detection problem. The au-thors of [29] provide a comprehensive study of the problem, classify all spam redirection techniques into 3 types (HTTP redirection, META Refresh, JavaScript redirection), analyze the distribution of various redirection types on the Web, and present a lightweight method to detect JavaScript redirec-tion, which is the most prevalent and di cult to identify type.
 The idea to use rank-time features in addition to query-independent features is introduced in [100]. Specifically, au-thors solve the problem of spam pages demotion after the query was issued guided by the principle that spammers fool ranking algorithms and achieve high positions using di  X  erent methods rather than genuinely relevant pages, and therefore spam pages should be outliers. Overall, 344 rank-time fea-tures are used such as number of query terms in title and frequency of a query term on a page, number of pages that contain a query term, n-gram overlaps between query terms and a page, for di  X  erent values of n and for di  X  erent skip n -grams . According to the experiment, the addition of rank-time features allows to increase precision by 25% at the same levels of recall. In the same work they study the problem of overfitting in web spam detection and suggest that train-ing and testing data should be domain-separated, otherwise testing error could be up to 40% smaller than the real. Since click spam aims to push  X  malicious noise  X  into a query log with the intention to corrupt data, used for the ranking function construction, most of the counter methods study the ways to make learning algorithms robust to this noise. Other anti-click-fraud methods are driven by the analysis of the economic factors underlying the spammers ecosystem. Interesting idea to prevent click spam is proposed in [92]. The author suggests using personalized ranking functions, as being more robust, to prevent click fraud manipulation. The paper presents a utility-based framework allowing to judge when it is economically reasonable to hire spammers to promote a website, and performs experimental study demon-strating that personalized ranking is resistant to spammers manipulations and diminishes financial incentives of site own-ers to hire spammers. The work [37] studies the robustness of the standard click-through-based ranking function con-struction process and also reports its resistance to fraudu-lent clicks.
 The work [60] studies the problem of click fraud for online advertising platform and particularly addresses the problem of  X  X ompetitor bankruptcy X . The authors present a click-based family of ads pricing models and theoretically prove that such models leave no economic incentives for spammers to perform malicious activity, i.e. short term competitor X  X  budget wasting will be annihilated by long term decrease in the ads placement price. [105] carefully analyzes the entire spammers X  ecosystem by proposing the spam double fun-nel model which describes the interaction between spam-publishers and advertisers via page redirections. In [17] an incentive based ranking model is introduced, which mainly incorporates users into ranking construction and pro-vides a profitable arbitrage opportunity for the users to cor-rect inaccuracies in the system. The key idea is that users are subject to an explicit information about revenue they might  X  X arn X  within the system if they correct an erro-neous ranking. It is theoretically proven that the model with the specific incentives (revenue) structure guarantees merit-based ranking and is resistant to spam. Having analyzed all the related work devoted to the topic of web spam mining, we identify a set of underlying principles that are frequently used for algorithms construction. In this work we surveyed existing techniques and algorithms created to fight against web spam. To draw a general pic-ture of the web spam phenomenon, we first provide numeric estimates of spam on the Web, discuss how spam a  X  ects users and search engine companies, and motivate academic research. We also presented a brief overview of various spam forms to make this paper self-contained and comprehensible to a broad audience of specialists. Then we turn to the dis-cussion of numerous algorithms for web spam detection, and analyze their characteristics and underlying ideas. At the end, we summarize all the key principles behind anti-spam algorithms.
 According to this work, web spam detection research has gone through a few generations: starting from simple content-based methods to approaches using sophisticated link min-ing and user behaviour mining techniques. Furthermore, current anti-spam algorithms show a competitive perfor-mance in detection, about 90%, that demonstrates the suc-cessful results of many researchers. However, we cannot stop here because spam is constantly evolving and still negatively a  X  ects many people and businesses. We believe that even more exciting and e  X  ective methods will be developed in the future.
 Among promising directions of research we identify click-fraud for online advertising detection and construction of platforms, which don X  X  have incentives for non-fair behaviour. For instance, pay-per-click models having this property will be very beneficial. Dynamic malicious redirection and de-tection of cloaking are still open issues. We also see the potential and need in anti-spam methods at the intersection of Web and social media. [1] J. Abernethy, O. Chapelle, and C. Castillo. Graph reg-[2] J. Abernethy, O. Chapelle, C. Castillo, J. Abernethy, [3] S. Adali, T. Liu, and M. Magdon-Ismail. Optimal [4] E. Amitay, D. Carmel, A. Darlow, R. Lempel, and [5] R. Baeza-Yates, P. Boldi, and C. Castillo. Generalizing [6] R. Baeza-Yates, C. Castillo, and V. L  X opez. Pager-[7] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and [8] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and [9] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and [10] A. Bencz  X ur, I. B  X  X r  X o, K. Csalog  X any, and T. Sarl  X os. Web [11] A. Bencz  X ur, K. Csalog  X any, and T. Sarl  X os. Link-based [12] A. A. Bencz  X ur, K. Csalog  X any, T. Sarl  X os, and M. Uher. [13] A. A. Bencz  X ur, D. Sikl  X osi, J. Szab  X o, I. B  X  X r  X o, Z. Fekete, [14] P. Berkhin. A survey on pagerank computing. Internet [15] A. Berman and R. Plemmons. Nonnegative Matri-[16] K. Bharat and M. R. Henzinger. Improved algorithms [17] R. Bhattacharjee and A. Goel. Algorithms and Incen-[18] M. Bianchini, M. Gori, and F. Scarselli. Inside pager-[19] E. Blanzieri and A. Bryl. A survey of learning-based [20] A. Borodin, G. O. Roberts, J. S. Rosenthal, and [21] A. Z. Broder. Some applications of rabin X  X  fingerprint-[22] A. Z. Broder, S. C. Glassman, M. S. Manasse, and [23] C. Castillo and B. D. Davison. Adversarial web search. [24] C. Castillo, D. Donato, L. Becchetti, P. Boldi, [25] C. Castillo, D. Donato, A. Gionis, V. Murdock, and [26] J. Caverlee and L. Liu. Countering web spam with [27] S. Chakrabarti. Mining the Web: Discovering Knowl-[28] K. Chellapilla and D. Chickering. Improving cloaking [29] K. Chellapilla and A. Maykov. A taxonomy of [30] Z. Cheng, B. Gao, C. Sun, Y. Jiang, and T.-Y. Liu. [31] E. Convey. Porn sneaks way back on web. The Boston [32] A. L. da Costa Carvalho, P. A. Chirita, E. S. de Moura, [33] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and [34] N. Daswani and M. Stoppelman. The anatomy of click-[35] B. Davison. Recognizing nepotistic links on the web. [36] B. D. Davison. Topical locality in the web. In Pro-[37] Z. Dou, R. Song, X. Yuan, and J.-R. Wen. Are click-[38] I. Drost and T. Sche  X  er. Thwarting the nigritude ul-[39] N. Eiron, K. S. McCurley, and J. A. Tomlin. Rank-[40] M. Erd  X elyi, A. Garz  X o, and A. A. Bencz  X ur. Web spam [41] D. Fetterly. Adversarial Information Retrieval: The [42] D. Fetterly, M. Manasse, and M. Najork. Detecting [43] D. Fetterly, M. Manasse, and M. Najork. On the evo-[44] D. Fetterly, M. Manasse, and M. Najork. Spam, damn [45] D. Fogaras. Where to start browsing the web. In Pro-[46] D. Fogaras and B. Racz. Towards scaling fully per-[47] Q. Gan and T. Suel. Improving web spam classifiers [48] G. Geng, C. Wang, and Q. Li. Improving web spam de-[49] G.-G. Geng, Q. Li, and X. Zhang. Link based small [50] googleblog.blogspot.com. [51] R. Guha, R. Kumar, P. Raghavan, and A. Tomkins. [52] Z. Gyongyi and H. Garcia-Molina. Link spam de-[53] Z. Gy  X ongyi and H. Garcia-Molina. Link spam al-[54] Z. Gyongyi and H. Garcia-Molina. Web spam taxon-[55] Z. Gy  X ongyi, H. Garcia-Molina, and J. Pedersen. Com-[56] T. H. Haveliwala. Topic-sensitive pagerank. In Pro-[57] M. R. Henzinger, R. Motwani, and C. Silverstein. [58] P. Heymann, G. Koutrika, and H. Garcia-Molina. [59] D. Hiemstra. Language models. In Encyclopedia of [60] N. Immorlica, K. Jain, M. Mahdian, and K. Talwar. [61] G. Jeh and J. Widom. Simrank: a measure of [62] G. Jeh and J. Widom. Scaling personalized web search. [63] R. Jennings. The global economic impact of spam. Fer-[64] R. Jennings. Cost of spam is flattening  X  our 2009 [65] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [66] A. Joshi, R. Kumar, B. Reed, and A. Tomkins. [67] G. Karypis and V. Kumar. Multilevel k-way partition-[68] J. M. Kleinberg. Authoritative sources in a hyper-[69] P. Kolari, A. Java, T. Finin, T. Oates, and A. Joshi. [70] Z. Kou and W. W. Cohen. Stacked graphical mod-[71] V. Krishnan and R. Raj. Web spam detection with [72] A. Langville and C. Meyer. Deeper inside pagerank. [73] R. Lempel and S. Moran. SALSA: the stochastic ap-[74] L. Li, Y. Shang, and W. Zhang. Improvement of hits-[75] J.-L. Lin. Detection of cloaked web spam by using tag-[76] Y.-R. Lin, H. Sundaram, Y. Chi, J. Tatemura, and [77] Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He, [78] Y. Liu, M. Zhang, S. Ma, and L. Ru. User behav-[79] C. D. Manning, P. Raghavan, and H. Schtze. Introduc-[80] O. A. Mcbryan. GENVL and WWWW: Tools for [81] G. Mishne, D. Carmel, and R. Lempel. Blocking blog [82] M. Najork. Web spam detection, 2006. [83] A. Y. Ng, A. X. Zheng, and M. I. Jordan. Link anal-[84] S. Nomura, S. Oyama, T. Hayamizu, and T. Ishida. [85] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. [86] L. Page, S. Brin, R. Motwani, and T. Winograd. The [87] G. Pandurangan, P. Raghavan, and E. Upfal. Using [88] Y. Peng, L. Zhang, J. M. Chang, and Y. Guan. An [89] J. Piskorski, M. Sydow, and D. Weiss. Exploring lin-[90] B. Poblete, C. Castillo, and A. Gionis. Dr. searcher [91] M. Rabin. Fingerprinting by Random Polynomials. [92] F. Radlinski. Addressing malicious noise in click-[93] G. Roberts and J. Rosenthal. Downweighting tightly [94] S. Robertson, H. Zaragoza, and M. Taylor. Simple [95] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. [96] G. Salton, A. Wong, and C. S. Yang. A vector [97] searchengineland.com. http://searchengineland.com/ [98] C. Silverstein, H. Marais, M. Henzinger, and [99] M. Sobek. Pr0 -google X  X  pagerank 0 penalty. badrank. [100] K. M. Svore, Q. Wu, C. J. C. Burges, and A. Ra-[101] M. Sydow, J. Piskorski, D. Weiss, and C. Castillo. Ap-[102] A. Tikhonov and V.Arsenin. Solutions of ill-posed [103] T. Urvoy, T. Lavergne, and P. Filoche. Tracking Web [104] G. Wahba. Spline models for observational data. [105] Y.-M. Wang, M. Ma, Y. Niu, and H. Chen. Spam [106] S. Webb, J. Caverlee, and C. Pu. Characterizing web [107] S. Webb, J. Caverlee, and C. Pu. Predicting web spam [108] B. Wu and B. Davison. Cloaking and redirection: A [109] B. Wu and B. D. Davison. Identifying link farm spam [110] B. Wu and B. D. Davison. Detecting semantic cloak-[111] B. Wu and B. D. Davison. Undue influence: elimi-[112] B. Wu, V. Goel, and B. D. Davison. Propagating trust [113] K. Yoshida, F. Adachi, T. Washio, H. Motoda, [114] C. Zhai. Statistical Language Models for Information [115] H. Zhang, A. Goel, R. Govindan, K. Mason, [116] B. Zhou and J. Pei. OSD: An online web spam de-[117] B. Zhou and J. Pei. Sketching landscapes of page [118] B. Zhou and J. Pei. Link spam target detection using [119] B. Zhou, J. Pei, and Z. Tang. A spamicity approach [120] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, [121] D. Zhou, C. J. C. Burges, and T. Tao. Transductive
