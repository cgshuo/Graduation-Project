 Unlike English text in which sentences are se-quences of words separated by white spaces, in Chi-nese text (as are some other languages including Arabic, Japanese, etc.), sentences are represented as strings of characters without similar natural delim-iters. Therefore, it is generally claimed that the first step in a Chinese language processing task is to iden-tify the sequence of words in a sentence and mark boundaries in appropriate places, which is refereed to as the task of Chinese Word Segmentation (CWS). (1) Input:  X   X   X   X   X   X   X 
Word segmentations in Chinese text do reduce ambiguities. In the example (1), the same span of text (the input) can convey entirely opposite mean-ings (the English sentences in parentheses) depend-ing on how word boundaries (CWS 1 and CWS 2) are labeled. Therefore, it is generally believed that more accurate word segmentations should benefit more the subsequent Chinese language processing tasks, such as part-of-speech tagging, named entity recognition, etc. There has been quite a number of research in the field of CWS to improve segmen-tation accuracy, yet its impact on the subsequent processing is relatively under-studied. Chang et al. (2008) explore how word segmentation improves machine translation; and Ni and Leung (2014) ex-plore how word segmentation impacts automatic speech recognition yet do not have conclusive find-ings. In this research, we aim to better understand how CWS benefits the subsequent NLP tasks, using semantic slot filling in spoken language understand-ing (SLU) and named entity recognition (NER) as two case studies.

In particular, we investigate the impact of Chinese word segmentation in three different situations.
First, assuming domain data (the data for a partic-ular subsequent task, e.g. SLU or NER) having no word boundary annotation (  X  4), we can apply word segmenters trained with publicly-available data to the domain data to get the word boundary. However, existing word segmenters may have a domain mis-match problem due to the fact that they may have different genre from the subsequent task and are usually segmented with different standards (Huang and Zhao, 2007). Therefore, we propose three tech-niques to solve this problem. Note, these techniques can be used together. 1) We use word segmentation outputs as ad-2) We adapt existing word segmenters with 3) We take advantage of the n-best list of word Second, assuming domain training data (e.g., NER) is already segmented with word boundary (  X  5), we are able to train a domain word segmenter with the data itself and apply it to the testing data. This allows us to see the differences between a word segmenter trained with in-domain data and one trained with publicly-available data.

Last, assuming both domain training and testing data have word boundary information (  X  5), it allows to explore the upper bound performance of the sub-sequent task with a perfect word segmenter.

Experimental results show that the proposed tech-niques do improve the end-to-end performance and we achieve an error rate reduction of 11% for SLU and 24% for NER over their corresponding baseline systems. In addition, we found that even a word seg-menter that is only moderately reliable is still able to improve the end-to-end performance, and a word segmenter trained with in-domain data is not neces-sarily better compared to a word segmenter trained with out-domain data in terms of the end-to-end per-formance. Word segmentation has received steady attention over the past two decades. People have shown that models trained with limited text can have a rea-sonable accuracy (Li and Sun, 2009; Zhang et al., 2013a; Li et al., 2013; Cheng et al., 2015). However, the fact is that none of existing algorithms is robust enough to reliably segment unfamiliar types of texts without fine-tuning (Huang et al., 2007). Several approaches have proposed to eliminate this issue, for example the use of unlabeled data (Sun and Xu, 2011; Wang et al., 2011; Zhang et al., 2013b) and partially-labeled data (Yang and Vozila, 2014; Taka-hasi and Mori, 2015). In our work, we encounter the same issue when applying word segmentation to the subsequent tasks and thus we propose three ap-proaches to solve this problem.

Word segmentation has been applied in several subsequent tasks, e.g. NER (Zhai et al., 2004), in-formation retrieval (Peng et al., 2002), automatic speech recognition (Ni and Leung, 2014), machine translation (Xu et al., 2008; Chang et al., 2008; Zhang et al., 2008; Zeng et al., 2014), etc. In gen-eral, there are two types of approaches to utilize word segmentation in subsequent tasks: pipeline and joint-learning . The pipeline approach creates word segmentation first and then feeds the segmented words into subsequent task(s). It is straightforward, but suffers from error propagation since an incor-rect word segmentation would cause an error in the subsequent task. The joint-learning approach trains a model to learn both word segmentation and the subsequent task(s) at the same time. A number of subsequent tasks have been unified into joint mod-els, including disambiguation (Wang et al., 2012), part-of-speech tagging (Jiang et al., 2008a; Jiang et al., 2008b; Zhang and Clark, 2010; Sun, 2011), NER (Gao et al., 2005; Xu et al., 2014; Peng and Dredze, 2015), and parsing (Hatori et al., 2012; Qian and Liu, 2012). However, the joint-learning process generally assumes the availability of manual word segmentations for the training data, which limits the use of this approach. Thus in this work, we focus on the pipeline approach, but instead of feeding the segmented words, we use word segmentation results as additional features in the subsequent tasks, which is more robust against error propagation. In this section, we describe how to integrate word segmentation information when domain data having no word boundary information, using SLU and NER as two case studies.

We first introduce the baseline system, and then describe the techniques that we propose to solve the domain mismatch problem when applying automatic CWS to the subsequent NLP tasks. 3.1 Baseline system Both of the SLU and NER can be formulated as sequence labeling tasks, and can be solved us-ing machine learning techniques such as Condi-tional Random Field (CRF), Recurrent Neural Net-work, or their combinations (Wan et al., 2011; Mesnil et al., 2015; Rondeau and Su, 2015). We adopt the tool wapiti (Lavergne et al., 2010), which is an implementation of CRF. In the base-line system, each Chinese character is treated as a labeling unit. Here is an example of our training sentences for SLU: X   X  | division  X  | division  X  | division  X  | street  X  | street  X  | street  X  | street  X  | locref  X  | locref  X  | unk  X  | query  X  | query X  (Find the restaurants near Sanyuanli Mogan Mountain road). The input features for training the baseline CRF model are character ngrams in the K-window and label bigrams. For computational efficiency, we use trigram within 5-character window. Given the current character c 0 , we extract the following char-acter ngram features: c  X  2 , c  X  1 , c 0 , c 1 , c 2 , c c 3.2 Using CWS as features When word segmentation information is not avail-able within the domain data, we can use publicly-available corpora such as the Chinese Tree Bank (Levy and Manning, 2003), to train an automatic word segmenter.

A dominant approach for supervised CWS is to formulate it as a character sequence labeling prob-lem, and label each character with its location in a word (Xue, 2003). A popular labeling scheme is  X  X IES X :  X  X  X  for the beginning character of a word,  X  X  X  for the internal characters,  X  X  X  for the ending character, and  X  X  X  for single-character word. Fol-lowing (Yang and Vozila, 2014), we train our au-tomatic word segmenter with CRF using the input features of character unigrams and bigrams, con-secutive character equivalence, separated character equivalence, punctuation, character sequence pat-tern, anchor of word unigram and bigram. This word segmenter achieves state-of-the-art or compa-rable performance.

A straightforward way to integrate word segmen-tation is the traditional pipeline approach. It uses word segmentation first and feeds the segmented words to subsequent task(s), named as Word Unit . However, this method suffers from the error propa-gation problem since an incorrect word segmenta-tion would cause an error in the subsequent task. Therefore, we proposed to use word segmentation outputs as additional features ( As Features ) in the subsequent tasks, as introduced below. We hypoth-esize the As Features is less sensitive to word seg-mentation errors since the CRF model can still rely on the character features when a word segmentation is not perfect.

Word Unit We can use segmented words instead
As Features We can still keep using characters 3.3 Adaptation with Partial-learning The publicly-available corpora for word segmenta-tion, however, may create a domain-mismatch prob-lem (especially for the SLU data). First, these cor-pora tend to be news articles and thus have differ-ent genre in content. Second, these corpora are usu-ally segmented with different standards (Huang and Zhao, 2007) and it is unclear which one would serve the purpose of the subsequent task.

Even if the NER/SLU task training data is not word segmented, the semantic slot and named en-tity labels actually provide valuable information on word boundaries. As illustrated in Fig. 1, the first character in an organization/person/location name can only be labeled as  X  X  X  or  X  X  X , while the last one can only be labeled as  X  X  X  or  X  X  X ; similarly, a character after a name can only be labeled as  X  X  X  or  X  X  X , while a character before a name can only be labeled as  X  X  X  or  X  X  X . We can thus create partially-labeled CWS data from SLU and NER la-bels. These partially-labeled data can then be used to adapt the out-of-domain word segmenter trained from publicly-available corpus.

T  X  ackstr  X  om et al. (2013) propose the approach partial-label learning to learn from partially-labeled data, and Yang and Vozila (2014) apply it to Chinese word segmentation. In partial-label training, each item in the sequence receives multiple labels, and each sequence has a lattice constraint, as shown in Fig. 1. The basic idea is to marginalize the probabil-ity mass of the constrained lattice in a cost function. The marginal probability of the lattice is defined as Equation 1, where C denotes the input character se-quence, L denotes the label sequence, and  X  Y ( C,  X  L ) denotes the constrained lattice (with regard to the in-put sequence C and the partial-labels  X  L ).
The optimization objective function is to maxi-mize the log-likelihood of the training set, in which likelihood is calculated via the probability mass of the constrained lattice, as shown in Equation 2. Here n denotes the number of sentences in the training set. BFGS can be used to optimize Equation 2. We ex-pect that this adaptation process should help to pro-vide better word segmentation information that fur-ther improves the subsequent task performance. 3.4 N-best CWS Only using the best word segmentation output as features for the subsequent tasks might not be suf-ficient (as we will show in our experiments). Indeed we can make use of the n-best word segmentation outputs. The task of SLU or NER is to find the best label sequence L , given the character sequence C , represented as arg max L P ( L | C ) . By including the word segmentation information, we can rewrite it by marginalizing over all possible word segmentations. arg max Where, W j is each possible word segmentation. This formula can be understood as two compo-nents: P ( W j | C ) is the word segmentation model and P ( L | W j ,C ) is the SLU/NER model. In prac-tice, we can use the n-best outputs associated poste-rior probabilities from the wapiti, for both P ( W j | C ) and P ( L | W j ,C ) . 3 In this section, we investigate the impact of CWS to the task of spoken language understanding (SLU) by making use of existing word segmenters trained with publicly-available data ( 1 st situation in  X  1). This is motivated by the fact that our SLU training and test-ing data are not pre-segmented by semantic word units.

We choose semantic slot filling in SLU because it is becoming popular as it is a critical component to support conversational virtual assistants, such as Ap-ple Siri, Samsung S Voice, Microsoft Cortana, Nu-ance Nina, just to name a few. The task of SLU is to convert a user utterance into a machine-readable se-mantic representation, which typically includes two sub-tasks: intent recognition and semantic slot fill-ing (Tur et al., 2013). Intent recognition is to de-termine the intention of the user utterance. For ex-ample, for the input utterance  X  X ook a ticket from Boston to Seattle X , SLU will determine that its in-tent is ticket-booking as opposed to music-playing . Semantic slot filling is to extract the designated slot values for the recognized intent from the input utter-ance. For example, SLU will extract  X  X epart:Boston X  and  X  X rrive:Seattle X  from the above user utterance. In this paper, we assume the availability and correct-ness of intent recognition, and focus only on seman-tic slot filling. 4.1 SLU experiments setting As described above, intent recognition is the first step in SLU, and the availability of which is assumed in this research work. We organize our training and testing data for semantic slot filling according to their intents. A single model for semantic slot filling is trained for each individual intent because different number of unique character 4,223 4,685 intents have different designated slots. For example, for the intent ticket-booking , the designated slots are the arrival and departure city/airport, airline, date, etc.; While the local-search intent is more interested in the city, address, street name, type of point of in-terest, etc. For evaluation, each model is applied to the corresponding intent X  X  testing data. At the end, we gather the automatic semantic labels of all intents in a pool and calculate F-measure.

Our SLU data consists of about 2 million sen-tences for training and 260 thousand sentences for testing, distributing into 170 intents. 4.2 Results and discussion We build two word segmenters from two public cor-pora, the Chinese Tree Bank 6 (CTB6) and the PKU corpus from the SIGHAN Bakeoff 2005, respec-tively. The data statistics of the two corpora are shown in Table 1.

The SLU performances are summarized in Ta-ble 2. Baseline using only character ngram features gives an F-measure of 93.92%. When switching to using automatic segmented words as the labeling units ( Word Unit ), the performance is a lot worse in both cases (87.10% for CTB6 and 88.68% for PKU). This indeed is not too surprising because errors in CWS propagate into SLU semantic slot filling. If an error results in a word crossing the boundary of se-mantic slots, it will definitely lead to an error in SLU semantic slot filling.

On the other hand, when supplying the automatic  X  X IES X  ngrams from CWS to SLU semantic slot fill-ing ( As Features ), we observe a nice gain in both cases, 94.41% for CTB6 and 94.13% for PKU. Us-ing the ngram  X  X IES X  as input features provides use-ful information of word segmentation to SLU se-mantic slot filling, while it is less sensitive to word segmentation errors. (2) Input:  X   X  [  X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X   X   X   X   X  ][  X   X  ]  X  [  X   X  ] 4
Example (2) illustrates that how CWS helps SLU semantic slot filling. For the sentence, the baseline system extracts  X   X   X   X   X  as a location name. How-ever, the word segmentation separates the words  X   X   X   X  (Hunan) and  X   X   X   X  (Finance), which reduces the probability score of  X   X   X   X   X  being a slot value because it crosses word boundaries. With CWS in-formation, the system is able to extract  X   X   X   X   X   X   X   X   X   X  (Hunan College of Finance and Economics) as a slot value. (3) Input:  X   X  [  X  X  X  X   X  X  X  X   X  X  X  X   X   X   X  ] (4) Input:  X  [  X  X  X  X   X  X  X   X  X  X  X   X  X  X  X  ]  X   X   X 
Adapting the word segmentation with SLU partially-labeled data gives further gain to seman-tic slot filling. In the case of CTB6 it reaches an F-measure of 94.47%, and 94.48% in PKU, using the ngram of  X  X IES X  labels from the adapted seg-menters. Here are two examples showing how the adaptation process further improves SLU. In the ex-ample (3), we have the incorrect word segmentation (CWS 1) before adaptation. It splits a word  X   X   X   X   X  (taobao.com) and thus labels  X   X   X   X   X   X   X  as a semantic slot. From the adaptation the system learns that  X   X   X   X   X  is a word, and it generates the correct word segmentation (CWS 2) and thus is able to cre-ate the correct semantic slot value  X   X   X   X   X   X   X   X  (the link of taobao.com). Similarly, in the example (4), the sentence is initially under-segmented (CWS 1) and it creates the incorrect semantic slot value  X   X   X   X   X   X   X . From the adaptation the system learns to put a word boundary between  X   X   X  and  X   X   X  and then the correct slot value  X   X   X   X   X   X  (Four Seasons Ho-tel) is extracted.

Finally, we take 10-best outputs from the adapted word segmenter, for each word segmentation gener-ate 10-best SLU outputs, sum up the probabilities, and search for the best semantic label sequence fol-lowing Equation 3. We further push the performance to an F-measure of 94.60% for CTB6 and 94.61% for PKU. Compared with the baseline system that uses character ngrams as input features, the infor-mation of CWS helps us achieve an error reduction of about 11%. In our experiments on SLU, we showed how CWS helps the subsequent task when no in-domain word segmentation data is available ( 1 st situation in  X  1). In this section, we investigate the impact of CWS to another important subsequent task: named en-tity recognition (NER). For the NER data we use, both the domain training and testing data have word boundary information, which allows us to explore the differences between word segmenters trained situation). It also allows us to see the performance of the subsequent task using manual word segmen-tation ( 3 rd situation). Moreover, it allows us to see the relationship between the performance of word segmentation and the end-to-end subsequent task. 5.1 NER experiments setting For NER experiments, we use the benchmark NER data from the third SIGHAN Chinese language pro-cessing Bakeoff (SIGHAN-3) (Levow, 2006). It consists of 46,364 sentences in the training set and 4,365 sentences in the testing set. These data are annotated with both word boundaries and NER in-formation. 5.2 Results and discussion Baseline system which only uses character ngram features (same configuration as the SLU task) gives the performance of 85.81% in F-measure, as shown
Oracle system uses character ngram features to-gether with manual in-domain word boundary in-formation during both training and testing, show-ing that perfect word segmentation information does help NER a lot. Again this suggests that good word segmentation does reduce ambiguities for the sub-sequent NLP tasks, as we argue in the introduction. Of course, since manual word segmentation is not generally available (esp. on testing), this raises the motivation of our research work: what is the impact of automatic CWS on NER and how to make the best out of it.
 To understand the impact of automatic CWS on NER, we discard the manual word segmentations in the NER data, and build two word segmenters from two public corpora, CTB6 and PKU respectively, same as we did for the SLU experiments. We also adapt them to NER with partial-label learning, and finally apply n-best CWS to NER decoding. Here we only report the results for As Features , as sum-marized in Table 3. Similar to SLU, when supply-ing the automatic  X  X IES X  ngrams from CWS to NER ( As Features ), we observe a nice gain in both cases of CTB6 and PKU . The NER F-measure improves to 86.40% and 87.05% respectively. In addition, adapting the word segmentation with NER partially-labeled data gives a further gain for both CTB6 and PKU, with an F-measure of 86.96% and 87.64% re-spectively. Note that, the adaptation process does improve the CWS performances for both CTB6 and PKU.
 In-domain CWS
NER system uses the NER training data to build a word segmenter and then apply it to the NER train-ing and testing data to extract the word segmenta-tion features. A naive thought is that it will result in a better NER performance than CTB6 and PKU since a word segmenter trained with the in-domain data should be better than one trained with publicly-available data due to the domain mismatch issue. As shown in Table 3, it is true that the word seg-mentation F-measures of NER are much better than CTB6 and PKU . However, to our surprise, the NER F-measure is only 83.45%, which is even worse than Baseline .

We hypothesize that this is due to the mismatch of the training CWS and testing CWS (as shown in Table 3, CWS F (train) and F (test)). When CWS ac-curacy is high on the training data, the NER model trained with such data puts more weight on word segmentation features rather than character features. However, during testing, the performance of CWS drops, resulting in more word segmentation errors, with a high chance to propagate to NER errors; even worse, a lot of these CWS errors are around NERs since a lot of NERs are OOVs and thus are challeng-ing to segment correctly. To test this hypothesis, we use 3-fold cross-validation to get the word boundary information during the CWS training, and thus the model is named as NER 3-fold . Note, although the performance of CWS decreases in the training, it has a more balanced CWS performance between train-ing and testing, which gives a better NER perfor-mance (improving 83.45% from NER to 86.80%). N-best CWS
The model N-best takes N -best outputs from the adapted word segmenter, for each word segmen-tation generate K -best NER outputs, sums up the probabilities and searches for the best named-entity label sequence following Equation 3. We can see a big jump in N-best performances for all the models in Table 3. This verifies our hypothesis that 1-best CWS is not sufficient.
To better understand how N-best helps NER, we vary the parameter N and the performance of NER ( K =1) is shown in Fig. 2. The N-best performance improves dramatically when N jumps from 1 to 2. After that the performance seems to quickly satu-rate. We also found that the performance does not change much when changing K . These results show that in practice we can set N =2 and K =1, which is cost-efficient.
 SIGHAN-3 evaluation
In the closed track evaluation of SIGHAN-3 (Levow, 2006), participants could only use the in-formation found in the provided training data. Our best model ( NER 3-fold ) belongs to this track since it uses only the word segmentation annotation in the training data set. Our model outperforms all the sub-missions as shown in Table 3. Furthermore, even if manual word segmentation does not exist in the data, the model CTB6 N-best and PKU N-best which us-ing existing word segmenters trained from publicly-available data can still outperform all the submis-sions in SIGHAN-3. Note that, these models use only character and word segmentation features with-out requiring additional name lists, part-of-speech taggers, etc. Chinese word segmentation is an important research topic and usually is the first step in Chinese natu-ral language processing, yet its impact on the sub-sequent processing is relatively under-studied. To our knowledge, this research work is the first attempt to understand in depth how automatic CWS impacts the two related subsequent tasks: SLU semantic slot filling and named entity recognition.

In this work, we proposed three techniques to solve the domain mismatch problem when applying CWS to other tasks: using word segmentation out-puts as additional features, adaptation with partial-learning and taking advantage of n-best list. All three techniques work for both tasks.

We also examined the impact of CWS in three different situations: First, when domain data has no word boundary information, we showed that a word segmenter built from public out-of-domain data is able to improve the end-to-end performance. In ad-dition, adapting it with the partially-labeled data de-rived from human annotation can further improve the performance. Moreover, marginalizing n-best word segmentations leads to further improvement. Second, when domain word segmentation is avail-able, the word segmenter trained with the domain data itself has a better CWS performance but it does not necessarily have a better end-to-end task perfor-mance. A word segmenter with more balanced per-formance on the training and testing data may obtain a better end-to-end performance. Third, when test-ing data is manually segmented, word segmentation does help the task a lot. This is not a typical use case in reality, but it does suggest that word segmenta-tion does reduce ambiguities for the subsequent NLP tasks.
 In the future, we can try to sequentially stack two CRFs (one for word segmentation and one of subse-quent task). We also would like to explore more sub-sequent tasks beyond sequence labeling problems. The authors would like to thank Paul Vozila, Xin Lu, Xi Chen, and Xiang Li for helpful discussions, and the anonymous reviewers for insightful comments.
