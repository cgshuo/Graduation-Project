 We present a general treatment of the problem of aggregat-ing preferences from several experts into a consensus rank-ing, in the context where information about a target ranking is available. Specifically, we describe how such problems can be converted into a standard learning-to-rank one on which existing learning solutions can be invoked. This transfor-mation allows us to optimize the aggregating function for any target IR metric, such as Normalized Discounted Cu-mulative Gain, or Expected Reciprocal Rank. When applied to crowdsourcing and meta-search benchmarks, our new al-gorithm improves on state-of-the-art preference aggregation methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Preference Aggregation, Meta-search, Crowdsourcing
Preference aggregation is the problem of combining multi-ple preferences over objects into a single consensus ranking. This problem is crucially important in many applications, such as information retrieval (IR), collaborative filtering and crowdsourcing. Across various domains, the preferences over objects are expressed in many different ways, ranging from full and partial rankings to arbitrary comparisons. For in-stance, in meta-search an issued query is sent to several search engines and the (often partial) rankings returned by them are aggregated to generate more comprehensive rank-ing results. In crowdsourcing, tasks often involve assigning ratings to objects or pairs of objects ranging from images to text. The ratings from several users are then aggregated to produce a single labelling of the data.

Research in preference aggregation has largely concen-trated on unsupervised aggregation, where the ground truth ranking is unknown and the aim is to produce an aggregate ranking that satisfies the majority of the observed prefer-ences. However, many of the recent aggregation problems are amenable to supervised learning, as ground truth rank-ing information is available. For example, in meta-search the documents retrieved by the search engines are often given to annotators who assign relevance labels to each document. The relevance labels provide ground truth preference infor-mation about the documents i.e., the documents with higher relevance labels are to be ranked above those with lower rel-evance. Similarly, in crowdsourcing, a domain expert typi-cally labels a subset of the data shown to the X  X rowd X . These labels are then used to evaluate the quality of annotations submitted by each worker. In these settings aggregating methods that aim to satisfy the majority often lead to sub-optimal results, since the majority preference can often be wrong. In meta-search for instance, many of the search en-gines are likely to return incorrect results for difficult long-tail queries, and we expect high quality rankings only from a few search engines that were specifically optimized for these queries. Consequently, we need the aggregating function to be able to  X  X pecialize X  and infer when to use the majority preference and when to concentrate only on a small sub-set of preferences. The specialization property is impossible to achieve with unsupervised preference aggregation tech-niques. There is thus an evident need to develop an effective supervised aggregation method that is able to fully utilize the labeled data.

In this paper we address this problem by developing a gen-eral framework for supervised preference aggregation. Our framework is based on a simple yet powerful idea of trans-forming the aggregation problem into a learning-to-rank one. This transformation makes it possible to apply any learning-to-rank method to optimize the parameters of the aggregat-ing function for the target metric. Experiments on a crowd-sourcing task from TREC2011 [17] and meta-search tasks with Microsoft X  X  LETOR4.0 [20] data sets show that our model significantly outperforms existing aggregation meth-ods.
A typical supervised preference aggregation problem con-sists of N training instances where for each instance n we are g iven a set of M n items. We also have a set of K  X  X xperts X  and each expert k generates a list of preferences for items in each instance. The preferences can be in the form of full or partial rankings, top-T lists, ratings, relative item compar-isons, or combinations of these. All of these forms can be converted to a set of partial pairwise preferences , which in most cases will be neither complete nor consistent. More-over, for each instance n we are also given a set of ground truth preferences over the items. The goal is to learn an ag-gregating function which takes expert preferences as input and produces an aggregate ranking that maximally agrees with the ground truth preferences.

In this work we concentrate on the rank aggregation in-stance of this problem from the information retrieval do-main. However, the framework that we develop is general and can be applied to any supervised preference aggregation problem in the form defined above. In information retrieval the instances correspond to queries Q = { q 1 ,...,q N } and items to documents D n = { d n 1 ,....,d nM n } , where M number of documents retrieved for q n . For each query q n experts X  preferences are summarized in a rank matrix R n , where R n ( i,k ) denotes the rank assigned to document d by the expert k  X  X  1 ,..,K } . Note that the same K experts rank items for all the instances, so K is instance indepen-dent. Furthermore, R n can be sparse, as experts might not assign ranks to every document in D n ; we use R n ( i,k ) = 0 to indicate that document d ni was not ranked by expert k . The sparsity arises in problems like meta-search where q n is sent to K different search engines and each search engine typically retrieves and ranks only a portion of the documents in D n . The ground truth preferences are expressed by the relevance levels L n = { l n 1 ,...,l nM n } which are typically as-signed to the documents by human annotators.

A ranking of documents in D n can be represented as a permutation of D n . A permutation  X  is a bijection  X  : D n  X  X  1 ,...,M n } mapping each document d ni to its rank  X  ( d ni ) = j , and d ni =  X   X  1 ( j ). Given the training queries the goal is to create an aggregating function such that, given a set of documents D with relevance levels L and ranking matrix R retrieved for a new query q , the ag-gregate ranking  X  produced by the function has maximal agreement with L .

Normalized Discounted Cumulative Gain (NDCG) [14] is typically used to evaluate the agreement between the ag-gregate permutation and the relevance levels. Given the aggregate permutation  X  , relevances L , and truncation level T  X  M , NDCG is defined as: where L (  X   X  1 ( i )) is the relevance level of the document in position i in  X  and G T ( L ) is a normalizing constant.
In contrast to the learning-to-rank problem where each document is represented by a fixed length, query dependent, and typically heavily engineered feature vector, in rank ag-gregation the rank matrix R is the only information avail-able to train the aggregating function. Additionally this ma-trix is typically very sparse. Hence there is no fixed length document description, as is required by most supervised methods. To overcome this problem we convert the rank matrix into a pairwise preference matrix and demonstrate that through this conversion the rank aggregation problem Figure 1: (a) An example rank matrix R where 4 documents are ranked by 3 experts. Note that in meta-search the rank for a given document can be greater than the number of documents in R. (b) The resulting pairwise matrix Y 2 for expert 2 (sec-ond column of R) after the ranks are transformed to pairwise preferences using the log rank difference method. can be cast into a learning-to-rank one which is well explored with many developed approaches readily available.
Given the M n  X  K ranking matrix R n our aim is to convert it into K M n  X  M n pairwise matrices Y n = { Y n 1 ,..., Y where each Y nk expresses the preferences between pairs of documents based on the expert k . To convert R n to Y n we use a transformation of the form Y nk ( i,j ) = g ( R n ( i,k ) , R n ( j,k )). In this work we experiment with three versions for g that were proposed by Gleich &amp; Lim [12]: 1. Binary Comparison 2. Rank Difference 3. Log Rank Difference In all cases both Y nk ( i,j ) and Y nk ( j,i ) are set to 0 if either R n ( j,k ) = 0 or R n ( i,k ) = 0 (missing ranking). Non-zero entries Y nk ( i,j ) represent the strength of the pairwise pref-erence of d ni over d nj expressed by expert k . Figure 1 shows an example ranking matrix and the resulting pairwise matrix after the ranks are transformed to pairwise preferences using the log rank difference method. Note that preferences in the form of ratings and top-N lists can easily be converted into V ariable Description Q = { q 1 , ...,q N } input queries D n = { d n 1 ,....,d nM n } documents for q n L n = { l n 1 ,...,l nM n } relevance levels for q n R n ( i,k ) ranking for d ni by expert k
Y n = { Y n 1 ,..., Y nK } pairwise representation of R n  X  ( d ni ) feature vector for d ni f (  X  ( d ni ) , W ) scoring function S n = { s n 1 ,...,s nM n } scores given by f to D n Y nk usi ng the same transformations. Moreover, if pairwise preferences are observed, we simply skip the transformation step and fill the entries Y nk ( i,j ) directly. Table 1 summa-rizes the notation used throughout this paper.

Working with pairwise comparisons has a number of ad-vantages, and models over pairwise preferences have been extensively used in areas such as social choice [10], infor-mation retrieval [16, 5], and collaborative filtering [22, 12]. First, pairwise comparisons are the building blocks of al-most all forms of evidence about preference and subsume the most general models of evidence proposed in literature. A model over pairwise preferences can thus be readily ap-plied to a wide spectrum of preference aggregation problems and does not impose any restrictions on the input type. Sec-ond, pairwise comparisons are a relative measure and help reduce the bias from the preference scale. In meta-search for instance, each of the search engines that receives the query can retrieve diverse lists of documents significantly varying in size. By converting the rankings into pairwise preferences we reduce the list size bias emphasizing the importance of the relative position.
Relevant previous work on rank aggregation can be di-vided into two categories: permutation-based and score-based. In this section we briefly describe both types of mod-els.
Permutation based models work directly in the permuta-tion space. The most common and well explored such model is the Mallows model [24]. Mallows defines a distribution over permutations and is typically parametrized by a cen-tral permutation  X  and a dispersion parameter  X   X  (0 , 1]; the likelihood of a ranking matrix R under this model is given by: where d ( R (: ,k ) , X  n ) is a distance between the ranking given by the expert k and the  X  n . For rank aggregation problems inference in this model amounts to finding the aggregate per-mutation  X  n that maximizes the likelihood of the observed rankings R . However, finding the most likely permutation is typically very difficult and in many cases is intractable [25]. A number of extensions of the Mallows model, e.g., [28, 22, 18, 19], have also recently been explored.

Another disadvantage of these models is that they are difficult to adapt to the fully supervised rank aggregation problem considered here. The dispersion parameter  X  and the parameters that define the distance function d are typ-ically estimated via maximum likelihood with  X  n clamped to the ground truth ranking. This learning procedure can-not be used to optimize a particular target metric, such as NDCG.
In score-based approaches the goal is to learn a set of real valued scores (one per document) S n = { s n 1 ,...,s which are then used to sort the documents. Working with scores avoids the discontinuity problems of the permutation space.

A number of heuristic score-based methods for rank aggre-gation have been proposed. For example, BordaCount [1], Condorcet [26] and Reciprocal Rank Fusion [8] derive the document scores by averaging (weighted) ranks across the experts or counting the number of pairwise wins. In statis-tics a popular pairwise score model is the Bradley-Terry [4] model: that document d ni beats d nj in the pairwise contest. Note that the scores do not depend on the expert k and thus rep-resent the consensus preference expressed by the experts. In logistic form the Bradley-Terry model is very similar to another popular pairwise model, the Thurstone model [29]. Extensions of these models include the Elo Chess rating sys-tem [11], adopted by the World Chess Federation FIDE in 1970, and Microsoft X  X  TrueSkill rating system [9] for player matching in online games, used extensively in Halo and other games. The popular learning-to-rank model RankNet [5] is also based on this approach. The Bradley-Terry model was later generalized by Plackett and Luce to a model for per-mutations [23, 27]. A Bayesian framework was also recently introduced for the Plackett-Luce model by placing a Gamma prior on the selection probabilities [13].

Recently, factorization approaches have been developed to model the joint pairwise matrix [12, 15]. There the pair-wise matrices are aggregated into a single matrix Y tot n k =1 Y nk which is then approximated by a low rank fac-torization such as: Y tot n  X  S n e T  X  eS T n . The resulting scores S n are used to rank the documents.

Finally, a supervised score-based rank aggregation ap-proach based on a Markov Chain was recently introduced [21]. In this model the authors use the ground truth prefer-ences to create a pairwise constraint matrix and then learn a scoring function such that the produced aggregate rankings satisfy as many pairwise constraints as possible. The main drawbacks of this approach are that it is computationally very intensive requiring constrained optimization (semidef-inite programming), and it does not incorporate the target IR metric into the optimization.

In general, none of the models mentioned above take the target metric into account during the optimization of the aggregating function. To the best of our knowledge no such approach has been developed.
In this section we describe our supervised framework for preference aggregation. To simplify notation, for the remain-der of this section, we drop the query index n and work with a general query q with M documents D = { d 1 ,....,d M } , relevance labels L = { l 1 ,...,l M } and ranking matrix R that has been converted to a set of pairwise matrices Y = { Y 1 ,..., Y K } using one of the techniques mentioned earlier.
The main idea behind our approach is to summarize the relative preferences for each document across the K experts by a fixed length feature vector. This transforms the pref-erence aggregation problem into a learning-to-rank one, and any of the standard methods can then be applied to opti-mize the aggregating function for the target IR metric such as NDCG. In the next section we describe an approach to extract the document features. Given the rank matrix R and the resulting pairwise matrix Y k for expert k (as shown in Figure 1), our aim is to convert Y k into a fixed length feature vector for each of the docu-ments in D . Singular Value Decomposition (SVD) based approaches for document summarization such as Latent Se-mantic Indexing are known to produce good descriptors even for sparse term-document matrices. Another advantage of SVD is that it requires virtually no tuning and can be used to automatically generate the descriptors once the pairwise ma-trices are computed. Because of these advantages we chose to use SVD to extract the features. For a given M  X  M pairwise matrix Y k the rank-p SVD factorization has the form: where U k is an M  X  p matrix,  X  k is an p  X  p diagonal ma-trix of singular values and V k is an M  X  p matrix. The full SVD factorization has p = M , however, to reduce the noise and other undesirable artifacts of the original space most applications that use SVD typically set p  X  M . Reducing the rank is also an important factor in our approach as pair-wise matrices tend to be very sparse with ranks significantly smaller than M .

Given the SVD factorization we use the resulting matrices as features for each document. It is important to note here that both U and V contain useful document information since Y k is a pairwise document by document matrix. To get the features for document d i and expert k we use: here U k ( i, :) and V k ( i, :) are the i  X  X h rows of U k respectively and diag(  X  k ) is the main diagonal of  X  k repre-sented as a 1  X  p vector. Note that the diag(  X  k ) component is independent of i and will be the same for all the docu-ments in D . We include the singular values to preserve as much information from the SVD factorization as possible. The features  X  ( d i , Y k ) summarize the relative preference information for d i expressed by the expert k . To get a com-plete view across the K experts we concatenate together the features extracted for each expert: Each  X  ( d i , Y k ) contains 3 p features so the entire represen-tation will have 3 Kp features. Moreover, note that since both K and p are fixed across queries this representation Fi gure 2: The flow diagram for the feature-based preference aggregation approach. (1) The ranking matrix R is converted to a set of pairwise matrices Y. (2) SVD is used to extract document features from each pairwise matrix Y k . (3) The learned scor-ing function is applied to the features to produce the scores for each document. The scores are then sorted to get the aggregate ranking. will have the same length for every document in each query. We have thus created a fixed length feature representation for every document d i , effectively transforming the aggrega-tion problem into a standard learning-to-rank one. Dur-ing training our aim is now to learn a scoring function f : R 3 Kp  X  R which maximizes the target IR metric such as NDCG. At test time, given a new query q  X  with rank ma-trix R  X  , we (1) convert R  X  into a set of pairwise matrices Y rank-p SVD; and (3) apply the learned scoring function to get the score for every document. The scores are then sorted to get the aggregate ranking. This process is shown in Fig-ure 2. The feature representation allows us to fully utilize the available labeled training data to optimize the aggregat-ing function for the target metric, which is not possible to do with the existing aggregation methods.

It is worth mentioning here that the SVD factorization of pairwise matrices has been used in the context of prefer-ence aggregation (see [12] for example). However, previous approaches largely concentrated on applying SVD to fill the missing entries in the joint pairwise matrix Y tot = and then use the completed matrix to infer the aggregate ranking. Our approach on the other hand uses SVD to com-press each pairwise matrix Y k and produce fixed length fea-ture vector for each document.
Given the document features extracted via the SVD ap-proach our goal is to use the labeled training queries to opti-mize the parameters of the scoring function for the target IR metric. The main difference between the introduced feature-based rank aggregation approach and the typical learning-to-rank setup is the possibility of missing features. When a given document d i is not ranked by the expert k the row Y k ( i, :) and column Y k (: ,i ) will both be missing (i.e., 0). To account for this we modify the conventional linear scoring function to include a bias term for each of the K experts: f (  X  ( d i ) , W ) = where W = { w k ,b k } K k =1 is the set of free parameters to be learned with each w k having the same dimension as  X  ( d i , Y k ), and I is an indicator function. The bias term b provides a base score for d i if d i is not ranked by expert k . The weights w k control how much emphasis is given to preferences from expert k . It is important to note here that the scoring function can easily be made non-linear by adding additional hidden layer(s) as done in conventional multilayer neural nets. In the form given by Equation 4 our model has a total of (3 p +1) K parameters to be learned. We can use any of the developed learning-to-rank approaches to optimize W ; in this work we chose to use the LambdaRank method. We chose LambdaRank because it has shown excellent em-pirical performance recently winning the Yahoo! Learning To Rank Challenge [7]. We briefly describe LambdaRank here and refer the reader to [6] and [5] for a more extensive treatment.

LambdaRank learns pairwise preferences over documents with emphasis derived from the NDCG gain found by swap-ping the rank position of the documents in any given pair, so it is a listwise algorithm (in the sense that the cost depends on the sorted list of documents). Formally, given a pair of documents ( d i ,d j ) with l i  X  = l j , the target probability that d should be ranked higher than d j is defined as: The model X  X  probability is then obtained by passing the dif-ference in scores between d i and d j through a logistic: The aim of learning is to match the two probabilities for every pair of documents with different labels. To achieve this a cross entropy objective is used: This objective weights each pair of documents equally thus placing equal importance on getting the relative order of document correctly both at the top and at the bottom of the ranking. However, most target evaluation metrics including NDCG are heavily concentrated on the top of the ranking. To take this into account the LambdaRank framework uses a smooth approximation to the gradient of a target evaluation measure with respect to the score of a document d i , and we refer to this approximation as  X  -gradient. The  X  -gradient for NDCG is defined as the derivative of the cross entropy ob-jective weighted by the difference in NDCG obtained when a pair of documents swap rank positions: Al gorithm 1 Learning Algorithm Inp ut: { ( q 1 , D 1 , L 1 , R 1 ) ,..., ( q N , D N , L
Parameters: learning rate  X  initialize weights: W for n = 1 to N do { feature extraction:  X  4.1 } end for repeat { scoring function optimization:  X  4.2 } until convergence Output: W Th us, at the beginning of each iteration, the documents are sorted according to their current scores, and the difference in NDCG is computed for each pair of documents by keeping the ranks of all of the other documents constant and swap-ping only the rank positions for that pair (see [6] for more details on  X  calculation). The  X  -gradient for document d is computed by summing the  X   X  X  for all pairs of documents ( d ,d j ) for query q : The |  X  NDCG | factor emphasizes those pairs that have the largest impact on NDCG. Note that the truncation in NDCG is relaxed to the entire document set to maximize the use of the available training data.

To make the learning algorithm more efficient the doc-ument features can be precomputed a priori and re-used throughout learning. This significantly reduces the com-putational complexity at the cost of additional storage re-quirement of O ( M n Kp ) for each query q n . The complete stochastic gradient descent learning procedure is summa-rized in Algorithm 1.
Our approach is primarily designed for supervised prefer-ence aggregation problems where sufficient expert preference data is available to learn a feature-based model for each ex-pert. However, to further examine the utility of the proposed method we also conducted experiments on the crowdsourc-ing problem where very limited preference data is typically available for each expert.
 For crowdsourcing experiments we use the dataset from Task 2 of the TREC2011 Crowdsourcing Track 1 [17]. The dataset is a collection of binary (relevant or not relevant) judgements from 762 workers for 19,033 documents. The documents are split into 100 topics and for each topic ground truth or  X  X old X  labels are provided for a subset of the docu-ments. The topics in this task can be thought of as queries in the traditional search problem. For each topic n the judge-ments for the documents are represented as a matrix R n h ttps://sites.google.com/site/treccrowd/home multiplied by 100.
 wh ere each entry R n ( i,k ) is either 1 (relevant), -1 (not rel-evant) or 0 (not observed). The ground truth (gold) labels L n are available only for a subset of the documents under each topic and are given by one of two values: 1 = relevant and 0 = not relevant. There are a total of 2275 gold labels: 1275 relevant and 1000 not relevant. The aim is to use the worker judgements to accurately rank the documents under each topic (note that this is a different problem from the Task 2 in TREC2011 where the goal was to predict each document X  X  relevance label). To evaluate the models we cre-ated 5 random splits of topics into 60/20/20 sets for train-ing/validation/testing. The results shown for each model are the average test set results for the five folds. For each topic the models are evaluated using only those documents for which the gold labels are available.

Across all topics the data contains 89,624 judgements and each document is judged by 5 workers on average. Moreover, out of the 762 workers 547 have less than 40 judgements so the dataset is very imbalanced with the majority of judge-ments coming from a small subset of the workers. Given the label sparsity there is not enough preference data to learn an accurate feature-based model for each worker. In-stead of extracting preference features for every worker, we used a hybrid approach. Separate features were extracted for workers that had 500 or more preferences in the training set, there were around 20 such workers in each fold. Prefer-ences for all the other workers were combined into a single preference matrix for which we extracted an additional set of features. Using  X  500 to denote the set of workers that have 500 or more preferences in the training set we can write the complete feature vector as:  X  ( d ni ) = {  X  ( d ni , Y k ) | k  X   X  500 } X   X  ( d ni , Y approach allows us to learn accurate feature-based models for those workers that have enough preferences in the train-ing data and combine these models with a consensus-based model for workers with few training ratings. Each pairwise matrix was computed using the binary comparison approach (see Section 2.1) since it is the only suitable approach for bi-nary judgements. We conducted experiments with three versions of the LambdaRank model. The first model, which we refer to as LR-consensus, did not use the per-worker features and only used features from the SVD factorization of the combined the individual worker preferences into account and ranks only based on consensus features. The second model, LR-labeled, uses the individual worker features as per equation Equation 5 but the features were computed using preferences only for the documents that had ground truth labels. The last model, LR-unlabeled, included the preferences for the unlabeled documents during feature computation. Through cross validation we found that setting p = 1 (SVD rank) gave the best performance which is expected considering the sparsity level of the pairwise matrices Y . For all settings, LambdaRank was run for 200 iterations with a learning rate of 0.01, and validation NDCG@10 was used to choose the best model.

We compare our feature based model with the condorcet and the majority vote approach which was shown to work well on the related TREC2011 Task 2 [3]. Table 2 shows test NDCG (N@T) at truncations 1  X  10. From the table it is seen that all LambdaRank models significantly outperform the majority vote baseline on the ranking task. It is also seen that the consensus based model LR-consensus performs significantly worse than the models that take the individual worker preferences into account. This indicates that ranking based on consensus alone leads to suboptimal performance, as we further demonstrate in the meta-search experiments in the next section. Finally, we see that using features com-puted from preferences for only the labeled documents (LR-labeled) does not perform as well as using all the preferences (LR-unlabeled). The LR-unlabeled model performs signifi-cantly better than the LR-labeled at all truncations. This indicates that preferences from the unlabeled documents do contain useful information and should be included during the feature extraction. This can be especially relevant when the number of labeled documents and/or preferences is small which is the case here.
For our meta-search experiments we use the LETOR4.0 benchmark datasets. These data sets were chosen because they are publicly available, include several baseline results, and provide evaluation tools to ensure accurate comparison between methods. In LETOR4.0 there are two meta-search data sets, MQ2007-agg and MQ2008-agg. MQ2007-agg con-tains 1692 queries with 69,623 documents and MQ2008-agg contains 784 queries and a total of 15,211 documents. Each query q n contains a ranking matrix R n with partial expert rankings of the documents under that query. There are 21 experts in MQ2007-agg and 25 experts in MQ2008-agg. In this setting the experts correspond to the search engines to which the query was submitted. In addition, in both data sets, each document is assigned one of three relevance levels: 2 = highly relevant, 1 = relevant and 0 = irrelevant. Fi-nally, each dataset comes with five precomputed folds with 60/20/20 splits for training/validation/testing. The results shown for each model are the averages of the test set results for the five folds. The MQ2007-agg dataset is approximately 35% sparse, meaning that for an average query the ranking are statistically significant.
 ma trix is missing 35% of its entries. MQ2008-agg is signifi-cantly sparser, with  X  65% entries missing.

The goal is to use the rank lists to infer an aggregate ranking of the documents for each query which maximally agrees with the held-out relevance levels. To evaluate this agreement, in addition to NDCG (N@T), we also compute Precision (P@T) and Mean Average Precision (MAP) [2]. MAP only allows binary (relevant/not relevant) document assignments, and is defined in terms of average precision (AP). For an aggregate ranking  X  , and relevance levels L , AP is given by: where L (  X   X  1 ( t )) is a relevance label for document in position t in  X  ; and P @ t is the precision at t : MAP is then computed by averaging AP over all queries. To compute P@t and MAP on the MQ-agg datasets the rel-evance levels are binarized with 1 converted to 0 and 2 con-verted to 1 (as per LETOR4.0 evaluation guidelines). All presented NDCG, Precision and MAP results are averaged across the test queries and were obtained using the evalua-tion script available on the LETOR4.0 website 2 .
To investigate the properties of our approach we con-ducted extensive experiments with various versions of the model. We experimented with binary, rank difference, and log rank difference methods to compute the pairwise ma-trices (see Section 2.1) and refer to these models as LR-I, resea rch.microsoft.com/en-us/um/beijing/projects/letor/ LR-R and LR-logR respectively. Through cross validation we found that setting p = 1 (SVD rank) gave the best perfor-mance. For all settings, LambdaRank was run for 200 itera-tions with a learning rate of 0.01, and validation NDCG@10 was used to choose the best model.

We compare the results of our model against the best methods currently listed on the LETOR4.0 website, namely the BordaCount model and the best of the three CPS mod-els (combination of Mallows and Plackett-Luce models) on each of the MQ-agg datasets. We also compare with the established meta-search standards Condorcet and Recipro-cal Rank Fusion (RRF) as well as the Bradley-Terry and Plackett-Luce models and the SVD-based method SVP. The above models cover all of the primary leading approaches in the rank aggregation research except for the Markov Chain model [21]. We were unable to compare with this method because it is neither publicly available nor listed as one of the baselines on LETOR, making standardized comparison impossible.

The NDCG and Precision results for truncations 1-5 as well as MAP results are shown in Table 3. From the table we see that all versions of the LambdaRank model significantly outperform the other aggregation methods, improving by as much as 5 NDCG points over the best baseline on each of the MQ-agg datasets. The results on the MQ2008-agg which is more than 65% sparse demonstrate that features extracted by the SVD approach are robust and generalize well even in the sparse setting. From the table we also see that the binary and log rank transformations lead to the best performance, with LR-logR significantly outperforming the other LambdaRank models on MQ2008-agg and having comparable performance to LR-I on MQ2007-agg.

To investigate the utility of representing each expert X  X  preferences with a separate set of features we ran experi-ments with a combined approach. For each query q n we combined all pairwise preference matrices into a single ma-matrix Y tot n (combined).
 relevant to this query. the SVD features from Y tot n only. Note that this model has no information about the individual expert preferences. We refer to this model as  X  X ombined X  and compare it to the full model which uses SVD features from each Y nk referred to as  X  X ndividual X . The results for the two datasets are shown in Figure 3. From the figure we see that the individual model significantly outperforms the combined one on both datasets. The performance of the combined model is compa-rable to the best baseline consensus method, the Reciprocal Rank Fusion. This is not surprising since Y tot n summarizes the consensus preference across the agents, and without ac-cess to the individual preferences the ranker can only learn to follow the consensus. Note however, that the gain from using the individual expert features is very significant which further supports the conclusion that specialization is very important for the supervised preference aggregation.
Figure 4 further demonstrates the importance of special-ization. The figure shows the expert ranking matrix together with the scores produced by the Reciprocal Rank Fusion and LR-logR for an example test query from MQ2008-agg. From the the ground truth relevance levels ( L ) it is seen that only document d 6 is relevant to this query. However, from the ranking matrix we see that the experts express strong net preference for documents d 1 , d 4 , d 5 and d 8 , whereas the pref-erences for d 6 are mixed with many positive and negative preferences. The Reciprocal Rank Fusion is a consensus-based approach and as such ranks documents d 1 , d 4 , d 5 d 8 above d 6 with NDCG@1-4 of 0. Other consensus-based methods produce similar rankings. LR-logR on the other hand, is able to ignore the consensus and concentrate on a small subset of the preferences, placing d 6 on top and pro-ducing a perfect ranking. Moreover, note that the scores for the other documents produced by the LR-logR are signifi-cantly lower than the score for d 6 , so the model is confident in this ranking. The query shown in Figure 4 is an exam-ple of a difficult query (often these correspond to long-tail queries) where the majority of experts generate incorrect preferences. For these queries the aggregated rankings pro-duced by the consensus-based methods will also be incorrect. Our feature-based supervised approach is able to fix this problem through specialization. By examining the queries in both MQ2008-agg and MQ2007-agg we found that both datasets contain a number of such queries and that our ap-proach performs significantly better on those queries than all of the consensus-based baselines.
We presented a fully supervised rank aggregation frame-work. Our approach transforms the rank aggregation prob-lem into a supervised learning-to-rank one. The transforma-tion is based on feature summarization of pairwise preference matrices. To compute the features we employ a low-rank SVD factorization. The extracted features allow us to use any supervised learning-to-rank approach to learn the ag-gregating function. This in turn allows us to optimize the aggregating function for any target IR metric. Experimental results with LambdaRank as the learning-to-rank approach show that our method outperforms the existing methods on supervised aggregation tasks. The improvements are espe-cially significant on difficult queries where the majority of preferences are wrong. Future work includes investigating other ways of producing effective fixed length representa-tions from full/partial preferences. [1] J. A. Aslam and M. Montague. Models for [2] R. Baeza-Yates and B. Ribeiro-Neto. Information [3] P. N. Bennett, E. Kamar, and G. Kazai. MSRC at [4] R. Bradley and M. Terry. Rank analysis of incomplete [5] C. J. C. Burges. From RankNet to LambdaRank to [6] C. J. C. Burges, R. Rango, and Q. V. Le. Learning to [7] O. Chapelle, Y. Chang, and T.-Y. Liu. The [8] G. V. Cormack, C. L. A. Clarke, and S. B  X  uttcher. [9] P. Dangauthier, R. Herbrich, T. Minka, and [10] H. A. David. The method of paired comparissons . [11] A. E. Elo. The rating of chess players: Past and [12] D. F. Gleich and L.-H. Lim. Rank aggregation via [13] J. Guiver and E. Snelson. Bayesian inference for [14] K. Jarvelin and J. Kekalainen. IR evaluation methods [15] X. Jiang, L.-H. Lim, Y. Yao, and Y. Ye. Statistical [16] T. Joachims. Optimizing search engines using [17] G. Kazai and M. Lease. TREC2011 Crowdsourcing [18] A. Klementiev, D. Roth, and K. Small. Unsupervised [19] G. Lebanon and J. Lafferty. Cranking: Combining [20] T. Liu, J. Xu, W. Xiong, and H. Li. LETOR: [21] Y.-T. Liu, T.-Y. Liu, T. Qin, Z.-M. Ma, and H. Li. [22] T. Lu and C. Boutilier. Learning Mallows models with [23] R. D. Luce. Individual choice behavior: A theoretical [24] C. L. Mallows. Non-null ranking models. Biometrika , [25] M. Meila, K. Phadnis, A. Patterson, and J. Bilmes. [26] M. Montague and J. A. Aslam. Condorcet fusion for [27] R. Plackett. The analysis of permutations. Applied [28] T. Quin, X. Geng, and T.-Y. Liu. A new probabilistic [29] L. L. Thurstone. The method of paired comparisons
