 Rank aggregation is a pervading operation in IR technology. We hypothesize that the performance of score-based aggregation may be affected by artificial, usually meaningless deviations consis-tently occurring in the input score distributions, which distort the combined result when the individual biases differ from each other. We propose a score-based rank aggregation model where the source scores are normalized to a common distribution before being combined. Early experiment s on available data from several TREC collections are show n to support our proposal. formation Storage and Retrieval ]: Information Search and Re-trieval  X  retrieval models. General Terms: Algorithms, Measurement, Performance. Keywords: Rank aggregation, score normalization, score distribution. Rank aggregation is a pervading operation in IR technology [5]. To name a few examples, rank aggregation takes place in the combina-tion of multiple relevance criteria in most search engines; in merg-ing the outputs of different engines for metasearch; in the combina-tion of query-based and preference-based relevance for personalized search [1]; or even in the comb ination of preferences from multiple users for collaborative retrieval. Both rank-based and score-based aggregation techniques have been expl ored in prior research [6]. We hypothesize that that the performance of score-based aggregation may be affected by artificial, us ually meaningless deviations con-sistently occurring in the input score distributions, which do not affect the performance of each ranking technique separately, but distort the combined result when the individual biases differ from each other, and therefore it should be possible to improve the results by undoing these deviations. In order to combine the scores produced by different sources, the values should be first made comp arable across input systems [2], which usually involves a normalization step [5]. In prior work, normalization typically consists of linear transformations [3], and other relatively straightforward, yet effective methods, such as normalizing the sum of scores of each input system to 1, or shift-ing the mean of values to 0 and scaling the variance to 1 [5]. But none of these strategies takes in to account the detailed distribution of the scorings, and they are thus sensitive to  X  X oise X  score biases. Furthermore, they normalize each single search result in isolation, and do not even take into account if the result is good or bad in comparison to other results from the same engine, whereby the best result of a very bad run ma y be assigned a similar normalized score as the best result of a very good one. We propose an effective, low-cost aggregation model where the source scores are normalized to a common score distribution, using a wider perspective of the hi storic behavior and particular trends of each rank source. Early experiments on available data from several TREC collections ar e shown to support our proposal. In our approach, we first generalize the notion of rank source as follows. Rather than considering a rank source as an input list of information objects, we define it by (we identify it to) a scoring function s :  X  s  X   X  , where  X  s is any retrieval space on which s is defined. For instance, for a search engine on a document collec-tion  X  , we would define  X  s =  X  X  X  , where  X  is the set of all que-each d  X  X  and q  X  X  . For a personalized content recommender we consider  X  s =  X  X  X  X  X  for a personalized search engine, and so on. In real applications, the scoring functions thus modeled are used by a) calling them on specific subsets  X  s  X   X  s , to be defined as the application requires, and b) using the values s ( x ) on x  X  X  duce a total order relation  X  s (a ranking) in  X  s . For instance, for a search engine, we would typically take  X  s =  X  X  { q }, where q  X  X  is the query entered by the user, so that (  X  s ,  X  s ) is the ranked result set for q from the search engine. Using this notation, we state th e rank fusion problem as follows. Let  X  be the set of rank sources to be merged, and let  X  X  with  X  s  X   X  s , be an arbitrary combination of input values for the rank sources, so that given  X  X  X  ,  X  s  X  X  s is the input to be ranked by s . For each  X  X  X  , we want to compute an aggregated score value ( ) s  X  R based on the individual scores s (  X  The way  X  is selected is arbitrary and we make no assumption about it in our technique. It is up to the application to build  X  in a senseful way. In most cases th is involves a fair amount of redun-dancy. For instance, it is very usual that  X  s and even  X  same for all s , e.g. when the application is a metasearch system that merges the output from diffe rent engines for a single query over the same collection. Howeve r this redundancy is irrelevant for the analysis, discussion, and definition of our theoretical model. On the other hand, it allows the maximum generality for the representation of arbitrary rank aggregation problems. Our model has the advantage that it makes it easier to model his-torical scoring data, which is at the core of our approach. Specifi-cally, we consider that the score functions s are called in succes-sive runs on different subsets  X  s  X   X  s , in a way that it is possible to collect the output values returned by s for each  X  s ing a statistic series of historical data H s , either in advance by collecting data during a certain period of regular use of the rank source, or dynamically at runtime. Now assuming we have a history H s for each source s , our score normalization technique works as follows. Given  X  X  X  , we com-pute the aggregated score () s  X  R by the following steps: a) Normalization. s (  X  s ) is normalized by a variant of the Rank-sim method [3], where instead of using a single run  X  ranked set of retrieval objects to ge t scores from, a larger set from several runs is used, namely H s , as follows: It can be seen that  X  s is an approximation to the cumulative distri-bution F s ( s (  X  s )) = P( x  X  s (  X  s )) over H biased by accidental characteristic s of the individual scoring sys-tem, which is compensated in step b of our method. b) Standardization. Assuming that we can define a common ideal distribution F : [0,1]  X  [0,1] free of any biases or noise, the potential biases of the individual score functions are compensated our method. One possible strategy would be to use exponential and Gaussian distributions, following the studies by Manmatha et al [4]. Alternatively, our current experi ment consists of approximating F as the cumulative statistical distribution obtained by a) normaliz-[3], and b) joining the normalized historical data from all the 
F t c) Combination. Finally, the normalized scores are merged e.g. by a linear combination or some other score-based technique. We have tested our techniques on f our test collections, namely the Web track of TREC8, TREC9, TREC9L, and TREC2001. For the comparative evaluation we have tr ied our technique with two refer-ence combination functions after the normalization step, to which we will refer as: a) DCombSUM, where the fused score is computed by the so-called CombSUM method [5]; and b) DCombMNZ, where ss technique named as CombMNZ in prior work [5]. We have compared these functions with other ones where the same combination step is used , but a different normalization method is applied. As a benchmar k for comparison, we have taken the results published in [6], which we label as SCombSUM (CombSUM with standard sc ore normalization), RCombSUM (CombSUM with Rank-sim nor malization), and SCombMNZ (CombMNZ with standard scor e normalization). Table 1 shows the average results over the four co llections. It can be seen that both DCombSUM and DCombMNZ are globally better that the other techniques. A lthough we only show the averaged results, this behavior is consistent over the four collections. Table 1. Average precision for 10 trials of the combination of 2 to 12 ranked lists, averaged over the 4 TREC collections. Based on the same data, Figure 1 gives an idea of the size of histori-cal data in H s needed for the method to reach a good performance. It can be seen that the requirements are far from expensive. 
Figure 1. Number of runs needed to reach performance. This research was supported by the EC (FP6-027685 MESH) and the Spanish Ministry of Science and Education (TIN2005-06885). [1] Castells, P. et al. Self-Tuning Personalised Information Retrieval [2] Croft, W. B. Combining approaches to information retrieval. In: [4] Manmatha, R., Rath, R., Feng, F. Modeling score distributions [5] Montague, M., Aslam, J.A. Relevance score normalization for [6] Renda, M. E., Straccia, U. Web metasearch: rank vs. score based 
