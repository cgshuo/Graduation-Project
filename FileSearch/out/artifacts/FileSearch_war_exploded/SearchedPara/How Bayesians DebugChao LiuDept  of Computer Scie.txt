
Manual debugging is expensive. And the high cost has motivated extensive research on automated fault lo-calization in both software engineering and data mining communities. Fault localization aims at automatically locating likely fault locations, and hence assists manual debugging. A number of fault localization algorithms have been developed in recent years, which prove effec-tive when multiple failing and passing cases are avail-able. However, we notice what is more commonly en-countered in practice is the two-sample debugging prob-lem, where only one failing and one passing cases are available. This problem has been either overlooked or insufficiently tackled in previous studies.

In this paper, we develop a new fault localization al-gorithm, named BayesDebug , which simulates some manual debugging principles through a Bayesian ap-proach. Different from existing approaches that base fault analysis on multiple passing and failing cases, BayesDebug only requires one passing and one failing cases. We reason about why BayesDebug fits the two-sample debugging problem and why other approaches do not. Finally, an experiment with a real-world program grep -2.2 is conducted, which exemplifies the effective-ness of BayesDebug .
Software reliability is one of the top concerns in modern industry. According to a report from the Na-tional Institute of Standards and Technology (NIST), software faults ( i.e. , bugs or errors) cost the U.S. econ-omy an estimated 59.5 billion dollars annually, or ap-proximately 0.6% of the GDP in 2002. But on the other hand, fault elimination through manual debug-ging is notoriously expensive. As computers become more and more powerful, one may naturally wonder whether computers can be used to automate the debug-ging process, or at least to some extent .

This problem, called fault localization , has been ex-tensively studied in both software engineering commu-nity [6,8,15] and data mining community (though less extensively) [9,10,16]. Fault localization aims at local-izing the underlying faults through automated analy-sis. With a localization result ( e.g. , what code regions likely contain the fault), a developer can prioritize the source code examination, and hence debug in a guided way. But one needs to keep in mind that although fault localization is sometimes called automated debugging , it is nevertheless a means of debugging assistance. Hu-man developers still need to precisely locate the fault and finally fix it.

Due to the practical importance, many fault local-ization algorithms have been developed in the past decade [6,8,13,15,16]. Among them, statistical debug-ging, represented by the algorithms Liblit05 [6] and Sober [8], is shown among the most effective [7]. In statistical debugging, the faulty program is first instru-mented with many program predicates. A predicate is a proposition about any program property, e.g. ,  X  idx &lt;= LENGTH  X . When a predicate is instrumented at a certain piece of code, it gets evaluated (as either true or false ) every time the associated code is executed. At run time, the evaluation history for each predicate is recorded, and finally each execution is profiled as a vector of predicate evaluations, with each dimension for one instrumented predicate.

When n passing case s ( i.e. , execution with correct outputs) and m failing case s ( i.e. , execution with incor-rect outputs) are available, statistical debugging algo-rithms, taking as inputs the profiled evaluation vectors, rank all instrumented predicates according to their cal-culated likelihood to be fault-relevant. In consequence, source code corresponding to the most relevant predi-cates is regarded the suspicious fault location. In pre-vious study [6,8], quality localization results have been observed with both Liblit05 and Sober when n and m are reasonably large, e.g. , in the tens or hundreds. For example, a number of previously unreported faults are found in bc-1.06 , EXIF-0.6.9 and Rhythmbox-0.6.5 , which have tens of thousands of lines of code. But, be-sides success stories, it is also noticed that when n and m decrease, the localization quality (not surprisingly) degrades.

Meanwhile, we also notice that in practice usually not many executions are available, and even worse, it is very likely that only one failing case is available. For example, when a failing case is encountered, a devel-oper rarely bothers with finding more cases before de-bugging. This puts forward the two-sample debugging problem: Given one failing execution and one passing execution, how can we obtain quality fault localization ? Here, a sample refers to a labelled execution, and an ex-ecution is  X  X abelled X  if it has been identified as either passing or failing. For automated fault localization, at least one passing case is needed, although a human developer may not need it.

In comparison with the situation when multiple passing and failing cases are available, the two-sample problem is more challenging and practically valuable. In the first place, with only two samples, existing sta-tistical approaches are no longer effective due to the small sample size (see details in Section 4). Secondly, the two-sample debugging problem is more commonly encountered in practice because labelled executions are expensive to collect. Specifically, because of the dis-tinct functionality of each program, developers need to prepare exclusive test cases for a given program. Also, developers need to label each execution as either pass-ing or failing themselves. Although some tools, like Korat [1], can help expedite test preparation, critical manual work is still unavoidable, especially for labelling executions. Finally, as we know, developers are usually reluctant to find more failing cases once one is encoun-tered.

In this paper, we present our investigation of the two-sample debugging problem. In the pursuit of a proper approach, we notice that the process of man-ual debugging exhibits some principles that could be potentially exploited by automated methods. To be specific, when a failing execution is encountered, the developer will trace it, and contrast it against what it should be. If  X  X hat it should be X  is considered as a passing execution, the manual debugging process just illustrates how to tackle the two-sample problem. A perusal of the above process reveals the following prin-ciple that successful manual debugging relies on: The developer has a clear notion of correctness ( i.e. , what the execution should be) and a notion of incorrectness ( i.e. , how the failing case executes); then the devel-oper contrasts the notion of incorrectness against that of correctness, and the place where the two notions di-verge is regarded as a possible fault location.
While inviting, a literal implementation of the above principle in computers is extremely hard. In conse-quence, several tradeoffs are made in this study. First, because no knowledge of the correct program behavior is known 1 , the notion of correctness is automatically derived from the given passing execution. Although the notion so derived could be far from what human developers think during debugging, it is, as we will see, nevertheless useful. Meanwhile, this derivation also fa-cilitates a higher level of automation. Secondly, notions of correctness and incorrectness are encoded based on predicate evaluations, but not in the form of artificial intelligence. Finally, notions are only contrasted at the end of the executions, rather than at certain suspi-cious locations during the executions as well, because these locations are exactly what a fault localization al-gorithm needs to find.

Following the above principle and tradeoffs, a new statistical debugging algorithm is developed in this study. As the first step, notions of correctness and in-correctness are formed based on the Bayes X  Theorem,
Specifically, let  X  (  X   X  [0 , 1]) represent the probability for a predicate P to be evaluated as true in passing ex-ecutions. Then P r (  X  ) is the prior notion of correctness about P before any evaluations are observed. With no prior belief, P r (  X  ) is a uniform prior U (0 , 1). Let X stand for the evaluations of P observed in the pass-ing execution, then P r (  X  | X p ) is the posterior notion of correctness after X p is observed. By treating X p as a series of independent Bernoulli trials, a simple cal-culation (based on Eq. (1)) shows that the posterior P r (  X  | X p ) is a Beta distribution. This distribution en-codes one X  X  belief in where, and in what probability,  X  should be between 0 and 1 after observing the cor-rect execution. Similarly, if X f is the evaluations in the failing execution, P r (  X  | X f ) embodies one X  X  poste-rior belief in  X  from the failing execution. In this way, the notions of correctness and incorrectness are math-ematically represented.

Once the closed form of P r (  X  | X p ) and P r (  X  | X f obtained, notion contrast is fairly straightforward al-though the actual mathematical derivation can be quite involved. As expected, the Kullback-Leibler divergence (a.k.a. KL divergence) is used to quantify the diver-gence between P r (  X  | X f ) and P r (  X  | X p ). Intuitively, the larger the divergence, the more likely the predicate P is fault-relevant. Finally, all instrumented predicates are ranked in the decreasing order of the divergence, and the source code corresponding to the top predicates is expected to be examined first. As this algorithm forms the notion of incorrectness and correctness based on the Bayes X  Theorem, it is named BayesDebug .
In summary, we make the following contributions in this paper. 1. We identify the two-sample debugging problem in 2. Inspired by manual debugging experiences, we de-3. This study provides yet another success story that
The rest of the paper is organized as follows. In Sec-tion 2, we first provide an example to illustrate how the notion of correctness and incorrectness is formed through a Bayesian approach. The development of BayesDebug and related proofs are presented in Sec-tion 3. We reason why BayesDebug , but not the existing algorithms, is suitable for the two-sample de-bugging problem in Section 4. Detailed experimental study and comparison are presented in Section 5, which is followed by discussion about related and future work in Section 6. Finally, Section 7 concludes this study. In this section, we illustrate the basic ideas of BayesDebug through a simple example. We hope this example could clarify the meaning of notions, and why notion divergence can be related to potential fault lo-cations.

Suppose we now encounter a failing execution f on a program P , together with one passing execution p . Let P denote one of the many instrumented predicates, and imagine a Bayesian sits right beside the predicate P , monitoring the evaluations of P during the two ex-ecutions. Let  X  be a random variable standing for the probability of P being evaluated as true in every eval-uation, we now illustrate how the Bayesian updates its belief in  X  as the program executes.

Before an execution starts, the Bayesian is totally uncertain about what value  X  could be between 0 and 1 in the passing and failing executions. The uncer-tainty is represented by a uniform prior as shown in Figures 1(a) and 1(d). Markers are for legibility with black/white printing. Then, let the execution p start. At some time in the execution, the Bayesian observes three true and one false executions of the predicate P . Based on this observation, the Bayesian will update its belief accordingly: rather than being any value be-tween 0 and 1 with equal probability,  X  is more likely to be larger than 0.5. Figure 1(b) exactly depicts this posterior, and it is a Beta distribution. Suppose five more true and one more false evaluations of P are observed until the end of p , the posterior belief of the Bayesian in  X  is shown in Figure 1(c), which repre-sents the notion of correctness held by the Bayesian on the evaluations of predicate P . Similarly, suppose P is evaluated three times as false and once as true in f , the notion of incorrectness is shown Figure 1, another Beta distribution.

As Figures 1(c) and 1(e) represent the Bayesian X  X  belief about  X  in passing and failing executions respec-tively, we then need to calculate the divergence be-tween them. The contrast is shown in Figure 1(f), and intuitively larger divergence implies more likelihood for predicate P to be fault-relevant because of their diver-gent evaluation patterns. A mathematical derivation of the divergence is presented in Section 3. In the end, a fault-relevance score is calculated for each in-strumented predicate, and source code corresponding to the most relevant predicates is taken as the possible fault location. In this section, we discuss in detail the design of BayesDebug . In Section 3.1, we first describe what predicates are instrumented in this study. Then we elaborate on how the notions of correctness and incor-rectness are formed, and how they are contrasted in Sections 3.2 and 3.3, respectively. Finally, Section 3.4 addresses some related issues to BayesDebug .
In order to profile executions, a subject program is first instrumented with predicates. In this study, two kinds of predicates, branch and call , are instrumented in subject programs. Specifically, for each branch B , two predicates B = true and B = false are instru-mented; and for each function call site C , six predicates are instrumented: C  X  0, C &gt; 0, C = 0, C 6 = 0, C &lt; 0, and C  X  0. By virtue of a compiler frontend, the in-strumentation is fully automated.

For each predicate P , the corresponding source code location L is called its instrumentation site . Equiva-lently, we say the predicate P points to L . During an execution, a predicate P is evaluated (as either true or false ) when its instrumentation site is executed. At the end of the execution, the evaluation history of each predicate is dumped out for subsequent analysis. Because this study treats each predicate equally and independent of each other, the following discussion ap-plies to any predicate P , and the given passing and failing executions in the two-sample debugging prob-lem are denoted by p and f , respectively.
Let X be a random variable representing the evalu-ation of predicate P in an execution. X = 1 if P evalu-ates true , and 0 if false . With independence assump-tion between evaluations, X conforms to a Bernoulli distribution with head-probability  X  , i.e. , Then, we are interested in whether  X  is the same be-tween the failing execution f and the passing execution p , and if not, how large the difference is. Intuitively, the larger the difference, the more likely the predicate P is fault-relevant.

Suppose the predicate P is evaluated n times, which are denoted by X = ( X 1 , X 2 , , X n ). The following lemma indicates that given a Beta distribution prior for  X  , the posterior about  X  is also a Beta distribution after X is observed.
 Lemma 1. Given a Beta distribution prior for  X  , i.e. , f (  X  |  X ) = Beta (  X ,  X  ) , where  X  denotes prior knowledge, and a series of observations X = ( X 1 , X 2 , , X n ) from the Bernoulli (  X  ) distribution, the posterior about  X  after X is observed is Proof. When X 1 is observed, the posterior of  X  is where B (  X ,  X  ) is the beta function, Therefore, the posterior about  X  after X 1 is observed is Beta (  X  + X 1 ,  X  +1  X  X 1 ). Through a simple induction on X 2 , X 3 , , we know the posterior about  X  after X is observed is
The proof of Lemma 1 describes how the posterior about  X  gets updated every time the predicate P is evaluated. At the beginning, with no prior belief in  X  , a uniform prior should be assigned to  X  , i.e. , f (  X  |  X ) = 1 / X ,  X   X  [0 , 1]. Because the uniform distribution is a special Beta distribution with parameters  X  =  X  = 1, Lemma 1 immediately gives the posterior belief for each predicate after either the passing execution p or the failing execution f is observed.
 Specifically, suppose X p = ( X 1 , X 2 , , X n ) and X f = ( X  X  1 , X  X  2 , , X  X  m ) represent the evaluations of a predicate P in p and f respectively, then the posterior of P from the passing execution p is  X  p  X  f (  X  | X p ) = Beta (1 + and similarly, the posterior of P from f is  X  f  X  f (  X  | X f ) = Beta (1 + For clear notation, we have saved  X  from the above expressions.

Go back to Figure 1. The distributions in Figures 1(c) and 1(e) are Beta (9 , 3) and Beta (2 , 4) respectively. In the next subsection, we mathematically quantify the divergence between f (  X  | X p ) and f (  X  | X f ).
Given the closed forms of f (  X  | X p ) and f (  X  | X f ), which are essentially two distributions, the Kullback-Leibler divergence is a natural choice to quantify the di-vergence. In its original form, given two continuous dis-tributions p ( x ) and q ( x ), the KL-divergence KL ( p || q ) is defined as With the closed form of f (  X  | X p ) and f (  X  | X f ), it is pos-sible to obtain an analytical form of the KL-divergence, and an analytical form, if obtained, could circumvent the instability and inefficiency of the numeric compu-tation of integrals.

For easy notation, let then the KL-divergence is calculated with p ( x ) = Beta ( c, d ) and q ( x ) = Beta ( a, b ). But, before plugging them into Eq. (2), we first give the following Lemma, which is essential for computing the KL-divergence be-tween two Beta distributions.
 Lemma 2.
 Z where c and d are positive integers,  X ( x ) = R Proof. Sketch of proof is in the appendix.

With Lemma 2, the following theorem gives an an-alytical form of the divergence between f (  X  f | X f ) and f (  X  p | X p ).
 Theorem 1 (Divergence Between Posteriors) . The KL-divergence between f (  X  p | X f ) and f (  X  f | X p ) , de-noted as KL (  X  f ||  X  p ) , is ln B ( a, b )
B ( c, d ) Proof. Sketch of proof is in the appendix.
Finally, we can take KL (  X  f ||  X  p ) as the fault-relevance score for the predicate P , and calculate it ac-cording to Theorem 1. Then all predicates are ranked in decreasing order, and top predicates are regarded more relevant to the fault. Since Bayesian modelling of  X  p and  X  f is used, this fault localization algorithm is named BayesDebug .
In this subsection, we discuss some related issues to the BayesDebug algorithm. First, the computa-tion efficiency. As shown in Theorem 1, the ana-lytical form of the KL-divergence is formidably com-plex. For example, computing the beta function B requires integrals, and the digamma function  X  fur-ther involves derivatives of integrals. However, thanks to the development in numeric computation of special functions [12], both B and  X  can be computed within O (1) time. Therefore, once the executions p and f exit, the fault-relevance score for each predicate can be calculated within O (1) time. Finally, a sorting of all instrumented predicates according to the scores is needed. Therefore, suppose k predicates are instru-mented, the computation complexity of BayesDebug is O ( k log( k )).

Secondly, as one may have noticed, the localization quality could be sensitive to the given passing and fail-ing cases. Intuitively, the failing execution f should be similar to the given passing one in order to obtain qual-ity localization result. If it is, the evaluation pattern of most predicates would be roughly the same, and in consequence, real fault-relevant predicates will be put at the top. So far, no formal justification exists for the  X  X imilarity implies quality result X  principle although some previous work on fault localization proposes sim-ilar conjectures [13,15]. Some experimental results on this will be presented in Section 5.
In this section, we compare BayesDebug , from the methodology point of view, with three well-established fault localization algorithms, namely, Sober [8], Liblit05 [6] and Delta Debugging (abbreviated as DeltaDebug ) [15]. Specifically, we discuss the methodology difference between BayesDebug and each of them. This discussion illustrates why BayesDebug is suitable for the two-sample debugging problem, and why the other three algorithms are not. Meanwhile, because we will compare BayesDebug with them through experiments in the next section, this section also serves as an introduction to the three algorithms.
Sober is a statistical debugging algorithm recently proposed by Liu et al. [8]. For each predicate P , Sober models each evaluation as an independent Bernoulli trial with head-probability  X  . Furthermore, Sober re-gards that there exist two distributions f (  X  |  X  p ) and f (  X  |  X  f ) that govern the head probability  X  for passing and failing executions respectively. For each passing (or failing) execution, the true evaluation ratio of P , calculated by dividing the number of true evaluations over the total number of evaluations, is taken as an ob-servation from f (  X  |  X  p ) (or f (  X  |  X  f ) correspondingly). When n passing and m failing executions are avail-able, the n and m observations constitute one sample from f (  X  |  X  p ) and f (  X  |  X  f ) respectively. Finally, the divergence between f (  X  |  X  p ) and f (  X  |  X  f ) is quantified in a similar way to the two-sample hypothesis testing. Readers interested in the details are referred to [7].
The above description reveals some similarities be-tween Sober and BayesDebug . First, both methods model predicate evaluations as independent Bernoulli trials. Secondly, both methods base fault localization on the head probability  X  . Moreover, in both methods, two probability distributions are used to describe the uncertainty of  X  . Finally, both methods relate the prob-ability divergence to the fault-relevance score. There-fore, at the first glance, BayesDebug follows the route of Sober .

However, a perusal of the two algorithms ex-poses important differences. First, BayesDebug and Sober approach the problem of fault localization in a Bayesian and a Frequentist way, respectively. This difference manifests itself in the meaning of the distri-bution of  X  . In Sober , f (  X  |  X  p ) is a fixed (although unknown) distribution that governs the head probabil-ity of predicate P in passing executions. In contrast, the f (  X  p | X p ) in BayesDebug is the posterior belief in  X   X  X  distribution after the evaluations X p are observed from the passing execution. f (  X  p | X p ) gets updated ev-ery time P is evaluated during the execution p while f (  X  |  X  p ) is regarded fixed throughout all passing execu-tions.

Secondly, while f (  X  p | X p ) is obtained from one exe-cution in BayesDebug , Sober essentially needs mul-tiple executions to estimate f (  X  |  X  p ). For this reason, Sober has a hard time to handle the two-sample de-bugging problem because only one observation is avail-able for the estimation of f (  X  |  X  p ). In contrast, since each evaluation is taken as an independent observation, BayesDebug actually collects multiple observations from each execution. Finally, the two algorithms also fundamentally differ in their approaches to quantifying the distribution divergence. Sober measures the di-vergence through a hypothesis testing approach, while BayesDebug simply gives an analytical form of the divergence. Therefore, BayesDebug is fundamentally different from Sober although they look similar at the first glance.

In previous study, Sober is observed to be the most effective debugging algorithm, evaluated when multi-ple failing and a great number of passing executions are available. However, for the two-sample debugging problem, Sober may not be able to correctly estimate the  X  distribution in both failing and passing executions (due to high estimation variance). In consequence, the divergence measured through hypothesis testing is also less credible. Experimental comparison between BayesDebug and Sober is presented in Section 5.
Besides Sober , Liblit05 is another representative of statistical debugging algorithms [6]. Different from both Sober and BayesDebug , Liblit05 sifts fault-relevant predicates by examining how more likely an execution fails when a predicate evaluates true than when the predicate is ever evaluated as either true or false . Specifically, Liblit05 defines the following two probabilities for each predicate P , and takes the probability difference as one of the two key components of P  X  X  fault-relevance score. The other component is invalid in the two-sample debugging problem setting, and is hence ig-nored in this discussion.

When multiple passing and failing executions are available, Context ( P ) is estimated by the ratio of fail-ing executions among all executions in which the pred-icate P is ever evaluated. Similarly, F ailure ( P ) is es-timated by the ratio of failing executions among all executions in which the predicate P is ever evaluated true . Increase ( P ) is then the difference between the two estimations. Because only two observations (one from the failing and one from the passing execution) are available for estimation, we envision that Liblit05 will also have a hard time as Sober for the two-sample de-bugging problem. This is confirmed in the experiments in Section 5.
Finally, we would like to compare BayesDebug with DeltaDebug , which is proposed by Zeller in [15]. Different from both Sober and Liblit05 , DeltaDebug rightly fits the two-sample debugging problem because it literally contrasts one failing execu-tion against a passing one. The contrast is carried on the memory graphs that are induced from the two exe-cutions. Intuitively, a memory graph is a graphic repre-sentation of the reference relationship between program variables. By systematically exchanging the content of the two graphs, DeltaDebug finally locates those failure-inducing variables, and these variables and their associated source code are regarded as a fault localiza-tion result.

The DeltaDebug service is available online at www.askigor.com , where a command-line utility is also ready for offline use. DeltaDebug is effective if the underlying fault incurs some memory abnormality, like access violations or reference circles that should not ex-ist. However, if the fault manifest no abnormality at the memory level, DeltaDebug is no longer effective. For example, most logic faults would only result in er-roneous outputs without program crashes. For such cases, predicate-based debugging algorithms are more effective than DeltaDebug , as we will see in the fol-lowing section. In this section, we evaluate the effectiveness of BayesDebug with experiments. We first describe the subject program and the seeded faults in Section 5.1. Then in Sections 5.2 and 5.3, we examine the effective-ness of BayesDebug , and compare it with Sober , Liblit05 and DeltaDebug . The sensitivity of local-ization quality is studied in Section 5.4, where the com-putation cost is also briefly discussed. Finally, Section 5.5 discusses some threats to validity, and closes this experimental study.
We choose grep -2.2 as the subject program in this study. The grep program searches one or more input files for lines containing a match to a specified pattern, and prints out matching lines. It has 9,745 lines of C code, as measured by the SLOCCount tool 2 . Two logic faults are manually seeded in the source code, and they are shown in Figure 2.

The first fault (on the left) is an  X  X ff-by-one X  error: an expression  X +1 X  is appended to line 553 in the grep.c file. The second fault (on the right) is a  X  X ubclause-missing X  error. The subclause (lcp[i] == rcp [i]) at line 2270 in file dfa.c is commented out.
Although these two faults are manually injected, they do mimic realistic logic errors. Explicitly, when developers are obscure about corner conditions, logic errors like  X  X ff-by-one X  or  X  X ubclause-missing X  may sneak in. Because logic errors, like these two, gener-ally do not incur segmentation faults, they are usually harder to debug than those with program crashes. In the next subsection, we illustrate how BayesDebug helps developers find these two faults separately.
We now examine the effectiveness of BayesDebug in localizing the two seeded faults respectively. Accord-ing to the instrumentation schema described in Sec-tion 3.1, 3464 branch and 1404 call predicates are instrumented in the grep subject. The BayesDebug is implemented within Matlab, and all experiments are carried out on a Pentium-IV Linux machine running Fedora Core 2.
 P 1 (beg != lastout)=true Line 574 of grep.c P 2 (lastout)=true Line 549 of grep.c
We first enable the first fault only, and construct two test cases such that one is passing and the other is failing. We denote them as p 1 and f 1 respectively. After applied to the execution traces of p 1 and f 1 , BayesDebug generates a ranking of all instrumented predicates, with the top two predicates shown in Table 1. For easy reference, the two predicates are marked at their instrumentation sites in Figure 2.

As we can see, the predicates P 1 and P 2 point to the faulty function for the first fault. Especially, the predicate P 2 is only 4 lines above the real fault location. Now let us pretend to be a developer encountering the failing case f 1 , and explore how the top predicates can help us find the fault.

Given the top-ranked predicates, it is natural to ask why they are ranked high. We find that P 1 is evaluated as true for ten times and none as false in f 1 , but 11 times as false and none as true in p 1 . As to P 2 , it is always evaluated as true in p 1 for 14 times, but only once as true and ten times as false in f 1 .

The different evaluation patterns of P 1 in p 1 and f 1 immediately shed some light on the underlying fault. P 1 always being false means that the variable beg is always equal to lastout at line 574 in p 1 . However, in f 1 , beg is always different from lastout . For this reason, we may guess that the value of either beg or lastout or both may deserve special attention in de-bugging. Moreover, the ten-time true evaluations of P 1 also explain the ten-time false evaluation of P 2 : the variable lastout is always reset to 0 at line 575, which immediately causes the false evaluation of P 2 at the next iteration. In this sense, the abnormal evalu-ation pattern of P 2 is only a subsequent symptom from that of P 1 , and it provides little new information for debugging. Therefore, we only need to pay attention to P , i.e. , examining locations where either the variable beg or lastout (or both) gets assigned.

With this guidance, a developer can place watches on the variables beg and lastout , and re-execute f 1 . With familiarity with the code and program seman-tics, the developer will notice the assignment to beg at line 553, and then locate the fault. In contrast, with-out BayesDebug , the developer may need to hunt the fault at large, and trace the execution step by step.
After fixing the first fault, let us now turn to the second. We construct another two test cases p 2 and f , and a run of BayesDebug on them gives a pred-icate ranking with the top predicate as  X  (lcp[i] !=  X  \ 0 X ) = true  X , which we denote as predicate P 3 . This predicate points to Line 2270 of the dfa.c file, and it is the exact location of the second fault. Therefore, a developer can trace the execution of f 2 , and put more attention on Line 2270 of the dfa.c file. Again, with code familiarity and programming expertise, the fault will be fixed without hunting faults at large.
We notice that for the first fault, the predicates P 1 and P 2 are the most fault-relevant predicates, and they are ranked at the top by BayesDebug . Similarly, for the second fault, the most relevant predicate P 3 is also ranked at the top. In the next subsection, we examine how the other three localization algorithms work under the same setting.
We applied both Sober and Liblit05 to the same execution traces for the two faults respectively. And Table 2 shows the effectiveness comparison, in terms of the rank of predicates P 1 , P 2 and P 3 in the final predicate ranking. A smaller rank means the predicate is ranked higher in the final ranking.

As we can see, Liblit05 ranks the predicates P 1 and P 2 at the 7th and the 23rd for the first fault, and ranks P 3 at the 24th place for the second fault. The result from Sober is similar: P 1 and P 2 are at the 10th and the 23rd for the first fault, and P 3 is the 11th for the second fault. We also examined the top predicates ranked by both Liblit05 and Sober for the two faults, and found that these top predicates are no enlightening for debugging. In comparison, the most relevant pred-icates are picked up by BayesDebug for both faults. With fault-relevant predicates deeply buried in the final predicate ranking, the localization results from both Sober and Liblit05 are only of limited utility to de-velopers.

We also subjected DeltaDebug to the two faults separately, and found that DeltaDebug is ineffective for these cases. Specifically, DeltaDebug got choked for the first fault with the passing execution p 1 and the failing execution f 1 : It failed to terminate within 48 hours. For the second fault, DeltaDebug imme-diately completed with executions p 2 and f 2 as inputs, but unfortunately generated an error message:  X  Tried all strategies -all failed  X , which meant that DeltaDebug failed to find any fault relevant loca-tions for the second fault. According to our discussion in Section 4.3, this result is as expected. As one may expect, the localization quality of BayesDebug or other algorithms in general, could de-pend on the given failing and passing cases. In order to verify this conjecture, we constructed five passing and five failing cases for each of the two faults, and exam-ined the localization quality of BayesDebug on the 25 pair-wise combinations of passing and failing cases. Because three passing cases resulted in the same pred-icate ranking with BayesDebug , no matter which of the five failing cases was paired with, two of them were discarded. Therefore, finally 15 combinations were ex-amined for each fault. We also examined the local-ization quality of both Sober and Liblit05 for the same combinations, and similar results were observed for the two algorithms. In the following, only Liblit05 is compared with BayesDebug .

Figure 3 shows the comparison for the 15 combina-tions on the two faults. The y -axis is the rank of the predicate in the result ranking, and the x -axis is for the indices of the 15 combinations. Especially, the 15 com-binations are arranged with failing cases as the main index. In other words, 1, 2 and 3 are for the combi-nations of the first failing case with the first, second and third passing cases respectively, and 4, 5 and 6 are for the second failing case with the three passing cases. Figures 3(a) and 3(b) plot the rank comparison of predicates P 1 and P 2 on the first fault, and Figure 3(c) for P 3 on the second fault.
 A first look at the three figures reveals that BayesDebug generally has a better localization qual-ity than Liblit05 . For most combinations, the fault-relevant predicates are ranked higher by BayesDebug than Liblit05 . However, we also observe that for some combinations, BayesDebug ranks the fault-relevant predicates much lower than Liblit05 . For example, in Figure 3(a), the predicate P 1 is ranked very low (around or even below 30th) by BayesDebug for the last three combinations. Because P 2 is related to P 1 , the same thing is observed for P 2 in Figure 3(b). More-over, we note that the last three combinations in Fig-ures 3(a) and 3(b) correspond to the combinations of the fifth failing case with the three passing cases. This observation indicates that although quality localization result is obtained by BayesDebug for the twelve pair-wise combinations between the first four failing cases and the three passing cases, when the fifth failing case is given, the localization is poor no matter which of the three passing cases is paired with. This is quite reasonable because even for human developers, not all failing executions are equally appropriate for debug-ging. For example, a long-running failing execution is usually harder to debug than a short one although they two can be due to the same fault.
 Meanwhile, we also notice that although BayesDebug gets stuck on the last three com-binations for the first fault, Liblit05 actually obtains even better result than the other twelve combinations. This observation shows that there is no absolute  X  X ood X  pair of failing and passing cases: a pair of executions can be good for one algorithm, but may be meanwhile bad for other algorithms. So far, there are no clear conclusions about what pairs of failing and passing cases are good, and this could continue to be an open problem.
 Finally, we briefly report the computation cost for BayesDebug . For the 15 combinations on Fault 1, the running time of 15 times of BayesDebug took 0.2358 seconds, while the running time of Liblit05 was 10.2669 seconds. For the 15 combinations on the second fault, BayesDebug and Liblit05 took 0.2492 and 10.2760 seconds, respectively. We believe that the time advantage of BayesDebug mainly comes from the analytical form of the fault-relevance score ( Theo-rem 1 ). But since both algorithms end within less than one second for each combination, the time advantage is not significant in practice.
All empirical studies are subjected to some threats to validity, and this study is no exception. In the first place, the two faults in the grep subject program are seeded by our authors although they do mimic realis-tic  X  X ff-by-one X  and  X  X ubclause-missing X  errors. More-over, this experiment is entirely based on the grep sub-ject program. For this reason, more case studies with real faults are needed to establish the effectiveness of BayesDebug in the future. The second threat to va-lidity comes from the hand-crafted test inputs used in Sections 5.2, 5.3 and 5.4. As shown in Section 5.4, the localization quality of BayesDebug could depend on the given pair of failing and passing cases, and dete-riorate significantly when a  X  X ad X  pair of test cases is given. We construct those test cases based on our un-derstanding of the code and our design to trigger the seeded faults. Therefore, it could be possible that these cases are proper for BayesDebug . But we also note that the unpleasant results from Sober and Liblit05 should not be attributed to the specific test cases be-cause the two algorithms are in principle short for the two-sample debugging problem, as discussed in Section 4. Finally, Section 5.2 illustrates how a developer finds the seeded faults with the localization result provided by BayesDebug . Ideally, this experiment should be carried out with independent developers rather than our authors. But due to the difficulty (and expenses) of controlled user study, currently most fault localization researches are evaluated by the authors [6,8,13,15].
In this section, we review related work and discuss some potential extensions. First, this study is related to software fault localization, a problem that has been actively pursued in recent years [6, 8, 10, 13, 15]. De-pending on whether the program crashes, there are two kinds of software faults: memory-related faults and logic faults. Memory-related faults typically incur ac-cess violations that finally result in program crashes. Because crashing scenes can provide valuable debug-ging information, memory faults are generally easier to debug. In addition, some mature tools, like Val-grind and CCured , can also effectively help developers debug by guarding each memory access. Delta Debug-ging [15] improves over these tools in its ability to find why a program crashes, rather than merely the sites of memory violations. Logic faults, on the other hand, are generally harder to tackle because there can be no memory abnormality even when the output is in-correct. Because no crashing scene is available, even human developers may find it tricky to debug. The predicate-based fault localization tools, like Liblit05 and Sober , are particularly suitable for finding logic faults, although they are also effective in find mem-ory related errors [6, 8]. Besides the predicate-based approach, Liu et al. also show that some logic faults can be located by mining program control flow graphs that are induced from multiple passing and failing ex-ecutions [10]. The BayesDebug algorithm developed in this paper shares the principle of predicate-based approach. But different from previous studies that as-sume the existence of multiple passing and failing ex-ecutions, BayesDebug is especially designed for the two-sample debugging problem, an important fault lo-calization problem that has been overlooked in previous studies.

Secondly, this work also relates to Bayesian statis-tics, which is most represented by Bayesian networks in data mining community [3]. Because Bayesian net-works can express complex dependencies between at-tributes and can encode both casual and probabilistic semantics, they are usually adopted where prior knowl-edge is known and critical. For example, Jaroszewicz and Scheffer discuss how to find unexpected patterns from data, where the unexpectedness is defined rela-tive to a Bayesian Network that encodes domain prior knowledge [4]. Besides dependency and domain knowl-edge representation, Bayesian networks are also known for its usage in classification [2]. However, although related, the BayesDebug algorithm is not an appli-cation of Bayesian networks to the fault localization problem. Instead, BayesDebug attempts to simulate human debugging heuristics through a Bayesian ap-proach. Although this simulation could be far from what human developers really think in debugging, the study shows that BayesDebug is nevertheless useful, which echoes the well-known saying that  X  X ll mod-els are wrong, but some are useful X  (due to George Box). To the best of our knowledge, this is the first piece of work that approaches fault localization in a Bayesian way. We envision that the localization quality of BayesDebug can be further leveraged when depen-dencies between predicates are encoded in a Bayesian network. Currently, all predicates are treated indepen-dent from each other, but they are in fact related.
Finally, this study falls into an emerging application domain in data mining: data mining for software engi-neering. Previous research indicates that proper min-ing of software data can produce useful results for soft-ware engineers. Livshits et al. apply frequent itemset mining algorithms to software revision history, which uncovers programming rules that developers are ex-pected to conform to [11]. Li et al. apply the CloSpan algorithm [14] to source codes, and successfully locate some copy-paste faults in the Linux system [5]. Liu et al. show that mining program control flow graphs can help developers find logic errors [9, 10]. These cases well exemplify the promise and usefulness of data min-ing in software engineering, and this study provides yet another example.
In this paper, we investigated the two-sample debug-ging problem, and developed a new fault localization algorithm, BayesDebug , to solve this problem. We compared BayesDebug with three well-known local-ization algorithms, and the experiment clearly demon-strated the effectiveness of BayesDebug . A few inter-esting problems remain, which constitute part of our future work.
 Proof. (Lemma 2) With c and d being positives, integral by parts gives Let and because
 X   X  ( n + 1) = n !
In the following, we prove that f c ( d ) = g c ( d )  X  c, d  X  1 by induction.  X  c (  X  1)  X  N , we have Then suppose f c ( d ) = g c ( d ) ,  X  c ( &gt; 1)  X  N , = = = = f c ( d )  X  f c +1 ( d ) = g c ( d )  X  g c +1 ( d ) (by induction) = (  X  1) = (  X  1) = g c ( d + 1) Therefore, we have proved f c ( d ) = g c ( d )  X  c, d  X  1, which immediately leads to the proof of this lemma. Proof. (Theorem 1) = = ln = ln = ln
