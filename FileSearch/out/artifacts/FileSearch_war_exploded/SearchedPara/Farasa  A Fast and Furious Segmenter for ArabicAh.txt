 Word segmentation/tokenization is one of the most important pre-processing steps for many NLP task, particularly for a morphologically rich language such as Arabic. Arabic word segmentation involves breaking words into its constituent prefix(es), stem,  X  prefix  X  X  X   X   X   X  (and), stem  X  X tAb X   X  H . A J X   X  (book), and a possessive pronoun  X  X A X   X  A K  X  (our). The task of the tokenizer is to segment the word into  X  X + shown to have significant impact on NLP applica-tions such as MT and IR.

Many Arabic segmenters have been proposed in the past 20 years. These include rule based analyz-ers (Beesley et al., 1989; Beesley, 1996; Buckwal-ter, 2002; Khoja, 2001), light stemmers (Aljlayl and Frieder, 2002; Darwish and Oard, 2007), and sta-tistical word segmenters (Darwish, 2002; Habash et al., 2009; Diab, 2009; Darwish et al., 2014). Statis-tical word segmenters are considered state-of-the-art with reported segmentation accuracy above 98%.
We introduce a new segmenter, Farasa ( X  X nsight X  in Arabic), an SVM-based segmenter that uses a va-riety of features and lexicons to rank possible seg-mentations of a word. The features include: like-lihoods of stems, prefixes, suffixes, their combina-tions; presence in lexicons containing valid stems or named entities; and underlying stem templates.
We carried out extensive tests comparing Farasa with two state-of-the-art segmenters: MADAMIRA (Pasha et al., 2014), and the Stanford Arabic seg-menter (Monroe et al., 2014), on two standard NLP tasks namely MT and IR. The comparisons were done in terms of accuracy and efficiency. We trained Arabic  X  English Statistical Machine Trans-lation (SMT) systems using each of the three seg-menters. Farasa performs clearly better than Stan-ford X  X  segmenter and is at par with MADAMIRA, in terms of BLEU (Papineni et al., 2002). On the IR task, Farasa outperforms both with statistically significant improvements. Moreover, we observed Farasa to be at least an order of magnitude faster than both. Farasa also performs slightly better than the two in an intrinsic evaluation. Farasa has been Features: In this section we introduce the features and lexicons that we used for seg-mentation. For any given word (out of con-text), all possible character-level segmentations are found and ones leading to a sequence of pref ix 1 +...+ pref ix n +stem+ suf f ix 1 +...+ suf f ix , where: pref ix 1 .. n are valid prefixes; suf f ix 1 .. m are valid suffixes; and prefix and suffix sequences are legal, are retained. Our valid prefixes are: f, w, suffixes are: A, p, t, k, n, w, y, At, An, wn, wA, yn, kmA, km, kn, h, hA, hmA, hm, hn, nA, tmA, .
 Using these prefixes and suffixes, we generated a list of valid prefix and suffix sequences. For example, sequences where a coordinating conjunction (w or f) precedes a preposition (b, l, k), which in turn precedes a determiner (Al), is legal, for example the book X ) which is segmented to (f+b+Al+ktAb not allowed to precede any other prefix. We used the following features: -Leading Prefixes: conditional probability that a leading character sequence is a prefix. -Trailing Suffixes: conditional probability that a trailing character sequence is a suffix. -LM Prob (Stem): unigram probability of stem based on a language model that we trained from a corpus containing over 12 years worth of articles of Aljazeera.net (from 2000 to 2011). The corpus is composed of 114,758 articles containing 94 million words. -LM Prob: unigram probability of stem with first suffix. -Prefix|Suffix: probability of prefix given suffix. -Suffix|Prefix: probability of suffix given prefix. -Stem Template: whether a valid stem template can be obtained from the stem. Stem templates are patterns that transform an Arabic root into a stem. For example, apply the template CCAC on the root (meaning: book). To find stem templates, we used the module described in Darwish et al. (2014). -Stem Lexicon: whether the stem appears in a lexicon of automatically generated stems. This can help identify valid stems. This list is generated by placing roots into stem templates to generate a stem, which is retained if it appears in the aforementioned Aljazeera corpus. -Gazetteer Lexicon: whether the stem that has no trailing suffixes appears in a gazetteer of person and location names. The gazetteer was extracted from Arabic Wikipedia in the manner described by (Darwish et al., 2012) and we retained just word unigrams. -Function Words: whether the stem is a function word such as  X  X lY X   X   X  X   X   X  (on) and  X  X n X   X   X  X   X  (from). -AraComLex: whether the stem appears in the AraComLex Arabic lexicon, which contains 31,753 stems of which 24,976 are nouns and 6,777 are verbs (Attia et al., 2011). -Buckwalter Lexicon: whether the stem appears in the Buckwalter lexicon as extracted from the AraMorph package (Buckwalter, 2002). -Length Difference: difference in length from the average stem length.
 Learning: We constructed feature vectors for each possible segmentation and marked correct seg-mentation for each word. We then used SVM-Rank (Joachims, 2006) to learn feature weights. We used a linear kernel with a trade-off factor between training errors and margin (C) equal to 100, which is based on offline experiments done on a dev set. During test, all possible segmentations with valid prefix-suffix combinations are generated, and the different segmentations are scored using the clas-sifier. We had two varieties of Farasa. In the first, Farasa Base , the classifier is used to segment all words directly. It also uses a small lookup list of concatenated stop-words where the letter  X  X  X   X   X   X  is dropped such as  X  X mA X   X  A X  ond, Farasa Lookup , previously seen segmentations during training are cached, and classification is ap-plied on words that were unseen during training. The cache includes words that have only one seg-mentation during training, or words appearing 5 or more times with one segmentation appearing more than 70% of times.
 Training and Testing: For training, we used parts 1 (version 4.1), 2 (version 3.1), and 3 (version 2) of the the Penn Arabic Treebank (ATB). Many of the current results reported in the literature are done on subsets of the Penn Arabic Treebank (ATB). Testing done on a subset of the ATB is problematic due to its limited lexical diversity, leading to artificially high results. We created a new test set composed of 70 WikiNews articles (from 2013 and 2014) that cover a variety of themes, namely: politics, economics, health, science and technology, sports, arts, and cul-ture. The articles are evenly distributed among the different themes (10 per theme). The articles contain 18,271 words. Table 1 compares segmentation accu-racy for both versions of Farasa with MADAMIRA, where both were configured to segment all possible affixes. We did not compare to Stanford, because it only segments based on the ATB segmentation scheme. Farasa lookup performs slightly better than MADAMIRA. From analyzing the errors in Farasa, we found that most of the errors were due to either: foreign named entities such as  X  X ynks X   X   X  X  JJ  X   X  ing: Palisky); or to long words with more than four (meaning  X  X nd to surprise both of them X ). Perhaps, adding larger gazetteers of foreign names would help reduce the first kind of errors. For the sec-ond type of errors, the classifier generates the cor-rect segmentation, but it receives often a slightly lower score than the incorrect segmentation. Per-haps adding more features can help correct such er-rors. Setup: We trained Statistical Machine Translation (SMT) systems for Arabic  X  English, to compare parison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (contain-ing 183K Sentences) and NEWS corpus (containing Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings de-scribed in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k -best batch MIRA (Cherry and Fos-ter, 2012). Among other features, we used lexical-ized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., and other default parameters. We used an unsuper-vised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems.

In each experiment, we simply changed the seg-mentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013).
 Results: Table 2 compares the Arabic-to-English SMT systems using the three segmentation tools. Farasa performs better than Stanford X  X  Arabic seg-menter giving an improvement of +0.25, but slightly worse than MADAMIRA (-0.10). The differences are not statistically significant. For efficiency, Farasa is faster than Stanford and MADAMIRA by a fac-MADAMIRA makes it cumbersome to run on big-ger corpora like the multiUN (UN) (Eisele and Table 3: English-to-Arabic Machine Translation, BLEU scores and Time (in seconds) Chen, 2010) which contains roughly 4M sentences. This factor becomes even daunting when training a segmented target-side language model for English-to-Arabic system. Table 3 shows results from English-to-Arabic system. In this case, Stanford per-forms significantly worse than others. MADAMIRA performs slightly better than Farasa. However, as before, Farasa is more than multiple orders of mag-nitude faster. Setup: We also used extrinsic IR evaluation to determine the quality of stemming compared to MADAMIRA and the Stanford segmenter. We per-formed experiments on the TREC 2001/2002 cross language track collection, which contains 383,872 Arabic newswire articles, containing 59.6 million words), and 75 topics with their relevance judgments (Oard and Gey, 2002). This is presently the best available large Arabic information retrieval test col-lection. We used Mean Average Precision (MAP) and precision at 10 (P@10) as the measures of good-ness for this retrieval task. Going down from the top a retrieved ranked list, Average Precision (AP) is the average of precision values computed at every rel-evant document found. P@10 is the same as MAP, but the ranked list is restricted to 10 results. We used SOLR uses a tf-idf ranking model. We used a paired 2-tailed t-test with p-value less than 0.05 to ascer-tain statistical significance. For experimental setups, we performed letter normalization, where we con-flated: variants of  X  X lef X ,  X  X a marbouta X  and  X  X a X ,  X  X lef maqsoura X  and  X  X a X , and the different forms of  X  X amza X . Unlike MT, Arabic IR performs better with more elaborate segmentation which improves matching of core units of meaning, namely stems. For MADAMIRA, we used the D34MT scheme, where all affixes are segmented. Stanford tokenizer only provides the ATB tokenization scheme. Farasa Table 4: Retrieval Results in MAP and P@10 and Processing Time (in hh:mm:ss). For statistical sig-nificance, w = better than words, s = better than Stanford, and m = better than MADAMIRA was used with the default scheme, where all affixes are segmented.
 Results: Table 4 summarizes the retrieval re-sults for using words without stemming and using MADAMIRA, Stanford, and Farasa for stemming. The table also indicates statistical significance and reports on the processing time that each of the seg-menters took to process the entire document collec-tion. As can be seen from the results, Farasa out-performed using words, MADAMIRA, and Stan-ford significantly. Farasa was an order of magni-tude faster than Stanford and two orders of magni-tude faster than MADAMIRA. The major advantage of using Farasa is speed, with-out loss in accuracy. This mainly results from op-timization described earlier in the Section 2 which includes caching and limiting the context used for building the features vector. Stanford segmenter uses a third-order (i.e., 4-gram) Markov CRF model (Green and DeNero, 2012) to predict the correct seg-mentation. On the other hand, MADAMIRA bases its segmentation on the output of a morphological analyzer which provides a list of possible analyses (independent of context) for each word. Both text and analyses are passed to a feature modeling com-ponent, which applies SVM and language models to derive predictions for the word segmentation (Pasha et al., 2014). This hierarchy could explain the slow-ness of MADAMIRA versus other tokenizers. In this paper we introduced Farasa, a new Ara-bic segmenter, which uses SVM for ranking. We compared our segmenter with state-of-the-art seg-menters MADAMIRA and Stanford, on standard MT and IR tasks and demonstrated Farasa to be sig-nificantly better (in terms of accuracy) than both on the IR tasks and at par with MADAMIRA on the MT tasks. We found Farasa by orders of magnitude faster than both. Farasa has been made available for tion.

