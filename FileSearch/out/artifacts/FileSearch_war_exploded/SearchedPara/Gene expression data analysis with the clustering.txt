 1. Introduction
In an attempt to understand complicated biological systems, many researchers have generated large amounts of gene expres-sion data under complex conditions of experiments so that clustering has been effectively applied in molecular biology for gene expression data analysis ( Shamir and Sharan, 2001 ). Total of genes in a dataset are assigned into different clusters of similar expression patterns according to a dissimilarity measure (usually correlation-based or distance-based) between any two genes by clustering algorithms. The goal of the clustering process is thus to identify the genes with the same functions or the same regulatory mechanisms.

Many clustering algorithms have been proposed for gene expression data analysis. The hierarchical clustering is one of the earlier methods applied to clustering of gene expression data. Eisen et al. (1998) used a variant of the hierarchical average-link clustering algorithms to identify groups of co-regulated yeast genes. However, the hierarchy of the algorithm is greatly affected by the minor change of the given data, which makes the cluster-ing show the lack of robustness and nonuniqueness. K-means is one of another popular methods used in gene expression data analysis due to its high computational performances ( Tavazoie et al., 1999 ). But it might converge to a local optimum, and its results is quite subject to the random initialization process, which means that different runs of K-means on the same data set might produce different clusters ( Kao et al., 2008 ). As one kind of neural network, self-organizing map (SOM) which presents high-dimensional data by the low dimensional data has also been used for gene expression data clustering ( Tamayo et al., 1999 ).
However it always produces an unbalanced solution and it is difficult to find clear clustering boundaries from results of the
SOM. Other common clustering methods include CAST algorithm ( Ben-Dor and Yakini, 1999 ), model-based clustering ( Yeung et al., 2001a ) and tight clustering ( Tseng and Wong, 2005 ). Especially, the last two algorithms were proposed to allow a noise set of genes (or so-called scattered genes) without being clustered. It is in view of the fact that very often a significant number of genes in an expression profile do not play any role in the disease or perturbed conditions under investigation.

Recently, some novel methods have been proposed for gene clustering technology. Qin (2006) devised an improved model-based Bayesian approach, known as the weighted Chinese restau-rant process (CRP), to cluster microarray gene expression data. Cluster assignment of CRP is carried out by an iterative weighted
Chinese restaurant seating scheme such that the optimal number of clusters can be determined simultaneously with cluster assign-ment. Affinity Propagation (AP), a new powerful algorithm based on message-passing techniques, was proposed by Frey and Dueck (2007) . In AP, each cluster is identified by a common exemplar that all other data points of the same cluster refer to, and the exemplars have to refer to themselves. Leone (2007) improved the original AP algorithm by relaxing its hard constraints and the resulting soft-constraint affinity propagation (SCAP) became more informative, accurate and led to more stable clustering. clustering ( Hall et al., 1999 ). The fundamental strategy of such clustering approaches is to imitate the evolution process of nature and evolve the solutions of clustering from one generation to the next. In contrast to K-means algorithm, clustering algorithms based on GA are insensitive to the initialization process and always converge to the global optimum eventually. However, these algorithms are usually computationally expensive, which impedes the wide application of them in the field of gene expression data analysis. To overcome this limitation and accel-erate the convergence, some hybrid methods have been proposed.
For an instance, Krishna and Murty (1999) proposed a new clustering method called Genetic K-means Algorithm (GKA), which hybridizes a genetic algorithm with the K-means algorithm.
This hybrid approach combines the robust nature of the genetic algorithm with the high performance of the K-means algorithm. Based on the GKA, Lu et al. (2003, 2004 ) proposed Fast Genetic K-means Algorithm (FGKA) and Incremental Genetic K-means
Algorithm (IGKA) for analyzing gene expression data. Moreover, for up to research of GA clustering in gene expression data analysis, Bandyopadhyay et al. (2007) devised a two-stage cluster-ing algorithm, which employs a recently proposed variable string length genetic scheme and a multiobjective genetic clustering algorithm. It is based on the novel concept of points having significant membership to multiple classes. An iterated version of the well-known Fuzzy C-means is also utilized for clustering. 1995 ), a population-based random search technique motivated by the behavior of organisms such as fishing schooling and bird flock, has been applied to data clustering ( Van der Merwe and
Engelbrecht, 2003 ; Tzay-Farn 2006 ; Yang et al., 2009 ; Du et al., 2008 ). As a method of swarm intelligence, PSO is easier to implement for clustering than GA, since it does not need any complex operation, such as selection, crossing and mutation in GA.
Particle Swarm Optimization (QPSO), has been proposed in order to improve the global search ability of the original PSO ( Sun et al., 2004a , 2004b , 2005 ). The iterative equation of QPSO is far different from that of PSO in that it is needs no velocity vectors for particles, has fewer parameters to adjust and can be imple-mented more easily. It has been proved that this iterative equation leads QPSO to be global convergent ( Fang et al., 2010 ).
The QPSO algorithm has been aroused the interests of many researchers from different communities.
 rithm works better than PSO in solving a wide range of contin-uous optimization problems ( Coelho, 2010 ; Omkar et al., 2009 ;
Sabat et al., 2009 ; Shayeghi et al., 2010 ; Sun and Lu, 2010 ; Zhang, 2010 ); researchers have showed that further improvement of
QPSO is possible and many efficient strategies have been pro-posed to improve this algorithm ( Coelho, 2008 ; Xi et al., 2008 ;
Huang et al., 2009 ). The main reason for developing these improved versions of QPSO is that like other evolutionary algo-rithm including PSO, this algorithm may also encounter the problem of premature convergence, particularly for the problems with high dimensionality and multiple local optima. Bearing this in mind, we have always been devoting ourselves to enhance the performance of the QPSO algorithm. Thus, in this paper, we proposed a novel variant of QPSO, called Muti-Elitist Quantum-behaved Particle Swarm Optimization (MEQPSO), in which a Multi-Elitist strategy for searching the global best position is employed to enhance the global search ability of the QPSO algorithm so as to avoid premature convergence efficiently. In the original QPSO, the particle is guided by the global best position as well as its personal best position. If the global best position traps into a local optimum, all the particles will be pulled toward this local optimal point and will have higher possibility to failing in search of the better region where the global optimal solution may be located, which can lead to premature conver-gence of the algorithm. On the other hand, in the proposed MEQPSO, the particle X  X  search is influenced by the position, which may not be the global position but may lie in a promising search region, so that the particles have much chance to search this region and find out the global optimal solution. As a result, MEQPSO may have better overall performance than the original QPSO, particularly for the hard optimization problems.
In this paper, we also show how to apply the MEQPSO algorithm to gene expression data clustering, which can be reduced to an optimization problem. In our work, like other related works ( Krishna and Murty, 1999 ; Lu et al., 2003, 2004 ), the task of the optimization Total Within-Cluster Variation (TWCV), although other objectives can be used, such as external dis similarities among clusters. Furthermore, in order to accelerat e the convergence rate of MEQPSO for clustering, we incorporate K-means clustering into the MEQPSO-based clustering algorithm as in GKA. In this hybrid clustering method, a one-step K-means algor ithm, called K-means operator (KMO), is executed after the updating of each particle in MEQPSO. The K-means algorithm used consists of two phases, one of which is calculating new cluster centers according to current particle, and the other of which is reassigning each data point to the cluster with the nearest cluster center to form the new partition. The hybrid clustering method takes the advantages of the strong global search ability of MEQPSO and fast clustering convergence speed, which is verified by testing on three gene expression datasets.
The rest of this paper is organized as follows. Section 2 provides a brief review for the QPSO algorithm. Section 3 describes the proposed MEQPSO and Section 4 presents how to use MEQPSO for gene expression data clustering. Section 5 gives the experimental results of the tested PSO and QPSO variants on some well known benchmark optimization functions. Section 6 provides the results of four groups of experiments on gene expression datasets. Finally, the paper is concluded in Section 7 . 2. Quantum-behaved particle swarm optimization 2.1. A brief introduction of PSO algorithm
The proposal of PSO algorithm was put forward as an optimi-zation technique by several scientists who developed computa-tional simulations of the movement of organisms such as flocks of birds and schools of fish. Since its origin in 1995, there have been many works done on the PSO algorithm ( Shi and Eberhart, 1998 ; Angeline, 1998 ; Clerc, 1999 ; Suganthan, 1999 ; Kennedy, 2003 ; Krohling, 2004 ; Liang and Suganthan, 2005 ; Wu, 2011 ). In the PSO with m individuals, each individual is treated as a volume-less particle in the D -dimensional space, with the position vector and velocity vector of particle i at the t th iteration represented as X ( t )  X  [ x i ,1 ( t ), x i ,2 ( t ), y , x i , D ( t )] and V particle moves according to the equations: x  X  t  X  1  X  X  x i , j  X  t  X  X  v i , j  X  t  X  1  X  ,  X  2  X  coefficients, and w is the inertia weight first introduced into Eq. (1) by Shi and Eberhart ( Shi and Eberhart, 1998 ). Vector pbest i ( t )  X  [ pbest i ,1 ( t ), pbest i ,2 ( t ), y pbest position (the position giving the best objective function value) of particle i called personal best ( pbest ) position, and vector gbest ( t )  X  [ gbest 1 ( t ), gbest 2 ( t ), y , gbest D particle among all the particles in the population and called global best ( gbest ) position. Without loss of generality, taking the minimization problems into account, we can update pbest i pbest i  X  t  X  X  be found by: gbest  X  t  X  X  pbest g  X  t  X  ,  X  4  X  where g  X  arg min numbers distributed uniformly on (0,1), that is R i , j ( t ), r Generally, the value of v i , j ( t ) is restricted in the interval  X  v 2.2. The QPSO algorithm
The main disadvantage of the PSO algorithm may be that it is not guaranteed to be global convergence ( Van den Bergh, 2001 ), and is prone to trap into local optima although it converges fast. Inspired by trajectory analysis of the PSO ( Clerc and Kennedy, 2002 ) and quantum mechanics, Sun et al. developed and proposed the Quantum-behaved Particle Swarm Optimization (QPSO) algorithm. This subsection presents the essential of the QPSO.
Trajectory analysis shows the fact that convergence of the PSO algorithm may be achieved if each particle converges to its local attractor p i ( t ), defined at the coordinates: p  X  t  X  X  j i , j  X  t  X  U pbest i , j  X  t  X  X  X  1 j i , j  X  t  X  U gbest coefficients c 1 and c 2 in PSO are generally set to be equal, i.e. c  X  c 2 , j j ( t ) is a sequence of uniformly distributed random number on (0,1), that is j j U (0,1).

In the QPSO algorithm, it is supposed that each single particle is spin-less one with quantum behavior. Thus state of the particle is characterized by wavefunction C , where 9 C 9 2 is the probability density function of its position. We assume that, at the t th generation, particle i moves in an D -dimensional space with a d potential well at p i , j ( t ) on the j th dimension (1 r j r D )to guarantee the convergence. Solving the corresponding Schrodin-ger equation and employing the Monte Carlo method, we get the following iterative equation according to which the particle X  X  position updates ( Sun et al., 2004, 2005 ): ( where u and k are two different random numbers generated according to a uniform probability distribution on (0,1), and C ( t )is known as the mean best position defined as the mean of the pbest positions of all particles: C  X  t  X  X  X  1 = m  X 
The two formulas of Eq. (7) are different in that the first one uses operation  X  X   X   X  X  while the second uses  X  X   X  X . When the particle X  X  current position is updated according to Eq. (7), a random number k should be generated. If the value of k is larger than or equal to 0.5, the particle X  X  current position is updated according to the first formula, or otherwise, it is updated accord-ing to the second one.

The parameter a in Eq. (7) is called Contraction X  X xpansion (CE) coefficient, which can be tuned to control the convergence speed of the particle. The simulation results show that the parameter a must be set as a o 1.781 so that convergence or boundedness of the particle could be guaranteed ( Sun et al., 2005 ). There are two simple method of tuning the value of a .
One is fixed the value during the search of the algorithm, and the other is varying a during the search. However, it is suggested by most of the literatures on QPSO that varying the value of a linearly from a larger initial value a 0 to a smaller final value a ( a o a 1 ) can lead the QPSO algorithm to a generally good performance. Particularly, it is recommended that a 0 and a should be set to be 1.0 and 0.5 respectively for desirable algorithmic performance. In this work, we also adopt an adaptive control method of tuning the value of a for further performance improvement. 3. The Multi-Elitist QPSO (MEQPSO) algorithm
Although QPSO possesses better global convergence behavior than PSO, it may encounter premature convergence, a major problem with PSO and other evolutionary algorithms in multi-modal optimization, which results in great performance loss and sub-optimal solutions. In a QPSO system, the gbest position guides all particles in the whole swarm to fly to itself in the course of the iterations. Once the global best position converges to a local optimal or sub-optimal point during the searching process, the particles will be misguided by this gbest position and thus fall into a bad search space without further exploration before the algo-rithm is over. This leaves the QPSO algorithm with great difficul-ties of escaping local optimal. To overcome the problem, we introduce a revised QPSO by exerting a Multi-Elitist strategy, which is drawn from the works reported in Swagatam et al. ( Swagatam et al., 2008 ), to update the gbest position of the QPSO algorithm.

In the Multi-Elitist QPSO (MEQPSO), a parameter b called growth rate is defined for each particle to reflect the degree of evolution. When the fitness value of a particle at the t th iteration value of its b is increased. After the pbest positions of all particles are updated in each iteration, the pbest positions, which have better fitness values than the gbest position obtained before, are moved into a candidate area. The update of the gbest position depends on the probability p c , referred to as the selection probability. Before the update operation, we generate a random number. If the random number is higher than p c and the candidate area is not empty, the global best position will be replaced by the personal best position with the highest growth rate b selected from the candidate area; otherwise the global best position becomes the particle having best fitness value in present population.

In the MEQPSO algorithm, the particle X  X  search is guided by the position which may be not the global position but may lie in a promising search region so that the particles have much chance to search this region and find out the global optimal solution. As a result, the MEQPSO may have stronger global search ability and better overall performance than the original QPSO, particularly for the hard optimization problems. The pseudo-code of the MEQPSO algorithm is outlined as follows: 4. Using MEQPSO for gene expression data clustering 4.1. K-means algorithm find a partition that minimizes TWCV measure. Although there are many variations of the K-means algorithm, we use one of its simple version in MEQPSO clustering algorithm.
 random configuration of cluster centers. At every iteration, each pattern is assigned to the cluster whose center is the closest center to the pattern among all the center centers. The cluster centers in the next iteration are the centroids of the patterns belonging to the corresponding clusters. The algorithm is termi-nated when there is no reassignment of any pattern from one cluster to another or the TWCV measure ceases to decrease significantly after an iteration. A major problem with this algo-rithm is that it is sensitive to the selection of initial partition and may converge to a local minimum of TWCV if the initial partition is not properly chosen. Therefore, in this work, we incorporate the
K-means method into QPSO and MEQPSO in order to employ the global search ability of the QPSO algorithms. 4.2. MEQPSO clustering algorithm genes and their corresponding N patterns. Each pattern is a
D -dimensional vector recording the expression levels of the genes under each of the D monitored conditions or at each of the D time points. The goal of using MEQPSO clustering algorithm is to partition a gene expression data set into user-defined K categories with Euclidean distance as similarity measure, such that this partition minimizes the Total Within-Cluster Variation, which is defined as follows.

Let { X 1 , X 2 , y , X N } be the N patterns and x nd denote the dth by a string, i.e., a sequence of numbers a 1 , y a N , where a value from {1,2, y , K } representing the cluster number that pattern X n belongs to in this partition. Let G k denote the kth cluster and Z k denote the number of patterns in G k . The centroid c  X  ( c k 1 , c k 2 , y , c kD ) of cluster G k is defined by c The within-cluster (WCV) variation of k th cluster is defined by WCV  X  k  X  X  and the Total Within-Cluster Variation (TWCV) is defined by TWCV  X 
As in GKA, the MEQPSO clustering algorithm maintains a population of coded solutions. The swarm is initialized randomly and evolves over iterations; the swarm in the next iteration is obtained by applying MEQPSO updating and K-means operator on the current swarm. The evolution takes place until a terminating condition is met. The detailed process of MEQPSO clustering is explained as follows: (1) Encoding: Each solution, that is, each particle in MEQPSO (2) Initialization: The initial swarm P (0) is selected randomly. (3) QPSO updating: At the beginning of the updating, the mean (4) K-means operator: The clustering algorithm with only above (5) Updating of personal and global best position: the most sig-
The process of the MEQPSO clustering algorithm is presented below. To start with, the initial swarm is generated as mentioned above and the subsequent swarms are obtained by applying QPSO updating Eq. (7) and KMO over the previous swarm. The algo-rithm is terminated when the limit on the number of iterations is reached.
 MEQPSO Clustering Algorithm: Input:
Output: Global best solution, gbest ; {Initialize the particles, the global and local best positions; For t  X  1: MAX _ ITER End
Output gbest ( t ) (the global best position); } 5. Performance evaluation and comparison on benchmark functions
To evaluate the overall performance of the proposed MEQPSO on function optimization, we tested the algorithm on the first ten functions in CEC2005 benchmark suite ( Suganthan et al., 2005 ). Functions f 1  X  f 5 are unimodal, while functions f 6  X  f Each algorithm ran 100 times on each problem using 20 particles to search the global best fitness value with each run executed for 3000 iterations. For performance comparison, PSO with inertia weight (PSO-In) ( Shi and Eberhart, 1998 ), standard PSO (SPSO) ( Braton and Kennedy, 2007) , comprehensive learning PSO (CLPSO) ( Liang et al., 2006 ), the original QPSO algorithm, the QPSO with hybrid prob-ability distribution (QPSO-HPD) ( Sun et al., 2006 )andtheQPSOwith tournament selection (QPSO_TS) ( Sun et al., 2007 )werealsotested by the benchmark functions. The other parameters for each algo-rithm were configured as recomm ended by the corresponding publications. The mean best fitness value and standard deviation out of 100 runs of each algorithm on each problem is presented in Table 1 .
 Fitness Value Fitness Value Fitness Value
For Shifted Sphere Function ( f 1 ), the QPSO-based algorithms including MEQPSO, which had the similar performance, generated better results than other methods. The results for Shifted Schwe-fel X  X  Problem 1.2 ( f 2 ) show that, MEQPSO got the better results than its competitors. For Shifted Rotated High Conditioned Elliptic Function ( f 3 ), the MEQPSO algorithm also appeared to outperform the other methods. QPSO-TS showed to be the winner among all the tested algorithms for Shifted Schwefel X  X  Problem 1.2 with Noise in Fitness ( f 4 ). Benchmark f 5 is Schwefel X  X  Problem 2.6 with Global Optimum on Bounds, and for this benchmark, MEQPSO yielded the best results. For benchmark f 6 , the Shifted Rosenbrock Function, performances of MEQPSO were superior to those of the other algorithms. The results for Shifted Rotated Griewank X  X  Function without Bounds ( f 7 ) suggest that QPSO-TS was able to find the solution of higher quality for the function compared to the other methods. Benchmark f 8 is Shifted Rotated Ackley X  X  Function with Global Optimum on Bounds. All the QPSO-based methods showed a better performances for this problem than the others. For Shifted Rastrigrin X  X  Function ( f 9 ), the MEQPSO algo-rithm yielded the best result. Benchmark f 10 is the Shifted Rotated Rastrigrin X  X  Function, which appears to be a more difficult problem than f 9 . For this benchmark, MEQPSO outperformed the other competitors in statistically significant manner.
Fig. 1 visualizes the convergence process of the algorithms on each benchmark function. It is shown that the QPSO-based algorithms had the better convergence property than their com-petitors in most cases. Among MEQPSO and the other QPSO-based algorithm, the former showed the improved convergence prop-erty except for the benchmark functions f 1 , f 4 and f 8 6. Experimental results for gene expression data clustering
In this section, a description of the performance metrics used in clustering evaluation is firstly presented. Then one artificial and four real data sets used in the clustering experiments are introduced. Finally, results from four groups of experiments are provided and analyzed. All the experiments were conducted on a
Lenovo PC computer with 2.93 G Hz CPU and 512M RAM. 6.1. Performance metrics
Assessing the clustering results and interpreting the clusters found are as important as generating the clusters ( Jain and Dubes, 1988 ). Cluster validity indexes correspond to the statistical mathematical functions used to evaluate the results of a cluster-ing algorithm on a quantitative basis. Cluster validation proce-dures are divided into two main categories: external and internal criterion analysis. External criterion analysis validates a clustering result by comparing it to a given  X  X old standard X , which is another 10 10 10 10 10 10 10 10 10 100 200 300 400 500 600 700 800 900 1000 1100 partition of the data set. Internal criterion analysis uses informa-tion from within the given data set to represent the goodness of fit between the input data set and the clustering results. The adjusted Rand index (ARI) ( Hubert and Arabie, 1985 ) and Silhou-ette index ( Rousseeuw, 1987 ) are most widely indexes respec-tively used for external criterion analysis and internal criterion analysis. Therefore, in this work, although there are hundreds of different criteria for evaluate the clustering results of gene expression, we employed the ARIand the Silhouette index to evaluate and compare different clustering algorithms. 6.1.1. Adjusted rand index between two partitions of the same set of objects. Suppose T is the true clustering of a gene expression data set based on domain knowledge and C a clustering result given by some clustering algorithm. Let a , b , c and d respectively denote the number of gene pairs belonging to the same cluster in both T and C , the number of pairs belonging to the same cluster in T but to different clusters in
C , the number of pairs belonging to different clusters in T but to the same cluster in C and the number of pairs belonging to different clusters in both T and C . The adjusted Rand index ARI ( T,C ) is then defined by
ARI  X  T , C  X  X  2  X  ad bc  X  perfect agreement between the external criterion and the cluster-ing result. The expected value of the adjusted Rand index in the case of random clusters is 0. According to Eq. (12), it is evident that a higher value of ARI indicates that C is more similar to T . 6.1.2. Silhouette index quality of any clustering solution C . Suppose that a represents the average distance of a point from the other points of the cluster to which the point is assigned and b represents the minimum of the average distances of the point from the points of the other clusters. Now the Silhouette width s of the point is defined by s  X  b a max f a , b g :  X  13  X  data points (genes) and it reflects the compactness and separation of clusters. It can be observed from Eq. (13) that the value of Silhouette index varies from 1 (when b  X  0) to 1 (when a  X  0).
Thus, the higher value of s indicates the better clustering result. 6.2. Data set and data processing namely, the rat CNS data, the galactose GAL data, the human fibroblasts serum data and the yeast cell cycle data were used to conduct our experiments.
 628 genes. It is generated from a hierarchical log-normal model for simulation of expression values in a cluster described as in ( Thalamuthu et al.2006 ). Totally 15 clusters of genes C n  X  ( C 1 , y , C 15 ) with dimension d  X  50 samples are simulated. The cluster size n c ( c  X  1, y ,15) is generated from n c 4 Poisson ( l ), l  X  10. The data vectors in each cluster are assigned to a cluster label c ( c  X  1, y ,15) when the AD_15 data set is created, and make this to be the external standard categories of this data set.
 transcription-coupled PCR to study the expression levels of 112 genes during rat central nervous system development over nine time points. These 112 genes were selected from four gene families according to prior knowledge of biology by Wen et al. (1998) . We used this 112 9 microarray as one of our experi-mental data sets and took these four functional categories as external standard categories.

The GAL data : To explore gene expression regulation informa-tion in the pathway of galactose utilization in the yeast Sacchar-omyces cerevisiae and interaction between genes and proteins, four replicate hybridizations were performed for each cDNA array experiment to generate the GAL microarray by Ideker et al. ( Ideker et al., 2001 ). Yeung et al. (2003) used a subset of 205 genes that are reproducibly measured, whose expression patterns reflect four function 80 data set is log-transformed and missing values in it were imputed by KNN algorithm ( Troyanskaya et al., 2001 ). The four categories were used as our external knowledge.
The human fibroblasts serum data : This data set contains the expression levels of 8613 human genes ( Iyer et al., 1999 ). The data set has 13 dimensions corresponding to 12 time points (0, 0.25, 0.5, 1, 2, 4, 6, 8, 12, 16, 20, and 24 h) and 1 unsynchro-nized sample. A subset of 517 genes used in our experiments whose expression levels changed substantially across the time points have been chosen ( Eisen et al., 1998 ).

The yeast cell cycle data: Yeast cell cycle data ( Spellman et al., 1998 ; Yeung et al., 2001b ) contains 6179 genes, of which 1663 genes are retained for this analysis after preprocessing and filtering. We deleted genes with missing values more than 20% or standard deviation at log-2 scale less than 0.4. For the genes retained for analysis, missing values were imputed by KNN algorithm.

In our experiment, the external standard functional categor-izations of the artificial data set AD_15, the rat CNS data set and the GAL data set are known previously. Therefore we set the cluster number K to be 15, 4 and 4 in the clustering experiment, which are identical to the external standard categorizations, and evaluated the performance of different clustering approaches on these three data sets using the adjusted Rand index. For the human fibroblasts serum and yeast cell cycle data sets, there is not any prior-knowledge about the classification. We chose different cluster number such as 5 X 10 on both data datasets and evaluated the performance of different clustering approaches using the Silhouette index. Each data set is normalized so that each row has mean 0 and variance 1 ( Z normalization) ( Sanghamitra. et al., 2007 ). A brief information about these data sets is listed in Table 2 . 6.3. Experimental results
This subsection provides four groups of experimental results for deep and wide evaluation of clustering based on QPSO and MEQPSO. Firstly, we tested the influence of K-means operator on QPSO for gene expression data clustering by comparing the QPSO with and without K-means operator (KMO) on the data sets. Secondly, the parameter selection issue in MEQPSO was investigated and the MEQPSO with different parameter settings were implemented on the data sets. Then, performance of all the forms of PSO that were mentioned in Section 5 was tested and compared for gene expression data clustering. Finally, the overall performance of the MEQPSO clustering algorithm was compared with some well-known clustering methods. 6.3.1. Influence of K-means on QPSO clustering
As indicated in the previous section, the K-means operator has a great impact on the QPSO clustering algorithms. We investi-gated the performance and convergence characteristics of QPSO clustering algorithm with and without KMO in this experiment. These two algorithms were executed on the artificial data set AD_15 for 100 iterations with a fixed population size  X  40. The Contraction X  X xpansion Coefficient a of QPSO was decreased linearly from 1.0 to 0.5 over the iterations. We recorded the results of each clustering algorithm by averaging 30 independent runs of the algorithm. The average ARI values of the clustering results are reported in Table 3 , and Fig. 2 shows the TWCV measure over iterations corresponding to this experiment.
In Fig. 2 , it can be obviously seen that the QPSO clustering algorithm with KMO quickly converged to the global optimum; however, the algorithm without KMO converged very slowly during the whole process of iterations. The reason why this situation occurs may be related to the characteristics of QPSO and K-means algorithm. QPSO is a population-based optimization technique, which searches for the optimal solution of a problem in the feasible search space. When the gene expression data clustering is converted to an optimization problem and QPSO is applied to solve this problem, due to a large amount of gene expression data leading to a tremendous solution space in accordance with the coding method proposed in this paper, it is very difficult to quickly find the best clustering only by the basic operation of QPSO. As a commonly used clustering algorithm,
K-means has fast convergence rate, but it is easy to fall into a local optimum. Learning from genetic K-means algorithm, we added a one step K-means operation in QPSO clustering, with the purpose of combining the global search ability of QPSO with the fast convergence of K-means algorithm, namely, using KMO to sig-nificantly accelerate the speed of convergence of the QPSO clustering algorithm. Table 2 shows the ARI values of QPSO clustering algorithm with and without KMO. It is evident from the table that ARI of QPSO clustering algorithm with KMO is close to 1 because of its fast convergence to global optimum. However the result of clustering algorithm without KMO is not satisfactory.
It also illustrate that KMO plays a very important role in QPSO clustering algorithm to speed up the convergence. 6.3.2. MEQPSO clustering algorithm with different parameter selections
The MEQPSO has some control parameters that affect its perfor-mance on different clustering problems. In this section we discuss the influence of two key parameters, i.e., the CE Coefficient a and the learning probability p c , on the MEQPSO clustering algorithm. (1) Contraction X  X xpansion Coefficient a : In QPSO, the Contraction X 
Expansion (CE) Coefficient a is a vital parameter to the conver-gence of the individual particle, and therefore exerts significant influence on convergence of the algorithm. For finding out the best control method for a , the MEQPSO clustering algorithm was run with several possible choices of controlling the CE Coefficient linearly from 1.0 at the beginning of the search to 0.5 at the end of the search ( Sun et al., 2004b ). The second method is inspired by the control of inertia factor o in PSO reported in Eberhart and
Shi (2001) ,thatis, a is assigned randomly within the scope of [0.5, 1] during the iterations of search process. The third method is to use adaptive mechanism ( Sun et al., 2005 ), in which the following error function is introduced to identify how close the particle is to the global best position, gbest :
D F  X  X  F i F gbest  X  = MIN  X  F i , F gbest  X  ;  X  14  X  gbest . The smaller the value of the error function for a certain particle, the closer to the gbest the particle is. The particles far away from the gbest should be given smaller value of a , whereas those close to the gbest should be given larger value of a . The following function is adopted to compute the value of a in adaptive parameter control method:  X  z  X  X  where z  X  ln ( D F ), provided all other parameters are fixed at the values shown in Table 4 , MEQPSO clustering algorithms using these three parameter control methods were executed on the human fibroblasts serum data set (with the cluster number K  X  5) for 30 independent runs. Fig. 3 shows how the
TWCV value of the global best solution (averaged over 30 runs) varied with the iterations of the clustering process over different values of a . It was observed that for these three b control methods, the adaptive control of a had the best convergence behavior of MEQPSO clustering. (2) The selection probability p c : In our proposed MEQPSO, whether the global best position is updated based on the Multi-Elitist
TWCV 6.3.3. Comparison among optimization-based methods for clustering In this subsection, we compare performance of the MEQPSO clustering algorithm with six other clustering algorithms based on PSO and QPSO on the five data sets given above. Among the other algorithms considered, there is one recently developed gene expression data clustering algorithm well known as the FGKA (Fast Genetic K-means algorithm). In order to investigate the effects of the quantum behavior and the Multi-Elitist strategy made in the Particle Swarm Optimization, we have compared the MEQPSO with the clustering methods based on PSO-In, SPSO, CLPSO, QPSO, QPSO-HPD and QPSO-TS that use the same particle representation scheme and fitness function as the MEQPSO. All the methods compared in this experiment reduced clustering to a problem of optimization and search a good classification in the space of feasible solutions using evolutionary search strategy based on swarm intelligence. The gene expression data sets used in this experiment have been introduced above in detail. We analyzed and compared these clustering algorithms on the aspects of clustering results, convergence and time of implemen-tation required for algorithms.

For all the competitor algorithms, we have selected their empirical parameter settings as reported in the corresponding 3660 3670 3680 3690 3700 3710 3720 3730 3740 3750 TWCV literatures. The control parameters for MEQPSO were chosen after experimenting with several possible values. Some of the experi-ments focusing on the effects of parameter-tuning in MEQPSO have been reported in the above subsection. These parameter settings, which are also used in this group of experiments, are recorded in Table 4 .

Table 5 shows the comparative results of the quality of the eight clustering algorithms implemented in this experiment on AD_15, rat CNS and GAL data sets. The values of the objective function (Total Within-Cluster Variation, TWCV) of the optimiza-tion and the adjusted Rand index are listed in Table 5 as the criteria to evaluate the respective clustering algorithm. The runtimes of the clustering algorithms were also considered in our comparison of the clustering algorithms. The mean and the standard deviation (within parentheses) for 30 independent runs (with different random initializations) of each of the eight algo-rithms are presented in Table 5 . The best solution in each case has been shown in bold.

Tables 6 and 7 compare the quality of the eight clustering algorithms on human fibroblasts serum and yeast cell cycle data sets. As the external standard functional categorizations about these two data sets are unknown previously, different cluster number K was selected for each clustering method (i.e., K  X  5, y ,10) in our experiments). Otherwise, the adjusted Rand index was also substituted by the Silhouette index to evaluate the performance of the clustering algorithms.
From Tables 5 X 7 , it can be seen that the FGKA clustering and the MEQPSO clustering had overall better performance than the other six competitive clustering algorithms, from the perspectives of both accuracy of convergence (evaluated by TWCV) and cluster validation (evaluated by ARI or Silhouette index). For the runtime of the algorithms, the FGKA algorithm is more time-consuming than the PSO-In, SPSO, CLPSO, QPSO, QPSO-HPD, QPSO-TS and MEQPSO algorithms, which are almost at the same level. It is because that the more complicated genetic operations like selec-tion and mutation are used in FGKA clustering algorithm while the evolution of the particles in particle swarm is more simple. Using the same particle representation scheme and fitness function and starting with the same initial process, MEQPSO clustering performed much better than the PSO-In, SPSO, CLPSO QPSO-HPD, QPSO-TS and QPSO clustering. This demonstrates the effectiveness of the quantum behavior introduced in particles and the multi-elitist strategy incorporated in the MEQPSO algorithms.
For comparing the convergence performance of the clustering algorithms based on PSO, QPSO variants and FGKA, we showed in Figs. 5 X 7 the variation of TWCV of different algorithms with the iterations (by averaging 30 independent runs) during the cluster-ing process on the AD_15, CNS and GAL data sets. Due to the space limitations, the convergence plots of different clustering algo-rithms on the other two experimental data sets are not provided. Convergence plots from Figs. 5 X 7 reveal that the convergence speed of PSO was slower than other three clustering algorithms. Though the FGKA and the other QPSO-based algorithm may converge to a smaller value of TWCV than MEQPSO at the beginning of the iterations, the multi-elitist strategy introduced in the MEQPSO clustering is able to make the algorithm carry out further searches to find out the better clustering result at last. 6.3.4. Comparison among MEQPSO clustering and other clustering methods
Since K-means clustering, hierarchical clustering and SOM clustering are the most common clustering algorithms and have been widely used for gene expression data analysis in many literatures, in this part of the experiments, we made a perfor-mance comparison between the MEQPSO algorithm and these three methods by testing the practicality of our proposed algo-rithm. Moreover, a Fuzzy C-means algorithm ( Ahmed et al., 2002 ) and several more recent improved K-means algorithms including weighted K Means (W-K Means) ( Huang et al., 2005 ) and entropy weighting K-means (EWKM) ( Jing et al., 2007 ) algorithms were also tested for a more broad performance comparison. For the clustering mechanism and principle of these algorithms are different from that of genetic algorithm and Particle Swarm Optimization, the comparison of the clustering validating is only considered in this section. Especially, different kinds of hierarch-ical clustering algorithm can be obtained according to the similarity measure methods adopted. In this experiment, we chose the average-link hierarchical clustering algorithm whose cluster similarity is defined by the average of the similarities between two data points from their respective cluster.
Table 8 shows the clustering validating of seven clustering algorithms on five gene expression data sets. The cluster numbers are specified in parentheses after the name of the data set. The values of ARI or Silhouette index are computed by averaging results of 30 independent runs of the clustering algorithms. It is obvious that the MEQPSO clustering algorithm outperformed the other competitor clustering algorithms in terms of the evaluation of the clustering results, which means that our proposed MEQPSO clustering method is a promising technique and can be widely applied in the field of gene expression data clustering. 7. Conclusions
This paper presented modified QPSO-based method, MEQPSO clustering algorithm, for hard clustering of gene expression data. In the new method, as other widely used clustering algorithm such as GKA, we described the clustering problem of gene expression data as an optimization problem. After the encoding of the clustering solutions is determined, MEQPSO algorithm partitions a gene expression data set into user-defined K cate-gories through some evolutionary operators to minimize the fitness function of Total Within-Cluster Variation. The most important feature of the MEQPSO clustering algorithm is that a Multi-Elitist strategy for searching the global best position of the QPSO is introduced to enhance the overall performance of the QPSO-based clustering algorithm (that is, the elitist concept can prevent the swarm from tending to the global best too early in the searching process). Furthermore, to improve the convergence speed, inspired by the well known clustering algorithm-Genetic K-means algorithm, we also introduced a one-step K-means operator in MEQPSO.
We provided some empirical guidelines for choosing the best suited parameters, such as the Contraction X  X xpansion Coefficient a and the learning probability p c , for the MEQPSO algorithm after a preliminary experiment with some possible sets of values. To investigate the performance of MEQPSO, we compared the new technique with other clustering algorithms based on several PSO and QPSO variants and some popular methods commonly used in gene expression data analysis. One artificial and four real gene expression data sets were selected to implement the experiments. The MEQPSO algorithm was shown to outperform the other algorithms based on population-based optimization techniques, like FGKA, PSO-In, SPSO, CLPSO, QPSO, QPSO-HPD, and QPSO-TS in terms of the accuracy of convergence and the cluster validation in a statistically way. Experimental results also show that the MEQPSO algorithm was better than the classical methods like K-means, average-link, SOM, Fuzzy C-means, weighted K-means and entropy weighting K-means in terms of the cluster validation.
One drawback of the MEQPSO clustering algorithm is the encoding strategy of the particles. Suppose that a gene expression data of large scale to be clustered, the length of the particles is drastically increased according to current coding presentation, which may lead to the deterioration of the runtime of MEQPSO. Moreover, the cluster number should be defined before the implementation of the algorithm by user, which implies that the MEQPSO algorithm cannot automatic select the cluster number during the clustering process. Future research should focus on overcoming these inadequate problems by improving the coding mechanism in MEQPSO.
 Acknowledgment
This work is supported by Natural Science Foundation of China (Project Number: 61170119), by Natural Science Foundation of Jiangsu Province, China (Project number: BK2010143), by the Fundamental Research Funds for the Central Universities (Project number: JUSRP21012), by the innovative research team project of Jiangnan University (Project number: JNIRT0702), and by the Foundation of Key Laboratory of Advanced Process Control for Light Industry(Jiangnan University), Ministry of Education, PR China. References
