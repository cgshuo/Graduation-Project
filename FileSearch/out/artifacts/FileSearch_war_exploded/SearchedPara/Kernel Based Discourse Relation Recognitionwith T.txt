 Discourse relation s capture the internal structure and logical relationshi p of coherent text, inclu d-ing Temporal , Causal and Contrastive relation s etc. The ability of recognizing such relations b e-tween text units including identifying and class i-fying provides important information to other natural language processing systems, su ch as language generation, document summarization, and question answering. For example, C ausal relation can be used to answer more sophist i-cated, non -factoid  X  X hy X  questions.

Lee et al. ( 2006) demonstrates that modeling discourse structure requires prior l inguistic ana l-ysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. Ho w-ever, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pi t-ler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchi cal structured inform a-tion in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to lingui s-tic intuition has its limitation, as it remains u n-clear what kinds of syntactic heuristics are effe c-tive for discourse analysis.

The more recent work from Lin et al. (2009) uses 2 -level production rules to represent parse tree information. Yet it doesn X  X  cover all the ot h-er sub -trees structural information which can be also useful for the recognition.

In this paper we propose using tree kernel based method to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the parse tree structures directly. These structural syntacti c features, together with other flat features are then incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification. The experiment shows that tree kernel is able to effectively i n-corpo rate syntactic structural information and produce statistical significant improvements over flat syntactic path feature for the recognition of both explicit and implicit relation in Penn Di s-course Treebank (PDTB; Prasad et al., 2008). We also illustrate th at tree kernel approach co v-ers more structure information than the produ c-tion rules, which allows tree kernel to further work on a higher dimensional space for possible better discrimination.

Besides, inspired by the linguistic study on tense and discourse anaphor (Webber, 1988), we further propose to incorporate temporal ordering information to constrain the interpretation of di s-course relation, which also demonstrate s stati s-tical significant improvements for discourse rel a-tion recognition on PDTB v2.0 for both explicit and implicit relations.

The organization of the rest of the paper is as follows . We briefly introduce PDTB in Section 2. Section 3 gives the related work on tree kernel approach in NLP and its difference with produ c-tion rules, and also linguistic study on tense and discourse anaphor. Section 4 introduces the frame work for discourse recognition, as well as the baseline feature space and the SVM classif i-er. We present our kernel -based method in Se c-tion 5, and the usage of temporal orderin g feature in Section 6 . Section 7 shows the experiment s and discussions . We conclude our works in Se c-tion 8 . The Penn Discourse Treebank (PDTB) is the largest available annotated corpora of discourse relations (Prasad et al., 2008) over 2,312 Wall Street Journal articles. The PDTB models di s-course relation in the predicate -argument view, where a discourse connective (e.g., but ) is treated as a predicate taking two text spans as its arg u-ments. The argument that the discourse con ne c-tive syntactically bounds to is called Arg2, and the other argument is called Arg1.

The PDTB provides annotations for both e x-plicit and implicit discourse relations. An explicit relation is triggered by an explicit connective. Example (1) shows an expli cit Contrast relation signaled by the discourse connective  X  X ut X  .
In the PDTB, local implicit relations are also annotated. The annotators insert a connective expression that best conveys the inferred implicit relation between adjacent sentences within the same paragraph. In Example (2), the annotators select  X  X ecause X  as the most appropr iate conne c-tive to express the inferred Causal relation b e-tween the sentences. There is one special label AltLex pre -defined for cases where the insertion of an Implicit connective to express an inferred relation led to a redundancy in the expression of th e relation. In Example (3), the Causal relation derived between sentences is alternatively lex i-c a lized by some non -connective expression shown in square brackets, so no implicit conne c-tive is inserted. In our experiments, w e treat Al t-Lex Relations the same way as normal Implicit r elations.
The PDTB also captures two non -implicit ca s-es: (a) Entity relation where the relation between adjacent sentences is based on entity coherence ( Knott et al., 2001 ) as in Example ( 4) ; and (b) No relation where no discourse or entity -based coh e-rence relation can be inferred between adjacent sentences. (4). But for South Garden, the grid was to be
Each E xplicit , I mplicit and AltLex relation is annotated with a sense. The senses in PDTB are arranged in a three -level hierarchy. The top level has four tags representing four major semantic classes: Temporal , Contingency , Comparison and Expansion . For e ach class, a second level of types is defined to further refine the semantic of the class levels. For example, Contingency has two types Cause and Condition . A third level of subtype specifies the semantic contribution of each argument. In our experiments, we use only the top level of the sense annotations. Tree Kernel based Approach in NLP. While the feature based approach may not be able to fully utilize the syntactic information in a parse tree, an alternative to the feature -based methods, tree kernel methods (Haussler, 1999) have been proposed to implicitly explore features in a high dimensional space by employing a kernel fun c-tion to calculate the similarity between two o b-jects directly. In particular, the kernel methods could be very effe ctive at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity b e-tween two discrete structured objects by directly using the original representati on of the objects instead of explicitly enumerating their features.
Indeed, using kernel methods to mine structu r-al knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 200 1 ; Moschitti, 2004) and relation extraction ( Zelenko et al., 2003; Zhang et al., 2006). Ho w-ever, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored.

Lin et al. (2009) has explored the 2 -level pr o-duction rules for discourse analysis. Howeve r, Figure 1 shows that only 2 -level sub -tree stru c-tures (e.g.  X   X  - X   X  ) are covered in production rules. Other sub -trees beyond 2 -level (e.g.  X   X  - X   X  are only captured in the tree kernel, which allows tree kernel to further leverage on information fr om higher dimension space for possible better discrimination. Especially, when there are enough training data, this is similar to the study on language modeling that N -gram beyond un i-gram and bigram further improves the perfo r-mance in large corpus.
 Tense and Temporal Ordering Information.
 Linguistic studies (Webber, 1988) s how that a tensed clause  X   X  provides two pieces of semantic information: (a) a description of an event (or si t-uation)  X   X  ; and (b) a particular configuration of the point of event (  X   X  ), the point of reference (  X   X  ) and the point of speech (  X   X  ). Both the ch a-racteristics of  X   X  and the configuration of  X   X  ,  X   X  and  X   X  are critical to interpret the relationship of event  X   X  with other events in the discourse mo d-el. Our observation on temporal ordering info r-mation is in line with the above, whi ch is also incorporated in our discourse analyzer. In the learning framework, a training or testing instance is formed by a non -overlapping clause(s)/sentence(s) pair. Specifically, since i m-plicit relations in PDTB are defined to be local , only clauses from adjacent sentences are paired for implicit cases. During training, for each di s-course relation encountered, a positive instance is created by pairing the two arguments. Also a set of negative instances is formed by paring each a rgument with neighboring non -argument clauses or sentences. Based on the training i n-stances, a binary classifier is generated for each type using a particular learning algorithm. Du r-ing resolution, (a) clauses within same sentence and sentences within thre e -sentence spans are paired to form an explicit testing instance; and (b) neighboring sentences within three -sentence spans are paired to form an implicit testing i n-stance. Th e instance is presented to each explicit or implicit relation classifier which th en returns a class label with a confidence value indicating the likelihood that the candidate pair holds a partic u-lar discourse relation. The relation with the hig h-est confidence value will be assigned to the pair. 4.1 Base Features In our system, the base fea tures adopted inclu de lexical pair, distance and attribution etc . as l isted in Table 1 . A ll these base features have been proved effective for discourse analysis in pr e-vious work. 4.2 Support Vector Machine In theory, any discriminative learning algorithm is applicable to learn the classifier for discourse analysis. In our study, we use Support Vector Machine (Vapnik, 1995) to allow the use of ke r-nels to incorporate the structure featur e.

Suppose the training set  X  consists of labeled vectors {  X   X  ,  X   X  } , where  X   X  is the feature vector of a training instance and  X   X  is its class label. The classifier learned by SVM is: where  X   X  is the learned parameter for a feature vector  X   X  , and  X  is another parameter which can be derived from  X   X  . A testing instance  X  is cla s-sified as positive if  X   X  &gt; 0 1 .

One advantage of SVM is that we can use tree kernel approach to capture syntactic parse tree information in a particular high -dimension space.
In the next section, we will discuss how to use kernel to incorporate the more complex structure feature. A parse tree that covers both discourse ar g u-ments could provide us much syntactic inform a-tion related to the pair. Both the syntactic flat path connecting connective and arguments and the 2 -level production rules in the parse tree used in previous study can be directly described by the tree struct ure. Other syntactic knowledge that may be helpful for discourse resolution could also be implicitly represented in the tree. Ther e-fore, by comparing the common sub -structures between two trees we can find out to which level two trees contain similar synta ctic information, which can be done using a convolution tree ke r-nel.

The value returned from the tree kernel r e-flects the similarity between two instances in syntax. Such syntactic similarity can be further combined with other flat linguistic features to c ompute the overall similarity between two i n-stances through a composite kernel. And thus an SVM classifier can be learned and then used for recognition. 5.1 Structural Syntactic Feature Parsing is a sentence level processing. However, in many cases two discour se arguments do not occur in the same sentence. To present their sy n-tactic properties and relations in a single tree structure, we construct a syntax tree for each p a-ragraph by attaching the parsing trees of all its sentences to an upper paragraph node. In this paper, we only consider discourse relations wit h-in 3 sentences, which only occur within each p a-Feature Names (F1) cue phrase (F2) neighboring punctuation (F3) position of connective if (F4) extents of arguments (F5) relative order of arguments (F6) distance between arguments (F7) grammatical role of arguments (F8) lexical pairs (F9) a ttribution ragraph, thus paragraph parse trees are sufficient. Our 3 -sentence spans cover 95% discourse rel a-tion cases in PDTB v2.0.

Having obtained the parse tree of a paragraph, we shall consider how to select the appropriate portion of the tree as the structured feature for a given instance. As each instance is related to two arguments, the structured feature at least should be able to cover both of these two argume nts. Generally, the more substructure of the tree is included, the more syntactic information would be provided, but at the same time the more noisy information would likely be introduced. In our study, we examine three structured features that contain dif ferent substructures of the paragraph parse tree:
Min -Expansion This feature records the m i-
Simple -Expansion Min -Expansion could, to 
Full -Expansion This feature focuses on the 
Figure 2. Min -Expansion tree built from go l-den standard parse tree for the explicit di s-course relation in Example (5). Note that to distinguish from other words, we explicitly mark up in the str uctured feature the arguments and connective, by appending a string tag  X  X rg1 X ,  X  X rg2 X  and  X  X onnective X  respectiv e-ly.
 5.2 Convolution Parse Tree Kernel Given the parse tree defined above, we use the same convolution tree kernel as described in ( Collins and Duffy, 200 2 ) and ( Moschitti, 2004 ) . In general, we can represent a parse tree  X  by a vector of integer counts of each sub -tree type (regardless of its ancestors):  X   X  = ( #  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  1 , ... , #  X   X 
This results in a very high dimensionality since the number of different sub -trees is exp o-nential in its size. Thus, it is computational i n-feasible to directly use the feature vector  X  (  X  ) . To solve the computational issue, a tree kernel function is introduced to calculate the dot pr o d-uct between the above high dimensional vectors efficiently.

Given two tree segments  X  1 and  X  2 , the tree kernel function is defined:  X   X  1 ,  X  2 = &lt;  X   X  1 ,  X   X  2 &gt; w here  X  1 and  X  2 are the sets of all nodes in trees  X  and  X  2 , respectively; and  X   X  (  X  ) is the indicator function that is 1 iff a subtree of type  X  occurs with root at node  X  or zero otherwise. ( Collins and Duffy, 2002 ) shows that  X  (  X  1 ,  X  2 ) is an i n-stance of convolution kernels over tree stru c-tures, and can be computed in  X  (  X  1 ,  X  2 ) by the following recursive definitions: (1)  X   X  1 ,  X  2 = 0 if  X  1 and  X  2 do not have the same syntactic tag or their children are different; (2) else if both  X  1 and  X  2 are pre -terminal s (i.e. POS tags) ,  X   X  1 ,  X  2 = 1  X   X  ; (3) else ,  X   X  1 ,  X  2 = where  X   X  (  X  1 ) is the number of the children of  X  1 ,  X   X  (  X  ,  X  ) is the  X  ( 0 &lt;  X  &lt; 1 ) is the decay factor in order to make the kernel value less variable with respect to the sub -t ree sizes. In addition, the recursive rule (3) holds because given two nodes with the same children, one can construct common sub -trees using these children and common sub -trees of further offspring.

The parse tree kernel counts the number of common su b -trees as the syntactic similarity measure between two instances. The time co m-plexity for computing this kernel is  X  (  X  1  X   X  2 ) . 5.3 Composite Tree Kernel Besides the above convolution parse tree kernel  X   X   X   X   X   X  1 ,  X  2 =  X  (  X  1 ,  X  2 ) defined to capture the synt actic information between two instances  X  1 and  X  2 , we also use another kernel  X   X   X   X   X  to ca p-ture other flat features, such as base features (d e-scribed in Table 1) and temporal ordering info r-mation (described in Section 6 ). In our study, t he composite kernel is defined in the following way:
Here,  X  (  X  ,  X  ) can be normalized by  X   X  ,  X  =  X   X  ,  X   X   X  ,  X   X   X   X  ,  X  and  X  is the coeff i-cient. In our discourse analyzer, we also add in te m-poral information to be used as features to pr e-dict discourse relations. This is because both our observations and some linguistic studies ( We b-ber, 1988 ) show that temporal ordering inform a-tion including te nse, aspectual and event orders between two arguments may constrain the di s-course relation type. For example, the connective 
Figure 4 . Full -Expansion tree for the explicit discourse relation in Example (5). word is the same in both Example (6) and (7), but the tense shift from progressive form in clause 6.a to simple past form in clause 6.b, ind i-cating that the twisting occurred during the state of running the marathon , usually signals a te m-poral discourse relation; while in Example (7), both clauses are in past tense and it is marked as a Causal relation.
Inspired by the linguistic model from Webber ( 1988 ) as described in Section 3 , we explore the temporal order of events in two adjacent se n-tences for discourse relation interpretation. Here event is represented by the head of verb , and the temporal order refers to the logical occurrence (i.e. before/at/after) between eve nts. For i n-stance, the event ordering in Example (8) can be interpreted as :
We notice that the feasible temporal order of events differs for different discourse relations. For example, in c ausal relations, cause event usually happens before effect event, i.e. So it is possible to infer a c ausal relation in Example ( 8 ) if and only if 8 .b is taken to be the cause event and 8.a is taken to be the effect event . That is, 8. b is taken as happening prior to his going into hospital .

In our experiments, we use the TARSQ I 3 sy s-tem to identify event, analyze tense and aspectual information, and label the temporal order of events. Then the tense and temporal ordering information is extracted as features for discourse relation recognition. In this se ction we provide the results of a set of experiments focused on the task of simultaneous discourse identification and classification. 7.1 Experimental Settings We experiment on PDTB v2.0 corpus. Besid es four top -level discourse relations, we also co n-sider Entity and No relations described in Section 2. We directly use the golden standard parse trees in Penn TreeBank. We employ an SVM coreference resolver trained and tested on ACE 2005 with 79.5% Precision, 66.7% Recall and 72.5% F 1 to label coreference ment ions of the same named entity in an article . For learning, we use the binary SVMLight developed by (Jo a-chims, 1998) and Tree Kernel Toolkits deve l-oped by (Moschitti, 2004). All classifiers are trained with default learning parameters.

The performance is e valuated using Accuracy which is calculated as follow:
Sections 2 -22 are used for training and Se c-tions 23 -24 for testing. In this paper, we only consider any non -overlapping clauses/ sentences pair in 3 -sentence spans. For training, there were 14812, 12843 and 4410 instances for Explicit , Implicit and Entity+No relations respectively; while for testing, the number was 1489, 1167 and 380. 7.2 System with Structural Kernel Table 2 lists the performance of simultaneous identification and classification on level -1 di s-course senses. In the first row, only base features described in Section 4 are used. In the second row, we test Ben and James (2007) X  X  algorithm which uses heuristically defin ed syntactic paths and acts as a good baseline to compare with our learned -based approach using the structured i n-formation. The last three row s of Table 2 report s the result s combin ing base features with three syntactic structured features (i.e. Min -Expans ion , Simple -Expansion and Full -Expansion ) d e-scribed in Section 5.

We can see that all our tree kernels outperform the manually constructed flat path feature in all three groups including E xplicit only , I mplicit only and A ll relations , with the accuracy inc rea s-ing by 1.8%, 6.7 % and 3.1% respectively . Esp e-cially, it shows that structural syntactic inform a-tion is more helpful for Implicit cases which is generally much harder than Explicit cases. We conduct chi square statistical significance test on A ll relation s between flat path approach and Simple -Expansion approach, which shows the performance improvements are sta tistical signif i-cant (  X  &lt; 0 . 05 ) through incorporating tree ke r-nel . This proves that structur al syntactic inform a-tion has good predication power for discourse analysis in both explicit and implicit relations. We also observe that among the three syntactic structured features, Min -Expansion and Simple -Expansion achieve similar performances which are better than the result for Full -Expansion . This may be due to that most significant information is with the argume nts and the shortest path co n-necting connectives and arguments. However, Full -Expansion that includes more information in other branches may introduce too many details which are rather tangential to discourse recogn i-tion. Our subsequent reports will focus on Si m-ple -Expansion , unless otherwise specified.
As described in Section 5 , to compute the structur al information, parse trees for different sentences are connected to form a large tree for a paragraph. It would be interesting to find how the structured in formation works for discourse relations whose arguments reside in different senten ces. For this purpose, we test the accuracy for discourse relations with the two arguments occurring in the same sentence , one -sentence apart, and two -sentence apart. Table 3 compares the learning systems with/without the structured feature present. From the table, for all three ca s-e s, the accuracies drop with the increase of the distances between the two arguments. However, adding the structured information would bring consis tent improvement against the baselines regardless of the number of sentence distance. This observation suggests that the structured sy n-tactic information is more helpful for inter -sentential discourse analysis.

We also conc ern about how the structured i n-f ormation works for identification and classific a-tion respectively. Table 4 lists the results for the two sub -tasks. As shown, with the structured i n-formation incorporated, the system (Base + Tree Kernel) can boost the performance of the two baseline s (Base Features in the first row andBase + Manually selected paths in the second row ), for both identification and classification respectiv e-ly . We also observe that the structural syntactic information is more helpful for classification task which is generally harder than identification. This is in line with the intuition that classific a-tion is generally a much harder task. We find that due to the weak modeling of Entity relations, many Entity relations which are non -discourse relation instances are mis -identifi ed as implicit Expansion relations. Nevertheless, it clearly d i-rects our future work. 7.3 System with Temporal Ordering Info r-To examine the effectiveness of our t emporal o rdering information, we perform experiments Base Features 67.1 29 48.6 Base + Manually selected flat path features Base + Tree kernel ( Min -Expansion ) Base + Tree kernel (Simple -Expansion) Base + Tree kernel ( Full -Expansion )
Table 3 . R esults of the syntactic structured kernel for discourse relations recognition with arg u-ments in different sentences apart.

Table 4 . R esults of the syntactic structured ke r-nel for simultaneous discourse identification and classification subtasks.

Table 2. Results of the syntactic structured ke r-nel s on level -1 discourse relation recognition. on simultaneous identification and classification of level -1 discourse relations to compare with using only base feature set as baseline . The r e-sults are shown in Table 5 . We observe that the use of temporal ordering information increases the accuracy by 3%, 3.6% and 3 . 2 % for Explicit , Implicit and All groups respectively. We conduct chi s quare statistical significant test on A ll rel a-tion s, which shows the performance improv e-ment is statistical significant (  X  &lt; 0 . 05 ). It ind i-cates that temporal ordering information can constrain the discourse relation types inferred within a clause(s)/senten ce(s) pair for both expl i-cit and implicit relations.

We observe that although temporal ordering information is useful in both explicit and implicit relation recognition, the contributions of the sp e-cific information are quite different for the two cases. In our experiments, we use tense and a s-pectual information for explicit relations, while event ordering information is used for implicit relations. The reason is explicit connective itself provides a strong hint for explicit relation, so tense and aspectual analysis which yields a reli a-bl e result can provide additional constraints, thus can help explicit relation recognition. However, event ordering which would inevitably involve more noises will adversely affect the explicit r e-lation recognition performance. On the other hand, for implici t relations with no explicit co n-nective words, tense and aspectual information alone is not enough for discourse analysis. Event ordering can provide more necessary information to further constrain the inferred relations. 7.4 Overall Results We also evaluate our model which combines base features, tree kernel and tense/temporal o r-dering information together on Explicit , Implicit and All Relations respective ly. The overall r e-sults are shown in Table 6. The purpose of this paper is to explore how to make use of the structural syntactic knowledge to do discourse relation recognition. In previous work, syntactic information from parse trees is represented as a set of heuristically selected flat paths or 2 -level production rules. However, the features defined this way may not necessarily capture all useful syntactic information provided by the parse trees for discourse analysis. In the paper, we propose a kernel -based method to i n-corporate the structural information embedded in parse trees. Specifically, we directly utilize the syntactic parse tree as a structure feature, and then apply kernels to such a feature, together with other normal features. The experim en tal results on PDTB v2.0 show that our kernel -based approach is able to give statistical significant improvement over flat syntactic path method. In addition, we also propose to incorporate tempo r-al ordering information to constrain the interpr e-tation of d iscourse relations, which also demo n-strate statistical significant improvements for discourse relation recognition, both explicit and implicit.

In future, we plan to model E ntity relations which constitute 24% of I mplicit + Entity + No r e-l a tion cases, thus to improve the accuracy of r e-l a tion detection .
 Base Features 67.1 29 48.6 Base + Te m-poral Ordering Information
Table 5 . Results of tense and temporal order information on level -1 discourse relations.
Table 6. Overall results for combined model (Base + Tree Kernel + Tense/Temporal).
