 Document expansion (DE) in information retrieval (IR) in-volves modifying each document in the collection by intro-ducing additional terms into the document. It is particularly useful to improve retrieval of short and noisy documents where the additional terms can improve the description of the document content. Existing approaches to DE assume that documents to be expanded are from a single topic. In the case of multi-topic documents this can lead to a topic bias in terms selected for DE and hence may result in poor retrieval quality due to the lack of coverage of the original document topics in the expanded document. This paper proposes a new DE technique providing a more uniform se-lection and weighting of DE terms from all constituent top-ics. We show that our proposed method significantly out-performs the most recently reported relevance model based DE method on a spoken document retrieval task for both manual and automatic speech recognition transcripts. H.3.3 [ INFORMATION STORAGE AND RETRIE-VAL ]: Information Search and Retrieval X  Query formula-tion, Relevance Feedback Document Expansion, Topic Modelling
Document expansion (DE) in information retrieval (IR) involves expanding the contents of documents in a retrieval collection so that indexed terms of the documents better describe the contents of the document for retrieval. This can be particularly valuable for noisy and short documents. In the former case the contents may be incorrectly indexed and in the latter the terms present may not sufficiently describe the contents to support effective retrieval.

An IR task that is particularly amenable to DE is spoken document retrieval (SDR) [6]. This is a challenging problem due to the vocabulary mismatch between the documents in the collection and the user query, which arises due to er-rors in automatic speech recognition (ASR) transcripts of the content and, particularly in the case of conversational speech, the failure to articulate details valuable for retrieval.
DE involves expanding each document in the collection by adding terms from a set of topically related documents, either from the search collection or elsewhere. We refer to this set of related documents as the neighbourhood of the current document. Retrieval on the expanded document in-dex is expected to produce better retrieval results, because each expanded document is likely to contain additional ben-eficial indexing terms obtained from the neighbourhood.
DE faces the critical problems of finding a good neighbour-hood for selection of the expansion terms, and selection of suitable terms from this neighbourhood. A neighbourhood comprising documents not related to the current document or additional unsuitable terms may add noise and degrade retrieval results. A typical method of computing the neigh-bourhood of a document is as follows. First, a set of doc-uments is retrieved, using the current document as a query with the help of a standard retrieval method. Documents retrieved at the top ranks are then selected as the neighbour-hood of the current document [6, 8]. This approach to DE can straightforwardly be applied to improve retrieval qual-ity of images based on document metadata [5], where the document annotations are typically very short, resembling keyword queries of ad-hoc IR or web search. However, using a full document as a query may pose a problem for mod-erately sized documents. This is because, while standard models are well suited for short keyword type queries, they generally do not perform well for cases where a query and the documents in the collection are of comparable length [7].
Recently reported work on DE has shown that the rel-evance model (RLM) [4], which is a statistical generative model, works particularly well for expanding short docu-ments such as tweets [2]. The tweets are usually short, focused on a topic and clean of ASR errors. The tweets are thus characteristically different from spoken documents, which may be moderately sized, multi-topical and noisier. We thus presume that RLM-based DE may not prove to be very effective for SDR, and this motivates us to devise a DE technique suitable for multi-topic spoken documents.
In this paper, we explore whether exploiting the underly-ing topical information of a document, D , can help in choos-ing a robust neighbourhood for DE with a balanced contri-bution from all the topics in D . Such a neighbourhood can lead to a more comprehensive selection of expansion terms, thus potentially improving retrieval effectiveness in compari-son to the RLM-based document expansion. Let us illustrate this with an example. Let D be a document comprising of a mixture of two topics, one pertaining to a product, say a re-mote control , and the other related to its cost . Consider two sets of terms from two neighbourhoods N 1 ( D ) and N 2 ( D ). Let N 1 ( D ) = { pushbutton, button, tv, switch, money } . The first four words are related to topic-1 ( remote controls ), whereas the fifth word is related to topic-2 ( cost ). Clearly, N ( D ) is biased towards topic-1. In contrast, let N 2 ( D ) be a set of terms with an even distribution of the topics, say { battery, button, euro, price } . Note that although N 1 can add 4 terms from the first topic, it can add only 1 from the second; whereas N 2 ( D ) can add 2 terms from each topic, and hence can lead to better expansion, due to a more comprehensive coverage of topics. In our proposed method, we compute this topic-level information firstly to ensure re-trieving a topic of uniform neighbourhood, and secondly to ensure a balanced selection of terms from each such topic.
The rest of the paper is organized as follows. In Section 2, we briefly survey the existing literature on DE. This is fol-lowed by Section 3, which describes our approach in detail. Section 4 evaluates the approach, and Section 5 summarizes the conclusions of our study.
All existing DE methods can be generalized to a linear combination of two term weighting functions, one for the original document terms and the other for the new terms in the document X  X  neighbourhood. Rocchio relevance feedback has been applied for DE by shifting the current document vector closer to the centroid of the neighbouring ones [6], as shown in Equation 1. Here, D is the document to be ex-panded, { D N j } R j =1 is the set of R neighbouring documents retrieved with D as the query, and D  X  is the expanded doc-ument. Equation 1 reweights the terms originally present in the cur-rent document by a factor  X  , and reweights additional terms from the neighbourhood by the factor (1  X   X  ) /R . Note that each document D N j of the neighbourhood of D in Equation 1 contributes equally to the expansion. In contrast, a method for re-weighting terms in a document from the neighbour-hood by a factor proportional to its similarity with the neigh-bourhood, was proposed in [8], as shown in Equation 2. The proportionality factor for a document D N j in the neighbour-hood is the ratio of its similarity with D , to the total similar-ity for all documents in the neighbourhood. The hypothesis is that documents in the neighbourhood retrieved at higher ranks are more similar to the current document and thus more reliable for expansion. This intuitive idea of using proportional similarity weights for different documents in the neighbourhood was shown to be theoretically well motivated by the relevance model (RLM) in [2]. In summary, the RLM involves computation of a model of relevance P ( w | R), given that such a model gener-ates the pseudo-relevant documents as well as the query [4]. In the context of DE, the RLM estimates a new document model for D , denoted D  X  , from the evidence that the new model D  X  generates both the current document D and its neighbourhood { D N j } R j =1 . Figure 1a) illustrates this, as-suming the current document is comprised of n terms, viz. D =( d 1 , . . . , d n ). The probability P ( w | D  X  ), approximated by the probability P ( w, d 1 , . . . , d n ), is given by The language model for the expanded document D  X  to be used during retrieval, is recomputed by a linear combination of the relevance model estimation P ( w | D  X  ) (the expansion factor as computed by Equation 3), and the original unigram document model for D , as shown in Equation 4.
 Note that Equation 4 is similar to Equation 2 because the similarity of a neighbourhood document D N j with D .
DE has thus evolved over time, starting with a simple vector space approach [6], moving onto applying different confidence measures for different documents in the neigh-bourhood [8], and establishing this intuitive notion from a theoretical perspective [2]. None of these methods, however, has attempted to utilize the topical information of the cur-rent document and its neighbourhood to achieve a uniform expansion of concepts for all different aspects of the cur-rent document. Our proposed model seeks to achieve this uniform balanced expansion of concepts within a document.
Our proposed model for DE uses the RLM-based model as a framework. We begin this section with a discussion on how the RLM-based model can be extended, and then describe the technical details of the extended model.
Motivation . Referring back to Equation 3, we can see that the RLM relies on the probability of co-occurrences of a new word, ( P ( w | D N j )), with that of an existing word, Figure 1: Plate diagrams of document expansion for a ) RLM (left) and b) LDA-smoothed RLM (right). ( P ( d i | D N j ) ), in the neighbourhood of the current document. Highly co-occurring terms contribute more to the expanded document model. However, such co-occurrences, if com-puted at the level of whole documents, can lead to the prob-lem of unbalanced selection of expansion terms For example, referring back to Section 1, the remote control related terms such as pushbutton , button etc. may have a higher degree of co-occurrence with terms in D , than the cost related words such as money , euro etc., whose contributions are thus down weighted. However, if co-occurrences are computed at the level of topics, it may be the case that the top co-occurring terms for the first topic are battery and button , whereas for the second topic, these could be money and price . This will lead to a better expansion, because it gives an even chance to the document to be retrieved against a query related to the production cost. These topic-level co-occurrences can be computed with the help of a topic-modelling approach, which we describe next.

LDA for topical co-occurrences . Latent Dirichlet allo-cation (LDA) has been shown to model the underlying topics of a collection of documents effectively [1]. Furthermore, it has been shown that term generation probabilities of a doc-ument, marginalized over a set of topics, improves ad-hoc IR effectiveness [9]. When applied in the context of RLM-based DE, this marginalization with topics leads to a computation of co-occurrences at the level of topics, rather than comput-ing it over the whole document in the RLM. Thus, it can lead to a more uniform contribution from each topic, rather than being biased towards a few in the estimated values of P ( w | D  X  ). We can thus assume that both the neighbourhood and the current document are generated from a set of K latent topics, as shown in Figure 1b). Marginalizing Equa-tion 3 over the K topic nodes yields Equation 5. We use Equation 5 to estimate the term weights for the expanded document model D  X  .
 P ( w | D  X  ) = = 1 R = 1
Implementation details . We perform off-line LDA in-ference over a collection of documents with a pre-configured number of topics, K , to get the output matrices  X  (document-topic) and  X  (word-topic) mappings. The optimal value of K is determined empirically. A subset of rows (those cor-responding to the neighbourhood document indices for the current document) of these matrices are then used to com-pute the probabilities P LDA ( w | D N j ,  X  ,  X  ) (LDA smoothing for the neighbourhood documents) and P LDA ( d i | D N j ,  X  ,  X  ) (LDA smoothing for the current document), as shown in Equation 5. As a final step, we substitute the value of P ( w | D  X  ) in Equation 4 to compute the linearly interpolated expansion model, similar to [2]. It is worth mentioning here that our method of DE, shown in Equation 5, is different from running RLM on top of LDA-smoothed documents models, an approach named RM+LBDM in [9]. In our approach, both the query (the current document) and the pseudo-relevant documents (the neighbourhood) are LDA-smoothed, instead of smoothing only the pseudo-relevant documents, as in RM+LBDM .
This section describes the SDR test collection data, the experimental settings and the results.
The SDR test collection used for our experiments is de-rived from the AMI corpus 1 , comprising of 100 hours of recorded planned meetings transcribed both manually and using automatic speech recognition (ASR). For our exper-iments, we use both the manual and ASR transcripts to assess the impact of ASR errors and the effectiveness of our method on both transcript types. The query set consists of text extracted from 25 PowerPoint slides which are supplied as part of the AMI dataset, the objective of the retrieval task being to locate the spoken content relevant to the topic of a given query slide. The relevant content is manually labelled across the spoken contents for each query [3].

The average length of each meeting in terms of spoken time duration is 1998 secs, with a range between 306 . 4 secs and 5297 . 84 secs. Retrieving these very long meeting doc-uments for a given query slide does not benefit a searcher, whose objective is to precisely locate the relevant time range(s) within a meeting for each query slide. Thus, to get more meaningful retrieval units, we segmented the documents in AMI corpus into chunks of fixed duration (180 secs), be-cause previous research using this test collection based on the AMI corpus reports that a fixed time-based segmenta-tion of 180 secs produced the best retrieval results [3]. The average length of these segments 2 is 99 . 49 words. These segments on average are much shorter than the TREC-8 documents (311 . 94) words, thus justifying the need for DE; but longer than tweets (21 . 92) [2], justifying the necessity of LDA smoothing to achieve a uniform selection of terms from each topic with the segments. In fact, one of the rea-sons for not using a tweet corpus for our experiements is that a) short uni-topical tweets are unsuitable to test the topical co-occurrence hypothesis for expansion of these mid-range length documents, and b) for tweets, temporal evidence is an important criterion for choosing a good expansion neigh-bourhood, which we do not pursue here.
Baselines . Our proposed DE method, DE LDA is com-pared against the following baselines: i) retrieval without DE, DE NO ; and ii) retrieval after expanding documents by RLM [2], DE RLM . We do not compare our results against approaches reported in [6, 8], since we have shown in Sec-tion 2 that these have the same mathematical form as [2].
Parameters . The parameters in our experiments were empirically optimized on the AMI test set of 25 queries. A common parameter for both DE RLM and DE LDA , is  X  (see Equation 4). This was empirically optimized to 0 . 6. This optimal value of  X  on the AMI test set is identical to the values reported in [8, 2], although these used different test collections. Another common parameter is the number of neighbourhood documents to be used for expansion, viz. h ttp://www.amiproject.org/ segment and document are used interchangeably henceforth F igure 2: a) Effect of varying K in DE LDA and b) avg. #terms added per topic, on ASR documents.
 R (see Equations 3 and 5). The value of R was varied in the range of [5 , 50], and was set to the optimal value of 20 in both cases. An additional parameter for DE LDA is the number of topics, K . To find its optimal value, we report the MAP for different values of K on the ASR data in the range of [1 , 100] in Figure 2a. Note that the leftmost point in Figure 2a, i.e. K =1, is identical to the method DE RLM The figure shows that the optimal value of K is 5. The intuitive reason for optimality at a small value of K is that the meeting documents are more homogeneous in content than news articles, for which, values of K as high as 800, are optimal [9]. Instead of optimizing the value of K separately on the manual transcripts, we used the optimal value of K as obtained for ASR.

Metrics . To evaluate the retrieval effectiveness, in addi-tion to standard metrics such as MAP, recall etc., we calcu-late the mean average segment precision (MASP) [3]. Unlike the binary valued relevance of MAP, relevance in MASP can be a real number in [0 , 1] according to the amount of relevant content present in a segment, and in addition to the rank of relevant content, MASP also considers the difference be-tween the starting time of the retrieved segment and that of the relevant content [3].
Table 1 shows the retrieval effectiveness obtained on the manual and ASR transcripts by the different approaches. The first observation is that the retrieval effectiveness with DE is higher than the baseline without DE, i.e. DE NO , both for the manual and ASR transcripts. The second ob-servation is that our approach to DE using LDA smooth-ing, significantly 3 outperforms the standard RLM-based DE method, again on both manual and ASR transcripts. Note that DE LDA achieves both higher recall and higher P@10. Moreover, the MASP results show that DE LDA is able to retrieve segments with more overlapping relevant content at higher ranks, in comparison to DE RLM . The retrieval effec-tiveness is better on the manual transcript in comparison to the ASR, as expected. Counterintuitively, the P@10 value on ASR is slightly higher than its manual counterpart. We conjencture that this is due to the fact that the manual transcript documents, due to the absence of incorrectly rec-ognized words, should have a higher number of topics than the ASR documents. In fact, setting K to 20 on the manual transcripts increases P@10 to 0 . 6480, which is higher than the value of P@10 for ASR transcript reported in Table 1.
To test the hypothesis that DE LDA is less biased towards a particular topic while selecting expansion terms, we plot M easured by Wilcoxon test with 95% confidence measure Table 1: Retrieval results using different document e xpansion methods on the AMI dataset. the distribution of expansion terms over the set of topics, averaged over all documents of the ASR collection in Fig-ure 2b. To plot the topic assignments for DE RLM , we simply used the topic mappings from DE LDA . Figure 2b shows that DE LDA indeed results in a uniform selection of terms from each topic as evident by a flatter graph, in comparison to the sharp peaks and valleys of DE RLM .
We proposed a novel DE method to ameliorate the vo-cabulary mismatch problem of short and noisy spoken doc-uments by extending RLM-based DE with topic-based LDA smoothing. This results in a more uniform selection of ex-pansion terms for each topic in the current document in comparison to the RLM-based method, which may be bi-ased towards certain topics. It is shown empirically that our proposed method significantly outperforms the RLM-based DE on a spoken document retrieval task.

In future, we would like to investigate the effect of dynam-ically varying the value of K per document.
 This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the CNGL project.
