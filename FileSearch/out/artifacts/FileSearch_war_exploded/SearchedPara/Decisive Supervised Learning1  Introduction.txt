 In traditional active learning, the learner chooses the most informative example in the unlabeled pool to query the oracle for its label. By doing so, the learner is likely to achieve high accuracy by using few labeled examples. Many variants of active learning [10,13] have been proposed (see reviews in Section 2), among which uncertain sampling [12] is one of the most intuitive and commonly used strategies. In uncertain sampling, the lea rner selects the most uncertain example which is closest to the decision boundary. By halving the version space, uncertain sampling can make the learning process converge quickly.

Most previous works on active learn ing assume the true labels can only be obtained by querying oracles. However, in many real world applications, labels can be acquired (revealed) alternatively by making predictions directly and tak-ing the consequence. For example, when a learning system sorts letters by using OCR (optical character recognition) devi ces of the post office, if the hand-written codes are ambiguous, or too difficult to recognize, they will be passed to the or-acles (human) for labels. However, if th e OCR can predict accurately the hand-written zip codes, the letter will be sorted and mailed to the recipient directly. If the prediction is not correct, the lette r will be returned and redelivered (cost or consequence of the wrong prediction).

In the above example, the acquired true labels can be given to the learner (OCR system) to further train the predictive model. We can see, besides querying oracles, making predictions provides another way to obtain the true labels in the learning process. In fact, this type of problems is ubiquitous in the real world. In order to effectively deal with the problems, we propose a new learning paradigm, where we consider two actions for acquiring true labels in the learning process. One action is to query oracles (e.g., human or experts). Certain cost has to be paid for each query. The other action is to make predictions (e.g., directly mailing the letter). After the prediction, the true label will be revealed. If the prediction is correct, no cost is paid; otherwise, misclassification cost has to be paid. To determine which action to cho ose for an example, we should compare the query cost and the misclassification cost. Since the correct action is unknown before each action is taken, we use expected misclassification cost .
Based on the querying cost and the exp ected misclassification cost, we can easily derive a probability interval as [  X ,  X  ] (see more details in Section 3.2) shown in Figure 1. For the examples with the probability falling in the interval, the learner will choose to learn them by querying an oracle for its label; otherwise, the learner will choose to make predictions on them directly. The interval is, in fact, the same as the rejection interval in t he classifiers with reject option [6,2], where the learner can reject to make predictions on an example when it is not certain about it. However, unlike our paradigm, it does not make predictions during the learning process.

During the learning process of our new paradigm, if the posterior probability produced by the learner is accurate, the n the action taken on each example will be always correct and optimal. Therefore, the action order in the learning process will not matter. When all unlabeled examples are learned, the learner can always learn the same predictive model. However, in reality, the posterior probability may not be very accurate particularly in the beginning of the learning. For those examples close to  X  or  X  , it is likely that wrong actions will be taken, which may consequently lead to a higher cost. We call those examples indecisive examples , and the actions taken on them indecisive actions . We call the examples far away from  X  and  X  decisive examples , and the actions taken on them decisive actions .
What is the optimal order of actions in the case where the probability is not perfectly accurate? To tackle the problem under the new paradigm, in this paper, we propose a novel learning algorithm, called Decisive Learner ( DL ). DL always takes the most decisive actions and attempts to make as few mistakes on the actions as possible in the learning process. As more examples are learned and the learner becomes more reliabl e, the indecisive examples may become decisive, thus fewer wrong actions will be taken. We also empirically study the performance of DL on 10 real-world datas ets, by comparing it with other typical learners under different cost settings. The experimental results demonstrate that DL has an overall best performance in terms of the total cost.

The main contributions of this work are in two folds. 1. We propose a new learning paradigm, where the learner can take two actions 2. Under the new paradigm, we propose a novel learning algorithm to find The rest of the paper is organized as fo llows. In Section 2, we will review some related work and discuss the difference with our work. In Section 3, we will introduce some preliminary for our problem setting and the new concept of decisive action. Section 4 will talk about the proposed algorithm to select actions. In Section 5, we will experimentally compare our algorithm with other typical learning algorithms. Discussion and future work will be presented in the last section. Our paradigm is bridging between the traditional active learning and classifica-tion with rejection. It is also similar to two-oracle setting in active learning. In this section, we will discuss the sim ilarities and differences with them.
In traditional active learning, learner tends to choose the most informative example in the unlabeled pool to query ora cle for its label. Uncertainty sampling [10] is one of the most intuitive and commonly used active learning strategies. It selects the most uncertain example which is closest to the decision boundary, which is one option in our learner DL.

As we mentioned in Section 1, traditional active learning has only one option to obtain the true labels, which is to query oracles. Most of the previous works assume that there exists at least one oracle who can provide the labels of the examples. In [15], the assumption is that we have one perfect oracle who can correctly give all labels. [14,5] study the setting where the oracle is noisy and may mislabel the examples. [4,14] explore the case where multiple oracles or labelers may contribute to the quality of the labels. We can see in those works querying oracles (human experts or labelers) has been regarded as the only approach to retrieve the true labels.
 In our paradigm, we can have two options (actions) to acquire the true labels. Besides querying oracles, the learner can make predictions directly and the true label will be revealed from the feedback (success or failure). Thus, we need to consider not only the cost to query oracles but also the cost of the wrong predictions. The best learning strateg y might not be always selecting the most informative (or uncertain) examples as in the traditional active learning. Our goal is to explore the best learning sequence that minimizes the total cost under this new paradigm.

For the classification with rejection (also known as abstaining classifiers), the learner can reject to make predictions o n the uncertain examples. The decision when to reject to make predictions als o relies on the ratio between the mis-classification cost and the reject cost. [ 7] studies how to effectively reduce the misclassification rate without considering the cost ratio. However, the existing works on classification with rejection only study how to minimize the cost given a predictive model, while in our paradigm we care more about the learning process that builds the predictive model with the minimal total cost.

In our paradigm, since making predictions can reveal the true labels, it can be regarded as another oracle. However, our paradigm is substantially different from the two-oracle setting [4] in active learning. In our paradigm, the  X  X racle X  (the model for making predictions) is u pdated with each new labeled example, while the two oracles in the two-oracle setting are static. In this section, we will first formally define our paradigm setting where the goal is to minimize the total cost during the learning process, and then introduce a new concept named decisive action in detail. 3.1 Paradigm Setting Given a set of labeled data L , a set of unlabeled data U and a learner M learned from L , M is allowed to select examples from U , retrieve the labels from an oracle O and update its model iteratively. Given an example in U ,itslabelcan be obtained by taking one of the two actions. The first action is to query the oracle O for its label by paying the querying cost C q . The second action is to make a prediction (positive or negative) on the example. The consequence of the prediction will reveal the true label. If the prediction is wrong, we have to pay the misclassification cost C FN for false negative and C FP for false positive pre-dictions. For each example with a posterior probability produced by the learner, the action to take can be determined. The goal of this learning problem is to find a proper learning sequence for the examples from U , such that the total cost is minimized. 3.2 Choice of Actions For an example x in U , how to choose the action depends on the costs of the two possible actions. Given the probability of being positive p (1 | x ) predicted by the learner, the expected misclassification cost will be P (1 | x )  X  C FN if 0  X  P (1 | x )  X  for false negative and false positive. If the expected misclassification cost is less than the cost of querying the oracle, we should make the prediction directly; otherwise, we should query the oracle fo r the label. Therefor e, we can calculate Without loss of generality, we transform Figure 1 into Figure 2 and look close into the region of [0.5, 1.0]. Instead of P (1 | x ) , the horizontal axis in Figure 2 changes to P ( d | x ) . Here, d is the prediction (0 or 1) made by the learner, which d =0 ;otherwise, d =1 . In the following, we will introduce two concepts that are related to the choice of actions.

The first concept we will introduce is action boundary . In Figure 2, due to the similarity,weonlylookintothethreshold  X  =1  X  C q /C FP for P ( d | x ) ,instead of two thresholds  X  and  X  in Figure 1. For examples with 0 . 5  X  P ( d | x ) &lt; X  ,we should query the oracle; while for examples with  X   X  P ( d | x )  X  1 , we should make predictions. We ca ll the threshold  X  action boundary . The position of  X  is not necessarily in the center of the axis, instead it relies on the cost of querying the oracle and the misclassification cost. If the oracle is too expensive to query, the value 1  X  C q /C FP will be small, and thus  X  will be closer to 0.5. If the wrong prediction is costly,  X  will be closer to 1.

The correct choice of the actions depends on the accuracy of the posterior probability P ( d | x ) . Here, the accurate probability means that the probability is perfectly calibrated [16]. Different classifiers have different calibration behaviors. For example, the scores produced by naive bayes , decision tree and SVM are not calibrated, while bagged decision tree and random forest can produce calibrated probabilities [11,16]. In addition, the insufficiency of training data may also lead to the inaccuracy of the probability, particularly in the beginning of the learning process when the learner does not observe enough examples. If a classifier can produce perfectly calibrated probability P ( d | x ) , then the action taken on each example in U will always be the correct choice.

The second concept, in terms of the choice of actions, is Indecisive and Deci-sive Actions . The inaccuracy of the posterior probabilities P (1 | x ) will easily lead to the wrong choice of the actions, particularly in the boundary area close to  X  as shown in Figure 2. For the boundary examples, the learner is not sure which action to take. Therefore, we call those boundary examples indecisive examples , and the actions taken on them indecisive actions . For the examples far away from the action boundary  X  (approaching 0.5 or 1), the learner is more certain about which action to take and the actions taken on them will be less likely to be mistaken. Hence, we call those examples decisive examples , and their actions decisive actions 1 . In fact, there is no clear threshold to distinguish decisive and indecisive actions. In Figure 2, we use the darkness to demonstrate the deci-siveness of the actions. The darker the color of the area, the less decisive the actions taken in the area. We can see that the decisiveness of the actions grad-ually decreases from  X  to the two ends (0.5 or 1). In Figure 2, the decisiveness of actions looks similar to the uncertain ty of examples, but they are different. The probabilities of uncertain examples are close to 0.5, while the probabilities of decisive actions varies within [0.5, 1] depending on C q and C FP .
Based on the decisiveness of action, in the next section, we will propose a new learning strategy to minimize the total cost. The key issue to the learning problem defined in Section 3 is how to correctly determine the learning sequence on the examples in U such that we can minimize the total cost. In the following, we will propose a novel learning strategy that selects examples from the most decisive to the most indecisive. 4.1 Algorithm We design a novel learner called Decisive Learner ( DL ). The basic idea of DL is that the decisive examples take precedence to be selected for learning since the actions taken on them are more likely to b e correct, and the indecisive examples will be left for learning later. As we mentioned in Section 1, when more examples are observed by the learner, the model built will become more accurate, the indecisive examples may become decisive, and consequently actions will be less likely to be mistaken. There are two types of decisive examples: examples (close to 0.5) to query the oracle and examples (close to 1) to make predictions. Both of the two types examples are beneficial for the learner. The examples with probabilities close to 0.5 can help the learner achieve high accuracy with few examples. Direct prediction on the examples with probabilities close to 1 makes good use of the current learner and is likely to obtain the true labels without any cost. Moreover, those (certain) exa mples can make the learner more robust. Therefore, in our algorithm, we take advantage of both types of examples by alternating them.

Specifically, the algorithms of decisive learner can be decomposed into the following four steps. 1. Splitting Probability Interval : Each of the probability ranges [0 . 5 , X  ) and 2. Selecting the Starting Interval : The learner chooses the most decisive 3. Learning in an Interval : The current learner checks if there is any example 4. Alternating Interval : If all examples in U are learned, we terminate the From the algorithm of DL, it is clear that the learner always learns the most decisive examples and attempts to make as few mistakes on the actions as pos-sible. This feature ensures that DL achieves a good performance in terms of the total cost. In this section, we will empirically study the performance of DL in terms of the total cost. We will compare it with other three typical learners. 5.1 Datasets and Cost Ratios We will evaluate the performance of DL on 10 UCI datasets [1], including abalone, adult-census, anneal, credit-g, diabetes, nursery, sick spambase, splice and waveform, with the size ranging from 898 to 32561.

To be more comprehensive, our evaluation will be conducted under different cost ratios (the misclassification cost C FN and C FP over the oracle querying cost C q ). As different values of C FN and C FP will not affect the comparison m /C q =4 and C m /C q =10 .Since  X  =1  X  C q /C m , the corresponding action boundary  X  are 0.6, 0.75 and 0.9. The reason we choose the minimum cost ratio as 2.5 is that we should make sure C m /C q  X  2 ; otherwise, for any P ( d | x ) , better than querying oracles regardless of P ( d | x ) and thus we do not need to conduct our research. 5.2 Other Learners In our experiments, DL will be compared with other three typical learners with different learning sequence under our learning paradigm. We will give a brief introduction of them in this subsection.
 1. Indecisive Learner : The first learner is named indecisive learner ( IL ), 2. Aggressive Learner : Aggressive learners (AGG) are those that choose the 3. Conservative Learner : In contrast to the aggressive learner, conservative Furthermore, for all the four learners (DL, IL, AGG and CON), we use bagged decision tree as the base classifier, due to its well-calibrated posterior probability as we mentioned in Section 3.2. 5.3 Experimental Setting For each of the 10 UCI datasets, we randomly select 100 examples as the labeled set, and use it to train the initial classifier for each of the four learners. The rest of the examples belong to the unlabeled set. For the four learners mentioned in Section 5.2, we calculate the total cost (the misclassification cost and the cost of querying the oracle) spent in the entire learning process. The less the cost, the better the learner. We run the four learners on the 10 datasets under the three cost ratios (Section 5.1) for 10 times. Friedman test and Wilcoxon signed-rank test will be chosen to statistically test the difference of the total cost of the four learners. It should be noted that after learning all the unlabeled examples, the four learners should have the same predictive model, since the model is built on the same set of examples. 5.4 Statistic Testing Methods The total cost in each repeat can be affected by the initial split of the dataset, thus the cost may have large variance in different repeats and even the data itself may not be normally distributed. In this case, Friedman test can be a reasonable choice for our statistic testing, since it uses the ranks of the data rather than their raw values to calculate the statis tic. Friedman test has been widely used to test whether there is a statistically significant difference between a group of values [3,8]. If significant difference exists in the group, we still need a post-hoc test on different pairs of groups to report their statistical difference. Wilcoxon signed-rank is a commonly used post-hoc test following Friedman test [3,8], thus we will use it in our experiment. 5.5 Comparative Results Table 1 demonstrates the average total costs of the four learners on the 10 UCI datasets, under three cost ratios. In order to evaluate the statistical difference, we also calculate the ranking of the four learners based on Wilcoxon signed-rank test. The ranking is calculated by the following steps. For each row in the table, we first sort the four learners by their average costs ascending. Then we compare the learner with the smallest mean to the one with the largest mean. If there is no significant difference, all the learners will be ranked as 1; otherwise, we continue to compare the smallest mean with the second larg est mean, until all the learners have been compared or no significant difference is found. In the next round, we will compare the second smallest mean with the largest mean, and repeat the same step. The process iterates until all learners are ranked.
In Table 1, the rank of each learner is presented in the bracket next to the average total cost. The four rows in the bottom of Table 1 present the average rank of the four learners over all the 10 datasets under the cost ratio (2.5, 4 and 10) respectively, as well as the overall average rank over 30 rows in the table. We can see clearly that DL is top ranked in 27 out of the total 30 comparisons and has the lowest average rank 1.1 over the 30 rows. The average costs on all the 10 datasets are also presented in Table 1, and we can see that DL is much better than the other three learners. Both the rank and the average costs illustrate that DL has the overall best perform ance in terms of the total cost.

Theoretically, IL is supposed to have the poorest performance since it always selects the most indecisive actions a nd is expected to make many mistakes. However, we observe that it is not exactly the case in Table 1. Overall, the average rank (2.4) of IL is slightly better than that (2.5) of AGG. A closer look reveals that under the cost ratio 4 and 10, IL indeed has the lowest rank among the four learners. The slight superiority of IL to AGG is due to the fact that IL performs much better than AGG when the cost ratio is 2.5. Under the cost ratio 2.5,  X  (1-1/2.5=0.6) is relatively close to 0.5. IL starts learning from the examples with probability around 0.6 while AGG from the examples with probability close to 0.5. Although the examples selected by IL are not as informative as those selected by AGG, they are still useful for the learner. Furthermore, IL can even save more costs by directly making predictions on the examples to the right side of  X  , as the expected cost of making predictions on those examples is lower than the cost to query the oracle ( (1  X  P ( d | x ))  X  C m &lt;C q ,where P ( d | x ) &gt; X  ).
From the average rank in Table 1, we can also observe that DL is more likely to have better performance when the cost rat io is high, as its average rank increases when cost ratio becomes higher. It means when the misclassification cost is much higher than the querying cost, it is safer and more desirable to use DL as the learner. This paper proposed a new learning paradigm where the learner is able to take two possible actions (querying oracles and making predictions) to acquire true labels for new examples. The new paradigm is different from the traditional active learning where oracles are the only source to obtain true labels. Under the new learning paradigm, we proposed a novel learning algorithm named decisive learner (DL), which always selects the mo st decisive examples and makes as few mistakes on the actions as possible in the learning process. In the experiments, we demonstrated the outstanding performance of DL in reducing the total cost, compared to three other typical learning strategies under the same learning paradigm. The proposed learning algorithm (decisive learner) can be applied to various real-world applications, such as optical character recognition and online advertising.

