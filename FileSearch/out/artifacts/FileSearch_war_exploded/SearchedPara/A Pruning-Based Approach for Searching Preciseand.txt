 Imbalanced data has been identified as one of ten most challenging problems in data mining [14]. A dataset is considered imbalanced when its class distribution is skewed. The skewed class distribution can be repres ented as a binary class, i.e., minority and majority class. The minority class is the one having a much smaller proportion of class examples, whereas the majority class contains a much higher proportion of class exam-traditional classifiers are biased towards the larger number of examples. For example, the C4.5 decision tree algorithm [10] assumes data is from a well balanced class dis-toward the majority class and treats the minority class as noise.

Solutions to the imbalanced class probl em can be broadly divided into two cate-cation methods. SMOTE [3] is a state-of-the-art of synthetic over-sampling algorithm that generates synthetic data along the line between minority data members and their selected nearest neighbors. The advantage of SMOTE is to have decision regions larger SMOTE selects its neighbors without regard to the majority class. This problem leads to misclassifying non-minority class examples into minority class region.
Recently, many synthetic over-sampling methods [2 X 4, 7, 8] have been proposed to overcome the imbalanced class problem. Most of the proposed algorithms are based on SMOTE and directly select seed examples to generate new synthetic data. Borderline-SMOTE [7] directly selects seed examples based on the decision boundary. Borderline data is used as a seed example to generate new synthetic data. Safe-Level-SMOTE [2] directly positions synthetic data between two minority examples based on the number of minority data neighbors. However, these methods are not intended to solve the over-generalization problem. To the best of our knowledge, MSYN [4] is the first synthetic over-sampling method that tries to overcome over-generalization. MSYN uses 1-NN X  X  margin to select a synthetic example. However, in case of a very small number of mi-nority data, MSYN has the same problem as other methods as well.

In this paper, we propose an algorithm to overcome the over-generalization problem to search for precise and generalized sets of minority data. Individually, precise and generalized sets are then used as seed sets to generate synthetic minority data. TRIM is a preprocessing algorithm for synthetic over-sampling methods. Therefore, it can be used as a preprocessor for most existing synthetic over-sampling methods. In this paper, TRIM is used as a preprocessing st ep to generate synthetic minority data for SMOTE, called TRIM-SMOTE. The experiment al results show significant performance improvements in terms of F-measure and AUC over SMOTE. For F-measure, TRIM-SMOTE statistical significance outperforms SMOTE in 26 out of 33 experiments. For AUC, TRIM-SMOTE significance outperforms SMOTE in 22 out of 33 experiments. Studies using synthetic minority over-sampling [2 X 4, 7, 8] as a technique to balance class distribution in the literature are designed based on the Synthetic Minority Over-sampling TEchnique (SMOTE) [3]. SMOTE employs k-nearest neighbors ( k -NN) as a range to couple two minority examples; new s ynthetic data is randomly generated along the line between the two minority examples. That is, SMOTE uses only minority data to generate new synthetic data without regard to the majority class. The way SMOTE generates new data can be interpreted as merging the two minority data into a disjunct with synthetic data. As a result, SMOTE makes decision regions larger and less specific There have been several other techniques to generate minority synthetic data; in this section we describe some significant works relevant to the work here.

Borderline-SMOTE (BSMOTE) [7] uses the basic assumption that data nearby the decision boundary has more chance to be misclassified than data far from the decision boundary. The author further proposed a criteria to identify borderline data based on k -NN; BSMOTE uses both minority and majority data in k -NN. The ratio of the num-ber of neighborhood minority examples is used as criterion to identify importance of data. As a result, synthetic data is genera ted in the overlapping region between two classes. Therefore, borderline SMOTE also suffers from over-generalization. The algo-rithm may cause severe over-generalization due to the focus sampling in the overlap area.

Safe-Level-SMOTE (SSMOTE) [2] has been proposed to directly position synthetic examples instead of random positioning. The saf e level criteria is based on the number of neighborhood minority data in k -NN. That is, the higher number of neighborhood minority data, the safer the position is. Since Safe-Level-SMOTE is based on SMOTE, new synthetic data are generate on a line between two minority examples. A new syn-thetic example is generated nearby a minority example that has higher number of neigh-borhood minority data.

To the best of our knowledge, Margin-guided Synthetic Over-sampling (MSYN) [4] MSYN uses 1-NN margin to estimate goodness of the synthetic data. The algorithm bias prefers synthetic data that has a large margin on both minority and majority classes. The synthetic data is ranked by the margin, MSYN then selects the top M values for use as synthetic minority data. MSYN trends to generate new synthetic data on a well separated region while avoiding the boundary region. Although MSYN can be used to avoid over-generalization, it uses all features to select a synthetic example. However, maintaining seed data. The goal of the TRIM algorithm is to avoid over-generalization. The basic idea is to identify sets of minority data having the best compromise between generalization and every splitting point is identified and evaluated based on the TRIM criteria, described as seed data to generate synthetic data via SMOTE, namely TRIM-SMOTE. 3.1 TRIM Criteria To avoid over-generalization, the seed data should be precise while maintaining their precision and generali zation. The higher the TRIM value, the more precise and gener-alized the seed data.
 where | minority | is the size of the minority data. To get better seed data, TRIM Gain is compared to TRIM .If T  X  Gain &gt; T RIM , a better seed data is obtained by a of minority data in the left and right subsets; let N be the total number of examples, N T  X  Gain can be calculated as The equation (2) is designed to capture two characteristics of SMOTE. The first charac-hull and filter them out.

Fig. 1 shows a dataset containing two classes. Minority examples are shown as di-amonds, and majority examples are stars. The convex hull of the minority class is il-lustrated by a dashed line. The relevant majority data is A , B , C , D ,and E .Therest decision trees, seed data are modeled with a hyper-rectangle. Thus, the irrelevant ma-data.

The standard maximum function is used in Equation 2 to focus on improvement of convex hull of the minority data. We want to focus on improvement of seed data while seed data. To obtain a more precise seed da ta, some minority examples are split from improvement. 3.2 TRIM Algorithm The TRIM algorithm uses a greedy approach to search for set of minority data while fil-optimum, it provides a good approximation to the optimal set. The following pseudo-code describes the algorithm.
 Algorithm TRIM(D) Input: data D
Output: list Seed 1. add D to Leaf 2. While Leaf and Candidate is not empty { 3. For each Leaf { 4. Initialize all splitting points S on Leaf i 5. calculate TRIM on Leaf i 6. For each S { 7. accept the highest T  X  Gain max that does not split any minority data at S max 8. } 9. if ( T  X  Gain max &gt;TRIM ) { 10. split Leaf i into Leaf left and Leaf right using S max 11. if ( subset left contains any minority data) 12. add Leaf left to Leaf 13. if ( subset right contains any minority data) 14. add Leaf right to Leaf 15. remove Leaf i from Leaf 16. } else { 17. add Leaf i to Candidate 18. remove Leaf i from Leaf 19. } 20. } 21. For each Candidate { 22. Initialize all splitting points S on Candidate i 23. calculate TRIM on Candidate i 24. For each S { 25. accept highest T  X  Gain max at S max splitting point 26. } 27. if ( T  X  Gain max &gt;TRIM ) { 28. split Candidate i into Candidate left and Candidate right using S max 29. if ( Candidate left contains any minority data) 30. add Candidate left to Leaf 31. if ( Candidate right contains any minority data) 32. add Candidate right to Leaf 33. remove Candidate i from Candidate 34. } else { 35. add Candidate i to Seed 36. remove Candidate i from Candidate 37. } 38. } 39. } 40. return Seed
In line 1, the algorithm starts with a single set containing all data. The algorithm consists of two main steps which are irrelevant data filtering and candidate splitting. This step focuses on splitting some minority data to get more precise seed data. Both This process iterates until no more better splittin g point is found.
 available splitting points and TRIM value of the whole dataset. In lines 6-8, T  X  Gain is sent to the second step at line 17.
 focuses only on minority data, as shown in line 25. Notice that each seed data will be processed in both steps at least once. In order to maintain the generalization, the algorithm does not split minority data from majority data that are located outside the majority and minority data.

The higher the T  X  Gain in Equation (2), the more precise and generalized data is obtained. In line 35, seed data will be generated when T  X  Gain  X  TRIM .This can be interpreted as meaning that no more precise and generalized data can be found. Although seed data is well-approximated, some seed data can be considered as noise. To filter out noise, a threshold minimum precision ( minPrecision ) is used to select tal number of data in Seed i . The precision of Seed i ( precision i ) can be expressed mathematically as shown in Equation (3). The precision value of a seed is compared against minPrecision . The seed set will be filtered out if precision &lt; minPrecision . Intuitively, if we set minPrecision too high, we lose some information. However, if we set minPrecision too low, noise can happen in the seed data. We experimentally selected minPrecision =0 . 3 as suitable to preserve information while filtering out noise.

The TRIM algorithm calculates T  X  Gain on every splitting point. Define M as the number of attributes and N be number of data values. The maximum number of splitting points is N  X  1 , and the maximum number of iterations is N  X  1 in the case where only one example data is split at each iteration. Hence, the time complexity of the algorithm is O ( MN 2 ) . However, in experiments the critical step was found to be data sorting, having complexity O ( MNlog ( N )) . In the following, the results of TRIM-SMOTE, SMOTE, and MSYN are compared us-ing 11 continuous datasets. All experiments were conducted using 10 random seeds on 10-fold cross validation with three levels of sampling, i.e., 100%, 300%, 500%. Two measurements, AUC [1] and F-measure [12], are used to evaluate performance. Each al-gorithm was used in at least 3,300 experiments. The experiments use WEKA X  X  C4.5 [6] 0.25 -M 2. For TRIM, minPrecision was set to 0.3; parameters for the other algo-rithms were set exactly same as in their papers.

In Table 1 the 11 UCI datasets [5] are sorted by percentage of minority class. The per-number of attributes (#Attributes), percentage of minority data (%Minority), number of i.e, minority class and majority class. However, the 1st -8th datasets have more than two classes. Therefore, we used one class as the minority class, and grouped the others The ME3 and others are considered as minor ity and majority classes, respectively. path and ionosphere datasets in term of AUC and F-Measure.
 As shown in Fig. 2, the three algorithms (decision tree C4.5, TRIM-SMOTE, and MSYN) exhibit comparable performance with respect to both AUC and F-measure. This can be interpretted to mean that performance improvement is not guaranteed by synthetic over-sampling methods. However, SMOTE is degraded on every percentage The reason is that MSYN and TRIM are designed to handle over-generalization while SMOTE suffers from this problem.
 In Fig. 3, every synthetic over-sampling method, i.e., SMOTE, MSYN, and TRIM-SMOTE yields lower performance with respect to F-measure when compared to C4.5. However, we obtained stable performance with MSYN and TRIM-SMOTE, i.e., 82.8% F-measure with 0.7% standardization. In contrast, SMOTE always yields lower perfor-mance on every increasing sam pling percentage. In terms of AUC, decision tree C4.5, TRIM-SMOTE, and MSYN show comparable performance while SMOTE is always lower. These four graphs illustrate the negative impact of over-generalization.
Performance of C4.5 (without sampling), SMOTE, MSYN, and TRIM-SMOTE on the eleven datasets in terms of F-measure and AUC are shown in Table 2 and 3. C4.5 is used as a baseline to evaluate improve ment or decreased performance of the three algorithms. The bottom rows provide a compa rison of each of the three algorithms with TRIM-SMOTE. The row labeled (Win/Draw/Lose Significant) summarizes the num-ber of cases where the algorithm significant outperforms, equals, or performs worse than TRIM-SMOTE. To evaluate statistical significant, Wilcoxon signed rank test [11] with p -value greater than or equal to 95% evaluates on every cross validation result. The second row shows number of cases where the algorithm obtains the highest perfor-mance among all four algorithms. The result is underlined when it obtains the highest performance.

Table 2 shows the experimental results evaluated using F-measure. The table shows that TRIM-SMOTE provided the best classification in 20 of 33 cases, and was almost similar to C4.5. The table shows that SMOTE generally underperformed both C4.5 and TRIM-SMOTE. For datasets having a very sm all number of minority examples, namely, arrhythmia-6, libras-10, glass-3, and breastTissue-fad, SMOTE and MSYN show lower performance than C4.5, whereas TRIM-SMOTE is most similar to C4.5, showing more stable performance.
 Table 3 shows experimental results evaluated using AUC. The table shows that SMOTE generally underperformed both C4.5 and TRIM-SMOTE. For the datasets hav-breastTissue-fad, SMOTE and MSYN show lower performance than C4.5, whereas TRIM-SMOTE is most similar to C4.5, showing more stable performance. For libras-10 and yeast-ME3, SMOTE shows highest performance in terms of AUC, while TRIM-SMOTE yielded the highest F-measure. Th is can be explained by the fact that SMOTE randomly generates synthetic data on every m inority data without regarding to major-ity data, whereas TRIM-SMOTE tries to maintain a precise and generalized set of seed data. As a result, TRIM-SMOTE produces comp arable recall but higher precision while SMOTE gain higher recall but lower precision.

These experimental results indicate that, of the four algorithms, SMOTE underper-We observed stable performance for MSYN on large datasets but less than C4.5 for small datasets. Synthetic minority over-sampling is a method that generates new minority examples to balance an imbalanced class distribution. The advantage is that synthetic data does not to synthetic data. Synthetic over-sampling has its own drawback: over-generalization. The problem is that the majority class region is confounded with synthetic minority examples. To overcome over-generalization, we propose an algorithm called TRIM as a preprocessing for synthetic over-sampling. TRIM searches for a set of precise mi-nority examples while maintaining their generalization. The precise seed set is used as an input to a synthetic minority over-sampling method. Thus, TRIM can be used as a preprocessing algorithm for many available synthetic over-sampling techniques such as SMOTE, BSMOTE, and MSYN. Prior to sampling, evaluation or interpretation of the seed data can also be conducted. Empirical results also show encouraging improvement over SMOTE and MSYN. TRIM-SMOTE is able to cope with the over-generalization problem more than MSYN. Its performance is stable on large dataset, and increased on small datasets. Experiments on multi-class and multivariate datasets are to be explored in the future study.
