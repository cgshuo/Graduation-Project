 1. Introduction
The task of supervised text classification, or categorization, is to learn to classify text documents into predefined categories based on a limited number of labeled training examples. Research in text categoriza-tion has received much attention over recent years from both the information retrieval and machine learn-ing communities and many methods have been proposed for this task. Very often, the documents being classified belong to the broad category of semi-structured documents.
With the increasing adoption of standards for structured document representation, such as XML, text col-lections will become even more structured. Structure can be helpful to humans for determining the source, topic, or other characteristics of a given document. In some cases, documents are explicitly organized by the contents of a certain structural element, for example, many people organize their email exclusively based on ignoring potentially useful information that can be drawn from document structure.

It is the purpose of this work to investigate how document structure may be exploited for automated text categorization. To this end, we examine three general approaches to exploiting structural information, namely tagging , splitting and stacking . Tagging is a simple preprocessing scheme which ensures that words appearing in different parts of the document are treated as different features. In the splitting approach, a separate model is built for each distinct document part. The models are combined in a way that is natural to the underlying classification algorithm. Finally, we consider stacking, in which a meta-classifier makes the final prediction based on the results of different models obtained by splitting.
 We show how these three general approaches can be combined with a number of different text classifiers. forms best and often outperforms the flat text model by a wide margin. We also point out some of the draw-backs of modeling document structure, such as data sparseness and out-of-vocabulary words. In order to ification with which we consistently achieve our best results. All methods were evaluated on four standard datasets containing different types of semi-structured documents, such as web pages, e-mail messages and news stories. To our knowledge, this work is also the most comprehensive and conclusive evaluation of dif-ferent approaches to exploiting structure in semi-structured document categorization to date.
The remainder of the paper is structured as follows. We review previous work on semi-structured doc-ument classification in the following section. Section 3 gives a brief description of the Naive Bayes (NB),
Support Vector Machine (SVM) and Fuzzy Set Membership (FSM) text classifiers. We introduce semi-structured documents and discuss some issues regarding their representation in Section 4 . Section 5 presents the different approaches to structured document classification that were considered in the study. We review our experimental setup and report on the results of our evaluation in Section 6 . The final section summa-rizes our findings and points out some promising directions for future research. 2. Related work
Although most text categorization research is limited to flat text representations, there exists a limited body of literature dealing with semi-structured documents.

Fu  X  rnkranz (1999) shows an improvement in accuracy when classifying hypertext documents by using the similar approach is adopted by Glover, Tsioutsiouliklis, Lawrence, Pennock, and Flake (2002) , who also consider combining such a model with a model trained on the local text. Chakrabarti, Dom, and Indyk (1998) also exploit the hyperlink structure of web pages and combine this with the prediction based on the local text. These approaches typically use document structure to determine the linked documents and to extract the relevant portions of text from linked documents. However, they are all limited to HTML pages or other types of linked documents. In this work, we are more concerned with the use of internal document structure.

A study by Yang, Slattery, and Ghani (2002) investigates a number of approaches to the classification of hypertext documents. Although they also use information gained from the HTML hyperlink structure, they do consider internal document structure as well. They show that using HTML meta tags and title words alone sometimes outperforms the flat text model. A previous study by the same authors ( Ghani, Slattery, &amp; Yang, 2001 ) shows that combining the output of these models with the output of the flat text model by simple averaging usually outperforms either representation alone.

Yi and Sundaresan (2000) propose a tree structure for modeling HTML documents where nodes corre-spond to document elements, such as headings, links, etc. They consider tagging words with the path to the node in which they occur and using a Naive Bayes classifier on the resulting feature vector. They also pro-
Bayes model for each distinct path. This approach is augmented with transition probabilities for the nodes in order to incorporate a model of the structure itself into the prediction. Experiments on two HTML data-sets indicate significant performance gains with tagging and further gains with their splitting approach.
Denoyer and Gallinari (2004) use a very similar splitting approach in combination with the Naive Bayes text classifier. This method is further refined by using an SVM classifier with a Fisher kernel on top of the original generative model. They report encouraging results on a number of datasets with different types of structured documents.

In email categorization, Brutlag and Meek (2000) observe that using only the features of certain header fields, such as sender and subject headers, can be beneficial for categorization. Recently, Klimt and Yang (2004) investigated email categorization using ridge regression to combine SVM models trained on different message parts. This method substantially improves on the flat text model for their email corpus. Their approach is similar to the stacking approach considered here. It differs in the choice of intermediate repre-sentation and the use of a different meta-classifier. Moreover, their evaluation is limited to email categorization. 3. Classification algorithms
We evaluated different approaches to structured document classification in combination with three text classifiers. We used two popular text categorization algorithms, namely Naive Bayes and the Support Vector Machine, and a variant of the Fuzzy Set Membership algorithm, which was recently proposed by
Wingate and Seppi (2004) . A brief description of each of these three algorithms is given in the following subsections. 3.1. Naive Bayes
The Naive Bayes classifier is commonly used in text categorization ( Lewis, 1998; Mitchell, 1997 ) due to its relatively good performance, favorable size and speed complexity and its ability to learn incrementally.
Many variants of Naive Bayes are found in the literature, depending on the underlying probability model that is used. The most common are Multivariate Bernoulli and Multinomial models, of which the Multi-nomial model usually performs best ( Eyheramendy, Lewis, &amp; Madigan, 2003; McCallum &amp; Nigam, 1998 ).
To classify a previously unobserved document d , the Multinomial Naive Bayes classifier selects the class c that is most probable with respect to the document text. An estimate for the conditional class probability p ( c j d ) is obtained using Bayes rule:
The prior probability p ( c ) is estimated from the training data as the relative frequency of training docu-from the probabilities of individual word occurrences over all words found in d : of the frequencies of all words in the document, i.e. the length of the document. The multinomial coefficient observed word distribution under a multinomial model. It is based on the assumption that each word
The individual word probabilities are derived from the training data by simply counting the number of word occurrences (word frequencies) where f ( w , c ) denotes the number of occurrences of word w in class c . Laplace smoothing is commonly used to avoid zero probabilities. The particular form of smoothing used here corresponds to adding 1  X  virtual  X  occurrence of each word in vocabulary V to complement data in the training document collections for each class. 3.2. Support vector machine The SVM ( Vapnik, 1995 ) is a kernel-based classification method suited to binary classification problems.
The SVM has favorable theoretical properties for text categorization and is also known to perform well on tion behind the SVM here (for details, see for example Burges, 1998 ).

The SVM seeks to find a hyperplane that separates positive and negative training examples with the wid-est margin, as depicted in Fig. 1 . A dual optimization problem to the problem of finding the maximum mar-gin hyperplane exists, which may be solved efficiently using quadratic programming. It turns out that the maximum margin hyperplane is a linear combination of the examples which lay closest to the decision boundary. These examples are called support vectors .
 plane, and vice versa. In the linear case, this means that the positive class is selected if where ~ a and b are the model parameters learned by the SVM.

The SVM may be generalized to non-linear class boundaries by replacing the dot product with a non-1999 ).

Different methods exist for extending the SVM to multi-class problems. We use the most straightforward and most often used one-vs-rest approach which builds one binary classifier for each class. All training doc-are selected.
 A number of different document representations have been studied for text categorization with SVMs (see Debole &amp; Sebastiani, 2003 ). In our experiments, documents were represented using Term Frequency/Inverse Document Frequency (TFIDF) vectors, a representation which is commonly used in combination with the SVM. TFIDF weights for each word w in document d are computed by the following formula: where f ( w , d ) is the number of occurrences of word w in document d and j { d number of documents in the training data that contain w . These weights are used as components of the doc-ument vector ~ d . 3.3. Fuzzy set membership
The FSM algorithm ( Wingate &amp; Seppi, 2004 ) is a simple ad hoc linear classifier. It was designed as an alternative to Naive Bayes, primarily to overcome the need for smoothing and other parameter estimation problems of NB, without sacrificing its speed and its ability to learn incrementally.

The original FSM algorithm, as proposed by Wingate and Seppi (2004) , selects the class c ( d ) according to the following class selection rule:
The sum in Eq. (6) is over all words w that appear in document d . f ( w , d ) denotes the number of occur-ment. The overall score for class c is thus the average probability of class c over all word occurrences.
We found that the original FSM method suffers from a high bias in favor of larger classes. In order to improve performance on datasets with skewed training data, we modified the original class selection rule to use relative word frequencies: where f * ( w , c ) denotes the relative frequency of word w in class c : modified algorithm, which we labeled FSM*, is very similar to the Rocchio-style AIM classifier ( Barret &amp;
Selker, 1995 ), which was applied to email categorization by Brutlag and Meek (2000) . 4. Modeling semi-structured documents
In this article, we focus on classification of semi-structured documents. Such documents may be broken down into elements or fields , each of which contains either structured (numeric or symbolic) data or non-structured textual data. In general, semi-structured documents are represented with a hierarchical tree structure , so that structural elements may include other (sub)elements. Two examples are shown in Fig. 2 .
We model documents as collections of components , defined by some mapping from the original tree structure. Each component contains the textual content of the corresponding structural elements. In the simplest scenario, a straightforward one-to-one mapping from structural elements to components can be used. When dealing with tree-structured documents, components can be defined by the most specific node pends on the particular semantics of document structure and its perceived relevance to classification, much like choosing appropriate attributes to describe examples in any problem domain. For example, a &lt; description &gt; element in an XML document may be used in different contexts to describe different entities. Therefore, &lt; description &gt; elements could be used in combination with some parent elements to define different components for descriptions of different entities. On the other hand, using the full path of structural elements may be impractical, since many elements may be considered irrelevant for classifica-tion (such as, for example, various formatting tags in HTML). 5. Exploiting document structure
In this section, we present three different methods for exploiting structural information for document categorization. In particular, we would like to decouple the strategy used for exploiting document structure from the underlying text classification algorithm. 5.1. Tagging
A common approach is to treat word occurrences in different document components as different fea-tures. We refer to this as tagging , since the most straight-forward implementation is to tag words with the name of the component in which they appear, thus producing different features for the same word in different contexts. This method has the advantage of being completely independent of the classification algorithm that is used and thus requires no modification of the underlying classifier.

It should be noted that tagging is often used by authors in comparative evaluations of text categorization algorithms, such as the one by Dumais, Platt, Heckerman, and Sahami (1998) , but no comparison to the flat text model is usually provided. Experiments by Yi and Sundaresan (2000) on two HTML datasets indi-cate that tagging can significantly boost accuracy. 5.2. Splitting
Another strategy for incorporating document structure is to train as many instances of the base classifier as there are structural components. A separate model is trained for each document part. We call this nent. Each document is represented by one such set of vectors. 5.2.1. Combining the individual component models
The splitting approach assumes that words are distributed differently in different document components and attempts to build a better classifier by modeling these distributions separately. Word occurrences re-main the primary features on which classification is based. The predictions of the individual component models are combined so that all word occurrences are considered equally important for classification, irre-spective of their structural context. The contribution of document components is therefore proportional to the amount of words that they contain. The way in which this is achieved generally depends on the classi-fication algorithm at hand. Details for the three classifiers that were considered in this paper are given below. 5.2.1.1. NB. Staying with the independence assumption, components are considered independent, so that the total probability of the observed document is merely a product of the individual component probabilities:
The first product is over all structural components s that are present in the document d where p probability of each document component. A separate model p frequencies f s ( w , d ) are maintained on a per-component basis. This method of combining per-component
Naive Bayes models has also been used by other authors ( Denoyer &amp; Gallinari, 2004; Yi &amp; Sundaresan, 2000 ). 5.2.1.2. SVM. The output of the SVM classifier is the distance from the test example to the separating hyperplane. The most straight-forward method of combining these predictions is a sum over all per-com-ponent models. However, this approach overvalues words occurring in shorter components. The reason for ponents to contribute a similar amount to the overall sum on average . In other words, the information about how many features contributed to the prediction of a particular component model is lost.
Platt reports empirical evidence that the output of a linear SVM classifier is often proportional to the log vidual component models seems like a good way of combining the predictions. One plausible approach is to multiply the component scores with the number of features that contributed to the score. The binary deci-sion rule for the SVM thus becomes: component and ~ d s is the corresponding feature vector of the document. A separate model h ~ a by the SVM for each component. If we assume that the output of the SVM is independent of the dimension of the input vector, then this approach ensures that all word occurrences are treated as equally important. 5.2.1.3. FSM*. The FSM* algorithm assigns a score to each class which is a sum over all features in the document vector (Eq. (7) ). After splitting the vector by document components, this sum is over all features individual component models:
Again, the first sum is over all structural components s that are present in document d . Relative word fre-quencies f s  X  w ; c  X  are maintained for each document component separately. 5.3. Stacking
The splitting approach discussed in the previous section has the advantage of modeling word distribu-tions in each document component separately. When using this approach, longer components will usually dominate the classification. Although this can be justified by the fact that such components contain more information on which the classification is based, in reality, we may expect that some document components are more discriminative than others. A reasonable solution is to use stacked generalization or stacking ( Wolpert, 1992 ) to combine the predictions of individual per-component models. In the stacking approach, which in turn makes the final prediction. To achieve this, a number of design decisions must be made. 5.3.1. Meta features
Firstly, we have to choose a suitable representation for the output of the level-0 models. The features of
Ting and Witten (1997) suggest that per-class probability estimates predicted by each of the underlying models are suitable meta-features. When classifying an example document d , the input for the meta-classi-fier is a fixed-length vector ~ e d of the following form:
We implemented stacking using per-component models of NB, SVM and FSM* as level-0 classifiers. The contain a normalization constant Z , which is chosen so that features after normalization: 5.3.1.2. SVM. In order to obtain probability estimates, we transform the output of the level-0 SVM clas-sifier for component s with the sigmoid function and re-normalize sification problem for class c and ~ d s denotes the feature vector of document d pertaining to component s . 5.3.1.3. FSM*. The FSM* classifier is a probabilistic classifier. To obtain meta-features, we simply re-nor-malize the output of the per-component FSM* models: 5.3.2. The meta classifier
We expect that a suitable meta-classifier will be able to learn which document components are more dis-criminative and weight the predictions of the corresponding models accordingly. The work of Ting and
Witten (1997) suggests that a linear combination of the predictions produced by the level-0 classifiers may work well in practice. We chose the SVM with a linear kernel as our meta-classifier. Besides being a linear model geared directly toward classification, we expect it to work well with the chosen intermediate representation ~ e d with all meta-features in [0 ... 1]. 5.3.3. Training the stacked classifier
Finally, a strategy must be devised for training the meta-classifier. In particular, unbiased training exam-ples containing predictions made by the level-0 classifiers should be provided. To this end, we create a k -trained on the remaining portion of the data. The predictions of these models on the held-out portion of the 0 classifiers are re-trained on the entire training set. At classification time, an example document is first classified by all level-0 (per-component) classifiers. The output of these models is combined by the meta-classifier to form the final prediction. 5.4. Drawbacks of structured models
Word frequencies in natural language are modeled well by the heavy-tailed Zipfian (power-law) distri-bution. This means that most words in a typical corpus are extremely infrequent, making inference of pre-set of parameters must be learned for each structural component, greatly increasing the problem of data sparseness.

A second problem is an increase in the number of words that are novel in a certain context, so called ment because they are novel in the context of a particular document component, even if the word is other-wise a good predictor of document class. It is intuitive to expect that a word which is common in one structural component is likely to occur in other components. Moreover, words generally have the same meaning irrespective of the structural context.

As an initial step toward tackling these issues, we experiment with including the flat text of each docu-ment as if it were yet another component. In the tagging approach, this allows the classifier to decide whether a word occurrence is relevant for classification in the context of a certain document component, stacking approach, the meta-classifier may use the flat text model as a back-off in the event that per-com-ponent models are weak, presumably due to insufficient training data. 6. Experiments and results
We conducted an extensive experimental evaluation of the different approaches to semi-structured doc-ument categorization that were discussed in previous sections. This section contains a description of our experimental setup and a summary of the results of these experiments. 6.1. Experimental setup
In all experiments, texts were first split into word tokens consisting of one or more consecutive alpha-betical characters delimited by white space or punctuation. All characters were converted to lower case.
All words in the resulting vocabulary were used for classification. No stemming, stop-word removal or fea-ture selection was used.

In experiments with the SVM, the TFIDF weights were computed separately for each train/test split, so that the IDF component reflected the document frequency of a word in the training set alone. All TFIDF document vectors were normalized with their Euclidean length. Words that appeared in less than two train-ing documents were excluded prior to training of the SVM. We used the SVM C = 1due to normalization. We used our own implementations of Naive Bayes and FSM*.

Unless otherwise noted, testing was done using 4-fold cross validation. To this end, we split the data ran-domly into four equal parts. On each cross validation fold, 25% of the data was used for testing and the remaining 75% of the data was used for training, so that each document appeared in the test set exactly once. We evaluated all methods using the same split in all experiments. 6.2. Datasets
Experiments were conducted on four publicly available datasets containing different types of semi-struc-tured documents. We give some basic statistics for all four corpuses in Table 1 .

The WebKb 2 dataset consists of HTML web pages from four different universities which are classified into seven different categories. Following McCallum and Nigam (1998) , we only used the four largest top-ical categories ( project , faculty , student and course ).

The 20 Newsgroups 3 dataset consists of 18,828 newsgroup posts, collected from 20 different public news-groups. The messages are distributed roughly evenly across all 20 classes. The Ling-pam 4 dataset contains spam and legitimate email messages posted to a linguistics newsgroup.
The distribution of examples is approximately five to one in favor of legitimate messages. We used the  X  X  X are X  X  version of this dataset and the predefined 10-fold cross validation split.

The Reuters-21578 5 dataset contains newswire articles from the Reuters news agency. Reuters-21578 is a multilabeled dataset, so each article may be assigned to zero or more categories. We perform multilabeled categorization by training a binary one-vs-rest classifier for each category. Experiments were performed on the standard  X  X  X odApte X  X  train/test split, which uses 12,902 documents of the original collection, of which categories. 6.3. Document modeling
Semi-structured documents were modeled as follows. Email messages were represented with sender , subject and body components. Note that the Ling-Spam corpus does not include sender headers, so we omit this component in experiments with the Ling-Spam dataset. Web pages were split into &lt; TI-We used title and body fields of news articles in the Reuters-21578 dataset.

We represent each document with the textual value of these components. We ignore any hierarchical relations between the components (e.g. we ignore the fact that &lt; H *&gt; components are nested within a ponents found in a document into one textual feature). 6.4. Performance measures
We measured classifier accuracy on unilabeled datasets, i.e. the proportion of test documents that are classified correctly by the classifier. For the multilabeled Reuters dataset, we report on micro-averaged break-even-point (BEP) which is defined in terms of the standard measures of precision and recall . Precision p is the proportion of true document-category assignments among all assignments predicted by the classi-fier. Recall r is the proportion of true document-category assignments that were also predicted by the clas-achieve in practice, a common approach is to use the arithmetic mean of recall and precision as an approxi-mation, i.e. BEP = ( p + r )/2. The micro-averaged BEP is computed by first summing the elements of all binary contingency tables (one for each category). Precision, recall and BEP are then computed from these accumulated statistics.

To determine statistical significance of the results, we compare methods using a one-sided micro sign test classification is correct or incorrect. For the multilabeled Reuters dataset, the test was performed on the outcomes of all binary document-category decisions, as suggested by Yang and Liu (1999) . We compared all pairs of methods for each dataset/classifier combination. This yields a total of 252 tests, to which we applied the Benjamini X  X ochberg correction ( Benjamini &amp; Hochberg, 1995 ) to control the false discovery rate due to multiple comparisons. 6.5. Evaluation of individual component models
To evaluate the usefulness of individual document components for our particular text categorization tasks, we conducted experiments using only the textual contents of individual components. The results ponent with the single exception of the &lt; BODY &gt; component in the WebKb dataset in combination with the
SVM classifier. As a rule of thumb, components that contain a larger amount of text are more informative for classification. The graphs suggest that all classifiers make good use of the abundance of features pro-vided by such components. One notable exception is the title component of Reuters news stories, which is more useful than the body component. As expected, the SVM clearly dominates in the flat text represen-performance is inferior to some of the simpler models for certain components with fewer features. This is consistent with the findings of Forman and Cohen (2004) , who show that simpler models, such as Naive
Bayes, may be superior to SVMs when little training data is available. 6.6. Main results
The main results of our experiments are given in Tables 2 and 3 . The first table contains categorization set. The tag + , split + and stack + captions denote tagging, splitting and stacking approaches in which the flat text of the document was also used as if it were a separate component.

As with flat text categorization, the SVM proves most effective in all structured approaches as well. Of the different approaches to structured document categorization, the stacking approach is the clear winner.
Its performance is superior to other models on all datasets, sometimes strikingly so. The results achieved with the stacking approach are always improved by including the flat text representation. This gain is sta-tistically significant in most cases.

The benefits of tagging and splitting approaches are less clear and vary among different combinations of this behavior. An interesting pattern emerges if we observe the relative performance of different methods, the flat text model performs worst. It performs second to last in the remaining two experiments. The stack-and the differences are rarely significant.

Generally, exploiting document structure seems most beneficial for the WebKb dataset. Here, tagging, splitting and stacking consistently improve performance over the flat text representation. The combination of Naive Bayes and splitting improves accuracy by 2.7%, which is comparable to the improvement of 2% reported by Denoyer and Gallinari (2004) , who use a similar approach on a different subset of WebKb, although they also consider transitions between structural elements in their model. The 82.8% accuracy of the flat text NB model is comparable to their NB baseline of 81%. Using the stack cation accuracy on the WebKb dataset increases by 8 X 9% compared to the flat text model. Classification error is almost halved in all cases.

The stack + method also performs well on the 20 Newsgroups and Reuters datasets. Although the abso-lute gains are not as pronounced as for WebKb, they are consistently significant. This is particularly rele-of any previous study which would consider exploiting document structure for categorization on these data-sets. Our Reuters results are comparable to those obtained by Dumais et al. (1998) . They report 92.0% micro-BEP using the SVM with a different document representation and mutual information feature selection. They use tagging in their experiments, i.e. they distinguish words that appear in the title from words in the body of a document. Our flat text 20 Newsgroups results with Naive Bayes are practically identical to the 85% accuracy reported by McCallum and Nigam (1998) . Our best stacking result with the SVM is considerably higher than the 91.3% accuracy reported by Bekkerman, El-Yaniv, Tishby, and
Winter (2003) , which was achieved with a combination of SVM and distributional word clusters. the best of our knowledge, our result of 93.7% is the best 20 Newsgroups result reported to date. Stacking also performs best on the Ling-Spam dataset, although the difference is not always significant.
Even the baseline algorithms achieve very good results on this dataset and thus leave little room for improvement. 7. Conclusion
We have presented three different strategies for exploiting internal document structure for text catego-rization and evaluated their performance on a number of datasets containing different types of semi-struc-tured documents. Of the methods that were considered, stacking is the clear winner in terms of classification performance. The potential benefits of tagging and splitting are less clear and vary among datasets. We be-lieve that stacking is the most viable approach when accuracy is the most important factor in the design of a text categorization system. A minor limitation of the stacking method is its computational overhead. We have also shown that incorporating the flat text model usually improves performance. This improvement is particularly consistent with stacking, yielding the winning combination in all trials.
There are many ways in which the stacking approach could be improved, and we intend to pursue these dictions of the level-0 classifiers are. This additional information could reflect the number of words that as proposed by Bennett, Dumais, and Horvitz (2005) . The use of meta-classifiers with richer hypothesis spaces should be investigated in order to take advantage of such additional information.

Throughout this work, we assume the representation of documents is prescribed in advance. However, choosing a suitable representation may prove difficult when dealing with complex, highly structured doc-uments, especially if the semantics of the document structure are not known. Automated methods for find-ing a good mapping from the original tree structure may therefore present an interesting avenue for future research.

Of the three text categorization algorithms used, the FSM algorithm is a newcomer in the field of text categorization. A modified version of this algorithm, FSM*, exhibits a combination of strong classification performance and favorable computational properties. Although not the primary goal of this paper, our re-sults do warrant further research into the FSM method.
 Acknowledgements
The work presented in this paper was supported by the Ministry of Higher Education, Science and Tech-nology of the Republic of Slovenia.
 References
