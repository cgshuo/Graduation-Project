 Julien Mairal julien.mairal@inria.fr INRIA LEAR Project-Team, Grenoble, France The principle of iteratively minimizing a majoriz-ing surrogate of an objective function is often called majorization-minimization ( Lange et al. , 2000 ). Each iteration drives the objective function downhill, thus giving the hope of finding a local optimum. A large number of existing procedures can be interpreted from this point of view. This is for instance the case of gradient-based or proximal methods (see Nesterov , 2007 ; Beck &amp; Teboulle , 2009 ; Wright et al. , 2009 ), EM algorithms (see Neal &amp; Hinton , 1998 ), DC program-ming ( Horst &amp; Thoai , 1999 ), boosting ( Collins et al. , 2002 ; Della Pietra et al. , 2001 ), and some varia-tional Bayes techniques ( Wainwright &amp; Jordan , 2008 ; Seeger &amp; Wipf , 2010 ). The concept of  X  X urrogate X  has also been used successfully in the signal processing lit-erature about sparse optimization ( Daubechies et al. , 2004 ; Gasso et al. , 2009 ) and matrix factorization ( Lee &amp; Seung , 2001 ; Mairal et al. , 2010 ). In this paper, we are interested in generalizing the majorization-minimization principle. Our goal is both to discover new algorithms, and to draw connections with existing methods. We focus our study on  X  X irst-order surrogate functions X , which consist of approxi-mating a possibly non-smooth objective function up to a smooth error. We present several schemes ex-ploiting such surrogates, and analyze their convergence properties: asymptotic stationary point conditions for non-convex problems, and convergence rates for con-vex ones. More precisely, we successively study: (see Tseng &amp; Yun , 2009 ; Shalev-Shwartz &amp; Tewari , 2009 ; Nesterov , 2012 ; Richt  X arik &amp; Tak  X a X c , 2012 ); spired by Nesterov ( 2004 ); Beck &amp; Teboulle ( 2009 ); gradient method (see Zhang , 2003 ; Harchaoui et al. , 2013 ; Hazan &amp; Kale , 2012 ; Zhang et al. , 2012 ); We present in this work a unified view for analyz-ing a large family of algorithms with simple con-vergence proofs and strong guarantees. In par-ticular, all the above optimization methods except Frank-Wolfe have linear convergence rates for mini-mizing strongly convex objective functions. This is remarkable for MISO, the new incremental scheme derived from our framework; to the best of our knowledge, only two recent incremental algorithms share such a property: the stochastic average gradi-ent method (SAG) of Le Roux et al. ( 2012 ), and the stochastic dual coordinate ascent method (SDCA) of Shalev-Schwartz &amp; Zhang ( 2012 ). Our scheme MISO is inspired in part by these two works, but yields dif-ferent update rules than SAG or SDCA.
 After we present and analyze the different optimization schemes, we conclude the paper with numerical exper-iments focusing on the scheme MISO. We show that in most cases MISO matches or outperforms cutting-edge solvers for large-scale  X  2 -and  X  1 -regularized logistic re-gression ( Bradley et al. , 2011 ; Beck &amp; Teboulle , 2009 ; Le Roux et al. , 2012 ; Fan et al. , 2008 ; Bottou , 2010 ). Given a convex subset  X  of R p and a continuous func-tion f : R p  X  R , we are interested in solving where we assume, to simplify, that f is bounded below. Our goal is to study the majorization-minimization scheme presented in Algorithm 1 and its variants. This procedure relies on the concept of surrogate functions, which are minimized instead of f at every iteration. 2 Algorithm 1 Basic Scheme input  X  0  X   X ; N (number of iterations). 1: for n = 1 ,...,N do 2: Compute a surrogate function g n of f near  X  n  X  1 ; 3: Update solution:  X  n  X  arg min  X   X   X  g n (  X  ) . 4: end for output  X  N (final estimate); For this approach to be successful, we intuitively need surrogates that approximate well the objective f and that are easy to minimize. In this paper, we focus on  X  X irst-order surrogate functions X  defined below, which will be shown to have  X  X ood X  theoretical properties. Definition 2.1 ( First-Order Surrogate).
 A function g : R p  X  R is a first-order surrogate of f near  X  in  X  when the following conditions are satisfied:  X  Majorization : we have g (  X   X  )  X  f (  X   X  ) for all  X   X  in arg min  X   X   X  g (  X  ) . When the more general condition g  X  f holds, we say that g is a majorant function;  X  Smoothness : the approximation error h , g  X  f is differentiable, and its gradient is L -Lipschitz contin-uous. Moreover, we have h (  X  ) = 0 and  X  h (  X  ) = 0 . We denote by S L ( f, X  ) the set of such surrogates, and by S L, X  ( f, X  ) the subset of  X  -strongly convex surrogates. First-order surrogates have a few simple properties, which form the building block of our analyses: Lemma 2.1 ( Basic Properties -Key Lemma).
 Let g be in S L ( f, X  ) for some  X  in  X  . Define h , g  X  f and let  X   X  be in arg min  X   X   X  g (  X  ) . Then, for all  X  in  X  ,  X  | h (  X  ) | X  L 2 k  X   X   X  k 2 2 ;  X  f (  X   X  )  X  f (  X  ) + L 2 k  X   X   X  k 2 2 .
 Assume that g is in S L, X  ( f, X  ) , then, for all  X  in  X  ,  X  f (  X   X  ) +  X  2 k  X   X   X   X  k 2 2  X  f (  X  ) + L 2 k  X   X   X  k The proof of this lemma is relatively simple but for space limitation reasons, all proofs in this paper are provided as supplemental material. With Lemma 2.1 in hand, we now study the properties of Algorithm 1 . 2.1. Convergence Analysis For general non-convex problems, proving convergence to a global (or local) minimum is out of reach, and classical analyses study instead asymptotic stationary point conditions (see, e.g., Bertsekas , 1999 ). To do so, we make the mild assumption that for all  X , X   X  in  X , the directional derivative  X  f (  X , X   X   X   X  ) of f at  X  in the direction  X   X   X   X  exists. A classical necessary first-order condition (see Borwein &amp; Lewis , 2006 ) for  X  to be a lo-cal minimum of f is to have  X  f (  X , X   X   X   X  ) non-negative for all  X   X  in  X . This naturally leads us to consider the following asymptotic condition to assess the quality of a sequence (  X  n ) n  X  0 for non-convex problems: Definition 2.2 ( Asymptotic Stationary Point). A sequence (  X  n ) n  X  0 satisfies an asymptotic stationary point condition if In particular, if f is differentiable on R p and  X  = R p , this condition implies lim n  X  +  X  k X  f (  X  n ) k 2 = 0 . Building upon this definition, we now give a first con-vergence result about Algorithm 1 .
 Proposition 2.1 ( Non-Convex Analysis).
 Assume that the surrogates g n from Algorithm 1 are in S L ( f, X  n  X  1 ) and are majorant or strongly convex. Then, ( f (  X  n )) n  X  0 monotonically decreases and (  X  n satisfies an asymptotic stationary point condition. Convergence results for non-convex problems are by nature weak. This is not the case when f is convex. In the next proposition, we obtain convergence rates by following a proof technique from Nesterov ( 2007 ) originally designed for proximal gradient methods. Proposition 2.2 ( Convex Analysis for S L ( f, X  ) ). Assume that f is convex and that for some R &gt; 0 , k  X   X   X   X  k 2  X  R for all  X   X   X  s.t. f (  X  )  X  f (  X  0 ) , (1) where  X   X  is a minimizer of f on  X  . When the surro-gate g n in Algorithm 1 are in S L ( f, X  n  X  1 ) , we have where f  X  , f (  X   X  ) . Assume now that f is  X  -strongly convex. Regardless of condition ( 1 ), we have where  X  , L  X  if  X  &gt; 2 L or  X  , 1  X   X  4 L otherwise. The result of Proposition 2.2 is interesting in the sense that it provides sharp theoretical results without mak-ing strong assumption on the surrogate functions. The next proposition shows that slightly better rates can be obtained when the surrogates are strongly convex. Proposition 2.3 ( Convex Analysis for S L, X  ( f, X  ) ). Assume that f is convex and let  X   X  be a minimizer of f on  X  . When the surrogates g n of Algorithm 1 are in S
L, X  ( f, X  n  X  1 ) with  X   X  L , we have for all n  X  1 , When f is  X  -strongly convex, we have for all n  X  1 , Note that the condition  X   X  L is relatively strong; it can indeed be shown that f is necessarily (  X   X  L )-strongly convex if  X &gt;L , and convex if  X  = L . The fact that making stronger assumptions yields better con-vergence rates suggests that going beyond first-order surrogates could provide even sharper results. This is confirmed in the next proposition: Proposition 2.4 ( Second-Order Surrogates).
 Make similar assumptions as in Proposition 2.2 , and also assume that the error functions h n , g n  X  f are twice differentiable, that their Hessians  X  2 h n are M -Lipschitz, and that  X  2 h n (  X  n  X  1 ) = 0 for all n . Then, If f is  X  -strongly convex, the convergence rate is su-perlinear with order 3 / 2 .
 Consistently with this proposition, similar rates were obtained by Nesterov &amp; Polyak ( 2006 ) for the New-ton method with cubic regularization, which involve second-order surrogates. In the next section, we fo-cus again on first-order surrogates, and present simple mechanisms to build them. The proofs of the different claims are provided in the supplemental material. 2.2. Examples of Surrogate Functions Lipschitz Gradient Surrogates.
 When f is differentiable and  X  f is L -Lipschitz, f ad-mits the following majorant surrogate in S 2 L,L ( f, X  ): In addition, when f is convex, g is in S L,L ( f, X  ), and when f is  X  -strongly convex, g is in S L  X   X ,L ( f, X  ). Note also that minimizing g amounts to performing a clas-sical classical gradient descent step  X   X   X   X   X  1 L  X  f (  X  ). Proximal Gradient Surrogates.
 Assume that f splits into f = f 1 + f 2 , where f 1 is differentiable with a L -Lipschitz gradient. Then, f admits the following majorant surrogate in S 2 L ( f, X  ): g :  X  7 X  f 1 (  X  ) +  X  f 1 (  X  )  X  (  X   X   X  ) + The approximation error g  X  f is indeed the same as in the previous paragraph and thus:  X  when f 1 is convex, g is in S L ( f, X  ). If f 2 is also  X  when f 1 is  X  -strongly convex, g is in S L  X   X  ( f, X  ). If Minimizing g amounts to performing a proximal gradi-ent step (see Nesterov , 2007 ; Beck &amp; Teboulle , 2009 ). DC Programming Surrogates.
 Assume that f = f 1 + f 2 , where f 2 is concave and dif-ferentiable with a L 2 -Lipschitz gradient. Then, the fol-lowing function g is a majorant surrogate in S L Such a surrogate forms the root of DC-(difference of convex functions)-programming (see Horst &amp; Thoai , 1999 ). It is also indirectly used in reweighted- X  1 algo-rithms ( Cand`es et al. , 2008 ) for minimizing on R p + a cost function of the form  X  7 X  f 1 (  X  )+  X  P p i =1 log(  X  Variational Surrogates.
 Let f be a real-valued function defined on R p 1  X  R p 2 . Let  X  1  X  R p 1 and  X  2  X  R p 2 be two convex sets. De-fine  X  f as  X  f (  X  1 ) , min  X   X   X  1 7 X  f (  X  1 , X  2 ) is differentiable for all  X  2 in  X   X   X  2 7 X  X  X  1 f (  X  1 , X  2 ) is L -Lipschitz for all  X  1 in R p  X   X  1 7 X  X  X  1 f (  X  1 , X  2 ) is L  X  -Lipschitz for all  X  2 in  X   X   X  2 7 X  f (  X  1 , X  2 ) is  X  -strongly convex for all  X  1 Let us fix  X  1 in  X  1 . Then, the following function is a majorant surrogate in S 2 L  X  X  (  X  f, X  ) for some L  X  X  &gt; 0: When f is jointly convex in  X  1 and  X  2 ,  X  f is itself convex and we can choose L  X  X  = L  X  . Algorithm 1 becomes a block-coordinate descent procedure with two blocks. Saddle Point Surrogates.
 Let us make the same assumptions as in the previous paragraph but with the following differences:  X   X  2 7 X  f (  X  1 , X  2 ) is  X  -strongly concave for all  X  1  X   X  1 7 X  f (  X  1 , X  2 ) is convex for all  X  2 in  X  2 ;  X   X  f (  X  1 ) , max  X  Then,  X  f is convex and the function below is a majorant surrogate in S 2 L  X  X  (  X  f, X  1 ): where L  X  X  , max(2 L 2 / X ,L  X  ). When  X  1 7 X  f (  X  1 , X  2 affine, we can instead choose L  X  X  , L 2 / X  .
 Jensen Surrogates.
 Jensen X  X  inequality provides a natural mechanism to obtain surrogates for convex functions. Following the presentation of Lange et al. ( 2000 ), we consider a con-vex function f : R 7 X  R , a vector x in R p , and define  X  f : R p  X  R as  X  f (  X  ) , f ( x  X   X  ) for all  X  . Let w be a weight vector in R p + such that k w k 1 = 1 and w i 6 = 0 whenever x i 6 =0. Then, we define for any  X  in R p When f is differentiable with an L -Lipschitz gradient, and w i , | x i |  X  / k x k  X   X  , then g is in S L  X  (  X   X  L  X  = L k x k 2  X  k x k 0 for  X  = 0;  X  L  X  = L k x k  X  k x k 1 for  X  = 1;  X  L  X  = L k x k 2 2 for  X  = 2.
 As far as we know, the convergence rates we provide when using such surrogates are new. We also note that Jensen surrogates have been successfully used in ma-chine learning. For instance, Della Pietra et al. ( 2001 ) interpret boosting procedures under this point of view through the concept of auxiliary functions .
 Quadratic Surrogates.
 When f is twice differentiable and admits a matrix H such that H  X  X  X  2 f is always positive definite, the fol-lowing function is a first-order majorant surrogate: g :  X  7 X  f (  X  ) +  X  f (  X  )  X  (  X   X   X  ) + The Lipschitz constant of  X  ( g  X  f ) is the largest eigen-value of H  X  X  X  2 f (  X  ) over  X . Such surrogates appear frequently in the statistics and machine learning liter-ature ( B  X ohning &amp; Lindsay , 1988 ; Khan et al. , 2010 ). We have shown that there are many rules to build first-order surrogates. Choosing one instead of another mainly depends on how easy it is to build the surrogate (do we need to estimate an a priori unknown Lipschitz constant?), and on how cheaply it can be minimized. In this section, we introduce a block coordinate descent extension of Algorithm 1 under the assumptions that  X   X  is separable X  X hat is, it can be written as a  X  the surrogates g n are separable into k components: We present a randomized procedure in Algorithm 2 fol-lowing Tseng &amp; Yun ( 2009 ); Shalev-Shwartz &amp; Tewari Algorithm 2 Block Coordinate Descent Scheme input  X  0 = (  X  1 0 ,..., X  k 0 )  X   X  = ( X  1  X  ...  X   X  k ); N . 1: for n = 1 ,...,N do 2: Choose a separable surrogate g n of f near  X  n  X  1 ; 3: Randomly pick up one block  X   X  n and update  X   X   X  n n : 4: end for output  X  N = (  X  1 N ,..., X  k N ) (final estimate); As before, we first study the convergence for non-convex problems. The next proposition shows that similar guarantees as for Algorithm 1 can be obtained. Proposition 3.1 ( Non-Convex Analysis).
 Assume that the functions g n are majorant surrogates in S L ( f, X  n  X  1 ) . Assume also that  X  0 is the minimizer of a majorant surrogate function in S L ( f, X   X  1 ) for some  X   X  1 in  X  . Then, the conclusions of Proposi-tion 2.1 hold with probability one.
 Under convexity assumptions on f , the next two propositions give us expected convergence rates. Proposition 3.2 ( Convex Analysis for S L ( f, X  ) ). Make the same assumptions as in Proposition 2.2 and define  X  , 1 k . When the surrogate functions g n in Algorithm 2 are majorant and in S L ( f, X  n  X  1 ) , the se-quence ( f (  X  n )) n  X  0 almost surely converges to f  X  and f (  X  0 )  X  f  X  &gt; LR 2 and n 0 , 0 otherwise. Assume now that f is  X  -strongly convex. Then, we have instead an expected linear convergence rate where  X  , L  X  if  X  &gt; 2 L or  X  , 1  X   X  4 L otherwise. Proposition 3.3 ( Convex Analysis for S L, X  ( f, X  ) ). Assume that f is convex. Define  X  , 1 k . Choose ma-jorant surrogates g n in S L, X  ( f, X  n  X  1 ) with  X   X  L , then ( f (  X  n )) n  X  0 almost surely converges to f  X  and we have sume now that f is  X  -strongly convex, then we have an expected linear convergence rate  X   X   X  The quantity  X  = 1 /k represents the probability for a block to be updated during an iteration. Note that updating all blocks (  X  =1) gives the same results as in Section 2 . Linear convergence for strongly convex ob-jectives with block coordinate descent is classical since the works of Tseng &amp; Yun ( 2009 ); Nesterov ( 2012 ). Results of the same nature have also been obtained by Richt  X arik &amp; Tak  X a X c ( 2012 ) for composite functions. In this section, we show how to use surrogates to gener-alize the Frank-Wolfe method, an old convex optimiza-tion technique that has regained some popularity in machine learning ( Zhang , 2003 ; Harchaoui et al. , 2013 ; Hazan &amp; Kale , 2012 ; Zhang et al. , 2012 ). We present this approach in Algorithm 3 .
 Algorithm 3 Frank-Wolfe Scheme input  X  0  X   X ; N (number of iterations). 1: for n = 1 ,...,N do 2: Let g n be a majorant surrogate in S L,L ( f, X  n  X  1 ). 3: Compute a search direction: 4: Line search:  X   X  , arg min 5: Update solution:  X  n ,  X   X   X  n + (1  X   X   X  )  X  n  X  1 . 6: end for output  X  N (final estimate); When f is smooth and the  X  X radient Lipschitz based surrogates X  from Section 2.2 are used, Algorithm 3 be-comes the classical Frank-Wolfe method. 4 Our point of view is however more general since it allows for ex-ample to use  X  X roximal gradient surrogates X . The next proposition gives a convergence rate. Proposition 4.1 ( Convex Analysis).
 Assume that f is convex and that  X  is bounded. Call R , max  X  the sequence ( f (  X  n )) n  X  0 provided by Algorithm 3 con-verges to the minimum f  X  of f over  X  and Other extensions of Algorithm 3 can also easily be designed by using our framework. We present for instance in the supplemental material a randomized block Frank-Wolfe algorithm, revisiting the recent work of Lacoste-Julien et al. ( 2013 ). A popular scheme for convex optimization is the ac-celerated proximal gradient method ( Nesterov , 2007 ; Beck &amp; Teboulle , 2009 ). By using surrogate functions, we exploit similar ideas in Algorithm 4 . When us-ing the  X  X ipschitz gradient surrogates X  of Section 2.2 , Algorithm 4 is exactly the scheme 2.2.19 of Nesterov ( 2004 ). When using the  X  X roximal gradient surrogate X  and when  X  = 0, it is equivalent to the FISTA method of Beck &amp; Teboulle ( 2009 ). Algorithm 4 consists of it-eratively minimizing a surrogate computed at a point  X  n  X  1 extrapolated from  X  n  X  1 and  X  n  X  2 . It results in better convergence rates, as shown in the next proposi-tion by adapting a proof technique of Nesterov ( 2004 ). Algorithm 4 Accelerated Scheme input  X  0  X   X ; N ;  X  (strong convexity parameter); 1: Initialization:  X  0 ,  X  0 ; a 0 = 1; 2: for n = 1 ,...,N do 4: Update solution:  X  n , arg min  X   X   X  g n (  X  ); 5: Compute a n  X  0 such that: 7: end for output  X  N (final estimate); Proposition 5.1 ( Convex Analysis).
 Assume that f is convex. When  X  = 0 , the sequence (  X  n ) n  X  0 provided by Algorithm 4 satisfies for all n  X  1 , When f is  X  -strongly convex, we have instead a linear convergence rate: for n  X  1 , This section is devoted to objective functions f that split into many components: The most classical method exploiting such a structure when f is smooth is probably the stochastic gradient descent (SGD) and its variants (see Bottou , 2010 ). It consists of drawing at iteration n an index  X  t n and up-dating the solution as  X  n  X   X  n  X  1  X   X  n  X  f  X  t n (  X  n  X  1 scalar  X  n . Another popular algorithm is the stochastic mirror descent (see Juditsky &amp; Nemirovski , 2011 ) for general non-smooth convex problems, a setting we do not consider in this paper since non-smooth functions do not always admit first-order surrogates.
 Recently, it was shown by Shalev-Schwartz &amp; Zhang ( 2012 ) and Le Roux et al. ( 2012 ) that linear conver-gence rates could be obtained for strongly convex func-tions f t . The SAG algorithm of Le Roux et al. ( 2012 ) for smooth unconstrained optimization is an approx-imate gradient descent strategy, where an estimate of  X  f is incrementally updated at each iteration. The work of Shalev-Schwartz &amp; Zhang ( 2012 ) for compos-ite optimization is a dual coordinate ascent method called SDCA which performs incremental updates in the primal ( 2 ). Unlike SGD, both SAG and SDCA require storing information about past iterates. In a different context, incremental EM algorithms have been proposed by Neal &amp; Hinton ( 1998 ), where surro-gates of a log-likelihood are incrementally updated. By using similar ideas, we present in Algorithm 5 a scheme for solving ( 2 ), which we call MISO. In the next propo-sitions, we study its convergence properties.
 Algorithm 5 Incremental Scheme MISO input  X  0  X   X ; N (number of iterations). 1: Choose surrogates g t 0 of f t near  X  0 for all t ; 2: for n = 1 ,...,N do 3: Randomly pick up one index  X  t n and choose a 4: Update solution:  X  n  X  arg min 5: end for output  X  N (final estimate); Proposition 6.1 ( Non-Convex Analysis).
 Assume that the surrogates g  X  t n n from Algorithm 5 are majorant and are in S L ( f  X  t n , X  n  X  1 ) . Then, the conclu-sions of Proposition 2.1 hold with probability one. Proposition 6.2 ( Convex Analysis).
 Assume that f is convex. Define f  X  , min  X   X   X  f (  X  ) and  X  , 1 T . When the surrogates g t n in Algorithm 5 are majorant and in S L, X  ( f t , X  n  X  1 ) with  X   X  L , we have Assume now that f is  X  -strongly convex. For all n  X  1 ,  X   X   X  Interestingly, the proof and the convergence rates of Proposition 6.2 are similar to those of the block co-ordinate scheme. For both schemes, the current iter-ate  X  n can be shown to be the minimizer of an ap-proximate surrogate function which splits into differ-ent parts. Each iteration randomly picks up one part, and updates it. Like SAG or SDCA, we obtain lin-ear convergence for strongly convex functions f , even though the upper bounds obtained for SAG and SDCA are better than ours.
 It is also worth noticing that for smooth unconstrained problems, MISO and SAG yield different, but related, update rules. Assume for instance that  X  X ipschitz gra-dient surrogates X  are used. At iteration n of MISO, The update rule of MISO can be shown to be  X  n  X  The next section complements the theoretical analysis of the scheme MISO by numerical experiments and practical implementation heuristics. In this section, we show that MISO is efficient for solv-ing large-scale machine learning problems. 7.1. Experimental Setting We consider  X  2 -and  X  1 -logistic regression without in-tercept, and denote by m the number of samples and by p the number of features. The corresponding opti-mization problem can be written where the regularizer  X  is either the  X  1 -or squared  X  -norm. The y t  X  X  are in { X  1 , +1 } and the x t  X  X  are vectors in R p with unit  X  2 -norm. We use four classical datasets described in the following table: name m p storage size (GB) alpha 250 000 500 dense 1 rcv1 781 265 47 152 sparse 0 . 95 covtype 581 012 54 dense 0 . 11 ocr 2 500 000 1 155 dense 23 . 1 Three datasets, alpha , rcv1 and ocr were obtained from the 2008 Pascal large scale learning challenge. 5 The dataset covtype is available from the LIBSVM web-site. 6 We have chosen to test several software pack-ages including LIBLINEAR 1.93 ( Fan et al. , 2008 ), the ASGD and SGD implementations of L. Bottou (ver-sion 2) 7 , an implementation of SAG kindly provided to us by the authors of Le Roux et al. ( 2012 ), the FISTA method of Beck &amp; Teboulle ( 2009 ) implemented in the SPAMS toolbox 8 , and SHOTGUN ( Bradley et al. , 2011 ). All these softwares are coded in C++ and were compiled using gcc . Experiments were run on a sin-gle core of a 2.00GHz Intel Xeon CPU E5-2650 using 64GB of RAM, and all computations were done in dou-ble precision. All the timings reported do not include data loading into memory. Note that we could not run the softwares SPAMS, LIBLINEAR and SHOTGUN on the dataset ocr because of index overflow issues. 7.2. On Implementing MISO The objective function ( 3 ) splits into m components ural to consider the incremental scheme of Section 6 together with the proximal gradient surrogates of Sec-tion 2.2 . Concretely, we build at iteration n of MISO a surrogate g  X  t n n of f  X  t n as follows: g  X  t n n :  X  7 X  l  X  l is the logistic function  X  7 X  log(1 + e  X  y t x t  X   X  ). After removing the dependency over n to simplify the notation, all the surrogates can be rewritten as g t :  X  7 X  a t + z t  X   X  + L 2 k  X  k 2 2 +  X  X  (  X  ), where a stant and z t is a vector in R p . Therefore, all surrogates can be  X  X ummarized X  by the pair ( a t , z t ), quantities which we keep into memory during the optimization. Then, finding the estimate  X  n amounts to minimiz-ing a function of the form  X  7 X   X z  X  n  X  + L 2 k  X  k 2 2 where  X z n is the average value of the quantities z t at iteration n . It is then easy to see that obtaining  X z n +1 from  X z n can be done in O ( p ) operations with the fol-One issue is that building the surrogates g t requires choosing some constant L . An upper bound on the Lipschitz constants of the gradients  X  l t could be used here. However, we have observed that significantly faster convergence could be achieved by using a smaller value, probably because a local Lipschitz constant may be better adapted than a global one. By studying the proof of Proposition 6.2 , we notice indeed that our con-vergence rates can be obtained without majorant sur-rogates, when we simply have: E [ f t (  X  n )]  X  E [ g t n for all t and n . This motivates the following heuristics: of the data to select a constant L  X  yielding the smallest decrease of the objective, and set L = L  X   X  ; After each pass over the data, if the rate of satisfied inequalities drops below 50%, double the value of L . Following these strategies, we have implemented the scheme MISO in C++. The resulting software package will be publicly released with an open source license. 7.3.  X  2 -Regularized Logistic Regression We compare LIBLINEAR, FISTA, SAG, ASGD, SGD, MISO1, MISO2 and MISO2 with T = 1000 blocks (grouping some observations into minibatches). LIB-LINEAR was run using the option -s 0 -e 0.000001 . The implementation of SAG includes a heuristic line search in the same spirit as MISO2, introduced by Le Roux et al. ( 2012 ). Every method was stopped after 50 passes over the data. We considered three regularization regimes, high (  X  = 10  X  3 ), medium (  X  = 10  X  5 ) and low (  X  =10  X  7 ). We present in Figure 1 the values of the objective function during the optimiza-tion for the regime medium , both in terms of passes over the data and training time. The regimes low and high are provided as supplemental material only. Note that to reduce the memory load, we used a minibatch strategy for the dataset rcv1 with T = 10 000 blocks. Overall, there is no clear winner from this experi-ment, and the preference for an algorithm depends on the dataset, the required precision, or the regulariza-tion level. The best methods seem to be consistently MISO, ASGD and SAG and the slowest one FISTA.
 Note that this apparently mixed result is a signifi-cant achievement. We have indeed focused on state-of-the-art solvers, which already significantly outperform a large number of other baselines (see Bottou , 2010 ; Fan et al. , 2008 ; Le Roux et al. , 2012 ).
 7.4.  X  1 -Regularized Logistic Regression Since SAG, SGD and ASGD cannot deal with  X  1 -regularization, we compare here LIBLINEAR, FISTA, SHOTGUN and MISO. We use for LIBLINEAR the option -s 6 -e 0.000001 . We proceed as in Section 7.3 , considering three regularization regimes yielding dif-ferent sparsity levels. We report the results for one of them in Figure 2 and provide the rest as supplemental material. In this experiment, our method outperforms other competitors, except LIBLINEAR on the dataset rcv1 when a high precision is required (and the regu-larization is low). We also remark that a low precision solution is often achieved quickly using the minibatch scheme (MISO2 b1000), but this strategy is outper-formed by MISO1 and MISO2 for high precisions. In this paper, we have introduced a flexible optimiza-tion framework based on the computation of  X  X urro-gate functions X . We have revisited numerous schemes and discovered new ones. For each of them, we have studied convergence guarantees for non-convex prob-lems and convergence rates for convex ones. Our methodology led us in particular to the design of an in-cremental algorithm, which has theoretical properties and empirical performance matching state-of-the-art solvers for large-scale machine learning problems. In the future, we are planning to study fully stochas-tic or memoryless variants of our framework. As in the incremental setting, it consists of drawing a sin-gle training point at each iteration, but the algorithm does not keep track of all past information. This is es-sentially a strategy followed by Neal &amp; Hinton ( 1998 ) and Mairal et al. ( 2010 ) in the respective contexts of EM and sparse coding algorithms. This would be par-ticularly important for processing sparse datasets with a large number of features, where storing (dense) in-formation about the past surrogates is cumbersome. JM would like to thank Zaid Harchaoui, Francis Bach, Simon Lacoste-Julien, Mark Schmidt, Martin Jaggi, and Bin Yu for fruitful discussions. This work was sup-ported by Quaero, (funded by OSEO, the French state agency for innovation), by the Gargantua project (pro-gram Mastodons -CNRS), and by the Center for Sci-ence of Information (CSoI), an NSF Science and Tech-nology Center, under grant agreement CCF-0939370.
