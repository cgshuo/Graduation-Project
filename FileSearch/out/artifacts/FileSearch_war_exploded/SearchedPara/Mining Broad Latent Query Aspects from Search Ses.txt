 Search queries are typically very short, which means they are of-ten underspecified or have senses that the user did not think of. A broad latent query aspect is a set of keywords that succinctly repre-sents one particular sense, or one particular information need, that can aid users in reformulating such queries. We extract such broad latent aspects from query reformulations found in historical search session logs. We propose a framework under which the problem of extracting such broad latent aspects reduces to that of optimizing a formal objective function under constraints on the total number of aspects the system can store, and the number of aspects that can be shown in response to any given query. We present algorithms to find a good set of aspects, and also to pick the best k aspects matching any query. Empirical results on real-world search engine logs show significant gains over a strong baseline that uses single-keyword re-formulations: a gain of 14% and 23% in terms of human-judged accuracy and click-through data respectively, and around 20% in terms of consistency among aspects predicted for  X  X imilar X  queries. This demonstrates both the importance of broad query aspects, and the efficacy of our algorithms for extracting them.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Search process, Query formulation General Terms: Algorithms Keywords: Latent user intents, query aspects, search sessions.
Search engines have become the primary mode of discovering and accessing content for a large fraction of web users. However, even though they use search engines for critical information access tasks, users are remarkably laconic in describing their information needs [15, 19], resulting in vague queries. This is due to several reasons. (1) Users often use search engines for performing research on unfamiliar topics. They might skip important terms in search queries simply because they are unacquainted with the topic-specific vocabulary. (2) They may be aware of the terms but believe them to be redundant; they may be unaware of the multiple ambiguous senses of their incomplete queries. (3) Search engines themselves reinforce this behavior by not properly taking into account the extra information when the users do provide long descriptive queries.
Discovering the intent latent in vague user queries is an active area of research. Methods proposed in prior art can mainly be grouped into two paradigms: query expansion and query suggestions . Query expansion approaches implicitly add extra terms to the search query [3, 12]. Such terms could include synonyms of query words, stemmed words, corrections of spelling mistakes, and so on. A special class of methods employ Pseudo-Relevance Feedback approaches [5, 22] to expand queries under the assumption that the top-k documents retrieved for the original query are relevant. In many cases, how-ever, queries have several possible intents. For example, the user intent behind a query like  X  X anon EOS-40D Digital SLR X  could be to find reviews and ratings for the camera, or shopping sites to buy the camera, or fan forums that discuss the camera, or simply to find information about the camera. As there is no single dominant sense of the query, no implicit query expansion can capture the true user intent with any confidence. In these cases, the best course of action is to help users explicitly specify the exact sense they had intended for their query; query expansion methods are not applicable to this problem.

The second type of methods, query suggestions, encourage users to explicitly specify the hidden intent behind their queries by of-fering them a list of semantically related queries. Users can then pick one of them to further refine their query. Examples include the  X  X lso try X  suggestions in Yahoo! and  X  X elated searches X  in Live Search. Yahoo! also provides a search interface called  X  X earch As-sist X  that lists concepts related to the user X  X  query, which the user can then use to restrict her search. A similar  X  X efinement X  functional-ity for queries in specific domains like Health is provided by the Google search engine. In addition to these examples, several meth-ods have been proposed in academic venues to find semantically related queries [8, 11, 17, 21]. A universal characteristic, however, of these methods is that the suggestions they offer are narrow and specific to the original query. As we explain next, this is not useful for finding broad aspects as is the goal of this paper.
 Broad Latent Aspects of Search Queries. In our analysis of the Yahoo! search query logs we noticed the existence of many latent in-tents that are very broad, and applicable to many classes of queries. We call these intents Broad Latent Aspects of queries. An exam-ple of this is  X  X eviews and Ratings. X  Users often query for product names but neglect to mention the term  X  X eview, X  even when what they really want are reviews for the product. The  X  X eviews and Rat-ings X  aspect is broad as well, since it is a latent aspect in queries for products, movies, restaurants, and many others. Other examples in-clude  X  X ictures, X  when the query mentions a well-known landmark or person, and  X  X ownload, X  when the query only names a software title or a famous photograph or painting, and so on.
Broad latent aspects of queries are characterized by the follow-ing two properties. First, these intents are applicable to a broad set of queries. As mentioned above, the  X  X eviews and Ratings X  aspect is useful for many types of product and service queries. Sec-ond, users frequently do not specify terms indicating such aspects in their queries. For instance, while many users do specify the term  X  X eview X  in their queries, a significant fraction only query for the product or service name even when they explicitly intended to find reviews. Together, these imply that a small set of such aspects, once discovered, can be used in inferring user latent intent for many in-complete queries. Since these aspects are global and reflect users X  major latent intents, they can potentially be used to design special-ized search services such as vertical searches.

In this paper, we propose methods to mine a list of broad latent aspects of search queries from query logs and present an approach to decide which broad aspects apply to future, possibly unseen, search queries. These query aspects are used to aid users in clarifying their information needs and providing the best search results.
 Query Reformulations in Search Sessions. We extract broad la-tent aspects in queries by mining search engine user sessions. In particular, we model the query reformulations within sessions to dis-cover these aspects.

User search sessions contain a lot of semantic information. As an example of a query reformulation in a user session, consider a user who wants to read reviews for the  X  X anon EOS-40D Digital SLR X  but only provides the model name as the query. When the search engine responds with a results page full of links to where the cam-era can be bought, the user reformulates the query by inserting the term  X  X eviews X  at the end. The user then finds the desired result on the subsequent search results page and clicks on it. For the purposes of our work,  X  X anon EOS-40D Digital SLR X  is referred to as the original query and  X  X eviews X  as the query qualifier . Such a refor-mulation can be used to discover the user X  X  latent intent. We thus extract such instances of query reformulations from user sessions, and aggregate the query qualifiers to construct broad query aspects. Broad Latent Aspects vs. Query Suggestions. The extraction of broad aspects from query logs, and their use in query refinement, has several advantages over standard query suggestion methods pre-sented in the literature. The first advantage has to do with the dis-covery and use of broad aspects and query suggestions. The broad nature of the query aspects ensures that enough data is available to reliably construct these aspects and predict when they apply to user queries. This is in contrast to query suggestions that are often applicable to specific queries and hence learned from significantly lesser amount of data. The availability of more data for analysis also implies that we can avoid presenting the user with redundant query refinement options, as is often the case with query sugges-tions. Finally, since by definition there are fewer broad aspects of queries than query suggestions, we can learn more expensive model for them and also maintain them better (the maintenance at commer-cial search engines might involve costs of manual effort).
The second, more central, advantage is subtle, and concerns the way users navigate the search results page. It has been shown in user eye-tracking studies 1 as well as by modeling user clicking be-havior [10] that users scan search result pages extremely fast and do not make complete determination of the relevance of results before clicking. Users get used to repetitive features in the search results page and use them to make clicking decisions. For example, the presence of bold words in the title of the result indicates to users that the title matched the query very closely while the indented search re-such as www.checkit.nl/pdf/eyetracking_ research.pdf sult indicates to the user that this search result is somehow related to the preceding one. When users are exposed to query suggestions, which by definition are specialized to the current query, they have to carefully read each suggested queries in order to decide whether to click on them. Since the users scan result pages very fast, they often skip the suggested queries as irrelevant content. By using a limited number of broad query aspects as options for refinement we seek to get the user accustomed to them. The users will then need less attention to interpret the aspects; indeed, they may even come to expect the aspects from the search engine  X  e.g., the  X  X eviews and Ratings X  aspect, when they search for a product name. Our Contributions. Our primary contributions are fourfold. First, we define broad query aspects and provide a clear problem formula-tion based on optimizing a well-defined objective function. The for-mulation directly models the search activity of users, specifically, the query reformulations. To the best of our knowledge, ours is the first attempt to use query reformulations for this purpose. Sec-ond, we propose algorithms that extract these broad query aspects by mining query reformulations in user session logs. Our proposed algorithms seek to generate semantically coherent aspects which can optimally cover the reformulations of the original queries. Third, we provide an optimal algorithm to pick the k most relevant aspects for any query, given a predefined set of latent query aspects generated by any algorithm. This algorithm might be of independent inter-est as well. Finally, we empirically demonstrate the accuracy of our proposed method on a large real-world corpus of search logs: a gain of 14% and 23% in terms of human-judged accuracy and click-through data respectively, and around 20% in terms of consistency among aspects predicted for  X  X imilar X  queries.
Aiding web search users in finding relevant web pages is an active area of research. Prior work in this field can broadly be categorized into the following groups.
 Query Suggestions. There is considerable literature on automat-ically suggesting queries that are related to the user X  X  information need. Jones et al. [17] learn a supervised model to select a sugges-tion for a query from a list of candidates by using features that de-pend on the query and the candidate. Cucerzan and Brill [11] present an approach that ranks candidates by the conditional probability of seeing them in the same session as the current user query. Vlachos et al. [21] and Chien and Immorlica [8] operate on the assumption that semantically related queries have similar temporal behavior of query occurrences. There are also many query expansion techniques that use pseudo-relevance feedback [5, 22] and click-through behav-ior [3, 12].
 Query Suggestions via Context. There is some work on personal-izing the query suggestions to the searching history of the user. Cao et al. [6] look at the past sequence of queries issued by the user while making a query recommendation. In a similar work Boldi et al. [4] solve an ATSP problem to obtain the sessionization of user history and then use queries from the same session to bias query sugges-tions. Chirita et al. [9] expand a user X  X  query with terms collected in user X  X  profile to bias search results.
 Concept Based Query Suggestions. The above methods all find queries related to the current user information need, and this can be error prone if the information need is rare or novel. A possible so-lution is to aggregate queries into concepts which are then used to map related queries to the user X  X  current information need. Fonseca et al. [13] use association rule mining to fetch past queries related to the current query, which are then clustered via locating cycles in a specially constructed query-relations graph. These clusters are then shown to the user, who picks the cluster that is to be used to expand the current query. Cao et al. [6] recommend queries by first clus-tering them into concepts in an offline process. There has also been significant work on assigning labels to query clusters; for example, Pasca and Van Durme [18] use correlations in query logs and web documents to create and label query clusters. Finally, Fuxman et al. [14] use a pre-defined taxonomy of concepts to learn a mapping from query to concept. This mapping is then used to recommend related queries in the context of sponsored search.
 Differences in Our Work. As mentioned in Section 1, we discover broad latent query aspects from reformulation activity in user ses-sions. Unlike query expansion [3, 12], these query aspects are de-signed for situations when there is no one dominant latent intent to a query. In contrast to query suggestions [8, 11, 17, 21], query aspects are global , applicable to many classes of queries, and can also be used to design specialized search services such as vertical searches. These aspects must in some sense be orthogonal to the topical nature of queries. Moreover, in our approach we seek to find aspects that users often neglect to include in their queries. We accomplish this by explicitly modeling the query reformulations in search sessions. This is different from all the works mentioned above.
In the Introduction, we motivated the use of query reformulations in search sessions for extracting broad latent query aspects. In this section, we first present a concrete formulation of our problem. The end result is an encoding of our problem in a precise objective func-tion. We then give algorithms that directly optimize this objective function to discover broad latent aspects and to pick appropriate as-pects in response to future queries.
Let A denote a set of broad latent query aspects. Each query aspect a i is in turn a set of query qualifiers  X  terms that are added to an original query during reformulations. Note that A does not have to cover the set of all query qualifiers exhaustively or mutually exclusively; A need not be a partitioning. However, primarily for the purposes of efficiency as shown later, we assume in our work that query aspects are disjoint.

The problem setting can now be stated compactly as follows. Our system discovers and maintains a set A of N query aspects. On receiving any original query q at run-time, it select k out of the N aspects and presents them to the user along with search results for q . We want to (a)  X  X over X  the set ` ( q ) of expected qualifiers of q as precisely as possible; the k query aspects must include most of the qualifiers in ` ( q ) but also exclude most extraneous qualifiers, and (b)  X  X over X  as many queries as possible from the search engine workload.

The restriction on N query aspects is due to costs associated with discovering and maintaining query aspects in a large search system, while the restriction on k query aspects derives from constraints on the real-estate on the search results page as well as user X  X  attention. Its easy to see that these restrictions result in a trade-off between the two requirements of our approach: to cover as many as queries as possible, as precisely as possible.

This trade-off needs to be encapsulated in a single reward func-tion, which will then let us compare different solutions (each solu-tion being a set A of N query aspects). Following the literature on information retrieval, natural choices for such a reward function are the precision and recall of the solution. The precision of a query aspect a i is the fraction of its qualifiers that it shares with ` ( q ) : rewards query aspects with few extraneous qualifiers, while recall rewards aspects which cover most qualifiers of the original query. Together, these are exactly our two desiderata. A standard way to combine them is via the F-measure, which is the harmonic mean of precision and recall [2]: The F-measure ranges from 0 to 1, with 0 indicating no similarity and 1 representing identical sets.

Now, we can define the goodness of a solution A as the F-measure between the set ` ( q ) of qualifiers of a query q and the union of the k query aspects picked for it, summed over all queries in the query workload Q and weighted by their frequencies n ( q ) :
This reward function represents a trade-off between two advan-tageous properties: it encourages  X  X overing X  as many qualifiers of ` ( q ) as possible, while penalizing for both qualifiers that are not covered, and overly broad query aspects. Note that the latter is a problem with objective functions based on several other common similarity measures: e.g., neither precision-at-k nor cosine similar-ity penalize for uncovered qualifiers. The two properties cannot be simultaneously satisfied to the maximum possible extent because of constraints on both the number of total aspects N and the number of aspects k that can be used to  X  X over X  any one query, making the trade-off necessary.

While the standard F-measure indeed has several desirable prop-erties, it does not take into account the relative frequencies of ele-ments in the sets being compared. This is important in our setting: the query aspects presented in response to a query should preferably be biased towards the most frequent reformulations of that query. For example, suppose an original query, say  X  X ariah carey X  , has three qualifiers: pictures (weight 100 ), wallpaper (weight 10 ), and photoshoot (weight 5 ). This tells us that a user querying for  X  X ariah carey X  is 10 times more likely to be looking for simple pictures than for desktop wallpapers. Then a query aspect consisting of pictures , wallpaper , and photoshoot , each weighted equally, does not provide a user with an appropriate choice for reformulation. A user inter-ested in, say, pictures will only have a 1/3 chance of finding this as-pect relevant. Ideally, the qualifiers in an aspect should be weighted with exactly the same relative weights, i.e., 100 : 10 : 5 . However, the F-measure in Equation 1 will score both the equal-weight and biased-weight versions of this aspect identically, considering both to perfectly cover the original query. Hence, it is crucial for us to use a similarity function that properly handles weights, and this is what we discuss next.
 Weighted F-measure. We must first extend the notation to the case of weights. Slightly abusing notation, we will use ` ( q ) to represent not only the set of qualifiers of query q but also the vector of weights of those qualifiers. Weights are non-zero only for qualifiers that appear in the historical data with q . Similarly, let a i represent the weight vector for a query aspect. We desire a similarity function between ` ( q ) and a i when both are weighted.

To achieve this, the trade-off represented by Equation 1 must now be recast in terms of weights. We accomplish this by positing that a suitable weighted similarity measure should possess the following two characteristics: (P1) An a i that has high weights exactly on the terms on which ` ( q ) also puts high weights should achieve a high similarity score, and (P2) if we take a qualifier that exists in ` ( q ) but not in a i , and add it to a i with a very small weight , the similarity should necessarily increase. Of course, if this new addition were to be weighted too heavily in a i , then the similarity score may either increase or decrease, depending on its relative weight in ` ( q ) .
Keeping these properties in mind, we propose extending the F-measure to the weighted case when a i and ` ( q ) are vectors. Thus, the set size computations of Equation 1 are replaced by dot products between vectors.

It is clear from Equation 3 that F ( ., . ) attains high values when items in both sets have equal weights, and low values when the weights diverge, thus satisfying property (P1) . It can be shown that (P2) holds as well, as long as a i is non-empty. The only remaining question is the setting of weights for the qualifiers in a Weighing qualifiers in ` ( q ) and a i . The weights on the two sets serve different purposes and this dictates useful ways of setting them. Our goal is to ensure that for a user issuing query q , the system sug-gests aspects that match what we expect to be reformulations for q . Hence, a natural value for the weight of a qualifier m in ` ( q ) is the number of times we expect to see m in a reformulation of q (we can estimate this from historical data); we call this the conditional frequency. On the other hand, when the system suggests an aspect a in response to a query q , the user X  X  perceived relevance of a depends on the qualifiers that together comprise a i . The weight of a qualifier l in a i should reflect the degree to which it dominates other qualifiers in a i . We estimate this weight as the global frequency of seeing l as a qualifier in any reformulations over the entire historical data; we call this the global frequency of l .

In this weighting scheme, for the F-measure to make sense, the weights on members of a i and ` ( q ) must be on the same scale. This is unlikely to be the case since global frequencies will tend to be much larger than conditional frequencies. An obvious way to ensure similar scales is to normalize the weights in a set to sum to 1 . This fix, however, runs afoul of property (P2) mentioned above. To illustrate this, let us consider the previous example  X  the query  X  X ariah carey X  , with set ` ( q ) of qualifiers being pictures (unnormalized weight 100 ), wallpaper (unnormalized weight 10 ), and photoshoot (unnormalized weight 5 ). Then a query aspect a consisting of pictures (unnormalized weight 10 ) and wallpaper (un-normalized weight 1 ) would score well since the highly weighted qualifiers of the original query are also highly weighted in the query aspect. However, if we now add photoshoot to our query aspect a then the normalization would reduce the relative weights for pic-tures and wallpaper , and by implication their contributions to the similarity score. This could lead to an overall decrease in the simi-larity between ` ( q ) and the new a i .

To bring the weights to the same scale while satisfying both (P1) and (P2) , we perform the following normalization: For each q , we scale all weights in ` ( q ) by a factor specific to q such that, after normalization, the squared 2-norm of ` ( q ) (i.e., k ` ( q ) k sum of squares of the global frequencies of the members of the set ` ( q ) . For example, if ` ( q ) has no overlaps with the qualifiers of any other query, then no scaling would be performed, which is intuitive since the conditional frequencies in this case are the global frequen-cies. Also, if there is a query aspect a i with exactly the same terms as ` ( q ) , then it would have the same frequencies as ` ( q ) as well, so the vectors a i and ` ( q ) would be identical and the similarity mea-Jaccard [20] 0 | X | | X  X  Y | | Y \ X |
Table 1: Similarity measures as special cases of Equation 6 sure F ( ` ( q ) , a i ) would achieve its highest possible value of 1 . It may easily be seen that with this normalization scheme, F ( ., . ) sat-isfies both desired properties.
 Finally, we can formally state our problem in its entirety:
The above problem formulation essentially involves two prob-lems: (1) find the best set of query aspects A , and (2) for any given query, pick the best k query aspects from the set A . Since the solu-tion to the first problem depends on our ability to solve the second, we present, in Section 3.2, an algorithm for the second problem and prove its optimality. This algorithm, combined with a clustering technique, then leads to our proposed solution for the first problem, which is discussed in Section 3.3.
Given a set A of query aspects, and a query q , how should we pick k aspects a 1 , . . . , a k  X  A so as to maximize the similarity measure F ` ` ( q ) ,  X  k i =1 a i  X  ? We present a general solution that can maximize any similarity function of the form where Y = { y 1 , . . . , y k } ( k fixed) is a set that must be picked from a universe Y of possible items,  X  X and  X  X are constants with  X 
X &gt; 0 , the function g X ( . ) is non-negative, and all functions and constants are indexed by an X that represents any known and rele-vant data. The connection to our problem of picking query aspects is clear: The universe Y corresponds to A , with each y i ing to some weighted query aspect. Similarly, X corresponds to the weighted ` ( q ) , which is known. When all the aspects are mutually exclusive, our F ( ., . ) objective function falls under this framework, along with several other similarity functions (Table 1). Thus, in ad-dition to being applicable to our particular problem, the solution to Equation 6 might be of independent interest as well.

The difficulty in solving this problem stems primarily from an important, and somewhat counter-intuitive, consequence of Equa-tion 6: The solution for large k need not be a superset of the so-lution for small k . Consider the following example. Let  X  and  X  X = 10 . Let Y = { y 1 , y 2 , y 3 } with the f X ( y mal Y  X  = { y 3 } , since this yields the objective value h (0 + 2) / (10 + 10) = 1 / 10 while h X ( { y 1 } ) = h X ( { y (0 + 1) / (10 + 1) = 1 / 11 &lt; h X ( { y 3 } ) . However, if k = 2 , the optimal Y  X  = { y 1 , y 2 } , since h X ( Y  X  ) = (0 + 1 + 1) / (10 + 1 + 1) = 1 / 6 , while the other solutions give smaller objective values: h
X ( { y 1 , y 3 } ) = h X ( { y 2 , y 3 } ) = (0+1+2) / (10+1+10) = 1 / 7 . Algorithm 1 Pick-k 3: while n &gt; 0 do 6: Y  X  Y  X  M 9: n  X  n  X  X  M | 10: end while 11: output: picked elements Y  X  X  This shows, among other things, that dynamic programming meth-ods would not work for this problem. The set cover problem has a similar flavor, but it requires all elements in the given set to be cov-ered and is thus NP-Complete, whereas our formulation allows for some elements to be left uncovered. This fact, along with the special structure of Equation 6, allows us to optimally solve our problem in polynomial time.
 Algorithm Description. Algorithm 1 presents the Pick-k algorithm for picking the set of elements Y from Y . Since X is known in any instance of the problem, we drop the subscript from all terms in the interests of clarity. Starting from an empty set Y = {  X  } , our proposed algorithm proceeds step by step, adding items from Y to Y in a greedy manner. However, the function that is maximized in each iteration keeps changing. The heart of the algorithm is in step 4, where the best items in each iteration are picked according to a function that depends on the number of elements n yet to be picked . Typically, there will only be one best element in M , though the general version presented in Algorithm 1 can handle ties as well. The time complexity of the algorithm is O ( k |Y| ) since there are k iterations, and all |Y| items must be considered in step-4 of each iteration. For Problem 1 , this becomes O ( kN ) , where N is the number of query aspects.
 Proof of Optimality. We need to prove that Algorithm 1 picks the optimal Y  X  = arg max Y h ( Y ) . Here, we provide a proof sketch, with the full proof being deferred to the appendix due to lack of space 2 .

For ease of exposition, assume that there are no ties in step 4 and each iteration adds only one element to Y (the proof can be easily extended to cover that case of multiple additions per iteration).
The algorithm maximizes Equation 6 by solving a sequence of sub-problems. Suppose that the set Y  X  ( k  X  n ) = { y  X  1 known to be a subset of Y  X  of size k  X  n . Now, for any set W  X  Y\ Y  X  ( k  X  n ) such that W has exactly n elements, we use Equation 6 to get: Let W  X  = arg max W h ( Y  X  ( k  X  n )  X  W ) . Now, we can define the following sub-problem: Problem 1 X : Given the 4-tuple ( Y  X  ( k  X  n ) ,  X ,  X , n ) , find any one ele-ment w  X   X  W  X  .

Next, we relate the solution of the sub-problem to the optimal solution Y  X  .

L EMMA 1. w  X   X  W  X   X  w  X   X  Y  X 
The appendix has been submitted as supplementary material, and is also available online at http://www.cs.cmu.edu/ ~deepay/AspectsAppendix.pdf
L EMMA 2. In Problem 1 0 , the 4-tuple ( Y  X  ( k  X  n ) ,  X ,  X , n ) and the 4-tuple equivalent.
 The optimality of Algorithm 1 can now be proved in two stages. First, assuming the correctness of our solution to each sub-problem, we show that the sequence of sub-problems generated by steps 6 -9 of our algorithm is correct. Second, we show that each sub-problem is solved correctly (step 4 ). In the following,  X  0 and  X  variables updated in steps 7 -8 of Algorithm 1.

T HEOREM 1. Assuming that step 4 of Algorithm 1 correctly solves the sub-problem ( {  X  } ,  X  0 ,  X  0 , n ) , the algorithm returns the optimal result Y  X  .

P ROOF SKETCH . The proof is by induction on the number of it-erations. Each iteration solves a particular sub-problem ( {  X  } ,  X  which is equivalent to ( Y,  X ,  X , n ) by Lemma 2. Here Y = Y is the current solution of step 6 . Then, by Lemma 1, each iteration yields a new element of Y  X  . The full proof is in the appendix.
T HEOREM 2. Step 4 of Algorithm 1 correctly solves the sub-problem ( {  X  } ,  X  0 ,  X  0 , n ) , i.e., the element s  X  iteration belongs to the optimal solution: s  X   X  W  X  .

P ROOF SKETCH . We prove that if s  X  does not belong to W then we can construct a new set Z which replaces an element of W  X  by s  X  such that h ( Z ) &gt; h ( W  X  ) , leading to a contradiction. The full proof is provided in the appendix.
In the previous section, we proposed a method to pick the best k query aspects for any given query. Now, we will present our ap-proach for constructing the set A of N query aspects.

Construction of the set A must adhere to the central goals and constraints of the problem formulated in Section 3.1. Recall that only N total aspects can be stored by the system, and only k aspects can be shown in response to a query. This leads us to construct a set A in which query qualifiers are grouped together into broad aspects so that only a few aspects can be used to cover a significant fraction of the query workload. On the other hand, the goal of providing pre-cise aspects to users prompts us to keep semantically distinct query qualifiers in separate groups. This trade-off between coverage and specificity of aspects leads us to propose a solution based on clus-tering of query qualifiers.
 Clustering query qualifiers. In our approach, each query aspect is modeled as a cluster of  X  X imilar X  query qualifiers. Two qualifiers are considered similar if the corresponding sets of original queries to which they tend to be added are similar. While any clustering al-gorithm and similarity metric can be used within our framework, in this paper we extend the well-known Star Clustering algorithm [1] for this purpose. We chose Star Clustering as it has been shown to be effective over many different datasets [1], and has several char-acteristics that help it optimize our objective function of Equation 4, albeit indirectly. Next we describe the extended algorithm, and then discuss the reasons behind its success on our problem.

Algorithm 2 presents our extended version of the star clustering algorithm. Each qualifier v in the historical data is attached to a vec-tor, whose dimensions are the set of original queries Q , and whose values are the frequencies with which v occurs as a qualifier for each q  X  Q . Pairwise cosine similarities are then computed be-tween qualifiers (typically only the most frequent 10 , 000 qualifiers in order to reduce computation), and only those pairs with similarity above a given threshold  X  are retained. This creates a graph among Algorithm 2 Modified Star Clustering 4: while n &lt; N and Left 6 = {  X  } do 6: spokes  X  X  i | ( hub , i )  X  X } 7: star  X  X  hub } X  spokes 8: A  X  A  X  X  star } 9: Left  X  Left \ star 10: n  X  n + 1 11: end while 12: output: set A of at most N query aspects the qualifiers. Now, a query aspect is created out of the qualifier that is most frequent (called hub) in the historical data, and all the qualifiers connected to it (spokes). These nodes are deleted and the process continues until N query aspects have been generated or the set of available qualifiers becomes empty. Note that the set of as-pects is mutually exclusive, but not necessarily exhaustive.
This is different from the original star clustering algorithm [1] in two ways. First, the aspects are now forced to be mutually exclu-sive, whereas in the original algorithm, only the hub nodes could not belong to multiple clusters. This was done so that the Pick-k algorithm could be used to efficiently pick the best query aspects for any query. Second, the original star clustering algorithm picks the highest degree nodes as hubs, whereas our approach uses the frequency of occurrence in the historical data. Hence, qualifiers picked as hubs are used for reformulating many different queries, or for very frequent queries, or both. This biases results towards the more important qualifiers; the query aspect stars built around such qualifiers should be frequently applicable to queries drawn from the query workload.

Empirical results in Section 4 demonstrate the strong performance of Algorithm 2. This might appear somewhat surprising, given the fact that the algorithm is not directly optimizing the objective func-tion (Eq. 4). However, careful consideration of the algorithm shows that it is indeed indirectly optimizing the objective. Two qualifiers ` and ` 2 are in the same query aspect a  X  A iff they have high cosine similarity to the hub qualifier ` h , which happens iff both ` and ` h share many original queries, as do ` 2 and ` h . But then, it is quite likely that ` 1 and ` 2 also share most of these queries. Hence, if a is among the best query aspects for such a query, then all three qualifiers ` 1 , ` 2 , and ` h contribute significantly to the numerator of Eq. 5, and hence to the full objective function. Thus, the value of the objective will tend to be high, even though it is not explicitly being maximized. We can contrast this with other clustering algorithms such as single-linkage clustering, where there is no reason for nodes at opposite ends of the same cluster to have occurred for the same original query  X  indeed, the reverse is to be expected. Thus, it is the special structure of the star clustering algorithm that leads to the good results observed empirically, and this is the reason why we chose to extend the star clustering algorithm for our purposes. Maximizing the objective directly. As noted above, star clustering optimizes the objective of Equation 4 only indirectly. We propose combining star clustering with a local search technique that directly increases the objective. Starting with the result of Algorithm 2, local search improves upon the solution in stages. In each stage, it finds a qualifier which can be moved from its current query aspect to a new one such that the total objective always increases. However, a straightforward application of this idea is infeasible: even a single local move could lead to a different set of top k query aspects being chosen for each query (the max in Equation 4), and recomputing these (using Algorithm 1) repeatedly would be very costly.
Our solution is based on the observation that the total objective does not need to be computed in each step: if the objective in-creases for a given assignment of k query aspects, it must necessar-ily increase with the optimal assignment of k aspects for each query. Hence, we first attempt to perform local moves without recomputing the best k aspects per query and only when such moves do not im-prove the objective do we perform the costly re-computations. This gives significant run-time savings. The space requirements are also low, as just k pieces of information need to be stored for each query (typically, k  X  5 ). Should this become too large, we can work with only a query sample drawn according to their frequencies in the his-torical query workload.
The questions we want to answer in this section are: (a) How does our approach perform in user studies and automated evaluation as compared to baselines? (b) How does each component of our overall approach affect the final accuracy? (c) How does the per-formance of our approach change under different parameter settings and constraints? To answer these questions we conduct experiments on logs from real-world search engine traffic.
We first describe our experimental design: datasets, evaluation measures, and baselines.
 Evaluation data set. The data was generated from U.S. search query logs of a commercial search engine collected over a period of two months. These logs were processed to extract query sessions , as follows: For each day in the data set, the queries were grouped by users 3 , ordered by time, and then broken up into sequences such that no two successive queries in any sequence were more than 10 minutes apart 4 . Each such sequence was defined to be an user ses-sion. While user session segmentation can be improved with more sophisticated algorithms (e.g., [16]), this simple low-cost heuristic performed adequately for our purposes.
 Given user sessions, we needed to extract query reformulations. A query reformulation was defined as the following sequence of user actions within a session: (1) user types an original query q , (2) user does not click on any result, (3) user types another query q that q is a prefix of q 0 , with a word/phrase r added as the suffix to q , and finally (4) user clicks on a result returned for the reformulated query. Now, r is defined as a qualifier for the original query q . Note that each session can yield multiple ( q, r ) pairs.

As a final step, the ( q, r ) pairs were split into a training set, de-rived from the first month of our search logs, and a testing set, de-rived from the first week of the second month X  X  query logs. For both sets, all occurrences of ( q, r ) pairs were aggregated to yield triples of the form ( q, r, count ) . All our experiments were run on this pro-cessed data.

From the logs in the training set, we selected the top 10,000 most frequent qualifiers (i.e., r ) and their triples for constructing the set A of N query aspects. This serves to reduce both the computa-tion complexity and the noise in the data, since qualifiers with low frequencies are too unreliable to be considered for broad query as-pects. We used the logs in the test set to automatically evaluate the
All user information such as user-IDs or IP addresses were re-moved.
The 10-minute interval was picked after some exploratory data analysis; our methods can use any time interval. performance of our approach. In order to get robust ground truth, we selected from the test set those original queries (i.e., q ) whose ac-cumulated counts of qualifiers are greater than 400; there are about 500 such cases. Each test case corresponds to a unique query, with its attendant qualifiers and frequencies.
 Competing approaches and baselines. We compared two config-urations of our approach against two baselines. In the M OD configuration we obtain aspects A using the Modified-Star cluster-ing procedure given in Algorithm 2 and employ Algorithm 1 to pick top k aspects from A for each test-set query. Our L OC S EARCH figuration is M OD S TAR plus the use of the local search procedure in constructing A .
 The first baseline, which we call O RG S TAR , employs the original Star clustering approach [1] to construct the final set of aspects A . In addition, the top k aspects to show to the user in response to a query are picked using Algorithm 1, as in M OD S TAR .

As an additional point of reference, we consider a solution that has one query qualifier per aspect (B ASELINE ) and uses Algorithm 1 to pick the optimal set of k aspects to show to a user. The set of aspects A is constructed by picking the N most frequent single keywords in the historical data. Despite the seeming simplicity of B
ASELINE , it can be shown that, for any query q whose qualifiers ` ( q ) do not overlap with those of other queries, Algorithm 1 sim-ply picks from among these N keywords the top k ones that occur most frequently in ` ( q ) . Also note that as N grows large, the query aspects picked for each query are more and more tuned to that par-ticular query and its qualifiers, yielding query suggestions [11] for that query in the limit. The fact that B ASELINE is only a special case of our framework demonstrates its flexibility: At one end, we get broad multi-keyword query aspects that are globally optimized over the entire query workload, and at the other end, we can get locally optimal single-keyword query suggestions.
 Evaluation criteria. We evaluated our approach using three differ-ent measures that measure orthogonal aspects of a user X  X  experience with our system.

First we performed a manual post-hoc evaluation of the accuracy of the predicted aspects. The ground truth for this evaluation was obtained in a user study. Two human judges were each presented with 250 queries and their top 3 predicted aspects, and asked to rate the  X  X oodness X  of the individual aspects. Each aspect in a set of k aspects was rated individually as  X  X ood X  if (a) the words and phrases belonging to the aspect were judged to form a coherent set, (b) the aspect was judged to be relevant to the query, and (c) the aspect was judged to be distinct from the other aspects. The output of our approach (L OC S EARCH ) and B ASELINE were judged this way, but the source of the predicted aspects was hidden from the judges. For uniformity of presentation, each aspect was given a name: For L S
EARCH , the qualifier with the highest global frequency in the clus-ter was used as the aspect name, while for B ASELINE , each aspect contained only one qualifier, which was used as the name. Using this data we define A CCURACY of an approach as the fraction of its predicted aspects that received  X  X ood X  ratings.

The second relevant question we evaluated was: Are the predicted aspects consistent across similar queries? For example, we would like all queries related to, say, places and locations, to yield the same set of aspects and thus provide a consistent and predictable user ex-perience. To measure this, we manually clustered the queries into 11 major semantic groups (plus one group for queries that did not fit into these categories) and computed the entropy of the predicted aspects in each group. Low E NTROPY in a query group implies that the set of predicted aspects is mostly the same, and is thus an indi-cator of consistency. Table 2 shows these 11 groups of queries.
A CCURACY and E NTROPY measure the precision and consis-tency of aspect predictions respectively. However, as pointed out in [11], it is difficult for a judge to accurately rate query sugges-tions without access to the context and task information inherent in a search session; the same is the case with judging predicted as-pects. As a third measure of performance, we used click-through information to further validate the results obtained via the above measures. Our automatically obtained test data is in the form of triples ( q, r, count ) , which reflect the number of times that users im-prove their query by appending r to query q . For any query q if we recommend aspect r with a high count, more users benefit from this recommendation. Thus we need to consider the actual count for a qualifier r in the evaluation. In Section 3.1 we showed how weighted F-measure captures the notion of usefulness of an aspect for a user query. In addition the F-measure in Equation 5 can take the actual count into account. Hence, in our experiments, we use the following two metrics: F@1 (weighted F-measure for the top recommended aspect) and F@3 (weighted F-measure for the top 3 recommended aspects). The normalization of qualifier sets ` ( q ) and aspects a i are described in detail in Section 3.1.
In this section, we compare the performance of different ways of recommending aspects. We first compare our proposed L OC S
EARCH against B ASELINE by manually evaluating the predictions of both. The results will demonstrate the superiority of the former in terms of both accuracy and non-redundancy of predicted aspects. Then we will present comparisons based on F@1 and F@3, which replicate these results, and also show that L OC S EARCH performs better M OD S TAR and O RG S TAR .

For all these methods, we set the number of aspects to N = 100 and use Algorithm 1 to optimally pick aspects for each query. For O RG S TAR and M OD S TAR , we set the similarity threshold to  X  = 0 . 25 . Post-hoc manual evaluation. The manual evaluation was performed as a user study described in detail in Section 4.1. Averaged over all the aspects predicted for all the queries judged, the A values for L OC S EARCH and B ASELINE were 0 . 804 and 0 . 702 re-spectively. This indicates that L OC S EARCH achieves a 14% im-provement over B ASELINE in terms of A CCURACY . The two judges agreed with each other 90 . 5% of the times indicating that accu-racy of L OC S EARCH approaches the upper-bound imposed by inter-rater agreement. Furthermore, for L OC S EARCH , at least 1 out of the 3 predicted aspects was deemed  X  X ood X  a remarkable 98 . 4% of queries; all 3 predicted aspects were considered  X  X ood X  52% of the time. While B ASELINE had a similar fraction of queries with at least one good prediction, only 28 . 6% of the queries elicited 3 good as-pects. This is because (a) B ASELINE can store only N = 100 query qualifiers, which is not enough, and (b) B ASELINE often predicts redundant aspects, such as pics and photos .

The A CCURACY values for L OC S EARCH and B ASELINE , bro-ken down across query groups, are presented in Table 2; significant differences between the approaches are in bold. As we can see, L
OC S EARCH significantly outperforms B ASELINE in most query groups. Place/Location and Male-Celebrities are the two groups of queries where B ASELINE outperforms L OC S EARCH . This is because these groups contained many queries that tended to have highly popular individual qualifiers that were not shared with other queries. For instance, while the location queries tended to have broad aspects such  X  X aps X  they also tended to have very popular specific qualifiers, like  X  X ricket X  for the query  X  X ndia X . Attempting to automatically discover when narrow query suggestions are useful in addition to broad aspects is a topic of further research. Table 2: Evaluation of L OC S EARCH versus B ASELINE : L OC S
EARCH has higher accuracy in predicting relevant aspects for a given query, and also better consistency (lower entropy) among aspects predicted for queries from the same query group. The last class is not used in entropy experiments since it is not a sin-gle semantic group.
 Consistency of aspect predictions. As mentioned in Section 4.1 we would like similar queries to provide a consistent and predictable user experience. Table 2 lists the E NTROPY values for L OC and B ASELINE across the different query groups. As is evident, L
OC S EARCH performs better than B ASELINE on almost all query groups. In the case of some groups such as Adult and Male-Celebrities , the E NTROPY values are reduced by huge amounts, 24% and 18% respectively. In fact, the only reason the reductions in entropy are not more dramatic is because the entropy of aspects suggested by B
ASELINE is kept artificially low by redundancies in the predicted aspects. This is also the reason why B ASELINE outperforms L S
EARCH for the Online-Products category: B ASELINE always pre-dicts both  X  X ownload X  and  X  X ownloads X  resulting in a low E value. An interesting observation is that low E NTROPY values do not always indicate high user satisfaction. This is shown by the Place/Location and Male-Celebrities groups of queries where L OC -S
EARCH outperforms B ASELINE in terms of E NTROPY but lags behind in terms of A CCURACY . Only E NTROPY and A CCURACY taken together give the complete picture.
 Evaluation on click-through data. The hardness of our problem imposes limitations on the range of observable F-measure values: A high F-measure can be achieved only if the qualifiers of a large set of queries can be covered by picking only up to k = 3 aspects from a set of only N = 100 total aspects. As this is unlikely to be true in general, we need to establish a scale according to which to judge the resulting F-measure values. The best way would be to compare against the performance of the optimal N aspects, but we have no way to find these. However, we can find the best possible N single-keyword aspects by picking the N query qualifiers that occur most frequently in the test set . Even when the ground truth on
Top word Other top words Example in aspect in aspect query pictures pics photos photo bmw download downloads freedownload downlod photoshop Table 3: Top keywords of the most frequent aspects, and exam-ple queries for which they are predicted. the test set is known to the aspect selection algorithm, the maximum possible F-measure was found to be only 0.262. In the following, all reported F-measure values are normalized relative to this maximum value, and we call this the Normalized F-measure .
 Figure 1 shows the results of B ASELINE , O RG S TAR , M OD and L OC S EARCH at F@1 and F@3. Just as in the manual evalua-tion, L OC S EARCH outperforms B ASELINE by a margin of 23% on F@1 and 11% on F@3. We also see that the clustering algorithm significantly affects result quality: Our proposed M OD S tering performs much better than O RG S TAR , because it takes the qualifier frequencies into account, which biases the aspects towards the more important and common qualifiers. For example, in M S
TAR , the three frequent keywords  X  X eview, X   X  X eviews X  and  X  X ating X  form a single aspect. However, O RG S TAR does not generate this aspect since this cluster is too small and none of the three keywords has a high enough degree. In fact, O RG S TAR performs worse than B
ASELINE , demonstrating that off-the-shelf clustering algorithms, applied directly to this problem, are unlikely to yield good results. Sample aspects. Table 3 shows the most important word, and other top words, of the top 6 aspects discovered by our method, along with example queries for they are picked. The aspects are clearly coher-ent: All keywords in an aspect are semantically similar to the top word. All synonyms, mis-spellings, and singular/plural versions of an aspect keyword also belong to the same aspect. In addition, the aspects are all distinctive, each aspect describing a well-defined con-cept that is different from those of the other aspects. These aspects are also what one would expect to be the most common reformula-tions of Web queries. This demonstrates how our algorithms achieve the right balance between query coverage and aspect specificity.
We now study the impact of different parameters on the perfor-mance of our algorithms. In particular, we consider the effects of the similarity threshold  X  used in M OD S TAR and L OC S EARCH the limit on the number of query aspects N .
 The similarity threshold  X  . The M OD S TAR algorithm (and conse-quently, L OC S EARCH as well) merges query qualifiers into an as-pect cluster using a similarity threshold parameter  X  . In Figure 2, Figure 2: The impact of similarity threshold parameter  X  on F@1 and F@3. When  X  = 1 , M OD S TAR reduces to B ASELINE Figure 3: The impact of number of aspect N on F@1 and F@3. The curves flatten out by N = 100 . we investigate the sensitivity of the F-measure to  X  . We also show the result of B ASELINE for comparison. As  X  increases, fewer qual-ifiers are considered to be similar, with each qualifier forming its own cluster in the limit of  X  = 1 . Thus,  X  = 1 is equivalent to B
ASELINE , as can be observed from the F-measure results. On the other hand, when  X  is too small, each aspect becomes large and thus less coherent, leading to low F-measure values. The best perfor-mance is obtained when we set  X  = 0 . 25 . We use this value in all other experiments.
 Number of Aspects N . We also study the impact on performance of the number of aspects N . Figure 3 shows the results of B LINE and M OD S TAR when we vary N from 25 to 200, for both F@1 and F@3. Clearly, when we enlarge the number of aspects N , all the methods are improved. This is because with increasing N , the set of aspects contains more query qualifiers and can thus yield good predictions for more test queries. In addition, the aspects themselves can be smaller and hence more coherent. The behavior of L OC S EARCH is similar, and is not shown in the interest of clarity. The plots flatten out by N = 100 , implying that a relatively small set of aspects can be sufficient to cover most web queries. This is because the frequency with which any given aspect is recommended in response to a test query is highly skewed: The  X  X ictures X  aspect is recommended for over 40% of the queries while there are 60 dif-ferent aspects that are recommended for less than 2% of the queries. Thus, a total of N = 100 aspects provide adequate performance, and this is the value we use in the other experiments.
A common problem with user queries is that they are underspec-ified, or that they have senses that the user did not think of. A broad latent query aspect is a set of keywords that succinctly represents one particular sense, or one particular information need, that can aid users in reformulating such queries. We extract these query aspects from query reformulations obtained from user session logs; since reformulations are exactly what the extracted query aspects are then used for, historical query reformulations are particularly well-suited for our problem domain.

We propose a new optimization-based formulation and algorithms for extracting such broad latent aspects from historical user session logs via an offline process. We also present an optimal and efficient algorithm to pick, at run-time, the top k query aspects to be shown to the user in response to her search query. Empirical studies on a large real-world corpus of search logs show significant gains over a strong baseline that uses single-keyword reformulations: a 14% gain in accuracy of aspects as judged by human editors, around 20% improvement in consistency among aspects predicted for  X  X imilar X  queries, and 23% gain in terms of weighted F-measure. All of these demonstrate the importance and accuracy of the broad query aspects that we find.
