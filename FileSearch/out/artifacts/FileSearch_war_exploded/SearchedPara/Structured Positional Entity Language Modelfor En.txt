 We investigate the problem of general entity retrieval for en-terprise websites. Our framework transforms the webpage content into a structured content representation, which cap-tures hierarchical information blocks and semi-structured data records information. To facilitate entity retrieval given a user query, we develop a structured positional entity lan-guage model suitable for ranking entities extracted from the webpage content incorporating the structured content rep-resentation. Different from existing language models for re-trieval, our proposed model considers both the proximity and the structured webpage content in a unified manner. Extensive experiments on the benchmark datasets demon-strate the effectiveness of our proposed framework. H.3 [ Information Storage and Retrieval ]: H.3.3 Infor-mation Search and Retrieval Information Retrieval; Entity Search; Semi-structured Infor-mation
When we visit a website, we usually want to find some in-formation about that organization/enterprise, such as what products they provide or who the current board members are. Much of the information need can be answered by one or more named entities, like person or organization names.  X 
The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: CUHK413510) and the Direct Grant of the Faculty of Engi-neering, CUHK (Project Code: 2050522). This work is also affiliated with the CUHK MoE-Microsoft Key Laboratory of Human-centric Computing and Interface Technologies. An entity retrieval system can return a list of entities, in-stead of just documents, that can directly answer the query. It has been shown that more that 40% of Web search queries are targeted on entities [23]. Thus such capability can save lots of users X  time of manual exploration. In this paper, we investigate the problem of entity retrieval for an enterprise website.

There are two related research areas, namely, enterprise search and entity retrieval. Both areas have been studied previously, but our goal is not exactly the same. For en-terprise search, TREC introduced the expert finding task in the Enterprise track, together with the document retrieval task [1]. The task of document retrieval can be regarded as Web search on a single website with the goal of returning relevant webpages or documents. The task of expert find-ing aims at locating suitable person names for an area of expertise within an enterprise website. Expert finding can be regarded as a special form of entity retrieval, requiring the retrieval of only person entities. TREC Entity track suc-ceeded the Enterprise track in 2009, where the expert finding task is extended to general entity retrieval in the Related Entity Finding (REF) task [3]. For each query, given the source entity name, together with its URL and the target entity type, we need to retrieve a set of entities satisfying the query narrative. Compared to the Enterprise track, an-swer entities do not need to be confined to an enterprise website. Some existing methods for entity retrieval follow a typical question answering approach [26, 27]. The problem we investigate in this paper shares some characteristics with entity retrieval, but we focus on finding general entities as answers in a given enterprise site. We locate the answer en-tities mainly from the corresponding enterprise website since the information on the enterprise website is more reliable. Thus the objective can also be regarded as an extension to the traditional enterprise search.

To retrieve relevant entities for a query, we make use of the structured content information of webpages. We observe that people tend to organize webpages into a hierarchical structure for clear presentation and easy navigation. Simi-lar to the fact that an ordinary document is organized into sections and subsections, a single webpage also typically pos-sesses hierarchical content structure. For example, the page segment shown in Figure 1 has three levels of headings, with the terms  X  X he Opinion Pages X  as the top-level heading, the left block with the heading  X  X RTICLES X  and the right block with the heading  X  X olumnist Schedule X  sharing the second-level heading  X  X olumnist X . Besides, in most websites, multi-ple webpages are organized in a hierarchical structure, which is usually indicated by the navigation menu and commonly referred as the logical sitemap [27]. When finding infor-mation from webpages, we humans often make use of clues or evidences from the structure and heading information. For example, consider the query  X  X ind the regular opinion columnists of the New York Times X  1 . In the webpage from The New York Times website shown in Figure 1, the an-swers, such as  X  X harles M. Blow X , are located in the right block with the heading  X  X olumnists Schedule X . Normally we will focus on that block of information to find answers after examining the webpage structure, ignoring names mentioned in other sections.
 Figure 1: Page segment from The New York Times website
Aiming at utilizing and exploiting the structured content of webpages, we develop a framework for tackling the entity retrieval problem in a rigorous manner by proposing a struc-tured positional entity language model. We first analyze and process webpages on an enterprise website to extract the site structure and transform the webpage content into a struc-tured content representation. The resulting representation captures the webpage content using hierarchical information blocks. Named entity detection is conducted to locate all entities in the content. To facilitate entity retrieval given a query, we develop a structured positional entity language model incorporating the structured content representation of the webpage. The proposed entity language model is used to measure the relevance of each entity in a webpage to the given query and facilitate the entity ranking. Different from existing language models for retrieval, our proposed model considers both the proximity and the structured page con-tent in a unified manner. The proximity principle prefers
It is derived from Query 41 of TREC Entity track in 2010. the entities near the occurrence of query terms, and at the same time, the structured content representation facilitates the consideration of the heading and the semi-structured information. Returning to the entity retrieval example de-scribed above regarding The New York Times columnists, the person entity  X  X harles M. Blow X  can be found as an an-swer based on the evidence that the query term  X  X olumnist X  appears near the entity in the heading of the same block. Furthermore, despite the fact that the text  X  X he Opinion Pages X  appears quite a distance away from the answer text in the raw webpage content, such text can be treated as a top-level heading text of the block containing the person names. This structured view of the webpage content facili-tates a desirable increase in the confidence that the person names are the answers since the term  X  X pinion X  appeared in the heading text matches with a similar term in the query.
Proximity-based language models have been employed in information retrieval, as well as expert finding. For exam-ple, the models proposed in [20] performs expert finding by employing proximity-based document representation. The positional language model in [18] makes use of proximity to tackle the document retrieval by constructing a language model for each position in the document. Another related research area is XML retrieval which has been extensively investigated in various INEX 2 tracks, such as the Linked Data Track [25]. Our proposed model differs from the above mentioned methods, as we consider both the structure of the webpage content and the proximity in an entity language model in a unified manner. Moreover, the entity retrieval problem investigated in this paper has a more challenging problem setting compared with XML retrieval. More details are presented in the next section.
Two closely related research areas are enterprise search and entity retrieval, both of which have been investigated previously. For enterprise search, TREC introduced the En-terprise track in 2005, featuring two tasks: document re-trieval and expert finding. The task of document retrieval can be regarded as Web search on a single website, with the goal of returning relevant webpages or documents. The task of expert finding can be regarded as a special form of entity retrieval, requiring the retrieval of only person entities for an area of expertise within an enterprise website. The lan-guage model approach for expert finding was first proposed by Balog et al. [2]. Petkova and Croft [20] used proximity-based document representation for expert finding. TREC Entity track succeeded Enterprise track in 2009, extending expert finding to general entity retrieval in the Related En-tity Finding (REF) task. Compared to the Enterprise track, answer entities do not need to be confined to an enterprise website. Most existing works follow a question answering approach. In 2011, Wang et al. [26] developed a method that combines their Document-Centered Model with affin-ity score between candidate entities and keywords to rank the entities. In 2010, Yang et al. [27] used the reconstructed logical hierarchical sitemap to enhance retrieval. In 2009, Fang et al. [10] proposed a hierarchical relevance retrieval model, considering document, passage, and entity for entity retrieval. Bron et al. [7] performed a detailed analysis on four core components, namely, co-occurrence models, type https://inex.mmci.uni-saarland.de/ filtering, context modeling, and homepage finding. Kaptein et al. [14] exploited Wikipedia as a pivot to perform entity ranking. Our enterprise entity retrieval problem can be re-garded as an extension to the traditional enterprise search.
The entity search using Semantic Web has been proposed and investigated recently [23, 6, 24, 13]. The semantic search makes use of existing Linked Data as sources to perform re-trieval. In our problem setting, the involved enterprises usu-ally do not have well-formatted semantic data, which makes it difficult to apply semantic search for an enterprise. In contrast, our framework makes use of the existing webpages on the enterprise website to conduct entity retrieval. The research area of XML retrieval heavily relies on the struc-ture of the XML file, and the problem has been extensively investigated in various INEX tracks such as the Linked Data Track [25]. However, usually the goal of XML retrieval is to return a list of relevant XML nodes. In some XML corpora, each major entity is already associated with a XML node in a well-structured context. The entity retrieval problem inves-tigated in this paper has a more challenging problem setting. With the objective of returning a list of relevant entities that can answer the user X  X  given query, we need to extract enti-ties from webpages and construct a structured context for entities, since webpages do not have a well-defined structure contrasting to XML files.

Proximity-based models have been used in information retrieval and expert finding. For example, Petkova and Croft [20] developed a proximity-based document represen-tation model to rank person entities for tackling the prob-lem of expert finding. Lv and Zhai [18] proposed the posi-tional language model (PLM) for information retrieval, and demonstrated the effectiveness over other proximity-based models. However, PLM is mainly used for document rank-ing. Our proposed entity language model differs from the above mentioned models, as we explicitly construct an en-tity language model for each entity by considering both the proximity and the structure of the webpage in a unified man-ner.

Entity retrieval is closely related to question answering, and there are some previous works investigating question answering using webpages. Pinto et al. [21] proposed to tackle question answering using semi-structured data found on webpages. Yin et al. [28] studied the problem of struc-tured knowledge extraction from attribute-value Web tables. Question answering systems usually take a three-step ap-proach, namely, query analysis, finding relevant documents, and answer extraction. They usually do not distinguish and take advantage when focusing on a domain or an enterprise on the Web.

There are various approaches to extract structure from webpages. Webpage segmentation aims at dividing webpage content into coherent groups. Kohlschutter and Nejdl [16] made use of text-density as a measure to identify the individ-ual text segments. Chakrabarti et al. [8] formulated the seg-mentation as a graph optimization problem. Beside the seg-mentation, semi-structured record extraction aims to extract structured content information presenting similar entities as well as their attributes, which is useful for various applica-tions such as knowledge base population [5]. Miao et al. [19] investigated a method based on the tag paths to preform record detection. Bing et al. [4] proposed RST structure to facilitate the record set detection. Our structured content representation of a webpage considers the semi-structured data as well as logical relationship of the page content.
As mentioned before, we develop a framework for tack-ling the entity retrieval problem by developing a structured positional entity language model. One core aspect of our proposed entity language model is the structured content generated from the webpage where the entity is located. A webpage has an inherent structure called Document Object Model (DOM). However, the DOM structure is not suitable for constructing entity language model, since it is mainly used to describe the page layout rather than semantic re-lationship among terms in the document. There are some existing work to represent structured document [17], how-ever they are mainly used to enhance document retrieval by content or by structure. In our framework, the structure of the document is represented as a tree whose nodes, referred to as information blocks , correspond to some segment of the webpage, such as a heading or a section containing several paragraphs. The leaf nodes comprise the minimal page seg-ment, such as a single paragraph, and any non-leaf nodes constitute a larger semantically relevant information block.
We start by crawling all the webpages under the enterprise domain, and performing site-level analysis such as webpage template detection and link extraction. The Wikipedia ar-ticles about the enterprise, if any, are also collected. All the collected webpages are transformed to a hierarchical struc-tured representation. Entity detection and resolution are then performed to find named entities.
A webpage template is defined as a segment that appears on more than two webpages on the same domain, such as the webpage footer. They are widely used in websites to provide a uniform appearance. We remove all the webpage templates using the boilerpipe library 3 [15] from all web-pages since they normally do not contain useful information for retrieval. A webpage may have multiple URLs, and we may get duplicated pages with exactly the same content in the crawling process. These pages are merged in this step.
Internal links between webpages within the same website play an important role. The anchor text usually gives a good description about the target page, thus it provides a site-level page context. For all the pages, we find the incoming links and the corresponding anchor texts from other pages in the same domain. The navigation menu in a website provides a logical organization of webpages. We locate the navigation menu and extract the website hierarchical logical structure following the method proposed in [27]. Take the official website of CIKM 2013 as an example. The  X  X all for Papers X  page is linked in the  X  X all For Papers X  item of the  X  X articipants X  menu. We extract the higher level menu text, such as  X  X articipants X  in this example, and attach it in the meta-heading of the page.
We transform each webpage into a structured represen-tation, named as information block . An information block is defined as a hierarchical structure, which either contains a list of children blocks, or corresponds to a page segment.
Available at https://code.google.com/p/boilerpipe/ Each block contains page content with relatively coherent format, and is optionally associated with a heading block. Two or more sibling blocks exhibiting similar layout format form a set of semi-structured data records. Figure 2: BNF representation of information block
The formal BNF definition for the notion of information block is depicted in Figure 2. We use the page segment shown in Figure 3 as an example. The content of the web-page can be split into page segments of different sizes, de-noted as &lt;block&gt; s. The whole page segment can be re-garded as an information block denoted as &lt;infoblock&gt; . A record set information block, denoted as &lt;recordsblock&gt; , is a special kind of information block that contains two or more similarly formatted record blocks denoted as &lt;record-block&gt; . There are six record blocks in Figure 3, where each block depicts a single book, and these six record blocks form a record set block. A ordinary Web table is also regarded as a record set block, with each table row as a record block. A record block consists of one or more fields denoted as &lt;field&gt; , which are aligned into field columns with fields in other record blocks within the same record set block. A field can have multiple field values, denoted as &lt;field-value&gt; , and field values in the same field column generally share the same format and content type. Fields in the same field column may have different number of field values. Re-turning to the example in Figure 3, each record block has six fields, namely, book image, title, author, published date, rating, and price. The price field has three values for the first four blocks, namely,  X  X book X ,  X  X rint &amp; Ebook X , and  X  X rint X , while the last two blocks only have one value. A block can have a heading block denoted as &lt;heading&gt; . In this example, the block with the content  X  X ath X  is a heading block, and it is associated with the record set block mentioned above.
To obtain such a hierarchical structured content represen-tation, we first perform data record detection on all the web-pages, using the RST method proposed in [4]. After the data records are detected, the fields within each data record is aligned by the Partial Tree Alignment algorithm [31]. Head-ing information are detected by the HTML tags h1 to h6 . HTML tables are preprocessed such that the potential field column heading information encoded in HTML &lt;th&gt; tag is preserved and assigned to the corresponding table columns.
For instance, the resulting structured content represen-tation for the page segment in Figure 3 is shown in Fig-ure 4. This hierarchical representation can be regarded as an ordered tree structure. In the representation, all the leaf blocks, denoted as gray blocks in Figure 4, correspond to some page segments.
After the structured content representation for each page is obtained, we perform named entity extraction on each in-formation block. Stanford Named Entity Recognizer [11] is used to perform NER on all the leaf blocks. The content in semi-structured record sets has not much context informa-tion and the entities in record sets may not be detected by Figure 3: Page Segment Example (taken from O X  X eilly website) Figure 4: Structured content representation for the page segment in Figure 3 the above NER tool. To handle such text data, we make use of existing entity repositories such as Wikipedia and Free-base to detect potentially missed entities. After the initial detection, we continue the detection by exploiting the struc-tured content representation. If multiple field values are detected as certain kind of entity, we have high confidence that other fields in the same field column also contain enti-ties with the same entity type. This technique enables the detection of entities not found in existing entity repositories.
Usually the same entity will be mentioned multiple times at different positions of the same page or across several pages. An entity may appear in different forms such as  X  X alifornia X  and  X  X A X . This raises the problem of entity res-olution, which recognizes different forms of the same entity and treats them to be the same. We rely on several clues to achieve this goal. Entity aliases in existing entity reposito-ries, such as topic aliases in Freebase and page redirections in Wikipedia, are used to conduct the resolution. Two entities pointing to the same internal webpage also have high prob-ability to be the same entity. We also take word synonyms and phrase abbreviations into consideration.
The language modeling approach is quite effective and has been widely used for document retrieval [22, 9, 29]. The idea is that since users usually issue queries using the keywords that would likely appear in a relevant document, a docu-ment is a good match to a query if the document is likely to generate the query. We adopt the similar idea in entity retrieval, where we first construct a language model for each entity, and then rank the entities based on the probability that the entity X  X  language model generates the keywords in the query. Different from language modeling for document retrieval, where the language model is estimated from the words in the document, we have no  X  X ocument X  for the en-tity. The only data we have is the webpage where the entity resides, but apparently we cannot directly use the webpage content to estimate the entity language model, since that document is not meant to solely describe the entity. We need to construct a virtual document for each entity that can describe the entity, based on the webpage where entities are extracted.

Inspired by the proximity-based approaches in [18, 20], we construct a virtual document based on the webpage content. Since the terms in the same webpage as the entity exhibit some relationships with the entity, the virtual document of the entity is constructed based on the terms in the webpage content. Specifically, the terms in the virtual document for the entity are propagated from the terms found in the web-page. The language model for an entity e in the webpage content D can be formulated as: where t denotes a term; V is the virtual document for the entity e ; c ( t,e,D ) is the total propagation count of the term t from all the positions in the webpage content D . The prop-agation count will take into consideration of the proximity, measured by the distance between the appearance of the term t and the entity e , as well as the structured content of the webpage.
Let us consider a webpage D , as exemplified in Figure 3, we transform it into a structured representation as exempli-fied in Figure 4. All the terms are found in the leaf blocks, denoted as {B l } . For a given entity e found in the leaf block B e corresponding to the webpage D , the propagation count for the term t is formulated as: where r ( B e , B ) is used to indicate the relevance for the terms in the block B with respect to the block B e containing the entity e ; d B e , B ( e,j ) is the distance function between the en-tity e and the term at the position j in the block B ; 1 t is a binary function indicating whether the term t appears at the position j in the block B ; k is the propagation ker-nel function. By designing different relevance function r and distance function d , we obtain different propagation schemas exploiting the structured content information in webpages. We first investigate various components that we need to con-sider when designing the propagation schema. Then we con-tinue to present two propagation models in Section 4.2.
The proximity principle, which considers the distance be-tween terms and entities, has been extensively used in docu-ment retrieval and expert finding [18, 20]. Proximity is also one major component in our entity language model. Terms near the entity should get higher propagation count since they possess stronger relationship with the entity. By incor-porating a non-uniform and non-increasing proximity kernel function, the proximity principle favors more on the terms that appears near the entity. The use of proximity can be integrated into our model by setting an appropriate distance function d B e , B ( e,j ) as depicted in Equation 2.
The block relevance function r measures the relationship between the entity block and the term block. Since the struc-tured content representation inherits semantic relationship between webpage segments, one intuitive relevance measure of two blocks is their relative positions in the tree-structured representation. The shortest path between two blocks, or equivalently, the average of the distances to their lowest common parent, is able to capture such notion of relevancy. Let us define the function that locates the lowest common ancestor block in the structured content representation for the blocks B i and B j as lca ( B i , B j ). The relevance measure r ( B e , B ) for the entity block B e and the term block B is proportional to their shortest path as follows: where v ( B i , B j ) is the number of vertices between blocks B and B j .
Heading blocks play an important role since they are in-tended to describe certain aspect for all the content un-der the heading. Thus, terms in the heading block should be considered as highly relevant context information even though the entity may be far away from the heading. We denote all the heading blocks as {H} , and define a function parent ( B ) to indicate the parent block for the block B . The set of heading blocks H e for the entity e can be represented as:
Site menu and page incoming link anchor texts, if any, can be integrated into the hierarchical heading model since they provide a description for the whole webpage content. For example, if a page with a title  X  X nsurance X  is under the  X  X er-vice X  menu, all the entities in this page will get the propaga-tion count from the term  X  X ervice X . We treat the site menu and anchor texts as the highest-level heading blocks.
One major component is to find the context for a given entity e . Basically, all the leaf blocks except heading blocks are regarded as the context blocks. However, if the entity e is located in a record block, we need to refine the con-tent based on the properties of record sets. We usually use records to represent parallel information, meaning that the terms in one record just provide information for that record. For example, if we want to locate the books written by some authors in Figure 3, only the entities in the blocks that share the same record block with the author name block are rel-evant. Thus it is appropriate to ignore other records in the same record set when we are constructing entity language model for entities appearing in a record. We denote all the record set blocks as {R} . Suppose that the entity e is lo-cated in the leaf block B l e , then the set of the record blocks R e which share the same record set with the entity e is de-fined as: This definition also includes the nested record blocks in higher-level record sets. When performing term propaga-tion, we just need to consider the blocks that are not in referred to as context blocks and denoted by C e : For the example page segment in Figure 3, if we are con-structing the context information for all the person entities appeared in the page, all the blocks under the same book record will be included as entity context such as the book title, but the blocks in other book records will be excluded.
As mentioned in Equation 2, a propagation schema is com-posed of two components, namely, the block relevance func-tion r and the distance function d . We propose two struc-tured propagation models by designing different propagation schemas.
Considering all the factors, the relevance function should depend on the block type, such as whether it is a heading block, and the relative position of the block, such as the shortest path between blocks. The distance function should take into consideration of the term position inside the block. We propose Structured Propagation Model 1 by designing each component in Equation 2 as follows: r ( B e , B ) = d where |B| denotes the total number of terms in the block B ; I B e ( e ) indicates the position of the entity e in the block B ; abs denotes the absolute function. The order of the leaf blocks, indicated by the symbols and  X  , is determined by their relative position in the original webpage.

In this model, the relevance measure r is different for head-ing blocks and context blocks, and we can adjust the impor-tance of heading blocks by the parameter  X  . Moreover, the distance function d just depends on the relative position of the term j inside the block B .
In Structured Propagation Model 1, two terms in differ-ent blocks may have the same propagation counts, if their corresponding blocks have the same shortest path to the en-tity block and the terms have the same relative position in-side the block. Another strategy is to consider the absolute term distance in the original webpage content. The rele-vance function r and the distance function d in Equation 2 can be written as: d the leaf blocks in H e or C e between B and B e , depending on whether B is a heading block or a context block. In this model, no two terms would have the same propagation counts.

We can continue to derive this model and obtain a form by concatenating the terms in the context blocks C e a single pseudo-context document I , and the terms in the heading blocks H e into a single pseudo-heading document J . Such derivation can support more efficient implementation of the model. As a result, the propagation count c ( t,e,D ) for term t can be written as: c ( t,e,D ) = (1  X   X  ) where d L ( e,j ) is the number of terms between the entity e and the position j in the pseudo-document L .
Our proposed structured information propagation, as de-picted in Equation 2, generalizes the traditional proximity-based retrieval models, such as the positional language model used in document retrieval [18] and the proximity-based en-tity retrieval model in [20]. If we set the block relevance measure r to be a constant and the distance function d to be the number of terms between the entity and the propagated term, it can be easily shown that the model is reduced to the proximity-based entity retrieval model proposed in [18]. Hence, the traditional proximity-based retrieval model is just a special case in our propagation model without con-sidering structured content information.
Any non-uniform, non-increasing function can be used as the propagation kernel function k as depicted in Equation 2. Following the previous work [18], we investigate three differ-ent representative kernel functions, namely, Gaussian kernel, Triangle kernel, and Circle kernel, as shown in Figure 5. 1. Gaussian kernel 2. Triangle kernel 3. Circle kernel where  X  controls the spread of kernel curves. One issue in language modeling estimation is smoothing. Since we only have a finite set of terms for each webpage, the maximum likelihood estimation may assign zero probability to the terms not found in the webpage, leading to undesir-able results. To tackle this problem, we include a collection language model to provide a background probability [30] for all the terms. We investigate two popular smoothing meth-ods, namely, Dirichlet prior and Jelinek-Mercer. where  X  and  X  are the smoothing parameters, p ( t |C ) is the collection language model, and Z e = P t  X  X  c ( t,e ) is the length of the virtual document for the entity e .
The last component in our framework handles the entity retrieval for a given query. User X  X  query is first analyzed to identify stop structure and key terms. Retrieved entities are ranked based on the probability that the entity language model, as presented in Section 4, generates the query terms.
When dealing with verbose queries, we follow the stop structure removal method proposed in [12]. A stop struc-ture is defined as a phrase which provides no information about the information need, such as  X  X ind the homepages of X  or  X  X ell me the X . Some of the query narratives mention the enterprise name. For example, for the Blackberry web-site, the query  X  X arriers that Blackberry makes phones for X  contains the enterprise entity name. These terms can be re-moved from the key terms since all the webpages under the enterprise website are all implicitly related to the enterprise name. Entity type information is considered in the retrieval model as described in Section 5.3.

A user may issue queries using different words with the same meaning. We conduct two kinds of query term expan-sion. One is for named entity terms and the other is for non-entity general terms. For named entity terms, various synonyms are added to the keyword terms from the Freebase aliases list, such as  X  X hilly X  for  X  X hiladelphia X . Acronyms in user X  X  query are expanded into full names and then added to the key terms. For non-entity terms, WordNet synonyms and hyponyms for the terms are included. For example, if the query contains the word  X  X usician X , then the term  X  X er-son X  or  X  X rtist X  will also be included in the key terms. This may broaden the query to some extent, nevertheless, it per-forms reasonably well in our experiments.
Using the Bayes X  rule, the probability that a candidate entity e is an answer entity for a given query Q can be written as: where p ( Q ) is the probability of the query; p ( e ) is the prior probability of the candidate entity e ; and p ( Q| e ) is the prob-ability of a query given the candidate. p ( Q ) is the same for all the candidate entities, and it is typical to assume the distribution of p ( e ) is uniform. Thus, the ranking of the candidate entities is proportional to the probability of the query given the entity p ( Q| e ).

The same entity may appear multiple times in the same page or across different pages. We find all the occurrences of the entities, and rank them by the maximal probability of the query Q given the entity e in the page content D as follows:
The structured positional entity language model p ( t | e, D ) related to the term t given an entity e expressed in Equa-tion 1 can be employed to compute p ( Q| e, D ). Precisely, we use the multinomial unigram language model to estimate the probability that the structured positional language model generates the query terms t as follows: where K q = L Q ! / ( tf t nomial coefficient for the query Q ; L Q is the length of the query Q ; and tf t in the query Q . K q can be ignored since it is a constant for a particular query. Thus, the entity ranking becomes as follows:
Our framework also considers evidence from the entity type information if the target entity type is given for the query. We mainly make use of entity type information to filter out irrelevant entities. Entities that do not meet the target entity type in the query are removed from the re-trieved entities in this component.

Precisely, suppose that the target entity type extracted from the query is denoted as T . For a particular entity candidate e , detected from the page content, it is associated with a set of entity types T e . We use the Wikipedia category structure to indicate whether the entity e satisfies the query target type T , denoted as p ( T| e ): p ( T| e ) = 1 cat ( T )  X  X  T e  X  X  par ( v ) ,v  X  T e }}6 = where par ( v ) gives all the parent categories for the entity type/category v in the Wikipedia category tree, and cat ( T ) maps the target entity type to some Wikipedia categories. This mapping can be easily prepared in advance.
We make use of the datasets used in TREC Entity track involving the ClueWeb09 corpus. There are two main sets of experiments to address several research questions. The first set of experiments aims at analyzing the performance of our model and conducting comparison to an existing en-tity retrieval model. We also analyze the effects of different kernel functions and smoothing methods. In the second set of experiments, we wish to compare the performance of our framework with the previously reported results by the par-ticipants in the Related Entity Finding (REF) task of the TREC Entity track. We run our experiments on the datasets derived from the TREC Entity track. An example query is given in Figure 6. For each query, besides the query narrative, the enterprise website (the entity URL specified by the ClueWeb09 ID) and the target entity type are also given. The evaluation dataset for the REF task of TREC Entity track in 2010, which is composed of 70 queries, is referred to as the TE10 dataset. The evaluation dataset for the REF task in 2011 contain-ing another 50 queries is referred to as the TE11 dataset. The webpages under the enterprise domain crawled in the ClueWeb09 corpus, together with the enterprise Wikipedia pages, if any, are used as sources to locate the answer enti-ties.
For some queries in the REF task of TREC Entity track, some answer entities only exist in webpages that are not un-der the enterprise websites. Another issue is that the REF task evaluates the performance based on the entity X  X  home-page. However, not all the entities have homepages, such as some person answer entities. To conduct experiments more suitable for our objective of enterprise entity retrieval, we selected a subset of queries that have answer entities in the enterprise website. In addition, we manually re-annotated all the answer entities to include the correct entity names, the entity X  X  homepages, and the corresponding Wikipedia pages if any. As a result, the evaluation can be done based on either the entity names, the entity X  X  Wikipedia page or the entity X  X  homepages. This dataset is referred to as the TE-E dataset and is publicly available 4 . Some characteris-tics of these three datasets are summarized in Table 1. The last row indicates the average number of webpages on the enterprise website.
 In our experiments, the Wikipedia data dump and the Freebase data dump were used to provide clues for entity extraction and resolution. For fair comparison with previ-ous TREC results, we used the Wikipedia database dump at October 17, 2009 and the Freebase data dump at March 20, 2009, which are roughly the same time when the ClueWeb09 corpus was crawled. The DOM tree structure is constructed using the lxml 5 HTML parser. The collection language model used in smoothing was estimated from the ClueWeb09 corpus, using a total of 251,446 webpages from the enterprise websites related to TREC Entity track. We use the stan-dard TREC evaluation program 6 and report three standard retrieval measures, namely, Mean Average Precision (MAP), Precision at ten (P@10), and R-Precision.

We also conduct a tuning process to find a suitable prop-agation kernel and determine the parameters in our frame-work. We used the first 20 queries in the TE10 dataset, which were released in 2009 in the TREC Entity track, as the tuning dataset. After the tuning process, Gaussian prop-agation kernel is adopted for term propagation and Dirich-let prior is adopted as the smoothing method for the entity language model estimation. The tuned parameters are as follows:  X  = 300,  X  = 0 . 8, and  X  = 200. If not specified, the following experiments will use this parameter setting.
The first set of experiments aims at assessing the per-formance of our model and conducting comparison to an existing entity retrieval model. We make use of the TE-E dataset to carry out this set of experiments. The perfor-mance of different propagation schemas, choices of kernel functions, and the smoothing methods are also investigated in this set of experiments. Structured Propagation Model 1 and 2 refer to our model as described in Section 4.2.1 and Section 4.2.2 respectively. The comparison model, referred to as the Proximity-based Model, denotes the proximity-based entity retrieval model proposed in [20].

Table 2 depicts the performance of different models. By considering the structured content, both of our models have better performance on MAP compared to the Proximity-based Retrieval Model. Structured Propagation Model 2
Available at http://www.se.cuhk.edu.hk/~textmine/?q= dataset/entity-retrieval
Available at http://lxml.de/
Available at http://trec.nist.gov/trec_eval/ further improves the performance compared to Structured Propagation Model 1. For most queries, our proposed model outperforms the comparison model, which does not con-sider the structured content representation. There are three queries that both models cannot return any correct entities, due to the fact that there are no explicit mentions of the answer entities in the corresponding enterprise website. For all the fifty queries, our named entity recognition component can detect 91.76% answer entities.

We also explore the behavior of different kernels with dif-ferent parameters. Figure 7 depicts the effect of kernels us-ing Structured Propagation Model 2. We can see that Gaus-sian kernel performs slightly better than other kernels. The figure for the Structured Propagation Model 1 is not shown since it attains similar behavior. F igure 7: Sensitivity to the parameter  X  of different kernels with Dirichlet smoothing (  X  = 200 )
We also investigate the influence of different smoothing methods and smoothing parameters, as shown in Figure 8 for Structured Propagation Model 2. Dirichlet prior smoothing performs better and it is relative insensitive to the smooth-ing parameter. F igure 8: Sensitivity to the smoothing parameter of Dirichlet prior smoothing (left) and Jelinek-Mercer smoothing (right). The legend is the same as that in Figure 7.

For the parameter  X  used in Equation 7, we find that the model is generally not sensitive to the precise value of  X  , as long as we set  X  &gt; 0 . 5 to prefer the terms found in the heading blocks.
The aim of the second set of experiments is to compare the performance with the previously reported results by the participants in the Related Entity Finding (REF) task of the TREC Entity track. In this set of experiments, we evaluated our Structured Propagation Model 2, the Proximity-based Retrieval Model in [20], and previous TREC Entity track results.

Since the REF task of TREC Entity track evaluates the entity retrieval performance based on entity homepages, we develop a homepage finding algorithm to find the homepages for the retrieved entities, described in the following subsec-tion.
Entity homepage usually refers to the official website of an entity, such as the personal webpage for a person entity. We develop a homepage finding algorithm based on a classi-fication method for the retrieved entities. Given a retrieved entity name, we query a search engine using entity name to retrieve a list of relevant pages. For each retrieved page, we generate a feature vector based on the page URL and its content. The feature vectors of the a set of benchmark en-tities are employed to train an SVM classifier which is used to determine whether a page is the homepage of a particular testing entity.

We exploit three types of features, namely, URL features, page content features, and Wikipedia features. URL fea-tures are summarized from the page URL, such as whether the URL fully or partially contains the entity name. Page content features include whether the page title contains the entity name, the frequency of the entity name in the page content, etc. If Wikipedia pages for the entity are retrieved from the search engine, we also extract the Wikipedia-based features, including whether a candidate URL is found in Wikipedia infoboxes, whether a candidate URL is found in Wikipedia external link sections, etc. We observe that the homepages of different entity types have different character-istics so that we train different classifiers for three groups of entities, namely, person, organization, and others.
The experiment results on the TE11 and TE10 dataset are shown in Table 3 and Table 4 respectively. By explicitly constructing the entity language model, both the proximity-based retrieval model and our structured propagation model outperform the best performance of the TREC participants. By considering the structure of the document, our struc-tured positional entity language model further improves the MAP by four to five percents. This demonstrates the im-portance of structure information embedded in webpages.
We investigate the problem of enterprise entity retrieval, which aims at returning entities as answers for a user query. To tackle this problem, we propose a structured positional entity language model. Combined with our structured con-tent transformation, we can handle entity retrieval in an ef-fective way. Extensive experiments on benchmark datasets demonstrate the effectiveness of our framework.

In the future work, we intend to exploit more sophisti-cated structured information to improve the entity retrieval. We intend to investigate the incorporation of visual clues to improve the webpage structure transformation.

