 Lots of document collections are well organized in hierarchi-cal structure, and such structure can help users browse and understand these collections. Meanwhile, there are a large number of plain document collections loosely organized, and it is difficult for users to understand them effectively. In this paper we study how to automatically integrate latent topics in a plain collection with the topics in a hierarchical struc-tured collection. We propose to use semi-supervised topic modeling to solve the problem in a principled way. The experiments show that the proposed method can generate both meaningful latent topics and expand high quality hier-archical topic structures.
 G.3 [ Probability and Statistics ]: Stochastic processes; I.2.7 [ Artificial Intelligence ]: [Natural Language Process-ing, Text analysis] Topical Integration, Hierarchical Topic Modeling
In the real world, we usually have a number of document collections, some of which have been well organized as hier-archical structure, and some others don X  X . For example, to provide an online question and answering service, it is very useful to make use of both cQA and online frequently asked question (FAQ) information. Most of the questions in the cQA systems such as Yahoo! Answers are well organized. However, FAQ does not contain the topical summary infor-mation. As a naive solution, we can generate some topical or clustering summary from the FAQ data independently. But such summary may be inconsistent to the hierarchi-cal structure representation of cQA data. Particularly, the topics from the FAQ data may be overlapped with or even redundant to the topics from the cQA data. Corresponding author.

It is useful to seamlessly integrate the observed topics in the hierarchical document collections with the latent top-ics in the plain document collections. First, the hierarchical structure information can definitely guide the topic discov-ery in the plain document collections. Since the hierarchical structure is carefully organized by the people, the observed nodes in the structure can naturally reflect the semantic top-ics in a collection. Such semantic topics can provide some supervision for topic modeling in the other document col-lections. Second, the plain document collections can com-plement the information in the hierarchical structure. For example, assume that we have a hierarchical collection about  X  X pple Inc. X  (as of Nov. 1, 2010), when  X  X pad X  was released, new documents about  X  X pad X  appeared. Thus we need to find and integrate the latent topic  X  X pad X  in the new gener-ated documents into the existing hierarchical topic summary about  X  X pple Inc. X .

In this paper, we study the integration problem for the topics in a hierarchical structured collection and the latent topics in a plain document collection. To the best of our knowledge, such an integration problem has not been studied in the existing work. We propose a generative method to solve this integration problem.
The Chinese Restaurant Process (CRP) [1] is a distribu-tion over partitions, and it can be described by the following metaphor. Imagine a restaurant with an infinite number of tables, and imagine customers entering the restaurant in se-quence. The d th customer sits at a table according to the following distribution, p ( c d = k | c 1:( d  X  1) )  X  m k if k is previous occupied where m k is the number of previous customers sitting at table k and  X  is a positive scalar. After D customers have sat down, their seating plan describes a partition of D items.
Our approach primarily consists of two stages: (1) topic integration with horizontal expansion and (2) vertical expansion , illustrated in Figure 1.
In this section, we will introduce a semi-supervised hierar-chical integration topic model, i.e., the Horizontal Expansion Hierarchical Latent Dirichlet Allocation (HEHLDA).
We assume that the generative process for a document in a plain collection is as following: 1) For a document from the plain collection, we need to choose a path from a topical tree; 2) Then we need to generate the terms in this docu-ment by the topics corresponding to the nodes on its path. After generating M documents, we have chosen a topical tree for these M documents, we can take the M documents and its topical tree as our hierarchical labeled collection, and re-maining documents as our plain collection. This metaphor gives us inspiration. To obtain topical integration, we can simulate the process to continue to generate the documents in the plain collection, and finally obtain a hierarchical topi-cal integration automatically. For simplicity, we assume the document length and depth of each leaf node in the tree to be static values, denoted as N and L l respectively. The derivation is still valid if the document lengths and depth of leaf nodes vary. Thus, the generative process can be formu-lated as follows:
According to the assumption and generative process above, in this section, we describe a Gibbs sampling algorithm for sampling from the posterior and corresponding latent top-ics in the HEHLDA model. The Gibbs sampler provides a method for simultaneously exploring the parameter space (the latent topics of the plain collection) and the model space (L-level trees).

In HEHLDA, we sample the per-document paths c m in plain collection and the per-word level allocations to top-ics in all paths of the whole topical tree z m,n .Thus,we approximate the posterior p ( c p m , z m |  X , X , w , c l ). The hyper-parameter  X  reflects the tendency of the customers in each restaurant to share tables,  X  denotes the expected variance of the underlying topics (e.g,  X  1 will tend to choose top-ics with fewer high-probability words), and w m,n denotes the n th word in the m th document. c m,l represents the restaurant corresponding to the l th topic in document m , and superscript  X  p  X  X nd X  l  X  in a notation denote  X  X rom plain collection X  and  X  X rom hierarchical labeled collection X , such as collection; if there is no superscipt in a notation, the docu-ment is from the whole collection; and z m,n , the assignment of the n th word in the m th document to one of the L avail-able topics. All other variables in the model  X   X  and  X   X  X re integrated out. The Gibbs sampler thus assesses the values
The Gibbs sampler can be divided into two main steps: the sampling of level allocations and the sampling of path assignments for plain documents.

First, given the values of the HEHLDA hidden variables, we sample the c p m,l variables which are associated with the CRP prior. The conditional distribution for c p m ,the L 1 ics associated with document m ,is: p ( c p m | z , w , c p  X  m , c l )  X  p ( w p m | z , w  X  where |
V | is the size of vocabulary,  X ( . ) denotes the standard gamma function, n w c m,l ,  X  m is the number of instances of word w that have been assigned to the topic indexed by c m,l , not includ-m implied by the nested CRP.

Second, given the current state of the HEHLDA, we sam-ple the z m,n variables of the underlying HEHLDA model as follows:
Note that HEHLDA builds a topical hierarchical struc-ture, in which each node corresponds to a topic, and each document is assigned to one path in the structure. We il-lustrate it by the center part in Figure 1. Here, we take all these topics as observed topics, compared with the new topics we will detect. We want to expand our hierarchi-cal topical summarization vertically, illustrated in the right part of Figure 1. We will use Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA) [13] to solve the prob-lem of vertical topical expansion. Like hierarchical Labeled LDA (hLLDA) [16], SSHLDA can incorporate observed top-ics into the generative process of documents. On the other hand, like hierarchical Latent Dirichlet Allocation (hLDA) [1], SSHLDA can automatically explore latent topic in data space, and extend the existing hierarchy of observed topics.
We demonstrate the effectiveness of the proposed method on large, real-world datasets on three tasks: topic modeling, document classification and clustering.
We first crawled question-answer pairs (QA pairs) from two top categories in Yahoo! Answers: Computers &amp; Inter-net and Health . This gives rise to an archive of 6,345,786 QA documents. We refer to the dataset as Y!A .Our ODP dataset contains the Web pages and their hierarchical struc-ture in two top category ( Home and Health )fromOpenDi-rectory Project Website  X  . We removed all categories whose number of Web sites is less than 3 for its sparseness. For each of Web sites in categories, we further extended its de-scription information by submitting the URL to Google and used the words in the snippet and title of the first returned result. The statistics of all datasets are summarized in Table 1. From this table, we can see that these two datasets are very different: Y!A dataset has much fewer categories than ODP dataset, and the depth of the hierarchical structure is much shallow, but it contains much more documents for each category.
 In our experiment, we randomly partition a dataset with L -height hierarchical structure into two parts. The random partition processes as follows: (i) first choose top-l levels sub-tree ( l&lt;L ) in the hierarchy, denoted as T l ; (ii) sample a subset of paths S p from the set of paths in T l according to http://dmoz.org/ Figure 2: A sub network discovered on Yahoo! Answer dataset using proposed method, and the whole tree has 74 nodes. In the figure, the blue nodes are observed topics with observed labels; The green nodes are the latent topics without labels from plain collection, which is result of HEHLDA; And the shaded nodes are the latent topics without la-bels from all the documents, which is the result of SSHLDA. Each topic represented by top 5 terms. a proportion  X  1 . (iii) and then sample some documents from the documents in these paths according to a proportion  X  2 The sampled documents and their corresponding paths will be taken as our hierarchical labeled collection, denoted as C ; and the remaining documents C p , are taken as the plain collection.

In this paper, for both datasets Y!A and ODP ,wechoose  X  1  X  X  1 / 3 , 1 / 2 , 2 / 3 } ,and  X  2 =3 / 4; meanwhile, we choose l = 3 for dataset Y!A and l = 4 for dataset ODP .Allex-perimental results are average values, i.e we sample S p 5 times according  X  1 , then obtain a experimental value for each samping collections, and finally compute average value over these 5 results. In particular, we run HEHLDA and SSHLDA models both for each sampling collection, with a burn-in of 10000 Gibbs sampling iterations, symmetric pri-ors  X  = 0.1 and free parameter  X  = 1.0; Then we compute scores by the measures introduced in latter sections, and ob-tain average values. For  X  , we can obtain the estimation of  X  c i by fixed-point iteration [15].
In our models, each latent topic is modeled as a word distribution. Naturally, the words with the highest proba-bility in a word distribution about a topic can be used as a description for this topic [4, 3]. We show an example in Figure 2, which is the topical integration resultant over Y!A dataset. The Hierarchical structure is generated by our in-tegration and expansion algorithm. In Figure 2, the blue nodes are those in the existing hierarchical structure; The green nodes are the latent topics generated in topic integra-tion and horizontal expansion (HEHLDA model); And the shaded nodes are the latent topics generated in the vertical expansion (SSHLDA model).

We have three findings from the example: (i) During hor-izontal topic integration, the new document from the plain collection can be assigned to either a node in the existing hierarchical structure (green nodes in Figure 2), or a node in the expanded latent topics (blue nodes). On the other hand, in the vertical expansion, we can discover finer-grained structure and latent topics (shaded nodes); (ii) During the horizontal integration and vertical expansion, our models can make use of the information from existing hierarchical structure, and it can help generate a logical, structural hi-Figure 3: Perplexities of hLDA, HEHLDA and  X  X EHLDA + SSHLDA X . The X-axis is the pro-portion of documents in BT tree, and Y-axis is the perplexity. (a) The results are run over the Y!A dataset, with observed height l =3 and topi-cal height L =4 ; (b) The results are run over the ODP dataset, with observed height l =4 and topical height L =8 . erarchy with parent-child relations Particularly, the latent topic about a parent node is usually general than a topic about a child node. For example, in Figure 2, we can find that the  X  X omputer X  node (with topical description  X  X ard, screen X  etc.) at the 3rd level contains a lot of children, three of which are about the topic about laptop, printer and mon-itor respectively. (iii) In a hierarchy of topics, if a topical node is derived directly from the existing hierarchical struc-ture, it usually contains other description information be-sides the high probability topical words. Many hierarchy organizers label these nodes manually, and these labels can help people understand the themes about the nodes and their descendant in the hierarchy. For example, when we know node  X  X rror files click screen virus X  in Figure 2 has its label  X  X omputers &amp; Internet X , we can understand the child topic  X  X ite yahoo email page address X  is about  X  X nternet X .
These observations show that topical integration is very interesting and useful.
Topic models are usually evaluated by its ability to gen-erate the unseen data. Perplexity is widely used in the lan-guage modeling and topic modeling community [4]. Lower perplexity score of a topic model indicates stronger general-ization ability to the new data. In our experiment, we keep 80% of the data collection as the training set and use the remaining collection as the test set.

Since the hierarchical topic integration is a novel prob-lem, we don X  X  have direct baseline models. Instead, we use hLDA model [1] as the baseline, since hLDA can also obtain a hierarchical topics for a plain collection. For comparison, we remove the structure information from the hierarchical structured document collection, and combine it with the plain document collection, and then model this combined plain collection by hLDA model. This method provide us a reasonable baseline.
 We present the results on the Y!A and ODP datasets in Figure 3. As our proposed method has two stages, and it would generate a model as a result of each stage. In the figure, HEHLDA indicated the result from the HEHLDA model and HEHLDA+SSHLDA indicates the result from both HEHLDA and SSHLDA model. From the figure, we can see that the perplexities of proposed models, HEHLDA and the combination of HEHLDA and SSHLDA, are lower than that of hLDA at different proportion value of observed paths. It shows that the proposed models have stronger generalization power than the hLDA model. We can test on different tree height parameter values, and the results are robust to the height selection.
The evaluation by perplexity just tells one side of the story: the accuracy of the topics generated by the model. Another important motivation of the integration problem is to expand a semantically meaningful hierarchical structure that can help users browse and understand a document col-lection. In this section, we investigate the accuracy of the expanded structure from our proposed model.

In our experimental dataset described in section 4.1, we partition the original hierarchical structured collection C (with a gold standard tree T ) into two parts: hierarchi-cal structured sub-collection C 1 (with the hierarchical tree T ) and plain collection C 2 . Our models can integrate the documents from C 1 and C 2 into a unified hierarchical struc-ture T . To evaluate the accuracy of the integration, we can compare the rebuilt tree T with the gold standard tree T . In our models, the documents from the hierarchical struc-tured collection ( C 1 ) are assigned to their original paths, so we need to compare the paths of those documents from the plain collection ( C 2 ).

We use classification and clustering metrics to evaluate two kinds of documents in C 2 :(i) C 2 , 1 : the documents that are selected from the paths in T 1 , and (ii) C 2 , 2 : those docu-ments that are selected from the paths that do not exist in T .
For a document from C 2 , 1 , we can check whether the its assigned path is exactly the same as its path in gold stan-dard tree T . So we can cast it as a classification problem, and evaluate it with a widely used classification metric micro-averaged F 1 ( Mi-F 1 ) [20]. We choose RBF-SVM as our baseline. The label of a node will be taken as the target value of all documents in this node, and words are used as features.

We show the result of Mi-F i in Table 2. Whatever the sampling proportion is, from Table 2, proposed method per-formances significantly better than that of the baseline on both datasets. Why our method performances better? One possible explanation is that baseline method does not con-sider the relation among nodes, and our method uses indi-rectly the hierarchical relation among nodes. On the other hand, with the sampling proportion increases, the Mi-F 1 creases. This is because the number of categories increases when sampling proportion increases, which will generally decrease the performance of classification.
Besides the documents that are assigned to T in the gold standard, there are still some documents ( C 2 , 2 ) are not as-signed to any paths in T ,since T is only a sub-tree of T . Since the expanded nodes in tree T do not have identities, we cannot compare these new paths to those in T .Alterna-tively, we use the clustering evaluation technique to measure the quality of these expanded structures.

In this paper, we use the FScore [12] to measure the qual-ity of the expanded structure. We take those nodes in T but not in T 1 as gold standard expanded paths, and each node is corresponding to a class. Similarly, each node in T but not in T 1 can be considered as a cluster generated by our models. In general, the higher the FScore values, the better the clustering solution is.

In this aspect, we demonstrate the strength of the hier-archical integration models by comparing it with a single-linkage clustering algorithm CLUTO [9]. It can generate the clusters for documents in C 2 , 2 . We denote the method as h-cluster .
 For Y!A , l =3and L  X  X  3 , 4 } ;for ODP , l =4and L  X  X  4 , 5 , 6 } . The clustering results are presented in Table 4. From the table, we can see that our proposed model can achieve consistent improvement over the baseline at smaller depth for Y!A and ODP . However, in higher height, h-cluster maybe performance better than our proposed method. This is reasonable because higher height means more clusters to form for proposed method, which will decrease the perfor-mance of clustering. For HEHLDA, it always performances better than h-cluster over Y!A and ODP . For example, for Y!A , the performance of HEHLDA can reach 0.5183, 0.5939 and 0.6987 with proportion  X  1  X  X  1 / 3 , 1 / 2 , 2 / 3 } h-cluster only achieve 0.40845, 0.5362 and 0.5826. The re-sult shows that our model can achieve about 26.9%, 10.8% and 19.9% improvements over h-clustering with proportion  X   X  X  1 / 3 , 1 / 2 , 2 / 3 } . The improvements are significant by t-test according to significance 95%. We can also obtain similar conclusion over ODP .
To the best of our knowledge, no previous study has ad-dressed the problem of integrating a hierarchy of label topics with latent topics in text documents. Topic model has been widely and successfully applied to mine topic patterns [3, 4]. Our work adds to this line yet another novel use of such models for topical integration.

Unsupervised non-hierarchical topic models are widely stud-ied, such as pLSA [8], LDA [4] and Concept TM [5, 7, 6] etc. However, the above models cannot capture the relation be-tween super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) [1], Hierarchical Dirichlet pro-cesses (HDP) [21], Pachinko Allocation Model (PAM) [11] and Hierarchical PAM (HPAM) [14] etc.

Although unsupervised topic models are sufficiently ex-pressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the supervised label set into their learning procedure. Several modifications [20, 2, 3, 10, 17, 19, 20, 20, 18] of LDA to incorporate supervision have been pro-posed in the literature.

None of these models, however, leverage dependency struc-ture, such as parent-child relation, in the label space. As far as we know, only hLLDA [16] is proposed to capture the structral relation.
In this paper, we try to use semi-supervised probabilistic topic modeling to solve a novel problem of hierarchical topi-cal integration which aims at integrating topics expressed in a well-written labels with latent topics hidden in plain col-lections to generate a unified hierarchical topical summary.
