 Social media responses to news have increasingly gained in importance as they can enhance a consumer X  X  news reading experience, promote information sharing and aid journal-ists in assessing their readership X  X  response to a story. Given that the number of responses to an online news article may be huge, a common challenge is that of selecting only the most interesting responses for display. This paper addresses this challenge by casting message selection as an optimiza-tion problem. We define an objective function which jointly models the messages X  utility scores and their entropy. We propose a near-optimal solution to the underlying optimiza-tion problem, which leverages the submodularity property of the objective function. Our solution first learns the utility of individual messages in isolation and then produces a diverse selection of interesting messages by maximizing the defined objective function. The intuitions behind our work are that an interesting selection of messages contains diverse, infor-mative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions are embodied by a rich set of con-tent, social and user features capturing the aforementioned aspects. We evaluate our approach through both human and automatic experiments, and demonstrate it outperforms the state of the art. Additionally, we perform an in-depth anal-ysis of the annotated  X  X nteresting X  responses, shedding light on the subjectivity around the selection process and the per-ception of interestingness.
 H.5.4 [ Information Interfaces and Presentation ]: Hy-pertext/Hypermedia; H.3.1 [ Information Storage and Re-trieval ]: Content Analysis and Indexing Algorithms, Measurement, Performance, Experimentation Social Media, Microblogging, Sampling, Summarization Social media services are increasingly playing a major role as platforms for people to express and share their opinions on current events. Responses to real-world events often come in the form of short messages, referring to specific topics, content or information sources. Such messages serve several purposes: to share a particular news article, to express an opinion about the ongoing events, or to add or refute infor-mation about the topic or the mentioned article. The exten-sive use of social media during recent major events (e.g. the Arab Spring and the Financial Crisis) shows that its use in these situations has become pervasive. On Twitter, for in-stance, a significant share of all tweets posted concerns news events [18].

Considering this large volume of messages being posted in the context of news, keeping track of messages that refer to the most popular articles can easily become overwhelm-ing. This information overload motivates the development of automatic systems that select and display only the most interesting messages. This social media message selection problem is illustrated in Figure 1: the selection system takes as input a news article and all the social media responses referring to that article, and outputs the most interesting subset of responses. Our work is motivated by a variety of existing and future applications in which interesting social media responses could be automatically coupled with tradi-tional news content, e.g., for displaying social responses near a news article for an enhanced reading experience.

Even though quantifying the interestingness of a selection of messages is inherently subjective, we postulate that an interesting response set consists of a diverse set of informa-tive, opinionated and popular messages written to a large extent by authoritative users. By decomposing the notion of interestingness into these indicators we can pose the task of finding an interesting selection of messages as an optimiza-tion problem. We aim at maximizing an objective function which explicitly models the relationship among the indica-Figure 1: The social media message selection prob-lem: selecting a small subset of messages sent in re-sponse to a news article. tors thus producing selections that the typical person finds most interesting. We expect each indicator to have some in-fluence on the messages ultimately selected to be part of the interesting set. Our method considers multiple content, social and user features to infer the intrinsic level of in-formativeness, opinionatedness, popularity and authority of each message, while simultaneously ensuring the inclusion of diverse messages in the final set.

We evaluate our approach through both human and auto-matic experiments and demonstrate that it outperforms the state of the art. In addition, we perform an in-depth analysis of the human evaluations, shedding light on the subjectivity and perception of interestingness in this particular task. To the best of our knowledge we have created the largest man-ually annotated news-response dataset to date, consisting of 45 news articles and 28,055 ratings per indicator anno-tated by 14 professional editors. Even though in this paper we focus on messages posted on Twitter, our method can be applied to responses to news in other platforms, for instance article comments or discussion forums.

The remainder of this paper is organized as follows. In Sec-tion 2 we first discuss related work. In Section 3 we present our proposed method, after which in Section 4 we describe our experiments and discuss the results. In Section 5 we conclude the paper with final remarks and future work. There is an increasing amount of literature on social me-dia sampling and summarization, where the goal is respec-tively to select a representative sample of messages on a given topic [4, 10] or to build a summary of an event from social media posts [31, 20, 6]. Closely related is also the task of update summarization, which aims to create a summary of new information given one or more documents that have already been read, focusing mainly on web documents [21] as well as social media, especially within opinions [15].
The idea of interestingness is also related to the work done on predicting re-posts of messages [25], where retweets are considered a proxy for interestingness. The authors of the paper use a varied set of features for predicting retweets, in-cluding emotion-related information, the presence of lexical indicators of interestingness and mood indicators; bag-of-words features are also explored but revealed low predict-ing power due to sparsity issues. Interestingness was also explored in the context of whole conversations within so-cial media [11], where it was decomposed into two modal-ities: the interestingness of both the participants, as well as the conversation. While the former can be characterized by participation in interesting conversations, the latter is characterized by the topic, the communication dynamics of the participants and the conversation itself. Another aspect that we consider as part of interestingness is content quality, one of the most important indicators in question-answering communities [1], where the model combines both the social aspects and the linguistic quality of the messages. In image tagging [13], an interesting object has been defined to occur more frequently during a specific time interval than outside it, corrected for object frequency.

Our method has been inspired by the outcomes of stud-ies analyzing the different types of social media reactions to different types of events, e.g. by the observation that break-ing news is often accompanied by  X  X nformational X  messages, whereas other news items are characterized by more  X  X on-versational X  reactions [23]. Furthermore, events can elicit different levels of excitement and participation, depending on both the nature of the event (life, work), the charac-teristics of the participants [30] and even their opinion [22]. These multiple facets have been effectively tackled by multi-objective optimization approaches [9]. Recently, significant attention has been directed at detecting and tracking events as they surface in microblogs [12, 27]. Social media mes-sages have also been used to predict the popularity aspect of news stories [3], as well as assist in their summarization [33]. Our problem statement looks at the reverse perspective by observing responses to news articles describing real-world events.

Overall, results presented in the literature point to a strat-egy that contextualizes the posted messages as much as pos-sible, generating not just content, social and communication aspects, but also diverse textual and linguistic descriptors. To construct rich individual and collective features we there-fore leverage insights from several existing approaches. Pri-marily, we build on existing work on sentiment and intensity analysis [29, 30], and apply basic redundancy detection [35]. Additionally, we incorporate user authority, both for specific topics as well as in general [2]. We formulate the social message selection problem as fol-lows.
 Problem statement: Given a news article and a set of related messages M , we seek a subset S  X  M of k messages which are the most  X  X nteresting X  to a typical reader in the context of the article.
 We represent  X  X nterestingness X  using a set of indicators . We identify four message-level indicators : informativeness, opinionatedness, popularity, authority ; and one set-level in-dicator, diversity . The intuition is that an interesting selec-tion of messages contains informative , opinionated and pop-ular messages referring to the news article. Furthermore, the messages should be written mostly by users who have authority on the topic. Looking beyond single messages, we posit that an interesting selection should contain messages that are diverse in content.
 Solution: We computationally model the four message-level indicators using an utility function r and the set-level diver-sity indicator using a normalized entropy function H 0 . The solution to the social message selection problem is then to find the subset S  X  that maximizes the objective function g ( S ), which measures the  X  X oodness X  of subset S as follows: where r ( m ) represents the utility score of a message m and H ( S ) is the normalized joint entropy of the entire set of messages S . We balance the effect of the collective diversity indicator on the sampling by specifying a suitable  X  , which we will define manually in our experiments. The maximiza-tion problem is thus defined as follows: In Section 3.3 we will describe how we derive r and H 0 , while we first focus on how to solve the optimization problem in Sections 3.1 and 3.2. In order to find an optimal solution for the maximization problem we could exhaustively search the space of all pos-sible message subsets. However this proves computationally prohibitive. We therefore consider the properties of our ob-jective function in order to find a fast, approximate solu-tion to the problem. To this end we prove that g ( S ) is sub-modular. This key property allows us to use a greedy algo-rithm [26] that provides efficient linear solutions within a deterministic error bound from the best possible solution.
A submodular function f on the set  X  is defined as the set function f : 2  X   X &lt; , where 2  X  is the power set of  X  and one of several equivalent conditions are satisfied, amongst which the following: Intuitively, this condition specifies that f has a natural di-minishing return property, meaning that the difference of the value of the function that a single element addition makes decreases as the size of the input set increases. Here, we attempt to demonstrate that our function g ( S ) is submodu-lar. Let us refer to P m  X  S r ( m ) as R ( S ) for brevity. We then insert the entire objective function in the submodularity in-equality condition mentioned above: Since the scoring function is a sum over all r ( m ), which are independent amongst each other, it holds that R ( S  X  X  m } )  X  R ( S ) = r ( m ) and thus  X R ( X  X  X  m } )  X   X R ( X ) =  X R ( Y  X  { m } )  X   X R ( Y ). Therefore, all the expressions of the scoring function cancel out and we are left with the expression for the submodularity of entropy:
In order to prove that g ( S ) is submodular we therefore have to simply show that entropy is submodular. Previous work has already established that the entropy of a sample is a monotonic non-decreasing submodular function [17]. We can thus conclude that g ( S ) is submodular. While maximizing a submodular function is still a computa-tionally difficult problem, maximizing entropy can be solved using a greedy approach, achieving an approximate solution that is within a constant factor from the optimal solution, Figure 2: The diagram of the proposed algorithm, showing the two criteria, joint individual message and set diversity. where no obvious heuristic would reduce this bound [16]. Here, since H 0 (  X  ) = 0, it follows that a greedy selection al-gorithm that selects a set of k elements is at most 1  X  1 worse than the optimal set of examples. Thus, to approxi-mate the optimal solution, we plug g ( S ) into our greedy algorithm as shown in Algorithm 1 and visualized in Fig-ure 2. Our algorithm initializes by picking the element m that ranks highest with the scoring function r ( m i it iteratively adds the element m i that would provide the biggest boost in the objective function if it were added to the set S , stopping at the desired sample size k . Algorithm 1 : Greedy iterative sampling algorithm Input : A collection of messages M , the sample size k Output : A sample set of messages S  X 
S  X  X  X { arg max while | S | &lt; k do end
S  X   X  X  X  S The two main components of the target function, r and H 0 are both computed from a large set of features. These fea-tures serve as computational proxies for the five indicators mentioned in the previous sections. Intuitively, most features are likely to be specific to the target social message platform; in this paper we therefore focus on Twitter-specific features, though most of them are generalizable.
 Features: Table 1 reports the list of features, where super-scripts s and d indicate features that are used for computing r and H 0 , respectively.  X  The content features capture the linguistic aspects that indicate how interesting, informative and opinionated a message is.  X  The social features represent the level of diffusion of a message, reflecting its popularity.  X  The user features capture the overall and topical authority of a user in the social network.
 Individual message scoring r ( m ) : Given a tweet and its corresponding scoring feature values (scaled to interval [0 , 1]), we use a supervised model as the scoring function r ( m ). Specifically, we use the prediction given by support vector regression ( -SVR [32]), trained using the manually obtained labels for interestingness described in detail in Sec-tion 4.2, using only examples that were labeled with 1 and 3, omitting those with the label of 2.
 Entropy of message set H 0 : Given a message set S and a set of selection features used to collectively select mes-sages, we treat these features as binary random variables. Let H 0 ( S ) denote the normalized entropy of the message set S given a probability model that uses the empirical proba-bility of the selection feature values: H 0 ( S ) = H ( S ) / log d in which d is the number of binary random variables, and H ( S ) =  X  P d i =0 p ( f i = 1) log p ( f i = 1), where p ( f the empirical probability that the feature f i has the value of 1 given all examples in S . The intuition behind maximizing entropy is that it favors adding examples to S with different non-zero features from the ones already in S , since adding an example with different features increases entropy more than adding an example already similar to the ones in S . Balancing individual and collective scores: Using the parameter  X  , we can balance the influence between scoring and entropy:  X  = 0 yields an entropy-only approach, which we denote as entropy , whereas  X  = 1 gives a scoring-only approach that we refer to as svr . The balanced combination at  X  = 0 . 5 is denoted as svr entropy . Our research is closely related to the diversity-based sam-pling method of De Choudhury et al. [10] and the social con-text summarization method of Yang et al. [33] that currently are considered state of the art. We first briefly describe these two methods and discuss their strengths and weaknesses in order to motivate our approach.
 Diversity-based sampling De Choudhury et al. [10] tackle tweet selection using a greedy approach that iteratively picks tweets that minimize the dis-tortion of entropy of the current subset, given a certain di-versity criterion. The approach prescribes building a prob-abilistic model of tweets given their diversity using social, content and network features. The rationale behind the ap-proach is that samples with a low entropy will be highly homogeneous with respect to the feature space, while those with a high entropy will be highly heterogeneous. In a so-cial media message selection context it is thus preferred for the method to produce a sample with high entropy in order to promote diversity in the selected messages. The sample generation is initiated by adding a random message to the sample, after which new samples are iteratively added, so that the normalized entropy of the sample is closest to the pre-specified diversity parameter. This iterative process is repeated until the desired sample size has been reached. In our evaluations we refer to this method as diversity . Social context summarization Yang et al. [33] propose an approach based on conditional random fields that simultaneously addresses message and document summarization by modeling messages and sen-tences as connected  X  X ings X  in a factor graph, where factors are assigned to individual message and sentence features, be-tween their instances on a single wing and across both wings. To determine the key sentences and the important messages, an approximate inference approach iteratively updates and propagates the beliefs between factors and labels, where the final labeling indicates which sentences and messages are se-lected. We refer to this dual-wing factor graph method as dwfg . In the same paper, a simplified model was also pre-sented, implemented as a logistic regression classifier, that enriches each tweet with the features of the sentence most similar to it rather than introducing sentence-tweet factors between similar tweet-sentence pairs as is the case in the original model. In the task of summarizing relevant tweets for a news article this method yielded good performance. We refer to this method as lr+ .
 Observations Even though the entropy measure used in the diversity approach has been demonstrated to be a good proxy for di-versity, it does not evaluate individual messages with respect to their individual characteristics. Furthermore, its content probability model assigns each message to a broad category, such as  X  X olitics X  and  X  X ports X ; while this works very well for obtaining a diverse sample on broader topics, this may not be a good fit in the case of responses to a single news story on a relatively narrow topic. The dual-wing factor graph and logistic regression methods take a richer set of features into account, but do not explicitly model diversity in the con-text of how people respond to news. Specifically, connecting messages only through their reply/retweet relations means that highly similar or even duplicate messages  X  a common occurrence in the context of news  X  may not be linked to each other, likely resulting in a substantial amount of re-dundancy being present in the final selection. Our method therefore offers the following improvements:  X  We focus on a comprehensive notion of interestingness in-stead of solely optimizing for message diversity ( diversity or for message summarization ( dwfg and lr+ ).  X  We detect and avoid redundancy between the content of messages by including a richer set of textual features.  X  We model the importance of messages beyond their literal content by including features that capture the wider so-cial, conversational and linguistic context, e.g. the amount of opinion, emotion and controversy of a message, the level of authority of the author on the topic, and the user X  X  in-tent of the message [24].  X  We provide a theoretic guarantee of the sample quality. In this section we describe our experiments for evaluating the performance of our proposed method against the previously mentioned approaches on the problem of selecting the most interesting tweets posted in response to news. We collected a set of tweets posted on Twitter between February 22, 2011 and May 31, 2011 that were written in the English language and that included a URL to an article pub-lished online by news agencies such as Reuters, Huffington Post, India Times and Al Jazeera. We crawled each of these links to obtain the original news article content, discarding redirected and unresolved links. We observed that the news-related tweets frequently had highly similar content to each other; when ignoring retweet markers, links, mentions and hashtags, and only purely looking at the words, just over half of the tweets were identical in our dataset. For each news ar-ticle, we therefore filtered out all such duplicate tweets and only retained the one with the earliest timestamp. For the experiments we ultimately picked a set of 45 news articles H , respectively.
 that spanned a wide range of topics, including business, poli-tics, technology, weather, disasters, and warfare. We ensured each article had on the order of 100 unique tweets associated with it, so that there would not only be sufficient tweets for the evaluated methods to work with, but also so that the tasks would not overly exhaust the annotators that created the gold standard. We asked a team of 14 annotators to create a gold standard collection by: a) rating the individual tweets sent in response to an article on the informative and opinionated indicators, and b) selecting approximately 10 tweets they found the most interesting as a set in the context of the news arti-cle. We did not ask the annotators to rate tweets for the authority and popularity indicators due to the difficulty of assessing these values at the tweet level and without con-text. Instead, we opted for a simple mechanical model for deriving the values of these indicators, as we will explain later in this section.

For the informative and opinionated indicators, the anno-tators were asked to assign a score to each tweet on a scale from 1 (the tweet decidedly does not exhibit the indicator) through 2 (the tweet somewhat exhibits the indicator) to 3 (the tweet decidedly exhibits the indicator), while allowing them to skip tweets that were either spam or not relevant to the article. To strengthen the gold standard collection, each task was assigned to three different annotators. Due to time constraints, the number of ratings performed by annotators varied. In total, 28,055 ratings were gathered, of which ap-proximately 70% were 1 s, 15% were 2 s, and 15% were 3 s. In other words, in aggregate, 70% of the tweets were not considered informative nor opinionated by the annotators.
As previously mentioned, we did not ask annotators to rate tweets for the authority and popularity indicators. First, the authority indicator is not straightforward to assess with-out context: while identifying major news outlets as author-itative (e.g., AP, Reuters, BBC, etc.) might be trivial, doing so for individuals is more nuanced. Second, the popularity indicator is directly related to how often a tweet is retweeted and replied to, which is difficult to assess when seeing a tweet by itself. Therefore, we opted for a simple mechanical model for both: for the authority indicator we combined the values of user authority and topic authority features of the users writing the tweets, and for the popularity indicator combines the retweet and reply counts for each tweet. We measured the inter-annotator agreement for all three tasks using Cohen X  X  linearly weighted kappa statistic [8]. We averaged the pairwise kappa values of all possible com-binations of annotators that had overlapping tweets they had rated, obtaining the overall kappa. Because the annota-tors had different scoring behaviors, i.e. they used different amounts of 1 s, 2 s and 3 s from each other, the maximum attainable kappa was less than 1.0. To place the kappa into context, we normalized it by dividing the obtained kappa by the maximum obtainable kappa. Even though the defi-nition of what constitutes a  X  X ood X  kappa varies across the literature, a higher kappa always indicates higher agreement. Overall, the annotators had a relatively high agreement on opinionated tweets with  X  cwopi = 0 . 61, while only fair agree-ment of  X  cwint = 0 . 35 for interesting tweets and just slight agreement of  X  cwinf = 0 . 20 for informative tweets. The agreement on the latter two indicators indicates a high sub-jectivity. The annotators approached the content with differ-ent background knowledge, which may have contributed to what they themselves considered informative. As an exam-ple of inter-annotator (dis)agreement, we show three tweets sent in response to an article about Japanese citizens return-ing cash they discovered in the rubble after their town was hit by a tsunami: The annotators agreed that the first tweet was interesting, possibly due to the personal touch of this well-written tweet. They also agreed that the second tweet was not interesting, presumably because it repeated the title of the news arti-cle. However, the annotators disagreed on the last tweet, where one annotator may have agreed with the conveyed idea, whereas another may have considered it someone X  X  per-sonal opinion and therefore not of interest. The latter case clearly exemplifies that rating tweets is a subjective matter. Using the gold standard annotations, we evaluate the meth-ods on predicting interestingness. We also evaluate the per-formance on opinionatedness and informativeness in order to estimate their difficulty.
 Methods: We evaluated the baseline methods diversity , dwfg and lr+ as presented in Section 3.4 against our pro-posed method svr entropy and its two variations svr and entropy . We also experimented with other scoring mecha-nisms for individual messages, such as using the likelihood estimates from a logistic regression classifier for probabilis-tic scoring, and with other diversity mechanisms at the col-lective level, such as using more sophisticated natural lan-guage processing methods, like group-wise textual entail-ment. However, these alternatives did not show any signif-icant benefit and we therefore do not include them in our evaluations.
 Training: The methods that required training were trained with negative and positive judgments. For interestingness positive examples are those included in the interesting set by the annotators for each article, and all others are negative. For opinionatedness and informativeness positives and neg-atives are respectively ratings 1 and 3 from all annotators on all articles. We randomly sampled the tweets to ensure a balanced distribution of positive and negative judgments. For the dwfg we labeled all sentences of the article as pos-itive , since we can consider a news article to be a summary of a newsworthy event.
 Evaluation measures: To measure performance, we com-puted both the rouge-2 [19] and F 1 scores for the infor-mative , opinionated and interesting indicators. These scores measure how closely the output of a method is to the  X  X deal X  output, i.e. those scored positively by the annotators. While F 1 only considers exact tweets to be correct, rouge-2 counts tweets with similar content as a partial match, reporting the recall of word bigrams of the obtained sample and the gold standard examples. We performed 10-fold cross-validation, using nine folds for training (when needed) and one for test-ing.
 Results: Table 2 contains the summary of our experimental results. While our proposed methods outperform the diver-sity , dwfg and lr+ baselines on the main task of predict-ing interestingness, the two evaluation metrics show different rankings of the methods.  X  For interestingness , the best rouge-2 performance was obtained by methods that have an entropy selection com-ponent ( entropy , svr entropy ), while F 1 favors svr based approaches.  X  For opinionatedness , we observed that while the rouge-2 scores were very similar across entropy and svr entropy , F 1 scores were again higher among svr -based approaches.
Although the margin was not statistically significant, the lr+ baseline was able to achieve the highest rouge-2 performance, likely due to enriching the feature set with the features of the most similar sentence. However, even though rouge-2 measured that the lexical overlap with the correct messages was high, F 1 scores show that the actual correct messages were not selected as often.  X  For informativeness , the overall results were lower than for other indicators. This result matched the lower inter-annotator agreement of the ratings. Upon inspection we noticed that for the most part, informative tweets did not contain article snippets, but instead contained novel text.
We also noted the same pattern of rouge-2 favoring en-tropy , while F 1 favoring svr .
 Table 2: Comparison of the tweet sets generated by the various methods in terms of their informative-ness, opinionatedness and interestingness as mea-sured by ROUGE-2 and F 1 at k = 10 . The asterisk marks the case where the measurement was signifi-cantly higher than all of the three baselines, having a T-test p-value below 0.05.

To summarize, the entropy approach excels at content fragment retrieval ( rouge-2 ), whereas approaches combin-ing a scoring and an entropy selection model are able to provide better performance on F 1 , offering a tunable trade-off between diversity and individual tweet scores.

We also noticed some key differences in behavior of the baselines we compared against. Whereas the dwfg method ensures content diversity by encoding the diffusion network into the factor graph, so that retweets do not get selected when the original tweet does, in the news domain many of the near-duplicates tweets were actually not retweets at all: when responding on the same article, several users posted very similar responses independently, resulting into lower performance on rouge-2 . While this is not captured by the dual-wing factor graph design, it is implicitly taken care of in diversity , where it diversifies the tweet content repre-sented with a set of topics [10]. However, the original versity scenario focused on sampling from tweet sets that are retrieved via a hashtag query, which are still diverse enough to be accurately described by topics. On the other hand, news responses tend to be topically narrower, requir-ing finer-grained content representation, favoring our diver-sity representation. Given the gold standard annotations, we aimed to infer to what extent the individual interestingness of a tweet can be decomposed into the individual indicators and how much in-fluence each of the indicators had. In this investigation, we averaged the ratings received or computed for the individual indicators. We used the gold standard labels for the informa-tive , opinionated and interestingness indicators as well as the automatically derived labels for the popularity and authority indicators in the same fashion as in Section 4.2. Authority was estimated by combining the features of topic author-ity and user authority. Popularity was derived by combining the number of retweets and replies for a given tweet. Af-ter normalizing indicators to the same interval, we fitted a least-squares linear model and obtained the following linear combination, supported by a coefficient of determination R of 0.38, having a correlation coefficient of 0.62: int = 0 . 60  X  inf + 0 . 29  X  opi + 0 . 03  X  pop + 0 . 10  X  aut (6) While this simplified model explains only 38% of the vari-ance, it gives us a glimpse of what constitutes  X  X nteresting-ness X . The primary driver is informativeness : while our an-notators had low agreement on how informative a tweet is, having at least one of them mark a tweet as such was suffi-cient for the signal to get picked up. The second driver was opinionatedness followed by authority and popularity . The low coefficient for popularity can also be explained by the fact that it strongly correlates with the authority indicator, meaning that the majority of the effect is already explained by authority. While we already have shown a promising approach com-pared to existing literature, we conducted further experi-ments in order to better understand the differences within the variations of the proposed approach and to understand the extent to which certain factors contribute to the interest-ingness of a set of tweets. We performed a second evaluation in which we used another team of annotators, none of whom had previously participated in the gold standard collection, using the crowdsourcing service CrowdFlower.

We evaluated the satisfaction of the annotators with the sets of tweets ultimately produced for the same collection of 45 news articles. Instead of asking the annotators to as-sign a numeric score to each set of tweets, we paired the methods and placed them in a head-to-head competition, where we requested the annotators to express preference for either set or to indicate whether they were equivalently in-teresting. This approach allows us to extrinsically evaluate the quality of the sets produced by each of the methods, rather than having the annotators score each set separately or pick from multiple sets at once, both of which could have given ambiguous results. Unlike ratings, comparative judg-ments generate highly reliable responses and clear individual differences. In order to limit the number of pairwise compar-ison evaluations, we chose to compare our preferred method svr entropy against diversity and dwfg , the principal methods proposed by the authors in their respective papers.
We requested at least five judgments for each task and appropriately mixed in trivial gold standard scenarios to fil-ter out malicious annotators. We selected judgments from annotators who had at least 75% precision on the gold fil-ter questions. After expressing a preference, the annotators were presented with a follow-up question to indicate why they had chosen one set of tweets over the other, allowing us to compare their preferences with the ratings the previous annotators had given to see what kind of, if any, correlations exist between the individual tweet and collective selection indicators, and the interestingness of the sample. Once again, we first analyze the agreement between annota-tors and then look at the outcome of the evaluation to see which method performed better than the others. Finally, we take a closer look at what constitutes an  X  X nteresting X  selection of tweets. We have structured the analysis in the following two scenarios: Combining diversity and scoring: We are interested in understanding whether diversity alone or scoring alone is enough for achieving an interesting selection.
 Comparison against baselines: Using our best perform-ing method so far, we are interested in knowing how it com-pares to existing baselines on the same problem. We aggregated the preference counts of the annotators and show the results in Table 3. We conducted a  X  2 statisti-cal test over the aggregate counts with the null hypothesis that both of the methods in a given pair are equivalent. We observed that a combination of diversity and scoring ( svr entropy ) produced better selections than diversity ( entropy ) or scoring ( svr ) alone. When evaluated sepa-rately, scoring ( svr ) was better than diversity ( entropy considering that in our dataset the tweets were already rela-tively diverse due to the prefiltering of duplicate tweets, scor-ing proved to be more important. Even though svr entropy was consistently good (but not the best) within the gold standard evaluation, in light of the preference judgment re-sults we consider it as the preferred option in further com-parisons. Looking at the head-to-head comparisons against the baselines, we observed that the diversity approach per-formed similarly to the dwfg approach, whereas both are outperformed by svr entropy .
 Table 3: Comparison of tweet selection methods based on annotator preference votes.
 We measured the inter-annotator agreement for the pref-erence judgments in two ways: we first used Cohen X  X  un-weighted kappa statistic to calculate all pairwise kappas when there was some overlap in the tasks between two anno-tators and we ultimately averaged them to obtain the overall  X  cu of 0.50, having a maximum  X  cumax of 0.81, thus yield-ing a mean normalized  X  cunorm of 0.62, showing moderate agreement. However, since every annotator only judged a limited number of examples and the overlaps were relatively sparse, we also measured a multi-rater Fleiss kappa [14]. Even though the overall  X  f was fair at 0.21, the results var-ied across different comparisons. While the Cohen and Fleiss kappas are not directly comparable, both point to fair-to-moderate overall agreement.
 Table 4: Inter-annotator agreement via Fleiss X  kappa  X  . Scores from 0.00 to 0.40 indicate slight agree-ment, while 0.40 to 0.75 indicate good agreement.

Agreement measurements in Table 4 show that in the cases with higher agreement, the results of the method com-parison were also more clear (with some exceptions). For in-stance, entropy vs. svr entropy demonstrated low agree-ment, but still had a significant difference in overall votes, meaning that the ratio of votes was more constant over all articles, as opposed to there being a particular article where one method would consistently dominate. The cases with moderate agreement, such as svr vs. svr entropy and di-versity vs. dwfg highlight situations where one method can objectively produce better samples. After the annotators had completed a pairwise comparison task, we posed a follow-up question by asking the annotators what they considered the reason for their preference. They answered a multiple-choice question selecting the indicators they considered that contributed to interestingness. Popu-larity and authority were omitted from the selection given that users cannot be expected to infer them from the situa-tion at hand, so both were calculated using the same process as in Section 4.2. If one sample had at least one more pop-ular or authoritative tweet than the other, that indicator was automatically counted as a reason. While this does not enable comparison between importance of individual indi-cators, it does allow for comparisons across various method pairs. Users were also asked to supply additional reasons beyond the pre-defined ones.

Table 5 shows the number of times the annotators stated a particular reason for preferring the selection produced by one method over the other, grouped by the preferred method. The most interesting observation is the prominence of opin-ionatedness in some scenarios: this was the primary driver between distinguishing svr and entropy , demonstrating the effectiveness of having the scoring operate on sentiment and intensity features. On the other hand, informativeness was more difficult to predict, making the samples harder to distinguish on informativeness than on opinionatedness, where differences are more obvious. Authority was less promi-nent in the aggregate. In some cases, the annotators re-marked that they knew the author, suggesting that the social proximity of the author of the tweet to the annotator (and subsequently any reader of the produced selections) serves as a potential personalization opportunity. Other comments given by the annotators outside of the pre-defined choices noted humor as an important factor for preferring one set over the other, which was not modeled by any of the meth-ods, as well as several instances of a set not being preferred due to containing profanity or too much controversy, indi-cating the importance of these features.
 Table 5: Distribution of preference reasons for the tested method pairs.
 We proposed an optimization-driven method to solve the so-cial message selection problem for selecting the most inter-esting messages posted in response to an online news article. Our method first learned the utility of individual responses in isolation and then produced a diverse selection of inter-esting responses by maximizing an objective function that jointly models the responses X  utility scores and the entropy. We proposed a near-optimal solution to the optimization problem that leveraged the submodularity property of the objective function. The intuition behind our work was that an interesting selection of messages contains diverse, infor-mative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions were embodied by a rich set of content, social and user features capturing these aspects.
We compared three variations of our method against state of the art approaches ( diversity and dwfg ) in two exper-imental settings: first, using a gold standard consisting of messages rated by professional annotators and second, using pair-wise comparative judgments obtained via crowdsourc-ing. We found that our method, svr entropy obtains the best overall performance among the tested approaches.
Furthermore, we investigated how various indicators affect interestingness of a message and a message set. Our pre-liminary results based on the collected gold standard data found that for individual messages, informativeness is most important but simultaneously the most difficult to judge, followed by opinionatedness and popularity . We also found that message set interestingness was tied most strongly to opinionatedness and then to informativeness .

There are many directions of future work: incorporating additional message-level or author-level indicators, or (mo-tivated by the low inter-annotator agreement for some indi-cators) focusing on personalized models based on the users X  topical interests or their social circles [5]. Another possi-ble direction is moving from extractive sampling to using methods based on abstractive summarization, rephrasing the contents of the social media posts and enabling shorter, more concise summaries. The real-time nature of social me-dia message streams can also be viewed as an incremental sampling setting, where we cannot assume that entire sam-ple is available ahead of time. Our proposed approach is amenable to extension to online sampling, provided some criterion for substituting already chosen messages with new ones.
