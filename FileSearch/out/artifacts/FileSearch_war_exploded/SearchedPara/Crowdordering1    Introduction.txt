
Toshiko Matsui 1 , Yukino Baba 1 , Toshihiro Kamishima 2 , and Hisashi Kashima 1 , 3 Crowdsourcing offers online marketplaces where specific tasks can be outsourced to a large group of people. With the recent expansio n of the use of crowdsourcing platforms, such as Amazon Mechanical Turk, various professional and non-professional tasks, in-cluding audio transcription, article writing, language translation, program coding, and graphic designing, can now easily be outsourced. The popularity of crowdsourcing is increasing exponentially in computer science as well, and researchers exploit it as an efficient and inexpensive way to process a large number of tasks that humans can per-form much more easily than computers, such as image annotation and web content categorization. Crowdsourcing has been successfully applied to such fields as natural language processing, computer vision, and human computer interaction [1 X 4].

One of the most challenging problems in crowdsourcing research is achieving quality control to ensure the quality of crowdsourcing results, because there is no guarantee that the ability of all workers is sufficient to complete the offered tasks at a satisfactory level of quality. Moreover, it is known that some untrustworthy workers try to receive remuneration while expending as little effort as possible, which results in outputs of no value. Most crowdsourcing platforms allow requesters to check the submitted results and to reject low-quality results; however, if their volume is large, realistically, they cannot all be checked manually.

One popular approach to the quality control problem is to use tasks with known cor-rect answers to evaluate the ability of each worker. This approach has been implemented on several commercial crowdsourcing platforms such as CrowdFlower; however, its us-age is limited because of the high cost of prepa ring the correct answers or the difficulty of determining one unique answer. Another promising approach is to introduce redun-dancy . A single task is assigned to multiple workers, and their responses are aggregated by majority voting [5] or more sophisticated statistical aggregation techniques that con-sider the characteristics of each worker or task, such as the ability of each worker and the difficulty of each task [6 X 8].

In most existing approaches, it is assumed that the tasks are binary questions to which binary answers (e.g.,  X  X es X  or  X  X o X ) are expected, or multiple-choice questions. Only a few methods have been proposed that exte nd the applicability of the aggregation-based quality control approach to more general crowdsourcing tasks [9]. Following the the same line, we consider item ordering tasks , where workers are asked to arrange multiple items in the correct order. Item ordering tasks, typical examples of which are the ranking of web search results and order ing of items in a to-do list in according to their dependencies [10], are frequently posted on crowdsourcing sites.

In this paper, we propose an aggregation-based statistical quality control method for item ordering tasks. We model the generative process of a worker response (i.e., an ordering of items) using a distance-based probabilistic ordering model [11]. The ability of each worker is naturally incorporated into the concentration parameter of the distance-based model. We also present an effective algorithm for estimating the true ordering, which is particularly efficient becau se the Spearman distance [12] is employed as the distance measure between two different orderings of items.
It should be noted that Chen et al. also p roposed a quality control method for item ordering tasks [13] based on a pairwise ranking model; however, their method focuses on finding the correct ordering of a single (large) set of items, whereas our method focuses on solving multiple different (relatively small) ordering tasks simultaneously. Additionally, since their method is based on pairwise comparisons, it is not always suitable for tasks where more than two items are needed to determine their correct order. Fig. 2 shows an example of such a task.

We describe our experiments in which word and sentence ordering tasks were posted on a commercial crowdsourcing marketplace. We compare our quality control method to an aggregation method that does not consider the abilities of workers. The experi-mental results show that our method achieves answers that are more accurate than those of baseline method.

In summary, this paper makes three main contributions: 1. We address the quality control problem for a set of item ordering tasks (Section 2). 2. We propose a generative model of worker responses to item ordering tasks that 3. We introduce an efficient algorithm to e stimate the true ordering from multiple We first define the crowdsourcing quality control problem related to item ordering tasks, where each ordering task requires crowdworkers to place given items in the correct order. We then present a model for aggregating the answers collected from multiple workers to obtain answers that are more accurate.

Let us assume I ordering tasks, whose i -thtaskhas M i items to be ordered. The true order is represented as a rank vector  X  i =( i, 1 , i, 2 ,..., i,M i ) ,where i,j indicates the position of item j of task i in the true order of the items of M i [11]. For example, for a task with five items indexed as 1 , 2 , 3 , 4 , and 5 , whose true order is given (1 , 2 ,...,M i ) .

We resort to crowdsourcing to obtain estimates for the true rank vectors. It is assumed that a total of K crowdworkers is employed. In the following, I ( k ) denotes the indices of tasks on which the k -th worker works, and K i denotes the indices of the workers who k -th worker gives to the i -th item ordering task.

Our goal is to estimate the true rank vectors {  X  i } i  X  X  1 , 2 ,...,I } given the (unreliable) To resolve the issue of the aggregation pr oblem of the crowd-generated answers to item ordering tasks, we present a statistical model of the generative process of worker responses, so that we apply statistical inference to estimate the true order from the observed responses. 3.1 Distance-Based Model for Orders We first review the probabilistic ordering model on which our generative model of crowdworker responses is based. We chose a distance-based model [11] from several variations of the ordering models. A distance-based model gives the probability of a rank vector  X   X  , given a modal order  X  and a concentration parameter  X  , namely, where d (  X  ,  X  ) denotes a distance between two rank vectors, and Z (  X  ) is a normalizing constant given as Specifically, we employ the Euclidean distance (also referred to as the Spearman dis-tance in the ranking model literature) due to its convenience for deriving an effective parameter estimation method, which will be described later. The distance-based model in which the Spearman distance is applied is called the Mallows  X  model [12]. 3.2 Extension of the Distance-Based Model for the Crowdsourcing Setting In crowdsourcing, some workers may have sufficient abilities to provide accurate or-ders, while some are unskilled and often submit wrong orders. To capture such worker characteristics, we incorporate the worker de pendent concentration parameters into the distance-based ordering model. Namely, it is assumed that the k -th worker has his/her own personal concentration parameter  X  ( k ) , and the generative model for the worker is then given as In this model, the answer of a worker wh o has a high concentr ation parameter  X  ( k ) is likely to be an accurate order whose distance from the true order (i.e., the modal order  X  ) is small. Therefore, we can interpret t he personal concentration parameter  X  ( k ) as the ability parameter of the k -th worker. Based on the distance-based crowd-ordering model introduced in the previous section, we introduce a maximum likelihood estimation method to obtain estimates for the true rank vectors as well as the worker ability parameters. Our strategy for optimization is to repeat two optimization steps: optimizing the true rank vector and optimizing the worker ability. 4.1 Objective Function We apply the maximum likelihood estimation to estimate the true rank vector {  X  i } i and the worker ability parameters {  X  ( k ) } k , given the crowd-generated rank vectors {  X  ( k ) i } i,k . The objective function for the maximization problem is the log-likelihood function L , given as 4.2 Optimization Our strategy for optimizing the objective function (1) w.r.t. {  X  ( k ) } k and {  X  i } i is to a convex function, and therefore, its solution depends on the initial parameters, we start with the solution assuming all workers have equal abilities, specifically,  X  ( k ) =  X  (  X  ) for an arbitrary pair of k and  X  .

One major virtue of our model is that the optimization problem is decomposable with respect to each worker and task, that is, each small optimization problem solved at each iteration step depends always on one single variable (a worker ability or a mode order), so that the computational cost linearly is dependent on the numbers of workers and tasks.
 Optimization w.r.t. True Rank Vectors. Given that all the worker ability parameters {  X  ( k ) } k are fixed, the true rank vectors {  X  i } i are obtained by maximizing the first term of the objective function (1). Optimization with respect to {  X  i } i is a combinatorial op-timization problem that is often computationally hard to solve; however, we are able to solve it efficiently by employing the Spear man distance as the distance measure d (  X  ,  X  ) .
The optimal true rank vector  X  i for task i is given as follows 1 . First, for each item m (= 1 ,...,M i ) , we calculate a weighted rank w i,m , which is a weighted mean of the ranks given by workers weighted by the worker abilities, The maximum likelihood estimator of the true item ordering is given by sorting the obtained independently of the others.
 Optimization w.r.t. Worker Ability Parameters. Optimization with respect to the worker ability parameters  X  ( k ) with fixed true rank vectors {  X  i } i is performed by nu-merical optimization. The objective function (1) is represented as the sum of the differ-ent objective functions { J ( k ) } k ,where J ( k ) for each k is defined as tion problem with only one variable.

Since only a single variable function J ( k ) (  X  ( k ) ) needs to be considered to optimize  X  ( k ) , the optimization is easily performed by applying a standard optimization method. In the experiments, we employed a simple gradient descent method. We collected two crowdsourced datasets, one for word ordering tasks, and the other for sentence ordering tasks. We experimentally evaluated the advantages of our model as compared to a baseline method. 5.1 Datasets We collected two datasets using Lancers 2 , which is a general purpose crowdsourcing marketplace. Table 1 gives the general statistics of the datasets.
 Word Ordering. Word ordering is a task whose objective is to order given English words into a grammatically correct sent ence. The word ordering problem can be a subproblem of machine translation between languages with different grammatical word ordering, such as English to Japanese translation. Although several methods have been proposed to solve this ordering problem [14], computer programs still cannot easily per-form this task. However, humans, especially the native speakers of the target language, can skillfully perform the word ordering tasks. The workers were given an English sen-tence with five or six randomly shuffled words, and asked to correct the order of the words. An example of the task is given in Fig. 2. Since we had the correct order of each sentence as the ground truth, we could evalu ate the accuracies of our estimation results. Sentence Ordering. Sentence ordering is a task in which given sentences are ordered such that the aligned texts logically make sense. It emulates several tasks that we pre-sume are posted in crowdsourcing marketpl aces, for example, to revise a piece of writ-ing such that its focal point is emphasized more clearly ,or ordering items in a to-do list by their dependencies [10]. In each sentence ordering task, a paragraph consisting of five or six sentences whose order was permuted was presented to the workers, and they were requested to arrange the sentences correctly. Fig. 3 shows an example of the sentence ordering task. 5.2 Results We applied our method to the two crowd-gene rated datasets, and calculated the Spear-man distance (i.e., the squared error) between each estimated rank vector and the ground truth rank vector. We also tested a baseline method that does not consider the workers X  ability. Concretely, we fixed the worker ability parameter  X  ( k ) =1 for all workers k , and then optimized the objective function (1) with respect only to {  X  i } i . It should be noted that our proposed method uses the solu tion of this baseline method as the initial parameters.

The number of workers involved in each task directly affects the monetary cost of posting tasks to an actual crowdsourcing ma rketplace. In order to investigate the im-pact on the estimation accuracy engendered by the number of workers assigned to each task, we randomly selected n (ranging from 3 to 15 ) workers from the all workers for each task, and only used the responses of the s elected workers for the estimation. We examined the averaged estimation errors of 50 trials. The results are shown in Fig. 4.
In the word ordering task, our proposed method drastically reduced the estimation er-ror of the baseline method when the number of workers assigned to each task was more than four. It is worth mentioning that the averaged squared error of our method was only 0 . 902 when all the collected responses were used, while the squared error easily reached 2 . When the order of a pair of items that were adjacent in the correct order were incorrectly estimated, the squared error was 2 . For example, a rank vector was that our method reduces the number of such errors by approximately half.

Our method outperformed the baseline method in the sentence ordering task as well, when the number of workers assigned to each task was more than five. Since the sen-tence ordering task is generally more difficu lt than the word ordering task, the averaged estimation errors of both the proposed and baseline methods in the sentence order-ing task increased as compared with those in the word ordering task. The best result achieved by our method was a squared error of 4 . 25 , which is relatively large; however, considering the expected squared errors when using random guessing is 21 . 2 , it can be said the result is acceptable. In addition to the Spearman distance, we compared our method and the baseline method in three different measures shown in Table 2. The re-sults in all the measures demonstrated the performance improvement of our method in both the word ordering and sentence ordering tasks.
Fig. 5 shows the relations between the estimated worker ability parameters {  X  ( k ) } k and the averaged squared errors of each worker (against the ground truths). These re-sults show that the true worker ability (i.e., the worker error versus the ground truths) certainly varies from person to person, and that the proposed method gives higher weights to superior workers, which explains its improved performance. In fact, the estimated worker abilities and the worker errors showed strong negative correlations of  X  0 . 853 and  X  0 . 695 for the word ordering tasks and the sentence ordering tasks, respectively.

Finally, we mention the scalability of our proposed method. Generally, estimated orders show convergence after five to ten iterations. The ability parameters for good workers require more iterations than those for inferior workers. As discussed before, the complexity of each iteration depends lin early on the numbers of workers and tasks.

In summary, we verified that the proposed method shows clear advantages as com-pared to the baseline method for estimating the correct orders in both the word ordering and sentence ordering tasks. We also confirmed that the proposed method precisely estimates worker ability. One of the fundamental challenges in crowd sourcing is controlling the quality of the obtained data. Crowdworkers are rarely tra ined and they do not necessarily have ade-quate ability to complete the tasks [3]. There also exist large differences between the skills of individual workers. Moreover, a number of malicious workers participate in crowdsourcing [15]. They are motivated by financial rewards and try to complete the tasks as quickly as possible with the minimum effort by providing illogical submissions.
A widely used approach is to obtain multiple submissions from different workers and aggregate them by applying a majority vote [5] or other rules. Dawid and Skene ad-dressed the problem of aggregating the me dical diagnoses of multiple doctors to achieve more accurate decisions [6]. Smyth et al. app lied the method to the problem of inferring the true labels of images from multiple noisy labels [16]. Whitehill et al. explicitly mod-eled the difficulty of each task [7], and We linder et al. introduced the difficulty of each task for each worker [8]. The usage of these methods is limited to the tasks that consti-tute binary or multiple-choice questions; however, the tasks in crowdsourcing comprise varied types of questions. A few methods have been proposed to extend the applicabil-ity of the aggregation-based quality control approach to more general crowdsourcing tasks [9].

Although the probabilistic models for ranki ng have been widely studied [11], only a few studies in the literature focused on item ordering tasks in the context of crowdsourc-ing. Chen et al. proposed a quality control m ethod for item ordering tasks [13] based on a pairwise ranking model; however, their method aims to find the correct ordering of a single, large set of items, while our method focuses on solving multiple differ-ent (relatively small) ordering tasks simultaneously. Additionally, since their method is based on pairwise comparisons, it is not always suitable for tasks where more than two items are needed to decide their correct pos itions. Wu et al. also employed the general distance-based model in the context of learning to rank from multiple annotators [17], while our approach employs a more specific dist ance measure, i.e., Spearman distance, so that the inference is mo re simple and efficient. We addressed the problem of quality control for item ordering tasks in crowdsourc-ing, where multiple workers are asked to perform each task, which consists of posi-tioning given items in the correct order. By extending a distance-based probabilistic ordering model to incorpor ate the ability of each worker, we built our proposed method for aggregating the collected orders to obtain more accurate orders in a setting where variability in the workers X  abilities exists. We also introduced an efficient algorithm to estimate the true orders that employs the Spearman distance as the distance measure in a distance-based ordering model. Experimental results on two kinds of crowdsourc-ing tasks, word ordering tasks and sentence ordering tasks, showed that our method successfully achieved more accurate orders than the baseline method, which does not consider the worker X  X  ability.
 Acknowledgments. Y. Baba was supported by the Funding Program for World-Leading Innovative R&amp;D on Science and Technology (FIRST Program).

