 Neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high di-mensional data to a low space. This paper begins by ex-plaining the algorithmic fundamentals of techniques for is o-metric data embedding and derives a general classification o f these techniques. We will see that the nearest neighbor ap-proaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neigh-borhood graphs and, consequently, may cause an algorithm fail to project data to a single low dimensional coordinate system. In this paper, we review three existing methods to construct k -edge-connected neighborhood graphs and pro-pose a new method to construct k -connected neighborhood graphs. These methods are applicable to a wide range of data including data distributed among clusters. Their fea-tures are discussed and compared through experiments. Categories and Subject Descriptors: H.2.8 [Database Management] : Database Applications  X  Data mining ; I.5.1 [Pattern Recognition] : Models  X  Geometric, Sta-tistical ; G2.2 [Discrete Mathematics] : Graph Theory  X  Graph algorithms General Terms: Algorithms, Experimentation Keywords: Data embedding, dimensionality reduction, manifold learning, graph connectivity
Traditional methods for feature extraction and dimension-ality reduction typically assume that the data reside on a hyperplane in high dimensional space. This assumption is usually too restrictive in many applications where data dis -tribute on nonlinear manifolds in high dimensional space. The problem of data embedding is defined as follows: given a set of high dimensional data points, project them to a lower Euclidean space so that the resulting configuration performs better than the original data in further processin g such as clustering, classification, indexing, and searchin g. As a generic approach for dimensionality reduction, data embedding has applications in many areas where data are usually assumed as distributions in high dimensional space . These areas include data mining, pattern analysis, informa -tion retrieval, and multimedia data processing.

A majority of data embedding methods are isometric. In other words, they take a metric space (inter-point distance s) as input and try to embed the data to a low dimensional Euclidean space such that the inter-point distances are pre -served as much as possible. Earlier isometric data embed-ding methods include classical multidimensional scaling [ 2], Kruskal X  X  metric multidimensional scaling (MDS) [11], Sam -mon X  X  nonlinear mapping (NLM) [9], and curvilinear com-ponent analysis (CCA) [3]. A recent advance in isometric data embedding is the use of geodesic distances [1, 14, 18, 24]. In implementation, geodesic distance between a pair of data points is usually estimated by length of the shortest path between the pair, computed by applying Dijkstra X  X  al-gorithm or Floyd X  X  algorithm, on a neighborhood graph. A neighborhood graph G = ( V, E ) is defined as a graph where the set V of vertices is the set of all data points and every edge in E connects a point to one of its neighbors. Once geodesic distances are estimated, an isometric data embed-ding method can be applied to the estimated geodesic dis-tances to produce a final configuration of data. Example projection strategies include Isomap [18] which applies cl as-sical multidimensional scaling to the estimated geodesic d is-tances, and curvilinear distance analysis (CDA) [14] which applies CCA to the estimated geodesic distances.

An important issue that is common to all methods based on geodesic distances is how to construct a neighborhood graph. Existing methods use one of the following two ap-proaches to define whether two points are neighbors: the first approach connects each point to its k nearest neigh-bors (the k -NN approach); the second approach connects each point to all the points within a pre-defined Euclidean distance  X  (the  X  -neighbor approach).

The success of data embedding depends on how well the constructed neighborhood graph represents the underlying data manifold. One problem with both k -NN and the  X  -neighbor approach is that they do not guarantee connect-edness of the constructed neighborhood graphs. For ex-ample, both approaches perform well when data are uni-formly distributed, and fail to project data when the data are under-sampled or spread among multiple clusters. Us-ing a disconnected neighborhood graph, geodesic-distance -based methods fail to estimate geodesic distances between data points across the disconnected components. Conse-quently, the data can not be projected into a single lower dimensional coordinate system.

Another closely related problem is how to choose a proper value of the parameter k or  X  . If the parameter were cho-sen too small, the neighborhood graph would not be con-nected. If it were chosen too large, on the other hand, a so-called  X  X hort-circuit X  problem [1] would occur where th e constructed neighborhood graph may contain short-circuit edges that do not follow the manifold. The choice of a proper value of k or  X  reflects a fundamental problem: what is global and what is local. For many applications, there may not ex-ist such a value of k or  X  to avoid both problems.
In this paper, we present methods to construct connected neighborhood graphs. We will review three existing methods and give a new one. The three existing methods construct k -edge-connected neighborhood graphs, the new method con-structs k -connected neighborhood graphs. The first method, called k -MST, was reported in [20]. It works by repeatedly extracting minimum spanning trees (MSTs) from the com-plete Euclidean graph of all data points. The rest three approaches use greedy algorithms. Each of them starts by sorting all edges in non-decreasing order of edge length. Th e second method, referred as Min-k -ST [23], works by finding k edge-disjoint spanning trees the sum of whose total lengths is a minimum. The third method, denoted as k -EC [22], works by repeatedly adding each edge in non-decreasing or-der of edge length to a partially formed neighborhood graph if end vertices of the edge are not yet k -edge connected on the graph. The fourth method, denoted as k -VC, is a new approach proposed in this paper. It works in a similar way as k -EC. However, it adds an edge to a partially formed neigh-borhood graph if two end vertices of the edge are not yet k -connected on the graph. Therefore, the k -VC approach guarantees that the set of edges obtained is a set of shortest possible edges to keep the neighborhood graph k -connected. Many data embedding algorithms have been developed. However, these existing algorithms are more or less describ ed separately in the literature. To have an overview of these algorithms, Section 2 identifies fundamental techniques fo r isometric data embedding. The resulting framework is used to systematize existing data embedding algorithms. In Sec-tion 3, we review the three existing methods to construct k -edge connected neighborhood graphs and present the new method. We discuss in Section 4 results of experiments and compare these methods for constructing neighborhood graphs. We conclude this paper with a summary of these methods and a short discussion of future work.
Many data embedding methods are isometric. Among these methods, the simplest one is a linear method. Let X = [ x 1 , . . . , x n ] be a p  X  n matrix of coordinates of n points in p -space. Given distance between every pair of points, a covariance matrix X T X can be obtained. A projection of the n points can be defined as Y =  X  1 / 2 V , largest eigenvalues of X T X and the rows of V are the eigen-vectors of X T X corresponding to the eigenvalues  X  1 , . . . ,  X  This linear embedding method is commonly referred as clas-sical multidimensional scaling (classical MDS) [2].
There are many nonlinear approaches, including the tetra-hedral method [19], for isometric data embedding. Some of them use iterative optimization. These algorithms include Kruskal X  X  metric multidimensional scaling (MDS) [11], Sam -mon X  X  nonlinear mapping (NLM) [9], and curvilinear com-ponent analysis (CCA) [3]. These iterative algorithms work in similar ways: Each algorithm starts with a random con-figuration of points in the destination q -space. It uses an error measure that is defined as a function of differences between input distances and the corresponding distances in the q -space. The algorithm iteratively reconfigures the coor-dinates of points in the q -space (usually through calculating the steepest gradient descent of the error measure) to mini-mize the error measure. The algorithm usually stops when the error measure falls below a user-defined threshold, or the number of iterations exceeds a user-specified limit.
These algorithms differ from each other by the error mea-sures they use and by the ways they reconfigure points in each iteration. Let d  X  ij represent the distance between a pair of points i and j in the original p -space, and d ij represent the distance between the projections of i and j in the des-tination q -space. The error measures used by MDS, NLM, and CCA are summarized in Table 1. We can see from Ta-ble 1 that NLM follows closely MDS. In fact, Kruskal[12] has demonstrated how a configuration that is similar to NLM could be produced from MDS. The difference between E
MDS and E NLM is that each term of E NLM is normal-ized by d  X  ij . Therefore, NLM emphasizes the preservation of short distances. That is the reason why NLM has an effect of unfolding a data manifold. Niemann and Weiss [16] have further used 1 ror measure and proposed a way to improve the convergence of NLM. The error measure becomes E NLM when p =  X  1 and becomes E MDS when p = 0. A value p &lt; 0 prefers the preservation of short distances. A value p  X  0 prefers the preservation of long distances. Compared with E MDS and E NLM , E CCA uses a weight function F with a parameter  X  . F has to be non-increasing to d ij to make CCA favor the preservation of short distances. The function F used in [3] is defined as a binary function: F ( d ij ,  X  ) = 1 if d ij F ( d ij ,  X  ) = 0 if d ij &gt;  X  . It makes CCA to completely ignore distances longer than  X  . CCA has been reported successful to unfold highly twisted data manifolds.

The success of using these algorithms for data embedding depends on the quality of input data. Being used alone, these algorithms usually take Euclidean distances as input . In the case that the data are distributed on a twisted man-ifold, however, Euclidean distance is not a good measure of dissimilarity between data points. A significant recent dev el-opment in this area is the use of geodesic distances [1, 14, 18 , 24]. Intuitively, geodesic distance between a pair of point s on a manifold is the distance measured along the manifold. In implementation, geodesic distance between a pair of data Table 2: Systematization of isometric data embed-ding algorithms points can be estimated by graph distance, that is, the dis-tance along the shortest path, between the pair on a neigh-borhood graph that connects every point to its neighbor points. As the number of data points increases, the graph distance is expected to converge to the true geodesic distan ce asymptotically. Once geodesic distances are estimated, an isometric data embedding algorithm can be applied to the estimated geodesic distances. Example algorithms include Isomap [18], CDA [14], and GeoNLM [21]. Isomap uses clas-sical MDS, CDA uses CCA, and GeoNLM used NLM. These algorithms and their relationships are shown in Table 2.
As long as good estimations of geodesic distances are ob-tained, all the above algorithms would perform well and there is no particular reason to choose one over the other. CDA and GeoNLM have ever been reported outperforming Isomap. However, the performance gains are simply be-cause the underlying CCA and NLM did better than classi-cal MDS to compensate badly estimated geodesic distances (especially the short ones) when the data manifold is highly twisted or folded. The construction of a quality neighbor-hood graph is the most important step in geodesic distance estimation and isometric data embedding.
A neighborhood graph over a set of data points distributed on a manifold should be constructed in such a way that: (1) it contains only short edges so that it follows the manifold; (2) its connectivity is guaranteed so that the geodesic dis-tance between every pair of data points can be estimated; (3) it has multiple edge connections between any partitions of the graph so that there are multiple choices of paths be-tween a pair of points across the partitions. An obvious choice for such a graph is a k -connected or k -edge-connected minimum spanning subgraph of the Euclidean graph of all data points. When k = 1, the problem of finding such a sub-graph becomes finding the minimum spanning tree, which can be trivially solved. When k  X  2, however, the problem is NP-hard [7, Problem GT31]. This is easy to understand from the fact that, when k = 2, the problem is reduced to the classical traveling salesman problem.

Geodesic distance estimation does not require a neigh-borhood graph to be strictly minimal spanning. Therefore, we change our objective to find a neighborhood graph that is k -edge-connected or k -connected and that can be com-puted efficiently. In this section, we briefly review three existing methods and the corresponding algorithms ( k -MST [20], Min-k -ST [23], and k -EC [22]) for constructing k -edge-connected neighborhood graphs. We will then present a new algorithm k -VC which constructs k -connected neighborhood graphs. These algorithms are summarized in Table 3
For the illustration purpose, Figure 1(a) shows a synthetic data of 1,000 points randomly distributed on a 4  X  1 rectangle which is then wrapped into a Swiss roll. Figure 1(b) displays its 5-NN ( k = 5) neighborhood graph superimposed on the data. Figure 1(b) also illustrates the shortest path betwee n an example pair of data points, A and B . Geodesic distance between the points A and B is estimated as the length of the shortest path.

The first algorithm, k -MST [20], constructs a k -edge con-nected neighborhood graph by repeatedly extracting mini-mum spanning trees (MSTs) from the Euclidean graph of all data points. Algorithms to extract MSTs have been well studied (see [8] for a survey). k -MST works in the follow-ing way: Let G = ( V, E ) denote the complete Euclidean graph of all data points where the weight of each edge is the Euclidean distance. Let MST 1 denote the set of edges of an MST of G , MST 1 = mst ( V, E ). Let MST 2 denote the set of edges of an MST of G with MST 1 removed, that is, MST 2 = mst ( V, E  X  MST 1 ). Similarly, let MST i denote the set of edges of an MST of G with  X  i  X  1 j =1 MST j removed, MST i = mst ( V, E  X   X  i  X  1 j =1 MST j ). k -MST constructs the neighborhood graph as ( V,  X  k i =1 MST i ).

Time complexity of Kruskal X  X  classical MST algorithm [10] is O ( | E | log | E | ) in the worst case and is often close to O ( | E | + | V | log | E | ) in the average case. For complete graphs where | E | = | V | ( | V |  X  1) / 2, this average case time complexity can be simplified as O ( | V | 2 ). Therefore, k -MST has a time complexity of O ( k | V | 2 ) in the average case. This time complexity is the same as the time complexity of k -NN and the  X  -neighbor approach.

The rest three algorithms (Min-k -ST, k -EC, and k -VC) can be classified as greedy algorithms and work in similar ways. Let G = ( V, E ) denote the Euclidean graph of all data points where the weight of each edge is the Euclidean distance. A greedy algorithm starts by sorting all edge in non-decreasing order of edge length. It initializes an empt y edge set F and repeatedly adds each edge e  X  E , in non-decreasing order of edge length, to F if F does not satisfy the corresponding connectivity criterion. After all edges are pro-cessed, the resulting set F of edges would be a set of shortest edges (because edges are processed in a non-decreasing or-der of edge length) to satisfy the corresponding criterion o f graph connectivity.
 Min-k -ST finds k spanning trees of minimum total length. Please note that the property of having k trees spanning a pair of vertices is an equivalence relation. k -EC builds a k -edge-connected neighborhood graph. The property of k -edge connectivity between vertices in an undirected graph i s also an equivalence relation. For Min-k -ST and k -EC, there-fore, performance can be improved by avoiding the test of the corresponding connectivity criterion for a coming edge whose end vertices are known within an equivalence class. If two end vertices of the coming edge belong to different classes and the edge cannot be added to the resulting neigh-borhood graph, the equivalence classes connected by the edge are merged. The algorithm that is used for Min-k -ST and k -EC is given in Algorithm 1. It uses a min-heap to avoid sorting all edges at the beginning. It continues until all vertices in the graph are in a single equivalence class. The algorithm is similar to Kruskal X  X  MST algorithm [10], which is not surprising because the latter is also a greedy al -gorithm taking advantage of the equivalence relation of edg e connectivity.
 Algorithm 1 Greedy algorithm for Min-k -ST and k -EC Input: G ( V, E ), a complete Euclidean graph Output: G  X  ( V, F ), a k -edge connected spanning subgraph 1: F =  X  ; 2: nc = | V | ; 3: Assign each v  X  V a unique equivalence class, class ( v ); 4: Build a min-heap H of edges in E using edge length as 5: while nc &gt; 1 do 6: ( a, b ) = remove-min( H ); 7: if class ( a ) 6 = class ( b ) then 8: if F and { ( a, b ) } meet the connectivity criterion 9: F = F  X  { ( a, b ) } 10: else 11: Merge class ( a ) and class ( b ); 12: nc = nc  X  1; 13: end if 14: end if 15: end while
Min-k -ST [23] finds a set of k edge-disjoint spanning trees the sum of whose total edge lengths is guaranteed to be a minimum. In Min-k -ST, the set E of edges of G form a matroid if we define a set F of edges to be independent if and only if F can be partitioned into k forests. Therefore, we can use the matroid greedy algorithm [13] to construct a neighborhood graph. After we run Algorithm 1, F is a basis (maximal independent set) of minimum total length. In other words, F is the union of k edge-disjoint spanning trees the sum of whose total lengths is a minimum.
The hard part of the Min-k -ST matroid greedy algorithm is at line 8 in Algorithm 1, that is, to test the independence of F  X  { ( a, b ) } . To do this we maintain a partition of F into k edge-disjoint forests F 1 , . . . , F k . When an edge e added, the k forests are updated to accommodate e 0 .
The idea of updating the k forests to accommodate an edge e 0 is to find an updating sequence h e 0 , e 1 , . . . , e that e 0 replaces the edge e 1 in F 1 , e 1 replaces the edge e 2 in F 2 , and so on, until e n can be inserted into a for-est F ( n mod k )+1 where the end vertices of e n are in differ-ent trees. The updating sequence can be found by using a breadth-first search: Starting from e 0 , we test whether e spans two trees in F 1 . If not, we test whether any edge in the loop created by e 0 in F 1  X  { e 0 } spans two trees in F not, for each e 1 in the loop, we test whether any edge in a loop created by e 1 in F 2  X  { e 1 } spans two trees in F so on. We continue this breadth-first search until we find an edge e n that spans two trees in the next forest. Once an updating sequence is found, the k edge-disjoint forests are updated to accommodate e 0 by following the updating sequence. Because each step in the update swaps in an edge to replace another edge in the loop created by the edge, it is guaranteed that each F i remains as a forest after the update.
Computational complexity of Min-k -ST follows that of the Kruskal X  X  algorithm. Building a heap of edges takes O ( | E | ) time, and each call of remove-min() takes O (log E ) time. Updating the k forests to accommodate an edge has an average complexity of O ( k | V | ). Using a set union/find algorithm, the union/find of equivalence classes takes near ly O (1) time. Since there are O ( k | V | ) edges inserted into the resulting graph, the total time complexity of Min-k -ST is | E | = | V | ( | V |  X  1) / 2, the time complexity can be simplified as O ( k 2 | V | 2 ).
 Neighborhood graphs constructed by using k -MST and Min-k -ST have the following properties: (1) Each neighbor-hood graph has exactly k ( | V |  X  1) edges; (2) It is minimally k -edge-connected; (3) If we cut it into two partitions, the cu t edges include the k shortest edges among all edges between the partitions. In particular, the k nearest neighbors of a point are always connected to the point. Therefore, neigh-borhood graphs constructed by using k -MST and Min-k -ST are supergraphs of the corresponding neighborhood graphs constructed by using k -NN.
The same as Min-k -ST, k -EC uses Algorithm 1 to produce k -edge connected neighborhood graphs. The difference of k -EC to Min-k -ST is at line 8 in Algorithm 1. k -EC defines the connectivity criterion as that the pair a, b of vertices are not yet k -edge-connected in F . According to Menger X  X  theorem, this connectivity test can be done by querying whether there are no more than k  X  1 edge-disjoint paths between a and b in F . Such a test can be performed by using network flow techniques [4, 17, 5] by assuming every edge in F has a unit flow capacity. After we run Algorithm 1, therefore, there are at least k edge-disjoint paths between every pair of data points. k -EC guarantees that the resulting edges in F give a set of shortest edges to keep the neighborhood graph k -edge-connected.

The classical algorithm to measure max-flow from a ver-tex to another vertex in a directed graph with flow capacity is the Ford-Fulkerson labeling algorithm [6]. Edmonds and Karp [4] modified the labeling algorithm so that each flow augmentation is taken along a path with the fewest num-ber of edges from the source to the destination. There also exist more efficient network flow algorithms [5, 17] which find flow augmentation paths phase by phase. Because our objective is to test the existence of k paths rather than to find the actual maximum flow, the Ford-Fulkerson label-ing algorithm [6] with the Edmonds-Karp algorithm [4] of finding augmenting paths would be a good choice for the k -connectivity test.
 Specifically, we construct a directed graph ( V, F  X  ), where F  X  is a set of directed edges: for each e  X  F , we have e  X  and and are directed in opposite directions. All edges in F  X  have unit flow capacity. An edge is called useable in its direction if it has no flow. An edge is called usable in its opposite direction if it has a unit flow.
 The network flow algorithm uses breadth-first search on a . The search goes along only usable edges until it reaches b . It is easy to see that this is the simple algorithm for finding the shortest path along useable edges when every edge is assumed having a unit length. A path between a and b is found by backtracking the breadth-first search. Once we found such a path, we augment every edge on the path with unit flow. Such an augmentation makes all the edges on the path not useable in the direction of the path in the next search. The breadth-first search is repeated k times, each one on the network with flows augmented by previous searches. If the k searches are successful, we know that a and b are k -connected in ( V, F ). If any of the k searches fails to reach b , we know that a and b are not yet k -connected and the edge ( a, b ) has to be added to F .

Computational complexity of k -EC follows that of Min-k -ST. Each k -edge-connectivity test takes at most O ( k | V | ) steps because each breadth-first search in the network max-flow algorithm takes at most O ( | V | ) steps. Therefore, the time complexity of k -EC is the same as Min-k -ST. The total time complexity can be expressed as O ( | E | + k | V | log | E | + k | V | 2 ). For a Euclidean graph, the time complexity can be simplified as O ( k 2 | V | 2 ).
A k -edge-connected neighborhood graph may not per-form well for geodesic distance estimation on clustered dat a, where a k -edge-connected graph may be only 1-connected. This section proposes a new approach, k -VC, to overcome this problem. k -VC constructs k -connected neighborhood graphs. It works in a similar way as k -EC. The major dif-ference to k -EC is querying for k -vertex-connectivity versus querying for k -edge-connectivity. Another difference is that k -vertex-connectivity between vertices is not an equivalen ce relation. Special attention has to be paid to reduce the com-plexity of the algorithm.

Similar to k -EC, the core part of the k -VC algorithm is the testing of the connectivity criterion. k -VC defines the connectivity criterion as the pair a, b of vertices are not k -vertex connected in F . According to Menger X  X  theorem, this connectivity criterion can be tested by querying whether there are no more than k  X  1 vertex-disjoint paths between a and b in F . Such a test can be performed using network flow techniques by assuming every vertex in F has a unit flow capacity. This can be done in a similar way as in k -EC, but on a different graph.

Given the undirected graph ( V, F ), we construct a di-rected network flow graph ( V  X  , F  X  ) as follows: for each e  X  F , of e and are directed in opposite directions. Each vertex, other than a and b , has flow capacity of 1. This unit vertex follows: each vertex v is split into two vertices, v  X  and v  X  X  ; a new edge e , which has a flow capacity of 1, connects from v  X  to v  X  X  ; all edges which formerly led to v now lead to v  X  , and all edges which came from v now come from v  X  X  . Clearly, the new edge e and its unit capacity in ( V  X  , F  X  ) specify the unit vertex flow capacity in ( V, F ). The query of the existence of k vertex-disjoint paths in ( V, F ) between a and b can now be translated to a query of the existence of k edge-disjoint
Another difference of k -VC to k -EC is that k -connectivity is not an equivalence relation and, therefore, Algorithm 1 cannot be directly used in k -VC. However, k -connectivity has the property that any two k -connected blocks of ver-tices have no more than k  X  1 vertices in common [15]. In particular, if both vertices a and b are k -connected to a set of more than k  X  1 vertices, a and b must be k -connected. k -VC can use this property to avoid the k -connectivity test between vertices within a k -connected block.

Time complexity of k -VC follows that of the k -EC. The time complexity for sorting the edges is O ( | E | log | E | ). The test of whether a and b are k -connected to a set of at least k vertices takes at most O ( | V | ) time. k -VC has O ( k applications of the k -connectivity test. Therefore, the total time complexity of k -VC can be expressed as O ( | E | log | E | + 1) / 2, the time complexity can be simplified as O ( | V | k | V | 2 ).
Figure 2 shows 2D projections of the synthetic Swiss-roll data in Figure 1 by applying classical multidimensional sca l-ing to the estimated geodesic distances. The corresponding neighborhood graph and two example shortest paths ( A to B and C to D ) are superimposed in each projection to illus-trate how geodesic distances are estimated and are kept in the projections of data points. Classical multidimensiona l scaling makes these projections in such ways that Euclidean distances between points reflect the estimated geodesic dis -tances, in other words, makes the shortest paths as straight as possible. Because neighborhood graphs contain many small holes, long geodesic distances (between A and B , for example) are generally better estimated than short geodesi c distances (between C and D , for example) on the neighbor-hood graphs. This explains why more advanced techniques (such as metric MDS, NLM, and CCA), which emphasize the preservation of short distances, may not outperform the simple classical multidimensional scaling.

To give quantitative comparison, we use residual variance as the error measure. Let E be the matrix of real geodesic distances. Let G be the matrix of estimated geodesic dis-tances. Let d be the matrix of Euclidean distances between projected data points. The residual variance used in Isomap [18] is defined as 1  X  R 2 ( G, d ) where R represents correla-tion coefficient. Using the syntactic Swiss-roll data as the test data, Figure 3 gives basic statistics of data projectio ns and neighborhood graphs constructed by using the five ap-proaches.

Figure 3(a) shows the variance 1  X  R 2 ( G, d ) of using the five approaches as functions of the projection dimension-ality. These projections are produced using neighborhood graphs constructed when k = 5. For each approach, the residual variance decreases as the projection dimensional -ity increases. The intrinsic dimensionality (which is 2 for the synthetic Swiss-roll data) can be estimated by looking for the  X  X lbow X  effect at which the residual variance ceases to decrease significantly with added dimensions. Residual variances are shown in log scale in Figure 3(a), where we can see that k -MST, Min-k -ST, and k -VC produce smaller variances and stronger elbow effects than k -NN and k -EC.
Figure 3(b) reports the residual variance 1  X  R 2 ( E, G ) of geodesic distance estimation by using the five approaches as the value of k increases. It does not give the variances of using k -NN when k &lt; 4. This is because k -NN fails to build a connected neighborhood graph when k &lt; 4. The problem of disconnected neighborhood graph would become more serious for the k -NN approach when data points were distributed among clusters. In contrast, all other approac hes guarantee the connectivity of the constructed neighborhoo d graphs for any positive k value. Residual variances in Fig-ure 3(b) are also shown in log scale. Among these approach, k -VC performs the best in estimating geodesic distances. As the value of k increases, the residual variances become stable and these approaches have similar performances.
 All the four approaches ( k -MST, Min-k -ST, k -EC, and k -VC) presented in this paper outperforms k -NN in the sense that they build connected neighborhood graphs when k is small or the data are distributed among clusters, in which cases k -NN would fail. Among these four approaches, we think k -VC performs the best. It outperforms other ap-proaches for geodesic distance estimation (as shown in Fig-ure 3(b)) and, consequently, produces good results for data projection and intrinsic dimensionality estimation. Furt her-more, k -VC constructs k -connected neighborhood graphs in-stead of k -edge-connected neighborhood graphs. Therefore, k -VC easily outperforms other approaches when data are distributed among clusters. A downside of k -VC is that it has a larger time complexity than other approaches.
We have explained the fundamentals of techniques for iso-metric data embedding and derived a general classification of algorithms for data embedding. We have identified neigh-borhood graph construction as the first and the most im-portant step in techniques based on geodesic distances. We have reviewed three existing methods for constructing k -edge-connected neighborhood graphs and proposed a new method for constructing k -connected neighborhood graphs. Because these methods guarantee the connectivity of the constructed neighborhood graphs, they are applicable to a wide range of data including data distributed among clus-ters. These methods are compared through experiments. Their features are discussed and summarized.

Future work will be on further exploration and applica-tion of the k -VC approach and other methods for geodesic distance estimation. One interesting topic is how to char-acterize the dimensions of projected data. Another topic is how to simplify these methods and to make them work with large data sets. Computational complexity is a big challeng e to bring these methods to practical application.
 This work was supported in part by the US National Sci-ence Foundation under grants IIS-0414857, EIA-0215356, and EIA-0130857.
 [1] M. Balasubramanian, E. L. Schwartz, J. B.
 Figure 2: 2D projections of the Swiss-roll data using classical MDS of the estimated geodesic distances [2] T. F. Cox and M. A. A. Cox. Multidimensional [3] P. Demartines and J. Herault. Curvilinear component [4] J. Edmonds and R. M. Karp. Theoretical [5] S. Even and R. E. Tarjan. Network flow and testing [6] L. R. Ford, Jr. and D. R. Fulkerson. Flows in [7] M. R. Garay and D. S. Johnson. Computers and [8] R. L. Graham and P. Hell. On the history of the [9] J. J. W. Sammon. A nonlinear mapping for data [10] J. Kruskal. On the shortest spanning subtree of a [11] J. Kruskal. Multidimensional scaling by optimizing [12] J. Kruskal. Comments on a nonlinear mapping for [13] E. L. Lawler. Combinatorial Optimization: Networks [14] J. A. Lee, A. Lendasse, N. Donckers, and [15] D. Matula. k -blocks and ultrablocks in graphs. [16] H. Niemann and J. Weiss. A fast converging algorithm [17] R. E. Tarjan. Testing graph connectivity. In Proc. 6th [18] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A [19] L. Yang. Distance-preserving projection of high [20] L. Yang. k -edge connected neighborhood graph for [21] L. Yang. Sammon X  X  nonlinear mapping using geodesic [22] L. Yang. Building k -edge-connected neighborhood [23] L. Yang. Building k edge-disjoint spanning trees of [24] H. Zha and Z. Zhang. Isometric embedding and
