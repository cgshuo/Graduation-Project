 Locality Sensitive Hashing (LSH) is widely recognized as one of the most promising approaches to similarity search in high-dimensional spaces. Based on LSH, a considerable number of nearest neighbor search algorithms have been pro-posedinthepast,withsomeofthemhavingbeenusedin many real-life applications. Apart from their demonstrat-ed superior performance in practice, the popularity of the LSH algorithms is mainly due to their provable performance bounds on query cost, space consumption and failure prob-ability.

In this paper, we show that a surprising gap exists be-tween the LSH theory and widely practiced algorithm anal-ysis techniques. In particular, we discover that a critical assumption made in the classical LSH algorithm analysis does not hold in practice, which suggests that using the ex-isting methods to analyze the performance of practical LSH algorithms is a conceptual mismatch. To address this prob-lem, a novel analysis model is developed that bridges the gap between the LSH theory and the method for analyz-ing the LSH algorithm performance. With the help of this model, we identify some important flaws in the commonly used analysis methods in the LSH literature. The validity of this model is verified through extensive experiments with real datasets.
 H.3.1 [ Content Analysis and Indexing ]: Indexing Meth-ods; F.0 [ Theory of Computation ]: General Locality Sensitive Hashing, Probabilistic Algorithm, Algo-rithm Analysis
Thenearestneighbor(NN)searchinhighdimensional spaces under some distance function is of great importance in areas such as database, information retrieval, data min-ing, pattern recognition and machine learning. When the dimensionality of the data is low, a number of access meth-ods ( R-tree [13], kd-tree [6]) have been shown to perform well in practice. As the dimensionality grows higher (e.g., larger than 50), however, it has long been recognized that these traditional tree-based indexing methods offer little im-provement over a linear scan that compares a query to every point of the dataset [21]. This phenomenon is often called  X  X he curse of dimensionality X .

To overcome the search time bottleneck, there have been several proposals of approximation algorithms that trade precision for speed [1, 4]. In this paper, we focus on one of the most promising approximate NN search algorithm-s called Locality Sensitive Hashing, which has been widely applied in a variety of domains, including web clustering, computer vision and computational biology [3, 17].
We should point out that LSH does not solve approxi-mate NN queries directly, and was originally proposed for a different problem called c -approximate r -near neighbor search (see Definition 1 in Section 2.2) in high-dimensional spaces [16]. The key idea behind LSH is that it is possi-ble to devise hash functions such that points close to each other in some metric space collide with higher probability than do points that are far from one another. If a family H of hash functions satisfies the condition listed in Defini-tion 2 (presented in Section 2.2), H is called locality sensi-tive. To date, several LSH families have been discovered for different similarity (dissimilarity) measures such as Ham-ming distance [16], l s distance [9] with s  X  [0 , 2], Jaccard coefficient [7], Cosine distance [8], etc.

Given a particular metric and the corresponding LSH fam-ily H , an efficient approximate NN search algorithm can be easily devised, which will be discussed in detail shortly. Simply speaking, during a preprocessing phase, a number of hash tables or buckets are created by hashing all points in the dataset using independent hash functions randomly chosen from H . To process a query, one only need to hash the query point and retrieve the elements stored in buckets containing that point.

Thanks to the soundness of the LSH theory, algorithm-s based on LSH are capable of providing, with some con-stant failure probability, excellent asymptotic performance in terms of space usage and query cost. Such theoretical re-sults were originally proved in the influential paper by Indyk and Motwani for their classic LSH algorithm [16], and ever since then, the method of proof developed in their paper has been widely used by the vast majority of other LSH variants, e.g., LSH-forest [5], LSB-tree [20], Multi-probe LSH [18], and C2LSH [11], in order to obtain the corresponding theo-retical guarantees.

Surprisingly, in our experimental study, we observe a sig-nificant deviation between the real and expected perfor-mance of the classic LSH algorithm proposed in [16]. For example, consider one typical experiment shown in Figure 1, where the expected success ratio represents the expected per-formance and the maximum, minimum and average success ratios are the actual best, worst and average performance, respectively 1 . In theory, the actual success ratios, including the maximum, minimum and average, should well match the expected performance [3], but the fact, shown in Figure 1, is that an obvious gap exists between the maximum/minimum and expected algorithm performance (12 percent maximum difference) 2 , which contradicts the results obtained using the widely-accepted analysis method that will be reviewed in Section 2.3.

Clearly, such a significant difference cannot be simply at-tributed to  X  X andom errors X  in the experiment. Instead, as will be discussed in the rest of this paper, this performance gap reveals a subtle problem that has been ignored for years. Motivated by this observation, we carry out a systematic in-depth investigation of LSH with the following important findings:
To avoid blurring the main problem we want to explain, the discussion of the parameters such as k , L , r and  X  in Figure 1 is deferred till Section 2.
In Figure 1, one can also see a nice match between the actual average success ratio and the expected performance, which well confirms our theoretical analysis that will be dis-cussed in Section 3.5.2.
The rest of this paper is organized as follows. A brief overview of LSH and the related work are provided in Sec-tion 2. Section 3 presents the main theoretical results of this paper. An Empirical study to validate our model is de-scribed in Section 4. Finally, we conclude this paper with asummaryofourresultsandabriefdiscussiononfuture work in Section 5.
Locality sensitive hashing is closely related to the problem of nearest neighbor (NN) search in high-dimensional spaces under some distance metric. Thus, before presenting the technical details of LSH, we first give the necessary notations and definitions as to NN search.

We use O to denote a set of data points, and assume all points o i  X  X  belong to a d -dimensional space d .Let o j denote the j th coordinate of o , j =1 , 2 ,...,d .Foranytwo points o and q in O , the distance ( l s norm) between them is defined as follows:
Given a dataset O , the nearest neighbor NN q  X  X  of a query point q under l s norm is defined as follows:
To simplify notations, we often skip the subscript s in the sequel.
NN search is aimed at, given a dataset and a query point, finding a way to (efficiently) report the nearest neighbor of the query. Essentially, the NN search problem is a concrete instance of an optimization problem since its goal is to find the point that minimizes a chosen objective function (the distance to the query point in this case) [3]. It has long been recognized that, for large enough d , most solutions to NN search offer little improvement over a linear scan that compares a query to every point from the dataset. This phe-nomenon is often called  X  X he curse of dimensionality X  [21]. In view of the difficulty in solving this problem directly, Indyk et al. propose to first deal with the r -near neighbor (rNN) search problem, which is originally termed as Point Location in Equal Ball (PLEB) in [16]. rNN search is the decision version of the NN search problem. A point o is called the r -nearneighborofquery q if the distance between o and q is at most r . The purpose of rNN search is to report some r -near neighbor of q if there exists at least one rNN of q . Consider the running example depicted in Figure 2, o 1 the NN of q and both o 1 and o 2 are r -near neighbors of q .
Algorithms supporting rNN search still suffer from the curse of dimensionality. Fortunately, in many areas, approx-imate nearest neighbors (ANN) are also acceptable [1]. Giv-en a query point q , a data point, say ANN q ,iscalledanANN of q under l s norm if it satisfies q  X  ANN q s  X  c q  X  NN where c&gt; 1 is the approximate factor. The decision ver-sion of the ANN search problem, namely, the c -approximate r -near neighbor search problem (previously known as the -PLEB problem [16]), is defined as follows:
Definition 1 ([3]). Given a set O of points in a d -dimensional space d and parameter r&gt; 0 , construct a data structure such that, given any query point q , if there exists an r -near neighbor of q in O ,itreportssome c -approximate r -near neighbor of q in O .

In Figure 2, o 1 and o 2 are obviously the c -approximate r -near neighbors (crNN) of q since their distances to q are less than r . Besides, o 3 is a crNN of q as well because it falls in the outer circle.

It has been shown that the algorithm for crNN search can be used as a building block to solve the ANN search prob-lem [14, 16]. Therefore, finding an efficient solution to crNN search is a pivotal step towards achieving the final goal. In-tuitively, the crNN search problem is much weaker than the corresponding exact version, namely, the rNN search prob-lem. The fact, however, is that devising an efficient deter-ministic algorithm for this problem remains difficult. To this end, by trading certainty for speed, Indyk et al. propose lo-cality sensitive hashing, which provides attractive sub-linear search time at the expense of some constant failure proba-bility.
Locality sensitive hashing is one of the most popular (ap-proximate) NN search algorithms in high-dimensional spaces. The rationale behind this algorithm is that, by using spe-cific hashing functions, we can hash the points such that the probability of collision for data points which are close to each other is much higher than that for those which are far apart.

In the rest of this paper, we use H to denote a family of hash functions mapping d to some universe U .Foranytwo points o and q , a hash function h is chosen from H uniformly Figure 2: A running example for illustrating the conceptsanddefinitionsdiscussedinSection2. at random. If the probability that these two points collide ( h ( q )= h ( o )) satisfies the following condition, the family is called locality sensitive.

Definition 2 (Locality sensitive hashing [16]). A family H of hash functions is called ( r, cr, p 1 ,p 2 )-sensitive if, for any two points o , q  X  d and h  X  X  , it satisfies the fol-lowing:
For an LSH family to be useful, it has to satisfy p 1 &gt;p
Given an LSH family H , the classic LSH algorithm, de-noted by LSH C , works as follows. For parameters k and L , h i,j (1  X  i  X  k, 1  X  j  X  L ) is drawn independently and uni-formly at random from H [16]. For each point o  X  X  ,the bucket g j ( o ) is computed, for j =1 ,...,L ,andthen o is inserted into the corresponding bucket. To process a query q , one has to compute g j ( q ) ,j =1 ,...,L , first, and then retrieve all points that lie in at least one of these bucket-s. For all candidate points retrieved, a filtering procedure is performed to calculate the distance of each point to the query. Basically, there are two principal filtering strategies according to [3]. 1. Interrupt the search after finding the first T points for 2. Continue the search until all points that collide with
These two strategies, combined with the data structure discussed earlier, result in different behaviors of LSH C particular, the first strategy is aimed at solving the random-ized crNN search problem, which is defined as follows.
Definition 3 ([3]). Given a set O of points in a d -dimensional space d ,andparameters r&gt; 0 ,  X &gt; 0 ,con-struct a data structure such that, given any query point q ,if there exists an r -near neighbor of q in O ,itreportssome c -approximate r -near neighbor of q in O with probability 1
Unlike its deterministic counterpart, the solution to ran-domized crNN search may produce false positive. To be specific, take, again, the scenario illustrated in Figure 2 as an example. Although o 4 falls outside the outer circle, it still has a chance to be reported as the crNN of q .
Strategy 1 is of great importance in theory as it owns attractive asymptotical query and space performance, pro-vided that the values of parameters k and L are properly chosen. Despite its theoretical significance, Strategy 1 is rarely used in practice because of the undesirable quality of result compared to Strategy 2.

Strategy 2 enables us to solve the randomized r -near neigh-borreportingproblem,whichisdefinedasfollows.

Definition 4 ([3]). Given a set O of points in a d -dimensional space d ,andparameters r&gt; 0 ,  X &gt; 0 ,con-struct a data structure such that, given any query point q , reports each r -near neighbor of q in O with probability 1
The problem defined above is a special case of the random-izedversionofthe r -near neighbor search problem, where all (not just some) rNNs of q are returned. Note that Strategy 2 provides much better result quality since all data points intheanswersetarerNNsofthequery,andthusnofalse positive is returned. There, however, might still be false negative if some rNNs do not collide with the query. To see this, consider the example shown in Figure 2. While both o and o 2 are rNNs of q , Strategy 2 might only report o 2 as the final result.

Instead of reporting all rNNs of the query, a practical im-plementation of Strategy 2 often sorts all rNNs in ascending order of their distances to the query, and only returns the nearest neighbor (the K-nearest neighbor) of the query by picking the first point (the first K points) from the sorted list. It is evident that Strategy 2, together with the sorting procedure, can provide much better quality of result com-pared to Strategy 1. The price, however, we have to pay is the much higher query time and the lack of theoretical guarantee on the running time of queries. The most appealing feature of the algorithms based on LSH is that, by setting proper values for parameter k and L , the query times of these algorithms are much lower than those of other competitors, either theoretically or practically. Next, we present some important theoretical results as to the classic LSH algorithm, under Strategy 1 and Strategy 2, respectively.

In Strategy 1, if we set k = log 1 /p 2 n and L = n  X  ,where  X  = ln 1 /p 1 ln 1 /p 2 , then for the randomized crNN search problem, the query time is dominated by O ( n  X  ) distance computa-tions and O ( n  X  log 1 /p 2 n ) evaluations of hash functions from H , with some constant failure probability  X &lt; 1 [12, 16, 9].
In Strategy 2, failure probability and query time also de-pend heavily on k and L . Different from Strategy 1, no theoretical guarantee exists for query time and, in the worst case, the cost to process a query might be as high as O ( n )[3]. Such a situation will occur if a huge amount of data points collide with the query point due to a bad choice of parameter values. Fortunately, for many real datasets, a careful choice of values for k and L leads to a sublinear query time [5, 20].
A commonly used parameter selection method for Strate-gy 2 is as follows [3, 15, 19]. Assume o is an r -near neighbor of q , and consider any parameter k .Foranyfunction g j , the probability that g j ( o )= g j ( q )isatleast p k 1 . Therefore, for some j =1 , 2 ,...,L , the probability that g j ( o )= g is at least 1  X  (1  X  p k 1 ) L .Bysetting L = log 1  X  p k (1  X  p k 1 ) L  X   X  .Asaresult,any r -nearneighborof q is re-turned by Strategy 2 with probability at least 1  X   X  .Howto choose a proper k is not a trivial issue and out of the scope of this paper, we would like refer interested readers to [3] for a detailed discussion.
In the rest of this paper, we use a slightly different de-scription of locality sensitive hashing for our purpose, which is defined as follows.

Definition 5. Afamily H is called locality sensitive if for any two points o , q  X  d , it satisfies where r is the distance between q and o ,and f H ( r ) is a monotonically decreasing function of r .

Please note that, although stated in a different form, Def-inition 5 is essentially equivalent to the original definition given in [16]. Actually, our definition is very similar to the LSH definition used in [8], which focuses on Jaccard and Cosine similarity measures.

For ANN search in a d -dimensional space under some l s norm, the analytical expressions of f H ( r ) have been discov-ered for s  X  [0 , 2]. Two particular instances ( s =1 , 2) will be discussed shortly in Section 3.2 and Section 4.1, respectively.
By Definition 5, it is easy to see that Pr H [ h ( q )= h ( o )] actually depends only on the distance r between q and o , and has nothing to do with their specific locations in the d -dimensional space. In other words, each coordinate of both q and o can take arbitrary values as long as the distance between them remains to be r . Hence, we will use Pr H [ r ] and Pr H [ h ( q )= h ( o )] ( q  X  o = r ) interchangeably in the following sections.
In this section, we reexamine the rationale behind LSH and illustrate what is the actual implication for practitioners if a family of hash functions is locality sensitive.
Recall that the existence of the LSH families is the foun-dation for various different LSH algorithms. To make it easy to follow, we use the LSH family for Hamming space as an example to elaborate our main argument. It is worth noting that the results presented in this paper are not confined to this special case only, but applicable to all LSH families.
In Hamming space, the data points are binary, that is, each coordinate is either 0 or 1, and the dissimilarity be-tween points is measured by the Hamming distance, which is defined as the number of positions at which the values are different. For binary data points, the Hamming dis-tance is equivalent to the Manhattan distance ( l 1 norm). Figure 3 depicts a simple example with two binary points, where d = 10 and the distance between q and o 1 is equal to 3.
A locality sensitive family of functions H H exists for Ham-ming space. To be specific, H H contains all functions h i map data points from (0 , 1) d to (0 , 1) such that h i ( o )= o Take Figure 3 as an example, in total there are 10 different hash functions in H H since d = 10. If we choose h 2 from tomapthesetwopointsintoa(0 , 1) space, we get h 2 ( q )=1 and h 2 ( o 1 )=0.

To see why H H is locality sensitive, let X  X  start with the concrete example depicted in Figure 3. Recall that the distance between o 1 and q is 3, and the dimensionality is 10. Therefore, if we choose a hash function h uniformly at random from H H to hash o 1 and q , the probability that Pr H H [ h ( q )= h ( o 1 )] is equal to 1  X  3 / 10.

To extend this observation to a more general case, consider two binary points q and o in a d -dimensional space under the Hamming metric. Assume the distance between q and o is r . It is not difficult to see that the probability that Pr
H H [ h ( q )= h ( o )] is equal to the proportion of coordinates on which p and o agree, that is, Pr H H [ h ( q )= h ( o )] = 1 q  X  o 1 /d =1  X  r/d .Since1  X  r/d decreases monotonically with r ,itfollowsthat H H is locality sensitive according to Definition 5.

Having H H at hand, we are now ready to scrutinize the real implication of LSH from a statistical perspective. Ac-cording to the Law of Large Numbers (LLN), a probabilistic interpretation of Pr H H [ r ]=1  X  r/d is that, for any two points o and q with distance r , if we choose a large num-ber, say 10 , 000, of hash functions uniformly at random from H
H , then the proportion of hash functions over which o and q collide will be very close to 1  X  r/d . Although this inter-pretation is easy to understand, it, however, does not lead to a feasible LSH algorithm directly.

To illustrate, consider a na  X   X ve (impractical) LSH algorith-m LSH N , which intends to solve the same target problem as LSH C . For ease of presentation, we assume k =1and L = 1. Given a dataset O and a query q , in order to find the r -near neighbors of q , LSH N works as follows: (1) gen-erates a large number, say 10 , 000, hash functions uniformly at random from H H and inserts all data points in O into the 10 , 000 hash tables; (2) for each data point o in O , computes the collision ratio, i.e., the proportion of these 10 , 000 hash functions over which o and q collide; if the collision ratio is greaterthan1  X  r/d ,thenreports o as as an r -near neighbor of q .

The problems with LSH N are obvious. First, to reduce the number of false negative caused by the probabilistic nature of this algorithm, one has to build a large number of hash tables, which may incur prohibitively huge storage cost. Sec-ond, even if the number of hash tables is sufficiently large, it is still difficult or even impossible to obtain a theoretical bound on failure probability, i.e., the probability of a false negative being produced.

While LSH N is impractical, almost all LSH theoretical re-sults are only valid for such an infeasible algorithm. Next, we will show the mismatch between the practical LSH algo-rithm and the widely practiced LSH analysis techniques.
After examining the exact meaning of LSH and the infea-sibility of LSH N , we focus on a practical LSH algorithm, i.e., LSH C , in this section. For ease of presentation, we continue to assume k =1and L =1.

Recall that, to build the core data structure, LSH C first randomly chooses a hash function, say h , from H H ,and then maps all data points in O ,using h ,intoa(0 , 1) space. Given a radius r , to find all rNNs of a query q , the algorithm (under Strategy 2) scans all data points lying in the bucket h ( q ), and reports points whose distances to q are less than r . As discussed in Section 2.3, the failure probability of this algorithm is no greater than 1  X  Pr H H [ r ]= r/d .
Although the above analysis seems quite reasonable (giv-en the fact that H H is locality sensitive and Pr H H [ r ] =1  X  r/d ), we argue that this commonly used analysis method is problematic, and thus may lead to questionable results. Specifically, we notice that a critical assumption in this method is not correct, which is stated as follows.
Observation 1. For any fixed hash function h  X  X  cho-sen by a concrete instance of LSH C , the probability that a random pair of points with distance r collides over h , denot-ed by Pr h [ r ] , may be not equal to Pr H [ r ] .
To make this observation easy to comprehend, consider two datasets shown in Figure 4, where the distance of each pair of points is equal to 1. As discussed earlier, Pr H [1] = 1  X  1 / 10 = 9 / 10 for both datasets. Unlike Pr H [1], however, Pr h [1] has different values for different datasets. For exam-ple, if the hash function h 3 ( o )= o 3 is chosen for use, then for dataset 1, the probability of collision for a random pair of points is 1, while for dataset 2, the probability of collision is only 3 / 4. In fact, a more careful observation will reveal that, even for the same dataset, choosing different hash functions, i.e., different h i , i =1 ,..., 10, may lead to different Pr Observation 1 tells us that (1) Pr h [ r ], instead of Pr should be used to analyze the performance of LSH C be-cause, for a specific LSH C instance being able to work, the chosen hash functions have to be fixed rather than  X  X andom-ly chosen X , and (2) Unlike Pr H [ r ], which is determined by the nature of the LSH family, Pr h [ r ] depends only on the specific hash function chosen from H and the characteristics of dataset. Consequently, in essence, Pr H [ r ]and Pr h [ r ]are totally different. The difference, however, is so subtle that it has been ignored for years. Here we omit the subscript H for H H intentionally to emphasize that our observation actually holds for all LSH families. Before giving a detailed explanation of this difference, we need to first review a few terms in probability theory.

An experiment is any procedure that can be infinitely re-peated and has a well-defined set of results. The collection of all results is called the sample space of the experiment. An element of the power set of the sample space is called an event. If the results that actually occur fall in a given event, the event is said to have occurred.
 InthecaseofLSH,anexperiment,forboth Pr H [ r ]and Pr h [ r ], has two inputs, namely, a hash function h i and a pair of points p j,r with distance r , and two possible results, that is, collision or no collision for a single trial, denoted by the binary-valued variable Col i,j ,where Col i,j =1means collision, and 0 otherwise. Although sharing the same pro-cedure of experiment, Pr H [ r ]and Pr h [ r ] differ significantly in the way the corresponding events are defined. To be spe-cific, the event associated with Pr H [ r ] is formally defined as { Col i,j | p j,r is fixed and the hash functions are random-ly chosen from H} , while the relevant event for Pr h [ r ]is the result set { Col i,j | h i is fixed and the pair of points is randomly chosen from O} . Figure 4: The probabilities of collision for different datasets using the same hash function are different. Pr h 3 [1] for dataset 1 is equal to 1, whereas for dataset 2, it is only 3/4. Neither of them is equal to Pr H H [1] , i.e., 9 / 10
Informally, to figure out Pr H [ r ], the experiment is repeat-ed by fixing the pair of points and randomly picking a hash function from H , whereas for Pr h [ r ], the hash function is a constant factor and the randomness of the experiment de-pends only on the characteristics of dataset, e.g., the cardi-nality of dataset and data distribution.

In the LSH literature, the performance analysis of almost all LSH algorithms relies on the basic assumption Pr h [ r ]= Pr
H [ r ], either explicitly or implicitly. The fact, however, is that while Pr H [ r ] is identical for all pairs of points with distance r , Pr h [ r ] often varies with h . Consequently, the correctness of theoretical results obtained using the existing analysis method is questionable, and should be reinvestigat-ed. Next, we will first give an important theorem to estab-lish the quantitative relation between Pr h [ r ]and Pr H Then, some flaws in the performance analysis of LSH C are identified.
Theorem 1. Given an LSH family H andaset O of d -dimensional data points, assume n hash functions are chosen uniformly at random from H ,andthenumberofpairsof points with distance r in O is m . Pr h [ r ] and Pr H [ r ] are related in the following formula:
Proof. For any hash function, say h i , randomly chosen from H , assume the number of pairs of points with distance r that collide over h i is s i , then the collision rate for h On the other hand, for any pair of points with distance r , say p j,r ,in O , assume the number of hash functions over which it collides is t j , then the collision rate for p j,r According to LLN, we have the following equations. By adding the system of equations together, we get
After simple equation transformation for (6) and (7) it follows
Since the total collision number of all pairs of points is a constant, we have By substituting this equation into (9) we have We prove the theorem .

Theorem 1 indicates that there exists an intrinsic relation between Pr h [ r ]and Pr H [ r ] although they are defined from different angles. With Theorem 1, we are now ready to reexamine the methods for LSH C performance analysis.
In this section, some flaws in the methods by which the theoretical performance of LSH C was previously analyzed will be demonstrated. Please note that the problems to be identified are only genuine for the LSH C analysis; for the impractical LSH algorithm ( LSH N ), all existing theoretical results still hold. Before presenting our main results, we need to make an assumption that is necessary for the fol-lowing discussion.

As shown in Theorem 1, although Pr h [ r ]and Pr H [ r ]are intrinsically connected, the relation between them is not de-terministic since towards Pr H [ r ]when m and n approach infinity. Thus, it is difficult to examine the existing methods merely under such an uncertain condition. As a workaround, we assume Pr
H [ r ]= that, according to LLN, this assumption is reasonable if n and m are sufficiently large.

Recall that Strategy 1 and Strategy 2 are two principal filtering approaches used in LSH C , and their performance has been discussed in Section 2.3. Next, we will investigate what X  X  wrong with the performance analysis for these two strategies.
The theoretical performance of LSH C under Strategy 1 was originally proved in Theorem 4 in [16], and the corre-sponding method of proof has been accepted as the standard analysis technique in the LSH literature thereafter. Being aware of the difference between Pr h [ r ]and Pr H [ r ], we, how-ever, point out the following observation:
Fact 1. An important premise, based on which the the-oretical results in Theorem 4 in [16] were derived, does not hold in practice actually.

Proof. A key step in the proof of Theorem 4 in [16] is to show that, given two data points o and q with distance less than r , g j ( o )= g j ( q ) holds for some j =1 ,...,L with a constant probability if proper values of k and L are chosen.
In order to obtain such a constant probability, the au-thors use Pr H [ g j ( o )= g j ( q )] = Pr H [ r ] k ( o indispensable premise in the proof. With the help of Ob-servation 1, we know that this premise should be replaced by
Under the assumption that Pr H [ r ]= easy to see that Pr H [ r ] k  X  vation 2 below. As a result, it follows that Pr H [ g j ( o )= g ( q )]  X  Pr H [ r ] k , which contradicts the widely-held belief that Pr H [ g j ( o )= g j ( q )] = Pr H [ r ] k first used in [16]
Without the assumption that Pr H [ r ]= Pr H [ g j ( o )= g j ( q )] will be less than, equal to or greater
Observation 2. Given m 1 , 2 ,...,m and C is a constant, thevarianceof u i is zero, i.e., u 1 = u 2 =  X  X  X  = u m ,andthe value of
It is worth noting that Fact 1 does not rule out the exis-tence of such a constant probability. The only implication of Fact 1 is that all theoretical results, which depend on the condition that Pr H [ g j ( o )= g j ( q )] = Pr H [ r ] tionable. Considering the uncertain relation between Pr H and be difficult to obtain a constant failure probability like the one given in [16].
Next, we look into LSH C under Strategy 2. Recall that in Strategy 2, given k and  X  , L should be at least equal to log 1  X  p k 1  X  in order to achieve a failure probability no greater than  X  .Inpractice, L isoftenchosensuchthat(1  X  p k 1 ) L Using our notations, L canberewrittenas
By Observation 1, the probability of collision for a random pair of points with distance r is P h i [ r ]. As a result, given the L value calculated by Equation (12), the actual failure prob-ability, denoted by Pr A [ r ], should be instead of  X  .Since Pr h [ r ] varies with h , Pr A [ r ] cannot be a constant as well. Interestingly enough, although Pr A [ r ] itself is a random variable since all hash functions are ran-domly chosen, its expectation is exactly equal to  X  . Theorem 2. E Pr A [ r ] =  X 
Proof. Omitted due to space limitation. The full version is available on request.

An immediate corollary of Theorem 2 is that the actual failure probability fluctuates around, rather than is always equal to, the specified failure rate  X  ,whichwillbejustified experimentally in Section 4.4.2
In this section, we conduct extensive experiments to verify the validity of our arguments presented in this paper. Sec-tion 4.1 describes the hash functions to be examined. Sec-tion 4.2 lists datasets with which the experiments are carried out. Section 4.3 elaborates on how data are preprocessed. Finally, the experimental results are reported in Section 4.4.
In [9], the authors propose a popular LSH family for l 2 norm, denoted by H E , based on the concept of p -stable dis-tributions. The LSH function is defined as follows: than Pr H [ r ] k with some unknown probabilities respectively, which still contradicts Pr H [ g j ( o )= g j ( q )] = Pr where o denotes the d -dimensional vector representation of adatapoint o ; a is a d -dimensional vector, each component of which is drawn independently from Gaussian distribution defined by the density function g ( x )= 1  X  2  X  e  X  x 2 / resents the inner product of a and o ; b is a real number chosen uniformly from [0 ,w ], and w is a constant that has to be specified a priori.

Under H E , the probability of collision for any two points with distance r is calculated as follows: where f ( x )=2 g ( x ).
 We choose H E to verify our arguments for two reasons: (1) H
E isaverypopularLSHfamilyandhasmanysuccessful applications in practice [20]; (2) A nice implementation of the LSH algorithm using H E , i.e., the E2LSH package, is publicly available [2]. Thus, it is easy for interested readers to repeat our experiments.
We perform the experiments with several real datasets, including mnist1k, mnist10k, audio10k and color10k. All data are normalized such that each dimension has domain [0 , 1]. The distance metric is Euclidean distance.
Mnist . The mnist dataset 4 consists of 60,000 handwrit-ten pictures, and each picture has 28  X  28 pixels, each of which is represented by an integer in the range of [0 , 255]. Thus, each data point is 784-dimensional. For our purpose, we use two subsets of the original mnist dataset, i.e., m-nist10k and mnist1k, because the number of points in them is already large enough to verify our claim. Mnist10k is a test set of 10,000 points used in [10]. Mnist1k has only 1000 points and can be downloaded along with the E2LSH package.

Audio and Color . The audio dataset 5 contains more than 50,000 192-dimensional data points, which are extract-ed from the LDC SWITCHBOARD-1 collection. This col-lection consists of 2,400 two-sided telephone conversations a-mong 543 speakers from different areas in the United States. Color is a 32-dimensional dataset 6 with more than 68,000 points, where each point describes the color histogram of an image in the Corel collection. For our purpose, we randomly choose 10,000 points from the audio and color datasets re-spectively to form the audio10k and color10k datasets, with which the experiments are performed.
The purpose of this experimental study is not to evaluate the performance of some NN search algorithm, but to ver-ify the validity of our theoretical analysis. To this end, all datasets are preprocessed in the following way.

For each dataset, all 2-combinations, i.e., all possible pairs of points, are enumerated out of the dataset, and the dis-tanceofeachpairiscalculated. Then,allpairsofpoints are arranged in ascending order of their distances. Let d and d max denote the minimum distances computed, respec-tively. In order to ensure comprehensive verification, start-ing at r = d min , we choose a series of radii with spacing http://yann.lecun.com/exdb/minist/ http://www.cs.princeton.edu/cass/audio.tar.gz http://kdd.ics.uci.edu/databases/CorelFeatures/ 0.1, i.e., r = d min +0 . 1 ,d min +0 . 2 ,... , within the range of [ d min ,d max ]. Foreachradius r , the pairs of points whose distances fall in the range of [ r  X  0 . 01 ,r +0 . 01] are selected to form the set S r . Herewedonotrequirethedistanceto be exactly equal to r because it is hard to find enough pairs of points satisfying this criterion, even for large datasets. In the empirical study to be discussed shortly, we experi-ment with a large number of S r s produced from the several datasets to confirm our arguments given in earlier sections.
In this set of experiments, we first choose 10,000 hash functions uniformly at random from H E ,andthenchoose 100 random pairs of points, denoted by p j,r , j =1 ,..., 100, from each S r 7 .Foreach p j,r , the hash-function-based colli-sion ratio, namely, the proportion of these 10,000 hash func-tions over which p j,r collides is calculated. Let CR p j the collision ratio of p j,r . The average hash-function-based collision ratio, denoted by CR p r , is the mean of all CR namely,
Since all experimental results with respect to different datasets are very similar, we only report the collision ra-tios of 10 pairs of points chosen from the mnist1k dataset in Table 1. The probabilities of collision ( Pr H E [ r ]), which are calculated according to Equation (14), and the average collision ratios are also listed.

As shown in Table 1, the collision ratios of these 10 pairs of points are very close at each different radius value, and the average collision ratio is considerably close to the probability of collision, which confirms our interpretation of Pr H E i.e., Pr H E [ r ] does not rely on any particular pair of data points; it is only determined by the nature of the given LSH family.
 Table 1: Samples of the hash-function-based colli-sion ratios Pr H E [ r ] 0.6825 0.6095 0.5451 0.4897 0.4426
Next, 100 hash functions h i , i =1 ,..., 100, are randomly chosen from H E , and for each S r in the mnist1k dataset, the dataset-based collision ratio, denoted by CR h i r , is computed. CR h i r is defined as the proportion of all p j,r in S r
For S r with less than 100 pairs of points, all pairs of points in it are selected. Table 2: Samples of the dataset-based collision ra-tios stdev of CR h r 0.1208 0.0725 0.0334 0.0150 0.0070 collide over h i . Please note that the way in which CR h i is calculated is totally different from CR p j r . The average dataset-based collision ratio, denoted by CR h r , is defined as themeanofall CR h i r ,namely,
The collision ratios of 10 representative hash functions are illustrated in Table 2. The average hash-function-based col-lision ratios ( CR p r ), the standard deviations of CR h i number of pairs of points in S r are also listed. From Table 2 one can see significant deviation of CR h i r from CR p r justifies our claim in Observation 1 as CR h i r and CR p good approximations of Pr h i [ r ]and Pr H E [ r ], respectively.
Another interesting fact that we can observe is that the standard deviation of CR h i r decreases as radius r becomes larger, which is mainly caused by the varying sizes of S r 0 . 4 ,..., 0 . 8. The impact of the cardinality of dataset on Pr h i [ r ] will be evaluated further in Experiment 2 and Ex-periment 3.

Recall that there is an intrinsic relation between Pr h i and Pr H [ r ] as proved in Theorem 1. In order to support this claim, we plot the minimum and maximum dataset-based collision ratios, namely, min ( CR h i r )) and max ( CR  X  i =1 ,..., 100, CR h r and CR p r under different radii in Fig-ure 5. As we can see from this figure, CR h r and CR p r almost coincide for all radii, which experimentally validates Theo-rem 1. Furthermore, the significant gap between the maxi-mum and minimum CR h i r bears out Observation 1 again.
Recall that, in Section 3.5, we argue that the actual failure probability will fluctuate around the value specified by users. In this set of experiments, we will experimentally confirm this argument with the dataset mnist1k.

Given a radius r , r = d min +0 . 1 ,d min +0 . 2 ,... ,foreach k , k =1 , 2 ,... , the corresponding L value is calculated using L = log 1  X  Pr H [ r ] k  X  ,where Pr H [ r ] is computed using Equa-tion (14) and  X  is the pre-specified failure rate by users.
The measure, denoted by SR k,L r , is the success ratio, name-ly, the proportion of all pairs of points in S r that collide over at least one g j ,j =1 , 2 ,...,L . Please note that small SR means high failure probability and vice versa.

For each k , we repeat the experiment 10 times and collect the minimum, maximum and average SR k,L r ,respectively. Due to space limitation, only some representative results are reported in Figure 1, where the minimum, maximum and average SR k,L r as a function of k are plotted, with r =0 . 4 and  X  =0 . 1. The values of 1  X  (1  X  Pr H [ r ] k ) L (the expected SR k,L r ) are also depicted since they may not be exactly equal to 1  X   X  due to the rounding of L . In other words, the expected success ratio is 1  X  (1  X  Pr H [ r ] k ) L rather than 1  X   X  actually. As we can see, there exists a significant difference between the minimum and maximum SR k,L r for all k values. While SR k,L r fluctuates around 1  X  (1  X  Pr H [ r ] its average, however, is considerably close to the expected success ratio, which justifies our claim in Theorem 2. A probabilistic interpretation of the above results is that the success ratio, with which LSH C reports rNNs of a query, is a probabilistic event itself!
In this set of experiments, we repeat the last experiments with more real datasets to show the impact of dataset char-acteristics on the performance of LSH C .

Specifically, for each dataset (mnist10k, audio10k and col-or10k), we first form a subset by randomly choosing 10% out of this dataset, and then put another 10% out of the remain-ing points into the subset, and so on and so forth until the original dataset is exhausted. For each subset generated, we repeat Experiment 1 and collect the same statistics as those discussed in Section 4.4.1. Since the experimental results ex-hibit similar trends with respect to different radii, we only report some representative statistics here.

To demonstrate how Pr h [ r ] is affected by the characteris-tics of a dataset, the standard deviations of CR h i r are collect-ed. Due to space limitation, only the result for minist10K is plotted in Figure 6. As we can see, the standard deviation at the same radius drops gradually as the proportion of points grows from 10% to 100%, which means that the larger the cardinality of a dataset, the closer Pr h i [ r ]isto Pr H addition, when the proportion of data points is fixed, the standard deviations at different radii are not identical, and vary in different trends for these three datasets. For exam-ple, when the proportion of points is set to 50% and r =1 . 2, the standard deviation is equal to 3 . 52  X  10  X  4 for mnist10k, whereas for color10k, it is boosted to 2 . 47  X  10  X  2 drastical-ly. This is because (1) for the same dataset, the numbers of pairs of points in different S r s are not identical; (2) data distributions of these three datasets are different, which in-dicates that data distribution has an important impact on Pr h i [ r ]aswell.

In sum, for the same dataset, the more (less) pairs of points in an S r , the smaller (larger) the variation in Pr When the standard deviation becomes very small, i.e., less than 10  X  3 ,most Pr h i [ r ] will be very close (even almost i-dentical) to Pr H [ r ]. In that case, the actual performance of LSH C will be considerably close to what is predicted by the existing analysis method (although it is not correct). These experimental results explain why the LSH algorithms work very well in practice, which we believe is one main cause for why such a gap exists for years. Figure 5: min ( CR h i r ) , max ( CR h i r ) , CR h r and CR (mnist1k) Figure 6: standard deviation vs. proportion of points (mnist10k)
In this paper, we observe that a fundamental gap exists between the LSH theory and the method for analyzing the LSH algorithm performance, and develop a novel analysis model to fill the gap between the LSH theory and practice. We believe that there are many interesting directions wor-thy of further investigation. A few quick examples include how to determine optimal parameters for the LSH algorithm under the proposed model, and is it possible to devise novel randomized (approximate) algorithms for NN search with provable constant failure probability.
 The work reported in this paper is partially supported by NSFC under grant number 60903160, NSF of Shanghai un-der grant number 13ZR1400800 and NSC grant 101-2221-E-006-219. [1] N. Ailon and B. Chazelle. Approximate nearest [2] A. Andoni and P. Indyk. E2lsh manual. In [3] A. Andoni and P. Indyk. Near-optimal hashing [4] S. Arya, D. M. Mount, N. S. Netanyahu, [5] M. Bawa, T. Condie, and P. Ganesan. Lsh forest: [6] J. L. Bentley. K-d trees for semidynamic point sets. In [7] A.Z.Broder,M.Charikar,A.M.Frieze,and [8] M. Charikar. Similarity estimation techniques from [9] M.Datar,N.Immorlica,P.Indyk,andV.S.Mirrokni.
 [10] R. Fagin, R. Kumar, and D. Sivakumar. Efficient [11] J. Gan, J. Feng, Q. Fang, and W. Ng.
 [12] A. Gionis, P. Indyk, and R. Motwani. Similarity [13] A. Guttman. R-trees: A dynamic index structure for [14] S. Har-Peled, P. Indyk, and R. Motwani. Approximate [15] J.He,S.Kumar,andS.-F.Chang.Onthedifficultyof [16] P. Indyk and R. Motwani. Approximate nearest [17] Y. Ke, R. Sukthankar, and L. Huston. An efficient [18] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and [19] M. Slaney, Y. Lifshits, and J. He. Optimal parameters [20] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Quality and [21] R. Weber, H.-J. Schek, and S. Blott. A quantitative
