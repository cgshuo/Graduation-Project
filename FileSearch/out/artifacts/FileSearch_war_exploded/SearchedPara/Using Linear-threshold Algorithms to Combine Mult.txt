 In this paper, we define a new type of inductive learning al-gorithm called a linear-max algorithm. This is an algorithm that learns using a special type of attribute called a sub-expert. We define a sub-expert as a vector attribute that has a value for each possible output class. The value for each class corresponds to the prediction strength the sub-expert gives to each class. A hypothesis prediction can be viewed as a sub-expert. At a minimum, if the hypothesis predicts a single class, the sub-expert can predict for that class and for the remaining classes. Some learning algorithms can use information, such a class probability estimates, to give prediction values for all the sub-expert classes. The linear-max algorithm takes a weighted sum of all the sub-experts and predicts the class that has the maximum value. We call this a linear-max function. The goal of the algorithm is to learn good weights for the sub-experts.
 This paper deals with the problem of classifying a sequence of instances. Each instance belongs to one of classes and is composed of sub-experts. After the algorithm predicts a label for an instance, the environment returns the correct classification. This information can then be used to im-prove the prediction function for the next instance in the sequence. These three steps, getting an instance, predict-ing the class, and updating the classifier, are called a trial. The goal of the algorithm is to minimize the total number of prediction mistakes made during the trials. This is com-monly referred to as the on-line mistake-bounded model of learning (Littlestone, 1989).
 It is intuitive to think of sub-experts as individual classify-ing functions that are attempting to predict the target func-tion. Even though the individual sub-experts may not be perfect, the linear-max algorithm attempts to learn a linear-max function that combines them and does well on the tar-get. In truth, this picture is not quite accurate. The reason we call them sub-experts and not experts is because even though an individual sub-expert might be poor at predic-tion, it may be useful when used in a linear-max function. The term experts more commonly refers to the situation where the performance of the algorithm is measured with respect to a single best expert (Cesa-Bianchi et al., 1997; Littlestone &amp; Warmuth, 1994) as opposed to a combination of sub-experts.
 Linear-max algorithms are related to linear-threshold al-gorithms and were motived by a desire to extend linear-threshold functions from two class prediction to multi-class prediction. We accomplish this by using a linear-threshold algorithm to help solve a linear-max problem. This tech-nique is useful both theoretically and practically. It allows the existing mistake bounds from the linear-threshold algo-rithms to be carried over to the linear-max setting. With no extra work, one can the apply the bounds of existing or future linear-threshold learning algorithms to multi-class problems.
 This paper is based on work in Mesterharm (2001) which gives a general framework and proof to extend a wide range of linear-threshold algorithms to multi-class problems. In this paper, we give a slightly less general result due to space considerations; please refer to the technical report for more details. While the proof deals with sub-experts, in Mester-harm (2001) we give a technique to use sub-experts to solve multi-class problems dealing with real-valued attributes. This produces algorithms that are similar to algorithms that can be derived with techniques given in Nilsson (1965) and Har-Peled et al. (2003). For this reason, we will focus on sub-experts in this paper, however, it is straightforward to combine the two techniques to get hybrid algorithms that combine sub-experts and attributes.
 There has been a large amount of previous work on using binary prediction for multi-class attribute based problems. Allwein et al. (2000) and Dietterich and Bakiri (1995) both look at learning separate binary classifiers to make multi-class predictions. Crammer and Singer (2001) gives a fam-ily of multi-class Perceptron algorithms with generalized update functions. Har-Peled et al. (2003) gives a method for handling more general multi-class learning problems such as ranking.
 For combining experts in an on-line setting much of the work focuses on learning a single best expert (Cesa-Bianchi et al., 1997; Littlestone &amp; Warmuth, 1994). For learning a combination of experts, Blum (1995) gives a ver-sion of a multi-class Winnow sub-expert algorithm, while Mesterharm (2000) generalizes that algorithm and gives a stronger mistake bound. This paper builds on the previous work by giving a technique to extend a wide range of linear-threshold algorithms to a combination of sub-experts while retaining many of the benefits including mistake bounds. Another goal of this paper is to show that sub-expert algo-rithms are useful for practical machine learning problems. Linear-threshold algorithms have been used successfully on many practical problems. Since linear-max problems are solved using linear-threshold algorithms, we should get similar real-world performance with both types of algo-rithms. In addition, the Winnow sub-expert extension has already given state of the art performance on a calendar scheduling problem in Blum (1995). In this paper, we per-form experiments to show that it is useful to extend other algorithms, besides Winnow, with sub-experts. We give ex-periments with sub-experts using Perceptron (Rosenblatt, 1962), normalized Winnow (Littlestone, 1989), and the Relaxed On-line Maximum Margin Algorithm (Romma) (Li &amp; Long, 2000), and we show that both Perceptron and Romma give superior performance on specific types of problems. In this section, we give the definitions of a linear-max al-gorithm and its related linear-threshold algorithm. Before we give the formal definitions we need to cover how sub-experts work, how they make predictions, and how the pre-dictions are combined. 2.1. Sub-experts First we give the definition of a sub-expert. A sub-expert makes predictions, one for each possible output class. These predictions correspond to the rating a sub-expert gives to each class; higher numbers corresponding to a bet-ter rating. Let be the prediction sub-expert gives for class . Notice that the rating of a class is relative. Looking at class and , a sub-expert prefers class over if . It is these differences between class predic-tions that are crucial to the operations of the multi-class al-gorithms in this paper. Sometimes, it is useful to bound the size of these differences. In those cases, assume for each expert and all that where .
 Here are some examples of the types of knowledge that sub-experts can encode. For these examples assume that imal preference to class 1 over class 2 or 3. This same sub-expert makes no distinction between class 2 and 3. A sub-expert that predicts again makes no distinc-tion between class 2 and 3 but maximally prefers class 2 or 3 over class 1. Lastly a sub-expert that predicts makes no distinction between any of the classes. Notice that adding a constant to each prediction class in a sub-expert does not change the differences between classes. For example is the same prediction as , and is the same prediction as . In gen-eral, we can represent all sub-experts by restricting all to , but sometimes a wider range of prediction values is useful for notational economy or algorithm efficiency. 2.2. Prediction Now we will show how to use several sub-experts to make a global prediction. Assume there are sub-experts, and each sub-expert is assigned a weight. Let be the vec-tor of weights where is the weight of sub-expert . We combine the information from the weights and the sub-experts to compute a vote for each class. Define the voting function for class and weights as the highest vote, . (On a tie the algorithm predicts any class involved in the tie.) We call the function computed by this prediction scheme a linear-max function since it is the class of the maximum value taken from a linear combination of the sub-expert predictions. Even though some sub-experts may not individually give accurate predictions, they may be useful in a linear-max function. For example, sub-experts might be used to add threshold weights. This is done by adding an extra sub-expert for each class. A threshold sub-expert would always predict with 1 for its corresponding class and 0 for the re-maining classes. While a threshold expert makes a poor classifier by itself, when combined in a linear-max algo-rithm, it gives a useful expansion of the set of target func-tions. 2.3. Linear-threshold Algorithm Definition We will show how any linear-threshold algorithm of the fol-lowing form can be transformed to a linear-max algorithm. Initialization Trials For the linear-threshold algorithm, the update is a function of the current weights and the attributes multiplied by the label. For example, Perceptron uses for updates on mistakes. 2.4. Linear-max Algorithm Definition Next we define the related linear-max algorithm. The im-portant thing to note is that the algorithm uses the same update function as the linear-threshold algorithm. This is the key that binds them together. We use the update proce-dure from a linear-threshold algorithm to perform updates on the related linear-max algorithm.
 Initialization Trials When the linear-max algorithm makes a mistake, we up-date with an instance that would cause the linear-threshold algorithm, with the same weights, to make a mistake. There is some freedom in picking this incorrect instance. For example, for a problem with classes let voting function. The algorithm predicts and as-sume the correct label . Consider the follow-ing instances for the linear-threshold algorithm: all have a label . The first three of these corre-spond to instances that would be misclassified. For exam-ple, . The last is correct based on the current weights. We could use any of the first three for the update, but to keep things simple, we pick .We conjecture that other possible instances, such as linear com-binations of the above, will give a sub-expert based family of algorithms similar to the ultraconservative algorithms of Crammer and Singer (2001). In this section, we prove the linear-max algorithm has the same bounds as the related linear-threshold algorithm. Throughout this section assume we are running a linear-max and a linear-threshold algorithm.
 First we will show that the algorithms always have the same weight values. Initialize both algorithms with the same weights. Assume we are given an instance of the linear-max problem. Use the linear-max algorithm to make a global prediction with the sub-experts. This prediction is such that for all . The environment will return the correct label . If the prediction is a mistake ( ) then create a new instance for the linear-threshold algorithm of the form . Because of our ear-lier assumptions on sub-experts, each . Let the label for this instance be . Input and into the linear-threshold algorithm and update the weights in both the linear-threshold and linear-max algorithms. Since both algorithms have the same update function and now have the same input parameters, both algorithms will make the same weight updates. Repeat this procedure for each trial. Using a simple inductive argument, both algorithms will always have the same weights.
 Now we will show that every time the linear-max algorithm makes a mistake then the linear-threshold makes a mistake. This can be seen by looking at the prediction procedure of the linear-max algorithm. A mistake can only occur in the linear-max algorithm if the vote for the correct label is less than or equal to the vote for the predicted label. This means that . This can be rewritten as Because the linear-threshold al-gorithm uses the same weights, the instance causes the linear-threshold algorithm to predict . Since we have constructed the instance such that the correct la-bel for the linear-threshold algorithm is , this causes a mistake. Therefore the number of mistakes made by the linear-max algorithm is upper-bounded by the number of mistakes made by the linear-threshold algorithm. Using this upper-bound, we can apply any mistake bound from a linear-threshold algorithm to its related linear-max algo-rithm.
 Of course, the linear-max algorithm only has a good bound if the the linear-threshold algorithm has a good bound. In order to get a good bound, the instances sent to the linear-threshold algorithm need to satisfy certain algorithm and proof dependent assumptions. For example, if the attribute to use the linear-max transformation for many Bayesian al-gorithms that make distributional assumptions. Fortunately many algorithms have bounds that only depend on the ex-istence of target weights that perfectly classify the data. Assume there exists a set of target weights that perfectly classify the sub-experts. These same weights must per-fectly classify any instance passed to the linear-threshold algorithm since every instance is of the form with label . These instances will always predict since weights for the linear-threshold algorithm will give the mistake-bound. While this perfect classification assump-tion may seem overly strong, the proof technique can be modified to allow noisy instances (Littlestone, 1989; Little-stone, 1991). Most of these modifications will carry over to the sub-expert setting, however to simplify our presenta-tion, any bounds we give in the rest of the paper refer to the no-noise framework. In this section, we give some transformations of linear-threshold algorithms into linear-max algorithms. This is done by replacing the generic update function from Section 3 with a specific update function from a linear-threshold algorithm. We also include some relevant details for the various algorithms such as mistake bounds. 4.1. Perceptron Algorithm This is the multi-class version of the classic Perceptron al-gorithm (Rosenblatt, 1962). It fits directly into the linear-threshold framework of the linear-max transformation. Initialization Trials The bounds for the linear-max Perceptron algorithm de-pend on certain problem dependent parameters. These are the same parameters that are used for the linear-threshold algorithm, but the values of the parameters come from the linear-max problem. Let trials . Let be a weight vector that correctly classifies all instances. Let  X  trials ; this is a mar-gin value where no instance occur. With these parameters, the number of mistakes is less than or equal to  X  (Duda &amp; Hart, 1973). 4.2. Romma Algorithm This is the multi-class version of the Romma algorithm (Li &amp; Long, 2000). Again it is a fairly straightforward to trans-form Romma to the the linear-max setting.
 Initialization Trials The bounds found in Li and Long (2000) for Romma are the same as the bounds given above for the Perceptron al-gorithm. 4.3. Normalized Winnow Algorithm This is the multi-class version of the normalized Win-now algorithm that learns linear threshold functions (Lit-tlestone, 1989). This algorithm is similar to the multi-class Winnow algorithm in Blum (1995). The main difference is that it allows expert predictions to be real numbers. Initialization Trials Again the bounds for the linear-max Winnow algorithm de-pend on certain problem dependent parameters. Let be a weight vector that correctly classifies all instances. Let  X  The number of mistakes is less than or equal to  X  (Littlestone, 1989). Mesterharm (2000) has a specific anal-ysis that derives the same mistake bound for this linear-max algorithm. 4.4. Other Linear-max Algorithms Of course, these are not the only linear-threshold algo-rithms that can be transformed into linear-max algorithms. For example, Quasi-additive algorithms are a recent and relatively unexplored class of linear-threshold learning al-gorithms (Grove et al., 1997) that include an uncount-ably infinite number of algorithms including Perceptron and normalized Winnow. The linear-max setting is suffi-ciently flexible to include all the Quasi-additive algorithms and more. In this paper, we only talk about Perceptron, Romma, and Winnow since these are popular efficient al-gorithms that have performed well in practice.
 We can also change the linear-max definition to allow more updates. If there are classes, consider the in-stances mentioned in Section 2.4. These instances can be used for updates. This is suggested in (Har-Peled et al., 2003) for on-line learning of the attribute based problem. We can even go further and repeatedly cycle through these instances until all are correctly classified. While these tech-niques should improve performance for problems without noise, it could lead to problems when noise becomes ex-cessive.
 Another variation is to use nonconservative algorithms. A nonconservative linear-threshold algorithm makes updates on some instances even though the algorithm has not made a prediction mistake on the instance. These more aggres-sive updates will not improve performance against an ad-versary, but often improve performance in practice (Block, 1962; Li &amp; Long, 2000). If the linear-threshold algorithm is nonconservative, we can make the linear-max algorithm nonconservative by passing some of the correctly classified instances mentioned above.
 Last, we can use these instances with a Support Vector Ma-chines (SVM) (Cortes &amp; Vapnik, 1995). These instances encode the information about the correct sub-expert pre-dictions. After trials there will be a total of instances, and we can pass all these instances to the SVM to learn weights for the sub-experts. We can then use the weights or the support vectors to make predictions in the linear-max setting. This allows us to use a SVM to com-bine multi-class hypotheses. It may also be possible to use a SVM with a kernel function or even a wider range of learning algorithms to expand beyond simple linear com-binations of sub-experts, but one needs to be careful about the effects of nonlinearity. In this section, we give experiments to show that multi-class algorithms give good performance, and that the var-ious versions, that correspond to different linear-threshold algorithms, give different and non-dominating results on realistic problems. We perform experiments with normal-ized Winnow, Perceptron, and Romma. For all these algo-rithms, we include a set of experts described in the problem and a set of threshold experts. For a class problem there are threshold experts with each expert always predicting 1 for its respective class.
 The problems we look at are often sparse in the number of experts active on a given trial and in the number of non-zero predictions made by an active expert. An expert is active if at least one of it X  X  label predictions is non-zero. Let be the number active experts in an instance. Let be the maximum number of label predictions that are non-zero in any expert. All three algorithms use time to predict and update on a trial.
 These predictions and updates are fairly straightforward to implement using a sparse representation for the instances. An instance is internally represented by a list of active ex-perts. Furthermore, for each active expert, the algorithm keeps a list of the labels that have non-zero predictions and their value. This representation allows an efficient im-plementation of any linear-max algorithm that is based on a linear-threshold algorithm that can perform predictions and updates in time where is the number of active real-valued attributes. Most algorithms, including Percep-tron and normalized Winnow, are already in this form, and many other algorithms can be modified to get up-dates. For example, even though the update function of the linear-threshold Romma depends on the norm of the to-tal number of attributes, we store extra information to allow time for all of the following experiments combined is a few minutes on a 700 MHz machine. 5.1. Artificial Data Our first experiments deal with artificial data. Let be the total number of experts where of these experts are rele-vant. For each trial, every relevant expert randomly picks a class from classes. The expert predicts 1 for the selected class and 0 for the other classes. The class that is selected most often, from the relevant experts, is the class of the in-stance. We place an order on the relevant experts to handle ties. If there is a tie in the voting for a class, the instance is labeled according to the prediction of the lowest expert that is involved in the tie. The irrelevant experts are active with probability . If an irrelevant expert is active it randomly picks a class and predicts 1 for that class and 0 for the remaining classes. In the rest of the paper, we will refer to this as the majority learning problem.
 Our primary purpose for the artificial data experiments is to clearly show that none of the three algorithms dominates. In figure 1, we show three graphs with a different algo-rithm giving the best result in each graph. For Winnow, we picked the multiplier parameter that gives the best perfor-mance. This multiplier is given in the graph key after the name Win. The graphs give the number of mistakes on the y-axis and the the number of trials on the x-axis. The slope of the graph corresponds to the error rate of the algorithm. Each experiment was run for 100,000 trials, and was aver-aged over 20 runs. We give confidence intervals at three points along the curve using a distribution.
 The first graph shows normalized Winnow performing best. The majority problem is described by , , now to perform best by using a small number of relevant experts with a large number of irrelevant experts. To fur-ther separate the algorithms, we made the instances dense with a large value of . This will force poor performance by Perceptron and Romma since their bounds depend on the 2-norm of the instance.
 The second graph is for Romma. The majority problem is described by , , , and . This problem avoids the region where Winnow dominates. The last graph shows Perceptron performing the best. The majority problem is described by , , , and . We had some difficulty in getting Percep-tron to perform best. Perceptron and Romma have similar bounds and, for the majority problem, Romma often out performs Perceptron. Eventually, we were able to get better performance with a sparse problem that has a large number of classes and only a few relevant experts. 5.2. Calendar Data Our second set of experiments deals with the Calendar Ap-prentice problem (Mitchell et al., 1994). In this problem, the goal of the algorithm is to predict information about a meeting that is entered into a calendar application. There are a total of 34 text features used to describe a calendar event. The algorithm predicts the day, duration, location, and start-time of a meeting. When predicting a particular label we include the other labels as features in the instance. The dataset is divided into two users. In this paper, we re-port results for user two with a total of 553 instances. Our reason for running the Calendar problem is to see if Perceptron or Romma can improve the performance on any of the label prediction tasks. Blum (1995) used a multi-class version of normalized Winnow to show a performance improvement over C4.5 on these problems. Since our algo-rithm is essentially the same, we did not expect to see much difference in the results of the Winnow algorithm. How-ever, while the Winnow results for user one was similar to for user two. We assume the discrepancy can be attributed to differences in feature selection.
 We follow the technique of Blum (1995) to create sub-experts for the calendar problem. For every trial, we look at all pairs of features and create an sub-expert that predicts based on the past occurrences of the feature pair. The sub-expert predicts 1 for the label that occurred most frequently over the last five times the feature pair appeared in an in-stance. In the case of ties, we split the vote evenly across the labels that tied on the prediction. The first time a feature pair appears the sub-expert does not make a prediction. The Calendar problem does present some difficulties for the formal linear-max setting. At the start of the algorithm, we do not know the number of classes, and we do not know the number of experts. There is not enough space to go into detail, but it is straightforward to add new classes and ex-perts as they appear. We also need to deal with experts that do not predict every trial. This is not a problem because experts who predict zero for all labels do not effect the pre-diction and do not get updated. For computational speedup, we just ignore these instances in the sparse instance repre-sentation.
 Table 1 gives the accuracy of the various algorithms on the four tasks. We include several versions of Winnow that correspond to different multiplier parameters. As can be seen in the table, Perceptron gives better performance on the day prediction task. In addition, we also find that choosing a large multiplier can improve normalized Win-now X  X  performance on the start task.
 Given that different algorithms perform well on different tasks, we need a way to combine the performance of the al-gorithms. The entry called Combined in Table 1 gives the accuracy of the Weighted Majority algorithm (WMA) (Lit-tlestone &amp; Warmuth, 1994) when combining the results of all the other algorithms. WMA keeps a weight for each ba-sic algorithms and predicts according to the weighted sum of the basic algorithm predictions. Each basic algorithm predicts one of the labels and every time a basic algorithm predicts incorrectly, its weight is divided by a multiplier. We looked at a few multiplier values, and while they all were similar, 1.5 gave slightly better performance. The average performance of the combined WMA is a few percentage points better than the best results for user two given in Blum (1995). However, we did perform slightly worse on the day and location problems. We assume this is due to a difference in representation. However, the point of our experiments is not to give the best performance possi-ble. Instead, we want to show, for a realistic problem, that non-Winnow linear-max algorithms can give good perfor-mance. We could most likely get better performance by either combining the algorithms in Blum (1995) with our own using WMA or further exploring the features that lead to the differences in performance.
 While WMA is similar to the multi-class techniques we describe in this paper, it has a few important differences. First, it always updates. It does not matter if the global pre-diction of WMA is correct, the weights are still updated. Second, the update only depends on whether the basic algo-rithm makes the wrong prediction. The update does not de-pend on the difference between the correct label and the al-gorithm X  X  predicted label. These differences are important. For this second layer of learning, we just want to quickly find which algorithm is making the least mistakes and pre-dict with that basic algorithm. Using the algorithms pre-sented in this paper, we will eventually find the better per-forming basic algorithm, however since these multi-class algorithms do not update as aggressively as WMA, they will make more mistakes in this process. If there is no good combination of experts to learn, it is better to use WMA. As can be seen, in Table 1, WMA often performs as well the best from the group of algorithms. At worst it makes a few extra mistakes. When no basic algorithm dominates all the problems, this allows WMA to give a performance average that exceeds all the basic algorithms. In this paper, we have given a general transformation to convert a linear-threshold learning algorithm into multi-class linear-max learning algorithm. The benefit of this transformation is that it allows the learning of multi-class functions while preserving the theoretical properties of var-ious linear-threshold algorithms.
 Linear-max algorithms learn target functions that are com-posed of sub-experts that make predictions on all classes. They are a natural choice for combining the predictions of different hypotheses such as those generated by different learning algorithms. However, for some problems, it may be difficult to come up with a large set of sub-experts. In the future, we want to come up with standard way to gen-erate useful sub-experts for different types of problems. Experiments in this paper show that a range of linear-max algorithms are useful in practice. Experiments with Per-ceptron, Romma, and Winnow show that no single algo-rithm dominates. In the future, we want to look at ways to effectively combines the performance of these different types of algorithms. As a preliminary investigation in this paper, we looked at combining several basic multi-class al-gorithms with WMA. The basic algorithms we combined included several versions of Winnow with different mul-tiplier parameters, but there are other possibilities for pa-rameters selection. Romma includes a parameter to make more aggressive updates on correctly classified instances (Li &amp; Long, 2000). In fact, all linear-threshold algorithms could benefit from this type of aggressive updating. The key problem is how to properly set the parameter. In the future, we want to look at ways to effectively search an infinite parameter space to get good performance. As can be seen with the Calendar experiments, parameter choice can effect performance on practical problems for these al-gorithms.
 We thank Haym Hirsh and Nick Littlestone for stimulating this work. and Haym Hirsh for reading over the paper and providing valuable comments and corrections.
 Allwein, E. L., Schapire, R. E., &amp; Singer, Y. (2000). Reduc-ing multiclass to binary: A unifying approach for margin classifiers. 17th International Conference on Machine Learning (pp. 9 X 16). Morgan Kaufmann.
 Block, H. D. (1962). The perceptron: A model for brain functioning. Reviews of Modern Physics , 34 , 123 X 135. Blum, A. (1995). Empirical support for winnow and weighted-majority algorithms: Results on a calendar scheduling domain. Proceeding of the Twelfth Interna-tional Conference on Machine Learning (pp. 64 X 72). Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold,
D. P., Schapire, R. E., &amp; Warmuth, M. K. (1997). How to use expert advice. Journal of the Association for Com-puting Machinery , 44 , 427 X 485.
 Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine Learning , 20 , 273 X 297.
 Crammer, K., &amp; Singer, Y. (2001). Ultraconservative on-line algorithms for multiclass problems. 14th Annual
Conference on Computational Learning Theory (pp. 99 X  115). Springer, Berlin.
 Dietterich, T. G., &amp; Bakiri, G. (1995). Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research , 2 , 263 X 286. Duda, R. O., &amp; Hart, P. (1973). Pattern classification and scene analysis . New York: Wiley.
 Grove, A. J., Littlestone, N., &amp; Schuurmans, D. (1997).
General convergence results for linear discriminant up-dates. Proceedings of the Tenth Annual Conference on Computational Learning Theory (pp. 171 X 183).
 Har-Peled, S., Roth, D., &amp; Zimak, D. (2003). Constraint classification for multiclass classification and ranking. Neural Information Processing Systems 15 . MIT Press. Li, Y., &amp; Long, P. (2000). The relaxed online maximum margin algorithm. Neural Information Processing Sys-tems Twelve (pp. 498 X 504). MIT Press.
 Littlestone, N. (1989). Mistake bounds and linear-threshold learning algorithms . Doctoral dissertation, Computer Science, University of California, Santa Cruz. Technical Report UCSC-CRL-89-11.
 Littlestone, N. (1991). Redundant noisy attributes, attribute errors, and linear-threshold learning using winnow. Pro-ceedings of the Third Annual Conference on Computa-tional Learning Theory (pp. 147 X 156).
 Littlestone, N., &amp; Warmuth, M. K. (1994). The weighted majority algorithm. Information and Computation , 108 , 212 X 261.
 Mesterharm, C. (2000). A multi-class linear learning algo-rithm related to winnow. Neural Information Processing Systems Twelve (pp. 519 X 525). MIT Press.
 Mesterharm, C. (2001). Transforming linear-threshold learning algorithms into multi-class linear learning al-gorithms (Technical Report dcs-tr-460). Rutgers Univer-sity.
 Mitchell, T., Caruana, R., Freitag, D., McDermott, J., &amp;
Zabowski, D. (1994). Experience with a personal learn-ing assistant. Communications of the ACM , 37 , 81 X 91. Nilsson, N. J. (1965). Learning machines: Foundations of trainable pattern-classifying systems . New York, NY: McGraw-Hill.
 Rosenblatt, F. (1962). Principles of neurodynamics: Per-ceptrons and the theory of brain mechanisms . Washing-
