 Nicolas Baskiotis nbaskiot@lri.fr Mich` ele Sebag sebag@lri.fr The performance of Machine Learning (ML) al-gorithms has been intensively studied in a gen-eral perspective, both empi rically and theoretically (see among many others (Holte, 1993; Wolpert &amp; Macready, 1995; Lim et al., 2000)). Currently, the rapid growth of ML and Data Mining applications also asks for easy-to-use and specific guidelines, estimating apriori whether any given algorithm is suited to a particular problem instance.
 How to select the best learning algorithm depending on the problem instance at hand has been considered a key question since the 90 X  X . This question was for-malised as a Meta-Learning problem in (Brazdil et al., 1994; Pfahringer et al., 2000; Bensusan &amp; Kalousis, 2001). The very elegant approach of meta-learning (MetaL), like all learning applications, heavily depends upon the selection of the examples and their represen-tation. Actually, how to re present MetaL examples, i.e. instances of learning problems, appeared to be a most difficult issue (Kalousis, 2002).
 This paper presents an alternative to Meta-Learning, inspired from the complexity paradigm developed in the Constraint Satisfaction community since the 90 X  X , where it is referred to as the Phase Transition (PT) paradigm (Hogg et al., 1996).
 The PT paradigm was ported to Inductive Logic Pro-gramming by Giordana and Saitta (2000); it provided a rigorous framework for investigating the scalability of existing algorithms and the impact of the complex-ity barrier on the learning p erformances (Botta et al., 2003). Along the same lines, the PT paradigm was ported to Attribute-Value Learning and used to study the feasibility of learning in k -term DNF languages (R  X  uckert et al., 2002).
 In this paper we investigate the use of the PT paradigm for constructing principled competence maps attached to any learnin g algorithm, character-ising the regions where this algorithm on average suc-ceeds or fails. On one hand, su ch competence maps can be exploited as look-up tables, providing all needed in-formation to select the best algorithm in a given region of the problem instance landscape, thereby achieving the Meta-Learning goal. On the other hand, the com-petence map attached to any particular algorithm al-lows for a precise identification of its failure region. Ultimately, the approach will hopefully lead to a bet-ter understanding of the practical frontiers of ML al-gorithms.
 The proposed approach is illustrated on the particular case of C4.5 (Quinlan, 1993), one long and still widely-used ML algorithm (Witten &amp; Frank, 1999). The C4.5 competence map built after principled and extensive experiments both demonstrates its general robustness and displays a non-trivial failure region. Specifically, the learning difficulty does not increase monotonically with the complexity of the underlying target concept. These results are discussed a nd some tentative inter-pretations are proposed.
 The paper is organised as follows. Section 2 briefly reviews related work and presents the PT paradigm. Section 3 describes a PT model for building C4.5 com-petence map. Section 4 pres ents this competence map and provides some interpretations for the observed reg-ularities 1 . A refined model is proposed and experi-mented in Section 5. The paper ends with some per-spectives for further research. This section briefly reviews works concerned with a priori estimation of a learner performance. 2.1. Meta-learning This estimation problem was formalised as a new learning problem in the Meta-Learning approach (MetaL) (Brazdil et al., 1994). MetaL thus faces two difficult issues: the selection and the representation of MetaL examples. A MetaL example most often in-volves a pair (ML problem instance, ML algorithm), labeled with the performance of the algorithm on the ML problem instance 2 .
 How to represent an ML problem instance, i.e. a set of ML examples, was tackled using diverse descrip-tors, e.g. number of examples, number of attributes, percentage of missing values, landmarkers (Pfahringer et al., 2000). The difficulty is due to the fact that these descriptors should account for the example distribu-tion in the ML problem inst ance; and characterising the example distribution is not easier than learning. A second difficulty concer ns the selection of the ML problem instances, most often derived through princi-pled perturbations of problems in the Irvine repository (Blake et al., 1998), e.g. increasing the rate of miss-ing values or incorporating irrelevant attributes. Two critical issues, the represen tativity of these problems and the choice of the pertu rbations considered, im-pose strong biases on the MetaL classifier (Kalousis, 2002). 2.2. Phase transition Relatedly, the Phase Transition paradigm was initially developed to better understand the performances of Constraint Satisfaction (CS) algorithms, and where the real ly hard problems are (Cheeseman et al., 1991). The notion of stochastic complexity was introduced for this purpose, opening new avenues of research (Hogg et al., 1996). Given order parameters on CS problems, i.e. constraint density and tightness, and a distribu-tion probability on the CS problem instances, stochas-tic complexity is viewed as a random variable con-ditioned by the order parameters. For given density and tightness values, stochastic complexity manifests as the actual complexities observed over all CS prob-lem instances with this density and tightness. Fol-lowing this paradigm, a regular complexity landscape can be observed: the actua l complexity is negligible in two wide regions, the YES and NO region, where the probability of satisfiability is respectively close to 1 and close to 0. These regions are separated by a narrow one, the so-called phase transition, where the probability of satisfiability abruptly falls from almost 1 to almost 0, and where the hardest problems on av-erage concentrate.
 The transportation of such a paradigm to Inductive Logic Programming (ILP), pioneered by Giordana and Saitta (Giordana &amp; Saitta, 2000), was meant to study the complexity barrier in ILP; it enabled observing and understanding its far-fetched effects on learning performances (Botta et al., 2003). 2.3. Feasibility of k -term DNF learning The PT paradigm was also exploited by R  X  uckert et al. to study the feasibility of learning formulas in Dis-junctive Normal Form, involving at most k disjuncts ( k -term DNF) (R  X  uckert et al., 2002). Considering as order parameters the number m of attributes (also re-ferred to as variables), the number p (respectively n )of positive (resp. negative) examples in the training set, and the number k of disjuncts, extensive experiments were conducted to estimate the probability of finding a target concept i) covering all p positive examples and rejecting all n negative examples; ii) expressed as a k -term DNF concept. Formally, a k -term DNF over variables { x 1 ,...,x m } is the disjunction of k terms, where a term is a conjunction of literals, and a literal is either a variable x i or its negation.
 In this approach, a (pessimistic) estimate of the learn-ing feasibility is provided using uniformly selected pos-itive and negative examples. However, such a model is not appropriate to the MetaL goal, as real-world train-ing examples are not usually selected and labelled from a uniform distribution. This paper focuses on estimating apriori the perfor-mance of learning algorithms. The presented approach is illustrated on the C4.5 al gorithm (Quinlan, 1993), assuming the reader X  X  familiarity with this well known ML algorithm. 3.1. Order parameters As opposed to the learning feasibility with respect to a given hypothesis space (R  X  uckert et al., 2002), the learning performance is evaluated with respect to the underlying target concept.
 In the rest of the paper, the target concept space is set to formulas in Disjunctive Normal Form. Accordingly, we consider the Rule mode of C4.5; in this mode, a set of decision trees is constructed, pruned and compiled into rules; the rules are then filtered and ordered on the training set, and the ruleset is used as a decision list on the test examples (Quinlan, 1993).
 In this way, the considered hypothesis search space coincides with the target concept space; the lack of syntactic language biases is meant to simplify the in-terpretation of the learning performance.
 Four order parameters are considered at this stage:  X  m is the number of boolean variables or attributes  X  k is the number of (distinct) terms C i in the target  X  l is the number of literals in a term, l  X  IN . A s - X  r is the imbalance ratio (fraction of positive ex-The choice of these parameters will be discussed in section 3.3. 3.2. Constructing a Competence Map For each ( m, k, l, r ) setting, 100 learning problem in-stances noted L i ( m, k, l, r ) ,i =1 ... 100 are generated; indices m, k, l, r will be omitted when clear from the context. A learning problem instance L is composed of a target concept, a training set and a test set. The target concept noted tc involves k distinct terms C ;each C i is the conjunction of l literals, set to a variable or its negation with equal probability, such that C i involves l distinct variables uniformly selected in { x 1 ,...,x m } .
 For each problem instance, a 400-example training set is generated: examples are uniformly generated in { 0 , 1 } m , labelled according to tc and filtered or uni-formly repaired until the desired fraction r of positive set is made of 400 examples evenly distributed among positive and negatives ones.
 For each learning p roblem instance L ,C4.5 3 learns from the training set a k -term DNF concept noted  X  tc ; the error Err ( L ) is the probability of  X  tc = tc ,esti-mated on the test set.
 The C4.5 error noted Err ( m, k, l, r ) averages Err ( L i ( m, k, l, r )) for i =1 ... 100. This hypersurface in the ( m, k, l, r ) landscape, viewed as a probabilistic error surface, defines the competence map of C4.5. 3.3. Discussion The main limitation of the above order parameters is that they do not induce a  X  X anonic X  representation of the target concept space with respect to the learning error. On one hand, a ( k, l )-term DNF might admit several logically equivalent but syntactically distinct expressions, corresponding to distinct order parame-ters values. Formally, this implies that the C4.5 error reported for a ( k, l ) setting can also reflect the error made with other settings. In the worst case, this might blur the competence map, smoothing the error and possibly hiding abrupt transitions in the performance landscape.
 On the other hand, C4.5 will have the same perfor-mance when the target con cept is replaced by its nega-tion (flipping the class of every example), although the order parameters attached to a ( k, l )-term DNF target concept and to its negation (a ( l k ,k )-term DNF in the worst case), differ. Similarly, this equivalence could lead to blurring the competence map.
 Such drawbacks of the order parameters can be de-tected by checking the variance of the error, that is, the precision of the competence map. Ultimately, the choice of the order parameters can mainly be justified with respect to the quality of the competence map, as defined below. This section presents and discusses the competence map constructed after the above model. 4.1. Experimental setting and goal Extensive experiments have been conducted for m  X  [1 , 30], k  X  [1 , 20], l  X  [1 ,m ], r = 1 / 1,000,000 learning problem instances for an overall computational cost of 12 days on a PC Pentium-IV. To each ( m, k, l, r ) setting is associated the error Err ( m, k, l, r ) of C4.5, measured as detailed in section 3.2.
 The goal of the experiments is both to check the rel-evance of the order parameters, and, equivalently, to produce a good quality competence map. The quality will be evaluated from both the precision (low vari-ance) and the intelligibility of the competence map. Ideally, the competence ma p should display a suffi-ciently regular behaviour, allowing for the detection of non trivial regularities. Ultimately, interpretations for these regularities should lead to a better understand-ing of the algorithm strengths and weaknesses. 4.2. Experimental results The error is in most regions very low, confirming the known robustness of C4.5 (Fig. 1). However, a failure region (error equal or greater than 20%) is observed as the term length l takesonmediumvalues( l  X  [5 , 10]), whenever the number m of variables is non negligi-ble ( m&gt; 15 in Fig. 1). It is no surprise that the learning difficulty increases with the total number m of variables, since the representativity of the training set (fixed to 200 positive and 200 negative examples in Fig. 1) decreases. The relationship between the error and the term length l appears less obvious: Err ( m, k, l, r ) first increases then decreases as l increases, for fixed m , r and k ; and the error is almost insensitive to the imbalanced ratio r .
 The fact that error increases as l first increases ( l  X  [1 , 6], Fig. 1) is naturally blamed on the myopic search of C4.5, greedily optimising the gain ratio criterion. Indeed, as the term length in creases, each one of its lit-erals becomes less discriminant; further, it is often the case that both a variable and its negation contribute to (appear in some terms of ) the target concept. Like in the standard XOR problem, the gain ratio crite-rion might thus miss the variables that contribute to the target concept. Therefore, a significant amount of look-ahead would be necessary to prevent the greedy search from becoming trapped in local optima due to erroneous early choices. In other words, the (univari-ate) gain ratio becomes a noisy selection criterion as the target concept involves more specific terms; hence the probability of making no errors along l selections based on this criterion gets exponentially low with l . When the term length l increases again ( l&gt; 10 in Fig 1), the error decreases. This empirical finding was un-expected since the learning difficulty is usually seen as proportional to the target concept complexity, and l is considered a factor of c omplexity. The fact that the failure region does not much depend on the imbal-ance ratio r is unexpected too, since imbalance exam-ple distributions are widely acknowledged as a factor of complexity. Still, Fig 2 shows that the error peak is observed for l =6or l =7( m = 25, k =15or k = 25, r =1 / 2 , 1 / 3 , 1 / 5 , 1 / 9) and the peak smoothly increases as r decreases. Similar r esults are observed for other values of m and k .
 The tentative interpretation offered for this finding is based on the phase transition effects and the learning bias toward generality (Botta et al., 2003). Specifi-cally, rules produced by C4.5 are not arbitrarily long as they must cover a significant number of training examples; on average their size is limited by the (log of the) size of the training set. In the experimental range, this maximal size, (noted l c ) is almost constant (#positive examples in [45 , 200] out of 400 examples). Therefore, the probability  X  ( m, l )foraleafinaC4.5 tree to be irrelevant (differ by at least one irrelevant literal from a generalisation of true conjunct) when learning a ( k, l )-term DNF concept is bounded by the probability of selecting at least one irrelevant literal out of l c choices. On the other hand, the probability of selecting an irrelevant feature decreases as l increases. The rise of the error as l increases up to l c is thus explained as the number of choices (hence the proba-bility of error) increases; the fall of the error for l&gt;l is explained as the error is the product of l c factors which all decrease as l increases 4 .
 More intensive experiments are required to test the above interpretation: in order to significantly modify the critical size l c , the size of the training set must be increased by one or several orders of magnitude. 4.3. Phase transition Following the CS inspiration, we also considered the satisfiability or coverage of ( k, l )-term DNFs, esti-mated as their probability of covering a uniformly se-lected example. For each l earning problem instance L , let P c ( L ) denote the fraction of examples labelled pos-itive out of 1000 uniformly extracted examples, and define P c ( m, k, l ) as the average of P c ( L i ( m, k, l 1 ..., 100. Fig. 3 shows as expected a sharp (expo-nentially fast) decrease of the concept coverage as the term length l increases. Interestingly, the region where the satisfiability abruptly drops broadly coincides with the failure region of C4.5, where the error is above 20% (Fig. 1). 4.4. Discussion The goal set in section 4.1 is only partially achieved. Although the above competence map displays inter-esting regularities, it does not allow for a precise esti-mation of the error when learning a ( k, l )-term DNF. Specifically, the error variance is high in the failure region.
 Also, the restriction to ( k, l )-term DNF languages is a severe one, as real-worl d concepts usually involve disjuncts of diverse genera lity. But several attempts made to relax this restriction and consider richer DNF languages, only result in increasing the error variance, and the imprecision of the competence map.
 These remarks lead us to consider another PT model. In this section, the model presented in section 3.2 is refined to produce a more precise and general compe-tence map of C4.5. 5.1. Observed vs Controlled Order Parameters The competence and coverage maps (Figs. 1, 3) sug-gest that C4.5 error might be related to the coverage P c of the underlying target concept.
 However, whereas parameters ( m, k, l ) allowed for di-rectly generating the learn ing problem instances, cov-erage P c is hardly a generative, co ntrollable parameter. Finding a k -term DNF target con cept, uniformly se-lected among the k -term DNF concepts with coverage P , is a difficult combinatorial problem.
 Therefore, an extended PT model is defined over k -term DNF formulas. This model takes as generative order parameters the number m of variables and the number k of terms. Target concepts are uniformly generated as in section 3.2, except for the fact that the term lengths are uniformly and independently selected in [1 ,m ]. A further requirement is that no term is subsumed by another term i n a same target concept. For each learning p roblem instance L , the coverage P ( L ) is measured (see below) and P c will be used as another order parameter, observed as opposed to generative of the model.
 The effects of noise in the data will be investigated in section 5.4. Due to space limitations, only balanced training sets ( r =1 / 2) will be considered in the rest of the paper. 5.2. Operational Competence Map We experimented the above model on 450,000 learning problem instances L , likewise composed of a target concept, a training set and a test set, generated as follows: For each m  X  X  30 , 40 , 50 , 60 , 70 , 80 } and k  X  { 5 , 10 , 15 , 20 , 25 } , 15,000 k -term DNF target concepts are generated. For each concept, the length l i of the i -th term is uniformly selected in [1 ,m ], where each term is constructed as in section 3.2, additionnally re-quiring that no term is subsumed by another one in a same target concept. The training and test sets at-tached to a given target co ncept are generated as in section 3.2 (balanced datasets).
 Foreachlearningp roblem instance L , its coverage P ( L ) is measured as the fraction of examples out of 1000 uniformly selected examples that are covered by the target concept; the error Err ( L )ismeasuredas detailed in 3.2. 5.3. Results Fig. 4 plots all 15,000 learning problem instances L considered for m =50and k = 15, with coordinates (
P c ( L ) ,Err ( L )). This figure shows that the error sig-nificantly rises when the coverage is below 50%. For all problem instances with coverage lower than 30%, the C4.5 error is above 20%. This trend is confirmed for other values of m and k . It appears that the coverage is a weak predictor of the error, especially when the coverage is close to 50%. Another quantity was con-sidered, the average term coverage P ac defined for each problem instance with target concept tc = C 1  X  ...  X  C k as the average coverage of terms C i . Fig5plotsall learning problem instances L considered for m =50 and k = 15, represented as ( P ac ( L ) ,Err ( L )). The legibility of Figs. 4 and 5 is hindered as the dis-tribution of problem instance is far from uniform with respect to P c , with a bias toward high coverage target concepts. Following (Chapelle et al., 2000), these dis-tributional effects are filtered using the convolution of the error with a Gaussian kernel of parameter K : Fig. 6 (left) displays the error behaviour versus the concept coverage P c for m =50and k in { 10 , 15 , 20 , 25 } , for the Gaussian parameter K = 100. Similar behaviours are observed for other values of m . A competence region, where the error is lower than 10%, is observed for high coverage concepts P c &gt; 50%, while an error peak is observed around P c = 10%. An even clearer picture is obtained in Fig. 6 (right), show-ing the error behaviour versus the average term cov-erage P ac ,for m =50and k in { 10 , 15 , 20 , 25 } ,with K = 100. In all settings, the error is lower than 5% for an average term coverage P ac &gt; 5% (competence region), while the error rises abruptly when P ac &lt; 4% (failure region).
 The competence maps obtained for other settings were similar, showing a broad comp etence region, where the error is less than 5%, and a smaller failure region, where the error is higher than 25%.
 These competence regions and failure regions, broadly observed for P ac &gt; 5% and P ac &lt; 4%, are separated by a narrow region where the error rises abruptly. Further studies will investigate this transition. Fi-nally, it appears that the coverage and the average term coverage together provide a good quality estima-tion of the C4.5 error, bounding apriori the gener-alisation error according to the coverage and average term coverage (Table 1). Specifically, for all problems with coverage less than 50%, an average term cover-age less than 4% implies an error greater than 20% ((
P ac &lt; 4%)  X  (( Err &gt; 20%), with support 60% and confidence 90%), and reciprocally (with confidence 85%). 5.4. Sensitivity to noise The sensitivity of C4.5 to noise was investigated, fo-cusing on label noise 5 . The training sets used in the above experiments were corrupted by flipping the ex-ample labels with probability =0 , 1 ,.. 20%. The test sets are unchanged. The above competence map (Fig. 7) demonstrates the known C4.5 robustness with re-spect to label noise. The predictive error smoothly increases with the data noise, almost linearly in the competence region (from 3% to 20% as goes from 0% to 20%) while it increases comparatively less in the failure region (from 35% to 41%). Indeed, several fundamental frameworks have been proposed for analysing the generalisation error from a theoretical perspective, ranging from PAC learning (Valiant, 1984; Kearns &amp; Vazirani, 1994) to statistical and non-parametric learning (Vapnik, 1998; Devroye et al., 1996); these frameworks have been foundational for the development of new and powerful algorithms (Schapire, 1990; Sch  X  olkopf et al., 1998). Indepen-dently, many empirical studies have been undertaken to evaluate ML algorithms on artificial and real-world problems (Lim et al., 2000).
 The work reported in this paper takes a different form, which is more familiar in empirical sciences, where principled experiments allow for gathering facts and organising them in such a way that non trivial regular-ities can be observed, and thereafter interpreted into a model of the phenomenon under study. Along these lines, we proposed a methodology for modeling a learn-ing algorithm, accounting for the complex interactions between efficient learning heu ristics, and specificities of the example distribution 6 .
 The model obtained thro ugh the competence map extensionally describes the algorithm behaviour, i.e. through look up tables. By exploiting (at the moment manually) these tables, some regularities are found. A first result is that C4.5 does better on more general concepts in the experiment range, which appears apos-teriori natural due to the greed y search bias effects. As an added value however, the competence map specifi-cally localises the competence region to problems with coverage above 30%. A second finding regards the phase transition observed, and the steep rise of the er-ror as the average term coverage decreases below 5%. Again, though this transition might be explained in the well-known framework of Small-Disjunct problems (Holte et al., 1989), the competence map brings in an added value as it shows precisely where the trouble begins.
 This work opens up several perspectives. The pro-posed methodology must be confronted to other algo-rithms (e.g. CN2) and target concept spaces. In paral-lel, the wealth of data gathered about C4.5 behaviour will be better exploited, e.g. providing analytical mod-els of the error and ideally identifying the deep causes for the failure cases. The abrupt transition of the er-ror will be investigated with respect to additional order parameters of k -term DNF learning (e.g. probability for examples in distinct terms of admitting a correct lgg, not covering any negative training example) and with respect to the order statistics of the gain ratio criterion, inspired from Stoppiglia et al. (2003). Modestly, the presented approach aims at a better un-derstanding of the frontiers of ML algorithms, using an empirical approach to see where the really hard prob-lems are .
 The authors thank Antoine Cornu  X  ejols, C  X  eline Rou-veirol, Erick Alphonse, Jacques Ales-Bianchetti and Mary Felkin for many discussions. Thanks are also due to the anonymous reviewers for their insights and suggestions.

