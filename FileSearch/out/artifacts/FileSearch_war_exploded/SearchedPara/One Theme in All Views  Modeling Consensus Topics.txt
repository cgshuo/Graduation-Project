 New challenges have been presented to classical topic models when applied to social media, as user-generated content suf-fers from significant problems of data sparseness. A variety of heuristic adjustments to these models have been proposed, many of which are based on the use of context information to improve the performance of topic modeling. Existing con-textualized topic models rely on arbitrary manipulation of the model structure, by incorporating various context vari-ables into the generative process of classical topic models in an ad hoc manner. Such manipulations usually result in much more complicated model structures, sophisticated in-ference procedures, and low generalizability to accommodate arbitrary types or combinations of contexts. In this paper we explore a different direction. We propose a general solu-tion that is able to exploit multiple types of contexts with-out arbitrary manipulation of the structure of classical topic models. We formulate different types of contexts as multi-ple views of the partition of the corpus. A co-regularization framework is proposed to let these views collaborate with each other, vote for the consensus topics, and distinguish them from view-specific topics. Experiments with real-world datasets prove that the proposed method is both effective and flexible to handle arbitrary types of contexts. H.3.3 [ Information Search and Retrieval ]: Text Mining Algorithms, Experimentation Topic modeling; multiple contexts; user-generated content; co-regularization
This study is done when the first author is visiting Univer-sity of Michigan.

This is the era when we witness how social media comple-ments, competes with, and eventually substitutes the role of traditional media. Online social communities such as Facebook and Twitter have gained increasing popularity and have eventually transformed into an essential compo-nent in our everyday life. Along with this  X  X ocial move-ment X  is the creation of a huge amount of user-generated content. According to recent statistics, more than 500 mil-lion microblogs (i.e., tweets) are posted by Twitter users every day 1 . Such a large volume of user-generated content implies a great opportunity for business providers, adver-tisers, social observers, as well as data mining researchers. Many interesting data mining tasks have been proposed and performed on the user-generated content in social media, which have not only led to a better understanding of user behaviors in online communities, but also led to more ef-fective techniques of content analysis, information retrieval, and recommender systems.

Textual documents in traditional media, such as newspa-pers, are professionally formatted and edited, characterized with a benign length of documents and a controlled size of vocabulary. User-generated content in social media, on the other hand, is characterized with extremely short docu-ments, extremely large and evolving vocabularies, and inac-curate uses of language. As an example, a microblog (a.k.a., tweet) in Twitter has a limited length of 140 characters. The sparsity and noise in these user-generated  X  X ocuments X  have introduced new challenges to classical text mining tech-niques that are effective for traditional media.

One good example is statistical topic modeling [7, 3], which has drawn a lot of attention recently because of its principled mathematical foundation and effectiveness in ex-ploratory content analysis. However, classical topic models usually fail to perform as effectively when applied to user-generated short messages [9]. To improve the performance of topic modeling for social media, a variety of heuristic ad-justments have to be applied to the classical models [9, 31].
This is perhaps not surprising given that a topic model essentially works by utilizing the document level word co-occurrences [26, 25]. When the co-occurrence information in a document is sparser and noisier, the performance is in-evitably compromised. To effectively apply topic modeling to social media, one has to resort to other types of informa-tion beyond word co-occurrences at the document level. http://www.telegraph.co.uk/technology/twitter/9945505/ Twitter-in-numbers.html
Luckily, a richer set of context information (e.g., time, lo-cation, authorship, friends, followers) is usually observable in social media as a compensation of the compromised qual-ity of the content in individual documents. Indeed, it has been a common practice to utilize various types of contexts in order to improve the quality of topic modeling in social media [22, 31, 16]. Many new topic models have been pro-posed with one shared intuition  X  to not only utilize the word co-occurrences at document level but also utilize the signals at the level of various types of contexts. While the intuition of utilizing context information in topic modeling is sound, most of these treatments rely on arbitrary manipulation of the graphical structure of classical topic models. In those contextualized topic models, different types of contexts are integrated into the generative process in an ad hoc manner (e.g., as specific model variables or priors) [24, 1, 18]. This results in models with more sophisticated structures and in-evitably more complicated inference procedures [1, 31]. Al-though these models work well with particular type of con-text, it is very hard to generalize any of them to handle other types of contexts, or a combination of multiple types of contexts. In real applications, the overhead of finding an effective model structure given a new type of contexts is considerably high.

In this paper, we take the initiative to explore a different direction. Our goal is to find a general solution of utiliz-ing various types of contexts in topic modeling without the overhead of arbitrarily manipulating the graphical structure of the classical topic models. We give a general definition to different types of contexts, interpreting them as multiple views of the  X  X artition of documents X  (see Figure 1), which provide different levels of word co-occurrence information. In Figure 1, the collection of text is partitioned by the con-text of time into  X  X seudo-documents X  that are posted at par-ticular time intervals; the context of author partitions the corpus into pseudo-documents that are posted by particular users. Under this general definition, the partition of the cor-pus with original document boundaries is also considered as a special type of context. Among the views defined by dif-ferent types of contexts, some are more resilient than others, which partition the content into  X  X ocuments X  with a benign length and a more discriminative distribution of topics.
Our intuition is to facilitate the collaboration among these views (different types of contexts) in order to improve the quality of the extracted topics. Indeed, a set of topics can be extracted by applying a classical topic model to the corpus partitioned with any of these views. A topic is robust and trustful if it stands out from multiple views (multiple types of contexts) instead of from just one single view. There-fore, if multiple types of contexts can  X  X ollaborate X  with each other in some way, or if a richer context can  X  X elp X  a poorer one in some sense, the topics extracted from the corpus are expected to be of a much better quality. This is especially encouraging since most existing contextualized topic models emphasize on the distinction between different types of con-texts instead of the collaboration of them. The distinction of contexts makes the sparse signals in the data even sparser, while the collaboration strengthens the signals. Instead, we encourage different views to collaborate with each other, re-inforce each other, and vote for a consensus of topics.
This collaboration process can be achieved through a novel co-regularization framework in which topics extracted from each view (type of context) are regularized by topics from Figure 1: Multiple views of a contextualized collec-tion of user-generated content. Each type of context (time, user, hashtag) defines a partition of the entire corpus. A particular value of a context (e.g.,  X 2008, X   X  U 2 , X  X #kdd2013 X ) defines a pseudo-document (rect-angular areas). Original document boundaries de-fine an organic view of the corpus. other views. The co-regularization framework simultane-ously maximizes the log-likelihood of the collection of doc-uments partitioned with regard to each individual views and meanwhile minimizes the disagreement among differ-ent views. An effective algorithm is proposed to optimize the co-regularized objective and the inference procedure re-mains as simple as those of the classical topic models.
We evaluate the proposed framework with two large-scale real world datasets. Experimental results show that when multiple contexts presented are sufficient for topic modeling, the collaboration of them can effectively improve the quality of topics extracted. The co-regularization framework out-performs other contextualized treatments of topic models, including those with manipulated graphical structures.
Much existing work has been presented which integrates various types of contexts information into topic models (e.g., [24, 31, 1]). Rosen-Zvi et al. proposed the author-topic model that utilized authorship information for modeling of scientific publications [24]. In their model each author is associated with a multinomial distribution over topics. For each word in a document, an author is uniformly sampled from the set of authors of the paper and then a topic as-signment is sampled from the multinomial distribution as-sociated with that author. Ahme et al. developed a model named multi-view topic model that utilizes ideological in-formation [1]. Topics of the corpus are divided into factual topics and ideological-specific topics. For each word in a document, a switching random variable is sampled from a Bernoulli distribution to determine whether the word is gen-erated from a factual topic or an ideological-specific topic, similar to flipping a coin. Many other contextualized topic models are proposed recently, with a common practice to in-tegrate particular types of contexts into the graphical struc-ture of classical models like the latent Dirichlet allocation (LDA). These methods introduced either additional layers to the model (e.g., [24, 14, 10, 30, 29, 8]) or a coin-flipping selection process to select among contexts (e.g., [1, 21]). Al-though these models work well with given types of contexts, they all resulted in more complicated model structures and consequentially more sophisticated inference procedures.
Another trend of literature related to our work is topic modeling of user-generated content. Here we present a few representative pieces of work, especially those with a focus on tweets [28, 22, 9]. Weng et al. deployed LDA on the tweets by aggregating all the tweets of the same user and treating each user as a document [28]. This in fact corre-sponds to the use of the user context to partition the data. Hong et al. performed an empirical study of topic modeling in twitter and several aggregation strategies are proposed to train LDA on a dataset of tweets [9]. Work of this kind usually employs a single type of context [31].

These contextualized topic models and treatments are all designed for specific types of contexts and hard to generalize to treat other types of context or a combination of multiple contexts. Our work differs from these by enhancing the col-laboration among multiple, arbitrary types of contexts with-out manipulating the graphical structure of classical topic models. This appears to be a more effective and more gen-eralizable solution of contextualized topic modeling.
Another research direction that is weakly tied to our in-tuition is Multi-view Clustering [2], in which multiple inde-pendent views of the data are available and each of them is assumed to be sufficient for clustering. The existing ap-proaches generally aim to exploit the multiple views of the data to discover the clusters that agree across the views. Bickel et al. proposed a Co-EM based framework for multi-view clustering [2]. The Co-EM algorithm iteratively per-forms the E-step in one view, the result of which is passed to an M-step in another view. Kumar et al. proposed a co-training approach for multi-view spectral clustering [12]. Specifically, the spectral embedding from one view is used to constrain the similarity graph used for the other view. In [13], they further proposed a co-regularization framework to regularize the clustering hypotheses across the views. In multi-view clustering, each view corresponds to a represen-tation of the same data points with different features and the target is to cluster the data points by making use of mul-tiple types features. Differently, in our problem a view is provided by a partition of the corpus with a type of context, which reflects the co-occurrence information among words at different levels. The target is to simultaneously assign words and documents into topics by utilizing the different levels of co-occurrence information, which is quite different from the multi-view clustering task.
We start with the intuition that topic modeling essentially relies on the signals of word co-occurrence in documents [26, 25]. Words that frequently co-occur in the same documents are likely to be grouped into the same topic, while words do not co-occur tend to be separated into different topics. Such a process echoes the famous assumption of the  X  context of situation  X  in linguistics, which was first coined by the anthropologist B. Malinowski [23] and then elaborated by J. R. Firth in the quote  X  You shall know a word by the company it keeps (J.R. Firth. 1957) [5]. X  The basic idea of Firth X  X  perspective is that the meaning of a word can be derived from the words with which it co-occurs.

It is a key assumption in all topic models that individual documents present concentrated identities on a few topics. Words in the document are more likely to present the same topic identities as the document. Two connections can be made between topic modeling and the aforementioned lin-guistic intuition. First, the meaning of a word is repre-sented by the identities of the word among a set of topics . Second, every document provides a context , in which the topic identities (meanings) of one word can be derived from the topic identities of the other words co-occurring in this context. This intuitive connection can be further elaborated using a classical topic model, the LDA [3].
The latent Dirichlet allocation (LDA) model assumes that each document is generated from a mixture of topics, with each topic corresponding to a multinomial distribution over all the words in the vocabulary. The detailed generative process of each document w is described as follows: 1. Sample a document topic proportion  X   X  Dirichlet(  X  ). 2. For each word w n in the document,  X  is the Dirichlet prior for the topic proportion. K is the total number of topics and  X  = {  X  k } k =1 ,...,K is the set of topic word distributions.

LDA extracts topics from text by calculating the poste-rior probability of the hidden variables, the document topic proportion  X  and the topic identities of the words z = { z given all observed words w = { w n } in the documents: This distribution is however computational intractable. In existing literature, variational inference [3] and Gibbs sam-pling [6] methods have been proposed to approximate the posterior distribution in order to achieve a tractable solu-tion. In either way, such a process essentially attempts to infer the topic identities of every word in a document.
We can clearly observe the critical role of the document-level word co-occurrence. Indeed, from Equation 1 one can observe that the topic identity of an individual word ( z not only depends on the word w n itself, but also depends on all the other words in the same document ( w ). This well elaborates the role of the context of situation (in this case a document). Such co-occurrence signal is carried through to the M-step, where the topic distributions are estimated with all words sharing the same identities of topics. Should the assumption of the  X  context of situation  X  fail, or should the contexts provide insufficient signals of meaningful word co-occurrences, a topic model is unlikely to perform well.
Topic modeling relies heavily on the contexts of situa-tions that provide sufficient and meaningful signals of word co-occurrences. Naturally, every document provides such a context. A topic model performs well when such  X  X oc-uments X  present sufficient signals of word co-occurrences. This condition is true in news articles and scientific papers, where topic models are proved to be effective. This con-dition is not true in user-generated content, where organic documents are extremely short. Classical topic models (e.g., LDA) fail because the document contexts can no longer pro-vide sufficient signals of word co-occurrences.

On the contrary of extremely short documents, a message in social media is usually associated with a rich set of meta-data (e.g., time, location, authorship, hashtags, etc.). Some of these metadata variables provide alternative and benign  X  X ontexts of situations X  rather than the organic documents. In many of these contexts, the assumption that  X  semanti-cally related words co-occur  X  still holds, and the signals of word co-occurrences become abundant. When the signals are inadequate within organic documents, we can resort to the various types of contexts for help. Motivated by this intuition, we formally define the essential concepts and the problem of multi-contextual topic modeling.

Definition 1. Context, View. A context is an arbi-trary subset of the corpus, which corresponds to either an organic document or text that share the same value of a metadata variable. A type of context refers to this meta-data variable, the values of which defines a partition of the corpus. A view of a text collection is defined as this parti-tion of the corpus according to a particular type of context.
Figure 1 provides an intuitive explanation of contexts and views in a collection of tweets. Tweets are associated with the metadata variables including the user , the posting time , and the hashtags (user created keywords starting with #). A corpus of tweets can then be partitioned by either different users , different time , or different hashtags . Clearly, we have generalized the notion of a  X  X ontext X  so that organic tweets become a special type of context. The organic document boundaries then define a special view of the corpus, among with many other views defined by other types of contexts (e.g., time, user, hashtags). A specific context (e.g., user = U 2  X ) becomes a  X  X seudo-document, X  which can be derived by aggregating all messages that share the same value of this type of context. For example, tweets written by the same user assemble a  X  X seudo-document X  under the user view ; tweets containing the hashtag  X #kdd2013 X  are grouped into a  X  X seudo-document X  under the hashtag view.

Definition 2. Topic, Topic Modeling. A topic  X  is defined as a multinomial distribution over words in the vo-cabulary V , i.e. { p ( w |  X  ) } w  X  V . Given a text collection D and a predefined number K , topic modeling aims to discover the K salient topics {  X  k } k =1 ,  X  X  X  K from D .

In classical topic models, the inference of topics is done with the natural partition of the corpus into documents . Now that a type of context (or a view ) also offers a partition of the corpus, one can actually conduct topic modeling with the  X  X seudo-documents X  partitioned using this view of cor-pus instead of using the original documents. By doing this, the topic model utilizes the signals of words co-occurrences at a context level instead of at the document level. A ro-bust topic should appear no matter whether the inference process is done using organic documents or using  X  X seudo-documents X  of a different view. When multiple types of con-texts appear, one could imagine a topic model leveraging the signals of word co-occurrences in different views. Topics that appear to be salient in all these views should be the most representative ones of the collection. In other words, these globally salient topics present the consensus among multi-ple types of contexts. Meanwhile, these global topics may present a specific projection onto particular types of context, which represent the view-specific interpretations of the con-sensus topics. In this paper, we aim to address the problem of finding such consensus topics and view-specific top-ics by incentivizing individual contexts to collaborate with each other. Formally, we define the problem as follows: Definition 3. Multi-contextual Topic Modeling.
 Let C be the set of context types (views). Each user-generated message m is represented by a pair of vectors ( w m where w m is a vector of words that represent the textual con-tent of the message and v m = ( v m 1 ,v m 2 ,  X  X  X  ,v m | C | the context information associated to m . v m i presents the observed value (e.g., the date Aug. 8th) of the i -th type of context (e.g., time). | C | represents the total number of dif-ferent types of contexts. Given a collection of user-generated messages { ( w m , v m ) } M m =1 , multi-contextual topic mod-eling aims to discover the consensus topics {  X  k } k =1 , 2  X  X  X  ,K that are the most robust across multiple types of contexts, corresponds to the specific instantiations of the consensus topics in each view.
 Note v m i can take either a scalar value or a set of values. For example, a scientific paper can have more than one au-thors, thus v m i may contain a set of names under the user context. Similarly, a tweet may contain multiple hashtags.
The problem is substantial challenging because the set of views ( C ) may be arbitrary. Multi-contextual topic model-ing calls for a method that could handle arbitrary types of contexts instead of introducing specific treatments to partic-ular types of contexts. Such a method should facilitate the collaboration among different contexts, with a procedure to  X  X ote X  for the consensus topics.
In this section, we describe our two methods to tackle the problem. We first introduce a naive treatment with a vari-ation of the classical LDA model. Like many existing con-textualized topic models, this model also introduces context variables into the graphical structure. The difference is that it provides a general flexibility to handle arbitrary types of context. Like the existing contextualized topic models, this model is also likely to suffer from the problem of data sparse-ness. In Section 4.2, based on the motivation to enhance dif-ferent types of contexts to collaborate with each other, we propose a co-regularization framework that provides a more principled solution to the problem.
It is common practice in contextualized topic models, such as the Author-Topic-Model (ATM), to integrate one or more context variables (e.g., the authorship ) into the generative process or the graphical structure of LDA. Such models are usually capable of handling one or a few given types of contexts (e.g., time, author), but ineffective to incorpo-rate multiple, arbitrary types of contexts. One straightfor-ward solution is to employ a generalization of ATM to multi-contextual topic modeling, by treating all different contexts (e.g.,  X  X ug. 12th, X  X  X hicago X  ) equally as  X  X uthors X  of a doc-ument. Such a treatment fails to distinguish different types of contexts, all of which are flattened into one single view.
Following the common practice but aiming at a generaliz-able solution, we introduce a multi-contextual LDA (mLDA) model that incorporates multiple, arbitrary context vari-ables as another contextual layer into the graphical structure of LDA. In the corresponding generative process, a particu-lar type of context (i.e., a context variable) is selected with a switching random variable. Specifically, the generative pro-cess is defined as follows: 1. For each topic k  X  X  1 ,...,K } , 2. For each context (pseudo-document) x from all views 3. For each message m ,
Gibbs sampling can be used for the inference of the multi-contextual LDA. The conditional probability of the hidden variables for each word w i is calculated as follows: p ( z i = k,x i = j,c i = l | w i = w, w  X  i , z  X  i , c  X  i where c i = l , x i = j and z i = k represent that the i th word is assigned to context type l , context value j of type l , and topic k . n m,l is the number of times that words in m are generated by context type l ; n j,k is the number of times that a word in context j is assigned to topic k ; and n k,w is the number of times that word w is assigned to topic k , all excluding the current word token. Note that a user-generated message may contain only a subset of all types of context. For example, not all tweets contain hashtags. In the model, we constraint the value of c i to the set of observed types of context of the current message. Clearly, when there is only one type of context (e.g., author), the multi-contextual model boils down to a special case that is identical to the author-topic model.

This simple extension of LDA provides a reasonable base-line for mining consensus topics from multiple contexts. The benefit of this model is apparent: it generally handles arbi-trary types and combinations of context information, with-out the need to find a specific manipulation of the model structure for each particular setup. However, we do fore-see a potential concern of such a model, or rather a general problem of all existing contextualized topic models that em-ploy a selection process over multiple contexts. That is, the model essentially works to split the words in each message and assign them to different types of contexts. Such a pro-cess inevitably makes the available information of a particu-lar context even sparser. This might not be a problem when the document-level information is abundant (e.g., scientific papers), but raises a serious concern for user-generated mes-sages in social media as every message is already very short. We will carefully analyze this issue in Section 5.

The key deficiency of such contextualized topic models (e.g., ATM, mLDA) is that different types of contexts are competing for resource (i.e., words), instead of collaborating to vote for the consensus topics. Our second attempt is to find a method that facilitates the collaboration among different types of contexts, without making the data sparser for each particular context.
As mentioned previously, each type of context provides an independent view of the corpus, or an independent partition of the content space. Based on each type of context we can derive a collection of  X  X seudo-documents X  and deploy an in-dependent topic modeling process on this collection. Such an approach enjoys the property that information in individ-ual contexts will not be further sparsified, and no arbitrary manipulation is needed on top of the classical topic models. What we need here is a mechanism to push different views to collaborate with each other in order to reach a consensus of the topics. We propose a co-regularization framework to make different types of contexts agree with each other on the topics discovered in each of their own views. Specifi-cally, we adopt a centroid-based regularization schema. We introduce a set of general topic distributions shared across different views (i.e., the consensus topics) and make the top-ics discovered through each individual view be close to these global consensus topics. In this way, the consensus topics serve as a bridge to make the view-specific topic modeling processes collaborate with each other. Specifically, we in-troduce the following regularization function to measure the disagreement between the consensus topic distributions and view-specific distributions:  X  = {  X  k } K k =1 is a set of the general topic distributions (con-sensus topics) and  X  c = {  X  c k } K k =1 is the set of topic distribu-tions discovered independently through the view of context type c (view-specific topics). d (  X  ,  X  ) is used to measure the distance between two distributions, and here we adopt the Kullback  X  Leibler (KL) divergence [11], i.e.,
Finally, we construct an objective function that consists of the log-likelihood functions of topic modeling in each view penalized by the above regularization term. That is, where l c (  X  c , X  c ) is the log-likelihood of the collection of pseudo-document derived by partitioning the content space using the context type c , i.e.,  X  is a regularization parameter used to trade-off between maximizing the likelihood of each view-specific topic mod-eling process and minimizing the disagreement among the topics discovered by each of the processes.

The objective function (5) is still computational intractable as the log-likelihood of each view-specific topic modeling process is intractable. We can still resort to variational in-ference for a tractable lower bound of the log-likelihoods. Omitting the equivalent details of the variational inference to LDA, we summarize the final updating equations below: The updating equations (7) and (8) are the same as the E-step in LDA model [3]. In Equation (9), we can see that the estimation of the view-specific topics depends on not only the identities of all the word tokens in the current view but also the consensus topics across views. In Equation (10), a consensus topic is achieved as the average of the view-specific topics of all views.

We anticipate that this co-regularization treatment would provide a more effective process to mining consensus top-ics than the multi-contextual LDA and additional benefit of mining view-specific topics. This is because it enhances the collaboration among different types of contexts without making the data sparser. In next section we present experi-ments using real-world datasets to verify our intuitions.
We introduce two real-world datasets for our experiments: one consists of messages (tweets) sampled from Twitter, the leading microblogging site and the other consists of titles sampled from DBLP 2 , the online bibliography database . http://www.informatik.uni-trier.de/~ley/db/ Twitter . We collect a sample of tweets using an official Twitter stream API between October 2nd, 2011 and Octo-ber 8th, 2011. Through the API, we retrieve tweets of a sample of 2,000 users who have posted at least 100 tweets during the time frame. Stopwords and words that appeared less than 100 times in the whole dataset are removed, which yields a vocabulary of 121,709 unique words 3 . We are able to identify three effective types of contexts in this dataset, including tweet , user , and hashtag . This says, the entire col-lection can be partitioned into  X  X seudo-documents X  as either individual tweets, the tweets posted by individual users, or tweets containing certain hashtags. The statistics of this dataset are summarized in Table 1.
 DBLP . Titles of scientific papers are good instantiations of short textual documents. Metadata information of a pa-per also provides a rich set of contexts. Such properties connect well with user-generated content, making titles of scientific papers a suitable dataset to verify the effective-ness of multi-contextual topic modeling. We download all the DBLP records that are labeled as conference proceed-ings. We identify three types of contexts for every record, including the title of the paper, the authors , and the confer-ence (book title) of publication. Words that appeared in less than 5 titles are removed, resulted in a vocabulary of 35,895 unique words. The statistics are summarized in Table 2.
The candidate models are classified into two categories: single-context based and multi-context based.
A few users are dropped who have no nonempty tweet left after this step
In all comparisons, we predefine the number of topics. We did not tune this parameter because our goal is not to find the optimal number of topics but to compare the treatments of contexts. All the results reported below are averaged over 10 independent runs with random initialization.
Finding an objective metric for the comparison of topic models is hard. Many existing studies utilized the perplexity or the likelihood of held-out data. However, such statistics cannot directly measure the quality (e.g., the semantic co-herence) of the learned topics. In [4], Chang et al. presented quantitative methods to measure the topical coherence of learned topics. They found that the likelihood of the held-out data is not always a good indicator of topic coherence. To tackle this problem, we introduce two metrics which di-rectly measure the quality of topics discovered.
 Topic Coherence. Recently, measuring the semantic co-herence of the learned topics has received increasing atten-tion [19, 17]. By measuring how semantic coherent the words in a topic are, one has a better sense of whether the extracted topics are interpretable and to what extent such topics help end-users for exploratory data analysis. In [19], Newman et al. proposed to use the point-wise mutual information (PMI) to measure the semantic coherence of topics. For each topic, the PMI-score calculates the average relatedness of each pair of the words ranked at top-N: PMI-Score( w ) = 2 where w are the top N most probable words of the topic. p ( w i ,w j ) is the probability of words w i and w j co-occurring in the same document while p ( w i ) is probability of word w appearing in a document. These probabilities are computed from a much larger corpus. N is set to 20 in our analysis. Entity Clustering. Another approach to evaluate topic models is to use the learned topics for an external task, and thus assess the quality of topics based on their performance on the task [15, 27]. In this paper, we utilize the learned topics for a clustering task of entities. We select author clus-tering as the external task to evaluate the topics extracted from the DBLP dataset, and user clustering to evaluate the topics extracted from the Twitter dataset. The number of clusters is set the same as the number of topics. Each entity (author or user) is assigned to the topic that is the most prevalent in the pseudo-document corresponding to that en-tity.

We select the problem of author/user clustering because there are well established metrics for such a task, even when a ground truth is not available. For example, when the interconnections among the authors/users are available, the metric modularity [20] is a well accepted metric to evaluate the clusters of the actors (communities) in a network. The modularity score measures the quality of the divisions of a network of actors, which is defined as: m is the total number of edges in a network. A is an adja-cency matrix where A ij is 1 if there exists an edge between node i and node j and 0 otherwise.  X  (  X  ,  X  ) is an indication function, which equals to 1 if node i and node j falls into the same cluster otherwise 0. k i is the degree of node i .
To calculate the modularity of a clustering of authors/users, we first construct a network of the actors. This is straightfor-ward since natural network structures of users/authors exist in both Twitter and DBLP. Specifically, the co-authorship network in DBLP is used to evaluate author clustering . If two authors have co-authored at least one paper, then an edge is defined between the two authors. We use the retweet network in Twitter to evaluate user clustering . If a user has re-tweeted at least one tweet of another user, then there exists an edge between them. The statistics of the two net-works are summarized in Table 3.

We selected the two metrics, PMI and modularity, because we believe they are direct indicators of the quality of topics rather than the likelihood statistics. A better topic will yield a higher PMI score, and a better set of topics will yield a higher modularity score.
We start with a summary of the results of all candidate models on the DBLP dataset. Note that there are three types of context, or three views defined on the DBLP dataset: author , conference , and title . The candidate models produce either the consensus topics or view-specific topics, or both. For single-context based methods, only view-specific topics are generated, when LDA is applied to the view. For ATM and mLDA, only consensus topics are generated. For the co-regularization methods, both view-specific topics and con-sensus topics are generated. We apply the co-regularization method on different combinations of the views.

The PMI scores of all candidate methods are presented in Table 4. First, we compare the topics discovered based on each single type of context. We can see that LDA based on the author view achieves better results than the same method based on the conference view, which is far better than the title view. This means the performance of LDA on the organic short documents is significantly worse than when applied to  X  X seudo-documents X  partitioned based on either the authors or the conferences. This reassures our observa-tion that when the lengths of documents are too short, they fail to provide sufficient signals of word co-occurrences. Clas-sical topic models like LDA do not work well in such kinds of datasets. Both the author context and the conference con-text provide adequate signals of word co-occurrences, where classical topic models perform reasonably well. Table 4: Topic coherence of topics in the DBLP dataset. Collaboration of multiple views improves both consensus topics and view-specific topics.

Let us then look at the PMI scores of multi-contextual topic models. Incorporating multiple types of contexts into the Author-Topic Model and the multi-contextual LDA model does improve over the organic LDA model (LDA with the title view), but the performance is inferior to the best sin-gle view of the collection (i.e., LDA with the author view). This seems to confirm our concern in Section 4.1 that nei-ther of the two models encourages different types of context to collaborate with each other. The contexts rather com-pete for resources, which makes the data sparser. As for the co-regularization based methods, we can clearly see that both the view-specific topics and the consensus topics are consistently better than those discovered by single-context based methods and by the two naive multi-contextual meth-ods. This suggests that the co-regularization framework in-deed makes different contexts collaborate, not only voting for better consensus topics but also helping each other ex-tract better view-specific topics. Not only did the stronger views ( author , conference ) help the weaker view ( title ) sig-nificantly, but also did the two strong views reinforce each other. The best consensus topics are achieved when all three views are employed, which is a strong signal of the effective-ness of multi-contextual topics modeling.
 Similar findings can be observed on the Twitter dataset. Remember that there are also three types of context, or three views defined on the Twitter dataset, namely tweet , user , and hashtag . The PMI scores of the candidate systems are summarized in Table 5. Again, LDA failed to perform well when the single tweet view is applied to. Unfortunately the extremely short tweets (up to 140 characters) provided so weak signals for a classical topic model. Hashtag seemed to be a rather strong view by its own, significantly out-performing the user view, which also performed reasonably well. Among the multi-contextual methods, it is clear that the combinations of the two strong views ( user and hash-tag ) outperformed all competitors in both view-specific top-ics and consensus topics. Combining all three views does not improve over the coupling of the two strong views, which should be attributed to the severe sparseness of individual tweets (in average 3.6 words after preprocessing).

Next, we investigate how these topics are useful in par-ticular data mining tasks, by using the view-specific topics for the task of author clustering on DBLP and user clus-tering on Twitter. Only the single-context model based on author/user view and the co-regularization models that in-volve the author/user view are kept in comparison because only these models output author/user -specific topics. The results are summarized in Table 6 and Table 7.
 Table 5: Topic coherence of topics in the Twitter dataset. Collaboration of strong views improves both consensus topics and view-specific topics.

Apparently, view-specific topics using the single view of author/user performed reasonably well in the clustering tasks of authors/users. This is not surprising as both the author view (in DBLP) and the user view (in Twitter) provided sufficient signals for topic modeling, and topic modeling is essentially a way of soft clustering. Interestingly, when com-bined with another strong view, namely conference in DBLP and hashtag in Twitter, the author -specific topics and user -specific topics achieved better performance in author/user clustering. This again confirmed the effectiveness of multi-contextual topic modeling and the co-regularization method. It is interesting to see that adding the weak view of title (in DBLP) and tweet (in Twitter) does not further improve the clustering performance. This is consistent with the results using the PMI metric, where the addition of weak views im-proved the consensus topics but not the view-specific topics of the strong views. The view of organic short documents is simply too weak to provide substantial novel signals for topic modeling as long as stronger views are employed. Single LDA(Author) 0.289 Multiple
In summary, the integration of context-based views sig-nificantly improves the application of classical topic models on short text documents. When multiple contexts are avail-able, the collaboration of them through the co-regularization framework further improves the consensus topics. The com-bination of strong views (views that provide sufficient co-occurrence information) also improves the view-specific top-ics respective to individual type of context. Neither the Author-Topic Model or the multi-contextual LDA model performs as effectively as co-regularization because the two models make the data even sparser through the competition between contexts.
We also investigate the sensitivity of the performance w.r.t the regularization parameter  X  . In Figure 3, we plot the PMI scores of both the view-specific topics and the consensus topics based on the user and hashtag views (Twitter) and based on the author and conference views (DBLP). When  X  equals 0, the view-specific topics are separately trained w.r.t individual views without any co-regularization. When  X  becomes larger, the view-specific topics become closer to each other as well as the consensus topics. In general, the performance of the co-regularization framework is not sen-sitive to the parameter. When the parameter is sufficiently large, the quality of the topics becomes smooth. In practice, the parameter  X  can be heuristically set as the total number of tokens divided by the number of topics.
In this paper, we investigated the problem of exploit-ing multiple types of contexts for topic modeling in user-generated content. Instead of designing specific manipula-tions of model structure, we proposed a general co-regularization framework to facilitate the collaboration of different types of contexts. The framework can be easily extended to data sources with arbitrary types and combinations of contexts. Experimental results on two real-world datasets showed that the co-regularization framework successfully incorporated multiple types of contexts, which outperformed contextu-alized topic models with manipulated graph structures of classical topic models. One interesting problem to be done is how to select the views when many types of contexts are available. Based on our results, the combination of strong contexts significantly outperformed the use of weak contexts. How to measure the strength of a context in topic modeling appears to be a promising future direction. This work is partially supported by the National Science Foundation under grant numbers IIS-0968489, IIS-1054199, CCF-1048168. It is also partially supported by the Na-tional Natural Science Foundation of China (NSFC Grant No. 61272343) and the China Scholarship Council (CSC, No. 2011601194).
