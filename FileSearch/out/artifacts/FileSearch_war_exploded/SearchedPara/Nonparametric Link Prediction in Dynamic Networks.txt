 Purnamrita Sarkar  X  psarkar@eecs.berkeley.edu Deepayan Chakrabarti  X  deepay@fb.com Michael I. Jordan  X  X  jordan@eecs.berkeley.edu The problem of predicting links in a graph occurs in many settings X  X ecommending friends in social net-works, predicting movies or songs to users, market analysis, and so on. However, state-of-the-art meth-ods suffer from two weaknesses. First, most methods rely on heuristics such as counting common neighbors, etc. while these often work well in practice, their the-oretical properties have not been thoroughly analyzed. (Sarkar et al. (2010) is one step in this direction). Sec-ond, most of the heuristics are meant for predicting links from one static snapshot of the graph. However, graph datasets often carry additional temporal infor-mation such as the creation and deletion times of nodes and edges, so the data is better viewed as a sequence of snapshots of an evolving graph or as a continuous time process (Vu et al., 2011). In this paper, we focus on link prediction in the sequential snapshot setting, and propose a nonparametric method that (a) makes weak model assumptions about the graph generation process, (b) leads to formal guarantees of consistency, and (c) offers a fast and scalable implementation via locality sensitive hashing (LSH).
 Our approach falls under the framework of nonpara-metric time series prediction, which models the evolu-tion of a sequence x t over time (Masry &amp; Tj X stheim, 1995). Each x t is modeled as a function of a mov-ing window ( x t  X  1 ,...,x t  X  p ), and so x t is assumed to be independent of the rest of the time series given this window; the function itself is learned via kernel regres-sion. In our case, however, there is a graph snapshot in each timestep. The obvious extension of modeling each graph as a multi-dimensional x t quickly runs into prob-lems of high dimensionality, and is not scalable. In-stead, we appeal to the following intuition: the graphs can be thought of as providing a  X  X patial X  dimension that is orthogonal to the time axis. In the spirit of the time series model discussed above, our model makes the additional assumption that the linkage behavior of any node i is independent of the rest of the graph given its  X  X ocal X  neighborhood or cluster N ( i ); in effect, lo-cal neighborhoods are to the spatial dimension what moving windows are to the time dimension. Thus, the out-edges of i at time t are modeled as a function of the local neighborhood of i over a moving window, re-sulting in a much more tractable problem. This model also allows for different types of neighborhoods to ex-ist in the same graph, e.g., regions of slow versus fast change in links, assortative versus disassortative re-gions (where high-degree nodes are more/less likely to connect to other high-degree nodes), densifying versus sparsifying regions, and so on. An additional advan-tage of our nonparametric model is that it can easily incorporate node and link features which are not based on the graph topology (e.g., labels in labeled graphs). Our contributions are as follows: (1) Nonparametric problem formulation: We offer, to our knowledge, the first nonparametric model for link prediction in dynamic graphs. The model is power-ful enough to accommodate regions with very different evolution profiles, which would be impossible for any single link prediction rule or heuristic. It also enables learning based on both topological as well as other ex-ternally available features (such as labels). (2) Consistency of the estimator: Using arguments from the literature on Markov chains and strong mix-ing, we prove consistency of our estimator. (3) Fast implementation via LSH: Nonparametric methods such as kernel regression can be very slow when the kernel must be computed between a query and all points in the training set. We adapt the local-ity sensitive hashing algorithm of Indyk &amp; Motwani (1998) for our particular kernel function, which allows the link prediction algorithm to scale to large graphs and long sequences. (4) Empirical improvements over previous methods: We show that on graphs with nonlinearities, such as seasonally fluctuating linkage patterns, we outperform all of the state-of-the-art heuristic measures for static and dynamic graphs. This result is confirmed on a real-world sensor network graph as well as via simula-tions. On other real-world datasets whose evolution is far smoother and simpler, we perform as well as the best competitor. Finally, on simulated datasets, our LSH-based kernel regression is shown to be much faster than the exact version while yielding almost identical accuracy. For larger real-world datasets, the exact ker-nel regression did not even finish in a day.
 The rest of the paper is organized as follows. We present the model and prove consistency in Sections 2 and 3. We discuss our LSH implementation in Sec-tion 4. We give empirical results in Section 5, followed by related work and conclusions in Sections 6 and 7. Consider the link prediction problem in static graphs. Simple heuristics like picking node pairs that were linked most recently (i.e., had small time to last-link), or that have the most common neighbors, have been shown empirically to be good indicators of future links between node pairs (Liben-Nowell &amp; Kleinberg, 2003). An obvious extension to dynamic graphs is to com-pute the fraction of pairs that had lastlink = k at time t and formed an edge at time t + 1, aggregated over all timesteps t , and use the value of k with the high-est fraction as the best predictor. This can easily be extended to multiple features. Thus, modulo fraction estimation errors , the dynamic link prediction prob-lem reduces to the computation and analysis of multi-dimensional histograms, or datacubes .
 However, this simple solution suffers from two critical problems. First, it does not allow for local variations in the link-formation fractions. This can be addressed by computing a separate datacube for each local neigh-borhood (made more precise later). The second, more subtle, problem is that the above method implicitly as-sumes stationarity, i.e., a node X  X  link-formation prob-abilities are time-invariant functions of the datacube features. This is clearly inaccurate: it does not allow for seasonal changes in linkage patterns, or for a tran-sition from slow to fast evolution, etc. The solution is to use the datacubes not to directly predict future links, but as a signature of the recent evolution of the neighborhood. We can then find historical neighbor-hoods from some previous time t that had the same signature, and use their evolution from t to t +1 to pre-dict link formation in the next timestep for the current neighborhood. Thus, seasonalities and other arbitrary patterns can be learned. Also, this combats sparsity by aggregating data across similarly-evolving commu-nities even if they are separated by graph distance and time. Finally, note that the signature encodes the re-cent evolution of a neighborhood, and not just the dis-tribution of features in it. Thus, it is evolution that drives the estimation of linkage probabilities. We now formalize these ideas. Let the observed se-quence of directed graphs be G = { G 1 ,G 2 ,...,G t } . Let Y t ( i,j ) = 1 if the edge i  X  j exists at time t , and let Y t ( i,j ) = 0 otherwise. Let N t ( i ) be the local neighborhood of node i in G t ; in our experi-ments, we define it to be the set of nodes within 2 hops of i , and all edges between them. Note that the neighborhoods of nearby nodes can overlap. Let ~ N where 0  X  g ( . )  X  1 is a function of two sets of fea-tures: those specific to the pair of nodes ( i,j ) under consideration (s t ( i,j )), and those for the local neigh-borhood of the endpoint i ( d t ( i )). We require that both of these be functions of ~ N t,p ( i ). Thus, Y t +1 ( i,j ) is as-sumed to be independent of G given ~ N t,p ( i ), limiting the dimensionality of the problem. Also, two pairs of nodes ( i,j ) and ( i 0 ,j 0 ) that are close to each other in terms of graph distance are likely to have overlapping neighborhoods, and hence higher chances of sharing neighborhood-specific features. Thus, link prediction probabilities for pairs of nodes from the same graph region are likely to be dependent, as expected. Assume that the pair-specific features s t ( i,j ) come from a finite set S ; if not, they are discretized into such a set. For example, one may use s t ( i,j ) = { cn t ( i,j ) ,`` t ( i,j ) } (i.e., number of common neighbors and the last time a link appeared between nodes i and j ). Let d t ( i ) = {  X  it ( s ) , X  + it ( s )  X  s  X  S } , where  X  are the number of node pairs in N t  X  1 ( i ) with feature vector s , and  X  + it ( s ) the number of such pairs which were also linked by an edge in the next timestep t . In a nutshell, d t ( i ) tells us the chances of an edge being created in t given its features in t  X  1, averaged over the whole neighborhood N t  X  1 ( i )  X  in other words, it captures the evolution of the neighborhood around i over one timestep.
 One can think of d t ( i ) as a multi-dimensional his-togram, or a  X  X atacube X , which is indexed by the fea-tures s . Hence, now onwards we will often refer to d ( i ) as a  X  X atacube X , and a feature vector s as the  X  X ell X  s in the datacube with contents (  X  it ( s ) , X  + it Finiteness of S is necessary to ensure that datacubes are finite-dimensional, which allows us to index them and quickly find nearest-neighbor datacubes.
 Estimator. Our estimator of the function g ( . ) is: Sim(  X  t ( i,j ) , X  t 0 ( i 0 ,j 0 )) into neighborhood-specific and pair-specific parts: K ( d t ( i ) ,d t 0 ( i 0 ))  X  I { s s ( i 0 ,j 0 ) } . In other words, the similarity measure Sim( . ) computes the similarity between the two neighborhood evolutions (i.e., the datacubes), but only for pairs ( i 0 ,j 0 ) at time t 0 that had exactly the same features as the query pair ( i,j ) at t (i.e., pairs belonging to the cell s = s t ( i,j )). This yields a different interpretation of the estimator:
Intuitively, given the query pair ( i,j ) at time t , we look only inside cells for the query feature s = s t ( i,j ) in all neighborhood datacubes, compute the average  X  i 0 t 0 ( s ) and  X  i 0 t 0 ( s ) in these cells after accounting for the similarities of the datacubes to the query neighbor-hood datacube, and use their quotient as the estimate of linkage probability. Thus, the probabilities are com-puted from historical instances where (a) the feature vector of the historical node pair matches the query, and (b) the local neighborhood was similar as well. Now, we need a measure of the closeness between neighborhoods. Two neighborhoods are close if they have similar probabilities p ( s ) of generating links be-tween node pairs with feature vector s , for any s  X  S . We could simply compare point estimates p ( s ) =  X  . ( s ) / X  . ( s ), but this does not account for the vari-ance in these estimates. Instead, we consider the full posterior of p ( s ) (a Beta distribution), and use the total variation distance between these Betas as a mea-sure of the closeness: where TV( .,. ) denotes the total variation distance be-tween the distributions of its two argument random variables, and b  X  (0 , 1) is a bandwidth parameter. Dealing with Sparsity. For sparse graphs, or short time series, two practical problems can arise. First, a node i could have zero degree and hence an empty neighborhood. In order to get around this, we de-fine the neighborhood of node i as the union of 2-hop neighborhoods over the last p timesteps.
 Second, the  X  . ( s ) and  X  + . ( s ) values obtained from ker-nel regression could be too small, and so the estimated linkage probability  X  + . ( s ) / X  . ( s ) is too unreliable for prediction and ranking. We offer a threefold solution. (a) We combine  X  . ( s ) and  X  + . ( s ) with a weighted av-erage of the corresponding values for any s 0 that are  X  X lose X  to s , the weights encoding the similarity be-tween s 0 and s . This is in essence the same as replacing the indicator in Eq. (1) with a kernel that measures similarity between features. (b) Instead of ranking node pairs using  X  + . ( s ) / X  . ( s ), we use the lower end of the 95% Wilson score interval (Wilson, 1927), which is a widely used binomial proportion confidence interval . The node pairs that are ranked highest according to this  X  X ilson score X  are those that have high estimated linkage probability  X  + . ( s ) / X  . ( s ) and  X  . ( s ) is high (im-plying a reliable estimate). (c) Last but not the least, we maintain a  X  X rior X  datacube, which is average of all historical datacubes. The Wilson score of each node pair is smoothed with the corresponding score derived from the prior datacube, with the degree of smooth-ing depending on  X  . ( s ). This can be thought of as a simple hierarchical model, where the lower level (set of individual datacubes) smooths its estimates using the higher level (the prior datacube). Now, we prove that the estimator is consistent. Recall that our model is as follows: graphs have n nodes ( n is finite). Let Q represent the query datacube d T ( q ). We want to obtain predictions for timestep T + 1. From Eq. (1), the kernel estimator of g for query pair ( q,q 0 ) at time T + 1 can be written as: The estimator  X  g is defined only when b f &gt; 0. The ker-nel was defined earlier as K b ( d t ( i ) ,Q ) = b D ( d where the bandwidth b tends to 0 as T  X   X  , and D ( . ) is the distance function defined in Eq. (1). This is similar to other discrete kernels (Aitchison &amp; Aitken, 1976), and has the following property Theorem 3.1 (Consistency) . mator of g , i.e., Proof. The proof is in two parts. Lemma 3.3 will show that var( b h ) and var( b f ) tend to 0 with T  X   X  . Lemma 3.4 shows that their expectations converge to g ( s,Q ) R and R respectively, for some constant R &gt; 0. Hence, ( b h, b f ) P  X  X  X  ( g ( s,Q ) R,R ). By the continuous mapping theorem, The next lemma upper bounds the growth of variance terms. We first recall the concept of strong mixing. For a Markov chain S t , define the strong mixing coeffi-F algebras generated by events in S i  X  t S 0 i and S i  X  t 0 respectively. Intuitively, small values of  X  ( k ) imply that states that are k apart in the Markov chain are almost independent. For bounded A and B , this also limits their covariance: | cov( A,B ) |  X  c X  ( k ) for some constant c (Durrett, 1995).
 Lemma 3.2. Let q it be a bounded func-tion of  X  it +1 ( s ) ,  X  + it +1 ( s ) and d t ( i ) . Then, Proof Sketch. Our graph evolution model is Marko-vian; assuming each  X  X tate X  to represent the past p + 1 graphs, the next graph (and hence the next state) is a function only of the current state. The state space is also finite, since each graph has bounded size. Thus, the state space may be partitioned as S = TR S C i , where TR is a set of transient states, each C i is an ir-reducible closed communication class, and there exists at least one C i (Grimmett &amp; Stirzaker, 2001). The Markov chain must eventually enter some C i . First assume that this class is aperiodic. Irre-ducibility and aperiodicity implies geometric ergod-icity (Fill, 1991), which implies strong mixing with exponential decay (Pham, 1986):  X  ( k )  X  e  X   X k for some  X  &gt; 0. Thus, P t,t 0 cov( q it ,q jt 0 )  X  P P O (1 /T ) , which goes to zero as T  X  X  X  . The proof for a cyclic communication class, while similar in principle, is more involved and is deferred to the appendix. Lemma 3.3. var ( b h ) and var ( b f ) tend to 0 as T  X  X  X  . Proof. The result follows by applying Lemma 3.2 with q ( . ) equal to K b ( d t ( i ) ,Q )  X  + it +1 ( s ) and K b ( d t ( i ) ,Q )  X  it +1 ( s ) respectively.
 Lemma 3.4. As T  X  X  X  , for some R &gt; 0 , Proof. Let denote the minimum distance be-tween two datacubes that are not identical; since the set of all possible datacubes is fi-nite, &gt; 0. E [ b h ( s,Q )] is an average of terms t  X  { p,...,T  X  1 } . Now, E [ K b ( d t ( i ) ,Q )  X  + summing Eq. (2) over all pairs ( i,j ) in a neighborhood with identical s t ( i,j ), and then taking expectations. Writing the expectation in terms of a sum over all possible datacubes, and noting that everything is bounded, gives the following: E = E [  X  it +1 ( s ) | d t ( i ) = Q ]  X  g ( s,Q ) P ( d t Recalling that E [ b h ( s,Q )] was an average of the above terms, E [ b h ( s,Q )] equals the following.
Using the argument of Lemma 3.2, we will eventually hit a closed communication class. Also, the query dat-acube at T is a function of the state S T , which belongs to a closed irreducible set C with probability 1. Hence, using standard properties of finite state space Markov chains (in particular positive recurrence of states in C ), we can show that the above average converges to a positive constant R times g ( s,Q ). An identical argu-ment yields E [ b f ( s,Q )] converges to R . The full proof can be found in the appendix. A naive implementation of the nonparametric estima-tor in Eq. (1) searches over all n datacubes for each of the T timesteps for each prediction, which can be very slow for large graphs. In most practical situations, the top-r closest neighborhoods should suffice (in our case r = 20). Thus, we need a fast sublinear-time method to quickly find the top-r closest neighborhoods. We achieve this via locality sensitive hashing (LSH) (Indyk &amp; Motwani, 1998). The standard LSH operates on bit sequences, and maps sequences with small Hamming distance to the same hash bucket. However, we must hash datacubes, and use the to-tal variation distance metric. Our solution is based on the fact that total variation distance between dis-crete distributions is half the L 1 distance between the corresponding probability mass functions. If we could approximate the probability distributions in each cell with bit sequences, then the L 1 distance would just be the Hamming distance between these sequences, and standard LSH could be used for our datacubes.
 Conversion to bit sequence. The key idea is to approximate the linkage probability distribution by its histogram. We first discretize the range [0 , 1] (since we deal with probabilities) into B 1 buckets. For each bucket we compute the probability mass p falling in-side it. This p is encoded using B 2 bits by setting the first b pB 2 c bits to 1, and the others to 0. Thus, the en-tire distribution (i.e., one cell) is represented by B 1 B bits. The entire datacube can be stored in | S | B bits. However, in all our experiments, datacubes were very sparse with only M | S | cells ever being non-empty (usually, 10-50); thus, we use only MB 1 B 2 bits in practice. The Hamming distance between two pairs of MB 1 B 2 bit vectors yields the total variation dis-tance between datacubes (modulo a constant factor). Distances via LSH. We create a hash function by just picking a uniformly random sample of k bits out of MB 1 B 2 . For each hash function, we create a hash table that stores all datacubes whose hashes are iden-tical in these k bits. We use ` such hash functions. Given a query datacube, we hash it using each of these ` functions, and then create a candidate set of up to O (max( `, r)) of distinct datacubes that share any of these ` hashes. The total variation distance of these candidates to the query datacube is computed explic-itly, yielding the closest matching historical datacubes. Picking k . The number of bits k is crucial in bal-ancing accuracy versus query time: a large k sends all datacubes to their own hash bucket, so any query can find only a few matches, while a small k bunches many datacubes into the same bucket, forcing costly and unnecessary computations of the exact total vari-ation distance. We do a binary search to find the k for which the average hash-bucket size over a query workload is just enough to provide the desired top-20 matches. Its accuracy is shown in Section 5.
 Finally, we underscore a few points. First, the entire bit representation of MB 1 B 2 bits never needs to be created explicitly; only the hashes need to be com-puted, and this takes O ( k` ) time. Second, the main cost in the algorithm is in creating the hash table, which needs to be done once as a preprocessing step. Query processing is extremely fast and sublinear, since the candidate set is much smaller than the size of the training set. Finally, we have found the loss due to approximation to be minimal in all our experiments. We first introduce several baseline algorithms, and the evaluation metric. We then show via simulations that our algorithm outperforms prior work in a variety of situations modeling nonlinearities in linkage patterns, such as seasonality in link formation. These findings are confirmed on several evolving real-world graphs: a sensor network, two co-authorship graphs, and a stock return correlation graph. Finally, we demonstrate the improvement in timing achieved via LSH over exact search, and the effect of LSH bit-size k on accuracy. Baselines and metrics. We compare our nonpara-metric link prediction algorithm ( NonParam ) to the fol-lowing baselines which, though seemingly simple, are extremely hard to beat in practice (Liben-Nowell &amp; Kleinberg, 2003; Tylenda et al., 2009):
LL : ranks pairs using ascending order of last time of linkage (Tylenda et al., 2009).

CN (last timestep): ranks pairs using descending order of the number of common neighbors (Liben-Nowell &amp; Kleinberg, 2003).

AA (last timestep): ranks pairs using descending order of the Adamic-Adar score (Adamic &amp; Adar, 2003), a weighted variant of common neighbors which it has been shown to outperform (Liben-Nowell &amp; Kleinberg, 2003).
Katz (last timestep): extends CN to paths with length greater than two, but with longer paths getting expo-nentially smaller weights (Katz, 1953).

CN-all , AA-all , Katz-all : CN , AA , and Katz computed on the union of all graphs until the last timestep . Recall that, for NonParam , we only predict on pairs which are in the neighborhood (generated by the union of 2-hop neighborhoods of last p timesteps) of each other. We deliberately used a simple feature set for common neighbors and last-link) and not using any external  X  X eta-data X  (e.g., stock sectors, university affiliations, etc.). All feature values are binned log-arithmically in order to combat sparsity in the tails of the feature distributions. Mathematically, our fea-ture ` t ( i,j ) should be capped at p . However, since the common heuristic LL uses no such capping, for fair-ness, we used the uncapped  X  X ast time a link appeared X  as ` t ( i,j ), for the pairs we predict on. The bandwidth b is picked by cross-validation.
 For any graph sequence ( G 1 ,...,G T ), we test link pre-diction accuracy on G T for a subset S &gt; 0 of nodes with non-zero degree in G T . Each algorithm is provided training data until timestep T  X  1, and must output, for each node i  X  S &gt; 0 , a ranked list of nodes in de-scending order of probability of linking with i in G T . For purposes of efficiency, we only require a ranking on the nodes that have ever been within two hops of i (call these the candidate pairs); all algorithms under consideration predict the absence of a link for nodes outside this subset. We compute the AUC score for predicted scores for all candidate pairs against their actual edges formed in G T . 5.1. Prediction Accuracy We compare accuracy on (a) simulations, (b) a real-world sensor network with periodicities, and (c) broadly stationary real-world graphs.
 Simulations. One unique aspect of NonParam is its ability to predict even in the presence of sharp fluctu-ations. As an example, we focus on seasonal patterns, simulating a model of Hoff (personal communication) that posits an independently drawn  X  X eature vector X  for each node. Time moves over a repeating sequence of seasons, with a different set of features being  X  X c-tive X  in each. Nodes with these features are more likely to be linked in that season, though noisy links also ex-ist. The user features also change smoothly over time, to reflect changing user preferences.
 We generated 100-node graphs over 20 timesteps using 3 seasons, and plotted AUC averaged over 10 random runs for several noise-to-signal ratios (Fig. 1). Non-Param consistently outperforms all other baselines by a large margin. Clearly, seasonal graphs have non-linear linkage patterns: the best predictor of links at time T are the links at times T  X  3, T  X  6, etc., and NonParam is able to automatically learn this pattern. However, CN , AA , Katz are biased towards predicting links between pairs which were linked (or had short paths connecting them) at the previous timestep T  X  1; this implicit smoothness assumption makes them suf-fer heavily. This is why they behave as bad as a ran-dom predictor (AUC 0.5).
 Baselines LL , CN-all , AA-all and Katz-all use informa-tion from the union of all graphs until time T  X  1. Since the off-seasonal noise edges are not sufficiently large to form communities, most of the new edges come from communities of nodes created in season. This is why CN-all , AA-all and Katz-all outperform their  X  X ast-timestep X  counterparts. As for LL , since links are more likely to come from the last seasons, it performs well, although poorly compared to NonParam . Also note that the changing user features forces the community structures to change slowly over time; in our experi-ments, CN-all performs worse that it would when there was no change in the user features, since the commu-nities stayed the same.
 Table 1 compares average AUC scores for graphs with and without seasonality, using the lowest noise setting from Fig. 1. As already mentioned, CN , AA , Katz per-form very poorly on the seasonal graphs, because of their implicit assumption of smoothness. Their vari-ants CN-all , AA-all and Katz-all on the other hand take into account all the community structures seen in the data until the last timestep, and hence are better. On the other hand, for Stationary , links formed in the last few timesteps of the training data are good predictors of future links, and so LL , CN , AA and Katz all per-form extremely well. Interestingly, CN-all , AA-all and Katz-all are worse than their  X  X ast time-step X  variants owing to the slow movement of the user features. We note, however, that NonParam performs very well in all cases, the margin of improvement being most for the seasonal networks.
 Real-world graphs. We first present results on a 24 node sensor network where each edge represents the successful transmission of a message 1 . We look at up to 82 consecutive measurements. These networks ex-hibit clear periodicity; in particular, a different set of sensors turn on and communicate during different pe-riods (as our earlier  X  X easons X ). Fig. 5.1 shows our results for these four seasons averaged over several cy-cles. The maximum standard deviation, averaged over these seasons is . 07. We do not show CN , AA and Katz which perform like a random predictor. Non-Param again significantly outperforms the baselines, confirming the simulation results.
 Additional experiments were performed on three evolving co-authorship graphs: the Physics  X  X epTh X  community ( n = 14 , 737 nodes, e = 31 , 189 total edges, and T = 8 timesteps), NIPS ( n = 2 , 865, e = 5 , 247, T = 9), and authors of papers on Citeseer ( n = 20 , 912, e = 45 , 672, T = 11) with  X  X achine learning X  in their abstracts. Each timestep looks at 1  X  2 years of pa-pers (so that the median degree at any timestep is at least 1). We also considered an evolving stock-correlation network: the nodes are a subset of stocks in the S&amp;P500, and two stocks are linked if the corre-lation of their daily returns over a two month window exceeds 0.8 ( n = 424, e = 41 , 699, T = 49). Table 2 shows the average AUC for all the algorithms. In the co-authorship graphs most authors keep work-ing with a similar set of co-authors, which hides sea-sonal variations, if any. On these graphs we perform as well or better than LL , which has been shown to be the best heuristic by Tylenda et al. (2009). On the other hand, S&amp;P500 is a correlation graph, so it is not surprising that all the common-neighbor or Katz mea-sures perform very well on them. In particular CN-all and AA-all have the best AUC scores. This is primar-ily because they count paths through edges that exist in different timesteps, which we do not.
 Thus, for graphs lacking a clear seasonal trend, LL is the best baseline on co-authorship graphs but not on the correlation graphs, whereas Katz-all works bet-ter on correlation graphs, but poorly on co-authorship graphs. NonParam , however, is the best by a large margin in seasonal graphs, and is better or close to the winner in others. 5.2. Usefulness of LSH The query time per datacube using LSH is extremely small: 0 . 3s for Citeseer, 0 . 4s for NIPS, 0 . 6s for HepTh, and 2s for S&amp;P500. Since exact search is intractable in our large-scale real world data, we demonstrate the speedup of LSH over exact search using simulated data. We also show that the hash bitsize k picked adaptively is the largest value that still gives excellent AUC scores. Since larger k translates to fewer entries per hash bucket and hence faster searches, our k yields the fastest runtime performance as well.
 Exact search vs. LSH. In Fig. 3(a) we plot the time taken to do top-20 nearest neighbor search for a query datacube. We fix the number of nodes at 100, and increase the number of timesteps. As expected, the exact search time increases linearly with the total number of datacubes, whereas LSH searches in nearly constant time. Also, the AUC score of NonParam with LSH is within 0.4% of that of the exact algorithm on average, implying minimal loss of accuracy from LSH. Number of Bits in Hashing. Fig. 3(b) shows the effectiveness of our adaptive scheme to select the num-(a) Time vs. #-datacubes (b) AUC vs. hash bitsize k ber of hash bits (Section 4). For these experiments, we turned off the smoothing based on the prior datacube. As k increases, the accuracy goes down to 50%, as a result of the fact that NonParam fails to find any matches of the query datacube. Our adaptive scheme finds k  X  170, which yields the highest accuracy. Existing work on link prediction in dynamic networks can be broadly divided into two categories: generative model based and graph structure based.
 Generative models. These include extensions of Exponential Family Random Graph models (Hanneke &amp; Xing, 2006) by using evolution statistics like edge stability, reciprocity, transitivity; extension of latent space models for static networks by allowing smooth transitions in latent space (Sarkar &amp; Moore, 2005), and extensions of the mixed membership block model to allow a linear Gaussian trend in the model param-eters (Fu et al., 2010). In other work, the structure of evolving networks is learned from node attributes changing over time (Kolar et al., 2010). Although these models are intuitive and rich, they generally a) make strong model assumptions, b) require computa-tionally intractable posterior inference, and c) explic-itly model linear trends in the network dynamics. Models based on structure. Huang &amp; Lin (2009) proposed a linear autoregressive model for individual links, and also built hybrids using static graph sim-ilarity features. In Tylenda et al. (2009) the authors examined simple temporal extensions of existing static measures for link prediction in dynamic networks. In both of these works it was shown empirically that LL and its variants are often better or among the best heuristic measures for link prediction. Our nonpara-metric method has the advantage of presenting a for-mal model, with consistency guarantees, that also per-forms as well as LL . We proposed a nonparametric model for link predic-tion in dynamic graphs, and showed that it performs as well as the state of the art for several real-world graphs, and exhibits important advantages over them in the presence of nonlinearities such as seasonality patterns. NonParam also allows us to incorporate features ex-ternal to graph topology into the link prediction al-gorithm, and its asymptotic convergence to the true link probability is guaranteed under our fairly general model assumptions. Together, these make NonParam a useful tool for link prediction in dynamic graphs.
