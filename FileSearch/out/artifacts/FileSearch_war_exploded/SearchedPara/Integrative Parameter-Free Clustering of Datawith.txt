 Integrative data mining is among the top 10 challenging problems in data mining iden-tified in [1]. Moreover it is essential for solving many of the other top 10 challenges, including data mining in social networks and data mining for biological and environ-mental problems. In this paper, we focus on integrative clustering . Clustering aims at finding a natural par titioning of the data set into mean ingful groups or clusters. Thus, clustering provides an overview on major patterns in the data without requiring much previous knowledge. During the last decades, c lustering has attracted a lot of attention as reflected in a huge volume of research papers, e.g. [2,3,4,5,6,7], to mention a few. We address the question of how to find a natural clustering of data with mixed type at-tributes. In everyday life, huge amounts of such data are collected, for example from credit assessments. The collected data include numerical attributes (e.g. credit amount, age), as well as categorical attributes (e.g. p ersonal status). A cluster analysis of credit assessment data is interesting, e.g., for target marketing. However, finding a natural clus-tering of such data is a non-trivial task. We identified two major problems: Either much previous knowledge is required, or there is no adequate support of mixed type attributes.
To cope with these two major problems, we propose INTEGRATE, a parameter-free technique for integrative clustering of data with mixed type attributes. The major benefits of our approach, which to the best of our knowledge no other clustering method meets all of them, can be summarized as follows:  X  Natural balance of numerical and categorical information in clustering supported  X  Parameter-free clustering;  X  Making most effective usage of numeri cal as well as categorical information;  X  Scalability to large data sets.
 The rest of this paper is organized as follows: Section 2 gives a brief survey of the large previous work. Section 3 presents a detailed derivation of iM DL , an information-theoretic clustering quality criterion suitable for integrative clustering. Section 4 presents our effective and efficient iterative algorithm INTEGRATE optimizing iM DL . Section 5 documents that INTEGRATE makes most effective usage of numerical as well as categorical information by comparing it to well-known and state-of-the-art clustering algorithms on synthetic and real data sets. Section 6 summarizes the paper. The algorithm k -prototypes [3] combines k -means [2] for clustering numerical data with k -modes for categorical data in order to cluster mixed type data. The attribute weights and the number of clusters have to be determined a priori. CFIKP [8] can pro-cess large data sets by k -prototypes in combination with a CF*-tree, which pre-clusters the data into dense regions. The problem for selecting the number of clusters remains. The algorithm CAVE [9] is an incremental e ntropy-based method which first selects k clusters, parametrized by the user, and the n assignes objects to these clusters based on variance and entropy. Knowledge of the similarity among categorical attributes is needed in order to construct the distance hierarchy for the categorical attributes. The cluster ensemble approach CEBMDC [10] overcomes the problem of selecting k but requires a threshold parameter that defines th e intra-cluster similarity between objects. The CBC algorithm [11] is an extension of BIRCH [4] for clustering mixed type data. It uses a weight-balanced tree that needs two pa ramters, defining the number of entries for (non)-leaf nodes. Furthermore, all entries in a leaf node must satisfy a particular thresh-old requirement. Ahmad and Dey [12] propose a k -means-based method for mixed type attributes, but the process of solving the optimization of the cost function is very com-plex and thus not scalable to large data sets. [13] uses standard fuzzy c-means on a set of features which is mapped to a set of feature vectors with only real valued components. This mapping is computationally intensive a nd is designed rather for low dimensional data. An extension of the cost function of entropy weighting k -means [14] to more efficiently specify the inter-and intra-cl uster similarities is proposed by the IWEKM approach [15]. Some papers have focused on avoiding the choice of k in partitioning clustering, e.g. X-Means [5], RIC [6] and OCI [7]. However, these clustering methods are designed for numerical vector data only. Notations. In the following we consider a data set DS with n objects. Each object x is represented by d attributes. Attributes are denoted by capital letters and can be either numerical features or categorical variable s with two or more values. For a categorical attribute A , we denote a possible value of A by a . The result of our algorithm is a disjoint partitioning of DS into k clusters C 1 , ..., C k .
 Likelihood and Data Compression. One of the most challenging problems in clus-tering data with mixed attribute type is selecting a suitable distance function, or uni-fying clustering results obtained on the different representations of the data. Often, the weighting between the different attr ibute types needs to be specified by parame-ter settings, cf. Section 2. The minimum description length (MDL) principle provides an theoretical foundation for parameter-free integrative clustering avoiding this prob-lem. Regarding clustering as a data compression problem allows us a unifying view, naturally balancing the influence of categorical and numerical attributes in clustering. Probably the most important idea of MDL which allows integrative clustering is relating the concepts of likelihood and data compression. Data compression can be maximized by assigning short descriptions to regular data objects which exhibit the characteristic patterns and longer descriptions to th e few irregular objects or outliers. 3.1 Coding Categorical Data Assume a data set, where each object is repr esented by one categorical attribute A with two possible values. It can be shown that the code length to encode this data is lower bounded by the entropy of A . Thus, the coding costs CC of A are provided by: By the application of the binary logarithm we obtain the code length in bits. If we have no additional knowledge on the data we have to assume that the probabilities for each value are equal. Hence, we need one bit per data object. Clustering, however, provides high-level knowledge on the data which allows for a much more effective way to reduce the costs. Even if the probabilities for the different outcomes of the attributes are approximately equal w.r.t. the whole data set, often different clusters with non-uniform proba bilities can be f ound. As an example, refer to Figure 1. The data are represented by two numerical attributes (which we ignore for the moment) and one categorical attribute which has two possible values, red and blue . Considering all objects, the probabilities for red and blue are equal. However, it is evident that the outcomes are not uniformly distributed. Rather, we have two clusters, one preliminarily hosts the red objects, and the other the blue ones. In fact, the data has been generated such that in the left cluster, we have 88% of blue objects and 12% of red objects. For the right cluster, the ratio has been selected recipr ocally. This clustering drastically reduces the entropy and hence CC ( A ) = 0.53 bits per data object, which corresponds to the entropy of A in both clusters.
 3.2 Coding Numerical Data To specify the probability of each data object considering an additional numerical at-tribute B , we assign a probability density function (PDF) to B . In this paper, we apply a Gaussian PDF for each numerical attribute. However, let us note that our ideas can be straightforwardly extended to other types PDF, e.g. Laplacian or Generalized Gaussian. Thus, the PDF of a numerical attribute B is provided by: If the data distribution of B is Gaussian with mean  X  B and standard deviation  X  B ,we minimize the costs of the data by a coding scheme which assigns short bit strings to objects with coordinate values that are in the area of high probability and longer bit strings to objects with lower probability. This principle is also commonly referred to as Huffman-Coding. The coding costs CC of B are provided by: Again, if we have no knowledge on the dat a, we would have to assume that each at-tribute is represented by a single Gaussian w ith mean and standard deviation determined from all data objects. As discussed for categorical data, clustering can often drastically reduce the costs. Most importantly, relating clustering to data compression allows us a unified view on data with mixed type attributes. Consider again the data displayed in Figure 1. In addition to the categorical attribute we now consider the numerical x -coordinate, denoted by X . To facilitate presentation, we ignore the y -coordinate which is processed analogously. The two green curves at the bottom represent the coding costs of the two clusters considering X . For both curves, the cost minimum coincides with the mean of the Gaussian whic h generated the data. The cluster on the right has been generated with slightly larger variance, resulting in slightly higher coding costs. The intersection of both cost curves represents the border between the two clusters provided by X , indicated by a green vertical line. In a ddition, for each cluster and each outcome of the categorical attribute, we have included a cost curve (displayed in the correspond-ing colors). Again, the intersection points mark the cluster borders provided by the categorical attribute. Consider, e.g., th e red vertical line. Red objects with a value in X beyond that point are assigned to the cluster on the right. Thus, in the area between the red and the blue vertical line, the categorical value is the key information for clustering. Note that all borders are not fixed but optimized during the run of our algorithm. 3.3 A Coding Scheme for Integrative Clustering We also need to elaborate a coding scheme describing the clustering result itself. The additional costs for encoding the clustering result can be classified into two categories: the parameter costs PC required to specify the cluster model and the id-costs IDC required to specify the cluster-id for each obj ect, i.e. the information to which cluster the object belongs.
 For the parameter costs, lets focus on the set of objects belonging to a single cluster C . To specify the cluster model, we n eed for each categorical attribute A to encode the probability of each value or outcome a . For a categorical attribute with | A | possible val-ues, we need to encode | A | X  1 probabilities since the remaining probability is implicitly specified. For each numerical attribute B we need to encode the parameters  X  B and  X  B of the PDF. Following a central result form the theory of MDL [16], the parameter costs to model the |C| objects of the cluster can be approximated by p/ 2  X  log 2 |C| ,where p denotes the number of parameters. The param eter costs depend logarithmically on the number of objects in the cluster. The considerations behind this are that for clusters with few objects, the parameters do not need to be coded with very high precision. To summarize, the parameter costs for a cluster C are provided by Here A cat stands for all categorical attributes and B num for all numerical attributes in the data. Besides the parameter costs, we need for each object to encode the information to which of the k clusters it belongs. Also for the id-costs, we apply the principle of Huffman coding which implies that we assign shorter bitstrings to the larger clusters. Thus, the id-costs of a cluster C are provided by: Putting it all together, we are now ready to define iM DL , our information-theoretic optimization goal for integrative clustering. For all clusters C we sum up the coding costs for all num erical and categorical attributes A . To these costs we need to add the parameter costs and the id-cost of the cluster, denoted by PC ( C ) and IDC ( C ) , respectively. Finally, we sum up these three terms for all clusters. Now we present the highly effective algorithm INTEGRATE for clustering mixed type attributes that is based on our new MDL criterion iM DL , defined in Section 3. IN-TEGRATE is designed to find the optimal clustering of a data set DS , where each object x comprises both numerical and categori cal attributes by optimizing the overall compression rate. First, INTEGRATE builds an initial partitioning of k clusters. Each cluster is represented by a Gaussi an PDF in each numerical dimension B with  X  B and  X 
B , and a probability for each value of the categorical attributes. All objects are then assigned to the k clusters by minimizing the overall coding costs iM DL . In the next step, the parameters of each cluster are recal culated according to the assigned objects. That implies the  X  and the  X  in each numerical dimension and the probabilities for each value of the categorical attributes, respectively. After initialization the following steps are performed repeatedly until convergence. First, the costs for coding the actual clus-ter partition are determined. Second, assignm ent of objects to clusters is performed in order to decrease the iM DL value. Third, the new parameters of each cluster are re-calculated. INTEGRATE terminates if no furt her changes of cluster assignments occur. Finally, we receive the optimal clustering for DS represented by k clusters according to minimum coding costs. 4.1 Initialization The effectiveness of an algorithm often heavily depends on the quality of the initializa-tion, as it is often the case that the algorithm can get stuck in a local optimum. Hence, we propose an initialization scheme to avoid this effect. We have to find initial cluster representatives that correspond best to the final representatives. An established method for partitioning methods is to initialize with randomly chosen objects of DS . We adopt this idea and take the  X  of the numerical attributes of k randomly chosen objects as cluster representatives. During initialization, we set  X  =1 . 0 in each numerical dimen-sion. The probabilities of the values for the categorical attributes are set to 1 | a | .Thena random set of 1 z n objects is selected, where n is the size of DS and z = 10 turned out to give satisfying results. Finally, we chose the clustering result that minimizes iM DL best, within m initialization runs. Typically m = 100 runs suffice for an effective result. As only a fraction of DS is used for the initialization procedure, our method is not only effective but also very efficient. 4.2 Automatically Selecting the Number of Clusters k Now we propose a further improvement of the effectiveness of INTEGRATE. Using iM DL for mixed type data we can avoid the parameter k . As an optimal clustering that represents the underlying data structure best has minimum coding costs, iM DL can also be used to detect the number of clusters. For this purpose, INTEGRATE uses iM DL no longer exclusively as selection criterion for finding the correct object to cluster assignment. Rather we now estimate the coding costs for each k where k is selected in a range of 1  X  k  X  n . For efficiency reasons INTEGRATE performs this iteration step on a z % sample of DS . The global minimum of this cost function gives the optimal k and thus the optimal number of clusters. Since INTEGRATE is a hybrid approach combining the benefits of clustering methods using only numeric attributes and those for categorical attributes we compare algorithms of both categories and algorithms that can also handle mixed type attributes. In partic-ular, we selected the popular k -means algorithm, the widely used method k -modes, the k -means-based method by Ahmad and Dey denoted by KMM and k -prototypes (cf. Section 2). For k -means and k -modes the numerical and categorical attributes were ig-nored. For evaluation we used the validity measure by [17] referred to as DOM in the following (smaller values indicate a better cluster quality), which has the advantage that it allows for clusterings with different numbers of clusters and integrates the class labels as  X  X round truth X . We report in each e xperiment the average performance of all clustering algorithms over 10 runs. 5.1 Synthetic Data If not otherwise specified the artificial data sets include three Gaussian clusters with each object having two numerical attributes an d one categorical attribute. To validate the results we added a class label to each object which was not used for clustering. Varying Ratio of Categorical Attribute Values. In this experiment we generated three-dimensional synthetic data sets with 1,500 points including two numerical and one two-valued categorical attribute. We va ried the ratio for each of the values of the categorical attributes from 1 : 0 to 0 : 1 clusterwise in each data set. Without need for diffi-cult parameter setting INTEGRATE performs best in all cases (cf. Figure 2). Even in the case of equally (5:5) distributed values, where the categorical attribute gives no infor-mation for separating the objects, the cluster quality of INTEGRATE is best compared to all other methods. As k -means does not take the categorical attributes into account the performance is relatively constant.
 Varying Variance of Clusters. This experiment aims at comparing the performance of the different methods on data sets with varying variances. In particular, we generated synthetic data sets each comprising 1,500 poi nts including two numerical and one two-valued categorical attribute that form three Gaussian clusters with a variance ranging from 0 . 5 to 2 . 0 . Figure 3 shows that INTEGRATE outperforms all competitors in all cases, in which each case reflects different degr ee of overlap of the three clusters. Even at a variance of 2 . 0 where the numerical attributes ca rry nearly no cluster information our proposed method shows best cluster quality as in this case the categorical attributes are used to separate the clusters. On the contrary, k -modes performs worst as it can only use the categorical attribute as single source for clustering.
 Varying Clustersize. In order to test the performance of the different methods on data sets with unbalanced clustersize we generated three Gaussian clusters with different variance and varied the ratio of number of points per cluster from 1 : 10 : 1 to 10 : 1 : 10 in five steps. It is obvious from Figure 4 that INETGRATE separates the three clusters best even with highly unbalanced cluster sizes. Only in the case of two very small clusters and one big cluster (1:10:1) k -modes shows a slightly better cluster validity. Varying Number of Numerical Dimensions. In this experiment we leave the number of categorical attributes to a constant value and successively add numerical dimensions to each object that are generated with a variance of  X  =1.8. INTEGRATE shows best performance in all cases (cf. Figure 5). All methods show a slight increase in cluster quality when varying the numerical dimensionality, except k -modes that performs con-stantly as it does not consider the numerical attributes.
 Varying Number of Categorical Dimensions. For each object we added three-valued categorical attributes where we set the probablity of the first value to 0 . 6 and the proba-bility of the two remaining values to 0 . 2 , respectively. Figure 6 illustrates that our pro-posed method outperforms the other methods and even k -modes by magnitudes which is a well-known method for clustering categorical data. Whereas KMM shows a heavy decrease in clustering quality in the case of two and four additional categorical at-tributes, our method performs relatively constant. Taking the numerical attributes not into account the cluster validity of k -means remains constant.
 Noise Dimensions. Figure 7 illustrates the performance of the different methods on noisy data. It it obvious that INTEGRATE outperfoms all compared methods when adding non-clustered noise dimensions to the data. k -means shows a highly increase in the DOM values which refers to decreasing cluster validity. Even in the case of nine noise dimensions INTEGRATE leads to the best clustering result. 5.2 Real Data Finally, we show the practical application of INTEGRATE on real world data, avail-able at the UCI repository http://archive.ics.uci.edu/ml/ . We chose two different data sets with mixed numerical and categorical attributes. An additional class attribute allows for an evaluation of the results. Table 1 reports the  X  and  X  of all meth-ods within 10 runs. For all compared methods we set k to the number of classes. Heart Disease. The Heart-Disease data set comprise s 303 instances with six numerical and eight categorical attributes each labele d to an integer value between 0 and 4 which refers to the presence of heart disease. Without any prior knowledge on the data set we obtained best cluster validity of 1 . 23 with INTEGRATE. KMM performed slighty worse. However, the runtime of INTEGRATE is 0.1 seconds compared to KMM which took 2.8 seconds to return the result.
 Credit Approval. The Credit Approval data set contains results of credit card applica-tions. It has 690 instances, each being describ ed by six numerical and nine categorical attributes and classified to the two classes  X  X es X  or  X  X o X . With a mean DOM value of 0.61 INTEGRATE separated the objects best into two clusters in only 0.1 seconds without any need for setting input parameters. 5.3 Finding the Optimal k On the basis of the data set illustrated in Figure 8(a) we highlight the benefit of IN-TEGRATE for finding the correct number of clusters that are present in the data set. The data set comprises six Gaussian clus ters with each object having two numerical attributes and one categorical attribute with two different values that are marked in  X  X ed X  and  X  X lue X , respectively. Figure 8(b) shows the iM DL of the data model for dif-ferent values of k . The cost function has its global minimum, which refers to the optimal number of clusters, at k = 6. In the range of 1  X  k  X  4 the plotted function shows an intense decrease in the coding costs and for k&gt; 6 a slight increase of the coding costs as in these cases the data does not optimally fit into the data model and therefore causes high coding costs. Note, that there is a local minimum at k =4whichwouldalsorefer to a meaningful number of clusters. We have introduced a new information-theoretic clustering method  X  INTEGRATE. We gave a solution to avoid difficult parameter s ettings guided by the information-theoretic idea of data compression. We have shown with extensive experiments that INTEGRATE uses the numerical and categorical inform ation most effectively. And finally, INTE-GRATE shows high efficiency and is therefore scalable to large data sets.

