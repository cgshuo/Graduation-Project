 Transaction data, such as shopping transactions [1], web query logs [11], and movie ratings [10], are important sources for knowledge discovery. People are increasingly releasing transaction data to the data mining research community for discovering knowledge that helps improve services. However, transaction data contains significant amount of personal and sensitive information. The release of such data to the public or a third party could breach privacy, as highlighted by recent incidents [2][10]. Transaction data must be anonymized before release.
Recently, several works started to addres s the transaction d ata anonymization problem [4][5][13][14]. However, these works suffer from a few limitations, namely, incurring high information loss, failing to enable standard data mining tools, and introducing invalid analysis results. Let us examine those prior works using the transaction data in Fig. 1 (a) and the taxonomy in Fig. 1 (b).

Global generalization [13]. The k m -anonymity in [13] requires that every sub-set of no more than m items is contained in at least k transactions. The global generalization technique (a.k.a. full subtree generalization [6]) is employed in [13], which is vulnerable to excessive distortion in the presence of outliers. Let k  X  -anonymity denote k m -anonymity with m being the longest transaction length. For example, to achieve 2  X  -anonymity ,asinthe4 th column in Fig. 1 (a), all items are generalized to the top level because of the outlier, { e, i } .
Suppression [14]. The ( h, k, p )-coherence in [14] demands that every subset of no more than p public items must be contained in at least k transactions and no more than h percent of these transactions contain a common private item. k m -anonymity is its special case with h = 100% and p = m .[14]employsthe total item suppression technique to enforce ( h, k, p )-coherence , which incurs high information loss when the data is sparse. E.g., in the 5 th column in Fig. 1 (a), all occurrences of b, c, i, x, y, and z, are removed as indicated by *.
Local generalization [5]. The transactional k-anonymity in [5] requires that each transaction has at least k duplicates. Such a requirement is stronger than k  X  -anonymity and introduces much more dist ortion than necessary. The multi-dimensional generalization technique [7] is employed in [5]. But, it destroys the domain exclusiveness property, e.g., the 3 rd column in Fig. 1 (a) shows the data anonymized by [5] where items T, P, and K coexist in the anonymized data, but their domains are not exclusive of each other. The analysis result based on such data are hard to interpret, e .g., according to such dat a, whenever a transaction contains K, it also contains f. But it is not true with the original data, e.g., the first transaction contains c and d (and hence K), but it does not contain f.
Band matrix method [4]. A method for grouping transactions and permuting the private items in each group to enforce l-diversity [9] is presented in [4]. How-ever, invalid analysis results could be derived from the anonymized data. The example in [4] explained this: in the original data, all customers who bought cream but not meat have also bought a pregnancy test ; while in the data anonymized by [4], only a half of such customers have bought a pregnancy test .
This paper, motivated by the limitations of the prior works, proposes to integrate the global generalization technique with the total item suppression technique for enforcing k m -anonymity . Our observation is that suppression can remove outlier items that otherwise will cause substantially generalization of many other items, and generalization can slightly generalize items that other-wise must be suppressed. While a single technique could not perform well, the integration can greatly reduce the overall information loss. Our approach has two strong properties: the anonymized data can be analyzed by standard data mining tools, and results derived from it are true in the original data. This is because both techniques preserve the dom ain exclusiveness property. For exam-ple, the last column in Fig. 1 (a) shows the data anonymized by our approach which suppresses item i and generalizes some other items.

Integrating generalization and suppression is non-trivial because the search space is much larger than only employing one of them. We propose a multi-round, top-down greedy search strategy to address the challenge. Extensive comparative experiments showed that our approach yields better data utility than the prior works and is efficient and scalable in anonymizing real world databases.
The rest of the paper is organized as follows. Section 2 describes the privacy requirement and the anonymization mode l, Section 3 presents the basic approach that integrates generalization with suppression, Section 4 proposes the key tech-niques that make our approach efficient and scalable, Section 5 evaluates the applicability of our approach, and Section 6 concludes the paper. A publisher wants to release a transaction database D = { t 1 ,t 2 ,...,t n } ,where each transaction t i corresponds to an individual and contains items from an item universe I = { i 1 ,i 2 ,...,i q } . An adversary tries to link a target individual to his/her transaction with a high probability. To do so, the adversary acquired knowledge from external sources. That is, the adversary knows that the transac-tion is in the released data and knows some items of the target individual. The publisher wants to prevent such a linking attack.
 Definition 1 (Privacy threats and k m -anonymity ): A subset of items is called an itemset .Anitemset X with | X | X  m is called a privacy threat if the number of transactions in D that support X , denoted by sup ( X ), is less than a user specified anonymity threshold k , i.e., sup ( X ) &lt;k .Atransaction tsupports X if X is a subset of t; D observes k m -anonymity [13] if there is no privacy threat supported by D .

Enforcing the privacy notion in Definition 1 assures that the adversary X  X  cer-tainty in making any linking attack is no more than 1 /k .
 Anonymization solutions: To enforce the privacy notion, the full subtree generalization technique [6][13] and the total item suppression technique [14] are integrated to anonymize D . We assume that a taxonomy tree H P for generalizing items is available. With the full subtree generalization technique, a generalization solution is defined by a cut on H P .Acutcontains exactly one item on every root-to-leaf path on H P , and is denoted by the set of such items . E.g., { P, f, g, M, e, i } denotes the cut depicted by a dash line on H P in Fig. 1 (b). With the total item suppression technique, to eliminate privacy threats in the generalized data, some constituent items of Cut are totally removed from all transactions. The set of items to be removed is called a suppression scenario of Cut .
In other words, an anonymization is defined by Cut and SS , a generalization cut and the suppression scenario associated with the cut. The anonymized data D is derived in two steps: first the original items in D are generalized to their taxonomic ancestors in Cut to get D = g ( D, Cut ), and then items in SS are suppressed from D to eliminate threats, which results in D = s ( D ,SS ). Running Example: Consider the transaction database D in the 2 nd column in Fig. 1 (a) and the taxonomy H P in Fig. 1 (b). Suppose that we enforce 2  X  -anonymity . By generalizing D to the cut { P, f, g, M, e, i } , only one privacy threat, { e, i } , exists in the generalized data D = g ( D, { P, f, g, M, e, i } ). If we suppress item i from D , we get the anonymized data D = s ( D , { i } ) where no privacy threat exists, as shown in the last column in Fig. 1 (a).
 Anonymization causes information loss. Given Cut and SS , a generalization cut and its associated suppression scenario, cost G ( Cut ) denotes the information loss incurred by generalizing D to get D = g ( D, Cut ), and cost S ( SS ) denotes that incurred by suppressing items in SS from D to get D = s ( D ,SS ). The total cost is cost ( Cut,SS )= cost G ( Cut )+ cost S ( SS ).

Anonymization can be measured by a variety of metrics. As most cost met-rics are additive, we can write cost G ( Cut )= x  X   X  Cut O ( x  X  )  X  IL G ( x  X  ), and currences in D of all leaf items that are descendants of x  X  ,IL G ( x  X  ) is the gen-eralization cost per occurrence of x  X  ,and IL S ( x  X  ) is an extra suppression cost in addition to IL G ( x  X  )if x  X  is suppressed.

We use LM [6] in our discussion, and assume that D only contains leaf items on H P .With LM , IL G ( x  X  )=(# leaves ( x  X  )  X  1) / (# leaves ( H P )  X  1), where # leaves ( x  X  )and# leaves ( H P ) denotes the number of leaves in the subtree rooted at x  X  and that in the taxonomy H P respectively. If x  X  is suppressed, it is deemed that all descendants of x  X  are generalized to the top level of H P , the overall information loss per occurrence of x  X  is 1, so the extra suppression cost is IL S ( x  X  )=1  X  IL G ( x  X  ). For example, for D in the last column in Fig. 1 (a), Cut = { P, f, g, M, e, i } , SS = { i } .WithLM, cost G ( Cut ) = 3.6, and cost S ( SS ) = 2. The total cost is cost ( Cut,SS ) = 5.6. An anonymization is defined by ( Cut, SS ), a generalization cut with its associ-ated suppression scenario, and can be found by two nested loops.

As the number of cuts is exponential in the number of items and so is the number of suppression scenarios for a cut, a complete enumeration for either loop is intractable. Therefore, we present a basic approach, heuristic generalization with heuristic suppression , namely HgHs. 3.1 Top-Down Greedy Search of the Lattice of Cuts The outer loop of HgHs enumerates generalizations (cuts) by a top-down greedy search of a lattice of all possible cuts [8], where a specific cut (child) is derived from a general cut (parent) by replacing one constituent item of the parent cut by its child items on the taxonomy tree.

Starting from the top-most cut which consists of only the root (item) of the taxonomy tree, the outer loop continues with the most promising child cut of the current cut, which is achieved by evaluating the suppression scenario for each child of the current cut by running the inner loop ( detailed in the next subsection ), and computing the anonymization cost. The outer loop stops when no child cut reduces the anonymization cost.

For example, Fig. 2 (a) des cribes the searching pro cess. The outer loop starts from cut 1 = { T } with cost ( cut 1 ,SS 1 ) = 23 where the suppression scenario SS 1 for { e, i } ,in D = g ( D, cut 2 ). The suppression scenario SS 2 for cut 2 ( computed by the inner loop described in the next subsection )is { i } .So cost ( cut 2 ,SS 2 )= cost G ( cut 2 )+ cost S ( SS 2 )=6 . 6+2 = 8 . 6. The search continues to evaluate the children of cut 2 . The best child is cut 4 since cost ( cut 4 ,SS 4 ) = 6.2 while cost ( cut 3 ,SS 3 ) = 10.2. The search stopped at cut 6 = { P, f, g, M, e, i } with SS 6 = { i } as no child of cut data D as shown in the last column in Fig. 1 (a). 3.2 Finding a Good Suppression Scenario for a Cut The inner loop of HgHs is responsible for finding an item suppression scenario SS to eliminate privacy threats from D = g ( D, Cut )where Cut is the cut currently being enumerated by the outer loop. SS is a subset of Cut , all occurrences of items in SS will be suppressed from D .

To determine SS , the inner loop greedily searches the so called suppression scenario enumeration tree, which is built per cut . Each node on the suppression scenario enumeration tree is denoted by a headlist and a taillist .Theitemsin headlist are to be kept , and the items not in headlist are to be suppressed. We also use the set notation to represent headlist and taillist . Thus, the suppression scenario represented by a node N is Cut -N. headlist . And its suppression cost taillist = Cut , i.e., all items are suppressed. The j th child node C of a parent node P is derived based on the j th item, i j ,inP. taillist such that C. headlist = P. headlist  X  X  i j } and C. taillist =thesuffixofP. taillist after i j . For example, Fig. 2 (b) is the suppression scenario enumeration for cut 3 in Fig. 2 (a). N 1 is the root with N 1 . headlist = {} and N 1 . taillist = cut 3 where items are listed in the descending order of suppression costs. N 2 is derived from N , by moving the first item Q from taillist to headlist , which means that all items except Q are suppressed.

Clearly, a suppression scenario represented by a node N is valid if and only if no threat in D is contained by N. headlist . Moreover, if a threat X is contained by N. headlist , then X is also contained by the headlist of any descendant of N. Therefore, if N is invalid, all its descendants are invalid, we can stop searching the subtree rooted at N. If items in headlist and taillist are in the descending order of suppression costs, the first valid child of any node is the most promising child of the node, as it is valid and reduces the suppression cost most.
For example, Fig. 2 (b) shows how the suppression scenario SS 3 for eliminating the threats, { H,K,Q } and { e,i } ,from D = g ( D, cut 3 ) is found. The inner loop of HgHs starts with N 1 which is valid. N 2 is the first child of its parent and is valid, and so is N 3 .AndN 4 is the first child of N 3 but it is invalid. The inner loop stoped at N 5 with N 5 .headlist = { Q, K, e } . So, the final suppression scenario SS 3 = cut 3  X  N 5 . headlist = { H, i } . Although our basic approach HgHs presented in Section 3 enumerates a limited number of anonymizations, the work for examining each enumerated anonymiza-tion is still non-trivial. In this section, we propose the key techniques to address the efficiency and scalability issues in this regard.
 4.1 Minimal Privacy Threats The outer loop of HgHs needs to know the set of privacy threats in D = g ( D, Cut ) for the current Cut . If such a set is empty, all threats are already eliminated by generalizing D . If it is not, we have to suppress some generalized items to eliminate all privacy threats from D . First, we claim that it suffices to generate the set of minimal privacy threats .
 Definition 2 (Minimal privacy threats): A privacy threat X is a minimal threat if there is no privacy threat that is a subset of X.
 Since every privacy threat contains some minimal privacy threat, if we eliminate all minimal privacy threats, we also eliminate all privacy threats. However, find-ing the set of minimal privacy threats on-the-fly is inefficient, since every threat occurs in multiple versions of the generalized data derived by different cuts and hence will be repeatedly generated while the number of cuts to be enumerated is still quite large.

Our approach is to generate the set of the minimal privacy threats sup-ported by all cuts on the taxonomy H P in an initialization step. For Cut being enumerated by the outer loop, we can re trieve privacy threats relevant to Cut from that set instead of generating D = g ( D, Cut ) and mining D on-the-fly. Given a suppression scenario SS for Cut , to see if all threats are removed from D = s ( D ,SS ), we check if no relevant threat is contained in Cut -SS .
For the running example, there are 25 threats in the set of the minimal privacy threats, from which we can retrieve the threats, { H, K, Q } and { e, i } , relevant to cut 3 in Fig. 2 (a), for searching suppression scenarios in Fig. 2 (b). 4.2 A Multi-round Approach The set of the minimal privacy threats supported by all cuts on the taxonomy H
P could be huge when H P is of a large scale and the maximum size m of privacy threats is large, which makes HgHs not scalable. We propose a multi-round approach, mHgHs, to address the scalability issue.

To find a solution, mHgHs runs HgHs in m rounds. The 1 st round finds ( Cut 1 best , SS 1 best ) on the original taxonomy H P , which defines an anonymization anonymization observing k i -anonymity , on the reduced taxonomy H i  X  1 P that is other words, mHgHs performs anonymization progressively. Each round works on a reduced taxonomy based on the precedent round, so the set of the minimal privacy threats supported by all cuts for each round is under control.
For the running example, mHgHs first finds Cut 1 best = { a, b, c, d, f, g, M, e, i } with SS 1 best = {} ,andgets H 1 P by removing nodes under Cut 1 best . Then, mHgHs works on H 1 P , and so on. After five rounds, mHgHs finds Cut 5 best = { P, f, g, M, e, i } with SS 5 best = { i } , which conforms 2 5 -anonymity . Our major goal is to investigate if our approach preserves more data utility than others approaches, and if our algorithm is scalable and efficient. We evaluate our algorithm mHgHs by comparing it with several state-of-the-art algorithms, the local generalization algorithm LG [5], the global generalization algorithm AA [13], and the suppression algorithm MM [14]. The executables of AA and MM were provided by the authors. We implemented LG as it is not available.
The POS dataset [15] and the AOL web query log dataset [11] are used in the experiments. The taxonomy tree for the POS dataset was created by [13]. We preprocessed the AOL dataset using WordNet [3] in creating the taxonomy tree. The AOL dataset is divided into 10 subs ets. We use the first subset to evaluate the basic features of all algorithms, and use all subsets to evaluate scalabilities. We measure information loss by NCP ,avariantof LM [6], as it was used by AA and LG. Experiments were performed on a PC with a 3.0 GHz CPU and 3.2 GB RAM. In the experiments, the default setting is k =5, m =7. 5.1 Information Loss Evaluation Fig. 3(a)-(b) show the information loss on the POS dataset. Fig. 3(c)-(d) show that on the AOL dataset. Among all the 4 algorithms, the information loss by MM is the highest for all cases with m  X  2, which is between 7.5% and 70% on POS and between 46% and 96% on AOL. This is consistent with the finding in [14] that MM is not good for sparse datasets as the POS dataset is quite sparse while the AOL dataset is even sparser.
 The information loss by AA is the second highest in general. In some cases on POS (with a small m and a large k ), LG incurs a little bit more. The information loss by AA on AOL is strikingly high, all around 39% even for m =1. Because the AOL dataset is extremely sparse, a lot of very infrequent items spread over the taxonomy. They have to be generalized to high levels, which brings their siblings to the same ancestors by AA. This situation is similar to our motivation example where as items e and i are infrequent, their siblings, P and Q, although quite frequent, have to be generalized to the top level together with e and i by AA. In such cases, suppressing a few outlier items will reduce information loss. This motivates our approach.

The information loss by LG is the third highest. As we pointed out in Section 1, LG exerts excessive distortion as it enforces the transactional k-anonymity principle which is too strong to b e necessary. LG does not make use of m .So, the curves of LG with a varying m are all horizontal lines.

Our algorithm, mHgHs, incurs the least information loss which is the ad-vantage of integrating suppression and generalization. The data utility gain of mHgHs over LG is moderate on POS, but it is significant on AOL. All the infor-mation loss with mHgHs is under 10% on AOL, while the worst case with LG is 27%, e.g., that by mHgHs is around 7.9% with k =5and m  X  5onAOLasin Fig. 3(d), while that with LG is 12%. The gap increases when enforcing a more restrictive privacy requirement, e.g., that by mHgHs with k =50and m  X  5on AOL is 9% as in Fig. 3(c), while that by LG is 17%. Notice that the information loss reported for mHgHs is also computed on the original taxonomy. 5.2 Efficiency and Scalability Evaluation We evaluated the efficiencies of all the algorithms. LG is the most efficient be-cause it employs a divide-and-conquer approach. But, it comes with an expense, i.e., the anonymized data does not observe the domain exclusiveness as dis-cussed in Section 1. Our algorithm, mHg Hs, is the second most efficient. Al-though mHgHs is less efficient than LG, the significant gain in data utility by mHgHs over LG is worth the longer runtime. AA and MM are less efficient. This is because the breadth-first search approaches they employ are not efficient in dealing with privacy threats with a large size. One exception is that AA is as efficient as LG on AOL because the search space is greatly pruned by AA, which unfortunately results in the second highest information loss on AOL. We also evaluated the scalabilities of algorithms on all the 10 subsets of the AOL query logs. The result showed that our algorithm mHgHs is quite scalable. This paper proposed to integrate generalization and suppression to enhance data utility in anonymizing transaction data. We presented a multi-round, top-down greedy search algorithm to address the performance issues. Extensive experi-ments show that our approach outperforms the state-of-the-art approaches. Acknowledgements. The research is supported i npartbytheNaturalSci-ences and Engineering Research Council of Canada, in part by the Science and Technology Development Plan of Zhejiang Province, China (2006C21034), and in part by the Natural Science Foundation of Zhejiang Provin ce, China ( Y105700). We thank Harshwardhan Agarwal for his help in experimental evaluation.
