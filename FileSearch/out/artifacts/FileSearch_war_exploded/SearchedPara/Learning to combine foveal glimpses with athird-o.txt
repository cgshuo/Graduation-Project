 Like insects with unmovable compound eyes, most current computer vision systems use images of uniform resolution. Human vision, by contrast, uses a retina in which the resolution falls off rapidly with eccentricity and it relies on intelligent, top-down strategies for sequentially fixating parts of the optic array that are relevant for the task at hand. This  X  X ixation point strategy X  has many advantages: If a system with billions of neurons at its disposal has adopted this strategy, the use of a variable resolution retina and a sequence of intelligently selected fixation points is likely to be even more advantageous for simulated visual systems that have to make do with a few thousand  X  X eurons X . In this paper we explore the computational issues that arise when the fixation point strategy is incorpo-rated in a Boltzmann machine and demonstrate a small system that can make good use of a variable resolution retina containing very few pixels. There are two main computational issues: Figure 1: A: Illustration of the retinal transformation r ( I , ( i, j )) . The center dot marks the pixel at position ( i, j ) (pixels are drawn as dotted squares). B: examples of glimpses computed by the retinal transformation, at different positions (visualized through reconstructions). C: Illustration of the multi-fixation RBM.
 To tackle these issues, we rely on a special type of restricted Boltzmann machine (RBM) with third-order connections between visible units (the glimpses), hidden units (the accumulated features) and position-dependent units which gate the connections between the visible and hidden units. We describe approaches for training this model to jointly learn and accumulate useful features from the image and control where these features should be extracted, and evaluate it on a synthetic dataset and two image classification datasets. Throughout this work, we will assume the following problem framework. We are given a training l t  X  X  1 , . . . , C } ) given the associated image I t . The standard machine learning approach would consist in extracting features from the whole image I t and from those directly learn to predict l t . However, since we wish to incorporate the notion of fixation into our problem framework, we need to introduce some constraints on how information from I t is acquired.
 To achieve this, we require that information about an image I (removing the superscript t for sim-plicity) must be acquired sequentially by fixating (or querying) the image at a series of K positions [( i information in the neighborhood of that pixel is extracted through what we refer to as a retinal high-resolution information (i.e. copies the value of the pixels) from the image only in the neigh-borhood of pixel I( i k , j k ) . At the periphery of the retina, lower-resolution information is extracted by averaging the values of pixels falling in small hexagonal regions of the image. The hexagons are arranged into a spiral, with the size of the hexagons increasing with the distance from the center ( i k , j k ) of the fixation 1 . All of the high-resolution and low-resolution information is then concate-tion is given in Figure 1. As a shorthand, we will use x k to refer to the glimpse given by the output of the retinal transformation r ( I , ( i k , j k )) . We now describe a system that can predict l from a few glimpses x 1 , . . . , x K . We know that this problem is solvable: [1] demonstrated that people can  X  X ee X  a shape by combining information from multiple glimpses through a hole that is much smaller than the whole shape. He called this  X  X northoscopic perception X . The shape information derived from each glimpse cannot just be added as implied in [2]. It is the conjunction of the shape of a part and its relative location that provides evidence for the shape of a whole object, and the natural way to deal with this conjunction is to use multiplicative interactions between the  X  X hat X  and  X  X here X .
 Learning modules that incorporate multiplicative interactions have recently been developed [3, 4]. These can be viewed as energy-based models with three-way interactions. In this work, we build on [5, 6] who introduced a method of keeping the number of parameters under control when in-corporating such high-order interactions in a restricted Boltzmann machine. We start by describing the standard RBM model for classification, and then describe how we adapt it to the multi-fixation framework. 3.1 Restricted Boltzmann Machine for classification RBMs are undirected generative models which model the distribution of a visible vector v of units using a hidden vector of binary units h . For a classification problem with C classes, the visible layer is composed of an input vector x and a target vector y , where the target vector follows the so-called  X 1 out of C  X  representation of the classification label l (i.e. y = e l where all the components of e l are 0 except for the l th which is 1).
 More specifically, given the following energy function: we define the associated distribution over x , y and h : p ( y , x , h ) = exp(  X  E ( y , x , h )) /Z . Assuming x is a binary vector, it can be shown that this model has the following posteriors: where A j  X  and A  X  i respectively refer to the j th row and i th column of matrix A . These posteriors make it easy to do inference or sample from the model using Gibbs sampling. For real-valued input vectors, an extension of Equation 1 can be derived to obtain a Gaussian distribution for the conditional distribution over x of Equation 3 [7].
 Another useful property of this model is that all hidden units can be marginalized over analytically in order to exactly compute where softplus( a ) = log(1 + exp( a )) . Hence, classification can be performed for some given input x by computing Equation 5 and choosing the most likely class. 3.2 Multi-fixation RBM At first glance, a very simple way of using the classification RBM of the previous section in the multi-fixation setting would be to set x = x 1: K = [ x 1 , . . . , x K ] . However, doing so would com-pletely throw away the information about the position of the fixations. Instead, we could redefine the energy function of Equation 1 as follows: where the connection matrix W ( i k ,j k ) now depends on the position of the fixation 2 . Such connec-tions are called high-order (here third order) because they can be seen as connecting the hidden ditioned on the position units (which are assumed to be given), this model is still an RBM satisfying the traditional conditional independence properties between the hidden and visible units. For a given m  X  m grid of possible fixation positions, all W ( i k ,j k ) matrices contain m 2 HR parame-ters where H is the number of hidden units and R is the size of the retinal transformation. To reduce that number, we parametrize or factorize the W ( i k ,j k ) matrices as follows diag( a ) is a matrix whose diagonal is the vector a . Hence, W ( i k ,j k ) is now an outer product of the D lower-dimensional bases in F ( X  X ilters X ) and P ( X  X ooling X ), gated by a position specific vector which rows of F and columns of P are used to accumulate the glimpse at position ( i k , j k ) into the hidden layer of the RBM. A similar factorization has been used by [8]. We emphasize that z ( i k , j k ) is not stochastic but is a deterministic function of position ( i k , j k ) , trained by backpropagation of gradients from the multi-fixation RBM learning cost. In practice, we force the components of z ( i k , j k ) to be in [0 , 1] 3 . The multi-fixation RBM is illustrated in Figure 1. The multi-fixation RBM must learn to accumulate useful features from each glimpse, and it must also learn a good policy for choosing the fixation points. We refer to these two goals as  X  X earning the what-where combination X  and  X  X earning where to look X . 4.1 Learning the what-where combination For now, let X  X  assume that we are given the sequence of glimpses x t 1: K fed to the multi-fixation RBM for each image I t . As suggested by [9], we can train the RBM to minimize the following hybrid cost over each input x t 1: K and label l t : where y t = e l t . The first term in C hybrid is the discriminative cost and its gradient with respect to the RBM parameters can be computed exactly, since p ( y t | x t 1: K ) can be computed exactly (see [9] for more details on how to derive these gradients) . The second term is the generative cost and its gradient can only be approximated. Contrastive Divergence [10] based on one full step of Gibbs sampling provides a good enough approximation. The RBM is then trained by doing stochastic or mini-batch gradient descent on the hybrid cost.
 In [9], it was observed that there is typically a value of  X  which yields better performance than using either discriminative or generative costs alone. Putting more emphasis on the discriminative term ensures that more capacity is allocated to predicting the label values than to predicting each pixel value, which is important because there are many more pixels than labels. The generative term acts as a data-dependent regularizer that encourages the RBM to extract features that capture the statistical structure of the input. This is a much better regularizer than the domain-independent priors implemented by L1 or L2 regularization.
 We can also take advantage of the following obvious fact: If the sequence x t 1: K is associated with a particular target label y t , then so are all the subsequences x t 1: k where k &lt; K . Hence, we can also train the multi-fixation RBM on these subsequences using the following  X  X ybrid-sequential X  cost: where the second term, which corresponds to negative log-likelihoods under a so-called conditional RBM [8], plays a similar role to the generative cost term of the hybrid cost and encourages the RBM to learn about the statistical structure of the input glimpses. An estimate of the gradient of this term can also be obtained using Contrastive Divergence (see [8] for more details). While being more expensive than the hybrid cost, the hybrid-sequential cost could yield better generalization performance by better exploiting the training data. Both costs are evaluated in Section 6.1. 4.2 Learning where to look Now that we have a model for processing the glimpses resulting from fixating at different positions, we need to define a model which will determine where those fixations should be made on the m  X  m grid of possible positions.
 After k  X  1 fixations, this model should take as input some vector s k containing information about the glimpses accumulated so far (e.g. the current activation probabilities of the multi-fixation RBM score should be predictive of how useful fixating at the given position will be. We refer to this model as the controller .
 Ideally, the fixation position with highest score under the controller should be the one which maxi-mizes the chance of correctly classifying the input image. For instance, a good controller could be such that i.e. its output is proportional to the log-probability the RBM will assign to the true target y t of the image I t once it has fixated at position ( i k , j k ) and incorporated the information in that glimpse. In other words, we would like the controller to assign high scores to fixation positions which are more likely to provide the RBM with the necessary information to make a correct prediction of y t . A simple training cost for the controller could then be to reduce the absolute difference between the sequences of glimpses generated while training the multi-fixation RBM. During training, these sequences of glimpses can be generated from the controller using the Boltzmann distribution which ensures that all fixation positions can be sampled but those which are currently considered more useful by the controller are also more likely to be chosen. At test time however, for each k , the position that is the most likely under the controller is chosen 4 .
 values of s k and ( i k , j k ) (though one could consider training a separate controller for each k ). A constant learning rate of 0.001 was used for training. As for the value taken by s k , we set it to which can be seen as an estimate of the probability vector for each hidden unit of the RBM to be 1, given the previous glimpses x 1: k  X  1 . For the special case k = 1 , s 1 is computed based on a fixation at the center of the image but all the information in this initial glimpse is then  X  X orgotten X , i.e. it is only used for choosing the first image-dependent fixation point and is not used by the multi-fixation RBM to accumulate information about the image. We also concatenate to s k a binary vector of size m 2 (one component for each possible fixation position), where a component is 1 if the associated position has been fixated. Finally, in order to ensure that a fixation position is never sampled twice, we impose that p controller (( i k , j k ) | x t 1: k  X  1 ) = 0 for all positions previously sampled. 4.3 Putting it all together Figure 2 summarizes how the multi-fixation RBM and the controller are jointly trained, for either the hybrid cost or the hybrid-sequential cost. Details on gradient computations for both costs are also given in the supplementary material. To our knowledge, this is the first implemented system for combining glimpses that jointly trains a recognition component (the RBM) with an attentional component (the fixation controller). A vast array of work has been dedicated to modelling the visual search behavior of humans [11, 12, 13, 14], typically through the computation of saliency maps [15, 16]. Most of such work, however, is concerned with the prediction of salient regions in an image, and not with the other parts of a task-oriented vision classifier.
 Surprisingly little work has been done on how best to combine multiple glimpses in a recognition system. SIFT features have been proposed either as a prefilter for reducing the number of possible fixation positions [17] or as a way of preprocessing the raw glimpses [13]. [18] used a fixed and hand-tuned saliency map to sample small patches in images of hand-written characters and trained a recursive neural network from sequences of such patches. By contrast, the model proposed here does not rely on hand-tuned features or saliency maps and learns from scratch both the where to look and what-where combination components. A further improvement on the aforecited work consists in separately learning both the where to look and the what-where combination components [19, 20]. In this work however, both components are learned jointly, as opposed to being put together only at test time. For instance, [19] use a saliency map based on filters previously trained on natural images for the where to look component, and the what-where combination component for recognition is a nearest neighbor density estimator. Moreover, their goal is not to avoid fixating everywhere, but to obtain more robust recognition by using a saliency map (whose computation effectively corresponds to fixating everywhere in the image). In that respect, our work is orthogonal, as we are treating each fixation as a costly operation (e.g. we considered up to 6 fixations, while they used 100 fixations). We present three experiments on three different image classification problems. The first is based on the MNIST dataset and is meant to evaluate the multi-fixation RBM alone (i.e. without the con-troller). The second is on a synthetic dataset and is meant to analyze the controller learning algorithm and its interaction with the multi-fixation RBM. Finally, results on a facial expression recognition problem are presented. 6.1 Experiment 1: Evaluation of the multi-fixation RBM In order to evaluate the multi-fixation RBM of Section 3.2 separately from the controller model, we trained a multi-fixation RBM 5 on a fixed set of 4 fixations (i.e. the same fixation positions for all im-(MNIST images are of size 28  X  28 ) and their order was chosen at random for every parameter up-date of the RBM. The retinal transformation had a high-resolution fovea covering 38 pixels and 60 hexagonal low-resolution regions in the periphery (see Figure 2 for an illustration). We used the training, validation and test splits proposed by [21], with a training set of 10 000 examples. The results are given in Figure 2, with comparisons with an RBF kernel SVM classifier and a single hidden layer neural network initialized using unsupervised training of an RBM on the training set (those two baselines were trained on the full MNIST images). The multi-fixation RBM yields per-formance comparable to the baselines despite only having four glimpses, and the hybrid-sequential cost function works better than the non-sequential, hybrid cost. 6.2 Experiment 2: evaluation of the controller In this second experiment, we designed a synthetic problem where the optimal fixation policy is known, to validate the proposed training algorithm for the controller. The task is to identify whether Figure 2: A: Pseudocode for the training update of the multi-fixation RBM, using either the hybrid or hybrid-sequential cost. B: illustration of glimpses and results for experiment on MNIST. there is a horizontal (positive class) or vertical (negative class) 3-pixel white bar somewhere near the edge of a 15  X  15 pixel image. At the center of the image is one of 8 visual symbols, indicating the location of the bar. This symbol conveys no information about the class (the positive and negative classes are equiprobable) but is necessary to identify where to fixate. Figure 3 shows positive and negative examples. There are only 48 possible images and the model is trained on all of them (i.e. we are measuring the capacity of the model to learn this problem perfectly). Since, as described earlier, the input s 1 of the controller contains information about the center of the image, only one fixation decision by the controller suffices to solve this problem.
 A multi-fixation RBM was trained jointly with a controller on this problem 6 , with only K = 1 fixation. When trained according to the hybrid cost of Equation 8 (  X  = 1 ), the model was able to solve this problem perfectly without errors, i.e. the controller always proposes to fixate at the region containing the white bar and the multi-fixation RBM always correctly recognizes the orientation of the bar. However, using only the discriminative cost (  X  = 0 ), it is never able to solve it (i.e. has an error rate of 50%), even if trained twice as long as for  X  = 1 . This is because the purely discriminative RBM never learns meaningful features for the non-discriminative visual symbol at the center, which are essential for the controller to be able to predict the position of the white bar. 6.3 Experiment 3: facial expression recognition experiment Finally, we applied the multi-fixation RBM with its controller to a problem of facial expression recognition. The dataset [23] consists in 4178 images of size 100  X  100 , depicting people acting one of seven facial expressions (anger, disgust, fear, happiness, sadness, surprise and neutral, see Figure 3 for examples). Five training, validation and test set splits where generated, ensuring that all images of a given person can only be found in one of the three sets. Pixel values of the images were scaled to the [  X  0 . 5 , 0 . 5] interval.
 A multi-fixation RBM learned jointly with a controller was trained on this problem 7 , with K = 6 fixations. Possible fixation positions were layed out every 10 pixels on a 7  X  7 grid, with the top-left Experiment 2: synthetic dataset Figure 3: A: positive and negative from the synthetic dataset of experiment 2. B: examples and results for the facial expression recognition dataset. position being at pixel (20 , 20) . The retinal transformation covered around 2000 pixels and didn X  X  use a periphery 8 (all pixels were from the fovea). Moreover, glimpses were passed through a  X  X re-processing X  hidden layer of size 250, initialized by unsupervised training of an RBM with Gaussian visible units (but without target units) on glimpses from the 7  X  7 grid. During training of the multi-fixation RBM, the discriminative part of its gradient was also passed through the preprocessing hidden layer for fine-tuning of its parameters.
 Results are reported in Figure 3, where the multi-fixation RBM is compared to an RBF kernel SVM trained on the full images. The accuracy of the RBM is given after a varying number of fixations. We can see that after 3 fixations (i.e. around 60% of the image) the multi-fixation RBM reaches a performance that is statistically equivalent to that of the SVM ( 58 . 2  X  1 . 5% ) trained on the full images. Training the SVM on a scaled-down version of the data ( 48  X  48 pixels) gives a similar performance of 57 . 8% (  X  1 . 5% ). At 5 fixations, the multi-fixation RBM now improves on the SVM, and gets even better at 6 fixations, with an accuracy of 62 . 7% (  X  1 . 5% ). Finally, we also computed the performance of a linear SVM classifier trained on the concatenation of the hidden units from a unique RBM with Gaussian visible units applied at all 7  X  7 positions (the same RBM used for initializing the preprocessing layer of the multi-fixation RBM was used). This convolutional approach, which requires 49 fixations, yields a performance of 61 . 2% (  X  1 . 5% ), slightly worse but statistically indistinguishable from the multi-fixation RBM which only required 6 fixations. Human vision is a sequential sampling process in which only a fraction of the optic array is ever processed at the highest resolution. Most computer vision work on object recognition ignores this fact and can be viewed as modelling tachistoscopic recognition of very small objects that lie entirely within the fovea. We have focused on the other extreme, i.e. recognizing objects by using multiple task-specific fixations of a retina with few pixels, and obtained positive results. We believe that the intelligent choice of fixation points and the integration of multiple glimpses will be essential for making biologically inspired vision systems work well on large images.
 Acknowledgments We thank Marc X  X urelio Ranzato and the reviewers for many helpful comments, and Josh Susskind and Tommy Liu for help with the facial expression dataset. This research was supported by NSERC.
