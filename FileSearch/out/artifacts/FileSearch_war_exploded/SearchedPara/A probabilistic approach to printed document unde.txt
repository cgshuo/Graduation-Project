 ORIGINAL PAPER Eric Medvet  X  Alberto Bartoli  X  Giorgio Davanzo Abstract We propose an approach for information extrac-tion for multi-page printed document understanding. The approach is designed for scenarios in which the set of possi-ble document classes, i.e., documents sharing similar content and layout, is large and may evolve over time. Describing a new class is a very simple task: the operator merely provides a few samples and then, by means of a GUI, clicks on the OCR-generated blocks of a document containing the infor-mation to be extracted. Our approach is based on probability: we derived a general form for the probability that a sequence of blocks contains the searched information. We estimate the parameters for a new class by applying the maximum likeli-hood method to the samples of the class. All these parameters depend only on block properties that can be extracted auto-matically from the operator actions on the GUI. Processing a document of a given class consists in finding the sequence of blocks, which maximizes the corresponding probability for that class. We evaluated experimentally our proposal using 807 multi-page printed documents of different domains (invoices, patents, data-sheets), obtaining very good results X  e.g., a success rate often greater than 90% even for classes with just two samples.
 Keywords Document understanding  X  Automatic model upgrading  X  Invoice analysis  X  Maximum likelihood 1 Introduction Information extraction from printed documents plays a key role in many areas: office automation, knowledge manage-ment, intelligence and so on. Manual processing is often not feasible or fully satisfactory, typically because of the high volume of documents to be processed and the high cost per-unit introduced by human operators. Augmenting the automation degree of information extraction from printed documents may thus have a substantial impact on many set-tings of highly practical interest.

Achieving full automation is still a challenge, due to the huge variability of content types and layouts. The involve-ment of human operators is hence required, in particular for providing a description of logical structures of docu-ments, hereinafter models , which systems use for extracting information from such documents. Models may be described either explicitly or by means of a training set of suitably annotated sample documents. A model effectiveness is usu-ally bounded to documents of the same class , i.e., docu-ments that share layout and content type such as invoices from a given firm or data-sheets for a given product category. A human operator is required whenever a new class is to be handled, in order to define the corresponding model. Clearly, the amount of work and skill required for defining a new model should be kept as small as possible, especially when the occurrence of new classes may be a frequent event or the number of documents of a given class may be small.
In this paper, we propose an approach for information extraction from multi-page printed documents that has very good performance and is designed to make the definition of a new model a very simple and lightweight operation. When a new model has to be defined, a training set composed of one or more documents of the new class is required. For each document in the training set, the system automatically transforms the document into a set of OCR-generated blocks, and all the operator has to do is selecting the block sequence containing the piece of information to be extracted (clearly, the system is capable of extracting multiple pieces of infor-mation from a given document, but this point is irrelevant to the discussion). The operator thus merely chooses from a set of blocks drawn over a picture of the document, which may be done without any specialized skill and with a few mouse clicks using a very simple GUI.

We represent models in terms of values for attributes of OCR-generated blocks that pertain to both layout and content, e.g., horizontal/vertical position, page number, text length and alike. During model construction, we use the attribute values extracted from the blocks selected by the operator to fit several univariate probability distributions X  one for each attribute X  X ccording to the maximum likeli-hood method: for each distribution, we choose the values of its parameters in order to maximize the probability of the observed attribute value on the documents in the training set. The resulting parameters constitute the internal representa-tion of a model. Given a new document of a class whose model is known (we remark that the matching of documents with classes is beyond the scope of this work X  X ee also Sect. 2 ), we identify the block sequence containing the infor-mation to be extracted by simply selecting the sequence that maximizes the corresponding probability X  X .e., the one that is most probable given the model. The practical application of each of these steps requires few trivial computations.
In general, the probability that a given block sequence contains the required information follows a multivariate dis-tribution of the attributes of the corresponding blocks. As will be clearer from the paper, the resulting expression is very complex and not practically useful. The fact that some attributes are textual increases complexity further. An impor-tant component of our contribution consists in the derivation of a more tractable expression, based on univariate distri-butions and amenable to fitting by means of the maximum likelihood method.

We evaluated the performance of our proposal on a dataset composed by 807 multi-page printed documents belonging to three radically different domains: invoices, patents and electronic components data-sheets. The experimental results show that our proposal is very effective, even when the train-ing set is composed of very few samples. The ability to handle effectively a very small training set allows keeping human involvement to a minimum and is a necessity whenever the arrival rate of documents belonging to the new model is very low. We also propose and evaluate a method for automatically upgrading a model definition, as more and more documents of the model are processed.
 The remaining part of this paper is organized as follows. Section 2 presents an overview about current state of the art in the field. In Sect. 3 , we present the scenario and our approach from a high level point of view: in particular, we introduce the model, which describes how the information should be extracted from a document of a given class. In Sect. 4 ,we show how we derive a general form for the probability dis-tribution, the main component of the model, explaining the base assumptions. In Sect. 5 , the details about how we build a model from a training set, along with a synthetic exam-ple of the procedure, are provided. Section 6 presents our experimental evaluation, and Sect. 7 summarizes this work conclusions. 2 Related work Document understanding is a process that consists in ana-lyzing documents in order to extract human understandable information and codify it into machine-readable form. The complexity of document understanding and its system engi-neering challenges, as well as insights into important works in this field, is discussed in [ 10 ].

Information extraction systems can be classified depend-ing on whether the class of the document to be processed is known or unknown. When the system does not need to know the document class, a fair amount of a priori knowledge about the specific application domain X  X .e., invoices, medical receipts and so on X  X s usually necessary and embedded within the system. For example, a table of  X  X ain primary tags X  X susedin[ 5 ] to identify labels of interesting infor-mation. This table utility is bounded to the domain of invoices and, perhaps more importantly, is language-depen-dent. A similar solution is followed by [ 11 ] where cer-tain words are marked as keywords. Our approach does not depend on such a priori knowledge, which allows us to address different application domains and languages equally well and, most importantly, without any domain-or lan-guage-specific precompilation.

Systems where the document class is known X  X ike ours X  are generally more effective at identifying and extracting the desired information, but they have to face the problems of (i) associating each document with the corresponding class and (ii) defining a document model for each class. Both issues require a human operator, with varying degrees of involve-ment and required skills depending on the specific system. The matching of documents with classes is beyond the scope of this work: in our system, we use an automated open world classifier based on the spatial density of black pixels and In general, there is a fair amount of research about the prob-lem of matching documents against a predefined set of stored templates. An interesting approach involves the notion of component block representation, similar to our block level description of a document, and is presented in [ 17 ]. Insights into document classification in this context can be found in [ 6 , 9 , 12 ].
Definition of document models usually requires an operator to identify labels and locations for all metadata of the corresponding class [ 15 ]. This operation is performed through a specialized GUI, and some amount of specific skill is required. This approach is followed, essentially, by most of the commercial solutions currently available, which require an operator to  X  X raw X  a model for the given class. In our work, we attempt to keep the model definition phase as simple and lightweight as possible, in particular by not requir-ing any specific IT-related skill X  X t seems fair to claim that any administrative operator will have the ability to identify the OCR-generated blocks containing the required informa-tion and click on them.

The approach presented in [ 7 ] is based on a learning pro-cedure similar to the systems mentioned above, as it requires a table (file) where logical objects of interest and related tags have been manually located on a number of sample invoices of the given class. The impact of the training set size on the approach effectiveness is not investigated. In our work, we assessed the performance of our approach with varying size for the training set and found very good performance even with very small training sets X  X n the order of 2 X 3 docu-ments. We also proposed and evaluated different policies for updating the system knowledge during its usage: we show experimentally that performance may improve by letting the system learn from its processing results, even without requir-ing feedback from operators. Our dataset is a superset of the publicly available dataset used by [ 7 ]. We found that our approach tends to perform better, although we could not perform a detailed quantitative comparison (see Sect. 6 for details). We attribute this tendency to the fact that our approach may successfully identify blocks whose textual content has been garbled by the OCR system, whereas OCR errors simply prevented the system in [ 7 ] from identifying the searched tags or labels.

The fact that our approach does not depend on the ability to identify any specific label or keyword indeed implies that OCR performance is not critical for the quality of the results. As will be clearer from the following sections, we identify relevant blocks basing on their probability of being consistent with some geometric properties (size, posi-tion, page) and textual information (text length, text align-ment, content) as specified by the model. It follows that even when the OCR fails to detect correctly such pieces of text as "Total" or "Price" , our system generally is still able to identify the relevant information. Of course, OCR performance remains crucial for correctness of the informa-tion actually extracted X  X everal workarounds may be used in practice to mitigate this issue, though (e.g., type checks, range checks and alike).

We have not performed a quantitative analysis of the impact of OCR performance on the quality of informa-tion location and extraction. Indeed, this is a topic where further research is needed even from a methodological point of view [ 13 , 16 ]. On a qualitative level, however, many of the documents in our dataset were of bad quality and resulted in bad OCR results.

An approach that describes documents in terms of attrib-uted relational graphs is presented in [ 8 ]: an excellent effec-tiveness in detecting the searched information is reported, but the experimental setting is limited to two classes. A similar method is proposed later in [ 2 ]: in order to expand the cover-age of the graphs-based approach, the authors rely on statis-tical decision trees and bi-and tri-grams applied to the block textual contents. The study aims at recognizing the document structure, i.e., at identifying general information (e.g., title, body, captions, ...) of textual documents. The model consists of a set of geometric and logical structures and is based on common-sense reasoning and statistical methods. The pro-posed method effectiveness is assessed on a dataset of about 800 single-page documents, whose complexity is set on three levels basing on the number of objects X  X hich roughly cor-responds to text areas X  X n the document. Another method for document structure recognition is proposed in [ 14 ]: the authors build fuzzy logical rules for block classification that involve considering both layout and textual features. Similar to our work, an operator can define a rule by providing a positive example by means of a GUI and some mouse clicks; yet, the author do not investigate on their system ability to learn from a training set composed of multiple documents.
A problem that is not closely related to information extrac-tion from printed documents, but which also relies on doc-ument analysis, is the identification of reading order. The system proposed in [ 1 , 21 ] extracts several features from color scanned documents written in English (e.g., coor-dinates, foreground and background color, font size and type,...) and then uses spatial inference and natural language processing in order to capture the relationship among blocks of printed text.

Many of the studies focusing on printed document under-to the economic relevance of these kinds of documents in terms of costs and volumes. In this respect, the processing cost of a printed invoice has been estimated in about $ 13 per document, and [ 18 ] reports that the number of printed paper forms that are handled yearly by Japanese public adminis-tration is greater than 2 billions. In our work, we propose an approach that is not tailored to a specific type of document, and we evaluated it on a dataset composed of documents from radically different application domains: not only invoices, but also patents and data-sheets.

Concerning printed forms, their automatic processing is the focus of [ 3 ]. The authors propose a document struc-ture grammar that can be represented in a Table Form Markup Language (TFML): a semi-automatic procedure is described to analyze an empty form layout and describes its structure in terms of TFML. The procedure heavily relies on printed rules, which are not always present in other kinds of documents X  X .g., those in our dataset: patents, data-sheets, invoices.

Although the problems involved in automated processing of invoices and forms are largely similar, the invoice analy-sis problem is perhaps more challenging, because invoice layouts exhibit a large diversity, due to the huge number of firms that emit invoices. Invoice analysis systems need to address the model definition phase, which is especially critical when the occurrence of new models is a frequent event. The authors of [ 19 ] quantify the economic impact of accountant personnel workload involved in building and maintaining invoice models for a widely adopted invoice rec-ognition software. The model definition phase is central in our work, and we also propose a method for automatically upgrading a model and thus mitigating the model mainte-nance cost. 3 Our approach overview 3.1 Scenario A document D is represented as a set of block, denoted by { b tent. The position is specified by the page number p and the coordinates x and y from the page origin (upper left corner). The size is specified by width w and height h . The content is specified by the single-line textual content l , expressed as a character sequence. A block b is hence defined as a tuple of block attributes b = p , x , y ,w, h , l . See Fig. 1 for an example of blocks with depicted attributes.

An OCR system may be used to obtain a representation of this kind from a physical document. Different OCR systems may lead to different representations for the same physical document. The details of this procedure are irrelevant to this discussion.

A document is associated with a schema and a document model. Intuitively, the former describes the information to be extracted from the document, whereas the latter describes how to identify that information. Documents of the same class, e.g., invoices produced by the same firm, share the same model.

More in detail, the schema consists of a set of typed ele-ments . The understanding of a document corresponds to find-ing a value for these elements, based on the content of the document, i.e., of its blocks. From a different point of view, a document corresponds to a row of a table whose columns are specified by the corresponding schema. For example, a schema could be given by Date , Total Amount , Docu-ment Number and so on (we omit the corresponding types for brevity).

The document model, model for short, consists of a set of rules , exactly one rule for each element of the schema. A rule describes how to extract the corresponding value from the document blocks. More details about the rules and spe-cific examples will be given below.

We remark that the correspondence between values and blocks is not one-to-one. More in details, the following con-ditions may or may not occur independently: (a) the value is contained in a single block; (b) no other information is contained in the block/blocks other than the value X  X r the portion of it. For an example, see Sect. 3.2 .

By understanding of a document, we mean the process of extracting from the document values for all items of the (b) schema associated with the document. Our contribution con-sists of: 1. a procedure for understanding a document automatically, 2. a procedure for constructing a model automatically, 3.2 Rule Each rule of a document model is associated with one and only one schema element e . A rule is composed of a cardi-nality ,a matching probability and an extraction function .The cardinality n e defines the length of the sequence of blocks, say B = b 1 ,..., b n e , to which the rule may be applied. The matching probability  X  P e ( B ) is the probability that B con-tains a value for element e . The extraction function f e a function for computing that value as v = f e ( B ) .
For example, the rule for the Date element could be com-posed of a matching probability which, informally, encodes this description: the Date is contained in a pair of blocks: the first stays on the first page, at about 2.2 cm from left page margin and 4 cm from top page margin, with a content simi-lar to "Day: 02/03" ; the second stays on the same page, at about 2 cm left of the first one and 0.5 cm lower, with a content in the form "Year: 2009" The extraction function for the same rule could, informally, say: extract and concatenate the block contents which fol-low the colon.
 The cardinality for this example rule is n e = 2.

The understanding of a document consists of the two following steps, for each rule n e ,  X  P e , f e in the model: 1. find the matching sequence B  X  , i.e., the n e 2. extract the value v from the matching sequence
The building of a model consists of generating a set of rules given a set of documents: for each element, a rule is generated starting from the corresponding values. Such pro-cedure is described in Sect. 5 .

A crucial component of our contribution consists in expressing the matching probability  X  P e in a tractable form and in finding a simple method for estimating its numerous parameters. More in detail, we want to express the match-ing probability, which concerns rather complex events (i.e., the event that a sequence of blocks contains a given value), as a function of simple univariate probability distributions of independent random variables obtained from the block attributes (i.e., p , x , y ,w, h , l ). The details about the way in which we derived the simple form for  X  P e are given in Sect. 4 .
The extraction function operates on the concatenation of the textual contents of the blocks of the matching sequence. In general, the value can be extracted using a regular expres-sion depending largely only on the given element. Yet, while the matching sequence could be found even in the presence of OCR errors, because of such errors, some refinement could be necessary in order to extract the correct value itself. 4 Matching probability details In this section, we describe how we derived a simpler form for the matching probability  X  P e (we will omit the e subscript for brevity). All the following reasoning applies to the docu-ment understanding task itself within the scenario described above and is not bounded to a specific category of documents. We estimate the probability  X  P ( B ) for a sequence B = b  X  P ( B ) = where p i = p i when i = 1 and p i = p i  X  p i  X  1 otherwise, x = x wise, and the same for y i . In other words, we express the position attributes of b i relatively to the position attributes block.

The rationale for this approximation is as follows. First, we assume that the size and content attributes of a block do not depend on any of the other blocks attributes. Second, we assume that the relative position of b i does not depend on the position X  X either absolute, nor relative X  X f any other block, although the absolute position of b i is in general dependent on the absolute position of b i  X  1 . It follows that the proba-bility that both dependent events  X  b i  X  1 is the i  X  1-th block of the matching sequence X  and  X  b i is the i -th block of the matching sequence X  occur is equal to the probability that both the independent events  X  b i  X  1 is the i  X  1-th block of the matching sequence X  and  X  b i is the i -th block of the matching sequence X  occur. For example, consider a rule for sequences of 2 blocks for the Total element of invoices, the first being the label and the second the actual value. If on a given doc-ument, the first block is lower on the page than the usual, possibly for the presence of many item lines in the invoice, the second block will also be lower, but, in general, at the same distance from the first block: that is, y 2 depends on y whereas y 2 = y 2  X  y 1 does not.

By the aforementioned assumptions,  X  P can simply be expressed as a product of probabilities P i related to single blocks. Each P i , which we call the block probability , con-cerns a single block that is a tuple composed of 6 attri-butes whose corresponding 6 random variables are in general dependent on each other. We identified a domain knowledge-based set of dependencies, which allowed us to elaborate and simplify the form of P i , as follows.

First, we can use marginalization in order to write P i bas-ing on the possible values for the page p i : P ( b where P ( b 1  X  p i = k ) is the joint probability of the follow-ing two events: b i is the i -th block of the matching sequence and p i = k . These two events are in general dependent. For example, consider an invoice class, where the total amount value may be located at the bottom of the first page or at the top of the second page: it follows that low values for y are more probable, if the page attribute is equal to 2, and great values for y are more probable, if the page attribute is equal to 1 X  X n other words, the y attribute of the block is depen-dent on the p attribute. We can rewrite the joint probability in terms of conditional probability on the page p i : P ( b i  X  p i = k ) = P ( b i | p i = k ) P ( p i = k ) the matching sequence, given that its page p i is equal to k . Concerning P ( p i = k ) , we assume that there is a finite set K i ={ k 1 , k 2 ,... } of possible values for p i , whose cor-P ( p and 0 otherwise and s i , k = 0if k /  X  K i . Thereby, we can write P i as follows: P ( b and h i are independent from the other three variables. In particular, note that, since blocks contain exactly one line of text, the height attribute h i is largely independent from its text content l i . Hence, we can write: Then, we split the x i ,w i , l i dependency in one between x and w i and another between w i and the text content l i .The dependency between x i and w i represents the fact that a given text could be aligned in three different ways: left, center or right (justified text may be handled in any of these three cases, for the purpose of this analysis). It follows that:  X  in case of left-alignment, x i and w i are independent;  X  in case of center-alignment, x c i = x + w i 2 and w  X  in case of right-alignment, x r i = x i + w i and w i
The dependency between w i and l i represents the fact that, in general, the longer the text content, the larger the block width. We define w i = w i L ( l the characters composing the block text content, being L ( the number of characters in l i : we assume that w i and l largely independent, since w i depends on the font size and type, rather than on the text content.
 forms depending on text alignment: P which can be summarized as: P where x i is a shortcut symbol that represents one among x , x c
Finally, we obtain the following general form for the matching probability:  X  P ( B ) = Note that each P is a univariate distribution.

Next, we assume that the size attributes ( w and h ) and the position attributes ( x and y ) can be described as random variables with normal distribution (denoted by N ( X ,  X  ) In a preliminary phase of our study, we considered using other distributions for the aforementioned random variables X  X n particular the Uniform Distribution X  X ut we found the nor-mal distribution models them better.

Concerning the textual content, P l i , k ( l i ) is the text the matching sequence; P l i , k hence operates on text, differ-ently from all other probabilities that operate on numbers. We assume that P l i , k ( l i ) can be expressed as a Markov chain of order 2. Its state space corresponds to the set of possible characters and 2 pseudo-characters representing the begin and end of the text. The probability M of the text l is defined as the probability of the state sequence corresponding to l . For example, the probability of the word "two" is given by: M ( "two" ) = P ( " t " | " " ) P ( " tw " | " t" ) where and represent the begin and the end of the text, respectively, and each transition probability corresponds to an element of the transition matrix T i , k . The details about how we set T i , k are given in Sect. 5 .

At the end, the final form for the matching probability is the following:  X  P  X  where we omitted the function arguments for readability.
In summary, defining a new model merely translates in choosing a suitable value for the parameters of the above formula (and for the cardinality n , i.e., the number of blocks) that we summarize in Table 1 . In the next section, we describe how we choose these values. 5 Model building We describe here the procedure for defining a model. The operator provides a set of documents and defines the schema of the new model being defined. For each document d ,the system processes d by means of an OCR and presents to the operator a graphical representation of d in terms of its com-posing blocks. The operator then selects on this representa-tion, for each element of the schema, the sequence of blocks containing the value of the element. An example of the result of this action for a document, for a single element, is shown in Fig. 1 . We call this sequence the true sequence . The operator might also specify that d does not contain a value for a given element. We require that the true sequences corresponding to the same element of the schema have the same length for each d : if this requirement is not met, a work-around for this limitation exists and consists in partitioning the training set in subsets in which each sequence of blocks has the same size. Once all documents in the set have been analyzed, the system processes the true sequences provided by the operator as described below. We will describe the procedure for one single element, since the rule for an element can be generated independently from rules for the other elements.

Concerning the rule cardinality n , we set it to the length of the true sequences X  X his value is either 1 or 2 in our exper-iments. Concerning the extraction function f , its building consists of choosing the most suitable candidate among a predefined set of extraction functions suitable for the given element. For example, consider the Date element, the set could be composed of few different regular expression (see Sect. 3.2 ) corresponding to different date formats. We will not further elaborate on this issue.
 The interesting part concerns the matching probability  X  P . The goal of the procedure consists in choosing a value for all parameters in Table 1 . The choice is made in order to fit the distributions in Eqn. 4 according to the maximum likelihood method. As will be clear, the actual processing is quite simple and may be embedded in a model-independent GUI easily.
Let D be the training set composed of the m true sequences that the operator provides as described before and let B  X  b , 1 ,..., b mum likelihood method, as follows.  X  X eset s i , k to the frequency of i -th blocks in D whose  X  For each i , k -th Normal Distribution of Eq. 4 , we esti- X  For each i and k , we choose the x i ,w i , l i dependency
Concerning the text probability M i , k , we proceed as fol-lows. Before actually processing a text l , we perform some simple elaboration, which consists of: (i) transform to low-ercase, (ii) replace all digit characters with "#" , (iii) replace all space characters with standard space character, (iv) replace all punctuation characters with "." and finally (v) replace any other character with "*" . We denote the result with l . Recall that we can describe a chain of order two using a transition matrix T of size a  X  a 2 , being a the number of states. In our case, given the aforementioned text elaborations, a = 32 and T indexes represent, respectively, a single character and a sequence of two characters: e.g., t matrix for M i , k , we start with a frequency matrix F with the same size of T i , k and each element set to 0. For exam-ple, after processing the sequence "banana" , we will have f Then, we process each true sequence textual content l  X  i increment the corresponding F elements. At the end, we set for each pair of indexes u ,v : t where N u is the number of f u ,v that is greater than 0. We use the term to make the text probability smoother, i.e., such that it assigns non-zero (yet low) probabilities also to textual contents that are not present in the training set.
We set our parameters as follows:  X  xy =  X  w h = 0 . 05 inches = 1 . 27mm , = 1 3 . 5.1 Model building example We provide here an example of building a model for one element. Figure 1 shows the top-right portions of two docu-ments of our dataset. The documents belong to the same class and represent patents (see next section for a description of our dataset). The true sequences for the Filing Date ele-ment, as selected by the operator, are highlighted. For each document, the operator simply selected, by means of a GUI, two blocks that were already detected by the OCR system.
In this example, the training set D is composed of m = 2 documents; thus, there are two true sequences composed of two blocks each: B  X  1 = b  X  1 , 1 , b  X  1 , 2 and B  X  2 = Values for blocks attributes follow (where applicable, values are expressed in mm): b , 1 = 1 , 12 . 5 , 48 . 7 , 8 . 6 , 3 . 1 , "Filed:" b , 2 = 1 , 28 . 2 , 48 . 7 , 18 . 8 , 3 . 1 , "Sep. 25, 1990" b , 1 = 1 , 11 . 0 , 64 . 4 , 7 . 8 , 3 . 1 , "Filed:" b , 2 = 1 , 25 . 1 , 64 . 4 , 18 . 8 , 3 . 1 , "Jun. 24, 1994"
Rule cardinality is set to n = 2. The values for the vari-ables derived as explained in Sect. 4 are hence computed by the system as: b , 1 = 1 , 12 . 5 , 48 . 7 , 1 . 43 , 3 . 1 , "Filed." b , 2 = 0 , 15 . 7 , 0 . 0 , 1 . 45 , 3 . 1 , "Sep. ##. ####" b , 1 = 1 , 11 . 0 , 64 . 4 , 1 . 30 , 3 . 1 , "Filed." b , 2 = 0 , 14 . 1 , 0 . 0 , 1 . 45 , 3 . 1 , "Jun. ##. ####"
At this point, all parameters for the matching probabil-ity X  X ee Table 1  X  X an be computed. Concerning the page attribute, we have: s s For the other parameters, only the values for ( i = 1  X  k or ( i = 2  X  k = 0 ) are needed, since all the blocks of the true sequences lay on the first page. These values are as follow (we omit the transition matrix for the sake of brevity):  X   X  The values denoted by the symbol  X  are obtained by lower-bounding the corresponding computed  X  estimates with the proper lower bound (i.e.,  X  xy or  X  w h ). 6 Experiments 6.1 Dataset In order to assess our approach effectiveness, we collected a real-world dataset composed of 4 groups of documents, totaling 807 multi-page documents divided in 85 classes. We partitioned the dataset basing on different document domains (invoices, patents or data-sheets) or, in case of Invoice-1 and Invoice-2 , basing on the dataset source.

The first group X  X hich we call Invoices-1  X  X s com-posed of 406 multi-page invoices, issued by 32 different busi-nesses, each issuer corresponding to a different class. The largest class contains 79 invoices; the first 15 largest classes account for about 80% of this group documents.

The second group X  Invoices-2  X  X s a subset of the dataset used by [ 7 ] and is composed of 156 single-page invoices, belonging to 33 classes. With respect to the orig-inal dataset used by the authors of the cited paper, we discarded the classes containing only one document. The largest class contains 25 documents, and the first 10 larg-est classes account for about 58% of this group documents.
The third group X  Patents  X  X s composed of 135 pat-ents obtained from 10 different patents sources, each patent source corresponding to a different class. The largest class contains 22 patents, and 7 classes contain 10 or more patents.
The last group X  Data-sheets  X  X s composed of 110 data-sheets of electronic components (e.g., Zener diodes) divided in 10 classes. Data-sheets of the same class share the producer and the component type. The 5 largest classes account for about 85% of this group documents.

Documents of the first two groups have been obtained by scanning the corresponding paper documents as black-and-white images at 300 dpi. Image quality is notably higher for documents belonging to the Invoices-2 group. Doc-uments of the other two groups have been obtained convert-ing only the first page of the corresponding PDF, which was available on-line, to black-and-white images at 300 dpi. In about half of the cases, these PDFs were likely obtained by scanning the corresponding paper documents.

Some classes of Invoices-1 consist of documents whose original paper size is smaller than the A4 page format: these invoices have been scanned as A4 pages and positioned in a variable way with respect to the scanner area, resulting in images in which the content position is variable.
Each document image has been converted to a set of blocks (see Sect. 3.1 ) using an OCR system. 1 The OCR system was instructed to deskew images, if needed, and configured to per-form as best as possible. Yet, some errors were introduced by the OCR system: in many cases, they were caused by various scanning artifacts. OCR errors can be classified in segmentation errors and text-recognition errors: the former result in blocks containing different text elements among different documents of the same class; the latter result in tex-tual contents X  X .e., l values X  X hich are quite different from what actually written on the paper. We noted that segmenta-tion errors usually imply text-recognition errors (see Fig. 2 ); moreover, poor print quality X  X .g., documents produced by dot matrix printers X  X sually caused text recognition errors. We did include in the dataset also the documents for which the OCR system introduced errors affecting the blocks that contained the values being searched: this choice allowed us to assess our approach effectiveness in a very realistic scenario, with respect to such OCR errors.

The text contained in our dataset documents is com-posed of printed English characters. The language is Italian for Invoices-1 and Invoices-2 , English for Data-sheets group and many documents of the Patents group and other languages for the other documents.
 We defined three schemata: the schema for groups Invoices-1 and Invoices-2 contains 8 elements, the schema for the Patents group contains 11 elements, and the schema for the Data-sheets group contains 8 ele-ments. Table 2 shows the elements composing the three schemata.

Then, we constructed the ground truth for all documents, i.e., an operator inspected visually each document and (i) (ii) searched all elements of the corresponding schema; (iii) for each element found, manually selected the correspond-ing true sequence. In all documents of our dataset, the true sequence turned out to be composed of 2 blocks at most. True sequences composed of two blocks were usually composed of a block corresponding to a label and a block correspond-ing to the actual element field: for example, Fig. 2 ashows a block containing a date label and a block containing the actual date value.

While manually building the ground truth, we found that the following situations occur: (i) classes whose documents never contain a given element X  X .g., a given producer of a given type of electronic component does not provide the Storage Temp. information; (ii) classes for which only some documents contain a given element X  X .g., given a pat-ent source, some patent obtained from that source has a Representative value while some other has not; (iii) classes whose documents may contain multiple occurrences for a given element X  X .g., a given document may contain a value for Total amount in more than one page or even more than once on the same first page. Concerning case iii, we manually clustered documents based on the occurrence of the corresponding elements X  X .g., all values for Total amount occurring on top of first page of documents of a given class were separated from Total amount values occurring on bottom of first page. We say that these val-ues correspond to exactly one sub-element  X  X .g., Total amount (1) .

Finally, we formed several test sets . Each test set corre-sponds to exactly one rule and contains all and only docu-ments that: (i) belong to the same given class and (ii) contain exactly one value for the given element or sub-element. We obtained 591 test sets with size ranging from 2 to 31 docu-ments. Figure 3 shows the number of test sets vs. their size. Note that more than 50% (304 on 591) of the test sets contains 5 or less documents. 6.2 Results with varying training set size We assessed our approach effectiveness with respect to the size m of the training set. To this end, we proceeded as follows for each test set: (i) we built the rule using a training set com-posed of m documents extracted from the test set; (ii) we processed each of the remaining documents of the test set and, for each document, we determined the block sequence found with the rule (i.e., the matching sequence). We consid-ered a success the case in which the found block sequence corresponded with the document true sequence and a fail otherwise. The success rate is hence given by the number of successes divided by the sum of successes and fails.
In order to average the effect of possible outliers in the training set (e.g., documents for which printing or scanning quality was significantly lower than other documents of that test set), we repeated the experiment with up to 20 differ-ent training sets extracted at random from the test set. For example, with m = 3 and a test set of 4 documents, we performed 3 experiments, each one evaluating the remaining one document; with m = 3 and a test set of 10 documents, we performed 20 experiments, each one evaluating the remain-ing 7 documents. Only test sets with more than m documents concurred to the experimental evaluation of the success rate for training set size m .

Figure 4 shows results in terms of success rate vs. training set size m , one line for each different dataset group.
The main finding is that in all cases, the success rate tends to be greater than 0.90, with a perfect result (i.e., 1.0) for the Invoices-2 group. Another important find-ing is that the proposed approach exhibits very good results even when trained with very few documents. For example, for Invoices-2 group, just a single training document is sufficient in order to obtain a success rate greater than 0.75, while with m = 2, it increases to more than 0.90.
 These results show also that the success rate is lower for Data-sheets and Patents groups. We verified that the rather low quality of the scanned documents of these two groups caused a much larger number of OCR errors, both segmentation and text-recognition errors, which negatively affected the success rate. 6.3 On-line retraining In this section, we analyze the possibility of on-line retrain-ing of the system. By this, we mean an operating proce-dure in which document models are defined based on a very small training set and update whenever new documents of that model are processed, either automatically or with mini-mal involvement of the operator.

The rationale for this operating procedure is based on practical and important considerations. First, a small train-ing set implies that the operator has to mark a smaller set of documents. Since the operator is involved for less time, the potential for cost savings is evident, in particular when the volume of documents and models is large. Second, although results in the previous section demonstrate that our approach is effective even with only a few samples, they also show that performance generally improve with the size of the training set. It follows that updating a document model when new samples become available is useful. Third, a large training set could not be available, or it could become available only after a long time. In these cases, it may be mandatory to start processing documents as soon as possible, even with slightly worse performance, and improve the system if and when new samples become available.

In order to address these considerations, we reasoned in term of retraining policies. A retraining policy defines whether and how an existing rule is affected by the elabo-ration of a new document of the given class. We considered three possible retraining policies. In all cases, we assume that the current training set D from which the rule was generated is available.  X  X nsupervised X  A new training set D is built adding each  X  X upervised X  The operator is asked to confirm that the  X  X ybrid ( t ) X  This policy corresponds to the  X  X uper-
Policies  X  X nsupervised X  and  X  X upervised X  require differ-ent amounts of involvement by the human operator. The first policy is fully automatic as it requires no intervention at all. The second policy requires the operator to either accept or reject, and possibly correct the processing result. In a practi-cal implementation, this step could correspond to just few clicks, given that a visual representation of the matching sequence is provided to the operator. Policy  X  X ybrid () X  is similar to  X  X upervised X , except that the involvement of the operator is no longer required after the training set size has reached the threshold.

We performed an experimental evaluation of the proposed policies as follows. We selected the 110 test sets (as defined in Sect. 6.1 ) with at least 10 documents. We chose to dis-card the other test sets, because their small size makes them not very meaningful for this specific experiment. For each of these 110 test sets, we generated 20 sequences using 20 set permutations, in order to average the effect of possible outliers in the sets. Then, for each sequence, we: (i) built a rule using a training set composed of only one element , corresponding to the first document of the sequence; (ii) for each subsequent document in the sequence, we fed it to our prototype and updated the rule according to the given policy (iii) and we proceeded with the next document.

Figure 5 shows results in terms of success rate vs. sequence index k ( k = 1 for the first document in the sequence, 2 for the second, and so on), one line for each different retraining policy.

As expected, the  X  X upervised X  policy delivers the best per-formance: a detection rate greater than 0.90 is reached after 7 documents have been evaluated. This result confirms what already found with the former experiments: recall that Fig. 5 shows the success rate averaged over all the 4 groups.
With the  X  X ybrid ( t ) X  policy, the success rate increases with k even when only 2 processing results have been con-firmed by the operator. This is interesting because the training set potentially contains noisy elements.

The  X  X nsupervised X  shows to be inadequate. We think the motivation of this result is in the rather low success rate that is obtained with k = 2 (i.e., with a training set size m = 1): this causes this policy update the training set with a new element which, in about 40% of the cases, is not the true sequence.
Clearly, the practical impact of the above findings will depend strongly on nature and scale of the system, in par-ticular, on the amount of feedback from operators that may actually be used for the retraining policies.

For example, a small-scale system could be synchronous, i.e., any new job for a given class will not be submitted before the previous job result has been received by the user. In this case, the user would perceive a detection rate as showed in the best line of Fig. 5 . A large scale system, on the other hand, could work asynchronously: users submit docu-ment-processing jobs and then extract and check their results sometime later. Several documents of a given class could be elaborated before a user feedback for a previous job of that class is available to the system X  X .g., few users requiring batch elaborations, or several users. In this case, each user might hence perceive an actual detection rate that stays, at the beginning, between the worst and the best lines of Fig. 5 . The key fact, however, is that the detection rate will ultimately tend to the greatest values found in our experimentation X  i.e., about 0.95 on the average, and up to 1.0 for good quality documents X  X asing on the assumption the users X  feedback is eventually provided to our system. 6.4 Performance All the experiments described before have been executed on prototype written in Java 1.6SE, running on a quad core 2.50 Ghz PC with 4 GB of RAM. The average time for build-ing a matching probability goes from less than 0.5 ms, for m = 1, to less than 3.5 ms, for m = 20, growing linearly with m . The average time for searching for a single element on a document, given its blocks representation, is about 15 ms. To place these figures in perspective, it suffices to note that an OCR execution on a single page document of our dataset takes about 12 s, on the same hardware. 7 Conclusions We have proposed an approach for information extrac-tion from multi-page printed documents. The system oper-ates on documents represented in terms of text blocks, as obtained using an OCR system, each block simply consisting of position and size attributes and textual content. The approach is based on a probabilistic framework, not bound to a specific domain or type of documents and does not require any hand-coded knowledge. We derived a simple parametric form for the probability that a given block sequence contains the required information. Our contribution consists also of a procedure for building a model for a new document class, given few samples, and a procedure for extracting all the rel-evant information from a document, using the model. The form that we derive for the probability is general yet simple enough to allow us applying the maximum likelihood method and to do so by means of a class-independent GUI that does not require any specialized IT-related skills.

We evaluated the performance of our proposal on a dataset composed by 807 multi-page printed documents belonging to three different domains: invoices, patents and electronic components data-sheets. We also proposed and evaluated a method for automatically upgrading a model definition as more and more documents of the model are processed. This method would allow keeping human involvement to a min-imum and would be a necessity whenever the arrival rate of documents belonging to the new model is very low. The experimental results show that our proposal is very effective, even when the training set is composed of very few samples. References
