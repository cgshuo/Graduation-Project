 Traditional anomaly detection on social media mostly fo-cuses on individual point anomalies while anomalous phe-nomena usually occur in groups. Therefore it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this paper, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detec-tion (GLAD) model. GLAD takes both pair-wise and point-wise data as input, automatically infers the groups and de-tects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further gen-eralize GLAD to its dynamic extension d-GLAD. We con-duct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies. H.2.8 [ Database Applications ]: Data mining anomaly detection; social media analysis; hierarchical Bayes modeling
Social media provide convenient platforms for people to share, communicate, and collaborate. While people enjoy the openness and convenience of social media, many mali-cious behaviors, such as bullying, terrorist attack planning, and fraud information dissemination, can happen. There-fore, it is extremely important that we can detect these abnormal activities as accurately and early as possible to prevent disasters and attacks.

By definition, anomaly detection aims to find  X  X n obser-vation that deviates so much from other observations as to arouse suspicion that it was generated by a different mech-anism X  [8]. Several algorithms have been developed specifi-cally for social media anomaly detection such as power-law models [2], spectral decomposition [18], scan statistics [15], and random walk [14, 17]. However, these algorithms only detect the individual point anomaly . For example, [2] pro-poses an  X  X ddBall X  algorithm to spot anomalous nodes in a graph. The algorithm extracts features from the egonet of the node and declares anomaly node whose features deviate from the power-law pattern.

In reality, anomaly may not only appear as an individual point, but also as a group. For instance, a group of people collude to create false product reviews or threat campaign in social media platforms; in large organizations, malfunc-tioning teams or insider groups closely coordinate with each other to achieve a malicious goal. Those appear as examples for another type of anomaly: group anomaly , which has not been thoroughly examined in social media analysis. In this work, we focus on group anomaly detection. We are inter-ested in finding the groups which exhibit a pattern that does not conform to the majority of other groups. This problem has found its applications in galaxy identification [19], high energy particle physics [13], anomalous image detection and turbulence vorticity modeling [20].

We identify three major challenges in group anomaly de-tection: (i) Two forms of data coexist in social media: one is the point-wise data, which characterize the features of an individual person. The other is pair-wise relational data, which describe the properties of social ties. In social science, a fundamental axiom of social media analysis is the concept that structure matters. For example, teams with the same composition of member skills can perform very differently depending on the patterns of relationships among the mem-bers [5]. Therefore, it is important to take into account both point-wise and pair-wise data during anomaly detec-tion. (ii) Group anomaly is usually more subtle than indi-vidual anomaly. At the individual level, the activities might appear to be normal [21]. Therefore, existing anomaly de-tection algorithms usually fail when the anomaly is related to a group rather than individuals. (iii) Empirical studies in social media analysis suggest the dynamic nature of indi-vidual network positions [12]. People X  X  activities and com-munications change constantly over time and we can hardly know the groups beforehand. Thus developing a method that can be easily generalized to dynamic setting is critical to anomaly detection in evolving social media data.
In this paper, we take a graphical model approach to ad-dress those challenges. We propose a hierarchical model, i.e, Group Latent Anomaly Detection (GLAD) model, to con-nect two forms of data. To handle the dynamic characteris-tics of the social media data, we further develop a dynamic extension of GLAD: the d-GLAD model. We show that GLAD outperforms existing approaches in terms of group anomaly detection accuracy and robustness. When deal-ing with dynamic social networks, the dynamic extension of GLAD achieves lower false positive rate and better data fitting. The major contributions of this paper can be sum-marized as follows: 1. We formulate the problem of group anomaly detection 2. We develop a graphical model called GLAD. GLAD 3. We conduct thorough experiments on both synthetic
We review the related models on group anomaly detection and illustrate the motivation behind our approach.
The Multinomial Genre Model (MGM) proposed in [19] first investigates the problem following the paradigm of La-tent Dirichlet Allocation (LDA) [4]. As a text processing tool, LDA assumes that each word is associated with a topic and a document is a mixture of topics. Similarly, MGM models a group as a mixture of Gaussian distributed topics with certain mixture rate and assumes there exists  X  X est X  mixture rates, corresponding to the mixture rates of nor-mal groups. Then it conducts group anomaly detection by scoring the mixture rate likelihood of each group. One draw-back of MGM is that the set of candidate mixture rates is shared globally by groups. It might leads to poor perfor-mance when groups have different sets of mixture rates. [20] further extends MGM to Flexible Genre Model (FGM) with more flexibility in the generation of topics. Specifically, the model considers the set of topic mixture rates as random variables rather than model hyper-parameters, which would adapt to diverse  X  X enres X  in groups, each of which is a typical distribution of topic mixture rates.

Another line of work takes a discriminative approach. [13] uses the same definition of group anomaly from [19]. It considers kernel embedding of the probabilistic distributions and generalizes one-class support vector machine from point anomaly detection to group anomaly detection. The pro-posed support measure machine (SMM) algorithm maps the distributions to a probability measure space with kernel meth-ods, which can handle the aggregate behavior of data points.
However, existing approaches separate the group anomaly detection task into two stages: group discovery and anomaly detection. They require the group information to be given before applying the anomaly detection algorithms. For ex-ample, in [19], the Sloan Digital Sky Survey (SDSS) dataset needs to be pre-processed before feeding into MGM. The authors first construct a neighborhood graph and then treat the connected components in the graph as groups. For the application on turbulence data, the FGM model [20] consid-ers the vertices in a local cubic region as a group. In SMM [13], the authors treat the high energy particles generated from the same collision event as a group.

The two-stage approaches identify the groups from the pair-wise data and infer the anomalies based on the point-wise data. This strategy assumes that the point-wise and pair-wise data are marginally independent. However, such independence assumption might underestimate the mutual influence between the group structure and the feature at-tributes. The detected group anomalies can hardly reveal the joint effect of these two forms of data. These motivate us to build an alla prima that can account for both forms of data and accomplish the tasks of group discovery and anomaly detection all at once.

Additionally, existing work can only deal with static net-work and fixed size groups. This is not feasible for the time-evolving nature of social media data. For example, in cor-porate networks, employees may switch teams from one to the other. The organization structure of a team may also change. As the dynamic setting needs to take into account the flexible group size and the changing mixture rates, we further adapt our model to the dynamic setting and formu-late the problem as a change point detection task.
Group anomaly detection in social media analysis may shed light on a wide range of real world problems such as corporate restructuring, team job-hopping and political in-clination shift to which our approach can apply. In section 3, we provide a formal definition of group anomaly in social media analysis. With the definition, we develop the GLAD model in section 4 and present its learning and inference al-gorithm. In section 5, we describe the dynamic extension of the GLAD model: the d-GLAD model, which can handle the dynamic social networks. Section 6 shows the empirical evaluation results of GLAD and d-GLAD on synthetic and real world datasets compared with existing baseline models.
The core of our group anomaly definition lies in the col-lective behavior of individuals. For example, a document is a mixture of various topics and a team is a mixture of dif-ferent roles. Therefore, we model the node features of each group as a mixture of components. Each component could be an article topic, a social role or a job title. Specifically, we can describe a component as either a discrete variable such as multinomial distribution or a continues variable like Gaussian distribution, depending on the data type of fea-tures. Here we use the term role as a general notion for the component. We assume that there are a fixed number of roles and each of which denotes a particular distribution of node features. All groups share the same set of roles but possibly with different role mixture rates. Normal groups follow the same pattern with respect to their role mixture rates, but the anomalous group has a role mixture rate that deviates from the normal pattern.

For the static GLAD model, we are interested in the distri-bution of the role mixture rates across the groups. According to our assumption, the mixture rates of normal groups are more likely to appear. For groups with very rare role mix-ture rates, we treat them as group anomalies. One example of this type of group anomaly comes from particle physics. It is widely accepted that the dynamics of known particles are governed by the Standard Model , which corresponds to the normal pattern. Unknown particles would contaminate the distribution of the Standard Model. Detecting those anoma-lies could potentially lead to the discovery of new physical phenomenon. In practice, we first identify the normal mix-ture rates. Then for each learned group, we evaluate the likelihood of its observations being generated with the nor-mal mixture rates. The lower the likelihood value is, the more anomalous the group would be.

For the dynamic d-GLAD model, we emphasize on the temporal aspect of the data and detect the change of the role mixture rate within the groups. For instance, in scientific area, it is valuable to study the evolution of research topics and detect the bursty time periods. In the dynamic setting, since the structure of groups change as well as their role mix-ture rates, detecting groups with rare mixture rate no long applies. Therefore, we think of the task as a change point detection problem and aim to detect the groups whose mix-ture rates change drastically from the previous time stamps. Compared with GLAD, we not only need to decide whether a group is anomaly or not, but also need to specify when the group appears anomalous.

Even though we use slightly different definitions of group anomaly for the GLAD model and the d-GLAD model, the key ideas behind our definitions are the same. Both defini-tions build upon the notion of role mixture rate, which es-sentially requires a precise inference of both the group mem-bership and role identity for each individual in the group. Suppose that we are given a social network with N nodes. We observe node activities X = { X 1 , X 2 , ...,X N } and their communications Y = { Y 1 , 1 , Y 1 , 2 , ...,Y N,N } . X p sists of V entries, denoting a feature vector of V dimensions. Y p,q  X  X  0 , 1 } is a binary valued variable, indicating the pair-wise relationship of nodes. These two forms of data are our inputs. Our goal is to analyze these data jointly and declare the group that has irregular role mixture rate as anomaly. In the following sections, we first describe the motivation for our hierarchical Bayes model and provide its generative pro-cess and the plate notation. Then we derive the inference algorithm using the variational Bayesian approach.
We model a social network with N individuals. Assum-ing that each person p is associated with a group identity G p and a role identity R p . By groups, we mean the clus-ters that capture the similarity suggested by the pair-wise communications. By roles, we refer to the mixture com-ponents that categorize the point-wise feature values of the nodes. For simplification, we fix the number of groups as M and the number of roles as K . Figure 1 shows the plate notation for the GLAD model. The motivation for us to assume two identities for an individual comes from the con-troversial viewpoints of what is the right metric for a com-munity. In community detection literature [7], some argue that a community is the one that has dense communications within clusters while others suggest that people in the same community should share common activity features. We get Figure 1: Plate notation for the Group Latent Anomaly Detection (GLAD) model. Shaded circles are observations, blank circles are latent variables and the variables without a circle are model pa-rameters. The blue rectangular resembles MMSB.
 The red polygon integrates the generating process of LDA. around the controversy by recognizing the arguments of both sides and assume two types of latent structures coexist in the data.

For each person p , he joins a group according to the mem-bership probability distribution  X  p . We impose a Dirichlet prior on the membership distribution. It is well known that the Dirichlet distribution is conjugate to the multinomial distribution. As we will show later, when dealing with la-tent variables, the Dirichlet prior facilitates the learning and inference of the model. We assume the pair-wise link Y p,q between person p and person q depends on the group identi-ties of both p and q with the parameter B . Furthermore, we model the dependency between the group and the role us-ing a multinomial distribution parameterized by a set of role mixture rate {  X  1: M } . The role mixture rate characterizes the constitution of the group: the proportion of the population that plays the same role in the group. Finally, we model the activity feature vector of the individual X p as the depen-dent variable of his role with parameter set {  X  1: K } . Table 1 summarizes the notations used in our model.
 We specify the generative process of the GLAD model in Algorithm 1. Our model unifies the ideas from both the Mixture Membership Stochastic Block (MMSB) model [1] and the Latent Dirichlet Allocation (LDA) model [4]. As shown in Figure 1, the blue dashed rectangular on the left side resembles MMSB which models the formation of groups using link information. The red dashed polygon integrates the generating process of LDA which is often used for topic extraction from documents. By assuming mixture of groups and roles, we allow each person to have multiple roles and multiple group memberships. Without loss of generality, we assume that the activity data have discrete values and choose to model X p with a multinomial distribution. When the activity data are distributed in other forms, we can easily adapt GLAD to model other type of X p .
Inference requires us to compute the posterior distribu-tions of the latent variables given the data. The normalizing term of the posterior distribution involves the calculation of the marginal likelihood of the data for which we resort to variational EM algorithms [10]. Algorithm 1 Generative Process of GLAD for individual p = 1  X  N do end for
Denote the set of model parameters as  X  = {  X ,B, X  1: M , X  the set of observed variables as v = { X 1: N ,Y 1: N } , and the set of the hidden variables as h = {  X  1; N , G 1: N , R 1: N aim is to estimate the posterior distribution p ( h,  X  | v ). We can first write out the complete joint likelihood of observed and latent variables as follows: p ( v,h |  X ) = Y
Computing the maximizer for the marginal likelihood of the data p ( v |  X ) = R h p ( v,h |  X ) dh requires the integration over all the latent variables in the equation above, which is intractable [1]. Therefore, we apply the variational Bayesian approach [10] to perform the inference approximately. The essence of the variational Bayesian approach is to choose a variational distribution q ( h ) to approximate the actual pos-terior distribution, so that the Kullback-Leibler divergence (KL-divergence) between p ( h,  X  | v ) and its approximation q ( h ) is minimized.

Rewriting the marginal log likelihood and plugging in the variational distribution, we have log p ( v |  X ) = D KL ( p || q ) +  X  log p ( v,h |  X )  X  q where we use  X  f  X  p to represent the expectation of the func-tion f with respect to the distribution p . Since the marginal likelihood log p ( v |  X ) is invariant to the choice of q , minimiz-ing the KL-divergence D KL ( p || q ) is equivalent to maximiz-ing the last two terms  X  log p ( v,h |  X )  X  q  X  X  log q ( h )  X  tice, we choose q ( h ) to be factorized over the latent variables with free parameters  X  = {  X  1: N , X  1: N , X  1: N } as follows:
Our goal is to find the optimal set of free parameters that provides a variational distribution closest to the actual pos-terior. Then our problem is to maximize the objective func-tion formulated as follows subject to probability constraints:
The objective function L , by plugging in the joint likeli-hood and the variational distribution and taking expecta-tions, is given by
We follow a variational EM procedure in order to maxi-mize L ( v,h,  X  ,  X ) over  X . Basically we iteratively update the free parameters by taking the derivative of the Lagrange function of the objective L over one parameter at a time given the value of others from the last iteration. Since { Y p,q } is symmetric, the objective function will result in a quadratic term with respect to  X  p . Taking the deriva-tive over the variational parameter would not have a closed form solution. A simple workaround is by assuming con-stant probability for the generation of { Y p,p } . We omit the tedious derivations and only present the final update formulas of each of the free parameters, as shown in Al-gorithm 2. For convenience, we denote f ( Y p,q ,B Y Algorithm 2 Variational Inference for GLAD initialize  X  p,m := 1 /M initialize  X  p,k := 1 /K initialize  X  p,m := 1 /M repeat until convergence
For the parameter estimation, we apply the empirical Bayes method on the variational likelihood. We maximize the Lagrange function of L ( v,h,  X  ,  X ) over model parameters  X  = {  X ,B, X  1: M , X  1: K } . Due to the compounding of Dirich-let distribution, there is no close form solution for the max-imizer w.r.t  X  . We apply the Newton-Raphson method to reach a numerical solution. The resulting parameter updat-ing functions for  X  and B are the same as those of MMSB [1] and the parameters  X  and  X  can be estimated as follows:
With our definition of group anomaly in section 3, we score the group anomalousness using  X  P p  X  G  X  log p ( R p |  X )  X  most anomalous group will have the highest anomaly score. We approximate the true log likelihood with the variational log likelihood to get  X  P p  X  G  X  log p ( R p |  X )  X  q .
A limitation of GLAD is that it only models the static network. This might be restrictive if we want to further con-sider dynamic networks. Besides the anomaly group whose mixture rate deviates significantly from other groups, we are also interested to study how the mixture rate evolves over time. Fortunately, GLAD can be easily extended to account for this dynamics. This leads to the dynamic extension of the GLAD model, which will be discussed in the next sec-tion.
We now generalize the GLAD model to take into account the dynamics in the social media. We refer the dynamic extension of GLAD as the d-GLAD model. To be consistent with our description for GLAD in section 4, we start with the model specification and then provide the model inference algorithm using both the variational Bayesian method and the Monte Carlo sampling technique.
Generalization of GLAD to d-GLAD stems from the tem-plate models [23], which use the model for a particular time stamp as a template, duplicate it over time and connect temporal components sequentially. Similarly, we can adapt GLAD to the dynamic setting by making a copy of GLAD for each time point. To simplify the model, we assume that the latent factors including role R p , group G p and mixture rate {  X  1: M } change over time but the membership distribu-tion {  X  p } and model parameters are fixed.

We model the temporal evolution of the role mixture rate for each group with a series of multivariate Gaussian dis-tributions. At a particular time point, the Gaussian has its mean as the value of the mixture rate. And the mixture rate of the next time point is a normalized sample from this Gaus-sian distribution. Since we require the mixture rate to be the parameters of a multivariate distribution over features, we apply a soft-max function to normalize the sample drawn from the multivariate Gaussian. The soft-max function is defined as S (  X  m ) = exp  X  m P equals one, d-GLAD reduces to the GLAD model. Figure 2 depicts the probabilistic graphical model of d-GLAD and the meanings of notations used. We summarize the generative process of d-GLAD in Algorithm 3.

In d-GLAD model, since the mixture rate of next time stamp is drawn from a multivariate Gaussian centering around the mixture rate of its previous time stamp, it imposes smoothness on the mixture rates across time, preventing the mixture rate from having drastic changes. The soft-max function maps the samples from the multivariate Gaussian to the parameters for the multinomial distribution. Similar idea can be seen from the generalization of LDA to the dy-namic topic model [3]. While it is true that d-GLAD model shares the constraints of GLAD on fixed group/role number and constant self-loop, it has certain intriguing advantages over static models. (i) d-GLAD captures the dynamics of the latent variables G p and R p , thus allows an individual to switch groups and roles over time.(ii) The smoothness of the mixture rate over time models the behavior of normal groups, so detecting groups whose mixture rates  X  t m undergo substantial change becomes easier.
 Algorithm 3 Generative Process of DGLAD for t = 1  X  T do do end for The variational inference of d-GLAD is similar to the GLAD model except for the longitudinal factor  X  (1: T ) 1: M add a variational distribution p (  X  1: T m |  X   X  1: T the original posterior where {  X   X  1: T } are variational param-eters. Then we apply the variational Kalman Filter tech-nique [3] to infer the sequential latent variables and learn the model parameters. The transition for the mixture rate of each group is Gaussian distributed:
We can write the variational distribution for the transition as follows:
Then we can apply similar variational EM procedure in-corporating the transitions to infer the variational parame-ters. Due to the numerical difficulty of variational Kalman filter method, we also implement a version of the Monte Carlo sampling for d-GLAD model, which is used in our empirical evaluations. The algorithm is elaborated in Al-gorithm 4. The inference of the transitional part {  X  1: T based on the Particle Filtering method [22]. The anomaly score of the d-GLAD model is measured by k  X  ( t ) m  X   X  ( t  X  1)
To evaluate the effectiveness of our model, we conduct thorough experiments on synthetic datasets and real world datasets. We study the applications of our approach by analyzing scientific publications and senator voting records.
To our knowledge, all existing algorithms are two-stages approaches: (i) identify groups, (ii) detect group anomalies. Algorithm 4 Monte Carlo Sampling of DGLAD repeat until Convergence We summarize these algorithms in Table 2. We use following approaches as baseline methods in comparison to GLAD and d-GLAD: 1. MMSB-LDA :First use the MMSB model to learn 2. MMSB-MGM : Group is learned using the same method 3. Graph-LDA : Run an off-the-shelf graph clustering 4. Graph-MGM : Get group membership with the graph
We experiment on two type of synthetic datasets. One is a synthetic dataset with injected group anomalies. The other is a benchmark dataset generated by a simulator with individual anomaly labels.
 Xiong 2011-a [19] clustering Mixture Genre Model Xiong 2011-b [20] clustering Flexible Genre Model
Muandet 2013 [13] simulator One class SMM
We generate a network with 500 nodes using GLAD in Al-gorithm 1. To evaluate the anomaly detection performance, we set the mixture rates of anomalous groups as [0 . 9 , 0 . 1] and normal groups as [0 . 1 , 0 . 9]. We vary the number of groups from 5 to 50 and inject 20% anomalous groups. The rest 80% groups are normal. Since we know the normal and anomalous mixture rates, we calculate the anomaly score of each group by directly computing the differences between the inferred mixture rate and the ground truth normal mix-ture rate. During the testing procedure, we rank the groups with respect to their anomaly score and retrieve top 20% groups. For all methods, we set the number of groups and number of roles the same as the ground truth.

We compare the learned groups of three grouping ap-proaches with the ground truth: GLAD, MMSB and Graph, for the case of 5 groups. The inferred group memberships are shown as adjacent matrices in Figure 3. For better visu-alization, we intentionally put the nodes that belong to the same group together. Ideally, we should observe dense links within groups and sparse links between groups. Therefore, the dark pixels in the plot would aggregate along the prin-cipal diagonal of the matrix. We use blue color to highlight the groups learned. The group discovery result of GLAD is the closest to the ground truth. The high connectivity in the graph and the lack of point-wise information could be the reasons for the poor performance of Graph and MMSB.
Figure 4 shows the anomaly detection performance with different number of groups for GLAD and four other base-lines. GLAD achieves the highest detection accuracy. It is also more robust over 10 random runs. Note that the differ-ences for the first stage of baselines are more obvious than the second stage. This is because the Bernoulli distribution limits the number of samples in the pair-wise data, making the first stage more difficult to learn.

We also report the simulation results on group anomaly detection for d-GLAD. The data is generated according to and GLAD for synthetic data. 10% group anomalies are injected. Figure 3: The 50  X  50 adjacent matrix re-arranged by the group membership discovered by three grouping approaches on a subset of synthetic data of 5 groups. Dark pixels denote links and white pixels denote no links. Blue block highlights the learned group mem-bership.
 Algorithm 3 with 5 time stamps. We manipulate the mix-ture rate of 50% of the groups at time point 4 as injected anomalies. Then we raise alarms if the group X  X  mixture rate deviates from the previous time by a certain threshold. In Figure 4(c), we display the false positive rate with different threshold values. For comparison, we train MMSB-MGM and GLAD at each time independently as baselines. It can be seen that d-GLAD achieves the lowest false positive rate, which demonstrates the gain of d-GLAD over static models on the dynamic dataset.
The benchmark data set is generated by a simulator from a federal funded program. It contains email communication records and working activities from 258 company employ-ees. Each employee is featured by 6 types of activities. The labeled dataset contains 39 individual anomalies and 5 of them cannot be detected by any existing algorithms. We set the number of groups as 20 as the optimal setting ob-tained from cross validation and calculate the anomaly score of each group by MCMC sampling. We treat all members in the most anomalous group as individual anomalies and com-pare them with the anomaly labels. Though the anomaly labels are point anomalies rather than group anomalies, the anomaly detection result reflects the potential of our ap-proach to tackle other type of difficult anomaly detection problems. The precision, recall and F1 score over 20 runs on the benchmark dataset is shown in Figure 5.

We can see that the GLAD model achieves comparable precision and recall with low variances. In contrast, the detection performances of the two-stage models fluctuates significantly. In terms of the F1 scores in Figure 5(c), both GLAD and MMSB-MGM beat the other algorithms while GLAD has a lower variance than MMSB-MGM. One possi-ble explanation is that the point-wise features prevent the size of the group to become either too large or too small, thus leading to more robust performance.
Researchers study the topics of papers seeking for con-cise representations of scientific publications, which contain both pair-wise data like co-authorship and point-wise data such as bag of words features. Detecting anomalous topic distributions in scientific publications can sharpen our un-derstanding of the structure of research communities and possibly reveal unusual research trends. In order to quantify our method, we resort to anomaly injection and construct a dataset with group anomaly labels. One way to construct group anomalies is the scenario that a conference paper cor-pus is contaminated by group of papers from conferences in other domains. with 39 true anomalies.

We create a dataset from a pre-processed Digital Bibliog-raphy and Library Project (DBLP) dataset from [6]. The dataset consists of conference papers from 20 conferences of four major area: database (DB), data mining (DM), infor-mation retrieval (IR) and artificial intelligence (AI). Each paper has a bag-of words feature vector with a vocabulary size of 11,771 and associated 28,702 authors information. The detailed statistics of the dataset are shown in the top half of Table 4. We set up the group anomaly detection sce-nario as follows: we randomly sample groups of papers from KDD and treat them as normal groups. Then we sample groups of papers from the other conferences (e.g, CVPR, ICML , SIGMOD) and inject them into KDD papers as group anomalies. If the two papers have at least one com-mon author, we add a link between them.

Accordingly, all conferences share four topics. But dif-ferent conferences might have difference point of emphasis, resulting in different mixture rates of topics. Our goal is to pick out the  X  X nomalous X  papers from the corpus. We sam-ple 50 groups of papers and inject 20% group anomalies. We apply different models with 50 groups and 4 roles to the data for inference of the membership and role distributions. Then we rank 50 groups with respect to their anomaly scores. We treat the top 20% groups as the detected anomalies. Table 3 shows the anomaly detection accuracy by GLAD and four other baselines. GLAD is superior to all four baselines mod-els for different combination of normal/abnormal settings. We also display the topics learned by the GLAD model. In Table 5, we show the top ten most representative words for the four topics, which well reproduce the topic results re-ported in [6].

Since the DBLP dataset does not contain time-specific in-formation which is not suitable for the d-GLAD model, we process another ACM dataset downloaded from ArnetMiner Table 4: Key statistics of the DBLP and ACM pub-lication datasets [16]. The dataset contains the publications from year 2000 to 2009 by 4,474 authors, mainly from the data mining com-munity. In order to study the topic evolution for academic scholars, we extract the abstracts of all publications and group them by authors and publishing years. For each au-thor, we construct a bag of words feature vector out of all the papers he/she has written in one year. And the communica-tion networks we generate are based on the co-authorship of the papers. Whenever two authors have collaborations in a certain year, we create a link between them for the network snapshot in that year.

Due to the lack of labels, it is difficult to directly evalu-ate our model on anomaly detection task. As an alterna-tive, we design a prediction task to compare the modeling performance of GLAD and d-GLAD on ACM publications. Specifically, we separate the papers into training and test-ing sets and measure the predictive model log-likelihood on the testing data. For d-GLAD, we train our model using a series of publications from previous years, and test on the year immediately after. For the GLAD model, as it is a static model, time independence assumption applies. We train the model using previous year and test on the next year. The model fitting results are shown in Table 6. Out Year 2001 2002 2003 2004 2005 2006 2007 2008 2009 Table 5: The most representative words learned by GLAD on DBLP dataset of four topics: database, data mining, information retrieval and artificial in-telligence.
 databases data web query of 9 training-testing experiments, d-GLAD model achieves higher log-likelihood than GLAD model for 6 times, indi-cating d-GLAD as a better fit for the evolving publication modeling.
We collect the voting records from the government website of United States 109th Congress 1 using the New York Time Congress API 2 . The records of 109th Congress contain 100 senators X  voting spanning two sessions from Jan 1st 2005 to Dec 31st 2006. We divide the 24 months records into 8 time slots, where each slot denotes a 3-month interval. Then we apply the method of [11] to construct a network from original yay/nay votes. For the nodes features, we collect the statistics of votes in six dimensions, namely House Joint Resolution(hjres), House of Representatives(hr), Presiden-tial Nomination(pn), Simple Resolution(s), sconres(Senate Concurrent Resolution) and Senate Joint Resolution(sjres). We evaluate GLAD on single aggregated network and d-GLAD on the 8 time slots time-varying data.

We set the number of groups as 2 and number of roles as 3 as the Senate consists of two major parties and maintains three types of committees. Figure 6 shows the groups in-ferred by GLAD. The blue nodes denote Democratic party members and the red ones are Republican. Compared with known facts, the model correctly reveals the party affiliation except for two outliers: Ben Nelson (Democratic) and James Jeffords (Independent). The underlying reason is that the votes of these two senators are often at odds with the lead-ership of his party, leading to false grouping. We conduct an anecdotal investigation and find that the congressional vote rating from the National Journal placed Ben Nelson to the right of five Senate Republicans in 2006. For James Jeffords, he served as a Republican until 2001, when he left the party to become an Independent and began caucusing with the Democrats. http://www.senate.gov/ http://developer.nytimes.com/docs/read/congress api Figure 6: Common votes graph with party labels inferred by GLAD for 100 senators on the aggre-gated network. Compared with ground truth, two outliers are highlighted due to their anomalous vot-ing behavior.

Since there are merely two groups, it is impetuous to say one party is more anomalous than the other. Instead, we use d-GLAD to detect time points when the role mixture rates change dramatically. In fact, d-GLAD raises an alarm at the 7th time-step for Democratic. A well known political event happened during this time is that Democratic sena-tor Joseph Lieberman lost the Democratic Party primary election and became a independent Democratic in Septem-ber 2006. Though it may be over-optimistic to draw the conclusion that this event causes the sudden change of role mixture rates, it serves as an evidence that the dynamics of the voting behavior is closely related to the party affiliation of members.
In this paper, we study the problem of group anomaly detection in social media analysis. We identify the group membership and the role for each individual and define the group anomaly with respect to the role mixture rate . We develop a hierarchical Bayes model: the GLAD model, for detecting the group anomaly. The GLAD model utilizes both the pair-wise and point-wise data and automatically infers the group membership and the role at the same time. To further account for the dynamic nature of social media, we generalize GLAD to the d-GLAD model as an extension for handling time series. We derive the variational Bayesian method as well as the Monte Carlo sampling for the model inference.

The superior performances of GLAD and d-GLAD are demonstrated on synthetic datasets and real world social media datasets. For example, GLAD successfully detects  X  X nomalous X  papers from the scientific publication corpus with injected anomalies. d-GLAD uncovers the party affil-iation changes in the Senate which correspond to political events. The type of group anomaly that we define here has also been investigated in the applications of astronomy [19], particle physics [13] and fluid mechanics [20]. But our work is the first study in group anomaly detection that aims to identify groups and detect anomalies simultaneously. Dur-ing this study, we mainly focus on the applications in social media analysis, but we believe that our model is capable of tackling similar problems in other domains.

Our current approach has a number of limitations. First, the lack of labeled data for group anomaly increases the dif-ficulty in evaluation. Here we create a benchmark dataset using anomaly injection. But the injected anomaly cannot fully represent the naturally occurred anomaly in practice. Secondly, we do not fully address the problem of model selec-tion for our approach. For the experiments on senator and publication datasets, we either manually choose the model hyper-parameters based on our prior knowledge or use cross-validation. Finally, given the complexity of the model and the particle filtering approach, the d-GLAD model is com-putational expensive. Thus it is not yet scalable to data with long time span.

For future directions, we will apply the variational Bayesian method for d-GLAD or parallel sampling procedure to achieve further speed-up for large-scale data. Furthermore, it would be interesting to build non-parametric models to automati-cally learn the number of groups and roles. Future research should also address better procedures and criteria for eval-uating the group anomaly detection methods in real world social media applications. The research was sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under the Anomaly Detection at Multiple Scales (ADAMS) program, Agreement Number W911NF-11-C-0200 and NSF research grants IIS-1134990. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government. [1] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. [2] L. Akoglu, M. McGlohon, and C. Faloutsos. Anomaly [3] D. M. Blei and J. D. Lafferty. Dynamic topic models. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] S. P. Borgatti, A. Mehra, D. J. Brass, and [6] H. Deng, J. Han, B. Zhao, Y. Yu, and C. X. Lin. [7] S. Fortunato. Community detection in graphs. June [8] D. M. Hawkins. Identification of outliers . Chapman [9] N. A. Heard, D. J. Weston, K. Platanioti, and D. J. [10] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [11] M. Kolar, L. Song, A. Ahmed, E. P. Xing, et al. [12] G. Kossinets. Empirical analysis of an evolving social [13] K. Muandet and B. Sch  X  A  X ulkopf. One-class support [14] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [15] C. E. Priebe, J. M. Conroy, D. J. Marchette, and [16] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. [17] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random [18] U. Von Luxburg. A tutorial on spectral clustering. [19] L. Xiong, B. Poczos, J. Schneider, A. Connolly, and [20] L. Xiong, B. P  X oczos, and J. G. Schneider. Group [21] V. Chandola, A. Banerjee, and V. Kumar. Anomaly [22] A. Doucet and A. M. Johansen. A tutorial on particle principles and techniques. MIT press, 2009.
