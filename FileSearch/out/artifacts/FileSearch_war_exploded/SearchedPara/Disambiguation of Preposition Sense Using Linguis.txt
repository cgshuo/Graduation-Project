 Classifying instances of polysemous words into their proper sense classes (aka sense disambigua-tion) is potentially useful to any NLP application that needs to extract information from text or build a semantic representation of the textual information. However, to date, disambiguation between preposi-tion senses has not been an object of great study. In-stead, most word sense disambiguation work has fo-cused upon classifying noun and verb instances into their appropriate WordNet (Fellbaum, 1998) senses. Prepositions have mostly been studied in the con-text of verb complements (Litkowski and Hargraves, 2007). Like instances of other word classes, many prepositions are ambiguous, carrying different se-mantic meanings (including notions of instrumental, accompaniment, location, etc.) as in  X  X e ran with determination X ,  X  X e ran with a broken leg X , or  X  X e ran with Jane X . As NLP systems take more and more semantic content into account, disambiguating be-tween preposition senses becomes increasingly im-portant for text processing tasks.

In order to disambiguate different senses, most systems to date use a fixed window size to derive classification features. These may or may not be syntactically related to the preposition in question, resulting X  X n the worst case X  X n an arbitrary bag of words. In our approach, we make use of the phrase structure to extract words that have a certain syn-tactic relation with the preposition. From the words collected that way, we derive higher level features.
In 2007, the SemEval workshop presented par-ticipants with a formal preposition sense dis-ambiguation task to encourage the development of systems for the disambiguation of preposition senses (Litkowski and Hargraves, 2007). The train-ing and test data sets used for SemEval have been re-leased to the general public, and we used these data to train and test our system. The SemEval work-shop data consists of instances of 34 prepositions in natural text that have been tagged with the ap-propriate sense from the list of the common Eng-lish preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the per-formance of preposition sense disambiguation sys-tems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our re-sults to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the  X  X irst sense X  and the  X  X ost frequent sense X  baselines (see section 3 and table 1). These baselines are determined by the TPP listing and the frequency in the training data, respectively. Our system beat the baselines and outperformed the three participat-ing systems. 2.1 Data Preparation We downloaded the test and training data provided by the SemEval-2007 website for the preposition sense disambiguation task. These are 34 separate XML files X  X ne for each preposition X , comprising 16557 training and 8096 test example sentences, each sentence containing one example of the respec-tive preposition.
 The preposition is annotated by a head tag, and the meaning of the preposition in question is given as defined by TPP.

Each preposition had between 2 and 25 different senses (on average 9.76). For the case of  X  X bout X  these would be 1. on the subject of; concerning 2. so as to affect 3. used to indicate movement within a particular 4. around 5. used to express location in a particular place 6. used to describe a quality apparent in a person
We parsed the sentences using the Charniak parser (Charniak, 2000). Note that the Charniak parser X  X ven though among the best availbale Eng-lish parsers X  X ccasionally fails to parse a sentence correctly. This might result in an erroneous extrac-tion, such as an incorrect or no word. However, these cases are fairly rare, and we did not manually correct this, but rather relied on the size of the data to compensate for such an error.

After this preprocessing step, we were able to ex-tract the features. 2.2 Feature Extraction Following O X  X ara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on both sides of the preposition. We thus focused on specific words that are syntactically related to the preposition via the phrase structure. This has the advantage that it is not limited to a certain window size; phrases might stretch over dozens of words, so the extracted word may occur far away from the actual preposition. These words were chosen based on a manual analysis of training data. Using Tregex (Levy and Andrew, 2006), a utility for expressing  X  X egular expressions over trees X , we created a set of rules to extract the words in question. Each rule matched words that exhibited a specific relationship with the preposition or were within a two word window to cover collocations. An example rule is given below.
 This particular rule finds the head (denoted by x ) of a verb phrase that governs the prepositional phrase containing the preposition, unless x is an auxiliary verb. Tregex rules were used to identify the follow-ing words for feature generation:  X  the head verb/noun that immediately dominates  X  the head verb/noun immediately dominated by  X  the subject, negator, and object(s) of the imme- X  neighboring prepositional phrases dominated  X  words within 2 positions to the left or right of
For each word extracted using these rules, we col-lected the following items:  X  the word itself  X  lemma  X  part-of-speech (both exact and conflated, e.g.  X  all synonyms of the first WordNet sense  X  all hypernyms of the first WordNet sense  X  boolean indicator for capitalization
Each feature is a combination of the extraction rule and the extracted item. The values the feature can take on are binary: present or absent. For some prepositions, this resulted in several thousand fea-tures. In order to reduce computation time, we used the following steps: For each preposition classifier, we ranked the features using information gain (For-man, 2003). From the resulting lists,we included at most 4000 features. Thus not all classifiers used the same features. 2.3 Classifier Training We chose maximum entropy (Berger et al., 1996) as our primary classifier, since it had been successfully applied by the highest performing systems in both the SemEval-2007 preposition sense disambiguation task (Ye and Baldwin, 2007) and the general word sense disambiguation task (Tratz et al., 2007). We used the implementation provided by the Mallet ma-chine learning toolkit (McCallum, 2002). For the sake of comparison, we also built several other clas-sifiers, including multinomial na  X   X ve Bayes, SVMs, kNN, and decision trees (J48) using the WEKA toolkit (Witten, 1999). We chose the radial basis function (RBF) kernel for the SVMs and left all other parameters at their default values. We measured the accuracy of the classifiers over the test set provided by SemEval-2007 and provided these results in Table 1. It is notable that our system produced good results with all classifiers: For three of the classifiers, the accuracy is higher than MELB, the winning system of the task. As expected, the highest accuracy was achieved using the maximum entropy classifier. Overall, our system outperformed the winning system by 0.058, an 8 percent improve-ment. A simple proportion test shows this to be sta-tistically significant at 0.001. !"##$% !"#$%&amp; *++ ,-,./0 !67)89:;)*#&lt;=#&gt;? @0/)A#BCDCE=)$&lt;##D 7F&gt;$C=EGC'&gt;)+' HI#):'J#D 7'KCGFG)#=$&lt;ELJ 7MN:)8O#)'=A):'&gt;APC=Q)2,,3? RS)8OF&lt;#$Q)2,,3?
T9!U)8&amp;EL#DBF)#$)'&gt;-Q)2,,3? 7ED$)V&lt;#WF#=$)D#=D#
Since our initial cutoff of 4000 features was ar-bitrary, we reran our Maximum Entropy experiment multiple times with different cutoffs. Accuracy con-sistently increased as the feature limit was relaxed, resulting in 0.764 accuracy at the 10k feature limit. These results are displayed in Figure 1.
For words extracted using these rules, we col -lected the following features: 
This resulted in several thousand features for the prepositions. We used information gain (Foreman, 2003) in order to find the highest ranking features of each class and limited our classifiers to the top 4000 features in order to reduce computation time. 2.3 Classifier Training
We chose maximum entropy (Berger, 1996) as our primary classifier because the highest performing systems in both the SemEval-2007 preposition sense disambiguation task (Ye and Baldwin, 2007) and the general word sense disambiguation task (Tratz et al., 2007) used it. We used the implemen -tation provided by the Mallet machine learning toolkit (McCallum, 2002). Then, for the sake of comparison, we also built several other classifiers including multinomial na X ve Bayes, SVMs, kNN, and decision trees (J48) using the WEKA toolkit (Witten, 1999). We chose the radial basis function (RBF) kernel for the SVMs and left all other pa -rameters at their default values.
We measured the accuracy of the classifiers over the test set provided by SemEval-2007 and pro -vided these results in Table 1. It is notable that our system produced good results with all classifiers: 
For three of the classifiers, the accuracy is higher than MELB, the winning system of the task. As expected, the highest accuracy was achieved using the maximum entropy classifier.

Overall, our system outperformed the winning system by 0.058, an 8 percent improvement. A simple proportion test shows this to be statistically significant at 0.001.

Since our initial cutoff of 4000 features was arbi -trary, we reran our Maximum Entropy experiment multiple times with different cutoffs. Accuracy consistently increased as the feature limit was re -laxed, resulting in 0.764 accuracy at the 10k fea -ture limit. These results are displayed in Figure 1.
The linguistic literature on prepositions and their use is copious and diverse. We restrict ourselves to the works that deal with preposition sense disam -biguation in computational linguistics.
 O'Hara and Wiebe (2003) make use of Penn 
Treebank (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features from the con -text, such as semantic roles, significantly aids dis -The linguistic literature on prepositions and their use is copious and diverse. We restrict ourselves to the systems that competed in the SemEval 2007 Prepo-sition Sense Disambiguation task. All three of the systems within the framework of the SemEval task used supervised learning algorithms, yet they dif-fered widely in the data collection and model prepa-ration. Ye and Baldwin (2007) participated in the Sem-Eval task using a maximum entropy classifier and achieved the highest accuracy of the participating systems. The features they extracted were similar to the ones we used, including POS and WordNet features, but they used a substantially larger word window, taking seven words from each side of the preposition. While they included many higher level features, they state that the direct lexical context (i.e., bag-of-words) features were the most effective and account for the majority of features, while syn-tactic and semantic features had relatively little im-pact.

Yuret (2007) used a n-gram model based on word substitution by synonyms or antonyms. While this proved to be quite successful with content words, it had considerable problems with prepositions, since the number of synonyms and/or antonyms is fairly limited.

Popescu et al. (2007) take an interesting approach which they call Chain Clarifying Relationship. They are using a supervised algorithm to learn a regu-lar language. They used the Charniak parser and FrameNet information on the head, yet the features they extract are generally not linguistically moti-vated. Using the phrase structure allows for more freedom in the choice of words for feature selection, yet still guarantees to find words for which some syntactic relation with the preposition holds. Extracting se-mantic features from these words (hypernyms, syn-onyms, etc.) allows for a certain degree of abstrac-tion, and thus a high level comparison. O X  X ara and Wiebe (2003) also make use of high level features, in their case the Penn Treebank (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepo-sitions. They show that using high level features X  such as semantic roles X  X f words in the context sub-stantially aids disambiguation efforts. They cau-tion, however, that indiscriminately using colloca-tions and neighboring words may yield high accu-racy, but has the risk of overfitting. In order to mit-igate this, they classify the features by their part of speech. While we made use of collocation features, we also took into account higher order aspects of the context, such as the governing phrase, part of speech type, and semantic class according to WordNet. All other things being equal, this seems to increase per-formance substantially.

As for the classifiers used, our results seem to confirm that Maximum Entropy classifiers are very well suited for disambiguation tasks. Other than na  X   X ve Bayes, they do not presuppose a conditional independence between the features, which clearly not always holds (quite contrary, the underlying syn-tactic structure creates strong interdependencies be-tween words and features). This, however, does not satisfactory explain the ranking of the other classi-fiers. One possible explanation could be the sensi-tivity of for example decision trees to random noise. Though we made use of information gain before classification, there still seems to be a certain ten-dency to split on features that are not optimal. We showed that using a number of simple linguis-tically motivated features can improve the accu-racy of preposition sense disambiguation. Utilizing widely used and freely available standard tools for language processing and a set of simple rules, we were able to extract these features easily and with very limited preprocessing. Instead of taking a  X  X ag of words X  approach that focuses primarily upon the words within a fixed window size, we focused on el-ements that are related via the phrase structure. We also included semantic information gathered from WordNet about the extracted words. We compared five different classifiers and demonstrated that they all perform very well, using our selected feature set. Several of them even outperformed the top system at SemEval. Our best result was obtained using a maximum entropy classifier, just as the best partici-pating system, leading us to believe that our primary advantage was our feature set. While the contribu-tion of the direct context (+/-7 words) might have a stronger effect than higher level features (Ye and Baldwin, 2007), we conclude from our findings that higher level features do make an important contribu-tion. These results are very encouraging on several levels, and demonstrate the close interaction of syn-tax and semantics. Leveraging these types of fea-tures effectively is a promising prospect for future machine learning research in preposition sense dis-ambiguation.
 The authors would like to thank Eduard Hovy and Gully Burns for invaluable comments and helpful discussions.

