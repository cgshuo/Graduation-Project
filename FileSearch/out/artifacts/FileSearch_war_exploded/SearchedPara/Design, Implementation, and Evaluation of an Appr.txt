 Previous research has motivated the idea of automatically determining when programmers are having difficulty, provided an initial algorithm (unimplemented in an actual system), and performed a small student-based evaluation to justify the viability of this concept. We have taken the next step in this line of research by designing and devel oping two-different systems that incorporate variations of the algorithm, implementing a tool that allows independent observers to code recorded sessions, and performing studies involving both student and industrial programmers. Our work shows that (a) it is possible to develop an efficient and reusable arch itecture for predicting programmer status, (b) the previous technique can be improved through aggregation of predicted stat us, (c) the improved technique correlates more with programmers X  pe rception of whether they are stuck than that of observers manually watching the programmers, (d) the observers are quicker than the developers to conclude that programmers are stuck, (e) with a ppropriate training, the tool can be used to predict even the observers X  perceptions, and (f) a group training model offers more accura cy than an individual one when the training and test exercises are the same and carried over a small time frame. H.5.3 Group and Organization Int erfaces: Computer-supported cooperative work. Human Factors Machine learning, data mining, architecture, software development, semantic awareness Often programmers get  X  X tuck X  wh ile coding, unable to make much progress despite all efforts to address some issue. It would environment extended to show a user X  X  Google Talk X  buddy list. be useful if an interested remote party could become aware of this situation through, for instance, a notification and/or status change in a buddy list. This idea extends the notion of continuous coordination [1] to continuous help, and provides a new kind of contextualized information in co llaborative software development [2]. An educational setting provides particularly compelling applications of this idea because an important goal is to help students and monitor their progress. In fact, based on the results of several previous studies, mentioned later, the true benefits of this idea could actually occur in industry. One way to support this idea is to allow programmers to manually change a status field displayed to potential helpers. However, there are several apparent problems with this approach. First, studies show students and new programmers are late to use help [3], and programmers often exhaus t other forms of help before contacting a teammate [4]. Even those who are willing to manually change their status are likely to not set it back, just as people forget to change their busy status in an IM tool or turn off the  X  X all steward X  light in a plane. Another approach is to allow a pair of developers to monitor the progress of each other, using local [5] or distributed [6] side-by-side programming. However, this approach does not scale to beyond a pair of programmers, requires continuous monitoring of the partner X  X  display, and is not guaranteed to succeed as an observer may not know if the actor is, in fact, stuck. Therefore, a superior approach is to develop a mechanism that automatically determines if a programmer is stuck (Figure 2) by mining logs of their interaction with the programming environment. Such an approach is bound to be iterative, consisting of the following steps: 1. Develop an initial na X ve algorithm for predicting the 2. Implement the algorithm in one or more programming 3. Ask selected developers in la b and/or field experiments to 4. Analyze the logs to refine the set of features. 5. Input these features to existing selected log-mining 6. If none of these algorithms makes a significant improvement, 7. Make the algorithm that gives the best results the current 8. Go to 2. Our previous work[7] carried out the first iteration of the process, and evaluated the resu lting algorithm (not implemented in any programming environment) in a study involving six student programmers, whose logs were used both in the training and evaluation phases. It leaves, howev er, several important questions unanswered. 1. Is it possible to develop a common set of extensible 2. Is it possible for the modules to have no impact on the 3. How well does the previous algorithm work when it is used 4. Is it better to train the modules using logs of the individual 5. What is the correlation between the perceptions of the 6. If these perceptions differ, how well can the predictions In the rest of the paper, we addr ess these questions . In Section 2, we survey related work providing the inspiration for and techniques used in the paper. In S ection 3, we describe the results of a small field study involvin g a na X ve implementation of the previous algorithm, and adapta tions to its semantics and implementation to overcome some of the problems exposed by this effort. In Section 4, we de scribe a lab study involving nine student and five industrial progr ammers, and a coding study in which two coders and the first aut hor classified recordings made during the lab study using a special tool we built for this work. In Section 5, we describe the result s of the study using an existing model for determining if programmers are stuck. In Section 6, we consider what happens when group and individual data from the lab and coding study are used train the tool. In Section 7, we consider privacy issues raised by this work, and present preliminary solutions to them. In Section 8, we discuss our findings and provide conclusions and directions for future work. The motivation for encouraging programmers to help each other is provided by a variety of previous research efforts, which have explored various degrees of couplings among developers: distributed, co-located, radi cally co-located, and pair programming. Herbsleb et al.[8] found that the productivity of distributed teams was lower than that of co-located teams. A more recent study by Cataldo [9] has similar conclusions based on software quality rather than productivity. It found that the number of errors in a project was positively correlated with the number of locations involved in the project. Teasley et al. [10] studied a higher degree of physical c oupling, called radical co -location, in which all team members work in a single war-room or bull-pen. They found that the productivity of radically co-located teams was higher than that of co-located on es. In radical co-location, even though the members of the team work in one room, they (can) use different workstations. Higher ph ysical coupling is achieved in pair programming, wherein two programmers sit next to each other, sharing a workstation, and working on a single task, with one programmer, called the driver, providing input, and the other programmer, called the navigator, offering advice. Some studies of pair programming have found that it offers faster task completion times, and more importa ntly, after taking into account the cost of fixing bugs, much better productivity [11, 12]. The reason higher coupling offers more productivity may lie in how much developers help eac h other. Pair programming is centered on the idea of the two programmers helping each other with every aspect of the task. Williams and Cockburn report that  X  X airs often find that seemingl y  X  X mpossible X  problems become easy or even quick, or at least possible, to solve when they work together [11]. X  Teasley et al. [10] found that in a war-room, if someone was having difficulty with some aspect of code, another developer in the war-room  X  X alking by and seeing the activity over their shoulders, would stop to provide help. X  The study by Herbsleb et al. [8] also showed the importance of a helpful software development. It fo und that in distributed team development, several forms of communication were more difficult: it was harder to find people, get work-related information through casual c onversation, get access to information shared with co-located co-workers, get timely information about plan changes, have clearly formed plans, agree about plans, be clear about assi gned tasks, and have co-workers provide help (beyond the call of duty). The study found that the perception of received help was the only factor that correlated with productivity. A related stud y by Hebsleb and Grinter [13] found that developers are less co mfortable asking remote rather than co-located software develope rs for help. A study by Cataldo [9] found that the number of e rrors correlated with uneven distribution of engineers across locat ions, which, together with the other studies, seems to suggest th at the team would benefit if a location with more engineers (which is likely to have more expertise, and perhaps, more t ime) helped the one with fewer engineers. Together, these studies seem to c onclude that (1) developers often hesitate to explicitly ask for help, even when they could use it, and (2) the greater the distance between them and potential helpers, the more their hesitation, and the more difficult it is for the latter to determine if the former need help. One approach to address the second problem, described in [14], makes distributed team members aware of each other X  X  interactions with the programmi ng environment. For example, [14] gives a scenario in which Bob, on seeing Alice stuck on debugging a particular class, de duces she could use help, and offers it. This distributed scen ario directly mimics the war-room scenario quoted above. Providing virtual channels that gi ve distributed users the feeling of  X  X eing there X  in a single location is an important goal of CSCW. However, Hollan and St ornetta have argued that if CSCW is to be truly successful , it should go  X  X eyond being there X  by providing capabilities not available in face-to-face interaction [15]. One approach to support this goal is to automatically infer when people are frustrated using came ras, posture seating chairs, pressure mouse, and wireless Bluetooth skin conductance test as sensors to collect data [16]. A pr oblem with this approach is the overhead including time and cost of using this extra equipment. An alternative approach is to determine this information by logging developers X  interaction w ith the system. An important step in this direction is made in [17], which describes a logging-based tool for monitoring student progress. Student teams use a wiki to interact with several tools including CVS, newsgroups, and a metrics module that analyzes students X  data. The wiki allows students to track their de velopment tasks, and analyzes tasks such as file modifications to measure the workload of teams. A problem with this approach is th at the rate of student progress is determined after the fact, when a project is checked-in, rather than incrementally, when the student could use help. This limitation can be addressed by logging inte ractions with the programming environment. The authors of [17] said they did not take this alternative because  X  X any students have a preferred programming environment and establishi ng a common one would be a challenge. X  It is possible to overcome this problem by creating such a logger for as many mainstream progra mming environments as possible. Before this step can be taken, it is important to determine if such an approach is feasible. There is reason to believe it can work. Previous work by Begole et. al. [18] logged email inter action, calendar appointments, and the locations of users to show that there are rhythms or patterns in work by Fogarty et al [19]. Deve lopers are randomly interrupted by a notification and their interactions with the programming environment are logged. Interruptib ility is measured from the time acknowledged. The specific acti ons developers perform right before they were interrupted are used to determine if these actions correlate with being interruptible. These approaches represent ge neral methods to mining data, which consists of two main step s: (a) an algorithm for deducing semantic awareness (out of office, interruptible) and (b) a scheme for training the system and evaluating the automatic scheme. Our previous work [7] applied this general approach to the problem of determining progress. This work extended an Eclipse plug-in [24] to log developers X  programmi ng actions and allowed the developers to indicate their stat us: stuck (which is considered, here, synonymous with having di fficulty) and making progress. Based on the event and status logs of six student programmers, it developed the following approach for automatically inferring the status. It categorized user input into five categories: navigation, edit (text insertion/deletion), re move (methods and/or classes), debug, and the programming envir onment losing/gaining focus. The logs were segmented into sections based on the number of events. Every 50 actions, the tool calculated the ratio of occurrences of each category of actions in that segment to the total number of actions in the se gment as percentage, and used these percentages as features over which patterns were identified. This event aggregation technique was used to predict developers X  status. The intuition behind the t echnique was that when the ratio of edit events to total number of events decreases, programmers are stuck. The approach correctly identified 90% of the time when the students were having difficul ty. This result is promising because it recognizes with high accuracy when student programmers are having difficulty even though having difficulty is a rare event. As mentioned in Section 1, this approach left several questions unanswe red, which are the focus of this paper. To determine how well the techni que developed in our previous approach [7] works in practice, we took two additional implementation and evaluation step s. (1) We incorporated the algorithm in both the Eclipse and Visual Studio programming environments. (2) Some members of our research group, and one industrial software developer, us ed the Eclipse and Visual Studio implementations for their daily work. We gained important lessons from these steps. The industrial developer complained about frequent false positives while building a new product  X  a workflow system. In relatively high number of false positives because of the navigations performed to build the working set of files. He also needed more time to determine if the predicted change of status was correct, and, thus, often wa s not sure about his status. The second author identified two a dditional problems. The cost of processing incremental input events was noticeable, and sometimes intolerable, on his 3-ye ar old laptop. Moreover, even when the tool accurately predicted he was having difficulty, seeing the status message hurt his ego, as he felt that the change in progress was caused by the difficulty of the problem rather than lack of appropriate skills! A fi nal problem had to do with the implementation architecture: the Visual Studio and Eclipse implementations performed the sa me functions, but did not share code. Therefore, when a change was made to the code in the Eclipse implementation, the code in Visual Studio had to also change. Put in another way, ther e would need to be a different implementation of the tool per programming environment, which increases programming time and effort. We took several steps to address these problems. To address the  X  X urt ego X  issue, we changed the status message from  X  X aving Difficulty X  to  X  X low Progress. X  In addition, we allowed developers to customize the message so that the second author could, for instance, report it as  X  X omplex Programming. X  To address the false positives faced by the industrial programmer, we developed a label aggregati on technique that complemented the event aggregation technique. As before, we computed the status every 50 events. However, we notified the developer every 250 events  X  the value reported was the dominant status in the last five segments. Together, the two aggregation te chniques take into account the fact that the status of a developer does not change instantaneously. In addition, we added an  X  X ndeterm inate X  status value to capture the fact that developers need time to decide if they are stuck. At startup, before 250 events were input, the tool reported the indeterminate value. We also allowed the developer to correct a predicted status to indeterminate. Indeterminate 2 0 100% Table 1 shows that the changes resulted in a high accuracy for the industrial developer. However, the table shows that th e aggregation scheme results in a large number of false negatives. In particular, it missed 7 of the 22 cases when the developer was having difficulty. To develop a more accurate scheme, we gathered more data points through a user study. Before this step can be taken, it was important to address the performance and implementation overhead of the Eclipse and Visual Studio implementations. A re usable architecture is crucial for this research because of its iterative nature. We were able to apply certain standard design patt erns and existing libraries to address the reuse issue. To ad dress the performance issue, we offloaded event processing to a separate process that worked asynchronously from th e programming environment.
 Figure 3 shows the architecture. Na turally, a separate module is needed per programming environment to intercept its events. In addition, a separate module is needed per programming environment to display the current status, which is done by using a Google talk plug-in. Thus, in our implementation we use two different event-interception and status-display modules  X  one pair for Eclipse, and one for Visual Studio. An event-interception module asynchronously forwards the events to a separate process, which make s the predictions. As the process was written in C#, serialized events could be sent directly from Visual Studio to this process. Java events, on the other hand, require conversion, and we were able to use standard (WOX and IKVM) libraries to do so. Consider now the modules in the predicting process. Events are received by the  X  X ommunication director X  of the system, the mediator, which mediates between a pipeline of other modules. The mediator gives the received event to the first module in the pipeline. In addition, it receives output from each of these modules and feeds it as input the next module, if such a module exists. The first module to receive inpu t from the mediator is the event aggregator module. This module aggregates 50 events and passes these events to the mediator. The mediator passes these events to the feature extractor module, which computes the ratios that are used to predict a status. The feature extractor passes the ratios to the mediator, and the mediator gives these ratios to the prediction manager. The prediction manager includes the decision tree algorithm (used in [7]), which us es previous data and the ratios to predict a status. This status is passed to the status aggregator, which aggregates each status and gives a final prediction to the mediator. The mediator delivers this status to the status displayer of the appropriate programming environment. The benefit of using the mediator pattern is that it allows modules to be loosely coupled so that any change in the flow of communication would not require a change to a module. For example, if the status manager ha d to be omitted, the mediator would have to change. However, the other modules in the system would stay the same. The iterative nature of this research requires the ability to easily change also the behavior of each of the individual modules in this pipeline. We used the standard Strategy pattern to achieve this goal. We give below specific uses for it in our context by considering each of the phases in the pipeline, and showing that multiple algorithms could be used in each phase. 1. Event aggregator: There are at least two algorithms that can //ea.setEventAggregationStrategy( new SlidingWindow ()); 2. Feature extractor: We currently extract features based on the 3. Prediction manager: We currently use two machine learning 4. Status manager: There are at least two ways to aggregate Our experience with the new arch itecture showed that (a) as expected, when multiple strategy objects were implemented for a the asynchronous processing did not result in perceptible delays in user-response times. We were now ready to do a controlled user study to evaluate the adapted algorithm and investigate additional adaptations based on this study. In a controlled user study, the problems must be chosen carefully. Our previous work [7] found that having difficulty is a rare event. Thus, we must try and ensure that developers face difficulty in the small amount of time available (1-4 hours) for a lab study, and yet do not find the problems impossible. We used problems from the Mid-Atlantic ACM programming competition. These problems are attractive because they have varying difficulty. We piloted several problems to find problems that were difficult but not impossible to solve by the subjects. Based on these pilots, we settle d on the problems shown in Table 2. The table characterizes the difficulty of each problem by showing the number of teams that solved the problem, the total number of teams, and the fraction of teams that solved the problem. The difficulty level of each problem was determined by the number of teams that solved the problem. For example, 100% of teams that attempted the Simple Question of Chemistry problem solved it, while only 16% of teams that attempted the Balanced Budget Initiative Problem solved it. Five industrial and nine student programmers participated in the study. Participants were instructed to correct an incorrect prediction by the system using st atus-correction buttons (Figure 5). By measuring how often the developers corrected their status, we could, as in [7], measure the accuracy of our approach with respect to the perceptions of the developers. However, there is a question as to whether participants would accurately report their status, given the hurt ego problem faced by the second author. Moreover, it is useful to compare the tool X  X  predictions about a developer X  X  st atus with that of a third party manually observing the developer. Therefore, the first author and two independent coders obse rved participants' programming activities and made an independent determination of their status. To allow coders to independently and asynchronously observe participants' programming activities, we used Microsoft Live Meeting X  to record the participants' screens. Live Meeting X  also allowed the first author to observe remote sessions. In fact, Tang et al. [20] argued that screen recording is an effective and 
Year Problem Title 2004 Balanced Budget 2002 unobtrusive technique when subjec ts do not feel it invades their privacy. We obtained participants' consent to record their screens. We recorded 40 hours and 44 minutes of video. To relieve coders from watching hours of video, we created a video observation tool, shown in Figure 4. This vide o tool shows all segments where the participant, first author (w hile observing the experiments and later when randomly sampling the video), or system indicated the (indeterminate). As it turned out, in our study, there was one indeterminate segment (indicated by a participant). We shall refer to these segments as  X  X tuck X  segments. As there were few such segments, we asked the coders to classify each of these segments. It was not reasonable, however, to ask them to classify all of the ot her segments, which would have involved watching over forty hour s of video. We could use a statistical sampling approach to reduce the number, but because having difficulty is a rare event, we would have had to sample the vast majority of segments to capture the false negatives. Therefore, we used the following, somewhat arbitrary approach to choose the  X  X aking progress X  segments. We randomly chose these segments, and made the nu mber of randomly sampled points indeterminate segments. If there were fewer than three having difficulty or indeterminate segments, we randomly sampled three segments. We shall refer to the randomly sampled segments as  X  X andom segments X . Each segment was two minutes of video. Coders were not aware of the status of each segment and had to classify the segment as making progress or slow progress. They were shown the video that corresponded to a particular participant and problem. If there were any segments for the coder to classify, they were shown on a line below the track bar. The se gments on the line corresponded with the particular point in the video the coder needed to classify. To classify segments, coders right clicked on the segment to label it as  X  X low progress X  (the me ssage displayed for  X  X aving difficulty X ), and left clicked to label it "making progress". An image of a mouse was provided to remind coders what each mouse button meant, and a lege nd was also provided to help coders remember that a black segment meant the segment was unlabeled, a red segment meant slow progress, and a green segment meant making progress. Two coders and the first author classified 26 stuck segments and 36 random segments. After the user study and coding phases were comp lete, we were able to answer the following ques tions: What is the correlation between (a) predictions of the two coders; (b) developers X  and coders X  perception of status, (c) predictions of the tool and the developers X  perception of the status , and (d) predictions of the tool and the coders X  perception of the status? As we see below, the answers depended on whether the segment involved was one of the  X  X tuck X  segments or random segments. Table 3 shows that coders agreed 88% of the time with each other on stuck segments, and 83% of the time on random segments, and overall they agreed 85% of the time. To determine the level of agre ement within the stuck (random) segments we counted the number of times observers agreed with each other and divided that by the total number of stuck (random) segments observed. Interestingly, coders agreed that in 50% of the random segments, which were classified by th e tool as  X  X aking progress, X  participants were actually having difficulty. We examined these eighteen cases individually and f ound three segments that were three minutes before a stuck segment, so in these cases, the observers were quicker than the tool in determining the status of these segments. In the remaining fifteen segments, the coders seemed to take the inactivity of developers as being stuck. The three early observations were not counted as incorrect. So what did the participants themselves feel about their status in case of these segments? By definition, they agreed completely with the predicted status for th ese segments, as these were the segments that were classified by the tool, participant, and first author as  X  X aking progress X  segments. We noticed that coders seemed to have a difficult time classifying participants when they were id le, and apparently thinking. The tool uses developers' actions to predict their status and does not take into account think times or when developers are idle. Therefore, we consider the fift een random segments as  X  X aking progress X  when computing the accuracy of the tool. Consider now the non-random or  X  X  tuck segments. X  Again, these are the segments classified either by the first author, or the participant, or the tool as  X  X aving difficulty X . These segments tell a very different story. Table 4 shows the agreement of the coders with the tool, the author, and the participants for these segments. Interestingly, coders agreed with the tool 100% of the time that participants were stuck. Perhaps even more interestingly, participants never corrected a  X  X av ing difficulty X  st atus predicted by the tool. In four of these segments, participants corrected the  X  X aking progress X  prediction of the tool. Three of those times, participants indicated they were having diff iculty, and one of those times participants indicated that they were not sure of their status (indeterminate.) In nine of th ese segments, the first author classified the  X  X aking progress X  pr ediction of the tool as actually  X  X aving difficulty X . The coders agreed with seven of these observations (77%). Coders agreed with the participant 75% of the time. The coders disagreed w ith the participant who indicated indeterminate as the status. The first author also reviewed this disagreement and agreed with the coders that the participant was indeed having difficulty. Several (preliminary) conclusions can be drawn from these results. What is perhaps most remarkable is that when the tool Segment 
Random 
Table 4: Coders X  agreement with the tool, first author, and predicts programmers are having difficulty, all three types of humans involved in making the prediction  X  the participants, the coders, and the first author, also think they are having difficulty. Thus, the tool does not seem to gi ve a false positive, which is a very strong result, and a significa nt improvement over the results in our previous work [7]. Moreover, if we take the particip ants X  perceptions as ground truth, the tool also gives negligible fals e negatives  X  only four segments out of 1222 segments in the entire study were corrected. On the other hand, if we take the coders  X  agreements as ground truth, the results are not so good, and it s eems, based on our sampling, the tool missed half of the positives (stuck status). There are two ways to interpret these data. The first relies on the viewpoint of the participants rath er than the coders. The argument for doing so is that the observers could not read the mind of the participants, and were probably looking only at idle times to deduce the developer status. Idle times, alone, are not sufficient to distinguish between thinking and having difficulty. Our tool, on the other hand, keeps track of a nd computes a larger number of factors, such as the navigation, edit, and focus ratios, and thus agrees more with the participants . In fact, when asked about the accuracy of the tool, participants commented that they were happy with it (Table 4). The numbers s hown in the table are represented by the following two comments: "I think it worked pretty well; It's non-intrusive, and only pops up with information when the status changes." " It knew when I was having issues cause it switched to slow progress and when I was flyin doing all the class design it said progress." The other interpretation relies on the observers (coders and first author) rather than the participants. The rationale for doing so is that participants tend to underrepor t their problems [21]. The false negatives of the tool can be explained by two factors: 1. The tool uses developers' actions to predict their status, and 2. The training set consisted of data from the six student Even under this interpretation, our tool seems useful because of the zero false-positive rates. It seems that if a choice has to be made between low false positives and negatives, the former is more desirable, as it does not unnecessarily waste the time of the developers and those who offer help. Missing some  X  X aving difficulty X  statuses is no worse than the current practice of not having any automatic predictions. Our tool did give several positives (thirteen), which were all correct under this interpretation. Thus, if it is cons idered desirable to automatically let others know about developers X  difficulties  X  an assumption of this research based on previous wo rk -then it seems better to use our tool than not use it. (under the second interpretation ) without increasing the false positive rate. One way to do so is train the system using the observers X  conclusions rather than developer corrections (assuming the former are true). Moreover, the accuracy can be further improved if the training da ta involved the same exercises as the ones used in the testing phase. We could either build a group model, in which the data of multiple developers is aggregated during the training phase, or an i ndividual model, where no aggregation is done. (The approach described so far was also a group model, but in it, the training group was smaller and solved different problems) Therefore, we decided to, next, explore these directions. To build the individual and our group models, we assumed the following ground truth. All segments classified by the participants as stuck, were indeed stuck segments. Participants implicitly classify segments as stuck when they do not correct a stuck prediction of the tool. They explic itly classify them as stuck when they correct a  X  X aking progress X  segment as  X  X low progress X . Of the remaining segments, if the first author and the two coders classified a segment as stuck, then it was also a stuck segment,. regardless of how the participant cl assified it. All other segments were making progress. To build and evaluate the indivi dual model, we used a standard technique, known as cross validation, which executes 10 trials of model construction, and splits the data so that 90% of the data are used to train the algorithm and 10% of the data are used to test it. In some of the participant's training sets, the number of  X  X aking progress X  segments vastly outnumbered the number of  X  X aving difficulty X  segments, resulting in low accuracy in predicting the "having difficulty" segments. This is an example of the class imbalance problem in classifi cation algorithms, wherein the accuracy of predicting an event can decrease as the frequency of a rare but important event decreas es. The SMOTE [22] algorithm implemented in the WEKA to olkit [23] overcomes this problem by replicating rare data records until that data are equal to the more common data. Therefore we used this scheme in the data sets of those participants who experienced th e class imbalance problem. In our case, we used an accuracy threshold of 90% to determine if a participant experienced this problem, which was the accuracy of our previous approach [7]. The accuracy of the model without SMOTE was 66% or less for participants who had difficulty 20% or less of the time. For participants who had difficulty more than 20% of the time, the accuracy of the model without SMOTE was 94% or more. Thus, according to our threshold, participants who had difficulty less than 20% of the time faced the class imbalance problem. For these participants , we used SMOTE to replicate the  X  X aving difficulty X  segments. In the case of the remaining participants, X  having difficulty X  was either less or about as frequent as  X  X aking progre ss X . Thus, there was never a need to use SMOTE to replicate the  X  X aking progress X  segments. Three of the twelve participants faced so much difficulty that they did not complete two of the three exercises. complete two of the three exercises. To build the model for a particular individual, we used that individual's data as both the training and test set. To build the group model, we aggregated the da ta from all of ou r participants except data from the participant whose status we were trying to automatically predict. The exclusion was meant to test if a tool trained by one set of developers co uld be used to predict the status of another. We used the group da ta to predict the status of each individual. The group data set did not s uffer from the class imbalance problem because some of the participants had difficulty just as much as they were making prog ress. As mentioned before, even those who made relatively smooth progress experienced some difficulty. The decision tree algorithm [23] was used to build both the individual and group models. Figures 5a and 5b show the accuracy of the tool. We considered four accuracies: (a) group stuck: the accuracy of the group model when predicting having difficulty, (b) individual stuck: the accuracy of the individual model when predicting having difficulty, (c) group overall: the accuracy of the group model when predicting both making progress and having difficulty, and (d) individual overall: the accuracy of the individual model when predicting both making progress and having difficulty. The accuracies are shown for all but two participants. These two participants were not included because their data was not collected correctly. We expected each individual's model to be more accurate than the group model, but surprisingly, the group model was more accurate in predicting both  X  X aving difficulty X  and  X  X aking progress X  than the individual model. This unintuitive result is likely because the group model has more data than the individual model. It is Table 5: Survey Questions and Results (Scale: 1 = Strongly Q1 I felt that the tool was Q2 I would prefer to use a possible that with more training, the individual model would perform better. Even then, it may not be the preferable approach because participants, probably, would not like training the tool. In fact, during the debrief one part icipant commented that pressing buttons " stopped my flow of thought" and another participant felt that pressing buttons "sort of broke my concentration" . We asked participants if they preferred to speak their status because this could help reduce breaking their concentration (Table 5). Participants did not like this feature either, and felt it would be disruptive to those around them. There were two participants whos e accuracy was 50% or below. We examined these cases and determined that the tool believed these participants were making progress while human observers believed the participants were stuc k. In each case, the participants were performing significant edits, which indicated to the tool that they were making progress. Howe ver, these edits involved a large number of deletions. This kind of activity suggests that, when extracting features, editing acti ons should be split into two categories: insertion and deletion of text. The evaluations above show that it is possible to increase the agreement between a tool and a set of observers by (a) keeping the exercises the same in the training and evaluation set, and (b) using the judgments of these observers in the training set. Additional iterations are required to determine if (a) a tool trained using one set of exercises can be used to predict the status for another set of tasks, and (b) judgments of one se t of observers can be used to agree with the judgments of another set of observers. So far, we have assumed th at letting others know about difficulties of others is good. Th is assumption is probably true when the observers are mentors/advisors, as suggested in [3]. However it is possible to have observers who judge programmers without actually helping them. Th ese judgers can use information about developers being stuck re peatedly in a negative manner which could cause programmers to lose respect in their team. Even when observers can be trus ted, the developers may want more time to investigate their problems. There are several ways to solve this problem. One approach is to block judgers, a feature readily available in Google Talk and other IM clients. The problem with this approach is that blocked judgers can realize that they are blocked, which could cause them to become hostile. Therefore, a superior approach is to allow programmers to decide which status they want to report. Figure 6 shows a preliminary scheme we have implemented to support this feature, which is also used by developers to train the system. This interface reports two statuses  X  the true status an d the reported status. Buttons are provided to change both statuses. Figure 6: Training user interf ace that show actual versus The buttons that change the true status are used to train the system and the buttons that change the reported status determine what automatically copied to the reported status field after a certain time lag. During this time, developers can manually disable the copying. Assuming that having difficulty is indeed a rare event, this user-interface does not impose substantial overhead. We have not formally evaluated these privacy controls, but we have gotten some initial feedback from those who have used them. Users would indeed like to customize not only what status is reported, but when it is reporte d, and to whom it is reported. Thus, this scheme must be extended to control the nature and timing of reported status for differ ent classes of observers such as (a) human observers and tools, (b) a team member sitting on the next seat, radically co-located, and distributed, (c) a close friend, mentor, and boss, and (d) team members who have and do not have the expertise to help solve a problem. Such elaborate customization could make the overhead required to use the tool high. Future versions of this scheme must allow for setting user-specific de faults. For example, the number of IM messages with team members can be used to identify close friends; organization charts can be used to find mentors and bosses; location information can be used to find the physical distance between developers and various observers; and the difficulty each team member has with different pieces of a project can be used to find expertise. In addition, the tool can adapt how developers morph the reported status . For instance, if they always report the indeterminate status to their boss, then the tool could ask them if they wish to set this value automatically for this observer. This paper contributes to both the general area of semantic awareness and the specific subarea of providing awareness about developers X  progress. To the best of our knowledge, other work on semantic awareness has not tied the judgments of third-party observers with those of a tool. Our work shows that (a) these judgments can be different from those of the actors about whom the awareness is being provided, (b) a special tool must be provided to gather third-party observations, and (c) it is possible to train an automatic tool to agree, to a high degree, with those of the observers. Our main contributions, of course, are in the subarea of difficulty prediction. We have identified a pipeline of modules for predicting and displaying difficul ty. We have also shown the usefulness of two well-known de sign patterns, Mediator and Strategy, in implementing the pi peline. We have created an architecture that allows the pipeline implementation to be reused by and execute asynchronously with multiple programming environments. Our evaluations show that label aggregation can be used to significantly improve the accuracy of a difficulty-prediction algorithm, and a group training model offers more accuracy than an individual one under certain circumstances. They also show that it is possible to build a tool that does not give false positives, regardless of whether the participant or observer judgments are used about the gr ound truth. Finally, we motivate and present new user-interfaces for customizing status messages and exporting the status to others. As mentioned before, it would be useful to formally evaluate a design space of status customiza tion and exporting interfaces, and determine if (a) a tool trained using one set of exercises can be used to predict the status for another set of tasks, and (b) judgments of one set of observers can be used to agree with the judgments of another set of observers. Perhaps the biggest unresolved issue raised by this work is whether the participants or observers should be relied upon to determine if developers are stuck. Perhaps more observations are needed to help resolve this issue. Another, more objective approach, is to make the following assumption: The fraction of  X  X av ing difficulty X  segments is proportional to the inherent difficulty of the problem. By using problems of known difficulty, we can determine whether the judgments of observers or deve lopers correspond more closely with problem difficulty. Once this issue is resolved, the next step would then be to deploy developers feel about the accuracy of the tool, (b) how often and to whom they export the  X  X aving difficulty X  stat us, and (c) how often and from whom they accept help. Naturally, based on this experience, we can expect additi onal iterations through the design process identified earlier. Assuming that observers are not reliable in characterizing developer X  X  difficulty level, a pa ir of programmers working side-by-side [6] could use difficulty notifications to determine if they should help each other. Moreov er, knowing the rate at which developers get stuck may be usef ul not only for determining if they need help. It could be used to (a) characterize the inherent difficulty of new problems, (b) determine the expertise of developers to solve certain kind s of problems, (c) estimate how long it will take them to complete their task, (d) compare the effectiveness of the various coup ling degrees, mentioned earlier, in reducing the number of times developers face difficulty. This paper provides a basis and motivation for carrying out these future research directions. This research was funded in part by NSF grants IIS 0312328, IIS 0712794, and IIS-0810861. We would like to thank the study subjects. 1. Redmiles, D., et al. Continuous Coordination: A New 2. Sarma, A., D. Redmiles, and T. R. Andr X  van der Hoek. , TR-3. Begel, A. and B. Simon. Novice software developers, all over 4. LaToza, T.D., Venolia, G., and Deline. R. Maintaining Mental 5. Nawrocki, J.R., et al., Pair Programming vs. Side-by-Side 6. Dewan, P., et al. Experiments in Distributed Side-by-Side 7. Carter, J. and P. Dewan. Are You Having Difficulty? In Proc 8. Herbsleb, J.D., et al. Distance , dependencies, and delay in a 9. Cataldo, M. Sources of Errors in Distributed Development 10. Teasley, S., et al. How does radical collocation help a team 11. Cockburn, A. and L. Williams, Th e Costs and Benefits of Pair 12. Williams, L., et al. Building Pair Programming Knowledge 13. Herbsleb, J. and R.E. Grinter. Splitting the Organization and 14. Hegde, R. and P. Dewan. Connecting Programming 15. Hollan, J. and S. Stornetta. Beyond Being There. In Proc . CHI 16. Kapoor, A., Burleson, et al., Automatic Prediction of 17. Liu, Y., Stroulia, E. A Light weight Project-Management 18. Begole, J.B., et al., Work Rhyt hms: Analyzing Visualizations 19. Fogarty, J., Ko, A., Aung. H. H., Golden E., Tang, K. and 20. Tang, J.C., et al. Unobtrusive But Invasive: Using Screen 21. Shrauger, J.S. and T.M. Osberg. The Relative Accuracy of 22. Chawla, N.V., et. al., Smote: Synthetic minority over-23. Witten, I.H., Frank, E. Data Mining: Practical Machine 24. Eclipseye X  X pying on eclipse. Bach elor X  X  thesis, University of 
