 Nowadays, many social websites allow users to vote each other to facilitate the trustworthiness evaluation. For example, users can give trust/distrust votes to other users based on their reviews of items on Epinions 1 and Ciao 2 .Wecan represent these users(nodes) and their trust/distrust votes(directed edges) as a trust graph. This trust graph can also b e transferred into a matrix, if user i trusts user j , then the corresponding matrix entry is 1, otherwise 0. Trust prediction is therefore to predict the missing values in this matrix and can be viewed as a special case of the general link pre diction in social networks[1].
 The challenges of developing trust prediction algorithms are three-fold: (1) Scalability , a trust graph often consists of millions of users, (2) Sparsity ,just extremely sparse trust relations are available, making trust prediction a diffi-cult task. (3) Skewness , a small proportion of users often specify most of trust relations while a large proportion of users specify few trust relations[2]. Hence, an ideal method of trust prediction is s upposed to efficiently handle large-scale data sets, while addressing the problem of sparseness and skew distribution in user-specified trust relations.
 In this paper, we propose a novel and scalable Structured Sparse Linear Model(SSLM) for social trust prediction. SSLM formulates the prediction prob-lem as a set of independent linear regression problems regularized by pairwise elastic net. Furthermore, we apply efficient hashing techniques and stochastic coordinate descent algorithm to expediting the computation of our model in training phase. Our main contributions are summarized as follows:  X  We treat trust prediction problem as a set of linear regression problems. To  X  We utilize efficient hashing techniques, i.e., one permutation hashing[4] and  X  We propose a scalable stochastic coordinate descent algorithm for solving  X  We conducted experiments on the data sets from three real social websites,
The rest of this paper is organized as follows: Section 2 briefly introduces some related works. In Section 3, we descr ibe our structured sparse liner model for social trust prediction and present how to apply hashing techniques and the efficient stochastic coordinate descent algorithm into our model. Section 4 reports the experimental evaluation and results. Finally, we conclude our work in Section 5. In recent years, many researchers have st udied social trust prediction problem. Existing trust prediction methods can be roughly divided into the following categories:
The first category is supervised methods[6][7], which first construct features from available sources and then train a binary classifier based on these features by considering existing trust relations as labels. However, online trust relations often follow a power law distribution, which makes the classification problem extremely imbalanced. The performance o f these methods is also sensitive to the sampled negative samples[8].
The second category is based on trust propagation. The authors in [9] pro-posed several atomic propagations such as direct propagation, cocitation prop-agation, transpose propagation and trust coupling propagation. TidalTrust[10], which is a continuous trust inference algorithm, leverages the path length from the source to sink and various properties of continuous ratings. However, trust propagation based methods strongly depend on existing trust relations among users and they might fail when existing trust relations are sparse[2].
The last category is low-rank approximation methods, which can be done us-ing the singular value decomposition[11] or trace norm minimization[12]. How-ever, there still exists some issues with th ese methods, i.e., the observed entries are assumed to be sampled uniformly at random, but typical real-world trust graphs are empirical examples of power laws[13]. Hence, the author in [1] pro-posed a robust matrix completion method to alleviate these problems. Some au-thors also utilized matrix factorization to seek a low-rank approximation[2][14].
Our approach is a global neighborhood-based approach that models the corre-lation structure among nodes in trust graphs based on a structured sparse linear model. Our model is derived from the previous work [15] for recommendation scenario by introducing the elastic net re gularizer capable of a utomatic selecting groups of correlated variables, in order to accomplish high-quality social trust predictions. In this section, we first introduce our structured sparse linear model for social trust prediction. Then, we present how to use hashing techniques to significantly improve the efficiency and scalability of our method. 3.1 The Proposed Model Previous studies in sociology and our life experience suggest that the persons in the same social circle often exhibit similar behavior and tastes[16]. This phe-nomenon signals that we can infer the trust value for an individual user from his or her social neighbors. As shown in Figure 1, the trust value from someone (e.g., Rose or Tom) to Jack can be expressed as a linear combination of trust values which this user has given to Jack X  X  social neighbors, assuming that the aggregation coefficients between Jack and his neighbors can be learnt in advance.
More formally, Given a user-user trust relation matrix A of size M  X  M , whose entry a ij is 1 if user i has trusted user j , otherwise the value is 0, M is the number of users. The missing trust value from user i to user j can be calculated as a sparse aggregation of trust values which user i has given to user j  X  X  social neighbors, and can be formulated as: where  X  a T i  X  R K is a row vector of trust values which user i has given to user j  X  X  nearest social neighbors, K is the number of user j  X  X  social neighbors, w j  X  R K is a sparse column vector of aggregation coefficients.

In order to effectively predict all the missing trust values, we need to learn a set of sparse aggregation coefficients vectors ( w j ,j =1 , 2 , ..., M )fromthe trust relation matrix A . We then propose the following regularized optimization problem to learn a structured sparse coefficients vector w j for user j : where a j  X  R M is the j -thcolumnofmatrix A , w j  X  R K is the coefficients vector of user j , and the constant  X  is the regularization parameter. Matrix  X  A  X  R M  X  K consists of K columns chosen from matrix A which indicate K most similar social neighbors of user j and are ranked in descending order of their similarity values. Note that each user is always trusted to himself or herself, so j -th trust value in a j should not be utilized to solve the coefficients vector w j . Fortunately, we can easily set the j -th row in  X  A to 0 to avoid it happening.
The second item of Eq.(2), | w j | T P | w j | , is the pairwise elas tic net[3] regular-izer term, where P is a symmetric and positive semidefinite(PSD) matrix with nonnegative entries. In this paper, we consider matrix P as I + 11 T  X  R ,then this regularizer term can be expressed as: where I is an identity matrix, 11 T is a matrix of all ones entries and R  X  R K  X  K is a similarity matrix whose entry r ij is the vector similarity between user i and user j in matrix  X  A . When learning a coefficients vector w j , enforcing this regularizer term will simultaneously incu r sparsity and structured grouping effect of the learnt coefficients vector w j , which encodes intrinsic and delicate relations between other users and user j .

Ifthenumberofuseris M , we need to solve M optimization problems shown in Eq.(2) to obtain M sparse aggregation coefficients vectors. Because these problems are independent to each other, they can be solved in parallel. Further-more, we will present a stochastic coordinate descent algorithm in Section 3.3 to solve each optimization problem significantly faster than the traditional way.
After obtaining the structured sparse aggregation coefficients vectors for M users, missing trust values can be effi ciently calculated as follows: where A  X  R M  X  M is the trust relation matrix, W  X  R M  X  M is a sparse matrix whose j -th column corresponds to an aggregation coefficients vector w j . Recall that we just calculate the aggregation coefficients for top-K similar users, we therefore need to transform w j to W j as follows: W ij = w kj ,ifuser i is the k -th most similar social neighbors of user j ,otherwise W ij = 0. Since matrix A and matrix W are both very sparse, matrix S can be calculated efficiently, whose entry s ij represents the predicted trust value from user i to user j .
Note that matrix P in Eq.(3) is not always PSD, so we can use a shrinkage parameter as describ ed in [3] to ensure P to be PSD, that is where  X  1+  X   X   X   X  1,  X  =  X  min (0 , X  min ( P )),  X  min ( P ) is the minimum eigenvalue of matrix P . Through the above transformation, matrix P is sure to be PSD and our formulation is a convex optimization problem. 3.2 Incorporating Hashing Techniques In our proposed model, generating two matrices  X  A and R are two important steps before solving each coefficients vector w j . Both steps need to calculate the similarities between each pair of users. In this paper, we just use the binary trust relation information to help computing the similarity values. Intuitively, if two users are trusted by almost the same set of users, they are very likely to be social trusted neighbors.

Although there are many similarity meas ures, i.e., Jaccard coefficient, cosine similarity and Pearson correlation coeffi cient, we decide to calculate the simi-larity values by the Jaccard coefficient b ecause of binary trust relation vectors. However, calculating the Jaccard coefficients between millions of users in real-world applications still costs a huge amount of time and memory space. Hence, we utilize hashing techniques that have been widely used for efficiently estimat-ing similarity in massive data to expedite calculating the Jaccard coefficients.
Figure 2 shows the workflow of our approach. We first apply the one permu-tation hashing[4] which is an effective a nd efficient MinHash technique to each column of the trust relation matrix A (i.e., a 1 , a 2 , ..., a M ) and generate short of signature as | s | X  X  a | / 30, and the average error between the original Jaccard coefficient and the estimated one is les s than 0.01. Hence, we can directly cal-culate the Jaccard coefficient on these short signatures to generate matrix R , which saves a lot of time and memory cost.

Moreover, in order to efficiently find the similar users to generate matrix  X  A ,we can further utilize the locality sensitive hashing(LSH) to hash these signatures into several buckets. After we hash these signatures for several times, similar users are more likely to be hashed into the same bucket than dissimilar ones and most of the dissimilar users will never be hashed to the same bucket. Hence, we only need to check a small number of users in some buckets. For example, to find out the users most similar to a specific user j (i.e., a 1 ), we only need to check several buckets (i.e., b 2 , b 100 ) which have the signature s j (i.e., s 1 )and consider users in these buckets as candidate similar users. We then sort them in descending order of the Jaccard coefficients between user j  X  X  signature (i.e., s 1 ) and candidates (i.e., s 3 , s 7 , s 5 , s 30 , etc.). 3.3 Stochastic Coordinate Descent Algorithm The traditional way to solve the optimization problem in Eq.(2) is the coordinate descent algorithm. Generally speaking, the coordinate descent algorithm holds all elements in w j fixed except for the i -th element w ij andthensolves  X  X   X  X  to get w ij accordingly. Cycling t hrough each element in w j iteratively until w j converges, we could get the final optimal solution. From Eq.(2), we have where Q =  X  A T  X  A , q =  X  A T a j and w ij represents the i -th element in w j .Wecan calculate the partial derivative as follows: We then set  X  X   X  X  part, which is z if z&gt; 0and0otherwise.
 Algorithm 1. Stochastic Coordinate Descent for solving Eq.(2)
In each iteration, a single element of w j is updated by Eq.(8). The computa-tional cost of calculating each w j is O ( K 2 t ), where t is the number of iterations. When the number of users is M , the computational cost of solving our model is O ( MK 2 t ), which is too expensive to deal wit h large-scale social trust predic-tions. Hence, we propose a stochastic coordinate descent algorithm to efficiently solve each problem in Eq.(2). Inspired by [17], we suggest to choose one element uniformly at random from the coordinate set, instead of cycling through all el-ements. This simple modification leads to a significant reduction of iterations while achieving the expected accuracy.

Algorithm 1 shows our stochastic coordinate descent algorithm. The algorithm initializes w j to be all 0. At each iteration, we pick an element i uniformly at random from the coordinate set { 1 , 2 , ..., K } to update w ij by Eq.(8). After T iterations, our stochastic algorithm will converge to the optimal solution w o j . The computational cost of calculating each w j is therefore reduced to O ( KT ). In most cases, T is much smaller than Kt . Hence, our stochastic coordinate descent algorithm significantly cuts down the computational time in the training phase. 3.4 A Simulation Study We conducted a simulation study to show the effectiveness and efficiency of our stochastic coordinate descent algorithm. We simulate data from the linear model: y = X  X   X  +  X  ,where X  X  R n  X  p is the design matrix, y  X  R n is the response vector,  X   X   X  R p are the unknown weights and  X   X  R n is a zero-mean i.i.d. Gaussian noise vector. n and p are the number of observations and predictors respectively. We can find an estimate  X   X  of  X   X  by solving the same optimization problem in Eq.(2):
The data set is simulated as follows: We set p = 5000. The rows of the design matrix X are i.i.d. draws from the Gaussian distribution N (0 ,  X  ), where the correlation matrix  X   X  R p  X  p has entries  X  ij =0 . 5 | i  X  j | .  X   X  is a sparse vector which has 1000 non-zeros entries drawn independently from the standard uniform distribution. The pair-wise similarity matrix R is constructed as R ij = | X T i X j | , where X i denotes the i -th column of X . We train our model on a training set of n = 2000. Parameters are set with a validation set of n = 1000. Then, we test the performance on a testing set with n = 4000.

Table 1 shows the results of the traditional and stochastic coordinate descent algorithm. We find that our stochastic algorithm not only achieves a comparable mean-squared error(MSE) on estimating y and  X   X  against the traditional one, but also beats the traditional algorithm in terms of efficiency. We also show the convergence of our stochastic algorithm in Figure 3. The MSE of estimating  X   X  goes down as the number of iterations increases. Hence, our stochastic algorithm is empirically proven to converge.
 In this section, we describe some experiments conducted to evaluate the perfor-mance of our Structured Sparse Linear Model(SSLM) and compare it with other popular methods. 4.1 Data Sets We utilized three real-world trust graph data sets to evaluate the performance of different methods, i.e., Epinions 3 , Slashdot 4 and Ciao 5 . Epinions and Ciao are both product review sites where users can give trust/distrust votes based on their opinions on others X  reviews on products. Slashdot allows users to tag each other as friends or foes based on their submitted technology news. Note that these original data sets only record trust relations, and most of distrust relations are unknown. Hence, we only utilized trust rel ations in our experiments. For each data set, we filtered out the users who were trusted by less than 5 users to obtain a data set that has sufficient trust relations. The statistics of our data sets are summarized in Table 2.
 4.2 Comparison Methods and Details We compared the performance of our SSLM with that of other four popular al-gorithms, i.e., k-nearest neighbors(KNN), SVD[11], non-negative matrix factor-ization(NMF) and matrix completion(MC)[12]. KNN predicts the missing trust values based on user-user similarity wh ich is calculated by Jaccard coefficient. SVD finds a low-rank matrix that approximates the original trust relation matrix and the NMF model decomposes the original matrix into two non-negative factor matrices to seek a low-rank approximat ion. The MC model also seeks a low-rank matrix that approximates the original one, but the rank is automatically chosen by the model itself.

We utilized Matlab to implement these algorithms and optimized the Mat-lab codes with C to make them more efficient. We also employed the Parallel Computing Toolbox in Matlab to solve our model in parallel. All the experi-ments were conducted on a workstation equipped with two 4-core Intel Xeon E5620(2.40GHz) CPUs and 16GB RAM. 4.3 Evaluation Metrics Since in many trust-related online applications, what we need is to obtain each user X  X  most trusted users, rather than predict the missing trust values. Hence, we create a testing set by ra ndomly selecting one of its t rusted users for each user as the testing example. The remaining trust relations are treated as the training set used to train the examined trust prediction models. Afterward, missing trust values are generated by each model. We can obtain a list of most trusted users for each user in descending order of predicted trust values. In order to measure the prediction quality, we define the prediction accuracy( PA ) as follows: where L i is a list of user i  X  X  most trusted users, T i is the trusted user of user i in the testing set and M is the total number of users. In our experiment, we set the length of each list | L i | = 10. PA measures the ability of each model to correctly predicate a list of t rusted users for each user.
 4.4 Experimental Results For all the above methods, we tuned the best parameters to made them achieve the best performance. We only repo rt the best performance results.
 Trust Prediction Results. The prediction accuracy( PA ) of different methods on three data sets are presented i n Table 3. We selected different K values in SSLM and denoted them as SSLM( K )( K = 100 , 300 , 500) to study their performance. The results in Table 3 show that SSLM can generate better PA than other methods over all the data sets. We also find that the PA of SSLM improves as the K value increases. That is because a larger K value means a larger size of neighborhood, the more relations between user j and its neighbors can be learned in the aggregation coefficients vector w j , which gives rise to the better prediction quality. Another observation is that the MC model has a comparable performance with SSLM(100). However, it often takes several hours(e.g., about 31 hours on the Epinions data set) to train the MC model, while our SSLM(100) only takes several minutes(e.g., about 3 minutes on the Epinions data set) for training the model. Moreover, SSLM(300) and SSLM(500) can achieve better PA than the MC model on all data sets.
 Efficiency Evaluation. We utilized the Epinions data set as an example to illustrate the efficiency of our hashing based procedure. Table 4 presents the time cost of computing similarities between al l pairs of users through two different methods. The first method is the origin al Jaccard coefficient and the second one is our hashing-based procedure. All methods were implemented in C to make sure that any time difference in performance is due to methods themselves. In Table 4, we find that our hashing-based procedure just takes about 1/6 time cost against the original Jaccard coefficient, which makes our SSLM model efficiently handle large-scale data sets.

We also evaluated the efficiency of the t raditional coordinate descent(CD) and stochastic coordinate descent(SCD) algorithm on the Epinions data set. We report the average time of each iteration of these two algorithms in Table 5. Both algorithms were written in C and ex ecuted serially to objectively compare their efficiencies. For three neighborhood sizes, Table 5 shows that the larger the K value is, the faster our stochastic coordinate algorithm runs in each iteration. With respect to SSLM(500), our stochastic approach cuts off the 39% average execution time of each iteration, compared to the traditional approach. Hence, our stochastic coordinate descent algorithm can significantly boost the efficiency of our proposed SSLM approach. In this paper, we propose a novel and scalable Structured Sparse Linear Model (SSLM) for social trust prediction. SSLM formulates the trust prediction prob-lem as a set of independent regularized linear regression problems. Scalable hash-ing techniques and stochastic coordinate descent algorithm are also incorporated into our model to reduce the computational cost of system training. The experi-ments on simulated and real-world data sets show that our approach outperforms other examined approaches in terms of prediction ability and scalability. In the future, we intend to extend our model to utilize additional user information, e.g., users X  profiles and item rating information, and derive more scalable stochastic coordinate descent algorithms.
 Acknowledgments. This work was supported by Zhejiang Provincial Nat-ural Science Foundation of China ( No. LQ13F020001), the Ch inese Knowl-edge Center for Engineering Sciences and Technology Project, 973 Program (No.2012CB316400), the Special Funds for Key Program of National Science and Technology (No. 2010ZX01042-002-003), the Program for Key Cultural In-novative Research Team of Zhejiang Province, the Fundamental Research Funds for the Central Universities and the Opening Project of State Key Laboratory of Digital Publishing Technology.

