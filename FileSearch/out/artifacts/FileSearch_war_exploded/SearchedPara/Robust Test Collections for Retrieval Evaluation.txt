 Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thou-sands of judgments. While these judgments are very use-ful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evalu-ate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems. Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Perfor-mance Evaluation General Terms: Experimentation, Measurement, Relia-bility Keywords: information retrieval, evaluation, test collec-tions, reusability
Consider an information retrieval researcher who has in-vented a new retrieval task. She has built a system to per-form the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judg-ments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informa-tive and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task? Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Above we gave an intuitive definition of reusability: a collection is reusable if we can  X  X rust X  our estimates of con-fidence in an evaluation. By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B ,wewould like there to be no more than 25% chance that our assess-ment of the relative quality of the systems will change as we continue to judge documents. Our evaluation should be robust to missing judgments.

In our previous work, we defined confidence as the proba-bility that the difference in an evaluation measure calculated for two systems is less than zero [8]. This notion of confi-dence is defined in the context of a particular evaluation task that we call comparative evaluation : determining the sign of the difference in an evaluation measure. Other eval-uation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.
We therefore see confidence as a probability estimate. One of the questions we must ask about a probability estimate is what it means. What does it mean to have 75% confidence that system A is better than system B ? As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change. If this is what it means, we can trust the confidence estimates. But do we know it has that meaning?
Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant. This assumption is almost certainly not realistic in most IR applications. As it turns out, it is this assumption that determines whether the confi-dence estimates can eb trusted. Before elaborating on this, we formally define confidence.
Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall). It is typically written as the mean precision at the ranks of relevant documents: where R is the set of relevant documents and r ( i ) is the rank of document i .Let X i be a random variable indicating the relevance of document i . If documents are ordered by rank, we can express precision as prec @ i =1 /i i j =1 X j . Average precision then becomes the quadratic equation lows us to number the documents arbitrarily. To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and non-relevant document A at rank 2. Average precision will be (
Let Z be the set of all pairs of ranked results for a com-mon set of topics. Suppose we have a set of m relevance judgments x m = { x 1 ,x 2 , ..., x m } (using small x rather than capital X to distinguish between judged and unjudged docu-ments); these are the judgments against which we compute confidence. Let Z  X  be the subset of pairs in Z for which we predict that  X  MAP =  X  1 with confidence  X  given the judgments x m . For the confidence estimates to be accu-rate, we need at least  X   X |Z  X  | of these pairs to actually have  X 
MAP =  X  1 after we have judged every document. If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.

If our confidence estimates are based on unrealistic as-sumptions, we cannot expect them to be accurate. The assumptions they are based on are the probabilities of rele-vance p i . We need these to be  X  X ealistic X .

We argue that the best possible distribution of relevance p (
X i ) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions. This is known as the principle of maximum entropy [13].

The entropy of a random variable X with distribution p ( X ) is defined as H ( p )=  X  i p ( X = i )log p ( X = i ). This has found a wide array of uses in computer science and information retrieval. The ma ximum entropy distribution is the one that maximizes H . This distribution is unique and has an exponential form. The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.

Theorem 1. If p ( X n | I,x m )= argmax p H ( p ) , confidence estimates will be accurate. where x m is the set of relevance judgments defined above, X n is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now). We forgo the proof for the time being, but it is quite simple.

This says that the better the estimates of relevance, the more accurate the evaluation . The task of creating a reusable test collection thus becomes the task of estimating the rele-vance of unjudged documents.

The theorem and its proof say nothing whatsoever about the evaluation metric. The probability estimates are entirely indepedent of the measure we are interested in. This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.
Furthermore, we could assume that the relevance of doc-uments i and j is independent and achieve the same result, which we state as a corollary:
Corollary 1. If p ( X i | I,x m )= argmax p H ( p ) , confidence estimates will be accurate.

The task therefore becomes the imputation of the missing values of relevance. The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness.
In our statement of Theorem 1, we left the nature of the information I unspecified. One of the advantages of our con-fidence estimates is that they admit information from a wide variety of sources; essentially anything that can be mod-eled can be used as information for predicting relevance. A will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic. Thus our model takes into account not only the dependence between experts, but also the dependence between experts X  performances on different tasks (topics). Each expert gives us a score and a rank for each document. We need to convert these to probabilities. A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance. The  X  X airwise preference X  method of Carterette &amp; Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.

Let q  X  ij be expert j  X  X  self-reported probability that docu-ment i is relevant. Intuitively it seems clear that q  X  ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant). The pairwise preference model can handle these two require-ments easily, so we will use it. Let  X  r j ( i ) be the  X  X elevance coefficient X  of the document at rank r j ( i ). We want to find the  X  s that maximize the likelihood function: We again include a beta prior on p (  X  r j ( i ) ) with parameters | R t | +1 and | N t | + 1, the size of the sets of judged rele-vant and nonrelevant documents respectively. Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t . This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.

After finding the  X  that maximizes the likelihood, we have bility that an unranked document is relevant is 0.
Since q  X  ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q  X  AE = q  X  BD . Therefore we only have to solve this once for each topic.

The above model gives topic-independent probabilities for each document. But suppose an expert who reports 90% probability is only right 50% of the time. Its opinion should be discounted based on its observed performance. Specifi-cally, we want to learn a calibration function q ij = C j ( q  X  ij ) that will ensure that the predicted probabilities are tuned to the expert X  X  ability to retrieve relevant documents given the judgments that have been made to this point.

Platt X  X  SVM calibration method [16] fits a sigmoid func-tion between q  X  ij and the relevance judgments to obtain q ij = C we only need to learn one calibration function for each ex-pert.

Once we have the calibration function, it is applied to adjust the experts X  predictions to their actual performance. The calibrated probabilities are plugged into model (2) to find the document probabilities.
 ad hoc 94 50 40 97,319 9,805 ad hoc 95 49 33 87,069 6,503 ad hoc 96 50 61 133,681 5,524 ad hoc 97 50 74 72,270 4,611 ad hoc 98 50 103 80,345 4,674 ad hoc 99 50 129 86,830 4,728 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume p =0 . 5 for all unjudged documents.
We obtained full ad hoc runs submitted to TRECs 3 through 8. Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4). Additionally, we obtained all runs from the Web track of TREC 13, the Robust 2 track of TREC 14, and the Terabyte (ad hoc) track of TREC 14. These are the tracks that have  X  X eplaced X  the ad hoc track since its end in 1999. Statistics are shown in Table 1. We set aside the TREC 4 (ad hoc 95) set for training, TRECs 3 and 5 X 8 (ad hoc 94 and 96 X 99) for primary testing, and the remaining sets for additional testing.

We use the qrels files assembled by NIST as  X  X ruth X . The number of relevance judgments made and relevant docu-ments found for each track are listed in Table 1.
For computational reasons, we truncate ranked lists at 100 documents. There is no reason that we could not go deeper, but calculating variance is O ( n 3 ) and thus very time-consuming. Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100.
We will compare three algorithms for acquiring relevance judgments. The baseline is a variation of TREC pooling that we will call incremental pooling (IP). This algorithm takes anumber k as input and presents the first k documents in rank order (without regard to topic) to be judged. It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.

The second algorithm is that presented in Carterette et al. [8] (Algorithm 1). Documents are selected based on how  X  X nteresting X  they are in determining whether a difference in mean average precision exists. For this approach p i =0 . 5 for all i ; there is no estimation of probabilities. We will call this MTC for minimal test collection .

The third algorithm augments MTC with updated esti-mates of probabilities of relevance. We will call this RTC for robust test collection . It is identical to Algorithm 1, ex-cept that every 10th iteration we estimate p i for all unjudged documents i using the expert aggregation model of Section 3.
RTC has smoothing (prior distribution) parameters that must be set. We trained using the ad hoc 95 set. We limited  X  X obust X  here means robust retrieval; this is different from our goal of robust evaluation.
 The amount we win on each pairwise comparison i is: y =1if X  MAP &lt; 0and0otherwise,and P i = P ( X  MAP &lt; 0). The summary statistic is W , the mean of W i . Note that as P i increases, we lose more for being wrong. This is as it should be: the penalty should be great for missing the high probability predictions. However, since our losses grow without bound as predictions approach certainty, we cap  X  W i at 100.

For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems. The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require sev-eral thousand. The distribution is therefore highly skewed, and the mean strongly affected by those outliers.
Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendall X  X   X  correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments. Kendall X  X   X  , a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study. It ranges from  X  1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped. As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability. We include it for completeness.
Running multiple trials allows the use of statistical hy-pothesis testing to compare algorithms. Using the same sets of systems allows the use of paired tests.

As we stated above, we are more interested in the median number of judgments than the mean. A test for difference in median is the Wilcoxon sign rank test. We can also use a paired t-test to test for a difference in mean.
For rank correlation, we can use a paired t-test to test for a difference in  X  .
The comparison between MTC and RTC is shown in Ta-ble 2. With MTC and uniform probabilities of relevance, the results are far from robust. We cannot reuse the relevance judgments with much confidence. But with RTC, the re-sults are very robust. There i s a slight dip in accuracy when confidence gets above 0 . 95; nonetheless, the confidence pre-dictions are trustworthy. Mean W i shows that RTC is much closer to 0 than MTC. The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates. The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems. Re-call from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.
 More detailed results for both algorithms are shown in Figure 2. The solid line is the ideal result that would give W = 0. RTC is on or above this line at all points until confidence reaches about 0 . 97. After that there is a slight dip in accuracy which we discuss below. Note that both
Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic). Performance on these is very high: W =0 . 41, and 99 . 7% accuracy when confidence is at least 0 . 9. This shows that even tiny collections can be reusable. For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0 . 9.

Rank Correlation: MTC and RTC both rank the 10 systems by E MAP (Eq. (1)) calculated using their respective probability estimates. The mean  X  rank correlation between true MAP and E MAP is 0 . 393 for MTC and 0 . 555 for RTC. This difference is significant by a paired t-test ( p&lt; 0 . 0001). Note that we do not expect the  X  correlations to be high, since we are ranking the systems with so few relevance judg-ments. It is more important that we estimate confidence in each pairwise comparison correctly.

We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP us-ing only those judgments (all unjudged documents assumed nonrelevant). We calculated the  X  correlation to the true ranking. The mean  X  correlation is 0 . 398, which is not sig-nificantly different from MTC, but is significantly lower than RTC. Using uniform estimates of probability is indistin-guishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.

Overfitting: It is possible to  X  X verfit X : if too many judgments come from the first two systems, the variance in  X  MAP is reduced and the confidence estimates become unreliable. We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%. In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior toacorrectprediction.

Overfitting is difficult to quantify exactly, because mak-ing more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences. Obviously having more relevance judgments should increase both confidence and accuracy; the differ-ence seems to be when one system has a great deal more judgments than the other.

Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments 2. one of the original runs vs. one of the new runs; 3. two new runs.
 Table 3 shows confidence vs. accuracy results for each of these three groups. Interestingly, performance is worst when comparing one of the original runs to one of the additional runs. This is most likely due to a large difference in the number of judgments affecting the variance of  X  MAP .Nev-ertheless, performance is quite good on all three subsets.
Worst Case: The case intuitively most likely to pro-duce an error is when the two systems being compared have retrieved very few documents in common. If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.

A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2]. We calculated this for all pairs, then looked at per-formance on pairs with low similarity. Results are shown in consistent across data sets. ments being shared by researchers, each group contribut-ing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100% confidence in every evaluation X  X nd there is a full test collection for the task.
We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing. It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].

The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. A sim-pler expert aggregation model might perform as well or bet-ter (though all our efforts to simplify failed). In addition to expert aggregation, we could estimate probabilities by look-ing at similarities between documents. This is an obvious area for future exploration.

Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it. In the meantime, capping confi-dence estimates at 95% is a  X  X ack X  that solves the problem.
We have many more experimental results that we unfor-tunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pair-wise comparison of systems.
 This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings, and c onclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the spon-sor.
