 Many real applications can be modeled using bipartite graph s, such as users vs. files in a P2P system, traders vs. stocks in a finan-cial trading system, conferences vs. authors in a scientific publica-tion network, and so on. We introduce two operations on bipar tite graphs: 1) identifying similar nodes (relevance search), a nd 2) find-ing nodes connecting irrelevant nodes (anomaly detection) . And we propose algorithms to compute the relevance score for each n ode using random walk with restarts and graph partitioning; we a lso propose algorithms to identify anomalies, using relevance scores. We evaluate the quality of relevance search based on semanti cs of the datasets, and we also measure the performance of the anom aly detection algorithm with manually injected anomalies. Bot h ef-fectiveness and efficiency of the methods are confirmed by exp eri-ments on several real datasets. A bipartite graph is a graph where nodes can be divided into tw o groups V 1 and V 2 such that no edge connects the vertices in the same group. More formally, a bipartite graph G is defined as G  X 
V 1  X  V 2 , E  X  , where V 1 = { a i | 1  X  i  X  k } , V 2 = { E  X  V 1  X  V 2 as shown in Figure 1 .
 Many applications can be modeled as bipartite graphs, for ex ample: 1. P2P systems: V 1 is a set of files, and V 2 a set of peers. An 2. Stock markets: The traders and stocks form V 1 and V 2 3. Research publications: Researchers publish in different con-In general, based on the application domain, the edges can be weighted. For instance, edge weights in the stock market graph can repr esent the trading volume, while in the research publication graph , they may represent the number of papers published by an author in a conference. For presentation purposes, we will only focus o n un-weighted graphs; our algorithms can be easily generalized t o other graph types.
 Under this setting, our work addresses two primary problems : 1. Relevance search(RS): Given a query node a in V 1 , RS com-2. Anomaly detection(AD): Given a query node a in V 1 , AD Nodes that belong to the same group ( V 1 or V 2 ) have the same type; it is the connections between the two types of objects that hold the key to mining the bipartite graph. Given the natural inter-g roup connections (between V 1 and V 2 ), our objective is to discover the intra-group relationships, such as the clusters and outlie rs within the group. For example, in the research publication biparti te graph, we have two natural groups of entities: conferences and auth ors. The relationship between these two groups is reflected by the edges. Based on these edges, we want to find the similar conferences a nd unusual authors that publish in different communities. An e ffective mining algorithm should thus be able to utilize these links a cross the two natural groups.
 Our algorithm for RS is based on the idea of random walks with restarts [ 9 ]. The method is simple, fast and scalable. In addition, we approximate the RS computation by graph partitioning to fur ther boost the performance.
 The algorithm for AD uses the relevance scores from RS to calc u-late the normality scores. Intuitively, a node (in V 2 ) is an anomaly if it links to two nodes (in V 1 ) that do not belong to the same neighbor-hood/community. For example, an author becomes an anomaly i f he/she publishes papers in conferences from two different fi elds. In the sequel, we will use neighborhood and community intercha nge-ably.
 Note also that a natural symmetry exists in the roles of neigh bor-hoods and anomalies. In particular, we can swap V 1 and V apply the same algorithms in order to obtain the relevance sc ore in V and the normality score in V 1 .
 In summary, the contributions of the paper are that: 1. we identify two important problems (Relevance search and 2. we develop the exact algorithms based on random walks with 3. we propose a fast approximate algorithm using graph parti -4. the results can be easily interpreted by the user; and 5. we evaluate the methods on real datasets to confirm their ap -Section 2 proposes the data model and the formal problem specifi-cation. Section 3 presents the algorithms. In section 4 , we evaluate the algorithms with real data. We discuss the related work in sec-tion 5 and conclude in section 6 . We will first define our data model and terminology, and then de -scribe the exact formulations of the RS and AD problems.
 Data model. The data is viewed as a bipartite graph G =  X  V ,
E  X  , where V 1 = { a i | 1  X  i  X  k } and V 2 = { t i | 1 V  X  V 2 . The graph G is conceptually stored in a k -by-n matrix M , where M ( i , j ) is the weight of the edge &lt; i , j we adopt the sparse matrix representation where the storage space is proportional to the number of non-zero elements in the mat rix. The value can be 0/1 for an unweighted graph, or any nonnegati ve value for a weighted graph. For example, the unweighted grap h in Figure 1 becomes the following matrix: The nodes in V 1 ( V 2 ) are called row(column) nodes. Note that a col-umn node links to a row node if the corresponding matrix eleme nt is not zero. Moreover, row node a connects to another row node b if there is a column node c linking to both a and b . We call that path a connection between a and b through c . Nodes a and b can have multiple connections via different column nodes. For examp le in the matrix above, rows 3 and 5 links through column 1, 2, 4 and n . We can construct the adjacency matrix M A of G using M easily: In particular, M A ( a , t ) denotes the element at a -th row and t -th col-umn in M A .
 Suppose we want to traverse the graph starting from the row no de a . The probability of taking a particular edge &lt; a , t &gt; to the edge weight over all the outgoing edges from a . More for-mally, P A ( a , t ) = M A ( a , t ) /  X  k + n i transition matrix P A of G is constructed as: P A = col norm where col norm ( M A ) normalizes M A such that every column sum up to 1.
 The main reasons to have M instead of working directly on M P are the computational and storage savings. Next, we define th e two problems addressed in the paper: Relevance search(RS). Given a node a  X  V 1 , which nodes in V are most related to a ? There are two ways to represent the neigh-borhoods: 1) select a set of nodes as the neighbors and the oth er nodes are not the neighbors (Hard Neighborhood); 2) assign a rel-evance score to every node where  X  X loser X  nodes have high sco res, and no hard boundary exists (Soft Neighborhood). In this pap er, we adopt the soft neighborhood, because the score can help iden tify neighborhood but also differentiate the neighbors. In part icular, we want to compute a relevance score to a for every node b  X  higher the score is, the more related that node is to a . More specif-ically, the node with the highest score to a is a itself; the nodes that are closer to a probably have higher scores than the other nodes that are further away from a .
 Anomaly detection (AD). What are the anomalies in V 2 to a query node a in V 1 ? Again we adopt the notion of soft anomalies by computing the normality scores for nodes in V 2 that link to a . Hence, the nodes with lowest normality score are the anomali es to a . In this section we discuss the algorithms that solve the two p rob-lems presented above. We first define relevance score and desc ribe how to compute the relevance scores for the row nodes (neigh-borhood formation) in section 3.1 . Then, based on the relevance scores, we define normality score and illustrate how to obtai n the normality scores for the column nodes (anomaly detection) i n sec-tion 3.2 . Given a row node a  X  V 1 , we want to compute a relevance score for each row node b  X  V 1 . The final result is a 1-by-k vector consisting of all the relevance scores to a .
 Intuition. Intuitively, we do multiple random walks starting from a , and count the number of times that we visit each b  X  V counts reflect the relevance of those nodes to a . The probability of visiting b  X  V 1 from a is the relevance score we want to obtain. In the following, we list some scenarios on which the row nodes h ave high relevance scores. b usually has a high relevance score to a if (1) b has many connec-tions to a as shown in Figure 2 ; or (2) the connections only involve a and b as shown in Figure 3 . Scenario (1) is obvious because the row nodes b and a have many connections through the columns nodes, which indicates the strong relevance between b and a . Sce-nario (2) is less obvious. The intuition is that the connecti on that only links a and b brings more relevance between a and b than the connections linking a , b and other nodes. The relevance score is not only related to the number of connections but also to the numb er of nodes involved in the connections. One observation is that t he node b with the highest relevance score is not necessarily the one w ith most connections to a . The reason is that those connections link to nodes other than a and b as well. Thus, the relevance is spread out among many different nodes. Nevertheless, all the scenario s above are well captured by our algorithm in spite of its simplicity . Algorithms. We propose three methods for computing relevance scores: (1) Exact RS implements the basic idea but can have slow convergence rates, (2) Approximate RS performs graph partition-ing first which calculate results approximately but much mor e effi-ciently.
 Exact RS. First, we transform the input row node a into a n )  X  1 query vector ~ q a with 1 in the a -th row and 0 otherwise. Sec-ond, we need to compute the ( k + n )  X  1 steady-state probability vector ~ u a over all the nodes in G . Last we extract the probabilities of the row nodes as the score vectors. Note that ~ u a can be computed by an iterated method from the following lemma.

L EMMA 3.1. Let c be the probability of restarting random-walk from the row node a. Then the steady-state probability vecto r satisfies where P A is already the column normalized.
 P ROOF . See [ 19 ] Algorithm RS E (Exact RS) Input: node a , bipartite matrix M , restarting probability c , tolerant threshold  X  0. initialize ~ q a = 0 except the a -th element is 1 ( q 1. construct M A (see Equation 1 ) and P A = col norm ( M 2. while ( |  X  ~ u a | &gt;  X  ) ~ u a = ( 1  X  c ) P A ~ u a + c ~ q a 3. return ~ u a ( 1 : k ) The algorithm simply applies Equation 2 repeatedly until it con-verges. And c is set to 0.15 for all experiments. The actual com-putation of the algorithm can utilize the bipartite structu re to have more saving. More specifically, we do not materialize M A and modify Equation 2 as follows: last n elements of ~ u a , respectively. The relevance score rs ~ u 1 : k ) . If we compute the relevance scores for all the nodes, we have a similarity matrix S .
 The saving is significant when the number of rows k and the number of columns n differ a lot. Therefore, Equation 3 is always recom-mended in practice, while Equation 2 is only for demonstrating the concept.
 Very often we have the input of more than one row node. The task is to compute the relevance score for every input row node. In stead of applying algorithm RS E for every input, we implement it in a more efficient way by running the algorithm in parallel for se veral inputs by vectorizing the code.
 Approximate RS. One problem with the previous approaches is the large memory requirement. In particular, the algorit hm is efficient when the entire matrix can fit in memory. However, on e observation from our experiments suggest that the relevanc e scores for the nodes are very skewed, with most nodes have almost zer o relevance scores, and only a few nodes having high scores. Th is suggests that we can possibly filter out many  X  X rrelevant X  no des before applying the RS computation. Based on this intuition , we apply graph partition first and perform RS only on the partiti on containing the query node. In particular, we use METIS [ 13 ] to partition the graph into  X  non-overlapping subgraphs of about the same size. Note that the graph partition is a one-time cost to pre-process the data. The pseudo code is the following: Algorithm RS A (Approximate RS) Input: the bipartite graph G , the number of partitions  X  , input node a 0. divide G into  X  partitions G 1 . . . G  X  (one-time cost) 1. find the partition G i containing a 2. construct the approximate bipartite matrix M 0 of G i edges cross two partitions) 3. apply RS E on a and M 0 4. set 0 relevance scores for the nodes that are not in G Based on the relevance scores for V 1 computed as shown above, we can compute the normality scores for the nodes in V 2 . A node with a low normality score is an anomaly.
 Given a column node t  X  V 2 , we first find the set S t of row nodes to which t links: S t = { a | &lt; a , t &gt;  X  E } . Let k t  X  X ormal X , then the relevance scores between any pair of elem ents in S t should be high. More formally, we compute the k t -by-k ilarity matrix RS t over S t . Note that RS t can be obtained by taking a subset of columns and rows from the k -by-k similarity matrix RS . For example in Figure 1 , for t = 1 , S t = { 1 , 3 , 5 } by-3 matrix where each element is a relevance score from a to b  X  S t . Note that (1) RS t is asymmetric, i.e., the relevance score from a to b may differ from the one from b to a , and (2) RS a strong diagonal, i.e., every node has a high relevance scor e to it-self. We ignore the diagonal for the normality score computa tion. In general, the normality score of t can be any function over RS We define ns ( t ) as the mean over all the non-diagonal elements in RS t . The lower the normality score ns ( t ) is, the more abnormal t is. Essentially, given an input t  X  V 2 , we first compute the relevance score vectors for every adjacent row node S t to t (using any of the RS methods described in section 3.1 ). Then we obtain the similarity matrix RS t and apply the score function on RS t . A computational trade-off is whether or not to pre-compute the relevance sco re vec-tors of all the row nodes. It usually depends on the number of r ow nodes involved. For example, if the dataset has a large numbe r of rows and the input queries are skewed, pre-computation is no t rec-ommended, because it incurs huge cost and most of them is wast ed due to the skewed distribution of the queries.
 Algorithm AD(Anomaly Detection) Input: input node t , bipartite transition matrix P 0. find the set S t = { a 1 , a 2 , ... } such that  X  a i  X  1. compute all the relevance score vectors ~ R of a  X  S t 2. construct the similarity matrix RS t from ~ R over S t 3. apply the score function over RS t to obtain the final normality score ns ( t ) 4. return ns ( t ) Examples of anomalies. Figure 4 shows the typical example of an anomaly t , which links to two row nodes a and b that commu-nicate to different sets of nodes. Without t , a and b are completely irrelevant. Note that one requirement of computing relevan ce is that a and b need to have enough connections to establish their identities. For example, a and b in Figure 4 still have a number of connections without t , while in Figure 5 , b has no other connections apart from t . This implies in Figure 5 the identity of b is unknown (or we do not have enough confidence to say whether b is very re-lated a or not). Therefore, t in Figure 5 will not be identified as an anomaly, while t in Figure 4 will.
 On the other hand, the example in Figure 5 is easy to be found by simply counting the degree of the row nodes and picking the on es with only one connection. Potentially, the number of such no des can be huge. The point is that our method aims at a non-trivial case of the anomaly, which tries to identify the connections across multiple neighborhoods. For example, author A and B write ma ny papers with different groups of authors. If there is a paper b etween A and B, it will be an anomaly, because we know A and B belong to different neighborhoods. However, if B only has one paper an d A is the co-author, we cannot decide whether the paper is an ano maly, because we do not know the neighborhood of B other than the sol e paper with A. In this section we evaluate the exact and approximate method s on neighborhood formation and anomaly detection. We focus on a n-swering the following questions: Q1 : How accurate is the exact RS algorithm? Q2 : How to choose restarting probability c ? Q3 : How accurate is the approximate RS algorithm? Q4 : Can the AD algorithm detect the injected anomalies? Q5 : What about the computational cost of different methods? After describing the experimental settings in section 4.1 , we answer Q1 in section 4.2 , using concrete examples from different datasets (i.e., compare exact RS algorithm vs.  X  X round truth X ). Sect ion 4.3 discusses the tradeoff in parameter selection. Section 4.4 uses quantitative metric to compare approximate RS methods vs. e x-act RS method (Q2). Section 4.5 answers Q3 by injecting artificial anomalies and evaluating the performance on that. Finally, section 4.6 answers Q4 by providing the quantitative evidence of the dra -matic computational saving on space and execution time. Datasets: We construct the graphs using three real datasets, whose size are specified in Table 2 .
 Conference-Author(CA) dataset: Every row represents a confer-ence; every column represents an author. The elements in the bi-partite matrix M are nonnegative integers. On average, every con-ference has 510 authors, every author publishes in 5 confere nces. Dataset Rows Columns Nonzeros Weighted IMDB 553388 204000 2269811 no Author-Paper(AP) dataset: Every row represents an author; every column represents a paper. The elements in the bipartite mat rix M are either 0 or 1. In particular, M ( i , j ) = 1 indicates that the i -th author is an author for the j -th paper. On average, every author has 3 papers, every paper has 2 authors. The distribution is very skewed as most of authors have only one paper.
 IMDB dataset: Every row is an actor/actress; every column is a movie. The elements in the bipartite matrix M are either 0 or 1. In particular, M ( i , j ) = 1 indicates that the i -th actor/actress is in the j -th movie. On average, every actor/actress plays in 4 movies , and every movies has 11 actors/actresses. Exact RS: We want to check whether the nodes with high rele-vance scores are closely related to the query node. The goal i s to ensure the result makes sense in the context of the applicati ons. More specifically, we select some rows from the three dataset s as the query nodes and verify the RS scores through user study. D ue to the page limit, we just show one example from each dataset. CA dataset: Figure 6 shows the top 10 neighbors of KDD confer-ence. As expected, the most related conferences are the othe r top data mining, machine learning and databases conferences: I CML, SIGMOD, VLDB, ICDM, ICDE, NIPS. This also implies that KDD is an interdisciplinary conference attracting researcher s from dif-ferent fields. Similar to our example, Klink et al. [ 14 ] devel-oped DBLP browser which uses an author-based similarity met ric to model the closeness of two conferences. i.e., two confere nces that have many common authors are highly similar.
 AP dataset: Figure 7 plots the top 10 neighbors of Prof. Jiawei Han. They are indeed the close collaborators to Prof. Han, wh o either have many joint papers with Prof. Han or have several e xclu-sive joint papers.
 IMDB dataset: For IMDB dataset, we perform the same set of experiment as above. Unlike the previous two datasets, the p eople in this dataset are not well-clustered, meaning that if a and b play in the same movie, it does not increase the likelihood that th ey will play together again in the future. Of course, they are except ions in the sequels of successful movies.
 We choose Robert De Niro as an example here. The persons with the highest relevance scores, as shown in Figure 8 , are Billy Crystal and Lisa Kudrow because they all perform in the same 2 movies ( X  X nalyze this X  and the sequel  X  X nalyze that X ). Furthermor e, they are the only main actors/actress in the movies. This is again due to the result of the combination of 2 scenarios in section 3.1 . There are only two parameters in RS E algorithm, namely the restart probability c and the convergence threshold  X  . The  X  is just the termination threshold for the matrix multiplication, whic h clearly affects the number of convergence. Basically, it determine s the minimal change on the relevance scores between two consecut ive rounds. We set it to 0.1 in all cases.
 The effect of c is similar to  X  but in a less obvious way. Given the  X  , the larger c is, the quicker the method converges. In the ex-treme,when c = 1, meaning that we jump back to the query node with 100% probability every timestamp, it is converged with 0 iter-ation. When c is closed to 0, the method probably takes very long time to converge if ever. Note that if c = 0, it will never converge since the graph is bipartite. The Markov chain will bounce ba ck and forth between two sets of vertices. In Figure 9 , the number of iterations before converging decreases quickly then goe s flat as c increases. Ideally from the efficiency point of view, we want to minimize the time required for the method. Note that the numb er of iteration needed drops significantly when the size of grap h re-duces. This gives us a lot of performance gain when we partiti on the graph.
 In general, the convergence speed depends on the ratio of the first and second eigenvalues of the Markov transition matrix. In p ar-ticular, since  X  1 = 1, the convergence speed is proportional to the second eigenvalue  X  2 . The smaller  X  2 is, the faster the conver-gence speed is. In fact, as pointed out by Haveliwala [ 10 ], the second eigenvalue of Markov transition matrix P A is exactly 1 Therefore, the number of iteration decreases as 1  X  c as shown in Figure 9 .
 On the other hand, when c is high, the relevance search only focus on the neighbors that are very close to the query node. In the e x-treme, when c = 1, the only relevant node is the query node itself, all the other row nodes have 0 relevance value. In Figure 10 we observe the percentages of irrelevant row nodes increases a s c . Ide-ally from the effectiveness point of view, we want to have a la rge coverage of the nodes with non-zero relevance score.
 To balance between efficiency and effectiveness, we choose c 0 . 15 in all the other experiments. We only show the result on DBL P dataset, and similar results are obtained from other datase ts too. In practice, The changes on c have very small effect on the ordering of the most relevant nodes to the query nodes. The effect is ma inly focused on the nodes that have little relevance to the query n ode, which are unimportant in most cases. In this sense, the metho ds are robust to the change of c . We partition each dataset into k partitions with equal size using METIS [ 13 ]. The RS computation for a row node a only involves the nodes in the same partition as a (we assign 0 relevance scores to the row nodes in other partitions). The goal is to show that the relevance scores of the high relevant nodes do not change muc h us-ing the approximate method(partition method). The metric w e use is precision , that is, the number of common top relevant nodes over the neighborhood size. We set the neighborhood size to 10 (i. e., retrieve the top 10 relevant nodes) and vary the number of par tition  X  as shown in Figure 11 as a function of precision . We observe the precision does not drop much, which suggests that the approx imate method works well. We also vary the neighborhood size as a fun c-tion of precision in Figure 12 , while setting the number of partition  X  = 10. We observe that the neighborhood can be fairly accuratel y captured over different ranges. Note that it does not make se nse to have large neighborhood size for this evaluation, because t he rel-evance scores will become very low (practically zero) for mo st of the nodes. In particular, the effective neighborhood size f or dataset AP is rather small, because the most of people only co-author with a small number of people. As a result, the precision drops fas ter as the neighborhood size increases. Due to lack of information about the real anomalies, we manua lly inject random connections between nodes. In particular, we in-ject 100 column nodes in each dataset connecting to k row nodes, where k equals the average degree of column nodes. The row nodes are randomly selected among the column nodes with large degr ee (greater than 10 times the average). The reason for not using all the row nodes is that most of row nodes have degree one and the inje c-tion to those nodes will not lead to an anomaly because statis tically we do not have enough information to tell whether the injecti on is an anomaly. Note that the difference between using exact and ap-num of iterations Figure 9: c value vs. number of iteration for converging for
DBLP Figure 10: c value vs. percentage of irrelevant nodes for DBL P.
When c  X  . 6, most of the nodes have 0 relevance score to the query node except for the ones that are very close to it. proximate RS is marginal. And we use approximate RS in the AD algorithm to reduce computational cost.
 Figure 13 plots the average normality scores of genuine and in-jected nodes over three different datasets. We observe a big gap of the normality scores between genuine and injected ones. H ence, we can easily identify the anomalies by looking at the ones wi th the lower scores within the same dataset. Note that only the r ela-tive score matters in detecting anomaly not the absolute sco re. And it is hard to compare the scores across datasets because of th e dif-ferent graph structure. All the computation of different methods boils down to the RS com-putation. The only difference is how large the matrix is. Int uitively, the computational cost is large if we work with the entire dat aset. It is usually beneficial to partition the dataset. The partit ion incurs a one-time cost which can be amortized over the future querie s (in-volving RS and AD computation). Figure 14 shows the computa-tion cost on neighborhood formation vs. the number of partit ions. Note that a dramatic cost reduction can be found when using th e approximate RS computation method (the partition method). There is a significant body on research related to our work, wh ich we categorize into four groups: graph partitioning,outlie r detection on graphs, random walks on graphs, and collaborative filteri ng. Graph Partitioning. Popular methods for partitioning graphs include the METIS algorithm [ 13 ], spectral partitioning techniques [ 12 ], flow-based methods [ 7 ] information-theoretic methods [ 6 ], and meth-ods based on the  X  X etweenness X  of edges [ 8 ], among others. These typically require some parameters as input; Chakrabarti [ 5 ] uses MDL criterion to automatically determine both the number of clus-ters and their memberships. Note that our work is orthogonal to this, and we can use any graph-clustering algorithm. In addi tion, as a by-product of our algorithms, the neighborhoods over no des can represent personalized clusters depending on different perspec-tives.
 in a general graph; however, we need to detect outlier nodes . Noble and Cook [ 15 ] study anomaly detection on general graph with la-beled nodes; yet, their goal is to identify abnormal substru cture in the graph, not the abnormal nodes . Aggarwal and Yu [ 1 ] propose algorithms to find outliers in high-dimensional spaces, but its ap-plicability to graphs is unclear: the nodes in a graph lie in a vector space formed by the graph nodes themselves, so the vector spa ce and the points in it are related. Newman proposed  X  X etweenne ss X , a measure of the centrality of a node in a graph. The algorithm i s also based on random walk with running time O (( m + n ) n 2 ) Figure 13: Normality scores between genuine and injected no des across 3 datasets Figure 14: Computation time(sec): y-axis vs. number of part itions: x-axis the number of edges and n the number of nodes in the graph. The betweenness measure is intuitively related to our normalit y scores (high betweenness means low normality). However the comput a-tion cost of betweenness is much higher.
 Random-walk on Graphs. Page-Rank [ 3 ] learns the ranks of web pages using the iterated power method on web graph M (ad-jacency matrix of the entire graph). The ranks of all webpage s are cast as an N-dimensional vector, and then the fixed point is fo und for the following equation: ~ r = ( 1  X   X  ) M  X  ~ r +  X  ~ is the damping factor and ~ p = [ 1 N ] N  X  1. Thus, there is an uni-form prior on all the web pages. In order to deal with personal ized query, Topic-Sensitive PageRank [ 9 ] increases the importance of certain web pages by putting non-uniform weights for ~ p . Similar random-walk approaches have been used into other domains; f or example, Mixed Media Graph(MMG) [ 16 ] applies random walk with restart on image captioning application. We plan to fur ther explore the random walk algorithm on bipartite graph and use it to identify anomaly nodes. Similar idea also appear in SimRank [ 11 ] which is a similarity measure between nodes in a graph with th e intuition that two nodes are similar if they are related by si milar nodes.
 Collaborative Filtering. Collaborative filtering is one of biggest successes on bipartite graphs, which provides automatic fil tering about the user interests based on the historical informatio n from many users (collaborating) [ 18 ; 2 ]. Many different similarity met-rics have been proposed such as Jaccard coefficient, cosine s imilar-ity, and Pearson correlation coefficient (see all in [ 17 ]), which is re-lated to the relevance search in this paper. However, the tra ditional similarity metrics do not apply when there is no common conne c-tion. For example, if two authors never published a paper tog ether, those traditional metrics will give zero similarity despit e the fact that they publish in the same field, while our method will be ab le to find the non-zero relevance between the two authors. Further more, the goal of collaborative filtering is to develop a recommend ation system to predict the users X  behavior but not to find anomalie s. A variety of datasets can be modeled as bipartite graphs, suc h as P2P networks, stock trades, author-paper relationships, a nd so on. This paper addresses two problems on such bipartite graphs: 1) relevance search; 2) anomaly detection. The main propertie s of the methods are: The main idea is to use random-walk with restarts and graph pa r-titioning. We evaluate the methods on several real datasets . Our experiments confirm the efficiency as well as the effectivene ss of the proposed methods. [1] C. Aggarwal and P. Yu. Outlier detection for high-[2] J. Breese, D. Heckerman, and C. Kadie. Empirical analysi s of [3] Sergey Brin and Lawrence Page. The anatomy of a large-[4] Deepayan Chakrabarti. Autopart: Parameter-free graph parti-[5] Deepayan Chakrabarti, Spiros PapADimitriou, Dharmen-[6] I. S. Dhillon, S. Mallela, and D. S. Modha. Information-[7] Gary William Flake, Steve Lawrence, and C. Lee Giles. Effi -[8] M. Girvan and M. E. J. Newman. Community structure in so-[9] T. Haveliwala. Topic-sensitive pagerank. In Proceedings of [10] Taher H. Haveliwala and Sepandar D. Kamvar. The second [11] Glen Jeh and Jennifer Widom. Simrank: a measure of [12] R. Kannan, S. Vempala, and A. Vetta. On clusterings  X  goo d, [13] George Karypis and Vipin Kumar. Multilevel k-way parti tion-[14] Stefan Klink, Michael Ley, Emma Rabbidge, Patrick Reut her, [15] C. C. Noble and D. J. Cook. Graph-based anomaly detectio n. [16] Jia-Yu Pan, Hyung-Jeong Yang, Pinar Duygulu, and Chris -[17] Berthier Ribeiro-Neto and Berthier Ribeiro-Neto. Modern In-[18] Upendra Shardanand and Pattie Maes. Social informatio n fil-[19] Gilbert Strang. Introduction to Linear Algebra . Wellesley-
