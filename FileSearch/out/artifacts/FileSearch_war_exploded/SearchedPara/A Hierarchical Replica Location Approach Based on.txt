 A data grid connects a collection of geogra phically distributed computers and storage resources that may be located around the wo rld, which enables users to share data and to reduce access latency and bandwidth consumption. A Replica Location Service is a core component of one grid data management architecture, which provides a mecha-nism for registering the replicas and discovering them according to given parameters. 
Recently, replica location mechanism has attracted much attention of scholars and fully. PRLS [2] proposed by Min CAI etc is a p2p replica location service based on a Distributed Hash Table (DHT), but local clustering property in data grid environment Mechanism based on DHT and small-world theory, but using of Gossip is apt to gen-erate bandwidth and computational redundancies. indexing structure that provides good scalability and search performance. Considering lead us to solve some issues of replica location. load balancing, named RepliLoc. load balancing. Storage load balancing is achieved via rough evenly distributing rep-lica index maps among nodes by using DHT and prefix matching. Computational load dissemination mechanism based on small world model (WS model) to accomplish replica location. On the other layer, we propose a replica location approach based on efficiency and effectiveness. In data grid environment, Resource Logi-cal Name (RLN) is a unique logical iden-tifier for desired data content. Each RLN has one or more physical copies, which identified by Replica Physical Address (RPA) to specify its location on a storage site. The replica location is to determine one or more RPAs of a given RLN. 
According to data grid and data-sharing characteristics in distributed environments, we design a two-layer (RCN), and two Catalog Index Nodes (CIN). When every RCN registers into a com-munity, it is identified by a monotony increasing decimal number given by RepliLoc . communities connected by CINs in a certain way to form Community alliance layer. or more new copies on DSNs and discover them. A CIN aggregates replica informa-tion stored at all RCNs in one community, and then generates an index by distributing their replica index maps ( RLN , CIN ) among CINs. need two steps: replica distribution and location on community layer and on commu-nity alliance layer. When replicas are created on DSN, users need register their infor-mation in RCN which DSN belongs to. After recorded them, RCN sends them to CIN simple. So we only describe replica location process in details in this section.  X  Cache Mechanism on Community Layer priately by the new one.  X  Replica Location on Community layer On community layer, our circular information dissemination mechanism based on WS model is described as follows: When querying replicas, RCN sends replica que-double circles till find the replicas or visit all RCNs in current community. 
The remote RCNs must meet following condition: one is that remote RCNs are se-restriction. That is, the number of newly selecting remote RCN and the number of all selected remote RCNs form a new circle. The distance of each RCN on new circle is from headstream. The algorithm of replica location on community layer is described by algorithm 1. Algorithm 1 (Replica Location on Community Layer) Step 1 RCN sends replica queries to two neighbors and remote RCNs; semination, turn to setp 4 , else continue sending them to their two neighbors; 
Step 3 If remote RCNs found the RPA, return results to RCN and end dissemina-theirs two neighbors and remote RCNs, turn to setp 2 ; 
Step 4 If RPA is found, RCN record IP of RCN that maintaining RPA, else turn to replica location on community alliance layer; 
Step 5 End. If no desired replicas in community, we need locate replica beyond communities. We distribute and locate replicas based on prefix matching and DHT On this layer. 4.1 Identification and Routing Table of CIN ted as 32-digit strings of radix-16. They are achieved by using a secure hashing algo-rithm like MD5. LI is dynamic and scalable according to the size of current CINs in 16 This CIN is the root of RI and maintains replica index maps of RI. 
To find root digit by digit, each CIN contains links to a set of neighbors that share of prefix matching, we design routing table as two-neighbor structure. That is, a CIN 4.2 Replica Distribution and Replica Location with Cache Mechanism implement replica distribution on community alliance layer. The process of distribut-ing replica index maps can be described by algorithm 2. Algorithm 2 (Replica Distribution on Community Alliance Layer) Step 1 CIN gets RI by applying MD5 to RLN; 
Step 2 CIN matches RI with neighbors in relative entry of routing table to find root of RI. The replica index maps are stored at the CINs passed by; Step 3 Stores maps at root; 
Step 4 End. 
On this layer, replica location is achieved by finding the root of RI based on prefix The process of locating replica can be described by algorithm 3. Algorithm 3 (Replica Location on Community Alliance Layer) 
Step 1 CIN gets RLN and matches it with cached information. If there are desired items, gets RPA and turns to Step 6 ; Step 2 CIN gets RI and matches it with routing table to find root of RI; 
Step 3 RI is matched with replica index maps stored at CINs passed by. If there are sends queries to these CINs; Step 5 CINs in results get RPA and return to CIN; Step 6 CIN appends RLN and its RPA to Cache table; Step 7 CIN returns results to RCN; 
Step 8 End. We focus on two preliminary performances in experiment: the performance of storage (CPU 2.0GHz, memory 256MB) r unning windows XP. Replica information and rep-lica index maps are stored at tables of SQL server2000 database. Java multithread is used to act as CINs, RCNs and DSNs. They communicate each other by Socket. Load balancing: Experiment 1 is to test storage load. Fig.2 shows the num-ber of replica index maps on every CIN after 1000, 2000 RLN is hashed to sixteen CINs. From Fig.2 we can see that replica index maps are rough evenly distributed on every CIN by using DHT and prefix matching, which roughly achieve storage load balancing; Experiment 2 is to test computational load. 100 query opera-tions on system that respectively con-tains 1, 15, 25, 40 CINs are executed. Each CIN maintains 1500 maps. Each community contains 15 RCNs that maintaining 100 replica information. Average query latency is measured beyond community without cached information on CIN, 100 query operations beyond community with cached information on CIN. Results of test are shown in Fig.3: Be-latency is shortened when there is cached information on CIN comparing to no cached information. Average Query Latency: Experiment 3 is to test average query latency in RepliLoc and compare it with PRLS. We execute the same query operations on the same number of nodes that maintain same replica information with PRLS. The result (shown in Fig.4) tells us that the average query latency of RepliLoc is shortened comparing with PRLS. maps are rough evenly distributed among CINs by using prefix matching and DHT to shorten path of dissemination and avoid matching and bandwidth redundancies gener-ated when users repeatedly accessing the same data. 
