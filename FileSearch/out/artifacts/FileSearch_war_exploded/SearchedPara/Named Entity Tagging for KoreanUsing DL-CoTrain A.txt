 In MUC-6 [8], MUC-7 [9] and IREX [10], various supervised learning methods for named entity task were applied and they gave a somewhat satisfactory perfor-mance. However, a large labeled corpus and named entity dictionaries or hand-written rules are generally needed for these supervised named entity recognition approaches. Moreover, practical and domain specific information extraction and question answering systems require the domain portability of a named entity recognition.
 pus, dictionaries or hand-written rules and a large unlabeled corpus which is easily obtainable. [3] satisfied these requests to some degree. However, they used a dependency-based full parser [4] to extract contextual features of a named entity, and current state-of-the-art parsing still causes some limitations of ro-bustness and practicality in named entity recognition systems, especially for Korean. Further, they did not empirically show how large a labeled corpus and which unlabeled corpus and seed rules are actually needed for practical named entity recognition.
 learning algorithm for Korean named entity recognition in chapter 2 and 3 in detail. Our experimental results are given in chapter 4 and conclusions are in chapter 5.
 We first detect named entities using a Korean POS tagger [2] and a LSP (Lexico-Syntactic Patterns) database. Then we extract two distinct views (lexical and contextual features) of named entities as input to both DL-CoTrain learner and a classifier, using a rule-based simple noun phrase chunker. 2.1 Detecting Named Entities The detection of a named entity uses LSP X  X  composed of POS tags and lemmas. As Korean does not have capitalization, Korean named entities are harder to detect than English, and can be made of diverse sources such as  X  X (MP/, Kim Dae-Jung), proper nouns together with common nouns  X   X (MP/ MC/ MC/, Inchon International Airport). 1 Republic of Korea), (2) tags and lemmas( X &amp;MP  X ), and (3) only proper noun tags( X &amp;MP &amp;MP). The named entity detecting module first normalizes mor-phemes within the maximum length 2 of a named entity with their POS tags and lemmas, and looks up the LSP database to find an LSP entry matching the normalized form. In this procedure, the matching order is following the above three entry types. 2.2 Lexical Features Lexical features are internal attributes of named entities for classification. We extract the following eight lexical features from the named entities, e.g. (Inchon International Airport) X  and  X (Mr. Kim Dae-Jung) X :  X  Full lexical (full lexical:  X   X ,  X  X )  X  Beginning word of a named entity for multiple words (beginning word:  X  X ,  X  Ending word of a named entity for multiple words (ending word:  X  X , N/A)  X  Beginning 2 or 3 syllables of first word (beginning 2syllables: N/A,  X  X )  X  Ending 2 or 3 syllables of last word (ending 2syllables: N/A,  X  X )  X  Suffix attached to a named entity (suffix: N/A,  X  X ) 2.3 Contextual Features Contextual features are external attributes of a named entity. They are extracted within the left and right 15 window size around the named entities in a sentence using a rule-based simple noun phrase chunker. We add contextual rules to the seed rules of DL-CoTrain, unlike [3] which only uses lexical rules as seed rules. The following are contextual features and their examples:  X  Left common noun directly attached to a named entity instance (left noun)  X  Head, a common noun, of noun phrase including a named en- X  Particle attached to a named entity (particle) e.g. particle =  X  j/  X  X or  X  Right nearest predicate (verb and adjective) of a named entity (predicate)  X  Combination of above two features (particle predicate) e.g. parti-We adapted DL-CoTrain of [3] as our learning algorithm. We stop iteration when the rules are not generated any longer. 3.1 Decision List Learning The decision list has rules sorted according to their strength in descending order, and each rule has a pair of collocation x and its decision  X  ,orclassandits strength. We regarded the strength of each rule as an estimate of conditional probability P (  X  | x ) like [3], such as where C ( x,  X  ) is the number of times that a collocation x and a class  X  appears together in training data, C ( x )=  X   X   X  C ( x,  X  ) indicates the number of times that x shows up in training data, k is the number of classes, and  X  is a smoothing parameter. 3.2 Classification of Named Entity The classification function of decision list, f : X  X   X  , can be formalized as follows: where X is a set of possible collocation and  X  is a set of classes. 3.3 DL-CoTrain Algorithm DL-CoTrain is performed with decision list (learning rules) and co-training (feature-split setting and augmenting pseudo-labeled data) frameworks. The in-put to the learning algorithm comprises seed rules extracted from a small set of labeled training corpus and pairs of lexical and contextual features of a large set of unlabeled named entity instances (candidates) detected automatically. During learning, lexical and contextual rules are increased with respect to each category, and unlabeled instances are pseudo-labeled by intermediate-generated rules. The output of DL-CoTrain is a final decision list obtained from the in-stances last pseudo-labeled. 4.1 Experiment Environment We used the same labeled training and test corpus of [11] 4 and 3.5 million POS-tagged corpus of [6] 5 as an unlabeled corpus. Table 1 and table 2 show the details of the corpora.
 ious genres, such as novels, essays, encyclopedia and travel sketches. The mixed domain corpus (E) consists of mixed news and non-news corpus to make it bal-anced. 4.2 How Large a Labeled Corpus Should Be? We first try to find out how large a labeled corpus should be for competitive performance. We extract seed rules from a quarter, a third, a half, and a full labeled training corpus in these experiments respectively, and use the corpus B for the news domain and the corpus E for the non-news domain as the unlabeled corpus, which showed the best performance in each domain. Figure ?? shows the overall results. In brief, using only a quarter of a labeled corpus without any named entity dictionary can give a performance comparable to the supervised one. Unlike [5] and [12], the performance is not always proportional to the size of the seed rule. Seed rules with ambiguous named entities (like country names) caused a lower performance in using a half or full of labeled corpus in non-news domain. By contrast, ambiguous named entities like country names in the specific news domain have a consistent category. 4.3 How Large and Which Unlabeled Corpus Is Necessary? We now extract seed rules (news domain: 30, non-news domain: 27) using only a quarter of labeled instances in these experiments. The performance with each corpus type (news vs. non-news) and that of the supervised counterpart [11] are given in table 3.
 almost similar to the supervised one using dictionaries in the news domain, a better performance was delivered in the non-news domain. These results show that the effect of the co-training in general domain is larger than that in specific domain. We presented a semi-supervised method for Korean named entity recognition. We do not use any named entity dictionary, but use a small set of labeled training corpus to extract seed rules, a large set of POS-tagged unlabeled corpus and a few LSP to detect the named entities. We use only a POS tagger and a simple noun phrase chunker, not a full parser, to extract more robust contextual features of a named entity. As a result, our method is more robust and practical than ones using a full parser or a named entity dictionary. This paper experimentally shows how large a labeled corpus and which unlabeled corpus is necessary for our method to compete with the supervised method. With only a quarter of labeled corpus, we could produce performance comparable to the supervised methods.
