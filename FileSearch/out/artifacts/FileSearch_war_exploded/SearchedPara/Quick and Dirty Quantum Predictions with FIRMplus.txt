 In this paper, we describe our approach to the Quantum Physics classification problem posed in KDD Cup 2004. We used our FIRMplus recursive partitioning random tree approach to classify the two particle types. We spent almost no time tuning parameters, which made our 3 rd place finish somewhat surprising. FIRM, random FIRM, recursive partitioning. company has developed is a com putational engine based on the Formal Inference-based Recurs ive Modeling (FIRM) recursive partitioning methodology [2]. In our extended version of FIRM, FIRMplus TM , we have included a very fast segmenting algorithm, as well as a multiple random tree prediction capability. Successful applications of FIRM abound in the sciences in such diverse areas as archaeology, cheminformatics, environmetrics, geology, bioinformatics, sociology, and zoology [6,9-11]. We were pleasantly surprised at our predic tion performance for this physics task considering our lack of subject matter knowledge, and how little we customized our approach to this problem. Early work on FIRM dates back to the 1970 X  X  where the algorithms were laid out by Hawkins [5] on how to optimally segment data. FIRM was released in 1982, with significant advances made by Hawkins and Musser over the years [3,4,7,8]. With a dependent n -vector y , and a covariate vector x , there are minimize the sum of squared deviations from the mean within each segment. The FIRM approach uses dynamic programming to find the optimal k -way segmenting, splits the data into the k subsets, and then recursivel y segments each subset until a stopping criterion is reached. The stopping criterion is based on a hypothesis test that determines the probability that the means or proportions of the dependent vector are statistically equivalent between segments. We have extended the original FIRM approach to use the appropriate binary log-likelihood model [4]. We used the NextFIRM multiple tree prediction approach developed by Musser [3,8], and we have devi sed a proprietary probabilistic dynamic programming approach to reduce the time of the Hawkins segmenting algorithm to Some characteristics of the FIRM approach that differentiate it from other recursive partitioning a pproaches such as CART [1] are the ability to find optimal multiway splits for continuous or ordinal data, a method for handli ng  X  X redictive missingness X  via a floating category for missing data, as well as its formal statistical hypothesis testing approach to model building. There was little chance of overfitting with our approach. We performed a single experiment to determine if binary splits gave better predictions than 3-way splits. We used 50% of the training data (25,000 observations) to build a model, and used the other 50% to validate the model. We used the 78 provided covariates with their associated missing values in the model building, and generated two sets of 100 random trees, one with 2-way splits and one with 3-way splits. The random tree generation approach is to select at random from among the top 10 statistically significant variables at each node in the tree and split using that vari able. Recursively within each daughter node, the best splits are found and one is chosen at random, and the process continues until there are no statistically significant splits to be performed at the leaf nodes. Since we have found in the past that overbuilding the trees gives the best prediction results, we used a lax Bonferroni-adjusted p-value threshold of p&lt;0.99 as a stopping criteria for tree-building. A prediction for a tree is the mean of the observations of the leaf node in which an observation falls. To get a multiple tree prediction for a single observation, we use the covariates of the observation to drop it down each tree and find the leaf node in which it falls. We then average the means of those leaf nodes from the training set to get the prediction for the test observation. Because the response in this case is binary, the resulting prediction ranges from [0...1]. We did no post-processing of results. We found that 3-way splits outperformed 2-way splits, categorizing about 0.5% more observations correctly in the test set. We then used the entire set of 50,000 training examples and built 40 random trees with 3-way splits and applied the resulting trees to predict the 100,000 unknow n contest observations. We submitted the same predictions for all 4 scoring criteria. The prediction accuracy we attained within our 50-50 train-test sampling of the training set was no different than the results we achieved when extrapolating our model to the final test set of 100,000 particles, about 72.8%. Could we have done better if we built more trees, created additional covariates that were sums or products of the original 78, or tuned our parameters some more? Perhaps, but the bottom line is that with a quick and dirty default set of parameters, our results ranked with the best pr ediction attempts even though we knew virtually nothing about the problem domain and spent only a couple of hours on the problem. The winners are to be commended for squeezing out the last epsilon in prediction possibility this data afforded. We nevertheless feel a sense of accomplishment in earning 3 rd place with an off-the-shelf solution. [1] Breiman, L.; Friedman, J.H.; Olshen, R. A.; Stone, C. J. [2] Hawkins, D. M. and G. V. Kass (1982).  X  X utomatic [3] Hawkins, D. M. a nd Musser, B. J., (1999),  X  X ne tree or a [4] Hawkins, D. M. (2001)  X  X itti ng multiple change-points to [5] Hawkins, D.M. (1972)  X  X n the choice of segments in [6] Hawkins, D.M. (2003)  X  X ivid e and model X , Scientific [7] Hawkins. D. M., and Musser, B. J. (2001),  X  X eature selection [8] Musser, B. J. (1999)  X  X xtensi ons to Recursive Partitioning X , [9] Rusinko, A. III; Farmen, M.W.; Lambert, C.G.; Brown, P.L. [10] Young, S.S.; Ekins, S. and La mbert, C.G. (2002)  X  X o many [11] Young, S.S.; Gombar, V.K.; Empt age, M.R.; Cariello, N.F. Christophe G. Lambert is the President and CEO of Golden Helix Inc. He earned his Ph.D. in computer science from Duke University in 1997. James E. Grover is a Scientific Applications Programmer at Golden Helix. He earned his M. S. in Applied Mathematics from the California Institute of Technology in 1972. 
