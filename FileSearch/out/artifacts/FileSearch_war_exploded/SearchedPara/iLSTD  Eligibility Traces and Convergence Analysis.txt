 A key part of many reinforcement learning algorithms is a policy evaluation process, in which the value function of a policy is estimated online from data. In this paper, we consider the problem updated after each time step.
 Temporal difference (TD) learning is a common approach to this problem [Sutton, 1988]. The TD algorithm updates its value-function estimate based on the observed TD error on each time step. The TD update takes only O ( n ) computation per time step, where n is the number of fea-tures. However, because conventional TD methods do not make any later use of the time step X  X  data, they may require a great deal of data to compute an accurate estimate. More recently, LSTD [Bradtke and Barto, 1996] and its extension LSTD(  X  ) [Boyan, 2002] were introduced as al-ternatives. Rather than making updates on each step to improve the estimate, these methods maintain compact summaries of all observed state transitions and rewards and solve for the value function which has zero expected TD error over the observed data. However, although LSTD and LSTD(  X  ) impractical for the large feature sets needed in many applications. Hence, practitioners are often faced with the dilemma of having to chose between excessive computational expense and excessive data expense.
 Recently, Geramifard and colleagues [2006] introduced an incremental least-squares TD algorithm, iLSTD , as a compromise between the computational burden of LSTD and the relative data ineffi-ciency of TD. The algorithm focuses on the common situation of large feature sets where only a small number of features are non-zero on any given time step. iLSTD X  X  per-time-step computational rate of learning similar to that of LSTD.
 In this paper, we substantially extend the iLSTD algorithm, generalizing it in two key ways. First, LSTD(  X  ) algorithms. We show that, under the iLSTD assumptions, the per-time-step computational complexity of this algorithm remains linear in the number of features. Second, we generalize the feature selection mechanism. We prove that for a general class of selection mechanisms, iLSTD(  X  ) converges to the same solution as TD(  X  ) and LSTD(  X  ), for all 0  X   X   X  1 . Reinforcement learning is an approach to finding optimal policies in sequential decision mak-ing problems with an unknown environment [ e.g. , see Sutton and Barto, 1998]. We focus on the class of environments known as Markov decision processes (MDPs). An MDP is a tuple, tion occurs, and  X   X  [0 , 1] is a discount rate parameter. A trajectory of experience is a sequence s sitioning to s 2 before taking a 2 , etc.
 Given a policy, one often wants to estimate the policy X  X  state-value function, or expected sum of discounted future rewards: In particular, we are interested in approximating V  X  using a linear function approximator. Let  X  : S  X &lt; n , be some features of the state space. Linear value functions are of the form than k n . Sparse feature representations are quite common as a generic approach to handling non-linearity [ e.g. , Stone et al. , 2005]. 1 2.1 Temporal Difference Learning the computation of a  X  -return , R  X  t ( V ) , at each time step: Note that the  X  -return is a weighted sum of k -step returns, each of which looks ahead k steps summing the discounted rewards as well as the estimated value of the resulting state. The  X  -return forms the basis of the update to the value function parameters: where  X  t is the learning rate. This  X  X orward view X  requires a complete trajectory to compute the  X  -return and update the parameters. The  X  X ackward view X  is a more efficient implementation that depends only on one-step returns and an eligibility trace vector: is guaranteed to converge [Tsitsiklis and Van Roy, 1997]. 2.2 Least-Squares TD Least-squares TD (LSTD) was first introduced by Bradtke and Barto [1996] and later extended with  X  -returns by Boyan [2002]. LSTD(  X  ) can be viewed as immediately solving for the value function parameters which would result in the sum of TD updates over the observed trajectory being zero. Let  X  t (  X  ) be the sum of the TD updates through time t . If we let  X  t =  X  ( s t ) then, Since we want to choose parameters such that the sum of TD updates is zero, we set Equation 1 to zero and solve for the new parameter vector, The online version of LSTD(  X  ) incorporates each observed reward and state transition into the b vector and the A matrix and then solves for a new  X  . Notice that, once b and A are updated, the experience tuple can be forgotten without losing any information. Because A only changes by a small amount on each time step, A  X  1 can also be maintained incrementally. The computation 2.3 iLSTD iLSTD was recently introduced to provide a balance between LSTD X  X  data efficiency and TD X  X  time idea is to maintain the same A matrix and b vector as LSTD, but to only incrementally solve for  X  . updates only single dimensions of  X  , each of which requires O ( n ) . By updating m parameters of  X  , which is a parameter that can be varied to trade off data and computational efficiency, iLSTD requires O ( mn + k 2 ) per time step, which is linear in n . The result is that iLSTD can scale to much larger feature spaces than LSTD, while still retaining much of its data efficiency. Although the original formulation of iLSTD had no proof of convergence, it was shown in synthetic domains to perform nearly as well as LSTD with dramatically less computation.
 we additionally prove sufficient conditions for convergence. The iLSTD(  X  ) algorithm is shown in Algorithm 1. The new algorithm is a generalization of the Line 5 updates z , and lines 5 X 9 incrementally compute the same A t , b t , and  X  t as described in Equation 1. Second, the dimension selection mechanism has been relaxed. Any feature selection iLSTD algorithm can be recovered by simply setting  X  to zero and selecting features according to the dimension of  X  with maximal magnitude.
 We now examine iLSTD(  X  ) X  X  computational complexity. Algorithm 1 : iLSTD(  X  ) Complexity 0 s  X  s 0 , z  X  0 , A  X  0 ,  X   X  0 , t  X  0 1 Initialize  X  arbitrarily 2 repeat 3 Take action according to  X  and observe r , s 0 4 t  X  t + 1 5 z  X   X  X  z +  X  ( s ) O ( n ) 6  X  b  X  z r O ( n ) 7  X  A  X  z (  X  ( s )  X   X   X  ( s 0 )) T O ( kn ) 8 A  X  A +  X  A O ( kn ) 9  X   X   X  +  X  b  X  ( X  A )  X  O ( kn ) 10 for i from 1 to m do 11 j  X  choose an index of  X  using some feature selection mechanism 12  X  j  X   X  j +  X  X  j O (1) 13  X   X   X   X   X  X  j A e j O ( n ) 14 end for 15 s  X  s 0 16 end repeat Theorem 1 Assume that the feature selection mechanism takes O ( n ) computation. If there are algorithm requires O (( m + k ) n ) computation per time step.
 Proof Outside of the inner loop, lines 7 X 9 are the most computationally expensive steps of iLSTD(  X  ). Since we assumed that each feature vector has at most k non-zero elements, and the plexity remains unchanged from iLSTD with the most expensive lines being 11 and 13. Because  X  the algorithm X  X  per-time-step computational complexity is O (( m + k ) n ) . We now consider the convergence properties of iLSTD(  X  ). Our analysis follows that of Bertsekas and Tsitsiklis [1996] very closely to establish that iLSTD(  X  ) converges to the same solution that TD(  X  ) does. However, whereas in their analysis they considered C t and d t that had expectations that converged quickly, we consider C t and d t that may converge more slowly, but in value instead of expectation.
 R On every round, C t and d t are selected first, followed by R t . Define F t to be the state of the to prove convergence of y t , we assume that there is a C  X  , d  X  , v ,  X  &gt; 0 , and M such that: Theorem 2 Given the above assumptions, y t converges to  X  ( C  X  )  X  1 d  X  with probability 1. The proof of this theorem is included in the additional material and will be made available as a companion technical report. Now we can map iLSTD(  X  ) on to this mathematical model: The final assumption defines the simplest possible feature selection mechanism sufficient for con-vergence, viz. , uniform random selection of features.
 Theorem 3 If the Markov decision process is finite, iLSTD(  X  ) with a uniform random feature selec-tion mechanism converges to the same result as TD(  X  ).
 Although this result is for uniform random selection, note that Theorem 2 outlines a broad range of possible mechanisms sufficient for convergence. However, the greedy selection of the original iLSTD algorithm does not meet these conditions, and so has no guarantee of convergence. As we will see in the next section, though, greedy selection performs quite well despite this lack of asymptotic guarantee. In summary, finding a good feature selection mechanism remains an open research question.
 As a final aside, one can go beyond iLSTD(  X  ) and consider the case where R t = I , i.e. , we take a step in all directions at once on every round. This does not correspond to any feature selection mechanism and in fact requires O ( n 2 ) computation. However, we can examine this algorithm X  X  rate of convergence. In particular we find it converges linearly fast to LSTD(  X  ).
 This may explain why iLSTD(  X  ) X  X  performance, despite only updating a single dimension, ap-proaches LSTD(  X  ) so quickly in the experimental results in the next section. We now examine the empirical performance of iLSTD(  X  ). We first consider the simple problem introduced by Boyan [2002] and on which the original iLSTD was evaluated. We then explore the larger mountain car problem with a tile coding function approximator. In both problems, we compare TD(  X  ), LSTD(  X  ), and two variants of iLSTD(  X  ). We evaluate both the random feature selection mechanism ( X  X LSTD-random X ), which is guaranteed to converge, 4 as well as the original iLSTD feature selection rule ( X  X LSTD-greedy X ), which is not. In both cases, the number of dimensions the proof X  X  assumption. which is also consistent with Boyan X  X  original experiments. Figure 2: Performance of various algorithms in Boyan X  X  chain problem with 6 different lambda values. Each line represents the averaged error over last 100 episodes after 100, 200, and 1000 episodes respectively. Results are also averaged over 30 trials. 5.1 Boyan Chain Problem The first domain we consider is the Boyan chain problem. Figure 1(a) shows the Markov chain together with the feature vectors corresponding to each state. This is an episodic task where the except from state 2 to 1 and state 1 to 0, where the rewards are -2 and 0, respectively. Figure 2 shows the comparative results. The horizontal axis corresponds to different  X  values, Note that in this domain, the optimum solution is in the space spanned by the feature vectors:  X  200, and 1000 episodes over the same set of observed trajectories based on 30 trials. As expected, LSTD(  X  ) requires the least amount of data, obtaining a low average error after only 100 episodes. With only 200 episodes, though, the iLSTD(  X  ) methods are performing as well as LSTD(  X  ), and totic guarantee, is actually performing slightly better than iLSTD-Random(  X  ) for all cases of  X  . Although  X  did not play a significant role for LSTD(  X  ) which matches the observation of Boyan [Boyan, 1999],  X  &gt; 0 does show an improvement in performance for the iLSTD(  X  ) methods. Table 1 shows the total averaged per-step CPU time for each method. For all methods sparse ma-trix optimizations were utilized and LSTD used the efficient incremental inverse implementation. illustrate the effect of a larger and more interesting feature space where this ratio is larger. Figure 3: Performance of various methods in mountain car problem with two different lambda values. LSTD was run only every 100 episodes. Results are averaged over 30 trials. 5.2 Mountain Car Our second test-bed is the mountain car domain [ e.g. , see Sutton and Barto, 1998]. Illustrated in forward, accelerate backward, and coast. The observation is a pair of continuous values: position about the mountain car problem are available online [RL Library, 2006]. As we are focusing on velocity, although the environment is stochastic and the chosen action is replaced with a random approximator. We used ten tilings ( k = 10 ) over the combination of the two parameter sets and in the Boyan chain domain.
 200,000 episodes of interaction with the environment.
 While some iterations performed competitively with LSTD(  X  ), others performed extremely poorly with little show of convergence. Hence, we did not include the performance line in the figure. This fact may suggest that the greedy feature selection mechanism does not converge, or it may simply the two graphs cannot be directly compared.
 provement of iLSTD(  X  ) over LSTD as can be see in Table 1. Again sparse matrix optimizations were utilized and LSTD(  X  ) used the efficient incremental ivnerse implementation. The computa-When the feature representation is sparse, though, iLSTD(  X  ) can still achieve results competitive with LSTD(  X  ) using computation more on par with the time efficient TD(  X  ). In this paper, we extended the previous iLSTD algorithm by incorporating eligibility traces without increasing the asymptotic per time-step complexity. This extension resulted in improvements in performance in both the Boyan chain and mountain car domains. We also relaxed the dimension selection mechanism of the algorithm and presented sufficient conditions on the mechanism under which iLSTD(  X  ) is guaranteed to converge. Our empirical results showed that while LSTD(  X  ) can while having similar performance to LSTD.
 This work opens up a number of interesting directions for future study. Our results have focused on two very simple feature selection mechanisms: random and greedy. Although the greedy mechanism does not meet our sufficient conditions for convergence, it actually performed slightly better on the examined domains than the theoretically guaranteed random selection. It would be interesting to perform a thorough exploration of possible mechanisms to find a mechanism with both good empirical performance while satisfying our sufficient conditions for convergence. In addition, it would be interesting to apply iLSTD(  X  ) in even more challenging environments where the large number of features has completely prevented the least-squares approach, such as in simulated soccer keepaway [Stone et al. , 2005].
 [Bertsekas and Tsitsiklis, 1996] Dmitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Pro-gramming . Athena Scientific, 1996. [Boyan, 1999] Justin A. Boyan. Least-squares temporal difference learning. In Proceedings of the Sixteenth International Conference on Machine Learning , pages 49 X 56. Morgan Kaufmann, San
Francisco, CA, 1999. [Boyan, 2002] Justin A. Boyan. Technical update: Least-squares temporal difference learning. Ma-chine Learning , 49:233 X 246, 2002. [Bradtke and Barto, 1996] S. Bradtke and A. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning , 22:33 X 57, 1996. [Geramifard et al. , 2006] Alborz Geramifard, Michael Bowling, and Richard S. Sutton. Incremental least-squares temporal difference learning. In Proceedings of the Twenty-First National Confer-ence on Artificial Intelligence (AAAI) , pages 356 X 361. AAAI Press, 2006. [RL Library, 2006] RL Library. The University of Alberta reinforcement learning library. http: //rlai.cs.ualberta.ca/RLR/environment.html , 2006. [Stone et al. , 2005] Peter Stone, Richard S. Sutton, and Gregory Kuhlmann. Reinforcement learn-ing for robocup soccer keepaway. International Society for Adaptive Behavior , 13(3):165 X 188, 2005. [Sutton and Barto, 1998] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction .
MIT Press, 1998. [Sutton, 1988] Richard S. Sutton. Learning to predict by the methods of temporal differences.
Machine Learning , 3:9 X 44, 1988. [Sutton, 1996] Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems 8 , pages 1038 X 1044. The MIT Press, 1996. [Tsitsiklis and Van Roy, 1997] John N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control ,
