 Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. De-pending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system X  X  training data), some translation outputs can be per-fect, while others are ungrammatical, missing im-portant words or even totally garbled. As a result, users do not know whether they can trust the trans-lation output unless they spend time to analyze the MT output. This shortcoming is one of the main obstacles for the adoption of MT systems, especially in machine assisted human translation: MT post-editing, where human translators have an option to edit MT proposals or translate from scratch. It has been observed that human trans-lators often discard MT proposals even if some are very accurate. If MT proposals are used prop-erly, post-editing can increase translators produc-tivity and lead to significant cost savings. There-fore, it is beneficial to provide MT confidence es-timation, to help the translators to decide whether to accept MT proposals, making minor modifica-tions on MT proposals when the quality is high or translating from scratching when the quality is low. This will save the time of reading and parsing low quality MT and improve user experience.
In this paper we propose an adaptive qual-ity estimation that predicts sentence-level human-targeted translation error rate (HTER) (Snover et al., 2006) for a document-specific MT post-editing system. HTER is an ideal quality measurement for MT post editing since the reference is ob-tained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly im-prove the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lex-ical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Spe-cia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be in-consistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to users.

In contrast to traditional static MT quality es-timation methods, our approach not only trains the MT quality estimator dynamically for each document-specific MT model to obtain higher pre-diction accuracy, but also achieves consistency over different document-specific MT models. The experiments show that our MT quality estima-tion is highly correlated with human judgment and helps translators to increase the MT proposal adoption rate in post-editing.

We will review related work on MT quality es-timation in section 2. In section 3 we will intro-duce the document-specific MT system built for post-editing. We describe the static quality estima-tion method in section 4, and propose the adaptive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discus-sion and conclusion in section 7. There has been a long history of study in con-fidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incor-porating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) pro-posed various regression models to predict the ex-pected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are ex-ploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010).

Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the  X  X uality Estimation X  shared task. Bic  X ici et al. (2013) proposes a number of features mea-suring the similarity of the source sentence to the source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) in-vestigates the impact of a large set of linguisti-cally inspired features on quality estimation accu-racy, which are not able to outperform the shal-lower features based on word statistics. Gonz  X  alez-Rubio et al. (2013) proposed a principled method for performing regression for quality estimation using dimensionality reduction techniques based on partial least squares regression. Given the fea-ture redundancy in MT QE, their approach is able to improve prediction accuracy while significantly reducing the size of the feature sets. In our MT post-editing setup, we are given docu-ments in the domain of software manuals, techni-cal outlook or customer support materials. Each translation request comes as a document with sev-eral thousand sentences, focusing on a specific topic, such as the user manual of some software.
The input documents are automatically seg-mented into sentences, which are also called seg-ments . Thus in the rest of the paper we will use sentences and segments interchangeably. Our par-allel corpora includes tens of millions of sentence pairs covering a wide range of topics. Building a general MT system using all the parallel data not only produces a huge translation model (unless with very aggressive pruning), the performance on the given input document is suboptimal due to the unwanted dominance of out-of-domain data. Past research suggests using weighted sentences or cor-pora for domain adaptation (Lu et al., 2007; Mat-soukas et al., 2009; Foster et al., 2010). Here we adopt the same strategy, building a document-specific translation model for each input docu-ment.

The document-specific system is built based on sub-sampling: from the parallel corpora we se-lect sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, di-vided by the input sentence X  X  length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT system building: word align-ment (HMM (Vogel et al., 1996) and MaxEnt (It-tycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Itty-cheriah and Roukos, 2007) and LM model train-ing. The top region within the dashed line in Fig-ure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Alto-gether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-of-the-art translation performance in vari-ous Arabic-English translation evaluations (NIST MT2008, GALE and BOLT projects). MT quality estimation is typically formulated as a prediction problem: estimating the confidence score or translation error rate of the translated sen-tences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the per-centage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER inter-changably.

In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. 4.1 Features for MT QE The features for quality estimation should reflect the complexity of the source sentence and the de-coding process. Therefore we conduct syntactic analysis on the source sentences, extract features from the decoding process and select the follow-ing 26 features:  X  17 decoding features, including phrase  X  Sentence length, i.e., the number of words in  X  Source sentence syntactic features, including  X  The length of verb phrases, because verbs are  X  The maximum length of source phrases in  X  The number of phrase pairs with high fuzzy  X  The average translation probability of the
The first 17 features come from the decod-ing process, which are called  X  X ecoding features X . The remaining 9 features not related to the de-coder are called  X  X xternal features X . To evaluate the effectiveness of the proposed features, we train various classifiers with different feature configura-tions to predict whether a translation output is use-ful (with lower TER) as described in the following section. 4.2 MT QE as Classification Predicting TER with various input features can be treated as a regression problem. However for the post-editing task, we argue that it could also be cast as a classification problem: MT system 17 decoding features only 89% 79% 9 external features only 85% 81% Table 1: QE classification accuracy with different feature configurations users (including the translators) are often inter-ested to know whether a given translation is rea-sonably good or not. If useful, they can quickly look through the translation and make minor mod-ifications. On the other hand, they will just skip reading and parsing the bad translation, and prefer to translate by themselves from scratch. Therefore we also develop algorithms that classify the trans-lation at different levels, depending on whether the TER is less than a given threshold. In our experi-ments, we set TER=0.1 as the threshold.

We randomly select one input document with 2067 sentences for the experiment. We build a document-specific MT system to translate this document, then ask human translator to correct the translation output. We compute TER for each sentence using the human correction as the refer-ence. The TER of the whole document is 0.31, which means about 30% errors should be cor-rected. In the classification task, our goal is to pre-dict whether a sentence is a Good translation (with TER  X  0.1), and label them for human correction. We adopt a decision tree-based classifier, experi-menting with different feature configurations. We select the top 1867 sentences for training and the bottom 200 sentences for test. In the test set, there are 46 sentences with TER  X  0.1. Table 1 shows the classification accuracy.

First we can see that as the overall TER is around 0.3, predicting all the sentences being neg-ative already has a strong baseline: 77%. How-ever this is not helpful for the human translators, because that means they have to translate every sentence from scratch, and consequently there is no productivity gain from MT post-editing. If we only use the 17 decoding features, it improves the classification accuracy by 9% on the training set, but only 2% on the test set. This is probably due to the overfitting when training the decision tree clas-sifier. While using the 7 external features, the gain on training set is less but the gain on the test set is greater (4% improvement), because the trans-lation output is generated based on the log-linear combination of these decoding features, which are biased towards the final translations. The exter-nal features capture the syntactic structure of the source sentence, as well as the coverage of the training data with regard to the input sentence, which are good indicators of the translation qual-ity. Combining both the decoding features and the external features, we observed the best accuracy on both the training and test set. We will use the combined 26 features in the following work. 4.3 MT QE as Regression For the QE regression task, we predict the TER for each sentence translation using the above 26 fea-tures. We experiment with several classifiers: lin-ear regression model, decision tree based regres-sion model and SVM model. With the same train-ing and test data set up, we predict the TER for each sentence in the test set, and compute the cor-relation coefficient ( r ) and root mean square error (RMSE). Our experiments show that the decision tree-based regression model obtains the highest correlation coefficients (0.53) and lowest RMSE (0.23) in both the training and test sets. We will use this model for the adaptive MT QE in the fol-lowing work. The above QE regression model is trained on a portion of the sentences from the input document, and evaluated on the remaining sentences from the same document. One would like to know whether the trained model can achieve consistent TER pre-diction accuracy on other documents. When we use the cross-document models for prediction, the correlation is significantly worse (the details are discussed in section 6.1). Therefore it is neces-sary to build a QE regression model that X  X  robust to different document-specific translation models. To deal with this problem, we propose this adap-tive MT QE method described below.

Our proposed method is as follows: we select a fixed set of sentence pairs ( S q , R q ) to train the QE model. The source side of the QE training data S q is combined with the input document S d for MT system training data subsampling. Once the document-specific MT system is trained, we use it to translate both the input document and the source QE training data, obtaining the translation T d and Figure 2: Correlation coefficient r between pre-dicted TER (x-axis) and true TER (y-axis) for QE models trained from the same document (top fig-ure) or different document (bottom figure). T q . We compute the TER of T q using R q as the reference, and train a QE regression model with the 26 features proposed in section 4.1. Then we use this document-specific QE model to predict the TER of the document translation T d . As the QE model is adaptively re-trained for each document-specific MT system, its prediction is more accurate and consistent. Figure 1 shows the flow of our MT system with the adaptive QE training integrated as part of the built. In this section, we first discuss experiments that compare adaptive QE method and static QE method on a few documents, and then present results we obtained after deploying the adaptive QE method in an English-to-Japanese MT Post-Editing project. As mentioned before, the main motivation for us to develop MT QE classification scheme is that translators often discard good MT proposals and translate the segments from scratch. We would like to provide translators with some guidance on reasonably good MT proposals X  X he sentences with low TERs X  X o help them increase the leverage on MT proposals to achieve improved productivity. 6.1 Evaluation on Test Set Our experiment and evaluation is conducted over three documents, each with about 2000 segments. We first build document-specific MT model for each document, then ask human translators to cor-rect the MT outputs and obtain the reference trans-lation. In a typical MT QE scenario, the QE model is pre-trained and applied to various MT outputs, even though the QE training data and MT out-puts are generated from different translation mod-els. To evaluate whether such model mismatch matters, we compare the cross-model QE with the same-model QE, where the QE training data and the MT outputs are generated from the same MT model.

We select one document LZA with 2067 sen-tences. We use the first 1867 sentences to train the static QE model and the remaining 200 sentences are used as test set for TER prediction. We com-pute the correlation coefficient ( r ) between each predicted TER and true TER, as shown in Figure 2. We find that the TER predictions are reason-ably correct when the training and test sentences are from the same MT model (the top figure), with correlation coefficients around 0.5. For the cross-model QE, we train a static QE model with 1867 sentences from another document RTW, and use it to predict the TER of the same 200 sentences from document LZA (the bottom figure). We observe significant degradation of correlation coefficient, dropping from 0.5 to 0.1. This degradation and unstable nature is the prime motivation to develop a more robust MT quality estimation model.

We select 1700 sentences from multiple pre-viously translated documents as the QE training data, which are independent of the test documents. We train the static QE model with this training set, including the source sentences, references and MT outputs (from multiple translation models). To train the adaptive QE model for each test docu-ment, we build a translation model whose subsam-pling data includes source sentences from both the test document and the QE training data. We trans-late the QE source sentences with this newly built MT model, and the translation output is used to train the QE model specific to each test document. We compare these two QE models on three doc-uments, LZA, RTW and WC7, measuring r and RMSE for each QE model. The result is shown in Table 2. We find that the adaptive QE model demonstrates higher r and lower RMSE than the static QE model for all the test documents.
Besides the general correlation with human judgment, we particularly focus on those reason-ably good translations, i.e., the sentences with low TERs which can help improve the translator X  X  pro-ductivity most. Here we report the precision, re-call and F-score of finding such  X  Good  X  sentences (with TER  X  0.1) on the three documents in Ta-ble 3. Again, the adaptive QE model produces higher recall, mostly higher precision, and signif-icantly improved F-score. The overall F-score of the static QE model X  X  0.17 F-score, this is rela-tively 64% improvement.

In the adaptive QE model, the source side QE training data is included in the subsampling pro-cess to build the document-specific MT model. It would be interesting to know whether this process will negatively affect the MT quality. We evaluate the TER of MT outputs with and without the adap-tive QE training on the same three documents. As seen in Table 4, we do not notice translation qual-ity degradation. Instead, we observe slightly im-provement on two document, with TERs reduction by 0.1-0.4 pt. As our MT model training data in-clude proprietary data, the MT performance is sig-nificantly better than publicly available MT soft-ware. 6.2 Impact on Human Translators We apply the proposed adaptive QE model to large scale English-to-Japanese MT Post-Editing project on 36 documents with 562K words. Each English sentence can be categorized into 3 classes:  X  Exact Match ( EM ): the source sentence is  X  Fuzzy Match ( FM ): the source sentence is  X  No Proposal ( NP ): there is no close match
EM sentences are excluded from the study be-cause in general they do not require editing. We we present the precision, recall and F-score of the  X  X ood X  sentences in the FM and NP categories, similar to those shown in Table 3. We consistently observe higher performance on the FM sentences, in terms of precision, recall and F-score. This is expected because these sentences are well covered in the training data. The overall F-score is in line with the test set results shown in Table 3.
We are also interested to know whether the pro-posed adaptive QE method is helpful to human translators in the MT post-editing task. Based on the TERs predicted by the adaptive QE model, we assign each MT proposal with a confidence label: High (0  X  TER  X  0.2), Medium (0.2 &lt; TER  X  0.3), or Low (TER &gt; 0.3). We present the MT pro-posals with confidence labels to human translators, then measure the percentage of sentences whose MT proposals are used. From Table 6 and 7, we can see that sentences with High and Medium confidence labels are more frequently used by the translators than those with Low labels, for both the FM and NP categories. The MT usage for the FM category is less than that for the NP category be-cause translators can choose FM proposals instead of the MT proposals for correction.

We also measure the translator X  X  productivity gain for MT proposals with different confidence TER-with Adaptive QE 30.69 30.78 29.56 Table 4: MT Quality with and without Adaptive QE measured by TER labels. The productivity of a translator is defined as the number of source words translated per unit time. The post editing tool, IBM TranslationMan-ager, records the time that a translator spends on a segment and computes the number of characters that a translator types on the segment so that we can compute how many words the translator has finished in a given time.

We choose the overall productivity of NP0 as the base unit 1, where there is no proposal presents and the translator has to translate the segments from scratch. Measured with this unit, for exam-ple, the overall productivity of FM0 being 1.14 implies a relative gain of 14% over that of NP0, which demonstrates the effectiveness of FM pro-posals.

Table 6 and 7 also show the productivity gain on sentences with High, Medium and Low labels from FM and NP categories. Again, the produc-tivity gain is consistent with the confidence labels from the adaptive QE model X  X  prediction. The overall productivity gain with confidence-labeled MT proposals is about 10% (comparing FM1 vs. FM0 and NP1 vs. NP0). These results clearly demonstrate the effectiveness of the adaptive QE model in aiding the translators to make use of MT proposals and improve productivity. Table 6: MT proposal usage and productivity gain in FM category. Table 7: MT proposal usage and productivity gain in NP category. Table 5: Performance on predicting Good sen-tences (TER  X  0.1) by adaptive QE model In this paper we proposed a method to adaptively train a quality estimation model for document-specific MT post editing. With the 26 pro-posed features derived from decoding process and source sentence syntactic analysis, the proposed QE model achieved better TER prediction, higher correlation with human correction of MT output and higher F-score in finding good translations. The proposed adaptive QE model is deployed to a large scale English-to-Japanese MT post edit-ing project, showing strong correlation with hu-man preference and leading to about 10% gain in human translator productivity.

The training data for QE model can be selected independent of the input document. With such fixed QE training data, it is possible to measure the consistency of the trained QE models, and to al-low the sanity check of the document-specific MT models. However, adding such data in the sub-sampling process extracts more bilingual data for building the MT models, which slightly increase the model building time but increased the transla-tion quality. Another option is to select the sen-tence pairs from the MT system subsampled train-ing data, which is more similar to the input docu-ment thus the trained QE model could be a better match to the input document. However, the QE model training data is no longer constant. The model consistency is no longer guaranteed, and the QE training data must be removed from the MT system training data to avoid data contamina-tion.

