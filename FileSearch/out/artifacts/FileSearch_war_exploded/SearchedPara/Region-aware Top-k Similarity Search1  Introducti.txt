 With the popularity of global position systems (GPS) in smartphones, location-based services (LBS) have recently attracted significant attention from both academic and industrial communities. These services generate large amounts description. Traditional LBS services assume that people are more interested in the POIs around them. That is, the attractiveness of a POI depends on its spatial proximity from people. However, in most cases, POIs within a certain distance are all acceptable to users and they may concern more about other aspects. For example, a hospital around 2km may be more attractive than closer ones if it is more accordance with users X  requirements.
 given a set of spatio-textual objects, a spatial region and several tokens, finds k most textual-relevant objects falling in this region. This problem can be applied to many existing LBS systems. For example, in Google map, users are allowed to adjust their interesting regions through  X  X oom-in X ,  X  X oom-out X  and  X  X ove X  operation. No matter which position and size the selected region is, we always display k objects which locate in this region and are most relevant to the query tokens. Different from traditional spatial-keyword search problem, consider the possibility that users X  input may contain some typo-errors or misspellings, we knowledge, none of current solutions can efficiently support both top-k require-ment and fuzzy-token distance. [ 1 , 14 ] focus on range-based spatial search and two kinds of methods to support our problem (Section 2.3 ), they are quite inef-ficient since they use the spatial and textual pruning separately. LBAK-tree [ 1 ] andIRtree[ 4 ] focus on spatial keyword search. They embed textual index into each node of R-tree. However, LBAK-tree cannot address  X  X op-k X  and IRtree cannot support  X  X uzzy X  (e.g., edit distance). To address the limitation, we pro-pose a hybrid-landmark index (HLtree) which generates high quality landmarks to dynamically divide objects into hierarchical clusters. We further devise a priority-based method to efficiently support fuzzy-token distance. We summarize our main contributions as follows: (1) We propose a hybrid-landmark index which integrates the spatial and textual pruning seamlessly. (2) We explore a priority-based algorithm and extend it to support fuzzy-token distance. (3) We devise a cost model to evaluate the landmark quality and pro-pose a deletion-based method to generate high quality landmarks. (4) Extensive experiments show that our method outperforms state-of-the-art algorithms and achieves high performance.
 The rest of this paper is organized as follows. We formulate our problem in Section 2 and introduce HLtree in Section 3 . Section 4 discusses the land-mark selection. We show experiments in Section 5 and make a conclusion in Acknowledgement. We first formulate the problem of region-aware top-k similarity search in Section 2.1 , and then show some related work in Section 2.2 . We extend state-of-the-art algorithms to support our problem in Section 2.3 . 2.1 Problem Statements Consider a collection of objects R = { r 1 ,r 2 ,...,r includes a spatial location L r and textual description {L ,
T r } . We use the coordinate of an object to describe its spatial location, denoted by L r =[ L r .x, L r .y ]. And we use a set of tokens to capture the tex-tual description, denoted by T r = { t 1 ,t 2 ,...,t |T r (e.g., Gym, Hotel, Restaurant) or users X  interests (e.g., jogging, yoga, pilates). A region-aware top-k similarity query q = {M q , T q ,k } M , textual description T q and a top-k parameter k . The spatial region can be retrieved through the screen of a computer or a phone. Since users can adjust the position and size of the region using  X  X oom-in X ,  X  X oom-out X  and  X  X ove X  opera-tion, we only consider the textual similarity of objects falling in this region. token-based similarity to quantify the textual distance. There are two advan-requirement. Sometimes query may contain typos and misspellings. (2) Token-based similarity (e.g., overlap) helps us find the potential relationship of tokens regardless of the order. Each token can be considered as a separate condition (e.g, Jogging, Yoga). By mapping tokens between the query and data, we are able to measure how well the user X  X  requirements are satisfied by a record. Thus, in this paper, we combine the edit distance and the overlap metric as a fuzzy-top-k similarity search problem (Definition 2 ).
 Definition 1 (Fuzzy-token distance). Given two sets of tokens { t 1 ,t 2 ,..., t m its most similar token s j in T s which has the minimum edit distance, i.e.,  X  s  X  X  s ,Ed ( t i ,s j )  X  Ed ( t i ,s ) . For all those token pairs ( t of these edit distance as the fuzzy-token distance between Definition 2 (Region-aware top-k similarity search). Given a collection finds a subset of objects S = { r 1 ,r 2 ,...,r k } which satisfy: (1) |S| = k ,(2)  X  r  X  X  , we have L r  X  X  q (3)  X  r i  X  X  ,  X  r Example 1. Consider the ten spatio-textual objects in Figure 1 . Suppose query q is { [(10 , 8) , (30 , 24)] , (  X  X iate X ,  X  X niverst X  ) , 1 in the query region. We compare their fuzzy-token distance. Take r  X  X iate X , we calculate its edit distance to each token in T Ed (  X  X iate X , X  X ilates X  )=2 and Ed (  X  X iate X , X  X niversity X  )=9 .Thus,itsbest match token is  X  X ilates X  since they have the minimum edit distance 2 . For the second query token  X  X niverst X , its best match token in T r Ed (  X  X niverst X , X  X niversity X  )=3 . Thus, we add them to get its fuzzy-token dis-tance Dis ft ( T q , T r 10 )=2+3=5 . Similarly, we have Dis Dis ft ( T q , T r 4 )=11 and Dis ft ( T q , T r 7 )=10 .Thus, r 2.2 Related Work Range-Based Spatial Search. Many well-known data structures such as R-tree [ 8 ] can address this problem. It organizes objects in a hierarchical way. When a query q comes, it iteratively visits nodes from the root using a top-down pattern. At each node, only those child nodes whose MBRs have intersection with M can be added to the visiting list for further extension.
 Spatial Keyword Search. There are many studies on spatial keyword the top-k topic. They embed inverted index or signature files to every node and Rtree in different orders. [ 1 , 14 ] support fuzzy search. These methods can-not address our problem directly. For example, the language model in IRtree based on the exact match between tokens. It cannot support fuzzy search at the character level (e.g., edit distance).
 Top-k Textual Similarity Search. Though many works have studied sharing the same prefix. It first builds a trie for all the tokens and then incre-mentally search the trie nodes. At the beginning, we find all the trie nodes n which can completely match any query prefix p , i.e., Ed ( n, p )=0and p At each step, we iteratively extend pairs ( n, p ) to get the pairs ( n ,p ) satis-fying Ed ( n ,p )= Ed ( n, p ) + 1. (2) [ 11 , 13 ] propose a q-gram based method. We define  X  X -gram X  in Definition 3 . For example, the 3-gram set of  X  X ogging X  is {  X  X og X , X  X gg X , X  X gi X , X  X in X , X  X ng X  } , denoted by Q  X  Jogging . According to Lemma 1 , edit distance evaluation can be easily transformed to counting the overlap of Definition 3 (q-gram). Given a token t and a positive integer q , a q-gram of t , we get a q-gram set for token t , denoted by Q t .
 Lemma 1. Consider two token s and t ,if Ed ( s, t ) &lt; X  , we have max( | s | , | t | )  X  q +1  X  q  X   X  . 2.3 Straightforward Solutions Based on the two prevalent techniques in Section 2.2 ,weembedthetrietreeand q-gram based inverted index into Rtree to address our problem: (1) Rtree+trie: We first build a R-tree for the spatial components of all the objects. Then in each leaf node, we build a trie tree for all the objects falling in this region. Given a M and then search each embedded trie to find top-k similar objects. Finally we combine these candidates to get final results. (2) Rtree+q-gram: Similar to the Algorithm 1. HLtree construction ( R ,M ) former method, we build a q-gram based inverted index instead of trie. Given a query q , we search R-tree using M q to locate the leaf nodes. We then divide T q into q-gram sets and probe the corresponding inverted lists. By utilizing the heap structure, we can quickly calculate q-gram overlaps and select the objects appearing in more than |T q | X  q +1  X  q  X   X  inverted lists as candidates. In this section, we introduce a hybrid-landmark tree called HLtree. We first introduce the basic idea in Section 3.1 and then show the construction of HLtree in Section 3.2 . The query processing methods will be presented at Section 3.3 . 3.1 Basic Idea The two methods in Section 2.3 have several drawbacks: (1) Trie-based method incrementally extends the pair Ed ( n, p )= x to get the pairs Ed ( n ,p )= x +1. Suppose the query is  X  X estroom X  and current edit distance threshold is 3. Even if Ed ( X  X estaurant X , X  X estroom X ) = 5 &gt; 3, the algorithm still need to visit its whose edit distance is less than the k th result will be visited. (2) Q-gram based method uses Q T q to probe the inverted index. Though some q-gram sets have few For both methods, the spatial and textual components are organized separately which may involve large numbers of false-positives. When locating leaf nodes with the query region, if the intersection part is too small compared to the leaf MBR, the algorithms will waste much time in calculating the textual similarity for non-intersecting candidates. Besides, if tokens distribute uniformly, then each leaf node contains similar textual index. When the query region is large, it will repeatedly calculate textual similarity for every intersecting leaf node. However, these calculations can be reduced to only once if we utilize the textual index at a higher level. To overcome these drawbacks, we propose a hybrid-landmark based index called HLtree. The basic idea is to dynamically select a set of spatial-textual landmarks which can best represent the distribution of objects. At each level, we divide objects into partitions according to the hybrid proximity to the landmarks. In this way, similar objects can be mapped into the same group while dissimilar ones are put separately. Given a query q , we only need to compare q with these selected landmarks to avoid visiting extremely dissimilar groups. We first introduce the concept of the hybrid landmark.
 Definition 4 (Hybrid Landmark). Given a partition p with a set of objects R , its corresponding hybrid landmark L p is a triple t, mbr, rads where t is a selected token, mbr is the MBR of R p and rads is the minimum and max-imum edit distance of all the objects in R p compared with t , i.e., L [min 3.2 Construction of HLtree HLtree Construction. We build HLtree in a top-down pattern. As Algorithm 1 shows, we use a queue Q to maintain current generated partitions (nodes) and then iteratively divide them into smaller ones. At the beginning, all the objects R are taken as a whole partition (the root node). We then add the root node and its corresponding object list to Q for further partition (Line 3 ). At each step, we retrieve the top element n, R n from Q where R n corresponding to node n .Weuse M to limit the capacity of each node. If contains less than M objects, we mark n as a leaf node and build a q-gram based inverted index (Line 7 ). Otherwise, we generate a set of high quality landmarks n.

L which can best represent the distribution of R n (The details of generation method will be presented in Section 4 ). We initiate | n.
 keep newly-generated partitions. For each object r in R n landmark l r and add r to its corresponding object list P for each landmark l in n. L , we create a new node and take it as a child of n .We also add pair n l , P l r to queue Q for further partition (Line 15 -18 ). Q-gram Prefix Based Index Construction. A main challenge is to avoid completely counting the overlaps when searching the q-gram based inverted two tokens T r and T s with q-gram sets Q T r = { g 1 ,g Q = { g 1 ,g 2 ,...,g |T a global ordering in advance (e.g., tf-idf). As Lemma 1 shows, if Ed ( we have | Q T r  X  Q T s | X  max( |T r | , |T s | )  X  q +1  X  last |T r | X  q  X  q  X   X  q-grams from Q T preserve the overlap constraint, i.e., Q T r [1 ..q X  +1]  X  if Q T r [1 ..q X  +1]  X  Q T s [1 ..q X  +1] =  X  , Ed ( T r , for any q-gram prefixes Q T r [1 ..i ]and Q T s [1 ..j ], if Q can deduce a lower bound of  X  as Lemma 2 . Based on this analysis, we develop our q-gram based inverted index. Given an object r , for each token t in generate its q-gram set Q t = { g 1 ,g 2 ,...,g | t | X  q +1 add triple ( rid, tid, i  X  1 q ) to the inverted lists corresponding to g of edit distance if no candidates have intersection with Q Definition 5 (Q-gram prefix). Given a q-gram sets Q { g Q T r , denoted by Q T r [1 ..i ] .
 Lemma 2. Consider two q-gram prefixes Q T r [1 ..i ]= { g Q
T s [1 ..j ]= { g 1 ,g 2 ,...,g j } .If Q T r [1 ..i  X  1]  X  Example 2. Consider objects in Figure 1 . Suppose the node capacity is 3 ( M = marks {  X  X ilates X  ,M N 1 , [0 , 3] ,  X  X ilates X  ,M N 2 , [0 , 6] ,  X  X niverse X  ,M all its tokens. Take r 7 as an example, T r 7 = {  X  X niverse X , X  X tudio X  p 7 locates in MBR M N 2 and M L 2 .Since Ed (  X  X niverse X  ,N Ed (  X  X niverse X  ,L 2 .t )=0 ,wemap ( r 7 ,  X  X niverse X  ) to node L map ( r 7 ,  X  X tudio X  ) to N 2 . After allocation, L 1 and L objects and are taken as leaf nodes. For each token in leaf node, we generate its q-gram set and update the inverted lists. For example, Q r q-gram  X  X tu X , then we add r 6 / 1 / 0 to the inverted list of  X  X tu X  where 1 is the token position in r 6 and 0 is the estimated edit distance in Lemma 2 . For nodes contain more than 3 objects, we continue this procedure. 3.3 Query Processing of Hybrid-Landmark Tree We first discuss the algorithm for the query containing only one token and then extend it to support the query with multiple tokens.
 Algorithm for Single-token Query. Similar to all the hierarchical structures, HLtree explores a priority-first traverse strategy. A priority queue Q is used to keep the priority of each node. At first, Q is empty and we add root, 0 .We then iteratively retrieve the top element n from Q .If n is a leaf node, we search its q-gram based inverted index. Otherwise, for each child node, we check its MBR intersection and estimate its visiting priority. Child nodes which pass the filtering rules are added to Q for further extension. Since spatial constraint can be checked in O(1) time, we use the textual distance as the visiting priority. Because T q only contains one token, then the textual metric is simplified to edit distance which follows the triangle inequality: given three tokens have | Ed ( T a , T b )  X  Ed ( T b , T c ) | X  Ed ( T a , T c )  X  partition l i , only the token t satisfying | Ed ( l j .t, Ed ( l .t, T q )+ Ed ( T q ,t ) can be the answer. We use the current k estimate Ed ( T q ,t ). That is, only the tokens between max pruning. Consider two landmarks l i and l j .If T q is quite closer to l l .t , i.e., Ed ( T q .t, l i .t ) Ed ( T q .t, l j .t ), then the token near in partition l j since it is only mapped to the nearest landmark. Thus, we can deduce a safety edit distance bound for each partition l j the current k th result is less than max i { Ed ( T q ,l j whole partition l j .
 Another problem is how to efficiently find results in leaf n . As Lemma 2 shows, each q-gram prefix corresponds to an edit distance lower bound. We utilize this property to avoid visiting tokens with low values. Suppose the query q-gram set Q can find it when visiting the inverted lists of g 1 ,g 2 ,...,g distance of unvisited objects can not be less than i  X  1 q queue Q g to keep candidate q-grams and their estimated values. At first, for each q-gram in Q T q ,weadd g i , i  X  1 q to Q g as a non-visited q-gram. At each time, we retrieve the q-gram g t with minimum estimated value from Q from the query, then the next inverted list has not been explored yet. We find the inverted list corresponding to g t and add the first element to Q we calculate its real edit distance and update current results. Then we update Q by adding the next element in the same inverted list.
 Lemma 3. Consider any two landmarks l i , l j and a query q . Token t which Algorithm for Multiple-token Query. Inspired by the TA algorithm [ 6 ], we extend our framework to support query with multiple tokens. Suppose the query is T q = { t 1 ,t 2 ,...,t m } . Based on the method above, for each token t incrementally find the next object r which is closest to t metric. That is, we can dynamically defer an implicit visiting list for each token t according to the edit distance proximity, denoted by T q = these object lists and combine the candidates. The method is similar to the previous one (search in q-gram based inverted lists). We use a priority queue Q ta to keep the processing objects in each list and their corresponding scores. At the beginning, we add the first object in each list O i We then iteratively retrieve the top element from Q ta and update the results by calculating its fuzzy-token distance. For each popped token, we incrementally find its next closest object and add it to Q ta . Notice that we do not need to calculate the entire ordering of each token. Similar to TA, once the sum of visiting frontiers (i.e, elements in Q ta ) is larger than the current k can stop the algorithm. For constraint of space, we omit the details. We first analyze the effectiveness of a landmark selection. At each node n ,we n depends on the ratio of the query time reduction and the additional space |R (  X  ) | is the quantity of visiting objects. Otherwise, suppose n is divided into m non-filtered child nodes ( sum i ( p ( n i )  X  c R  X  X R ( n to visit n i ). The analysis of the additional space consumption is similar. For constraint of space, we omit this explanation ( I (  X  ) is the number of objects in the inverted index. c L is the space cost of a landmark and c of an object in the inverted index). We use the tightness degree of spatial and textual components to measure the probability a child node will be visited. The intuition is that, the closer a newly-generated partition is, the more precise it can be used in the filter stage. Thus, we formulate p ( n each token only to one child, then the extra space cost turns to c |I index, then we have |R ( n ) | = i |R ( n i ) | . Thus, the time cost c i ( p ( n i )  X  c R  X  X R ( n i ) | ) equals to c R  X  i ((1 algorithm performance is only affected by three factors: p ( n simplicity, we fixed m and |R ( n i ) | can be estimated as location and textual distance. Traditional methods (e.g., k-means, hierarchical clustering) are infeasible in our problem since: (1)  X  X -means X  cannot update the cluster center under the edit distance metric; (2)  X  X ierarchical clustering X  needs to calculate the edit distance for every object pair.
 A natural idea is to alternatively divide the spatial and textual components to generate landmarks with proper granularity. We can random select tokens and divide their tokenMBRs into partitions. However, this method is quite inef-each token to the closest partition needs to calculate its distance from all the landmarks which leads to heavily overloads. Thus, we propose a deletion-based method. The idea is twofold: (1) we can use the inverted index to avoid calcu-we select a substring to represent the textual center. Given a token t , we delete any i characters to generate its i-deletion set, denoted by d d ( X  X izza X ) = {  X  X izza X  } and d 1 ( X  X izza X ) = {  X  X zza X , X  X zza X , X  X iza X , X  X izz X  each node n , we first build an inverted index using the 0-deletion of all its tokens. For each inverted list l t corresponding to the token t ,if l than quad-tree). Then we have four spatial regions, each centered with token t and radius [0 , 0]. If objects in any new generated region M exceed M ,t, [0 , 0] as a selected landmark. For the remaining tokens, we repeat this the textual parts are quite dissimilar, we then partition the remaining objects only according to the spatial region. 5.1 Experimental Settings Datasets. We conducted extensive experiments on two datasets: Twitter and USA . Table 1 summarizes these two datasets. The Twitter dataset is a real dataset. We crawled 1 million tweets with location and textual information from Twitter 1 .The USA dataset is a synthetic dataset which randomly combines the Points of Interests (POIs) in US and the publications in DBLP.
 Experimental Environment. All the algorithms were implemented in C++ and run on a Linux machine with an Intel(R) Xeon(R) CPU E5-2650 @ 2.00GHz and 48GB memory. The algorithms were complied using GCC 4.8.2.
 Parameter Setting. Unless stated explicitly, parameters were set as follows by default: k = 20 and the average number of query tokens is 4. 5.2 Evaluating Queries with Single and Multiple Tokens In this section, we evaluated our methods HLtree and HLtree + by com-paring them with two existing solutions Rtree+Trie algorithm ( RTrie )and Rtree+Qgram algorithm ( RQGram ). HLtree randomly selected tokens to generate hybrid landmarks while HLtree + explored a deletion-based method as Section 4 . We suppose that there are three kinds of query requirements corresponding to the different size of the query regions: (1)Small Size: region like a university; (2)Medium Size: region like a district; (3)Large Size: region like a city. Thus, we randomly selected 1000 objects from USA and Twitter as the center points and then extended them by 1496  X  1496, 20000  X  20000, 125040 meters respectively. In each evaluation, we executed 1000 queries and compared the average processing time. Experimental results show that our methods out-perform the state-of-art algorithms and achieve high performance on all the evaluations.
 Evaluation on the Number of Query Tokens. To evaluate the perfor-mance under different number of query tokens, we fixed k to 20 and varied query tokens from 1-5. The results are shown in Figure 4 .Take Twitter as an example, HLtree + achieved the best performance. It was 4-10 times faster than HLtree and 2-30 times faster than RTrie and RQGram . When we increased the query tokens, RTrie and RQGram increased sharply while HLtree + changed very slowly. The reason is that the TA Algorithm in HLtree + helps quickly locate similar candidates when dealing with queries with multiple tokens. Figure 4(b) and Figure 4(c) had similar performance.
 Evaluation on k . To evaluate k , we varied k from 1 to 500. The result is shown in Figure 5 . For constraint of space, we only show the result of USA . HLtree + was 8-15 times faster than HLtree , was 40-75 times faster than RTrie and was 10-19 times faster than RQGram . Notice that when k increased, HLtree +, HLtree and RQGram almost kept a straight line while RTrie increased obviously. The reason is that for RTrie , all the prefixes whose edit distance is less than the current k th candidate should be visited.
 Evaluation on Scalability. We evaluated the time and index scalability. We varied object size from 0.1-1 million. As Figure 6(a) shows, RTrie and RQGram increased drastically while HLtree and HLtree + almost achieved a linear scala-bility. Figure 6(b) shows the index time. The memory of HLtree + is larger than HLtree because of the intermediate deletion-based inverted index (Figure 6(c) ).
