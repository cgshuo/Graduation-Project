 Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regular-ized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.
The recent years have witnessed a rapid growth of data in variety and volume. The sheer amount of data has led to increasing interest in scalable optimization. Stochastic gra-dient descent (SGD) is one of the most popular methods. It has been successfully applied to large scale natural language processing [11 ], deep learning [ 7], matrix factorization [ 10], image classification [ 17 ], and latent variable models [22 ].
Traditional SGD processes one example per iteration. This sequential nature makes SGD challenging for distributed in-ference. A common practical solution is to employ mini-batch training, which aggregates multiple examples at each iteration. However, the synchronization cost of mini-batch training is potentially still too large for large scale applica-tions. For instance, in a distributed implementation, ma-chines may need to communicate with each other for every mini-batch in order to synchronize the shared variables, such as gradients or parameters [ 8]. Given that both bandwidth and latency of networks are often 100x worse than physical memory, this overhead cannot be ignored.

Although large mini-batches are preferable to reduce the communication cost, they may slow down convergence rate in practice [ 4]. That is, if SGD converges by T iterations, the mini-batch training with batch size b may need more than T/b iterations. The increase in computation diminishes the benefits of the reduced communication cost due to large b . In addition, the I/O costs increases if the data is too large to fit into memory so that one need to fetch the minibatch from disk or network [ 25 ].

This paper considers the problem that we want to use large mini-batches to reduce communication cost but at the same time retain good convergence properties . It is known that for general convex objective functions, the convergence of SGD is O (1 / size b , the convergence is O (1 / total number of examples examined is bT while there is only a  X  with increasing minibatch size.

To address this issue we propose an alternative mini-batch update strategy that does not slow down in convergence as the mini-batch size increases. The key observation is that, when a mini-batch is large, it is desirable to solve a more complex optimization problem, rather than simply update the solution by the gradients. Specifically, in each iteration, we solve a conservative risk minimization subproblem. It consists of two components: the original objective function on the mini-batch and a conservative penalty. Accordingly we are able to gain more from a mini-batch before moving to the next. The conservative penalty reduces variance and prevents divergence from the previous consensus. For our goal we need two ingredients: a more sophisticated update strategy and secondly, an efficient means of solving the con-servative subproblem such that the increase of computation does not overwhelm the reduced synchronization cost.
Many previous works aimed at improving mini-batch SGD optimization. [ 11 ] proposed to use asynchronous commu-nication. [ 23] studied the accelerated version. At a more fundamental level, [ 20 , 27 ] consider the problem of solving subproblems in parallel, followed by averaging. They can be viewed as the extreme case where the mini-batch size is the entire partition. These strategies, however, are wasteful since no communication occurs during the compute phase.
Our approach differs from previous work by the addition of a conservative penalty and the use of each data partition in a nontrivial manner beyond simple gradient computation:
For concreteness of our exposition we need to introduce the inference problem formally. Our goal is to solve Here  X  i :  X   X  R is a convex loss function and w is a shared parameter. This general form addresses a large group of machine learning problems. We give two examples: Risk Minimization [ 12]: Here the objective is to mini-Graphical Model Inference [ 15, 14]: In undirected graph-A much larger family of problems has been characterized by the ADMM algorithms of [ 2]. They can all be viewed as special cases of the above setting where different  X  i ( w ) only act on a subset of variables.
We begin with a brief review of a naive variant of mini-batch SGD. During training it processes a group of exam-ples per iteration. For notational simplicity, assume that n is divisible by the number of mini-batches m . Then we partition the examples into m mini-batches, each of size b = n/m . Note that this assumption is not required nei-ther for the proof nor for the implementation. Likewise, the pre-partitioning step is also not necessary in practice, however, it simplifies the exposition of what follows.
Given a random minibatch I  X  { 1 ,...,n } of size b , we can define the objective function on I as In the simple case that  X  = R d the mini-batch SGD employs the following stochastic update rule: at each iteration t , we pick mini-batch I t  X  X  1 ,...,n } of size b at random and let Whenever  X  has a nontrivial shape we would need to add a projection step, which finds the nearest neighbor of w t the feasible set  X  [ 26].

For convex  X  i , this method converges to the minimum ob-jective value at a rate of O (1 / number of iterations [ 8]. Although b times more examples are processed in an iteration, the mini-batch training can converge much slower than that of standard SGD with the same number of processed examples. In practice, the conver-gence rate slows down dramatically in terms of the number of examples processed, when we use a large mini-batch size.
The above empirical finding was a key motivation for our approach. To gain some intuition note that for general do-mains  X  the update ( 5) can be rewritten as an optimization problem on a mini-batch: Note that this can be regarded as an approximation of  X  I the loss on the minibatch plus a conservative penalty relative to w t  X  1 . While the above optimization problem is easy to solve, the first order Taylor approximation of  X  I t ( w ) might be too coarse to achieve sufficient progress towards the op-timal solution. Such an aggressive trade-off of fast conver-gence in favor of computational efficiency is highly unde-overhead of switching to the next mini-batch, e.g. due to process synchronization, data reads from disk and network communication.

Note that SGD often uses a small step size due to the vari-ance of the randomly chosen mini-batch. However, when the size of a mini-batch increases, its variance decreases. More sophisticated methods may be used towards faster conver-gence rate. In this paper, we propose to update the param-eter by solving the following subproblem at iteration t : It consists of two components: the first part minimizes the objective function on mini-batch I t , aiming to achieve full utilization of this mini-batch. The second component is a conservative constraint which limits dramatic changes of the parameter to avoid overutilization.

Algorithm 4 shows the proposed algorithm. Compared to the SGD rule, we need to solve the more complex con-servative subproblem for each mini-batch. For the sake of simplicity in the theoretical analysis, we assume that the op-timization is performed exactly; in practice, an approximate solution will be sufficient, particularly in the early stages of inference. If the computational cost for this approximate optimization is not too expensive compared to SGD, then this method has similar overall complexity per step relative to SGD, while at the same time drastically reducing the amount of network communication required between steps. Algorithm 1 Single node template Input: Initial w 0 , conservative coefficients  X  1 ,..., X  1: for t = 1 ,...,T do 2: randomly choose mini-batch I t  X  X  1 ,...,n } of size b 3: solve the conservative subproblem: 4: end for
The advantage of solving the conservative subproblem ( 6) is that the convergence does not slow down dramatically when the mini-batch size increases. This is reflected in our main result. Before stating the theorem, we need to intro-duce the notion of a Bregman divergence for convex func-tions f as follows: This is the difference between f ( w ) and the value of the first-order Taylor expansion of f at w 0 , when evaluated at w . The properties of Bregman divergence include: Non-negativity: D f ( w ; w 0 )  X  0 Convexity: D f ( w ; w 0 ) is convex with respect to w Linearity: D f ( w ; w 0 ) is linear with respect to f , namely We need the following assumption for our theorem: Assumption 1. We assume that for all t : E Note that D  X  ( w ; w t  X  1 ) = E I t h D  X  I eral w that does not depend on I t . However, since w t de-pends on I t we require some  X  t &gt; 0 to satisfy the condition. Essentially, Assumption 1 bounds the amount of  X  X urprise X  we can expect when replacing the full Bregman Divergence by one on the subset plus a conservative penalty.

Note that the assumption holds as long as we pick  X  greater than or equal to the smoothness parameter of  X  : In other words, the counterpart of strong convexity, namely that there exists a quadratic upper bound on the amount of change, suffices to guarantee this condition. In practice, however, one may be more aggressive and allow a much smaller  X  t when the mini-batch size is large. In fact, one may show that a choice of  X  t = O (1 /b ) is sufficient. We have the following theorem:
Theorem 1. Consider the stochastic update rule (6) . As-sume that  X  i is  X  -strongly convex for all i : Under Assumption 1 and when choosing the update param-eter  X  t =  X  +  X  ( t  X  1) , we have for all w  X   X   X  : For convex functions, the modulus of strong convexity  X  = 0 vanishes. This amounts to a constant update rate  X  . In this case, choosing minimizes the right hand side of the bound. Note that there is no a-priori guarantee that a correspondingly small  X  is feasible. However, since the variance decreases with O (1 /b ) for increasing minibatch size, the scaling of  X  = O (1 / is appropriate. This yields the following aggregate regret bound This means that if mini-batch size is b , after T steps, we have a convergence bound of 1 / mini-batch size does not affect convergence in terms of the number of training examples processed by the algorithm. For strongly convex  X  &gt; 0, we can achieve a regret bound of O (log T/ (  X bT ) +  X / (
For convenience, we define the regularized mini-batch loss Our proof relies on three lemmas. First, we upper bound k w over all examples. That is, the gradients differ via k X   X  (  X  w ter, namely the variance of gradient over a mini-batch, is bounded from above by A 2 /b . Finally we characterize the progress from time t  X  1 to t . The proofs of these auxiliary lemmas are presented in the Appendix.

Lemma 1. Let be the minimizer of the conservative version of the risk. Then the difference between the full solution  X  w t minibatch solution w t is bounded by
Lemma 2. Assume that w  X   X  and assume that we ran-domly choose a mini-batch I of size b independent of w . Then the expected deviation between gradients is bounded by where B 2 = 1
Lemma 3. Given any w  X   X   X  , the expected improvement in terms of Bregman Divergence is bounded via
Proof of Theorem 1. Under the assumption that  X  i is  X  -strongly convex, it follows by construction that h strongly convex with modulus  X  t +  X  . Consequently the Bregman divergence is bounded by Together with Lemma 3, we have Here the first equality follows from the definition of  X  t second equality follows from the definition of h t and simple algebra; the third equality uses the fact that we are drawing I independently. Hence we have Summing over t = 1 ,...,T , we obtain the desired bound.
Our analysis assumes that we solve the conservative sub-problem ( 6) exactly; in practice, we only need to perform this optimization approximately. In this section, we pro-pose two approximate approaches and their distributed im-plementation.
Optimization algorithms solving the original problem ( 1) can often be applied to the conservative subproblem ( 6): the latter consists of a part of the former with a simple quadratic term with respect to the parameter. While the most suitable optimization methods vary for different objective functions, a natural idea is to reuse the one to solve ( 6) but to stop it earlier. It is understood that real applications are complex; here we propose two simple but general methods that allow us to solve (6 ).

The first one is a direct extension of SGD. Note that, if we set  X  = 0, then SGD equals to performing gradient de-scent with a single pass of the mini-batch with w t  X  1 as the start point. We relax the single pass constraint such that we could obtain a more accurate solution of the conservative subproblem. The algorithm, named EMSO-GD, is shown in Algorithm 2. It solves (6 ) by gradient descent. Standard stopping criteria can be used to achieve early stopping. For instance, we may stop when the relative objective improve-ment is less than a thresholds. In practice we found it most convenient to use the simplest strategy: limit the maximal iteration number L . That is, the for loop will stop if we pass the mini-batch L times. A major benefit of this strategy is to simplify the synchronization of the distributed implemen-tation we will introduce in the next section.
 Algorithm 2 EMSO-GD: solve ( 6) by gradient descent Input: previous parameter w t  X  1 , mini-batch I t Output: new parameter w t 2: for ` = 0 ,...,L do 3: update 4: end for
The second method is motivated by [25 ], where coordi-nate descent is applied to solve the dual form of the linear SVM in a mini-batch. Unlike [ 25 ], we directly solve the subproblem by coordinate descent in the primal form. Al-gorithm 3 shows the proposed algorithm, which is named EMSO-CD. In each time, EMSO-CD chooses a random coor-dinate j  X  [1 ,p ], where p is the total number of coordinates, and then solves the one dimension problem This minimization problem may have a closed form solution. But generally it could be solved by the Newton method. Similar to EMSO-GD, we use the maximal iteration number as the early stop criteria.
 Algorithm 3 EMSO-CD: solve (6 ) by coordinate descent Input: previous parameter w t  X  1 , mini-batch I t Output: new parameter w t 2: for ` = 0 ,...,Lp do 3: randomly choose coordinate j  X  [0 ,p ] 4: update 5: end for
In distributed computing, we assume there are d machines, which are connected by network. Then the conservative subproblem could be solved by all these machine together. Specifically, we first divide a mini-batch into d partitions, next assign one partition to each of the machines, and then obtain the solution via communication. One possible ap-proach is that all machines communicate in each iteration when solving the subproblem. However, this may introduce a large amount of communication.

Instead, we propose to use a more communication friendly approach where each machine solves the conservative sub-problem independently, and then all machines average their name KDD04 URL CTR # examples 146 K 2.4 M 142 M # features 74 3.2 M 28 M # entries 11 M 277 M 8.4 G # features per example 73  X  0.8 116  X  17 59  X  26 label ratio +1 :  X  1 1:111 1:2 1:15 results. The algorithm is shown in Algorithm 4. Note that there is no restriction in terms of the choice of methods for solving the subproblems locally. In particular, the algo-rithms introduced in the previous section apply.
 Algorithm 4 EMSO: Efficient Mini-Batch for Stochastic Optimization Input: initialization w 0 , conservative coefficients {  X  1: partition examples into m mini-batches I 1 ,...,I m 2: for t = 1 ,...,T do 3: randomly choose mini-batch I t 4: partition I t into I t, 1 ,...,I t,d 5: for i = 1 ,...,d do { in parallel } 6: machine i get partition I t,i 7: solve the conservative subproblem on I t,i by 8: end for 9: average w t = 1 d P d i =1 w ( i ) t via communication 10: end for
To evaluate the proposed algorithms, we used three bi-nary classification datasets of varying scales, which are listed on Table 1. KDD04 comes from the particle physics task at KDD Cup 2004 1 , whose goal is to classify two types of parti-cles generated in high energy collider experiments. The URL dataset aims to detect malicious URLs 2 . CTR is a private dataset containing displayed advertisements which are ran-domly sampled within a period of three months. The goal is to predict whether or not an advertisement will be clicked. KDD04 is a dense dataset, while the other two are extremely sparse. The largest dataset CTR has more than 100 million examples and the raw text file size is approximately 300 GB.
For the sake of simplicity we study the case of classification via logistic regression. There the objective function can be written in the form of ( 1) by letting Here we let ( y i ,x i ) to be the i -th sample pair. We evaluated five algorithms, as summarized in Table 2. With the exception of LibLinear all algorithms were imple-mented in C++ using MPI for communication. The linear http://osmot.cs.cornell.edu/kddcup/datasets.html http://sysnet.ucsd.edu/projects/url/ name update iteration distributed L-BFGS [18] batch yes LIBLINEAR [9] batch no Mini-Batch SGD (5) mini-batch yes EMSO-GD (10 ) mini-batch yes EMSO-CD (11 ) mini-batch yes algebra operations are mainly performed by eigen3 3 , which is a highly efficient C++ template library.
 L-BFGS This is a parallelized version of the classical memory-LibLinear This is the single-machine implementation as Mini-Batch SGD This effectively computes subgradients EMSO-GD This uses the parameter-averaging approach EMSO-CD The key difference to EMSO-GD is that it uses All experiments were carried on a cluster, where each ma-chine is equipped with four AMD Opteron Interlagos 16 core 6272 CPUs, 128GB memory and 10Gbit Ethernet. http://eigen.tuxfamily.org/ http://www.csie.ntu.edu.tw/~cjlin/liblinear/
A first sanity check is to ascertain that the convergence results regarding mini-batch methods on a single node hold. For this purpose we increase the batch size from 10 10 5 . The objective values after processing 10 7 examples are shown on Figure 1. As expected, when the mini-batch size increases, there is an increment of objective value for mini-batch SGD due to the rather crude approximation of the dataset by a first order Taylor expansion. That is, the con-vergence in terms of examples processed slows down. This degeneration is worst on the dataset KDD04 , which is dense and extremely unbalanced in terms of its labels. This prob-lem is alleviated by EMSO-GD. It performs 5 iterations of gradient descent in a mini-batch and therefore potentially gains more information about the higher order structure than SGD.

However, the convergence is much more stable when solv-ing the conservative subproblem by coordinate descent. As can be seen from Figure 1, the objective value of EMSO-CD does not increase. It even slightly decreases, with the increasing mini-batch sizes. A possible explanation is that, even though each mini-batch is passed only twice, the coor-dinate descent with the previous parameter as a warm start gives satisfactory solutions to the conservative subproblem. So it provides sufficient progress to compensate for the loss due to increasing the mini-batch size.

In summary, solving conservative optimization problems on a minibatch is beneficial when compared to a naive gra-dient computation.
In a next step we compare the algorithms listed in Table 2 by objective value versus run time. We use the same setting as Section 4.3 for the mini-batch algorithms, but only report the result with the best batch size, namely 10 5 for EMSO-CD and 10 3 for the other two algorithms. Figure 2 shows the results. It can be seen that the convergence the two batch algorithms, L-BFGS and LibLinear is similar: slow at the beginning but fast at the end. This is not too unsurprising given LibLinear X  X  heritage.

For the mini-batch algorithms, EMSO-GD is compara-ble to SGD: While EMSO-GD converges faster in terms of number of minibatch iterations, it consumes 5 times more computational time than SGD. Note that, even with a larger mini-batch size, EMSO-CD is 10 times faster than the other two, and furthermore, it is faster than the batch algorithm at the end.
Recall that a major benefit of large mini-batch sizes is the potential reduction in synchronization cost. Figure 3 shows the contribution of synchronization cost to the overall run-time of the algorithm when using 12 machines. Even with such a small number of computers the proportion is con-siderable. As expected, this cost decreases with increasing mini-batch size. It is due to the increase in the amount of computation between synchronization passes. Furthermore, because both EMSO-GD and EMSO-CD solve a more com-plex optimization problem in each mini-batch than SGD, their synchronization cost is correspondingly smaller than that of SGD. In addition, although coordinate descent passes a mini-batch twice in our experiment, it requires significant more exponentiation operations than the gradient descent (a F igure 1: Objective value versus mini-batch size after in total 10 7 examples are processed in a single node. From top to bottom, datasets are KDD04 , URL , and CTR , respectively, where CTR is downsampled to 4 millions examples due to the limited capacity of a single node. F igure 2: Objective value versus run time by a single node. Datasets are the same as Figure 1, which are KDD04 , URL , and CTR from top to bottom. fast special functions library would probably address this is-sue). As a result, it consumes more CPU time than gradient descent, even though the latter processes a min-batch five Figure 3: The fraction of synchronization cost as a function of minibatch size when communicating between 12 and 12 nodes. Table 3: Run time and speedup for EMSO-CD to reach the same objective value when running on 5, 10 and 20 nodes. times, and therefore has a further decreased synchronization cost.

The convergence results under various mini-batch sizes are shown in Figure 4. We first fix the total number of examples processed to be 5  X  10 6 . As can be seen from the top figure, the results are similar to the single node re-sults of Figure 1. That is, EMSO-GD slightly improves SGD while EMSO-CD is resilient to increase the mini-batch size. The bottom of Figure 4 shows the results by fixing the run time to be 1 , 000 seconds. The trend then is total different. Because the portion of synchronization cost decrease, more time can be allocated to process the examples, therefore a large mini-batch size is faster. Furthermore, although SGD is comparable to EMSO-GD in the single node experiment, as shown in Figure 2, the latter outperforms the former here, due to the communication cost in distributed environment. In addition, there are clear advantages for EMSO-CD to use large batch size, as it converges faster when the mini-batch size increases.
We conclude our experimental evaluation by assessing run-time results for varying numbers of nodes. We primarily compare the following two algorithms: EMSO-CD and L-BFGS. Figure 5 shows the convergence results. As can be seen, both algorithms benefits from an increase in the num-ber of nodes. But L-BFGS gains more than EMSO-CD, because the former passes the whole training data in each iteration so the portion of synchronization cost is small X  15% comparing 30% of EMSO-CD. However, EMSO-CD is Figure 4: Objective values when varying the mini-batch size using 12 nodes. Top: for a fixed total number of examples set to 5  X  10 6 . Bottom: for fixed runtime set to 1000 seconds. still 10 times faster than L-BFGS, due to the faster conver-gence rate.

Table 3 shows the quantitative speedup for EMSO-CD to reach specific objective values. When the number of nodes doubled from 5 to 10, there is an average 1 . 75x speedup for both objective values, and if the nodes number is increased by 4 times, the speedup increases to 2 . 5x.
The idea of using mini-batch in stochastic optimization has been studied by a number of researchers. For example, it was shown in [ 8] that distributed mini-batch gradient can achieve a convergence rate of O (1 / comparable to that of serial SGD when the minibatch size is small. Additional studies include [6 , 24 , 23 ].
There is also a large volume of works to improve the stan-dard mini-batch approach. For example, [16 ] proposed to style updating. In addition, [ 19 , 13] argued to reduce the stochastic variance via gradients computed on the whole dataset. Figure 5: Objective function value versus run time for both EMSO-CD and L-BFGS using varying numbers of nodes.
Another line of research focuses on the practical perfor-mance, especially when data cannot fit into memory. For example, [ 25 ] studied solving linear SVM in the dual form that having both I/O and computational threads working together can further improve the performance. [5 ] studied how to select the data block.
 Our work is different from previous ones in several aspects. First, we propose a novel mini-batch algorithm that solves a regularized optimization problem in primal form at each step. We show that the method can also achieve the optimal convergence rates theoretically, and presented practical im-plementations of the approach. The practical performance of the resulting methods outperform minibatch SGD under various scenarios.

In this paper we proposed a variant of mini-batch SGD whose convergence rate does not degrade when the batch size increases. It solves a conservative subproblem in each iteration to maximize the utilization of the mini-batch while at the same time controlling the variance via a conservative constraint. We showed that it enjoys an optimal conver-gence rate and proposed practical distributed implementa-tions. Furthermore, we demonstrated its efficiency on serial and distributed experiments on large scale datasets. [1] J. Besag. Spatial interaction and the statistical [2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and [3] R. Byrd, S. Hansen, J. Nocedal, and Y. Singer. A [4] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. [5] K.-W. Chang and D. Roth. Selective block [6] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. [7] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, [8] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. [9] R.-E. Fan, J.-W. Chang, C.-J. Hsieh, X.-R. Wang, and [10] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. [11] K. Gimpel, D. Das, and N. A. Smith. Distributed [12] T. Hastie, R. Tibshirani, and J. Friedman. The [13] R. Johnson and T. Zhang. Accelerating stochastic [14] M. I. Jordan. An Introduction to Probabilistic [15] F. Kschischang, B. J. Frey, and H. Loeliger. Factor [16] B. Kulis and P. L. Bartlett. Implicit online learning. [17] Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, [18] D. C. Liu and J. Nocedal. On the limited memory [19] D. Mahajan, S. S. Keerthi, S. Sundararajan, and [20] G. Mann, R. McDonald, M. Mohri, N. Silberman, and [21] S. Matsushima, S. Vishwanathan, and A. Smola. [22] D. Mimno, M. Hoffman, and D. Blei. Sparse stochastic [23] S. Shalev-Shwartz and T. Zhang. Accelerated [24] M. Tak  X a X c, A. Bijral, P. Richt  X arik, and N. Srebro. [25] H.-F. Yu, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. [26] M. Zinkevich. Online convex programming and [27] M. Zinkevich, A. J. Smola, M. Weimer, and L. Li.
Proof of Lemma 1. Since w t = argmin w  X   X  h t ( w ), we have from the first order KKT condition at w t : In addition, the first order KKT condition of ( 8) at  X  w bined with the fact that h t ( w ) =  X  I t ( w ) +  X  t 2 k w  X  w implies that By substracting the first inequality from the second inequal-ity, and rearranging terms, we obtain: By additivity of Bregman divergences we have Similarly D h t ( w t ;  X  w t )  X   X  t 2 k  X  w t  X  w t k equality is Cauchy-Schwarz inequality.

Proof of Lemma 2. This bound is essentially a conver-sion of variances from a minibatch I to the full set when using sampling without replacement. To simplify notation we use the abbreviation of  X  i :=  X   X  i ( w )  X   X   X  ( w ) and  X  I =  X   X  I ( w )  X  X  X   X  ( w ). Note that by construction and therefore B 2 = E i k  X  i k 2 and B 2  X  A 2 . The latter inequality follows since A 2 is a uniform upper bound on the variance over all w  X   X . This yields The last equality used the fact that  X  i has zero-mean. Proof of Lemma 3. We have where the equalities follow from algebraic manipulations and the definition of Bregman divergence; in the inequality, we used the first order KKT condition of (6 ) at w t , implying that Taking expectation, we have where the second inequality follows from E ( D  X  ( w t ; w D t ( w t ; w t  X  1 ))  X  0, which is a consequence of Assump-tion 1. The equality holds because Note further that  X  q E k X   X  ( w  X  q E k X   X  ( w  X  A 2 / (  X  t b ) , where the first inequality follows from Cauchy-Schwarz in-equality, the second inequality is due to Lemma 1, and the third inequality is due to Lemma 2. Plugging the above estimate into ( 14 ), we obtain the desired bound.
