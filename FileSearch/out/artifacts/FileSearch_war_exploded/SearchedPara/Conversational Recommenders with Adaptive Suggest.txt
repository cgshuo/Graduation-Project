 We consider a conversational recommender system based on example-critiquing where some recommendations are suggestions aimed at stimulating preference expression to acquire an accurate preference model. User studies show that suggestions are particularly effective when they present additional oppor tunities to the user according to the look-ahead principle [32].

This paper proposes a strategy for producing suggestions that exploits prior knowledge of preference distributions and can adapt relative to users X  reactions to the displayed examples.

We evaluate the approach with simulations using data acquired by previous interactions with real users. In two different settings, we measured the effects of prior knowledge and adaptation strate-gies with satisfactory results.
 H.1.2 [ Models and Principles ]: User Machine Systems X  human factors, software psychology Human factors Recommender systems, example-critiquing interfaces, personalized search
People increasingly face the difficult task of having to select the best option from a large set of multi-attribute alternatives, such as choosing an apartment to rent, a notebook computer to buy, or fi-nancial products in which to inv est. Knowledge-a nd utility -based recommender systems are tools that help people find their most desired item based on a model of their preferences [4, 3, 5, 20, 24]. For their performance, it is crucial that this preference model be as accurate as possible. This poses new challenges for human-computer interaction at the cognitive level, which have been poorly Figure 1: : The generic system-user interaction model of a preference-based search and recommender system. addressed so far, but are key to the user success rate of such systems on e-commerce sites.

Utility theory provides a solid ma thematical foundation for rec-ommendations [11]. However, it assumes complex preference mod-els that cannot be obtained in e-commerce scenarios because peo-ple are not willing to go t hrough lengthy preference elicitation pro-cesses. Furthermore, they are usually not very familiar with the available products and their characteristics. Thus, their preferences are not well established, but constructed while learning about the available products [17]. To allow such construction to take place, users must be able to explore the space of possible options while building their preference model.
A good way to do this is through a mixed-initiative system based on example critiquing (see Figure 1). Example critiquing was first introduced by [30] and works by showing k examples to a user in each interaction cycle. Showing a set of examples allows the sys-tem to correct for an incomplete or inaccurate model of the user X  X  preferences [9]. If the target item is not among the k examples, then a set of user critiques will be collected to refine the existing model. Example critiquing allows users to express preferences in any or-der, on any criteria, and with any effort they are willing to expend [21]. It has been used in various product search and recommender tools [14, 5, 26, 20, 28, 25].

In an example critiquing inter action, user X  X  preferences are vol-unteered , not elicited: users are never forced to answer questions about preferences they might not be sure about. Thus, users will only state preferences that they actually have, and they can tailor the effort they spend on stating their preferences to the importance of the decision that they are making.

Example critiquing was first proposed in [30] and has since been used in several recommender systems, such as FindMe [5], ATA [14], SmartClient [20], ExpertClerk [26] and the system of Shearin &amp; Lieberman [25]. The ATA system [14] is particularly important, as it was the first to incorporate the notion of suggestions, which is crucial to our work.

Evaluating example critiquing interfaces has been an active area of research lately. Pu and Kumar [22] showed that example cri-tiquing interfaces enable users to perform decision tradeoff tasks more efficiently with considerab ly less errors than non-critiquing interfaces. More recently, Pu and Chen [19] showed that the imple-mentation of tradeoff support can increase users X  decision accuracy by up to 57%.

In dynamic critiquing [24], a popular family of example cri-tiquing interfaces, a metaphor of navigation through the product space is implemented; the interface proposes pre-computed cri-tiques (simple and compound) that can be selected by the users. McCarthy et al. [15] showed that users who more frequently ap-plied compound critiques in a critiqui ng interface were able to re-duce interaction cycle from 22 to 6 .
We consider an example-critiquing framework where 1. Preferences are stated as reactions to displayed options (as 2. These critiques are used as feedback in the next interaction
If certain preferences are missing from the current model of the user, the system may provide solutions that do not satisfy those unknown preferences. If the user is aware of all of her preferences, she realizes the necessity to state them to the system. However this is not usually the case, because the user might not know all the available options. Moreover, stating a preference costs some user effort and she would make that effort only if she perceives this as beneficial.

The influence of current examples is known as the anchoring effect [7]. To enable the user to refocus the search in another di-rection, many researchers have suggested to display alternatives or diverse examples, in addition to the best options (candidates). In one user study it was observed that the majority of critiques ( were a reaction to seeing an additional opportunity rather than see-ing unsatisfactory examples [32].

Different strategies for sugges tions have been proposed in the lit-erature. Linden [14] used extreme examples, where some attribute takes an extreme value. Others use diverse examples as sugges-tions [27, 28, 26].

However, an extreme example might often be an unreasonable choice: it could be a cheap flight that leaves in the early morn-ing, a student accommodation where the student has to work for the family, an apartment extremely far from the city. Moreover, in problems with many attributes, there will be too many extreme or diverse examples to choose from, while we have to limit the display of examples to few of them.

The user should be motivated to state new preferences by op-tions that are reasonable choices (given the previously stated pref-erences) and have a potential of optimality (a new preference is required to make them optimal).
 This was expressed in the lookahead principle [23]: Model-based suggestions are calculated based on that principle. Results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78%
The lookahead principle was implemented by considering Pareto-optimality: suggestions are evalu ated according to t heir probability of becoming Pareto-optimal. The advantage of this method is that it is qualitative, since it does not rely on any particular parametriza-tion: an option is Pareto-optimal if there is no other option that is better or equally preferred with respect to all the preferences and strictly better for at least one preference.

To become Pareto-optimal, the new preference has to make the current solution escape the dominance with better solutions (the  X  X ominators X ).

An example of a conversational recommendation system is FlatFinder, a tool for finding student accommodation using example critiquing, using data available from the faculty housing program (containing around 200 items). Figure 2 shows an example of an interaction with FlatFinder. The user has posted preferences for a cheap ac-commodation (price less than 450) with a private bathroom. The tool shows 3 best options, which are all rooms in a family, with bus or no public transport at all. It also displays 3 suggestions, which show that paying a bit more, the user can have a studio or a small private apartment where both are closer to the university and the centre of town; the second also is close to a metro station. The last option is still a room but is closer to the center than the options currently showed as best options. Seeing the suggestions, the user could realize that she cares about these features, and then state a preference on the transportati on, for the accommodation type or on distance from centre and university. We have used FlatFinder in extensive user studies reported in [23, 31, 33].
In artificial intelligence, several people have dealt with the prob-lem of understanding the behavior of an autonomous agent on the basis of observations that his next actions can be anticipated [2, 10]. For instance, Lang [13] considered a qualitative approach for inferring the agent X  X  goal and his plausible next actions under the assumption that agents are goal-seeking and have a given prefer-ence relation over plans of actions.

In the context of a preference-ba sed search problem, the acqui-sition of the user model might be facilitated if the system could anticipate the behavior of the the user given observations of their past behavior. Recently, several researchers followed this intuition by considering an adaptive strategy for utility-based elicitation and question/answer interface.

Chajewska et al. [6] proposed an elicitation procedure that mod-els the uncertainty of the utility function and selects assessment questions to maximize the expected value of information. Boutilier [1] has extended this work by taking into account values of fu-ture questions to further optimize decision quality while minimiz-ing user effort. The elicitation procedure itself is a optimized based on a partially observable Markov decision process(POMDP) where the goal is to minimize the user effort.

Stolze considers how to optimize a an online question/answer interface, by adapting the questions taking into account the distri-bution of possible answers and solving a decision tree optimization problem [29].

Other works focused on how to improve decision aid tools using prior knowledge. Price and Messinger in [18] optimize the set of examples given an expectation of the user X  X  preferences, without actually asking the users to state their own preferences.
In this paper we consider a mixed initiative tool for preference-based search in which preferences are stated as critiques to shown examples (user-motivated critiques) and suggestions are presented to stimulate preference expression.

We consider how suggestions can profit from prior knowledge and propose a strategy for adapting the suggestions based on the user X  X  reactions. The interaction cycle of a conversational recom-mender system with adaptive suggestions is shown in Figure 3.
As far as we know, we are the first to propose an adaptive strategy of preference acquisition in a conversational recommender system based on example-critiquing.
We assume that O is the set of options o 1 , .., o n defined over A , set of attributes { a 1 , .., a n } of domains D 1 , .., D value that o takes on attribute a i .

Domains can be qualitative or numeric. A qualitative domain (such as colors or names) consists of an enumerated set of possibil-ities; a numeric domain has numerical values (as price, distance to center), either discrete or continuous. For numeric domains, we consider a function range ( Att ) that gives the range on which the attribute domain is defined. We define qualitative (respectively nu-meric) attributes those with qualitative (numeric) domains.
Preferences are stated on indivi dual attributes. A preference r applies to a particular attribute a i and results in a total or partial order on the values in the domain D i of a i .

A preference model R consists of a set of preferences. We as-sume that a preference r i is expressed by a cost function c a preference always applies to the same attribute we can use c instead of c i ( a i ( o )) .

We assume that the cost functions correctly express the user X  X  ceteris paribus preferences, i.e. that for any pair of options o o that are identical in all preferences except preference r prefers o 1 over o 2 if and only if c i ( s 1 ) &lt;c i (
The individual costs obtained b y each preference are merged with a particular combination function, for example a weighted sum.

The candidate best options can be found by sorting the database items according to their cost. This is known as the top-k query [8]. The set of options retrieved { o 1 , .., o k } is such that C C ( o 2 )  X  ..  X  C ( o k ) and for any other option  X  o in the database C (  X  o )  X  C ( o k ) .
We model preferences by standardized functions that correctly reflect the preference order of individual attribute values but may be numerically inaccurate.

Pareto-optimality is the stronge st concept that can be applied without knowledge of the numerical details of the penalty func-tions.

An option o is (Pareto) dominated by another option  X  o (equiv-alently we say that  X  o dominates o )if Figure 3: With adaptive suggestions, the tool learns from the user by observing the reaction to displayed examples. For instance, if we have shown to the user an option with value subway for attribute  X  X ransportation X  and she has not stated any critique about the kind of transportation, the probability that the user has a preference for subway as transportation will decrease. In the next interaction cycle, the system is likely to choose options with a different value.
An option o is Pareto-optimal if it is not Pareto-dominated by any other option.

The dominating set O + ( o ) is the set of options that dominates the option o .The equal set O = ( o ) is the set of options that are equally preferred with o .

P ROPOSITION 1. A dominated option o with respect to R be-comes Pareto-optimal with respect to R  X  r i if and only if o is
Thus, the dominating set O &gt; and the equal set O = of a given option are the potential dominators when a new preference is con-sidered.
We suppose that the system knows that each user has preference value functions in a known parameterized family. Here we assume a single parameter  X  , but the method can be generalized to handle cases of multiple parameters.
We use r i,  X   X  to denote the preference on attribute i with parameter  X   X  , corresponding to the cost function c i (  X   X , o ) .
For simplicity, we let the parameter  X  represent the reference point of the preference stated by the user.

Consider a preference statement like  X  X  prefer German cars X  , meaning that cars manufactured in Germany are preferred to cars manufactured in another country. It can be represented by the fol-lowing cost function c i considering the attribute a i being the coun-try and  X  = german.
For numeric domains we consider that the user can choose be-tween a fixed set of qualitative statements like:
We suppose that for a given implementation we will choose a particular form of representing these statements. The designers of the application will consider the kind of preference statements that the user can state through the user interface and the way to translate the statements into quantitative cost functions. A similar approach is taken by Kiessling in the design of PREFERENCESQL [12], a database system for processing queries with preferences.
A reasonable possibility is to represent their cost functions as graded step functions, with the penalty value increasing from 1 . In case where the degree of violation can worsen indefinitely, a ramp function can be used. For example, in a system for flight reservations where the user states that she prefers to arrive before a certain hour (  X  ), a possible function is the following:
We suppose to have a prior probab ility distribution over the pos-sible preference models, learnt from previous interactions with the system. We consider
Such distribution will be updated by the user during the interac-tion based on the user X  X  action.
According to our look-ahead principle, we choose suggestions that have the highest likelihood of becoming optimal when a new preference is added. Since we would like to avoid sensitivity to the numerical error of the cost functions, we use the concept of Pareto-optimality.

In this section we show how to compute the probability that a given option becomes Pareto optimal (breaks all dominance rela-tions with options in its dominating set). These formulas depend on the probability distributions p a i and p i (  X  ) that we have intro-duced before. At the beginning of the interaction, they are ini-tialized by prior knowledge, considering the preference models of previous users. During the interaction, they are updated according to the observations of the user X  X  actions.
We first consider the probability of breaking one dominance re-lation alone. To escape dominance, the option has to be better than its dominator with respect to the new preference.

The probability that a preference on attribute i makes o 1 ferred to o 2 can be computed integrating over the values of  X  for which the cost of o 1 is less than o 2 . where H is the function H ( x )  X  if ( x&gt; 0) then 1 else
For a qualitative domain, we iterate over  X  and sum up the prob-ability contribution of the cases in which the value of  X  makes o preferred over o 2 .

For breaking the dominance relation with all the options in the dominating set when a new preference is set on attribute a dominating options must have a less preferred value for a that of the considered option. For numeric domains, we have to in-tegrate over all possible values of  X  , check whether the given option o has lower cost and weigh the probability of that particular value of  X  . where H  X  is a modified Heaviside function that assigns value 1 whenever the difference of the two costs is 0 or greater. ( H if ( x  X  0) then 1 else 0 ).

For qualitative domains, simply replace the integral with a sum-mation over  X  .
In general the cost functions in use for a particular system can lead to substantial simplification of the calculations of the integrals. Considering the cost function the probability of breaking a dominance relation between option o and o 2 simplifies to the probability that the value of option o for attribute i is the preferred value, when it differs from the value of o 2 .
To make an interesting suggestion, it is sufficient that an option has a chance of becoming Pareto-optimal, i.e. escape dominance in at least one of the attributes in the set A of all attributes modeling the options. This is given by the combination of the events that the user has a preference on a particular attribute a i , and the event that this preference will break all attribute dominance in that attribute: where  X  i ( o, O  X  ) is the probability of simultaneously escaping many dominance relations and p a i , as said before, is the probability that the user has a preference over attribute a i .

To generate suggestions that are adapted to the user, we update the value p a i according to the user X  X  actions. The computation of  X  depends on p i (  X  ) , that is also updated according to the user X  X  behavior.

Following the lookahead principle, the options for which p greatest are chosen as the best suggestions
Model-based suggestions can become very effective if they can adapt to the user, responding to his actions. In particular, after the user has made a query and some example options have been shown ( candidates and suggestions ), the user might or might not state an additional preference. Observing the reaction of the user to the examples, the system can refine the uncertainty over the preference model. Suggestions stimulate the expression of those preferences on the values that are shown; therefore, if the user does not state any preference, the likelihood that there is a preference on those values will decrease.

After each interaction cycle, we need to update the p a i for all attributes i, on the basis of the user X  X  reaction to the shown example.

We suppose to know a model of the user X  X  reaction behavior, composed of the following probabilities (that refer to the situation in which at least one relevant option is displayed).
We will expect the second to be relatively small: from our expe-rience in the user tests, users of example-based tools state prefer-ences only when these are relevant (in contrast to the form-filling approach, where users are more likely to state preferences they do not have).

We use the Bayesian rule to update the probability of a particular preference being present in the user model, in the condition that a relevant option is presented to the user in the set of displayed exam-ples. The probability that a preference on attribute i with parameter  X  is present when the user has stated or not stated any critique is the following:  X   X   X   X   X   X   X  where p (  X  r )=1  X  p ( r ) .

Once we know the new value for p ( r i, X  ) , we can compute the new values for p a i and p i (  X  ) that will be used to generate sugges-tions at the next interaction cycle.
The new value for p a i , the probability that there is a preference over a particular attribute, is obtained by the cumulative addition of the joint probability p ( r i, X  over  X  ) , and the the parametric distribu-tion p i (  X  ) by dividing the joint probability p ( r i, X  probability p a i .
Thanks to our simplification in the type of cost functions, the al-gorithm for updating the pr obability distribution is not particularly complicated. The parameter  X  represents the  X  X eference value X  of the preference (the most preferred value for qualitative domains; the extreme of acceptable values for numeric domains).

For each of the displayed options o and for each of the attributes a , we update the probability p ( r i,a i ( o ) ) (where  X  = tioned on whether the user has stated an additional preference on i or not, as we have shown in the previous paragraph.

The algorithm is shown in Algorithm 1, where update refers to the probability update perfo rmed by Equation 8, 9 and 10.
Algorithm 1 : Belief update newPref contains the indexes of attributes on which a prefer-ence has been stated in the last interaction.
In previous work, we carried out user studies to validate the effectiveness of example-based t ools for preferen ce-based search, with and without suggestions ([23, 33]). In these experiments, model-based suggestions were calculated without prior knowledge (thus, assuming constant probability distribution) and no belief up-date.

We measured decision accuracy. Using the traditional form-filling approach, only 25% of users found their most preferred so-lution. This fraction only increased to 35% when they could repeat the use of this interface as many times as they liked. On the other hand, example-critiquing reached a 45% accuracy. We obtained the strongest increase in accuracy, to 70%, when using example-critiquing with suggestions.

In the next section, we evaluate the performance of adaptive sug-gestions using prior knowledge and Bayes reasoning.
We evaluated our adaptive strategy of generating suggestions with simulations. Assuming that our look-ahead strategy is correct, we look at the effectiveness of the suggestions in stimulating pref-erence expression. The simulated user behaves according to an op-portunistic model by stating a preference whenever the suggestions contain an option that would become optimal if that preference was added to the model with the proper weight.

To obtain realistic prior distributions we considered logs from previous user studies using the FlatFinder tool. A total of 100 in-teraction logs, each constituting a complete search process, were considered.

For the adaptive suggestions, we need an estimate for the proba-bility that the user states a preference given that an option is shown (in the Bayesian update formula, Equation 8). We substitute this with the value of the estimated probability of becoming pareto op-timal, according to our look-ahead principle.
We run simulations in two settings. In the first, we used the logs to draw random preference models and measure the percent-Table 1: The average number of discovered preferences. We compare the model-based suggestions with prior knowledge, the standard model-based suggestion strategy assuming uni-form distribution, the strategy of maximizing diversity and the extreme strategy. age of time that the look-ahead principle is satisfied (an option that is selected by the system as a suggestion becomes Pareto optimal). We call this measure the hit rate . We compare different strate-gies of suggestions: model-base d suggestions using prior knowl-edge, model-based suggestions assuming uniform distribution, ex-tremes strategy (suggesting options where attributes take extreme values, as proposed in [14]), the diversity strategy (computing the 20 best solutions according to the current model and then generat-ing a maximally diverse set of 5 of them, following the proposal of [16]).

Considering the logs of previous interactions with the tool, we could set up the value for p ( r i, X  ) , the probability that the user has a preference on attribute i and its parameter is  X  , assigned to the occurrence of the preference r i, X  ) .

The results (Table 1) show that prior knowledge of the distribu-tion of the preferences can greatly help in making better sugges-tions (87% against 61%).
To better evaluate suggestions in a more realistic scenario, we considered the simulation of an interaction between a simulated user and the example-critiquing tool.

In the simulations, we set p ( st | X  r i , X  )=0 , meaning that the user states only preferences that she really has. This is reasonable because the user can only state preferences on her own initiative. Therefore, p (  X  st | X  r i , X  )=1 .

We need to define p ( st | r i , X  ) . As recalled before, in a user study it was observed that the majority of critiques ( 79% ) were a reaction to seeing an additional opportunity rather than seeing unsatisfac-tory examples [32], providing evidences for the correctness of the lookahead principle. Therefore, we assume a probabilistic model of the user coherent with the lookahead principle where o is the option displayed and p opt ( o ) the estimated prob-ability of optimality. We think that this is a cautious approach: in many real situations the user will state a preference when a new opportunity is shown, even if optimality is not achieved.
Given these assumptions, any time that an option is shown and the user does not critique a given preference, the new value for the probability that the preference is present p new ( r i, X  )= can be written as: p
The belief update depends on p opt ( o ) , that is an estimation of the quality of the suggestion. This means that if the system shows an option that has no chance of becoming optimal ( p opt ( then p ( r ) will remain unchanged. Instead if p opt ( o suggestion is shown), p ( r ) will go to 0 if no reaction is observed.
To handle the probability distribu tion of continuous attributes, we discretized the domains in few intervals, because otherwise the probabilistic update would be untractable.

We split the data (100 logs of user interactions) in two sets. 1. A learning data-set, used to represent prior knowledge to feed 2. A test data-set, used to genera te preference models of simu-
We ran several simulations with a random split of samples be-tween the learning and the test data-set.

In the simulations, users have a set of preferences generated ac-cording to a particular probabilistic distribution. Every step of the simulation represents an interaction with the recommender system. Based on the preferences that are known to the system a set of can-didates and suggestions are retrieved at each step. When the sug-gestions retrieved by the strategy satisfies the look-ahead principle, a new preference is stated and considered in the user model. In this case, we say that a new preference is discovered and the sugges-tions were appropriately chosen.

According to our user studies, on average the users interact on average for 6.25, of which in 3.30 cycles the users did not either remove nor add preferences. We implemented the simulation so that the interaction continues until either the user model is complete or the simulated user states no further preference twice in a row (choosing a prudent approach).

If all preferences are stated, the preference model is complete (a full discovery of the preferences). In this case, it is guaranteed that the user can find the target (his true best option) by a series of trade-off actions that modify the relative importance of the preferences.
The results are shown in Table 2. We compare the basic for-mulation of model-based suggestions (assuming uniform distribu-tion), model-based suggestions with prior knowledge and adaptive suggestions. We measure the fraction of preferences discovered.
Even with a simple probabilistic model of the user, the gains obtained by adaptive suggestions become considerable, specially when 3 or more suggestions are shown (72% of runs achieve full discovery for adaptive suggestions, 58% prior distribution, 42% normal suggestion). There is a certain gain of adaptive suggestions, that are better than suggestions generated according to prior distri-bution. It is important to note that the performance of suggestions with prior knowledge degrades with respect to the first simulation setting.

It would be interesting to compare the effects of different choices for the value p ( st | r i , X  ) of the probabilistic behavior of the user.
The internet allows access to an unprecedented variety of infor-mation, but forces people to see the world through the restricted lens of a computer. Currently, only a small minority of users man-age to reliably find the items they are looking for, and better rec-ommendation tools are badly needed.

We believe that the mixed-initiative approach for preference-based search presented in this paper is a major step in this direc-tion. Such tools model a user X  X  preferences and show a set of op-tions generated on the basis of this model. The user reacts by either picking one as her choice or modifying her preference model. To Table 2: The average fraction of discovered preferences and the percentage of full discovery (interactions that acquire a complete preference model) in the simulation with probabilis-tic profile acquired from real logs. We compare the adaptive model-based suggestions, model-based suggestions with prior knowledge and the standard model-based suggestion strategy assuming uniform distribution. increase the quality of the final decisions, the system provides sug-gestions to stimulate preference expression. The main contribution of this paper is to further increase the recommendation accuracy by refining the uncertainty over users X  preferences and making sug-gestions that are adaptive to the users reactions.

We evaluated the effectiveness of suggestion strategies in elic-iting user preferences. We divide the available data logs of user interactions in two parts: one is used for learning and the other for testing the strategy. We showed that our approach works sig-nificantly better than standard model-based suggestions. While a considerable part of the improvement is due to prior knowledge, the adaptation of suggestions according to the reactions gives an important improvement, especially in cycles in which the user does not state additional preferences.

In the simulations, we manage to discover the complete set of preferences almost all the time, a dramatic improvement from ear-lier results using suggestions w ith fixed probabilities. In earlier work, we have shown that sugges tions with fixed probabilities al-ready increase decision accuracy from 25% to 70%, so we would expect an even higher accuracy from adaptive suggestions, thus bringing us closer to our goal of a practically reliable conversa-tional recommendation system. We are hoping to verify this expec-tation through additional user studies soon. [1] C. Boutilier. A pomdp formulation of preference elicitation [2] R. I. Brafman and M. Tennenholtz. Modeling agents as [3] R. D. Burke. Hybrid recommender systems: Survey and [4] R. D. Burke, K. J. Hammond, and B. C. Young.
 [5] R. D. Burke, K. J. Hammond, and B. C. Young. The FindMe [6] U. Chajewska, D. Koller, and R. Parr. Making rational [7] A. T. Daniel Kahneman, Paul Slovic. Judgement under [8] R. Fagin. Fuzzy queries in multimedia database systems. In [9] B. Faltings, M. Torrens, and P. Pu. Solution generation with [10] P. Gorniak and D. Poole. Predicting future user actions by [11] R. L. Keeney and H. Raiffa. Decisions with Multiple [12] W. Kiesling. Foundations of preferences in database systems. [13] J. Lang. A preference-based interpretation of other agents X  [14] G. Linden, S. Hanks, and N. Lesh. Interactive assessment of [15] K. McCarthy, J. Reilly, L. McGinty, and B. Smyth.
 [16] D. McSherry. Diversity-conscious retrieval. In Proceedings [17] J. Payne, J. Bettman, and E. Johnson. The Adaptive Decision [18] R. Price and P. R. Messinger. Optimal recommendation sets: [19] P. Pu and L. Chen. Integrating tradeoff support in product [20] P. Pu and B. Faltings. Enriching buyers X  experiences: the [21] P. Pu, B. Faltings, and M. Torrens. Effective interaction [22] P. Pu and P. Kumar. Evaluating example-based search tools. [23] P. Pu, P. Viappiani, and B. Faltings. Increasing user decision [24] J. Reilly, K. McCarthy, L. McGinty, and B. Smyth. Dynamic [25] S. Shearin and H. Lieberman. Intelligent profiling by [26] H. Shimazu. Expertclerk: Navigating shoppers buying [27] B. Smyth and P. McClave. Similarity vs. diversity. In [28] B. Smyth and L. McGinty. The power of suggestion. In [29] M. Stolze and M. Str X bel. Utility-based decision tree [30] F. N. Tou, M. D. Williams, R. Fikes, D. A. H. Jr., and T. W. [31] P. Viappiani, B. Faltings, and P. Pu. Evaluating [32] P. Viappiani, B. Faltings, and P. Pu. The lookahead principle [33] P. Viappiani, B. Faltings, and P. Pu. Preference-based search
