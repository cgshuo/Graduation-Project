 Christopher Painter-Wakefield PAINT 007@ CS . DUKE . EDU Duke University, Durham NC Feature selection and regularization are becoming increas-ingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and appli-cability of RL (Parr et al., 2007; Mahadevan &amp; Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011). Very often (though not always (Farahmand et al., 2008)) sparseness is viewed as a desirable goal or side ef-fect of regularization. If the true value function is known to be sparse, then the reasons for desiring a sparse solution are clear. Even when the form of the true value function is not known, sparsity may still be desired because sparsity can act as a regularizer, and because sparse solutions tend to be more understandable to humans and more efficient to use. Favoring sparsity can lead to faster algorithms in some cases (Petrik et al., 2010).
 One optimization-based approach to the problem of fea-ture selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on L 1 regularization has adapted techniques from the super-vised learning literature (Tibshirani, 1996) for use with RL (Kolter &amp; Ng, 2009). Another approach that has re-ceived renewed attention in the supervised learning com-munity is that of using a simple algorithm that greedily adds new features (Tropp, 2004; Zhang, 2009). Such algo-rithms have many of the good properties of the L 1 regular-ization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data despite the myopia associated with greediness. The most basic greedy algorithm for feature selection for regression, matching pursuit, uses the correlation between the residual and the candidate features to decide which feature to add next (Mallat &amp; Zhang, 1993). This paper considers a variation called orthogonal matching pursuit (OMP), which recomputes the residual after each new fea-ture is added, as applied to reinforcement learning. It is related to BEBFs (Parr et al., 2007), but it differs in that it selects features from a finite dictionary. OMP for RL was explored by Johns (2010) in the context of PVFs (Mahade-van &amp; Maggioni, 2007) and diffusion wavelets (Mahadevan &amp; Maggioni, 2006), but aside from this initial exploration of the topic, we are not aware of any efforts to bring the the-oretical and empirical understanding of OMP for reinforce-ment learning to parity with the understanding of OMP as applied to regression.
 This paper contributes to the theoretical and practical un-derstanding of OMP for RL. Variants of OMP are analyzed and compared experimentally with existing L 1 regularized approaches. We demonstrate that perhaps the most natu-ral scenario in which one might hope to achieve sparse re-covery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, lacks theoretical guarantees but empirically outperforms OMP-BRM and prior methods both in approximation accuracy and efficiency on several benchmark problems. We aim to discover exact or good, approximate value functions for Markov reward processes (MRPs): M = ( S,P,R, X  ) . Given a state s  X  S , the probability of a tran-expected reward of R ( s ) . We do not address the question of optimizing the policy for a Markov Decision Process, though we note that policy evaluation, where P = P  X  , by some policy  X  , is an important intermediate step in many algorithms. A discount factor  X  discounts future rewards such that the present value of a trajectory s t =0 ...s t = n P The true value function V  X  over states satisfies the Bellman equation : where T is the Bellman operator and V  X  is the fixed point of this operator.
 In practice, the value function, the transition model, and the reward function are often too large to permit an explicit, exact representation. In such cases, an approximation ar-chitecture is used for the value function. A common choice is  X 
V =  X  w , where w is a vector of k scalar weights and  X  stores a set of k features in an n  X  k feature matrix. Since n is often intractably large,  X  can be thought of as populated by k linearly independent basis functions ,  X  1 ... X  k , which define the columns of  X  . We will refer to a basis formed by selecting a subset of the features using an index set as a subscript. Thus,  X  I contains a subset of features from  X  , where I is a set of indices such that basis function  X  i is included in  X  I if i  X  X  .
 For the purposes of estimating w , it is common to replace  X  with  X   X  , which samples rows of  X  , though for conciseness of presentation we will use  X  for both, since algorithms for estimating w are essentially identical if  X   X  is substituted for  X  . A number of linear function approximation algo-rithms such as LSTD (Bradtke &amp; Barto, 1996) solve for the Algorithm 1 OMP Input: X  X  R n  X  k ,y  X  R n , X   X  R .
 Output: Approximation weights w .

I  X  X } w  X  0 repeat until c j  X   X  or  X  I = {} w which is a fixed point: where  X   X  is the  X  -weighted L 2 projection and where  X  0 P  X  in the explicit case and is composed of sampled next features in the sampled case. We likewise overload T for the sampled case.
 We make use of the notation  X   X  to represent the pseudo-inverse of  X  specifically defined as  X   X   X  ( X  T  X )  X  1  X  T general we assume that the weighting function  X  is implicit in the sampling of our data, in which case the projection operator above is simply  X  =  X  X   X  .
 We also consider algorithms which solve for w minimizing the Bellman error: This is the Bellman residual minimization (BRM) approach espoused by Baird (1995). 3.1. OMP for Regression Algorithm 1 is the classic OMP algorithm for regression. It is greedy in that it myopically chooses the feature with the highest correlation with the residual and never discards fea-tures. We say that target y is m-sparse in X if there exists an X opt composed of m columns of X and correspond-ing w opt such that y = X opt w opt , and X opt is minimal in the sense there is no X 0 composed of fewer columns of X which can satisfy y = X 0 w . Of the many results for sparse recovery, Tropp X  X  (2004) is perhaps the most straightfor-ward: Theorem 1 If y is m-sparse in X , and then OMP called with X , y , and  X  = 0 will return w opt in m iterations.
 In the maximization, the vectors x i correspond to the columns of X which are not needed to reconstruct y ex-actly. In words, this condition requires that the projection of any suboptimal feature into the span of the optimal fea-tures must have small weights. Note that any orthogonal basis trivially satisfies this condition.
 Tropp also extended this result to the cases where y is ap-proximately sparse in X , and Zhang (2009) extended the result to the noisy case. 3.2. L 1 Regularization in RL In counterpoint to the greedy methods based on OMP, which we will explore in the next section, much of the re-cent work on feature selection in RL has been based on least-squares methods with L 1 regularization. For regres-sion, Tibshirani (1996) introduced the LASSO, which takes matrix X and target vector y and seeks a vector w which minimizes k y  X  Xw k 2 subject to a constraint on k w k While Tibshirani uses a hard constraint, this is equivalent to minimizing for some value of  X   X  R + .
 Loth et al. (2007) apply the LASSO to Bellman residual minimization. Replacing the residual y  X  Xw in equation 3 with the Bellman residual, we obtain Trivially, if we let X =  X   X   X   X  0 and y = R , we can substitute directly into equation 3 and solve as a regression problem. The regression algorithm used by Loth et al. is very similar to LARS (Efron et al., 2004).
 A harder problem is applying L 1 regularization in a fixed point method akin to LSTD. The L 1 regularized linear fixed point is the vector w solving introduced by Kolter &amp; Ng (2009). Kolter and Ng pro-vide an algorithm, LARS-TD, which closely follows the approach of LARS. Johns et al. (2010) followed LARS-TD with an algorithm, LC-TD, which solves for the L 1 regular-ized linear fixed point as a linear complementarity problem. It is instructive to note that LARS bears some resemblance to OMP in that it selects as new features those with the highest correlation to the residual of its current approxima-tion. However, where OMP is purely greedy and seeks to Algorithm 2 OMP-BRM Input: Output: call OMP with X =  X   X   X   X  0 , y = R ,  X  =  X  use all active features to the fullest, LARS is more mod-erate and attempts to use all active features equally , in the sense that all active features maintain an equal correlation with the residual. One aspect of the LARS approach that sets it quite apart from OMP is that LARS will remove fea-tures from the active set when necessary to maintain its in-variants.
 Intuitively, LARS and algorithms based on LARS such as LARS-TD have an advantage in minimizing the number of active features due to their ability to remove features. LC-TD also adds and removes features. These methods do suffer from some disadvantages related to this ability, however. LARS-TD can be slowed down by the repeated adding and removing of features. Worse, both LARS-TD and LC-TD involve computations which are numerically sensitive and are not guaranteed to find the desired solution in all cases since (unlike in the pure regression case) the task of finding an L 1 regularized linear fixed point is not a convex optimization problem. We present two algorithms for policy evaluation: OMP-BRM and OMP-TD. 1 As the names suggest, the first algo-rithm is based on Bellman residual minimization (BRM), while the second is based on the linear TD fixed point. Algorithm 2, OMP-BRM, is the simpler algorithm in the sense that it essentially performs OMP with features  X   X   X   X  0 using the reward vector as the target value. OMP-BRM is different from the OMP-BR algorithm introduced by Johns (2010), which selected basis functions from  X  . Algorithm 3, OMP-TD, applies the basic OMP approach to build a feature set for LSTD. 2 OMP-TD is similar in ap-proach to the approximate BEBF algorithm of Parr et al. (2007), in which each new feature is an approximation to the current Bellman residual. In OMP-TD, rather than ap-proximate the Bellman residual, we simply add the feature Algorithm 3 OMP-TD Input: Output:
I  X  X } w  X  0 repeat until c j  X   X  or  X  I = {} which, among features not already in use, currently has the highest correlation with the residual. After adding a fea-ture, the new fixed point is computed using the closed form LSTD fixed point equation, and the new residual is com-puted. The main results of the BEBF paper apply to OMP-TD: mainly, that each new feature improves a bound on the distance between the fixed point and the true value function V  X  , as long as the correlation between the feature and the residual is sufficiently large. 4.1. Sparse Recovery in OMP-TD Theorem 2 Even if V  X  is m -sparse in an orthonormal ba-sis, OMP-TD cannot guarantee exact recovery of V  X  in m iterations.
 P
ROOF (By counterexample) Consider the Markov chain in figure 1. The arcs indicate deterministic transitions. Sup-pose R ( S 2) = R ( S 3) = R ( S 4) = 1 , R ( S 5) = 0 , and R ( S 1) =  X  (  X  +  X  2 +  X  3 ) , then V  X  = [0 , 1 +  X  +  X   X , 1 , 0] . With an orthonormal basis  X  defined by the indica-tor functions  X  i ( s ) = I ( s = s i ) , V  X  is 3-sparse in  X  , with opt = { 2 , 3 , 4 } . Starting from the empty set of features, the residual vector is just R . OMP-TD will pick the vector with the correlation with the residual, which will be  X  1 vector that is not in opt . 2 The next feature added by OMP-TD could be S 4 , then S 3 and S 2 . Selecting a single vector not in opt suffices to es-tablish the proof, which means that we could have short-ened the example by removing states S 2 and S 3 and con-necting S 1 directly to S 4 . However, the longer chain is useful to illustrate an important point about OMP-TD: It is possible to add a gratuitous basis function at the very first step of the algorithm and the mistake may not be evident until an arbitrary number of additional basis functions are added. This example is easily extended so that an arbi-trary number of gratuitous basis functions are added before the first basis function in opt is added by making multiple copies of the S 1 state (together with the corresponding in-dicator function features). Such constructions can defeat modifications to OMP-TD that use a window of features and discard gratuitous ones (Jain et al., 2011) for any fixed-size window.
 An algorithm that chooses features from  X  based upon the Bellman residual (OMP-BR in the terminology of Johns (2010)), would suffer the same difficulties as OMP-TD in this example. The central problem is that the Bellman error may not be a trustworthy guide for selecting features from  X  even if  X  is orthogonal. 4.2. Sparse Recovery in OMP-BRM Lemma 1 If V  X  is m -sparse in  X  , then R is at least m -sparse in ( X   X   X P  X ) .
 P
ROOF Since V  X  = ( I  X   X P )  X  1 R , we have The implication is that we can perform OMP on the basis ( X   X   X P  X ) and, if there is a sparse representation for R in the basis, we will obtain a sparse representation of V  X  as well. This permits a sparse recovery claim for OMP-BRM that is in stark contrast to the negative results for OMP-TD. Theorem 3 If V  X  is m -sparse in  X  , and for X =  X   X   X P  X  , then OMP-BRM called with  X  ,  X  0 = P  X  , R ,  X  , and  X  = 0 will return w such that V  X  =  X  w in at most m iterations.
 P ROOF (sketch) The proof mirrors a similar proof from Tropp (2004) and is provided in detail in the full version of the paper 3 . Lemma 1 implies that R is m -sparse in X. Since OMP-BRM does OMP with basis X and target R , the sparse recovery results for OMP for regression apply directly. 2 Tropp X  X  extension to the approximate recovery case also applies directly to OMP-BRM (see full version of the pa-per 3 ). For the noisy case, we expect that the results of Zhang (2009) could be generalized to RL, but we defer that extension for future work. 4.3. Sparse Recovery Behavior We set up experiments to validate the theory of exact re-covery for OMP-BRM, and to investigate the behavior of OMP-TD in similar circumstances. First, we generated a basis for the 50-state chain problem (see section 5) in which the first three basis functions provide an exact reconstruc-tion of V  X  , and the remaining 997 features are randomly generated, which satisfies equation (4). (Generating such a basis required first generating a much larger (50 x 3000) matrix, then throwing out features which violated the exact recovery condition, and finally trimming the matrix back down to 1000 features. The first three features were con-structed by finding two random features which highly cor-relate with V  X  , then adding in a third feature which was the reconstruction residual using the first two features.) By using the resulting basis in OMP-BRM with exact data (i.e., where  X  0 = P  X  ), we found that, for sufficiently small threshold value, OMP-BRM uses exactly the first three fea-tures in its approximation, in accordance with theory. We also tried OMP-BRM with noisy data by sampling 200 state transitions from the 50-state chain problem. In this case, we found that OMP-BRM reliably selected the first three fea-tures before selecting any other features. Interestingly, the same basis proved to enable exact recovery for OMP-TD as well. Using the same 200 samples, OMP-TD selected the first three features before selecting any other features. While we have theoretical results for OMP-BRM that sug-gest we should be able to perform optimal recovery under certain conditions on the feature dictionary, practical prob-lems contain noise, which current theory does not address. In addition, the desired conditions on the feature dictionary may not hold and it can be difficult to verify if they do hold since these conditions are a property of both the fea-tures and the transition function for OMP-BRM. For OMP-TD, we have negative worst-case results, but Section 4.3 gives hope that things may not be so dismal in practice. To gain some understanding of how these algorithms perform under typical conditions, we performed experiments on a number of benchmark RL problems. Figure 2 shows our main results, the approximation accuracy achieved by each of four algorithms on each problem. Table 1 summarizes the benchmark problem properties and some experimental settings. The algorithms studied included: OMP-BRM : The OMP-BRM algorithm as described above, but with some additional machinery to improve perfor-mance in actual use. It is well known (Sutton &amp; Barto, 1998) that BRM is biased in the presence of noise, i.e., when samples are taken from a stochastic transition func-tion. One solution to this problem is to use double samples for each transition. In each of our experiments, we ran with and without doubled samples, and we report the behavior of the better performing option. Table 1 records which exper-iments use doubled samples. Figure 3 shows how doubling samples affects performance on the 50-state chain problem. In all cases, the number of samples in the Samples column refers to the number of starting states. 4 In the doubled case we also add in a small amount (0.01) of L 2 regularization in order to keep the algorithm well behaved (by keeping the matrices to be inverted well conditioned).
 OMP-TD : The OMP-TD algorithm as described above, but with a small amount (0.01) of L 2 regularization when com-puting the final solution at each threshold. This change seemed to be important for some of the harder benchmarks such as puddleworld and two-room, where without regu-larization, the LSTD solution at various threshold values would occasionally exhibit unstable behavior. We use reg-ularization on all problems, as it seems to cause no issues even for benchmarks which do not need it.
 LARS-BRM : This is our implementation of the algorithm of Loth et al. (2007), which effectively treats BRM as a regression problem to be solved using LARS. Note that there is no provision for sample doubling in this algorithm, which may affect its performance on some problems. LARS-TD : This is the LARS-TD algorithm of Kolter &amp; Ng (2009). Again we found it useful sometimes to include a small amount (0.01) of L 2 regularization, giving the  X  X las-tic net X  (Zou &amp; Hastie, 2005) solution. The additional reg-ularization does not always benefit; however, we report in figure 2 the better of the two for each benchmark. Table 1 records which experiments use L 2 regularization in LARS-TD.
 In figure 2, we have plotted performance of the various al-gorithms on a variety of benchmark problems. The vertical axis is the root mean square error with respect to the true value function, V  X  . For the discrete state problems, V is computed exactly, while for the continuous state prob-lems V  X  is computed at a large number of sampled states by Monte Carlo rollouts. The number of trials over which the values are averaged is reported in table 1. The horizon-tal axis gives the threshold/regularization coefficient value  X  for the value function V  X  plotted.
 In general, the meaning of  X  for the OMP algorithms is dif-ferent than for the LARS algorithms. However, there is a strong similarity between the two in that, for both, a solu-tion at value  X  implies that there are no further features with a correlation with the current residual of  X  or larger. This is explicit in the OMP algorithms, and implicit in the fixed point conditions for LARS-TD (n.b. Kolter &amp; Ng (2009), equation 9) and LARS-BRM. Surprisingly, given that OMP is greedy and never removes features, we find that the spar-sity of the solutions given by the algorithms is similar for the same values of  X  ; e.g., see figure 4.
 As figure 2 shows, OMP-TD is generally competitive with, or better than, LARS-BRM and LARS-TD on most of the benchmark problems. In addition, we note that the OMP-based algorithms are considerably faster than the LARS-based algorithms; see figure 5 for a comparison of compu-tation time on the puddleworld problem.
 While OMP-TD generally leads in our benchmarks, we should point out some caveats. With very small numbers of samples (even fewer than shown in our experiments) OMP-TD was somewhat more prone to unstable behavior than the other algorithms. This could simply mean that OMP-TD requires more L 2 regularization, but we did not explore that in our experiments. An indication that OMP-TD can require more L 2 regularization than the other algorithms is evident in our OMP-TD experiments for Mountain Car, where the high variance for low values of  X  arises from just two (out of 100) batches of samples. In these cases, it ap-pears that OMP-TD is adding a feature which is essentially a delta function on a single sample. Without additional L regularization, OMP-TD produces very poor value func-tions for these batches.
 For all of the experiments except for blackjack, the fea-tures are radial basis functions (RBFs). There are multiple widths of RBFs placed in the state space in grids of various spacing. For blackjack, the basis functions are indicators on groups of states. All features are normalized, although we tried both with and without normalization, and the re-sults were qualitatively similar.
 More information about each of the experimental domains can be found in the full version of the paper 3 . In this paper we have explored the theoretical and practi-cal applications of OMP to RL. We analyzed variants of OMP and compared them experimentally with existing L 1 regularized approaches. We showed that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, em-pirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems. There are two natural directions for further development of this work. Our theoretical results for OMP-BRM built upon the simplest results for sparse recovery in regression and do not apply directly to more realistic scenarios that involve noise. Stronger results may be possible, building upon the work of Zhang (2009). A more interesting, but also more challenging, future direction would be the theo-retical development to explain the extremely strong perfor-mance of OMP-TD in practice despite negative theoretical results on sparse recovery. For example, there could be an alternate set of conditions on the features that frequently hold in practice and that can be shown theoretically to have good sparse recovery guarantees.
 This work was supported by NSF IIS-1147641. Opinions, findings, conclusions or recommendations herein are those of the authors and not necessarily those of NSF. The au-thors also wish to thank Susan Murphy and Eric Laber for helpful discussions.

