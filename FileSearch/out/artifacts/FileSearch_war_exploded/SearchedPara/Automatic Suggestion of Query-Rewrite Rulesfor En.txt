 Enterprise search is challenging for several reasons, notably the dynamic terminology and jargon that are specific to the enterprise domain. This challenge is partly addressed by having domain experts maintaining the enterprise search en-gine and adapting it to the domain specifics. Those admin-istrators commonly address user complaints about relevant documents missing from the top matches. For that, it has been proposed to allow administrators to influence search results by crafting query-rewrite rules, each specifying how queries of a certain pattern should be modified or augmented with additional queries. Upon a complaint, the adminis-trator seeks a semantically coherent rule that is capable of pushing the desired documents up to the top matches. How-ever, the creation and maintenance of rewrite rules is highly tedious and time consuming. Our goal in this work is to ease the burden on search administrators by automatically suggesting rewrite rules. This automation entails several challenges. One major challenge is to select, among many options, rules that are  X  X atural X  from a semantic perspective (e.g., corresponding to closely related and syntactically com-plete concepts). Towards that, we study a machine-learning classification approach. The second challenge is to accom-modate the cross-query effect of rules X  X  rule introduced in the context of one query can eliminate the desired results for other queries and the desired effects of other rules. We present a formalization of this challenge as a generic com-putational problem. As we show that this problem is highly intractable in terms of complexity theory, we present heuris-tic approaches and optimization thereof. In an experimental study within IBM intranet search, those heuristics achieve near-optimal quality and well scale to large data sets. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Query formulation General Terms: Algorithms, Performance, Theory Keywords: Query reformulation, query-rewrite rules, en-terprise search
While search engines on the Web are highly successful in content retrieval, enterprise search remains an important living challenge [2, 11, 12]. The retrieval community identi-fied various sources of difficulty [5, 7, 10, 11], including the sparseness of link structure and anchor text, low economic incentive of content providers to promote easy search access, and a strong presence of dynamic, domain-specific terminol-ogy and jargon. Another practical difficulty lies in the fact that enterprise search deployments are typically managed by administrators who are doma in experts but not search ex-perts. Hence, although these administrators well understand the specific content and search needs of the domain, trans-lating that knowledge into tuning the underlying retrieval model is a nontrivial (and so metimes impossible) task.
To address the above challenges, research and industrial groups [8, 9, 21] advocated the design of search architectures that are (1) easily comprehensible (featuring fairly trans-parent ranking mechanisms), and (2) easily customizable through intuitive runtime rules to program domain knowl-edge. One important type of runtime rules is that of query-rewrite rules (or rewrite rules for short) that, in essence, introduce human-controlled query reformulation [18, 19] at runtime. By augmenting or modifying the posed search query, rewrite rules enable administrators to easily satisfy a wide range of maintenance needs such as: adaptation of terminology (e.g., produce a query where green card is re-placed with permanent residency ), query specialization (e.g., h5n1 instead of seasonal flu ,or issi , 1 the company X  X  software installation tool, instead of download ), query generalization (e.g., intranet account management instead of intranet pass-word change ), noise removal (e.g., elimination of web page from a query like james allan web page ), and so on.
This work is motivated by a deployment of the above ar-chitecture in the IBM internal search. There, the type of tasks that dominate the effort of administrators is that of pushing up relevant documents for specific search queries. Such a task is typically triggered by an employee complaint mentioning a specific query and a relevant document (URL) missing from the top results (e.g., the first page). For exam-ple, such a complaint can be that http://issi.ibm.com/lnotes is missing from the top results for lotus notes download .In turn, the administrator can phrase a rewrite rule to accom-modate the potential mismatch between the query and the document. Those rewrite rules are in the spirit of (actually, a generalization of) the  X  X uery template X  rules [1, 20]. In
IBM Standard Soft ware Installer.
We replaced each example URL with a simplified variant. particular, a rule is usually applicable to multiple queries (rather than to just one), and it usually results in augmen-tation (rather than replacement ) of the query by the newly generated query. For example, the administrator can intro-duce the rule download  X  issi that,ineffect,incorporates the results for lotus notes issi into those for lotus notes down-load whenever the latter query is posed. In Section 2 we describe our engine architecture and rules in more detail.
Manually formulating rewrite rules to handle complaints is an extremely tedious and time consuming process. First, the administrator needs to inspect the specified document in order to come up with relevant rules. For each relevant rule, a rough estimation is made on its effect on other queries (and for that, query logs may be consulted). Next, the ad-ministrator tries out her ru le of choice (in a sandbox en-vironment). If the desired effect (pushing up the desired match) is not realized, this process is repeated with alterna-tive rules, sometimes until the conclusion that the problem lies in the ranking algorithm and/or the backend analyt-ics. In this work, we seek to aid administrators of enterprise search by automatically producing rewrite-rule suggestions that are already validated as problem solvers. Towards this goal, we need to overcome two major challenges, which are the subject of this paper. These challenges are generic to essentially every search system that features administration by rewrite rules, and we confront these challenges in a level of abstraction that departs from our specific testbed. Challenge 1: Generating Intuitive Rules. A straight-forward approach towards suggesting rewrite rules is to gen-erate candidates from the desired document (or specific parts thereof), and filter out the candidates that fail to achieve the desired effect. However, the number of possible rules obtained in this way can be overwhelmingly large. For ex-ample, in our experimentation over real data (within IBM intranet search), our automation often reaches around 100 suggestions, and in some cases around 1000. More impor-tantly, the vast majority of the suggested rules do not make sense to the administrator; that is, they are not rules of the kind she would devise or perceive as intuitive. For illustra-tion, Figure 1 shows some of the suggestions we obtained. In the top part of the table in the figure, relating to seasonal flu , the top two are reasonable but the other hardly make sense. Note, however, that determining what  X  X easonable X  or  X  X ense making X  means may require domain knowledge. As an example, IBM uses SCIP for consulting on organi-zational reconstruction; thus, the top rewrites for change management in the bottom part of Figure 1 make sense. Yet one can hardly accept the other rules in that part.
We refer to a rule that makes sense as natural .Formu-lating natural rules is necessary, as administrators should be confident in the semantic justification of each rule. Put differently, comprehensiveness, which is at the heart of the architecture philosophy, is violated if rules are inconsistent with human judgment. (This problem does not arise in on-line query reformulation/expansion [4] that affect only the internals of the search process.) Thus, crucial to realizing automatic rule suggestion is a component that classifies rules into natural and unnatural ones. In Section 3 we present a machine-learning approach to realizing such a component.
A related problem is that of recommending query alter-natives to end users [3, 13, 22]. However, a query alternative applies just to the user-posed query, hence is different from a rewrite rule. Furthermore, the techniques there are heav-ily based on query logs and/or anchor text, which may be sparse in the enterprise Intranet. The same holds true for the work of Kraft and Zien [14], aiming to find reformulations that are  X  X losely related X  to the original query. Moreover, in these works a recommendation (or reformulation) has no effect beyond the specific engine-user interaction at hand; it is not the case here, as we discuss in the next challenge. Challenge 2: Cross-Query Effect. As a rewrite rule can affect multiple queries, its inclusion may actually reduce the overall search quality. Further, a rule can cancel the posi-tive effects achieved by previous rules. To illustrate, in the example of Figure 4 (which we discuss in detail through-out Section 4) the rule spreadsheets  X  symphony rewrites spreadsheets download into symphony download , resulting in the document d 2 being a top match. Following a complaint on the query lotus notes download , we introduce the rule r : download  X  issi . However, r 1 also affects our previous spreadsheets download by pushing the desired d 2 below d 1
To address the above problem, the search administrator maintains a representative benchmark of queries and de-sired matches (possibly weighted by popularity) that in-cludes those indicated in past complaints. To estimate the effect of a candidate rule, search quality is evaluated (using a quality estimator of choice, e.g., DCG) by running the en-gine against the benchmark. If a negative effect is detected, the administrator can avoid the rule, consider a new rule, or just accept the loss in quality. But here, automation can con-tribute significantly in terms of both effort and quality. We can allow the system to automatically suggest the selection of a subset of the rules to optimize quality (w.r.t. the un-derlying estimator). Moreover, the administrator can then choose multiple rules (instead of just one) to address a spe-cific complaint, and thereby enrich the search space for the system. In the above example, for instance, if the system had the rule notes download  X  notes issi at its disposal, it could suggest to use it instead of r 1 , and thus maintain the top results for both spreadsheets download and lotus notes download . In Section 4, we formalize this idea as a combi-natorial optimization problem. Unfortunately, this problem turns out to be computationally hard (NP-hard), and even hard to approximate within any nontrivial ratio. Neverthe-less, we propose greedy heuristics and optimizations thereof.
A related optimization problem has been recently studied by Ozertem et al. [19], where one is given a collection of query reformulations (each applying to one query) to select from, and the goal is to maximize the probability of a click in a user model built from the query log.
 Organization. The rest of the paper is organized as fol-lows. In Section 2 we briefly describe our search architecture. We study the above two challenges in Sections 3 and 4, re-spectively. In Section 5 we present an experimental study within IBM intranet search, and we conclude in Section 6.
In this section, we outline the architecture of the engine deployed in IBM intranet search [21], to the extent needed as background for this work. 3 Figure 2 depicts a simplified description of the conceptual runtime flow. The front-end takes the input search query q and, by applying rewrite rules, produces a set Q of queries. In turn, the queries in Q are evaluated against the index, resulting in a list of results for each query. The results are then aggregated together to produce the final ranked list.

Similarly to existing search methodologies [6, 15, 23], this engine leverages structural information from Web pages (such as links, anchor text, headers and titles) in retrieval. The basic idea is to produce high-quality fields associated with each document, and at runtime use these fields for ranking. Rewrite rules program the query rewriting component of Figure 2 to affect the search as needed. The rewrite rules in our engine are in the spirit of the query-template rules [1, 9, 20]. Rather than giving an elaborate specification of the rule language, we give examples (in a simplified language). The following rule fires when the query is of the form x info , where x is belongs to a dictionary of products; it introduces a new query with the term info removed.
 The next rule, involving a regular expression, fires when the query contains the word lotus followed by either presenta-tions or spreadsheets ; it introduces a new query with the two words replaced by lotus symphony .
 The next rule fires when the query contains msn search ,and introduces a new query with msn search replaced by bing . The plus sign in  X  + assigns preference to the new query (containing bing ) over the original query (containing msn search ). In effect, results for the new query are rewarded
More details on this search engine can be found in the IBM page of Infrastructure for Intelligent Information Systems http://www.almaden.ibm.com/cs/disciplines/iiis. compared to those for the original query. We do not discuss here the details of this reward , as they are of low importance to this work.

To simplify our study, this work is restricted to rewrite rules of the form CONTAINS: s  X  + t , which we denote simply as s  X  t ,where s and t areterms. Wefocuson this particular type of rules since, besides their simplicity, we found that they are the most commonly used in our en-terprise. Moreover, with this restriction the problems we consider are already challenging. While some of our contri-butions can be easily extended to more general rules, others require nontrivial future effort.
In this section, we consider the first challenge discussed in the introduction, where we explained the intuition underly-ing the notion of natural rewrite rules and their importance to the realization of automatic rule suggestion in enterprise search. Our approach is to first generate a (possibly large) set of candidate rules by pure syntactic means, and then to classify them through (supervised) machine learning. Candidate generation. The use case we consider involves aquery q and a desired match (document) d that is miss-ing from the top results. Our generation of candidate rules s  X  t is fairly straightforward: we produce a set S of left-hand-side candidates, a set T of right-hand-side candidates, and output the Cartesian product S  X  T .Theset S consists of all the n -grams (subsequences of n tokens) of q ,where n is bounded by a constant (5 in our implementation). In princi-ple, we could similarly choose T as the set of all n -grams of d (which could result in a huge number of candidates, even if n is bounded). But in our implementation, T consists of the n -grams just from the high-quality fields of d (produced during back-end analysis). Recall that those fields were dis-cussed in Section 2. As an example, suppose that q is the query  X  change management info  X  and one of the considered fields is  X  welcome to scip strategy &amp; change internal practice . X  The following are among the candidate rules.
Even with the restriction to high-quality fields, the above step may generate a large number of candidate rules, most of which are unnatural (hence, useless to the administrator), as illustrated in Figure 1. We address this problem by taking a machine-learning approach: we identify a set of features and learn classification models from manually labeled data to classify rules into natural and unnatural ones. Features. Table 1 lists the feature set we use for classify-ing rules. These features fall into three categories: syntactic features, features based on query-log statistics, and features based on corpus statistics. Next, we briefly discuss the fea-tures in each category. We denote the examined rule as s  X  t ,where s and t are strings of words.

The syntactic features can be indicative of the syntac-tic coherence of a rule. For example, using the Boolean features beginSW ( r )a nd endSW ( r ) follows our observation that when s or t begins or ends with stop words, the rule is rarely deemed natural. We consider two types of stop words: those from conventional (English) dictionaries and those from domain-specific dictionaries (including words like  X  X elcome, X   X  X ndex X  and  X  X omepage X ). Table 1: Features for natural-rule recognition; the considered rule is s  X  t ,and u refers to either s or t
The category of query-log statistics has the single feature log refFreq ( s, t ), representing common wisdom on query re-formulation. In our case, we analyzed the query log of (four months of) intranet search at IBM. For a pair q 1 and q 2 queries, refFreq ( q 1 ,q 2 ) is the number of sessions that begin with q 1 and contain q 2 (posed later than q 1 in the session).
Under the category of corpus statistics we have numerical statistics on s and t drawn from our engine index. The num-ber freq ( u ) counts the documents containing u as an n -gram (where u is s in one feature, and t in another). Similarly, freq ( s  X  t ) counts the documents containing both s and t as n -grams. We use log freq ( u )andlog freq ( s  X  t ) to capture the popularity of s and t as well as correlation thereof. Finally, HQfreq ( u ) is the frequency of u in our high-quality fields, which roughly reflects the popularity of u in titles. Classification models. We implemented two classifica-tion models over our vector f of features. The first model is simply a linear classifier, which we trained on manually labeled data via SVM. The second model, which we found to provide a significant improvement over the first, generalizes the first by incorporating a Decision Tree (DT for short). More specifically, the second mo del is a restricted version of a DT with linear-combination splits [16, 17], and we call it rDTLC for short. Given the vector f of features, a DT with linear splits has a condition the a i and  X  are to be learned in training). Our rDTLC restricts this family by (1) bounding the depth (by 3 in our implementation), and (2) having univariate splits (i.e., one-variable comparisons to thresholds) in all but the bot-tommost levels. Figure 3 illustrates our rDTLC. Note that here we essentially follow Kraft and Zien [14], who studied a problem of a similar flavor, that used a linear classifier and a standard decision tree (rather than rDTLC).
 Using an algorithm for learning a linear classifier (e.g., SVM), learning an rDTLC can be done straightforwardly by selecting thresholds from the feature values in the train-ing data, trying out every combination of the upper-level thresholds  X  i , and taking the combination that minimizes the classification error. For efficiency sake, we implemented a fix-and-optimize heuristic where we optimized one thresh-old at a time while fixing the others.
We now consider the second challenge discussed in the in-troduction: selecting a set of rewrite rules from a large set of previously defined rules, so as to maximize the overall quality of search results. We formalize this task as a combi-natorial optimization problem, discuss its (intractable) com-plexity, and propose heuristic solutions.
A rule-administration setting (or just administration set-ting for short) is essentially an abstraction of the search-engine X  X  interplay between queries, rewrite rules, and docu-ments. It consists of a set R of rewrite rules and a graph G that we call the rule-administration graph (or just admin-istration graph for short). A rule in R transforms an input user query q into a rewritten query q . For brevity and clear distinction between the two types of queries, we refer to q (the user query) simply as a query ,andto q (the rewritten query) as an r-query . A query and an r-query are simply finite sequences over an infinite alphabet of tokens (words), and a rule s  X  t in R is defined similarly to the previous sections (i.e., it replaces s , a subsequence of tokens in q , with a sequence t ). The administration graph is a tripartite graph describing the relationships between queries, r-queries and documents. Before we give the precise definitions, we introduce our running example for this section.

Example 4.1. Figure 4 shows an administrator setting ( R, G ). The set R is depicted in the top of the figure and the graph G is depicted in the bottom.

Formally, an administration graph is a directed, edge-weighted graph G =( V, E, w ), where:
Example 4.2. Consider again our running example. Each side of the tripartite graph G is surrounded by a dashed rectangle. Let q  X  V q be the query lotus notes download . The outgoing edges of q include the (zero-weight) reformu-lation to the r-query q = lotus notes issi in V r , and the query matching to the document d 1  X  V d (which, for presentation sake, is associated with the URL http://issi.ibm.com/lnotes). The weight of the latter edge is 2, representing the extent to which the engine scores the matching of d 1 to q .Ahigher score (namely 5) is assigned to the matching of d 1 to q ,as indicated by the weight of the r-query matching ( q ,d 1 ).
In principle, the same string can be represented by two distinct nodes: a query in V q and an r-query in V r .Soa G R
V Figure 4: The administration setting ( R, G ) of the running example node in V q  X  V r should have an identifier ,whichweomit from the formal model to simplify the presentation.
Let G be an administration graph, and let q  X  V q be a query. Observe that a path in G from q to d consists of either one edge (in E qd ) or two edges (one in E qr and the other in E rd ). If G has a path (of length one or two) from the query q to the document d ,thenwedenoteby score( d | q ) the maximal weight of a path from q to d .Fora query q and a natural number k , we denote by top k [ q | series of k documents with the highest w ( q, d ), ordered in descending w ( q, d ); if fewer than k documents are reachable from q via directed paths, then top k [ q | G ] is restricted to only the reachable documents (hence, top k [ q | G ]mayhavefewer than k documents). We implicitly assume an underlying linear order among the documents to resolve ties.

Example 4.3. Consider again our running example (Fig-ure 4). Let q be the query spreadsheets download .Thereare two paths from q to the document d 2 : a direct edge, and through the r-query symphony download . Since the latter has the maximal weight, 3, among all the paths from q to d , we get that score( d 2 | q ) = 3. We can similarly verify that score( d 1 | q ) = 4. In particular, top 1 [ q | G ]istheseries( d and top 2 [ q | G ] (as well as top 3 [ q | G ]) is the series ( d
For a rewrite rule r and query q ,let r ( q ) denote the r-query that is obtained from q by applying r .An administration setting is a pair ( R, G ), where R is a set of rewrite rules and G is an administration graph, such that the set E qr of reformulations of G is the set { ( q, r ( q )) | q  X  V q  X  For a reformulation e =( q, q )  X  E qr , the set of rules r with r ( q )= q is denoted by R ( e ).

Example 4.4. Consider again the administration setting ( R, G ) of our running example. For the edge e 1 from q lotus notes download to q 1 = lotus notes issi we have R ( e { r 1 ,r 4 } (where the r i are specified in the top of the fig-ure). Similarly, for q 2 = email client issi and the reformula-tion e 2 =( q 2 ,q 1 )wehave R ( e 2 )= { r 2 } . Consider an administration setting ( R, G ). Given a subset R of R ,wedenoteby G R the administration graph that is obtained from G by removing from E qr every reformulation e that is produced by none of the rules in R (i.e., R ( e )=
Example 4.5. Consider again our running example (Fig-ure 4), and let R be the set { r 2 ,r 3 ,r 4 } (i.e., R \{ The graph G R is depicted in Figure 5. Observe that un-like G , the graph G R has no edge from the query q 1 = spreadsheets download to the r-query q 1 = spreadsheets issi , because R ( e 1 )=  X  ,where e 1 is the edge ( q, q )of G (re-call that R ( e 1 )= { r 1 } ). However, there is an edge from the query q 2 = lotus notes download to the r-query q 2 = lotus notes issi since, although for e 2 =( q 2 ,q 2 )theset R ( e lost r 1 , this set still contains r 4 .

Let G be an administration graph. A desideratum is a function  X  : V q  X  2 V d that maps each query q  X  V q to aset  X  ( q )  X  V d of desired matches .A quality measure  X  determines a quality score 4 for each query q based on the series top k [ q | G ]andtheset  X  ( q ), for a natural number k of choice. We denote this score by  X  (top k [ q | G ] , X  ( q )). As an example, precision at k is the following  X  :  X  (top k [ q | G ] , X  ( q )) =
As another example, DCG k (without labeled relevance scores) is the following  X  : where top k [ q | G ]=( d 1 ,...,d j ), and each a i is 1 if d and 0 otherwise.

The top-k quality of G , denoted  X  k ( G,  X  ), is obtained by summing up the scores across all the queries: For readability, we may omit  X  ( q )and  X  from the term from the context.

Example 4.6. Consider the following desideratum  X  for our running example: The reader can verify that top 1 [ q | G ]=( d 1 ) for all three queries q in V q . Thus, for each of the functions  X  of Equa-tions (1) and (2) we get  X  1 ( G )=1+1+0=2.
In principle,  X  can represent a utility function that is not necessarily a vanilla quality measure, such as ad benefits [18]. 1: S  X  X  X  2:  X   X  1 3: while  X  &gt; 0 do 4: r  X  argmax r  X  R \ S 5:  X   X   X  k ( G S  X  X  r } )  X   X  k ( G S ) 6: if  X  &gt; 0 then 7: S  X  S  X  X  r } 8: return S Now, consider the graph G R of Figure 5 (discussed in Example4.5). Therewehavetop 1 [ q | G ]=( d 1 )for q = lotus notes download and q = email client issi , but top 1 ( d 2 )for q = spreadsheets download .Inparticular,  X  1 ( G R 3 for the two aforementioned functions  X  .

Abstract rule optimization is the following problem. We are given an administration setting ( R, G ), a desideratum  X  , and a natural number k . The goal is to find a sub-set S of R that maximizes  X  k ( G S ); that is, S is such that  X  ( G S )  X   X  k ( G R ) for every subset R of R . Such S is an optimal solution . A weaker goal is to find an  X  -approximate optimal solution ,where  X   X  1 is either a number or a nu-meric function of the input; such a solution is a set S  X  that satisfies  X   X   X  k ( G S )  X   X  k ( G R ) for all R  X  R . Example 4.7. Suppose that  X  is one of the functions of Equations (1) and (2), and consider again the administration setting ( R, G ) of our running example (Figure 4). Suppose that the input for abstract rule optimization contains, in addition to ( R, G ), the desideratum  X  of Example 4.6 and k = 1. In Example 4.6 we discussed the graph G R of Fig-ure 5 (where R = { r 2 ,r 3 ,r 4 } ). As  X  1 ( G R ) = 3 is clearly optimal, R is an optimal solution in this case.

To simplify the discussion on computational complexity, we assume that  X  is fixed (i.e., not part of the input), and that  X  (top k [ q | G ]) is computable in polynomial time. Next, we show that abstract rule optimization is hard to approx-imate. We make the following assumption on the quality measure  X  : there is a positive constant c , such that for all queries q ,if  X  ( q ) consists of exactly one document then  X  (top 1 [ q | G ]) = c if top 1 [ q | G ]=  X  ( q ), and  X  (top otherwise (i.e., if top 1 [ q | G ] does not contain the single docu-ment in  X  ( q )). Note that this assumption holds in standard measures that are parameterized by a restriction to the top-k results (assuming that differe nt results are not weighted by different levels of relevancy), such as DCG k , normalized DCG k , and precision at k . We say that a function with this property is reasonable at one . The following theorem states that abstract rule optimization is extremely hard (to even approximate), no matter which quality measure  X  is used, as long as  X  is reasonable at one. The proof is in the appendix.
Theorem 4.8. Whenever  X  is reasonable at one, abstract rule optimization is NP-hard to approximate by any constant factor, or even by | V q | 1  X  for every &gt; 0 .
 1: S  X  X  X  2: T  X  X  ( q, d )  X  V q  X  V d | d  X   X  ( q ) } 3: for all ( q, d )  X  T do 5: if  X  k ( G S  X  X  r } ) &gt; X  k ( G S ) then 6: S  X  S  X  X  r } 7: return S
Due to the inherent hardness of our problem, in the fol-lowing section we present heuristic approaches (that are ex-plored experimentally in Section 5).
In this section, we devise two simple heuristic algorithms, each following a greedy approach. We call the first algorithm globally greedy and the second locally greedy . The reasons for the algorithm names will be clear from their descriptions. Later, we discuss the optimization of these algorithms.
The globally greedy algorithm, depicted in Figure 6 un-der the name G-Greedy , applies the following very simple approach. It initializes an empty solution S  X  R (line 1), and iteratively adds to S the best missing rule r (found in line 5). The best rule r is such that the difference  X  between the quality obtained by adding r ,namely  X  k ( G S  X  X  r } the current quality, namely  X  k ( G S ), is maximal. The al-gorithm terminates (and returns S ) once  X  is non-positive (hence, no improving rule can be found).

In our experiments we found that the globally greedy al-gorithm performs very well in terms of the quality of the returned solution S . However, this algorithm is extremely slow, due to its inherent cubic time: the loop of line 3 can take up to | R | iterations, and then line 4 (finding r ) entails traversing over all r  X  R \ S and computing  X  k ( G S  X  X  r which requires computing top k [ q | G ] (and summing up the  X  (top k [ q | G ])) for all queries q  X  V q . Later, we discuss opti-mizations of this algorithm. But even the optimized version of this algorithm hardly scales up. Therefore, we consider the next, lower complexity algorithm.
The locally greedy algorithm, L-Greedy , is depicted in Fig-ure 7. Similarly to the globally greedy algorithm, the locally greedy one initializes an empty solution S (line 1) and incre-mentally adds rules (lines 2 X 6). The main difference between the algorithms is that in the main loop of the locally greedy one, we search for the rule r (to add to S ) in a practically tiny subset of the set R of rules (rather than in all of R ).
More specifically, line 2 constructs the set T of all tasks , where a task is a pair ( q, d ) such that q is a query (in V and d is a desired document for q (i.e., q  X   X  ( d )). Then, the main loop (line 3) traverses all tasks in an arbitrary order. For a considered task ( q, d ), we define rel k ( q, d )tobetheset of all the rules r that are relevant to the task, that is: (1) r is on a path from q to d , or more formally, for some q  X  we have e =( q, q )  X  E qr , r  X  R ( e ), and ( q ,d )  X  E (2) taken alone r can push d to the top-k results of q ,or more formally, d  X  top k [ q | G { r } ]. In line 4, a rule r that maximizes  X  k ( G S  X  X  r } )  X   X  k ( G S ) is added to S ,provided that this maximum value is positive.
In both the globally greedy and the locally greedy al-gorithms, a significant portion of the computation takes place on computing the quality of the system on interme-diate sets of rules (that is, the computations of  X  ure 7). This computation is done to obtain the difference  X  ( G S  X  X  r } )  X   X  k ( G S ), where S is the current set of rules and r is a considered candidate rule. Let  X ( S, r )denote that difference. We can optimize the computation by ob-serving that  X ( S, r ) is affected by only a few of the queries, namely those on which r fires.
 Formally, consider a query q ,arule r and a set S  X  R . Define  X ( q, S, r )=  X  (top k [ q | G S  X  X  r } ])  X   X  (top From (3) we get that  X ( S, r )= by V q ( r ) the set of queries q  X  V q , such that r  X  R ( e )for some edge e  X  E qr emanating from q . An easy observation is that  X ( q, S, r ) = 0 whenever q/  X  V q ( r ), and therefore  X ( S, r )=
The optimization is as follows. During the computation we maintain each  X  (top k [ q | G S ]) for the current S .(So,at the beginning we compute  X  (top k [ q | G  X  ]).) When a rule r is considered, we iterate over the queries in V q ( r ) and compute these  X ( q, S, r )into X ( S, r ) to be used in the next steps. Hereafter, the optimized versions of G-Greedy and L-Greedy are denoted by G-Greedy-opt and L-Greedy-opt , respectively.
In realistic scenarios, queries may carry different levels of importance, which may be expressed by means of query weights. For instance, this weight can be the frequency in which the query is posed. In the experimental study of the next section, we use as weights frequencies derived from query logs. The abstraction presented in this section can in-corporate weighted queries in a straightforward manner. In particular, the abstract model assigns a weight w ( q )toeach query q  X  V q , and in the definition of  X  k ( G ) (see (3)), each addend  X  (top k [ q | G ]) is multiplied by w ( q ). The greedy algo-rithms automatically adjust to weights by using the weighted  X  . An intuitive traversal order on T in the locally greedy algorithm (Figure 7) is by decreasing w ( q ); we indeed apply this order in the experiments of the following section.
In this section, we describe an empirical evaluation of our proposed solutions over the IBM intranet search engine, with datasets provided by the company X  X  search administrators.
More specifically, our dataset was obtained from a list of 1894 suggested matches provided by IBM CIO Office, where a suggested match consists of a query q and a document d de-sired as a top match for q . Generally, the queries involved in the suggested matches are frequent, and hence are recorded for quality maintenance. Note that a query can be matched against multiple documents, and a document may be sug-gested for multiple queries. As mentioned in Section 3, by analyzing the high-quality fields (e.g., titles and URLs) of desired documents, we generated for each suggested match a set of query-rewrite rules, each of which, in the absence of any other rule, is capable of pushing the desired document up to the top-5 results for the given query. This produced a total of 11907 rules. Our metho dology for extracting input from these suggested matches and rules will be described in the following sections. Our experiments ran on a Linux SUSE (64-bit) server with eight 4-core Intel Xeon (2.13GHz) processors and 48GB of memory. The algorithms were im-plemented in Java 1.6 and ran with 12GB allocated memory.
We first evaluated the classification models of Section 3 for recognizing natural rules. Among all generated rules, we randomly selected and manually labeled 1187 rules as either natural or unnatural. Such labeling is subject to human judgment; moreover, labeling often required domain-specific knowledge, and for that we needed to inspect the pages rel-evant to the terms involved in the rule. Using this labeled dataset, we evaluated the accuracy of the classifiers SVM and rDTLC. The results reported in this section were con-sistently obtained by performing a 5-folder cross validation.
We also explored the question of how the classifiers per-form on rules for more popular suggested matches. We an-alyzed the query logs of four months of intranet search at IBM and estimated the popularity of each suggested match by counting the number of sessions that have a click on the suggested document for the corresponding query. Therefore, each quality measure has two versions X  X he unweighted ver-sion is denoted by uw and the weighted version is denoted by w . In addition, we retrained the two classifiers by in-corporating the weights on rules, thereby obtained a third version denoted by wt ( weighted training ). Whether or not to train the classifier with weights is a matter of a choice that depends on the specific scenario, and for that reason we evaluated both the w and the wt versions.

Figure 8 reports the accuracy (ratio of correctly classified rules) for SVM and rDTLC. As shown, when trained without weights rDTLC outperforms SVM by about 10% for both the unweighted and weighted measures. When the classifiers are trained with weights, rDTLC has a smaller improvement (about 3%) and both classifiers achieve high accuracy.
Even with classification, the number of natural rules may still be large. An administrator may be interested in only the top-k rule suggestions for some k .Weexploredthe naive strategy of ranking the top-k candidate rules by de-creasing confidence of the classifier at hand. We leave for future work the explorati on of more sophisticated learn-ing to rank strategies for this task. Nevertheless, we ob-serve that this naive approach is already beneficial compared to a random selection of k candidates (a strategy denoted as RND ). Specifically, Figure 9 gives the Mean Recipro-cal Rank (MRR) of the different rankers. Figures 10 and 11 give the normalized Discounted Cumulative Gain at the top-k results (nDCG k )where k =1 , 3 , 5 for the unweighted and weighted cases, respectively. We can see that when the classifiers are not retrained with weights, rDTLC performs significantly better than SVM (and RND), but its advantage over SVM disappears when they are retrained with weights; in particular, when trained with weights both are almost always capable of suggesting natural rules for all k =1 , 3 , 5.
Figure 10: nDCG k (unweighted) for rule ranking
We now present an experimental study of our solutions for the abstract rule optimization problem (Section 4). Datasets. We constructed several datasets from our collec-tion of suggested matches. Each dataset is an administration setting constructed by selecting a subset M of the suggested matches, and a subset R of the rules automatically created for M .(Observethat M implies a desideratum  X  .) Later, we will explain how exactly M and R are selected for each dataset. Once M and R are selected, the administration graph G is constructed as follows. The queries in M form the set V q of queries. The set V r of r-queries is obtained by applying the rules in R to the queries in V q .Theset V d documents contains all the documents in M ,aswellasthe top-5 results for each query and r-query in V q  X  V r ,asob-tained by invoking our search engine without rewrite rules. The edges of G are defined in the obvious manner.
 Algorithms. We compared the G-Greedy and L-Greedy al-gorithms and their optimized versions G-Greedy-opt and L-Greedy-opt , as described in Section 4.3. We also considered several baseline algorithms: G-Random , L-Random and All-Rules . G-Random and L-Random are similar to their greedy Figure 12: nDCG k (unweighted) for the labeled dataset Figure 13: nDCG k (weighted) for the labeled dataset counterparts, except that their selection of rules is random. More precisely, G-Random selects a random subset of rules among all the rules, and L-Random randomly selects an effec-tive rule for each suggested match. The algorithm AllRules simply selects all the rules. 5 Measures. We consider two quality measures  X  : the nor-malized Discounted Cumulative Gain at the top-k results (nDCG k ), and the Mean Reciprocal Rank (MRR). Since our administration graph is restricted to the top-5 matches for each query and r-query, MRR is restricted to the top-5 re-sults as well (as it would not make sense otherwise).
To estimate the gap between the solution provided by each of the algorithms and the optimal solution, we computed an upper bound on the optimum for each quality measure  X  ,as follows. We evaluated the sum of Equation (3), where each addend  X  (top k [ q | G ] , X  ( q )) is computed as if each suggested match is ranked as high as possible using a rule from the our collection. So, in principle, we allow this bound to place two documents in the same rank. Note that this number is not smaller than, and may be strictly larger than, the optimum (namely  X  k ( G S ) for an optimal solution S ).
 Weights. We used both unweighted and weighted quality measures. In the weighted version, each query is weighted by the number of sessions where it is posed, as recorded in the aforementioned four months of query logs. For the unweighted version, the algorithms L-Greedy , L-Greedy-opt and L-Random iterate over the suggested matches (which
Albeit straightforward, AllRules represents a common prac-tice of our search administrators. form the set T in Figure 7) in a random order obtained by applying a random permutation thereof. For the weighted version, iteration over the suggested matches is ordered by decreasing weight of the involved queries.
In the first set of experiments, we used the labeled dataset obtained by taking as M the suggested matches that were used for labeling rewrite rules (see Section 5.1), and as R the set of rules that are manually labeled as natural. The resulting administration graph contains 135 queries, 300 r-queries, 423 documents, and a total of 1488 edges. Note that only a small portion of the suggested matches were used for labeling; the others, for which the rules are not labeled, will be used later in Section 5.2.2.

We first examined the quality of solutions produced by each of the algorithms. Figures 12 and 13 report the nDCG with k =1 , 3 , 5, for the unweighted and weighted cases, re-spectively. Figure 14 reports the MRR for both unweighted and weighted cases. (The groups labeled with BM will be discussed later.) Observe that on all quality measures, L-Greedy and G-Greedy score significantly higher than the other alternatives. In fact, they already reach the upper bound, and hence provide optimal solutions.

The goal of the next experiment is to explore the solutions in the presence of suggested matches that hit the top search results without requiring rules (yet, they can be affected by introducing rules). This setting represents a typical scenario where a search administrator needs to address problematic queries without compromising the overall search quality. For that, we enhanced the set M , from the construction of the labeled dataset, with 373 such suggested matches that are used as a benchmark . The groups of bars labeled with BM in Figure 14 report the MRR for both the unweighted and the weighted cases. Note that the greedy algorithms get better scores than the other alternatives, and again reach the upper bound. But now, the other alternatives get high scores as well, since many suggested matches in the benchmark are not affected by the rules in R . The results for nDCG k show a similar picture, and are therefore omitted.
 Next, we compare the execution cost of the algorithms. AllRules , G-Random and L-Random entail a negligible run-ning time (less than 0 . 2 ms). We focus on the globally and locally greedy algorithms, and examine the contribution of the optimization. Table 2 summarizes the running times for two different combinations of  X  and k . The columns enti-Figure 15: nDCG k (unweighted) for the extended dataset Figure 16: nDCG k (weighted) for the extended dataset tled  X  X abeled X  and  X  X M X  refer to the labeled dataset and the one enhanced with the benchmark, respectively. (The columns entitled  X  X xtended X  will be discussed later.) Ob-serve that the locally greedy algorithms are over one order of magnitude faster than their globally greedy counterparts. In addition, the optimized versions are generally over one order of magnitude faster than their unoptimized counter-parts. In particular, the optimized version of our locally greedy algorithm is capable of finding an optimal solution in real time for the typical usage scenarios.
The results of the previous experiments demonstrate the effectiveness and feasibility of the greedy algorithms. To further evaluate the scalability of the greedy algorithms, we constructed a larger extended dataset . To create this dataset, we used as M the set of all the suggested matches, and as R the set of all the rules automatically generated for them (including those that are not labeled). The resulting administration graph contains 1001 queries, 10990 r-queries, 4188 documents, and a total of 36986 edges.

Again, the running times for two different combinations of  X  and k are shown Table 2, now by the columns entitled  X  X xtended. X  The globally greedy algorithms (including the optimized one) do not scale to the extended dataset. For the locally greedy algorithms, the improvement achieved by the optimization is again by over an order of magnitude.
Finally, we evaluated the quality of the solutions over the extended dataset. Figures 15 and 16 report the nDCG k , with k =1 , 3 , 5, for the unweighted and weighted cases, re-spectively. Figure 17 reports the MRR for both unweighted and weighted cases. Consistently with the previous experi-ments, L-Greedy outperforms the other alternatives. The gap between L-Greedy and the upper bound is generally around one percent, and is barely notable in the figure. These re-sults demonstrate that even in the extreme case where the administrator adopts all candidate suggestions, L-Greedy-opt can find a practically optimal solution within a reasonable amount of time (around 2 minutes).
The rule-based architecture aims to provide search admin-istrators with the means of adapting the search engine to the content and dynamics of the enterprise. In this paper, we explored the incorporation of automation in the man-ual practice of administrators. Specifically, we studied the problem of suggesting natural rewrite rules, and proposed corresponding machine-learned classifiers for rules. We also studied the problem of selecting rules, from a given collec-tion, with the goal of optimizing the quality on a benchmark. We presented a theoretical model that captures this task as a combinatorial optimization problem, analyzed its theoreti-cal complexity, and proposed heuristic algorithms to accom-modate its hardness. Experiments on a real enterprise case (IBM intranet search) indicate that the proposed solutions are effective and feasible.

While the testbed for this work has been the IBM inter-nal search, the two challenges we studied hold in essentially every search system that supports administration by rewrite rules. Moreover, the setting and solutions we proposed are at a level of abstraction that hardly ties them to our specific testbed. In future work, we plan to focus on extending our techniques to handle significantly more expressive rules. Proof of Theorem 4.8. We show a reduction from the maximum clique problem: given an undirected graph H , find a clique of a maximum size, where a clique is a set C of nodes such that every two members are adjacent. Zuckerman [24] showed that for all &gt; 0, it is NP-hard to approximate this problem to within n 1  X  ,where n is the number of nodes.
Given an undirected graph H , we construct the adminis-tration setting ( R, G ) as follows. For each node v of H ,let N ( v ) denote the set of neighbors of v , and we define two unique words a v and b v . For each such v , R contains the rule a v  X  b v , G has the query q v that consists of a v (in some order) by all the a u where u/  X  N ( v ), and G has two unique documents d gd v and d bd v .Ther-queriesof G are all those obtained by applying a rule in R toaqueryof G .Let q be an r-query of G .If q begins with a v ,then G has the r-query matching ( q ,d bd v ) with the weight 2; and if q begins with b v ,then G has ( q ,d gd v ) with the weight 1. There are no query matchings in G . Finally, the desideratum  X  maps each q (where v is a node of H ) to the singleton  X  ( q v )= { d
Observe the following for a subset R of R and a node v of H .If R contains a rule a u  X  b u for some node u/  X  N ( v ), then q v is reformulated into an r-query that begins with a v and thus matches d bd v with the weight 2, implying top 1 [ q v | G R ]=( d bd v ). If R contains a v  X  b v but no rule a u  X  b u where u/  X  N ( v ), then q v is reformulated only into an r-query that begins with b v and thus matches d gd with the weight 1, implying top 1 [ q v | G R ]=( d bd v contains none of a v  X  b v and a u  X  b u ( u/  X  N ( v )), then top 1 [ q v | G R ]=  X  . From this observation, the correctness of the reduction is proved straightforwardly: for each subset R  X  R ,if  X  1 ( G R )= c  X  m (where c is taken from the definition of reasonable at one ), then we can obtain from G
R (in polynomial time) a clique of size m in H ;onthe reverse direction, from each clique of size m in H we can construct a subset R  X  R with  X  1 ( G R )= c  X  m .
