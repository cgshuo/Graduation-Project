 WELLY NAPTALI, MASATOSHI TSUCHIYA, and SEIICHI NAKAGAWA Toyohashi University of Technology 1. INTRODUCTION A statistical language model (LM) plays an important role in automatic speech recognition (ASR) systems. Together with the acoustic model, LM has been used to reduce the acoustic search space and resolve acoustic ambiguity. Given a speech signal A , ASR systems find the corresponding word sequence  X  W ac-cording to: where P A ( A | W ) is the probability of A given W based on acoustic model, P
L ( W ) is the probability of W based on LM, and guage weight). So generally, the LM task is to assign a probability to a word sequence. This task is usually simplifi ed as to predict the current word (next word) given the history (already predicted words), that is, LM as a generative model for word sequence.
 of LM in ASR systems. However, simply based on the Markov assumption, it is hard for this model to improve further because it uses only short-range depen-dencies, that is, n  X  1 preceding words, and does not care about the long-range information, for example, topics. Starting in the early 1990s, several attempts were made to capture long-range dependencies. The cache-based model [Kuhn and de Mori 1992] used a longer word history (window) to increase the proba-bility of reoccurring words. Then there was a trigger model [Rosenfeld 1996], a generalization of the cache-based model, but its training is computationally very expensive.
 Kneser and Steinbiss [1993] with its topic mixture model. The model contains many word-based n -grams where each of them was trained using a different corpus with a different topic and combined in a linear way. Then Iyer et al. [1994] and Iyer and Ostendorf [1996] introduced automatic clustering to split a large corpus into topic-specific cor pora. They used a simple combination of inverse document frequencies instead of using the Latent Semantic Index-ing (LSI) method, which is well known in information retrieval research. LSI was introduced to LM by Bellegarda et al. [1996] with name Latent Seman-tic Analysis (LSA) for the first time. LSA maps words into a semantic space where two semantically related words are placed close to each other, which in turn could form a topic. Unlike words, topics are unobservable. Thus it needs a tool such as LSA to reveal the meanings behind the words. The work is followed by Bellegarda [1998].
 ism, Hofmann [1999] and Gildea and Hofmann [1999] formulated a Probabilis-tic LSA (PLSA) which was claimed to have a stronger basic on mathematical theory than LSA. Knowing that PLSA is a little bit tricky when facing a new document in the test phase, Blei et al. [2003] proposed Latent Dirichlet Alloca-tion (LDA), which is a generative model. There has been much work to further extend PLSA and LDA mainly to introduce various structures or constraints into the model. For example, in Chen [2009], instead of word-document ma-trix, he used a word-word matrix to represent the training corpus. Then, in Liu and Liu [2007, 2008] a topic-dependent LM using LDA to detect topics was proposed. However, this method needs pre-analysis of the test dataset to de-termine topic mixtures, thus it is impossible to apply this method against a real-time ASR system.
 topic of an event is decided by voting on noun history. The work is based on the belief that noun contains latent topic information. First, LSA is applied to map nouns into a semantic vector space, and then a vector quantization (VQ) is conducted to define the topics. The distance between a noun vector and the topic centroid is defined as the confidence measure. A fixed size word history sequence (window) is observed to decide the topic of an event. Unlike other topic-dependent LMs, the topic is integrated as a part of the word sequence in the n -gram model in this new LM.
 ods (i.e., statistical n -gram LM, cache-based LM, and LDA-based LM) that will be used to compare to our proposed method. Section 3 describes our proposed method, from defining the topics and confidence measure, then deciding a topic of a given word sequence through voting on noun, and finally we formulate the proposed LM, topic dependent class (TDC). Section 4 describes the experiments and gives the performance results for the proposed model as well as the base-lines on The Wall Street Journal corpus and the Mainichi Shimbun corpus. Section 5 discuss the difference between the two corpora, and a comparison on number of parameters between TDC and the LDA-based LM. The article ends with the conclusions and possible future works. 2. THE BASELINES 2.1 Statistical n -gram LM The word-based n -gram LM [Jelinek and Mercer 1980] is the most common LM currently used in ASR systems. It is a simple yet quite powerful method based on the assumption that the current word depends only on the n  X  1 preceding words. Given word sequence W , word-based n -gram predicts the probability according to the following equation: usually simplified as trying to predict the current word w i using information lem, where the infrequent words have unreliable probability, besides using a good smoothing method, a word-based n -gram LM is usually combined with a class-based n -gram LM [Brown et al. 1990]. In class-based n -gram, the words are mapped into classes. Thus, the infrequent words will have a support from the frequent words in the same class. A class-based n -gram LM is defined as where C i is a class of word w i . Finally, the model is interpolated in a linear way according to: where  X  is a weight constant. modeling research. It is impossible to calculate all probable parameters of one language because the training corpus is always limited. It is also improper to assign zero probability to a word which does not occur in the training corpus. This is where a backoff method takes part: to estimate the probability of un-seen event. This research used the Katz backoff LM which is defined by the following equation: where where d a is a discount coefficient factor for an event that occurs a times. Using absolute discounting, d a is calculated in according to: where s is a constant value, which is usually equal to f 1 same amount of probability as the Good-Touring discounting [Yung et al. 2005]. 2.2 Cache-Based LM The cache model is based on the notion that words appearing in a document will increase the probability of appearing again in the same document. Given ahistory h , the unigram cache model is defined by the following equation: where f ( w i  X  h ) denotes how many times w i occurs in the history h .Inthis research, we used a cache model with a fixed word history (window) size. The cache model is usually used in conjunction with the word-based n -gram using linear interpolation, where  X  is a weight constant. Since a cache LM is trained based on only a lim-ited word history, the interpolation weight  X  is usually very small. There are other variations in the usage of this cache model, for example, n -gram caches. Instead of a single word w i , the model uses more than one word (unigram). But for short documents, where the number of words appearing is limited, the benefit of this model will be very small. 2.3 Known-Topic LM Based on n -gram This method is the same as the one that was proposed in Kneser and Stein-biss [1993], where there is available manually tagged topic information on a given corpora. The topic mixture based on an n -gram (TM) model is defined as follows: where  X  z this  X  such as described in Kneser and Steinbiss [1993] and Iyer et al. [1994]. In this research, we used a fixed  X  for every topic in the test dataset. By assuming that the topic of the text under consideration was fixed and known, this model gives a lower bound on what could be achieved when the number of topics is suitable. This model is then linearly interpolated with the general word-based n -gram LM that was trained on all corpora, where  X  is a weight constant. 2.4 Topic Mixture LM Based on LDA This method is similar to method in the previous subsection, except that this method use automatic clustering to obtain topic specific corpora. The latent Dirichlet allocation based LM used in this research is based on Liu and Liu [2007] and Liu and Liu [2008]. This method (referred to as LDA-ADAPT in the remainder of this article) was proposed to avoid the mismatch problem due to differences in domain, topic, or styles occurring in a general LM. The method splits a large corpus into some topics a ccording to the LDA process by analyz-ing named-entity words. Then a mixture model is built with mixture weights calculated from analyzing the test corpus in advance. This is a disadvantage if we wish to build a real-time system. An LDA-ADAPT LM with K topics is defined as follows: where mixture weight  X  is calculated in according to where where f ( w i , z ) represents the frequency of a word w i generated from a topic z , which can be obtained from LDA analysis, and f ( w i , d ) is the frequency of word w i in the document d of a test set. Finally, the LDA-ADAPT is linearly interpolated with the general word-based n -gram LM, where  X  is a weight constant. 3. THE PROPOSED METHOD 3.1 Topics and Confidence Measure Nouns in a sentence play an important role in the whole discourse and are the core of the underlying LM. The associations between nouns are support-ing factors in defining topics [Chen 1995]. We use LSA to reveal these hidden relations to form topics. LSA extracts semantic relations from a corpus, and maps them into a semantic vector space. The discrete indexed words are pro-jected into the LSA space by applying Singular Value Decomposition (SVD) to a matrix representing the corpus (matrix representation). Let A be a repre-sentation matrix with M  X  N dimension. SVD decomposes the matrix A into three other matrices U , S ,and V , where k =min( M , N ). Because the solution X  X  dimensionality is too large for computing resources, and the original matrix A is presumed to be noisy, the LSA matrices ( U and V matrix) dimension is set to smaller than the original, where l k and  X  A is the best least square fit approximation to A . The resulting matrix U corresponds with the rows of matrix A ,andmatrix V corresponds with the columns of matrix A (see Figure 1). These LSA matrices are then used to project the words into the l -dimension LSA vector space. corpus in which the rows correspond to n ouns and the columns to documents. We apply a term frequency-inverse document frequency (TFIDF) weight to each noun w i in each document d j , where | D | is the total number of documents. After applying SVD, the resulting matrix U contains information about words while matrix V contains informa-tion about the documents. So the matrix U is used to project nouns into the LSA space.
 an l -dimensional vector space according to the following equation: where U is a projection matrix with V  X  l dimension, and c i is a discrete vector of noun w i ,wherethe i -th element of the vector is set to 1 and all other
V  X  1 elements are set to 0. For instance, if V = 5, the discrete vector for word w i is represented by the i th row vector of matrix U .Soeachword w i has a continuous vector Since w i is a vector which representing noun w i , any familiar clustering method could be applied.
 into topics. A VQ algorithm is iterated using cosine similarity between nouns until the desired number of clusters (topics) is reached. A code (centroid) word in a VQ codebook corresponds to a topic vector. A confidence measure  X  is defined as the distance between a word vector and its class centroid. So in this case,  X  ( w i ) can be calculated using the same cosine similarity between noun w i and its topic class C i , where C i is the centroid vector of topic class C i , This score (0  X   X   X  1) indicates how confident a noun w i is to be in the topic class C i .Thelargerthe  X  score, the more typical a word w i is in the class C i . 3.2 Topic Voting on Noun History ular event in an n -gram model is denoted as Z i  X  n ( m ), where m is the window size. Topic Z i  X  n ( m ) is obtained by observing m words in the outer contexts of noun classes. Outer contexts of the near n -gram is chosen to avoid information eled and is best modeled in word-level by word-based n -gram. Mathematically Z  X  n ( m ) can be written as where Note that  X  (see Equation (25)) and  X  is defined only for nouns, otherwise 0 is assigned. If there are no nouns inside window Z i  X  n ( m ), as happens at the beginning of a document, a dummy topic class C  X  is defined. 3.3 Topic Dependent Class LM Humans may not be able to understand a conversation in which the topic is unknown. In the same vein, it is easier for an LM to predict the current word w i if the topic is known since this reduces the search space of possible candidates for the current word. A topic dependent class (TDC) is proposed to provide such information to the LM. A TDC with window size m is an LM in which the probability of a word sequence W = w 1 ,w 2 , ..., w N is calculated according to: previous section. Figure 2 shows the illustration of TDC LM.
 equation is similar to the factored LM [Bilmes and Kirchhoff 2003] in which word history instead. A statistical word-based n -gram LM is used to capture the local constraint using linear interpolation: where  X  is a weight constant. deciding a word sequence belongs to only one topic (hard-voting) as described in Section 3.2. A word sequence may usually belong to multiple topics. Based on this idea, we also perform a soft-decision on voting in the test phase. 2 In-stead of choosing the best topic in Equation (27), we choose K -best topics and then interpolate them linearly. So Equation (28) becomes where mixture weight  X  is calculated in according to where  X  is the score that was obtained during voting (see Equation (27)). 3.4 Backoff for Unseen Events In an n -gram LM, when the model encounters unseen events, it is usually backed off by the shorter ( n  X  1)-gram. In our model, we follow a similar ap-proach in handling unseen events. We use the Katz backoff with an absolute dataset, or f ( Z i  X  n ( m ) ,w i  X  n +1 i ) &gt; 0, then otherwise where P is the discounted probability and  X  is the backoff weight. Note that the backoff method is not performed by eliminating word w n  X  1 in our the statistical word-based 1-gram. See Figure 3 for a TDC backoff illustration. using the following equation: where f is the observed frequency of a particular sequence and d is the dis-counting coefficient factor. We use an absolute discounting method in Equation (8) to determine d .
 Z window to become Z i  X  n +1 ( m ). Recording these kinds of events is computa-tionally expensive and therefore, as the window size is quite large, a topic switch happens rarely (see Table XVIII for details). The assumption is thus made that such a word exchange does not affect the topic switch too much, 3 or Z 4. EXPERIMENTS AND RESULTS To evaluate the proposed LM for ASR, one may run the recognition experiment and calculate the accuracy. However, it takes the whole ASR system process. A more simple and widely used approach is to calculate its perplexity ( PP ), as defined by Although perplexity does not always agree with the word recognition accu-racy [Klakow and Peters 2002], but it is the first approximation towards better LM [Nakagawa and Murase 1992]. To decompose the matrix representation, we use the SVDLIBC toolkit. 4 All nouns in this experiment are mapped to a 200 dimensional LSA space based on empirical study [Naptali et al. 2009]. Then VQ clustering is carried out in this LSA space using the Gmeans toolkit [Dhillon et al. 2001] for a given number of topic. No pruning is applied in the TDC LM, nor in the baseline methods. We compare our proposed method with several baseline methods, namely a word-based/class-based LM, a cache-based LM, an n -gram-based topic dependent LM, and an LDA-based topic dependent LM. For each models for except the cache-based model, there are the stand-alone model and the interpolated model which were combined linearly with interpolation weight, unless stated otherwise,  X  optimized on (0 &lt; X &lt; 1) with stepsize 0 . 1. For the cache-based LM, there is only interpolated model with lin-exhaustive search approach was taken to observe the maximum performance (lower boundary of perplexity) that can be achieved by each of the interpolated models. 5 4.1 WSJ Corpus The data was taken from The Wall Street Journal ( WSJ ) corpus between the years 1987 and 1989. It contains a diverse range of articles, including opinion-editorial pieces, financial reporting, and world news. The three year corpus was divided into training (WSJ8789) and test datasets. We also pro-vide smaller training corpora from WSJ8789 for analysis purposes, they are 20% of WSJ8789 (WSJ0 2), 10% of WSJ8789 (WSJ0 1), and 5% of WSJ8789 (WSJ0 05). The vocabulary used is ARPA X  X  official  X 20o.nvp X  (20 k most common WSJ words with nonverbalized punctuation). By adding a beginning sentence to map all out-of-vocabulary (OOV) words, the total vocabulary size is 19 , 982 words. Details of the word size, document size, and OOV rate for the training and test datasets are given in Table I. To filter nouns in the vocabulary, we used the TreeTagger toolkit [Schmid 1994].

At first, we tried to explore the behavior of TDC model against the increas-ing window size and number of topics. For increasing window size experiments, we used a fixed 20 topics. While for increasing topics, we used an 80-window size. The TDC 3-gram perplexity for increasing window sizes and numbers of topic classes on WSJ8789 are given by Figures 4 and 5, respectively. From these results we can see that increasing the window size improves the per-formance. But when increasing the number of topics, the perplexity in the standalone model (see Equation (28)) deteriorates. However, in the interpo-lated model (see Equation (29)), the perplexity decreases. The reasons behind this are explored in the Subsection 4.1.1. Based on these results, we conducted a separate experiment for a large window size and number of topics as shown by Table II. From all these results, the best perplexity achieved by the TDC 3-gram is 137 . 3 for the stand-alone model and 96 . 0 for the interpolated model, whereas the most known word-based 3-gram LM has perplexity 111 . 6. Com-parisons with each of the baseline methods are discussed in the following sub-sections. 4.1.1 Comparison with Statistical n-gram LM. To build n -gram LMs, we used the HTK LM toolkit [Yung et al. 2005] and the same smoothing method as used in the TDC LM (Katz backoff with absolute discounting). The class-based LM was employed with 2000 classes using word exchange algorithm [Kneser and Ney 1993]. The perplexity is given in Table III. The best perplex-ity achieved by a conventional n -gram LM is 98 . 9 with the interpolated class-based 4-gram, while the interpolated TDC 3-gram achieved 96 . 0 (see Table II) with window size 640 and 80 topics. This represents a relative improvement of 13 . 98% over the word-based 3-gram LM for perplexity.
 and or class-based n -gram model. This is because in the TDC models the train-ing corpus is shrunk according to the number of topics. For instance, when the number of topics is 20, each topic is trained with 1 20 of the corpus on average. To support this statement, we conducted various experiments using a smaller corpus; 5%, 10%, and 20% of the WSJ8789 corpus (WSJ0 05, WSJ0 1, and WSJ0 2) were used with two models, the word-based 3-gram and the TDC 3-gram with a 320 window size and 20 topics. Figure 6 gives the relation between perplexity and training corpus size. A comparison between the word-based 3-gram trained on WSJ0 05 corpus and the TDC 3-gram trained on WSJ8789 corpus shows that the performance of the TDC 3-gram is far better, thus con-firming the hypothesis. Furthermore, the stand-alone TDC 3-gram is compara-ble with the word-based 3-gram model trained on 30%  X  40% n -gram LM. This also explains why the performance of the TDC LM in the standalone model de-teriorates when the number of topics increases with a fixed window size. This problem can be solved by performing soft-clustering on the test phase, which will be discussed in Section 4.1.5. model in the sense that the TDC model remembers nouns that have been seen before (in the window). Thus the TDC model should be comparable with a cache-based model with the same window size. To construct the cache-based LM, we used the SRILM toolkit [Stolcke 2002]. The window sizes were var-ied between 10 and 640. The results are shown in Figure 7. According to this figure, the TDC LM clearly outperforms the cache-based LM with respect to perplexity. The performance of the cache-based LM improves as the win-dow size varies from 20 to 320 with perplexity 100 . 7, but then deteriorates in window size 640. With regards the TDC LM, using 20 topics, the perplexity improves up to 99 . 2 as the window size increases. Furthermore, when using 80 topics, the perplexity reached 96 . 0 with window size 640. comparison, we tried to make every parameter as similar as possible. We only considered nouns through a noun-document matrix. The LDA matrix was ob-tained using the Matlab Topic Modeling Toolbox. 6 We used the HTK LM toolkit becauseitisabletointerpolatemorethan9LMs,unliketheSRILMtoolkit which was used in their original article. Comparison with the performance of the TDC 3-gram model is given in Table IV. The window size used in TDC model is equivalent to the LDA-ADAPT context word that is used to capture the long-range constraint, that is, the average number of words in the test document is 416.
 ADAPT 3-gram, the perplexity calculations show that the stand-alone LDA-ADAPT achieved a perplexity of 104 . 5. While the interpolated LDA-ADAPT 3-gram achieved 100 . 6with  X  =0 . 6. The standalone TDC 3-gram model performs worse than the LDA-ADAPT 3-gram. This might be caused by the pre-analysis of the test dataset of the LDA-ADAPT model to get the optimal value of mixtures, so that the topic mismatch is unlikely to happens. On other words, the model needs to wait until the test data is complete to calculate the mixtures. This kind of model is suitable for rescoring tasks, but not in decoding. On the other hand, TDC LM predicts the current word based on one topic that has been decided through voting of noun history, which can be done in real time. Another reason is that LDA-ADAPT using topic mixture (soft-clustering) to predict the current word, so that the prediction involves all training data. The TDC LM predicts the current word based on only one topic (hard-clustering), which means the prediction is based on the training data for that one topic only. Despite the worse performance in the stand-alone, the interpolated TDC 3-gram model gives better results with perplexity 99 . 1. The performance achieved by the interpolated LDA-ADAPT 3-gram is comparable to the interpolated TDC 3-gram LM with the window size of 80. As we already saw in Figure 4, the TDC 3-gram is still be able to improve the perplexity in-creasing the window size. The same analysis is also valid for a 30 topics model.
We also tried to compare the model with 40 topics, but the HTK LM toolkit was not able to load all of the LMs. So we do the same approach as the LDA-ADAPT X  X  author did [Liu and Liu 2007, 2008], we interpolate only 30 topic-specific word-based 3-gram LMs which have the highest topic mixture weight  X  (see Equation (13)). Then  X  is normalized so that  X   X  =1. The stand-alone perplexity is 112 . 9 and the interpolated model gives perplexity 101 . 3with  X  =0 . 5. Compared to 20 topics, the LDA-ADAPT performance is de-creasing since it used only 30 topic-specific word-based 3-gram LMs and ignore the other 10 topic-specific word-based 3-gram LMs in the perplexity calcula-tion. In the other words, the overall training size is decreasing. Meanwhile, the perplexity for TDC LM is 153 . 8and99 . 6 for the standalone and interpo-lated, respectively, with window size 80. 4.1.4 Comparison in Various Training Data Size. Here we used four data-sets given in Table I to see the proposed method X  X  performance in various train-ing data size against all the baselines. The perplexity of 5 LMs with the best parameters are shown in Figure 8. In this figure we can see that overall the best result is achieved by TDC LM. Other models have different results for different training size. In WSJ0 1andWSJ0 2, the cache-model gives compa-rable result but then deteriorates in WSJ8789. LDA-ADAPT 3-gram gives per-formance only slightly better than the word-based 3-gram when the training size is small. In WSJ0 05, the cache-based model gives improvements about 33% relative against word-based 3-gram, but the TDC model gives even more improvements about 44% relative. These facts show that the TDC model is robust against small training dataset. voting was made on 1-best (= hard-voting) up to 10-best. Table V shows soft-voting based perplexity using 80 topics and 640 window size. With 3-best topics, the performance of TDC 3-gram stand-alone model could overperform the baseline word-based 3-gram. The interpolated model achieved the best perplexity 92 . 7, that is 3 . 4% relative improvements against hard-voting, and 16 . 9% relative improvements against the baseline word-based 3-gram. The stand-alone model keeps improving up to 8-best, while increasing voting to 9-best and 10-best gives no further improvements. This could happen because when using k -best voting, the current word (sequence) probability is estimated using k topic specific LMs. In other words, it is predicted by the model that was trained on a corpus of k times larger than the model with hard-voting (1-best). With soft-voting, the topic decision becomes more reliable and stable. 4.2 Mainichi Shimbun Corpus The training data were taken from the Mainichi Shimbun corpus (Japanese news articles) for the years 1991 to 1998 (MS9198). The test dataset is part of Mainichi Shimbun for January, 1999. Mainichi Shimbun contains various topic such as sports, culture, social, s cience, economics, and entertainment. Normally, Japanese text does not have spaces between words. For this task, we used the MeCab toolkit 7 (yet another part-of-speech and morphological an-alyzer), and converted the corpus into basic units, word + part-of-speech. The vocabulary size is 20,000 words, taken from the most frequent words. With known symbol &lt; UNK &gt; to map all OOV words, the total vocabulary size is 20 , 001 words. The statistics for the training and test datasets are given in Table VI. classes and a large window size, 80 and 320, respectively. The baseline cache-based LM was executed with an increasing window size from 20 to 640, and the best perplexity was achieved with a window size of 320 with  X  =0 . 08. All model perplexities are given in Table VII. Of all the baseline methods, it can be seen that our model gives the best perplexity 60 . 8with  X  =0 . 5. This is a relative improvement of about 15 . 20% on the word-based 3-gram LM.  X  =0 . 5 means that the contribution of the TDC LM in capturing the global constraint is similar to the contribution of the word-based 3-gram LM in capturing the local constraint.

Soft voting on the test phase is also conducted on this dataset. When vot-ing 7-best topics, the standalone TDC model achieves the best performance with perplexity 61 . 6 as shown in Table VII. This model overperforms all the baselines as shown on the table. While the interpolated model achieves its best performance on 3-best topics and  X  =0 . 6 with perplexity 58 . 0. This gives relative improvements around 19 . 1% against the word-based 3-gram. 4.2.1 Comparison with LDA-Based Topic-Dependent LM. Because of ma-chine resource limitation (memory size of 1G Bytes), the comparison with LDA-ADAPT could not be conducted using the same data set as the others. We used only one year of Mainichi Shimbun , that is year 1991 (MS91) for train-ing corpus. Number of words, documents, and also the out-of-vocabulary rate for training and test datasets are shown in Table VI. The LDA-ADAPT was conducted using 20 topics and the interpolated model gives perplexity 112 . 3. The interpolated TDC 3-gram is only able to achieve perplexity 114 . 1, which is slightly worse than LDA-ADAPT. However, the interpolated TDC 2-gram gives slightly better result, with perplexity 112 . 2. These might be caused by the insufficiency of training data size. As we can see in Table VIII, the perfor-mance of word-based 4-gram is also worse than word-based 3-gram. Unfor-tunately we were not able to confirm this statement on LDA-ADAPT because of resource limitation. By looking at other baselines, the cache-model achieve the best perplexity among all models, with perplexity 109 . 9. These baseline model X  X  performances are in the reverse order when comparing to the WSJ ex-periments. If we compare the TDC and ca che-based model X  X  performance when using one year and eight years training corpus (i.e., Table VIII and Table VII), it seems that the TDC is getting better when the training dataset increases. We will investigate this further in the following subsection. training sets for this purpose, they are MS9192, MS9193, MS9194, MS9195, and MS9196. Each of them is increased by one year from Mainichi Shimbun corpus 1992 to 1996. So in total, we conducted experiment on 7 training datasets (see Table VI for details). Note that the vocabulary set used in these datasets is similar to MS91 instead of MS9198 to avoid zero count. Perplexity for each baseline and the proposed TDC model on these datasets can be seen in Figure 9. As we can see from the figure, the cache-based LM performs the best among others from one year of Mainichi Shimbun to five years. The TDC performance increases when the trainin g data size is also increased. It per-forms better than the baselines when using more than five years of the training corpus. This behavior is somewhat different when comparing to WSJ experi-ments. This happens because these two corpora have a different characteristic. Voting on the Test Phase. Each documents in the Mainichi Shimbun corpus contains topic information that has been assigned manually by human. There are 16 topics, but there are also some documents that do not have topic infor-mation. We put this un-tagged documents in one topic, so in total there are 17 topics. In this experiment, we used MS9198 dataset. We build a topic mixture LM based on word-based 3-gram and compare this model with TDC 3-gram with the same number of topics and 320 window size. The result is shown in Figure 10. The TDC seems to outperform topic mixture LM based on n -gram. In the standalone model, TDC achieved 9 . 1% relative improvements. In the interpolated model, relative improvements vary between 1 . 4%  X  5 . 6%. from 1-best up to 10-best. For stand-alone model, the best result is achieved at 4-best with perplexity 64 . 0. That is 10 . 7% relative improvements against the baseline word-based 3-gram with perplexity 71 . 7. While for the interpolated model, 3-best voting gives the best perplexity (see Figure 10). TDC 3-gram provements against hard-voting on the stand-alone and the interpolated model respectively.
 4.3 Evaluation on Automatic Speech Recognition We conducted rescoring experiments usi ng in-house large vocabulary contin-uous speech recognition (LVCSR), SPOJUS (SPOken Japanese Understand-ing System) [Zhang et al. 2008], with a context-dependent syllable based acoustic model trained on JNAS (Japanese News Article Sentence). The LM is a word-based 3-gram with Katz-backoff and absolute discounting trained on seven years of Mainichi Shimbun (1991-1993 and 1995-1998) contains 198 , 887 , 247 words in 754 , 767 documents. The test data contains 100 sen-tences from Mainichi Shimbun read speech (JNAS) between period from October-December 1994. The speech data were spoken by adult males. Al-most of these sentences do not correspond to a special topic, but not a general topic. We used the two-pass decoder to generate a 1000-best hypothesis. Base-line word error rate (WER) is 16 . 8% 8 and the perplexity of word-based 3-gram on the reference of the test dataset is 39 . 3. TDC 3-gram was also trained on the same training set with 80 topics and 320 window size. It has a perplexity 55 . 1 for the standalone model and 36 . 4 for the interpolated model with  X  =0 . 4 (see Table IX). This means 7 . 4% relative improvements against the baseline. The rescoring result is shown on Table X. With the standalone TDC, the WER is increasing up to 17 . 8% as it has the worst perplexity. Other results in Table X seem to agree with the perplexity results on Table IX. The best achieved WER is 16 . 0%, that is, 4 . 8% relative improvements against the baseline. We also conduct the same rescoring experiments on data spoken by adult females. Table XI shows the result. Baseline WER is 17 . 4%, while the rescoring gives the best WER 16 . 8% with  X  =0 . 2. That means 3 . 5% relative improvements on WER. A statistical significance was conducted according to Strik et al. [2000; 2001], using a combination of the Number of Error per Sentence (NES) met-ric and Wilcoxon Signed Rank (WSR) test. The adult male speech, with WER improvements from 16 . 8% to 16 . 0% is significant at 11 . 2% level ( p 2-tail). For adult female speech, the WSR test gives p value 11 . 6%. Then, by combining both NES from these two experiments, the WSR test gives an even smaller p value, p &lt; 2 . 5%. 5. DISCUSSIONS 5.1 Noun and Topic Chen [1995] shows the importance of noun-noun and noun-verb pairs in topic identification. In order to see the importance of noun and verb in our frame-work, we compared TDC using noun only (TDC-noun), TDC using verb only (TDC-verb), and TDC using noun and verb (TDC-noun+verb) to infer a topic. The comparison was conducted using WSJ8789 corpus (see Table I). The per-plexity of TDC 3-gram conducted on various window sizes and number of top-ics can be seen in Figure 11 and Figure 12, respectively. Table XII shows the results for a large number of topics and window size. The standalone TDC-verb might be comparable to TDC-noun, but the interpolated model does not give too much improvements. This indicates that TDC-verb capture the same feature that has been well modeled by a word-based 3-gram as a background model [Broman and Kurimo 2005]. TDC-noun+verb performs worst among the others. With noun and verb, there might be too many words (noun and verb) to be observed inside a window and the topic switch becomes more often, even when increasing the window size. This makes the topic decision unreliable. All these results suggest that we only need to observe nouns in order to infer a topic. In other words, noun relations contain a hidden topic information. 5.2 LSA, PLSA, and LDA LSA, PLSA, and LDA are three well-known analysis tools for finding the mean-ing behind words (i.e., polysemes and synonyms). Hofmann [1999] and Gildea and Hofmann [1999] show that PLSA outperforms LSA in their topic-based LM. While Blei et al. [2003] show that LDA is better than PLSA in docu-ment classification. However, LSA performs better than PLSA and LDA in automatic text categorization for automated essay grading [Kakkonen et al. 2008]. So it would be interesting to compare the performance of LSA, PLSA, and LDA in our TDC framework. Note that PLSA and LDA give the probabil-as a confidence measure  X  ( w i ), so that we can skip the VQ clustering. We will denote these models as TDC-PLSA and TDC-LDA. In this section, our model will be denoted as TDC-LSA+VQ. We also conducted TDC-PLSA+VQ and TDC-LDA+VQ where we used PLSA and LDA to map words into a continuous space in the place of LSA. The results can be seen in Figure 13 and Figure 14 for increasing window size and increasing number of topics, respectively. Table XIII shows the perplexity obtained usin g large parameters. The results show that the TDC LM performs better when using LSA than PLSA and LDA. PLSA performs worst among others. On the other hand, LDA seems better when it is used to map words into a continuous space (TDC-LDA+VQ). These might be caused by the expectation maximization (EM) that is used in PLSA and LDA. We used the maximum number of iteration of 100 for LDA and 10 for PLSA, since when using 100 iteration, PLSA performed much worse. 5.3 TDC Backoff and Perplexity The Z i  X  n +1 ( m )  X  Z i  X  n ( m ) assumption in TDC backoff process makes the to-tal probability of some word sequences, that backed-off probability does not always equal 1. A normalization proce dure can be conducted, but it will be time consuming as it has to calculate the probability of all words in the vocab-ulary for a given word sequence. To see how this assumption affects perplex-ity, we conducted the following experiments on WSJ0 05, and MS91 corpora against its test dataset. Note that WSJ0 05 and MS91 are the smallest train-ing dataset in this research, means that the backoff happens more often than the others.
 how many words that have less or more than 1. To accommodate programming computation error, we made the f ollowing categorization: P TDC &lt; 0 . 99, 0 . 99  X  P TDC  X  1 . 01, and 1 . 01 &lt; P TDC . The TDC 3-gram topic num-ber is set to 20 and we vary the window size. The results can be seen in Table XIV for both models. As we can see in the table, the total probability could be less or more than 1. However, we will see that these quantities are quite small, so that it does not affect the perplexity. We can also see when the window size is increasing, the number of event that has total probability less or more than 1 is decreasing. Thus, these results support our assumption.
 with the one that applied probability normalization during perplexity calcula-tion (normalized). Table XV shows this comparison on the standalone and the interpolated model. In the interpolated model, the best perplexity is achieved with  X  =0 . 3 for both model. This table shows that both perplexity gives almost similar results. Moreover, the difference is unnoticeable in the interpolated the computation complexity. 5.4 WSJ and Mainichi Shimbun The TDC experiments on Mainichi Shimbun gives different results compared to the experiments on WSJ .On WSJ , the proposed method is very robust against small training dataset. But it is not valid on Mainichi Shimbun .The TDC model shows good results when the training data contains more than 115 million words, that is about 63 times larger than WSJ0 05. Even though Mainichi Shimbun corpus is larger than WSJ corpus, the ratio between n -gram word counts and data size is smaller than WSJ corpus. It means that the WSJ corpus has more variety in terms of n -gram word sequence than Mainichi Shimbun . Table XVI shows the comparison between n -gram word counts and training data size. From the table, we can see that the ratio of 3-gram counts is 1:1.6 on WSJ0 05 while the ratio on MS9195 is 1:4.7. In Figures 15 and 16, we can see the growth of 3-gram counts against training data size with the linear trend line and its linear equation on WSJ and Mainichi Shimbun , respectively. and word-based 4-gram. From Table XVII we can see that the word-based 4-gram perplexity in WSJ corpus shows better results after WSJ0 05. But in Mainichi Shimbun , although the corpus size is quite large, the word-based 4-gram perplexity shows better results after MS9193. The same things also happen with the TDC model. Figure 17 shows the perplexity of interpolated TDC 3-gram and TDC 2-gram on MS91, MS9192, MS9193, and MS9194 cor-pora. From this figure, we can see that the performance of TDC 3-gram is worse than TDC 2-gram in MS91 and MS9192. 5.5 Parameter Numbers Comparison of TDC and LDA-ADAPT One may argue that performance improvements will also increase the number of parameters of the model. So in this chance we compare the complexity of our model against LDA-ADAPT. Note that we compare the performance of the interpolated model, and since both used the same general word-based 3-gram LM, the number of parameters of this model (i.e., 3,491,000 parameters) could be omitted. We compare the 3-gram counts for both models, such as shown in Figure 18 using WSJ8789 experiment. While the performance of TDC in-creases when the window size become larger, there is a considerable amount of decreasing in number of parameters, around 100,000 -400,000 parameters when increasing window size from one to another of 10, 20, 40, 80, 160, and 320 window size. This is because when the window size is increasing, the topic in TDC model does not change too often (see Table XVIII for details). This will make the number of n -gram word sequences in a certain topic is limited. TDC 3-gram model with 20 topics and 80 window size has 19,972,000 parameters. The LDA-ADAPT, with the same number of topics and a comparable perfor-mance, has more parameters, which is about 20,688,000. The TDC has even more smaller parameters when comparing the LDA-ADAPT to the TDC with the equivalent window size, that is, 416, which gives about 991,000 (about 5%) smaller parameters with a significantly better performance. 6. CONCLUSIONS AND FUTURE WORK We have demonstrated the superiority of the TDC LM over several baseline methods, namely the statistical word-based/class-based n -gram LM and the cache-based LM. The TDC also performed better than topic dependent LM based on n -gram and based on LDA. The TDC LM achieved a relative perplex-ity improvement over the word-based 3-gram of 13 . 98% and 15 . 20% for the WSJ and Mainichi Shimbun corpora, respectively. Soft-voting on the test set gives even more improvements.

For future work, we will investigate soft-decision on voting in training phase. A soft-clustering on defining topics also possible to be explored. Then soft-clustering and soft-voting could be combined in several ways in training or test phase. Adding a cache capability in TDC also might improve the per-formance [Kneser and Steinbiss 1993], since both model capture a different aspect of the language [Broman and Kurimo 2005]. Finally, it would be inter-esting to combine all the methods proposed in this research. Although some models might capture the same aspect of the language.

