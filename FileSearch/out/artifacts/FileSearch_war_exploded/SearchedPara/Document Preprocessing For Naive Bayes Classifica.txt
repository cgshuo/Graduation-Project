 Naive Bayes classifier has long been used for text catego-rization tasks. Its sibling from the unsupervised world, the mixture of multinomial models, has likewise been success-fully applied to text clustering problems. Despite the strong independence assumptions that these models make, their at-tractiveness come from low computational cost, relatively low memory consumption, as well as ability to handle het-erogeneous features and multiple classes. Recently, there has been several attempts to improve the accuracy of Naive Bayes by performing heuristic feature transformations, such as IDF, normalization by the length of the documents and taking the logarithms of the counts. We justify the use of these techniques and apply them to two problems: classifi-cation of products in Yahoo! Shopping and clustering the vectors of collocated terms in user queries to Yahoo! Search. The experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multino-mial model.
 Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning General Terms: Algorithms, Experimentation.
 Keywords: Classification, clustering, datatransformations, performance, Naive Bayes, mixture of multinomials.
Naive Bayes classifier has been a subject of multiple re-search studies [2, 5] and almost on every occasion it was noted that the Naive Bayes X  performance can be jeopardized by several independent problems. These include the imbal-ance of training documents in classes, mismatch between the actual text distribution and the multinomial distribution as-sumption, double counting of evidence. More sophisticated SVMsandmaximumentropyclassifiers typicallyoutperform Naive Bayes. However, this comes at the expense of high, if at all affordable, computational cost.

As an example, consider the data we use in Yahoo! Shop-ping for categorizing the products. Each product, that is currently active on Yahoo! Shopping site, is represented as a bag of terms. After straightforward frequency-based fea-ture selection, this leads to a total of slightly over 200,000 individual feature terms. The total number of classes equals about 60 at present and is expected to grow. This by it-self represents a challenge for SVMs, as they do not have an efficient way of handling the multiclass problems. One-against-others solution is going to  X  X njoy X  even more severe problems with imbalanced classes than Naive Bayes. Error-correcting output coding [1] remains a viable option, but in any case the computational penalty of training a single SVM is going to be multiplied by at least the number of classes, i.e. possibly grow 1 to 2 orders of magnitude in our case. But still by far the main problem for SVMs comes with the number of training data points, which are in our case products, constantly growing in number. The most ad-vanced SMO algorithm [9] has at least quadratic complexity in the number of data points per iteration. There has been work on speeding up SVM training includes various forms of data sampling, boosting [7] and squashing [8]. However, in our experience handling multiclass problems is still an is-sue while the preprocessing steps above can easily result in either inaccurate classifier (e.g., sampling) or are too com-putationally expensive (e.g., squashing).

Maximum entropy classifiers are not going to have as big a problem handling multiple classes as SVMs, but they are probably even slower than SVMs if trained with a commonly used generalized iterative scaling [4]. There has been also work on speeding up the training of maximum entropy mod-els. However, even with these speed ups, the algorithms are still far from the simplicity and speed of naive Bayes. Thus, the advertised superiority of SVMs and maximum entropy classifiers over Naive Bayes generally comes with a bou-quet of hard-to-deal-with problems, and from this perspec-tive Naive Bayes classifier looks quite attractive, especially if some of its drawbacks can be alleviated.
 This was exactly the goal of several recent studies [11, 10]. The first two papers discuss approaches with introducing de-pendencies between features and performing clustering prior to classification. These methods are quite a bit more com-plex than the regular Naive Bayes and, although valid, are not considered here. The last paper by Rennie et. al. [10] looks appealing to us as the only manipulations authors per-form are on the data itself, and the Naive Bayes classifier can still be learned in one pass through the data. The au-thors illustrate various problems that Naive Bayes suffers from and propose heuristic ways of handling them: 1. Systemic Problem 1: imbalance between the number 2. Systemic Problem 2: feature independence assump-3. Other Problems: mismatch between multinomial as-
In this paper, our main goal is to study the effects of data transformations on text classification with Naive Bayes clas-sifier and text clustering with multinomial mixture models. In Section 2 we briefly define the models. In Section 3, we implement the proposed heuristics on the Yahoo! Shopping data and perform a comparison of the plain Naive Bayes classifier and various combinations of heuristics. We confirm that some of the transformations above are indeed helpful for boosting the accuracy of the Naive Bayes classifier. In Section 4 we turn to the problem of unsupervised learn-ing of the multinomial mixture model on the Yahoo! User query logs, where the problem is to identify similar terms by studying their collocation vectors with other terms. We apply the multinomial mixture model to this problem and illustrate the effects of the data transformations above for this learning task. We find a single data transformation that appears to be highly helpful in identifying semantically sim-ilar terms. We propose an explanation to this phenomenon and draw conclusions in Section 5.
We only present brief model descriptions here, for a more detailed discussion please refer to [3] for Naive Bayes classi-fier and to [6] for multinomial mixture models.

We assume that the data consists of documents, each rep-resented as a bag of words, i.e. as a map from the set of terms occurring to their count in the text. The length of the document is defined as a sum of counts all terms ex-tracted from its text. The set of all terms is referred to as alphabet A , with size | A | . For purposes of classification the document is then assigned a unique class label from a finite set of labels C . The assumption then is that the terms in every class (in classification) or cluster (in clustering) were generated by a multinomial mixture model: where t i is the i -th term and n i is its count in the current document D ,  X  k is the document-independent prior proba-bility of the class (cluster) k ,and  X  ik is the probability of the observing term i in the class (cluster) k . Model parameters also satisfy the normalization conditions.

Given the data, i.e. the set of N documents { D } ,onecan formulate a maximum likelihood or maximum a-posteriori parameter estimation problem, the optimization of which leads to the following set of equations for classification: where f ik is the frequency of term i in the documents of class k , f k is the frequency of documents of class k the data and  X  k and  X  are two user-defined smoothing pa-rameters (0 value corresponds to no-smoothing maximum likelihood parameter estimates).

Clustering is going to involve similar equations but is go-ing to be iterative (EM algorithm) and involve an additional estimation of the class posterior ( P ( k | D )):
Thus, there is much similarity between the classification and clustering approaches based on the multinomial model, with the main difference being that for clustering class labels needs to be predicted and each document can potentially contribute to the estimates of model parameters for a given class. The reason why mixture of multinomials induces a soft clustering over the documents, is that once the model is fit to thedata, for each document one can compute theclass-posterior distribution P ( k | D )asinEquation3andinterpret the result as a probability with which a given document D belongs to each of the | C | fit clusters.
We ran a series of experiments with different data trans-formations preceding Naive Bayes fitting on active Yahoo! Shopping data. We used all labeled products in Y! Shopping as of February 7, 2004. We parsed, tokenized and stemmed thetitle and description from every product, so thatafter ag-gregating the counts of same tokens a product is represented as a  X  X ag of tokens X . The total alphabet size for the data at hand was about quarter of a million and the total number of products (retained after straightforward frequency-based feature selection) was a little under 100,000. The total num-ber of categories used for classification was 58.
We ran a 20-fold cross-validation experiment for each of the normalization choices listed in the schema of Table 1 under several heuristic ways of changing the parameters of the decision boundary outlined in [10] and listed in the row stands for Complementary Class Fitting. All numbers are percents. annotation of Table 1. We analyzed both data transforma-tions and heuristics in the introduction Section 1. In each row of the Table we computed the best accuracy and put it in bold font. The best accuracy from all runs is additionally boxed.

Notice that out of 8 possible ways of transforming the data and across 4 heuristics, the most promising are either doing IDF on its own or in combination with TF trans-form. The best heuristic appears to be weight renormal-ization, advertised to alleviate the feature independence as-sumption. The overall boost in accuracy resulting from the use of IDF and weight renormalization (82 . 82%) compared to the raw Naive Bayes (80 . 66%) is a significant 2 . 2%. Using the complement class heuristic on its own or together with weight renormalization failed to improve the performance, and for certain data transformations even made it worse than for the raw Naive Bayes classifier. The same conclu-sion applies to length normalization, that masked the effect of doing TF/IDF transforms on our data. We revisit this phenomenon in Section 4, where the length normalization exhibited the opposite behavior, turning out to be the only data transformation that resulted in creating semantically meaningful clusters on terms extracted from Yahoo! user query logs.

Overall, we conclude that some of the basic data trans-forms are capable of significantly boosting the accuracy of the Naive Bayes classifier, and as such should always be ex-perimented with in real-world applications.
As was mentioned in Section 2, the classification and clus-tering approaches based on the multinomial model are very similar, thus before we started off experimenting with the query logs, our intuition was that we would find yet another proof that data transformations are of little help. This, how-ever, proved to be incorrect.

The data we had at hand consisted of all user queries on main Yahoo! Search site accumulated over a period of one week. Each unique query was preprocessed by removing punctuation, special symbols followed by the identification of meaningful n -grams. In what follows, each retained n -gram is referred to as unit . Then for each unit U , we iden-tified all other units U 1 ,...,U k ,withwhich U co-occurred in at least one query in the query corpus and recorded the co-occurrence counts of U and U i in the data vector corre-sponding to unit U . We further discarded all units whose total co-occurrence count (similar to the length of the text document) is below 10. This lead to a total of 22,276 units in the space of 132,481 collocated units.

The goal was to cluster the units into semantically re-lated groups by exploiting the similarities between the col-locations of units. To see why this is possible, consider the sport brands Adidas, Nike, Puma etc., all of which are likely to co-occur in queries with the same set of units, such as  X  X ports X ,  X  X ogging X ,  X  X ki X , or X  X atalogue X . Similarly, theair-port names LAX, JFK, Heathrow etc., are likely to co-occur with other distinguishing terms, such as  X  X ravel X ,  X  X arking X ,  X  X light X  and others. Some of the units can have certain affin-ity to multiple clusters, such as, for instance, Amazon would belong to  X  X nternet Shopping Portal X  and  X  X iver X  clusters.
We started off by fitting a mixture of multinomials to the raw count data and soon discovered that clusters are really strange and non-intuitive, specifically all seemingly similar objects, such as airport names, celebrity names, city names etc., were spread across multiple clusters. To study this effect we concentrated on the following 3 units:  X  X arn X ,  X  X arns X  and  X  X ack X  and tried fitting a two-component mix-ture model to the vectors of collocations for them. The data for this experiment in given in Table 2. Our intuition was that  X  X arn X  and  X  X arns X  should have been placed to-gether, and  X  X ack X  alone in a separate cluster. Analyzing the data reveals that  X  X arn X  and  X  X arns X  have at least 7 high-frequency common collocated units (compared to only 2 common unit between  X  X arns X  and  X  X ack X ) and should indeed be together. However, we consistently obtained a highly confident response that instead  X  X arn X  was placed alone in a separate cluster.

The key to a solution lied in understanding that shorter 1 documents or sets of collocations have considerably more influence over parameter estimates than the longer ones. Indeed, the multinomial model prescribes to normalize the components of  X  vectors to sum to 1. Thus, the shorter documents take over the longer ones simply because their count profile looks on average higher, and consequentlymore likely, after a  X  X um to 1 X  normalization. When the length of the document is much higher compared to the other doc-uments (as for  X  X arn X ), whether due to a single peaking feature, such as  X  X ottery X  in  X  X arn X , or due to just many co-occurring features, this document appears essentially to be highly unlikely to have been generated by any cluster. Moreover, adding this document to a cluster already con-taining a shorter document will reduce the likelihood of this shorter document. To support this finding, we evaluated the log-likelihoods of the data consisting of  X  X arn X ,  X  X arns X  and  X  X ack X  under all different ways of clustering them into two clusters with a multinomial mixture. The results of this experiment are reported in Table 3. Notice, that on the raw data, placing  X  X arn X  on its own in a separate cluster has more incentive than grouping  X  X arn X  and  X  X arns X  despite in a sense of total count the collocated terms with low frequency are omitted. the clustering.
 of their high similarity in the unit space. Further notice, that length normalization helped to get things back to the expected behavior.

This domination effect is cited as a primary reason for using length normalization in [10], as it results in equating the contributions of every document. Placing the original vectors of collocations of the surface of the unit sphere did fix the problem, on both the 3 unit data set as well as on the full set of units, thus allowing us to claim that length normalization also makes similar documents look more simi-lar. An apparent disconnect of this claim with the results of our classification experiments can be explained by the fact that in manual categorization some of the similar records are sometimes purposefully placed into different classes, in which case the transformation only harms.

Table 4 contains some of the unit clusters discovered from the full data. Notice that we are able to discover semanti-cally coherent clusters. Several comments are in order here: 1. The length normalization is equivalent to considering 2. We tried all other transformations advertised in the In-3. Smoothing needs to be applied very cautiously as, if 4. As Table 3 prompts, it is quite easy to end up having
In this paper, we advocated the use of Naive Bayes clas-sifier for practical multiclass text categorization, citing its virtues, includinglowcomputational cost, relatively lowmem-ory consumption, ability to handle heterogeneous features and multiple classes, and often competitiveness with the top of the line models. We further performed the experimental evaluation of several data transformations advertised by [10] to help alleviate the restrictive assumption of the multino-mial model made by Naive Bayes. Our evaluation has sin-gled out the length normalization as a measure capable of not only equating the influenceof every document on thepa-rameter estimates, but also making the similar documents even more similar. This transformation allowed us to per-form clustering of units based on the vectors of collocation with other units into semantically related groups. No other transform, as well as fitting the model to the original data, resulted in even remotely as nicely interpretable clusters as with length normalization. Furthermore, in the applica-tion of the transform to the Yahoo! Shopping classification data, we confirmed the finding that there could be a discon-nect between the similarities within the class and across the document corpus, resulting in more similar document be-ing placed in different classes by manual categorizers. One example when stressing the similarity can hurt could be doc-uments with extremely short description of what the item is and extraneous elaborations on the shipping policy. How-ever, other transforms, that are better tailored to boosting discriminative power, such as TF, IDF and weight normal-ization heuristic, improved the classifier accuracy by over 2 . 2%. Other manipulations, including complement class fit-ting failed to improve performance over the regular Naive Bayes model.

We conclude that the conclusions of the  X  X o free lunch X  theorem still hold valid, every specific case and data set call for a separate evaluation and there is no universally best model, even for a given type of data. A diligent researcher can just populate his toolbox with different methods that are reportedly known to work, and use his own judgement as to what approach would be the best in any given case. We would like to thank Kamal Ali, Pavel Berkhin, Cliff Brunk, Igor Cadez, Darya Chudova and Padhraic Smyth for many productive discussions. [1] A. Berger. Error-correcting output coding for text [2] Pedro Domingos and Michael J. Pazzani. On the [3] R.O. Duda and P.E. Hart. Pattern Classification and [4] I.Csiszar. A geometric interpretation of darroch and [5] A. McCallum and K. Nigam. A comparison of event [6] G. McLachlan and K. Basford. Mixture Models . [7] D. Pavlov, J. Mao, and B. Dom. Scaling-up support [8] Dmitry Pavlov, Darya Chudova, and Padhraic Smyth. [9] J. Platt. Using sparseness and analytic QP to speed [10] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling [11] R. Vilalta and I. Rish. A decomposition of classes via
