 Statistical significance measures how well a claimed proposition is supported by Given a set of examples (e.g. itemsets, sequences or graphs) and associated class labels, recent methods such as Limitless Arity Multiple-testing Procedure subsequences or subgraphs) that are significantly associated with the class label. Unlike earlier methods [ 17 ], these approaches provide a guarantee that the proba-bility of at least one false discovery occurring (i.e. family-wise error rate, FWER) is smaller than a pre-defined threshold (conventionally 0.05 or 0.01). However, these methods fail to take into account the various biases that can be introduced by confounding variables (e.g. age or gender of patients) in observational medical data. Figure 1 provides a simple illustration of the type of issues that can arise when potential confounding effects are ignored, leading to unacceptable results. While all patients (represented by their gene expression levels) are separated into two classes along their disease status, we can see that one gender is heavily over-represented in each group. Ignoring the confounding effect of gender on this dataset would likely find significant genes that are related to the gender of the patient, rather than the disease.
 To remove the bias introduced by confounding factors, statisticians typically the very large number of candidate combinations, the correction factor for multi-ple testing can grow extremely large, removing any chance of finding statistically significant patterns. Reducing the correction factor using the same technique as out non-significant patterns (without testing them), which is notoriously difficult when using such a method based on logistic regression models.
 To achieve this goal, we turned to exact logistic regression popular than likelihood ratio testing but commonly used in statistical and bio-logical communities via tools such as SPSS [ 8 ]orR[ 19 ]. The term  X  X xact X  essentially indicates that the test statistics are computed exactly without large-sample approximations, whereas a likelihood-based approach uses asymptotic statistics. Crucially, the computation of exact statistics does not require an iter-ative process to update the parameters until convergence [ 1 ].
 In this article, we propose a novel pattern mining algorithm, LAMP-ELR (Limitless Arity Multiple-testing Procedure with Exact Logistic Regression), regression. LAMP-ELR can be used in a number of different scenarios: control-ling FWER even when the dataset is affected by confounding variables, and integration , which is a useful technique to merge similar experimental data taken from different sources so as to provide a larger set for analysis. We show that our algorithm contributes to these scenarios by applying it to both synthetic data and the Predictive Toxicology Challenge (PTC) dataset [ 3 ].
 Statistical tests based on logistic regression exist to deal with confounding effects these procedures only perform testing of one hypothesis at a time, requiring us to adjust for multiple testing to control for false discoveries [ 2 ]. known to be overly conservative in computing the FWER and cannot be applied for significant pattern mining, where the number of combinations can grow expo-nentially. More recently, multiple testing procedures for controlling the FWER Bonferroni-like multiple testing procedures with Tarone X  X  p-value bound strat-egy [ 14 ] to improve the sensitivity of the correction through frequent itemset mining. However, none of the algorithms can take into account possible con-founding effects introduced by a covariate, making it poorly suited for many types of real-life data. This section lays out the theoretical foundations of significant pattern mining. Given a set of transactions, each labelled with a positive or negative class, and a pattern X , the transaction set can be divided between those where the pat-tern occurs and the rest, producing a contingency table. Statistical association between pattern occurrence and class label is measured using p-values, which are computed by hypothesis testing such as Fisher X  X  exact test and  X  -squared test. If the p-value is smaller than a threshold, the pattern is regarded as statistically significantly associated with the class label.
 number of such hypotheses. In itemset mining with n items, for example, the number of possible patterns can be as large as 2 n  X  1. Such analysis causes serious false discoveries, known as the multiple testing problem. We therefore need to control the FWER. If we note null hypotheses as H if V describes the number of false discoveries, the FWER can be expressed as P ( V&gt; 0 | i =1 H i ).
 FWER is controlled under a pre-specified value (usually  X  =0 . 05 or 0.01). For example, the Bonferroni correction adjusts the threshold to  X  =  X / by calculating the FWER bound as Thus, it is clear that Bonferoni X  X  threshold keeps the FWER below  X  . Unfortu-nately, Bonferroni correction is inappropriate for use with pattern mining: since grows exponentially with the number of items,  X  sinks to a very small value, making new discoveries extremely unlikely. Recently, Terada et al. [ 15 ] have shown that this issue can be mitigated by employing the following trick, first proposed by Tarone [ 14 ]. Suppose one can bound the p-value p i from below with a function f i depending only on marginal counts, i.e. p i  X  f i . We call this bound a min-p bound f &gt; X  , the hypothesis can never be rejected because P ( p this property, the FWER bound of Eq. 1 can become tighter: LAMP uses customised pattern mining algorithms to find the maximum value of  X  that keeps the FWER below  X  . The resulting  X  is normally much larger than Bonferroni X  X  correction factor, resulting in more significant discoveries. However, LAMP cannot handle confounding effects introduced by a covariate, making them poorly suited for many types of real-life data. 4.1 Logistic Regression In biological and medical domains, the logistic regression model is the method of choice for deriving p-values with confounding variables [ 12 ]. Let us consider evaluating the association between a binary outcome y  X  X  0 , 1 tory variable x 1  X  X  0 , 1 } . A categorical covariate x 2 to be known as a confounding variable. Using dummy coding for the categori-cal variable, the logistic model, with  X  denoting the probability of y being 1, is defined as where P  X  X  0 , 1 } is the boolean variable resulting from the evaluation of pred-icate
P . To measure the statistical significance of x model  X  1 =0.
 Incorporating this test into pattern mining is difficult because the p-value is based on large-sample approximations and very inaccurate for biased contin-gency tables [ 4 ]. Deriving a min-p bound requires considering the most biased table, so the min-p can be unreliable. 4.2 Exact Inference Here we introduce the concept of exact logistic regression [ 7 ]. Although based on the same model as regular logistic regression, its p-value is computed exactly, without relying on large-sample approximations, which makes it possible to derive an easily-computable min-p bound.
 examples, where the ith example is a tuple { y i ,x 1 i ,x ality, the examples are assumed to be sorted with respect to x Let us define q k as the number of examples whose covariate is k ( x vectors y , x 1 , x 2k respectively denote the q -dimensional outcome, explanatory and k th (out of K ) covariate values of the q examples. Then, the sufficient sta-tistics for  X  ,  X  1 and  X  2 are defined as  X  0 = 1 y , X   X  respectively.
 like to find out if the observation y is special in that it shows particularly high correlation to x 1 . If the obtained level of explanatory correlation  X   X  predictable from the existing information  X   X  0 and  X   X  2 statistically significant. In exact logistic regression, the sample space is defined as the set of all sample vectors whose positive class size and covariate correlation are constrained to the observed value, called a fiber [ 6 ], It is equivalently represented as To calculate the p-value with respect to the explanatory variable, the null distribution of  X   X  1 is defined as uniform sampling from Y . Then, the p-value is defined as The p-value is computed by making K contingency tables for each value of x . The vectors y k and x 1k denote the outcome and explanatory values in the covariate category k , respectively, and let m k = y k y x 1k . n k represents the number of samples with in the covariate category k . The joint probability of obtaining these tables is described as where t =( t 1 ,...,t K ). Then, the p-value ( 2 ) is rewritten as The min-p bound given marginal counts s k ,m k ,n k , corresponding to the p-value of the most biased table, can be written as This section presents LAMP-ELR: an algorithm that uses a min-p bound ( 3 )to solve the multiple testing problem and find statistically significant patterns. Let E denote a set of items. Let f ( X ) denote the minimum p-value (i.e. min-p) for itemset X  X  E ,and  X   X  R denote a threshold for p-values that discriminates between significant and non-significant patterns. Then, the number of all testable patterns can be described as If the following bounding condition is satisfied: the FWER is bounded by  X  . We compute the largest  X  satisfying this condition. Algorithm 1. LAMP-ELR algorithm, which handles confounding variables. 5.1 Algorithm for K Contingency Tables We propose a depth-first algorithm, called LAMP-ELR, which follows a similar strategy to a fast version of the LAMP [ 9 ], to handle the case of K contingency tables. Our algorithm finds the optimal  X  by using a key point: When an item e is added to the itemset X , the min-p bound for the itemset becomes larger. Theorem 1. If s k  X  s k , f k ( s k )  X  f k ( s k ) .
 Proof. If s x +1) / ( n k  X  x +1) &lt; 1, f k ( x ) &lt;f k ( x  X  1). If s Therefore, Theorem 1 holds.
 Theorem 2. For itemsets X and X = { X  X  e } , f ( X )  X  f ( X ) Proof. Let s s holds.
 first search to collect as many testable patterns as possible while conforming to the bounding condition (Eq. 4 ). Upon starting, the threshold of min-p is set as  X  =  X  and the priority queue holding eligible patterns is empty: S = all the patterns traversed so far whose priority (min-p) is below  X  .If  X  it is clear that  X   X   X / X  (  X  ), since  X  (  X  ) refers to the number of patterns whose min-p is smaller than  X  and | S | refers to those among the patterns traversed so far. We therefore reduce the current  X  until the bounding condition is satisfied. On lines 12 X 14, the current itemset, X , is extended by adding one item e and the function MinPDecrease is recursively called. If f ( X need to explore all further patterns that contain X  X  e due to the monotonicity of f ( X ) (Theorem 2 ). Inserting into the priority queue S ( S.insert ()) and removing the element with highest priority ( S.pop ()) take O (log the maximum priority ( S.max priority ()) can be done in constant time. The following theorem proves that the obtained threshold bounds the FWER. Theorem 3. Let  X  end denote the value of  X  at the end of the algorithm. It sat-isfies the bounding condition ( 4 ) , i.e.,  X  Proof. Due to the pruning condition on line 12 of Algorithm 1 , only elements with min-p value strictly smaller than  X  can be added to S , therefore the value of  X  = S.max priority () never increases with each iteration. Let S S at the end of the algorithm. Since  X  can only decrease, the patterns whose min-p is smaller than  X  end do not get pruned out and are included in S patterns whose min-p is larger than or equal to  X  end are eliminated at line 8 and  X &lt; X / | S | ,wehave:  X  end &lt; X / X  (  X  end ).
 Algorithm 1 is designed for itemset mining, but extensions to sequence or graph mining are straightforward, as long as depth-first search is adapted. 5.2 Speed and Memory Usage Improvements Because Algorithm 1 does not make use of the collection of itemsets X after completing the calculation of min-p, instead of keeping those patterns in S ,we can simply store their total count. We modify Algorithm 1 using a special type of priority queue C , instead of S , to store the histogram counter: insertion into C only happens (with value 1) when the priority key did not previously exist, and its value is incremented otherwise. In most typical implementation of the priority queue structure, such an operation, which we note C.insert or increment (), can easily be implemented with the same computational complexity as a traditional insertion, in O ( log | C | ). We first evaluated the performance of our procedure using synthetic dataset with an application to significant itemsets detection. Our algorithm was implemented by modifying lcmplusplus 1 , a C++ implementation of the LCM algorithm, which is currently the fastest frequent itemset mining method available [ 16 ]. All exper-iments were run under Mac OS 10.4.4 on a 1.7 GHz Intel Core i7.
 The synthetic datasets consist of 1000 transactions, shared equally between positive and negative labels. All positive and negative transactions are assigned to Group 1 or 2 according to a bias factor r (similar to the gender bias in Fig. 1 ). A ratio of r out of all positive transactions are assigned to Group 1, along with 1  X  r of all negative transactions. All other transactions are assigned to Group 2. This dataset contains 100 items, and each item appears with a default probability of 0.1 within each transaction. We introduce true patterns containing three items each. The 5 true patterns correlate with the class of the transactions: each pattern appears in a randomly chosen 20 % of positive trans-actions and is absent from a random 20 % of negative transactions. The 5 patterns simulate a confounding effect by correlating to the group of the transac-tions: each shadow pattern appears in a random 20 % of Group 1 transactions and is absent from a random 20 % of Group 2 transactions. We generated 3 groups at different levels of confounder bias: an unbiased dataset ( r = 0.5), low-bias dataset ( r = 0.7), and high-bias dataset ( r = 0.9).
 terns obtained through frequent itemset mining using a traditional implementa-tion of the LAMP (performing one-sided Fisher X  X  exact tests) on one side, and LAMP-ELR (performing exact logistic regression tests) on the other. A sum-mary of the results can be seen in Fig. 3 . Both methods detect true patterns equally well independent of r . However, as suspected, when group bias is intro-duced and increased, LAMP tends to select more of the shadow patterns along with the true patterns, with a steadily decreasing ratio of true positives among the discoveries. LAMP-ELR, on the other hand, is able to identify and reject patterns whose occurrence is due to the confounder, rather than the actual class. To demonstrate the usefulness of our method, we tested it on the PTC dataset [ 3 ]. The dataset is made of graph structures representing chemical compounds and labelled with an indication of carcinogenicity over four groups: Female Mouse (FM), Male Mouse (MM), Female Rat (FR) and Male Rat (MR). Statistics for this dataset are summarised in Table 1 .
 experiment was run on a machine with two Intel Xeon E5-2680v2 CPUs at 2.8 GHz and 64 GB of RAM. The significance level  X  was set to 0.05. 7.1 Significant Subgraphs In order to outline the advantage of our method for integrated analysis, we compared the results of significant subgraph mining over each individual FM, MM, FR and MR dataset, using LAMP, with the analysis of the dataset obtained by merging all four (adding the subgroup origin as a covariate), using LAMP-ELR. Result statistics are compiled in Table 2 .
 The correction factor for each of the smaller individual dataset analysis is about 100 times smaller than that of the integrated dataset. However, the smaller numbers of samples lead to higher pattern p-values, producing no significant patterns despite the lower correction. When integrating the datasets, the total number of transaction approximately quadruples and we are able to identify 9250 subgraphs with statistically significant toxic effect. Meanwhile, the use of a confounder variable identifying the individual dataset from which each transac-tion is taken, guarantees that the patterns identified are not caused by artefacts or other subset biases. 7.2 Performance Evaluation We assessed the efficiency of our pruning method by comparing it with a naive brute-force version which calculates the minimum p-value for any itemset occur-ring at least once. Figure 4 shows the time performances when considering sub-graphs of increasingly large edge sizes. As could be expected, the calculation time exponentially increases in the brute-force approach. By contrast, the run-ning time of LAMP-ELR increases linearly up to approximately 25 edges, with no noticeable change afterward. For instance, the brute force algorithm requires 11,644 s when considering subgraph of edge size no more than 15, whereas LAMP-ELR can finish in 2,885 s without edge size limit. Our pruning tech-nique succeeds in dramatically reducing the running time, enabling us to detect high-dimensional combinatorial effects while taking into account potential effects from confounder variables.
 on the number of patterns sharing identical values of min-p, which is used as a (unique) key in the priority queue structure C . Figure 5 shows the frequency of each unique value of min-p present in the PTC dataset analysis. A large number of itemsets share identical values of min-p. For example, 115,816 patterns have the identical min-p value 1 . 44 E  X  10. Eliminating this redundancy from storage and insertion computation is directly related with the overall speed gain of our optimised method over a standard approach.
 In this work, we developed a significant pattern mining method based on exact logistic regression statistics that can account for potential confounding effect from a covariate. This is, to our knowledge, the first such method to combine a covariate-aware model with optimised multiple testing procedures to keep sig-nificance sensitivity up while limiting the effect of confounders. In future work, we plan to improve confounder detection for cases where they might not be known in advance. Several methods have been proposed to identify confounders using probabilistic models [ 5 ], that are not currently compatible with pattern mining problems, but show potential promises for our work.
