 Modern text analytics applications operate on large volume s of temporal text data such as Web archives, newspaper archives , blogs, wikis, and micro-blogs. In these settings, searching and mi ning needs to use constraints on the time dimension in addition to key-word constraints. A natural approach to address such querie s is using an inverted index whose entries are enriched with vali d-time intervals. It has been shown that these indexes have to be par ti-tioned along time in order to achieve efficiency. However, wh en the temporal predicate corresponds to a long time range, req uiring the processing of multiple partitions, naive query process ing incurs high cost of reading of redundant entries across partitions .
We present a framework for efficient approximate processing of keyword queries over a temporally partitioned inverted ind ex which minimizes this overhead, thus speeding up query processing . By using a small synopsis for each partition we identify partit ions that maximize the number of final non-redundant results, and sche dule them for processing early on. Our approach aims to balance th e estimated gains in the final result recall against the cost of index reading required. We present practical algorithms for the r esulting optimization problem of index partition selection. Our exp eriments with three diverse, large-scale text archives reveal that o ur proposed approach can provide close to 80% result recall even when onl y about half the index is allowed to be read.
 H.3.3 [ Information Search and Retrieval ]: Search process; H.2.4 [ Database Management ]: Systems X  Query Processing Experimentation, Algorithms, Performance Time-travel Search, Partitioned Inverted Index, Partitio n Selection, Synopses
Large-scale versioned text data is increasingly becoming a bun-dant in the form of archives of the Web, corporate/CRM record s, wikis, blogs, micro-blogs, etc. The history of information evolu-tion buried in these collections is an important source of ac tionable intelligence in a variety of applications. It is often neces sary to re-trieve documents from these collections that satisfy certa in content predicates, expressed typically through keyword queries, as well as temporal predicates, e.g., on the time when they were publis hed or accessible on a Web server. As a concrete example, conside r a business analyst looking for web pages predicting and anal yzing upcoming releases of tablet computers. If only keyword quer ies are used to retrieve pages from a Web archive, irrelevant inform ation about earlier releases of tablet computers may corrupt the a nalytics results. By including a temporal constraint on the publicat ion or discovery time of web pages, such undesirable results can be elim-inated. Queries that combine the content and temporal predi cates are termed as time-travel queries .

Processing time-travel queries is much more expensive than pro-cessing plain keyword queries without temporal constraint s. Us-ing standard preprocessing techniques from information re trieval, a naive implementation could build inverted lists that store , for each term, information about all documents that contain this ter m, in-cluding the document lifespan. However, when processing ti me-travel queries, a large fraction of reads from the inverted l ists would be wasteful  X  i.e., do not contribute to the final result  X  name ly all the entries that do not qualify for the temporal predicat e of the query.

In [8, 9] these issues were addressed for the restricted clas s of time-travel queries referring to a single point of time in th e past. That approach partitioned inverted lists along time, resul ting in list partitions that contain all entries whose lifespan overlap s with the time interval assigned to the partition. For processing a ti me-point query in the resulting temporally partitioned index, it is s ufficient to consider just one partition for each term, leading to sign ificant reduction in I/O and computational costs during query proce ssing. As a natural side-effect of this temporal partitioning, doc uments with long lifespans are replicated across several adjacent partitions.
For the much more practical and general class of time-travel queries where the temporal predicate is a time range , the straight-forward query processing of [8] quickly becomes inefficient as a consequence of this replication, as repeatedly reading ent ries that are replicated in several partitions spanning the query tim e range wastes I/O operations. The simple alternative of not partit ioning is also not desirable, as it penalizes queries with shorter tim e-range or time-point predicates by having to scan complete lists. The effects of this are further exacerbated in the typical text analytic s process involving multiple interactive steps of query reformulati on, expan-sion, and refinement, which require quick turnaround times. Figure 1: Processing a time-travel query  X  X  B" @ [ t b , t partition selection
In analytical settings like those outlined before, a user do es not always require exact query results, but often is satisfied wi th a good approximation computed quickly. In many application domai ns, like news archive search, the same information is available from multiple articles, so missing a few of them is acceptable. Si milarly, a subset of results is usually sufficient for checking if cont ent or temporal predicates require further tuning.

This paper introduces an approach for approximate processi ng of time-travel queries in versioned text collections that a llows to trade in some result quality for improved retrieval time. It exploits the following observation: if we have an oracle that provide s the information that a partition largely consists of entries th at are repli-cated in already processed partition(s), we can avoid proce ssing this partition without significantly compromising the final resu lt qual-ity. We aim at selecting a set of partitions which can generate the best approximation of query result while not exceeding a giv en cost limit for processing them. This cost limit can be specified as a limit on processing time, which can then be transformed into bound s on the abstract costs such as the number of partitions opened fo r read-ing or the number of index entries read that we use in this work . Al-ternatively, the user can also stop the processing at any tim e when she determines that the results are already satisfying (or t he query needs to be refined); our greedy methods support this by selec ting partitions first that are likely to contain many unseen answe rs. general idea of our approach. This figure shows two inverted l ists for two terms A and B built over documents with lifetimes. On t he left side of the figure, individual inverted lists are shown, each span-ning the entire time interval. The gray shaded region repres ents a temporal predicate that spans a small time range over these l ists. In the absence of temporal partitioning, query processing n eeds to scan both lists entirely and filter out entries that do not sat isfy the temporal predicate. When the index is partitioned, however , the processing can be speeded up by reading only the relevant par ti-tions that overlap with the temporal predicate, represente d on the right side of the figure. Thus, in our example, a total of 6 part itions  X  A 1 , A 2 , A 3 and B 1 , B 2 , B 3  X  have to be processed to determine the answer set of { d 1 , d 2 , d 3 }  X  marked as thick red line-segments. However, a closer inspection of Figure 1 reveals that the sam e an-swer set can be obtained by processing only 2 partitions , A B , since the replicas of index entries for documents in the ans wer set are fully available within these two partitions.
In this paper, we propose a query processing approach which places the ability to control the performance directly in th e hands of the analyst posing the query. In particular, we consider t he fol-lowing problem:
P ROBLEM 1. Given an upper-bound on the I/O cost incurred during query processing, select a subset of partitions so th at the re-call of the final result is maximized without exceeding the sp ecified upper-bound.
 We formally model this partition selection problem as an optimiza-tion problem, distinguishing two alternative formulation s: a) Size-based Partition Selection  X  where the I/O cost of accessing a par-tition is proportional to the size of the partition, and b) Equi-cost Partition Selection  X  where the latency of accessing a partition is predominant, irrespective of the size of the partition.

Making use of KMV synopses for cardinality estimates under s et operations [10], we develop algorithms for efficiently solv ing such partition selection problems. In particular, the contribu tions of this paper are: 1. An optimal algorithm, based on dynamic programming, for 2. An efficient greedy alternative for partition selection t hat can 3. A detailed experimental evaluation on three large-scale real-
Our experimental evaluation shows that our proposed method s can compute more than 80% of final results even when the I/O bud -get is set as low as 50% of the total size of the partitions that satisfy the temporal predicate.
We operate on a document collection D . Each document d  X  D from the collection has a unique identifier id d and consists of terms drawn from a vocabulary V , i.e., d  X  V . Furthermore, each doc-ument has an associated valid-time interval [ b d , e d ) that conveys when the document existed in the real world. For simplicity o f presentation, we assume that each document exists in exactl y one version, and we will talk only about documents from now on. Th is restriction can be easily lifted by identifying a version of a docu-ment by the identifier and the begin timestamp of the version, and returning every version satisfying the temporal predicate and con-taining the queried text as a result.

We assume that documents are indexed on a per-term basis, in the spirit of an inverted index, and that the index is tempora lly par-titioned. In detail, we let P v denote the set of partitions of the inverted list L v for term v  X  V . Every partition P v,j has an as-sociated time interval [ b v,j , e v,j ) and contains (identifiers of) all documents in L v that existed at any time during the time interval associated with P v , i.e., Further, for the scope of this work, we assume that time inter vals associated with partitions for term v are disjoint, i.e.,
Such temporally partitioned inverted lists have first been p ro-posed for the Time-Travel Inverted Index (TTIX) in [8]. It ma in-tains, for each partition, a list with entries for all docume nts in that partition that are augmented by validity-time intervals, w hich are thus of the form &lt; id d , b d , e d , tf &gt; where id is the frequency of the list X  X  term in the document. The parti tion-ing strategies introduced in [8] trade-off extra storage-c osts and query-processing gains.

A time-travel query, as considered in this work, consists of a set of terms Q = { q 1 , . . . , q m } and a time interval [ b result of the query Q @[ b q , e q ] is defined as the set of documents that contain all terms from Q and existed at any time during the time interval [ b q , e q ] , that is formally: Queries for which b q = e q holds, so that the query time-interval collapses into a single time point, will be referred to as time-point queries .
In many stages of our proposed approach, we depend criticall y on obtaining high quality cardinality estimates under unio n and in-tersection of large sets of document ids. For this purpose, w e utilize recently proposed KMV synopses [10]. In a precomputation st ep, we build and store on disk the synopses for each partition of t he temporally partitioned index, which we use during our parti tion se-lection process. In this section, we provide a brief backgro und on KMV synopses.

Beyer et al. [10] introduced KMV synopses as effective sketches for sets that support arbitrary multiset operations includ ing union, intersection, and differences. A KMV synopsis for a multise t S is created as follows: Fix a hash function h of the form h :  X ( S ) 7 X  0 , 1 , . . . , M where  X ( S ) contains the distinct values in S and M = O ( |  X ( S ) | 2 ) . The hash function h is applied to each of value in  X ( S ) , and the k smallest of the hashed values form the KMV (for k minimum values ) synopsis L S of S .

KMV synopses can deal with a variety of multiset operations (union, intersection, difference). The following equatio n computes an unbiased estimate for |  X ( S ) | , the number of distinct values in S , from the KMV synopsis, where U k is the value of the k  X  X h smallest value:
Given two multisets A and B with their KMV synopses L A and L B of size k A and k B , respectively, it is possible to esti-mate the number of distinct values in the union of A and B as D  X  = |  X ( A  X  m B ) | (where  X  m denotes the union of two mul-tisets). Let L = L A  X  L B be defined as the set including the k smallest values in L A  X  L B , where k  X  = min ( k A , k B KMV synopsis of size k  X  describing L A  X  m L B . D  X  is estimated by the following equation: A similar estimator can be developed for D  X  = |  X ( A  X  m the number of distinct values in the multiset intersection o f A and B ; we omit details for space reasons.
We first focus on the special case where the time-travel key-word query Q @[ b q , e q ] consists of only a single query term, i.e., Q = { q } . Our objective when selecting partitions to process the time-travel keyword query is to retrieve as many of the origi nal query results as possible, while not violating a user-speci fied I/O bound. Our optimization criterion, to put it differently, i s to maxi-mize the relative recall as the fraction of original query results re-trieved. The user-specified I/O bound, which constrains the space of valid solutions, can either be entry-based ( size-based partition selection ) or partition-based ( equi-cost partition selection ). In the former case, we are allowed to read up to a fixed number of index entries; in the latter case, we are allowed to select up to a fix ed number of partitions.

It should be noted that while developing algorithms for vari ous partition selection problems, we assume that we are given wi th an oracle that can efficiently provide us the cardinalities of i ndividual partitions as well as the result of their intersection/unio n. As we show later, KMV synopses can be used to build a practical alte rna-tive that approximates this idealized oracle.
The input to this optimization problem is the set of affected par-titions P q,j with 1  X  j  X  m , and the user-specified I/O bound  X  as the fraction of entries of affected partitions that we are allowed to read. Formally, the problem can be stated as where x j is an indicator variable which denotes whether the parti-tion P q,j is selected.

The above problem can be solved using dynamic programming over an increasing number of partitions affected. We build a dy-namic programming table, DP , such that each cell DP [ c ][ p ] repre-sents the set of selected partitions for the prefix subproble m which considers the affected partitions {P q, 1 P q,p } , and the capacity is set to c . Theorem 1 proves that the recall for such a subprob-lem can be computed by reusing the solution of the subproblem s DP [ c  X  ][ p  X  ] ( 0  X  c  X   X  c for integral values of c X ).
T HEOREM 1. Let r ( S c,k ) denote the recall obtained by S subset of partitions selected from the ordered set, {P q, 1 and satisfying the capacity c . Let  X  S c,k be the selection of partitions such that the recall is maximized, i.e.,  X  S c,k = argmax r ( S Then, r ( S c,k ) = max
P ROOF . Assume that we have optimal solutions for all sub-problems with capacities less than c , for the set of partitions {P q, 1 , , P q,i } where 0 &lt; i &lt; k . Now we consider com-puting the optimal selection set  X  S c,k for the subproblem with a capacity c , and an ordered set of partitions {P q, 1 P us denote the optimal recall value to be OP T c,k and assume that OP T c,k &gt; r (  X  S c,k ) .

Case 1  X   X  S c,k does not include P q,k : This means OP T r (  X  in the  X  S c,k . As a consequence, we have a new optimal solu-tion for the subproblem for capacity c and the set of partitions {P q, 1 , P q,k  X  1 } . But, this is contrary to our initial assumption that we already have the optimal recall values for the prefix s ub-problem. Thus, by contradiction or claim in the theorem hold s.
Case 2  X   X  S c,k to obtain OP T c,k includes partition P index of the partition selected just before P q,k to be k  X  . Hence, This is contrary to our assumption and hence by contradictio n our claim in the Theorem holds true.

Lemma 1 paves way for efficiently filling the dynamic program-ming table for the defined optimization problem.

L EMMA 1. For a set of partitions belonging to a term, the over-laps of contents of partition P q,i with P q,i + k ,  X  k  X  0 have the following property:
Each of the DP table cell contains a pair of values  X  the last pa r-tition selected, lp , for the corresponding subproblem (i.e., the se-lected partition with the maximum begin-time), and the opti mal re-call value r . Since the choice of partitions cannot be made indepen-dently, the computation of recall for a newly selected parti tion takes into account only the entries that are not already included i n previ-ously selected partitions. Using Lemma 1, we can efficiently com-pute the optimal recall for each subproblem since all overla ps with the preceding partitions to lp are already covered in lp . The DP-based algorithm, outlined in Algorithm 1, has a time complex ity of O ( n 2 ( P j |P q,j | )) and a space complexity of O ( n ( P Algorithm 1 Partition Selection -Dynamic Programming solution 2: DP [ 0 .. c max ][ 0 .. n ] // dynamic programming table 3: 4: for i = 0 .. c max do 5: DP [ i ][ 0 ] =  X  6: end for 7: 8: for k = 1 .. n do 9: for i = 0 .. |P q,k |  X  1 do 10: DP [ i ][ k ] =  X  // no partitioning possible 11: end for 12: for i = |P q,k | .. c max do 13: for k  X  = 0 .. k  X  1 do 14: // update if recall is better than current value 15: r k  X  = DP r [ i  X  |P q,k | ][ k  X  ] + ( |P q,k |  X  ( P 16: end for 17: k  X  = argmax r k  X  18: // update the DP table with the best partitioning 19: DP r [ i ][ t ] = max { DP r [ c j ][ k i  X  1] , r k  X  20: DP lp [ i ][ t ] = argmax DP r [ i ][ t ] 21: end for 22: end for 23: 24: return DP [ c max ][ n ]
The optimal partitioning can be easily computed by retracin g the path taken by the best solution seen at DP lp [ c max ][ n ] .
The inputs to this problem are the set of affected partitions P and bound  X  as a fraction of the number of affected partitions, N, to be read. We use the previous notations and formally state t he problem as It can be observed that this is a special case of the problem de fined in Section 3.1 above, obtained by setting the value of 1 as the cost of each partition. However, when we employ uniform cost per par ti-tion, improvements are possible in both space and time compl exity of the algorithm, since the optimal value is independent of t he sum of the sizes of the partitions read. The resulting time compl exity of the algorithm is O ( n 3 ) and a space complexity of O ( n ) .
While the selection algorithms outlined above allow for pol yno-mial run times, they are not efficient enough to be applied dur ing query processing. Alternatively, we propose the use of an (1  X  approximation obtained by reducing our partition selectio n prob-lem into an instance of budgeted maximum coverage (BMC) prob-lem. We begin by recalling the definition of BMC [12]: collection of sets S = {S 1 , S 2 , . . . , S m } with associated costs { c } is defined over a domain of elements X = { x 1 , x 2 , . . . , x with associated weights { w i } . The goal is to find a collection of sets S  X   X  S , such that the total cost of the elements in S not exceed a given budget L, and the total weight of every elem ent covered by S  X  is maximized.

It should be noted that BMC is known to be an NP-Hard prob-lem, while partition selection is clearly in P. Nevertheles s, we re-duce the partition selection into BMC (by ignoring the tempo ral contiguity in partitions), in order use an efficient greedy a pproxi-mation algorithm, GreedySelect [12].
 tion selection problem for single terms can be cast into an in stance of the Budgeted Maximum Coverage (BMC) problem [12] in the following way: The affected partitions, P q,j  X  X , are the analogous to the sets in the BMC problem with the entries in the partitions being the elements of the respective set . For size-based partition selec-tion the cost for each set is its cardinality; for equi-cost s election, the cost for each set is unity. The cost budget is exactly the I /O bound IO_BOUND.

As a result, we can use the approximation algorithm, GreedySe-lect  X  X hown in Algorithm 2 X  proposed by Khuller et al. [12] which has a constant approximation guarantee of (1  X  1 e ) . In this algo-rithm, every partition is associated with a cost ( c i ) and a benefit ( B i ). Let the selection set be denoted as  X  S for simplicity. The cost in the partition (for size-based selection) or 1 (for equi-c ost selec-tion). Its benefit, B i , is the number of unread/uncovered entries in P q,j , i.e., |P q,j \  X  s  X   X  S s | . We additionally define IO_BOUND as the bound on the amount of I/O allowed (in terms of number of en -tries or number of partitions read). Each iteration of GreedySelect consists of a selection step , where the best partition in the current state is chosen and added to the selection set  X  S , followed by an update step , where the benefits of the remaining partitions are ad-justed. GreedySelect chooses the best partition based on a greedy heuristic that picks at each iteration a partition that maxi mizes the benefit/cost ratio B i c
In the case of partition selection for single-term queries, every document entry read from a partition qualifies as an answer, g iven Algorithm 2 GreedySelect : Approximate Partition Selection 1: c max =  X  IO _ BOUND  X  2:  X  S =  X  3: A = P q 4: C = 0 5: 6: repeat 7: Select P q,i  X  A that maximizes B i c 8: if C + c i  X  c max then 9:  X  S =  X  S  X  P q,i 10: C = C + c i 11: end if 12: A = A\P q,i 13: until A =  X  14: 15: Select a partition P q,t that maximizes B t over S 16: if B (  X  S )  X  B t then 17: output  X  S 18: else 19: output {P q,t } 20: end if that the time-range of the partition overlaps with query tim e-range. Unlike this simpler setting, for multi-term queries there i s an ad-ditional constraint imposed by the conjunctive semantics of query evaluation which requires that every result document also c ontain all the query keywords. Mimicking the conventional query proce ss-ing (over standard inverted lists), multi-term queries can be evalu-ated by intersecting partitions of individual query terms. Now, the partition selection aims to increase the coverage of entrie s that be-long to this intersection space of partitions.

We denote by SPS and EPS the size-based and equi-cost parti-tion selection respectively, and define them formally for mu lti-term queries as follows: or, and, where x ij is an indicator variable which denotes whether a partition P i,j is selected for processing.
The intersection space, in the objective function above, is the intersection of the unions of the affected partitions. Usin g the distributive property of the set intersection operator we c an rep-resent the above into unions of intersection ( T 1  X  j  X  m S T 1  X  j  X  m P i,j ). Let each of these resulting smaller intersec-tions, consisting of one partition each from every term, be r epre-sented as a tuple x . It is easy to see that these tuples come from the Cartesian product among partition sets P q or for a m-term query X = P 1  X  . . .  X  P m (denoting the Cartesian product set as X ). We formally define x , an element of this Cartesian product set X , as : Figure 2:  X  -set for the affected partitions of a time-travel query over terms  X  q 1  X  and  X  q 2  X 
We can now use GreedySelect over X , where each element x is equivalent to a partition in single-term selection scena rio. The benefit of x is defined as the cardinality of the documents in the intersection of the partitions in x which are not in the selection set  X  S . In other words, the benefit or contribution of x represents the number of new documents which are present in every element partition of x . The cost definition of x depends on the sizes of the element partitions in x . Similar to our assumptions earlier Size-based selection sets the cost of x as the sum of the sizes of the participating/element partitions not in  X  S . Equi-cost selection on the other hand defines the cost of x as the number of participating partitions not in  X  S .

The reconstructed inputs to the algorithm GreedySelect is now the set X , with defined benefits and cost for each of its elements, and the IO_BOUND derived from  X  or  X  e depending on the vari-ation of problem used. GreedySelect now proceeds conventionally by greedily choosing the x with the best benefit by cost ratio. Ob-serve that the choices of elements from X are not independent. Se-lection of a certain element, x , might result in reducing the cost (not the case in single-term selection) of others which have at least one of the constituent partitions common with x . Hence in the up-date step, apart from updating the benefit of x , we also update its cost. Because of this varying costs characteristic, the app roxima-tion guarantee for the algorithm does not apply but is seen to work well in practice.

The Cartesian set of partitions might be large, particularl y if the number of terms in a query or the partitions per term or both ar e larger in number. In such a scenario GreedySelect can progressively get inefficient because of the numerous update cycles. To all eviate this we operate on a constrained set,  X  -set  X  X , which has a car-dinality linear in the number of participating partitions a s opposed to high number of combinations in X . This constrained set is ob-tained by defining a  X  -join operation over the term-partition sets P  X  X  such that each element t of the resulting tuple is an element of X has the property that there is non-zero time-overlap betwee n all of the constituent partitions.
For example, in Figure 2, the queries q 1 and q 2 have 3 partitions each with a X of cardinality 9. However, the resulting  X  -set has a cardinality 5 after the  X  -join operation.
 We further show for Equi-cost based Partition selection, GreedySelect chooses elements only from the  X  -set. In other words, GreedySelect over the Cartesian set is equivalent to GreedySelect over the constrained  X  -set.
T HEOREM 2. GreedySelect for Equi-cost based selection on the entire Cartesian set X chooses elements which also belong to the  X  -set.

We prove this theorem by contradiction, by first choosing an e le-ment from X \  X  and showing that we can replace this element with a better candidate from  X  . Formally to prove this theorem we need to introduce the notion of unselected-partition space . The selection set  X  S is the set of already selected partitions and let us for conve -nience define  X  S d as the actual set of result documents covered by the partitions in  X  S . A time range in the intersection-space is said to be unselected if none of the partitions in  X  S have time-ranges over-lapping with the given range. In other words an unselected-s pace refers to a range where none of the partitions have been selec ted, or /  X   X  S , at the current state of the algorithm.
 Benefit of a candidate, introduced earlier, is non-zero if T of partitions in x which are unselected. We proceed to prove the following lemma which is essential in the proof for Theorem 2 .
L EMMA 2. For any unselected-partition space, the best benefit is given by a candidate from the  X  -set.

P ROOF . We prove this by contradiction. Let there be a candi-date, x  X  X \  X  which has a higher benefit than any of the candi-date c  X   X  . Let P v,i  X  x has the maximum partition begin time b v,j . Using Lemma 1, we can replace partitions of the other terms P w,j ( w 6 = v ) which contain the time b v,j to obtain a higher or equal overlap and hence better benefit. Since the new candida te obtained by the replacement belongs to  X  , this is contrary to our assumption hence our claim holds.
 We now proceed to the proof for the theorem.

P ROOF . We prove this by induction on the number of iterations i of the greedy-select algorithm. i = 1 : For the first iteration the entire intersection-space is un se-lected, i.e.,  X  S =  X  . Given that there is enough budget for selection for m-keyword query i.e. m  X  IO _ BOUND we always select partitions from the collapsed set according to lemma 2. i  X  i + 1 : Choosing from  X  for the first i iterations induces only multiple unselected regions of the intersection-space. Be cause of the nature of the  X  -join certain time-ranges are completely cov-ered/selected and certain ranges are unselected. The parti tions which are not in the unselected-space will always have zero b en-efit hence we can safely discard them.

Now, choosing a candidate x  X   X  X \  X  could have a cost value c (where 0  X  c  X  m ) depending on the number of constituent partitions already in the selection set. To prove that the ch oice of the candidate is still made from the  X  -set we argue as in the proof of Lemma 2. Assume that there is a better candidate x  X  X \  X  (best benefit/cost ratio), and a non-zero cost c . We can always replace the partitions x i  X  x  X  x i /  X   X  S with another partition of the same term in the following ways:
Case 1  X  x i /  X   X  S  X  x i  X  x : In the case of x having no partitions from the selection set  X  S , i.e., we use Lemma 2 to choose a better candidate from  X  since there are only unselected regions from where a choice can be made.
Since we operate only within unselected regions, we denote t he minimum time boundary in the region as left region boundary a nd the maximum time boundary as the right region boundary. For cases 2 and 3, we consider candidates x with non-zero benefit, and non-zero cost less than m ,i.e.,
Case 2  X  Suppose that the partition x  X  i  X  x  X  | x  X  i  X  belong to the right region boundary. We can always choose a re -placement partition r j for x  X  j  X  x  X  | x  X  j /  X   X  S , where r belong to the same term, such that the new replacement candid ate r  X   X  has a better benefit than x  X  . More specifically, the replace-ment candidate r  X   X  has the following selected and unselected partitions It is easy to observe that the replacement candidate r has the same cost as its counterpart x  X  , and a better or equal benefit value, thus giving it an overall better benefit/cost ratio. Since such a r eplaced candidate belongs to the  X  -set, this is contrary to our assumption and our claim holds.

Case 3  X  Similar to case 2, if x  X  has partitions belonging to the left boundary of the region, we can replace the unselected pa rtitions of each term by a replacement partition which contains/over laps with the maximum end time among the partitions which belong to the selection set in x , i.e., t met = max { e x  X  The new replacement candidate r , set belongs to the  X  -set, and has a better or equal benefit than x contrary to our assumption.
While the previous two sections presented the theoretical u nder-pinnings for the partition selection problem, in this secti on, we dis-cuss a few issues during their implementation that we faced i n prac-tice and present solutions we used.
 In our algorithmic descriptions above, we assumed that if a p ar-tition overlaps with the query time-range, then its contrib ution to the final answer set is from all the entries in the partition. In other words, we ignored the fact that even within the partition, a p ossi-bly large number of entries may not satisfy the temporal pred icate if the temporal boundaries of the partition are not complete ly con-tained within the range specified by the temporal predicate. Note that this affects the estimates of the benefit values of the partitions in the boundaries of the query time  X  thus the benefits of at mos t 2 partitions per term are in error.

This error can be significantly improved if we adjust the valu e of benefit of a partition to account for incomplete overlap al ong the time axis. A straightforward approach for this, which we emp loy in our implementation, is to scale the benefits by the fraction of temporal overlap between the query and the partition. In pra ctice, we observed that this simple scaling (which can be seen to be s im-ilar to making uniformity assumption during cardinality es timates) works very well.
 Another issue that comes up when we are using only the estimates of benefit provided by partition(s) towards the final answer s et is that during partition selection, we may estimate the cardin ality of overlap between partitions to be zero  X  although they may hav e low-cardinality overlap. Thus, we may wrongly estimate zer o ben-efit for all partitions . When faced with such a situation, partition selection algorithms described in Sections 4 and 3 simply te rmi-nate  X  even if the specified I/O budget allows for more partiti ons to be read.
To avoid this undesirable behavior, the partition selectio n algo-rithm is modified to ignore the estimates of benefits when all the unselected partitions have zero estimated benefits. At this stage, partitions are selected in decreasing order of their size as long as the I/O budget is not violated.
In this section, we present and discuss the results of a detai led ex-perimental evaluation of our algorithms in terms of their ef fective-ness in achieve high recall levels while keeping within the s pecified budget on the index accesses.
All our algorithms, including the underlying time-travel i nverted index framework, were implemented using Java 1.6. All exper -iments were conducted on Dell PowerEdge M610 servers with 2 Intel Xeon E5530 CPUs, 48 GB of main memory, a large iSCSI-attached disk array, and Debian GNU/Linux (SMP Kerne l 2.6.29.3.1) as operating system. Experiments were conduct ed us-ing the Java Hotspot 64-Bit Server VM (build 11.2-b01).
For our experiments we used three different datasets, all de rived from real-world data sources.
 WIKI. The English Wikipedia revision history [14], whose uncom-pressed raw data amounts to 0.7 TBytes, contains the full edi ting history of the English Wikipedia from January 2001 to Decem-ber 2005. We indexed all versions of encyclopedia articles e xclud-ing versions that were marked as the result of a minor edit (e. g., the correction of spelling errors etc.). This yielded a total of 1,517,524 documents with 15,079,829 versions having a mean (  X  ) of 9.94 versions per document at standard deviation (  X  ) of 46.08. UKGOV. This is a subset of the European Archive [1], con-taining weekly crawls of eleven governmental websites from the U.K. We filtered out documents not belonging to MIME-types text/plain and text/html to obtain a dataset that to-tals 0.4 TBytes. This dataset includes 685,678 documents wi th 17,297,548 versions (  X  = 25.23 and  X  = 28.38).
 NYT. The New York Times Annotated corpus [2] comprises more than 1.8 million articles from the New York Times published b e-tween 1987 and 2007. Every article has an associated time-st amp which was taken as the begin time for that article. The end tim e for each article was chosen to be 90 days after the begin time, giv ing every document a validity time of 90 days. This is done to refle ct the real world setting where the news articles are publicly a vailable only for a limited period from their publication, and also to coarsely model the commonly used time-decaying relevance model for n ews articles.

Note that each of these datasets represents a realistic clas s of time varying text collection typically used in temporal tex t analyt-ics. Specifically, WIKI corresponds to an explicitly versio n con-trolled text collection, UKGOV is an archive of the evolving Web, and NYT is an instance of archive of continually generated ne ws-paper content. For ease of experimentation, we rounded the t ime-stamps of versions to the nearest day for all datasets.

We compiled three dataset-specific query workloads by extra ct-ing frequent queries from the AOL query logs, which were tem-porarily made available during 2006. For the WIKI dataset we extracted the 300 most frequent queries which had a result cl ick on the domain en.wikipedia.org and similarly for NYT and UKGOV we compiled 300 queries which had a result hit on nytimes.com and 50 queries which had result hit on .gov.uk domains. Using these keyword queries, we generated a time-t ravel Index UKGOV NYT WIKI Fixed-7 11G 13G 13G Synopsis Index -5% sample 146MB 134MB 146MB Synopsis Index -10% sample 291MB 258MB 290MB Fixed-30 4.4G 3.5G 6.3G Synopsis Index -5% sample 61MB 39MB 75MB Synopsis Index -10% sample 122MB 74MB 149MB query workload with 3 instances each for the following 2 diff erent temporal predicate granularities: 30 days and 1 year.
The time-travel inverted index is stored on disk using flat fil es containing both the lexicon as well as inverted lists. At run time, the lexicon is read completely into memory, and for a given qu ery the appropriate partition is retrieved from the index flat fil e on disk. These inverted lists are stored using 7-Bit compression. Th e syn-opses of partitions were maintained in a separate flat file in a similar fashion.

For temporal partitioning of the index, we employ a very simp le approach in which a partition boundary is placed after a fixed time window. We avoided using more sophisticated partitioning s trate-gies from [8] as they can not be easily maintained incrementa lly, and also due to their high computational overheads. We prese nt re-sults for two time window sizes: (i) 1 week (referred to as Fixed-7 partitioning), and (ii) 1 month (referred to as Fixed-30 partition-ing). Unless otherwise mentioned, all the results presente d in this paper are from Fixed-7 partitioning. Results for other partitioning strategies (including those from [8]) will be made availabl e in a technical report version of the paper.

The estimates from the KMV synopses [10] that we chose to im-plement are naturally dependent on their size in relation to the base data size. We experimented with two sizes of synopses: 5% and 10% of the partition size (with minimum size set to 100). Unle ss otherwise mentioned, we report results for 10% size of the KM V synopsis. A synopsis index was generated during index const ruc-tion time and stored as flat files on disk. Instead of storing th e list of hashed double values of the KMV synopsis, the correspondi ng document identifiers(integers) were stored for better comp ression (Table 1). The doc ids were translated to their respective do ubles during query time for the necessary KMV intersection estima tion.
Finally, we employed a practically infeasible oracle for partition selection, which computes the ideal values of set operations (inter-section and union) between partitions. Our oracle computes these values by evaluating the query completely, without any part ition selection, and then uses them in partition selection to over come the errors due to estimates from the KMV synopses.
We conducted experiments aimed at evaluating the effective ness of partition selection, in terms of the recall obtained for t he final answer set for each query, as we vary the specified I/O budget. The budget bounds were incremented in steps of 0.1, starting fro m 0, and the recall values obtained for each instance of the time-travel keyword query were averaged. During averaging, we ignored t ime-travel keyword queries that affect only one partition each o f the terms involved. We also ignored queries which have no result s as they contribute to false-positives for partition selectio n.
For comparing I/O performance of different techniques, we m ea-sure the number of index entries read after applying the part ition selection  X  denoted as RWS , and the number of index entries read without applying partition selection  X  denoted as RWOS . The ra-tio RW S RW OS , called Ratio of Index Reads , is denoted as RIR . We also measure the actual query runtime to show their correspo ndence with the Ratio of Index Reads .
In the first set of experiments, we demonstrate the impact of u s-ing partition selection in identifying the set of partition s that maxi-mize the final result recall, while adhering to the specified I /O bud-get. As described before, the I/O budget can be specified in tw o forms  X  based on size of partitions, or based on the number of p ar-titions. For both the budget formulations, we ran the full qu ery workload for each of the datasets, and present aggregated re sults individually for every query time-window. The results are p lotted in Figure 3 for size-based selection, and in Figure 4 for equi -cost partition selection approaches.

These graphs unequivocally demonstrate that the partition selec-tion can be very effective in speeding up the time-travel que ry pro-cessing with minimal impact on the quality of final results.
Going further, we notice that for queries with a time-window of one month, the selection algorithm selects the most relevan t parti-tion thus providing high levels of recall by reading close to 50% of the affected entries/partitions. Similar behavior is al so seen for Equi-cost partition selection, which manages to read the co rrect set of partitions to obtain as high a recall as possible.

In case of queries with yearly time-window, relevant entrie s are spread over a larger number of partitions. This allows for a g reater flexibility in choosing the partitions for processing. This allows selection algorithms in both Equi-cost and Size-based vari ations, to report higher quality results even with very low I/O budge t.
Overall, from these results, one can observe that the partit ion se-lection under either size-based or equi-cost model show ver y sim-ilar performance behavior. Therefore, we omit the results o f equi-cost partition selection in the rest of the section.
The focus was on measuring runtime in a cold-cache setup. We start with a cold-cache and flush it after each query executio n step. Each time-travel query from the workload was evaluated for 1 0 I/O bounds (0.1 through to 1.0) and the average time taken (in mil lisec-onds) for each of these bounds are presented in Tables 2, 3 and 4. The first column represents the tunable input I/O bound param eter  X  , which indicates the fraction of the affected entries to be r ead. This is followed by the reporting the recall attained along w ith the average runtime for the specified bound. We compare the resul ts of selection based retrieval with two competitors: (a) standard unpartitioned inverted index list, unpartitioned , (b) partitioned lists not supporting partition selection, no-sel  X  The reported runtime for each query is the sum of time taken by the synopsis based query plan computation (partition selectio n) and the actual query processing time with the selected partitions. We ex-clude the query plan computation time for the specific bound 1 .0 in the results since the bound eventually results in selecti on of all the partitions. The runtime for this bound incidentally is a lso the time taken for no-sel . The runtime results further corroborate the observations presented before. Observe that in case of exec uting time-travel queries against an unpartitioned index takes a lmost 3 secs for WIKI in Table 2, 1 sec for NYT in Table 4 and as large as 12 secs for UKGOV in Table 3 irrespective of the query time-range. Having a partitioned index improves on this as is indi cated by the no  X  sel values in the tables. However partition selection over such an partitioned index further reduces execution ti me to give recall values of almost 0.8 in only 50%-60% time. In case of comparatively smaller collections, say NYT, one might pote ntially argue about the performance not being significant in terms of ab-solute runtime but in larger corpora (like UKGOV) it can make a significant difference in performance as is shown in Table 3.
The anytime nature of the selection algorithm also means tha t the user can terminate the query processing at any instant she wi shes and can still get the maximum recall at that stage of the compu ta-tion. A quick preview at the results after 3/4 of a second can p rove beneficial with almost 90% recall (UKGOV monthly query) or 85 % recall (WIKI yearly query).
The first set of experiments were aimed at quantifying the im-pact of using KMV synopses for the estimation of benefits, and the effect of different synopses size. For each dataset, we m ea-sure the average recall obtained for each granularity of tim e-travel queries, using 5% and 10% synopses, and compare them with tho se of idealized oracle outlined earlier. The results of this experiments over indexes with Fixed-7 partitioning, are shown in Figure 5, for queries with temporal predicates of 1 year and 1 week ranges. We can make the following observations from these plots: (i) The gap between a 5% KMV synopsis and 10% synopsis is negli-gibly small, prompting our choice of using 5% KMV synopsis. ( ii) Although oracle-based estimates are, as expected, better o verall, improvements over using KMV synopsis estimates are not sign if-icant. KMV synopsis are stored as arrays of doubles and much smaller than individual postings. Moreover the estimate co mputa-tion is fast and they can be compressed and kept in memory for computing the selection set efficiently.

These results also provide a first glimpse of the effectivene ss of the partition selection methods themselves  X  in the best cas e for NYT dataset, partition selection methods are able to answer with more than 80% recall when the I/O budget is as small as 20% of the RWOS.
We experimented with two different granularities of partit ion-ing  X  viz., 7-day and 30-day time-ranges, resulting in Fixed -7 and Fixed-30 index configurations. Fixed-7 has a higher number o f par-titions, thus can be seen as having smaller partition sizes i n com-parison to Fixed-30. Clearly, this allows efficient process ing of time-point or short duration queries. However, it deterior ates for larger time-range queries if no partition selection is made . On the other hand, performance with partition selection shown in F igure 6 for queries with 1-year time-window, shows that even for sma ller partitions sizes this issue can be effectively alleviated.
Research in Information Retrieval has recently paid attent ion to temporal information associated with documents. Alonso et al. [5] give an overview of relevant research directions. Closest t o the ideas presented here is the work on time-travel text search [ 8] that allows users to search only the part of a document collection that existed at a given time point. To support this functionality effi-ciently, posting lists from an inverted index are temporall y parti-tioned either according to a given space bound or required pe rfor-mance guarantee. Postings whose valid-time interval overl aps with multiple of the determined temporal partitions are judicio usly repli-cated and put into multiple posting lists, thus increasing t he overall size of the index.
Whereas the related research discussed thus far focuses on t ex-tual documents as one specific type of data, research in tempo -ral databases has taken a broader perspective and targeted g en-eral data that comes with attached temporal information. In dex structures tailored to such data like the Multi-Version B-T ree [7] or LHAM [13] are related to the present work, since they also, im plic-itly or explicitly, rely on a temporal partitioning and repl ication of data. It is therefore conceivable to apply our proposed tech niques in conjunction with one of these index structures. Join proc essing techniques for temporal databases [11] are a second class of related work whose focus, to the best of our knowledge, has been on pro -ducing accurate query results opposed to the approximate re sults that our techniques deliver.

As data volumes grow, many queries are increasingly expensi ve to evaluate accurately. However, an approximate but almost accu-rate answer that is delivered quickly is often good enough. A pprox-imate query processing techniques [3, 4] developed by the da tabase community aim at quickly determining an approximate answer and, to this end, typically leverage data statistics (often appr oximated using histograms), sampling, and other data synopses. In co ntrast to our scenario, approximate query processing techniques t arget scenarios with a well-designed relational schema that impl ies cer-tain reasonable queries (e.g., based on foreign keys). When cast into a relational schema, our scenario gives rise to million s of rela-tions (corresponding to terms and their corresponding part itions).
This work presented techniques to efficiently process time-travel queries with temporally partitioned inverted indexes. By c arefully selecting partitions of the index which contribute most to r ecall at any stage of the processing, our methods reduce the number of du-plicate reads of the same items. Our experimental results sh owed that recall levels of at least 80% can be achieved by reading o nly 40% of the index entries, significantly reducing query proce ssing time. This is particularly useful in text analytics with mul tiple rounds of query reformulations where fast retrieval of a rep resen-tative subset of results is needed. This work opens up intere sting questions for future research, e.g.: How to organize index s truc-tures so that only essential entries are read, thus improvin g effi-ciency? How to apply partition selection techniques to inde x struc-tures which have overlapping partitions? How to further imp rove query processing by encoding and skipping techniques?
