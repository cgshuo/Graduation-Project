 Topical classification of web queries has drawn recent interest because of the promise it offers in improving retrieval effectiveness and efficiency. However, much of this promise depends on whether classification is performed before or after the query is used to retrieve documents. We examine two previously unaddressed issues in query classification: pre versus post-retrieval classification effectiveness and the effect of training explicitly from classified queries versus bridging a classifier trained using a document taxonomy. Bridging classifiers map the categorie s of a document taxonomy onto those of a query classification problem to provide sufficient training data. We find that training classifiers explicitly from manually classified queries outperfo rms the bridged classifier by 48% in F1 score. Also, a pre-re trieval classifier using only the query terms performs merely 11% worse than the bridged classifier which requires sni ppets from retrieved documents. H.3.5 [ Information Storage and Retrieval ]: Online Information Services  X  Web-based services Measurement, Reliability, Experimentation Query Classification, Web Search Topical web query classification can be leveraged by search services to improve efficiency and effectiveness. But, whether this classification is available for use in the retrieval process, or only after, is a key concern. If it is to be used in query routing, for example, pre-retrieval cl assification is by definition required. Most text classi fication research focuses on classifying documents, whic h contain enough terms to adequately train machine learning approaches. The task of classifying web queries is different in that web queries are short, providing very few inherent features. Therefore, most approaches use the documents retr ieved by a query as features to classify it. The 2005 KDD Cup focused on the topical classification of web queries. The lack of substantial training data led many participants to tu rn to external sources to train their systems [2]. This typically consisted of training a document classifier using taxonomie s of web pages such as the Open Directory Project (http://www.dmoz.org), and then bridging the categories of that taxonomy onto those desired for the query classification. Many participants achieved satisfactory F1 scores, the ha rmonic mean of precision and recall, but did not go any further to analyze success and failure. We focus on two unaddressed questi ons: the effect of bridging a document classifier to the query classification problem, and the relative effectiveness of pre versus post-retrieval query classification techniques. We hypothesize that the concepts described by queries of a certa in class ( X  X ews X  queries, for example) do not necessarily correspond with those of documents classified into a category of the same name. We know, for example, that relevance feedback is effective because the language of queries and that of documents often differs. The KDD Cup dataset consisted of 800,000 web queries each to be classified into up to five of 67 possible topical categories. A training set of 111 classified que ries was provided, and three human assessors independently judged 800 randomly selected queries for the test set. Several runs made use of external information. Shen and colleague s used an ensemble of several bridged classification techniques to create the winning submission [3]. These incl uded mapping web taxonomies from Google X , Looksmart X , and a crawl of the ODP hierarchy to the 67 categories employed at the KDD Cup based on synonymy via WordNet (http://wordnet.princeton.edu) and submitting category names as queries to Google X . The pages in these taxonomies, their snippe ts (query-biased summaries), and titles were used as training da ta. Each test query was then processed to retrieve its snippets which are submitted to each classifier and their results are combined. This approach resulted in an F1 score of 0.44, not far from the mean F1 score of 0.50 when evaluating manual labelers against one-another. However, their baseline of pre-retrieval performance (using only the query terms without the snippets from Google) performed 40-50% worse in F1 than their bridged post-retrieval techniques. Also, they found that using only the sni ppets of documents in training consistently outperformed using their full text, which they attribute to the introduction of noise. We compare and combine query classifiers that can be applied before gathering the retrieved documents (pre-retrieval classifiers), a bridged document cl assifier trained from pages in the ODP (as used in the KDD cup), and explicit query classifiers trained on the retrieved documents of classified queries. We use the 20,000 queries manually classified into 18 general topical categories available in previous work by Beitzel, et al [1]. This provides us enough training data to effectively test our explicit classifiers, as compared to only the 111 training queries in the KDD dataset. We partitioned the queries into 1/3 training, 1/6 tuning, and 1/2 testing. For th e post-retrieval classifiers (all support vector machines) we used the training queries to build the model and the tuning queries to select the threshold at which we report F1 in testing. To trai n the explicit classifiers and test each of the post-retrieval classifiers, we processed each query with Google to obtain the top te n retrieved documents and their snippets. Each SVM classifier us es the default configuration of LIBSVM (http://www.csie.ntu.edu. tw/~cjlin/libsvm). The pre-retrieval classifier we evaluate is an ensemble of exact match, perceptron, and selectional preference classifiers described in Beitzel, et al [1]. These me thods leverage both labeled and unlabeled query logs for training, expanding on the training queries based on category phrase statistics. Since they are independently trained, they only require a tuning set to select the optimal threshold for this task. Therefore, we use the training and tuning sets combined to set this threshold for these methods. Classification uses only the query string itself. The bridged post-retrieval document cl assifier is an SVM trained using web pages in the ODP w ith their categories manually bridged to one of the 18 in the testing set by the authors. Although these documents were spread across thousands of very specific ODP categories, in most cases, one of their general parent categories corresponded reasonably to one of the 18 in the testing set. To isolate the effect of bridging document categories to query ones, we use the same retrieved documents as for the explicit classifier to train our document classifier, simply replacing their known query class with the bridged class from the ODP. Classification uses the snippets from the retrieved documents. Finally, our explicit post-retrieval query classifiers were trained on the 6,666 queries in our training set based on their manually assigned cat egories. We evaluate two variants of explicit classifier, one trained and tested using only the snippets of retrieved documen ts, and one trained and tested using both the snippets and full text of the top ten documents retrieved. To determine how these three categories of query classifiers compare to each other, we first examine the overall optimal performance for each classifier. Then, we combine the classifiers to try and exploit their differences for overall improved performance. The performance of each classifier over our 10,000 query testing set using the threshold of optimal F1 from the tuning set is detailed in Table 1 . Surprisingly, one can achieve nearly as effective performance from pre-retrieval classifiers that use only the query string itself for classification as that of the generic text cla ssifier which requires the retrieved documents. The post-retrieval classifiers learned explicitly from classified query logs improve upon this substantially, with a 48% relative improvement in F1. Clearly, performance is lost when treating query classification as a generic topical text classification problem by mapping document taxonomies to query ones. Like Shen, et al., we find that using only snippets outperforms including the full text of documents for classification [3]. Further analysis is warranted, but like them we hypothesize the full text introduces too much noise. Based on the results from the individual classifiers, we hypothesized that differences in classifi ers would provide for improved performance if they were combined. We fused them together, using the classifications from hi gher-precision classifiers first, and backing off to higher-recall classifiers when necessary. Despite their very different focus, however, this combination of pre-retrieval classifiers with th e best post-retrieval one (explicit using snippets) does not provide substantial improvement. With the additional information available post-retrieval, the imprecision of the pre-retrieval techniques prevents them from adding substantial value. By howev er slight margin, this fusion does represent the best post-retrie val performance we achieve. To examine the effectiveness of pre-versus post-retrieval classification in more detail, we show the overall precision/recall tradeoffs of the pre-and best post-retrieval (fused) classifiers in Figure 1 . The ability of retrieved document classifiers to achieve greater recall than the query-log-based, pre-retrieval, classifier s is expected due to the larger number of features available. 
Figure 1: Best Pre-and Post-Retrieval Precision and Recall We have evaluated three differi ng approaches to topical web query classification. We find that training explicitly from classified queries outperform s bridging a document taxonomy for training by as much as 48% in F1. We have also shown that pre-retrieval classification us ing only the query string can provide surprisingly effective results, enabling adjustments to the retrieval process to improve effectiveness and efficiency. However, our fusion of multiple approaches did not yield improved performance. In future work we will analyze the differences between these me thods and develop improved combination strategies. 1. Beitzel, S.M., Jensen, E.C ., Lewis, D.D., Chowdhury, A., 2. Li, Y., Zheng, Z. and Dai, H.K. KDD Cup-2005 Report: 3. Shen, D., Sun, J., Yang, Q. and Chen, Z., Building bridges 
