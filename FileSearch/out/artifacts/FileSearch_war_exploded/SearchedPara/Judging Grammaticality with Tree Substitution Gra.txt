 as for machine translation. As a result, the output of such text generation systems is often very poor grammatically, even if it is understandable.
Since grammaticality judgments are a matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the exten-sive work produced over the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Follow-ing Cherry and Quirk (2008), we conduct experi-ments in a classification setting, where the task is to distinguish between real text and  X  X seudo-negative X  text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature tem-plates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them. Since these fragments are in turn learned au-tomatically from a Treebank with a Bayesian model, their usefulness here suggests a greater potential for adapting to other languages and datasets. Tree substitution grammars (Joshi and Schabes, 1997) generalize context-free grammars by allow-ing nonterminals to rewrite as tree fragments of ar-bitrary size, instead of as only a sequence of one or tions with larger fragments, whereas ungrammatical sentences will be forced to resort to small fragments. This is the central idea explored in this paper.
This raises the question of what exactly the larger fragments are. A fundamental problem with TSGs is that they are hard to learn, since there is no annotated corpus of TSG derivations and the number of possi-ble derivations is exponential in the size of a tree. The most popular TSG approach has been Data-Oriented Parsing (Scha, 1990; Bod, 1993), which takes all fragments in the training data. The large size of such grammars (exponential in the size of the training data) forces either implicit representations (Goodman, 1996; Bansal and Klein, 2010)  X  which do not permit arbitrary probability distributions over the grammar fragments  X  or explicit approxima-tions to all fragments (Bod, 2001). A number of re-searchers have presented ways to address the learn-ing problems for explicitly represented TSGs (Zoll-mann and Sima X  X n, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these ap-proaches, work in Bayesian learning of TSGs pro-duces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). Our ex-periments make use of Bayesian-learned TSGs. We experiment with a binary classification task, de-fined as follows: given a sequence of words, deter-mine whether it is grammatical or not. We use two datasets: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), and the BLLIP  X 99 dataset, 1 a collection of automatically-parsed sen-tences from three years of articles from the Wall Street Journal.

For both datasets, positive examples are obtained from the leaves of the parse trees, retaining their to-kenization. Negative examples were produced from a trigram language model by randomly generating sentences of length no more than 100 so as to match the size of the positive data. The language model was built with SRILM (Stolcke, 2002) using inter-polated Kneser-Ney smoothing. The average sen-tence lengths for the positive and negative data were 23.9 and 24.7, respectively, for the Treebank data 4. The Charniak parser (Charniak, 2000), run in The parsing models for both datasets were built from sections 2 -21 of the WSJ portion of the Treebank. These models were used to score or parse the train-ing, development, and test data for the classifier. From the output, we extract the following feature sets used in the classifier.  X  Sentence length ( l ).  X  Model scores ( S ). Model log probabilities.  X  Rule features ( R ). These are counter features  X  Reranking features (C&amp;J). From the Char- X  Frontier size ( F 4.2 Results Table 2 contains the classification results. The first block of models all perform at chance. We exper-imented with SVM classifiers instead of maximum entropy, and the only real change across all the mod-els was for these first five models, which saw classi-fication rise to 55 to 60%.

On the BLLIP dataset, the C&amp;J feature sets per-form the best, even when the set of features is re-stricted to local ones. However, as shown in Table 3, this performance comes at a cost of using ten times as many features. The classifiers with TSG features outperform all the other models.

The (near)-perfect performance of the TSG mod-els on the Treebank is a result of the large number of features relative to the size of the training data: Table 4 lists the highest-weighted TSG features as-sociated with each outcome, taken from the BLLIP model in the last row of Table 2. The learned weights accord with the intuitions presented in Sec-tion 3. Ungrammatical sentences use smaller, ab-stract (unlexicalized) rules, whereas grammatical sentences use higher rank rules and are more lexical-ized. Looking at the fragments themselves, we see that sensible patterns such as balanced parenthetical expressions or verb predicate-argument structures are associated with grammaticality, while many of the ungrammatical fragments contain unbalanced quotations and unlikely configurations.

Table 5 contains the most probable depth-one rules for each outcome. The unary rules associated with ungrammatical sentences show some interest-ing patterns. For example, the rule NP  X  DT occurs 2,344 times in the training portion of the Treebank. Most of these occurrences are in subject settings over articles that aren X  X  required to modify a noun, such as that , some , this , and all . However, in the BLLIP n-gram data, this rule is used over the defi-nite article the 465 times  X  the second-most common use. Yet this rule occurs only nine times in the Tree-bank where the grammar was learned. The small fragment size, together with the coarseness of the nonterminal, permit the fragment to be used in dis-tributional settings where it should not be licensed. This suggests some complementarity between frag-ment learning and work in using nonterminal refine-ments (Johnson, 1998; Petrov et al., 2006). Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distin-guishing machine translation (MT) output from hu-man reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator be-tween grammatical text and n-gram samples. Mut-ton et al. (2007) also addressed this problem by com-bining scores from different parsers using an SVM and showed an improved metric of fluency. grammatical ungrammatical (WHNP CD) (NN UNK-CAPS) (NP JJ NNS) (S VP) (PRT RP) (S NP) (WHNP WP NN) (TOP FRAG) (SBAR WHNP S) (NP DT JJ) (WHNP WDT NN) (NP DT) used) is fruitful (Wong and Dras, 2010; Post, 2010). Parsers were designed to discriminate among struc-tures, whereas language models discriminate among strings. Small fragments, abstract rules, indepen-dence assumptions, and errors or peculiarities in the training corpus allow probable structures to be pro-duced over ungrammatical text when using models that were optimized for parser accuracy.

The experiments in this paper demonstrate the utility of tree-substitution grammars in discriminat-ing between grammatical and ungrammatical sen-tences. Features are derived from the identities of the fragments used in the derivations above a se-quence of words; particular fragments are associated with each outcome, and simple statistics computed over those fragments are also useful. The most com-plicated aspect of using TSGs is grammar learning, for which there are publicly available tools.
Looking forward, we believe there is significant potential for TSGs in more subtle discriminative tasks, for example, in discriminating between finer grained and more realistic grammatical errors (Fos-ter and Vogel, 2004; Wagner et al., 2009), or in dis-criminating among translation candidates in a ma-chine translation framework. In another line of po-tential work, it could prove useful to incorporate into the grammar learning procedure some knowledge of the sorts of fragments and features shown here to be helpful for discriminating grammatical and ungram-matical text.

