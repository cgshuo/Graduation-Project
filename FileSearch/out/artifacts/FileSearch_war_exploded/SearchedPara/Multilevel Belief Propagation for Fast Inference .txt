
Graph-based inference plays an important role in many mining and learning tasks. Among all the solvers for this problem, belief propagation (BP) provides a general and efficient way to derive approximate solutions. However, for large scale graphs the computational cost of BP is still de-manding. In this paper, we propose a multilevel algorithm to accelerate belief propagation on Markov Random Fields (MRF). First, we coarsen the original graph to get a smaller one. Then, BP is applied on the new graph to get a coarse result. Finally the coarse solution is efficiently refined back to derive the original solution. Unlike traditional multi-resolution approaches, our method features adaptive coars-ening and efficient refinement. The above process can be re-cursively applied to reduce the computational cost remark-ably. We theoretically justify the feasibility of our method on Gaussian MRFs, and empirically show that it is also ef-fectual on discrete MRFs. The effectiveness of our method is verified in experiments on various inference tasks.
The recent years have witnessed a surge of interests in graph-based inferences in the field of data mining and ma-chine learning. The basic setting of graph-based inference is: given a graph in which nodes represent random variables and edges represent the statistical dependencies between the nodes, we want to know the most probable configuration of the states of these variables provided some observations. Graph-based inference provides a general way to accom-plish many learning and mining tasks, such as classifica-tion, regression and clustering. Its usage arises in a wide variety of problems. For example, random walk on graphs are applied to do classification [25] and measure similar-ity ( e.g. [16]); Gaussian Markov random fields (Gaussian MRF) serves as the basis for several semi-supervised learn-ing algorithms ( [30, 18]); In computer vision, MRF is usu-ally used as the underlying model to facilitate image recov-ery [9] and segmentation ( e.g. [27]).

Useful as it is, acquisition of the exact solution to the general inference problem is notoriously hard. Therefore, several approaches have been developed to find approxi-mate solutions, such as Monte Carlo methods [9], varia-tional methods ( mean fields [10]), graph cut [3], and be-lief propagation , which is the focus of this paper. Belief propagation (BP) solves inference problems by passing lo-cal messages. It was originally proposed for inferences on singly connected Bayesian networks where exact solutions are guaranteed [13]. In spite of this restriction, people have directly applied it to other graphical models ( e.g. MRF and conditional random fields (CRF) ) and graphs with loops ( loopy BP ) and achieved many empirical successes [22]. Recently, several theoretical analysis ([21], [23], [12]) on the convergence and optimality of solutions of loopy BP have been made, and the reason of loopy BP X  X  effectiveness is gradually revealed.

However, although considered as an efficient inference engine, belief propagation is still often too slow to be prac-tical on large scale graphs, which is common in data mining and computer vision. To address this problem, we propose a multilevel strategy to accelerate belief propagation. Our ba-sic idea is intuitive: a large graph can be approximated by a small graph whose nodes are the aggregations of the nodes in the original graph. One source of this idea is the way people perceive the world. According to the Gestalt laws in psychology, we perceive objects as well-organized, integral patterns, thus the grouping of raw data is very important. Studies in human visual system also show that hierarchical processing seems to play a critical role in perception [7].
In our algorithm, a graph is first recursively coarsened to reduce the problem scale. Then we run BP on the new problem to obtain a coarse result. Finally, this re-sult is refined back level by level to get the solution of the original problem. Both the obtainment and refinement of the coarse result can be done rapidly, therefore the overall cost of inference is decreased. Unlike traditional wavelet-based multi-resolution approaches [24, 8], this algorithm is derived under the principle of adaptive aggregation and energy-preserving approximation . These advantages enable us to significantly reduce the running time of belief propa-gation without compromising the solutions X  quality. A rigid justification of our method is provided on Gaussian MRFs. We show that our method is closely related to the algebraic multigrid technique [17], which aims at solving large scale linear systems efficiently. Thus, we name it multigrid belief propagation (MGBP). Experiments show that our approach is both efficient and effective for inference tasks.
The rest of this paper is organized as follows. In section 2 we briefly introduces graph-based inferences and belief propagation. Then the algorithm details are described in section 3. We justify our method in section 4. In section 5 and section 6 we make discussions and introduce some related works. Experimental results are presented in section 7 and finally we make our conclusions.
Probabilities play a central role in modern machine learning and data mining. Many problems can be formu-lated as the inference of the states of random variables. Al-though these inferences can be done in a pure algebraic way, it is usually highly advantageous to use graphs as diagram-matic representations to facilitate analysis and manipula-tions. Concretely, we can represent a probabilistic model by a graph G =( V , E ) , where each vertex v i in V is as-sociated with a random variable x i and the edges in E rep-resent the statistical dependencies between these variables. These graph representations are called probabilistic graph-ical models [2]. In data mining, machine learning and other related fields, Markov random fields (MRF [5]) is a com-monly used graphical model. It is an undirected graphical model in which nodes represent variables and arcs represent compatibility relations between variables. We will focus on MRF in this paper.

Without much loss of generality, we only consider pair-wise MRFs, and assume that every node has an observation node attached. Denote the nodes as X =[ x 1 ,  X  X  X  ,x n ] T observations as Y =[ y 1 ,  X  X  X  ,y n ] T , and the compatibility functions as  X  ij (  X  ,  X  ) . According the Hammersley-Clifford theorem [13], the joint distribution can be written as where Z is called the partition function which ensures that the joint distribution is properly normalized. Usually, the compatibility  X  ij is parameterized by the edge weight w ij which measures the similarity between x i and x j .Iftwo nodes are similar, then their states should also be close to achieve a high compatibility in the network.

Figure 1. A lattice Markov random field for image modeling. Filled circles are obser-vations, while others are their underlying nodes. The arcs represent spatial compati-bility constraints between nodes.

The usage of MRF can be found in many problems in data mining and machine learning. For example, it can be used to describe the relations between different objects in probabilistic modeling ( e.g. [6]). When constructed upon data samples, it provides a discrete characterization of the data X  X  manifold structure i.e. prior distribution [1]. In com-puter vision, MRF is a natural model for the spatial con-straints between pixels. Figure 1 illustrates a lattice MRF model that is commonly used in image processing.
There are mainly two types of inferences on MRFs. The first one aims at achieving the maximum marginal probabil-ities on each node (MM assignment), i.e. , x i is determined by [22] Another one derives the maximum a posteriori probability on the graph (MAP assignment), i.e. , the configuration of X is determined by [22] These inferences tasks on general MRFs are difficult mainly due to the calculation of the partition function Z . There-fore, several algorithms are developed to provide feasible approximate solutions, one of which is belief propagation.
Belief Propagation (BP) [13] utilize the conditional inde-pendence properties in the network to derive efficient solu-tions. Corresponding to the MM and MAP inferences, there are two types of BP [22]. One is belief update (BU) aka the sum-product algorithm for MM inferences, and another is belief revision (BR) aka the max-product algorithm for MAP inferences. The BP algorithm can be summarized as: 1) nodes deliver their distribution information ( messages )
Figure 2. A diagrammatic representation of belief propagation. m ij is the message from x i to x j . (a) The calculation of the message from m ij . (b) The calculation of belief at x i . to others through (and affected by) edges; 2) the distribu-tion ( belief ) at a node is formed by combining messages it received. A detailed introduction on BP can be found in [22]. Here we show that the BU rules are m where m ij is the message from x i to x j , m ii is the message from y i to x i , and b i is the belief at x i .  X  and  X  are normal-ization constants and N ( x i ) \ x j means all the neighboring nodes of x i except x j . The BR rules can be obtained by replacing x sentation of BP is shown in figure 2.

The computational load of belief propagation is concen-trated on calculating the messages which has the complex-ity of O ( ek 2 T ) , where e is the number of edges, k is the number of possible states ( k =1 for Gaussian MRFs) and T is the number of iterations which usually equals to the graph X  X  diameter. It can be seen that belief propagation could be rather slow on densely connected graphs or graphs with large diameters. To decrease this cost, we could cut down the scale of the graph to reduce e and T , and further reduce T by providing a good start point for belief propaga-tion. This is the motivation of our multilevel approach.
In this section we introduce our multigrid belief propa-gation algorithm. We denote graphs as G , node sets as V , the cardinality of V as |V| = n , edge sets as E , and beliefs as B T =[ b 1 ,b 2 ,  X  X  X  ,b n ] where b i is the belief at node x And Superscripts are used to indicate the coarse levels. This algorithm first recursively coarsens the original graph G 0 get smaller ones G l . Then it solves the problem on G l to derive coarse beliefs. Finally, it uses the coarse beliefs to initialize the belief propagation on the original graph G 0 Algorithm 1: Multigrid Belief Propagation (MGBP) Input: levels K .

Coarsening: Initial Solution:
Refining: Output: that convergence can be achieved more rapidly. The above procedure is summarized in Algorithm 1.

There are two key points in Algorithm 1. First, the se-lected coarse nodes should be as representative as possible. We use an algebraic multigrid (AMG) [17] like coarsening method to achieve this goal. Second, given the results on G l +1 , belief propagation on G l should be efficiently initial-ized to a start point that is close to the true solution. This is done by carefully constructing the edges E l +1 for G l +1 initializing the belief propagation on G l using interpolation. In the rest of this section, we will describe the details of this algorithm. Here we assume that G 0  X  X  edge weight ma-trix W 0 is given, whose (i,j)-th entry w ij is the edge weight between node x i and x j 1 .
In this section, we show how to select a representative node set V l +1 from V l . We need to split V l into coarse node set V l +1 and fine node set F l subject to V l +1  X  X  l = V and V l +1  X  X  l =  X  . Coarse nodes in V l +1 form the vertices in G l +1 , while fine nodes will not present in the new graph.
To ensure that the coarse nodes in V l +1 are indeed repre-sentative, the fine nodes in F l should be  X  X lose related X  to the coarse nodes. Hence, we constrain V l +1 and F l to sat-isfy the condition that each fine node is strongly influenced by coarse nodes, where  X  X trongly influence X  means: Definition 1 (strongly influence) A node x i strongly influ-ences node x j if
Figure 3. The coarsening process. Filled and hollow circles belong to different classes re-spectively. Darker line means stronger con-nection. G 0 is the original graph and G 1 is the coarsened graph. where  X   X  (0 , 1) is a control parameter, and w ij is edge weight between x i and x j .

The above settings are commonly used in fast, multi-scale algebraic multigrid techniques [17]. Briefly, we itera-tively select nodes that strongly influence others into V and put the influenced ones into F l . This process can be completed in linear time w.r.t. the number of nodes [15]. The value of  X  in (6) is usually chosen within [0 . 25 , 0 . 75] . If  X  is large, then the quality of coarsening will be higher but more nodes will be selected into V l +1 . Typically, about less than half of the nodes in V l are selected into V l +1 Therefore, the scale of the graph drops exponentially after recursive coarsening. A detailed description of this proce-dure can be found in [17]. An illustration of this coarsening process is shown in figure 3.
Before we go further and calculate the new edge weights, we need to establish a rule to relate the nodes from dif-ferent levels to form a hierarchical structure. In this paper we choose the linear interpolation strategy, i.e. , beliefs for nodes in V l can be estimated from those of V l +1 by where k traverses all the coarse nodes in V l . P l +1 is the n l  X  n l +1 interpolation matrix and p l +1 discrete MRFs, Eq.(7) interpolates the belief vectors. For Gaussian MRFs it is the interpolation of mean values. The value of P l +1 is calculated by the following rules. For nodes in V l +1 their beliefs are kept unchanged after be-ing interpolated. For nodes in F l , their beliefs are interpo-lated from their neighboring coarse nodes, that is,
Figure 4. Calculation of the edge weight be-tween blocks. Nodes in the same box forms a block. Filled circles are the selected coarse nodes (block representatives). The block connection is the aggregation of nodes con-nections weighted by their interpolate contri-butions. which means that
Having the new node set V l +1 and the interpolation based hierarchy, we still need a new edge set E l +1 to con-struct G l +1 . Here we follow the iterated weighted aggrega-tion (IWA) process to calculate the new edge weights. More concretely, for G l +1 its edges weight matrix W l +1  X  X  (k,l)-th entry is computed by where p l +1 ij is the (i,j)-th entry of P l +1 . (10) can be written in a more compact form as The meaning of (10) is clear: the connection between blocks is the aggregation of nodes connections weighted by their contributions in interpolation, as shown in figure 4.
Another issue needs to be addressed is the impact of coarsening on the observations. After coarsening, some of the observation nodes are dropped, so the overall evidence on the coarse graph is  X  X eakened X . Assuming that evi-dences are uniformly provided on each observation node, we balance the drop of total evidence by strengthen the re-maining evidences with the rate where O l is the observed nodes in V l , and |O l | is its cardi-nality. In practice, this balance is achieved alternatively by weakening the edge weights by 1 /r . For semi-supervised learning tasks, we often force that all the labeled nodes are selected into higher level graphs so the evidences need not to be adjusted.
As stated before, to solve the whole problem we first need to seek for an initial solution as the  X  X eed X , and then refine it recursively to get the solution on the original graph. By recursively coarsening the original graph, we obtain at level K a coarse graph G K . We run BP on G K to derive the beliefs B K for nodes in V K . Since G K is small, this BP X  X  convergence can be achieved rapidly. Then, we refine the initial solution B K back level by level to the original graph .
 The refinement from B l +1 to B l is a 3-step process. First, we calculate estimated beliefs  X  B l based on B l +1 established in section 3.2, this is done by interpolating B using Eq.(7), which can be completed very fast. We claim that this estimation is close to the true solution, which will be justified on Gaussian MRF in section 4.

Then, we use the estimated beliefs  X  B l to set the start point for the belief propagation on G l . This is done by ini-tialize the messages on G l according to  X  B l . Considering the calculation of messages in (4) and beliefs in (5), we propose to initialize the messages using (4) with its terms m we find this approximation works fine.

Finally, we run BP on G l using the above initial mes-sages. Since its start point is close to the true solution, this BP is expected to converge rapidly. The above steps are applied recursively until the beliefs on G 0 is obtained.
In the previous section, we have described our multi-grid belief propagation algorithm in detail. In this section, we show that on Gaussian MRFs this algorithm can pre-serve the original objective function and solves the infer-ence problem in a algebraic multigrid way. Therefore, its performance is guaranteed.
We first introduce some background. Gaussian MRF is a type of MRF whose joint distribution is Gaussian [23] i.e. where M =[  X  1 ,  X  X  X  , X  n ] T is the mean value of X , Z is the partition function , and G is the free energy [26]
G = where C ij is the 2  X  2 potential matrix between x i and x On Gaussian MRFs, MM assignment and MAP assignment are achieved by the minimization of G .
 A most commonly applied energy function for Gaussian MRF is the quadratic form: where w ij is the edge weight. Usually, w ij indicates the similarity between x i and x j . Thus, this energy func-tion will force that 1) closely connected nodes have similar states and 2) nodes X  states are close to their observations. The first term can be considered as the smoothness con-straint and the second one is the fitting quality with  X &gt; 0 being the control coefficient. Eq.(15) can be vectorized as where L = D  X  W is the combinatorial Laplacian [1] on the graph and D is a diagonal matrix with its i-th diagonal entry d ii = j w ij . The above settings can be found in various applications such as semi-supervised learning [30] and computer vision [8]. In the rest of this section, we will focus on this form of free energy. It is proved that upon convergence the assignments obtained by Gaussian BP are exact on networks with arbitrary topology [23].
Now we show that the way we coarsen graphs in sec-tion 3 actually preserves the free energy of Gaussian MRFs within the framework of algebraic multigrid (AMG) coars-ening and interpolation-based refinement.

After the AMG coarsening, we obtain the coarse node set V l +1 which is believed to be representative of V l . Then the energy of G l +1 with assignment M l +1 is where w l +1 kl is the edge weight on graph G l +1 and  X  l +1 the new control coefficient. And the energy of G l using the interpolated assignment  X  M l = P l +1 M l +1 is Ideally, parameters in G l +1 should satisfy the condition G l +1 ( M l +1 )= G l ( M l +1 ) so that the quality of the coarse solution M l +1 in G l +1 will reflect the quality of its inter-polated solution  X  M l in G l . To achieve this, we consider the first and the second terms separately.

For the first terms ( smoothness ), we force them to be identical. It can be proved that this constraint will lead to the following edge weights on G l +1 : It can be shown that the IWA weights from Eq.(10) usually provides a good approximation to Eq.(19) according to [15] and our own experiments. Therefore, Eq.(10) can approx-imately keep the smoothness term in the energy function unchanged. Although Eq.(10) can be replaced by Eq.(19), we still prefer the IWA weights because it is more intuitive.
For the second terms ( fitting quality ), we preserve it by adjusting  X  l +1 . Assuming that after interpolation the fitting errors are uniformly distributed on all the nodes that have observations, the value of total fitting error k (  X  k  X  y proportional to the number of terms in the sum. Therefore, we can adjust the value of  X  l +1 to compensate the drop of total error using where O l is the observed nodes in V l , as in Eq.(12). This compensation coincides with our balance measure taken in section 3.3. Thus, both the smoothness and fitting quality of G l is preserved.

To sum up, the multigrid belief propagation algorithm is able to preserve the energy function of Gaussian MRFs between levels, therefore a low-energy assignment of G l +1 will also yield a good solution  X  M l in G l . Therefore, we can be confident that the interpolated solution  X  M l is close to the true solution M l .
Our approach on Gaussian MRF is closely related to the algebraic multigrid (AMG) [17], which aims at solving large scale linear systems efficiently. Considering minimiz-ing the energy (16), let  X  X / X  M =0 we get Supposing that the compensation (20) is accurate, it is easy to verify that after coarsening and interpolation, Eq.(21) be-comes If we consider graph G 0 as an algebraic grid [17], then Eq.(21) is a linear system on this grid. Further if we use the AMG technique to seek an efficient solution of Eq.(21) based on the Galerkin principal [17], then the equation sys-tem Eq.(22) is obtained. So our method actually reduces the problem scale in the same way as AMG does. Therefore, we call our method Multigrid Belief Propagation (MGBP).
Recall that the cost of BP is O ( ek 2 T ) . Our multilevel al-gorithm can significantly accelerate BP from three aspects. First, as coarsened level by level, the scale of the graph drops exponentially, so the number iteration T needed for convergence is reduced. Second, T is further decreased by setting BPs to good start points. The situation for the number of edges e is a bit complicated. On densely con-nected graphs, the reduction of graph scale will also reduce e . However, on some sparsely connected graphs e may in-crease after coarsening, and during the recursive coarsen-ing process e may first rise and then drop. In this case, the coarse level of MGBP should be selected to achieve a bal-ance between speed and accuracy.

We have shown that on Gaussian MRFs MGBP actually accelerates BP in an AMG way. Their key difference is that, AMG only solves Eq.(22) to derive the solution, whose quality depends on how well the coarse node set V 1 can represent V 0 . Sometimes this solution is overly smooth. On the other hand, MGBP only use this solution as a start point to accelerate belief propagation. Thus the solution obtained by MGBP is accurate upon convergence. And we can also make a trade-off between quality and speed by controlling the number of BP iterations. If we refine the beliefs using only interpolation, then MGBP is equivalent to the AMG solver on Gaussian MRFs, which is the essence of [20].
The implementation of MGBP on Gaussian MRFs is straightforward. When implementing it on discrete MRFs, the difference is that instead of manipulating the scalar mean values, we handle the belief vectors, which can also be done by matrix multiplication (For a detailed introduc-tion about discrete BP, readers are referred to [22, 8]). We are not able to justify the validity of MGBP on discrete MRFs as on Gaussian MRFs. Yet, we believe that MGBP has a reasonable motivation, and in practice its performance is found promising in various inference tasks, as shown in section 7.
Multilevel technique has long been applied to acceler-ate learning on graphs [24]. Usually, pyramidally orga-nized trees are constructed to develop efficient algorithms. MGBP also follows this paradigm. However, traditional approaches often coarsen a graph by wavelets ( e.g. Haar Figure 5. Coarsening using Haar wavelet.
 New nodes are created to represent blocks in
G 0 . The coarsening compromises the bound-ary. wavelet) as shown in figure 5. These methods can only be applied to regular graphs such as lattices. It ignores the characteristics of graphs and do uniform coarsening, which often compromises graphs X  details. Most importantly, we do not really know the relation between coarse results and the true solutions. On the other hand, MGBP can be ap-plied to graphs with various structures. It coarsens graphs adaptively by aggregating closely connected nodes together. Finally, in MGBP the coarse graphs are designed to approx-imate the original graph so less refining operations are re-quired to derive the true solutions. [8] proposed a multi-scale strategy to run belief prop-agation efficiently on images. The framework is similar: first coarsen and then refine. They coarsen the graphs us-ing Haar wavelet. After running BP on the coarse graph G l +1 , they use the messages in G l +1 to initialize the mes-sages on G l so the convergence can be accelerated. This ap-proach is quite similar to MGBP. However, their method can only be applied to lattice MRFs such as images. Besides, they ignore the characteristics of each image and do uni-form coarsening. On the contrary, by adaptive coarsening MGBP can better approximate the original graphs so fewer BP iterations are needed for refinement in MGBP than in their method.

The AMG technique have also been used in other prob-lems. For image segmentation, [15] uses a quadratic en-ergy function to indicate the quality of solution, and then recursively coarsen the image lattices to form a small graph where salient segments are easy to detect. During coars-ening the energy is approximately retained so the quality of coarse result is guaranteed. Moreover, they propose to modify the coarse graphs X  edges according to block-wise similarity, which is based on features that is not available at the pixel level. This approach may help improve the quality of the energy function, and can also be applied in MGBP for similar tasks.
In this section, we test MGBP on inference problems including graph-based semi-supervised learning and low-level vision problems. An important issue to be addressed before applying MGBP is the construction of the original graph G 0 . The topology of the graphs are determined by specific problems. Usually, we use the k NN graphs where two nodes are connected iff one of them is in the other X  X  k nearest neighbors. to model data distributions, and use lat-tices to model images. The edge weight can be calculated by several approaches such as the heatkernel and LLE reconstruction [14, 19]. Generally, this problem is not fully solved yet. A comprehensive discussion on constructing the edges can be found in [29]. In our experiments, we choose to use the heat kernel , i.e. , the edge weight is calculated by where d ( x i ,x j ) is the distance between x i and x j and T is the temperature parameter. Details will be described in each experiment.

Usually in the refining stage of MGBP, a very small num-ber of iterations is sufficient for the convergence of initial-ized BP. Thus, a good way to use MGBP is to first run BP on the coarsest graph until convergence (this will not take long since the graph scale is small), and then run BP on the rest of levels for 2 or 3 iterations. We use this strategy to run MGBP in this section unless indicated otherwise. For ordinary BP, we run it to the convergence.

In implementation, Gaussian BP and MGBP are written ments are run on the same computer. For MGBP, we report the total running time including coarsening and interpola-tion.
Semi-supervised learning (SSL) [4] receives lots of at-tention in machine learning and data mining recently. It aims at solving problems where a large number of samples are available but only a few of them are labeled. Among ex-isting approaches, graph-based transduction [29] is a most active one. Here we use belief propagation to solve this problem. We use the data samples to form an MRF and in-fer the class labels of unlabeled samples given the labeled ones. As a common setting, k NN graphs are constructed to model the relations between samples. When the edges are determined, their weights are calculated using heat kernel.
The Gaussian MGBP is applied to solve binary clas-sification problems. We use the  X  X oft label X  assumption [30], in which the mean value of each node is an indica-tor  X   X  [  X  1 , +1] which implies the significance whether a sample is positive or negative. Initially, the labeled positive samples have  X  =+1 and the negative ones have  X  =  X  1 . After inference, an unlabeled sample is classified as positive if it has  X &gt; 0 , or otherwise it is negative. This is a common assumption in semi-supervised classification ( e.g. [28, 30]). Further, discrete MGBP is applied to solve multi-class prob-lems. In this case the modeling is more straightforward: the state of a node is the class it belongs to. After inference, un-labeled samples are assigned to their most probable classes. The potential function for discrete MGBP is the Potts model [3].

The nearest neighbor (NN) classifier is used as the base-line. We compare MGBP to ordinary BP and two popu-lar graph-based methods in SSL including learning with local and global consistency (LLGC) [28], and harmonic Gaussian field (HGF) [30] (HGF is compared only in bi-nary cases). In each run, a small number of samples from each class are randomly selected and labeled, and the same graph are used for all the methods. Mean performance of 50 independent tests are reported. 7.1.1 Digits Recognition In this experiment, we study the performance of MGBP in digits recognition tasks. The data we use here is USPS 2 , which is a set of hand-written digit images with size 16  X  and pixel values ranging from 0 to 255. We use digits  X 1 X ,  X 2 X  to test two-class classification, and  X 1 X ,  X 2 X ,  X 3 X ,  X 4 X  to test multi-class performance. Sample numbers for the above digits are 1269, 929, 824, 852 respectively. The graph is constructed using 5 nearest neighbors. For the heat kernel, we use the Euclidean distance for d ( x i ,x j ) and the value of T is 380 tuned by cross validation. For MGBP, the graph is coarsened to level 5.

The results are shown in figure 6. In the two-class prob-lem Gaussian MGBP has a similar performance with BP and other methods. In the 4-class problem, discrete MGBP achieves an impressive accuracy. The fact that MGBP may outperform BP is probably due to the noise-reduction effect of coarsening. Figure 6 (c) shows the running time and ac-curacy of discrete MGBP using different coarse levels in the above 4-class problem. The accuracy is stable during coars-ening until the level goes too high. The rise of running time at level 3 is caused by the increase of edges, as analyzed in section 5. In this case, we can see that coarsening the graph to level 5 is a good choice. 7.1.2 Text Classification In this experiment, we test MGBP on a text classification task. The data set we adopt is a subset of 20-newsgroup 3 We choose the rec topic which contains autos , motorcycles , baseball , and hockey . The Rainbow package [11] is used to process the documents with options: passing all words through the Porter stemmer before counting them; tossing out any token that is on the stoplist of the SMART system; skipping any headers; ignoring words that occur in fewer than 6 documents. Then we normalized the samples to the TFIDF representation. Finally, 3970 8014-dimensional samples are obtained. We solve this 4-class problem by dis-crete MGBP.

The graph is constructed using 10 nearest neighbors. For the heat kernel, the value of T is set to 1. For MGBP, we coarsen the problem to level 3. Figure 7 shows the accu-racies of different methods. We can see that MGBP also achieves satisfactory accuracy in this task. Yet, the accel-eration is here not obvious because the original graph X  X  di-ameter is already small, and the coarsening increases the number of edges in it.
Figure 7. 4 class classification result on the 20-newsgroup data. Discrete MGBP is used in this task.
MGBP can also be used to solve the MRF-based image segmentation problem. In this experiment, first a few pixels in an image are labeled by the user to indicate the classes (segments) they belong to, and then the algorithm classi-fies the rest of pixels and forms the complete segmentation. Applications of this procedure can be found in fields such as medical image analysis.

We use Gaussian MGBP for two-way segmentations and discrete MGBP for the multi-way cases. Following the tra-ditional setting of computer vision, we use lattice graphs to model the spatial relations between pixels. For the heat kernel, we use the Euclidean RGB-color distance, and T is roughly tuned for each image. All the images are re-scaled to size 200  X  200 and then coarsened to level 5. To demon-strate the accuracy of MGBP X  X  energy-preserving coarsen-ing, we refine the coarse result back using only interpola-tion.
 els are displayed as dots whose colors indicate class labels.
Typically, segmentations using MGBP are done in less than 2 seconds, while it takes around 2 minutes for ordinary BP to finish the same tasks. We can see that MGBP achieves satisfactory results using much less time. 7.3 Image Restoration
We also apply MGBP to the image restoration problem [9]. Given a degraded image, by restoration we want to re-covery the original image i.e. infer the most probable color of the pixels given the degraded observation.
 The original image ( 179  X  122 ) is degraded by additive
Gaussian noise, as shown in figure 9 (a) and (b). We model this image as a lattice Gaussian MRF, where a node X  X  value indicates its true color and the degraded color serves as the observations. Pixel values are normalized between 0 and 1. For heat kernel, we use color difference as the distance and set T =10 manually. These parameters are kept the same when we run ordinary BP and MGBP respectively.
For MGBP, we coarsen the image to level 3, and run only one iteration of BP on each level. Ordinary BP is run until similar results are obtained.
 The results are shown in figure 9. It can be seen that
MGBP reduces the computational cost significantly while achieving a good approximation to the original restoration.
In this paper, we propose the multigrid belief propaga-tion (MGBP) algorithm to do approximate inferences on large graphs efficiently. Our basic strategy is to first solve a small, approximate problem, and then refine the coarse solution back to the original problem. The acceleration is achieved in two ways. First, we significantly reduce the scale of the problem by recursively coarsening the graph.
Second, we initialize the belief propagation to a good start point so that the iteration will converge rapidly. Using the algebraic multigrid techniques and energy-preserving coarsening , our method is able to construct small graphs that well approximate the original graph. Consequently, the
Figure 9. Image restoration from Gaussian noise. (a) Original image. (b) Degraded im-age. (c) Restored by Gaussian BP (7 sec, 13 iterations). (d) Restored by Gaussian MGBP (0.9 sec, 3 coarse level). coarse result can be refined very efficiently. We provide a justification for MGBP on Gaussian MRFs and verify em-pirically that it is also effective on discrete MRFs. Experi-ments show that our method can remarkably reduce the run-ning time of belief propagation while preserving the quality of solutions.
 Acknowledgment Funded by Basic Research Foundation of Tsinghua Na-tional Laboratory for Information Science and Technology (TNList).

