 1. Introduction Internet resources are typically specified using the string representation of  X  X niform Re-source Locators X  (URLs). URLs are a subset of the Uniform Resource Identifiers (URIs) billboards, and publications, and in user-maintained collections such as bookmarks and visited site history files.

The dynamic nature of the web X  X hankhunthod et al. (1996) report the average lifetime of an HTML text object to be 75 days X  X esults in URLs that quickly decay and become inaccessible (Pitkow 1999, Ashman 2000). According to our earlier work (2003) around 27% of the URLs referenced in IEEE Computer and the Communications of the ACM articles from 1995 X 2000 were no longer accessible at the end of the period. In addition, after four years almost 50% of the referenced URLs are inaccessible. Lawrence et al. (2001) have identified similar trends for published URLs. A number of solutions have been proposed for been identified (Ashman 2000) include prohibiting change, maintaining document versions, 6 SPINELLIS to document locations, implementing forwarding mechanisms, automatically detecting and correcting broken links, and creating all links dynamically. For links to published papers hypertext links across time.

Although URNs, PURLs and the Handle mechanism may offer a long-term solution, they have up to now not been universally adopted. Thus, individual user bookmarks and publicly distributed URLs quickly become obsolete as documents change names, directories, or are hosted on different places. Although a search engine (Lawrence and Giles 1999, Takeda 2000) can be used to relocate a document after a web server X  X   X 404 X  X ocument not found X  response, this is a tedious and error-prone procedure. In this paper we describe a way to automate the searching task by providing a more persistent alternative to a URL. Such an alternative can be used both to provide page bookmarks that are relatively immune to URL changes and as a centralized, alternative method for creating URNs without the active cooperation of the content creators.

Our scheme involves having a search engine calculate for every URL an augmented, per-sistent version. The augmented URL, containing the original URL and words that uniquely contents have changed location. Users who save the persistent URL to bookmark a page can ing for documents containing the words embedded in the URL. In addition, web sites can ability that these links will become unavailable when the respective page contents change location. As an example, given the URL http://moving.org/target whose contents could be uniquely identified by the words gloxinia and obelisk the corresponding persistent URL + obelisk (to make the example clearer we have not URL-encoded the original URL). When the user tries to access the above URL the search engine infrastructure at resolve.com will (up to date) index for a document containing the words gloxinia and obelisk .If one of the location, otherwise a  X 404 Not found X  error will be returned.

The remainder of this paper outlines the current process of document retrieval and the discriminants (Section 3), and sketches a prototype implementation of the concept (Section 4). In Section 5 we discuss the method X  X  performance in terms of retrieval accuracy, time e xtensions and applications of our technique. 2. Web document retrieval In general, URLs consist of a scheme (e.g. http, ftp, mailto) followed by a colon and a INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 7 Internet host use the following common syntax: The double slash indicates that the scheme data complies with the Internet scheme syntax. The default port for the HTTP scheme is 80 and is usually omitted.

Fo ra web page of a given URL to appear on a browser X  X  screen, a number of differ-ent technologies and protocols must work in concert. In addition, the changing realities of the web and the Internet have vastly complicated the simple end-to-end request-reply the complicated chain of actions needed to retrieve a web page will lead to a failed URL reference.

The path appearing in a URL will nowadays not necessarily match with a correspond-ing namespaces. Some of them are: the creation of a separate namespace for every local user, the definition of protection domains and access mechanisms, the support of aliases to map namespaces to local directories, and the dynamic creation of content using tech-nologies such the common gateway interface (CGI), servlets, and server-modified pages tion allows a server to provide different pages based on technical or cultural character-istics of the incoming request (e.g. bandwidth, display technology, languages the user can understand).
 The HTTP protocol defines 24 different errors that can occur within an HTTP exchange. In practice, while verifying thousands of published URLs we encountered the following errors: 400 Bad request : The syntax used for the request could not be understood by the server.
This may signify a badly formed URL often coupled with a browser bug. when citations are given to URLs that exist within a domain of services that require model. proper authorization can not be used to retrieve the page. It is conceivable that URLs that are not part of the public Internet end up as citations when the authors fail to re-alize that they have special privileges to access certain repositories that do not apply to the global Internet population. As an example, our organization has transparent access to a collection of on-line journals with authentication based on the client IP address.
URLs to this collection provided by unsuspecting users will typically generate a 403 error. 404 Not found : This infamous and quite common response signifies that the server has not found anything matching the Request-URI. This error is typically generated when web 8 SPINELLIS site maintainers change file names that are part of the given URL path or entirely remove the referenced material. Note that this protocol error can be followed by customized content X  X ypically HTML text that informs the user of the problem and provides alter-native navigation options. 500 Internal server error : The server encountered an unexpected condition which prevented it from fulfilling the request. This error can occur when a server is wrongly configured, f ails. rary overloading or maintenance of the server. Errors of this type sometimes appear on a misconfigured server, or servers overwhelmed by traffic. 504 Gateway time-out : The server, acting as a proxy or gateway did not receive a timely other auxiliary server (e.g. domain name server X  X NS) it needed to access in attempt-ing to complete the request. When HTTP requests are transparently intercepted by errors. 901 Host lookup failure : The host name could not be mapped to an IP address. This error (which is not part of the HTTP protocol) signifies a problem in retrieving the IP address of the server using the DNS services. Likely causes include changes of host names, and DNS server failures or connectivity problems.

Not all of the above problems can be solved by the provision of persistent URLs. A persistent URL will help in cases where the original document has changed its name (in-cluding the path to its name) resulting in a  X 404 Not found X  error, and in cases where the domain hosting the URL is renamed resulting in a  X 901 Host lookup failure X  error. Changes in a document X  X  access authorization are not a technical but a legal problem, while the 500-class server errors are more appropriately handled by a robust network infrastruc-ture and mechanisms such as proxies, mirrors, archives, and content delivery networks. In addition, persistent URLs can not deal with documents that are deleted or modified, un-less the corresponding URLs are designed to work in concert with an appropriate archive repository. 3. Unique document discriminants query. Consider three documents and their respective word contents 1: ABCF ,2: ABDF , and 3: AKCDF . The minimal unique identifiers (discriminants) for these documents are for document 1: A  X  B  X  C , for document 2: D  X  B , and for document 3: K .I n cal-culating the minimal discriminants we also take into account the length of each word to minimize the length of the respective search engine query. Given such a discriminant a search engine query with that discriminant, will result in a single matching element: the identified document, irrespective of the document X  X  location (URL). As an example, our INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 9 application calculated that the words sedam protectable currently uniquely identify the web page http://research.unc.edu/otd/inventors/overview.html. Thus a search engine query for the above terms will result in a single result: the corresponding document. We theorize that for large collections and a regularly updated search engine index, the discriminants will continue to uniquely and correctly identify the document, even as new documents are added to the collection. The form submission mechanism of many search engines allows one to create a URL that will automatically perform the above search and return the corre-sponding result; as an example the Google search engine URL for the page we described w ould be http://www.google.com/search?&amp;q = sedam + protectable. Furthermore, a simple operation to be transparently performed.

The computing infrastructure of a large search engine is ideally placed to efficiently perform both the calculation of the persistent URLs and their resolution. These two can then be provided as an extra service to the engine X  X  users. In addition, the search engine a bookmarked persistent URL or follows such a URL on the web.

The persistent URLs will most likely be less easy to remember than the URLs they are derived from. However, these URLs will be primarily stored as bookmarks and hyperlinks and automatically processed, rather than memorized and communicated by humans. For identify the document are suitable for this purpose.
 The idea of locating documents by words those documents contain is not novel. Phelps and W ilensky (2000) proposed the construction of ro b ust hyperlinks by means of the similarly document frequency X  X DF), while also favoring terms with a high term frequency (TF) within the document, capping TF at 5 to avoid diluting a term X  X  rarity. The IDF of each term is derived from a search engine, while the rest of the signature calculation can be performed locally. Park et al. (2002) expanded on this idea by evaluating four basic and four hybrid lexical signature selection methods based on the TF, document frequency X  X F, and IDF of those terms. For example, one of their proposed methods TFIDF3DF2 involves selecting two words based on increasing DF order, filtering out words having a DF value of one, and selecting three additional words maximizing TFIDF. An important contribution of their work is an evaluation of the documents that the search engine returns in response to a lexical signature query in terms of uniqueness, appearance of the desired document at the top of the result list, and relevance of any other document links returned.
Our approach differs from the two methods we outlined in that it uses the search engine as allows us, instead of having a fixed algorithm (such TFIDF3DF2) identifying a document X  X  discriminants, to flexibly select from each document the discriminant that maximizes an objective function. Although the objective function we used is based on uniqueness and URL length, different functions such as the relevance of the returned documents could also be used. 10 SPINELLIS 3.1. Discriminant calculation T rying all document X  X  term combinations to find a unique discriminant is a futilely expen-sive exercise. The number of n different terms that can be selected from a document is given by combinations for each document. Although selecting fewer terms (in practice we found that unique discriminants consisted on average of 1.47 terms) lowers the above figure, the complexity X  X  exponential nature makes the exhaustive search prohibitively expensive on (out of the total 10 30 possible ones).

An efficient deterministic solution to the problem would be preferable to the exhaustive problem is intractable, NP-complete. 3.2. Intractability proof complete by demonstrating that a tractable P-time solution to the problem could be used to solve the subset sum problem, known to be NP-complete (Garey and Johnson 1979). We can formally express our problem as follows: Let D ={ T 1 ,..., T n } (our document) be a be a document X  X  discriminant. The number of documents k that the discriminant D with m =| D | identifies is expressed as the cardinality of the intersection of the corresponding sets: A unique discriminant is one for which k = 1.

Similarly, the subset sum problem can be expressed as follows: Let A ={ a 1 ,..., a n } be a set of positive integers. Given an integer s find a set A  X  A with m =| A | so that
If the n elements of the set A have values 0 ... m we can solve the subset sum problem in terms of the discriminant cardinality problem by using set intersection in the place of INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 11 addition. For each element a i  X  A we construct a set and let B ={ B 1 ... B i } . The sets we constructed have the property that given a set of positive integers Q ={ q | q  X  X  1 ... n }} Using our hypothetical discriminant cardinality P-time algorithm we find a B  X  B such that The a i elements that correspond to the B subset will satisfy the subset sum problem Having shown that the subset sum problem X  X nown to be NP-complete X  X an be reduced to the discriminant cardinality problem we have proved that the discriminant cardinality problem is also NP-complete. 3.3. Genetic algorithm algorithms (GAs) (Holland 1975, Goldberg 1989, Forrest 1996) are global optimization techniques that avoid many of the shortcomings exhibited by local search techniques on difficult search spaces, such as our unique discriminant selection problem. Goldberg (1994) describes a number of diverse GA applications, while Karr (1993) presents their use for modeling, design, and process control. GAs rely on modeling the problem as a population of organisms. Every organism represents a possible valid solution to the problem. Organisms are composed of alleles representing parts of a given solution. Standard genetic recombina-the existing organisms. In addition, mutations can randomly change the composition of ex-creates new organisms by combining existing ones based on their fitness. This procedure is 12 SPINELLIS repeated until the variance of the population reaches a predefined minimum value or another heuristic criterion is satisfied.
 N
D a particular discriminant x identifies and, less, on its length L as determined by the number of words N W and the length of each word L i : As an exception to the above function definition, organisms that fail to identify a single document ( N D ( X ) = 0) are given a fitness rank of 0.

An important characteristic of a genetic algorithm X  X  implementation concerns the repre-sentation of each candidate solution. A good representation should ensure that the appli-cation of standard crossover recombination operators (where a new organism is composed implementation we used was an ordered set of terms. Thus, for a document containing the w ords [ ABCDEF ]t wo organisms could be [ ACF ] and [ BE ]. Following experimentation, we found that a boolean vector sized to represent all possible terms of a document X  X ith dis-criminant terms represented by true values X  X as a more efficient implementation allowing our code to function in a third of the original runtime. Using a boolean vector scheme, the two above organisms would now be represented as [ TFTFFT ] and [ FTFFTF ].

Using the integers 0 and 1 for representing the true and false boolean values, the genetic algorithm for selecting the minimal unique discriminant out of N different terms can be described in the following steps: 1. [Initialize a population of size S .] Set P 0 ... S , 0 ... N  X  rand[0 ... 1] . 7. [Make new population the current population.] Set P  X  P . INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 13 parameters. In our implementation we used the parameters that Grefenstette (1986) derived using meta-search techniques namely:  X  a population size S of 50,  X  a crossover rate of 0.6,  X  a mutation rate of 10  X  4 ,  X  a generation gap of 1 (the entire population is replaced during each generation),  X  no scaling window, and  X  an elitist selection strategy (the organism with the best performance survives intact into the next generation).
 mutation rates, and the selection of organisms were produced using the subtractive method algorithm (Knuth 1981, pp. 171 X 173).

In addition we used some domain-specific heuristics:  X  we select candidate terms from a subset of the terms with the lowest frequencies across the complete document collection,  X  we bias a term X  X  selection according to its global frequency, and  X  we ensure that the globally most frequently used terms are not used as candidates. All the above heuristics are based on the premise that less frequently used terms are more likely to be part of a unique document discriminant. 4. Prototype implementation We implemented a set of programs that process a (presumably all-encompassing) set of web pages, and calculate for each page a minimal set of search terms (words) that can be used to uniquely identify that page within the set. To test our implementation we took advantage of the data set provided during the 2002 Google search engine (Brin and Page 1998) programming contest. The package provided to the contest participants included a programming framework for processing a pre-parsed document collection (the so called ripper program) and a large collection of web documents. The breadth and architecture of the ripper programming framework strongly indicate that applications based on it could be easily ported to run on the actual Google infrastructure. The 5.9 GB data set we used consists of 916,429 pre-parsed HTML documents containing about 28  X  10 6 terms. 4.1. Implementation overview We calculate the discriminants in two steps: 1. We create an index of all documents where each term occurs; in actual practice a search 14 SPINELLIS 2. For each document we use the genetic algorithm to try combinations of the terms it As usual the devil is in the details, especially when dealing with the 1 million documents we processed and the 3  X  10 9 documents currently indexed by typical search engines.
Our application consists of tools for calculating unique discriminants on a single pro-cessing node (useful for proving the concept, trying out the data subset, and experimenting with the algorithms), and in an environment of multiple nodes (for processing the data set of a commercial search engine). In addition, we implemented simple tools for querying the results and obtaining the corresponding URLs. 4.2. Indexing Some applications have dealt with the problem by compressing the data in-memory (Moffat 1992, Spinellis 1994), but this approach would still not accommodate our problem X  X  scope and resource constraints.

We handled the problem by accumulating a term-to-document index in memory and monitoring the memory subsystem X  X  performance. Once the system begins to persistently page (indicating that the memory X  X  capacity has been reached and performance will rapidly have been processed, the partial results are merged into a single file ( idxdata ) contain-ing terms and documents where each term occurs. An index file ( idxdata.idx ) allows rapid serial access to individual terms without having to traverse a term X  X  document list. In addition, a separate file ( idxdata.hash ) allows rapid hash-code based access to indi-vidual terms. As the string hash function, we use the one recently proposed for very large collections by Zobel (2001). The hash file is created after the merge phase and can thus be optimally sized, using the Rabin-Miller prime number probabilistic algorithm (Schneier 1996, pp. 259 X 260), to minimize collisions. 4.3. Stand-alone operation The data-flow diagram of our system X  X  stand-alone operation appears in figure 1. A new handler of the Google ripper named index reads-in preparsed documents and creates (in stages by merging intermediate results) an index of the documents where each term resides ( idxdata ) and two files for accessing that index ( idxdata.idx and idxdata.hash ). A fourth file, topnodes is set to contain the frequencies of the 100,000 most frequently used terms; it is used as cache during the discriminant calculation phase.
 The second phase is also implemented as a separate ripper handler named bookmark . This reads the terms of each document, selects the least frequently used ones, and creates the bookmarks file containing for each document its URL and the set of terms forming INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 15 16 SPINELLIS its unique discriminant. The bookmark.idx file created allows the location of a given document URL and discriminant based on the corresponding document identifier.
Tw o query programs we wrote, cgi query and cmd query , will search the term index document URLs and the corresponding unique discriminants. The CGI application presents the discriminants as a URL; the end-user can bookmark this URL and thus visit our search engine to locate the document in the future. The HTML results also contain a link to a Google search with the same discriminant as a search expression. This search will not in larger than the one we processed. One of the discriminants we tried did however identify and correctly locate one page that was moved (renamed): a Google search for the page http://www.umass.edu/research/ogca/new.htm uniquely identified through the discriminant  X  X gca fy02 X  now yields http://www.umass.edu/research/ogca/news/oldnews.htm. 4.4. Distributed operation As is apparent from figure 2, the system X  X  distributed operation is a lot more complicated than the stand-alone case. It does however provide a framework for creating discriminants for orders of magnitude larger collections using a large number of commodity processing nodes. The work distribution strategy is based on two premises: 1. Documents are uniformly distributed across all processing nodes. Each node calculates To divide the term load across the nodes, we run a stand-alone instance of ripper  X  index on a small representative subset of documents. A separate text file contains a list of all processing nodes. The program divide :  X  e xamines the term index and divides it uniformly across processing nodes,  X  assigns a separate numeric initial document-id to each node, and  X  copies the generated files to all nodes.
 On each node we then run an instance of ripper  X  index to process the node-specific pre-processed pages. The program scatter is then run on each node to split and copy the resulting term index according to the terms assigned to each node. Each node will thus receive its share of terms as indexed by all other nodes. The make index program merges node. This file is accessed by the node X  X  index server program to provide term document occurrences to other nodes. The ripper distr program run on each node communicates with the index server responsible for a given term to obtain the global list of a term X  X  documents. To reduce network communication overhead initial term frequencies (our algo-INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 17 18 SPINELLIS distribution corresponds to the global one. The generated unique document discriminants identification number for locating the node where the respective discriminant is used. 5. Evaluation Important aspects of algorithms expected to process the global web include, apart from the 5.1. Discriminant performance After processing the Google sample document collection we found that we needed an aver-age of 1.47 terms to uniquely identify a document. The average length of each discriminant (figure 3) was 10.77 characters which, as a document identifier, compares extremely favor-ably with the 43.9 characters of the collection X  X  average document URL length (excluding the initial http:// . The number of terms for each document X  X  discriminant was distributed documents processed.

The property of the calculated discriminants to uniquely identify a document, while not system calculated will locate a single document, while another 10% identify two documents. In total 76% of the discriminants will locate less than 10 documents in the sample document such as the number of terms of the candidate set, the number of common terms to eliminate, and the size of the organism pool. Many of the pages for which a unique identifier was not calculated contain very little textual material. As an example our system X  X  spectacular INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 19 (it was identified by the term  X  X umanities X , which also matches another 29,497 pages) can be easily explained by the fact that this home page consisted entirely of text and pictures presented in graphical hypertext form using image maps. 5.2. Algorithm performance On average the GA was run for 10.6 generations to calculate each discriminant. However, the distribution of the GA generations that were required was highly skewed: the corresponding mode was 2, median 5, and the standard deviation 16.5. To evaluate the performance of the GA over the heuristic selection of words, we calculated discriminants for a subset of 13,000 documents using a procedure that followed the selection traits of the GA, but not its evolutionary strategy. Specifically, for every document we created S = 50 organisms and let those mutate G times, where G wa s the number of generations the GA had run for that document. The initial organism was not random, but as was the case for the GA, was created using the allele probability selection vector. The results from this quasi-random selection were 34% worse than those obtained from the GA operation. In 58% of the cases 20 SPINELLIS the two methods yielded the same result. Although the advantage offered by the GA may not seem impressive, we believe that (a) only the GA will scale to handle the three orders of magnitude larger collection of the global web, and, (b) the GA can be easily adapted to w ork with more demanding objective functions, whereas the heuristic techniques can not. 5.3. Time requirements We indexed the sample document collection (916,429 documents 3,152,415 unique low-ercased terms) on a 733 MHz, Celeron CPU, 128 MB RAM, 40 GB IDE disk machine in 18,605 real, 7,064 user, and 1,533 system seconds, at a throughput of 29.26 documents/s. The process used 18 intermediate indexed files each of about 75 MB in size (the first one wa s 190 MB) with an intermediate file being dumped every six minutes. The hashed term database performed adequately, but not spectacularly with 2,111,778 total term name col-lisions (resulting on an average of 1.49 disk index accesses per term) and 173 maximum term name collisions.

The time to perform the unique discriminant calculations varied, because we split the w orkload among 20 different machines. The time required on 733 MHz, Celeron CPU, IDE disk machine for processing the file pprepos.00 (16,564 documents) was 58,721 real, 184,054 user, and 9,604 system seconds giving a processing time of 3.54 s/document. Systems with a SCSI disk subsystem performed better.

We unfortunately lacked appropriate resources (a large farm of networked processing implementation of our system. We were however able to obtain a lower bound of the expected performance by running the programs on a small number of workstations. By extrapolating time of T as T  X  T  X  2 . 2. 5.4. Space requirements The indexing operation utilizes the maximum amount of main memory available, but is constrained by design to stop its memory usage growth once thrashing occurs. In our case, it processed without a problem the complete sample data set on machines with 128 MB RAM. The off-line space requirements are comprised of the space needed to store the term index and its hash file; this was for the sample document collection 826,504,382 bytes for the index and 50,438,784 bytes for the hash file, giving an overhead of approximately 957 bytes per document.
 The space requirements of the discriminant calculation phase are more difficult to judge. When performing calculations over the sample collection we observed a maximum resident set size 62,140 KB. This number is likely to grow with a larger document set, but not by much, since it reflects the space needed to store the document instances of a document X  X  are stored in memory, the term selection process can be easily adjusted to dynamically select terms that will load in the main memory a fixed number of document occurrences, thereby providing a concrete bound to the memory usage.
 INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 21 5.5. Scalability W ill our approach gracefully scale to cover a search engine X  X  complete page collection? We consider as a test case Google X  X  2  X  10 9 page collection and for our estimates we use a number of 8,000 processing nodes reported in the technical press as comprising Google X  X  infrastructure (Wagner, 2001).

Computing the term index will therefore require: The communications overhead will be roughly equivalent to that of copying the index files across the network; given an index file size of this will add an overhead of 8 minutes using a switched 100 Mbit network with a 50% merged without a problem.

The time to compute the discriminants will be larger. At 3.54 s/document the calculation of all discriminants will require One should keep in mind that this process will be required to run very infrequently for the entire document collection and can then be run incrementally as new documents are probability even as the structure of the web changes).
 Note that all our time figures are based on the results we obtained using low-end 733 MHz Celeron PCs with IDE hard disks. In addition, the indexing phase can be omitted and the index structure. The discriminant calculation algorithm only needs access to an oracle that answers the question of how many documents are matched by a given term combination. We assume that a search engine X  X  infrastructure is engineered and tuned to efficiently answer the above query and should therefore preferably be used by our algorithm. 6. Conclusions and further work to address an important shortcoming of today X  X  web in a scalable, and efficient manner. The almost a fourth of the size of conventional URLs. 22 SPINELLIS
However, the document identifiers we provide are not suitable as universal replacements for the URLs currently employed. In document retrieval terms their use involves a trade-off of noise over silence. Our calculation method is not perfect and there are cases where our algorithms will either fail to calculate a discriminant for a web page (e.g. when the page In addition, changes to the web contents and a search engine X  X  coverage can make identi-fiers that were calculated to be unique match additional documents. Although we have not studied the temporal behavior of the discriminants we calculate, an obvious pathological case involves documents cloned from a given source document through small additions or changes. This cloning is a common operation and is related to the scale-free topology of the web (Barab  X  asi et al. 2000). Cloned documents will very probably match the dis-discriminants are most suited for situations where a human can make an informed choice for using them and remains in the loop during their use. As an example, personal book-marks are particularly well suited for being stored using unique discriminants (or include unique discriminants as a fail over mechanism). Bookmarks are highly prone to aging since they are typically not formally maintained; in addition, once a bookmark matching several documents is followed, the user can intelligently choose between the different pages.
During our work, we noted a number of improvements that could be employed for optimizing the algorithm X  X  performance and for further increasing the usefulness of the obtained results. These include:  X 
Optimize the GA, tuning its configuration by replacing the generic parameters we used e xamination of the discriminants we calculated, we found some instances of duplicate discriminants and multiple documents identified by the same discriminant that could have been easily avoided.  X  Experiment with different stochastic algorithms such as simulated annealing (Cerny 1985,
Va n Laarhoven and Aarts 1987, Koulamas et al. 1994) and tabu search techniques (Glover 1990).  X 
Explore the possibility of using our results for locating duplicate documents. Since our algorithm tries very hard to find unique discriminants, failure to find unique discrimi-nants could signify the existence of virtually duplicate documents. This technique can be strengthened by resetting the random number generator seed values before processing each document.  X 
Investigate the impact of cloning and modification on the temporal effectiveness of dis-criminants, experimenting with different objective functions based on the notion of sim-ilarity and search engine ranking.  X 
Study and improve the distributed algorithm operation on a large network of hosts.  X  accurately identify the precise location of pages that have not moved.
 We end our description, by noting how the realization of the application we outlined was information retrieval was the domain where our problem was formulated, algorithms and INDEX-BASED PERSISTENT DOCUMENT IDENTIFIERS 23 data structures provided the framework to obtain the solution, operating system concepts allowed us to bound the indexing memory requirements, complexity theory gave us the theoretical background for searching for algorithmic solutions, stochastic approaches were used for sidestepping the problem X  X  NPC characteristics, and networking and distributed systems technology provided the framework for developing the distributed implementation. Increasingly, the immense scale of the web is necessitating the use of multidisciplinary approaches to tackle information retrieval problems.
 Acknowledgments During the system X  X  implementation phase I benefited from fruitful discussions with Elisa Fragkaki, Stavros Grigorakakis, Vasilis Kapouleas, and Michalis Vazirgiannis. The string hashing expression used in the hashing functor was written by Justin Zobel. The prime number generator used for sizing the hashed database contains code developed by Bradley Smith and Greg Heileman at the University of New Mexico. The distributed version of the system relies on the socket ++ library developed by Gnanasekaran Swaminathan and patched for Linux and FreeBSD by Lauri Nurmi. Finally, I would like to thank the paper X  X  anonymous referees for their insightful and astute comments on an earlier version of this paper.
 References 24 SPINELLIS
