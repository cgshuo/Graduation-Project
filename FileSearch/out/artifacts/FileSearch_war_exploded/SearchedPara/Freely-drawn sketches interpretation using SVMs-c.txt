 1. Introduction
Currently, the pen-centric computing becomes popular. Micro-softs Tablet PC has established a beachhead as a commercially available platform for pen-centric applications. Batteries, displays, and computing hardware continue to improve. There are increas-ing demands for the software that deals with complex data, such as images, 3D models, and video, using the electric pen input.
Online recognition has two main advantages. First, it interprets and displays the pen input when entered for users convenience.
For example, an engineer drawing of a mechanical system receives feedback to distinguish the different parts of the system, which sets the stage for users further input. Second, an online sketching system can utilize temporal information as well as geometric information of the drawing process.

Sketch recognitions are often characterized by the presences of spatial or geometric constraints, and temporal features. These aspects pose a number of technical challenges in developing sketch recognition systems. Numerous techniques have been proposed for sketch recognition within the past several years.
However, some sketch approaches utilize the model or template based methods without regarding to the temporal information, such as Alvarado and Davis (2004) and Shilman (2002) , which formulate the recognition problem on the basis of objects X  implicit or explicit representations in terms of their structure (such as components, subcomponents, and their relationships). Therefore, these approaches may require high computational burden that makes them unsuitable for real-time recognition of realistic sketches, and require experts to specify domain knowledge or manually input structural descriptions of the objects. Other similar recognition algorithms were presented in Gennari et al. (2005) , and Shilman and Viola (2004) .

Recently, the Hidden Markov Models (HMMs) and SVMs have been explored on the recognition of the handwriting gesture, the degraded text and the symbols due to their robustness to cope with incomplete information and distortions. In Su et al. (2009) ,it proposes a segmentation-free strategy based on the first order HMMs to handle the off-line recognition of Chinese X  X  handwrit-ing. In Labush et al. (2008) , the authors train SVMs on the extracted features to obtain classification performance in the digit recognition task. Moreover, there appears a few methods for recognizing sketch using HHMs by integrating the temporal information. In Anderson et al. (2004) , a HMMs-based method was described that uses chain-code-like features to recognize the isolated symbols. In Sezgin and Davis (2005) , a sketch recognition system utilizing HMMs was presented which does segmentation and recognition by combining outputs of HMMs.

Compared with HMMs, we find that (i) unlike HMMs, the proposed SVMs-chain can estimate some complex distribution pattern of the training data robustly with the stable computation cost; (ii) the SVMs-chain provides a simplistic probability model that directly deals with prior knowledge rather than presetting some parameters of the structures and the mixtures for HMMs; (iii) the accuracy of the inference by the SVMs-chain seems to be less sensitive than the HMMs to the loss of access to sets of observations.

Therefore, in this paper, a new statistical learning algorithm for the sketch recognition X  X VMs-chain is proposed. It incorpo-rates the strategies of graphic model methods, such as HMMs, with SVM X  X  robustness in estimation of distribution pattern. It transforms the discriminant function of SVMs, into a pseudo probability density function, which can learn the distribution pattern of the training data in the feature space.

The main contributions of the paper are listed as follows: (i) a new method for extracting the feature vertices combined (ii) a modified primitive representation for human-handed (iii) the pseudo probability density function is presented for (iv) developing a sketch recognition based on our proposed
The paper is organized as follows. Existing algorithms for feature vertices extraction are briefly introduced and the adaptive window algorithm is presented in Section 2.1 . Section 2.2 describes the way of encoding the free hand sketch. Section 3 presents the algorithm of probability distribution estimated by our proposed pseudo probability density function. Section 4 presents the evaluation results to show a significant advance in this proposed method compared with HMMs. 2. Sketch encoding
Compared with the traditional approaches by abstracting features from static sketches, the real-time data collection by digitalizing pen input can provide more information for facilitat-ing the sketch recognition. The existing approaches for feature extraction can be rawly divided into two classes: one is the global-level feature extraction, which works when the complete sketch is available. The class of these methods is implemented in
Kimura et al. (1993) , Fonseca and Jorge (2000) , Fonseca et al. (2002) . The other is the stroke-level feature extraction, for example, the primitives representation in Sezgin and Davis (2006) . Both classes have different emphasis and strengths. For example, the global-level features provide the global geometric information, such as bounding box, area of the convex hull, centroid. However, it is only efficient in the recognition of some simple sketches. In contrast, the stroke-level features provide more detailed information deriving from each stroke and their sequential order. This makes the use of temporal information feasible. Similar works are introduced in Williems et al. (2009) , which shows that using the stroke-based features can improve the classification accuracy by 10% and 50% compared with the global feature representations. 2.1. Feature vertex detection
In the early stage of sketch recognition, a sequence of vertices along the freely-drawn line sampled in the temporal order is obtained for preprocessing and the structure of strokes can be recorded. A fast and stable algorithm to extract the structural information, represented with the feature vertices, from these sampled points of the original sketch is needed.
 vertex selection complicated: (i) the rawly sampled points are redundant for representing the structure of a sketch; (ii) the few points recorded are insufficient to depict a sketch. Therefore, an appropriate subset of these points should be saved as the feature vertices in order to make some balance between both. 2.1.1. Adaptive windows
A t ( t ) of each point along the strokes into account. It is a supplementary angle of the included angle defined by three successive points shown in Fig. 1 . The feature vertices, which can effectively reserve the basic structure of sketch, are those points positioned at the corners of sketch. Hence, they are usually accompanied with the larger value of A t ( t ) than their neighboring points. The point with local maximal A t ( t ) can be selected as the candidates of feature vertices. In addition, the two endpoints of a set with large values.
 maximal A t ( t ) as shown in Fig. 2 . The turning angle of a single point is compared with its neighborhoods within the limited 0.5 1.5 2.5 3.5 point and the width of the window, respectively. The candidate points with the local maximal turning angle are shown in Fig. 2 .
The number of local maximum will decrease accordingly when increasing the window width w .

Algorithm 1. Adaptive windows algorithm. 1: for i  X  0to N 1 do 2: for j  X  1to N = 2 do 3: MaxWindowWidth[i]  X  j; 4: if A t  X  i o A t  X  i  X  j or A t  X  i o A t  X  i j then 5: MaxWindowWidth[i]  X  j-1; 6: break; 7: for i  X  0to N 1 do 8: for k  X  0to MaxWindowWidth  X  i do 9: MaximaCount[k]  X  X  ;
First of all, a proper width for th e window is required. An existing linear approximation approach implemented in Sezgin and Davis (2004) to calculate an  X  X  X ptimal X  X  window size cannot necessarily guarantee the performance. The selected feature vertices are more than needed even given an optimal window width w opt .Inthis paper, a dynamic approach is proposed to select the most effective feature vertices from a huge number of measurable points sampled by the hand drawing input device. Between each pair of candidate along the stroke. A set of measurable vertices, denoted by C  X  v the sampled points positioned between the two selected vertices for stroke to the direct segment which links both selected vertices, v and v i  X  1 ,asshownin Fig. 3 ,where E u , v  X  k  X  represents the accrued error between two selected vertices, v i and v j ,with k splitting accrued error between v i 1 and v i  X  2 .

Let M represents the total number of candidate vertices along a stroke and N v denotes the number of the finally selected feature dynamic programming following the recursion formulas in error may vary due to the selection of the feature vertices.
Through minimizing the evaluation function T  X  E 0 , N 1  X  N in (3), a optimal subset of vertices would be acquired: E  X  0  X  X  E  X  k  X  X  min i  X  0 , ... , M 2 j  X  i  X  1 , ... , M 1 k Z 1  X  2  X  N v  X  0 , ... , M  X  3  X 
The algorithm is presented in Algorithm 2 . The time consump-tion apparently depends on the number of the candidate vertices, M , with the window width, w opt . In our study, a hand-drawn strokes usually contain no more than 30 vertices. To reduce the time and the space complexity of the algorithm, the appropriate window width w opt is less than 30.
 Algorithm 2. Dynamic programming. 1: i  X  0; j  X  1; 2: for p  X  0to N 1 do 3: if v  X  i o p o v  X  j and j o M then 4: E ij  X  0  X  E ij  X  0  X  Distance ij  X  p ; 5: else 6: i  X  X  ;j  X  X  ; 7: for k  X  1to M 2 do 8: for i  X  0to M 2 do 9: for j  X  i  X  1to M 1 do 10: for l  X  i  X  1to j k do 11: if E il  X  0  X  E lj  X  k 1 o E ij  X  k then 12: E ij  X  k  X  E il  X  0  X  E lj  X  k 1 ;
The proposed dynamic window approach has several advan-tages over previous scale based algorithm. For example, the discrete convolution is required in the scale-space filtering proposed by Sezgin and Davis (2004) , while in this approach, only simple comparisons are needed. The new approach dose not need a coarse-to-fine tracking as introduced in the work ( Witkin, 1984 ), since the local maximum will not change during reducing the size of windows. Fig. 4 shows the pretty good results obtained by utilizing the window algorithm and the dynamic programming. v ( i -1) 2.2. Feature vector sequence generation
Normally, freehand sketch require preprocessing in the recog-nition system. Before analyzing, multiple strokes of a sketch are quantified with the use of the vector coding technique. The feature vector provides a way of encoding approach which utilize a constant length vector to represent the extracted feature of strokes. Finally, a group of feature vectors are orderly arranged in the form of a sequence to produce a complete presentation of a single stroke. 2.2.1. Primitives generation
The strokes of a sketch for a certain type of object are denoted with
S  X  S 1 , S 2 , ... , S n  X  4  X  where the sketch S consists of a set of ordered strokes S n A N is the total number of strokes in S .

These strokes typically contain some critically useful information  X  the drawing style  X  regarding to the habit of drawers, such as the sketching order, the variations of sketching speed, some tricks of breaking strokes. Besides these, the task also requires a encoding technique capable of filtering out various undesired disturbances, for example, the hand-arm vibration syndrome. Furthermore, to compress the data volume, it is necessary for a more efficient representation to minimize the intrinsic redundancy of raw strokes.

The adaptive window method proposed to select the critical vertices in previous section generates a set of vertices, ordered by time, which allows a proper segmentation of a stroke. To produce a group of primitives, a light operation is needed by orderly linking these vertices with the directed segments. For conveni-ence, they are denoted with
P where P i represents a set of ordered primitives P i , k , 1 r k r l the stroke S i . l i is the length of P i .

Two types of primitives are introduced, the explicit and the implicit primitives. The explicit primitives, denoted with
P , 1 r k r l i , are the approximation of a real stroke, while the implicit primitive is a logical one, which virtually links the discontinuous strokes by the temporal order, denoted with the sketch. In details, an explicit primitive is generated by linking both neighboring feature vertices. Its direction is consistent with the dynamic process of sketching. Differently, an implicit primi-ately sequential strokes, denoted with S i 1 and S i separately. As shown in Fig. 5 , the primitive representations of hand sketch are illustrated. The explicit primitives are denoted with the solid segments; the implicit primitives are marked with the dash segments, respectively. There are two primary advantages of out noises, such as the vibration. Secondly, to compress repre-sentation of sketch by reducing redundancy in the raw strokes. 2.2.2. Primitives sequence
Listing the primitives from the observation (4) for an object in the temporal order, we have
O  X  ... , P i 1 , P n i 1 , i , P i , ... where O is the representation of sketch S for an object with the primitives sequence, which is partially illustrated in Fig. 6 .
Normally, sketching is a continuous-time process. For conve-nience, the process is discretized to generate the corresponding feature sequences. Finally, the representation (5) of a sketch could be denoted formally with primitives as
O  X  ~ P 1 , ... , ~ P k , ... , ~ P m  X  6  X  where m is the total number of primitives in (5), ~ P k represents to the k th primitive in (5), an explicit or implicit primitive. 2.2.3. Feature vector and feature sequence perspectives, such as, the length, the relative angle between two continuous primitives, the direction. These feature vectors are denoted with F i , j ,1 r j r l i , for the explicit primitive P in (5), is denoted with
FS O  X  ~ F 1 , ... , ~ F m  X  8  X  feature vector is represented with a continuous variable vector, which is a six-tuple shown as  X  l k , D l k , r k , D r k , sgn k , state k  X  , 2 r k r m where l k and D l k represent the length of P k and the ratio of P
P k 1 respectively; the r k and D r k represent the slop angle of P the included angle between P k 1 and P k given by the magnitude of the start point to the end point of primitives P k 1 and P respectively; sgn k contains the direction of the stroke moving primitives, for explicit, state k is set with 1, or else, with 0. value, since the previous primitive is not available. 2.2.4. Fixed-length sequence
The object can be represented with a fixed-length feature sequence in order to employ the proposed SVMs-chains in the recognition system without affecting by the variation of the length. Normally, these variations are reasonable due to the varied drawing styles and the occasional breaking up during sketching, as shown in Fig. 7 .

For example, the object of the right pointed arrow shown in Fig. 8 has two types of drawing styles with the length of 7 and 6 respectively. The maximal length, for example, 10, is used to unify the length of both drawing styles. Utilize a linear time-warping function  X  k  X  X  where m is the origin length, the L is a maximal predefined length, satisfying m r L .

By the linear time warping function (10), the origin feature vector sequence can be naturally extended to the one of given length. The extended representation of (6) and the corresponding feature sequence is shown below FS 3. SVM-based estimation for the support of distribution
Support Vector Machines (SVMs) have received wide consider-able attention in pattern recognition, regression estimation and solution inverse problems. Recently, there are some attempts to use SVM in the problems like estimating the support of distribu-tion in Sch  X  olkopf et al. (2000 , 2001) . 3.1. Algorithm
Considering the training data set X  X  x 1 , x 2 , ... , x the number of the observations, assuming x i A R n . Let F : X ! F , there F is a feature map from the original space to dot product feature space. Typically, there are some kernels available for evaluating such dot product in the image of F : k  X  x , y  X  X  X  F  X  x  X  , F  X  y  X  X  X  13  X 
The different kernel functions are listed below, more information on kernel function can be found in Christianini and Shawe-Taylor (2000) : (1) Polynomial: (2) Gaussian Radial Basis Function, or Gaussian kernel: (3) Sigmoid kernel:
There are many more including Fourier, splines, B-splines, etc. ( Christianini and Shawe-Taylor, 2000 ). In this paper, the com-monly used kernel, Gaussian kernel, is adopted for SVM. data from the origin space to the feature space F , and then adjusting parameters in order to find a hyperplane which could separate data set from each other with the maximum margin using the kernel trick. This problem could be transformed into solving the following quadratic problem: where v A  X  0 ; 1  X  is a parameter which determines the trade-off between the regularization term J w J and the errors in the training set, w and r are initiated with zero, which will be re-estimated with considering the nonzero slack variables x i , which are penalized in the objective function (17). Finally, the prediction function f  X  x  X  X  X  w , F  X  x  X  X  r  X  18  X  will be positive for most examples x i in the training set and the testing examples locating inside t he support of distribution, which depends on the training set; the output of the function will be a negative value for those testing examples that locate outside the distribution support region. Constructing a Lagrange function with the objective function and the corresponding constraints by intro-ducing multipliers a i , b i Z 0 L  X  w , x , r , a , b  X  X  1 2 J w J 2  X  1 h to zero yielding w  X  a i  X  1 h b i r 1 h , where the training data x i with the non-zero valued a i are called
Support Vectors. Considering (13), the dual problem is obtained min
The problem can be solved using the standard optimization techni-be transformed into positive normalized form by truncating b and multiplying a normalizing constant, then we obtain a probability density function (23) ^ f  X  x  X  X   X  w , F  X  x  X  X  R where assuming that v  X  1, then the constraint P h i  X  1 a satisfied.

The function will give the comparably higher probabilities for the examples contained in the supports of distribution than those outside the supports. The farther the testing example is from the regions of supports, the lower probability it will be given, because it is natural to suppose that, the larger the distance of an example has from the support regions where most training examples  X  X  X rowed X  X  in the feature space, the less possible this example is classified as a member of them. Although the pseudo probability tion from which training examples are drawn, it still has a good performance for our application. 3.2. SVMs-chain train
SVMs-chain is a set of linearly aligned SVMs. Each SVM is created to estimate the distributing pattern for a feature vector in its feature space. Each feature vector will be assigned to one of
SVMs in this chain according to its position in the feature sequence, as shown in Fig. 9 .

Atrainingsampleforthetriangleshapeisdecomposedinto primitives sequence which contains five primitives including explicit and extended to a SVMs-chain with a fixed length through the linear time warping function. Since such length is equivalent with the length of the SVMs-chain assigned for the triangle shape. Therefore, each feature vector can be uniquely mapped into a SVM node, denoted by a grey circle, as shown in Fig. 9 .

After collecting a satisfied amount of samples, each SVM in this chain produces an appropriate estimation of the distribution of corresponding feature vector in its feature space.

The probability, or the score, can be obtained for categoriza-tion through the following methods: Prob  X  O 9 l  X  X  where, the function ^ f  X  X  is defined in (23), the feature vectors ~
F , 1 r i r L , are defined in (12), l is the set of the parameters for to a certain object.

The procedure of training the SVMs-chain is summarized below: Specifying a SVMs-chain with a fixed length to a object
Preprocess sketches Training SVMs-chain with the observation
Generally, for each candidate object, only one SVMs-chain is assigned. The probability of a feature sequence belonging to each candidate is evaluated by these SVMs-chain utilizing (24). Then, the testing sketch can be recognized as a object, of which the specific SVMs-chain votes the highest score compared with the others for its feature sequence, as shown in Fig. 10 . 3.3. Discussion
HMMs method forms a large class of stochastic processes, which is based on an unobserved discrete Markov chain f S the evolution of the states of a system. Given a realization of the pendent with the distribution of each O i depending on the corresponding state S i only.
 valued parametric model, and then assume that the observations are drawn from a specific probability at each time instant. The commonly used parametric model is the finite mixture of Gaus-sian Probability Density Functions (PDFs).
 satisfying amount for mixtures in each state, and its directly resul-ted clustering problem results from Gaussian mixture modeling.
Although there have been several learning algorithm, including EM algorithm ( Render and Walker, 1984 )andthe K -means algorithm ( Jain and Dubes, 1988 ). However, these approaches are based on the assumption that the number of Gaussians in the mixture is known before. Unfortunately, this assump tion is practically impossible. In such a situation, the selection of an appropriate number of Gaussians must be made jointly with the estimation of the parameters, which becomes a rather difficult task without the guaranteed performance.
In order to avoid these problems existing in HMMs for continuous observations, the SVMs-chain provides alternative of utilizing Gaus-sian Mixture Model (GMM) without considering the proper number for mixtures.
 Two examples are provided for illustrating performance of
SVMs of estimating the unbalance data randomly sampled from an unknown probability distribution in feature space. In the first example, a simple probability distribution is given in Fig. 11 .Itis consisted of two Gaussian distributions with the variances 0.1 and 0.5 and the centers ( 0.3,0.3) and (0.2, 0.2) respectively.
For generality, the second example tested SVMs with more complicated distribution consisted of a weighted summation of 80 Gaussian distributions with the uniform variance 0.1, shown in
Fig. 13 . Before training, the samplings are required which are randomly drawn from the given probabilities in both examples.
Based on these samples, SVMs could provide such estimations, which depend on the settings of scale parameter c , as shown in Figs. 12 and 14 .

In Table 1 , the varied values of parameter c , which is defined in (15), are assigned for both examples. In Fig. 12 , for two Gaussian mixture distributions, A2, B2, C2 and D2 illustrate its different estimations by SVMs with varied settings of c , from 0.02 to 0.50.
In details, A1, B1, C1 and D1 demonstrate that some significant samples, Support Vectors (SVs), of which the markers are high-lighted with red in order to be distinguished with others, are extracted from the raw samples. Through SVMs learning, they are used as the center of Gaussian kernels in the estimation (23). Different weights are assigned to such kernels through coeffi-cients a i 4 0 , 1 r i r h in (23), where h is the amount of raw samples. In Fig. 12 , the weights of distinct kernels are illustrated with the different sizes of the cross markers; the larger the marker is, the more the corresponding kernel is weighted.
Table 1 conveys some detailed information behind Figs. 12 and 14 . NSVs present the number of SVs which are selected by SVMs as kernels; PCTs is the percentage of SVs among the whole samples. After observation, it is easy to found that NSVs is declined while the value of parameter c is increased. This would result in distinct performances of SVM X  X  estimation. For example, the smaller value of c is set. Compared between two examples illustrated in Figs. 12 A2 and 14 E2 respectively, it can be found that the estimated pseudo probability density will produce a  X  X  X ight X  X  estimation on the basis of the acquired training samples when the c is set with a small value. Around 57.7% of total samples of the first example, or 72.2% of total samples of the second example, are selected as the centers of Gaussian kernel. On the contrary, it will lay more dependence on the overall distribution of samples in a wide range when a larger value of c is set. Since fewer samples will be selected as the kernel centers of GMM, SVMs can only loosely approximate the given probability distribution, as illustrated in Figs. 12 D2 and 14 H2. In both cases, the number of mixture components is automatically determined depending on the setting of c and the pattern of the samples X  distribution.

However, in HMMs method, the amount of mixtures needs to be set before. And the proper number is usually unknown. Different from HMMs,theapplicationofSVMsont he estimation of the probability than the choice for the amount o f mixture components of HMMs, especially for some complex distributions. In addition, the computa-tional cost, which depends on the size of training data, seems to be less correlated with the value of c , or the number of mixture components, or kernels, used in its estimation, while the computa-tional cost of HMMs varies significantly due to the change of the amount of mixture components used in this model. 4. Evaluation 4.1. Recognition rates
A sketch recognition system named SketchUI is developed to evaluate the performance of SVMs-chain, as shown in Fig. 15 . Meanwhile the HMMs referred in Rabiner (1989) as a benchmark methods has also been implemented. The selected structure of HMMs adopts the Bakis form (left to right without skip). HMMs parameterizes each state which generates the observations through a given PDF, for example, GMMs. A more concise pdf could be given by applying a GMMs to those multidimensional feature vectors, however the heavy computational cost is needed and the proper number for mixture components is hard to predict. For these reasons, the single Gaussian conditional dis-tribution for each dimension of the feature vector is utilized. It becomes computationally difficult to estimate the parameters of the covariance matrix for a Gaussian PDF, when the observations derived from a complicated high-dimensional PDF are insuffi-cient. Hence, for simplicity, it is assumed that the covariance matrix is diagonal. Both SVMs-chain and HMMs are trained with the same data sets, assuring the fairness of the experiment.
The developed SketchUI recognizes nine shapes, including circle , diamond , rectangle , triangle , line , arrow , p , m and shapes are divided into two groups as Geometry Figures and Gestures and Alphabets to show their intrinsic properties. Geometry
Figures are simple convex polygons while Gestures and Alphabets contain more complicated strokes thus could not be recognized by traditional geometric feature based approaches. Various sketch samples are collected from different people to train and to test the SketchUI . The results are shown in Table 2 , with c  X  1.0. tion rate for Geometry Figures and Gesture and Alphabets , nearly 90%, while HMMs is no more than 70%. For the various samples,
SVMs-chain maintains its recognition rate above 80%, despite the triangles which sometimes would be hard to distinguish from 0 10 20 30 40 50 60 70 80 90 diamonds. When the training sets are expanded by providing distinct structural descriptions of the shapes, the performance of
SVMs-chain would be significantly improved, suppose a proper value for the parameter c is given, while the performance of HMMs would be quickly degenerated ( Fig. 16 ).

Finally, the result figures are presented that depict the perfor-mance of the proposed sketch recognition approach by the use of
SVMs-chain for various applications. For example, Fig. 17 is the representation of a mechanical system made up of belt pulleys, and Fig. 18 is the recognition result of a freehand schematic drawing of circuit, and Fig. 19 is a simple freehand drawing for a mathematical formula. 5. Conclusions
In this paper, an adaptive window is firstly utilized to extract the feature vertices to obtain a concise sketch representation, and the redundant information and the disturbing noises could be removed from the implicit structure of freely drawn sketches.
Then, two types of primitives, explicit and implicit, are con-structed. Unlike previous works that treat the sketches as the static images, this paper explores a new way for sketch X  X  repre-sentation while incorporating geometric and temporal informa-tion. Finally, the efficiency of the proposed SVMs-chain for recognition is validated with the experiment results. References
