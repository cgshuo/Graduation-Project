 Motivated by our work with political scientists who need to manually analyze large Web archives of news sites, we present SpotSigs , a new algorithm for extracting and match-ing signatures for near-duplicate detection in Web crawls. Our spot signatures are designed to favor natural-language portions of Web pages over advertisements and navigational components frequently added by these sites.

The contributions of SpotSigs are twofold: 1) by com-bining stopword antecedents with short chains of adjacent content terms, we create robust document signatures with a natural ability to filter out noisy components of Web pages that would otherwise distract pure n-gram-based approaches such as Shingling; 2) we provide an exact and efficient , self-tuning matching algorithm that exploits a novel combina-tion of collection partitioning and inverted index pruning for high-dimensional similarity search. Experiments confirm an increase in combined precision and recall of more than 24 percent over state-of-the-art approaches such as Shingling or I-Match and up to a factor of 3 faster execution times than Locality Sensitive Hashing (LSH), over a demonstra-tive  X  X old Set X  of manually assessed near-duplicate news articles as well as the TREC WT10g Web collection. Categories and Subject Descriptors: H.3.7: Digital Li-braries. Collection, Systems Issues.
 General Terms: Algorithms, Performance, Reliability, Ex-perimentation.
 Keywords: Stopword Signatures, High-dimensional Simi-larity Search, Optimal Partitioning, Inverted Index Pruning.
Detecting near-duplicate documents and records in large data sets is a long-standing problem. Syntactically, near du-plicates are pairs of items that are very similar along some dimensions, but different enough that simple byte-by-byte comparisons fail. On the World Wide Web, document du-plication is particularly problematic, and in fact any of the major search engines attempts to eliminate highly similar documents from its search results.

Moreover, we encountered the problem in the related con-text of our Web Sociologists Workbench [26]. This project constructs tools for political scientists, historians, and soci-ologists to help them analyze large Web archives. Our users X  first project was to analyze news coverage around a Califor-nia state election. We had crawled numerous online news sites, and had filtered obviously uninteresting pages auto-matically. The remaining pages were to be tagged manually by content semantics, such as unions, feminism, celebrity, or local issue. These semantic differences were subtle enough that machine learning algorithms of sufficient accuracy could not readily be constructed. Human coders went to work on this collection, using a specially constructed visual tagging tool. Unfortunately, they failed to complete the task. Exces-sive amounts of near duplicates overwhelmed their efforts. Figure 1: Near-duplicate Web pages 1): identical core content with different framing and banners (ad-ditional ads removed).

There are in fact two types of near duplicates for this ap-plication. Figure 1 shows a pair of same-core Web pages that only differs in the framing, advertisements, and navigational banners added each by the San Francisco Chronicle and New York Times. Both articles exhibit almost identical core con-tents, reporting on presidential candidate Obama X  X  take on faith during the 2008 U.S. pre-election phase, because both were delivered by Associated Press. Figure 2 is an example of the opposite case from Yahoo! Finance, showing two daily summaries of the NASDAQ and Dow Jones indexes. In par-ticular for domains like stock markets, news sites often use very uniform layouts and the actual contents-of-interest only constitute a fraction of the page. Hence, though visually even more similar than the pair in Figure 1, the pair in Fig-ure 2 should not be identified as near duplicates. Typically, our sociologists would only consider Figure 1 X  X  same-core pair to be near duplicates, since the core articles are their focus. Near duplicates would not be discarded but could be collected into a common set which is then tagged in batch. Figure 2: Near-duplicate Web pages 2): identical framing but slightly different core content.

The frequent presence of many diverse semantic units in individual Web pages makes near-duplicate detection par-ticularly difficult. Framing elements for branding, and ad-vertisements are often freely interspersed with other con-tent elements. The branding elements tend to be deliber-ately replicated across the pages of individual sites, creating a  X  X oise level X  of similarity among pages of the same site. Another difficulty that many Web pages present to near-duplicate detection algorithms is that coherent units of text are often small and lexically fragmented. This fragmenta-tion occurs because HTML tables are used for layout control. The insertion of images, ads, or entirely unrelated material that is arranged to either side of a visually uninterrupted text segment may thereby partition the text lexically: a lin-ear scan through the page source finds the text interrupted by all kinds of neighboring layout material. Algorithms for duplicate detection must therefore be particularly robust.
Even if care is taken to avoid exact duplicates during the collection of Web archives, near duplicates frequently slip into the corpus. For example, while long-term archives for news sites are usually closed, many news sites contain small archives of material published during recent days. A crawler that harvests such a site daily will keep stumbling into those areas. However, rather than re-collecting identical pages, the crawler is served with different framing ads upon each access, which introduces near duplicates into the archive.  X 
SpotSigs provides a robust scheme for extracting char-acteristic signatures from Web documents, thus aiming to filter natural-language text passages out of noisy Web page components. While our approach is similar to Shin-gling [7], it provides a more semantic pre-selection of shin-gles. The SpotSigs parser only needs a single pass over an incoming token stream, which is much more efficient, eas-ier to implement, and less error-prone than sophisticated and inherently expensive tools for layout analysis, and it remains largely independent of the input format.  X 
We propose an exact and self-tuning, highly parallelizable algorithm for similarity search in high-dimensional feature spaces that yields no false negatives nor positives due to the matching procedure itself. Yet we are able to show very competitive runtimes to the fastest known but more error-prone hashing schemes like I-Match [12, 22] and Locality
Sensitive Hashing (LSH) [20, 15] X  X f the similarity thresh-old is high. Moreover, SpotSigs creates clustered output in the form of all linked pairs of near duplicates.  X 
For low similarity thresholds, using our signature scheme still significantly improves recall of potentially faster but more  X  X rittle X  hashing approaches such as I-Match. It also allows for a more effective and robust parameterization of
LSH which may decrease the amount of tuning required for these methods [1, 23].  X 
We provide a thorough experimental evaluation over a very challenging, manually curated  X  X old Set X  of near-duplicate news articles and over a large-scale Web corpus, compar-ing SpotSigs with state-of-the-art approaches such as Shin-gling and Jaccard/Cosine similarity for measuring docu-ment resemblance, as well as I-Match and LSH for efficient matching. Broder et al. [7] proposed a Shingling algorithm, coined DSC, as a method to detect near duplicates by comput-ing a sketch of the document. A subset of shingles, or n-grams, from each document is chosen as its sketch, and sim-ilarity between two documents is computed based on the common Jaccard overlap measure between these document sketches. To reduce the complexity of Shingling for pro-cessing large collections, the use of  X  X uper shingles X  (DSC-SS) was later proposed by Broder in [5]. DSC-SS makes use of meta-sketches, i.e., sketches of sketches, with only a minor decrease in result precision. Hod and Zobel [17] investigate a variety of approaches for filtering good shin-gles, while B  X  uttcher and Clarke [9] focus on information-theoretic measures such as Kullback-Leibler divergence in the more general context of search. Recently, Henzinger [16] combined two algorithms for detecting near-duplicate Web pages, namely Broder X  X  DSC and Charikar X  X  [10] random projection algorithm. Henzinger improved on precision com-pared to using the constituent algorithms individually. More-over, sophisticated clustering schemes allow for incorporat-ing additional knowledge in the form of explicit constraints to the clustering process [21, 29]. Incorporating informa-tion about document attributes or the content structure for near-duplicate clustering [30] has also been suggested.
Another scheme for detecting similar documents is finger-printing based on work by Manber [24], and subsequent work by Brin, Davis and Garcia-Molina [27], Garcia-Molina and Shivakumar [28], and more recently by Manku et al. [25]. A document fingerprint is typically a collection of integers that represent some key content in the document, wherein hashes of words or entire sentences are concatenated to some bit string to generate a characteristic fingerprint of the doc-ument. These fingerprinting schemes vary in the specific hash function used, as well as in the choice of strings used for hashing. Hash-value-based schemes like [7] pick strings whose hash values are multiples of an integer. Position-based schemes [4], on the other hand, select strings based on their offset in a document. Conrad et al. [14] and Chowdhury et al. [12] choose word strings with high inverse document frequency (IDF) [2] using collection statistics. Their I-Match algorithm [12, 22] makes use of external collection statistics and improved on recall by introducing multiple fingerprints (based on different lexica) per document.
 Locality Sensitive Hashing (LSH), proposed by Indyk and Motwani [15, 20], is an approximate similarity search tech-nique that scales to both large and high-dimensional data sets. LSH can be tuned by concatenating k signatures from each data object into a single hash value for high precision, and by combining matches over l such hashing steps X  X sing independent hash functions X  X or good recall. Min-hashing investigated by [6, 13, 18] has the interesting property that the probability of a match (i.e., a hash collision) between two data objects exactly confirms to the Jaccard similarity of their feature sets, which allows LSH in combination with Min-hashing to converge quickly to the full recall over near-duplicate documents with high Jaccard similarity. More self-tuning, iterative LSH approaches like LSH-Tree [3] or Hamming-LSH [13], however, typically increase the number of signature extraction and hashing steps involved. While behaving significantly more robust than a single LSH step, the increased amount of hashing may become the actual de-limiting factor for runtime performance.
The points (or  X  X pots X  ) in the page at which spot signa-tures are generated are all the locations where one out of a previously chosen set of anchor words occurs. We call the anchor words antecedents , which are typically chosen to be frequent within the corpus. The most obvious, largely domain-independent choices for natural-language text are stopwords , like is , the , do , have etc., which are likely to oc-cur in every document and whose occurrences are distributed widely and uniformly within any snippet of natural-language text. Hence spot signatures are expected to achieve more semantic-driven document surrogates than other signature schemes in that they tend to occur mostly in the natural-language passages of Web documents and skip over adver-tisements, banners, and the navigational components.
A spot signature s j of a location in a document simply consists of a chain of words that follow an antecedent word a at a fixed spot distance d j . We use the notation a j ( d to denote a spot signature that is computed by finding a contiguous spot chain of c j words, each of which is not itself a stopword. For example, the(2,3) denotes a spot signa-ture that is computed wherever the antecedent the occurs in a document. For a spot distance d = 2 and chain length c = 3, the signature would then consist of the second, forth and sixth word after the occurrence of this antecedent. If one of these words after the antecedent is itself a stopword, we move on to the next non-stopword and then continue building the chain. We also allow the chain to be cut off if the whole signature would exceed the boundary of the document and at least one non-stopword is found after the antecedent. Chains of spot signatures may also overlap if further antecedents are contained within the range of the previous chain.
 We hence call the quantity A = { a j ( d j ,c j ) } a spot set . We may apply multiple types of spot signatures to a single document. For example, is(3,1) , that(5,2) ,and is(2,1) are all proper spot signature specifications that can be applied to each document. When evaluating whether document B is a near duplicate of document A , their spot set resemblance is computed using the common Jaccard similarity measure. Consider the last sentence in each of the two pages of Figure 1:  X  X t a rally to kick off a weeklong campaign for the South Carolina primary, Obama tried to set the record straight from an attack circulating widely on the Internet that is designed to play into prejudices against Muslims and fears of terrorism. X 
Choosing the articles a , an , the and the verb is as an-tecedents with a uniform spot distance of 1 and chain length of 2, we obtain the set of spot signatures S = { a:rally:kick , a:weeklong:campain , the:south:carolina , the:record:straight , an:attack:circulating , the:internet:designed , is:designed:play which already characterizes the core content of the page very well. Note that the banners in Figures 1 and 2 hardly con-tain any of these otherwise frequent antecedents. Moreover, for the stock market example in Figure 2, no signatures at all would be extracted, such that the algorithm would con-servatively skip over considering this pair as duplicates.
Our key observation is that for any given similarity thresh-old  X  , we can provide tight upper bounds for the Jaccard similarity of two documents just by looking at a single doc-ument property each, e.g., by comparing the cardinality of their signature sets or the length of their signature vectors. This lets us drastically prune the search space between a) potentially matching documents and b) the vector dimen-sions at which we need to compare potential matches X  X f the similarity threshold is high. To address a), we show how to derive an optimal partitioning of the collection into buck-ets of potentially matching documents, which minimizes the size of the partitions while guaranteeing not to miss any po-tential match (thus ruling out false negatives and positives due to the matching procedure). As for b), we introduce an efficient approach for a two-dimensional inverted index pruning with early termination of the index traversals. Both steps are developed on the basis of the very same bounding approach for (multi-)set Jaccard similarity.

Even though the worst-case complexity of our algorithm remains quadratic, we empirically show that SpotSigs can outperform linear but more error-prone approaches such as LSH for sufficiently large ranges of similarity thresholds  X  . In contrast to LSH or I-Match, our approach is exact and self-tuning as it automatically adapts to the best-possible partitioning of the spot signature sets for any given threshold  X  , without the need for any further algorithm-specific tuning parameters.
Let sim ( A, B )= | A  X  B | / | A  X  B | be the default Jaccard similarity as defined over two sets A and B , each consisting of distinct spot signatures s j in our case. A simple, yet tight upper bound for the Jaccard similarity is since | A  X  B | X  min( | A | , | B | )and | A  X  B | X  max( | Without loss of generality, for | A | X | B | , we thus find:
We now switch to a vector representation of spot signa-tures for documents, where the vectors d 1 and d 2 correspond to the sets A and B , respectively. Two document vectors d d 2 have similar length if | d 1 | / | d 2 | X   X  ; hence only similar-length pairs can be near duplicates with respect to the pre-defined threshold  X  . That is, the bound in Inequality (2) tells us that if we would like to consider only document pairs d 1 ,d 2 with similarity sim ( d 1 ,d 2 )  X   X  , we can safely disre-gard any such pair where sim ( d 1 ,d 2 )  X | d 1 | / | d 2 is equivalent to omitting those pairs d 1 ,d 2 from the pair-wise vector comparisons, where without ever inspecting their spot signature sets or perform-ing any vector operations for comparison. This observation will be key for partitioning the collection, as it allows us to efficiently map documents into buckets of similar signature vector lengths in a single, linear pass through the collection.
The default Jaccard similarity is defined over binary vari-ables only, i.e., elements being present in a set or not. We are now interested in generalizing Jaccard to also take weighted variables, hence multi-sets of spot signatures, into account, continuing to maintain tight upper bounds for the similar-ity of two documents just given the lengths of their cor-responding signature vectors. The rationale behind this is that multi-sets of signatures may characterize a document better, since they allow for capturing the frequency of in-dividual signatures and hence the length of the document more accurately. For two multi-sets A and B , we define the weighted multi-set generalization of Jaccard to be where freq A ( s j ) denotes the frequency of SpotSig s j spot signature multi-set A .Then sim ( A, B ) can similarly to sim ( A, B ) be upper-bounded by: sim ( A, B )  X  Again without loss of generality, for | d 1 | = s and | d 2 | = s
Here, d 1 and d 2 are frequency-weighted vectors represent-ing multi-sets of spot signatures A and B , which allows for a compact (and sparse) representation of these multi-sets. Note that for the special case when A and B are sets, the two Inequalities (2) and (6) provide exactly the same bound for sim and sim , respectively; and in fact, both measures yield exactly the same similarity value in this case.
According to the pruning condition in Inequality (3), we may safely ignore all pairs of documents during the match-ing process whose SpotSig vectors exceed a certain difference in length. Consequently, before comparing any signature sets, we partition the documents into buckets of potentially matching pairs. Then only spot sets within the same or at most two subsequent buckets are near-duplicate candidates and need to be compared. We hence aim at finding a con-tiguous partitioning [ p k ,p k +1 ) over the discrete distribution of vector lengths | d i | X  [1 , X  ), such that all pairs d similar vector lengths are  X  X lose X  to each other. Note that it is not possible to eliminate all borderline cases for any arbi-trary distribution of | d i | by a single contiguous partitioning [ p ,p k +1 ) X  X here could always be a borderline pair that is highly similar but is spread across two different partitions X  but we can in fact find a partitioning such that any two similar documents can at most be located in two contiguous partitions. We thus define the following three conditions for an optimal partitioning of documents in a collection.
For a given range of vector lengths | d i | X  [1 , X  ], find a partitioning [ p k ,p k +1 )with1  X  p k  X   X  such that: (A) For all pairs d i ,d i with | d i | X | d i | , | d i | | d i | X  [ p k ,p k +1 ) , d i and d i belong to either the same parti-tion or two subsequent partitions, i.e., | d i | X  [ p k  X  1 (no false negatives) . (B) There is no pair d i ,d i with | d i | X | d i | , | d i and | d i | X  [ p k ,p k +1 ) , that is mapped into the same parti-tion, i.e., | d i | /  X  [ p k ,p k +1 ) (no false positives) . (C) The width of all partitions p k +1  X  p k is minimized (i.e., the amount of partitions is maximized) for 1  X  p k  X   X  with respect to conditions (A) and (B) (minimality) .

While (A) and (B) would suggest finding the partitions by looking at all combinations of documents, which would be expensive and could yield multiple valid partitions, there is only one optimal partitioning that minimizes the parti-tions X  widths as demanded by (C). This allows for a simple constructive (but approximate) solution, using the pruning condition as given by (3): Starting with p 0 =1 , for any given p , choose p k +1 as the smallest integer p k +1 &gt;p k such that p +1  X  p k &gt; (1  X   X  ) p k +1 ,while p k +1  X   X  .
Although the above series is not in closed-form, the p k values can easily be found numerically by iterating over the integer range [1 , X  ). Then, according to (3), any SpotSig vec-tor with length | d i | = p k canatmosthaveadistancetoa similar and larger vector d i of | d i | X  X  d i | &lt; (1  X  is the case when | d i | &lt;p k +1 and thus | d i | X  [ p Hence d i and d i will be located in the same partition in this case. More generally, for | d i | X  [ p k ,p k +1 ), any larger Spot-Sig vector d i with length difference of at most | d i | X  X  (1  X   X  ) | d versely, any shorter SpotSig vector d i with length differ-alengthof | d i | X  p k  X  1 .Hence any possible pair of simi-lar documents must either be within the same partition or within two immediate neighbor partitions as demanded by (A). Conversely, no pair of non-similar documents can be within the same partition as demanded by (B).

By minimizing the partition widths according to (C), we also minimize the number of documents being mapped into the same partition (and thus the number of potentially match-ing pairs). That is, for both variations of Jaccard simi-larity defined in Subsection 4.1, the partition boundaries p  X  [1 , X  ) are merely a function of the threshold  X  ,regard-less of the data. Note that this remains an approximate so-lution as there could still be a smaller partitioning that does not violate condition (B). However, with the above strategy, conditions (A) and (B) will be satisfied for any distribution of vector lengths | d i | as a consequence of the bound provided in (3), and the partitions can be created a priori without knowing the actual data. Furthermore, this approximation converges to X  X nd finally reaches X  X he optimal solution as the distribution gets more dense, i.e., it is equal to the opti-mal solution when all distinct vector lengths | d i | within the given range [1 , X  ) occur in the collection.

Further note that all documents d i , which exceed the ex-pected range of spot signature lengths  X  ,canbemapped into an additional partition [  X ,  X  )for | d i | X   X  .Then  X  is a fairly robust tuning parameter and can deliberately be cho-sen large (or with  X  being just above the maximum spot signature length of all documents in a given collection X  X f known a priori).
In addition to the above partitioning approach, which breaks the overall quadratic runtime into much smaller sets of candidates for pairwise comparisons, we can further ac-celerate the deduplication step within the partitions through the use of auxiliary inverted indexes. Using inverted indexes for the actual deduplication instead of the vectors themselves immediately reduces the amount of pairwise vector compar-isons to only those pairs within a partition that share at least one common spot signature. Furthermore, it allows for a novel and elegant form of index pruning, again exploit-ing the very basic similarity threshold as given by pruning condition (3).

That is, for each partition k and spot signature s ij in doc-ument d i , we store a list of pointers to all document vectors d i containing s ij (including d i itself) sorted in descending order of vector lengths | d i | . Then, starting with the least frequent signature s ij in d i , we start processing these in-verted lists until no yet unseen document d i can still match d i with high similarity, which lets us reduce the amount of necessary vector operations using several early breaking conditions. Altogether, the expected performance can be optimized by traversing the inverted lists for s ij in ascend-ing order of their length within each partition (hence by the document frequency of s ij in the given partition k )andby processing multiple documents from different partitions in parallel, e.g., in random order of vector lengths. The exact behavior is captured by Algorithm 1. Note that in doing so, SpotSigs already generates clustered output in the form of a complete near-duplicate graph, with any document being connected to all of its near duplicates, which might other-wise require an expensive post-processing step.
Algorithm 1 defines the exact behavior of the SpotSigs deduplication step, which is in principle a highly optimized nested loop with parallel processing of disjoint partitions and various break conditions.
 Algorithm 1 SpotSigs Deduplication
Here, the equality check d i = d i in line 13 may refer to both, equal documents identified by some id or memory lo-cation, or exact duplicates identified by a single checksum or hash code comparison. Furthermore, remembering the set of already checked documents checked i for each d i in lines 13 and 21 avoids potentially redundant Jaccard similarity com-parisons. These could otherwise arise due to multiple spot signatures pointing to the same document from different in-verted lists.

Crucial for good runtime performance are the continue and break conditions in lines 16, 18, and 26. That is, we can apply a two-dimensional pruning of the inverted index traversal using two additional bounds  X  1 and  X  2 for the dis-tance || d i | X  X  d i || of d i to any yet unseen document d cated in the same partition and/or inverted list. Thus, for each d i ,  X  1 is incremented by the frequency freq ( s ij nature s ij in d i whenever processing an index list list been finished; and  X  2 is updated to | d i | X  X  d i | at each access to the next d i in list kj .  X  1 then is the minimum-possible distance of d i  X  X  length to any other, yet unseen document in the same partition; and  X  2 is the distance to any other, yet unseen document in the same inverted list. Both in turn provide lower-bounds for the actual distances, which gives rise to multiple early breaking conditions:  X   X  1  X | d i | X  X  d i | for d i  X  partition k and d i /  X  checked | d i | X | d i | (line 26)  X   X  1 +  X  2  X | d i | X  X  d i | for d i  X  partition k and d i  X  list and | d i | X | d i | (line 18)  X   X  1  X   X  2  X | d i | X  X  d i | for d i  X  partition k and d i  X  list and | d i | &gt; | d i | (line 16)
The iteration into the next neighbor partition through line 31 will occur at most once per document and hardly affects runtime in practice. Note that the similarity check for any d i ,d i pair is symmetrical, hence it suffices to check each document only against its right neighbor partition.
Furthernotethatwesortall s ij  X  d i at most once by their local document frequency in partition k . Then the obtained ordering of signatures can be reused as approximation of the document frequencies in the next neighbor partition k +1 for the potential second iteration as well, which prevents us from the overhead of a second sorting. In practice, this strategy proved to outperform a sorting by global document frequencies, which would in turn be an approximation of the document frequencies in both the actual partition k as well as the neighbor partition k +1.
Suppose we have three documents d 1 = { s 1 :5 ,s 2 :4 ,s d = { s 1 :8 ,s 2 :4 } ,and d 3 = { s 1 :4 ,s 2 :5 ,s 3 :5 } | d | = 12, and | d 3 | = 14, a threshold of  X  =0 . 8, and all three documents are mapped into the same partition [ p k ,p k +1 We can now deduplicate, for example, d 1 using only a single pairwise Jaccard computation on top of the index structures as shown in Figure 3. Since s 3 is the least frequent signature in d 1 with respect to this partition, we first navigate into the inverted list for s 3 to find d 3 as a potential match. As this is the first list considered, we get  X  1 = 0, and in compari-son to d 3 we get  X  2 = | d 1 | X  X  d 3 | =  X  1, such that none of the continue/break conditions hold yet. We thus compute sim ( d 1 ,d 3 ) = (4+4+4) / (5+5+5) = 0 . 8 and indeed identify d 1 ,d 3 as a near-duplicate pair. Now, as we finish process-ing this list, we increment  X  1 by freq d 1 ( s 3 ) = 4, such that the break condition in line 26 already holds, which termi-nates the index processing for d 1 . Note that a subsequent processing of d 3 will not be dispensable, since d 3 might in turn be similar to documents that do not necessarily have to be similar to d 1 .
SpotSigs is implemented as a compact Java prototype in only about 500 lines of code. All experiments were per-formed on a dual Xeon-3000 quad-core CPU with 32 GB RAM. All processing steps for the SpotSigs, LSH, and I-Figure 3: Threshold-based inverted index pruning. Match implementations, like parsing, sorting, and cluster-ing are multi-threaded, using 8 threads to fully utilize the machine X  X  8 physical cores.

As for effectiveness, we report micro-averages for preci-sion, recall, and/or F1-measure [2] as quality measures. We distinguish absolute (i.e., user-perceived) precision and re-call values obtained from manual judgments for the Gold Set, thus showing the advantage of our signature extraction, and relative recall evaluated against a synthetic recall base for the much larger TREC collection, thus primarily aiming to show the advantage of the efficient SpotSigs matcher.
For all runs, we consider normalized IDF values computed separately for each signature scheme and corpus as idf j = log ( N/df j ) /log ( N ), where N is the corpus size, and df the document frequency of signature s j . The partitioning range  X  is fixed to 1,000 for all runs as well, creating a static partitioning of 501 partitions for  X  =1 . 0, still 41 partitions for  X  =0 . 9, and only 3 partitions for  X  =0 . 1. Asasimple form of noise reduction, all HTML markup is removed from both collections prior to signature extraction.  X 
SpotSigs  X  Spot signatures in combination with the efficient matching algorithm for multi-set Jaccard as described in
Section 4. SpotSigs by default performs a clustering step within each partition (on top of the inverted index struc-tures) to identify all near-duplicate pairs.  X 
LSH-S  X  Locality sensitive hashing (LSH) [20] using spot signatures (S) which in turn serve as input for computing the Min-hash [18] signatures. Since high-dimensional Min-hash permutations are impractical (and in fact intractable) for sparse data [18, 19], we use m random linear functions of the form  X  i ( x )=(  X  i  X  d +  X  i )mod P ,where  X  i and  X  are random integers drawn from the interval [0 ,D  X  1], and P is the first prime larger than the dimensionality D of the signature vectors, similarly to the approach inves-tigated by [6]. Candidates within each LSH bucket are post-processed by an additional pairwise clustering step (by calculating exact Jaccard similarities) to filter out false positives and provide pairwise output similar to SpotSigs.  X 
I-Match  X  The original I-Match algorithm [22] using a sin-gle SHA1 hash over a concatenation of tokens filtered by stopwords, special-character tokens, and IDF. Because of the single SHA1 hash used, I-Match does not need to fil-ter out false positives in an additional clustering step but merely returns all pairs of documents that get hashed to the same SHA1 bucket to create comparable output to
SpotSigs and LSH.  X 
I-Match-S  X  An improved I-Match algorithm using sequenc-es of spot signatures (S) instead of simple token sequences. The runtime of the hashing-based LSH and I-Match ap-proaches is linear in the number of documents n and dimen-sions m ,with O ( klnm ) for LSH and O ( nm ) for I-Match, versus O ( n 2 m ) in the worst case for SpotSigs. Despite our unfavorable asymptotical behavior, we show that we are em-pirically able to outperform the competitors in both runtime and recall for high similarity thresholds  X  .
Our first collection consists of 2,160 manually selected, near-duplicate news articles distilled from a large Stanford Web Base [11] crawl in 2006, with articles from SFGate.com , Herald.com , Chron.com , MCClatchy.com , etc. The articles have been clustered into 68 directories with an overall size of 102 MB. This  X  X old Set X  serves as our most valuable refer-ence collection, since these near-duplicates have been man-ually judged by human assessors which provides an ideal recall base from an IR point-of-view. The huge variations in the layouts used by different news sites makes this an extremely difficult setting for any deduplication algorithm. As shown in Figure 4, the macro-average Cosine similar-ity of documents (using single tokens with TF  X  IDF weights) across these directories is only 0.64. This demonstrates the difficulty in processing this manually selected collection of near-duplicate news articles, as even documents within the same directory vary substantially in their overall content. Figure 4: Average Cosine similarity within near-duplicate clusters in the Gold Set.

The processing time for parsing and extracting spot sig-natures from these 2,160 news articles is I/O bound and constantly takes 17 seconds for all algorithms. Runtime for the actual deduplication step is then almost negligible and takes 1.7 seconds at  X  =0 . 44 for SpotSigs and up to 9.4 sec-onds at  X  =0 . 4 for Shingling, each at their best F1 spots. That is, simple Shingling creates higher-dimensional signa-ture vectors than SpotSigs (denoted as #Spots in Table 1).
Figure 5 compares F1 for SpotSigs (using our best sig-nature scheme) against 3-Shingling and single-token signa-tures, each for Jaccard similarity and Cosine measure (with the latter using TF  X  IDF weights). Signatures exceeding an IDF range of [0 . 2 , 0 . 85] were filtered out for all competitors as an effective means of noise-and dimensionality reduction. Figure 5: SpotSigs vs. Shingling, each for Jaccard (left) and Cosine (right), as functions of  X  .

The plots show that SpotSigs with multi-set Jaccard con-sistently performs best overall. Using the spot signatures yields an impressive boost from 0.71 to 0.94 in F1 compared to 3-Singling for Jaccard, and an increase from 0.53 to 0.89 for Cosine. For Jaccard this means a 24 percent relative in-crease in F1, and even 40 percent for Cosine, respectively. 3-Shingling itself does not yield a conclusive improvement in combined recall and precision over single tokens for this hard setting. Note that using IDF statistics already provides a more reliable filtering than the mod-p shingling proposed by [7] that would simply keep only every p th shingle.
We now consider variations in the choice of SpotSigs an-tecedents, thus aiming to find a good compromise between extracting characteristic signatures while avoiding an over-fitting of these signatures to particular articles or sites. Fig-ure 6 shows that we obtain the best F1 result from a combi-nation of articles and flexions of the verbs be , can , will , have , do* , mostly occurring in text contents and less likely to oc-cur in ads or navigational banners. Using a full stopword list (e.g., the official 571 list used in SMART [8]) already tends to yield overly generic signatures but still performs significantly better than IDF-filtered Shingling. Similarly, we find a chain length of 3 and a uniform spot distance of 2 to slightly increase F1 by about 1 absolute percent each (figures omitted). Hence, although the spot signature ex-traction is highly tunable, it proves to be fairly robust for a variety of different stopword antecedents, spot distances, and chain lengths.
F1
Table 1 summarizes our results for SpotSigs versus the hashing-based LSH-S and I-Match(-S) variants. Recall that our entire matching approach requires only the configurable parameter  X  and an optional IDF range, whereas LSH-S additionally introduces k and l , and I-Match does not even allow for setting a similarity threshold.

For LSH-S, we keep the IDF range fixed to [0 . 2 , 0 . 85], thus using exactly the same spot signatures as input for both SpotSigs and LSH-S. With k fixed to 6, LSH-S needs about l = 64 different hash tables to converge to the same recall as SpotSigs (figures omitted). We choose l =32toachieve about 90 percent absolute recall on the Gold Set, which conforms to about 98 percent of relative recall compared to the SpotSigs baseline, and we use this setting also for our TREC experiments. Larger values of k , i.e., more specific Min-hash signatures, would in turn require larger values of l , i.e., more hash functions, for good recall.

For I-Match, we vary the bounds of the IDF interval, thus significantly reducing the size of the feature spaces. We achieve the best result for [0 . 4 , 0 . 75] (essentially confirming results from [12]), yielding 9,473 distinct spot signatures at a perfect precision of 1.0, but a recall of only 0.03, and F1 of 0.05. Particularly remarkable is that recall for I-Match increases by an order of magnitude when switching from term-based feature vectors to the spot signatures used for I-Match-S, with only a minor decrease in precision from 1.0 to 0.96. Overall, I-Match and I-Match-S achieve the highest ab-solute (i.e., user perceived) precision but remain inherently lossy in recall due to the single SHA1 hash function used (coined  X  X rittleness X  in [12]). As discussed in [22], I-Match recall may be improved by 40 X 60 percent by employing sev-eral underlying lexica, which is in principle a similar effect as the one LSH achieves by using multiple hash functions.
There is no manually curated gold set of duplicates for larger benchmark collections such as WT10g, consisting of 1.6 Mio documents in 10 GB text data. Thus, based on our Gold Set results, we employ I-Match-S as a highly precise but relative recall base of near duplicates to evaluate Spot-Sigs versus LSH with a focus on runtime. Furthermore, we limit this setting to documents with at least 5 spot signa-tures, i.e., documents with significant amounts of natural-language text. This reduces the amount of documents con-sidered from 1,691,565 to 1,171,960, as there are very many short  X  X unk X  documents in the collection, and deduplicating those is not in the scope of using spot signatures. We again observe constant parsing and signature extraction times of about 157 seconds for the WT10g collection.
Figure 7 shows that SpotSigs starts off by a factor of about 3 better runtimes than LSH-S at  X  =1 . 0, and it is still 2.6 times faster than LSH-S at  X  =0 . 9 (see Table 2 for details), until the plots cross at about  X  =0 . 78. This confirms to a deduplication runtime of only 14.2 seconds at  X  =1 . 0 and 17.1 seconds at  X  =0 . 9 for SpotSigs. We also see that the general runtime behavior of SpotSigs remains quadratic after all, as the number of partitions and threshold-based pruning effectiveness decreases with decreasing  X  .
Table 2 also depicts two more baseline runs for SpotSigs at  X  =0 . 9. Using only a single partition (NoPart) already ac-counts with an overall runtime of 196.7 seconds, whereas a run over this single partition without the threshold-based pruning (NoPrune) even takes 2.8 hours. Note that full pairwise vector comparisons without any index structure are clearly intractable for a collection this size, and a respective test run had to be stopped after several days. Figure 7: Runtime (left) and relative recall (right) for SpotSigs and LSH-S as functions of  X  ,usingI-Match-S as recall base.

For LSH-S, computing the k  X  l Min-hash signatures for each document consumes a major amount of time, even more than sorting the inverted lists in SpotSigs. Thus, both methods generally yield comparable runtimes when LSH-S is tuned to provide sufficient recall, e.g., at k = 6 and l =32, and the confidence threshold  X  for SpotSigs is reasonably high.

Memory consumption (MB) is computed as a lower bound for the data structures used, counting 4 bytes for an inte-ger and 2 bytes for a short integer (used for capturing the frequency values for weighted Jaccard). For example, for LSH-S with 25,033,143 overall spot signatures contained in all vectors, we thus get 25,033,143  X  (4+2) bytes for the Spot-Sig vectors, plus 32  X  1,171,960  X  4 bytes for the l Min-hashes, and 1,171,960  X  4 bytes for the document ids. SpotSigs, on the other hand, needs to keep the auxiliary inverted indexes in memory instead of the Min-hash signatures, which ac-count with one additional 4-byte pointer for each spot sig-nature. Tables 1 and 2 show that all setups considered here easily fit into the main memory of current machines.
SpotSigs proved to provide both increased robustness of signatures as well as highly efficient deduplication compared to various state-of-the-art approaches. We demonstrated that for a reasonable range of similarity thresholds, sim-ple vector-length comparisons may already yield a very good partitioning condition to circumvent the otherwise quadratic runtime behavior for this family of clustering algorithms. Moreover, unlike other approaches based on hashing, the SpotSigs deduplication algorithm runs  X  X ight out of the box X  without the need for further tuning, while remaining exact and efficient. For low similarity thresholds or very skewed distributions of document lengths, however, LSH remains the method-of-choice as it provides the most versatile and tunable toolkit for high-dimensional similarity search.
The SpotSigs matcher can easily be generalized toward more generic similarity search in metric spaces, whenever there is an effective means of bounding the similarity of two documents by a single property such as document or sig-nature length. Future work will focus on efficient access to disk-based index structures, as well as generalizing the bounding approach toward other metrics such as Cosine.
