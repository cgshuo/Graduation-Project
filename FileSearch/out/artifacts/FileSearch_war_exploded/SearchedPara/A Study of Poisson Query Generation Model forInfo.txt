 Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson dis-tribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods. We show that the Poisson model has several ad-vantages over the multinomial model, including naturally ac-commodating per-term smoothing and allowing for more ac-curate background modeling. We present several variants of the new model corresponding to different smoothing meth-ods, and evaluate them on four representative TREC test collections. The results show that while their basic mod-els perform comparably, the Poisson model can outperform multinomial model with per-term smoothing. The perfor-mance can be further improved with two-stage smoothing. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval Models General Terms: Algorithms Keywords: Language models, Poisson process, query gen-eration, formal models, term dependent smoothing
As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4]. Among many variants of language mod-els proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents. In such a model, given a query q and a document d , we com-pute the likelihood of  X  X enerating X  query q with a model estimated based on document d , i.e., the conditional prob-Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. ability p ( q | d ). We can then rank documents based on the likelihood of generating the query.

Virtually all the existing query generation language mod-els are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18]. The multino-mial distribution is especially popular and also shown to be quite effective. The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a nat-ural choice for modeling the occurrence of a particular word in a particular position in text. Compared with multivari-ate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms. However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivari-ate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommo-dating per-term smoothing and weighting. Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.

In this paper, we propose and study a new family of query generation models based on the Poisson distribution. In this new family of models, we model the frequency of each term independently with a Poisson distribution. To score a docu-ment, we would first estimate a multivariate Poisson model based on the document, and then score it based on the like-lihood of the query given by the estimated Poisson model. In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing. Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but with-out the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.

As in the existing work on multinomial language models, smoothing is critical for this new family of models. We de-rive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multi-nomial distributions. We find that while with some smooth-ing methods, the new model and the multinomial model lead to exactly the same formula, with some other smooth-ing methods they diverge, and the Poisson model brings in more flexibility for smoothing. In particular, a key difference is that the Poisson model can naturally accommodate per-term smoothing , which is hard to achieve with a multinomial model without heuristic twist of the semantics of a genera-tive model. We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model. This advantage is seen for both one-stage and two-stage smoothing. Another potential advantage of the Poisson model is that its corre-sponding background model for smoothing can be improved through using a mixture model that has a closed form for-mula. This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.
The rest of the paper is organized as follows. In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing meth-ods which lead to different retrieval functions. In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of re-trieval. We then design empirical experiments to compare the two families of language models in Section 4. We discuss the related work in 5 and conclude in 6.
In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document. In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution. Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20].
Let V = { w 1 , ..., w n } be a vocabulary set. Let w be a piece of text composed by an author and  X  c ( w 1 ) , ..., c ( w be a frequency vector representing w , where c ( w i , w ) is the frequency count of term w i in text w . In retrieval, w could be either a query or a document. We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.

Suppose t is the time period during which the author com-posed the text. With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occur-rences of w i , follows a Poisson distribution with associated parameter  X  i t , where  X  i is a rate parameter characterizing the expected number of w i in a unit time. The probability density function of such a Poisson Distribution is given by Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = | w | .
With n such independent Poisson processes, each explain-ing the generation of one term in the vocabulary, the likeli-hood of w to be generated from such Poisson processes can be written as p ( w |  X ) = where  X  = {  X  1 , ...,  X  n } and | w | = to these n independent Poisson processes with parameter  X  as a Poisson Language Model .

Let D = { d 1 , ..., d m } be an observed set of document sam-ples generated from the Poisson process above. The maxi-mum likelihood estimate (MLE) of  X  i is Note that this MLE is different from the MLE for the Pois-son distribution without considering the document lengths, which appears in [22, 24].

Given a document d , we may estimate a Poisson language model  X  d using d as a sample. The likelihood that a query q is generated from the document language model  X  d can be written as This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q , and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.
In practice, we have the flexibility to choose the vocabu-lary V . In one extreme, we can use the vocabulary of the whole collection. However, this may bring in noise and con-siderable computational cost. In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the non-query terms. As a compromise, we may conflate all the non-query terms as one single pseudo term. In other words, we may assume that there is exactly one  X  X on-query term X  in the vocabulary for each query. In our experiments, we adopt this  X  X seudo non-query term X  strategy.

A document can be scored with the likelihood in Equa-tion 1. However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero prob-ability to the term, causing the probability of the query to be zero. As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p (  X | d ).
In general, we want to assign non-zero rates for the query terms that are not seen in document d . Many smoothing methods have been proposed for multinomial language mod-els[2, 28, 29]. In general, we have to discount the probabili-ties of some words seen in the text to leave some extra prob-ability mass to assign to the unseen words. In Poisson lan-guage models, however, we do not have the same constraint as in a multinomial model (i.e., we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word. Instead, we only need to guarantee that In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions.

Following the risk minimization framework in [11], we as-sume that a document is generated by the arrival of terms in a time period of | d | according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e.,  X  d =  X   X  d, 1 , ...,  X  d, | V |  X  .
A document is assumed to be generated from a poten-tially different model. Given a particular document d, we want to estimate  X  d . The rate of a term is estimated inde-pendently of other terms. We use Bayesian estimation with the following Gamma prior, which has two parameters,  X  and  X  :
For each term w , the parameters  X  w and  X  w are chosen to be  X  w =  X   X   X  C,w and  X  w =  X  , where  X  is a parameter and  X  C,w is the rate of w estimated from some background language model, usually the  X  X ollection language model X . The posterior distribution of  X  d is given by which is a product of | V | Gamma distributions with param-eters c ( w, d ) +  X  X  C,w and | d | +  X  for each word w . Given  X 
This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28].
Another straightforward method is to decompose the query generation model as a mixture of two component models. One is the document language model estimated with max-imum likelihood estimator, and the other is a model esti-mated from the collection background, p (  X | C ), which assigns non-zero rate to w .

For example, we may use an interpolation coefficient be-tween 0 and 1 (i.e.,  X   X  [0 , 1]). With this simple interpola-tion, we can score a document with
Using the maximum likelihood estimator for p (  X | d ), we
We can also use a Poisson language model for p (  X | C ), or use some other frequency-based models. In the retrieval formula above, the first summation can be computed efficiently. The second summation can be actually treated as a document prior, which penalizes long documents.

As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo  X  X on-query-term X , denoted as  X  X  X . Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score ( d, q )  X 
As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query. In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model  X  d and the other being a query background language model p (  X | U ). p (  X | U ) models the  X  X ypical X  term frequencies in the user X  X  queries. We may then score each document with the query likelihood computed using the fol-lowing two-stage smoothing model: p ( c ( w, q ) |  X  d , U ) = (1  X   X  ) p ( c ( w, q ) |  X  d where  X  is a parameter, roughly indicating the amount of  X  X oise X  in q. This looks similar to the interpolation smooth-ing, except that p (  X |  X  d ) now should be a smoothed language model, instead of the one estimated with MLE.

With no prior knowledge on p (  X | U ), we could set it to p (  X | C ). Any smoothing methods for the document language model can be used to estimate p (  X | d ) such as the Gamma smoothing as discussed in Section 2.2.1.

The empirical study of the smoothing methods is pre-sented in Section 4.
From the previous section, we notice that the Poisson lan-guage model has a strong connection to the multinomial lan-guage model. This is expected since they both belong to the exponential family [26]. However, there are many differences when these two families of models are applied with differ-ent smoothing methods. From the perspective of retrieval, will these two language models perform equivalently? If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits? In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models.
Let us begin with the assumption that all the query terms appear in every document. Under this assumption, no smooth-ing is needed. A document can be scored by the log likeli-hood of the query with the maximum likelihood estimate: This is exactly the log likelihood of the query if the docu-ment language model is a multinomial with maximum likeli-hood estimate. Indeed, even with Gamma smoothing, when tion 5, it is easy to show that which is exactly the Dirichlet retrieval formula in [28]. Note that this equivalence holds only when the document length variation is modeled with Poisson process.

This derivation indicates the equivalence of the basic Pois-son and multinomial language models for retrieval. With other smoothing strategies, however, the two models would be different. Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored. Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model. In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for explor-ing advanced techniques on various retrieval features, which could not be achieved with multinomial language models.
One flexibility of the Poisson language model is that it provides a natural framework to accommodate term depen-dent (per-term) smoothing. Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discrimi-native the query terms are. [7] also predicted that differ-ent terms should have a different smoothing weights. With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29]. This parameter can be made specific for different queries, but always has to be a constant for all the terms. This is mandatory since a multinomial language model has the constraint that perspective, different terms may need to be smoothed dif-ferently even if they are in the same query. For example, a non-discriminative term (e.g.,  X  X he X ,  X  X s X ) is expected to be explained more with the background model, while a content term (e.g.,  X  X etrieval X ,  X  X ush X ) in the query should be ex-plained with the document model. Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e.,  X  in Formula 2 and Formula 3) specifically for each term. Since the Poisson language model does not have the  X  X um-to-one X  constraint across terms, it can easily accom-modate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models. Below we present a possi-ble way to explore term dependent smoothing with Poisson language models.

Essentially, we want to use a term-specific smoothing co-efficient  X  in the linear combination, denoted as  X  w . This co-efficient should intuitively be larger if w is a common word and smaller if it is a content word. The key problem is to find a method to assign reasonable values to  X  w . Empirical tun-ing is infeasible for so many parameters. We may instead estimate the parameters  X  X  X  = {  X  1 , ...,  X  | V | }  X  by maximiz-ing the likelihood of the query given the mixture model of p ( q |  X  Q ) and p ( q | U ), where  X  Q is the  X  X rue X  query model to generate the query and p ( q | U ) is a query background model as discussed in Section 2.2.3.

With the model p ( q |  X  Q ) hidden, the query likelihood is p ( q |  X  , U ) = Z If we have relevant documents for each query, we can ap-proximate the query model space with the language models of all the relevant documents. Without relevant documents, we opt to approximate the query model space with the mod-els of all the documents in the collection. Setting p (  X | U ) as p (  X | C ), the query likelihood becomes p ( q |  X  , U ) = guage model for document d .

If we have prior knowledge on p (  X   X  d | U ), such as which doc-uments are relevant to the query, we can set  X  d accordingly, because what we want is to find  X  that can maximize the likelihood of the query given relevant documents. Without this prior knowledge, we can leave  X  d as free parameters, and use the EM algorithm to estimate  X  d and  X . The updating functions are given as and
As discussed in [29], we only need to run the EM algo-rithm for several iterations, thus the computational cost is relatively low. We again assume our vocabulary containing all query terms plus a pseudo non-query term. Note that the function does not give an explicit way of estimating the co-efficient for the unseen non-query term. In our experiments, we set it to the average over  X  w of all query terms.
With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for ver-bose queries, where the query terms have various discrimi-native values. In Section 4, we use empirical experiments to prove this hypothesis.
Another flexibility is to explore different background (col-lection) models (i.e., p (  X | U ), or p (  X | C )). One common as-sumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29]. Similarly, we can also make the assumption that the collection model is a Poisson lan-guage model, with the rates  X  C,w = this assumption usually does not hold, since the collection is far more complex than a single document. Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc. Treating the col-lection model as a mixture of document models, instead of a single  X  X seudo-document model X  is more reasonable. Ex-isting work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27]. All the approaches can be easily adopted using Poisson language models. How-ever, a common problem of these approaches is that they all require heavy computation to construct the background model. With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.

Poisson Mixture [3] has been proposed to model a collec-tion of documents, which can fit the data much better than a single Poisson. The basic idea is to assume that the collec-tion is generated from a mixture of Poisson models, which has the general form of p (  X |  X  ) is a single Poisson model and p (  X  ) is an arbitrary prob-ability density function. There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katz X  X  K-Mixture [9]. Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].

All these mixtures have closed forms, and can be esti-mated from the collection of documents efficiently. This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval. For example, the prob-ability density function of Katz X  X  K-Mixture is given as p ( c ( w ) = k |  X  w ,  X  w ) = (1  X   X  w )  X  k, 0 +  X  w where  X  k, 0 = 1 when k = 0, and 0 otherwise.

With the observation of a collection of documents,  X  w and  X  w can be estimated as where cf ( w ) and df ( w ) are the collection frequency and document frequency of w , and N is the number of docu-ments in the collection. To account for the different docu-ment lengths, we assume that  X  w is a reasonable estimation for generating a document of the average length, and use  X  model can be easily used to replace P (  X | C ) in the retrieval functions 3 and 4.
In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages. For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization. Intuitively, when the docu-ment has more unique words, it will be penalized more. On the other hand, if a document is exactly n copies of another document, it would not get over penalized. This feature is desirable and not achieved with the Dirichlet model [5]. Po-tentially, this component could penalize a document accord-ing to what types of terms it contains. With term specific settings of  X  , we could get even more flexibility for document length normalization.

Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage. With model-based feedback, we could again relax the combi-nation coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model. We could also utilize the  X  X elevant X  documents to learn better per-term smoothing coefficients.
In Section 3, we analytically compared the Poisson lan-guage models and multinomial language models from the perspective of query generation and retrieval. In this sec-tion, we compare these two families of models empirically. Experiment results show that the Poisson model with per-term smoothing outperforms multinomial model, and the performance can be further improved with two-stage smooth-ing. Using Poisson mixture as background model also im-proves the retrieval performance.
Since retrieval performance could significantly vary from one test collection to another, and from one query to an-other, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web). To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence de-scription), and long-verbose (LV, multiple sentences) queries. The documents are stemmed with the Porter X  X  stemmer, and we do not remove any stop word. For each parameter, we vary its value to cover a reasonably wide range.
We compare the performance of the Poisson retrieval mod-els and multinomial retrieval models using interpolation (Jelinek-Mercer, JM) smoothing and Bayesian smoothing with con-jugate priors. Table 1 shows that the two JM-smoothed models perform similarly on all data sets. Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same re-trieval formula, the performance of these two models are jointly presented. We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing meth-ods. The parameter sensitivity curves for two Jelinek-Mercer Figure 1: Poisson and multinomial performs simi-larly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1. Clearly, these two methods perform similarly either in terms of optimality or sensitivity. This similarity of performance is expected as we discussed in Section 3.1.

Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smooth-ing methods, the Poisson model has great potential and flexibility to be further improved. As shown in the right-most column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries. This model is given in For-mula 4, with a Gamma smoothing for the document model p (  X | d ), and  X  w , which is term dependent. The parameter  X  of the first stage Gamma smoothing is empirically tuned. The combination coefficients (i.e.,  X ), are estimated with the EM algorithm in Section 3.2. The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smooth-ing model are plotted in Figure 2. The per-term two-stage smoothing method is less sensitive to the parameter  X  than Dirichlet/Gamma, and yields better optimal performance. Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma
In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models.
To test the effectiveness of the term dependent smooth-ing, we conduct the following two experiments. In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a  X  w for each unique term. Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p (  X | d ) to be zero. We then use a simple Laplace method to slightly smooth the document model be-fore it goes into the EM iterations. The documents are then still scored with Formula 3, but using learnt  X  w . The results are labeled with  X  X M+L. X  in Table 2.
 Table 2: Term dependent smoothing improves re-trieval performance With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases. However, in some cases (e.g., Trec7/SV), it performs poorly. This might be caused by the problem of EM estimation with unsmoothed document models. Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved signif-icantly. This indicates that there is still room to find better methods to estimate  X  w . Please note that neither the per-term JM method nor the  X  X M+L. X  method has a parameter to tune.

As shown in Table 1, the term dependent two-stage smooth-ing can significantly improve retrieval performance. To un-derstand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing frame-work, we design another experiment to compare the per-term two-stage smoothing with the two-stage smoothing method proposed in [29]. Their method managed to find coefficients specific to the query, thus a verbose query would use a higher  X  . However, since their model is based on multi-nomial language modeling, they could not get per-term co-efficients. We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms. We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2. Again, we see that the  X  X er-term X  two-stage smoothing outperforms the  X  X er-query X  two-stage smoothing, especially for verbose queries. The improvement is not as large as how the per-term smoothing method improves over Dirichlet/Gamma. This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent. This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial. In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method.
In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multino-mial models. Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p (  X | C ), we use Katz X  X  K-Mixture model, which is essentially a mixture of Poisson distributions. p (  X | C ) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.
 Table 3: K-Mixture background model improves re-trieval performance The performance of the JM retrieval model with single Poisson background and with Katz X  X  K-Mixture background model is compared in Table 3. Clearly, using K-Mixture to model the background model outperforms the single Pois-son background model in most cases, especially for verbose queries where the improvement is statistically significant.
Figure 3 shows that the performance changes over differ-ent parameters for short verbose queries. The model using K-Mixture background is less sensitive than the one using single Poisson background. Given that this type of mixture Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Bi-nomial, could help the performance.
To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.
Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4]. The most popular and fun-damental one is the query-generation language model [21, 13]. All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multi-variate Bernoulli distribution [21, 17, 18]. We introduce a new family of language models, based on Poisson distribu-tion. Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three dif-ferent retrieval models which is related to our comparison of Poisson and multinomial. However, the Poisson model in their paper is still under the document generation frame-work, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents. Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katz X  X  K-Mixture [9] has shown to be effective to model and retrieve documents. Once again, none of this work explores Poisson distribution in the query generation framework.

Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multi-nomial language models. [7] analytically shows that term specific smoothing could be useful. We show that Pois-son language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a gen-erative model, and is able to efficiently better model the mixture background, both analytically and empirically.
We present a new family of query generation language models for retrieval based on Poisson distribution. We de-rive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing. We compare the new models with the popular multinomial retrieval models both analytically and experimentally. Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences. In par-ticular, we show that Poisson has an advantage over multino-mial in naturally accommodating per-term smoothing. We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models. Furthermore, we show that a mix-ture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model. Our work opens up many interesting di-rections for further exploration in this new family of models. Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work. It is also appealing to find ro-bust methods to learn the per-term smoothing coefficients without additional computation cost.

We thank the anonymous SIGIR 07 reviewers for their useful comments. This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. [1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [2] S. F. Chen and J. Goodman. An empirical study of [3] K. Church and W. Gale. Poisson mixtures. Nat. Lang. [4] W. B. Croft and J. Lafferty, editors. Language [5] H. Fang, T. Tao, and C. Zhai. A formal study of [6] D. Hiemstra. Using Language Models for Information [7] D. Hiemstra. Term-specific smoothing for the [8] T. Hofmann. Probabilistic latent semantic indexing. [9] S. M. Katz. Distribution of content words and phrases [10] O. Kurland and L. Lee. Corpus structure, language [11] J. Lafferty and C. Zhai. Document language models, [12] J. Lafferty and C. Zhai. Probabilistic IR models based [13] J. Lafferty and C. Zhai. Probabilistic relevance models [14] V. Lavrenko and B. Croft. Relevance-based language [15] X. Liu and W. B. Croft. Cluster-based retrieval using [16] E. L. Margulis. Modelling documents with multiple [17] A. McCallum and K. Nigam. A comparison of event [18] D. Metzler, V. Lavrenko, and W. B. Croft. Formal [19] D. H. Miller, T. Leek, and R. Schwartz. A hidden [20] A. Papoulis. Probability, random variables and [21] J. M. Ponte and W. B. Croft. A language modeling [22] S. Robertson and S. Walker. Some simple effective [23] S. E. Robertson, S. Walker, S. Jones, [24] T. Roelleke and J. Wang. A parallel derivation of [25] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language [26] J. Teevan and D. R. Karger. Empirical development of [27] X. Wei and W. B. Croft. Lda-based document models [28] C. Zhai and J. Lafferty. A study of smoothing [29] C. Zhai and J. Lafferty. Two-stage language models
