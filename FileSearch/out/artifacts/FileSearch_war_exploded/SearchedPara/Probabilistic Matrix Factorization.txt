 One of the most popular approaches to collaborative filterin g is based on low-dimensional factor models. The idea behind such models is that attitudes or pref erences of a user are determined by a small number of unobserved factors. In a linear factor mode l, a user X  X  preferences are modeled by linearly combining item factor vectors using user-speci fic coefficients. For example, for N users and M movies, the N  X  M preference matrix R is given by the product of an N  X  D user coefficient matrix U T and a D  X  M factor matrix V [7]. Training such a model amounts to finding the best rank-D approximation to the observed N  X  M target matrix R under the given loss function. A variety of probabilistic factor-based models has been pro posed recently [2, 3, 4]. All these models can be viewed as graphical models in which hidden factor vari ables have directed connections to variables that represent user ratings. The major drawback o f such models is that exact inference is intractable [12], which means that potentially slow or inac curate approximations are required for computing the posterior distribution over hidden factors i n such models.
 Low-rank approximations based on minimizing the sum-squar ed distance can be found using Sin-gular Value Decomposition (SVD). SVD finds the matrix  X  R = U T V of the given rank which min-imizes the sum-squared distance to the target matrix R . Since most real-world datasets are sparse, most entries in R will be missing. In those cases, the sum-squared distance is computed only for the observed entries of the target matrix R . As shown by [9], this seemingly minor modification results in a difficult non-convex optimization problem whic h cannot be solved using standard SVD implementations.
 [10] proposed penalizing the norms of U and V . Learning in this model, however, requires solv-ing a sparse semi-definite program (SDP), making this approa ch infeasible for datasets containing millions of observations. Many of the collaborative filtering algorithms mentioned ab ove have been applied to modelling user ratings on the Netflix Prize dataset that contains 480,1 89 users, 17,770 movies, and over 100 million observations (user/movie/rating triples). Howev er, none of these methods have proved to be particularly successful for two reasons. First, none of t he above-mentioned approaches, except for the matrix-factorization-based ones, scale well to lar ge datasets. Second, most of the existing algorithms have trouble making accurate predictions for us ers who have very few ratings. A common practice in the collaborative filtering community is to remo ve all users with fewer than some minimal number of ratings. Consequently, the results reported on th e standard datasets, such as MovieLens and EachMovie, then seem impressive because the most difficu lt cases have been removed. For example, the Netflix dataset is very imbalanced, with  X  X nfre quent X  users rating less than 5 movies, while  X  X requent X  users rating over 10,000 movies. However, since the standardized test set includes the complete range of users, the Netflix dataset provides a mu ch more realistic and useful benchmark for collaborative filtering algorithms.
 The goal of this paper is to present probabilistic algorithm s that scale linearly with the number of observations and perform well on very sparse and imbalanced datasets, such as the Netflix dataset. In Section 2 we present the Probabilistic Matrix Factorizat ion (PMF) model that models the user preference matrix as a product of two lower-rank user and mov ie matrices. In Section 3, we extend the PMF model to include adaptive priors over the movie and us er feature vectors and show how these priors can be used to control model complexity automat ically. In Section 4 we introduce a constrained version of the PMF model that is based on the assu mption that users who rate similar sets of movies have similar preferences. In Section 5 we repo rt the experimental results that show that PMF considerably outperforms standard SVD models. We a lso show that constrained PMF and PMF with learnable priors improve model performance signifi cantly. Our results demonstrate that constrained PMF is especially effective at making better pr edictions for users with few ratings. Suppose we have M movies, N users, and integer rating values from 1 to K 1 . Let R the rating of user i for movie j , U  X  R D  X  N and V  X  R D  X  M be latent user and movie feature matrices, with column vectors U vectors respectively. Since model performance is measured by computing the root mean squared error (RMSE) on the test set we first adopt a probabilistic lin ear model with Gaussian observation noise (see fig. 1, left panel). We define the conditional distr ibution over the observed ratings as variance  X  2 , and I 0 otherwise. We also place zero-mean spherical Gaussian pri ors [1, 11] on user and movie feature vectors: The log of the posterior distribution over the user and movie features is given by ln p ( U, V | R,  X  2 ,  X  2 V ,  X  2 U ) =  X  where C is a constant that does not depend on the parameters. Maximiz ing the log-posterior over movie and user features with hyperparameters (i.e. the obse rvation noise variance and prior vari-quadratic regularization terms: where  X  of the objective function given by Eq. 4 can be found by perfor ming gradient descent in U and V . have been observed, the objective given by Eq. 4 reduces to th e SVD objective in the limit of prior variances going to infinity.
 In our experiments, instead of using a simple linear-Gaussi an model, which can make predictions outside of the range of valid rating values, the dot product b etween user-and movie-specific feature predictions: that the range of valid rating values matches the range of pre dictions our model makes. Minimizing the objective function given above using steepest descent t akes time linear in the number of obser-vations. A simple implementation of this algorithm in Matla b allows us to make one sweep through the entire Netflix dataset in less than an hour when the model b eing trained has 30 factors. Capacity control is essential to making PMF models generali ze well. Given sufficiently many fac-tors, a PMF model can approximate any given matrix arbitrari ly well. The simplest way to control the capacity of a PMF model is by changing the dimensionality of feature vectors. However, when the dataset is unbalanced, i.e. the number of observations d iffers significantly among different rows or columns, this approach fails, since any single number of f eature dimensions will be too high for some feature vectors and too low for others. Regularization parameters such as  X  above provide a more flexible approach to regularization. Pe rhaps the simplest way to find suitable setting of the parameters in the set, and choose the model tha t performs best on the validation set. The main drawback of this approach is that it is computationa lly expensive, since instead of training a single model we have to train a multitude of models. We will s how that the method proposed by [6], originally applied to neural networks, can be used to de termine suitable values for the regular-ization parameters of a PMF model automatically without sig nificantly affecting the time needed to train the model. As shown above, the problem of approximating a matrix in the L matrices that are regularized by penalizing their Frobeniu s norm can be viewed as MAP estimation in a probabilistic model with spherical Gaussian priors on t he rows of the low-rank matrices. The complexity of the model is controlled by the hyperparameter s: the noise variance  X  2 and the the parameters of the priors (  X  2 mizing the log-posterior of the model over both parameters a nd hyperparameters as suggested in [6] allows model complexity to be controlled automatically bas ed on the training data. Using spherical priors for user and movie feature vectors in this framework l eads to the standard form of PMF with  X  are more sophisticated than the simple penalization of the F robenius norm of the feature matrices. For example, we can use priors with diagonal or even full cova riance matrices as well as adjustable means for the feature vectors. Mixture of Gaussians priors c an also be handled quite easily. In summary, we find a point estimate of parameters and hyperpa rameters by maximizing the log-posterior given by where  X  spectively and C is a constant that does not depend on the parameters or hyperp arameters. When the prior is Gaussian, the optimal hyperparameters can be found in closed form if the movie and user feature vectors are kept fixed. Thus to simplify lear ning we alternate between optimizing the hyperparameters and updating the feature vectors using steepest ascent with the values of hy-perparameters fixed. When the prior is a mixture of Gaussians , the hyperparameters can be updated by performing a single step of EM. In all of our experiments we used improper priors for the hy-perparameters, but it is easy to extend the closed form updat es to handle conjugate priors for the hyperparameters. to the prior mean, or the average user, so the predicted ratin gs for those users will be close to the movie average ratings. In this section we introduce an addit ional way of constraining user-specific feature vectors that has a strong effect on infrequent users .
 Let W  X  R D  X  M be a latent similarity constraint matrix. We define the featu re vector for user i as: where I is the observed indicator matrix with I Y can be seen as the offset added to the mean of the prior distrib ution to get the feature vector U for the user i . In the unconstrained PMF model U at zero (see fig. 1). We now define the conditional distributio n over the observed ratings as We regularize the latent similarity constraint matrix W by placing a zero-mean spherical Gaussian prior on it: squared errors function with quadratic regularization ter ms: with  X  V , and W to minimize the objective function given by Eq. 10. The train ing time for the constrained PMF model scales linearly with the number of observations, w hich allows for a fast and simple implementation. As we show in our experimental results sect ion, this model performs considerably better than a simple unconstrained PMF model, especially on infrequent users. 5.1 Description of the Netflix Data According to Netflix, the data were collected between Octobe r 1998 and December 2005 and repre-of 100,480,507 ratings from 480,189 randomly-chosen, anon ymous users on 17,770 movie titles. addition to the training and validation data, Netflix also pr ovides a test set containing 2,817,131 user/movie pairs with the ratings withheld. The pairs were s elected from the most recent ratings for plagues many empirical comparisons in the machine learning literature, performance is assessed by submitting predicted ratings to Netflix who then post the roo t mean squared error (RMSE) on an on the same data, which is 0.9514.
 To provide additional insight into the performance of diffe rent algorithms we created a smaller and much more difficult dataset from the Netflix data by randomly s electing 50,000 users and 1850 movies. The toy dataset contains 1,082,982 training and 2,4 62 validation user/movie pairs. Over 50% of the users in the training dataset have less than 10 rati ngs. 5.2 Details of Training To speed-up the training, instead of performing batch learn ing we subdivided the Netflix data into mini-batches of size 100,000 (user/movie/rating triples) , and updated the feature vectors after each mini-batch. After trying various values for the learning ra te and momentum and experimenting with of parameters worked well for all values of D we have tried. 5.3 Results for PMF with Adaptive Priors To evaluate the performance of PMF models with adaptive prio rs we used models with 10D features. This dimensionality was chosen in order to demonstrate that even when the dimensionality of fea-tures is relatively low, SVD-like models can still overfit an d that there are some performance gains to be had by regularizing such models automatically. We comp ared an SVD model, two fixed-prior PMF models, and two PMF models with adaptive priors. The SVD m odel was trained to minimize the sum-squared distance only to the observed entries of the target matrix. The feature vectors of the SVD model were not regularized in any way. The two fixed-pr ior PMF models differed in their regularization parameters: one (PMF1) had  X  had  X  sian priors with spherical covariance matrices on user and m ovie feature vectors, while the second model (PMFA2) had diagonal covariance matrices. In both cas es, the adaptive priors had adjustable means. Prior parameters and noise covariances were updated after every 10 and 100 feature matrix updates respectively. The models were compared based on the RMSE on the validation set. The results of the comparison are shown on Figure 2 (left pane l). Note that the curve for the PMF with diagonal covariances. Comparing models based on the lo west RMSE achieved over the time of training, we see that the SVD model does almost as well as the m oderately regularized PMF model (PMF2) (0.9258 vs. 0.9253) before overfitting badly towards the end of training. While PMF1 does not overfit, it clearly underfits since it reaches the RMS E of only 0.9430. The models with adaptive priors clearly outperform the competing models, a chieving the RMSE of 0.9197 (diagonal covariances) and 0.9204 (spherical covariances). These re sults suggest that automatic regularization through adaptive priors works well in practice. Moreover, o ur preliminary results for models with higher-dimensional feature vectors suggest that the gap in performance due to the use of adaptive covariance matrices did not lead to a significant improvemen t over the spherical covariance matrices, diagonal covariances might be well-suited for automatical ly regularizing the greedy version of the PMF training algorithm, where feature vectors are learned o ne dimension at a time. 5.4 Results for Constrained PMF For experiments involving constrained PMF models, we used 3 0D features ( D = 30) , since this choice resulted in the best model performance on the validat ion set. Values of D in the range of [20 , 60] produce similar results. Performance results of SVD, PMF, a nd constrained PMF on the toy dataset are shown on Figure 3. The feature vectors were in itialized to the same values in all three models. For both PMF and constrained PMF models the reg ularization parameters were set to  X 
U =  X  Y =  X  V =  X  W = 0 . 002 strained PMF model performs much better and converges consi derably faster than the unconstrained PMF model. Figure 3 (right panel) shows the effect of constra ining user-specific features on the predictions for infrequent users. Performance of the PMF mo del for a group of users that have fewer always predicts the average rating of each movie. The constr ained PMF model, however, performs considerably better on users with few ratings. As the number of ratings increases, both PMF and constrained PMF exhibit similar performance.
 One other interesting aspect of the constrained PMF model is that even if we know only what movies the user has rated, but do not know the values of the ratings, t he model can make better predictions than the movie average model. For the toy dataset, we randoml y sampled an additional 50,000 users, and for each of the users compiled a list of movies the user has rated and then discarded the actual ratings. The constrained PMF model achieved a RMSE of 1.0510 on the validation set compared to a RMSE of 1.0726 for the simple movie average model. This ex periment strongly suggests that knowing only which movies a user rated, but not the actual rat ings, can still help us to model that user X  X  preferences better. the PMF and constrained PMF models the regularization param eters were set to  X   X 
W = 0 . 001 unconstrained PMF model, achieving a RMSE of 0 . 9016 . A simple SVD achieves a RMSE of about 0.9280 and after about 10 epochs begins to overfit. Figure 4 (l eft panel) shows that the constrained PMF model is able to generalize considerably better for user s with very few ratings. Note that over 10% of users in the training dataset have fewer than 20 rating s. As the number of ratings increases, the effect from the offset in Eq. 7 diminishes, and both PMF an d constrained PMF achieve similar performance.
 user/movie pairs occur in the test set, so we have an addition al category: movies that were viewed but for which the rating is unknown. This is a valuable source of information about users who occur The constrained PMF model can easily take this information i nto account. Figure 4 (right panel) shows that this additional source of information further im proves model performance.
 When we linearly combine the predictions of PMF, PMF with a le arnable prior, and constrained PMF, we achieve an error rate of 0.8970 on the test set . When the predictions of multiple PMF models are linearly combined with the predictions of multip le RBM models, recently introduced by [8], we achieve an error rate of 0.8861, that is nearly 7% be tter than the score of Netflix X  X  own system. In this paper we presented Probabilistic Matrix Factorizat ion (PMF) and its two derivatives: PMF with a learnable prior and constrained PMF. We also demonstr ated that these models can be effi-ciently trained and successfully applied to a large dataset containing over 100 million movie ratings. Efficiency in training PMF models comes from finding only poin t estimates of model parameters and hyperparameters, instead of inferring the full posteri or distribution over them. If we were to take a fully Bayesian approach, we would put hyperpriors ove r the hyperparameters and resort to MCMC methods [5] to perform inference. While this approach i s computationally more expensive, preliminary results strongly suggest that a fully Bayesian treatment of the presented PMF models would lead to a significant increase in predictive accuracy.
 We thank Vinod Nair and Geoffrey Hinton for many helpful disc ussions. This research was sup-ported by NSERC.

