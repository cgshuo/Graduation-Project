 1. Introduction
This paper addresses research problems about the generality of the findings of previous research works automatically by extracting terms from relevant documents. If the query formulation uses full relevance infor-mation, the query terms will be extracted from the whole set of relevant documents. Similarly, if query formu-lation uses partial relevance information, the query terms will be extracted from a restricted subset of relevant documents. Effective query formulation finds its immediate applications in relevance feedback (RF) ( Ruthven &amp; Lalmas, 2003 ); previous research works and our work in this paper investigate query formulation in such an application context.
 Previous informative research works (e.g., Haines &amp; Croft, 1993 ) used collections that were popular then.
These collections became less popular with the advent of the (D)ARPA/NIST Text REtrieval Conferences (TRECs). Some of the previous works used small document collections (e.g., Lopez-Pujalte, Guerrero-Bote, of query formulation algorithms. Small collections have the associated, well-known problems that restrict their experimental findings to such test collections. Therefore, there is a need to carry out a confirmation study for larger collections.
 Buckley et al. (1995) used the TREC-3 ad hoc test collection which is larger than previous works (e.g., Lopez-
Pujalte, Guerrero-Bote, &amp; Anegon, 2003 ). More recently, Spa  X  rck Jones, Walker, and Robertson (2000a) com-bined the use of TREC-2 and TREC-3 ad hoc test collections for retrieval based on full relevance information (i.e., retrospective experiments). However, TREC-2 and TREC-3 title queries are relatively long compared with the later TREC ad hoc test collections (e.g., TREC-6), because these TREC collections may have been designed for other tasks (e.g., routing). This means that benchmarking the performance of RF is still needed for those who evic, 2002 ). The Reliable Information Access (RIA) workshop ( Buckley &amp; Harman, 2003 ) has also benchmarked the retrieval effectiveness of the vector space retrieval model (VSM) using automatically formulated queries, in which terms are extracted from relevant documents. The resultant mean average precision (MAP) is as high as 80+%. The generated queries based on full relevance information may contain terms that occur only in a single (relevant) document (i.e., their document frequencies are one). Such terms tend to have large inverse document rithm during RF. Therefore, it is conceivable that good retrieval effectiveness is obtained, because such query terms act almost like document identifiers picking a relevant document by each of such query terms. It is not known to what extent the high retrieval effectiveness RF is due to such query terms. Consequently, there is still a need to benchmark retrieval effectiveness performance of existing retrieval models using partial and full rele-vance information in a RF environment.

We are motivated by the importance of benchmarking using existing retrieval models with partial and full relevance information in a RF environment. This is because the performance level of benchmarking usually cannot be applied across different collections. Therefore, setting up a level of performance of comparison is necessary. Typical comparison is done with a baseline that specifies the current performance, and the upper bound performance is seldom provided. This means that we do not have an idea about how much more improvement can be reasonably expected through further studies. Using full relevance information, the prac-be estimated, since most retrieval models perform well with full relevance information. Such a benchmark may be used for comparing with the retrieval effectiveness of RF as well. The difference between performance with partial and full relevance information indicates whether there is room for improvement. A systematic study of query formulation using partial relevance information can also show which factors have major impact on retrieval effectiveness. This shows the important research areas to focus on for effective retrieval. To support a systematic study, a framework on how to add relevance information in a RF environment is needed, but such a framework is still missing in most previous works. Therefore, we are motivated to develop this frame-work which is called idealized relevance feedback (IRF).

We focus our study on IRF for several reasons. IRF is relatively straightforward to implement, so it is rel-results to a RF environment. Its performance can still serve as a benchmark for comparison with the more advanced query formulation methods in retrospective experiments, since more advanced methods cannot guarantee that the retrieval effectiveness results are always better than IRF.

Our contributions include a framework for benchmarking retrieval models using partial or full relevance information in a RF environment. Based on this framework, we confirm that the following previous research findings are applicable to the TREC collections (i.e., TREC-6, TREC-7, TREC-8 and TREC-2005) that are larger than and different from the TREC-3 collection as follows: several term selection algorithms perform about equally well; retrieval effectiveness improves with more relevance information; retrieval effectiveness is considerably higher when term weights are determined from relevant and irrelevant documents instead of assigning uniform term weights; and queries of 100 X 200 terms are the most effective.

In addition to previous findings, we also show that manually picking the size of the query that gives the best fixed size that is determined by the best MAP. This performance gain, which is harnessed by manually picking the best query size, decreases as more pieces of relevance information are used.

We have experimented with over 140 term ranking functions that select terms from some subset of the rel-evant documents for query formulation. The highest MAPs for TREC-6, TREC-7, TREC-8 and TREC-2005 are 0.637, 0.624, 0.591 and 0.630, respectively, where the query terms are positively or negatively weighted.
Jones, Walker, &amp; Robertson, 2000b ) that obtained average precisions of about 40+%. Apart from the confir-our experiments when (i) we normalize the term weights by their rank, (ii) we select terms in the top K retrieved, relevant or non-relevant documents, (iii) we include terms in the initial title queries, and (iv) we use the best query size for each topic instead of the average best query size where the best query sizes produce at most five percentage points improvement in the MAP value.

The significance of our contribution is in setting a reference level of retrieval effectiveness so that other retrieval models with partial or full relevance information can be compared with. This level of retrieval effec-tiveness is obtained for various TREC test collections (i.e., TREC-6, TREC-7, TREC-8 and TREC-2005). Our framework provides a common basis to study RF with partial or full relevance information.

The rest of this paper is organized as follows. Section 2 describes our RF environment. This includes details about our framework. Section 3 reports the selection of effective query terms using a greedy term selection algorithm. Section 4 presents the results of RF with partial relevance information. Section 5 reports the results of RF where query terms are assigned with positive weights. Section 6 reports the results of RF where query terms are assigned with positive or negative weights. Section 7 presents the results of RF with full relevance information for TREC-2005 that has about double the number of documents in TREC-6, TREC-7 and
TREC-8 collections. It also summarizes the best attained results measured by different retrieval effectiveness measures. Finally, Section 8 concludes our work and highlights our future work. 2. Relevance feedback environment
This section reports our relevance feedback (RF) environment. First, we describe the set-up which includes the data collection used in our investigation, and the general conditions of our RF environment. Second, we present our framework of investigation. Third, we describe our term selection algorithm that is based on a greedy method. Finally, we report our experiments to justify the use of TREC title queries as the initial queries in our RF experiments. 2.1. Environment set-up
Table 1 shows the details about the different TREC ad hoc test collections that are used in our investiga-tion. The average query size is between two and three terms per query. This is similar to the average web query size ( Spink et al., 2002 ). More specifically, TREC-6 ( Voorhees &amp; Harman, 1998 ) contains documents that are testing the impact of document length on retrieval effectiveness. TREC-7 ( Voorhees &amp; Harman, 1999 ) contains many queries that are hard to achieve good retrieval effectiveness compared with TREC-8 ( Voorhees &amp;
Harman, 2000 ). TREC-2005 is also investigated in order to observe the impact of collection size on retrieval effectiveness.
In our experiments, we simulate a user making relevance judgments by assuming that the user makes iden-tical relevance judgments as the TREC evaluator. We call this:
We also assume that the user does not change her/his relevance judgments when the user sees more rele-vance documents. According to Robertson (1977) , this is known as
As in Wu, Luk, Wong, and Kwok (2007) , we restrict our query terms to those with document frequencies that are larger than one, because this avoids the situation that the selected query terms in a relevant document only identify that relevant document. This restriction does not apply to query terms that are in the initial query formulated by the user, because the initial query is manually formulated rather than automatically formulated. The restriction is summarized in the following assumption:
For simplicity, the identical judgment , the independence assessment and the non-identifying term assumptions are assumed to be true for all the experiments reported in this paper. In practice, the identical judgment and the independence assessment assumptions are not always true. The impact of these two assumptions on retrieval effectiveness can be studied within our framework (see Section 2.2 ), but it is left for future work due to space limitation. In our subsequent experiments, the non-identifying term assumption is applied to extracting terms that are weighted negatively as well as positively, so that our ranking algorithm cannot uniquely identify the seen irrelevant documents.
 As in many experimental studies in information retrieval ( Voorhees, 1998 ), we assume that:
This assumption was scrutinized before (e.g., Kuriyama, Kando, Nozue, &amp; Eguchi, 2002; Keenan, Sme-aton, &amp; Keogh, 2001; Zobel, 1998 ) and its validity has not been undermined. It is not included in our frame-work because this is a common assumption in IR evaluation ( Voorhees, 1998 ) instead of an assumption about relevance feedback.

We used the Porter stemming algorithm ( Porter, 1980 ) and the vector space model (VSM) with pivoted unique normalization ( Singhal, Buckley, &amp; Mitra, 1996 ). VSMs are used because RF can be interpreted as moving the query vector to the target location in the vector space. The pivoted unique normalization is used because it is better than the cosine normalization ( Singhal et al., 1996 ). The pivot is the average document length, and the slope is automatically adjusted according to the query size as in ( Chung, Luk, Wong, Kwok, &amp; Lee, 2006 ). The MAPs of our vector space model with pivoted unique normalization are shown in Table 1 .
These MAPs are obtained using title queries without pseudo relevance feedback. They are similar to the MAPs of those found in the TREC evaluation (e.g., Singhal, 1997 ).

We report the mean average precision (MAP) of our experiments. The precisions for the top k documents (abbreviated as P@ k ) are not reported because they are not appropriate for IRF with full relevance informa-tion. This is because the upper bound of P@ k depends on the number of relevant documents of a query. If the number of relevant documents is less than k , then P@ k cannot reach 100%. If the number of relevant docu-ments is much larger than k , it is relatively easy for a retrieval system to score high P@ k values. A possible upper bound of P@ k is to compute the macro-average of the maximum attainable P@ k values across different queries because the maximum attainable P@ k may be less than 100% for some queries. Table 1 shows the macro-averaged maximum attainable P@10 and P@30 values for the different test collections. Note that the number of relevant documents for TREC-2005 is much larger than those of other TREC collections, so its P@30 is close to 100%. An alternative to P@ k is to measure the R -precision, but it is correlated with
MAP. For completeness, we report the best R -precision and P@30 values later (i.e., Section 7 ) in this paper. 2.2. Our framework
Our framework is a RF environment where some of the practical limitations (e.g., the number of docu-ments judged) are lifted or relaxed for the purpose of investigation. We call our framework idealized relevance feedback (IRF) because the lifted practical limitations in our framework correspond to making impractical assumptions that idealize the RF environment. IRF targets its queries toward Salton X  X  optimal query q ton, 1971, 1989 ): where d is the document vector, R is set of relevant documents, S is the set of irrelevant documents, card  X  X  is assumptions in IRF is the practical limitations of RF. This set is summarized in Table 2 . Each assumption in
IRF is discussed in turn as follows, apart from the first three assumptions that are assumed to be always true in this paper.

First, we assume that the Salton X  X  optimal query q o for a user is displaced slightly by his/her preferences expressed by his/her query vector q (i.e., q o q s  X  q ). This displacement accounts for the idiosyncrasies of the user X  X  internal graded relevance judgment but it is assumed that this displacement only affects the ranking of the documents but not the ultimate retrieval effectiveness performance. This is possible as the ranking of relevant documents may swap ( Robertson &amp; Zaragoza, 2007 ) with each other without affecting the MAPs that are based on the user X  X  externalized binary relevance. The RF biased by the user query q can be expressed as the following successive approximation to q o : where q m is the query vector generated at the m th iteration, f returns the cardinality of the set, and a , b and c are parameters.

Second, it is assumed that document d is represented by another vector d
The difference between retrieval effectiveness using d and d research works (such as ( Harman, 1992 )).

Third, as m tends to infinity, it is assumed that U 1  X  R and V relevant documents and non-relevant documents for a particular topic, respectively. As a result, the asymp-totic query q 1 is equal to q o : Fourth, it is assumed that terms in the document vectors have unity (or equal) weights. This simplifies the
IRF process to focus on term selection rather than term weight assignment. This focus is interesting because it is not known how much retrieval effectiveness is due to selecting the effective terms for query expansion and how much is due to assigning effective term weights. Given that this assumption is true, Eq. (1) is further sim-plified to the following that is expressed in set notation:
It might be obvious that Salton X  X  optimal query q s will always achieve good (or near optimal) results because there are many negatively weighted terms in the non-relevant documents but not in the relevant doc-uments, so that the non-relevant documents are filtered by these negatively weighted terms. However, prior research (e.g., Dunlop, 1997 ) finds that the influence of negative weighted terms on retrieval effectiveness is uncertain. Therefore, we investigate IRF without asserting assumption (8) in later sections in order to examine how negative weighted terms can achieve a positive impact on retrieval effectiveness reliably. We are also inter-ested to investigate IRF after asserting this assumption because we need to know the performance limits of
IRF with just positively weighted or unweighted terms, as used by some PRF algorithms (e.g., Buckley, Allan, &amp; Salton, 1993 ). In this case, if a  X  b  X  1, the asymptotic query q
IRF equation: 2.3. Greedy term selection algorithm
According to assumption (5) in Table 2 , q 1 may be approximated by a smaller query q can obtain performance similar to card  X  q 1  X  P n . The term selection algorithm should find the best perform-ing query q 1 ; max as an approximation of q 1 . The performance limit MAP as where n terms are selected from the relevant document set, R , instead of the non-relevant document set, S ,or the initial query q , and MAP  X  :  X  is the function that returns the MAP value of the query in its argument (i.e., q 1 ; max ) given our baseline retrieval system. However, there is more than one query with exactly n terms from q 1 . Let us denote a particular asymptotic query with n terms as q bination of n terms. Then, q 1 ; n can be specified as
The number of queries that can be formulated by picking exactly n terms from q
Therefore, identifying q 1 ; n is not a trivial computational problem given that n and m could be hundreds or thousands.

We propose to use a greedy term selection algorithm in Fig. 1 to formulate the asymptotic query q for verification. Also, such a greedy algorithm is commonly used in RF and PRF, so the work here has direct relevance to existing RF and PRF methods. Finally, the results obtained may serve as a benchmark for other more sophisticated term selection algorithms to compare with (e.g., genetic algorithm).

The algorithm is based on the idea of selecting highly ranked terms. It basically ranks the terms by a term from the ranked list of terms to Q 1 ; n approximating the best query q selection algorithm assumes that: which may not be entirely unrealistic because only a near best MAP performance is required and because n can be increased to a level where MAP  X  Q 1 ; n  X  MAP  X  Q 1 ; n  X  1
In details, our greedy term selection algorithm ( Fig. 1 ) uses a term ranking function x  X  :  X  (will be defined terms but with a set R of relevant documents for a particular topic, the desired number n of terms in the for-step 2, all the terms in the relevant documents are added to the set P and then added to P if they are neither stop words nor numerals, and they must occur in more than one doc-ument in the collection C . This is designed to avoid formulating trivial optimal queries where each relevant document is picked up by one term that only occurred in that relevant document. In step 4, the weight of each good for retrieval or not. Then, the terms are ranked by these weights in step 5. Finally, the top n terms are user query, then the returned query has initial query bias. 2.4. Initial query bias
In this section, we examine the impact of the initial query bias assumption (4) (in Table 2 ) on retrieval effec-tiveness. Interests in assumption (4) stem from the problem of topic lapse in practical RF ( Salton, 1971; Salton &amp; Buckley, 1990 ). Also, Buckley, Dimmick, Soboroff, and Voorhees (2006) indicate that systems which use the topic title perform better because pooling is biased to judge documents with title query terms since large col-lections contain many documents with title query terms. Therefore, we are interested to investigate whether including the title queries will have an impact on retrieval effectiveness. Apart from assumption (4), all other assumptions in Table 2 are assumed to hold and are studied later in this paper.

We compare the performance of our basic IRF without the user query (i.e., q is an empty set in greedy term selection algorithm in Fig. 1 ), and our basic IRF with the user query. The user queries in this experiment are the title queries (T) because these are realistic queries for many IR applications. Our VSM is used and the experiment is carried out for the TREC-6, TREC-7 and TREC-8 test collections. The term ranking function x  X  :  X  (in Fig. 1 ) in this experiment is W 4 (i.e., F 4) ( Robertson &amp; Spa  X  rck Jones, 1976 ): term t in the set R of relevant documents for a particular topic, and card  X  :  X  is the number of documents in the given set.
 In order to have more observation points, we use nine different query sizes to formulate the test queries
Q 1 ; n : n = 3, 25, 50, 75, 100, 125, 150, 175 and 200. The reason for selecting three terms as the minimum size is for comparison with the title queries (T) that usually have three terms (i.e., there are 56%, 56% and 44% title queries with three terms in TREC-6, TREC-7 and TREC-8, respectively). On the other hand, the maximum size of long queries (TDN) in TREC-6, TREC-7 or TREC-8 is about 150. We add two more observation points, 175 and 200, in order to ensure the operating range is large enough for us to decide on a suitable query size for later experiments.

Fig. 2 shows the experimental results of the MAP values of the optimal queries Q ure shows that  X  X ith title X  achieves better MAP value than  X  X ithout title X  for all the test query sizes in all of the test collections. This better MAP value may be due to the fact that the same user formulated the query and identified the relevant document set, or that it may be due to the title bias introduced in pooling ( Buckley et al., 2006 ). However, the MAP difference between  X  X ith title X  and  X  X ithout title X  diminishes as the query size ness, as expected. Also, the title query terms may be picked up by the greedy term selection algorithm, so the performance difference reduces as query size increases. We use Wilcoxon 1-tailed sign test to examine whether using the title query will achieve better retrieval effectiveness or not. We choose  X  X op 100 X  queries to run this test (i.e., n  X  100) because the MAPs of these queries are close to the asymptotic performance in Fig. 2 . The
Wilcoxon sign test results conclude that using title query can achieve better MAP value than without using title query with 99.7% confidence. Hence, we believe that initial query bias assumption (4) is valid. The title query will be added to Q 1 ; n in all the subsequent experiments. 3. Dimensionality selection This section examines the impact of the dimensionality reduction assumption (5) on retrieval effectiveness. Assumption (5) has been widely examined in practical RF but seldom in the IRF context. The advantage of examining assumption (5) in the IRF context is that the practical problems of RF are separated from the intrinsic problems of assumption (5) itself. This assumption has two aspects to examine: (a) what dimensions should be selected, and (b) how many dimensions should be used. Apart from assumption (5), assumption (4) is assumed to hold by using title queries, whereas assumptions (6), (7) and (8) are asserted to simplify the study, and they are studied later. 3.1. What dimensions There are many past research works that studied how to select terms ( Allan, 1996; Chung &amp; Lee, 2004; Carpineto &amp; Romano, 2002; Dunlop, 1997; Efthimiadis, 1993; Fan, Wang, &amp; Xi, 2004; Gu &amp; Luo, 2004;
Khan &amp; Khor, 2004; Mano &amp; Ogawa, 2001; Robertson, 1990; Salton &amp; Buckley, 1988; Salton &amp; Buckley, of IRF. In this paper, we use a greedy algorithm to select terms similar to previous works. In the algorithm tion 2.4 , we have applied a well-known function, W 4, to rank the term, and the generated queries have achieved 0.534, 0.522 and 0.464 MAP values for TREC-6, 7 and 8, respectively. In this section, we mainly focus on exploring the effects of using different term ranking function x  X  :  X  . 23 basis term ranking functions are combined to yield 149 unique term ranking functions for our experiments. Some of these term ranking functions have been widely used in IR, but some have not.

The 23 basis term ranking functions can be divided into two major classes: inter-document term ranking functions x 1  X  t  X  which are shown in Table 3 , and intra-document term ranking functions x shown in Table 4 . The combined term ranking function x  X  t  X  is the product of x standard TF-IDF term ranking functions ( Salton &amp; Buckley, 1988 ):
Two types of inter-document term ranking functions can be distinguished: point-based ranking functions (i.e., ranking functions 1 X 4 in Table 3 ), and distribution-based ranking functions (i.e., ranking functions 5 X 14 in
Table 3 ). All point-based ranking functions are based on the document frequency statistics. Some distribu-tion-based term ranking functions are based on statistics (e.g., chi-square), information theory (e.g., relative entropy and EMIM) or (Bayesian) statistical decision theory (e.g., RSV). For the chi-square and relative en-tropy ranking functions, we provide two more variations of the probability definitions other than the one in the original paper ( Carpineto, Mori, Romano, &amp; Bigi, 2001 ): one is based on the probability of whether the term is in the document (i.e., ranking functions 5, 6, 8 and 9 in Table 3 ), and the other is based on the prob-ability that a term in the document is the desired term (i.e., ranking functions 7 and 10 in Table 3 ). The intra-document term ranking functions can be divided into three types: without any normalizations (i.e., ranking functions 15 X 17 in Table 4 ); with intra-document normalization (i.e., ranking functions 18 X 20 in Table 4 ), and with inter-and intra-document normalization (i.e., ranking functions 21 X 23 in Table 4 ). Each type has three term ranking functions based on the maximum term frequency, or the minimum term frequency or the total term frequency of a term in the documents.

In this experiment, we use the greedy term selection algorithm in Fig. 1 to formulate the asymptotic query, except that step 4 has been modified to cater for a second term ranking function because many terms have tied term ranking function scores after visual inspection. We use W 4  X  :  X  as the second term ranking function for all addition, the number of terms n appended to the title queries is 100.

The MAP values obtained using different term ranking functions are shown in Table 5 . We have observed many interesting results. First, the term ranking function (7) using CHI value in TREC-6, 7 and 8 without any intra-document ranking function (i.e., labeled  X  X one X  in Table 5 and x  X  :  X  X  1). Second, in terms of multiplying intra-document ranking function x
MaxTF and MinTF actually degraded the MAP values (i.e., worse than with x
CHI 3 , KLD 2 and KLD 3 with SumTF in TREC-6 achieved better results. This implies that adding raw term fre-quency may not be a good method to improve the retrieval effectiveness. Third, better MAP values were obtained by multiplying x 2  X  :  X  with intra-document normalized term frequencies (i.e., MaxNTF , MinNTF and SumNTF ) than without normalized term frequencies (i.e., MaxTF , MinTF and SumTF ). Fourth,
NMaxNTF and NSumNTF , which normalized the term frequency with respect to the document and the entire collection, achieved better MAP values than others except EMIM . Fifth, the results reveal that minimum term frequency (normalized or not) does not enhance the retrieval effectiveness compared with the case without intra-document term ranking. Sixth, CHI 3 achieved better MAP value than CHI
KLD 2 achieved the worst performance among other KLD term ranking functions. This implies that macro-average of the normalized probability of term occurrence is a more appropriate probability function for term rankings. Seventh, RSV 1 is the best among other RSV term ranking functions. Eighth, RSV form worst than others. Finally, the common inverse document frequency (i.e., IDF ), has a MAP better than 0.5, except for TREC-8. Therefore, the existing IDF function is already effective in selecting good terms for RF.

Table 6 summarizes the retrieval effectiveness of the better term ranking functions that are highlighted in bold font in Table 5 . We can see that the x 2  X  :  X  of the better ranking functions is either the full normalized maximum term frequency or the full normalized summed term frequency. Moreover, it seems that the com-test collections, except when x 1  X  :  X  is unity. Furthermore, the combination of CHI summed term frequency ( CHI 3 NSumNTF ) produces the overall best results for TREC-6 and TREC-7 test collections, and W 4 combined with the full normalized maximum term frequency ( W 4 NMaxNTF ) produces the overall best results for TREC-8. However, these overall best results are not statistically significantly dif-ferent from the results of other better term ranking functions in Table 6 . 3.2. How many dimensions
The other aspect of the dimension reduction assumption (5) is how many dimensions should be used, which is the same as deciding the number of query terms n in the asymptotic query q that evaluates different term ranking functions, the number of appended terms is fixed to the top 100 to elim-inate the effects of query size. In this experiment, on the other hand, we pick the seven better term ranking functions in Table 6 and vary the query size from 0 to 200: 0, 3, 25, 50, 75, 100, 125, 150, 175 and 200.
The experimental results can be used to examine the impact of different query sizes and to determine if any better retrieval effectiveness will be obtained.

Fig. 3 shows the experimental results. The MAP values of all the seven term ranking functions do not change much for the query size of 100 or larger (i.e., Top 100 terms). These experimental results are then ana-lyzed with the analysis of variance (ANOVA) test to determine whether the changes are significant. The
ANOVA test is based on a single factor model. The factor is the query size and the comparison is based on the mean average precisions. The statistical test is one way since the hand picked results have the highest performance. According to the ANOVA test with 95% confidence, queries with size of 100 have significant improvement in MAP compared with those query sizes smaller than 100 (i.e., from size 0 to 75); however, no significant improvement can be gained when comparing with those query sizes larger than 100 (i.e., from size 125 to 200). Therefore, we believe that query size of 100 is already the (near) best.

Nevertheless, many researchers ( Buckley &amp; Harman, 2003 ) have reported that the sizes of the optimal query of different topics are different. It means that we cannot use the same query size for all the topics. In order to investigate this effect, we compare the MAP of using the best query size for each topic and the MAP of using the same query size 100 for all topics with two of our best term ranking functions, CHI
W 4 NMaxNTF . From Table 7 we observe that using the best query size in each topic is always better than using the same query size 100. However, according to the ANOVA test with 95% confidence, these improve-ments are not statistically significant. This supports the use of fixed-size query expansion. 4. Effects of partial relevance information
In this section, we explore the asymptotic assumption (6) by using different numbers of top ranked retrieved documents from the initial retrieval list. In general, as the number, m , of RF iterations increases, the number of retrieved and relevant documents in U m gets larger, and the corresponding recall of retrieval increases. As m tends to infinity, U 1 is R . In practice, this increase of the size of U bers of top K retrieved documents from the initial retrieval list. Therefore, we can plot the performance against the number of top retrieved documents and the recall which would represent the trend of the IRF.
In this experiment, we want to observe how the relevance feedback (RF) algorithm performs when it only has partial relevance information. We assume that the user judged the top K retrieved documents in the initial the initial ranked retrieval list. Then, the top K retrieved documents are classified into relevant documents U and non-relevant documents V j by the user. Finally, the query q similar to Eq. (2) but it discards the asymptotic assumption (6). It is because we are interested in investigating how many top K retrieved documents in practical RF can formulate the asymptotic query q mate optimal query q o ):
Two types of user queries, title (T) and long (TDN), are used to examine the impact of the initial query q as well as the initial retrieval list on performance. The MAP values obtained by selecting terms from the relevant documents in the top 10, 20, 40, 80, 160, 320, 640, 1000, 10,000, possibly 100,000 and the entire retrieved list are plotted for visualization. The seven better term ranking functions listed in Table 6 are used to formulate the query q j with size of 100. In Fig. 4 , we plotted the IRF performance at 556,077, 528,155 and 528,155, which are the corresponding numbers of documents in TREC-6, 7 and 8 collections, respectively. Moreover, the average number of retrieved documents in the entire retrieval list for title initial query is equal to 59,387, 35,781 and 42,395 for TREC-6, 7 and 8, respectively, and for TDN initial query it is equal to 409,902, 305,827 and 297,049 for TREC-6, 7 and 8, respectively, which are close to the corresponding collection sizes. In Fig. 5 , we plot the mean recall of each query for the different top K retrieved documents against the MAP, and the
IRF is plotted with a recall of 100%. The numbers of top retrieved documents are labeled near the data points in both figures.

Fig. 4 shows that the MAP values of different top K retrieved documents are lower than IRF. This implies that many practical RF will achieve a performance lower than IRF as the user examines a limited number of top K retrieved documents in practice. Moreover, Figs. 4 and 5 illustrate that using more top retrieved doc-uments achieves better mean recall and therefore MAP for all kinds of initial queries and term ranking func-tions. This implies that if the user examines more top K retrieved documents, the retrieval effectiveness of practical RF will be improved. However, Fig. 5 suggests that if the initial retrieval list of the practical RF (say from the top retrieved documents to the entire retrieval list in Fig. 5 ) does not have (near) 100% recall, then it is unlikely that the MAP of the practical RF making relevance judgment of documents in the initial retrieval list can approach the MAP performance of its IRF. So, it is not necessary to examine the whole initial observe that the retrieval effectiveness is increased sharply until the top 100. Therefore, we believe that the selected feedback terms from the top 100 retrieved documents are effective in RF as well as PRF. Furthermore, for long queries, using the entire retrieval list can achieve similar performance to the IRF. This implies that we is about 5% lower than that of the IRF version. This suggests that for title queries RF needs at least two iter-ations, in order to ensure that the retrieval list has an adequately high recall to obtain the potential IRF performance.

In Fig. 5 , when the recall is zero, the mean average precision (MAP) is obtained using the title queries or the long queries without any relevance information. It seems that if the number of relevant documents is too small (i.e., less than 10% of the total number of relevant documents), the amount of MAP increases compared with no relevance information is barely noticeable. When over 10% of the total number of relevant documents is seen, the retrieval system can improve the initial MAPs steadily with increasing number of seen, relevant doc-uments. Similar to Buckley et al. (1995) , we fitted a linear regression to the data points for CHI and W 4 NSumNTF term weighting schemes. Only two schemes are selected for regression because the curves in Fig. 5 are very similar. We observe that all the square correlation coefficients in Table 8 are over 90%. This 1995 ). 5. Positively weighted terms
This section examines the unweighted assumption (7) that allows us to assign weights to query terms instead of using unity weights for all query terms. However, only positive weights are considered (i.e., c  X  0), and negative weights will be examined in the following section. Three other assumptions hold in this case. The asymptotic query q 1 is simply modified from Eq. (1) to Eq. (5) instead, and step 7 of the greedy term selection algorithm in Fig. 1 is modified to Q 1 ; n a r  X  t  X  are the term weight functions of user query q and feedback terms Q assign the weight to term t . In this experiment, the marginally best term ranking function (i.e.,
CHI 3 NSumNTF ) is used to formulate the query with size of 100. The weights are mixed using the parameter a and b (  X  1 a ):
Five term weight normalization schemes are proposed in Table 9 . First, Rocchio positive scheme (1) is the standard Rocchio (1971) RF, except that there are no negatively weighted terms. The weights of user query q are calculated by the reciprocal of the number of terms in the user query. The weights of feedback terms are calculated by the term ranking function x  X  :  X  , which is CHI regular scheme (2) ( Ide &amp; Salton, 1971 ) is similar to scheme (1) but the term weights are linearly mixed as shown in the following equation:
Third, city-block normalization scheme (3) can be considered as scheme (2) where the feedback term weights are normalized by the sum of its term weights. Fourth, rank normalization scheme (4) can also be considered the term ranking function: CHI 3 NSsumNTF in this experiment. The last scheme, unity weight scheme (5), is essentially one without any weighting since every term is as important as the other.

Fig. 6 shows the MAP values of the queries using five different term weight normalization schemes and dif-ferent parameter values a and b . It is obvious that the MAP values of the Ide-regular scheme (2) and unity weight scheme (5) are fairly independent of the parameter values, whereas the other three schemes are sensitive to different parameter values. Moreover, only the rank normalization scheme (4) with specific parameter val-ues (i.e., when a  X  0 : 1) achieves higher MAP value than the unity weight scheme (5). Therefore, we only choose schemes (4) and (5) to further examine whether the rank normalization scheme (4) can perform better if the number of terms selected is different. In addition, when the user query is ignored (i.e., when a  X  0), most of the schemes achieve lower MAP value than those with user query (i.e., when a = 0.1 X 0.9, b = 0.9 X 0.1) except the Rocchio positive scheme (1). This indicates that the initial query bias assumption (4) is valid in either unweighted query or weighted query.

In order to determine the suitable query size for positively weighted terms, we picked the best two term weight normalization schemes, the rank normalization scheme (4) with a  X  0 : 1 and the unity weight scheme (5) with a  X  0 : 8, and vary the query size from 0 to 300 to examine the effects. The reason for adding four more observation points (i.e., 225, 250, 275 and 300) is because the MAP values are continuously increasing from query size of 0 to 200. These points are used to determine if any better retrieval effectiveness can be obtained.
The experimental results are shown in Fig. 7 . From the figure we can see that the MAP values of scheme (4) are always slightly better than scheme (5). The best MAP value of the scheme (4) is obtained when the query size is about 175.

Table 10 shows the MAP value of rank normalization scheme (4) with top 175 positive terms and with the best query size in each topic. The MAP value of the best query size is better. However, according to the result of the ANOVA test with 95% confidence, these improvements are not statistically significant. Hence, we con-clude that the MAP values may not be improved by using various query sizes for different topics in positively weighted terms.

Comparing the best MAP value of the queries using positively weighted terms (the results in Table 10 ) and unweighted terms (the results in Table 7 ) for fixed query size, the MAP value using positively weighted terms is about 4% higher than using unweighted terms. However, from Table 11 we can see that this difference in MAP value is not statistically significant ( Voorhees &amp; Buckley, 2002 ). Therefore, we conclude that positively weighted terms can only slightly enhance the retrieval effectiveness. 6. Negatively weighted terms
In this section, we examine the impact of non-negative weight assumption (8) on retrieval effectiveness. This assumption adds terms with negative weights. In the literature, negative weights did not always have a positive impact on performance ( Dunlop, 1997; Ide, 1971 ). Here, the assumptions (2), (3), (4) and (5) hold and the asymptotic query q 1 is similar to Eq. (1) , but it uses a subset V uments of the user query rather than the whole set S of non-relevant documents for the topic. The reason for using the top K retrieved documents is because the whole set S of non-relevant documents is too large in
TREC test collections and it is similar to practical RF. The asymptotic query is modified as follows:
An enhanced greedy term selection algorithm, greedy weighted term selection algorithm, is used to select the query terms from relevant documents and/or non-relevant documents. The aim is to select good positive terms that can retrieve exactly relevant documents and good negative terms that can suppress the highly ranked but non-relevant documents. The detailed algorithm is shown in Fig. 8 . Initially, the algorithm starts with no good terms but with a set R of relevant documents for a particular topic, a subset V and non-relevant documents for the user query, the desired number n terms in the output query Q 1 ; n in the relevant documents and non-relevant documents are added to the P 4, any terms in P 0 and N 0 will be stemmed and then added to positive set P and negative set N , respectively, if they are not stop words and not numerals, and they must occur in more than one document in the collection
C . In step 5, the weight of each term t in the positive set P and each term t in negative set N is calculated term list L P and negative ranked term list L N . In steps 7 and 8, the positive query Q
Q 1 ; n N are generated by using the top n p terms and top n This selection is based on the negative term selection scheme /  X  L weight function r q  X  t  X  to calculate the weights of user query q , and use r terms in the positive query Q 1 ; n parameters a ; b and c .

Fig. 9 shows various subsets of terms that can be selected to assign positive and negative weights. This figure explains some terminologies in the negative term selection scheme. Let TC be all the terms in the document collection. Let TR be the set of terms in the relevant document, for a particular topic. Let TV be the set of terms in the top K retrieved and non-relevant documents for the user query q . These two subsets of terms define three smaller subsets of terms that are useful for defining term selection schemes as follows. Let us define A j  X  TR TV j . This set of terms appears in the relevant documents but not in the non-relevant doc-uments of the top K retrieved documents. These terms are like beacons of the relevant documents and they are weighted positively. Another set B j of terms is the intersection of TR and TV relevant and non-relevant documents of the top K retrieved documents and these terms do not seem to have any discrimination ability. In general, these terms are assigned a zero weight. The set Z
TV j TR . These terms only appear in the retrieved and non-relevant documents. Therefore, these terms should be strongly negatively weighted.
 Six negative term selection schemes /  X  :  X  are defined using different combinations of the subsets of terms,
A ; B j and Z j . Table 12 shows the combination of these subsets for different term selection schemes. First, pos-itively weighted scheme (1) is the same as to the previous section for selecting positively weighted terms. This scheme is included for comparison. Moreover, the true positive terms scheme (2) only picks terms that appear in the relevant documents but not in the top K retrieved, non-relevant documents. Therefore, extracting more (negatively weighted) terms for V j from the top K retrieved, non-relevant documents reduces the number of does not explicitly assign negative weights to terms during retrieval. In scheme (3), only terms that can indicate clearly whether documents are relevant or non-relevant are used because these terms only occur in the relevant documents or they only occur in the retrieved, non-relevant documents but not both. Furthermore, all subsets scheme (4) uses all the subsets of terms. Furthermore, negatively weighted scheme (5) selects terms that appear in the top K retrieved, non-relevant documents. Finally, the true negative terms scheme (6) only picks terms that appear in the top K retrieved, non-relevant documents. Terms that appear in any of the relevant docu-ments will not appear in the query for this scheme.

Fig. 10 shows the MAP of different negative term selection schemes against different values of c . This query is composed by adding the top 175 ranked positively weighted terms and the top 175 ranked negatively weighted terms to title queries. These negative weighted terms are extracted from the top 100 retrieved, non-relevant documents which use title query as the initial user query. There are two reasons for choosing the top 100 retrieved documents: (1) the average number of relevant documents for each topic in TREC-6, 7 and 8 is about 100, and (2) in Figs. 4 and 5 , we find that the performance increases sharply until top 100. Moreover, the marginally best term ranking function CHI malization scheme, rank normalization scheme (4), with a  X  0 : 1 and b  X  0 : 9 are used in this experiment.
There are many interesting findings in Fig. 10 . First, scheme (2) does not add any negatively weighted terms into the query, so that c does not have any effect on this scheme. Second, the MAP value of scheme (1) is larger than scheme (2), but after approaching a maximum, higher c values have a negative impact on MAP for this scheme. Third, the performance of scheme (3) is similar to scheme (2) because this scheme only uses terms with a clear signal that the terms should be weighted positively or negatively. Fourth, the MAP of scheme (4) is the highest among the other schemes because this scheme includes all the subsets of terms (i.e., A itively, this is expected since the impact of positively weighted terms does not necessarily decrease, and the number of terms lost in A j is compensated for the increase in the number of terms in B 0.4 for scheme (4) so that the negative weights in the query will not overwhelm the positively weighted terms.
Finally, schemes (5) and (6) are lower than the other schemes because these schemes only use terms with neg-ative impact.

In order to determine the suitable query size for negatively weighted terms, we picked the best two negative term selection schemes, the positively weighted scheme (1) with c  X  0 : 6, and the all subsets scheme (4) with c  X  0 : 4 and vary the query size from 0 to 300 to examine the effects. The experimental results are shown in
Fig. 11 . It shows that when more top ranked negative terms are added to the query using scheme (1) or scheme (4), the MAP value increases to a maximum and then begins to taper off. The best MAP is achieved with about top 250 negative terms by using scheme (4) with parameters a  X  0 : 1, b  X  0 : 9 and c  X  0 : 4.
Table 13 shows the MAP value of negative term selection scheme (4) with the top 250 negative terms and with the best query size in each topic. The MAP value of the best query size is better. However, according to the results of the ANOVA test with 95% confidence, these improvements are not statistically significant.
Hence, we conclude that the MAP values may not be improved by using various query sizes for different topics in negatively weighted terms.

Finally, from Table 14 we can see that the improvement in MAP between the best MAP value achieved in this section (using both positively and negatively weighted terms, labeled  X  X eg X ), and the best MAP value achieved in Section 5 (using the top 175 ranked positively weighted terms, labeled  X  X os X ) is about 4%. This is not statistically significant with 95% confidence ( Voorhees &amp; Buckley, 2002 ). However, the improvement in MAP between the best MAP value achieved in this section and in Section 3.2 (using the top 100 ranked unweighted terms, labeled  X  X n X ) is about 7%. This difference is statistically significant. Therefore, we conclude that negatively weighted terms have a significant MAP improvement compared with IRF using unweighted terms. 7. Larger collection and performance summary
This section reports the best MAP of IRF using TREC-2005 collection which has approximately doubled the number of documents in TREC-6, TREC-7 and TREC-8. We are interested in this collection because: (1) the number of documents in this collection is larger than the number of documents in disk 1 and disk 2 for the combined TREC-2 and TREC-3 data that are used by Spa  X  rck Jones et al. (2000a, 2000b) , (2) the topics are the hard ones in TREC-6, TREC-7 and TREC-8 test collections, and (3) this collection has title queries as short as those found in web retrieval ( Spink et al., 2002 ).
 According to Table 15 , the retrieval effectiveness of the IRF with different weighted term schemes for the
TREC-2005 collection is at a similar level as the retrieval effectiveness of the TREC-6, TREC-7 and TREC-8 collections. This is achieved despite the TREC-2005 collection contains the hard topics of TREC6, TREC-7 and TREC-8. Similar to TREC-6, TREC-7 and TREC-8, the retrieval effectiveness of IRF with positively weighted terms is higher than the IRF with unweighted terms. Likewise, the retrieval effectiveness of IRF with weighted terms is higher than the IRF with positively weighted terms. It appears that the results for TREC-6, 7, 8 and 2005 are similar to each other.

For completeness, Table 15 also shows the R -precision and the P@30 values of our IRF with unweighted terms, positively weighted terms and weighted terms for the four reference TREC collections.
The R -precisions of our IRF are similar to the corresponding MAP values (i.e., within three percentage points). The P@30 values for TREC-2005 are at least seven percentage points higher than the correspond-ing P@30 values for the other TREC collections. Intuitively, these higher P@30 values for TREC-2005 may be explained by the fact that there are more relevant documents for TREC-2005 than for other TREC collections (i.e., 6000 versus 4000 relevant documents for a batch of 50 queries). This shows that
P@ k is not a suitable performance measure for comparison because it depends on the number of relevant documents. 8. Conclusion and future work
We study automatic query formulation in a relevance feedback (RF) environment that is idealized in a systematic way by our new framework called idealized RF . Within this framework, our findings are: (i) sev-eral term ranking functions perform about equally well, and one of the best is a new variant using the chi-square statistics, (ii) retrieval effectiveness improves with more relevance information, and (iii) queries of 100-200 terms are the most effective. In addition, the level of retrieval effectiveness (in terms of MAP) of our IRF is about 55 X 65% instead of 40+% as in the previous findings even though one of the TREC ad hoc test collections that we have used contains about double the number of documents compared with the TREC-3 test collection that is used in previous work. In addition to confirming previous findings, we found in our experiments that better retrieval effectiveness can be obtained when (i) we normalize the term weights by their rank, (ii) we select terms in the top K retrieved, relevant or non-relevant documents, (iii) we age best query size where this produces at most five percentage points improvement in the mean average precision (MAP) value.

We have also obtained new results for massive query expansion ( Buckley et al., 1995 ). We have added negatively weighted terms to massive query expansion. This statistically significantly improves retrieval effectiveness (in terms of MAP) when it is compared with the unweighted massive query expansion. How-ever, it does not improve the retrieval effectiveness statistically significantly when it is compared with the positively weighted massive query expansion. We have also obtained results for massive query expansion that only assigns equal or unity weights to each query term. We also show that picking the best query size for massive query expansion may improve retrieval effectiveness further but this is not statistically significant.
 We are surprised that the simple greedy term selection algorithm may achieve relatively high MAP values. In the future, we are interested in using more sophisticated algorithms (e.g., genetic algorithm) to select query terms for a higher upper bound retrieval effectiveness. In particular, we are interested in term selection algo-rithms that find effective query terms with unity term weights because such algorithms make the query expan-sion simpler without the need to determine different effective term weights for different query expansion terms.
When the upper bound is better established and well accepted, we are interested to find fewer query expansion ( Vines &amp; Zobel, 1999 ).
 Acknowledgements
This work is supported by the CERG Project # PolyU 5199/04E. Robert Luk thanks CIIR, UMASS, for facilitating him to develop in part the basic IR system, when he was on leave there.
 References
