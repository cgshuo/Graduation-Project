 For a training dataset with a non-exhaustive list of classes, i.e. some classes are not yet known and hence are not rep-resented, the resulting learning problem is ill-defined. In this case a sample from a missing class is incorrectly classi-fied to one of the existing classes. For some applications the cost of misclassifying a sample could be negligible. However, the significance of this problem can better be acknowledged when the potentially undesirable consequences of incorrectly classifying a food pathogen as a nonpathogen are considered.
Our research is directed towards the real-time detection of food pathogens using optical-scattering technology. Bacte-rial colonies consisting of the progeny of a single parent cell scatter light at 635 nm to produce unique forward-scatter signatures. These spectral signatures contain descriptive characteristics of bacterial colonies, which can be used to identify bacteria cultures in real time. One bottleneck that remains to be addressed is the non-exhaustive nature of the training library. It is very difficult if not impractical to col-lect samples from all possible bacteria colonies and construct a digital library with an exhaustive set of scatter signatures.
This study deals with the real-time detection of samples from a missing class and the associated problem of learn-ing with a non-exhaustive training dataset. Our proposed method assumes a common prior for the set of all classes, known and missing. The parameters of the prior are esti-mated from the samples of the known classes. This prior is then used to generate a large number of samples to simulate the space of missing classes. Finally a Bayesian maximum likelihood classifier is implemented using samples from real as well as simulated classes. Experiments performed with samples collected for 28 bacteria subclasses favor the pro-posed approach over the state of the art.

The dataset and our implementation of the proposed ap-proach is available on the web via the link: http://www.cs.iupui.edu/  X  dundar/kdd2009.htm I.5.2 [ Pattern Recognition ]: Design Methodology X  clas-sifier design and evaluation Algorithms non-exhaustive learning, bayes classifier, novelty detection, anomaly detection, bacteria detection
The goal of statistical learning is to build robust models that, when deployed in a real-life application, should gener-alize well to yet unseen examples of the sample population. Among other factors that influence the generalizability of a learning algorithm, the quality of the training dataset is perhaps the most critical. Although a detailed discussion of what makes a training dataset high quality goes beyond the scope of this study, a representative training dataset is essen-tial to the realization of any supervised learning algorithm with high predictive accuracy.

There is not a well-defined definition of what makes a training dataset representative, but two things to consider are: is the list of classes (of informational value) complete, i.e. exhaustive ? If yes, are there sufficiently large number of samples available from each class? It is very difficult to come up with a clear-cut answer to the second question, as how many samples considered sufficient for each class depends on many factors: the dimensionality of the data, the type of model being used (discriminative or a generative), and the number of parameters to estimate with the data, to name few. Besides, this would not matter much if the list of classes is not exhaustive. In other words, if the existing set of classes is incomplete, i.e., misses one or more classes of informational value, no matter how many samples we have for the existing classes the training dataset would still have to be considered unrepresentative.

The easiest way to deal with this problem, as most tradi-tional supervised algorithms do, is to ignore it. When this direction is taken, a sample of a class that is not represented in the training dataset would be incorrectly classified to one of the classes available in the training dataset. This could be a reasonable strategy for some domains where the cost of misclassifying a sample is negligible. However, within the framework of our current research, one could better appre-ciate the criticality of a more rigorous approach, considering the potentially unfortunate consequences of incorrectly clas-sifying a pathogen as nonpathogen.

Learning with a non-exhaustive training dataset is an ill-defined problem, and to our knowledge there are not many studies in the machine-learning literature that explicitly ad-dress this issue. One area of machine learning that has drawn much attention lately is anomaly detection. Both anomaly detection and the current problem of learning with a non-exhaustive training set aim to detect samples that are not represented in the training data, and in that regard they can be considered similar. However, there is a well-defined line between the two. In the dictionary an anomaly is de-fined as something peculiar, irregular, abnormal, or difficult to classify. So anomalies are outliers and they could be as different from each other as they are from normal cases [15]. More specifically, anomalies do not necessarily have infor-mational value and it is very difficult if not impractical to model them. On the other hand, samples originating from a missing class have informational value and just like any class available in the training set they could be modeled, were they known during training.

Another line of work that is more similar to the current research is developed under the concept called  X  X ovelty de-tection X  [12, 11, 8]. Unlike anomalies, novelties originate from a hidden or missing class and thereby have informa-tional values. Novelty detection is sometimes referred to as  X  X ovel class detection X . We use the term non-exhaustive learning as opposed to novel class detection in this study, mostly because the former is a more comprehensive prob-lem involving classification (for samples of known classes) in addition to novelty detection.

Despite the subtle difference in the interpretations of a-nomaly and novelty detection, and non-exhaustive learning, most of the early work in the anomaly and novelty detection domain, centered around support estimation [14, 10], and density-based models, can also be applied to non-exhaustive learning. In addition to these, the traditional supervised classification algorithms can be modified to accommodate for learning with a non-exhaustive training set by redefining the decision function to include an indecisive region . When tested by a classifier, if a sample falls onto this region, it is considered to originate from an unrepresented/missing class.
After reviewing the state of the art concerning these ideas, we propose a Bayesian approach based on the assumption that all classes are distributed according to a Gaussian dis-tribution with a common covariance matrix. In this ap-proach, we first define a hyperprior over the mean vectors of class distributions and estimate its parameters with sam-ples from known classes. Then, we use this hyperprior to simulate the space of unknown classes. The final classifier is implemented using real as well simulated data and a new sample is rejected, i.e. the sample originates from a class not represented in the training set, when the likelihood is maximized for one of the simulated classes, otherwise the sample is accepted, i.e. the sample originates from one of the classes in the training set.

The rest of the paper is organized as follows. In Section 2 we present the detection of bacteria cultures using optical-scattering technology as a case study. Section 3 reviews and discusses the early work performed mainly in the area of anomaly detection. We present the details of the proposed approach in Section 4. Several experiments are performed to compare the state of the art and the proposed approach in Section 5. Finally, we conclude with a brief analysis of our results and by providing future research directions.
Traditional bacteria recognition methods based on anti-bodies or genetic matching are labor intensive, time con-suming, and involve multiple steps. Moreover, samples are destroyed by these type of tests and are thus unavailable for further confirmatory assessment. The tests rely on the use of specific reporter molecules such as antibodies or nucleic-acid probes coupled with fluorophores or enzymes, thus limiting their broad application for multipathogen detection, or de-tection of unknown pathogens.

Light scattering is a fundamental optical process whereby electromagnetic waves deviate from a rectilinear path as a result of non-uniformities in the medium that they traverse. Light-scattering technology has been used before to interro-gate bacterial cells in suspension [18, 19], as well as in flow [13, 9]. The scope of this approach was very narrow and only a limited number of bacterial species could be detected successfully. Recently, we have discovered that interrogation of bacterial cultures on the surface of agar in a semi-solid state could provide a possible differentiation via distinctive forward-scattering patterns [4, 1, 2]
The BARDOT (BActeria Rapid Detection using Optical scattering Technology) system uses a laser (635 nm) to illu-minate individual colonies and create a forward-scatter sig-nature that is collected and subsequently analyzed (See Fig-ure 1). In our earlier works we successfully demonstrated that scattering properties of Listeria, E. coli, Salmonella, Staphylococcus , and Vibrio colonies can be used to differen-tiate the species occurring in food samples as well as those isolated from experimentally infected animals [3].
Forward-scattering patterns of bacterial colonies show de-scriptive characteristics. Zernike moment invariants and Haralick descriptors are used to capture these descriptive features and to construct a scatter-signature image library. Figure 1: Representative examples of scatter pat-tern from four bacterial strains. The differences in the patterns can be seen with naked eye.
 We have previously reported examples of BARDOT appli-cation for classification and/or recognition of known (previ-ously examined) bacterial species and strains. The classifi-cation techniques utilized for classification of BARDOT pat-terns included nonsupervised methods, partial least squares, linear discriminant analysis, neural networks, and support vector machines [4, 3]. The results obtained by classification varied widely and depended not simply on the classification technique used, but mostly on the type, quality, and num-ber of features. However, no systematic study of methods performance has been attempted up to date. Therefore the comparison summarized in Section 5.2 is the first analysis of this kind reported for BARDOT data.
One of the main advantages of a label-free classification technology such as BARDOT is the fact that it can po-tentially recognize and classify bacterial species or strains for which there are no available antibodies or genetic mark-ers. It is well known that some infectious agents are char-acterized by a high mutation rate, which can influence their pathogenicity. Therefore application of molecular-biology techniques for detection and classification are problematic owing to the dependence of these techniques on very spe-cific reagents.

BARDOT relies only on the physical properties of the bac-terial colonies and can acquire scatter patterns of colonies re-gardless of their genetic makeup; therefore it can be adapted to automatically recognize any new forms of the pathogens of interest, just by retraining the classifier using a new set of scatter patterns formed by colonies. However, the clas-sification approach currently used with BARDOT relies on supervised training. In order to retrain the classifiers em-ployed by BARDOT one would have to acquire a pure iso-late of an unknown pathogen, measure the light-scattering characteristic of the colonies, and use the scattering features to define a new class. In practice, isolating a new class of bacteria, and subsequently obtaining a sufficient number of training samples, may turn out to be impractical or even impossible. The described situation also brings a procedu-ral and logical conundrum: one would have to employ an independent testing procedure to find that a certain colony represent a new class, not present in the BARDOT library.
The outlined problem could be solved if the analysis strat-egy employed by BARDOT allowed for novelty detection prior to or simultaneous with the process of classification. If a novelty detection procedure were successfully imple-mented, BARDOT methodology would be capable of rais-ing an alarm when a new class of pathogens was detected in tested samples. This crucial advancement in the treat-ment of BARDOT patterns coupled with the simplicity of BARDOT measurement would make this technique espe-cially attractive for integration with highly automated sys-tems operating for long periods without human supervision.
If all samples of the known classes are considered as posi-tive, and all samples of the missing classes are considered as negative, then the problem can be cast as a one-class prob-lem with multiple subclasses with samples available for all subclasses of the positive class and no samples for the nega-tive class. In this setting, the missing samples of the negative class can be considered as anomalies/novelties. One promis-ing approach that has been heavily explored in this domain is the support vector domain description technique (SVDD) [14]. This method seeks to fit a tight spherical boundary into the data to include most of the samples and reject possible outliers. We also have more traditional techniques based on density estimation to deal with one-class data. This group usually fits a Gaussian density into the data, and rejects a sample as an outlier if the likelihood is below some threshold. Finally, we would like to include a third group of techniques derived from discriminative classifiers into our discussion to evaluate the performance of these classifiers for identifying future samples.
Support vector domain description (SVDD) is a kernel-based approach for estimating the support of a distribution, by fitting a spherical boundary around the target dataset [14]. To avoid accepting outliers the volume of the sphere is minimized. This problem is formulated as a constrained optimization problem with the data included in the problem in dot-product form. Hence the kernelized form of the prob-lem is readily obtained [17]. Different kernel functions can be used to change the ball-shaped spherical boundary into more flexible boundaries, which allows for moderately pre-cise control of the estimated support of the data. In what follows we review the basic theory for SVDD and discuss its use for learning with non-exhaustive training datasets as outlined in this study.

We assume that each sample for the target class is char-acterized by a feature vector x i  X  R d , where d is the dimen-sionality of the feature space, i = 1 ,...,n , and n is the num-ber of training samples available for the target class. The dot product between two samples is denoted by ( x We want to minimize the radius of the sphere while making sure all data remains within the sphere. This is cast as the following constrained optimization problem. where r is the radius of the sphere and a  X  R d is its center. The parameter C controls the trade-off between the volume of the sphere and the total error induced by the samples left outside the sphere. The dual form of the problem in (1) is, where  X  i are the Lagrange multipliers and the center of the sphere a is expressed as the linear combination of the sam-ples x i , i.e. a = P n i  X  i x i . Therefore only samples x nonzero  X  i is needed in expressing a . These samples are called support vectors.

A new sample z is accepted when its distance to the center is smaller or equal than the radius: k z  X  a k 2 = ( z  X  z )  X  2
A sphere is certainly not flexible enough to fit data with complex shapes. Since the data x i are in the dot-product form in both the training and testing phases, using the ker-nel concept initially introduced for support vector machines (SVM) [17], the dot products in (2) and (3) can be replaced by a predefined kernel function to obtain more flexible mod-els. Replacing dot products with a predefined kernel cor-responds to implicitly mapping the data from the input space into a new feature space and evaluating (2) for this new feature space. The spherical shape obtained in the fea-ture space may correspond to arbitrary shapes in the input space. Throughout this study we used the Gaussian kernel, K ( x i ,x j ) = exp (  X  X  x i  X  x j k 2 / X  2 ). Adjusting the width of the kernel  X  yields arbitrarily different shapes in the input space.

Our discussion of SVDD so far has been limited to one-class problems with only one subclass available. Recall that we are dealing with a one-class problem with samples avail-able from all subclasses of the positive class, where positive class here is defined as the superset of all classes available in the training dataset. One way to approach this prob-lem is to fit all class data with one boundary, but this will lose potentially useful subclass information. A more effec-tive strategy would be to fit one spherical boundary to each subclasses of the positive class, i.e. to each of the classes in the training dataset, and reject a new sample as belonging to the negative class, i.e. a missing class, if the sample does not fall inside the support of any subclasses in the positive set.
In this approach the data are distributed according to a probability density function f ( x |  X  ). For a new sample z the likelihood is computed as ` ( z ) = f ( z |  X  ) and z is considered an outlier if ` ( z ) is less than some predefined threshold  X  . The density function f ( x |  X  ) is usually not known. The most popular choice for modeling an unknown distribution is a Gaussian model. The parameters  X  = {  X ,  X  } are estimated using the training data and after removing the terms in the log-likelihood expression that do not depend on z , the deci-sion function to accept or reject a new sample z is obtained as follows: minant of the covariance matrix  X .

For non-exhaustive learning, a Gaussian density is fit for each class,  X  k , k = { 1 ,...,K } in the training dataset and the decision function in (4) is updated as follows: where K is the total number of known classes.
Discriminative models of the form f ( x ) = w T x + w 0 are used for the design of binary classifiers. The classifier coef-ficient w and the bias term w 0 are learned from the training data by optimizing an objective function usually consisting of two conflicting goals: minimizing the complexity of the classifier and minimizing the error committed on training samples. Support vector machine (SVM) [17], relevance vec-tor machine (RVM) [16], linear Fisher X  X  discriminant (LFD) [6], and logistic regression [7] are all members of this group. The binary decision function for a new sample z is expressed as
In this binary setting the range of the discriminant func-tion f ( x ) is divided into two regions, positive and negative. A third region can be introduced to accommodate samples with discriminant values close to the decision boundary. Un-der this setting the decision function in (6) can be updated as where is a designated number.

For the multiclass problem, a one-against-one approach can be taken to design K ( K  X  1) / 2 binary classifiers, where K is the number of classes. A new sample, z , is classified by all K ( K  X  1) / 2 and after each classification the sam-ple is assigned to either the current positive/negative class or labeled as indecisive using the decision function in (7). Then the total number of times z is assigned to each of the K classes is computed. If the maximum of these numbers is less than the total number of times a sample is labeled as indecisive, then the sample is rejected as belonging to a potentially missing class.

This concludes our review of the potentially useful meth-ods from the literature to deal with a non-exhaustive train-ing dataset. We will implement these techiques to obtain a baseline performance in Section 5 when evaluating the per-formance of the proposed work with the bacteria dataset.
A training dataset is non-exhaustive when one or more of the classes with informational value are not represented by any samples. When a classifier is trained with this dataset, a sample of a yet unseen class will be misclassified with probability one, making the corresponding learning problem ill-defined. A two stage design of the classifier can help alle-viate this problem and make learning with a non-exhaustive training dataset a more reasonable goal to achieve. The first stage is a one-class classifier, which identifies whether the sample is a member of one of the classes in the training set or a member of a yet unseen/missing class. If the sample is confirmed as belonging to one of the classes in the training set, then the sample is fed to the second stage where it is classified to one of the existing classes. If the sample is con-firmed as belonging to an unseen class at the first stage, an alert is raised and the sample is saved for follow-up analysis.
The classifier of the second stage can be trained using any supervised classification algorithm. For an exhaustive training dataset we implemented four supervised classifiers from the literature for this task and summarized the results in Table 5.2. What makes this problem challenging is the design of the first stage classifier, where we only have sam-ples from the known classes, i.e. the positive class. In what follows we present a Bayesian approach to learning with a non-exhaustive training dataset. We start with the notation first. The symbols  X ,  X , and  X  denotes the set of all , known and missing classes respec-tively with  X  =  X   X   X ; A , K and M are their corresponding cardinalities with A = K + M . The conditional distribu-tion of a class  X  i  X   X  is defined by the density function f ( x |  X  i ) with  X  i being distributed according to a common prior  X  (  X  |  X  ) shared across all classes. Let x ij  X  R is the dimensionality of the feature space, j = { 1 ,...,n be the number of training samples for class  X  i ,  X  i  X   X . For notational simplicity all samples belonging to class  X  i  X   X  are denoted in the matrix form as X i = [ x i 1 ...x in j
The decision that minimizes the Bayes risk under a 0/1 loss-function assumption assigns a new sample z to the class with the highest posterior probability. More specifically, where i = { 1 ,...,A } . The classifier obtained by evaluating this decision rule is known as maximum a posteriori classifier (MAP). Using Bayes X  rule the above decision rule can be rewritten as follows: p ( z ) is the evidence . The evidence p ( z ) is same for all classes and hence can be removed from the above formulation.
The class distributions f i ( x |  X  i ) are not known. Before we estimate f i ( x |  X  i ) for  X  i  X   X , i.e. known classes, us-ing the training dataset, we make some general assump-tions that we can also carry to f i ( x |  X  i ) for  X  most common and also an effective way to deal with data of unknown nature is to assume normal distributions for all classes,  X  i  X  N (  X  i ,  X  i ),  X  i = {  X  i ,  X  i } . Here we go one step further and assume that all classes share the same covari-ance matrix,  X  i  X  N (  X  i ,  X ),  X   X  i  X   X , where  X  i and  X  are estimated using the training samples from the known classes as follows:
Here e n i is a vector of ones with the size of the vector equivalent to the number of training samples in class  X  i the bar sign on  X   X  i and  X   X  indicates the estimated values.
Since  X  is already estimated from the data and thus known, the prior  X  (  X  |  X  ) is only defined over the mean vectors  X  Next, we assume a Gaussian prior over  X  i with mean m and covariance matrix S , i.e.  X  i  X  N ( m,S ),  X  = { m,S } . Both m and S are estimated from the samples  X   X  i ,  X  i  X   X , which were in turn estimated in (11). Once  X  m and  X  S are obtained we can use the prior distribution  X  (  X  |  X  m,  X  S ) of the mean vec-tors to simulate the space of missing classes, i.e.  X . More specifically, using this distribution we generate a very large number of samples with each sample corresponding to the mean vector of a supposedly missing class. To differentiate between the mean vectors estimated from the data and the mean vectors simulated from the prior, we use  X   X  latter. Similarly, we use  X   X  to differentiate the set of simu-lated classes from the set of missing classes  X . The Bayes decision function in (9) is implemented using real as well as simulated classes and a new sample z is classified according to the following decision function
That is, if the class that maximizes the posterior is a sim-ulated class, then z is rejected as belonging to a potentially missing class.
We are estimating two sets of parameters. First, {  X  i ,  X  } ,  X  i  X   X  using x ij  X  R d with n i samples and then { m,S } using  X   X  i  X  R d with K samples, where K is the number of known classes. For large d , feature selection will help alleviate the problem of the curse of dimensionality [5] and will significantly improve parameter estimation for the first set of parameters. However, the estimation of m and S will continue to suffer from the limited number of samples K . For d &gt; K the inverse of  X  S does not exist. Even though the inverse of S is not required to generate samples to simulate the mean vectors of the missing classes, an ill-conditioned matrix  X  S will prevent us from evaluating  X  i (  X  i One possible solution would be to use simpler covariance models involving the trace or the diagonal forms of  X  S , but this will eliminate useful correlation information between features. Therefore we restrict our model by assuming that all classes are a priori likely given  X  S and  X  m and drop  X  in (9). The maximum a posteriori classifier (MAP) in (9) becomes a maximum likelihood (ML) with this change and the decision function in (9) becomes
Algorithm 4.1. Algorithm for Training (i) For each class in the training dataset, i.e.  X  (ii) Then estimate m and S for  X  (  X  | m,S ) using estimates (iii) Generate M samples from the prior distribution  X  (  X  |  X  m,
Algorithm 4.2. Algorithm for Detection (i) Evaluate `  X  real = max i f i z |  X   X  i ,  X   X  for  X  (ii) Evaluate `  X  sim = max i n  X  f i z |  X   X  i ,  X   X 
A total of 28 subclasses from five different bacteria classes were considered in this study. The classes available are Es-cherichia coli, Listeria, Salmonella, Staphylococcus and Vib-rio . Table 1 shows the list of subclasses and classes consid-ered in this study together with the number of samples col-lected for each subclass using the BARDOT system detailed in Section 2.
In this experiment we design and implement several state-of-the-art classifiers for the task of classifying already known bacteria subclasses. The objective here is to train a 28-class classifier to classify new samples at the subclass level, i.e. when a yet unseen sample from one of the 28 subclasses emerges, the classifier accurately assigns the sample to its subclass of origin.
Classifiers are validated using a 10-fold cross-validation approach as follows. First, we randomly shuffle the training data and split the data into 10 groups using stratified sam-pling to make sure that each of the ten folds has roughly an Table 1: The 28 subclasses from five genera (classes) considered in this study. The last column lists the number of samples collected for each strain using the BARDOT system. equal number of samples from each subclass. Then at each stage one fold is left out as testing data and a classifier is trained with the remaining 9 groups. Next, the test data are classified with this classifier and class labels assigned to each sample are recorded. Once all 10 groups are tested, the estimated labels are compared with the actual labels and the classifier accuracy is obtained. This process is repeated ten times and the classifier accuracies averaged over ten runs are recorded together with the standard deviations as the 10-fold cross-validation performance of the classifier.
The classification methods considered are support vector machines (SVM) [17], linear Fisher X  X  discriminant (LFD) [6], maximum likelihood classifier implemented with Gaussian distributions (ML), and support vector domain description (SVDD) [14]. The first two (SVM, FLD) uses discriminative models, SVDD is a one-class classifier and ML Classifier is density-based approach. In what follows we briefly discuss the classifier design and parameter selection for each algo-rithm.

Support vector machine: SVM is a binary classifier that optimizes a hyperplane of the form f ( x ) = w T x + w maximize the margin between the two classes. Multi-class design with SVM can be achieved using either the one-against-one or the one-against-all scheme. Here we adopted the one-against-one approach. For the 28-class problem (28  X  27) / 2 = 378 unique pairs of classes exist. Therefore we trained 378 binary classifiers during a single run of the training phase. For each binary classification problem one of the classes is assumed positive and the other one negative. A new test sample z is classified by all 378 classifiers and after each classifier z is assigned to the class denoted posi-tive if f ( z ) &gt; 0, otherwise it is assigned to the class denoted negative. At the end, the number of times z is assigned to each class is counted and z is permanently assigned to the class with the highest number of hits.

The tuning parameters for SVM are the type of the kernel function, its parameter(s) and the cost C of misclassifying a sample. Here we used the popular Gaussian kernel function K ( x i ,x j ) = exp  X  X  x i  X  x j k 2 / X  2 . The width of the kernel function  X  and C are chosen jointly to optimize the 10-fold cross-validation performance of the classifier. We considered different values for C = { 10 , 100 , 1000 } . The pair of values with the best 10-fold cross-validation performance is found as (  X  = 1 ,C = 100).

As a standard data-preprocessing step we normalize each feature to between -1 and 1 and implement the classifier with the normalized data.

Linear Fisher X  X  discriminant: LFD is a binary classifier that projects the high-dimensional data onto a line and per-forms classification in this one-dimensional space. The pro-jection, w , is chosen such that the ratio of the scatter matri-ces (between and within classes) is maximized. Like SVM, the classifier function can be expressed as a hyperplane of the form f ( x ) = w T x + w 0 . For the multiclass implementa-tion of LFD we follow the same approach outlined above for SVM.

The only design parameter for LFD is the regularization constant. Regularization of the classifier coefficients w is achieved by adding a small scalar times the identity matrix to the within-class scatter matrix, i.e.  X  S w = S w +  X I , where I is the identity matrix and  X  is the tuning parameter used to regularize the classifier. Here  X  = 5  X  10  X  4 is chosen from the set of  X  = { 5  X  10  X  6 , 10  X  6 , 5  X  10  X  5 , 10  X  5 10  X  4 , 5  X  10  X  3 , 10  X  3 , 5  X  10  X  2 , 10  X  2 } as the value that optimizes the 10-fold cross-validation performance.
Support vector domain description: The SVDD approach is used to estimate the support of each of the 28 subclass distributions. Here SVDD is used as a supervised classifier. A new sample z is assigned to the class whose center a is closest to z . The tuning parameters for this approach are the width  X  of the Gaussian kernel function and the cost C of rejecting a sample as an outlier. As with the SVM classifier above,  X  , and C are chosen jointly to optimize the 10-fold cross validation performance of the classifier. We considered five different values for  X  = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 5 } and three different values for C= { 1, 10, 100 } . The pair of values with the best 10-fold cross-validation performance is found to be (  X  = . 7 , C = 10).
Bayes maximum likelihood classifier: This approach as-sumes Gaussian distributions with a common covariance ma-trix for each subclass. The covariance matrix and mean vec-tors are estimated using equations (10) and (11).

The dimensionality of the dataset is d=240. We are esti-mating 240 2 = 57600 parameters for the common covariance matrix and 28  X  240 = 6720 parameters for the mean vec-tors using a total of 2054 training samples. To avoid the curse of dimensionality [5], a sequential forward-backward propagation feature-selection algorithm is implemented with this classifier in a wrapper framework. This approach starts with an empty subset and performs a forward selection suc-ceeded by a backward attempt to eliminate a feature from the subset. During each iteration of the forward selection exactly one feature is added to the feature subset. To deter-mine which feature to add, the algorithm tentatively adds to the candidate feature subset one feature that is not already selected and tests the 10-fold cross-validation performance of the classifier built on the tentative feature subset. The feature that results in the highest classification accuracy is added to the feature subset. During each iteration of the backward elimination, the algorithm attempts to eliminate the feature whose elimination results in the highest classi-fication accuracy. This process continues until no improve-ment is gained.

We run this feature-selection algorithm 10 times and record the features selected after each run. At the end of the ten runs we sort the features according the total number of times each feature is selected descending order and choose the top 80 features for the final classifier. A new sample z is classi-fied using the decision function in (13).
The 10-fold cross-validation performance obtained by each of the four classifiers is shown in Table 2. Results show that LFD and SVM are equally competitive, with LFD yielding slightly better results. The maximum likelihood classifier looks promising but SVDD is not competitive as a super-vised classifier.
 Table 2: The classifier accuracies achieved by the four classifiers averaged over ten runs.
In this experiment we implement classifiers to detect sam-ples of known bacteria subclasses, i.e. subclasses represented in the training data, in order to separate them from samples belonging to unknown classes.
In the previous experiment we assumed the training dataset to be exhaustive and implemented several classifiers to clas-sify samples of the bacteria subclasses. That was mainly a classification task and as such we used classifier accuracy as a performance metric to evaluate classifiers. In this experi-ment we assume that the training dataset is non-exhaustive. The goal is now to design a classifier that accurately de-tects samples from the known classes and rejects samples of the unrecognoized classes. In this framework, classifiers can be more properly evaluated using sensitivity and specificity metrics. Here sensitivity is defined as the number of samples from known classes accepted by the classifier divided by the total number of samples from known classes, and specificity is defined as the number of samples from missing classes re-jected by the classifier divided by the total number samples from missing classes. For each classification method multi-ple sensitivity and specificity values are obtained at different operating points to plot the receiver operating characteris-tic (ROC) curves. When comparing different classification methods, the method with the largest area under the curve is considered the best. The 10-fold cross-validation approach of the previous experiment is modified to conform it to the current task as follows.

First, we randomly shuffle the training data and split the data into 10 groups using stratified sampling to make sure that each of the ten folds has roughly an equal number of samples from each strain class. This step is the same as the one in the previous experiment. Then one group is left out as testing data and from the remaining 9 groups, samples of each of the 28 subclasses are removed, one subclass at a time, to create 28 different non-exhaustive training sets. This way all 28 subclasses are considered missing in turn, albeit only one at a time. Next, 28 different classifiers are trained, one for each of the 28 non-exhaustive training sets, and test samples are classified with these classifiers as ac-cept or reject . Once all ten groups are covered this way, the sensitivity and specificity of the classification method is computed by averaging out the sensitivities and specifici-ties achieved by the 28 individual classifiers. This process is repeated ten times and the sensitivity and specificity val-ues averaged over ten runs are recorded together with the standard deviations as the 10-fold cross-validation perfor-mance of the classification method for detecting samples of the known bacteria subclasses.
The classification methods of Experiment 1 are considered for this experiment as well. Following the discussion in Sec-tions 3 and 4, this time they are cast as one-class classifiers.
SVM and LFD: The binary decision rule used in Exper-iment 1 for SVM and LFD is now modified to include an indecisive range for f ( x ) as in (7). Once a test sample z is classified by all K ( K  X  1) / 2 = 378 binary classifiers us-ing h ( z ) in (7), it is rejected if the total number of times it is denoted as indecisive outnumbers any individual class assignment. The same set of parameters optimized in Exper-iment 1 for SVM and LFD are used for this part. Multiple operating points on the ROC curve are obtained by varying from 0 to 0 . 2 in increments of 0 . 01.

SVDD: Previously a new sample z was assigned to the class whose center a is closest to z . This time a new sam-ple z is accepted if it falls within the support of one of the bacteria subclasses; otherwise it is rejected. The param-eter C was optimized as 10 in Experiment 1. The same value is used here. Multiple operating points on the ROC curve are obtained by considering the set of values  X  = of the kernel function. Each different value of  X  corresponds to a different operating point on the ROC curve.

ML classifier: The same set of 80 features selected in Ex-periment 1 for ML classifier is used. The decision to accept or reject a new sample is made by evaluating the decision function in (5). Since common covariance matrix is assumed for all classes, the determinant term in (5) will be same for all classes and hence can be dropped.

ML classifier with real and simulated bacteria subclasses (MLS): The same set of 80 features selected in Experiment 1 for the ML classifier is also used here. The classifier is trained following the steps in Algorithm 4.1. Since sub-classes are considered missing, one at a time, as described in Section 5.3.1), m and S are estimated using  X   X  i remaining 27 bacteria subclasses. Then the steps in Algo-rithm 4.2 are followed and a new sample z is accepted if the likelihood is maximized for one of the real subclasses and re-jected if the likelihood is maximized for one of the simulated subclasses. Multiple operating points on the ROC curve are obtained by generating varying numbers of mean vectors with the prior in step 3 of Algorithm 4.1, i.e. values of M = considered in this experiment.
The ROC curves obtained for classification methods are plotted in Figure 2 together with the corresponding error bars. In contrast to Experiment 1, the discriminative clas-sifiers (SVM and LFD) performed very poorly in this ex-periment. The corresponding ROC curves are only slightly better than a random classifier. SVDD does not look very promising either. The ML classifier implemented only with known classes show some promise but the proposed approach is by far the best.
 Figure 2: ROC curves obtained for the five classifi-cation methods considered in Experiment 2.

The training takes place offline. Even though offline train-ing time is not critical, we provide some numbers to serve as comparison between SVDD and MLS algorithm. All five runs of the MLS algorithm took little less than two days of computer time. The five runs for the SVDD took less than five hours. All experiments are run on an Intel-based com-puter (Intel Core 2 Duo T9300 2.50Ghz). The time taken for classification is negligible for both approaches.
We ran the MLS algorithm for a varying number of sim-ulated subclasses. The specificity and sensitivity values at M=50,000 are 84% and 70% respectively. Generating speci-ficities above 84% will require increasing M beyond 50,000, which will further increase the computational time.
We believe SVDD suffers mainly from independent mod-eling of each subclass. In the current framework the support for each subclass is estimated independent of other sub-classes. As a result, the supports of some classes overlap with each other. A new sample frequently falls within the support of more than one subclasses at the same time. This negatively impacts the prediction accuracy of SVDD.
The number of samples as well as the number of subclasses available in the training dataset is critical to a robust im-plementation of the proposed MLS technique. As we have more samples from each subclass, more reliable estimates of the mean vector and covariance matrix can be obtained for each subclass. In addition to improving the estimate of the conditional distribution for each subclass, this will more importantly help with the modeling of the prior. The pa-rameters of the priors are estimated using the mean vectors estimated with each subclass data. However, this does not address the problem of limited sample size for the estima-tion of the prior mean and covariance matrix. In the current study these parameters are estimated using only 27 samples (one subclass is considered missing in turn at a time) in an 80-dimensional space.
In this study we proposed an approach for learning with a non-exhaustive training dataset. The technique is based on Bayesian modeling of subclasses with real and simulated data. The training dataset is used to estimate the condi-tional distributions of each subclass. A Gaussian distribu-tion with a common covariance matrix is assumed for all subclasses. A Gaussian prior is defined on the mean vectors and the parameters of this distribution are estimated using the estimates of the mean vectors obtained for each class conditional distribution. A large number of samples from this prior are generated to simulate the set of missing sub-classes. Finally, a Bayesian maximum-likelihood classifier is implemented using real and simulated data.

This research was mainly motivated by the need for real-time detection of bacteria cultures in food chains. We ap-plied our technique to the dataset collected for this purpose, which was composed of samples from 28 bacteria subclasses. Even though the proposed approach is based on relatively strong assumptions, results with this dataset clearly favor the proposed approach over the state of the art, which is mainly centered around support vector domain description and well known discriminative classifiers.

Before this system can be deployed in real time to safely and accurately detect samples of potentially harmful bac-teria, the performance of the classifier needs to improve. Toward this end, we believe relaxing some of the model as-sumptions might help. More specifically, the common co-variance matrix could be somewhat restrictive. Dropping this assumption and defining another prior for the covari-ance matrix (in addition to mean vectors) simulates more flexible models, and might facilitate modeling of complex data distributions. [1] E. Bae, P. P. Banada, K. Huff, A. K. Bhunia, J. P. [2] P. P. Banada, S. Guo, B. Bayraktar, E. Bae, B. Rajwa, [3] P. P. Banada, K. Huff, E. Bae, B. Rajwa, [4] B. Bayraktar, P. P. Banada, E. D. Hirleman, A. K. [5] R. E. Bellman. Dynamic Programming . Princeton [6] K. Fukunaga. Introduction to Statistical Pattern [7] T. H. J. Friedman and R. Tibshirani. Additive logistic [8] J. Mu  X noz-Mar  X  X , L. Bruzzone, and G. Camps-Valls. A [9] B. Rajwa, M. Venkatapathi, K. Ragheb, P. P. Banada, [10] B. Sch  X  olkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. [11] B. Sch  X  olkopf, R. Williamson, A. Smola, [12] E. J. Spinosa and A. C. Carvalho. Support vector [13] H. B. Steen. Light scattering measurement in an arc [14] D. M. J. Tax and R. P. W. Duin. Support vector [15] J. Theiler and D. M. Cai. Resampling approach for [16] M. E. Tipping. The relevance vector machine. In [17] V. N. Vapnik. The Nature of Statistical Learning [18] P. J. Wyatt. Identification of bacteria by differential [19] P. J. Wyatt and D. T. Phillips. Structure of single
