 Word meanings change over time. Detecting shifts in meaning for particular words has been the focus of much research recently. We address the complementary problem of monitoring shifts in vocab-ulary over time. That is, given a small seed set of words, we are interested in monitoring which terms are used over time to refer to the underlying concept denoted by the seed words.

In this paper, we propose an algorithm for monitoring shifts in vocabulary over time, given a small set of seed terms. We use distri-butional semantic methods to infer a series of semantic spaces over time from a large body of time-stamped unstructured textual docu-ments. We construct semantic networks of terms based on their rep-resentation in the semantic spaces and use graph-based measures to calculate saliency of terms. Based on the graph-based measures we produce ranked lists of terms that represent the concept underlying the initial seed terms over time as final output.

As the task of monitoring shifting vocabularies over time for an ad hoc set of seed words is, to the best of our knowledge, a new one, we construct our own evaluation set. Our main contributions are the introduction of the task of ad hoc monitoring of vocabulary shifts over time, the description of an algorithm for tracking shift-ing vocabularies over time given a small set of seed words, and a systematic evaluation of results over a substantial period of time (over four decades). Additionally, we make our newly constructed evaluation set publicly available.
 H.3.1 [ Content Analysis and Indexing ]: Linguistic processing; H.3.3 [ Information Search and Retrieval ]: Retrieval models Vocabulary shift; distributional semantics
Word meanings change over time [11, 27]. Detecting shifts in meaning for particular words has been the focus of much research recently [11, 13, 14, 18, 19, 29]. In this paper we address the com-plementary problem of monitoring shifts in vocabulary over time. Rather than taking a word as an anchor to monitor its (shifts in) meaning over time, we take the meaning as an anchor, and monitor the evolving set of words that are used to denote it. As an example, consider music storage media. Nowadays, we carry music with us on iPods and mp3 players. Before that there were compact discs. Prior to cds there were records, and music cassettes. Few of the words that were used in, say, the 1950s to describe the media used for storing music are still in use today. Following this example, the question we set ourselves to answer is  X  X hat words were used previously, where nowadays the words  X  X p3 player X  and  X  X Pod X  are used? X  An algorithm for monitoring vocabulary shifts over time has the words  X  X Pod X  and  X  X p3 player X  as its input; as output it produces ranked lists of words per time period, e.g., every 5 years, of the words in that period that represent the concept underlying the initial input words. In what follows, we refer to such ranked lists of words per time period as vocabularies . The initial set of input terms we call seed terms .

Not all concepts evolve as dramatically as the sound carrier ex-ample above does, where the entire vocabulary changes in the course of a few decades. Often, many terms in the vocabulary remain rel-evant over time. A successful system for monitoring vocabulary shifts over time should strike a balance between an adaptive strat-egy that responds to changes in vocabulary, and a more conserva-tive approach that keeps the vocabulary stable.

The problem setting we address is inspired by collaborations with digital humanities scholars in the field of history. Changes in discourse over time are a popular topic of studies in the human-ities [10, 12, 15, 23]. Lists of keywords are usually maintained manually. However,  X  X f]inding the right keywords demands expert knowledge of the field of study and a great deal of perseverance and creativity X  [15]. The methods for finding shifts in vocabulary over time that we propose in this paper are aimed at automating this task in a time-aware fashion. The resulting vocabularies are returned to the humanities scholars, as an indication of changes in discourse in the underlying corpus. They may be used for exploratory ends, to discover unfamiliar relevant historical terms. Additionally, as discussed in our future work section  X 6, if the vocabularies are of sufficient quality, they can be used for time-aware query expansion in an document retrieval setting for an historical corpus.
There has been extensive work on the related but different prob-lem of concept drift in the context of ontologies and taxonomies; see, e.g., [27]. Any semantic ontology of terms should adapt over time in order to keep up with changes in meaning of the terms it contains. In this paper, however, we approach concept drift from a user perspective and not from an ontology perspective. This means that we do not assume pre-defined ontologies to be available and we do not aim to infer ontologies. Our primary motivation is to track evolving vocabularies over time for a user-provided topic of interest. This motivation leads to the following set of requirements: 1. Words as retrieval unit  X  Rather than outputting documents, 2. Ad hoc  X  An algorithm for monitoring shifts in vocabulary 3. Broad time coverage  X  An algorithm for monitoring shifts 4. Comprehensible outcome  X  The output produced by an al-We note that an additional implication of the ad hoc requirement (requirement 2 in the list) is that no in-depth historical or domain knowledge of a user should be necessary. I.e., a user should not be required to have extensive knowledge of the concepts the input words are about nor of the underlying corpus. Rather, an effec-tive method for monitoring shifts in vocabulary over time should provide new insights about the concepts and the corpus.

The comprehensible outcome requirement (requirement 4) en-tails that an optimal rate should be found for emitting vocabularies, regardless of a method X  X  internals. If the rate is too low, too many vocabularies are produced, which leads to too much repetition. A rate that is too high would cause interesting shifts to go unnoticed. Precursory discussions with domain experts in the area of the his-tory of ideas revealed that five years periods were deemed optimal.
We propose an algorithm for monitoring shifts in vocabularies over time given a small user-provided set of seed terms and a period of reference. We note that this task is related to, but different from, tracking topics over time [9, 28], where topic models such as LDA and PLSA are used to monitor changes in a predefined number of topics. A crucial difference between topic modelling approaches and the method we propose is that, rather than relying on a pre-defined number of fixed topics, we allow for ad hoc queries.
Briefly, our proposed algorithm proceeds as follows. We first use distributional semantic models to infer a series of semantic spaces over time from a large body of time-stamped textual documents. We then construct semantic networks of terms based on their repre-sentation in the semantic spaces and use graph-based measures to calculate saliency of terms. Finally, we output shifting vocabularies over time X  X .e., for a small set of seed words we output ranked lists of terms for a consecutive series of periods in time. The words in the vocabularies are meant to denote the same concept as the seed words do. As there is, to the best of our knowledge, no evaluation
We note that alternatively, the optimal rate of emitting vocabu-laries could be determined programmatically. In theory, it could even differ between sets of seed words. The evaluation of such an approach would require extra, non-trivial annotator effort and we consider it outside the scope of the present paper. set available that allows for the intrinsic evaluation of monitoring shifts in vocabularies over time, we construct our own.

Our main contributions are: In the next section,  X 2, we discuss related work. In  X 3 we describe our method of tracking vocabularies over time. Our experimental setup is detailed in  X 4 while the results of the experiments are pre-sented and analysed in  X 5. In  X 6 we conclude.
In this section we describe previous research related to the vari-ous aspects of our method of monitoring shifts in vocabularies over time.
Topic models, like LDA and PLSA have been used extensively to monitor topics over time, starting with seminal work in [6, 28]. In [12] topic models are used to model the history of scientific ideas through time. The setting is similar, but different to the one ad-dressed here, as word distributions of topics are inferred from the entire dataset and vocabulary shift is not modelled directly. Rather, the changes over time are modelled as shifts in the probability dis-tribution of topics over the years. A related setting is addressed in [9] where topics and vocabulary are monitored over time.
The most important difference between topic model-based ap-proaches, such as the ones discussed above, and the method we present in this paper is that our approach allows for an ad hoc set-ting. Topic models aim to infer a fixed set of latent topics from a corpus. This is the case even when non-parametric methods are em-ployed [7], for which the number of topics is not fixed but inferred from the data. The non-parametric models allow for more flexibil-ity, but once the algorithm has ran, there is a fixed set of topics it inferred. The inferred topics can be investigated to see interesting patterns over time, but if the topic of interest to the user is not in the inferred set of topics, there is no way around this.
Evaluation, from the perspective of the topics, is typically extrin-sic, rather than intrinsic. The top-10 words for a selection of topics is shown in [12] but not evaluated. In [9] perplexity of the inferred topics is used as evaluation metric.
In the humanities, changing vocabularies are researched as well, in the field of intellectual history or the  X  X istory of ideas X . In the context of the history of ideas, a distinction is made between the intension of an idea and its extension in [4]. The intension is the meaning of a concept, the extension comprises its mentions:  X  X he extension of [a] concept differs through time. When confronted with certain changes in extension in the data, one likely conjecture is that the meaning of the concept [. . . ] has changed X  [4]. In this paper we regard the words used to denote this meaning as its ex-tension, rather than sentences or entire articles as in, e.g., [27]. Al-though the intension of a concept changes as its extension changes, we assume that the intension changes gradually over time (e.g., the intension of the concept of nuclear weapons is relatively stable over time, while the names of particular instances, and the words used to refer to nuclear weapons might change over time as the tech-niques involved evolve). By monitoring shifts in vocabularies over time, we aim to monitor shifts in the extension of a concept. We assume that the intension of a concept is continuous enough over time to allow for such monitoring. By adhering to this assumption we follow e.g. Kuukkanen who introduces a distinction between the core of a concept and its margins when discussing conceptual change:  X  X he main idea is that conceptual continuity requires the stability of the core of the concept, but not necessarily that of the margin, which is something that enables a description of context-specific features X  [20]. While we do not explicitly model the core or margin of concepts, we do assume conceptual continuity.
The goal of topic tracking systems, given a stream of documents, is to extract documents from the stream which are relevant to a set [1] or entities [8]. As events and entities may evolve over time, many adaptive document filtering algorithms have been proposed [3, 16, 25]. A sliding window approach is used on a stream of doc-uments in [25], a component we also use in our method of moni-toring shifting vocabularies over time in  X 3.

Document filtering algorithms typically contain a profile of the events or entities they monitor in the form of a (weighted) list of words which can adapt over time. Maintaining such a profile is clearly analogous to the task addressed in this paper, although we aim to track the words that are used to describe the meaning of a concept, rather than events or entities. Furthermore, it is important to note that in our present setting of vocabulary tracking the aim is to list terms that are semantically very similar to one another, while in the document filtering case it is beneficial for a filtering profile to cover a range of aspects as diverse as possible concerning the event or entity in question.
Research on detecting semantic shifts for words has seen a surge of interest recently. In [18] word vectors are trained on a corpus spanning over more than a century, with word2vec [21]. The vec-tors are trained on an incrementally growing time window, rather than a sliding window as we propose to do here. Several exam-ples are shown to illustrate that dramatic semantic changes over time can be detected by monitoring the distance between the word vector of word in the initial model, that contains the least recent documents, and the vector from models trained on the windows including more recent material. Similarly, in [29] words are moni-tored over centuries. A number of examples is presented that show that changes in meaning as well as additional meanings of words can be detected. In [11] semantic change between words is mea-sured with a distributional semantics method. The Google Books Ngram corpus is used to construct co-occurrence vectors of words in two decades (the 1960s and the 1990s, which is roughly the same time frame we use in our experiments). The task is to detect whether or not words have undergone a drastic semantic change, and human annotators were asked to annotate for a hundred words whether or not their meaning changed over the decades. In [13, 14] co-occurrence statistics are used to find related words to a specific term, which are monitored to find the sudden shifts in meaning.
We should note that, though monitoring the shifts in meanings of words over time is very related to the setting in these papers, there is a key difference in what we are trying to achieve. To il-lustrate, consider the main example used in [18]: the word  X  X ay X . The meaning of this word shifted considerably over the last cen-tury. Rather than focussing on the word  X  X ay X  itself to monitor its shift in meaning, the question we ask is: what words came in its place? Apparently, the meaning of the word  X  X ay X  evolved, and it now (largely) means something else from what it used to mean. So, which terms took its place? Which terms were used in a later time frame, to denote the meaning that was previously referred to by  X  X ay? X  Our aim is to track the concept underlying a particular set of seed words (of which there can be more than one). Crucially, in our adapative approach for monitoring vocabulary shifts over time, we allow the original seed words to disappear completely. How-ever, as the task in this work is related to the one addressed in, e.g. [13, 14], we construct our baseline accordingly.
Distributional semantic approaches are based on the intuition that words appearing in similar contexts tend to have similar mean-ings. Words are typically represented as vectors where the vectors incorporate information about the context. Recent advances in neu-ral network language models have led to new ways of computing word vectors, where more training material can be leveraged than was previously feasible. In our experiments in  X 4 we use word2vec [21] to infer word vectors. Word vectors, also referred to as word embeddings, embed words in a semantic space. This means that the word vectors for words with a similar meaning are close in the semantic space they are embedded in.

We note that using word2vec is not the only way to get dis-tributional semantic word representations. Methods based on co-occurrence have been used for tasks similar to ours as described above. An alternative more similar to word2vec is the GloVe algo-rithm [24]. We use word2vec in our experiments as it has proven to yield high-quality word embeddings [2, 22]. The same goes for the GloVe algorithm but it needs considerably more resources in terms of training time and memory consumption, which is a drawback given the large size of our corpus. There is no theoretical restric-tion, however, on the choice of distributional semantic model in the algorithms we propose in  X 3.
The evaluation used to assess the quality of the approaches dis-cussed above is frequently based on small number of positive ex-amples [12 X 14, 18, 19, 29]. Following [11] we perform explicit intrinsic evaluation, where we ask human annotators to judge the quality of the output of our algorithms directly.
 The research presented in this paper extends previous work in a number of ways. Firstly, rather than focussing on monitoring the change in meaning of particular words over time, as in e.g. [11, 13, 14], we monitor the underlying concept, by monitoring the set of words that is used to denote it over time. We describe an algorithm for constructing a semantic network of related terms and for main-taining this network over time. Secondly, we do not rely on a fixed set of topics or pre-defined ontologies, but allow for ad hoc input: a small set of input words, specified by the user. Thirdly, we per-form systematic intrinsic evaluation of our methods for generating shifting vocabularies over time.
In this section we describe our algorithm for monitoring shifts in vocabulary over time. By vocabulary we mean a ranked list of unique terms.
Our algorithms for monitoring shifts in vocabulary over time use three components: sliding time windows , generation algorithm and aggregation algorithm .

We use time windows of multiple years in length (we experiment with 5 and 10 year time windows in our experiments in  X 5) and ex-tract documents out of our corpus that were published within the time window. The window length is in years and every next win-dow starts one year later than the previous window. If we use, e.g., ten-year windows, and the overall time period starts in 1950, we have a 1950 X 1959 window, a 1951 X 1960 window, etc. From the documents published within a time window we compute a seman-tic model (see  X 2.5). So we have one semantic model for each sliding window in time. The computation of the semantic models is a pre-processing step. It is done only once for a given corpus.
As mentioned in  X 1 when discussing requirement 4, the opti-mal period for outputting vocabularies is five years. However, the sliding windows are one year apart. Because of this, our method for constructing vocabularies over time comprises two algorithms. The first algorithm, which we refer to as the generation algorithm , out-puts a series of vocabularies, one for each sliding time window, us-ing a semantic network from the semantic model constructed from the documents in the time window. The second algorithm, which we refer to as the aggregation algorithm , aggregates over the vo-cabularies generated by the generation algorithm to produce the final vocabularies for the desired time period.

The generation algorithm uses graph-based measures to extract the most salient words from a semantic network for a given time window. The salient words are used as input to the next iteration of the algorithm. In short, the generation algorithm takes the original user-provided words as its input and adaptively updates this seed set by iterating over the sliding time windows.

Our algorithms for generating shifting vocabularies over time are completely unsupervised. No labelled training data is needed. Fur-thermore, no pre-defined ontologies are necessary. Only a large amount of unlabelled data is needed to derive word vectors from.
In what follows we describe three methods of generating shifting vocabularies over time. The adaptive method uses both the gener-ation algorithm and the aggregation algorithm. The non-adaptive only uses the aggregation algorithm to aggregate over vocabular-ies generated from the sliding time windows. The hybrid method combines the vocabularies produced by the adaptive and non-adap-tive methods. As the sliding time windows are used by all three methods, we first turn to discussing these.
As detailed in  X 2.2 the intuition underlying our model for mon-itoring shifts in vocabulary over time is that word meanings, and the semantic relations between words, shift gradually and continu-ously over time [4, 20, 27]. To make use of this continuum when constructing semantic models, we split the time period we are mon-itoring in multiple time windows, and calculate a semantic model from each of these windows. I.e., we extract all documents from the corpus that were published in the desired time window and train a word2vec model on their text contents.

To be sensitive to rapid changes, it would be beneficial to have short time windows. However, previous research has proven that the quality of the semantic models inferred by word2vec is higher when more training data is used [21]. We solve this conflict in re-quirements on the size of the training data for the semantic models by using overlapping time windows. By taking an extended pe-riod of time, we obtain a sufficient amount of data for constructing high-quality semantic models. As the windows are only one year apart from each other, changes in the semantic relations between words can be detected between subsequent models, while the vast majority of relations will remain stable, due to the overlap.
In this section we describe the generation algorithm and the ag-gregation algorithm, for our adaptive method of monitoring shifting vocabularies over time. Figure 1: Schematic representation of the generation algorithm for generating vocabularies over time.

In Figure 1 a schematic overview is given of the generation algo-rithm for generating vocabularies over time from sliding windows. Every iteration consists of an expansion step and a pruning step. In the expansion step, a semantic graph is constructed from a list of seed terms and a semantic space. In the pruning step, the top terms, according to a graph-based measure, are extracted from the graph. This vocabulary is the input to the next iteration. As can be seen from this schematic overview, the original input words do not necessarily end up in the vocabulary one (or more) iterations later.
In Algorithm 1 the pseudocode for the generation algorithm is provided. At the very first iteration, the input consists of the seed set as provided by the user (Algorithm 1, line 1). As a key require-ment of our method is limited effort by the user, we use only a few terms (typically one or two) as input. In the outer loop is carried out K times (line 2), once for every semantic model, derived from the K sliding time windows. In the expansion step (line 4 X 8), we construct a weighted, directed, partial semantic graph from the set of seed terms, given the semantic space from the next time window. To do this, we obtain the related terms for every word in the seed set, with a minimum similarity  X  (line 5). Per seed set term we take at most n related terms. The terms obtained in this manner are the vertices of our graph. Form these vertices we construct a semantic graph (line 9). The edges in the graph are directed and weighted. We draw an edge from vertex w i to w j if w j is in the list of related words of w i . The weight on the edge is determined by the strength of the association between w i and w j in the semantic space. The network is partial as we do not construct an extensive network of all possible vertices (i.e., all word types in the corpus), but rather extract the part of the network in the vicinity of the seed terms. In the pruning step the top-n terms are selected relative to their degree centrality in the semantic network (line 10).

We use elementary variants of degree centrality: in-degree and out-degree. More involved measures like PageRank can be consid-ered as well, especially when larger parts of the graph are extracted. Required : W = [ w 1 ...w | W | ] : a set of seed terms
Required : Series of semantic spaces S = [ sem 1 ...sem K Required :  X  : minimum similarity Required : n : maximum number of terms to return
Result : List of vocabularies v 1 ...v | K | 1 v 0 = W ; 2 for k  X  1 ... | K | do 3 vertices = []; 4 foreach w  X  v k  X  1 do 5 foreach w related  X  related _ words ( w,sem k , X ,n ) 6 vertices = vertices  X  w related ; 7 end 8 end 9 semanticNetwork = drawEdges(vertices); 10 v k = top-n nodes from semanticNetwork w.r.t. degree 11 end
Algorithm 1: Generation algorithm: adaptively generating vocab-ularies from sliding time windows e.g. by finding related words of related words, and so on. How-ever, preliminary experiments showed that the relation between the original seed terms and related terms of related terms can quickly become arbitrary. A method relying on such terms would run a considerable risk of topic drift.

We compute four measures of degree centrality: number of in-links, weighted sum of inlinks, number of outlinks and weighted sum of outlinks. The choice of degree centrality measure is a pa-rameter of our model. We discuss the effect of this parameter on the results of our experiments in  X 5.2.
 Direction in time. In this section we describe a forward pass, where we start with the oldest time window and progress towards the future. The same method can be applied the other way around, as would, e.g., be appropriate for the iPod example in  X 1. In Al-gorithm 1 this means that we start with v | K | in line 1, range over k  X  X  K | ... 1 in line 2 and iterate over w  X  v k +1 in line 4.
For each semantic space, generated from documents in overlap-ping time windows one year apart, the generation algorithm gen-erates a vocabulary. If we monitor, e.g., a period of four decades, 40 vocabularies are generated, one for every overlapping window. The final output presented to the user, however, should be one vo-cabulary for every 5 year period, so 8 vocabularies, in the example case. To generate the final output vocabularies, we aggregate over the vocabularies generated by the generation algorithm.

The aggregation step producing the final vocabularies is distinct from the principal underlying method of generating vocabularies for all overlapping time windows. If the final vocabularies should be generated for periods of 4 or 6 years, rather than 5, the output of the generation algorithm could be used unaltered, and only one parameter needs to be changed in the aggregation algorithm.
Algorithm 2 lists the pseudocode of our method for aggregating over the vocabularies output by Algorithm 1 to produce the final output vocabularies. The first step in each iteration (line 2) is to se-Required : List of vocabularies V = v 1 ...v | K |
Required : List of time frames T = [  X  1 ... X  | T | ] for which to Required : n : maximum number of terms to return
Result : List of vocabularies v  X  1 ...v | T | 1 for t  X  1 ... | T | do 2 V 0 = [ v  X  V | v relevant to  X  t ] ; 3 foreach v  X  V 0 do 4 foreach w  X  v do 5 score w += f weight ( v, X  t )  X  score w,v ; 6 end 7 end 8 v  X  t = top-n terms w sorted by score w ; 9 end
Algorithm 2: Aggregation algorithm: Aggregating vocabularies output by the generation algorithm to produce the final output vo-cabularies. lect a set of vocabularies relevant to the time period at hand  X  select all vocabularies constructed from time windows that have an overlap with  X  t . This step is needed as the length of the time win-dows is a parameter of the model and might not be the same as the length of  X  i . We can, e.g., use 10-year windows in the generation algorithm, while we output vocabularies for 5-year periods in the final step (i.e. the length of every period  X  t is 5 ).

In the inner loops of Algorithm 2 we iterate over the words in the selected vocabularies (line 3 X 7). We compute a score for all words, which consists of their score in vocabulary v (their de-gree centrality, see previous section) weighted by a weight func-tion f weight ( v, X  ) that assigns a weight to a vocabulary v for a time frame  X  .
 vocabulary v  X  t is constructed from a semantic space, derived from the texts of documents published in a time window, spanning a number of years. The time window has an overlap with time period  X  that we want to output a vocabulary for. Therefore, a weighting is needed which expresses how much vocabulary v should contribute to v  X  t , the final vocabulary we output for  X  t .

The most straightforward way of weighting is to weight all vo-cabularies equally (i.e., apply no weighting at all). However, the central years in the period the vocabulary is derived from are most likely to best capture its semantics (e.g., if we look at the decade 1970 X 1979, the documents in the early 1970s might still have echoes of the late sixties, while in the late 1970s, the 1980s might already become apparent; the middle years will define the vocabulary most clearly). We implement this intuition by assuming that the prob-ability of the contribution of years to a vocabulary v is given by a Gaussian distribution, where the mean of the distribution is the cen-tre of the period, and we assume a standard deviation of 1 . 0 . We model the distribution of the years in  X  in a similar fashion, where the mean is the central year of  X  . Given these two distributions we can use the Jensen-Shannon divergence as a proxy for the weight of v with respect to  X  : where we have  X  2 v =  X  2  X  = 1 .

We note that simple overlap metrics, like, e.g.. Jaccard distance, do not measure what we want, as a the Jaccard distance between two periods, where one period overlaps completely with the other, is always the same, regardless whether they overlap in the central region of the longer period or not.
Using the adaptive method for generating vocabularies, it is well possible that none of the words in the original seed set are present after a few iterations. This is a desired effect, but it also introduces the risk of topic drift. I.e., the adaptive algorithm might focus on an aspect of meaning that was not intended by the user, which can cause the vocabularies being generated to drift in the wrong direc-tion. To counter this effect, we also include runs in our experiments where the initial seed set is kept static. That is, we omit Algo-rithm 1, and instead output the n words most related to the words in the original seed set for every sliding time window. To generate the final vocabularies we do employ Algorithm 2.

We refer to this method, that follows a static seed set for gener-ating shifting vocabularies over time as non-adaptive method.
To combine the exploratory effect of the adaptive approach with the more conservative approach of the non-adaptive approach, we combine the runs of both methods of producing shifting vocabu-laries over time to produce hybrid runs. In particular, we replace the least central terms of the vocabularies produced by the non-adaptive method by the top i vocabulary terms produced by the adaptive method with respect to degree centrality. In  X 5 we report results for different values of i .
To measure the quality of the different methods of generating vo-cabularies over time we perform a systematic, intrinsic evaluation. Our research questions are: RQ1 Given that we have an exploratory, adaptive approach and a RQ2 How do the parameters of the generation algorithm and the The first research question, RQ1, concerns the balance between an exploratory response to change in vocabularies, which introduces the risk of topic drift, and a static, conservative approach, which does not allow for substantial evolution of vocabularies. In  X 5.1 we report on the results for our experiments regarding this question.
The second research question, RQ2, concerns our algorithms for generating vocabularies over time more specifically. As detailed in  X 3 we construct semantic networks to find salient terms in specific time periods. We are interested in evaluating whether, e.g., the weighting of edges is beneficial or not, or whether selecting nodes based on in-degree yields better results than using out-degree.
We analyse the performance regarding all parameters of our al-gorithms of generating shifting vocabularies over time in  X 5.2. In the remainder of this section we detail the aspects of our experi-mental setup.
The natural ground truth data for our task of monitoring shifting vocabularies over time are the shifting vocabularies themselves. We make use of human annotators to obtain this ground truth data. The annotators X  task is, given all unique words occurring in a corpus of timestamped documents, to indicate which words are relevant to a particular topic of interest in a certain time period. As it is not feasible for annotators to judge all word types in a corpus, we employ a pooling approach, which we detail below. Below, we also provide the characteristics of the seed terms.

Given a small number of seed terms, and a short text describ-ing the underlying information need, the annotators were asked to judge terms per period on a 3 point scale: irrelevant , related and perfect . The related category is used for borderline cases in which a result is not completely off the mark, but is not exactly right either.
There were 6 annotators in total, all of whom are academic histo-rians, well-acquainted with both the corpus and the evaluation time period. None of the authors took part in the annotation effort.
Following, e.g., [11] we use the pairwise Pearson correlation to determine inter-annotator agreement. The Pearson correlation co-efficient is . 555 with a p-value &lt; 10  X  5 . It shows that the judge-ments are highly correlated between annotators and that the aver-aged judgements can reliably be used to evaluate our experiments.
The sets of seed terms and the ground truth annotations are pub-licly available. The material can be downloaded from http:// ilps.science.uva.nl/resources/shifts .
 Pooling. We produce output using each of the methods for gener-ating vocabularies over time that we consider, and all combinations of parameters. We pool these results, similar to how the runs of IR systems are pooled in a classical TREC-style evaluation [26]. In our setting, however, the unit of retrieval is a word for a given time period, rather than a document. Annotators are presented with the aggregated results of all runs combined.
 Corpus. Our corpus is a collection of Dutch newspapers, digi-tised by the Royal Library of the Netherlands. 2 We use four decades, 1950 up until 1990, as our evaluation period as this period is long enough for interesting changes to occur and modern enough for the OCR quality to be reasonable. 3
The corpus contains 26 614 346 documents (newspaper articles) in the four decades we consider. Together, they comprise 1 940 841 unique words and 2 141 992 571 tokens. Figure 2 gives an overview of the numbers of tokens per year. As can be observed from this figure, the tokens are not evenly distributed across the years, but there is no bias towards either modern or historical documents. We used the Python NLTK Punkt Sentence Tokenizer [5] and remove additional unicode non-word characters.
 Seed terms. There are 21 sets of seed terms in our experiments, which are provided by Dutch historians, who are familiar with the corpus and the time period selected. The terms are inspired by their own, real-life, research questions and by observations they
The full newspaper corpus, and more, can be queried at
No official numbers concerning the OCR quality throughout this corpus are available. Anecdotal evidence suggests though that modern material is of higher quality. made from the corpus. As discussed in  X 3, an algorithm for gener-ating vocabularies over time can run either forwards or backwards in time. It was left up to the historians to decide on the most natural direction in time, per set of seed terms. In Table 1 we present an overview of the seed sets, together with the direction in time. The bottom 5 rows in Table 1 list 5, so-called, a-historical seed sets. The concepts denoted by these seed sets are assumed, by the his-torians, to stay relatively stable over the entire evaluation period. We include the a-historical seed sets for two reasons. Firstly, we want to avoid a bias in the test set towards changing concepts, i.e., we do not want the test set to only consist of examples of which it is apparent that they evolve over time, as this would put the non-adaptive methods at an unfair disadvantage. Secondly, we want to check for over-generating, by which we mean, in this context, gen-erating changing vocabularies while there is in fact no change. A method that is too exploratory might always find new terms and might show evolving list of words erroneously. To be able to mea-sure such behaviour, we add the a-historical seed term sets.
On average the seed term lists are 2 . 1 words in length. The ground truth vocabularies (i.e. the list of relevant words per time period) are 9 . 32 words in length on average.
Our algorithms for generating shifting vocabularies over time produce ranked lists of words. The Cranfield-style evaluation set-ting allows us to use traditional IR evaluation metrics suitable for evaluating ranked lists, NDCG and MAP, in addition to the standard F 1 metric.
For the generation algorithm, we use 5-year and 10-year slid-ing time windows to compute semantical spaces from. Preliminary experiments showed that values between . 6 and . 7 are reasonable values for  X  . Hence we experiment with  X   X  [ . 6 ,. 65 ,. 7] . For de-gree centrality we use 4 variants, as described in  X 3.2.1: sum of inlinks, weighted sum of inlinks, sum of outlinks, weighted sum of outlinks.

The aggregation algorithm has only one parameter: the vocabu-lary weighting function. We experiment with a uniform weighting function (i.e., no weighting), and the JSD-weighting function, de-scribed in  X 3.2.2.

As discussed in  X 2.5 we use word2vec [21] to generate word vectors for every time window. We employ default settings; Skip-gram architecture, with hierarchical softmax and no negative sam-pling, vector dimensionality of 300 , window size of 5 , and mini-mum word frequency of 5 .

In all experiments, the vocabulary size n is set to 10 .
As noted in  X 2, the work described in [13, 14] is related to our present setting. Following this work, we construct our baseline by using a time slice approach. However, we use neural network language models to construct semantic models to derive semantic proximity from, rather than co-occurrence measures as in [13, 14], as the computation of a full co-occur-rence matrix on the corpus used in our experiments is intractable, due to its size. For every time window  X  t our baseline methods outputs the top-n most re-lated words derived from a semantic model trained on the docu-ments published in time window  X  t .
We begin by answering our research questions and proceed by contrasting the adaptive approach and the non-adaptive approach, described in  X 3.2 and  X 3.3, respectively.
To answer RQ1 we conduct experiments with all methods de-scribed above and all parameter settings. Table 2 contains an overview of the results yielded by the best parameter setting. 4 Table 2: Results for JSD weighting with 10-year periods,  X  = . 65 , in-degree over weighted edges. Statistically significant dif-ferences from the baseline, measured with a two-tailed paired t-test, is marked for p &lt; . 02  X  and p &lt; 10  X  6  X 
Method F 1 p r NDCG MAP hybrid ( i = 1 ) . 384  X  . 537  X  . 406  X  . 646  X  . 343  X  hybrid ( i = 2 ) . 391  X  . 544  X  . 414  X  . 650  X  . 346  X  hybrid ( i = 3 ) . 392  X  . 548  X  . 411  X  . 653  X  . 345  X  hybrid ( i = 4 ) . 389  X  . 545  X  . 405  X  . 651  X  . 343  X  hybrid ( i = 5 ) . 385  X  . 541  X  . 399  X  . 649  X  . 339  X  adaptive . 344 . 551  X  . 298 . 514  X  . 237  X  non-adaptive . 367  X  . 521  X  . 389  X  . 630  X  . 332  X  baseline . 303 . 450 . 296 . 554 . 266
The key observation from Table 2 is that the hybrid approach outperforms both the baseline, and the adaptive method and non-adaptive method separately, on all metrics, regardless of the value of i . It is important to note that the parameter setting reported in Table 2 consistently yields the highest results on all metrics for the hybrid method, regardless of the value of i .
The non-adaptive method outperforms the baseline by itself. The adaptive method only does so in terms of F 1 . As is clear from Ta-ble 2, though, the adaptive method can add valuable information
Note that due to macro-averaging, the macro-F 1 scores can and do end up lower than the individual macro-precision and macro-recall scores. Figure 3: Comparison of results between the adaptive and non-adaptive run for the seed words  X  X d, compact disc. X  Direction is backwards in time. to the non-adaptive method. In this section we present a number of examples to illustrate the difference between the two. To high-light the difference, we only show examples of the non-hybrid runs in this section. These runs contributed to the results in the rows labeled  X  X daptive X  and  X  X on-adaptive X  in Table 2.

In Figure 3 the results are displayed for the non-adaptive run and the adaptive run for the seed words  X  X d, compactdisc. X  The direction for this example is backward, i.e., we start with the seed words in the 1990 X 1994 period and go backward in time.

As we can clearly see from the figure, the performance of the non-adaptive run quickly degrades over time (recall that we are go-ing backward in time). Interestingly, the adaptive run, after a glitch in the 1970 X 1974 period, manages to pick up to get decent per-formance again for the time periods in the 1950s and 1960s. This indicates that the network approach, in which a network of related terms is promoted, can be beneficial. Figure 4: Comparison of results between the adaptive and non-adaptive run for the seed word  X  X olocaust. X  Direction is back-ward in time.

We see a similar pattern in the results for the seed word  X  X olo-caust X  in Figure 4. Again, we are going backward in time for this example. The performance of the non-adaptive run steadily de-grades as we go back in time. This can be explained by the fact that the word  X  X olocaust X  barely occurs in the corpus prior to 1978. The term was introduced in Dutch discourse by an American tele-vision series by that name. Initially, the term was used primarily to refer to the series, but gradually it became a more general term that now means the same as it does in English.

In Figure 5 the results are displayed for the seed word  X  X ultina-tional. X  The word  X  X ultinational X  rarely occurs in the 1950s and Figure 5: Comparison of results between the adaptive and non-adaptive run for the seed word  X  X ultinational. X  Direction is backward in time. 1960s in the Dutch digitised newspapers. 6 This is clearly reflected in Figure 5 and both the adaptive and the non-adaptive method suf-fer from this. Close inspection of the documents in which the word does occur in this period reveal that it is used in a political context (where it means international) rather than in a business context as later on. Importantly, the adaptive run is able to recover its drop in performance, while the non-adaptive run is unable to do so, and keeps getting zero performance.

The examples in this section clearly show the limitations of non-adaptive approach that simply follows a static set of words and the words related to them over time. If the words in the seed set sim-ply do not exist in the period of interest (as in the  X  X d X  example), change in meaning (the  X  X ultinational X  example), or are not used throughout the entire period of interest (the  X  X olocaust X  example), a static approach can only fail.
As discussed in  X 4.1 the evaluation set contains 5 a-historical seed term sets to check for overgenerating. In Table 3 we display the results on the a-historical subset of the ground truth seed sets, based on the same parameter settings used for Table 2.
 Table 3: Results for adaptive and non-adaptive method on a-historical seed sets only
As we can observe from Table 3 the results between the adaptive and non-adaptive runs are comparable. None of the differences is statistically significant for  X  = . 05 for a two-tailed paired t-test. We conclude from these results that our adaptive method for generating shifting vocabularies over time does not overgenerate. That is, if no changes occur in a vocabulary concerning a particular topic, none are in fact picked up by the adaptive method.
To answer research question RQ2 we analyse the effect of the parameters of the generation algorithm and the aggregation algo-rithm. For the generation algorithm the parameters are the length of the sliding time window, minimal semantic distance  X  and the method of computing degree centrality. For the aggregation algo-rithm we have one parameter, the vocabulary weighting function. Figure 6: Comparison of results per metric, grouped by time window length. (Best viewed in color.) ing time windows affects both the adaptive method and the non-adaptive method. In Figure 6 performance of all runs  X  adap-tive, non-adaptive and hybrid, all parameter settings  X  is plot-ted, grouped by window length. As is clear from the figure, using 10 year sliding windows yields better results in a vast majority of cases, for all metrics. In Table 4 the t-statistics and p-values are listed per metric for a paired t-test between the results per window length (the results are paired per parameter setting).
 Table 4: Results of two-tailed paired t-test between the perfor-mance of all results per window length, paired by parameter setting
From these findings we conclude that using a longer time win-dow to train a semantic model yields better performance for our current task, which supports the claim made in [21] that more train-ing data yields better semantic models. Do note, though, that, due to the adaptive nature of our task, we can not use arbitrarily long time windows, as the changes in meaning and vocabulary we are interested in might go unnoticed that way.
 Minimum distance. As discussed in  X 3.2.1 the  X  parameter controls which related words are taken into account for construct-ing semantic networks. In Table 5 the results across different levels of  X  are displayed for all methods of generating shifting vocabular-Table 5: Top results for different settings of minimum similar-ity  X  , all other settings as in Table 2
Method  X  F 1 p r NDCG MAP hybrid ( i = 1 ) . 7 . 367 . 521 . 389 . 630 . 332 hybrid ( i = 2 ) . 7 . 368 . 523 . 385 . 630 . 332 hybrid ( i = 3 ) . 7 . 372 . 530 . 388 . 634 . 335 hybrid ( i = 4 ) . 7 . 370 . 529 . 381 . 632 . 333 hybrid ( i = 5 ) . 7 . 366 . 525 . 372 . 628 . 331 adaptive . 7 . 316 . 678 . 241 . 485 . 220 ies over time, that use the generation algorithm (the non-adaptive method only uses the aggregation algorithm). The results are con-sistently lower than the results in Table 2, regardless of the method. This clearly indicates that a value of  X  = . 65 is to be preferred for all methods, adaptive, non-adaptive or hybrid.
 Degree centrality. Regarding the different ways of calculating degree centrality we observe a very consistent pattern: choosing in-degree always yields better results than choosing out-degree. The best performance with out-degree, in terms of F 1 , other settings as in Table 2 is yielded by the hybrid method, with i = 1 . It yields an F 1 of . 370 , NDCG of . 632 and MAP of . 333 , all of which is lower than the scores of the best performing hybrid runs.

Putting weights on the edges consistently leads to performance superior to unweighted edges. The best performance, in terms of F , without weighted edges, and other settings as in Table 2 is yielded by the hybrid method with i = 1 , which yields an F . 369 , NDCG of . 632 and MAP of . 333 .
 method, not weighting the vocabularies leads to a small increase in performance: F 1 .368, NDCG . 632 and MAP . 333 , regardless of the value for minimum similarity  X  . These differences, however, are not statistically significant for  X  = . 5 for a two-tailed paired t-test. Furthermore, for the hybrid method, applying weighting for gen-erating vocabularies over time nearly always yields better results when i &gt; 1 . These findings suggest that weighting of vocabularies is beneficial for generating shifting vocabularies over time.
In 9 cases of the 21, merging adaptive and non-adaptive runs for the hybrid runs led to performance that was less than the best performing of the two. In this section we will discuss three such ex-amples. Typically, the decrease in performance was small (~1%). Table 6: Results of the hybrid run ( i = 3 ) for seed set  X  X arx-ism, X  for the last time period (the direction in time is forward). Words occurring in the ground truth set are marked with a *.
Period Vocabulary 7 1990 X 1994 communism*, marxism*, capitalism, human-Table 6 shows the vocabulary output for the hybrid run ( i = 3 ) with seed set  X  X arxism X  for the 1990 X 1994 period. The direction is for-ward in time. This means that we start with the concept of marxism in 1950 and follow it as time progresses. As we can see from the results, the adaptive run has picked up on related terms and has be-come too general (the concepts, though they are related, are main-stream socio-economical movements, ideologies and isms). Much more on-topic words, like, e.g.,  X  X eninism X  and  X  X talinism, X  which were used in the late 1990s in the newspaper corpus are picked up by the non-adaptive run.

We see a different pattern for the run with the seed set  X  X ydrogen bomb X  in Table 7. Here, the adaptive run nearly loses track of the nuclear weapons completely, and rather focusses on missiles.
The examples in this section show that the adaptive method for monitoring shifting vocabularies over time can be susceptible to
The original words are in Dutch, translations by the authors
The term  X  X tomic warheads X  was not annotated as correct, even though it means the same as  X  X uclear warhead, X  because it was hardly ever used, while  X  X uclear warhead X  was used abundantly. Table 7: Results of the hybrid run ( i = 3 ) for seed set  X  X ydro-gen bomb X  for the last time period (forward direction in time). Words occurring in the ground truth set are marked with a *.
Period Vocabulary 1990 X 1994 launching facilities, rockets, ballistic, launch-topic drift. It can loose specificity (the  X  X arxism X  example) or it can drift in the wrong direction (the  X  X ydrogen bomb X  example). Especially in cases like this, a combination with a more conserva-tive, non-adaptive approach is beneficial.
We introduced the task of ad hoc monitoring of vocabulary shifts over time. We presented several algorithms for monitoring vocab-ularies over time and perform systematic, intrinsic evaluation of their results. Our results show that our approach of combining an exploratory method of generating shifting vocabularies over time with a conservative approach consistently and significantly beats a baseline inspired by related research, and that it consistently per-forms better than the two approaches it combines.

Intrinsic evaluation of semantic methods is difficult. Construct-ing a manually labelled dataset as we did is costly and labour-intensive. We hope that disclosing the full evaluation set is ben-eficial to research in this area.

High-quality, on-topic vocabularies over time can beneficial in many cases both in IR and in digital humanities research. The vo-cabularies can be used as a way of exploring data, as is the under-lying scenario in this paper. Furthermore however, they could be used for time-aware query expansion, where the query expansion depends on the timestamps of documents in a corpus.

Future work should focus on longer evaluation periods, e.g. a century of material. Furthermore, additional graph-based measures could be taken into account. Moreover, different types of shift in vocabulary might be discerned. Similar to how document ranking systems are tailored towards query intent, systems for monitoring shifting vocabularies over time could be optimised in terms of op-timal parameter settings or choice of algorithm, depending on the type of vocabulary shift they aim to monitor.

The performance of an adaptive method for monitoring shifting vocabularies may degrade or improve over time. However, tradi-tional evaluation metrics like NDCG or MAP are time-agnostic. Additional insights could be obtained when a time-aware evalua-tion metric, such as, e.g., proposed in [17] in the context of docu-ment filtering systems, would be applied to the present setting.
