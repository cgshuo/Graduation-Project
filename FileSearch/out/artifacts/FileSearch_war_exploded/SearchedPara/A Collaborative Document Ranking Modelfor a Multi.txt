 It is well-known in information retrieval (IR) domain that one critical issue is the understanding of users X  search goals hi dden behind their queries, in attempt to get a better insight of how to explore the information space towards relevant documents [2]. Most retrieval approaches consider that a query addresses mainly a single topic whereas multi-faceted search-based approaches [1,13] have high-lighted the need of consider ing the topical coverage of the retrieved documents towards different aspects of the query t opic, named facets. One multi-faceted query example, extracted from the TREC Interactive dataset [17], is  X  X ubble Telescope Achievements X . For this query, users have manually identified, for in-stance, several aspects:  X  X ocus of camera X ,  X  X ge of the universe space telescope X  and  X  X ube pictures X . The key emerging ch allenges from multi-faceted search are how to infer the different query facets and how to exploit them jointly to select relevant results. To tackle the underlying issues, a first category of work at-tempts to enhance the query representation in order to identify the query facets [5,22], whereas a second line of work in the same category [4,12] considers result diversification towards query facets.

Another category of work [15,19] arisen from the Collaborative IR (CIR) domain underlines that collaboration could benefit complex tasks and more par-ticularly exploratory queries. Indeed, complex problems can be difficultly solved within ad-hoc IR due to the single searcher X  X  knowledge or skills inadequacy [19]. A collaborative framew ork enables overcoming th is lack considering that a group of users may analyze more in-depth the different query facets in contrast to a single user who performs individually the first level of the information need. Shah [19] introduces the notion of the sy nergic effect of collaborative search in so far as  X  X he whole (is) greater than the sum of all X  . Therefore, CIR is another response to tackle multi-faceted search issues in which multiple users search documents together in response to a share d information need considering their different knowledge expertise and points of view with respect to the same query.
In this paper, we present a collaborative document ranking model suited to solve a multi-faceted query. Our approach allows to leverage the users X  different knowledge expertise and assigns them implicit knowledge-based roles towards at least one query facet. These facets are modeled through document and user topical-based representations using the LDA generative model. Documents are allocated to experts using the Expectation-Maximization algorithm.

In section 2, we discuss related work. Section 3 describes our collaborative ranking approach involving an expert group. Section 4 presents our experimental evaluation. Finally, section 5 concludes the paper and introduces future work. The multi-faceted search issue can aris e within two search settings, namely the individual-based search and the collaborative based-one.

Individual-based search is a classic IR setting in which one user aims at satis-fying an information need. We address par ticularly in this paper a complex in-formation need expressed by means of a mul ti-faceted query. There are two lines of work in this area. The first one remains on identifying explicit query facets prior to performing a facet-based retrieval model [5,6,9]. Authors use different tools or methods addressed at the query l evel such as termin ological resources for building a hierarchy among the detect ed facets [5]. Other methods [9] remain on the analysis of the user navigation for classifying query facets using document features. In contrast, generative probabilistic-based models [3,6] aim at modeling documents as topical-based vectors in w hich each component expresses a query facet. Another line of work [4,24] dealing w ith multi-faceted search focuses on the diversification of the search results with out highlighting the query facets. The key idea is to select a diversified document subset either using a term-frequency distribution [4] or a graph modeling approach [24].

Unlike individual-based search, the collaborative-based one is a retrieval set-ting in which several users collaborate for satisfying a shared information need. Considering a multi-faceted information need, the integration of collaboration within a search task enables to benefit from the synergic effect of collaboration [19] in so far as people with different backgrounds and points of view search relevant documents together with respect to a multi-faceted shared information need. Collaboration pr inciples [10], namely awareness , sharing of knowledge and division of labor , enable to leverage from users their different skills and search abilities by warming collaborators of actions of their pairs, transmitting the in-formation flow among users and avoiding redundancy. Previous work in the CIR domain [10,16,18,21] rely on different retrieval strategies, such as designing user roles and relevance feedback techniques, in order to explore the different query facets. We distinguish two main categories of work depending on if they integrate user roles or not. The first category remai ns on a relevance feedback process with-out integrating user roles [10,16], providing iteratively a ranked list of documents either at the user level [10,16] or at the group level [16]. The second category of work, more close to ours, consists in integrating a supplementary layer based on user roles [18,21] by assigning to each collaborator a specific task for avoiding redundancy. Pickens et al. [18] model the prospector and miner roles which re-spectively aim at ensuring the quality of the selected documents and favoring the diversity of the search results in response to the multi-faceted information need by means of query suggestion. This model is guided by the division of labor and sharing of knowledge principles throughout a relevance feedback process.Shah et al. [21] introduce another couple of roles, namely the gatherer and the surveyor relying on different predefined tasks: respectively, performing a quick overview of documents for detecting relevant ones or a better understanding of the topic to explore other subtopic areas in acco rdance to the multi-faceted query.
In this paper, we focus on a collaborati ve context in which a multi-faceted information need is solved by a group of users. Unlike previous work based on explicit user roles [18,21], our model assigns to users implicit roles considering their domain knowledge mined through a r elevance feedback process. Therefore, our model enables to leverage users X  skills in which they are the most effective, in opposition to other models [18,21] based on explicit roles which assign prede-fined users roles or domain expertise. For this purpose, we model topical-based profiles for estimating the knowledge expertise of users and propose an iterative document ranking model using a learning-based algorithm in order to assign documents to the most likely suited users according to their domain knowledge. Several underlying concerns of multi-faceted search within a CIR context may arise, such as identifying the most suitable users for solving a multi-topical in-formation need or performing a collaborative retrieval task according to the dif-ferent query facets. We focus here on the second concern. Mo re specifically, we address two main research questions: 1) How to mine the query facets? 2) How to build collaborative document rankings? 3.1 Mining Query Facets First, given an initial document dataset D and a multi-faceted query Q ,weex-tract a subset D  X  of n documents which satisfies the condition of being relevant with respect to the query topic and ensuring a broad topical coverage. For this purpose, we use the Maximal Marginal Relevance (MMR) [4] that selects docu-ment D  X   X  X  with the highest relevance and marginal diversity score considering subset D  X  : where sim 1 ( D i ,Q )and sim 2 ( D i ,D i ) express the similarity between document D i and query Q , respectively document D i .  X 
Second, we use the generative probabilistic algorithm named LDA [3] applied on the diversified document dataset D  X  in order to identify latent topics. Each topic is assimilated to a facet of query Q . The algorithm computes a word-topic distribution  X  w | t and document-topic distribution  X  D estimating the probability of word w and document D i given topic t .Theoptimal number of topics T for the document dataset D  X  is generally tuned using a likelihood measure: where W is the set of words extracted from document dataset D  X  . The proba-bility p ( w | t ) corresponds to the word-topic distribution  X  w | t .
Each document D i  X  X   X  is represented using a topical distribution D i = ( w 1 i ,...,w ti ,...,w Ti )where w ti represents the weight of mined topic t by the LDA algorithm for document D i expressed by the probability  X  D E documents and inferred on the document representation using the LDA inference where w ( k ) tj represents the expertise of expert E j towards topic t at iteration k . 3.2 Collaborative Document Ranking Expert-Based Document Scoring. The expert-based document scoring aims at reranking documents with respect to ea ch expert X  X  domain expertise towards the query facets. It provides a first attempt for document ranking given experts which is, then, used within the collaborative learning method. We estimate, at iteration k , the probability p ( k ) ( d i | e j ,q )ofdocument D i given expert E j and query Q as follows: where d i , e j and q are random variables associat ed respectively with document D ,expert E j and query Q . Considering the probability p ( e j | q )isnotdiscrimi-nant and assuming that expert e j and query q are independent, we obtain: We first estimate the probability p ( d i | q )ofdocument D i given query Q as: with D  X  is the reference dataset. The probability p ( d i )ofdocument D i is inde-pendent of the query and can be estimated as a uniform weight: p ( d i )= 1 |D| .
Considering the facet-based distribution of document D i , we estimate p ( q | d i ) by combining two similarity scores RSV LDA ( Q | D i )and RSV BM 25 ( Q, D i )re-spectively based on the LDA-based document ranking model detailed in [11] and a BM25-based document scoring: where p ( w | t ) represents the probability of term w given topic t estimated by  X  w | t in the document representation. We estimate the probability p ( k ) ( e j | d i )asa cosine similarity sim cos comparing the two topical distributions of document D i and knowledge profile  X  ( E j ) ( k ) associated to expert E j at iteration k as: Expert-Based Document Allocation. Here, we aim at allocating documents to the most suitable experts considering document scores computed in Equation 3. For this aim, we use the learning Expectation Maximization algorithm [7]. The EM-based collaborative document allocation method runs into two stages, as detailed in Algorithm 1. Notations are detailed in Table 1. 1. Learning the document-expert mapping. The aim is to learn through an EM-based algorithm how experts are likely to assess the relevance of a docu-ment. This method is divided into two steps: -The E-step estimates the probability p ( c j =1 | X ( k ) i ) of relevance of document D i towards expert E j considering all experts at iteration k considering docu-model that considers Gaussian probability laws  X  j to model the relevance c j of documents for expert E j at iteration k :
If we consider the fact that document irrelevance towards expert E j ,noted c = 0, can be formulated by the probability of being relevant for another expert, noted c l =1  X  l = { 1 ,...,m } with l = j , the denominator corresponds to the sum of the probabilities expressing the document relevance, noted c l =1,towards Algorithm 1. EM-based collaborative document ranking expert E l . Thus, we replace the probabilities pij ( k ) and p ( k ) il by the Gaussian -The M-step updates the parameters  X  j and allows estimating the  X  X xpected Complete Data Log Likelihood X : The algorithm convergence is reached when the log-likelihood is maximized. 2. Allocating documents to experts. The key issue is to detect which expert is more likely to assess the relevance of a document. Similarly to the proba-bilistic model assumption, we assign to each matrix element M EM,k ij the odds value odds ( M EM,k ij ), computed as the ratio between the probability of relevance M ij and the probability of irrelevance, estimated by cation is done using this output matrix M EM,k where document D i is allocated to expert E j by maximizing the probability p ( c j | X ( k ) i )thatdocument D i is relevant for expert E j considering its scores X ( k ) i :
Moreover, we propose an additional layer which ensures division of labor by re-moving from document allocation towards user u j , documents already displayed within collaborators X  lists. Considering that it does not exist online collaborative search log, except propri-etary ones [15,19], the retrieval effectiveness of our model was evaluated through a simulation-based framework which is an extension of the experimental frame-work proposed in Foley et al [10]. We used the same dataset, namely the TREC 6-7-8 Interactive one 1 which models users X  interactions within an interactive-based IR task. One of the goals of users who perform this task is to identify several instances, namely aspects, re lated to the information need [17]. 4.1 Experimental Setup Dataset. We use the TREC Financial Times of London 1991-1994 Collection (Disk 4 of the TREC ad-hoc Collection) which includes 210 158 articles. We performed our effectiveness evaluation o n 277 collaborative query search ses-sions extracted from dataset runs and built upon 20 TREC initial topics. The latter are analyzed for checking whether they are multi-faceted by estimating the diversity coverage of the top 1000 documents. We use the Jaccard distance between documents in pairs which avoid s document length bias. We retained the whole set of 20 TREC topics, characterized by diversity coverage values very close to 1. Considering our collaborative model, we retain the 10 participants who have provided  X  X ich format data X  including the list of documents selected by the user and their respective selection t ime-stamp label. F or each TREC topic, we retained as relevance judgments the respective feedback provided by TREC participants. For improving their reliability, we ensured the agreement between users by considering only documents which have been assessed twice as relevant. The agreement level is tuned in section 4.2 for testing the stability of our model. Collaboration Simulation. Here, we extend the experimental protocol pro-posed in [10] by building groups of experts. Considering that experts use generally a more specific vocabulary [21,23], we analysed the expertise level Expertise ( U j ,Q )ofeachuser U j with respect to query Q using relevance feed-back expressed through their respective TREC runs. For this purpose, we es-timate the average specificity of the selected document set D S Q ( U j ) using the specificity indicator Pspec [14] for search session S Q related to query Q : with L s ( D i )= avg t is noted df t and N is the number of documents in the collection.

For each query Q , we performed a 2-means cla ssification of users from all the participant groups who have achieved an interactive task considering their respective expertise level as criteria. We identified as experts users who belong to the class with the highest average expertise level value. Within each participant group and for each query, we perform all the combinations of size m , with m  X  2, for building the set of 277 groups of experts. We notice that 19 TREC topics enable to form at least one group of experts of size m  X  2. For each group of experts, we identified the time-line of relevance feedback which represents the whole set of selected documents by all the experts of the group synchronized chronologically by their respective time-stamp feature. We carefully consider the time-line of the collaborative search se ssion by ensuring that every document assessed as relevant is displayed in the user X  X  document list. Moreover, assuming that a user focuses his attention on the 30 top documents within a ranked list [10], only those are displayed to the user.
 Metrics. We highlight here that CIR implies a different effectiveness evaluation approach compared to ad-hoc IR. Indeed, even if the goal of a collaborative document ranking model is to select relevant documents, the main objective remains on supporting the collaboration within the group [20]. For estimating the retrieval effectiveness of our model at the session level, we used metrics proposed in [20]. We consider measures at rank 30 considering the length of the displayed document lists. The evaluation metrics are the following: -Cov @ R : the coverage ratio at rank R for analysing the diversity of the search results displayed during the whole search session: where  X  is the set of TREC topics, E Q represents the set of groups e of experts who have collaborated for solving query Q and L e,Q is the set of displayed lists related to query Q for group e . Coverage ( L e,Q ) corresponds to the number of distinct documents displayed to expert group e for query Q . The total number of documents displayed throughout the same session is noted | l | .
 -RelCov @ R , the relevant coverage ratio at rank R which adds the supple-mentary layer to the coverage measure by including the condition that distinct displayed documents should be relevant: where RelevantCoverage ( L e,Q ) corresponds to the number of distinct relevant documents displayed to expert group e for query Q . -P @ R : the average precision at rank R : where D SelRel ( Q, l ) represents the number of relevant documents retrieved within document list l related to query Q .
 Baselines. We performed four scenarios considering either an individual-based search or a collaborative-based one: -W/oEMDoL is the individual version of our model which integrates only the expert-based document scoring presented in section 3.2. -W/oDoL is our collaborative-based mode l detailed in section 3 by excluding the division of labor principle. -W/oEM is our collaborative-based model by excluding the expert-based document allocation, detailed in section 3.2. -OurModel is our collaborative-based model detailed in section 3. 4.2 Results Here, we present the results obtained throughout our experimental evaluation. We adopt a learning-testing method through a two-cross validation strategy in order to tune the retrieval model paramet ers and then test its effectiveness. For this purpose, we randomly split the 277 search sessions into two equivalent sets, noted Q A and Q B . In what follows, we detail the parameter tuning, for both baselines and our model, and the re trieval effectiveness results.
 Parameter Tuning. First, we tune the weighting parameter  X  in Equation 1, used for the diversification of the search results using the MMR score [4]. The diversity criteria considered for building the subset of documents D  X  of size n = 1000 is inversely proportional to the value of  X  . For both subsets of queries Q A and Q B , the retrieval effectiveness is optimal for a  X  value equal to 1.
Second, we estimate the optimal number of topics used for the topical distribu-tion of documents. For each multi-faceted query, we perform the LDA algorithm with a number of topics T from 20 to 200 with a step of 20. The number of topics is tuned using the likelihood, presented in Equation 2. We retain T = 200 as the optimal value for both query sets Q A and Q B . Considering that previous work [8] found that the number of query subtopics is lower than 10 for most of the queries, we add a supplementary layer for modeling query facets by considering the top f facets among the 200 ones of the topical distribution of the query valued by the probability  X  Q | t . Documents and experts are therefore represented by extracting these f facets from their topical vector.

Third, we jointly tune the number f of the top facets and the parameter  X  which combines a similarity score based on the topical modeling and the BM25 algorithm within the document scoring step in equation 6. In order to tune  X   X  [0; 1] and f  X  [1; 10], we rank, for each TREC topic Q  X   X  , the top 1000 diversified documents D i  X  X   X  according to their topical distribution. We retain the value  X  =0 . 6and f = 5 which maximizes the retrieval effectiveness for both sets of queries Q A and Q B .
 Model Effectiveness Evaluation. Table 2 compares the obtained results us-ing our collaborative model with those obtained by the three scenarios, detailed in section 4.1. The reported precisions s how that our model generally overpasses the baseline models without the EM step, namely W/oEM and W/oEMDoL . More particularly, the improvement is significant compared to the scenario W/oEM for the query testing set Q B with a value of +20 . 61%. This highlights the importance of allocating documents to e xperts, as detailed in section 3.2, con-sidering the whole set of experts in contrast to a ranking algorithm only based on a personalized document scoring towar ds a particular expert, as detailed in equation 4. We notice that the scenario performed without division of labor, namely W/oDoL , provides better results than ou r model considering the preci-sion measures. On a first point of view, we could conclude that division of labor is inappropriate for solving collaboratively a multi-faceted information need. How-ever, this statement is counterbalanced by the analysis of coverage-based mea-sures, detailed in section 4.1. In fact, we notice that our model provides in most of the cases higher values of the coverage-based measures with several significant improvements around +17 . 69% for the Cov @30 metric and up to +55 . 40% for the RelCov @30 one. These results show that ou r model ensures both diversity of the displayed documents throughout the coverage measure Cov @30 and rele-vance of these diversified documents within the displayed lists by means of the RelCov @30 measure. This contrast between precision measures and coverage-based ones is explained by the fact that the latter takes into account the residual relevance feedback within the displayed document lists wher eas the second one does not consider document redundancy between successive displayed lists. In summary, the results show that our collaborative ranking model is more appro-priate for satisfying a multi-faceted information need compared to an individual one, namely through scenario W/oEMDoL ; moreover, as reported by coverage-based measures we confirm that our model favors the topical diversity of the search results thanks to the division of labor principle.
 Complementary Analysis. We perform further analysis to show the impact of two parameters, namely the relevance agr eement level and the group size, on the model effectiveness. Figure 1 plots the retri eval effectiveness variation within these two parameters. We can see that our model curve overpasses baseline curves for both parameters. The decreasing trend of curves in Figure 1(a) can be explained by the fact that higher is the agreement level, fewer documents are assessed as rel-evant within the search session and this favors the search failure regardless of the retrieval model. From Figure 1(b), we notice that the curve of our model is gen-erally stable even with the increasing size of the collaborator group. These state-ments confirm that the retrieval model improvements are stable within different configurations of collaborative search settings. In this paper, we propose a collaborative ranking model for satisfying a multi-faceted information need cons idering a group of experts. We propose an iterative relevance-feedback process for automatically updating expert X  X  document list by means of the Expectation-Maximization learning method for collaboratively ranking documents. Our model was evaluated using a collaboration simulation-based framework and has shown effective results. Future work will focus on the design of other formal methods to emphasize division of labor and the modeling of user profile through his behavior in addition to his relevance feedback.
