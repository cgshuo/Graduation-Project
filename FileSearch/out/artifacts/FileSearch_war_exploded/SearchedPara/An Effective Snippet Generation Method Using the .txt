 A (page or web) snippet is document excerpts allowing a user to understand if a document is indeed relevant without accessing it. This paper proposes an effective snippet generation method. The pseudo relevance feedback tec hnique and text summarization techniques are applied to salient sentences extraction for generating good quality snippets. In the experimental results, the proposed method showed much better performance than other methods including Google and Naver. H.3.1 [Content Analysis and Indexing]  X  abstracting methods Algorithms, Performance, Experimentation Snippet, Pseudo Relevance F eedback, Text Summarization Today X  X  Web search engines use text summaries to help users make relevance decisions. Most summaries, such as those by Google, are based on the query terms from the user X  X  search: query-based summaries. Since th ese Web search engines still require users to read many closel y-ranked documents to search for their information seeking goals, the query-based summaries would be helpful in Information Retrieval or topic-detection. Snippet summaries are query-based summaries in Web search engines that show which query terms appear in a document and the words around those query terms [1]. commercial Web search engines, and it proposes a new effective snippet generation method. Th e proposed method is based on pseudo relevance feedback. The pseudo relevance feedback technique is a well known approach to query expansion in Information Retrieval. This pse udo relevance feedback technique is applied to a query-based summarization task to create high-quality snippets. In the experiments, the proposed method showed significantly improved performance in both data sets from two commercial search engines: Google and Naver 1 . The idea of pseudo relevance feedback is to take the results that are initially returned from some query and to use information about whether or not those results are relevant to perform a new query. To apply the pseudo relevance feedback technique to query-based summarization, firs t we must find the relevant sentences automatically. In the query-based summarization, words or phrases from sections of text are compared to a user X  X  query terms, which are mostly relate d to user X  X  interest. From this point of view, a sentence including a query term can be regarded as a relevant sentence. All the terms of the relevant sentences become candidates for query e xpansion. The Term Selection Value (TSV) of each candidate term is estimated by a relevance weighting function in the probability model [2]. The top k candidate terms are selected for query expansion. The importance score of each sentence is estimated by the sum of each expanded query term X  X  TSV and the locati on information of the sentence. The salient sentences are extracted according to the calculated importance score, and the snippet of each document is created from them. All the noun terms from relevant sentences become candidate terms for query expansion. To choose salient terms among the candidate terms, the relevance weight (TSV) of each term is estimated by the following function depending on the distribution of candidate terms in relevant a nd non-relevant sentences. This is based on the basic probability model proposed by Robertson and Sparck Jones [2,3]. After all the candidate terms are sorted according to their TSVs, the top k candidate terms are selected as expanded query terms. k is set to six by a simple experiment in this paper. All the sentences including initial query terms become candidates to be extracted as a query-based summary for snippet. This section describes how to extract salient sentences from the set of the candidate sentences. The importance score of each candidate sentence is estimated by using th e TSVs of expanded query terms and the location information of the sentence within a document. The final importance score is calculated by formula (3). In formula (2), RWscore(S i ) denotes the importance score which is calculated by the sum of releva nce weights (TSVs) of expanded importance score is estimated by the linear combination of a normalized RWscore(S i ) and a location score. RWscoreMax denotes the maximum RWscore value within a document. N is the total number of sentence in the document, and S i denotes the i-th located sentence. The parameter  X  is set to 0.4 by a simple experiment. As a result, the top m candidate sentences are selected as a summary for snippet. Two data sets were constructe d from Google and Naver search engines. Four categories (science &amp; technology, sports &amp; entertainment, politics &amp; economy, and society &amp; culture) were selected and 20 queries were extracted from each category. At first, top 10 Web documents from the result of each search engine were chosen for these data sets. These data sets consist of 80 queries and 1,600 documents. Some of these 1,600 documents were removed because they have the same number of sentences including query terms as the number of sentences from the snippet of each search engine. The final data set is composed of 80 queries and 1,126 documents. Thr ee people participated in extracting the most salient sentence from a document by voting mechanism. They were not members of our research group, and they were not computer-related majors. Each query and related documents were provided to them. 
Hence the number of sentences in a summary extracted by the proposed method is equal to that in a snippet from Google or Naver. The accuracy is used as a performance measure in this paper. If an extracted summary or a snippet from Google or Naver includes the most salient sentence selected by man, it is regarded as a correct summary. Otherwise, it is regarded as an incorrect summary. In addition, the proposed query expansion technique using pseudo relevance feedback is veri fied by comparison with the title method. It is a variation of pseu do relevance feedback which adds the title to the initial query [4]. The importance score of each sentence is measured by similarity between the expanded query with title and each sentence as follows: where t j denotes the presence or absence of j -th term in the sentence, S i . (3) except using Sim(T,S i ) instead of RWscore(S i ) . Table 1 shows that the proposed method outperforms Google and Naver in performance. Moreover, the proposed query expansion method is also much superior to the title method in both data sets. Table 1. Performance results in data sets (Google and Naver) Naver 
Data Set Google 
Data Set This paper presented the outstanding snippet generation method using the pseudo relevance feedback technique. The effectiveness of the proposed method was verified by the experiments on Google and Naver data sets. However, the execution time of the proposed method can make it difficult to apply the proposed method to practical IR systems. Since most commercial search engines analyze input queries to provide the order information of popular queries to users, the snippets of the popular queries can be previously made by using the proposed method in batch process. This research was performed for the Intelligent Robotics Development Program, one of 21 st Century Frontier R&amp;D Programs funded by the Ministry of Commerce, Industry and Energy of Korea. [1] McDonald, D.M. and Chen, H. Summary in Context: [2] Robertson, S.E. and Sparck Jo nes, K. Relevance Weighting [3] Harman, D. Toward Interactive Query Expansion. In [4] Goldstein, J., Kantrowitz, M., Mittal, V., and Carbonell, J. 
