 Semi-supervised clustering leverages side information such as pairwise constraints to guide clustering procedures. De-spite promising progress, existing semi-supervised clustering approaches overlook the condition of side information be-ing generated sequentially, which is a natural setting arising in numerous real-world applications such as social network and e-commerce system analysis. Given emerged new con-straints, classical semi-supervised clustering algorithms need to re-optimize their objectives over all data samples and con-straints in availability, which prevents them from efficient-ly updating the obtained data partitions. To address this challenge, we propose an efficient dynamic semi-supervised clustering framework that casts the clustering problem into a search problem over a feasible convex set, i.e. , a convex hull with its extreme points being an ensemble of m data partitions. According to the principle of ensemble cluster-ing, the optimal partition lies in the convex hull, and can thus be uniquely represented by an m -dimensional proba-bility simplex vector. As such, the dynamic semi-supervised clustering problem is simplified to the problem of updating a probability simplex vector subject to the newly received pairwise constraints. We then develop a computationally ef-ficient updating procedure to update the probability simplex vector in O ( m 2 ) time, irrespective of the data size n . Our empirical studies on several real-world benchmark datasets show that the proposed algorithm outperforms the state-of-the-art semi-supervised clustering algorithms with visible performance gain and significantly reduced running time. H.2.8 [ Database Applications ]: Data mining; I.5.3 [ Pattern Recognition ]: [Clustering] Semi-supervised clustering, sequential constraints, convex hull, probability simplex.
Although data clustering has been successfully applied to plenty of domains [27, 5, 10, 42, 37], it remains as an ill-posed problem due to its unsupervised nature [31]. Semi-supervised clustering [2] could address this limitation by ef-fectively exploring available supervision to guide the cluster-ing process. Such supervision, also known as side informa-tion, is often expressed in the form of pairwise constraints, i.e. , must-links between the data pairs that belong to the same cluster and cannot-links between the data pairs that belong to different clusters. The key idea of semi-supervised clustering algorithms is to find the optimal data partition which is consistent with both the given pairwise constraints and inherent feature representation of data objects to be clustered.

Despite the promising progress, one issue often overlooked by existing semi-supervised clustering approaches is how to efficiently update the clustering results when the pairwise constraints are dynamic, i.e. , the new pairwise constraints are generated sequentially. This condition stands natural and is closely related to many real-world applications. For example, one representative application in social network analysis is to identify user communities based on users X  pro-files as well as their social connections. If we respectively treat user profiles and connections as features and pairwise constraints, this application is essentially a semi-supervised clustering problem. Since new connections are being formed over time, user communities should also be frequently up-dated. Similar situations also occur in various real-world e-commerce platforms, which typically require to group items or customers based on their profiles ( i.e. , features) and dy-namic co-purchasing histories ( i.e. , pairwise constraints).
We notice that although the subject of evolving clustering has been intensively studied, to the best of our knowledge, no previous study has focused on the problem of efficiently updating semi-supervised clustering results given sequential constraints. To tackle this challenging problem, in this paper we propose an efficient dynamic semi-supervised clustering framework for large-scale data mining applications [48, 22, 40, 41]. The key idea is to cast the semi-supervised cluster-ing problem into a search problem over a convex hull. More specifically, the proposed framework consists of two com-ponents: (i) an offline step for constructing a convex hull, and (ii) an online step for efficiently updating the clustering results when the new pairwise constraints come in. In the first step, we employ the ensemble clustering technique [51] to generate m ensemble partitions of n input data points to be clustered. According to the principle of ensemble cluster-ing [51, 54, 25], the optimal data partition can be approxi-mated by a convex combination of the ensemble partitions. Since all the convex combinations of m ensemble partitions form a convex hull with m extreme points, the optimal data partition should lie in the inner space spanned by this convex hull. This observation relaxes the data clustering problem to a novel problem of learning m combination weights, or e-quivalently, an m -dimensional probability simplex vector  X  Since m is usually significantly smaller than the number of data points n , it enables us to efficiently update the data partitions through updating the m -dimensional probability simplex vector  X  . In the second step of the framework, we design an efficient updating scheme that is able to update  X  in a time complexity of O ( m 2 ), which does not depend on the number of the data points to be clustered.

Compared with the existing approaches engaged in semi-supervised clustering, our proposed approach owns the fol-lowing advantages: 1. By applying the ensemble clustering technique, our ap-2. By simplifying the problem of clustering n data points
To evaluate the performance of the proposed dynamic semi-supervised clustering approach, we conduct empirical studies on several real-world benchmark datasets. The ex-perimental results show that the proposed approach surpass-es the competing approaches in both accuracy and efficiency.
In this section, we divide the related work into three cate-gories, namely semi-supervised clustering, clustering accord-ing to user X  X  feedback, and dynamic network clustering.
Most semi-supervised clustering methods can be catego-rized into two main groups [6]: constrained clustering meth-ods and distance metric learning based methods. The con-strained clustering methods employ side information to con-fine the solution space, and only seek feasible data parti-tions consistent with given constraints. Among them, hard constraints based methods only consider the cluster assign-ments such that all the constraints are strictly satisfied. For instance, Wagstaff et al. [57] modified the K -means clustering and self-organizing map algorithms to adjust the cluster memberships towards the given pairwise constraints. In [50], a generalized Expectation Maximization (EM) algo-rithm was applied to ensure that only the mixture models
A probability simplex vector is a vector whose elements are non-negative and sum up to 1. matching all the constraints are considered. The hard con-straints based methods tend to be more sensitive to noise since some constraints may make the corresponding cluster-ing problems infeasible [18]. To overcome this issue, a lot of studies have treated side information as soft constraints [3, 4, 17, 36, 45]. Instead of satisfying all the constraints, the soft constraints based methods aim to preserve those con-straints as many as possible, while penalizing the number of violated constraints. In [3, 45, 4], probabilistic models were proposed to deal with semi-supervised clustering tasks, in which pairwise constraints were treated as Bayesian priors. In [32], pairwise constraints were formed as an additional penalty term in the objective of spectral learning. Law et al. [36] proposed a graphical model that considers pairwise constraints as random variables; then an EM-based algorith-m was applied to model the uncertainty of the constraints. Kulis et al. [35] further enhanced the performance of K -means clustering by learning a kernel matrix which incor-porates the given pairwise constraints. In [61], an inductive matrix completion technique was employed to solve the con-strained clustering problem, where data features were treat-ed as side information.

The second group of semi-supervised clustering methods depend on distance metric learning (DML) techniques [60]. In a typical fashion, these methods first learn a distance met-ric with given pairwise constraints, such that the data pairs with must-link constraints have smaller distances than those pairs with cannot-link constraints. They then derive a linear transform from the learned distance metric and readily ap-ply it to yield a new vector representation for the raw data points. The final data partition is simply obtained through running existing clustering algorithms over the transformed data representation. Various distance metric learning algo-rithms have been incorporated into semi-supervised cluster-ing. Xing et el. [60] formulated distance metric learning as a PSD constrained convex programming problem. In [19], an information-theoretic method was exploited to learn a Mahalanobis distance function. Weinberger et el. [59] pro-posed a nearest-neighbor classifier to enforce that the data pairs from different classes are separated by a large mar-gin. In [1], relevant component analysis was conducted to learn a distance metric by assigning larger weights to rel-evant dimensions and smaller weights to irrelevant dimen-sions, respectively. This work was further improved in [29] by simultaneously exploring both must-link and cannot-link constraints. In addition to the linear DML algorithms, a number of nonlinear DML algorithms have also been pro-posed to construct nonlinear mappings from the original da-ta space [55, 49, 28, 12].

In recent years, an increasing amount of literature has begun to study the problem of evolving clustering. For in-stance, clustering results can be updated according to user X  X  feedback [15, 7]. In particular, Cohn et el. [15] considered a scenario that users can iteratively provide different types of feedbacks regarding the clustering quality, and then de-veloped an EM-like algorithm to update the distance metric towards achieving a higher-quality clustering. The algorith-m proposed in [7] is a variant of the complete-link hierar-chical clustering algorithm, which combined the feedback stemming from pairwise constraints to learn a new distance metric. Although the aforementioned algorithms can im-prove the clustering performance by leveraging user X  X  feed-back, they all suffer from a scalability issue since learning distance metrics is computationally expensive, especially in high dimensions. The main focus of evolutionary cluster-ing [11, 13] is to learn a clustering that fits the current data well and does not shift dramatically from the historical data. Chakrabarti et el. [11] developed the evolutionary versions of both the K -means and agglomerative hierarchical clus-tering algorithms. Chi et el. [13] then extended this idea to develop the evolutionary spectral clustering algorithm. Be-sides, dynamic network clustering [11, 13, 52, 39, 33] was suggested to solve the community evolution problem when a network to be clustered changes continuously. In [52], a parameter-free algorithm called GraphScope was proposed to mine time-evolving graphs obeying the principle of Min-imum Description Length (MDL). FacetNet [39] employed probabilistic community membership models to identify dy-namic communities within a time-evolving graph. Kim and Han [33] further allowed a varying number of communities and presented a particle-and-density based algorithm to dis-cover new communities or dissolve existing communities. Al-beit looking similar, dynamic network clustering is different from the focus of this paper due to the following reasons: (i) dynamic network clustering algorithms only use links to guide clustering but ignore the important feature informa-tion; (ii) they rely on a large amount of link information to conduct clustering, while our studied dynamic clustering only requires a small number of pairwise constraints. Due to the flexibility of handling both data features and dynamic relationships, our proposed semi-supervised clustering ap-proach better fits conventional clustering applications.
In this section, we first present a general framework for semi-supervised clustering, followed by the proposed efficient algorithm for dynamic semi-supervised clustering.
Let X = ( x 1 ,..., x n ) be a set of n data points to be clus-tered, where each data point x i  X  R d , i  X  [ n ] is a vector of d dimensions. Let M t be the set of must-link constraints gen-erated until time t , where each must-link pair ( x i , x implies that x i and x j should be in the same cluster. Sim-ilarly, let C t be the set of cannot-link constraints generated until time t , where each cannot-link pair ( x i , x j )  X  C plies that x i and x j should belong to different clusters. For ease of presentation, we also define  X  t = M t  X  X  t to include all pairwise constraints generated until time t . Similar to most studies on data clustering, we assume that the number of clusters r is given a priori. Throughout this paper, we use a binary matrix F  X  { 0 , 1 } n  X  r to represent the result of partitioning n data points into r clusters, where F ij indicates that x i is associated with the j -th cluster. We further denote F as the set of all possible clustering results where F k,  X  and F  X  ,i indicate the k -th row vector and i -th column vector of matrix F , respectively. Let  X  ( x , x 0 kernel function used to measure the similarity between two data points x and x 0 , and let K = [  X  ( x i , x j )]  X  the kernel matrix. The goal of semi-supervised learning is to find the clustering result that is consistent with both the kernel similarities in K and pairwise constraints in  X  t . To measure the discrepancy between the kernel similarity K and a clustering result F , we define the distance between K and F as As indicated by the above measure, the smaller the distance d ( K,F ), the better the consistency between the clustering result F and the similarity matrix K . We note that an al-ternative approach is to measure the distance by tr( F &gt; where L = diag( K 1 )  X  K is the graph Laplacian [56].
To measure the inconsistency between the clustering re-sult F and pairwise constraints, we introduce two loss func-tions, one for must-links and the other for cannot-links. More specifically, given a must-link ( x i , x j )  X  X  t , we define the loss function `  X  ( F i,  X  ,F j,  X  ) as Likewise, given a cannot-link ( x i , x j )  X  X  t , we define the loss We note that a loss function similar to (3) has been used in label propagation with cannot-links [44]. The justification of using loss function ` + (  X  ,  X  ) for cannot-links is provided in the following proposition.

Proposition 1. Let a  X  R d + be a fixed vector. Let b  X  be the minimizer to the following optimization problem where q  X  1 . Then we have b  X   X  a .
 As indicated by Proposition 1, by minimizing the loss func-tion ` + ( F i,  X  ,F j,  X  ), the resulting solution under no other con-straints will satisfy F j,  X   X  F i,  X  , implying that x i assigned to different clusters.

Using the distance measure d ( K,F ) and the loss functions ` + and `  X  , we can cast semi-supervised clustering into the following optimization problem where the threshold  X  decides the level of consistency be-tween the clustering results and the kernel similarity.
The main challenge of dynamic semi-supervised clustering arises from the fact that pairwise constraints are sequentially updated over time. A naive approach is to solve the opti-mization problem in (4) from scratch whenever pairwise con-straints are updated. A more efficient approach is to exploit the fact that only a small portion of pairwise constraints are updated at each time, leading to a small change in the clus-tering result. Based on this intuition, we can use the existing solution as an initial solution to the optimization problem with updated constraints. Despite the simplicity, this ap-proach can significantly reduce the running time, and has been widely used in clustering social networks with dynam-ic updates [11, 13]. Although this simple approach reduces the number of iterations due to the appropriate initializa-tion, it still needs to solve the optimization problem in (4), which is computationally expensive when the number of da-ta points n is very large. To address this challenging issue, we propose an efficient dynamic semi-supervised clustering algorithm that is highly efficient for clustering large-scale data sets.
The proposed algorithm is based on a key observation that the number of different clustering results F in the set  X  = { F  X  X  : d ( K,F )  X   X  } is not very large when  X  is relatively small and the eigenvalues of K follow a skewed distribution.
To see this, we denote by  X  1 ,..., X  n the eigenvalues of K ranked in descending orders, and v 1 ,..., v n the correspond-ing eigenvectors. {  X  k } follows a q -power law if there exists a constant c such that  X  k  X  ck  X  q , where q &gt; 2. The follow-ing lemma summarizes an important property of K when its eigenvalues follow a q -power law.

Lemma 1. Define V = ( v 1 ,..., v n ) and  X  = V &gt; x for a unit vector k x k 2 = 1 . If x &gt; K  X  1 x  X   X  , we have given that the eigenvalues of K follow a q -power law. The above lemma shows that when the eigenvalues of K fol-low a power law, V &gt; x is an ` 1 sparse vector if x &gt; This observation provides a key foundation for our analysis.
Define  X  n (  X ,r ) to be the maximum number of partitions in  X  for r -way clustering such that the difference between any two partitions is at least  X  . The theorem below bounds  X  (  X ,r ).

Theorem 1. where C is an universal constant and The proof of Lemma 1 and Theorem 1 are deferred to the Appendix.
 Remark: Theorem 2 implies that when  X  is large enough,  X  (  X ,r ) is upper bounded by O ( n O ( r nificantly less than the number of possible r -way clustering partitions r n .

Based on the above results, there is a relatively smal-l number of significantly different clustering results in the subspace  X . Hence, to improve the computational efficien-cy of dynamic semi-supervised clustering, a natural thought is to pre-compute all the possible clustering results in  X , and find the best clustering result in  X  that is consistent with most of the dynamically updated pairwise constraints. However, since the number of different clustering results in  X  is still large, it is computationally intractable to identify all of them.

To address this problem, we propose to construct a convex hull e  X   X  X  to approximate the set  X . The key advantage of using a convex hull approximation is that all the solution-s in e  X  can be represented by convex combinations of extreme points 2 . Thus, in order to find the best cluster-ing result, we only need to compute the best combination weights of e  X  X  X  extreme points. Since the number of combi-nation weights to be determined is much smaller than the number of data points to be clustered, this enables us to compute combination weights in an extremely efficient way.
Specifically, the proposed clustering process is composed of two steps: an offline and an online step. In the offline step, we generate multiple partitions of the same dataset X and use such partitions to construct a convex hull e  X . In the online step, an efficient learning algorithm is developed to update the combination weights based on the newly received pairwise constraints. Below, we describe the two steps in detail.
In this step, we generate the convex hull e  X  using the tech-nique of ensemble clustering [51]. The main idea behind ensemble clustering is to combine multiple partitions of a same dataset into a single data partition, hoping to exploit the strength of different clustering results, and compensate for their limitations.

According to [26], multiple partitions of a same dataset can be generated by (i) applying different clustering algo-rithms [24], (ii) using the same algorithm with different ini-tializations and parameters [53, 23], (iii) employing different pre-processing and feature extraction mechanisms [9], and (iv) exploring different subsets of features [24, 21]. In or-der to efficiently generate ensemble partitions for large-scale data sets, we employee the last approach by first random-ly selecting m ( m n ) different subsets of features in X , followed by applying the approximate kernel K -means algo-rithm [14] to each feature subset. Since the cluster labels of such ensemble partitions can be arbitrary, we then ap-ply the Hungarian algorithm [34] to realign their labels. We denote by P = { P 1 ,P 2 ,...,P m } the m realigned ensemble partitions, where each partition P i  X  F , i  X  [ m ] divides X into r disjoint subsets.

The following proposition shows that the ensemble par-titions P 1 ,...,P m are the m extreme points of the convex hull conv { P 1 ,...,P m } .

Proposition 2. The convex hull e  X  = conv { P 1 ,...,P m } = {  X  1 P 1 +  X  X  X  +  X  m P m : has m extreme points, with each of them being equal to P i [ m ] .
 Proposition 2 shows that the data partitions P i , i  X  [ m ] , are not interior points of any line segments lying entirely in the convex hull e  X . Hence, all the solutions in be represented by convex combinations of the m ensemble partitions in the set P .

We claim that the convex hull e  X  is a good approximation of the set  X  = { F  X  X  : d ( K,F )  X   X  } , due to the following two reasons:
An extreme point of a convex set S , is a point x  X  X  , with the property that if x =  X y + (1  X   X  ) z with y,z  X  S and  X   X  [0 , 1], then y = x or z = x . 1. The ensemble partitions are generated by exploring a 2. According to the widely applied median partition based
By exploiting the convex hull as a search space, we are able to cast the problem of clustering n data points into the problem of learning a m -dimensional probability simplex vector  X  . In more detail, other than directly computing the best F , we solve the following convex optimization problem to find the optimal  X  Since m is a small number that is much less than the data size n , the optimization problem in (5) can be solved in an extremely efficient way 3 . When no pairwise constraints is involved, the problem of semi-supervised clustering reduces to a standard ensemble clustering problem. The clustering result in terms of the probability simplex vector, denoted by  X  , can be simply set to ( 1 m ,..., 1 m ). In the following, we introduce the online step for efficiently updating data parti-tions when pairwise constraints are generated sequentially.
In this subsection, we develop an online step to efficient-ly update the probability simplex vector given new pairwise constraints. To simplify the presentation, we divide M t set of must-link constraints received until time t , into two subsets: M a t that includes all the must-link constraints re-ceived before time t and M b t that includes the new must-link constraints added at time t . Similarly, we divide the cannot-link set C t into C a t and C b t . We also denote by  X  1 sequence of probability simplex vectors computed based on the updated constraints.

Using the above notations, we rewrite the optimization problem in (5) as
We note that the ensemble partitions are computed offline and therefore do not affect the efficiency of online computa-tion, which is the main concern of this work.
 Algorithm 1 The projection of a vector onto the probabil-ity simplex [58] Input : a vector v  X  R m to be projected 1: Sort v into w : w 1  X  w 2  X  X  X  X  X  X  w m 2: Find k = max { j  X  [ m ] : w j  X  1 j ( P k i =1 w i  X  1) &gt; 0 } 3: Compute  X  = 1 k ( P k i =1 w i  X  1) Return: u  X  R m s . t . u i = max( w i  X   X , 0) , i  X  [ m ] where
L t ( F ) =
L t ( F ) = Since  X  t  X  1 is the minimizer of the objective L a t ( F (  X  )), we can approximate L a t ( F (  X  )) as As a result, the optimization problem in (6) is further sim-plified as where parameter  X  is introduced to balance between two ob-jectives, i.e. , ensuring that the learned  X  is not far away from  X  t  X  1 , and also consistent with the new pairwise constraints. Compared to (6), the main advantage of problem (7) is that it only involves the new constraints that are added to the system at time t and does not need to store and work with the constraints received before time t .

In the following, we discuss how to efficiently update the probability simplex vector given the new pairwise constraints. By incorporating F into the optimization problem, we rewrite problem (7) as where P k ( i, :) and P k ( j, :) represent the i -th and the j -th row vector of the matrix P k , respectively. The optimization problem (8) can be efficiently solved by a gradient descend method. Specifically, in the q -th iteration, we update the probability simplex  X  by where  X  is a step size and P  X   X  is an efficient algorithm that projects a m -dimensional vector onto the probability sim-plex in O ( m log m ) time, as described in Algorithm 1.
Note that  X  f (  X  q ) has a closed-form solution as  X  f (  X  q ) = 2 [ X where U ( ij ) and V ( ij ) are two m  X  r matrices, satisfying and Then the probability simplex can be updated via where I m is the m  X  m identity matrix. Since the matrices U can efficiently update  X  using equation (10).
 Space-Efficient Relaxation: Despite low time complexi-ty, the updating scheme (10) suffers from a large space com-plexity to store all the matrices U ( ij ) U ( ij ) &gt; and V We now discuss how to reduce the storage cost by relaxing the optimization procedure.

Note that the k -th row of the matrix U ( ij ) should be ei-ther of these two cases: (i) containing all zero entries if the ensemble partition P k put object i and object j in the same cluster, or (ii) containing one positive entry (= 1) , and one negative entry (=  X  1) if the ensemble partition P k put ob-ject i and object j in different clusters. Then the diagonal elements of the matrix U ( ij ) U ( ij ) &gt; either equal 0 or equal a positive value (= 2). Thus the matrix essentially assigns less weight to the ensemble partitions that mistakenly put the object i and object j in different clus-ters when they share a must-link connection. Likewise, the matrix essentially assigns less weight to the ensemble partitions that mistakenly put the object i and object j in the same cluster when they share a cannot-link constraint. After updating  X  q +1 from  X  q , the ensemble partitions that are consistent with the new pairwise constraints are assigned larger weight-s, while the ensemble partitions that are not consistent with the new pairwise constraints are assigned smaller weights. This leads to a relaxed updating procedure 4
P  X   X  { (1  X  2  X  X  )  X  q + 2  X  X   X  t  X  1  X  C X  X where e ( ij ) is an m -dimensional vector with the k -th ele-ment equaling 0 if the ensemble partition P k is consistent onal matrices. Thus the relaxation that only considers their diagonal elements can lead to information loss, a problem that will be investigated in our future work. with the pairwise constraints ( x i , x j ), and 1 otherwise. The parameter C &gt; 0 is introduced to ensure that the third term in (11) is comparable with the first two terms.

Given the learned probability simplex  X  , we can generate a soft labeling matrix as a linear combination of the ensemble partitions Then the hard partition can be easily obtained by applying the efficient K -means clustering algorithm [8] to P or round-ing, i.e., assigning the i -th data point to the k -th cluster if P ik is the largest entry in the i -th row of P .

The time complexity to update the probability simplex vector is O ( pm 2 ), where p is the number of pairwise con-straints added at time t . Given the updated  X  t , the time complexity to generate a hard partition is O ( mnr ). Since generally speaking, both m and r are much smaller than n , the total time complexity of the proposed algorithm is O ( n ), which cannot be further improved since all n data points must be go through at least once for assignment.
In this section, we empirically demonstrate that our pro-posed semi-supervised clustering algorithm is both efficient and effective.
Four real-world benchmark datasets with varied sizes are used in our experiments, which are:
In order to generate m ensemble partitions, we need to randomly sample  X  d out of d features in each time. Two criteria are adopted in determining the value of  X  d . First, should be small enough to make ensemble partitions diverse. Second,  X  d should be reasonably large to generate reliable ensemble partitions since the quality of the starting point  X  0 depends on the quality of the ensemble partitions. In our experiments, we set  X  d = d d/ 20 e .

In addition, as a key factor which affects the performance of the proposed clustering algorithm, the number of ensem-ble partitions m introduces a trade-off between the clus-https://archive.ics.uci.edu/ml/datasets/Covertype http://yann.lecun.com/exdb/mnist/ tering quality and efficiency. As m increases, the cluster-ing quality tends to improve at the cost of increased com-puting time. In the following, we analyze how the clus-tering performance will be influenced by m . To this end, we conduct the experiments on the two largest datasets, Covtype and MNIST8m. On both of the two datasets, we begin with the unsupervised data partition  X  0 generated from m = { 50 , 100 , 150 , 200 , 250 , 300 } different ensem-ble partitions. Then we randomly generate 500 pairwise constraints based on the ground truth categorizations. For each m , we apply the proposed algorithm to update the data partition and use normalized mutual information (NMI for brevity) [16] to measure the coherence between the updated clustering result and the ground truth categorizations. This experiment is repeated ten times, and the clustering perfor-mance NMI, computed as the average over the ten trials, is plotted in Figure 1.

As expected, the clustering performance NMI keeps in-creasing when m becomes larger. The performance gain is due to the fact that a larger m indicates not only a larger search space but also a larger overlap between  X  and the convex hull  X   X . In addition, a larger number of ensemble partitions usually provide a more diverse clustering result, leading to a higher chance of finding the data partitions that are consistent with most or even all of the pairwise constraints. We also notice that the clustering performance NMI of the proposed algorithm gradually stabilizes as m in-creases to 300. This is not surprising, since when the number of ensemble partitions is already large enough, adding more partitions cannot provide more information because the new partitions are likely to coincide with some existing ensemble partitions. Such observations hence offer the guidance to appropriately choose m : on one hand, m should be reason-ably large to provide a diverse and sufficiently large search space; on the other hand, m should be relatively small to reduce the computational cost. We set m = 300 throughout all the remaining experiments.
To examine the effectiveness and efficiency of the pro-posed semi-supervised clustering algorithm, we compare it a-gainst the following distance metric learning and constrained clustering algorithms: (a) PGDM , the probabilistic glob-al distance metric learning algorithm [60], (b) LMNN , the large margin nearest-neighbor classifier [59], (c) ITML , the information-theoretic metric learning algorithm [19], (d) R-CA , the relevant component analysis based metric learning algorithm [1], (e) DCA , the discriminative component anal-ysis based metric learning algorithm [29], (f) CCSKL , the constrained clustering algorithm via spectral kernel learn-ing [38], and (g) PMMC , the pairwise constrained maxi-mum margin clustering algorithm [62]. We refer to the pro-posed clustering algorithm as S emi-supervised C lustering with S equential C onstraints, or the abbreviation SCSC .
In our experiments, we begin with u randomly generat-ed pairwise constraints, denoted by the tier t 1 . In each of the following tiers, another set of u randomly sampled pair-wise constraints are generated, and all the compared semi-supervised clustering algorithms are called to update their data partitions based on the newly generated pairwise con-straints. Specifically, we rerun all the compared algorithms by adding the new pairwise constraints into the old ones. We repeat such steps from tier t 1 to tier t 5 , finally result-Figure 1: The clustering performance normalized mutual information (NMI) vs. the number of en-semble partitions m = { 50 , 100 , 150 , 200 , 250 , 300 } . ing in a total of 5 u randomly sampled pairwise constraints. Since the MNIST8m and Covtype datasets are much larger than the COIL20 and USPS datsets, we set u = 500 for the former two datasets, and u = 100 for the latter two datsets. All the experiments are performed on a Linux machine with Intel Xeon 2 . 4 GHz processor and 64 GB of main memory. Each experiment is repeated ten times, and the average clus-tering performance NMI and the average running time are reported. We mark the running time as N/A if an algorithm cannot converge to yield meaningful data partitions within 2 hours.

Figure 2 displays the curves of the clustering performance for all the referred semi-supervised clustering algorithms, where we exclude the four baseline algorithms ( PGDM , LMNN , CCSKL , and PMMC ) on the two large dataset-s, i.e. , MNIST8m and Covtype, since the data partition-s cannot be updated by such algorithms within 2 hours. In comparison to the other competing algorithms, the pro-posed dynamic semi-supervised clustering algorithm SCSC accomplishes the best performance on all the four datasets. It is important that the proposed SCSC algorithm outper-forms all the competing algorithms from the very beginning, i.e. , when only a small number of pairwise constraints are given. The reason is that by generating a convex hull from a set of ensemble partitions, we actually reduce the possi-ble search space dramatically and all the inner points in that convex hull can map to reasonably good data partitions. Al-so, since the starting point  X  0 is produced by incorporating the strengths of multiple ensemble partitions,  X  0 should al-ready be close to the optimal solution. Therefore, a simple local search should be good enough to recover the optimal partition. On the negative side, the variance of the pro-posed SCSC algorithm is relatively large, especially com-paring with some distance metric learning algorithms such as RCA , DCA and ITML . We conjecture that the large DCA [29], CCSKL [38], and PMMC [62] from tier t 1 to t on four datasets.
 variance may be caused by the randomness in generating ensemble partitions. In our future work, we will investigate and endeavor to address the problem of generating less un-certain ensemble partitions.

Finally, we evaluate the computational efficiency of the proposed SCSC algorithm. Table 1 shows that the updating procedure of SCSC is extremely efficient. In particular, SCSC is able to update the partitioning results of more than 8 million samples in about 5 seconds.
In this paper, we proposed a dynamic semi-supervised clustering algorithm which can efficiently update clustering results given newly received pairwise constraints. The key idea is to cast the dynamic clustering process into a search problem over a feasible clustering space that is defined as a convex hull generated by multiple ensemble partitions. Since any inner point of the convex hull can be uniquely represented by a probability simplex vector, the dynamic semi-supervised clustering problem can be reduced to the problem of learning a low-dimensional vector. Given a set of sequentially received pairwise constraints, we devised an up-dating scheme to update the data partition in an extremely efficient manner. Our empirical studies conducted on sever-al real-world datasets confirmed both the effectiveness and efficiency of the proposed algorithm. [1] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. [2] S Basu, A Banerjee, and R J. Mooney.
 [3] S. Basu, M. Bilenko, and R. J. Mooney. A [4] R. Bekkerman and M. Sahami. Semi-supervised [5] S. Bhatia and J. Deogun. Conceptual clustering in [6] M Bilenko, S Basu, and R J. Mooney. Integrating [7] M. Bilenko and R. J. Mooney. Adaptive duplicate [8] D Cai. Litekmeans: the fastest matlab implementation [9] Marine Campedel, Ivan Kyrgyzov, and Henri Maitre. [10] Claudio Carpineto and Giovanni Romano. A lattice [11] D. Chakrabarti, R. Kumar, and A. Tomkins.
 [12] Shiyu Chang, Charu C. Aggarwal, and Thomas S. [13] Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng. [14] R. Chitta, R. Jin, T. C. Havens, and A. K. Jain. [15] D. Cohn, R. Caruana, and A. McCallum.
 [16] T. Cover and J. Thomas. Elements of Information [17] Ian Davidson and S. S. Ravi. Agglomerative [18] Ian Davidson and S. S. Ravi. Clustering with [19] Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, [20] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. [21] C. Domeniconi and M. Al-Razgan. Weighted cluster [22] W. Fan and A. Bifet. Mining big data: current status, [23] X. Fern and C. E. Brodley. Random projection for [24] X. Fern and C. E. Brodley. Solving cluster ensemble [25] Lucas Franek and Xiaoyi Jiang. Ensemble clustering [26] Ana L. N. Fred and Anil K. Jain. Combining multiple [27] Hichem Frigui and Raghu Krishnapuram. A robust [28] T. Hertz, A. Hillel, and D. Weinshall. Learning a [29] S.C.H. Hoi, W. Liu, M.R. Lyu, and W.Y. Ma.
 [30] J.J. Hull. A database for handwritten text recognition [31] A. K. Jain. Data clustering: 50 years beyond k-means. [32] S D. Kamvar, D Klein, and C D. Manning. Spectral [33] Min-Soo Kim and Jiawei Han. A particle-and-density [34] H. W. Kuhn and Bryn Yaw. The hungarian method [35] B. Kulis, S. Basu, I. S. Dhillon, and R. J. Mooney. [36] M Law, A P. Topchy, and A K. Jain. Model-based [37] Q Li and B Kim. Clustering approach for hybrid [38] Z. Li and J. Liu. Constrained clustering by spectral [39] Yu-Ru Lin, Y. Chi, S. Zhu, H. Sundaram, and B. L. [40] W. Liu, J. He, and S. Chang. Large graph [41] W. Liu, J. Wang, and S. Chang. Robust and scalable [42] X. Liu and W. B. Croft. Cluster-based retrieval using [43] G. Loosli, S. Canu, and L. Bottou. Training invariant [44] Z. Lu and M.  X  A. Carreira-Perpi  X n  X an. Constrained [45] Z. Lu and T. K. Leen. Semi-supervised learning with [46] Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. [47] Y. Plan and R. Vershynin. Robust 1-bit compressed [48] A. Rajaraman and J. D. Ullman. Mining of massive [49] C. Shen, J. Kim, L. Wang, and A. Hengel. Positive [50] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. [51] Alexander Strehl and Joydeep Ghosh. Cluster [52] J. Sun, C. Faloutsos, S. Papadimitriou, and P. S. Yu. [53] Alexander P. Topchy, Anil K. Jain, and William F. [54] Alexander P. Topchy, Anil K. Jain, and William F. [55] L. Torresani and K. Lee. Large margin component [56] Ulrike Von Luxburg. A tutorial on spectral clustering. [57] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr  X  odl. [58] Weiran Wang and Miguel  X  A. Carreira-Perpi  X n  X an. [59] K.Q. Weinberger, J. Blitzer, and L.K. Saul. Distance [60] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance [61] J. Yi, L. Zhang, R. Jin, Q. Qian, and A. K. Jain. [62] H. Zeng and Y. Cheung. Semi-supervised maximum Note that  X  i and v i , i  X  [ n ] are the eigenvalues and eigen-vectors of the kernel matrix K . Since  X  = V &gt; x , we have which implies Thus, we have We complete the proof by using the facts k x k 2 = 1 and c  X  n .

In order to show that the number of significantly different partitions in  X  is small, we first consider a simple case where the number of classes is 2. In this case, we can simplify the domain  X  as We define  X  n (  X  ) the maximum number of partitions in  X  such that the difference between any two partition vectors v 1 and v 2 is at least  X  . The theorem below bounds  X  n (  X  ).
Theorem 2. where C is an universal constant and
Proof. We first notice that k v k 2 = step, we relax  X  2 into  X  0 2 as follows Define  X  n ( X  0 2 , X  ) the maximum number of vectors such that the distance between any two vectors is at least  X  . Since the distance between any two partitions that differs by at least  X  entries is at least 2 Using the result of Lemma 1 and the covering number the-orem [47] to bound, we have where s is defined in (12).

The above result for two-way clustering can be easily ex-tended into multiple-way clustering.

We prove Proposition 2 by contradiction. Suppose at least one ensemble partition P i is not the extreme point of the con-vex hull  X   X  = conv { P 1 ,...,P m } , then P i should be an inner point of  X   X . According to the definition of extreme points, it is evident that all the extreme points of  X   X  should be feasible partitions in the set F . We denote them as F 1 ,...,F l Therefore, P i can be represented by a convex combination of F 1 ,...,F l : where  X  is a l -dimensional probability simplex vector. Since each row of P i contains only one 1 and all the other elements are 0,  X  j should be equal to 0 for any F j 6 = P i . Otherwise P i cannot be equal to A = P l j =1  X  j F j since there must exist at least one index ( a,b ) such that P i ( a,b ) = 0 while A ( a,b ) &gt; 0. Since P i 6 = F j ,  X  j = 1 ,...,l , we have  X  = 0 which contradicts with the constraint 1 &gt;  X  = 1. This verifies that P i is not a convex combination of other extreme points.
Since the number of the extreme points in  X   X  is at most m and each partition P i , i  X  [ m ] , is not a convex combination of the remaining m  X  1 ensemble partitions, the convex hull  X   X  has exactly m extreme points with each of them equaling to P i , i  X  [ m ].
