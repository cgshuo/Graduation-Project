 Outlier detection and ensemble learning are well established research directions in data mining yet the application of en-semble techniques to outlier detection has been rarely stud-ied. Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors. We show analytically and experimentally that an outlier detec-tor based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the re-sults of the same outlier detector on the complete dataset. Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intu-ition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection. As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depend-ing on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.
 H.2.8 [ Database Applications ]: Data mining outlier detection; ensemble
An outlier is  X  X n observation (or subset of observations) which appears to be inconsistent with the remainder of that  X 
This work was done while the author was on leave of ab-sence from Ludwig-Maximilians-Universit  X  at M  X  unchen, Ger-many.  X 
This work was done while the author was on sabbatical leave from University of S  X ao Paulo, S  X ao Carlos, Brazil. set of data X  [6]. Detecting outliers is an important task in many practical applications. Some applications of outlier detection, such as detecting measurement errors, are mostly concerned with removing the outliers from the data as a form of  X  X oise X . Other applications, such as credit card abuse detection, or the identification of unusual measure-ments in scientific data, are concerned with finding outliers because their deviating behavior from the rest of the data may require specific actions or provide opportunities for new insights.

Various approaches to outlier detection have been pro-posed, based on different notions of outliers, or targeted towards specific applications that require the identification of outliers. Here, we are interested in unsupervised , non-parametric outlier detection methods that assign a score to each data object and thus allow a ranking of objects accord-ing to their degree of outlierness.

Parametric, statistical approaches [6, 35] fit certain distri-butions to the data by estimating the parameters of these distributions from the given data. A problem with these ap-proaches is that distribution parameters such as mean, stan-dard deviation, and covariances are rather sensitive to the presence of outliers. Possible effects of outliers on the param-eter estimation have been termed  X  X asking X  and  X  X wamp-ing X  . Outliers can mask their own presence by influencing the values of the distribution parameters (resulting in false negatives), or swamp inliers to appear as outlying due to the influenced parameters (resulting in false positives) [6, 19].
Non-parametric approaches do not assume a specific dis-tribution of the data, but estimate (explicitly or implic-itly) certain aspects of the probability density. Non-para-metric methods include the well-known  X  X istance-based X  and  X  X ensity-based X  methods. Both distance-based and density-based methods basically aim at providing a rather simple estimate of the density around points, which can be seen as an approximation of statistical kernel density estimates. Distance-based methods such as DB-outlier [25] and its vari-ants are based on the k nearest neighbor ( k NN) distances [5, 34], trying to find so-called global outliers as points that are, roughly speaking, far away from the rest of the data. Density-based methods such as LOF [10] and its variants try to find so-called local outliers as points that are, roughly speaking, located in an area of relative low density com-pared to their k NN (intended to indicate points that are outliers with respect to the nearest mode in the data dis-tribution). The density around points in these methods is also estimated based on k NN distances. One problem with distance-based and density-based methods is that they can also suffer from effects similar to masking and swamping , due to the simplicity of (and thus error in) the density es-timates. Another problem is the typically high runtime of these approaches, due to the fact that their computation in-cludes at least finding the k NN of each data point (resulting in an at least quadratic complexity w.r.t. the database size). In this paper, we address both problems of distance-based and density-based methods. We propose and study a general approach to improve both the quality and the performance of such outlier detection methods by combining into an en-semble results of a base method on subsamples of the data. Previous work on outlier ensembles is very limited and only shows empirically that ensembles of outlier detectors have the potential to improve the quality, compared to that of their base methods [30, 36], at an increased runtime cost.
Our work is novel and advances the area of outlier detec-tion in the following respects:  X  We argue theoretically and demonstrate empirically that it is possible to construct ensemble members for outlier de-tection methods which perform individually already bet-ter than the base method, in general.  X  Combining those outlier detectors into an ensemble ren-ders the performance gain not only more robust but can improve the performance even further.  X  At the same time, when using small sample sizes for the ensemble members, we can gain considerable speed-up in runtime compared to running a standard ensemble and, for small ensemble sizes, even compared to running the base method on the whole data set.  X  The proposed principle is fundamental and flexible. It does not rely on specific data types. It can be combined with various conventional outlier detection techniques.
The rest of the paper is organized as follows: We dis-cuss related work on outlier detection and ensembles for out-lier detection (Section 2). We provide theoretical reasoning to support outlier detection ensembles in general and the claimed properties of our method in particular (Section 3). We provide experimental results to support our claims em-pirically (Section 4). We conclude the paper in Section 5.
The distance-based notion of outliers (DB-outlier) [25] was the first database-oriented approach in the area of unsuper-vised outlier detection, which initiated a new line of research on this topic in the data mining community. Variants of DB-outliers consider the distances to the k nearest neighbors of each object and use these distances to rank the objects [34], or, they use the sum of distances to all points within the set of k NN (called the  X  X eight X ) as an outlier degree [5]. These methods are also called global methods in that the com-puted outlier scores represent global density scores for each point. The so-called local methods, e.g. LOF [10], consider instead local density scores, which are ratios between the density around an object and the density around its neigh-boring objects. Variants of the local outlier model include LoOP [27], and LOCI [33]. Also the distance-based method LDOF [44] is related in reasoning about local comparisons. It has been shown recently [37], however, that the differ-entiation between global and local methods is not strictly dichotomous but that there are degrees of locality .
Much research has aimed at improving the efficiency of unsupervised outlier detection by algorithmic techniques, for example based on approximations or improved pruning tech-niques for mining the top-n outliers [4, 7, 22, 23, 26, 42]. An analysis of such efficiency improving techniques for outlier detection algorithms has been provided by Orair et al. [32]. These techniques, however, do not aim at improving the ap-proximations of the underlying statistical notion of outlier-ness. They only approximate a specific algorithmic model.
Ensemble techniques, on the other hand, have the poten-tial to improve the performance of their components in terms of the quality of the detected outliers, rather than in terms of runtime (but we will show in this paper that it is even possible to gain performance improvements when construct-ing certain types of outlier ensembles). The first approach to improve outlier detection by ensemble techniques, based on  X  X eature bagging X , was proposed by Lazarevic and Ku-mar [30], combining different results of the same algorithm (namely LOF [10]) applied to different, randomly selected feature subsets. Feature bagging is a common procedure to induce diversity of ensemble members in ensemble classifi-cation [11] or ensemble clustering [8, 14, 40].

Subsequent research on outlier detection ensembles fo-cused on the issue of comparability of scores for score com-binations, using Sigmoid functions and mixture modeling to fit outlier scores, provided by different detectors, into comparable probability values [17], or scaling by standard deviation [31], or statistical reasoning about score distri-butions [28], enabling the combination of different outlier detection methods into one ensemble. Schubert et al. [36] proposed a similarity measure to appropriately compare dif-ferent outlier rankings (based on scores) and to allow for the assessment of the diversity of different outlier detectors. As an application, they propose a greedy ensemble approach, demonstrating the importance of diversity for the perfor-mance of an ensemble. In all these papers, although outlier detection ensembles have been discussed and improved, no new method of inducing diversity has been pursued.

Except for feature bagging [30], all other existing ensem-ble methods for outlier detection [17, 28, 31, 36] are meta-methods and could be used on top of our sample-based method (or on top of feature bagging, as in [28,31,36]). They do not propose original means to induce diversity when using a selected base outlier detection method.

In general, while the motivation for ensemble methods for outlier detection is borrowed from the rich tradition in the literature on supervised ensemble learning [11,12,21,41], the theoretical foundation for ensemble learning in the un-supervised setting is far less mature. The same holds true not only for outlier detection ensembles but also for cluster-ing ensembles despite the far more abundant literature on practical approaches in that area [18]. Although the prob-lem setting is considerably different, let us finally note that sampling has been used in ensemble clustering to induce diversity. Different subsamples of the data set have been clustered and the resulting clusterings were combined into a consensus clustering [13, 16, 20, 39].
In this section, we will discuss the potential benefits of using outlier detection ensembles based on subsampling.
Previous approaches using ensemble learning for outlier detection [17, 28, 30, 31, 36] transferred techniques without any theoretical foundation of why, what has a clear theoret-ical background in supervised learning, should also work in unsupervised outlier detection. Such a view can be loosely argued for when we consider outlier detection methods as  X  X lassifiers X . When assuming that a threshold on outlier scores is used to distinguish between outliers and inliers, we can view the outlier method as classifying all objects into one of these two classes: outliers and inliers  X  even though, no labels are used in the  X  X raining X  phase when the model (ranking) is built. If we succeed to construct diverse enough outlier detectors for the same data set, we can hope to im-prove the overall performance over the individual members by combining them into an ensemble. The  X  X eneric X  argu-ment given is that all the ensemble members are commit-ting errors but on different cases, if the members are inde-pendent, i.e., diverse, or, in other words, if the errors are uncorrelated. While such a  X  X eneric X  view may potentially explain some of the performance gains, we will show in the following subsections that there are more specific reasons for why (under some general assumptions) an ensemble of out-lier detection methods can improve the performance over its individual members.
In this paper, we are focusing on distance-based and densi-ty-based outlier detection methods, which, as discussed in the introduction, compute outlier scores that are based, im-plicitly or explicitly, on some form of density estimates. One can view these methods as trying to identify the outliers in a given data set X with respect to an unknown probability density f , which represents the process that has  X  X enerated X  the majority of the data set (at least the inliers). The data set X itself can be viewed as a sample drawn from the true, but unknown underlying density distribution, and the meth-ods try to estimate the density f ( x ) around points x using a more or less  X  X ough X  density estimate  X  f X ( x ) (in order to compute outlier scores in some way).

Assuming the correctness of the underlying outlier model of the methods, it is clear that the quality of a method X  X  result depends on the quality of the density estimate  X  f and that the results will improve if the estimate can be im-proved. For this case, we can show formally that a diverse ensemble of such outlier detectors does in fact show an im-proved expected performance over the individual ensemble members, under some general conditions.

Given a true, smooth p.d.f. f ( x ) and a data set X , we can express and estimate  X  f X ( x ) of f ( x ) based on X as: where v X ( x ) is a random variable describing the error of the estimate due to the finite sample.

The quality of the estimate  X  f of f decides over success and failure of the outlier detection. However, the density esti-mates used by the considered outlier detection algorithms may not be reliable and stable in all regions of the data space, due to the natural intrinsic randomness associated with a single sample that the data set represents. If we are able to obtain multiple density estimates for each point x (e.g., as we propose via subsamples), we can obtain more reliable and stable density estimates by averaging the mul-tiple density estimates for each point. The rationale for this is the following: The output of outlier methods is a ranking of all points x in terms of outlier scores that, in essence, depends on the ranking of the points according to Ideally, we want a ranking of the points x according to f ( x ). If we have multiple density estimates for each point that we average, we can consider the estimate itself as a random variable and averaging 1 these estimates for each point gives us the expectation of this variable as: In this formulation, one can clearly see that the ranking of objects w.r.t. E {  X  f X ( x ) } is the same as the ranking w.r.t. the true density f ( x ) (the  X  X deal ranking X ), if just the ex-pectation of the error v X ( x ) in the individual estimates is the same for every point x . This is obviously the case when the random variable that describes the error would not de-pend on x , in which case E { v X ( x ) } = E { v X } =  X  one would also obtain the  X  X deal X  ranking when the error is not independent on x ; for instance, when the error would vary between points but the expectation is the same for each point, we would also have the same ranking. We can even obtain the same ranking as the  X  X deal X  ranking if the expec-tations E { v X ( x 1 ) } and E { v X ( x 2 ) } differ for two points x and x 2 , as long as the difference does not cause an inver-sion between the actual ranks E {  X  f X ( x 1 ) } and E { respectively. Furthermore, if we consider that for success-ful outlier detection, the methods only have to distinguish between outliers and inliers, we can even allow inversions between ranks, as long as rank inversions occur only within outliers or within inliers. Only a rank inversion between an outlier and an inlier would be problematic. In the next subsection, we will argue that for the proposed ensemble technique using subsamples, the expectation of the error in the density estimate E { v X ( x ) } does depend on the location x and its surrounding density, but that the method has the desirable property that it can increase the  X  X ap X  in ranks be-tween the outliers and the inliers, making inversions in rank between these groups of points even less likely.
Subsampling is theoretically well suited to introduce di-versity into an ensemble of otherwise identical distance-based or density-based outlier detection methods. Every member of the ensemble will determine the outlier score of every ob-ject in the database, but only using a small subset of the data to estimate the density around points. Learning den-sity estimates for outlier detection on smaller samples can actually improve the detection rate of outliers, compared to learning these estimates on the whole data set that con-ceptually represents just a somewhat larger sample of an unknown distribution f . We will see in the empirical evalu-ation that in practice, surprisingly small sample sizes (such as 20% or in many cases even just 10%) are typically not leading to a deteriorated but to a considerably improved quality of the outlier detection for a sample-based ensemble of outlier detectors. One reason for the improved perfor-mance of an ensemble is, as expected, just the combination of the results of multiple outlier detectors. Compared to us-ing the dataset as the only sample drawn from f , drawing multiple subsamples X from this sample can minimize the effect of the randomness associated with a single sample.
Note that averaging the scores to build an ensemble has been, heuristically, common practice [17, 28, 30, 31, 36], but now it finds also a theoretical justification.
Another, more interesting reason for the improved perfor-mance is that the base method applied to a smaller subsam-ple of a given data often shows an improved outlier detection rate, compared to the same method applied to the whole data set. As we will argue formally in the following, this is due to the fact that distance-based and density-based meth-ods are essentially using simple (not volume normalized) k nearest neighbor distances to estimate density.

To understand the effects of sample based k nearest neigh-bor distances, consider a sphere of radius r in a d -dimensional Euclidean space, containing n data points uniformly dis-tributed within the sphere. The expected Euclidean dis-tance from a point to its k nearest neighbour ( k NN) is given by [9]: For a given data set, let r be a constant value small enough so that, for two spheres having the same radius r but lying on different positions of the data space, the data points within both spheres are approximately uniformly distributed. Now, suppose that the number of data points within each of these spheres is different, given by n 1 and n 2 ( n 1 6 = n means that the densities of the data in the respective regions of the space are different (as their volumes are the same). For example, one sphere might be located inside a dense cluster, whereas the other one might lie on a sparse area containing background noise. Then, it follows from (1) that the expected k NN distances in the corresponding regions of the space are given by: If one randomly removes a fraction 1  X  m of the data ob-jects with equal probability, the expected number of remain-ing objects within those two spheres are given by n 1 m and n m , respectively. In this case, the expected k NN distances become: The difference in the expected distances are therefore:  X  1 = r k  X  2 = r k In relative terms , if we divide  X  1 and  X  2 by the original expected distances (for the full dataset, i.e., before the sub-sampling), we get: The result in (6) says that the expected k NN distances within the spheres increase proportionally as a function of the subsampling rate m . This result reflects the intuition that, in relative terms, the contrast between the densities of the spheres is kept constant, which justifies the use of a Figure 1: Behaviour of the expected 5 -NN distances for two spheres with radius r = 1 , in a 2D Euclidean space, containing 1000 m (circles) and 100 m (trian-gles) objects uniformly distributed ( m is a fraction of the data). subsampling procedure with even sampling probabilities. In an ensemble setting, for instance, this means that one can get multiple (sub)samples that exhibit variability (diversity) in terms of their observations, but keep the same expected density profile as the full dataset.

The above result is important but it does not explain all implications of subsampling when using unnormalized k nearest neighbor distances. In absolute terms , Equations (4) and (5) tell us that the expected difference in the k NN dis-tances will be greater for a less dense sphere, i.e.,  X  1 if n 1 &lt; n 2 . This means that the expected k NN distances  X  X iverge X  in absolute terms when the data are downsampled to a fraction m of their original size. In other words, the absolute differences between the expected k NN distances in areas of different densities tend to increase as a function of the subsampling rate. This effect is illustrated in Figure 1 for r = 1, d = 2, k = 5, n 1 = 100, n 2 = 1000, and m ranging from 0.1 to 1.

Such an effect can be beneficial for outlier detection, since it can make it easier to distinguish between outliers and inliers. Particularly when also using an ensemble as dis-cussed above, the gap in the ranks between outliers and in-liers can increase, making inversion of ranks between these two groups less likely.
Note that the implementation of our proposal is not as simple as to take subsamples and then run the outlier de-tection algorithms on these subsamples. This way we would very likely completely miss information on the outlierness of many objects that are not contained in any subsample, and many objects would get scores only from some of the subsamples. Instead, for each ensemble member, we draw a subsample from the database and compute the neighbor-hood of each object in the database based on the subsam-ple. This way, using subsample-based ensembles can also lead to a considerable speed-up, compared to other types of ensembles and, for small subsamples and ensemble sizes, even compared to running the base method on the whole data set. We will demonstrate in the experimental evalua-tion that sample sizes small enough to achieve substantial runtime improvements are good choices in practice, lead-ing to good outlier detection rates. In this subsection, we show the expected runtime improvements by studying the theoretical complexities.
While other ensemble methods require a multiple of the computing time compared to the base learner, the theoreti-cal behaviour of a subsample based ensemble is faster (and requires less resources) than other types of ensembles. The typical complexity of a base method is O ( n 2 ), due to the required k NN queries over a database of n objects. The runtime of a  X  X tandard ensemble X  such as feature bagging is essentially s times the runtime of the base method, where s is a factor that is determined by the number of base learners used in the ensemble (i.e., the size of the ensemble). This factor is reduced in the case of feature bagging. Using only a subset of the dimensions makes individual distance com-putations faster by some constant factor.

For sample based ensembles, on the other hand, the com-plete ensemble can even be faster than the base method on the complete dataset, because of the quadratic runtime in n of the base method. While the base method requires k NN queries for each object on the complete database (hence O ( n 2 )), using a subsample of size m  X  n , 0 &lt; m &lt; 1, reduces this to O ( n 2  X  m ). The runtime of a sample based ensemble is essentially s times the runtime of the base method, using a much smaller data set for the neighborhood computation.
For an ensemble size of 10 base learners and sample size of 10%, the sample-based ensemble would require roughly the same runtime than a single base method on the full dataset but 10 times less time than an ensemble with the same number s of ensemble members based on other means of diversity. For larger ensembles, the ensemble requires only a small multiple of the base method but still only 10% (or the equivalent of the sample size m ) of a standard ensemble. For example, if we use 25 ensemble members and sample size 10%, the ensemble will require roughly 2.5 times the runtime of the base method.
For the reasons discussed in Section 2, the canonical com-petitor is feature bagging (FB) [30]. As base methods we use LOF [10], LDOF [44], and LoOP [27].

For the setup of experiments, we have to consider various parameters. For both ensemble methods (feature bagging and subsampling), we choose a fixed number of 25 ensemble members. We follow the original setup of the feature bag-ging method, combining the scores of the ensemble members by computing the average. For the subsampling, we consider various sample sizes. Each of the base methods requires a size k of the neighborhood. Hence we will show experimen-tal results (i) with a fixed choice of k and varying sample size; (ii) with a fixed sample size, varying k ; and (iii) with fixed choices of k and sample size, comparing different base methods. When we fix k , we choose a value that gives a rea-sonable result quality (i.e., better than random) for the base method and compare that to the ensemble variants. Finally (iv), for the synthetic dataset collections, where the individ-ual datasets follow the same general characteristics, we show an average behaviour over all datasets of the collection.
We report the area under the receiver operating charac-teristic curve (ROC AUC), which plots the true positive rate vs. the false positive rate, a common measure for evaluation of outlier detection methods [17, 28, 30, 31, 36]. The experi-ments are performed using ELKI [2, 3].
For a statistical assessment, we generate two independent sets of 30 synthetic datasets (batch1 and batch2). For each dataset, we choose randomly values for the following param-eters in the given range: dimensionality d  X  [20 ,..., 40], number of clusters c  X  [2 ,..., 10], for each cluster inde-pendently the number of points n c i  X  [600 ,..., 1000]. For each cluster, the points are generated following a Gaussian model as follows: For each cluster c i , and each attribute a , we choose a mean  X  c i ,a from a uniform distribution in [  X  10 , 10] and a standard deviation  X  c i ,a from a uniform dis-tribution in [0 . 1 , 1]. Then for the cluster c i , n c jects (points) are generated attribute-wise by the Gaussians random rotations and the covariance matrix  X  correspond-ing to the theoretical model is computed by the correspond-ing matrix operations [38]. Then, we compute for each point the Mahalanobis distance to its corresponding cluster center, using the covariance matrix  X  of the cluster. For a dataset dimensionality d , the Mahalanobis distances for each clus-ter follow a  X  2 distribution with d degrees of freedom. We label as outliers those points that exhibit a distance to their cluster center larger than the theoretical 0.975 quantile, in-dependently of the actually occurring Mahalanobis distances of the sampled points. This results in an expected amount of 2 . 5% outliers per dataset.

As real datasets we use the datasets Satimage, Lympho-graphy, and Segment (used also by Lazarevic and Kumar [30]). Additionally, we chose from the UCI machine learn-ing repository [15]: Wisconsin breast cancer (WBC) and Waveform Database Generator (waveform). While Lazare-vic and Kumar consider outlier detection as equivalent to rare class detection, we argue that outliers are bound to be rare, but objects of a rare class are not necessarily outliers. Therefore, we use a different preprocessing for some of the datasets: For Satimage, we combined train and test set and transformed the dataset to an outlier task by taking a sam-ple of 10% from class 2, evaluating the downsampled class as outliers vs. the rest. 2 For Lymphography, we merged the small classes 1&amp;4 as outliers vs. the rest. For Segment, we chose classes GRASS, PATH, and SKY for downsampling, in turn, to 10%, which renders the remaining objects of these classes outliers (resulting in three different datasets). For the datasets WBC and waveform we also select a mean-ingful outlier class for downsampling ( X  X alignant X , and  X 0 X , respectively). With this method of using classification data for evaluation of outlier detection methods we are conform with the literature [1, 24, 29, 43, 44].

Overall, this results in 60 synthetic and 7 real data sets.
For a fair comparison, we use a preprocessing of the neigh-borhood computation for all methods on equal terms, as fa-cilitated by the framework ELKI [2]. As in our experiments we use 25 ensemble members, we study the runtime of a typical base method (LOF), the subsampling ensemble (10% sample size) and feature bagging, when scaling the number of objects in the database. As demonstrated in Figure 2,
Lazarevic and Kumar used the smallest class 4 as outlier vs. rest, but this is an example where the rare class does not constitute outliers, as the classes 3-7 are all very similar. Accordingly, they report performance very close to a random result on this dataset. Figure 2: Runtime of LOF, subsampling ensemble, and feature bagging when increasing database size.
Figure 3: Quality with increasing ensemble size. the subsampling ensemble is close to the base method while feature bagging requires a multiple of the runtime.
As discussed in Section 3.3, the efficiency depends on the sample size and on the ensemble size. We do not evaluate the ensemble size further, let us just consider an example on one of the synthetic datasets to study the behaviour with adding more ensemble members (Figure 3). We see a strong increase in quality between 2 and 10 ensemble members, then, up to 25 ensemble members, the quality increases further, steadily but slowly. This improved performance comes at moderate runtime cost. Nevertheless, we fix the ensemble size to 25 in the following experiments.
For illustration of results with variances we use box plots where the box extends from the lower to upper quartile val-ues of the data, with a line at the median. The whiskers extend from the box to show the range of the data. The length of the whiskers extend to the most extreme data point within 1.5*(75%-25%) data range. Occasionally oc-curring single data points beyond that range are plotted as flier points past the end of the whiskers. Note however that the source of variance in the plots will differ: in synthetic data, we give the distribution over the 30 datasets, in real data, we give the distribution over the individual ensemble members.
 Synthetic Data. First, we show as a statistical assess-ment the results of the subsample-based ensemble over all the synthetic datasets of batch1. Here the box plots visual-ize the distribution of the results for the same sample size, the same base method, and the same parametrization of the base method for all datasets in the batch for the subsampling ensemble, the base method (sample size 1.0), and the feature bagging ensemble (FB). Figure 4 shows examples for a fixed k = 3 for the base methods LDOF, LOF, and LoOP. The behaviour on batch2 (not shown) follows the same general Figure 4: ROC AUC for ensembles X  X ifferent sam-ple sizes as well as feature bagging (FB) X  X nd base method (sample size=1.0), on the 30 datasets of batch1. pattern. We varied k from 2 to 10 and got similar results. The smaller sample size leads to larger improvements. Real Data. Having shown the ensemble performances over a set of 30 datasets for the synthetic data, we now analyze the behaviour on individual real datasets. Here, we show in the whisker plots the variance in the ROC AUC achieved by the individual ensemble members based on subsamples of different sample size (zero variance for sample size 1.0, which reflects the performance of the deterministic base method on the complete data), and feature bagging (FB). The ROC AUC of the ensembles (subsampling and feature bagging) are visualized by a diamond.

Figures 5, 6, and 7 show the results for the three base methods on the datasets Lymphography, WBC, and Satim-age-2, respectively. We choose the same k for all base meth-ods such that at least some of the base methods get rea-sonable results. For the larger dataset satimage-2, the k needs to be larger as well. Comparing these plots, we see a different behaviour of the base methods as some datasets are easy for some base methods while some other datasets are relatively hard. In particular, LDOF does not retrieve sensible results on all three datasets. In all cases, however, the subsampling ensemble improves. Feature bagging does Figure 5: ROC AUC for ensemble members of the subsampling ensemble for different sample sizes (boxes), the base method (sample size=1.0), and en-sembles (diamonds) X  X n top of subsamples and fea-ture bags (FB) X  X n dataset Lymphography. not perform always that convincingly, in some cases it drops to (or below) random quality. Only for LDOF and LoOP on Lymphography (Figures 5(a), 5(c)), feature bagging can recover from the weak performance of the base learner.
As a general picture from these and other results, we see that the smaller sample size actually has the larger poten-tial of improvement. Although the smaller sample keeps not as much information about the dataset (and the un-known underlying density-distribution), from the point of view of ensemble learning, these findings make sense, as the smaller samples will actually provide the most diverse en-semble members, and it also shows the practical applicabil-ity of the reasoning we provided in Section 3.2. In most cases, we find the 10%-sample to work best. However, the break-even point between too much loss of information and too high similarity of ensemble members differs from dataset to dataset. We have also examples where the 10%-sample is already too small such as in Figure 5(a). That is possibly related to the fact that the lymphography data are relatively small.

However, we fix the sample size to 0.1 for the follow-ing experiments and explore the behaviour of base method, Figure 6: ROC AUC for ensemble members of the subsampling ensemble for different sample sizes (boxes), the base method (sample size=1.0), and en-sembles (diamonds) X  X n top of subsamples and fea-ture bags (FB) X  X n dataset WBC. subsampling ensemble and feature bagging ensemble over a range of k . We see, as an example, in Figure 8, a slight but steady increase of the ROC AUC with k for the base methods and the subsampling ensemble while the feature bagging ensemble appears to be much more instable. While increasing k does not, in general, increase the quality of the results, we observe the same pattern of stability of the base method and the subsampling ensemble and higher variance of the feature bagging ensemble on other datasets as well.
For the three datasets based on segment, for k = 20 (again a selection that gives reasonable results for most of the base methods), we show results for all three base meth-ods in Figure 9. Again, the subsampling ensemble compares favourably against the base method as well as against fea-ture bagging.
Although we compared the sample-based ensemble against feature bagging [30], let us finally note that these two ap-proaches are not strictly competitors. Feature bagging is likely to be an interesting approach in the context of very Figure 7: ROC AUC for ensemble members of the subsampling ensemble for different sample sizes (boxes), the base method (sample size=1.0), and en-sembles (diamonds) X  X n top of subsamples and fea-ture bags (FB) X  X n dataset Satimage-2. high-dimensional data [45]. Sampling should be helpful when the datasets are growing too large. On the other hand, fea-ture bagging is not meaningful for low-dimensional data, as the ensemble members are bound to be too similar. And sampling on too small data is probably not too promising. However, these two problems (too small datasets with only a few dimensions) are not really problems of todays research. It might be an interesting question for future work to inves-tigate the integration of both techniques, building ensembles on subsets of features and subsets of data objects simulta-neously.
 This work has been partially supported by NSERC (Canada), FAPESP (Brazil), and CNPq (Brazil). [1] N. Abe, B. Zadrozny, and J. Langford. Outlier Figure 8: ROC AUC for base methods and corre-sponding ensembles varying k on dataset waveform.
 F igure 9: ROC AUC for all methods, k = 20 , on different datasets (variants of segment). [2] E. Achtert, S. Goldhofer, H.-P. Kriegel, E. Schubert, [3] E. Achtert, H.-P. Kriegel, E. Schubert, and A. Zimek. [4] F. Angiulli and F. Fassetti. DOLPHIN: an efficient [5] F. Angiulli and C. Pizzuti. Fast outlier detection in [6] V. Barnett and T. Lewis. Outliers in Statistical Data . [7] S. D. Bay and M. Schwabacher. Mining distance-based [8] A. Bertoni and G. Valentini. Ensembles based on [9] M. M. Breunig, H.-P. Kriegel, P. Kr  X  oger, and [10] M. M. Breunig, H.-P. Kriegel, R. Ng, and J. Sander. [11] G. Brown, J. Wyatt, R. Harris, and X. Yao. Diversity [12] T. G. Dietterich. Ensemble methods in machine [13] S. Dudoit and J. Fridlyand. Bagging to improve the [14] X. Z. Fern and C. E. Brodley. Random projection for [15] A. Frank and A. Asuncion. UCI machine learning [16] A. L. N. Fred and A. K. Jain. Robust data clustering. [17] J. Gao and P.-N. Tan. Converting output scores from [18] J. Ghosh and A. Acharya. Cluster ensembles. WIREs [19] A. S. Hadi, A. H. M. Rahmatullah Imon, and [20] S. T. Hadjitodorov, L. I. Kuncheva, and L. P. [21] L. K. Hansen and P. Salamon. Neural network [22] W. Jin, A. Tung, and J. Han. Mining top-n local [23] W. Jin, A. K. H. Tung, J. Han, and W. Wang.
 [24] F. Keller, E. M  X  uller, and K. B  X  ohm. HiCS: high [25] E. M. Knorr and R. T. Ng. A unified notion of [26] G. Kollios, D. Gunopulos, N. Koudas, and [27] H.-P. Kriegel, P. Kr  X  oger, E. Schubert, and A. Zimek. [28] H.-P. Kriegel, P. Kr  X  oger, E. Schubert, and A. Zimek. [29] H.-P. Kriegel, M. Schubert, and A. Zimek.
 [30] A. Lazarevic and V. Kumar. Feature bagging for [31] H. V. Nguyen, H. H. Ang, and V. Gopalkrishnan. [32] G. H. Orair, C. Teixeira, Y. Wang, W. Meira Jr., and [33] S. Papadimitriou, H. Kitagawa, P. Gibbons, and [34] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient [35] P. J. Rousseeuw and M. Hubert. Robust statistics for [36] E. Schubert, R. Wojdanowski, A. Zimek, and H.-P. [37] E. Schubert, A. Zimek, and H.-P. Kriegel. Local outlier [38] T. Soler and M. Chin. On transformation of [39] A. Strehl and J. Ghosh. Cluster ensembles  X  a [40] A. Topchy, A. Jain, and W. Punch. Clustering [41] G. Valentini and F. Masulli. Ensembles of learning [42] N. H. Vu and V. Gopalkrishnan. Efficient pruning [43] J. Yang, N. Zhong, Y. Yao, and J. Wang. Local [44] K. Zhang, M. Hutter, and H. Jin. A new local [45] A. Zimek, E. Schubert, and H.-P. Kriegel. A survey on
