 With the rapid development of web social network applications such as Facebook, Twit-Query Language (SPARQL) [4] can express BGP queries for RDF, which can be directly applied to the OSN subgraph query. In general, the nature of RDF model makes it suita-ble for large-scale complex OSN management. 
Figure 1 illustrates an example for a fraction of OSN graph representing relations expressed in SPARQL as: Select ?user1, ?company, ?user2 Where { } match all the triple patterns. This is also a typically SPARQL BGP query. In Figure 1 we show the target subgraph by dotted line. We observe that BGPs can easily express the query for OSN chains of relationships. 
A number of RDF data management systems have been developed in the past few scale data. In [8], RDF datasets are stored in Hadoop file system (HDFS), but it has to of its lack of index. H2RDF [9] utilizes the MapReduce [10] framework to process the leading to low efficiency. query efficiency. 
In this paper, we put forward a hybrid two-layer RDF management framework for work are summarized as follows: 1. We present a three-table indexing schema for storing RDF data implemented 2. We propose a pipelined in-memory approach to process SPARQL BGP itera-3. We conduct comprehensive experiments on benchmark datasets to demonstrate The rest of the paper is organized as follows. Section 2 reviews the related work. In Section 3, we present our framework architecture which is made up of storage schema and query strategy. Section 4 reports the experimental results. Finally we conclude the paper and give directions for the future studies in Section 5. Based on the implementation infrastructure and query mechanism, existing RDF data based on the standalone mode, and the other is distributed systems. Centralized Systems. Jena [15] is a semantic web framework that first developed by IBM. It utilizes a relational model to manage data, in other words, all data are stored very poor query performance for large-scale datasets. But it provides users abundant APIs to parse the SPARQL query and inference. are spo, sop, pso, pos, osp and ops. But in our work, we show that only three indices are needed to answer all the triple patterns. 
RDF-3X [5] is considered the state-of-the-art system in centralized RDF data form joins, presenting problems with large input. which limiting their storage and processing capacity. Distributed Systems. Jaeseok et al. [9] introduces us an iterative MapReduce method to process SPARQL BGP query. The RDF triples are stored in HDFS in an N-Triples index can be considered. queried by MapReduce framework. In storage layer, RDF data are indexed into three tables. The SPARQL queries are executed in centralized mode or in distributed mode lead to low query efficiency. 
Trinity [19] is a distributed, in-memory system. It uses graph exploration instead of join operations and greatly boosts SPARQL query performance. The main drawback of Trinity is that its performance is limited by the main memory of the cluster, since memory, as distributed shared memory increases the complexity for maintenance. 
For OSN queries, they focus on efficient gr aph traversal, which need special efforts on querying processing. All existing works lack the guarantee for the performance of such queries. data into main memory. As the basic primitives of a query are selection and join, con-sidering large-scale OSN data, such implementation brings two challenges. First, how buted massive data store. Second, how to implement distributed in-memory join. To solve these challenges, we propose a two-layer RDF management architecture. Figure 2 presents its overview. We utilize HBase as the distributed indexing and sto-creates the query algebra. Then our join planner generates the join order based on the algebra and triple patterns. We pipeline the iterative joins in Spark RDD (main mem-ory layer) and output the final result. distributed RDF management is focus on the strategies used in selection and join pri-execution. 3.1 Index Schema An RDF triple consists of a subject, predi cate and object. We materialize three permu-which are named based on the components order of the triples stored. These three of efficiently by only a table range scan. Table 1 shows how all eight triple patterns can map to a table scan of the three tables. Triple Pattern No.1 and No.5 are very unusual most frequently used triple patterns are No .2, No.4 and No.8, which are bounded with predicate, leaving the subject or the object unbounded. 
To be able to provide efficient indices access in a distributed environment we store the three tables using HBase and thus achieve the desired index scan and search capa-bilities. HBase is an open-source implementation of Google Bigtable [20]. A data row are further grouped into column families. A B+ tree-like index on row key is provided RDF storage schema. 
We propose a new approach of storing RDF triples in HBase. In these three tables, data are stored in row keys and column names. Each table has only one column fami-which called column qualifier in HBase. Table POS and OSP are in the similar form. 
We show through an example how RDF triples are stored in our approach. Table 2 shows several example triples taken from Figure 1 in Introduction section. form. 
The advantages of our indexing schema can be outlined as follows: 1. The multi-valued properties are well handled. There are a number of multi-2. As many as possible fields are put into row key which can be indexed. We put create HFiles (the HBase file format) directly and then loaded them into HBase tables instead of calling HBase APIs for each triple insertion. The import procedure consists of two MapReduce jobs for each of the three tables. The first job parses the original HDFS to HBase table through LoadIncrementalHFiles [11]. 3.2 Join Execution triple patterns. four triple patterns which have two shared variables user1 and user2. User1 and user2 ARQ [15], including checking the query syntax , replacing the prefix in the triples and then creating the query algebra. As RDF has a fixed simple data model, it is not un-join key variables which decides the join or der. We select a join key according to the quent join iterations. Algorithm 1. Matching a triple pattern over a table Input: a triple pattern Output: matched table scan 1: // check for SPO table 2: if tp.sp is bounded  X  tp.op is a variable then 3: tableName = SPO 4: if tp.pp is bounded then 5: rowKey = tp.sp + tp.pp 6: type = 1 7: else rowKey = tp.sp 8: end if 9: end if 10: // check for POS and OSP table (we omit the detail) 11: // set the range of scan 12: scan.setStartRow(rowKey) 13: if type == 1 then 14: scan.setStopRow(rowKey) 15: else scan.setStopRow(rowKey + ~) 16: end if 17: return scan the index schema, we can get all the scans matched the related triples. The matching pattern is No.7 or No.8 in Table 1 for SP O table. Due to the space limit, we omit the detail in line 10 as it does the similar check for POS and OSP table. In lines 11-16 we set the range of the scan. 
According to the join order created by heuristics, we pipeline all the iterative joins choose to implement the iterative join process on top of Spark [12]. Spark is a large-viding a fault-tolerant implementation of distributed datasets called RDD. Especially for iterative processing, the opportunity to store the data in main memory can signifi-cantly speed up processing. We pick Spark as the underlying framework, because of its nature of in-memory computing. 
Algorithm 2 is the pseudo-code of the pipelined in-memory iterative join process. The diate results in RDD in main memory, in order to avoid disk I/O in line 8. Hadoop reads HBase query results as input. Scans are first encoded to bytes, and then transformed into RDD as TableInputFormat. In this way data are transformed locally from HBase to RDD, reducing unnecessary network I/O. Algorithm 2. Iterative joins in RDD Input: list of execution sequence of join keys L Output: final query result 1: initialize midRDD as the first scan 2: for joinKey in L do 3: get all triple pattern numbers related to joinKey in nums 4: for num in nums do 5: get the scan according to triple[num] //Algorithm 1 6: transform scan to newRDD 7: resRDD = newRDD.join(midRDD) 8: midRDD = resRDD.cache 9: end for 10: end for 11: resRDD = midRDD 12: return resRDD 
In the whole iterative join process, the data structure in RDD keeps in binary tuple format, which contains a key part and a value part. With the SPARQL query example #2 as follows: Original data format for triple pattern #1: Original data format for triple pattern #2: key-value formats as the input of joins are as follows: Input format for triple pattern #1: Input format for triple pattern #2: For convenience we separate the key part and value part with delimiter  X - X , it actually formation and output in the same format. After join triple pattern #1 and #2 with input above, the intermediate result are as follows: Output format: will be found in the value part of the RDD. 
If the query does not have any shared variables, means that there is no need to join, there will not be any Spark jobs. The result of the triple patterns get from the HBase tables will be directly outputted as the final result. present our evaluation results. 4.1 Experiments Set-Up Cluster Configuration: Our experimental environment consists of a variable number of worker nodes and a single node as the Hadoop, HBase and Spark master. The mas-ter node and the worker nodes have the same configuration, which contains two AMD Opteron X  4180 6-core CPUs at 2.6GHz, 48GB of RAM and 9TB disk. We utilize Hadoop v1.0.4, HBase v0.94.20 and Spark v1.0.0 respectively with default settings. Baseline Frameworks: We compare the performance of our framework against Itera-tive Mapreduce (IterMR for short) [9] and H2RDF [18]. We reimplement the method in IterMR and utilize the source code open-sourced by the authors of H2RDF. Lehigh University Benchmark (LUBM) [21] synthetic dataset. The LUBM dataset triples by varying the number of university entities. We create 5 datasets with 1k, 5k, 10k, 20k, 30k universities respectively, with random seed 0. The statistics of the data-sets are in Table 3. In this evaluation, LUBM (n) indicates that the dataset contains n universities. 
The raw data format from LUBM generator is OWL/XML, we first use Jena rdfcat some of which require inference to answer. We remove the hierarchy of the predicate by querying for explicit predicate with no subclasses or subproperties. 4.2 Experiments Results We first recorded the data importing time with the increase of the size of dataset. The time in data importing phase, mainly because it does not create any index on the data-set. For our framework we not only import the datasets to HDFS, but also bulk load them into three HBase tables to create index on row keys. H2RDF additionally brings in data compression and statistics computing in this phase, which we will consider in our future work to refine the join order. 
We divide the 14 LUBM standard queries into two categories: high-selective que-ries and low-selective queries. High-selective query means the query has small input Query 1 and 3 represent the high-selective queries while queries 2, 6 and 9 stand for low-selective queries. These queries provide good mixture of high-selective and low-dataset and the queries execution time are showed in Figure 5. Each query experiment we have run 5 times and take the average execution time. 
We see that IterMR performed all five queries over 1000 seconds mainly due to its queries (Q2, Q9). The main reason is that H2RDF executes high-selective queries in model. SparkRDF performs nearly one order of magnitude better than H2RDF in Q2 thod. Q6 contains no join process, and SparkRDF performs a little better than H2RDF the comparative experiment, especially in low-selective and complex queries. We will introduce the method of cost model in our future work. selective queries because it is one of the most complicated queries tested, requiring a bounded object. The scalability evaluation results are presented in Figure 6. up by adding more nodes in the front, and then the effect gradually abates due to the fact that the query cannot fully utilize the cluster resources. In the meanwhile the Q1 Q1 and Q2 is that the input and result size of low-selective queries (Q2) depends on result size of high-selective queries (Q1) is almost bounded. From the above, we can see that the query processing of our framework is highly scalable, especially for com-plicated low-selective queries. In this paper, we present a two-layer RDF management framework capable of storing SPARQL BGP queries. A heuristic approach is applied to decide the join order in join method implementing on Spark. In this way we directly pipeline the intermediate join SparkRDF is preferable and more efficient if the query includes low-selective joins. 
In the future, we would like to extend the framework in several directions. First, we will store the statistics about the dataset in data loading phase. Dynamic programming mode or standalone mode, since the high-selective queries works better in standalone mode. Finally we will handle OWL reasoning and more complex SPARQL patterns. 
