 Ruslan Salakhutdino v RSALAKHU @ CS . TORONTO . EDU Department of Computer Science, Uni versity of Toronto 6 King X  s Colle ge Rd, M5S 3G4, Canada Gatsby Computational Neuroscience Unit, Uni versity Colle ge London 17 Queen Square, London WC1N 3AR, UK The problem of Maximum Lik elihood (ML) parameter es-timation for latent variable models is an important prob-lem in the area of machine learning and pattern recogni-tion. ML learning with unobserv ed quantities arises in man y probabilistic models such as density estimation, di-mensionality reduction, or classification, and generally re-duces to a relati vely hard optimization problem in terms of the model parameters after the hidden quantities have been inte grated out.
 A common technique for ML estimation of model param-eters in the presence of latent variables is Expectation-Maximization (EM) algorithm [3]. The EM algorithm al-ternates between estimating the unobserv ed variables given the current model and refitting the model given the esti-mated, complete data. As such it tak es discrete steps in parameter space similar to first order method operating on the gradient of a locally reshaped lik elihood function. In spite of tremendous success of the EM algorithm in prac-tice, due to its simplicity and fast initial progress, some au-thors [11 ] have argued that the speed of EM con vergence can be extremely slo w, and that more complicated second-order methods should generally be favored to EM. Man y methods have been proposed to enhance the con vergence speed of the EM algorithm, mostly based on con ventional optimization theory [7, 8]. Several authors [11 , 1] have also proposed hybrid approaches for ML learning, adv ocat-ing switching to an approximate Ne wton method after per -forming several EM iterations. All of these approaches, al-though sometimes successful in terms of con vergence, are much more comple x than EM, and dif ficult to analyze; thus the y have not been popular in practice.
 Our goal in this paper is to contrast the EM algorithm with a direct gradient-based optimization approach. As a concrete alternati ve, we present an Expectation-Conjugate-Grad ient (ECG) algorithm for maximum lik elihood estimation in la-tent variable models, and sho w that it can outperform EM in terms of con vergence in certain cases. Ho we ver, in other cases the performance of EM is superior . To understand these beha viors, we study the con vergence properties of the EM algorithm and identify analytic conditions under which EM algorithm exhibits Ne wton-lik e con vergence beha vior , and conditions under which it possesses extremely poor , first-order con vergence. Based on this analysis, we in-troduce a simple hybrid EM-ECG algorithm that switches between EM and ECG based on estimated quantities sug-gested by our analysis. We report empirical results on syn-thetic as well as real-w orld data sets, sho wing that, as pre-dicted by theory , this simple algorithm almost never per -forms worse than standard EM and can substantially out-perform EM X  s con vergence. We first focus on the analysis of the con vergence properties of the Expectation-Maximization (EM) algorithm. Con-sider a probabilistic model of observ ed data x which uses latent variables z . The log-lik elihood (objecti ve function) can be lower bounded by the dif ference between expected complete log-lik elihood and negati ve entrop y terms: L () = ln p ( x j ) = R ln p ( x ; z j ) d z
R p ( z j x ; ) ln p ( x ; z j )
R p ( z j x ; ) ln p ( x ; z j ) d z R p ( z j x ; ) ln p (
The EM algorithm is nothing more than coordinate ascent in the functional G ( ; ) , alternating between maximizing F with respect to for fix ed (E-step) and with respect to for fix ed (M-step) [10 ].
 The algorithm implicitly defines a mapping: M : ! 0 from parameter space to itself, such that t +1 = M ( t ) . If iterates of t con verge to (and M () is continuous), then = M ( ) , and in the neighborhood of , by a Taylor series expansion: where M 0 ( ) = @M a linear iteration algorithm with a con vergence rate matrix M 0 ( ) (which is typically nonzero) [9].
 For most objecti ve functions, the EM step ( t +1) ( t ) in parameter space and true gradient can be related by a trans-formation matrix P ( t ) , that changes at each iteration: (W e define r conditions, the transformation matrix P ( t ) is guaranteed to be positi ve definite with respect to the gradient. 1 In particular , if The second condition C2 may seem very strong. Ho we ver, for the EM algorithm C2 is satisfied whene ver the M-step has a single unique solution. 2 The important consequence of the abo ve analysis is that (when the expected complete log-lik elihood function has a unique optimum), EM has the appealing quality of al-ways taking a step ( t +1) t having positi ve projection onto the true gradient of the objecti ve function L ( t ) mak es EM similar to a first order method operating on the gradient of a locally reshaped lik elihood function. For maximum lik elihood learning of a mixture of Gaus-sians model using the EM-algorithm, this positi ve definite transformation matrix P ( t ) was first described by Xu and Jordan[18 ]. We extended their results by deri ving the ex-plicit form for the transformation matrix for several other latent variables models such as Factor Analysis (FA), Prob-abilistic Principal Component Analysis (PPCA), mixture of PPCAs, mixture of FAs, and Hidden Mark ov Models [13 ]. 3 We can further study the structure of the transformation matrix P ( t ) and relate it to the con vergence rate matrix M 0 . Taking the negati ve deri vatives of both sides of (2) with respect to t , we have:
I M 0 ( t ) = P 0 ( t ) r L ( t ) P ( t ) S ( t ) (4) objecti ve function, M 0 output deri vative matrix for the EM mapping and P ( t ) = @P () @ j = t is the tensor deri vative of P ( t ) with respect to t . In  X  X lat X  regions of L () , where r infinite), the first term on the RHS of equation (4) becomes much smaller than the second term, and P ( t ) matrix becomes a rescaled version of the negati ve inverse Hessian: In particular , if the EM algorithm iterates con verge to a local optima at , then near this point (i.e. for suf ficiently lar ge t ) EM may exhibit superlinear con vergence beha vior . This is also true in  X  X lateau X  regions where the gradient is very small even if the y are not near a local optimum. The nature of the con vergence beha vior is controlled by the eigen values of the matrix M 0 ( t ) . If all eigen values tend to zero, then EM becomes a true Ne wton method, 4 rescaling gradient by exactly the negati ve inverse Hessian. As the eigen values tend to unity , EM tak es smaller and smaller stepsizes, giving poor , first-order , con vergence. Dempster , Laird, and Rubin [3] sho wed that if EM iterates con verge to , then which can be interpreted as the ratio of missing informa-tion to the complete information near the local optimum. Thus, in the neighborhood of a solution (for suf ficiently lar ge t ), we can mak e the follo wing novel link between the transformation matrix P , the con vergence rate matrix M 0 , and the exact inverse Hessian S : This analysis of EM has a very interesting interpretation which is applicable to any latent variable model: When the missing information is small compared to the com-plete information, EM exhibits approximate Ne wton be-havior and enjo ys fast, typically superlinear con vergence in the neighborhood of . If fraction of missing informa-tion approaches unity , the eigen values of the first term (7) approach zero and EM will exhibit extremely slo w con ver-gence.
 Figure 1 illustrates the abo ve results in the simple case of fitting a mixture of Gaussians model to well-clustered data  X  for which EM exhibits Ne wton-lik e con vergence  X  and not-well-clustered data, for which EM is slo w. As we will see from the empirical results of the later sections, man y other models also sho w this same effect. For example, when Hidden Mark ov Models or Aggre gate Mark ov Mod-els [14 ] are trained on very structured sequences, EM ex-hibits superlinear con vergence, in particular when the state transition matrix is sparse and the output distrib utions are almost deterministic at each state.
 The abo ve analysis moti vates the use of alternati ve opti-mization techniques in the regime where missing informa-tion is high and EM is lik ely to perform poorly . In the fol-lowing section, we analyze exactly such an alternati ve, the Expectation-Conjugate Gradient (ECG) algorithm, a sim-ple direct optimization method for learning the parameters of latent variables models. The key idea of the ECG algorithm is to note that if we can easily compute the deri vative @ the complete log lik elihood, then kno wing the posterior p ( z j x ; ) we can compute the exact gradient r L () : This exact gradient can then be utilized in any standard manner , for example to do gradient (as)descent or to control a line search technique. (Note that if one can deri ve exact EM for a latent variable model, then one can always deri ve ECG by computing the abo ve inte gral over hidden variables.) As an example, we describe a conjugate gradient algorithm: When certain parameters must obe y positi vity or linear constraints, we can either modify our optimizer to respect the constraints, or we can reparameterize to allo w uncon-strained optimization. In our experiments, we use simple reparameterizations of model parameters that allo w our op-timizers to work with arbitrary values. (Of course, an ex-tra term must be included in the gradient calculation due to the application of the chain rule through such reparam-eterizations.) For example, in the MoG model we use a  X  X oftmax X  parameterization of the mixing coef ficients ric positi ve definite, we use the Choleski decomposition (or log variances for diagonal covariance matrices). In HMMs, we reparameterize probabilities also via softmax functions. Note that ECG is dif ferent than Jamshidian and Jennrich X  s proposed acceleration of EM[6 ]. Their method always mo ves in the same direction as EM but uses a generalized conjugate gradient algorithm to find the best step size; ECG computes the true gradient and mo ves in a search direc-tion determined by the conjugate gradient algorithm, which may be dif ferent than the EM step direction.
 Of course, the choice of initial conditions is very important for the EM algorithm or for ECG. Since EM is based on optimizing a con vex lower bound on the lik elihood, once EM is trapped in a poor basin of attraction, it can never find a better local optimum. Algorithms such as split and mer ge EM [16 ] were developed to escape from such con-figurations. It turns out that direct optimization methods such as ECG may also avoid this problem because of the nonlocal nature of the line search. In man y of our exper -iments, ECG actually con verges to a better local optimum than EM; figure 2 illustrates exactly such case. As we have seen, the relati ve performance of EM versus direct optimization depends on the missing information ra-tio for the given model and data set. The key to practical speedups is the ability to design a hybrid algorithm that can estimate the local missing information ratio M 0 ( t ) to de-tect whether to use EM or a direct approach such as ECG. Some authors have attack ed this problem by finding the top con ventional numerical methods, such as finite-dif ference approximations, or power methods [4]. These approaches are computationally intensi ve and dif ficult to implement, and thus the y have not been popular in practice. Here, we propose using the entr opy of the posterior over hidden variables , which can be computed after performing an E-step, as a crude estimate of the local missing infor -mation ratio. This entrop y has a natural interpretation as the uncertainty about missing information, and thus can serv e as a guide between switching regimes of EM and ECG. For man y models with discrete hidden variables this quantity is quite easy to compute. In particular , we define the Normalized Entrop y term: H with z being discrete hidden variable taking on M values, and N observ ed data vectors x n . We now simply switch between EM and ECG based on thresholding this quantity: As we will see from the experimental results, this simple hybrid EM-ECG algorithm never performs substantially worse, and often far better than either EM or ECG. We now present empirical results comparing the perfor -mance of EM, ECG, and hybrid EM-ECG for learning the parameters of three latent variable models: mixture of Gaussians, Hidden Mark ov Models, and Aggre gate Mark ov Models. In man y latent variable models, per -forming inference (E-step) is significantly more expensi ve compared to either the parameter updates (M-step) or extra overhead beyond the E-steps in the CG step of ECG. To compare the performance of the algorithms, we therefore simply compare the number of E-steps each algorithm executes until its con vergence. We first sho w results on synthetic data sets, whose properties we can control to verify certain aspects of our theoretical analysis. We also report empirical results on several real world data sets, sho wing that our algorithms do work well in practice. Though we sho w examples of single runs, we have confirmed that the qualitati ve beha vior of the con vergence results presented in all experiments is the same for dif ferent initial parameter conditions. For all of the reported experiments, we used tol = 10 8 , = 0 : 5 . We have not investigated carefully methods for setting the parameter , which controls the propensity of the algorithm to switch to ECG in favor of EM. 5.1. Synthetic Data Sets First, consider a mixture of Gaussians (MoG) model. We considered two types of data sets, one in which the data is  X  X ell-separated X  into five distinct clusters, with each clus-ter containing 400 points, and another  X  X ot-well-separated X  case in which the five mixture components with each con-taining 400 points, overlap in one contiguous region. Fig-ure 3 sho ws that ECG and Hybrid EM-ECG outper -form standard EM in the poorly separated cases. For the well-separated case, the hybrid EM-ECG algorithm never switches to ECG due to the small normalized entrop y term, and EM con verges very quickly . This is predicted by our analysis: in the vicinity of the local optima the direc-tions of the vectors P () r come identical (fig. 1), suggesting that EM will have ap-proximately Ne wton con vergence beha vior .
 We then applied our algorithms to the training of Hid-den Mark ov Models (HMMs). Missing information in this model is high when the observ ed data do not well deter -mine the underlying state sequence (gi ven the parameters). We therefore generated two data sets from a 5-state HMM, with an alphabet size of 5 characters. The first data set ( X  X liased X  sequences) was generated from a HMM where output parameters were set to uniform values plus some small noise N (0 ; 01 I ) . The second data set ( X  X  ery structured sequences X ) was generated from a HMM with sparse transition and output matrices. For the ambiguous or aliased data, ECG and hybrid EM-ECG outperform EM substantially . For the very structured data, EM performs well and exhibits second order con vergence in the vicinity of the local optimum.
 Finally , we experimented with Aggre gate Mark ov Models (AMMs) [14 ]. AMMs model defines a discrete conditional probability table p rank approximation. In the conte xt of n-gram models for word sequences, AMMs are class-based bigram models in which the mapping from words to classes is probabilistic. In particular , the class-based bigram model predicts that word w total number of classes. Here, the concept of missing information corresponds to how well or poor a set of words determine the class labels C based on the observ ation words that follo w them. The right panels of figure 3 sho w training of a 2-class 50-state AMM model on ambiguous (aliased) data, in which words do not well determine class labels, and on more structured data, in which the proportion of missing information is very small. ECG and hybrid EM-ECG are superior to EM by at least a factor of two for ambiguous data; for structured data EM sho ws the expected Ne wton-lik e con vergence beha vior . 5.2. Real World Data Sets In our first experiment, we cluster a set of 50,000 8 8 grayscale pix el image patches 5 using a mixture of Gaus-sians model. The patches were extracted from 768 512 natural images, described in [17 ] (see fig 5 for an example of a natural image, and sample patches). To speed-up the experiments, the patch data was projected with PCA down to a 10-dimensional linear subspace and the mixing pro-portions and covariances of the model were held fix ed. The means were initialized by performing K-means. We exper -imented with mixtures having M=2 up to M=65 clusters. Figure 4 displays the con vergence of EM, ECG, and Hy-brid EM-EC algorithms for M=5, M=50 and M=65. The experimental results sho w that with fewer mixture compo-nents EM outperforms ECG, since the components gen-erally model the data with fairly distinct, non-contiguous clusters. As the number of mixtures components increases, clusters overlap in contiguous regions and the normalized entrop y term gro ws, suggesting a relati vely high proportion of the missing information. In this case ECG outperforms EM by several orders of magnitude. Hybrid EM-ECG al-gorithm is never inferior to either EM or ECG (using our untuned setting of switching threshold = 0 : 5 ). Our second experiment consisted of training a fully con-nected HMM to model DN A sequences. For the training, we used publicly available  X  X ENIE gene finding data set X , pro vided by UCSC and LBNL [5], that contains 793 un-related human genomic DN A sequences. We applied our dif ferent algorithms on (the first) 66 DN A sequences from multiple exon and single exon genes, with length varying anywhere between 200 to 3000 nucleotides per sequence. The number of states ranged from M=5 to M=10 and all the parameter values were randomly initialized. Figure 4 sho ws the con vergence of EM, ECG, and Hybrid EM-ECG algorithms for M=5,8,10. This data set contains very com-ple x structure which is not easily modeled by HMMs, re-sulting in a very high proportion of missing information. As a result, hybrid EM-ECG and ECG substantially out-perform EM in terms of con vergence.
 In our last experiment, we applied Aggre gate Mark ov Models to the data set consisting of 2,037 NIPS authors and corresponding counts of the top 1,000 most frequently used words of the NIPS conference proceedings, volumes 1 to 12. 6 The goal was to model the probability that an author A will use word W using a small number of  X  X oft X  classes ( t ): P ( W j A ) = P T again, we observ e that for this simple model, this data set has a lar ge fraction of missing information. Figure 4 displays the con vergence of EM, ECG, and EM-ECG algorithms for T=3,6,9. with hybrid EM-ECG and ECG having superior con vergence over EM. Although we have focused here on discrete latent variables, the ECG and hybrid algorithms can also be deri ved for la-tent variable models with continuous hidden variables. As an example figure 6 illustrates con vergence beha vior of the Probabilistic Principal Component Analysis (PPCA) latent variable model[12 , 15], which has continuous rather than discrete hidden variables. Here the concept of missing in-formation is related to the ratios of the leading eigen values of the sample covariance, which corresponds to the ellip-ticity of the distrib ution. For  X  X o w-rank X  data with a lar ge ratio EM performs well; for nearly circular data ECG con-verges faster . 7 In some degenerate cases, where the proportion of missing information is very high, i.e. M 0 ( ) approaches identity , EM con vergence can be exponentially slo w. Figure 6 (right panel) illustrates such example for the case of HMM train-ing using almost random sequences. It tak es about 7,000 iterations for ECG and EM-ECG to con verge to the ML estimate, whereas even after 250,000 iterations EM is still only approaching the local optimum.
 In this paper we have presented comparati ve analysis of EM and direct optimization algorithms for latent variable models, and developed a theoretical connection between these two approaches. We have also analyzed and deter -mined conditions under which EM algorithm can demon-strate local-gradient and approximate Ne wton con vergence beha viors. Our results extend those of Xu and Jordan[18 ] who analyzed the con vergence properties of the EM algo-rithm in the special case of Gaussian mixtures, and apply to any exponential family model.
 Moti vated by these analyses, we have proposed an alterna-tive hybrid optimization method that can significantly out-perform EM in man y cases. We tested the proposed algo-rithms by training several basic latent variable models on several synthetic as well as real world data sets, reporting con vergence beha vior and explaining the results with refer -ence to our analysis.
 Our con vergence analysis can also be extended to a broader class of bound optimization techniques, such as iterati ve scaling (IS) algorithms for parameter estimation in max-imum entrop y models[2 ] and the recent CCCP algorithm for minimizing the Bethe free ener gy in approximate inference problems[19 ]. These analyses allo w us to gain a deeper understanding of the nature of these algorithms and the conditions under which certain optimization techniques can be expected to outperform others. Based on these extended analyses we are designing accelerated fitting algorithms for these models as well.
 Ackno wledgments We would lik e to thank Yoshua Bengio and Dre w Bagnell for man y useful comments and Carl Rasmussen for pro viding an initial version of conjugate gradient code.
