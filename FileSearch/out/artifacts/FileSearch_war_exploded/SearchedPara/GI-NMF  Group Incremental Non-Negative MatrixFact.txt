 Non-negative matrix factorization (NMF) is a well known method for obtaining low rank approximations of data sets, which can then be used for efficient indexing, classification, and retrieval. The non-negativity constraints enable probabilistic interpretation of the re-sults and discovery of generative models. One key disadvantage of the NMF, however, is that it is costly to obtain and this makes it dif-ficult to apply NMF in applications where data is dynamic. In this paper, we recognize that many applications involve redundancies and we argue that these redundancies can and should be leveraged for reducing the computational cost of the NMF process: Firstly, online applications involving data streams often include temporal redundancies. Secondly, and perhaps less obviously, many appli-cations include integration of multiple data streams (with potential overlaps) and/or involves tracking of multiple similar (but differ-ent) queries; this leads to significant data and query redundancies, which if leveraged properly can help alleviate computational cost of NMF. Based on these observations, we introduce Group Incremen-tal Non-Negative Matrix Factorization (GI-NMF) which leverages redundancies across multiple NMF tasks over data streams. The proposed algorithm relies on a novel group multiplicative update rules (G-MUR) method to significantly reduce the cost of NMF. G-MUR is further complemented to support incremental update of the factors where data evolves continuously. Experiments show that GI-NMF significantly reduces the processing time, with minimal error overhead.
In many applications, data have a multitude of features that can be used for indexing and retrieval. In practice, however, using more features (or having more feature dimensions to represent the data) is not always an effective way for managing data. This is com-monly known as the dimensionality curse and has been shown to negatively affect many key data management tasks, from similarity searches to the analysis of data for patterns [1, 4].
T his work is partially funded by NSF grants #1339835 and #1318788. This work is also supported in part by the NSF I/UCRC Center for Embedded Systems established through the NSF grant #0856090.

Feature selection and dimensionality reduction techniques [4, 35] usually involve some (often linear) transformation of the vec-tor space containing the data to help focus on a few features (or combinations of features) that best discriminate the data in a given corpus. These include the Principle Component Analysis (PCA) [2], Karhunen-Loeve Transform, KLT [16], and singular value decomposition, SVD [8]), and non-negative matrix factor-ization (NMF) [4].

Consider for example a feature-object matrix X representing the content of a set of data objects; X ij denotes the occurrence of fea-ture j in object i . SVD decomposes X into U , S , and V matrices (such that X = USV T ), a diagonal core matrix ( S ) describes the importance of the r features and, thus, less important features can be identified and dropped to obtain a low rank approximation. The SVD low rank approximation is unique and optimal. However, a major drawback of SVD is that it might produce negative compo-nents which are not ideal for interpreting the data representations. Furthermore, the factor matrices, U and V , are usually dense (even the input data is sparse); as a result the singular value decompo-sitions of large data matrices are often impossible due to memory limitations. While the Karhunen-Loeve Transform (KLT) works somewhat differently (and is optimal in preserving the variance  X  i.e., minimizes the loss in the discrimination power in the data), it also returns two factor matrices and a core matrix and largely shares (advantages and) disadvantages of SVD.
NMF overcomes some of these shortcomings: factor matrices are non-negative and these non-negative factor matrices are usu-ally sparse. The non-negativity constraints enable probabilistic in-terpretation of the results and discovery of generative models: in-tuitively, NMF provides basis vectors and participant weights for each data dimension. More formally, given an n  X  m data ma-trix X , NMF seeks factor matrices, ( n  X  r matrix) W and ( r  X  m matrix) H , such that
For a given value of r  X  n, m , the two matrices, W and H , de-fine a low rank r approximation of the original matrix X , such that each entry, x i,j in the original matrix, X , is an additive linear com-bination of the feature strengths ( W i, 1 , . . . , W i,r matrix, W , weighted by the contributions ( H 1 ,j , . . . , H features described by the corresponding column vector in the sec-ond factor matrix, H . While r is often assumed to be given, there are non-parametric methods, where r is determined from data [29]. Figure 1: Overview of the continuous NMF query model (with  X  s harable data sources and  X  NMF queries  X  each query may also have its private (unshared) data source).
In this paper, we focus on applications where multiple NMF queries (e.g. triggers) are continuously executed on time-evolving data streams, such as those from sensors (for example providing continuous readings in different zones of a smart building to sup-port various optimization and prediction tasks) or from social me-dia (for example to support personalized recommendations or vari-ous collaborative filtering tasks) [14, 32, 25, 22, 23, 17]. Figure 1 shows an overview of the problem setting, where we have We describe which data source is used for which query (or con-versely, which query access which data sources) with a  X   X   X  data source-query relationship matrix, M . We denote
At time stamp time i , for each query, q j  X  X  , we have a feature-object data matrix, D i,j , extracted from the data sources describing the objects that query q j received from all relevant sharable data sources, plus any private data source.

D EFINITION 1 (G ROUP NMF (G-NMF)). For a given tar-get rank, r , the group NMF (G-NMF) problem seeks to identify the set L i = { L i, 1 , . . . , L i, X  } , of low rank ( r ) NMF approximations of each D i,j . More specifically, each L i,j is a pair h W where W i,j and H i,j are the rank-r NMF of the matrix D Sharable data sources make new data available to these continu-ous NMF queries at their own updating rates. Each NMF query X  X  private data source also streams in new data. For NMF query, q , at time stamp, time i +1 , the feature-object matrix D i +1 ,j
D the new data from sharable data sources, D ( j ) , which cover query q and  X  D Unshared i +1 ,j are the new data from q j  X  X  private data source. For a given target rank, r , and the set, L i , of rank-r NMFs of the matrices D i,j at time time i , the group incre-mental NMF (GI-NMF) problem seeks to identify the set L tions of each D i +1 ,j at time time i +1 .  X 
As an example, consider a media service like Twitter where users subscribe to popular data provider accounts and these sources are shared by many users. In this context, GI-NMF queries would help the service provider to efficiently keep track of the index terms and content clusters and provide accurate and timely recommendations. As a second example, consider a building energy management sys-tem (BEMS), which divides a large building into zones, such that (a) each zone has sensors collecting locally relevant data, but (b) building zones are also organized such that there are sensory infor-mation shared by multiple zones (such as outside air temperature, number of individuals in a floor or office space, the amount of cool-ing air pushed by a central AC unit at a given point in time). GI-NMF queries could help the BEMS analyze the sensory data from different sensors for real-time patterns (such as faults and optimiza-tion opportunities).
In this paper, we note that the above problems involve many re-dundancies that can be leveraged for reducing the computational cost of the NMF process needed to support better information rout-ing, recommendations, and search. Firstly, data streams may in-clude temporal redundancies that can be leveraged for reducing the cost for characterization of each query. Secondly, and perhaps less obviously, many applications include integration of multiple data streams (with potential overlaps) and/or involves tracking of mul-tiple similar (but different) queries; this leads to significant data and query redundancies, which if leveraged properly can help al-leviate computational cost of NMF. Relying on these, we develop algorithms to tackle Group Non-Negative Matrix Factorization (G-NMF) and Group Incremental Non-Negative Matrix Factorization (GI-NMF). More specifically,
Both G-NMF and GI-NMF algorithms rely on novel update rules which simultaneously leverage (a) the incremental nature of the multiplicative update rule (MUR) based solutions to the NMF prob-lems and (b) the special group structure of the error terms min-imized during the MUR process. Experiment results reported in Section 4 under different parameter settings confirm that G-NMF and GI-NMF are efficient and accurate in maintaining decomposi-tions. We conclude the paper in Section 5.
NMF provides various advantages over other decomposition techniques. First of all, the non-negativity constraints enable prob-abilistic interpretation of the results: NMF gives basis vectors and participant weights for each dimension of the data. Moreover, the factors of NMF are usually sparse and, thus, it would be much cheaper to store NMF factor compared to SVD X  X  dense factors. Consequently, NMF is widely used in diverse domains including social media analysis, text mining, and recommendations.
These advantages of NMF, however, come with two key draw-b acks: First of all, unlike SVD which has eigenvector based al-gorithm which returns an exact solution, NMF problems are often solved iteratively using constraint minimizers, such as Alternating Least Square (ALS) method [7] (which iteratively fixes one factor and minimizes the error with respect to the other), the Multiplica-tive Update Rule (MUR) method [15] (which directly updates the two factors until a stopping criteria is met), and the Projected Gra-dient Descent (PGD) method [18]. These often only achieve lo-cally optimal results [34]. Consequently, unlike SVD (where there is a unique solution), NMF usually may not have a unique solu-tion. Secondly, (despite the relative sparsity of the factor matrices) solving NMF problems often take significant computation time. It has been shown that NMF is a relaxed form of K-Means clustering problem, which is proven to be NP-Complete [10] and the basic so-lutions do not scale to the needs of many applications: (a) Finding the two factors of NMF using naive algorithms can be extremely costly as iterative approaches require O ( nmr ) work until conver-gence where r is the number of iterations. (b) Moreover, if the original matrix is large and dense, it may be impossible to store it in memory, even though the two factors will be potentially sparse.
There are various state of the art techniques that address these challenges, at least partially. In [19], Liu et al presented an algo-rithm which partitions the data and arranges the computations in a way to maximize data locality and opportunities for parallelism. Other parallel NMF approaches include [30] and [31]. Other ap-proaches to improve the scalability of NMF include gradient-based optimization methods [28], which target faster convergence and in-cremental methods [5], which are applicable in data streaming envi-ronments. In [3], Bucak and Gunsel introduce a new cost function for each incremental iterations and develop new update rules for the two factors. [29] do not provide new update rules, but, instead, try to reduce the number of basis vectors needed given an upper bound on the reconstruction error. In [33], Sun et al. proposed several incremental learning methods for tensors (generalization of matri-ces to modalities larger than two), where tensors are first matricized and then treated as vector streams. In [20], Papadimitriou et al. pro-posed an incremental subspace learning algorithm, SPIRIT, which is fast and adaptable to finding the best number of rank, In [6], Chen proposed fast and highly accurate incremental SVD algorithm. Un-fortunately, these algorithms do not ensure the non-negative prop-erty, they also cannot leverage redundancies among data sources.
We now describe our Group non-negative matrix factorization (G-NMF) algorithm, which takes advantage of the data source-query matrix, M in efficiently identifying the r latent semantics of a group of queries.

Let us consider the time instance time i , where the feature-object 1 data matrix, D i,j , of the query q j is an n  X  m where n is the number of features and m j is the number of objects q has received so far. Note that D i,j is a combination of the data provided by the shared data sources (i.e., D ( j ) and the data from the unshared data source associated with query q j . In other words, D matrix corresponding to shared data source D k , Q i,j is the feature-object matrix corresponding to the unshared data source of query
W hile it is customary to represent the data matrices in the form of object-feature matrices, for convenience of the later discussion, in this paper we will assume a feature-object representation, where rows represent features and columns represent objects. q , and  X  indicates horizontal concatenation of the data matrices (i.e., columns of object vectors from different shared data sources are packed next to each other horizontally). As formalized above, we are interested in the r latent semantics (i.e., rank-r low rank NMF approximations) of each query results.

As described in the introduction, the scenario includes a signif-icant amount of redundancies (especially across queries that ac-cess great amount of data from the shared data sources). In par-ticular, for any pair, q j and q h , of queries that access at least one shared data source, there will be certain degree of shared object columns as they will be receiving objects from the same shared data source. Therefore, performing NMF individually on all queries would be wasteful. In Figure 1, we see an example where query 1 accesses shared data sources {1, 2,  X  } and query 2 accesses shared data sources {1, 2, 3}. In addition, each query has its own unshared data sources. The naive way of analyzing the la-tent semantics of these two queries would be performing NMF on D at these two data matrices indicates that these two queries share two data source ( D 1 and D 2 ), which means the columns in the feature-object matrices corresponding to the objects from these two shared data sources will be exactly the same in the two queries resulting data matrices. Figure 2 shows that an alternative approach to de-composing each query X  X  data matrix, would be to decompose each data source X  X  data matrix first to obtain the subfactors for each data source and, then, combine these corresponding subfactors based on the matrix M which describes the relationships between the data sources and queries (along with the query X  X  unshared data source) to compute the NMF factors.
The key challenge, then, is to efficiently combine sub-factors of the shared data source (and the unshared data source) into the factors of the queries. In the proposed G-NMF algorithm, this is achieved by adopting and extending the MUR-based tensor block decomposition technique (originally proposed for the paralleliza-tion of tensor decomposition operations [27]) for efficiently com-bining sub-factors arriving from different sources. We refer to this as the group MUR (or G-MUR) process. Let us consider a query q , with an n  X  m non-negative matrix D . Let us further assume that the matrix D  X  W  X  H can be (approximately) factorized into two factor matrices, W  X  R n  X  r and H  X  R r  X  m , as specified in Equation 1.

Let us first partition D into p  X  m partitions (accord-ing to the matrix M which describes the relationship between the shared data sources and queries) along its columns ( D = [ D (1) , D (2) , ...D ( p ) ] ), such that for each k  X  p , D matrix from a shared data source in the database or a unshared data source. Note that, we can approximate each submatrix, D ( k ) own factors: Let us next split the factor matrix H into p parts, H = [ H (1) , H (2) , ..., H ( p ) ] , where each k  X  p corresponds to a shared data source or a unshared data source. Given these, it is also true that each sub-matrix. D ( k ) , can also be approximated, via the com-mon factor W , as D ( k )  X  W  X  H ( k ) , where, H ( k ) indicates the part in the overall factor H of (data matrix D ) corresponding to partition k . Given these, the error term, E = k D  X  W  X  H k
W e drop the subscript j when we are focusing on only one query. b e reworded as As we discuss next, this error term (which includes data and low-rank approximations from a group of shared data sources) gives rise to an effective MUR-based algorithm for obtaining the NMF for D . The proposed group-MUR (G-MUR) relies on two update rules, one for W and the other for H , both leveraging the special group structure of the error term.
To obtain the update rule for factor matrix W , we need to take the derivative of the error function E with respect to the factor W . Relying on the fact [21] that the Frobenius norm of a matrix X can be rewritten in terms of the trace, T r ( X ) , of X as and that the trace has the following three properties we first expand the cost function from equation 3 based on equa-tions 4 and 5 as E =
Secondly, relying on the fact [21] that the derivative of trace with respect to the first order and second order of matrix X are we take the derivative of the error function E with respect to the factor W as follows:
Note that if we first set the derivative of the error term, E , with respect to W to 0 (to find point at which E is minimum) and then replace D ( k ) with its approximate decomposition in equation 2, we obtain the following update rule for W : In other words, we can fix H and improve W by using matrices H , H ( k ) , H ( k ) , and W ( k ) . Last two of these correspond to the factorization of a shared data source X  X  (or unshared data source X  X ) data matrix; on the other hand, query X  X  factor matrix H (and its partitions H ( k ) ) is initially unknown and thus selected randomly and improved in the G-MUR process as described next.
When updating the factor matrix H for a fixed W , we note that the error term specified in equation 3 relies on the partitions [ H (1) , H (2) , ..., H ( p ) ] of H . Therefore, we take the derivatives of E with respect to individual H ( k ) , separately: Once again, setting the derivative of E with respect to H to 0 and replacing D ( k ) with its approximation in Equation 2, we obtain the following update rule for H ( k ) :
Thus, given an initial decomposition of D  X  W  X  H , we can fix W and improve each partition H ( k ) of H by using matrices W , H ( k ) , and W ( k ) , last two of which correspond to the factorization of the shared data source X  X  (or unshared data source) data matrix. Given these group update rules for W and H , we propose the Group-NMF algorithm presented in Algorithm 1.
 Intuitively, the G-NMF algorithm for efficiently obtaining the NMF of the query data matrices by leveraging the NMF of the shared data source X  X  data matrices: the task of factoring the query data matrices is essentially transformed to factoring shared data sources X  data matrices and combining the relevant subfactors of shared data sources to obtain the corresponding factors for the queries. This is especially useful in scenarios where the number of shared data sources is significantly smaller than the number of continuous NMF queries, which is often the case.

To see the advantage of using G-NMF, let us consider the follow-ing case: Let us assume that we have K shared data sources, each Algorithm 1 G -NMF (using G-MUR)) Input: Output: 1: repeat 2: Update W using [RULE1] and factor matrices of relevant 3: Update H ( k ) for each relevant shared data source, D 4: Combine all relevant H ( k ) into the final factor H for the 5: until the stopping condition for G-MUR is met. of size m  X  m . Let us also assume that we have Q NMF queries, each using K 2 o f the shared data sources and has an equal amount of private data sources (i.e., each NMF query involves an m  X  m ma-trix). Assuming the target rank is r , with the basic NMF algorithm, the execution time would be O ( KQm 2 r ) [ ? ]. With G-NMF, how-ever, the total work is O ( Km 2 r + K Q 2 m 2 r + 2 Qr 2 (a) the target rank r is far smaller than m and (b) Q and K are po-tentially very large, G-NMF would provide significant gains over basic NMF in this scenario. G-NMF can be further improved if there are clusters of data sources that are shared across large num-bers of queries. If this is the case, G-NMF can be performed hierar-chically: first G-NMF would be used to maintain the rank-r factor matrices W C and H C of a given cluster, C  X  T , of shared data sources (by treating the cluster as a virtual queries ) and then these factor matrices would be used to maintain the factor matrices of the individual query who share this cluster (by treating the cluster as a virtual data source ).
Since data will be inserted continuously and therefore, even if the number of shared data sources is relatively small, repeatedly applying NMF on the shared data sources X  data would be waste-ful and costly. Instead, we need a mechanism by which we lever-age temporal redundancy and maintain the NMF incrementally, by considering only newly inserted data. In this subsection, we in-troduce group incremental non-negative matrix factorization (GI-NMF), which updates the subfactors of each the shared data source incrementally and then combines the resulting subfactors to obtain the factor matrices of the queries. The key advantages of this ap-proach are that (a) the number of shared data sources is often far less than the number of queries and this reduces the number of matrices we need to explicitly maintain and (b) often shared data sources X  data matrices are far smaller than queries X  data matrices (which access multiple shared data sources), which significantly reduces computation costs during the incremental update process.
Below, we first introduce a vector-incremental update scheme, which updates the shared data sources X  factors incrementally for each update vector. As experiments in Section 4 show, when us-ing this vector-incremental approach, if the number of updates is large, the cumulative cost for the updates can be prohibitively large. Therefore, in Section 3.3, we provide a block-incremental update scheme, which avoids these costs by updating shared data sources X  factors for more than one vector at each update cycle.
In this section, we introduce an incremental update scheme for vector streams, by extending the vector-incremental approach pro-posed in [3]. More specifically, the authors assume that the newly arriving vector affects the previous encoding matrix H only by adding a new column . Relying on this assumption, authors propose a cost/error function with respect to the old data matrix, newly ar-riving vector, and their relative weights. In this section, we build on this (a) by extending it into a group incremental framework and (b) considering block updates (updates involving blocks of data)  X  as opposed to vector updates (involving individual data vectors).
To develop the update rules for incrementally updating the shared data sources X  matrices, let us first assume that, at time time i , the shared data source has a feature-object matrix M call that the standard error function of NMF at time stamp time where, W i and H i are the two non-negative factors which recon-structed back to M i . As new data received by the shared data source, we need to modify the error function accordingly. Let us assume that a single new object arrives at time stamp time can revise the error function for the newly arriving object as where (a) m i +1 is the vector representing the new object that ar-rives at time stamp time i +1 , (b) h i +1 is the encoding vector for the new object using the new basis vectors described by W  X  is the weighting factor for the contribution of the old objects to to the error at time stamp time i +1 . Since, as in [3], we adopt the assumption that the new object simply appends a new column to H i (to give H i +1 ), the contribution of the old objects to the error at time stamp time i +1 can be computed as
Given the above (and relying on the properties of the Frobenius norm stated in Equations 4 and 5), we can rewrite E i +1 as where W i and H i indicate the NMF factors of time stamp time
Given this and using Equation 6, we can obtain the derivatives of the cost function, E i +1 , with respect to W i +1 and h i +1 the new update rules for the two factors. In particular, setting the derivative with respect to W i +1 , to 0 gives the update rule for factor W i +1 : Note that h i +1 is initially selected randomly and improved in the GI-MUR process.
Similarly, setting the derivative of E i +1 respect to h i +1 to 0 gives the update rule for factor h i +1 : Since, as in [3], we adopt the assumption that the new object simply appends a new column to H i , appending h i +1 to H i to gives the factor H i +1 at time stamp time i .
One major advantage of this process is that we can incrementally update the shared data source X  X  factors W and H without involv-ing the original data matrix which significantly reduces the storage requirement. Moreover, the intermediate matrices, W T i m M i H i , need to be computed only once during one update cycle, which reduces the online computation cost significantly.
Note that, using these two update rules, we can maintain the two subfactors of each shared data source individually. The NMF fac-tors of a given NMF query X  X  data matrix can then be obtained from the subfactors of the relevant shared data sources as described next.
In the previous subsection, we have discussed how to obtain the non-negative factors for each shared data source X  X  data matrix in a given update cycle. Given these, we extend the Group-MUR (G-MUR) technique discussed in Section 2.1 to combine these subfac-tors (using the update rules [RULE1] and [RULE2]) according to each data source-query relationship to obtain the factors for each individual query.
Remember from Section 2.1 that, for a given user query s , with an n  X  m non-negative matrix D (  X  W  X  H ) , we partition D into p  X  m partitions along its columns ( D = [ D (1) , D (2) , ...D such that for each k  X  p , D ( k ) is a data matrix from a shared data source or a unshared data source (according to the data source-query matrix M which describes the coverage relationships be-tween the shared data sources and continuous NMF queries). where W ( k ) and H ( k ) are the non-negative factors corresponding to shared data sources k and H ( k ) indicates the part in the overall factor H of (data matrix D ) corresponding to shared data source k . Given this, an NMF query X  X  non-negative factor W at time stamp time i +1 can be updated as [ RULE5 ] W i +1  X  Here, p is the number of shared data sources that the query is ac-corresponding shared data sources, which are incrementally main-tained using [RULE3] and [RULE4]. H i +1 is the non-negative fac-tor of this user query at time stamp time i +1 , while H ( k ) partitions of H i +1 as we have described before. Note that H (and H ( k ) i +1 ) are initially selected randomly and improved in the GI-MUR process as described next.
Since group update rules(G-MUR) for H ( k ) (the part of user query X  X  factor matrix H corresponding to shared data source k ) is given in [RULE2] as we can update the H ( k ) i +1 at time stamp time i +1 using the update rule Given this, horizontally concatenating all H ( k ) i +1 will give the query X  X  non-negative factor, H i +1 , at time stamp time i +1
As we mentioned earlier, the vector incremental update scheme for shared data sources X  factors, described in Section 3.1, has a significant drawback: if the number of updates is large, then though the cost for updating one vector is small, the cumulative cost for updating the shared data sources X  factors can be very large. To tackle this problem, in this subsection, we introduce a block update scheme for the shared data sources.

Let us focus on one of the shared data sources, D k , at time stamp time i . Assume that, at time time i , the feature-object matrix, M of shared data source D k was of size n  X  m and let the updates at time time i +1 be represented as a feature-object matrix,  X  M of size n  X  m  X  . We can naturally save significant amount of time if we can obtain the updated non-negative factors by considering the update matrix,  X  M i +1 , as a whole instead of considering the individual object updates one at a time.

Recall that the G-NMF algorithm we have introduced in Sec-tion 2.1 treats the feature-object matrix M i +1 = M i  X   X  M as the horizontal concatenation of M i and  X  M i +1 . Our key ob-servation is that since we already have the non-negative factors of M i (  X  W i  X  H i ) from the previous update cycle, if we can obtain the non-negative factors for the update matrix  X  M  X  W i  X   X  H i ) , we may be able to combine these to obtain the fac-
Note that since  X  M i +1 is in practice much smaller than M factors  X  W i and  X  H i can be obtained efficiently by directly ap-plying a naive NMF algorithm, such as the multiplicative update rules in [15]. Therefore, the remaining challenge is to revise the update rules [RULE3] and [RULE4] for shared data sources X  fac-tors for block updates. We propose to achieve this by rewriting G-MUR, [RULE1] and [RULE2], presented in Section 2.1 in a way that accounts for  X  W i and  X  H i .
Since, the non-negative feature-object matrix M i +1 consist of two partitions, M i and  X  M i +1 , with known factor matrices, we can revise the update rule [RULE1] in Section 2.1 to obtain the factor W at time stamp time i +1 as follows: Here (a) W i and H i are the non-negative factors of M i , (b)  X  W and  X  H i +1 are the non-negative factors of  X  M i +1 at time stamp tions of the shared data source X  X  factor matrix H i +1 , correspond-ing to M i and  X  M i +1 , respectively. Note that H ( M i H ( X  M i +1 ) i +1 are initially selected randomly and improved in the GI-MUR process as described next. Algorithm 2 B lock Incremental GI-NMF Input: Output: 1: for all each shared data source D j do 2: Apply naive NMF algorithm to obtain non-negative factors 3: repeat 4: Update W D j i +1 using [RULE3/B]. 6: until stopping condition for GI-MUR is met. 7: end for 8: for all each query q j do 9: repeat 10: Update W q j i +1 using [RULE5], shared data source X  fac-11: Update H q j i +1 using [RULE6], shared data sources X  fac-12: until stopping condition for GI-MUR is met. 13: end for
Unlike before where we were maintaining a single matrix H , when performing block updates of the shared data source X  X  fac-tor matrix H , we need to incrementally maintain two matri-this by revising the update rule [RULE2] in Section 2.1 to ac-count for  X  W i +1 ,  X  H i +1 , and W i +1 (maintained by update rule [RULE3/B]): [ RULE4 / B 1 ] H ( M [ RULE4 / B 2 ] H ( X  M Given these, the factor matrix H i +1 is obtained by concatenating
When the block GI-MUR process for the shared data sources fin-ishes, we have W i +1 and H i +1 for each shared data source. Given these, we then apply update rules [RULE5] and [RULE6] discussed in Section 3.2 to suitably combine the shared data sources X  fac-tors to obtain the individual query X  X  factor matrices at time stamp
The pseudo-code of the GI-NMF algorithm for incrementally up-dating shared data sources X  and queries X  factor matrices using the block update scheme is presented in Algorithm 2. As we see in the experiment results reported in the next section, the GI-NMF algorithm provides significant improvements in performance, es-pecially when query overlaps are large and the database updating rates are high. These multiplicative rules have a relative low time complexity but the convergence is not achieved fast [26].
In this section, we evaluate the efficiency and effectiveness of our Group Incremental NMF algorithm for different scenarios and parameter settings (Table 1). In particular, we consider two contin-uous query applications based on a document clustering task, which has been shown to benefit from NMF based analysis, in particular for bootstrapping generative models of the data [11, 34].
Synthetic data sets allow us to freely vary the characteristics of the data and updates to observe the accuracy and efficiency of our algorithms under different scenarios: We generate u ( D new random data objects per update cycle for k th sharable data source X  X  and u ( q j ) objects per cycle from private data source for j th query, as described in Section 1.3. Note that the total number of objects received by the j th query per update cycle is u ( q P
M jk =1 u ( D k ) . The random objects have a density of 0.4, which means an object includes around 40 percent of the features. Also, we assume that at every c = 10 steps, the system refreshes their entire NMF (to prevent error accumulation), with c  X  u ( D ments for k th shared data source and c  X  ( u ( s j )+ P M objects for the j th query.
While synthetic data streams enable us to study the algorithms under different settings, they may not necessarily correspond to real data content and topic evolution. To address this, we generated a second data stream and associated queries using the popular and well-studied DBLP data set [9]: more specifically The above setup enables us to observe whether G-NMF and GI-NMF schemes can indeed leverage the various redundancies in data streams with real content: c hosen from a uniform j 50% 100% 25% Important: Due to performance limitations of the baseline NMF (which con not leverage sharing), in these experiments, we co n-sider only 50 registered queries (i.e., 50 authors with the highest number of publications). As we later see in Section 4.6, the pro-posed GI-NMF algorithms become increasingly advantageous as the number of queries increases. Therefore, the GI-NMF gains pre-sented in Section 4.5.2 on the DBLP data set should be seen as lower-bounds in performance gain.
In the experiments, we compare for execution time and reconstruction accuracy. In particular, we define reconstruction error overhead ( err rel ) as len where ,-./$0.1$"$234$1562/7$ Figure 3: Efficiency results for the synthetic trace data set u n-der default settings Note that a low-rank decomposition of X i would lead to a recon-struction error, even if it is obtained using full NMF followed by selection of the top r components. Therefore, the denominator of the above term is not equal to 0 . All experiments were conducted using an 8-core Intel(R) Xeon(R) X5570 2.93GHz CPU and 24GB memory Machine. The codes were executed using Matlab 2012b. To ensure the fairness for all MUR involved processes, we use a fixed number (25) of itera-tions. We also add a very small  X  value ( 10  X  9 ) to the denominator of all update rules to avoid zero denominators. For vector based incremental algorithms, I-NMF and GI-NMF(V), we use  X  = 0 . 96 as the weight factor of existing documents relative to new ones (as recommended by [20]). As the two figures show, the proposed G-NMF and block GI-NMF processes which leverage the overlaps provide significant im-provements on the execution time, with only a negligible error over-head ( 0 . 5% over the error of NMF). Importantly, the standard I-NMF scheme (which does not leverage any redundancies) perform +,-./0123-4./%522.2%67,28,9:% Figure 4: Accuracy results for the synthetic data stream unde r default settings very poorly  X  even worse than NMF in execution time. The vector incremental GI-NMF significantly reduces the cost, but still cannot compete with NMF when update rate per cycle is high. The pro-posed block GI-NMF, however, performs extremely fast and with very competitive accuracy.
While the absolute values of the time and error rates are differ-ent (due to lower degree of coverage,  X  5 queried conferences per author), the overall trends with the DBLP data confirms the above observations: standard I-NMF is very costly and results in higher error rates. Note that, for block based GI-NMF, as the number of authors increases the time gain increases, whereas the error over-head remains relatively small. We next detail the impacts of various parameters on the performance of block based GI-NMF using syn-thetic data sets.
Table 4 presents the impacts of the various parameters on the ef-ficiency and effectiveness on the random data stream. In particular, we focus on NMF and block GI-NMF techniques which prove to be competitive with high update rates.

The Table 4 shows that block GI-NMF is consistently more ef-fective in terms of time gain as the problem size (number of terms, percentage of shared data sources involved, updating rate, unshared data source updating rate) increases, again with negligible further loss. Experiments confirm that block GI-NMF saves more time and is more accurate especially for low rank NMF applications.
Recognizing that, in continuous NMF workloads, there are sig-nificant redundancies that can be leveraged for characterizing data sources/queries based on the data/document streams they access, we proposed a group incremental non-negative matrix factoriza-tion (GI-NMF) approach for time-evolving data sets. The proposed GI-NMF algorithm solves multiplicative update rule (MUR) sub-problems by partitioning the original data matrices into submatri-ces (representing data shared by multiple queries) and combining the subfactors into the final factors. Moreover, the GI-NMF al-gorithm maintains each partition X  X  subfactors incrementally as the matrices change over the time in order to scale to the needs of time-evolving data. Experiment results showed that the proposed block GI-NMF technique provides significant time gains in maintaining NMF results, with negligible impact on accuracy. [10] Chris Ding, Xiaofeng He, and Horst D. Simon. On the Refresh E rror Ovrhd.
 [11] D. Donoho and V. Stodden. When does non-negative [12] F. Fabret, H. . Jacobsen, F. Llirbat, J. Pereira, K. A. Ross, [13] Shuli Han, Yujiu Yang, Wenhuang Liu. Incremental [14] Z. Hmedeh, H. Kourdounakis, V. Christophides, C. du [15] Daniel D. Lee and H. Sebastian Seung. Algorithms for [16] A. Levy and M. Lindenbaum. Sequential Karhunen-Loeve [17] X. Li, S. Huang, K. S. Candan, M. L. Sapino. Focusing [18] C.-J. Lin. Projected Gradient Methods for Non-negative [19] C. Liu, H.C. Yang, J.Fan, L.W. He, and Y.-M. Wang. [20] S. Papadimitriou, J. Sun, and C. Faloutsos. Pattern [21] K. B. Petersen and M. S. Pedersen, The Matrix Cookbook. [22] M. Garofalakis, J. Gehrke, R. Rastogi. Querying and [23] M. M. Gaber, A. Zaslavsky, S. Krishnaswamy, Mining [24] M. Petrovic, I. Burcea,and H.-A. Jacobsen. S-ToPSS: [25] M. Petrovic, H. Liu, and H.-A. Jacobsen. CMS-ToPSS: [26] A. H. Phan and A. Cichocki. PARAFAC algorithms for [27] A. H. Phan and A. Cichocki. Advances in PARAFAC [28] J. D. M. Rennie and N. Srebro. Fast maximum margin [29] S. Rebhan, W. Sharif, J. Eggert, Incremental learning in [30] S. A. Robila and L. G. Maciak. A parallel unmixing [31] S. A. Robila and L. G. Maciak. Considerations on [32] A.Silberstein, J.Terrace, B.Cooper, R.Ramakrishnan. [33] J. Sun, D. Tao, S. Papadimitriou, P. Yu, and C. Faloutsos. [34] Wei Xu, Xin Liu and Yihong Gong. Document clustering [35] Z. A. Zhao and H. Liu. Spectral Feature Selection for Data
