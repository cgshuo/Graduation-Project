 Product reviews play an important role in the decision process of online shop-ping, because product reviews influence and form consumers X  opinions and subse-quently affect sales [3, 5, 7]. However, users usually only want to read a handful of helpful reviews instead of scanning all the product reviews to make their decision-s. Therefore, automatically identifying helpful reviews is an important task, and has received considerable research attentions in recent years [14, 11, 26, 4, 25]. helpful reviews. Widely used features include external features , (e.g. date [12] and product type [16]) and intrinsic features (e.g. semantic dictionaries [26] and emotional dictionaries [11]). Compared to external features, intrinsic features can provide some insights and explanations for the prediction results, and support better cross-domain generalisation. In this work, we investigate a new form of intrinsic features: the argument features.
 forming reasons and of drawing conclusions and applying them to various fields [15], such as legal texts[18] , scientific articles [6], persuasive essays[21], online forums[13, 20, 23], and many others. Recently, various applications in the field of computational argumentation have been gaining more attention, [21] used argument mining to assess the argumentation quality of essay, [23] explored argumentative features to rank comments in the online forum.
 particular state of affairs [8]. An argument usually consists of a claim and some premises offered in support of the claim. For example, consider the following review excerpt:  X  The stuff was amazing, and went out of their way to help us  X ; the text segments before the comma is a claim, and the texts after the comma is a premise supporting the claim. Our hypothesis is that, the helpfulness of a review is closely related to some argument-related features, e.g. the percentage of argumentative sentences, the average number of premises in each argument, etc.
 in 110 hotel reviews, so as to use these  X  X round truth X  arguments to testify the effectiveness of argument-based features for detecting helpful hotel reviews. Empirical results suggest that, the best argument-based feature outperform the state-of-the-art features in accuracy and has comparable F1-score and AUC. Combined with full argument-based features, each single baseline feature has a significant improvement in accuracy, F1-score and AUC, by up to 7.35%, 12.7% and 12.1% on average, resp. Furthermore, we use the effective argument-based features to give some insights into which product reviews are more helpful. We use the Tripadvisor hotel reviews corpus built by [17] to test the performance of our helpful reviews classifier. Each entry in this corpus consists of the review texts, how many people have viewed this review (the number Y) and how many people think this review is helpful (the number X).
  X  X round truth X  argument structures. More specifically, we not only annotate the claims, premises and their supporting relations, but also annotate four addition-al argument components that are widely observed in hotel reviews: major claims (a summary of a review, e.g.  X  X  have enjoyed the stay in the hotel X ), recommen-dations (e.g.  X  X efinitely recommend this hotel X ,  X  X o not come to this hotel if you look for some clean places to live X ), background (an objective description that does not give direct opinions but provides some background information; e.g.  X  X e arrived at midnight X ) and premise supporting an implicit claim (PSIC) (e.g.  X  X ust five minutes X  walk to the down town X ; this clause actually support some implicit claims like  X  X he location of the hotel is good X ).
 independently annotate argument component type of each sub-sentence; sub-sentences that do not belong to any of the above component type are labelled as  X  X on-argumentative X . We use the Fleiss X  kappa metric [9] to evaluate the quality of the obtained annotations, and the results are presented in Table 1. We can see that the lowest Kappa scores for type premise is still above 0.6, suggesting that the quality of the annotations are substantial [10]; in other words, there exist little noises even in the ground truth argument structures. We aggregate the annotations using majority voting.
 3.1 Baseline Features In line with [26], we consider the helpfulness as an intrinsic feature of product reviews, and thus only consider the following intrinsic features as our baseline features.
 sentences, average length of sentences, number of exclamation marks, and the percentage of question sentences.
 words ( tf &lt; 3) to build the unigram vocabulary. Each review is represented by the vocabulary with tf-idf weighting for each appeared term.
 tional word (defined in the GALC dictionary [19] plus one additional dimension for the number of non-emotional words.
 word to some semantic tags; similar to the GALC features, the semantic features include the number of occurrences of each semantic tag. 3.2 Argument-based Features Based on the corpus we manually annotated, we acquired various of argument components in each review. We exploit the results of argument components and propose four sets of features as follows.
 helpful is the ratio of different argument component numbers. For example, we may interested in the ratio between the number of premises and that of claims; a high ratio suggests that there are more premises given each claim, indicating that the review gives considerable evidences. To generalise this component type ratio concept, we propose combination ratio : we compute the ratio between two combinations of argument components. For example, we may be interested in the ratio between the number of MajorClaim+Claim+Premise and that of Non-argument. As there are 7 types of labels, the number of possible combinations is 2 7 -1 = 127, and thus the possible number of combination ratio pairs is 127  X  126 = 16002. In other words, the component feature is a 16002-dimensional real vector.
 statistics of each argument component type: for example, suppose a review has only two claims, one has 10 words and the other has 5 words; we may want to know the average number of words contained in each claim, the total number of words in claims, etc. In total, for each argument component type, we consider 5 types of statistics: the total number of words in the given component type, the length (in terms of word) of the shortest/longest component of the given type, and the mean/variance of the number of words in each component of the given type. Thus, there are in total 7  X  5 = 35 features to represent the statistics. given a review, we may want to know the ratio between the number of words in Claims+MajorClaims and that in Premises. Thus, the combination ratio can also be applied here. We consider only the combination ratio for two statistics: the total number of words and the average number of words in each component; hence, there are 16002  X  2 = 32004 dimensions for the combination ratio for the statistics. In total, there are 32004 + 35 = 32039 dimensions for the component token number features.
 tatistics of the letter number of some components, which may reflect some in-formation the token number does not contain: For example, if a review with more letters and few words may be hard to understand for readers. Considering several premises supporting a claim, with more words and few letters, readers can get the information easily. Similar to the component token number features above, we design 5 types of statistics and their combination ratios. Thus, the dimension for position features is the same to that of component token number features.
 components, the location of some argument components may also be helpful to predict the helpfulness of reviews: for example, if the major claims of a review are all at the very beginning, we may think that this review highlights its main points and thus may be more helpful. For each component, we use a real number to represent its position: for example, if a review has 10 sub-sentences in total and a component overlaps the second sub-sentence, then the position for this component is 2 / 10 = 0 . 2. For each type of argument component, we may be interested in some statistics for its positions: for example, if a review has several premises, we may want to know the location of the earliest/latest appearance of premises, the average position of all premises and its variance, etc. As in component token number features and letter number features, we design the same features for component position. Following [17, 11], we model the helpfulness prediction task as a classification problem; thus, we use accuracy, macro F1 and area under the curve (AUC) to as evaluation metrics. Similar to [17], we consider a review as helpful if and only if at least 75% opinions for the review are positive, i.e. X/Y  X  0 . 75 (see X and Y in Sect.2). To reduce feature dimension and to improve the performance, for each feature set we test, we only use its top 150 features in terms of information gain. In line with most existing works on helpfulness prediction [11, 26], we use the LibSVM [2] as our classifier. The results of 10-fold cross-validation for different features are presented in Table.2. Note that  X  X ullArg X  and  X  X ullBase X  means that we use all the argument-based features and all baseline features, resp. ate argument-based features improve the performance of the review helpfulness identification. Considering each baseline feature singly, our proposed component token number features outperform the best baseline feature UGR in accuracy, up to 64.5%. Combined with full argument-based features, each single baseline features improve significantly in accuracy, F1-score and AUC, by up to 7.35%, 12.7% and 12.1% on average, resp.
 improve the helpfulness prediction. Next, we will give more detailed analyses on which argument-based features make the most contribution and, furthermore, give some insights into which reviews are more helpful from the argumentation perspective. Argument-based features can not only improve the performance of review help-fulness but also interpret what makes a review helpful. We analyse the features according to the information gain and find that among the top 150 features, more than half features are from the component token number feature set, and a quarter of features are from the component letter number feature set, suggest-ing that these two feature sets are most effective in identifying helpful reviews. Among the 80 component token number features, three-quarters of features are combination ratio for the sum of token number, and the remaining features are combination ratio for the mean token number.
 tokens it contains, the more information the review delivers, and the more likely the review is helpful. In fact, helpful reviews are tend to occur in those long re-views, which generally provide with more experience and feedback about service. Among the 46 component letter number features, 87% features are combination ratios for sum of the number of letters. This observation, again, suggests that longer reviews are more likely to be helpful.
 gain features, which also contribute to the helpfulness. We interpret that the position of each component influences the logic of reviews. In the process of forming reasons and persuading readers, the position of components heavily affect the chain of reasoning, especially faced with a confused order of each components, it will make readers hardly to convince and accept the opinions. In this work, we novelly use some argument-based features to identify helpful reviews. We manually annotated 110 hotel reviews, propose some argument-based feature sets, and compare the performance of argument-based features with that of some classic features. Empirical results suggest that, using only argument-based features has the highest accuracy compared to using any single classic feature set; when using all argument-based feature sets together, the performance significantly outperforms that of using any single classic feature set. In addition, we analyse the effective argument-based features, so as to explain which reviews are more helpful from an argumentation perspective.
 known as argumentation mining), our work suggests that it is promising to apply the argument-based features to large-scale helpful reviews identification tasks. The argument-based features will have widely useful applications in other fields.

 Product reviews play an important role in the decision process of online shop-ping, because product reviews influence and form consumers X  opinions and subse-quently affect sales [3, 5, 7]. However, users usually only want to read a handful of helpful reviews instead of scanning all the product reviews to make their decision-s. Therefore, automatically identifying helpful reviews is an important task, and has received considerable research attentions in recent years [14, 11, 26, 4, 25]. helpful reviews. Widely used features include external features , (e.g. date [12] and product type [16]) and intrinsic features (e.g. semantic dictionaries [26] and emotional dictionaries [11]). Compared to external features, intrinsic features can provide some insights and explanations for the prediction results, and support better cross-domain generalisation. In this work, we investigate a new form of intrinsic features: the argument features.
 forming reasons and of drawing conclusions and applying them to various fields [15], such as legal texts[18] , scientific articles [6], persuasive essays[21], online forums[13, 20, 23], and many others. Recently, various applications in the field of computational argumentation have been gaining more attention, [21] used argument mining to assess the argumentation quality of essay, [23] explored argumentative features to rank comments in the online forum.
 particular state of affairs [8]. An argument usually consists of a claim and some premises offered in support of the claim. For example, consider the following review excerpt:  X  The stuff was amazing, and went out of their way to help us  X ; the text segments before the comma is a claim, and the texts after the comma is a premise supporting the claim. Our hypothesis is that, the helpfulness of a review is closely related to some argument-related features, e.g. the percentage of argumentative sentences, the average number of premises in each argument, etc.
 in 110 hotel reviews, so as to use these  X  X round truth X  arguments to testify the effectiveness of argument-based features for detecting helpful hotel reviews. Empirical results suggest that, the best argument-based feature outperform the state-of-the-art features in accuracy and has comparable F1-score and AUC. Combined with full argument-based features, each single baseline feature has a significant improvement in accuracy, F1-score and AUC, by up to 7.35%, 12.7% and 12.1% on average, resp. Furthermore, we use the effective argument-based features to give some insights into which product reviews are more helpful. We use the Tripadvisor hotel reviews corpus built by [17] to test the performance of our helpful reviews classifier. Each entry in this corpus consists of the review texts, how many people have viewed this review (the number Y) and how many people think this review is helpful (the number X).
  X  X round truth X  argument structures. More specifically, we not only annotate the claims, premises and their supporting relations, but also annotate four addition-al argument components that are widely observed in hotel reviews: major claims (a summary of a review, e.g.  X  X  have enjoyed the stay in the hotel X ), recommen-dations (e.g.  X  X efinitely recommend this hotel X ,  X  X o not come to this hotel if you look for some clean places to live X ), background (an objective description that does not give direct opinions but provides some background information; e.g.  X  X e arrived at midnight X ) and premise supporting an implicit claim (PSIC) (e.g.  X  X ust five minutes X  walk to the down town X ; this clause actually support some implicit claims like  X  X he location of the hotel is good X ).
 independently annotate argument component type of each sub-sentence; sub-sentences that do not belong to any of the above component type are labelled as  X  X on-argumentative X . We use the Fleiss X  kappa metric [9] to evaluate the quality of the obtained annotations, and the results are presented in Table 1. We can see that the lowest Kappa scores for type premise is still above 0.6, suggesting that the quality of the annotations are substantial [10]; in other words, there exist little noises even in the ground truth argument structures. We aggregate the annotations using majority voting.
 3.1 Baseline Features In line with [26], we consider the helpfulness as an intrinsic feature of product reviews, and thus only consider the following intrinsic features as our baseline features.
 sentences, average length of sentences, number of exclamation marks, and the percentage of question sentences.
 words ( tf &lt; 3) to build the unigram vocabulary. Each review is represented by the vocabulary with tf-idf weighting for each appeared term.
 tional word (defined in the GALC dictionary [19] plus one additional dimension for the number of non-emotional words.
 word to some semantic tags; similar to the GALC features, the semantic features include the number of occurrences of each semantic tag. 3.2 Argument-based Features Based on the corpus we manually annotated, we acquired various of argument components in each review. We exploit the results of argument components and propose four sets of features as follows.
 helpful is the ratio of different argument component numbers. For example, we may interested in the ratio between the number of premises and that of claims; a high ratio suggests that there are more premises given each claim, indicating that the review gives considerable evidences. To generalise this component type ratio concept, we propose combination ratio : we compute the ratio between two combinations of argument components. For example, we may be interested in the ratio between the number of MajorClaim+Claim+Premise and that of Non-argument. As there are 7 types of labels, the number of possible combinations is 2 7 -1 = 127, and thus the possible number of combination ratio pairs is 127  X  126 = 16002. In other words, the component feature is a 16002-dimensional real vector.
 statistics of each argument component type: for example, suppose a review has only two claims, one has 10 words and the other has 5 words; we may want to know the average number of words contained in each claim, the total number of words in claims, etc. In total, for each argument component type, we consider 5 types of statistics: the total number of words in the given component type, the length (in terms of word) of the shortest/longest component of the given type, and the mean/variance of the number of words in each component of the given type. Thus, there are in total 7  X  5 = 35 features to represent the statistics. given a review, we may want to know the ratio between the number of words in Claims+MajorClaims and that in Premises. Thus, the combination ratio can also be applied here. We consider only the combination ratio for two statistics: the total number of words and the average number of words in each component; hence, there are 16002  X  2 = 32004 dimensions for the combination ratio for the statistics. In total, there are 32004 + 35 = 32039 dimensions for the component token number features.
 tatistics of the letter number of some components, which may reflect some in-formation the token number does not contain: For example, if a review with more letters and few words may be hard to understand for readers. Considering several premises supporting a claim, with more words and few letters, readers can get the information easily. Similar to the component token number features above, we design 5 types of statistics and their combination ratios. Thus, the dimension for position features is the same to that of component token number features.
 components, the location of some argument components may also be helpful to predict the helpfulness of reviews: for example, if the major claims of a review are all at the very beginning, we may think that this review highlights its main points and thus may be more helpful. For each component, we use a real number to represent its position: for example, if a review has 10 sub-sentences in total and a component overlaps the second sub-sentence, then the position for this component is 2 / 10 = 0 . 2. For each type of argument component, we may be interested in some statistics for its positions: for example, if a review has several premises, we may want to know the location of the earliest/latest appearance of premises, the average position of all premises and its variance, etc. As in component token number features and letter number features, we design the same features for component position. Following [17, 11], we model the helpfulness prediction task as a classification problem; thus, we use accuracy, macro F1 and area under the curve (AUC) to as evaluation metrics. Similar to [17], we consider a review as helpful if and only if at least 75% opinions for the review are positive, i.e. X/Y  X  0 . 75 (see X and Y in Sect.2). To reduce feature dimension and to improve the performance, for each feature set we test, we only use its top 150 features in terms of information gain. In line with most existing works on helpfulness prediction [11, 26], we use the LibSVM [2] as our classifier. The results of 10-fold cross-validation for different features are presented in Table.2. Note that  X  X ullArg X  and  X  X ullBase X  means that we use all the argument-based features and all baseline features, resp. Table 2: Results for argument-based features and baseline features. Comp.Num, Comp.Token, Comp. Letter and Comp. Position stand for Component number ate argument-based features improve the performance of the review helpfulness identification. Considering each baseline feature singly, our proposed component token number features outperform the best baseline feature UGR in accuracy, up to 64.5%. Combined with full argument-based features, each single baseline features improve significantly in accuracy, F1-score and AUC, by up to 7.35%, 12.7% and 12.1% on average, resp.
 improve the helpfulness prediction. Next, we will give more detailed analyses on which argument-based features make the most contribution and, furthermore, give some insights into which reviews are more helpful from the argumentation perspective. Argument-based features can not only improve the performance of review help-fulness but also interpret what makes a review helpful. We analyse the features according to the information gain and find that among the top 150 features, more than half features are from the component token number feature set, and a quarter of features are from the component letter number feature set, suggest-ing that these two feature sets are most effective in identifying helpful reviews. Among the 80 component token number features, three-quarters of features are combination ratio for the sum of token number, and the remaining features are combination ratio for the mean token number.
 tokens it contains, the more information the review delivers, and the more likely the review is helpful. In fact, helpful reviews are tend to occur in those long re-views, which generally provide with more experience and feedback about service. Among the 46 component letter number features, 87% features are combination ratios for sum of the number of letters. This observation, again, suggests that longer reviews are more likely to be helpful.
 gain features, which also contribute to the helpfulness. We interpret that the position of each component influences the logic of reviews. In the process of forming reasons and persuading readers, the position of components heavily affect the chain of reasoning, especially faced with a confused order of each components, it will make readers hardly to convince and accept the opinions. In this work, we novelly use some argument-based features to identify helpful reviews. We manually annotated 110 hotel reviews, propose some argument-based feature sets, and compare the performance of argument-based features with that of some classic features. Empirical results suggest that, using only argument-based features has the highest accuracy compared to using any single classic feature set; when using all argument-based feature sets together, the performance significantly outperforms that of using any single classic feature set. In addition, we analyse the effective argument-based features, so as to explain which reviews are more helpful from an argumentation perspective.
 known as argumentation mining), our work suggests that it is promising to apply the argument-based features to large-scale helpful reviews identification tasks. The argument-based features will have widely useful applications in other fields.

