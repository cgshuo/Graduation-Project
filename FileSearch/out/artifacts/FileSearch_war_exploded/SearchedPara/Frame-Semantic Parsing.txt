 Google Inc.
 Massachusetts Institute of Technology Priberam Labs Instituto de Telecomunicac  X   X  oes Carnegie Mellon University Carnegie Mellon University lexicon. We solve the problem of frame-semantic parsing using a two-stage statistical model expressed semantic arguments. At inference time, a fast exact dual decomposition algorithm
Both components are feature-based and discriminatively trained on a small set of annotated frame-semantic parses. On the SemEval 2007 benchmark data set, the approach, along with a margins. Additionally, we present experiments on the much larger FrameNet 1.5 data set. We have released our frame-semantic parser as open-source software. 1. Introduction
FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing consider-able information about lexical and predicate-argument semantics in English. Grounded define X  X  semantic representation that blends representations familiar from word-sense richly structured frame-semantic structures with high coverage will require data-driven techniques beyond simple supervised classification, such as latent variable modeling, semi-supervised learning, and joint inference.
 parsing, the problem of extracting from text semantic predicate-argument structures such as those shown in Figure 1. We aim to predict a frame-semantic representation proaches (Baker, Ellsworth, and Erk 2007). We use a probabilistic framework that cleanly integrates the FrameNet lexicon and limited available training data. The probabilistic framework we adopt is highly amenable to future extension through new features, more relaxed independence assumptions, and additional semi-supervised models.
 (Section 6), in which arguments to each frame are identified and labeled with a role from that frame. Experiments demonstrating favorable performance to the previous state of the art on SemEval 2007 and FrameNet data sets are described in each section. Some novel aspects of our approach include a latent-variable model (Section 5.2) and a semi-supervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of words not in the FrameNet lexicon; a unified model for finding and labeling arguments 10 (Section 6) that diverges from prior work in semantic role labeling; and an exact dual frame together, thereby incorporating linguistic constraints in a principled fashion. tations) 1 achieves the best published results to date on the SemEval 2007 frame-semantic conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: niques suited to shallow semantic parsing problems, novel methods in semi-supervised learning that improve the lexical coverage of our parser, and making frame-semantic structures a viable computational semantic representation usable in other language technologies. To set the stage, we next consider related work in the automatic prediction of predicate-argument semantic structures. 2. Related Work frame-semantic parsing. First, we will briefly discuss work done on PropBank-style semantic role labeling, following which we will concentrate on the more relevant prob-used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior work on joint structure prediction relevant to frame-semantic parsing. 2.1 Semantic Role Labeling has been a great deal of computational work using predicate-argument structures for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed by CoNLL shared tasks on semantic role labeling (Carreras and M ` arquez 2004, 2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank. phrase-structure syntax trees from the Wall Street Journal section of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) annotated with predicate-argument various semantic roles. The two main verbs in the sentence, created and pushed ,are the predicates. For the former, the constituent more than 1.2 million jobs serves as the semantic role ARG1 and the constituent In that time serves as the role for the latter verb, roles ARG1 , ARG2 , ARGM-DIR ,and ARGM-TMP
PropBank defines core roles ARG0 through ARG5 , which receive different interpretations for different predicates. Additional modifier roles ARGM-* and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning.

PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses inte-ger linear programming for inference and uses several global constraints to find the best 12 discriminative log-linear models is presented by Toutanova, Haghighi, and Manning (2005), where global features looking at all arguments of a particular verb together are incorporated into a dynamic programming and reranking framework. The Computa-tional Linguistics special issue on semantic role labeling (M ` arquez et al. 2008) includes other interesting papers on the topic, leveraging the PropBank conventions for labeling shallow semantic structures. Recently, there have been initiatives to predict syntactic dependencies as well as PropBank-style predicate-argument structures together using one joint model (Surdeanu et al. 2008; Haji  X  c et al. 2009).
 unclear what the core roles ARG1 or ARG2 represent linguistically. To better understand the roles X  meaning for a given verb, one has to refer to a verb-specific file provided along with the PropBank corpus. Although collapsing these verb-specific core roles into tags
ARG0 -ARG5 leads to a small set of classes to be learned from a reasonable sized corpus, analysis shows that the roles ARG2  X  ARG5 serve many different purposes for different verbs. Yi, Loper, and Palmer (2007) point out that these four roles are highly overloaded and inconsistent, and they mapped them to VerbNet (Schuler 2005) thematic roles to get improvements on the SRL task. Recently, Bauer and Rambow (2011) presented a method to improve the syntactic subcategorization patterns for FrameNet lexical units using VerbNet. Instead of working with PropBank, we focus on shallow semantic parsing of sentences in the paradigm of frame semantics (Fillmore 1982), to which we turn next. 2.2 Frame-Semantic Parsing information about lexical items and predicate-argument structures. A semantic frame and phrases that can potentially evoke it in a natural language utterance. Each frame represented by the frame. In a frame-analyzed sentence, predicates evoking frames are known as targets , and a word or phrase filling a role is known as an argument .
Figure 2(b) shows frame-semantic annotations for the same sentence as in Figure 2(a). (In the figure, for example, the C ARDINAL NUMBERS frame,  X  and  X  E  X  denotes the role Entity .) Note that the verbs created and pushed evoke the frames ing lexical units 2 from the FrameNet lexicon, create .
 The PropBank analysis in Figure 2(a) also has annotations for these two verbs. While semantic parse labels each frame X  X  arguments with frame-specific roles shown in the figure, making it immediately clear what those arguments mean. For example, for the the Time when the jobs were created. FrameNet also allows non-verbal words and phrases to evoke semantic frames: in this sentence, million evokes the frame and doubles as its Number argument, with 1.2 as Multiplier and more than as the Precision of the quantity expression.
Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) con-tains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs, and prepositions among its lexical units. Finally, FrameNet frames organize predicates according to semantic principles, both by allowing related terms to evoke a common defining frames and their roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates.
 in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame arguments given the frame; Thompson, Levy, and Manning (2003) used a generative model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first used maximum entropy models to find and label arguments given the frame. Shi and
Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad  X  o (2006) introduced the Shalmaneser tool, which uses naive
Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet, containing around 300 frames and fewer than 500 unique semantic roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared task data set, and also the newer FrameNet 1.5, 3 which lists 877 frames and 1,068 role types X  X hus handling many more labels, and resulting in richer frame-semantic parses. frames which need to be recognized along with their arguments X  X as undertaken as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth, 14 containing a little more than 2,000 sentences with full text annotations. The LTH system of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the
Johansson and Nugues broke down the task as identifying targets that could evoke determining the arguments that fill the semantic roles of a frame. They used a series of SVMs to classify the frames for a given target, associating unseen lexical items to the full text annotation corpus as well as the FrameNet exemplar sentences were annotated sentences as training data, model the whole problem with only two statis-tical models, and obtain significantly better overall parsing scores. We also model the argument identification problem using a joint structure prediction model and use semi-supervised learning to improve predicate coverage. We also present experiments on recently released FrameNet 1.5 data.
 vestigated various uses of FrameNet X  X  taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado and Lapata 2005; F  X  urstenau and Lapata 2009b). Others have explored the application
Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pad  X  o and Erk 2005). 2.3 Semi-Supervised Methods
Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank-and FrameNet-style representations, a few improve-ments over vanilla supervised methods using unlabeled data are notable. F  X  urstenau and
Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information as well as the seeds are used to train statistical model(s) for SRL. The authors ran experiments using a set of randomly chosen verbs from the exemplar sentences of FrameNet and found improvements over supervised methods. In an extension to this work, F  X  urstenau and Lapata (2009a) present a method for finding examples for unseen verbs using a graph alignment method; this method represents sentences and their syntactic analysis as graphs and graph alignment is used to project annotations from seed examples to unlabeled sentences. This alignment problem is again modeled as a linear program. papers. Although this line of work presents a novel direction in the area of SRL, the published approach does not yet deal with non-verbal predicates and does not evaluate the presented methods on the full text annotations of the FrameNet releases. mation from unlabeled data by using a latent words language model. Latent variables are used to model the underlying representation of words, and parameters of this model are estimated using standard unsupervised methods. Next, the latent information is used as features for an SRL model. Improvements over supervised SRL techniques are observed with the augmentation of these extra features. The authors also compare their method with the aforementioned two methods of F  X  urstenau and Lapata (2009a, 2009b) and show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conven-tions and only labels verbal and nominal predicates X  X n contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston,
Ratle, and Collobert (2008), who use neural embeddings of words, which are eventu-ally used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed.
 and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discov-ering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing
Most high-performance SRL systems that use conventions from PropBank (Kingsbury and Palmer 2002) and NomBank (Meyers et al. 2004) utilize joint inference for seman-investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been using dynamic programming or integer linear programs (ILPs); we treat both problems together here. 4 mulations for complex structure prediction tasks like dependency parsing (Riedel and
Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combina-torial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011).
 decomposition strongly relies on breaking down the original problem into a  X  X ood X  16 decomposition, that is, one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. proposed the Alternating Directions Dual Decom-position (AD 3 ) algorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. We adopt that algorithm as it perfectly suits the problem of argument identification, as we observe in Section 7. also contribute an exact branch-and-bound technique wrapped around AD structure of the FrameNet lexicon and the data sets used to train our models. 3. Resources and Task
We consider frame-semantic parsing resources consisting of a lexicon and annotated sentences with frame-semantic structures, evaluation strategies, and previous baselines. 3.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manually identified general-purpose semantic frames for English. 6 Listed in the lexicon with each frame are a set of lemmas (with parts of speech) that can denote the frame or some aspect of it X  X hese are called lexical targets . The set of LUs listed for a frame in FrameNet may not be exhaustive; we may props, and attributes. We use the term argument to refer to a sequence of word tokens annotated as filling a frame role. Figure 1 shows an example sentence from the training data with annotated targets, LUs, frames, and role-argument pairs. The FrameNet lexicon also provides information about relations between frames and between roles (e.g., INHERITANCE ). Figure 3 shows a subset of the relations between five frames and their roles.
 graphic exemplar sentences (primarily from the British National Corpus) annotated terns for the frame in question, these sentences only contain annotations for a single frame.
 train our models hurt performance as evaluated on SemEval 2007 data, which formed larger than the number of sentences in training data that we consider in our experiments sample, do not have complete annotations, and are not from a domain similar to the (Section 5.2). 3.2 Data
In our experiments on frame-semantic parsing, we use two sets of data: 1. SemEval 2007 data: In benchmark experiments for comparison with previous 2. FrameNet 1.5 release: A more recent version of the FrameNet lexicon was released 18
Preprocessing. We preprocessed sentences in our data set with a standard set of anno-tations: POS tags from MXPOST (Ratnaparkhi 1996) and dependency parses from the
MST parser (McDonald, Crammer, and Pereira 2005); manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum 1998) for lemmatization. Our models treat these pieces of information as observations. We also labeled each verb in the data as having ACTIVE or from the SRL system described by Johansson and Nugues (2008). 3.3 Task and Evaluation Methodology
Automatic annotations of frame-semantic structure can be broken into three parts: frame identification (Section 5, not unlike word-sense disambiguation), and argument identification (Section 6, essentially the same as semantic role labeling). cision, recall, and F 1 -measure for frames and arguments; it also provides a score that precision, recall, and F 1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels.
Ellsworth, and Erk 2007). For our experiments, statistical significance is measured using a reimplementation of Dan Bikel X  X  randomized parsing evaluation comparator, a strat-ified shuffling test whose original implementation 11 is accompanied by the following description (quoted verbatim, with explanations of our use of the test given in square brackets): 3.4 Baseline A strong baseline for frame-semantic parsing is the system presented by Johansson and
Nugues (2007, hereafter J&amp;N X 07), the best system in the SemEval 2007 shared task. That system is based on a collection of SVMs. They used a set of rules for target identification which we describe in Appendix A. For frame identification, they used an SVM classifier to disambiguate frames for known frame-evoking words. They used WordNet synsets to extend the vocabulary of frame-evoking words to cover unknown words, and then used a collection of separate SVM classifiers X  X ne for each frame X  X o predict a single evoked frame for each occurrence of a word in the extended set.
 guments. Both phases used SVMs. Thus, their formulation of the problem involves a multitude of independently trained classifiers that share no information X  X hereas ours uses two log-linear models, each with a single set of parameters shared across all contexts, to find a full frame-semantic parse.
 2007. However, because we are not aware of any other work using the FrameNet 1.5 full text annotations, we report our results on that data set without comparison to any other system. 20 4. Target Identification sequences) evoke frames in a given sentence. In other semantic role labeling schemes (e.g., PropBank), simple part-of-speech criteria typically distinguish targets from non-evoke frames under certain conditions. One complication is that semantically impov-erished support predicates (such as make in make a request ) do not evoke frames in the context of a frame-evoking, syntactically dependent noun ( request ). Furthermore, only temporal, locative, and directional senses of prepositions evoke frames. targets that appear in the exemplar sentences and a given training set. For a sentence in new data, we considered as candidate targets only those substrings that appear in this master list. We also did not attempt to capture discontinuous frame targets: for example, we treat there would have been as a single span even though the corresponding LU is there be . V . 13 to the ones described by Johansson and Nugues (2007, see Appendix A), with two exceptions. First, they identified locative, temporal, and directional prepositions using a dependency parser so as to retain them as valid LUs. In contrast, we pruned all types of prepositions because we found them to hurt our performance on the development set due to errors in syntactic parsing. In a second departure from their target extraction rules, we did not remove the candidate targets that had been tagged as support verbs for some other target. Note that we used a conservative white list that filters out targets whose morphological variants were not seen either in the lexicon or the training data.
Therefore, when this conservative process of automatic target identification is used, our system loses the capability to predict frames for completely unseen LUs, despite the fact that our powerful frame identification model (Section 5) can accurately label frames for new LUs. 15
Results. Table 3 shows results on target identification tested on the SemEval 2007 test set; our system gains 3 F 1 points over the baseline. This is statistically significant with while our system succeeds. A considerable proportion of these units have more than
Baseline: J&amp;N X 07 87.87 67.11 76.10 one token (e.g., chemical and biological weapon . N , ballistic missile . model. The baseline also does not label variants of there be .
 been ), which we correctly label as targets. Some examples of other single token LUs that the baseline fails to identify are names of months, LUs that belong to the (e.g., iranian . A ), and directions (e.g., north . A or north-south .
 5. Frame Identification
Given targets, our parser next identifies their frames, using a statistical model. 5.1 Lexical Units
FrameNet specifies a great deal of structural information both within and among frames. For frame identification we make use of frame-evoking lexical units, the (lem-matized and POS-tagged) words and phrases listed in the lexicon as referring to specific frames. For example, listed with the B RAGGING frame are 10 LUs, including boast . the same LU may be associated with multiple frames; for example, gobble .
LU. All targets in the exemplar sentences, our training data, and most in our test data, correspond to known LUs. (See Section 5.4 for statistics of unknown LUs in the test sets.) lexicon X  X nd to avoid the possibility of lemmatization errors X  X ur frame identification model will incorporate, via a latent variable, features based directly on exemplar and training targets rather than LUs. Let L be the set of (unlemmatized and automati-cally POS-tagged) targets found in the exemplar sentences of the lexicon and/or the evoking a particular frame f . 17 Let L l and L l f denote the lemmatized versions of
L , respectively. Then, we write boasted . VBD  X  L B RAGGING indicate that this inflected verb boasted and its lemma boast have been seen to evoke the
B RAGGING frame. Significantly, however, another target, such as toot your own horn ,might be used elsewhere to evoke this frame. We thus face the additional hurdle of predicting frames for unknown words. 22 several domain-critical frames that were not already present in version 1.3 of the lexicon.
For our experiments we omit frames attested in neither the training data nor the exem-plar sentences from the lexicon. 18 This leaves a total of 665 frames for the SemEval 2007 data set and a total of 877 frames for the FrameNet 1.5 data set. 5.2 Model
For a given sentence x with frame-evoking targets t, let t sequence). 19 Let t l i denote its lemma. We seek a list f = f target. In our model, the set of candidate frames for t i for 4.7% of the annotated targets in the SemEval 2007 development set). In both cases, we let F i be the set of candidate frames for the i th target in x. We denote the entire set of frames in the lexicon as F .
 exemplars nor the training data, our model includes an additional variable, variable ranges over the seen targets in L f summed over via the latent variable. The prediction rule requires a probabilistic model over frames for a target:
We model the probability of a frame f and the prototype unit , given the target and the sentence x as:
This is a conditional log-linear model: for f  X  F i weights, and g is a vector-valued feature function. This discriminative formulation is very flexible, allowing for a variety of (possibly overlapping) features; for example, a feature might relate a frame type to a prototype, represent a lexical-semantic relation-ship between a prototype and a target, or encode part of the syntax of the sentence. tion (Burchardt, Erk, and Frank 2005; Johansson and Nugues 2007, e.g., by expanding discriminative model, relating known lexical units to unknown words that may evoke frames. Here we are able to take advantage of the large inventory of partially annotated  X   X   X   X   X  the lemmatized sequence of words in the prototype and their part-of-speech tags  X   X   X   X  exemplar sentences. Note that this model makes an independence assumption: Each frame is predicted independently of all others in the document. In this way the model is similar to J&amp;N X 07. However, ours is a single conditional model that shares features and weights across all targets, frames, and prototypes, whereas the approach of J&amp;N X 07 consists of many separately trained models. Moreover, our model is unique in that it uses a latent variable to smooth over frames for unknown or ambiguous LUs. and its WordNet lexical-semantic relationship with the target t frame f . Our model uses binary features, which are detailed in Table 4. 5.3 Parameter Estimation
Given a training data set (either SemEval 2007 data set or the FrameNet 1.5 full text annotations), which is of the form x ( j ) ,t ( j ) ,f ( j ) frame identification model by maximizing the training data log-likelihood: that the training problem is non-convex because of the summed-out prototype latent 24 variable for each frame. To calculate the objective function, we need to cope with a sum over frames and prototypes for each target (see Equation (2)), often an expensive operation. We locally optimize the function using a distributed implementation of L-
BFGS. 24 This is the most expensive model that we train: With 100 parallelized CPUs using MapReduce (Dean and Ghemawat 2008), training takes several hours. takes only a few minutes on one CPU for the test set. 5.4 Supervised Results
SemEval 2007 Data. On the SemEval 2007 data set, we evaluate the performance of our frame identification model given gold-standard targets and automatically identified targets (Section 4); see Table 5. Together, our target and frame identification outperform the baseline by 4 F 1 points. To compare the frame identification stage in isolation with system as input. With partial matching, our model achieves a relative improvement of
Note that for exact matching, the F 1 score of the automatic targets setting is better than the gold target setting. This is due to the fact that there are many unseen predicates in the test set on which the frame identification model performs poorly; however, for the automatic targets that are mostly seen in the lexicon and training data, the model gets high precision, resulting in better overall F 1 score.
 art for this task, and offers several advantages over J&amp;N X  X  formulation of the problem:
It requires only a single model, learns lexical-semantic features as part of that model rather than requiring a preprocessing step to expand the vocabulary of frame-evoking words, and is probabilistic, which can facilitate global reasoning. were not present in the white list that we used for target identification (see Section 4). lemmas, the evaluation script assigns a score of 0.5 or more, suggesting that our model predicts a closely related frame. Finally, for 190 of the 210 lemmas, a positive score is assigned by the evaluation script. This suggests that the hidden variable model helps under exact X  X ut not partial X  X rame matching, the F 1 score using automatic targets is commensurate with the score for oracle targets. 26 to predict frames for unseen lemmas. However, our model outperforms J&amp;N X 07 by 4F 1 points. The partial frame matching F 1 score of our model represents a significant identified by J&amp;N X 07 and frames classified by our frame identification model resulted better target identification. Note from the results that the automatic target identification model shows an increase in precision, at the expense of recall. This is because the white list for target identification restricts the model to predict frames only for known LUs.
If we label the subset of test set with already seen LUs (seen only in the training set, excluding the exemplars) with their corresponding most frequent frame, we achieve an exact match accuracy between 52.9% and 91.2%, depending on the accuracy of the unseen LUs (these bounds assume, respectively, that they are all incorrectly labeled or all correctly labeled).

FrameNet 1.5 Release. The bottom three rows of Table 5 show results on the full text nearly doubled, we see large improvements in frame identification accuracy. Note that we only evaluate with gold targets as input to frame identification. (As mentioned in
Section 3.2, some documents in the test set have not been annotated for all targets, so 50.1% of the targets in the test set were ambiguous (i.e., they associated with more than one frame either in FrameNet or our training data). On these targets, the exact frame identification accuracy is 73.10% and the partial accuracy is 85.77%, which indicates that the frame identification model is robust to target ambiguity. On this data set, the most frequent frame baseline achieves an exact match accuracy between 74.0% and 88.1%, depending on the accuracy of the unseen LUs.
 marginalizing over a latent variable , whose values range over targets known to associate with the frame f being considered (see Equations (1) and (2)) in training. How much do the prototypes, captured by the latent variable, contribute to performance?
Instead of treating as a marginalized latent variable, we can fix its value to the observed target. 26 must instantiate features (see Table 4) for all 4,194 unique targets observed in training.
Because each of these features needs to be associated with all 877 frames in the partition function of Equation (2), the result is an 80-fold blowup of the feature space (the latent variable model had 465,317 features). Such a model is not computationally feasible in our engineering framework, so we considered a model using only features observed to fire at some point in the training data (called  X  X upported X  features), 72,058 supported features. In Table 5, we see a significant performance drop (on both both with our latent variable model with all features and with only supported features feature set incorporating helpful unsupported features.
 seen, and our full frame identification model only labeled 23.1% of the frames correctly for those unseen targets; in terms of partial match accuracy, the model achieved a score of 46.6%. This, along with the results on the SemEval 2007 unseen targets, shows that there is substantial opportunity for improvement when unseen targets are presented to the system. We address this issue next. 5.5 Semi-Supervised Lexicon Expansion
We next address the poor performance of our frame identification model on targets that were unseen as LUs in FrameNet or as instances in training data, and briefly describe a technique for expanding the set of lexical units with potential semantic frames that they can associate with. These experiments were carried out on the FrameNet 1.5 data only. We use a semi-supervised learning (SSL) technique that uses a graph constructed from labeled and unlabeled data. The widely used graph-based SSL framework X  X ee
Bengio, Delalleau, and Le Roux (2006) and Zhu (2008) for introductory material on this topic X  X as been shown to perform better than several other semi-supervised algorithms on benchmark data sets (Chapelle, Sch  X  olkopf, and Zien 2006, chapter 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and
Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and
Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Subramanya and
Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted natural language types (say, word n -grams or in our case, syntactically disambiguated constrain structured prediction models. Applications have ranged from domain adap-tation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised 2011).
 and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag.
We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin sepa-rately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data.

FrameNet data. For a pair of LUs, we measured the Euclidean distance between their frame distributions. This distance was next converted to a similarity score and inter-details about the interpolation and refer the reader to full details given in Das and Smith (2011).
 interpolated similarity metric. The resulting graph has 64,480 vertices, 9,263 of which are labeled seeds from FrameNet 1.5 and 55,217 of which are unlabeled. Each vertex has a possible set of labels corresponding to the 877 frames defined in the lexicon. Figure 4 shows an excerpt from the constructed graph.
 28 based SSL objective functions. Das and Smith (2012) compare several other graph-based denote the set of all vertices in our graph,  X  V  X  denote the set of all frames. Let N ( v ) denote the set of neighbors of vertex v vertex v  X   X  V , we have a supervised frame distribution  X  q weighted according to the aforementioned interpolated similarity score, denoted w for the edge adjacent to vertices u and v .Wefind q by solving:
NGF -2 :argmin
We call the objective in Equation (4) NGF -2 because it uses normalized probability dis-tributions at each vertex and is a Gaussian field ; it also utilizes a uniform third term in the objective function. This is a multiclass generalization of the quadratic cost criterion (Bengio, Delalleau, and Le Roux 2006), also used by Subramanya, Petrov, and Pereira (2010) and Das and Petrov (2011). Our second graph objective function is as follows: and is a Jensen-Shannon field , utilizing pairwise Jensen-Shannon divergences (Lin 1991;
Burbea and Rao 2006) and a sparse 1,2 penalty (Kowalski and Torr  X  esani 2009) as the third term. Das and Smith (2012) proposed the objective function in Equation (5). It seeks at each graph vertex a sparse measure, as we expect in a lexicon (i.e., few frames have nonzero probability for a given target). These two graph objectives can be optimized by iterative updates, whose details we omit in this article; more information about the motivation behind using the 1,2 penalty in the UJSF -procedure, and an empirical comparison of these and other objectives on another NLP task can be found in Das and Smith (2012). 5.5.3 Constraints for Frame Identification. Once a graph-based SSL objective function is minimized, we arrive at the optimal set of frame distributions q rule, t i is the i th target in a sentence x, and f i is the corresponding evoked frame. We now add a constraint to that rule. Recall from Section 5.2 that for targets with known lemmatized forms, F i was defined to be the set of frames that associate with lemma t in the supervised data. For unknown lemmas, F i was defined to be all the frames in the lexicon. If the LU corresponding to t i is present in the graph, let it be the vertex v such targets t i covered by the graph, we redefine F i as: supervised data), F i is the set of all frames. Note that in this semi-supervised extension of our frame identification inference procedure, we introduced several hyperparam-the graph) and M (the number of highest scoring frames per vertex according to the induced frame distribution). We choose these hyperparameters using cross-validation by tuning the frame identification accuracy on unseen targets. (Different values of the first three hyperparameters were chosen for the different graph objectives and we omit their values here for brevity; M turned out to be 2 for all models.) partial match. Performance is shown on the portion of the test set containing unknown of the set of truncated frame distributions (filtered according to the top M frames in q for a vertex v ) for all the LUs in a graph. For comparison with a semi-supervised baseline, we consider a self-trained system. For this system, we used the supervised frame identification system to label 70,000 sentences from the English Gigaword corpus with frame-semantic parses. For finding targets in a raw sentence, we used a relaxed target identification scheme, where we marked as potential frame-evoking units all targets seen in the lexicon and all other words which were not prepositions, particles, proper nouns, foreign words, or WH-words. We appended these automatic annotations annotated data. These data were next used to train a frame identification model. set-up is very similar to that of Bejan (2009) who used self-training to improve frame approach (Table 6).
 both the supervised model as well as the self-training baseline by a margin of 30 absolute. The best model is UJSF -1,2 , and its performance is significantly better than inducing penalty) than NGF -2 , requiring less memory during frame identification frame components were zero ( q i = 0). The improvements of the graph-based objectives but the best model still has statistically significant improvements over the supervised model (p &lt; 0 . 01). 6. Argument Identification
Given a sentence x = x 1 , ... , x n , the set of targets t = t frames f = f 1 , ... , f m corresponding to each target, argument identification is the task similar to the problem of semantic role labeling, but uses a richer set of frame-specific labels than PropBank annotations. 6.1 Model
Let R f marked as core roles; these roles are conceptually and/or syntactically necessary for any given use of the frame, though they need not be overt in every sentence involving the frame. These are roughly analogous to the core arguments
Non-core roles X  X nalogous to the various ARGM-* in PropBank X  X oosely correspond to cluding relations to other roles in the same or related frames, and semantic types with structural elements comprising the frame lexicon by considering the frame.
 the set of contiguous spans that (a) contain a single word or (b) comprise a valid subtree of a word and all its descendants in the dependency parse produced by the MST parser.
This covers approximately 80% of arguments in the development data for both data sets.
 filled; in the SemEval 2007 development data, the average number of roles an evoked frame defines is 6.7, but the average number of overt arguments is only 1.7. span to S . 30 prediction for each A i ( r k ) (for all roles r k  X  R f
We use a conditional log-linear model over spans for each role of each evoked frame: and ignores all frames except the frame the role belongs to. Our model departs from the traditional SRL literature by modeling the argument identification problem most one overt argument, which is consistent with 96.5% of the role instances in the
SemEval 2007 training data and 96.4% of the role instances in the FrameNet 1.5 full text annotations.
 been used by some previous frame in the sentence (supposing some arbitrary ordering classification approach, 31 permits this sort of argument sharing among frames. Word tokens belong to an average of 1.6 argument spans, including the quarter of words that do not belong to any argument.
 mapping  X  A t for target t . Features for our log-linear model (Equation (8)) depend on the preprocessed sentence x; the target t ;arole r of frame f ; and a candidate argument span s  X  S . 32 For features using the head word of the target t or a candidate argument span s , we use the heuristic described in footnote 21 for selecting the head of non-subtree spans.
 overall biases). The symbol indicates that the feature template also has a variant that is conjoined with r , the name of the role being filled; and indicates that the feature 32 dependency type  X  template additionally has a variant that is conjoined with both r and f , the name of the frame. 33 The role-name-only variants provide for smoothing over frames for common types of roles such as Time and Place ; see Matsubayashi, Okazaki, and Tsujii (2009) for a detailed analysis of the effects of using role features at varying levels of granularity.
Certain features in our model rely on closed-class POS tags, which are defined to be all Penn Treebank tags except for CD and tags that start with features that encode a count or a number are binned into groups: ( [  X  9,  X  5],  X  4,  X  3,  X  2,  X  1, 0, 1, 2, 3, 4, [5, 9], [10, 19], [20, 6.2 Parameter Estimation
We train the argument identification model by:
Here, N is the number of data points (sentences) in the training set, and m is the number of frame annotations per sentence. This objective function is concave. For experiments (Bottou 2004) with no Gaussian regularization ( C = 0). 34 tuning on the development set, and the best results were obtained with a batch size of 2 and 23 passes through the data.
 obtained best results for C = 1 . 0. We did not use stochastic gradient descent for this data set as the number of training instances increased and parallelization of L-BFGS on a multicore setup implementing MPI (Gropp, Lusk, and Skjellum 1994) gave faster training speeds. 6.3 Decoding with Beam Search
Naive prediction of roles using Equation (7) may result in overlap among arguments independently of the others. We want to enforce the constraint that two roles of a single frame cannot be filled by overlapping spans. 35 Toutanova, Haghighi, and Manning (2005) presented a dynamic programming algorithm to prevent overlapping arguments for SRL; however, their approach used an orthogonal view to the argument identi-roles. That formulation admitted a dynamic programming approach; our formulation of finding the best argument span for each role does not.

Algorithm 1. The algorithm produces a set of k -best hypotheses for a frame instance X  X  exponential number of hypotheses. After determining which roles are most likely not incorporating a subset of roles are extended with high-scoring spans for the next role, always maintaining k alternatives. We set k = 10,000 as the beam width. 34
Algorithm 1 Joint decoding of frame f i  X  X  arguments via beam search. top extracts the k most probable spans from S , under p  X  extends each span vector in D 0:( j  X  1) with the most probable non-overlapping span from
S , resulting in k best extensions overall.

Require: k &gt; 0, R f
Ensure:  X  A i , a high-scoring mapping of roles of f i to spans with no token overlap among 1: Calculate A i according to Equation 7 2:  X  r  X  R f i such that A i ( r ) =  X  ,let  X  A i ( r )  X  X  X  5: Arbitrarily order R + f 7: Initialize D 0:0 to be empty 8: for j = 1to n do 10: end for 11:  X  j  X  X  1, ... , n } ,  X  A i ( r j )  X  D 0: n 1 [ j ] 12: return  X  A i 6.4 Results
Performance of the argument identification model is presented in Table 8 for both data sets in consideration. We analyze them here.

SemEval 2007 Data: For the SemEval data set, the table shows how performance varies given different types of input: correct targets and correct frames, correct targets identification task. Given gold targets and frames, our argument identification model (without beam search) gets an F 1 score of 68.09%; when beam search is applied, this increases to 68.46%, with a noticeable increase in precision. Note that an estimated 19% of correct arguments are excluded because they are neither single words nor complete subtrees (see Section 6.1) of the automatic dependency parses. the syntactic parse to determine candidate spans, it could still improve; this suggests 36 that the model has trouble discriminating between good and bad arguments, and that may be beneficial.
 frame parsing performance. There is a 22% absolute decrease in F credit is given for related frames), suggesting that improved frame identification overall performance. Rows 4 X 6 compare our full model (target, frame, and argument
F 1 points for both exact and partial frame matching. As with frame identification, we compared the argument identification stage with that of J&amp;N X 07 in isolation, using the automatically identified targets and frames from the latter as input to our model. As shown in row 5, with partial frame matching, this gave us an F (row 6 in Table 8). This indicates that our argument identification model X  X hich uses a single discriminative model with a large number of features for role filling (rather than argument labeling) X  X s more accurate than the previous state of the art.

FrameNet 1.5 Release: Rows 7 X 12 show results on the newer data set, which is part of the FrameNet 1.5 release. As in the frame identification results of Table 5, we do not show results using predicted targets, as we only test the performance of the statistical models. First, we observe that for results with gold frames, the F with naive decoding, which is significantly higher than the SemEval counterpart. This indicates that increased training data greatly improves performance on the task. We also observe that beam search improves precision by nearly 2%, while getting rid of overlap-ping arguments. When both model frames and model arguments are used, we get an
F 1 score of 68.45%, which is encouraging in comparison to the best results we achieved cation further improves parsing performance. We observe the best results when the
UJSF -1,2 graph objective is used for frame identification, significantly outperforming the fully supervised model on parsing (p &lt; 0 . 001) for all evaluation metrics. The im-provements with SSL can be explained by noting that frame identification performance tification. Figure 5 shows an example where the graph-based model UJSF -an error made by the fully supervised model for the unseen LU discrepancy . frame identification and full frame-semantic parsing. 7. Collective Argument Identification with Constraints
The argument identification strategy described in the previous section does not capture some facets of semantic knowledge represented declaratively in FrameNet. In this section, we present an approach that exploits such knowledge in a principled, unified, and intuitive way. In prior NLP research using FrameNet, these interactions have been largely ignored, though they have the potential to improve the quality and consistency constraint: avoiding argument overlaps. It is, however, approximate and cannot handle other forms of constraints.
 ments of a target given its semantic frame. Although we work within the conventions of
FrameNet, our approach is generalizable to other SRL frameworks. We model argument identification as constrained optimization, where the constraints come from expert knowledge encoded in FrameNet. Following prior work on PropBank-style SRL that dealt with similar constrained problems (Punyakanok et al. 2004; Punyakanok, Roth, and Yih 2008, inter alia), we incorporate this declarative knowledge in an integer linear program.
 decomposition (Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010; Martins et al. 2011a). We derive a modular, extensible, parallelizable approach in which semantic con-straints map not just to declarative components in the algorithm, but also to procedural ones, in the form of  X  X orkers. X  Although dual decomposition algorithms only solve a relaxation of the original problem, we make our approach exact by wrapping the algorithm in a branch-and-bound search procedure. 39 comparison with beam search, which violates many of these constraints, the presented exact decoder is slower, but it decodes nine times faster than CPLEX, a state-of-the-art, proprietary, general-purpose exact ILP solver. 40 38 7.1 Joint Inference
Here, we take a declarative approach to modeling argument identification using an ILP and relate our formulation to prior work in shallow semantic parsing. We show how derive the constraints in our ILP. Finally, we draw connections of our specification to graphical models, a popular formalism in artificial intelligence and machine learning, and describe how the constraints can be treated as factors in a factor graph. not considering its index in a sentence x; let the semantic frame it evokes be f .Tosolely evaluate argument identification, we assume that the semantic frame f is given, which is traditionally the case in controlled experiments used to evaluate SRL systems (M ` arquez et al. 2008). Let the set of roles associated with the frame f be of candidate spans of words that might fill each role is enumerated, usually following an overgenerating heuristic, which is described in Section 6.1; as before, we call this set of spans S . As before, this set also includes the null span denotes that the role is not overt. Our approach assumes a scoring function that gives a strength of association between roles and candidate spans. For each role r span s  X  S , this score is parameterized as: scoring function is identical in form to the numerator X  X  exponent in the log-linear model described in Equation (8). The SRL literature provides many feature functions of this form and many ways to use machine learning to acquire  X  . Our presented method does not make any assumptions about the score except that it has the form in Equation (10). have that: z  X  X  0, 1 } d , where d = | R f | X | S | . z r , s
Given the binary z vector, it is straightforward to recover the collection of arguments identification task can be represented as a constrained optimization problem:
Inthelastline,Aisa k  X  d matrix and b is a vector of length k . Thus, Az k inequalities representing constraints that are imposed on the mapping between roles and spans; these are motivated on linguistic grounds and are described next. Uniqueness. Each role r is filled by at most one span in expressed by: There are O ( | R f | ) such constraints. Note that because extensively in prior literature (Punyakanok, Roth, and Yih 2008, Section 3.4.1). Overlap. SRL systems commonly constrain roles to be filled by non-overlapping spans.
For example, Toutanova, Haghighi, and Manning (2005) used dynamic programming over a phrase structure tree to prevent overlaps between arguments, and Punyakanok,
Roth, and Yih (2008) used constraints in an ILP to respect this requirement. Inspired by the latter, we require that each input sentence position of x be covered by at most one argument of t . We define: We can define our overlap constraints in terms of position i : same effect as beam search, as described in Section 6.3, which tries to avoid argument overlap greedily.
 sentences: (1) A blackberry (2) Most berries meaning. In Example (1), two roles X  X hich we call Entity 1 entities that are similar to each other. In the second sentence, a phrase fulfills a third already captures the function of the former; a similar argument holds for the
Entities roles. We call this phenomenon the  X  X xcludes X  relationship. Let us define a set of pairs from R f that have this relationship: 40
Using the given set, we define the constraint:
O ( | Excl
Pairwise  X  Requirements . X  The sentence in Example (1) illustrates another kind of con-straint. The target resemble cannot have only one of Entity 1
For example, (3) * A blackberry forward. We define the following set for a frame f :
This leads to constraints of the form been used previously in the SRL literature, enforcing joint overtness relationships be-tween core arguments and referential arguments (Punyakanok, Roth, and Yih 2008,
Section 3.4.1), which are formally similar to our example. 7.1.2 Integer Linear Program and Relaxation. Plugging the constraints in Equations 12, 14, 15, and 16 into the last line of Equation (11), we have the argument identification problem expressed as an ILP, since the indicator variables z are binary. Here, apart from the ILP formulation, we will consider the following relaxation of Equation (11), which replaces the binary constraint z  X  X  0, 1 } d by a unit interval constraint z a linear program: spent by the optimization community to devise efficient generic solvers. An example baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17).
Like many of the best implementations, CPLEX is proprietary. 7.1.3 Linguistic Constraints from FrameNet. Although enforcing the four different sets of definitive linguistic information present in the FrameNet lexicon. From the annotated instantiated multiple times by different spans in a sentence. This justifies the uniqueness constraint enforced by Equation (12). Use of such a constraint is also consistent with prior work in frame-semantic parsing (Johansson and Nugues 2007). Similarly, we found that in the annotations, no arguments overlapped with each other for a given target. Hence, the overlap constraints in Equation (14) are also justified. from FrameNet, too. Examples (1) and (2) are instances where the target resemble evokes the S IMILARITY frame, which is defined in FrameNet as: observed, such as Dimension (the dimension along which the entities are similar), so forth. Along with the roles, FrameNet also declares the  X  X xcludes X  and  X  X equires X  relationships noted in our discussion in Section 7.1.1. The case of the is not unique; in Figure 1, the frame C OLLABORATION , evoked by the target partners ,also has two roles Partner 1 and Partner 2 that share the  X  X equires X  relationship. In fact, out of 877 frames in FrameNet 1.5, 204 frames have at least a pair of roles for which the relationship. 7.1.4 Constraints as Factors in a Graphical Model. The LP in Equation (17) can be repre-sented as a maximum a posteriori inference problem in an undirected graphical model. discussed the equivalence of linear programs and factor graphs for representing dis-crete optimization problems. All of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner and Martins et al. The uniqueness constraint in Equation (12) corresponds to an X OR factor, while the overlap constraint in Equation (14) corresponds to an A T M OST O NE factor. The constraints in Equation (15) enforcing the  X  X xcludes X  relationship can be represented with an O each  X  X equires X  constraints in Equation (16) is equivalent to an X factor. 42
Equation (17) using dual decomposition, and how we adapt it to efficiently recover the exact solution of the ILP (Equation (11)), without the need of an off-the-shelf ILP solver. 7.2  X  X ugmented X  Dual Decomposition
Dual decomposition methods address complex optimization problems in the dual, by dividing them into simple worker problems (subproblems), which are repeatedly solved until a consensus is reached. The simplest technique relies on the subgradient algorithm (Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010); as an alternative, Martins et al. (2011a, 2011b) proposed an augmented Lagrangian technique, which is more suitable when there are many small components  X  X ommonly the case in declarative constrained problems, like the one at hand. Here, we present a brief overview of the latter, which is called AD 3 .
 denote by i( m ) the vector of indices of variables linked to that factor. (Recall that each factor represents the instantiation of a constraint.) We introduce a new set of variables, u z , ... ,z M , where each z m  X  [0, 1] | i( m ) | ,andadd M constraints z all the pieces must agree with the witness (and therefore with each other). Each of the M constraints described in Section 7.1 can be encoded with its own matrix A b m (which jointly define A and b in Equation (17)). For convenience, we denote by c the score vector, whose components are c ( r , s ), for each r and define the following scores for the m th subproblem: in the following equivalent form:
The AD 3 algorithm is depicted as Algorithm 2. Like dual decomposition approaches, it repeatedly performs a broadcast operation (the z m parallel, one constraint per  X  X orker X ) and a gather operation (the u-and  X  -updates).
Each u-operation can be seen as an averaged voting which takes into consideration each worker X  X  results.
 ments, which will affect the next round of z m -updates. The only difference with respect z -update also has a quadratic penalty that penalizes deviations from the previous Algorithm 2 AD 3 for Argument Identification
Require: role-span matching scores c : = c ( r , s ) r , s 1: initialize t  X  1 2: initialize u 1 uniformly ( i.e. , u ( r , s ) = 0 . 5, 3: initialize each  X  1 m = 0,  X  m  X  X  1, ... , M } 4: repeat 5: for each m = 1, ... , M do 6: make a z m -update by finding the best scoring analysis for the m th constraint, 7: end for 8: make a u-update by updating the consensus solution, averaging z 9: make a  X  -update: 10: t  X  t + 1 11: until convergence
Ensure: relaxed primal solution u  X  and dual solution  X   X  primal and dual residuals that measure convergence; we refer the reader to that paper for details.
 to one constraint in the ILP, which corresponds to one linguistic constraint. There is no need to work out when , during the procedure, each constraint might have an effect, as in beam search. 7.2.1 Solving the Subproblems. In a different application, Martins et al. (2011b, Section 4) showed how to solve each z m -subproblem associated with the X of the A T M OST O NE factor; a solution with the same runtime is given in Appendix B. 7.2.2 Exact Decoding. It is worth recalling that AD 3 algorithms, solves a relaxation of the actual problem. Although we have observed that therefore it is desirable to have a strategy to recover the exact solution. Two observations 44 are noteworthy. First, the optimal value of the relaxed problem (Equation (17)) provides an upper bound to the original problem (Equation (11)). This is because Equation (11) has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original problem X  X  optimal value. Second, dur-bound search that finds the exact solution of the ILP. The procedure works recursively as follows:
Although this procedure may have worst-case exponential runtime, we found it empir-ically to rapidly obtain the exact solution in all test cases. 7.3 Results with Collective Argument Identification
We present experiments only on argument identification in this section, as our goal is to exhibit the importance of incorporating the various linguistic constraints during our inference procedure. We present results on the full text annotations of FrameNet 1.5, and do not experiment on the SemEval 2007 benchmark, as we have already established our constraint-agnostic models as state-of-the-art. The model weights  X  used in the scoring function c were learned as in Section 6.1 (i.e., by training a logistic regression model to maximize conditional log-likelihood). The AD 3 parameter  X  was initialized to 0.1, and we followed Martins et al. (2011b) in dynamically adjusting it to keep a balance between the primal and dual residuals.
 argument identification approach: 43 3. CPLEX , LP : This uses CPLEX to solve the relaxed LP in Equation (17). To han-4. CPLEX , exact : This tackles the actual ILP (Equation (11)) with CPLEX. 5. AD 3 , LP : The relaxed problem is solved using AD 3 . We choose a span for each role 6. AD 3 , exact : This couples AD 3 with branch-and-bound search to get the exact precision, recall, and F 1 scores. As with experiments in previous sections, we use the evaluation script from SemEval 2007 shared task. Because these scores do not penalize constraint violations, we also report the number of overlap,  X  X xcludes, X  and  X  X equires X  constraints that were violated in the test set. Finally, we tabulate each setting X  X  decoding time in seconds on the whole test set averaged over five runs. 10,000, which is extremely slow; a faster version of beam size 100 results in the same precision and recall values, but is 15 times faster on our test set. Beam size 2 results in slightly worse precision and recall values, but is even faster. All of these, however, result in many constraint violations. Strategies involving CPLEX and AD similarly to each other and to beam search on precision and recall, but eliminate most or all of the constraint violations. With respect to precision and recall, exact AD 46
CPLEX strategies, but is only twice as fast as AD constraint violations. The exact algorithms are slower than the LP versions, but com-exact and LP versions. We found that relaxation was tight 99.8% of the time on the test examples.
 search decoder misses the Partner 2 role, which is a violation, while our AD identifies both arguments correctly. Note that beam search makes plenty of linguistic violations. We found that beam search, when violating many  X  X equires X  constraints, often finds one role in the pair, which increases its recall. AD where beam search finds one role ( Partner 1 ) while AD 3 no roles. Figure 7 shows another example contrasting the output of beam search and
AD 3 where the former predicts two roles sharing an  X  X xcludes X  relationship; AD not violate this constraint and tries to predict a more consistent argument set. Overall, standard measures of accuracy.
 automatic frames as well. When the semi-supervised graph objective UJSF -for frame identification, the performance with AD 3 is only a bit worse in comparison with beam search (row 11 in Table 8) when frame and argument identification are evaluated together. We get a precision of 72.92, a recall of 65.22 and an F search. 8. Conclusion
We have presented an approach to rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on full text annota-tions released along with the FrameNet lexicon, and expedient heuristics. The frame identification model uses latent variables in order to generalize to predicates unseen in either the FrameNet lexicon or training data, and our results show that, quite often, presented an extension of this model that uses graph-based semi-supervised learning to better generalize to new predicates; this achieves significant improvements over the fully supervised approach. Our argument identification model, trained using maximum 48 labeling arguments. Our system achieves improvements over the previous state of the art on the SemEval 2007 benchmark data set at each stage of processing and collectively. We also report stronger results on the more recent, larger FrameNet 1.5 release. incorporating declarative linguistic knowledge as constraints. It outperforms the naive local decoding scheme that is oblivious to the constraints. Furthermore, it is significantly faster than a decoder employing a state-of-the-art proprietary solver; it is only twice as slow as beam search (our chosen decoding method for comparison with the state of the art), which is inexact and does not respect all linguistic constraints. This method is easily amenable to the inclusion of additional constraints.
 set, frame-semantic parsing performance significantly increases when we use the
FrameNet 1.5 release; this suggests that the increase in the number of full text anno-tations and the size of the FrameNet lexicon is beneficial. We believe that with more annotations in the future (say, in the range of the number of PropBank annotations), our frame-semantic parser can reach even better accuracy, making it more useful for NLP applications that require semantic analysis.
 prove the coverage of the frame-semantic parser by improving our semi-supervised learning approach; two possibilities are custom metric learning approaches (Dhillon, Talukdar, and Crammer 2010) that suit the frame identification problem in graph-based in frame identification. The argument identification model might also benefit from semi-supervised learning. Further feature engineering and improved preprocessing,
FrameNet lexicon does not contain exhaustive semantic knowledge. Automatic frame and role induction is an exciting direction of future research that could further enhance our methods of automatic frame-semantic parsing. The parser described in this article is available for download at http://www.ark.cs.cmu.edu/SEMAFOR .
 Appendix A. Target Identification Heuristics from J&amp;N X 07
We describe here the filtering rules that Johansson and Nugues (2007) used for identify-ing frame evoking targets in their SemEval 2007 shared task paper. They built a filtering component based on heuristics that removed words that appear in certain contexts, and kept the remaining ones. 45 These are:
Note that J&amp;N X 07 used a syntactic parser that provided dependency labels correspond-choice (the MST parser) does not provide.

B. Solving A T M OST O NE A T M OST O NE A T M OST O NE subproblems in AD
The A T M OST O NE subproblem can be transformed into that of projecting a point ( a , ... , a k ) onto the set
This projection can be computed as follows: 1. Clip each a j into the interval [0, 1] ( i.e. ,set a j 2. Otherwise project ( a 1 , ... , a k ) onto the probability simplex: Martins et al. (2011b).
 Acknowledgments 50 References 52 54
