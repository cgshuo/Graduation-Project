 Collaborative Filtering (CF) requires user-rated training ex-amples for statistical inference about the preferences of new users. Active learning strategies identify the most infor-mative set of training examples through minimum interac-tions with the users. Current active learning approaches in CF make an implicit and unrealistic assumption that a user can provide rating for any queried item. This paper intro-duces a new approach to the problem which does not make such an assumption. We personalize active learning for the user, and query for only those items which the user can pro-vide rating for. We propose an extended form of Bayesian active learning and use the Aspect Model for CF to illus-trate and examine the idea. A comparative evaluation of the new method and a well-established baseline method on benchmark datasets shows statistically significant improve-ments with our method over the performance of the baseline method that is representative for existing approaches which do not take personalization into account.
 H.4.m [ Information Systems ]: Miscellaneous Algorithms, Design, Theory, Experimentation, Human Fac-tors Active Learning, Collaborative Filtering, Bayesian Active Learning, Aspect model, Personalization
Collaborative Filtering(CF) [1, 2, 3, 4] has now become a popular and important technique for finding user-relevant information, e.g. movies, books and music, based on simi-larity of interests between groups of users. Current state-of-art machine learning algorithms cannot recommend movies, music, and such other items, to users based on their con-tent. CF approaches identify interest groups of users and recommend movies based on memberships of users to differ-ent interest groups. Current CF approaches can be divided into two major categories, model-based and memory-based (also known as instance-based or lazy learning). Memory-based techniques are similar to K-nearest neighbor algo-rithms as learning is delayed till the prediction phase. The model-based approaches make the intuitive assumption that users/items can be grouped based on their interests, and all model-based techniques apply different techniques to dis-cover these latent or hidden interest -groups. Some of the popular model-based approaches are Aspect Model [2, 3], Flexible Mixture Model [4], and the Multiple Cause Vector Quantization model [5].

Active learning [6, 7, 8, 9] aims to train classifiers/models using least amount of training/labeled data, because ob-taining labeled data can be a very costly or infeasible pro-cess. Active learning approaches identify instances from an unlabeled pool which will be very beneficial in train-ing the model, thereby reducing the labeling cost since non-informative instances are not labeled. Active Learning ap-proaches differ mainly in the characterization of what consti-tutes most beneficial training instances. Popular approaches select instances which will lead to maximum reduction in the model uncertainty/entropy [7], or which will lead to maxi-mum reduction in prediction error [6, 10]. Committee-based approaches [9, 10] create an ensemble of classifiers using sub-sets of the training data and identify instances from an un-labeled pool which lead to maximum disagreement in pre-diction among the classifiers. Active Learning has been ex-tensively studied for classification [7, 8, 11], where the goal is to identify unlabeled instances to be labeled according to membership in a class.

Active Learning for Collaborative Filtering is a relatively new concept; here the goal is to solicit ratings for minimal set of movies from a user to learn about the user X  X  preference pattern. This is very essential because when new users join existing models, the system knows little about their pref-erences and the system would like to understand the user preference pattern with the least amount of training exam-ples so as to not annoy the user with lot of questions.
Previous work on Active Learning applied to Collabora-tive Filtering [5, 12], have made one common implicit as-sumption, that users would be able to provide rating to any item that is requested by the system. This assumption, in reality, is not true , because, to rate a movie, a person has to first procure the movie, and watch it. This can be very time consuming, as opposed to, for example, in text classifi-cation where users can quickly label text documents just by skimming through the snippets. Moreover, users would not like a system which solicits ratings for movies the user may not even watch and such a dialog can be frustrating. Since Collaborative Filtering deals with user-personalization, we believe Active Learning should also be personalized to so-licit ratings about items that a user would usually watch. In this paper, we provide one such personalization approach for active learning applied to Collaborative Filtering.
Aspect Model [2, 3] is a probabilistic latent semantic model in which users are considered to be a mixture of multiple interests or aspects. Thus each user u  X  U has a probabilistic membership in each of the aspects, z  X  Z . Furthermore, users in the same interest groups have same movie-rating patterns, thus, users and items (e.g. movies m  X  M ) are independent from each other given the latent class variable, z . Each item m can be rated on a finite scale (e.g. R = { 1 , 2 , . . . , 5 } ), with the rating r  X  R . Thus the probability of each tuple in the dataset can be computed as follows: Equation 1 consists of two parts, p ( r | m, z ) and P ( z | u ). It can be observed that the first term p ( r | m, z ) does not depend on the user and represents the group-specific model. The global-model consists of such group-specific models. The second term P ( z | u ) is the user-personalization term. The user-model  X  u consists of such user-personalization terms i.e.  X  u = {  X  u z :  X  u z = P ( z | u ) ,  X  z  X  Z } .
The Flexible Mixture Model (FMM) [4], is a modified version of the Aspect Model in which there are two layers of latent aspects { z u , z m } , one grouping the users with similar interests ( z u ) and one grouping the items with similar pa-trons ( z m ). The probability of a tuple ( r, u, m ), is factored similar to the Aspect Model: P ( r, u, m ) = X
Both the Aspect Model and the Flexible Mixture Model are probabilistic clustering models and are fit to the data using Expectation Maximization (EM) [13, 2].

Users generally follow different distributions to rate the items. For example, some users provide very extreme ratings while some users provide moderate ratings. For learning models which generalize over such diverse users, the ratings are usually normalized so that ratings for each user have zero mean and unit variance [2].
When new users join a system, although the system al-ready has a strong global-model, the user-model for new users is very weak, since the system has very few ratings available from such new users. Thus, the goal of active learn-ing in CF is to obtain ratings for more items from the new user. Instead of randomly selecting movies for rating from the user, active learning algorithms minimize the number of such queries required to learn a stronger user-model.
One of the popular techniques of active learning is to so-licit a rating for an item which will minimize the expected entropy of the user model. In the context of Collabora-tive Filtering, this leads to the following equation for a user u , where  X  u z denotes the user-group mixing probabilities P ( z | u ), and  X  u z | m,r denotes the model posterior after re-training the user-model based on a newly obtained rating r for movie m from the user i.e. P ( z | u, m, r ):
Equation 3 denotes the expected entropy of the user-model after being trained over additional information of rating movie m with rating r . Since the exact rating r is not known for the unrated movies, the expected value of rating based on the current model P ( r | u, m ) is used, as shown in the equation. Minimizing entropy leads to pure interest groups, in which each user has strict adherence to just one interest group. [12] demonstrate that minimizing entropy is counter-productive for active learning in the collaborative filtering domain, since in reality, users can be a mixture of multiple interests.

As a remedy to this problem, [12] propose a Bayesian Se-lection (BS) approach similar to the active learning approach towards parameter estimation in Bayesian networks [6]. This approach identifies item m , such that the updated model  X  u | m,r will be accelerated towards the true user model  X 
Equation 4 is a negated KL-Divergence equation which is maximized when the estimated distribution is equal to the true distribution being modeled.

Since the true user model is unknown beforehand, it is estimated as the expectation over the posterior distribution of the user model. Similar to equation 3, the rating r is unknown for unrated movies and the expected value is used instead. [12] provides a computationally efficient way of approximating the posterior distribution.
The items selected by the Bayesian selection approach will accelerate the user-model to the true user-model, only if the user rates all the items which the system solicits ratings for. However, in reality, users cannot provide ratings for all items. For example, in reality, users do not watch all movies, do not buy/use all items, and hence cannot provide ratings for all items.

Thus, in addition to selecting the examples which will ac-celerate the model towards the true user model  X  true u , the ac-tive learning algorithm should also try to select items which have a very high probability of getting a rating from a user. Thus we introduce a new term P ( m | u ) into Equation 4, that is the probability of getting a rating, on the item m from the user u . Thus, the new objective function now becomes: m The multiplication of the Bayesian Selection criterion from Equation 4 and the personalization term P ( m | u ) acts as a soft-AND which is maximized when both the multiplicants are maximized. We demonstrate that this is a reasonable ap-proach for Personalized active learning in our experiments. We approximate the probability P ( m | u ) as follows:
It should be noted that we use an indicator function I ( u, m ) for computing P ( m | z ) because, we are interested in comput-ing the probability of getting any rating, instead of P ( r | m, z ), i.e. the probability of getting a particular rating r for a movie m , in which case we would have used P ( r | u, m ). It should also be noted that P ( m | u ) is not computed as an expectation since this term is dependent on the user model only through P ( z | u ) based on the current user-model.
Although our proposed method extends existing Bayesian active learning techniques for Aspect Model, it should be noted that our proposed method for personalization is gen-eral and intuitive and can be applied to any other Active Learning based Collaborative Filtering algorithm.
We use the MovieLens 1 and MovieRating 2 datasets for the empirical analysis of our approaches. The detailed char-acteristics of the two datasets are presented in Table 1.
The datasets were randomly split into training/test sets as shown in Table 2. The Aspect Model is first trained from training set. This gives the global model p ( r | m, z ) as defined in Equation 1. Active learning involves soliciting ratings from new users. The test set forms this set of new users. Thus we start with very few initial ratings for each user, about 3, simulating a new user. The user-model  X  u is learnt from these ratings. For each test-user, remaining movies are split into two sets, the active-selection set and the evaluation set. Active learning algorithms select movies for rating from the set of active selection candidates. In previous approaches, the active-selection set consisted only of movies for which ratings are available in the dataset . This is unrealistic, since a system cannot know beforehand which movies will be rated by the user. This is also evident from the dataset characteristics reported in Table 1 in which www.grouplens.org/system/files/ml-data.zip. This dataset is similar to the EachMovie dataset (prevalent in ear-lier literature) which is no longer available for usage. www.grouplens.org/node/76 http://www.cs.usyd.edu.au/  X  irena/movie data.zip average number of movies rated by a user is less than 10% of the total number of movies. We use a more realistic setting, in which the active-selection set for each test-user consists of all the movies (rated and unrated) which are not included in 1) the initial-ratings and 2) the evaluation set for that user. This makes active-selection more challenging and the setting realistic. The evaluation set is a held-out set of movies for the test-user which is used for evaluating the performance of the Collaborative Filtering system.

We have set the number of user-classes to 5 and 10 re-spectively for the MovieRating and MovieLens datasets as reported in previous empirical studies and Active Collabo-rative Filtering literature [12]. We evaluate the system using two kinds of metrics:
CF performance: This includes the usual CF metrics,i.e. the Mean Absolute Error (MAE) and Mean Squared Er-ror(MSE). MAE and MSE measure the deviation of the pre-dicted ratings of the movies in the test set from the actual ratings available in the dataset. In the equations below, M denotes the evaluation set for the user u and n ( M ue ) denotes the number of movies in the set.
 Since we use multiple test-users, the reported MAE (and MSE) is the average over individual MAE (and MSE) for each test-user as shown in Equation 8, where U t is the set of test users and n ( U t ) denotes the number of test users.
Failures: The system solicits ratings for movies from the user and the user may not provide ratings for some of them. Such instances are known as failures to obtain rating from the user. This metric is very essential for comparison of the personalized approach with previous approaches, since it clearly identifies the number of queries from the active learning algorithms that will go unanswered, and thus not useful for the active learning system. High failure rate also means a system which is likely to annoy the user with ques-tions for which the user does not have any answer. In our experimental setup, a failure occurs whenever the system solicits rating that does not occur in the dataset. The sys-tem thus cannot be re-trained and wastes an active-learning cycle and proceeds to the next iteration.
The Active learning cycle is shown in Figure 1. The global model is learnt over all the training-set users. The active learning cycle starts for each of the test users after the global model is learnt. The system learns a user-model for a test-user based on the available intial ratings, or the labeled-set and the system is then evaluated over the evaluation set. The system then selects a movie for rating from the active-selection set of the user. If the user provides rating for the movie, the system adds this movie-rating pair to MovieRating MovieLens the labeled-set and retrains the system model. If the user does not provide rating for the movie, then it is noted as a failure. The system cannot be retrained in case of a failure. In either case the system is evaluated in terms of MAE and MSE on the evaluation set of the user. This completes one active-iteration. In the next step the system asks rating for another movie from the user and the cycle continues.
For the Aspect Model, we model P ( r | m, z ) with gaussian distributions N ( r ;  X  m,z ,  X  m,z ). We use a multinomial model for the user-group mixing proportions P ( z | u ), as proposed in [2]. Users generally do not follow the same distribu-tion to provide ratings to items, and so it is common in the collaborative filtering techniques to normalize user-ratings to have zero mean and unit variance. It should be noted that the normalization parameters are learnt only over the ratings available to the system, not over the items in the active-selection candidates and evaluation set.
The proposed Personalized Active Learning algorithm is compared against following approaches towards item selec-tion:
We replicate the experiments in [12] with our implemen-tation of the baseline in Figure 2. In our comparative eval-uation, the baseline performs similar to [12] using the ex-perimental setup reported in [12]. It should be noted how-ever that [12] performs active-selection only on the subset of movies for which ratings are available in the dataset, thereby making the assumption that user can rate any movie. We call this setup constrained because the active-selection set is constrained to rated items. This is unrealistic, since the sys-tem will not know beforehand, which items will be rated by the user. Therefore, for comparison with personalized Ac-tive Learning, we will be using the entire set of movies, not just the ones for which ratings are available in the dataset. We call this the unconstrained setup.
 For the sake of conciseness, we call our own approach the Personalized Bayesian Selection (PBS) .
The results for our experiments are presented in Figure 3, for the first 10 active iterations of the system. Figure 2: Constrained Baseline implementation: Active learning trends over 10 active-iterations It can easily be observed that our approach, Personalized Bayesian Selection outperforms Bayesian and Random Se-lection on both datasets using both metrics, MAE and MSE. Moreover, since Bayesian Selection does not take the proba-bility of getting a rating into account, it performs even worse as compared to plain Random Selection. This is expected because Bayesian selection tries to identify items that will provide the most information about user-model, but in the process often selects items which the user would not be able to provide a rating for. This is evident from the Table 3 where we report the mean number of failures, over all active-users for 10 active iterations.
 Table 3: Mean number of Failures over 10 active it-erations (averaged over 300 active-users for MovieR-ating and 600 active-users for MovieLens)
As shown in Table 1, average number of ratings per user is about 87.7 (out of 1000) and 106.05 (out of 1620) for the MovieRating and MovieLens datasets respectively, each user Figure 3: Unconstrained setup: Active Learning trends over 10 active-iterations has on an average only 1 item rated out of 10. Thus, it is ex-pected that random selection has about 9 failures out of 10 active-iterations. Bayesian Selection has more failures than Random Selection because most informative items may not be rated by the user. Personalized Bayesian selection sub-stantially reduces the number of failures and selects atleast 4 out of 10 items for which the active-user can provide a rating.

It should be noted that active CF systems will solicit rat-ings for new movies from the user till the system has sub-stantial number of ratings from the user. A system with higher failure rate will tend to ask more questions to obtain comparable number of ratings from the user. Users typically do not favor systems which ask too many questions. Thus, a personalized active CF system is more likely to be favored by the user than an unpersonalized one.
We conducted paired t-tests [14] for comparing the per-formance of the PBS, BS and RS approaches. Given two systems (say A and B ) and a set of n items, the MAE (or MSE) values were computed for both systems, for each test item, denoted as a i , b i for i = 1 , 2 , . . . , n . We used the pair-wise difference d i = a i  X  b i to examine the null hypothesis that expected difference d = P i d i n is zero against the alter-native hypothesis d &gt; 0, meaning system A is better than system B . The p-value is computed using the t-distribution with the degree of freedom n  X  1 for Table 4: p-values for the significance tests compar-ing various active learning approaches System A System B MovieRating MovieLens RS BS 0.04 0.0019 PBS BS 6.2e-05 2.1e-13 PBS RS 0.0040 8.9e-08
Small p-value (  X  0 . 01) means statistically significant ev-idence against the null hypothesis. Table 4 reports the p-values obtained in our tests using the MAE values of each system after 10 active learning iterations, i.e. after select-ing 10 items and adding their ratings (if rated) to the cur-rent training set. It can be observed that our approach (PBS) significantly outperformed Bayesian Selection (BS) and Random Selection (RS). Random Selection performed better than Bayesian Selection with statistically moderate evidence, i.e., the p-values were less than 0 . 05 and above 0 . 01 for these comparisons. The personalization term P ( m | u ) in the active-selection Equation 7 consists of two terms, P ( z | u ), the user-group mixing probabilities and P ( m | z ), the probability of getting a rating for a movie m in group z .

In the Table 5, we present lists of movies in two exemplary interest-groups learnt for the MovieRating dataset. We show Table 5: Exemplary movie clusters sorted on P ( m | z ) for the MovieRating dataset top 10 movies in each group sorted in descending order of P ( m | z ). It can be observed that cluster A consists of movies which thriller fans will view and cluster B consists of movies which usually science-fiction fans will view. If a user has watched some of the movies in a cluster , it is very likely that the user has also watched other movies in that interest group. Thus it is reasonable to ask the user to rate the other movies in that cluster to learn a stronger user-model. Unpersonalized methods do not follow such an approach and movies will be identified for rating from any of the interest groups, even from those in which the user has never watched any movies. This is the main reason for the higher failure rate of the unpersonalized approaches.
We have shown that personalization of active learning is essential for getting benefit from a Bayesian active learn-ing approach to collaborative filtering. Personalization can be achieved in a relatively simple way: by selecting the most informative items for a user and the likelihood of the user to rate those items. Our experiments on benchmark datasets provide strong evidence for theoritical advantage of our approach over a baseline that is representative for well-established active learning methods without personalization in collaborative filtering. rate those items.

For future work, we plan to incorporate implicit feedback into the model. Thus, when a system selected item fails to get a rating, we can use failure as additional information to update the user-aspect probabilities P ( z | u ). Google re-cently demonstrated techniques for making Aspect Model scalable to the number of news articles for collaborative-filtering based personalization of its news services [15]. A similar scalability study on Active Learning techniques for Collaborative Filtering is required, in addition to the scala-bility analysis of our personalized approach. This work is supported in parts by the National Science Foundation (NSF) under grant IIS-0434035 and IIS-0704689. Any opinions, findings, conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
