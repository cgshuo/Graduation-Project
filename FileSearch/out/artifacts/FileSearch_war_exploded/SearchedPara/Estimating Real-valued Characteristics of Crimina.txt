 Offender profiling concerns making inferences about a crimi-nal from the crime(s) he has committed. Where descriptions of the crimes are recorded electronically, text mining tech-niques provide a means by which recorded characteristics of the offenders can be linked with features of his crimes as revealed in the text. Past studies have used Language Mod-elling to identify characteristics that can be described by a categorical variable e.g. gender. Here we adapt the Lan-guage Modelling approach to allow estimation of numerical quantities such as age and distance travelled.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Measurement, Theory Language Modelling, Bayesian Analysis, Data Mining, Crime Data, Offender Profiling
Police routinely collect large quantities of crime data con-sisting typically of descriptions of how a crime was commit-ted and comprise both free text and structured data. Where a crime has been solved, further data is then available about the offender. The purpose of this study was to use this data for the purpose of offender profiling  X  defined as infering characteristics of offenders from descriptions of the crime or crimes they have committed. Traditionally, offender pro-filing has been conducted subjectively with little empirical basis. Recent work has sought to apply a quantitative ap-proach such as Canter and Fritzon X  X  analysis of arson [3] where one of the characteristics of interest was age. Lan-guage Modelling techniques have also been used for the pro-filing problem where characteristics of the offender were cat-egorical e.g. gender or ethnic group [1, 2] where the descrip-tion of the crime was reduced to a bag of words. However such an approach is problematic for charcteristics defined on a numerical scale such as age or distance travelled. Thus we attempt here to modify the Language Modelling approach to allow estimation of a continuous rather than categoric variables. Instead of there being two or more competing language models which could have generated the document in question we assume a single model with a parameter and seek to estimate this parameter.

The work presented here was part of the UK Government funded project, iMOV (Interactive Modus Operandi Visual-isation). The project is a collaboration between computer scientists and psychologists; the mathematical models de-veloped are therefore rooted in the theories of Investigative Psychology.
It is assumed that the crime description has been rendered into a bag of words comprising both free text and structured data held in a police data archive. A lemmatiser based on WordNet [4] was used and stopwords were removed. We can then identify three distinct steps when applying traditional Language Modelling to a classification problem and this pro-vides a useful template for explaining continuous language models.
Continuous language models seek to estimate some at-tribute relating to the document and thus we assume instead a single language model with a parameter v representing age, distance or some other quantity. Without loss of generality, we will henceforth consider age as the parameter of interest. Using the above template, firstly, we use logistic regression to assign a function to each term in the vocabulary; this function yields a probability for a given age. The second step produces a probability of the document being gener-ated as a function of age by multiplying the probabilities of each term. Finally we use the continuous version of Bayes X  Theorem by assuming a prior distribution of ages over solved offences and calculating the posterior distribution of the age of the offender of the crime under analysis. A point esti-mate can be made by, say, taking the mean of this distribu-tion. Although there are two varieties of unigram language model used on bag-of-words data: multinomial and multiple Bernoulli, the approach described here only works with the latter.
We can express the probability of each term in the vocab-ulary as a function of the parameter v representing age of the offender.
 This function, which yields a value between 0 and 1, must be estimated from some set of training data. The training data for each term t i will consist of a value for v and the value 0 or 1 depending whether the term is present or absent in each training document. So, since we wish to estimate a probability function from Boolean data, we use logistic regression with an affine function:
It is then possible to estimate the values of a i However the approximation algorithm may fail for certain term, which are either very sparse or alternatively occur in all documents (such as certian terms form structured data. These terms are disregarded.
The probability of the language model generating the doc-ument (or strictly speaking its index) for a given value of v may be written as: Since v is modelled as a continuous variable, the continuous form of Bayes X  theorem gives us: Here P ( D ) can be calculated by assuming that the area un-der the distribution pdf ( v | D ) will be 1. The prior distribu-tion pdf ( v ) can be estimated from the training data. Since here we wish only to calculate the mean of the posterior dis-tribution, the fact that this consists of a number of discrete points and is thus rather lumpy does not matter. However, if we wished to produce a graphical representation of the likely ages (or distances) a continuous distribution could be fitted.
The purpose of the experiment outlined here was to demon-strate firstly that the continuous language models have pre-dictive power and secondly that they perform comparably with a dichotomous model comprising two constant language models for categories defined by splitting the continuous variable at the median. For reasons of space the full results cannot be given. The data was drawn from a digital police archive from a large city for crimes reported over a num-ber of years; both age and distance travelled to the crime were estimated using the continuous language model. Ten datasets were identified by crime type, nine with the age characteritic and one with distance travelled.

The leave-one-out (LOO) or jack knife strategy was used to partition test and training data. Usually, this technique consists of removing each point from the data set and using all the remaining data to train the model. The model is thus tested on each removed data point in turn. However, this potentially leads to the serial crime problem where several crimes are committed by the same offender and thus the training set contains crimes by the same offender as the test set. Although we expect to find commonalities in behaviour between people of the same age, this will never be as strong as the behavioural consistency of one individual. Thus, the test set comprises all the offences by one offender and the training set comprises all offences committed by other of-fenders. This procedure is then run once for each offender.
The constant and continuous Language Models yield dif-ferent results. The constant models predict a category whereas the continuous model predicts a distribution of values from which we may take some measure of central tendency such as the mean. Thus we compared the chi-squared significance of the 2  X  2 contingency table against Pearson X  X  correlation co-efficient between the mean estimated age and the actual age. Seven data sets showed signifcance at 5% for both models. Two were significant for the continuous model only  X  this included the set containing distances. This shows that the continous model finds correlations where the dichotomous model cannot. We would like to thank Prof. David Canter and Dr. Donna Youngs for many interesting discussions which as-sisted development of this model and also for providing the data on which the model was tested. We would also like to thank Mark Baillie for his useful comments on the paper. [1] R. Bache, F. Crestani, D. Canter &amp; D. Youngs, A [2] R. Bache &amp; F. Crestani, Towards an Automated [3] D. Canter &amp; K.Fritzon, Differentiating arsonists: A [4] C. Fellbaum (Ed.): WordNet -An Electronic Lexical
