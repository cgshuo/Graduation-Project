 Query logs, the patterns of activity left by millions of users, con-tain a wealth of information that can be mined to aid personal-ization. We perform a large-scale study of Yahoo! search en-gine logs, tracking 1 .35 million browser-c ookies over a period of 6 months. We define metrics to address questions such as 1) How much history is available?, 2) How do users X  topical interests vary, as reflected by their queries?, and 3) What can we learn from user clicks? We find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user. We show that users exhibit consistent topical interests that vary between users. We also see that user clicks indicate a variety of special interests. Our findings shed light on user activity and can inform future personalization efforts.
 Categories and Subject Descriptors: H.4 [Information Systems Applications]: Miscellaneous General Terms: Algorithms, Experimentation.
 Keywords: Personalization, Query Logs, User History, User Inter-ests, Categorization, Clustering.
Personalization holds much promise for dramatically increasing the impact of online services such as web search. Query logs, the patterns of activity left by millions of users, contain a wealth of information that can be mined to aid personalization. We perform a large-scale analysis of Yahoo! search engine logs, tracking 1.35 million browser-cooki es over a period of 6 months. We define and track measures to address three types of questions: 1) The extent of short and long term history available, 2) Consistency and con-vergence rate of users X  general topical interests as reflected by their queries, and 3) Finer grain information available on user interests derived from users X  clicks on search results. We find that there is significantly more expected history for the user of a random query than for a random user. We show that users exhibit consistent top-ical interests that vary between users. We also see that user clicks can reveal users X  special interests. Such information makes per-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. sonalized applications possible, including web search, targeted ad-vertising, and recommendations. We briefly describe an important application next: web search.

Interacting with search engines has traditionally been an imper-sonal affair, with the returned results a function only of the query entered. Unfortunately the average query length is consistently re-ported to be around two, so many queries are too short to disam-biguate the user X  X  information need. Moreover, users often view only the first page of results, which makes precision critically im-portant. These limitations have motivated researchers to look be-yond the query and consider how a search X  X  context can provide further evidence about the user X  X  information need. A broader con-text could include features such as the user X  X  geographic location, the time and date, and the user X  X  previous interactions with the search engine [9]. A user X  X  interactions with a search engine con-sist of performing searches and clicking on result pages. A user X  X  prior actions make up her history .Wedefine contextualization as integrating a user X  X  history into the results ranking. We break contextualization into two types: 1) personalization is considering a user X  X  long-term interests, and 2) adjustment is reacting to the user X  X  short-term action history. Personalization and adjustment are complementary approaches to integrating user history. With adjustment, a search engine could quickly react to a user X  X  actions. This would require very little prior history, perhaps starting as soon as the second query. As a polysemous word,  X  X aguar X  is an in-herently ambiguous query. In an effort to cater to all users, ma-jor search engines distribute their top 10 results among the various meanings (car, cat, football, osx, et c). More relevant results could be provided on the first page if such a query was disambiguated. Adjustment could make this possible. For example,  X  X aguar X  is no longer ambiguous when immediately preceded by the queries  X  X mw X  and  X  X ercedes X . In contrast, personalization requires hav-ing some length of history available. However integrating long-term interests could address the  X  X old-start X  situation when a user searches after a period of inactivity, or begins a new search need. For example, we suspect our reader s might appreciate it if results from Citeseer X  X  domain tended to be ranked higher for them. Since web search sessions are typically short, a significant portion of queries will fall into this cold-start situation. There have been numerous analyses of query logs [1, 5, 15, 6]. Much has been published with relation to the distribution of queries according to aspects such as query length and query frequency [13, 5, 6], and query type and topicality [2, 15]. Other work has focused on user behavior at the query session level, showing aspects such as reformulation rates can be relatively high [16]. Our study comple-ments previous research by focusing on users that issue the queries over a long period of time. For the purposes of exploring person-alization (or more generally cont extualization) opportunities, we group the queries by users that issue them. There is also much work on personalization in general, as well as personalization to aid web search in particular. To achieve personalization user profiles are created from explicit participation and/or implicit (behavioral) user feedback [10, 4, 3, 8, 14], and researchers have identified various distinctions such as short versus long term profiles, incorporating context, and different types of profiles such as content-based versus collaborative information [7, 9]. The use of learning from impres-sions and clicks to improve the ranking is also an exciting direction [10, 11, 9]. Our present study evaluates personalization opportu-nities via users X  queries and clicks (implicit feedback) and should inform future personalization research.
Our data source was six months of query logs from the Yahoo! search engine. We tracked two types of actions: searches and clicks. A search is followed by zero or more clicks on results pages. All actions record the timestamp, browser cookie, IP address, and the associated query. If a user was logged in, then a unique identi-fier corresponding to their Yahoo! account was also recorded. Click actions also store the destination url, and that url X  X  ranking within the search results
User tracking lays the foundation for contextualization. We must be able to record the user X  X  actions. Websites generally have three identifiers available for tracking users: IP address, browser cookie, and login account. Each of these tracking methods represents a tradeoff between coverage, accuracy, and persistence. A complete personalization system could potentially use a combination of these tracking methods. In this study we track users with browser cook-ies. Cookies seem to provide the most appealing tradeoff between coverage and accuracy. Persistence is a concern for long-term per-sonalization, and we further examine this in the next section. Track-ing by cookies also offers the best chance that this study X  X  results will be generally applicable. The cookie behavior of Yahoo users should reflect what many websites experience. This would not be true for user accounts.

Our data was a random sample of the cookies active on the Ya-hoo! search engine over a period of six months. We used our origi-nal sample and a subsample of it in this study. The original sample contained about 1.35 million cookies , 26 million sear ches, and 20 million clicks. Some of our tests required considering only users with a minimal amount of action history. For these tests, we de-fined a high activity user subsample, which contains only the users who submitted 20 or more searches, and whose activity ranged over 30 or more days. There were 235,183 cookies with more than 20 searches, and 190,445 cookies activity spanning 30 days. The inter-section formed our high activity sample, containing about 115,000 cookies, 14 millio n searches, and 11 million clicks . Figure 1 com-pares the samples. The 8% of cookies in the high activity sample account for 50% of all user activity.
Contextualization requires a record of the actions a user has per-formed. Within search engines this is a log of queries and clicks. We examine the amount of available user history in three ways. First we measure cookie persistence to give us an idea of how much time history is available. Second, we look at the user activity distri-bution, observing how much action history can be expected. Third, we consider the interplay between time and actions. We look at how much action history is available within fixed time windows.
Having chosen cookies as our method for tracking users, our first concern is cookie expiration. The amount of historical data we can collect is proportional to cookie persistence. For example, if 75% of cookies expire after one day, then adjustment will be the only contextualization option for a majority of users. To measure cookie persistence over time we look at how many days a cookie remains active. A cookie is active between its first and last observed actions. For the cookies active on the first day of our sample, we measure how long they remained active over the following six months. We also did the equivalent test for the set of cookies active on the last day, looking backwards in time. There were 33,836 cookies active on the first day, and 31,048 on the last.

Figure 2 plots the days in our sample on the x-axis, and the y-axis shows the percent of active users remaining. The first thing to notice is the steep initial drop of about 30% after the first day. After this the curve quickly becomes a linear decay function. Note that the curves artificially converge to zero at the ends. This is due to the finite nature of our sample. Most of those users were probably still active, but were cut off by the end of the time frame. We expect the linear attrition rate would continue without this horizon limitation. While looking forward gives us a feeling for cookie persistence, it isn X  X  the view available during contextualization. Instead a system has to react at query time, looking backwards through the user X  X  history. The figure shows that the forward and backward cookie decay functions are symmetric. Figure 3 shows the percent of users that will still be active if we look back N days. For example, of the users active on the last day, about 40% of cookies were at least a month old. This diagram shows that a notable portion of the cook-ies persist over significant periods of time. Although 30% expire after the first day, over 40% of cookies persist for at least a month.
We have seen that some cookies persist over months. However, time alone cannot help our contextualization efforts. Instead we must learn from the user X  X  actions: searches and clicks. We know there is an average of 19 searches per cookie. In this section we take a more granular look at the distribution of how much history users accumulate before their cookie disappears. Since there is a fairly consistent ratio between searches and clicks, we only refer to searches here. The results will apply equally to both. Figure 4 is a plot showing the number of searches N on the x-axis, and the number of users who performed exactly N searches on the y-axis. It is a log-log plot, so the curve fits a power law well. This means that a significant majority of users perform very few searches over the lifetime of a cookie. In fact, 25% of observed cookies only had one search, 59% had five or less, 82% did fewer than the average of 19 searches, while 17% did more. How will this power-law activity distribution impact our personalization aspirations? The situation may seem grim at first. It is probably safe to say that five queries over six months are insufficient for personalization. Alternatively, we could ask what percent of searches come from users with suffi-cient history. In other words, what percent of the query stream can we personalize?
Figure 5 again shows the number of searches N on the x-axis, this time with the y-axis showing the percent of queries coming from users who performed at least N searches. This is like picking a random search from the data sa mple and finding the probability that the search X  X  user performed at least N searches overall. For ex-ample, we can see that about 50% of the query stream comes from users who performed at least 100 queries over the 6 month period (see also Figure 3). The amount of user history looks much more promising from this query stream perspective. Figure 4 equally weighs every cookie, where Figure 5 equally weighs every search. We believe the second view is more important for evaluating per-sonalization potential. Due to the power-law distribution, 50% of the searches come from only 3.5% of observed cookies. Most cook-ies don X  X  come back. Either the users were fleeting visitors, or their cookie was invalidated for some reason.
Recent history is used for adjustment, and long-term history is needed for personalization. In this section we look at how much history is available at query time, considering time windows of various sizes. As in the previous section, we will only discuss searches; the results apply equally to clicks. Assume you used a search engine once on a particular day. Over a period of 10 minutes you submitted five searches. For each search submission, a contex-tualization system would look back in time at your prior actions. Assume the system only considers the last 30 minutes for adjust-ment. Then there were five 30 minute time windows, each ending at the time of a search. The window for the first search contained zero prior searches, and the last window contained four. On that day you had an average of three searches per 30 minute time win-dow. We measured the activity distribution within time windows of varying sizes: 15 minutes, one hour, one day, and one month. Natu-rally the shorter windows are more relevant for adjustment, and the one month window is more relevant for personalization. We ran these tests on our high activity user sample. Each search has four associated time windows: one of each size. To collect the data, we skipped the first month of searches. This was necessary because at the start of our sample time period we don X  X  have a month X  X  worth of historical data. For the remaining 5 months, we recorded the query distributions for every time window.

Figure 6 is a log-log plot with the x-axis showing the N ,the number of searches, and the number of windows containing ex-actly N searches. The window sizes shown are 15 min, one hour, one day, and one month. You can see that it fits a power-law dis-tribution reasonably well. This means that most sessions contain few queries. This is what we might expect. The windows with very high query counts were probably the result of automated searches.
Figure 8 shows a cumulative view of this data. It shows the num-ber of searches ( N ) on the x-axis, and the y-axis shows the percent of windows containing at least N searches. For example, we can see that the one month time window of 35% of searches will contain 10 or more earlier searches. Figure 7 displays this data in tabular form. For active cookies, sessions appear to be long enough for adjustment to be a viable goal.
The idea of personalization is grounded on two related assump-tions about user behavior. Our first assumption is that users have reasonably consistent interests. A user X  X  history will only be useful if her previous actions help us predict her future interests. This will be true if her interests are consistent over time. Our second assump-tion is that users have different interests from one another. After all, if everyone has the same interests there is no need for personaliza-tion! We explore these themes in terms of the general topics of the queries users submit. Note that knowing the range of topics that a user is primarily interested can have a number of personalization oriented applications, such as reordering returned pages as well as displaying ads of interest [9]. We employ a query categorizer to determine the topic(s) of each query. Based on query topics, we estimate an interest distribution for each user. We examine whether and how fast individuals X  interest distributions converges. We find positive evidence for convergence. We next check whether interest distributions are different across users (distinctness).
We used 22 general topical categories ( X  X ravel X ,  X  X omputing X ,..), and utilized an automated query categorizer obtained via machine learning techniques [17, 12]. The categorizer assigned a confidence probability to each cate gory it suggested. It d ropped categories with less than 10% confidence. The categorizer is imperfect: it does not have complete coverage. Furthermore, its coverage and errors for various topics could be different. A user X  X  interests are reflected by her queries over time. The categorizer provides a probability vector (category probability assignment) for each individual query. We summed the probability vectors of all the queries of a user, and l normalized the total vector ( i.e. , x | x | 1 ,where | x obtain the final distribution or interest distribution F for each user. This vector, if computed over sufficiently large number of queries, is a reflection of the proportion of time the user spends querying on each topic, or the stationary distribution of the user X  X  topical in-terests. However a user X  X  interests may drift and she may not have a stationary distribution. The population X  X  global (interest) distri-bution G is the normalized sum over all queries from all users in the sample 1 . In essence, this represents the interest distribution of
In computing G each query gets an equal weight. We also looked at the sum where each user gets equal weight, and the results were nearly identical.
Figure 7: % of Time Windows Containing at Least N Queries. the entire population. Some exampl e topics with their probabilities in G are: (Travel,0.116) , (Toys and Hobbies,0.073) , (Health and Beauty, 0.068) ,and (Automotive, 0.066) .
Common distance measures for distributions include KL-Divergence and those based on l 1 and l  X  ( | x |  X  =max i | x i | ,and d | x  X  y | p ). KL-Divergence is not as easily interpretable, and pun-ishes severely (assigns large distance) for near zero probabilities, thus we opted for the d p distances. The d  X  distance amounts to taking the maximum difference over the dimensions (the 22 cate-gories), while d 1 distance amounts to taking the sum of the absolute differences over the dimensions.

If a user X  X  queries were drawn from a fixed distribution, the ob-served distribution will eventually converge to this underlying dis-tribution. In Figures 9 and 10 we plotted distribution distances to check whether this convergence occurs. The distances for two users, one with approximately 50 and another with 200 queries are shown. The Y axis shows the distribution distances (both d and d  X  ). One curve shows the distance between the user X  X  current cumulative distribution ( C ) and the user X  X  final distribution ( F ). The other curve shows the distance between C and the population X  X  global (interest) distribution G . The initial drops in both distances is due to the fact that with small query samples the probabilities are skewed. If a user has interests different from TD, we should see a rapid stabilization of the distance of C from TD, and this appears to be the case for user 2. C equals F after all queries of the user have been observed ( C is guaranteed to converge to F ). However, if we see a relatively rapid decrease in d p ( C,F ) to close to 0, then that X  X  evidence that the user has a stationary distribution and that F (for the queries we have computed) is close to it. This may be the case for user 2. We saw a similar pattern for several users X  con-vergence profiles that we visually examined: below approximately 100 queries, there was no visual evidence of either hypothesis (dif-ference from G or stationary F ), but with more than 100 queries, there is some evidence.

Another way to test consistenc y involves chronologically split-ting the user X  X  queries into a first half and a second. If a user X  X  inter-est distribution is consistent in time, then we expect the sets to have similar category distributions. The halves should be more similar than the halves of different users. A consistent user X  X  two halves would also be closer to each other than to the global distribution G. Figure 11 shows the distances as a function of subpopulation of users with a threshold on number of queries. We observe that while both distance to G and between halves decrease, the difference and the ratio of the two increases significantly as we restrict to popu-lation of users with more queries. This chart presents compelling Figure 9: Evolution of the distance of the cumulative distribution for Figure 10: Evolution for User 2 with 200 queries. There is strong evidence that users tend to be consistent (have a stationary interest distribution), that with 100s of queries, the F we obtain is close to the user X  X  stationary distribution, and that such F is different from G.

The goal of personalization is to provide different content to users with different interests. The value of such personalization is limited by how different users are from each other. As users appear different from global distribution G, that is good evidence that users must be different from each other (there are at least two clusters of users). We measured d 1 and d  X  between the first-half of a pair of randomly picked users. For the populations that we computed such distances, the distances were significantly higher on average than the distance between the two halves of the same user from the same population. For example, for pairs of users with at least queries each, the average d 1 and d  X  were respectively 1 0 . 32 (compare with d s 1 =0 . 74 and d s  X  =0 . 177 for users with 75 queries in table 4.2), and for users with at least 500 averages were 1 . 076 and 0 . 244 (compare with d s 1 =0 . d  X  =0 . 114 for users with 400 queries in table 4.2).

Let us briefly and informally discuss the effect of error of the query categorizer on our results. There are three type of errors that the categorizer can make: random, systematic, and malicious. Random errors include cases when an arbitrary wrong category is picked for a given query. Systematic error includes cases in which some categories may have low coverage or recall relative to others, or when a category is a false positive disproportionately higher than other categories (wrong queries are disproportionately assigned to such a category). Malicious error includes the case in which the categorizer knows what we are computing and systematically mis-classifies some queries so that the errors are in a direction that we cannot anticipate. Malicious error is not possible for our catego-rizer. We remark that both random and systematic error tend to make users look consistent but also more like one another. Since we X  X e seen evidence that users X  interests are different from G and one another, we believe our conclusion that users tend to be consis-tent is sound as well.
Are some clicks more informative than others? Intuitively this appears to be the case. For example, a click on acm.org seems to tell us much more about the user X  X  interests than a click on ya-hoo.com. It would be useful to have a metric quantifying how in-formative a click is. There is an abundance of context for every click. In this section we abstract away most contextual details and only consider the user X  X  identity (cookie) and the domain of the clicked url. Looking at domains instead of full urls has the benefit of reducing sparsity. Our high activity user sample contains about 6 million unique urls, and only 1.5 million unique domains.
How can we quantify the amount of information in a click? The goal of contextualization is to predict a user X  X  future interests and actions. So there are two requirements: a click is informative if 1) it presents us with new information, and 2) this information helps us predict the user X  X  future actions. In our example, the click on yahoo.com doesn X  X  provide much new information about the user. This is because the domain is popular. In information theory, the amount of information gained from observing an event is propor-tional to how surprising the event was. Applying this to clicks, the prior probability that a user will vi sit yahoo.com is already high, so observing that click has little impact on our existing beliefs. This reasoning leads to the idea that clicks on rare domains will be more informative than clicks on popular ones.

Although rarity is a necessary condition, not all clicks on rare domains are informative. Even if we weren X  X  expecting the domain to be clicked, this new knowledge is only helpful if it improves our ability to predict the user X  X  actions. Consider when a user clicks on a spam site. Perhaps the page title was misleading. Can we learn much from such a click? Probably not. While this is an extreme example, to differing degrees many clicks are not what the user hoped for. Click data is very noisy. We are seeking out the clicks that have predictive power. We define a click X  X  predictive power as proportional to the proba bility that a user will re turn to the clicked domain. In other words, a click on a sticky domain enables us to predict that the user will come back to that specific domain. So the most informative clicks are on rare and sticky domains. These concepts are not orthogonal. A popular domain is inherently sticky due to the volume of traffic it receives. This suggests that domains fall into one of three general categories: 1) Popular and sticky, 2) Rare and sticky, and 3) Rare and unsticky.
Rare domains are ones that few users ever click on. Sticky do-main are those that users often return to, if they visit a first time. To find domains which are both rare and sticky, we need to define a concrete measurement for each. Let P ( C d 1 ) be the probability of a user clicking on domain d at least once. We use this value as a measurement of a domain X  X  popularity. Let P ( C d n | C d probability of a user clicking on domain d an n th time, given the first n  X  1 clicks. To quantify a domain X  X  stickiness, we simply use P ( C d 2 | C d 1 ) . These probabilities can easily be estimated from our data sample. Let | C d 1 | be the number of users who clicked on a Figure 11: Chart of average distances between first half distribution F domain d and let | sample | be the number of users in the entire sam-ple. Then P ( C d 1 ) can be estimated as | C d 1 | / | sample P ( click on the same url multiple times during a search. We filtered these repeat clicks. We also filtered clicks on the same domain within a certain period of time. We wanted to find the domains that users returned after their current information need passed. We tried values of 15 minutes, one hour, and one day, with similar results for all. We applied our metrics to the high activity user sample. We classified a domain d as rare if P ( C d 1 ) &lt;. 1 . In other words, rare domains were defined as those that were clicked by fewer than of all users. Next we dropped any domains with | C 2 | X  10 to statistical significance reasons. The remaining domains formed the candidate set (the set of rare domains with enough data for sig-nificance). The candidate set contained 95,529 domains. We then decided that domains with P ( C d 2 | P ( C d 1 )  X  10% would be labeled as sticky. This left 6,035 domains that were both rare and sticky.
Many of the results fall into what may be called special inter-ests . They are not generally popular, but people have a clear rea-son to return. Special interest domains fall under several cate-gories: Ethnic (persianblog.com), communities (cupid.com, chris-tiancafe.com, tagged.com), many state lotteries (calottery.com), fi-nance (navyfcu.org, providianonline.com), children (neopets.com, cartoonetwork.com), specialty news (drudgereport.com, rep-am.com), online games (runescape.com), specialty sales (dreamhorse.com), soap operas (soapzone.com), pornography, etc. Our click informa-tion metric could be immediately applied as a simple heuristic for result re-ranking. A search result could be bumped up from rank 100 to the front page if it was a special-domain that the user had clicked on. For example, we suspect our readers might appreciate it if results from Citeseer X  X  domain tended to be ranked higher for them. With short-term adjustment, a single click on Citeseer could dramatically improve results for the rest of your search, even with-out any prior user history. Clicking on a domain often implies an interest in additional related domains. A related application of such click information would be collaborative filtering.
We explored three main questions: the extent of history avail-able, the convergence of users X  queries to topical distribution pro-files, and the information about special interest sites or stickiness of rarely clicked sites via patterns of repeated clicks. We found that users X  topical interest distributions appears to become distinct from the population, and converge to a stationary distribution, but only after a few hundred queries. Future work in this direction includes using larger sets of categories, different types of classes (such as  X  X avigational X  versus  X  X nformational X ), as well as exploring uses of such interest distributions fo r clustering the users. We showed how the pattern of repeated site clicks helps identify special inter-est sites. This information may improve web search ranking and in effect serve as implicit bookmarks, o r help identify communities of users with special interest and inform collaborative filtering. Web search is likely to remain a major window onto users X  interests and needs, and there is much more to be done in effectively utilizing query logs.

