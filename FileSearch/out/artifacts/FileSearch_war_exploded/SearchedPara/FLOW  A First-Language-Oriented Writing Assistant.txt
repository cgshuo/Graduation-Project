 Writing in a second language (L2) is a challenging and complex process for foreign language learners. Insufficient lexical knowledge and limited exposure to English might interrupt their writing flow (Silva, 1993). Numerous writing instructions have been proposed (Kroll, 1990) as well as writing handbooks have been available for learners. Studies have revealed that during the writing process, EFL learners show the inclination to rely on their native languages (Wolfersberger, 2003) to prevent a breakdown in the writing process (Arndt, 1987; Cumming, 1989). However, existing writing courses and instruction materials, almost second-language-oriented, seem unable to directly assist EFL writers while writing. 
This paper presents FLOW 1 (Figure 1), an interactive system for assisting EFL writers in 
In this paper, we propose a context-sensitive disambiguation model which aims to automatically choose the appropriate phrases in different contexts when performing n-gram prediction, paraphrase suggestion and translation tasks. As described in (Carpuat and Wu, 2007), the disambiguation model plays an important role in the machine translation task. Similar to their work, we further integrate the multi-word phrasal lexical disambiguation model to the n-gram prediction model, paraphrase model and translation model of our system. With the phrasal disambiguation model, the output of the system is sensitive to the context the writer is working on. The context-sensitive feature helps writers find the appropriate phrase while composing and revising. 
This paper is organized as follows. We review the related work in the next section. In Section 3, we brief our system and method. Section 4 reports the evaluation results. We conclude this paper and point out future directions to research in Section 5. 2.1 Sub-sentential paraphrases A variety of data-driven paraphrase extraction techniques have been proposed in the literature. One of the most popular methods leveraging bilingual parallel corpora is proposed by Bannard and Callison-Burch (2005). They identify paraphrases using a phrase in another language as a pivot. Using bilingual parallel corpora for either a word or a phrase, could be source or target languages. 2.3 Multi-word phrasal lexical disambiguation In the study more closely related to our work, Carpuat and Wu (2007) propose a novel method to train a phrasal lexical disambiguation model to benefit translation candidates selection in machine translation. They find a way to integrate the state-of-the-art Word Sense Disambiguation (WSD) model into phrase-based statistical machine translation. Instead of using predefined senses drawn from manually constructed sense inventories, their model directly disambiguates between all phrasal translation candidates seen during SMT training. In this paper, we also use the phrasal lexical disambiguation model; however, apart from using disambiguation model to help machine translation, we extend the disambiguation model. With the help of the phrasal lexical disambiguation model, we build three models: a context-sensitive n-gram prediction model, a paraphrase suggestion model, and a translation model which are introduced in the following sections. The FLOW system helps language learners in two ways: predicting n-grams in the composing stage and suggesting paraphrases in the revising stage (Figure 2). 3.1 System architecture Composing Stage During the composing process, a user inputs S . FLOW first determines if the last few words of S is a L1 input. If not, FLOW takes the last k words to predict the best matching following n-grams. Otherwise, the system uses the last k words as the query to predict the corresponding n-gram translation. With a set of prediction (either translations or n-grams), the user could choose an appropriate suggestion to complete the sentence in the writing area. Revising Stage In the revising stage, given an input I and the user selected words K , FLOW obtains the word sequences L and R surrounding K as reference for prediction. Next, the system suggests sub-sentential paraphrases for K based on the information of L and R . The system then searches and ranks the translations. 3.2 N-gram prediction In the n-gram prediction task, our model takes the last k words with m 2 English words and n foreign source sentences S as the input. The output would be a set of n-gram predictions. These n-grams can be concatenated to the end of the user-composed sentence fluently. Context-Sensitive N-gram Prediction (CS-NP) The CS-NP model is triggered to predict a following n-gram when a user composes sentences consisted of only English words with no foreign language words, namely, n is equal to 0. The goal of the CS-NP model is to find the English phrase e that maximizes the language model probability of the word sequence, { e 1 , e 2 , ... e m , e }: Translation-based N-gram Prediction (TB-NP) When a user types a set of L1 expression f = { f 1 , f 2 ... f n }, following the English sentences S , the FLOW system will predict the possible translations of f . A simple way to predict the translations is to find the bilingual phrase alignments T( f ) using the method proposed by (Och and Ney, 2003). However, the T( f ) is ambiguous in different contexts. Thus, we use the context { e 1 , e 2 , ... e m } proceeding f to fix the prediction of the translation. Predicting the translation e can be treated as a sub-sentential translation task: pivot-based method proposed by Bannard and Callison-Burch (2005). Although the pivot-based method has been proved efficient and effective in finding local paraphrases, the local paraphrase suggestions may not fit different contexts. Similar to the previous n-gram prediction task, we use the na X ve-Bayes approach to disambiguate these local paraphrases. The task is to find the best e such that e with the highest probability for the given context R and L. We further require paraphrases to have similar syntactic structures to the user-selected phrase in terms of POS tags, Pos . Translation-based Paraphrase Suggestion (TB-PS) After the user selects a phrase for paraphrasing, with a L1 phrase F as an additional input, the suggestion problem will be: The TB-PS model disambiguates paraphrases from the translations of F instead of paraphrases P . In this section, we describe the experimental setting and the preliminary results. Instead of training a whole machine translation using toolkits such as Moses (Koehn et. al, 2007), we used only bilingual phrase alignment as translations to prevent from the noise produced by the machine translation decoder. Word alignments were produced using Giza++ toolkit (Och and Ney, 2003), over a set of 2,220,570 Chinese-English sentence pairs in Hong Kong Parallel Text (LDC2004T08) with sentences segmented using the CKIP Chinese word segmentation system (Ma and Chen, 2003). In training the phrasal lexical disambiguation model, we used the English part of Hong Kong Parallel Text as our training data. To assess the effectiveness of FLOW, we selected 10 Chinese sentences and asked two students to translate the Chinese sentences to English sentences using FLOW. We kept track of the sentences the two students entered. Table 1 shows the selected results.
 In this paper, we presented FLOW, an interactive writing assistance system, aimed at helping EFL writers compose and revise without interrupting their writing flow. First-language-oriented and context-sensitive features are two main contributions in this work. Based on the studies on second language writing that EFL writers tend to use their native language to produce texts and then translate into English, the first-language-oriented function provides writers with appropriate translation suggestions. On the other hand, due to the fact that selection of words or phrases is sensitive to syntax and context, our system provides suggestions depending on the contexts. Both functions are expected to improve EFL writers X  writing performance. 
In future work, we will conduct experiments to gain a deeper understanding of EFL writers X  writing improvement with the help of FLOW, such as integrating FLOW into the writing courses to observe the quality and quantity of students X  writing performance. Many other avenues exist for future research and improvement of our system. For example, we are interested in integrating the error detection and correction functions into FLOW to actively help EFL writers achieve better writing success and further motivate EFL writers to write with confidence. 
