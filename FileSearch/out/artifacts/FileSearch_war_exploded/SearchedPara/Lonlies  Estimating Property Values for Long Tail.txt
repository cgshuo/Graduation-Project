 Web search engines often retrieve answers for queries about popular entities from a growing knowledge base that is pop-ulated by a continuous information extraction process. How-ever, less popular entities are not frequently mentioned on the web and are generally interesting to fewer users; these entities reside on the long tail of information. Traditional knowledge base construction techniques that rely on the high frequency of entity mentions to extract accurate facts about these mentions have little success with entities that have low textual support.

We present Lonlies , a system for estimating property values of long tail entities by leveraging their relationships to head topics and entities. We demonstrate (1) how Lonlies builds communities of entities that are relevant to a long tail entity utilizing a text corpus and a knowledge base; (2) how Lonlies determines which communities to use in the estimation process; (3) how we aggregate estimates from community entities to produce final estimates, and (4) how users interact with Lonlies to provide feedback to improve the final estimation results.
Search engines can provide direct answers to queries about popular entities, e.g., capitals of countries and birth dates of celebrities. These answers are obtained from structured knowledge bases that contain information about entities that are frequently queried and are generally interesting to a large number of users. Answering a user query becomes a matter of understanding and representing the user X  X  intent in an meaningful way, then finding the corresponding nodes and edges in the knowledge base graph that contain the answer.
While some knowledge bases extend their content by al-lowing users to manually add information, many recent ap-proaches [4, 7] have focused on automating the extraction of facts from unstructured data such as text. Most fact ex-traction techniques rely on the abundance of text and utilize the redundancy of information that is extracted from multi-Figure 1: Mentions of Head and Long Tail Entities ple data sources in order to statistically validate the newly extracted facts. For example, Google X  X  Knowledge Vault [4] extracts fact triples using four different types of extraction systems, and the confidence of a triple increases as more extractors produce the same triple and more unique web sources contain the triple. The knowledge base is then aug-mented with facts that are extracted with high confidence, growing its size to cover a larger domain of information.
Figure 1 models the frequency of entity mentions in the text as a distribution, representing the entities on the x-axis and the frequency of mentions of these entities on the y-axis. The  X  X ead X  entities have many mentions (at least f 1 in the figure) that allow for efficient extraction using traditional methods. Entities that reside on the long tail (beyond the point t 1 ) pose a challenge to automatic knowledge base con-struction methods due to the insufficiency of relevant web content; the extracted facts cannot be verified and questions about them cannot be directly answered.

Lonlies estimates property values for long tail entities and does not rely on direct extraction of these values from text. Lonlies utilizes fewer mentions of these entities and information from a knowledge base to produce probabilistic estimates for the property values. In Figure 1, Lonlies can extract properties of long tail entities (between t 1 and t that have enough frequency of mentions to assess their rela-tionships to head topics, but do not have enough redundancy to allow for efficient traditional fact extraction. The intu-ition behind Lonlies is to leverage the text corpus to find occurrences of the long tail entity e l with other head entities with known properties and exploit those head entities as an  X  X nformation context X  around e l , which we call communities, to estimate a value for a property p of e l . Communities are perceived as potential populations that e l might be a mem-ber of, and they provide a more general context about e l The main insight behind Lonlies depends on two observa-tions:
We combine these two notions in the following estimator for e l .p : where M ( e l , c i ) measures the likelihood of e l being a member of a community c i , and c i .  X  p is an estimator for the property p that produces possible values for p using the p values of entities in c i . c i .  X  p associates each estimate with a confidence interval that reflects the quality of the estimation. f is an aggregation function that combines estimates from a set of communities C .

The ability of Lonlies to find correct estimates for e l .p depends on (1) the estimation power of the constructed com-munities (i.e., how accurately c i .  X  p represents the values of p for entities in c i ); and (2) the confidence that e l belongs to the communities used in the estimation. These two con-straints together verify that estimates are produced from relevant communities that confidently produce representa-tive p values. The error in estimating e l .p is the compound error of the membership assessment function M and the er-ror in computing c i .  X  p estimates from communities.
Lonlies alleviates the risk of low-quality estimates for e .p by being conservative in assessing the membership M and when producing estimates from c i as we explain in Sec-tions 2.3 and 2.4. Lonlies produces estimates only for re-sults with high confidence. This, of course, lowers the recall of the long tail entities that Lonlies can handle, but main-tains a high precision for the entities that are answered.
While this formula estimates e l .p from the p values of the constructed communities, the technique can be easily ex-tended to leverage external rules and correlations of proper-ties to estimate one property from another (e.g., to estimate age from birth date or nationality from country of birth ).
Figure 3 illustrates the system architecture of Lonlies We give an overview about the functionality of the four modules that constitute the stages of the estimation pro-cess and then discuss the details of each module separately. The modules rely on the availability of a text corpus and a knowledge base. The input to Lonlies is a long tail query entity, e l , and a target property to estimate, p .
The first step of Lonlies is retrieving a set of text doc-uments that are relevant to the long tail query entity e Lonlies extracts head entities that appear in the retrieved documents and exist in a knowledge base.

The co-occurrence of the long tail entity with head entities in text documents implies relationships to these entities in the knowledge base; the co-mentions represent what nodes in the knowledge base graph e l might have been linked to if it was added to the knowledge base. Hence, we refer to these head entities as anchor KB entities E H .

Lonlies does not classify the relationship between the long tail entity and anchor head entities that appear with it in the text. In fact, the co-occurrence merely indicates that a relationship between them exists. The Membership Assessment module validates these potential relationships.
Given the list of head entities that the long tail query entity is mentioned with, E H , the goal of this module is to construct a set of communities C that can produce high-quality estimates for p .
 Definition 2.1. A community c consists of a set of head entities that are retrieved from a knowledge base. Entities of a community c reflect nodes in the knowledge base graph.
Communities are constructed by traversing the knowledge base graph starting from E H entity nodes. In Lonlies , we implemented two methods to construct communities.
 ize a k-hop graph around all E H entities from the knowledge base. We then run a community detection algorithm simi-lar to [6] to detect communities in the materialized graph. We modified the betweenness function of the algorithm to separate communities that have coherent p values instead of dense connected subgroups. This method produces commu-nities with consistent p values. from each entity in E H and construct a community from all entities in the KB graph that share a particular relationship to it. For example, a community c may include all entities that work at the University of Waterloo . Not all constructed communities can produce high-quality, useful estimates, and the estimation power of a community depends on the prop-erty p that we are estimating. For example, a community that consists of people born in the United States can con-fidently estimate the nationality , but the same community cannot help in estimating age or occupation .

After constructing a possibly large number of communi-ties, Lonlies calculates multiple statistical features for each community, including the confidence of its produced esti-mates which depends on the size of the community and the homogeneity of the p values of its members, depending on the type of p . Lonlies computes the variance of the p values if p is a numerical property, or Simpson X  X  diversity index of the p values for categorical properties. Lonlies associates a confidence score with the produced estimates and discards communities with low estimation confidence. This drasti-cally reduces the number of communities to consider in the following stage.
As mentioned in Section 2.1, the co-occurrence of the long tail entity with another head entity does not necessarily in-dicate a strong relationship between them. Therefore, not all constructed communities are related to e l .

This module produces a membership score that reflects the confidence in membership of e l in a community c , M ( e l for each of the constructed communities. The membership
The index reflects the probability that two entities taken at random from the community have the same p value. in each community is determined independently from the membership in other communities.

We view this problem as a binary classification task where an entity is labeled  X  X ember X  or  X  X ot member X  in the com-munity. Lonlies trains a classifier to compute the prob-ability of an entity X  X  membership in the community. To obtain training data, Lonlies retrieves documents from the text corpus where the community entities appear and ex-tracts classification features from the text. Each entity in the community acts as a training data point. We run the trained classification model against the features extracted for e l . Lonlies runs two classification tools, Stanford Clas-sifier [5] and DeepDive [7], and assigns the higher score to the community.
The Estimation module receives a set of estimates of p , together with their confidence and membership scores of the communities that produced them. Lonlies uses a straight-forward method to aggregate these estimates, using the con-fidence and membership scores as weights. The result is a distribution of values for p with a final confidence score as-signed for each estimate.
The demonstration audience will be able to interact with the system by issuing queries about properties of long tail entities. The system will show the details of each step in the estimation to explain how it produced the final results. Users can provide feedback about the quality of each step and notice how the system employs the feedback in adjust-ing the estimation results. Lonlies uses Freebase [2] as its knowledge base and retrieves relevant text documents from Google Search and the ClueWeb09 [3] corpus that is indexed by Apache Lucene. We provide a walk-through of an exam-ple query and explain what the audience may expect to see.
Users enter two inputs: the name of the long tail entity and properties of interest. As an example, assume that we are interested in the Nationality , Political Party , and Alma Mater of the entity Hans Goff , who is a member of Hillary Clinton X  X  presidential campaign. Goff is considered a long tail entity since he does not exist in the knowledge base; however, he is frequently mentioned on the web with other head entities and topics.
 Lonlies retrieves a set of documents that are relevant to the query and provides a list of the anchor KB entities that are extracted from these documents. When the user clicks on an anchor entity, Lonlies shows how many times this head en-tity appears with the query entity and the list of documents where they appear together, as shown in Figure 2.
 list of anchor entities, Lonlies constructs a set of commu-nities. Users see a visualization of each community, which is represented as a set of (possibly connected) nodes that are retrieved from the knowledge base. The system presents some statistical details about the community including its confidence of estimating the target properties, the member-ship score that reflects how much the system believes that the query entity belongs to it, and its size (Figure 2).
When the user clicks on a community, Lonlies shows a detailed view of it. The details include the distribution of values within the community, the features used in determin-ing the membership of entities in this community, and a justification of the produced score of membership.
Users can provide feedback about communities by con-firming the membership of the query entity to them or by removing irrelevant communities from the estimation pro-cess. The communities are sorted by a score that combines the estimation confidence and the membership of the long tail entity. The higher the rank of a community, the more impact it has on the estimation results. Therefore, it is more valuable to present high-ranking communities to the user and get feedback about most influencing communities. estimated, Lonlies shows an appropriate visualization of the produced estimates. For categorical properties, lies shows the top-k estimates ordered by their confidence scores (Figure 4 shows the top-2 estimates). For numeri-cal properties, Lonlies shows a distribution of the property being estimated with its mean and variance.
Knowledge base construction methods, such as DeepDive [7] and Knowledge Vault [4], train property-specific extractors (e.g., using distant supervision) to extract specific relation-ships from text. Knowledge Vault also exploits a knowledge base to compute the prior of the extracted facts and pre-dict links in the knowledge base graph. This prior is used together with extractions from multiple extraction systems on different data sources to validate the correctness of ex-tracted facts. The success of these approaches relies heavily on the redundancy of information in the text in order to extract accurate property values.

Other question answering techniques [1] utilize query logs to find query-relevant web pages and paid crowd-sourcing to manually extract text snippets and author answers from the retrieved web pages.
We presented Lonlies , a system that estimates property values for long tail entities. Lonlies leverages the head entities that are co-mentioned with a long tail entity in a text corpus, together with information from a knowledge base, in order to construct a set of communities of entities. Lonlies aggregates the estimates that are produced from communities that (1) are relevant to the long tail entity, and (2) have high estimation power of its members, producing a distribution of the target property value. This work was generously supported by Google Faculty Research Award granted to Ihab F. Ilyas in 2014. [1] M. S. Bernstein, J. Teevan, S. Dumais, D. Liebling, and [2] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [3] J. Callan, M. Hoy, C. Yoo, and L. Zhao. Clueweb09 [4] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, [5] C. Manning and D. Klein. Optimization, maxent [6] M. E. Newman and M. Girvan. Finding and evaluating [7] F. Niu, C. Zhang, C. R  X e, and J. W. Shavlik. Deepdive: [8] E. H. Simpson. Measurement of diversity. Nature , 1949.
