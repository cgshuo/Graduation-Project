
Yong Ge 3 Enhong Chen 1 Ranking fraud in the mobile App market refers to fraudulent or deceptive activities which have a purpose of bumping up the Apps in the popularity list. Indeed, it becomes more and more frequent for App develops to use shady means, such as inflating their Apps X  sales or posting phony App ratings, to commit ranking fraud. While the importance of preventing ranking fraud has been widely recognized, there is limited understanding and research in this area. To this end, in this paper, we provide a holistic view of ranking fraud and propose a ranking fraud detection system for mobile Apps. Specifically, we investigate two types of evidences, ranking based evidences and rating based evidences, by modeling Apps X  ranking and rating behaviors through statistical hy-potheses tests. In addition, we propose an optimization based aggregation method to integrate all the evidences for fraud detection. Finally, we evaluate the proposed system with real-world App data collected from the Apple X  X  App Store for a long time period. In the experiments, we vali-date the effectiveness of the proposed system, and show the scalability of the detection algorithm as well as some regu-larity of ranking fraud activities.
 H.2.8.d [ Information Technology and Systems ]: Database Applications -Data Mining Ranking Fraud Detection, Mobile Apps
The number of mobile Apps has grown at a breathtaking rate over the past few years. For example, as of the end of April 2013, there are more than 1.6 million Apps at Ap-ple X  X  App store and Google Play. To stimulate the develop-ment of mobile Apps, many App stores launched daily App  X  Co ntact Author.
 leaderboards, which demonstrate the chart rankings of most popular Apps. Indeed, the App leaderboard is one of the most important ways for promoting mobile Apps. A higher rank on the leaderboard usually leads to a huge number of downloads and million dollars in revenue. Therefore, App developers tend to explore various ways such as advertis-ing campaigns to promote their Apps in order to have their Apps ranked as high as possible in such App leaderboards.
However, as a recent trend, instead of relying on tradi-tional marketing solutions, shady App developers resort to some fraudulent means to deliberately boost their Apps and eventually manipulate the chart rankings on an App store. This is usually implemented by using so-called  X  bot farms  X  or  X  human water armies  X  to inflate the App downloads and ratings in a very short time. For example, an article from VentureBeat [3] reported that, when an App was promoted with the help of ranking manipulation, it could be propelled from number 1,800 to the top 25 in Apple X  X  top free leader-board and more than 50,000-100,000 new users could be ac-quired within a couple of days. In fact, such ranking fraud raises great concerns to the mobile App industry. For exam-ple, Apple has warned of cracking down on App developers who commit ranking fraud [2] in the Apple X  X  App store.
In the literature, while there are some related work, such as web ranking spam detection [11, 13, 14], online review spam detection [10, 15, 16], and mobile App recommenda-tion [12, 17, 18, 19], the problem of detecting ranking fraud for mobile Apps is still under-explored. To fill this crucial void, in this paper, we propose to develop a ranking fraud detection system for mobile Apps. Along this line, we iden-tify several important challenges. First, ranking fraud does not always happen in the whole life cycle of an App, so we need to detect the time when fraud happens. Second, due to the huge number of mobile Apps, it is difficult to manu-ally label ranking fraud for each App, so it is important to have a way to automatically detect ranking fraud without using any benchmark information. Finally, due to the dy-namic nature of chart rankings, it is not easy to identify and confirm the evidences linked to ranking fraud.
 Indeed, our careful observation reveals that fraudulent Apps do not always be ranked high in the leaderboard, but only in some leading events , which form different leading sessions . Note that we will introduce both leading events and leading sessions in detail later. In other words, ranking fraud usually happens in these leading sessions. Therefore, detecting ranking fraud of mobile Apps is actually to de-tect ranking fraud within leading sessions of mobile Apps. Specifically, we first propose a simple yet effective algorithm Fi gure 1: The framework of the ranking fraud de-tection system for mobile Apps. to identify the leading sessions of each App based on its his-torical ranking records. Then, with the analysis of Apps X  ranking behaviors, we find that the fraudulent Apps often have different ranking patterns in each leading session com-pared with normal Apps. Thus, we characterize some fraud evidences from Apps X  historical ranking records, and de-velop three functions to extract such ranking based fraud evidences. Nonetheless, the ranking based evidences can be affected by some legitimate marketing campaigns, such as  X  X imited-time discount X . As a result, it is not sufficient to only use ranking based evidences. Therefore, we further pro-pose two functions to discover rating based evidences, which reflect some anomaly patterns from Apps X  historical rating records. In addition, we develop an unsupervised evidence-aggregation method to integrate these two types of evidences for evaluating the credibility of leading sessions from mobile Apps. Figure 1 shows the framework of our ranking fraud detection system for mobile Apps.

It is worth noting that all the evidences are extracted by modeling Apps X  ranking and rating behaviors through statis-tical hypotheses tests. The proposed framework is scalable and can be extended with other domain-generated evidences for ranking fraud detection. Finally, we evaluate the pro-posed system with real-world App data collected from the Apple X  X  App store for a long time period. Experimental re-sults show the effectiveness of the proposed system, the scal-ability of the detection algorithm as well as some regularity of ranking fraud activities.

Overview. The remainder of this paper is organized as follows. In Section 2, we introduce some preliminaries and how to mine leading sessions for mobile Apps. Section 3 presents how to extract ranking and rating based evidences and combine them for ranking fraud detection. In Section 4 we make some further discussion about the proposed ap-proach. In Section 5, we report the experimental results on two long-term real-world data sets. Section 6 provides a brief review of related works. Finally, in Section 7, we conclude the paper.
In this section, we first introduce some preliminaries, and then show how to mine leading sessions for mobile Apps from their historical ranking records.
The App leaderboard demonstrates top K popular Apps with respect to different categories, such as  X  Top Free Apps  X  Figure 2: (a) Examples of leading events; (b) Exam-ples of leading sessions of mobile Apps. and  X  Top Paid Apps  X . Moreover, the leaderboard is usually updated periodically (e.g., daily). Therefore, each mobile App a has many historical ranking records which can be denoted as a time series, R a = { r a 1 ,  X  X  X  , r a i ,  X  X  X  r +  X  means a is not ranked in the top K list; n denotes the number of all ranking records. Note that, the smaller value r i has, the higher ranking the App obtains.

By analyzing the historical ranking records of mobile Apps, we observe that Apps are not always ranked high in the leaderboard, but only in some leading events . For example, Figure 2 (a) shows an example of leading events of a mobile App. Formally, we define a leading event as follows.
Definition 1 (Leading Event). Given a ranking th-reshold K  X  [1 , K ] , a leading event e of App a contains a time range T e = [ t e start , t e end ] and corresponding rankings of a , which satisfies r a start  X  K &lt; r a start 1 , and r r
Note that we apply a ranking threshold K which is usu-ally smaller than K here because K may be very big (e.g., 1000), and the ranking records beyond K (e.g., 100) are not very useful for detecting the ranking manipulations.
Furthermore, we also find that some Apps have several adjacent leading events which are close to each other and form a leading session . For example, Figure 2(b) shows an example of adjacent leading events of a given mobile App, which form two leading sessions. Particularly, a leading event which does not have other nearby neighbors can also be treated as a special leading session. The formal definition of leading session is as follows.

Definition 2 (leading Session). A leading session s of App a contains a time range T s = [ t s start , t s adjacent leading events { e 1 , ..., e n } , which satisfies t t makes T s  X  T s . Moreover,  X  i  X  [1 , n ) , we have ( t t end ) &lt;  X  , where  X  is a predefined threshold.
Intuitively, the leading sessions of a mobile App represent its periods of popularity, so the ranking manipulation will only take place in these leading sessions. Therefore, the problem of detecting ranking fraud is to detect fraudulent leading sessions. Along this line, the first task is how to mine the leading sessions of a mobile App from its historical ranking records. There are two main steps for mining leading sessions. First, we need to discover leading events from the App X  X  his-torical ranking records. Second, we need to merge adjacent Fi gure 3: An example of different ranking phases of a leading event. leading events for constructing leading sessions. Specifically, Algorithm 1 demonstrates the pseudo code of mining leading session for a given App a .
 A lgorithm 1 Mining Leading Sessions 2: for each i  X  [1 , | R a | ] do 6: //found one event; 11: // e is the last leading event before e in E a ; 13: else then 14: //found one session; 17: go to Step 7; 19: return S a
In Algorithm 1, we denote each leading event e and session s as tuples &lt; t e start , t e end &gt; and &lt; t s start tively, where E s is the set of leading events in session s . Specifically, we first extract individual leading event e for the given App a (i.e., Step 2 to 5) from the beginning time. For each extracted individual leading event e , we check the time span between e and the previous leading event e to de-cide whether they belong to the same leading session based on Definition 2. Particularly, if ( t e start  X  t e end ) be considered as a new leading session (i.e., Step 6 to 13). Thus, this algorithm can identify leading events and sessions by scanning a  X  X  historical ranking records only once.
In this section, we study how to extract and combine fraud evidences for ranking fraud detection.
According to the definitions introduced in Section 2, a leading session is composed of several leading events. There-Figure 4: Two real-world examples of leading events. fore, we should first analyze the basic characterizes of leading events for extracting fraud evidences.

By analyzing the Apps X  historical ranking records, we ob-serve that Apps X  ranking behaviors in a leading event al-ways satisfy a specific ranking pattern, which consists of three different ranking phases, namely, rising phase , main-taining phase and recession phase . Specifically, in each lead-ing event, an App X  X  ranking first increases to a peak position in the leaderboard (i.e., rising phase), then keeps such peak position for a period (i.e., maintaining phase), and finally decreases till the end of the event (i.e., recession phase). Figure 3 shows an example of different ranking phases of a leading event. Indeed, such a ranking pattern shows an im-portant understanding of leading event. In the following, we formally define the three ranking phases of a leading event. Definition 3 (Ranking Phases of a Leading Event). Give a leading event e of App a with time range [ t e start where the highest ranking position of a is r a peak , which be-longs to  X  R . The rising phase of e is a time range [ t e  X  R . The maintaining phase of e is a time range [ t e b where r a c  X   X  R and  X  t i  X  ( t e c , t e end ] satisfies r recession phase is a time range [ t e c , t e d ] , where t
Note that, in Definition 3,  X  R is a ranking range to de-cide the beginning time and the end time of the maintaining phase. t e b and t e c are the first and last time when the App is ranked into  X  R . It is because an App, even with ranking manipulation, cannot always maintain the same peak posi-tion (e.g., rank 1) in the leaderboard but only in a ranking range (e.g., top 25). If a leading session s of App a has ranking fraud, a  X  X  ranking behaviors in these three rank-ing phases of leading events in s should be different from those in a normal leading session. Actually, we observe that each App with ranking manipulation always has an expected ranking target, such as top 25 in leaderboard for one week, and the hired marketing firms also charge money accord-ing to such ranking expectation (e.g., $1000/day in top 25). Therefore, for both App developers and marketing firms, the earlier the ranking expectation meets, the more money can be earned. Moreover, after reaching and maintaining the expected ranking for a required period, the ranking ma-nipulation will be stopped and the ranking of the malicious App will decrease dramatically. As a result, the suspicious leading events may contain very short rising and recession phase. Meanwhile, the cost of ranking manipulation with high ranking expectations is quite expensive. Therefore, the fraudulent Apps often have a short maintaining phase with high ranking positions in each leading event.

Figure 4 (a) shows an example of ranking records from one of the reported suspicious Apps [4]. In this figure, we can see th at this App has several impulsive leading events with high ranking positions. In contrast, the ranking behaviors of a normal App X  X  leading event may be completely different. For example, Figure 4 (b) shows an example of ranking records from a popular App  X  Angry Birds: Space  X , which contains a leading event with a long time range (i.e., more than one year), especially for the recession phase. In fact, once a normal App is ranked high in the leaderboard, it often owns a lot of honest fans and may attract more and more users to download. As a result, this App will be ranked high in the leaderboard for a long time. Based on the above discussion, we propose some ranking based signatures of leading sessions to construct fraud evidences for ranking fraud detection.  X  EVIDENCE 1. As shown in Figure 3, we use two shape parameters  X  1 and  X  2 to quantify the ranking patterns of the rising phase and the recession phase of App a  X  X  leading event e , which can be computed by where K is the ranking threshold in Definition 1. In-tuitively, a large  X  1 may indicate that the App has been bumped to a high rank within a short time, and a large  X  2 may indicate that the App has dropped from a high rank to the bottom within a short time. Therefore, a leading session, which has more leading events with large  X  1 and  X  2 values, has higher probability of having ranking fraud. Here, we define a fraud signature  X  s fo r a leading session as follows. where | E s | is the number of leading events in session s . Intu-itively, if a leading session s contains significantly higher  X  co mpared with other leading sessions of Apps in the leader-board, it has high probability of having ranking fraud. To capture this, we propose to apply statistical hypothesis test for computing the significance of  X  s fo r each leading session. Specifically, we define two statistical hypotheses as follows and compute the p-value of each leading session.

Here, we propose to use the popular Gaussian approxi-mation to compute the p-value with the above hypotheses. Specifically, we assume  X  s fo llows the Gaussian distribution,  X   X  N (  X  ,  X  ) , where  X  a nd  X  c an be learnt by the popu-lar maximum-likelihood estimation (MLE) method from the observations of  X  s in all Apps X  historical leading sessions. Then, we can calculate the p-value by where erf( x ) is the Gaussian Error Function as follows, Intuitively, a leading session with a smaller p-value P has more chance to reject Hypothesis 0 and accept Hypothesis 1 This means it has more chance of committing ranking fraud. Thus, we define the evidence as
EVIDENCE 2. As discussed above, the Apps with rank-ing fraud often have a short maintaining phase with high ranking positions in each leading event. Thus, if we de-note the maintaining phase of a leading event e as  X  t e m ( t as r e m , we can define a fraud signature  X  s for each leading session as follows, where K is the ranking threshold in Definition 1. If a lead-ing session contains significantly higher  X  s compared with other leading sessions of Apps in the leaderboard, it has high chance of having ranking fraud. To capture such sig-natures, we define two statistical hypotheses as follows to compute the significance of  X  s for each leading session.
Here, we also propose to use the Gaussian approxima-tion to calculate the p-value with the above hypotheses. Specifically, we assume  X  s follows the Gaussian distribution,  X  s  X  X  (  X  ,  X  ) , where  X  and  X  can be learnt by the MLE method from the observations of  X  s in all Apps X  historical leading sessions. Then, we can calculate the evidence by
EVIDENCE 3. The number of leading events in a lead-ing session, i.e., | E s | , is also a strong signature of ranking fraud. For a normal App, the recession phase indicates the fading of popularity. Therefore, after the end of a leading event, it is unlikely to appear another leading event in a short time unless the App updates its version or carries out some sales promotion. Therefore, if a leading session con-tains much more leading events compared with other leading sessions of Apps in the leaderboard, it has high probability of having ranking fraud. To capture this, we define two sta-tistical hypotheses to compute the significance of | E s | each leading session as follows.

Since | E s | always has discrete values, we propose to lever-age the Poisson approximation to calculate the p-value with the above hypotheses. Specifically, we assume | E s | follows the Poisson distribution, | E s | X  X  (  X  s ) , where the parameter  X  s can be learnt by the MLE method from the observations of | E s | in all Apps X  historical leading sessions. Then, we can calculate the p-value as follows, Therefore, we can compute the evidence by The values of the above three evidences  X  1 ( s ),  X  2 ( s ) and  X  ( s ) are all within the range of [0 , 1]. Meanwhile, the higher evidence value a leading session has, the higher prob-ability this session has ranking fraud activities.
The ranking based evidences are useful for ranking fraud detection. However, sometimes, it is not sufficient to only use ranking based evidences. For example, some Apps cre-ated by the famous developers, such as Gameloft, may have some leading events with large values of  X  1 due to the devel-opers X  credibility and the  X  X ords of mouth X  advertising ef-fect. Moreover, some of the legal marketing services, such as  X  X imited-time discount X , may also result in significant rank-ing based evidences. To solve this issue, we also study how to extract fraud evidences from Apps X  historical rating records.
Specifically, after an App has been published, it can be rated by any user who downloaded it. Indeed, user rat-ing is one of the most important features of App advertise-ment. An App has higher rating may attract more users to download and can also be ranked higher in the leaderboard. Thus, rating manipulation is also an important perspective of ranking fraud. Intuitively, if an App has ranking fraud in a leading session s , the ratings during the time period of s may have anomaly patterns compared with its historical ratings, which can be used for constructing rating based ev-idences. Thus, we define two rating fraud evidences based on user rating behaviors as follows.

EVIDENCE 4. For a normal App, the average rating in a specific leading session should be consistent with the average value of all historical ratings. In contrast, an App with rating manipulation might have surprisingly high rat-ings in the fraudulent leading sessions with respect to its historical ratings. Here, we define a fraud signature  X  R for each leading session as follows, where R s is the average rating in leading session s , and R a is the average historical rating of App a . Therefore, if a leading session has significantly high value of  X  R s compared with other leading sessions of Apps in the leaderboard, it has high probability of having ranking fraud. To capture this, we define statistical hypotheses to compute the significance of  X  R s for each leading session as follows.

Here, we use the Gaussian approximation to calculate the p-value with the above hypotheses. Specifically, we assume  X  R s follows the Gaussian distribution,  X  R s  X  X  (  X  R ,  X  where  X  R and  X  R can be learnt by the MLE method from the observations of  X  R s in all Apps X  historical leading sessions. Then, we compute the evidence by
EVIDENCE 5. In the App rating records, each rat-ing can be categorized into | L | discrete rating levels, e.g., 1 to 5, which represent the user preferences of an App. The rating distribution with respect to the rating level l i in a normal App a  X  X  leading session s , p ( l i |R s;a ), should be con-sistent with the distribution in a  X  X  historical rating records, p ( l i |R a ), and vice versa. Specifically, we can compute the distribution by p ( l i |R s;a ) = ber of ratings in s and the rating is at level l i , N s ( : ) total number of ratings in s . Meanwhile, we can compute p ( l i |R a ) in a similar way. Then, we use the Cosine distance between p ( l i |R s;a ) and p ( l i |R a ) to estimate the difference as follows.
 Therefore, if a leading session has significantly higher value of
D ( s ) compared with other leading sessions of Apps in the leaderboard, it has high probability of having ranking fraud. To capture this, we define statistical hypotheses to compute the significance of D ( s ) for each leading session as follows.
Here, we use the Gaussian approximation to compute the p-value with the above hypotheses. Specifically, we assume D ( s ) follows the Gaussian distribution, D ( s )  X  N (  X  D where  X  D and  X  D can be learnt by the MLE method from the observations of D ( s ) in all Apps X  historical leading sessions. Then, we can compute the evidence by
The values of two evidences  X  4 ( s ) and  X  5 ( s ) are in the range of [0 , 1]. Meanwhile, the higher evidence value a lead-ing session has, the more chance this session has ranking fraud activities.
After extracting ranking and rating based fraud evidences, the next challenge is how to combine them for ranking fraud detection. Indeed, there are many ranking and evidence ag-gregation methods in the literature, such as permutation based models [9], score based models [6] and Dempster-Shafer rules [5]. However, some of these methods focus on learning a global ranking for all candidates. This is not proper for detecting ranking fraud for new Apps. Other methods are based on supervised learning techniques, which depend on the labeled training data and are hard to be exploited. Instead, we propose an unsupervised approach based on ranking similarity to combine these evidences.
Specifically, we define the final evidence score  X  ( s ) as a linear combination of all the existing evidences as follows. where N = 5 is the number of evidences, and weight w i  X  [0 , 1] is the aggregation parameter of evidence  X  i ( s ), which satisfies gregation becomes how to learn the proper parameters { w i from the historical ranking records.

A basic assumption in our evidence aggregation approach is that effective evidences should rank leading sessions from a similar conditional distribution, while poor evidences will lead to a more uniformly random ranking distribution [8]. In other words, evidences that tend to be consistent with the plurality of evidences will be given higher weights and evi-dences which tend to disagree will be given smaller weights. Specifically, given a set of leading sessions, we first rank them by each evidence score and obtain N ranked lists. Let us denote  X  i ( s ) as the ranking of session s returned by  X  ( s ), then we can calculate the average ranking for leading session s by, Then, for each evidence score  X  i ( s ), we can measure its consistence using the variance-like measure, If  X  i ( s ) is small, the corresponding  X  i ( s ) should be given a bigger weight and vice versa. Therefore, given an App set A = { a i } with their leading sessions { s j } , we can define the evidence aggregation problem as an optimization problem that minimizes weighted variances of the evidences over all leading sessions, that is
In this paper, we exploit the gradient based approach with exponentiated updating [7, 8] to solve this problem. To be specific, we first assign w i = 1 N as the initial value, then for each s , we can compute the gradient by, Thus, we can update the weight w i by where w i is the last updated w i , and  X  is the learning rate, which is empirically set  X  = 0 . 01 in our experiments.
Finally, we can exploit Equation (14) to estimate the fi-nal evidence score of each leading session. Moreover, given a leading session s with a predefined threshold  X  , we can determine that s has ranking fraud if  X  ( s ) &gt;  X  .
In this section, we provide some discussion about the pro-posed ranking fraud detection system for mobile Apps.
First, the download information is an important signature for detecting ranking fraud, since ranking manipulation is to use so-called  X  X ot farms X  or  X  X uman water armies X  to inflate the App downloads and ratings in a very short time. How-ever, the instant download information of each mobile App is often not available for analysis. In fact, Apple and Google do not provide accurate download information on any App. Furthermore, the App developers themselves are also reluc-tant to release their download information for various rea-sons. Therefore, in this paper, we mainly focus on extracting evidences from Apps X  historical ranking records and rating records for ranking fraud detection. However, our approach is scalable for integrating other evidences if available, such as the evidences based on the download information.
Second, the proposed approach can detect ranking fraud happened in Apps X  historical leading sessions. However, sometime, we need to detect such ranking fraud from Apps X  current ranking observations. Actually, given the current ( a) Top Free 300 data set (b ) Top Paid 300 data set Figure 5: The distribution of the number of Apps w.r.t different rankings. ranking r a now of an App a , we can detect ranking fraud for it in two different cases. First, if r a now &gt; K , where K is the ranking threshold introduced in Definition 1, we believe a does not involve in ranking fraud, since it is not in a lead-ing event. Second, if r a now &lt; K , which means a is in a new leading event e , we treat this case as a special case that t end = t e now and  X  2 = 0. Therefore, it also can be detected by the proposed approach.

Finally, after detecting ranking fraud for each leading ses-sion of a mobile App, the remainder problem is how to es-timate the credibility of this App. Indeed, we can also use Equation (14) to evaluate each App. To be specific, we de-fine an App fraud score F ( a ) for each App a according to how many leading sessions of a contain ranking fraud. where s  X  a denotes that s is a leading session of App a ; [[ x ]] = 1 if x = T rue , and 0 otherwise; and  X  t s = ( t t start +1) is the time range of s . Intuitively, an App contains more leading sessions, which have high ranking fraud scores and long time ranges, will have higher App fraud scores.
In this section, we evaluate the performances of ranking fraud detection using real-world App data. The experimental data sets were collected from the  X  X op Free 300 X  and  X  X op Paid 300 X  leaderboards of Apple X  X  App Store (U.S.) from February 2, 2010 to September 17, 2012. The data sets contain the daily chart rankings 1 of top 300 free Apps and top 300 paid Apps, respectively. Moreover, each data set also contains the user ratings and comment information. Table 1 shows the detailed data characteristics.
Figures 5 (a) and 5 (b) show the distributions of the num-ber of Apps with respect to different rankings in these data sets. In the figures, we can see that the number of Apps with
Th e information was collected at 11:00PM (PST) each day. (a ) Top Free 300 data set (b ) Top Paid 300 data set Figure 6: The distribution of the number of Apps w.r.t different numbers of ratings. (a ) Top Free 300 data set (b ) Top Paid 300 data set Figure 7: The distribution of the number of Apps w.r.t different numbers of leading events. low rankings is more than that of Apps with high rankings. Moreover, the competition between free Apps is more than that between paid Apps, especially in high rankings (e.g., top 25). Figures 6 (a) and 6 (b) show the distribution of the number of Apps with respect to different number of ratings in these data sets. In the figures, we can see that the distri-bution of App ratings is not even, which indicates that only a small percentage of Apps are very popular.
Here, we demonstrate the results of mining leading ses-sions in both data sets. Specifically, in Algorithm 1, we set the ranking threshold K = 300 and threshold  X  = 7. This denotes two adjacent leading events can be segmented into the same leading session if they occur within one week of each other. Figure 7 and Figure 8 show the distribution of the number of Apps with respect to different numbers of contained leading events and leading sessions in both data sets. In these figures, we can see that only a few Apps have many leading events and leading sessions. The average num-bers of leading events and leading sessions are 2 . 69 and 1 . 57 for free Apps, and 4 . 20 and 1 . 86 for paid Apps. Moreover, Figures 9 (a) and 9 (b) show the distribution of the num-ber of leading sessions with respect to different numbers of contained leading events in both data sets. In these figures, we can find only a few leading sessions contain many lead-ing events. This also validates the evidence  X  3 . Indeed, the average number of leading events in each leading session is 1 . 70 for free Apps and 2 . 26 for paid Apps.
To the best of our knowledge, there is no existing bench-mark to decide which leading sessions or Apps really con-tain ranking fraud. Thus, we develop three intuitive base-lines and invite five human evaluators to validate the effec-( a) Top Free 300 data set (b ) Top Paid 300 data set Figure 8: The distribution of the number of Apps w.r.t different number of leading sessions. ( a) Top Free 300 data set (b ) Top Paid 300 data set Figure 9: The distribution of the number of leading sessions w.r.t different number of leading events. tiveness of our approach EA-RFD ( E vidence A ggregation based R anking F raud D etection).
The first baseline Ranking-RFD stands for Ranking evi-dence based Ranking Fraud Detection, which estimates rank-ing fraud for each leading session by only using ranking based evidences (i.e.,  X  1 to  X  3 ). These three evidences are integrated by the method introduced in Section 3.3.
The second baseline Rating-RFD stands for Rating evi-dence based Ranking Fraud Detection, which estimates the ranking fraud for each leading session by only using rating based evidences (i.e.,  X  4 and  X  5 ). These three evidences are integrated by the method introduced in Section 3.3. Above two baselines are used for evaluating the effectiveness of dif-ferent kinds of evidences.
 The third baseline E-RFD stands for Evidence based Ranking Fraud Detection, which estimates the ranking fraud for each leading session by both ranking and rating based ev-idences without evidence aggregation. Specifically, it ranks leading sessions by Equation 14, where each w i is set to be 1 / 5 equally. This baseline is used for evaluating the effec-tiveness of our ranking aggregation method.

Note that, according to Definition 3, we need to define some ranking ranges before extracting ranking based evi-dences for EA-RFD, Rank-RFD and E-RFD. In our experi-ments, we segment the rankings into 5 different ranges, i.e., monly used in App leaderboards.
To study the performance of ranking fraud detection by each approach, we set up the evaluation as follows.
First, for each approach, we selected 20 top ranked lead-ing sessions (i.e., most suspicious sessions), and 20 bottom Fi gure 10: The screenshots of our ranking fraud evaluation platform. ranked leading sessions (i.e., most normal sessions) from each data set. Then, we merged all the selected sessions into a pool which consists 114 unique sessions from 84 unique Apps in  X  X op Free 300 X  data set, and 128 unique sessions from 71 unique Apps in  X  X op Paid 300 X  data set.

Second, we invited five human evaluators who are familiar with Apple X  X  App store and mobile Apps to manually label the selected leading sessions with score 1 (i.e., Fraud ) and 0 (i.e., Non-fraud ). Specifically, for each selected leading ses-sion, each evaluator gave a proper score by comprehensively considering the profile information of the App (e.g., descrip-tions, screenshots), the trend of rankings during this ses-sion, the App leaderboard information during this session, the trend of ratings during this session, and the user com-ments during this session. Moreover, they can also download and try the corresponding Apps for obtaining user experi-ences. Particularly, to facilitate their evaluation, we develop a Ranking Fraud Evaluation Platform , which ensures that the evaluators can easily browse all the information. Also, the platform demonstrates each leading session in random orders, which guarantees there is no relationship between leading sessions X  order and their fraud scores. Figure 10 shows the screenshot of the platform. The left panel shows the main manu, the right upper panel shows the user com-ments for the given session, and the right lower panel shows the ranking related information for the given session.
Third, after human evaluation, each leading session s is assigned a fraud score f ( s )  X  [0 , 5]. Thus, we can exploit the popular metric Normalized Discounted Cumulative Gain (NDCG) for determining the performance of ranking fraud detection by each approach. Specifically, the discounted cu-mulative gain given a cut-off rank K can be calculated by where f ( s i ) is the human labeled fraud score. The N DCG @ K is the DCG @ K normalized by the IDCG @ K , which is the DCG @ K value of the ideal ranking list of the returned re-sults, i.e., we have N DCG @ K indicates how well the rank order of given ses-sions returned by an approach with a cut-off rank K . The Table 2: The consistence of human evaluation on Top Free 300 data set.
 T able 3: The consistence of human evaluation on Top Paid 300 data set.
 la rger N DCG @ K value, the better performance of ranking fraud detection.
Here, we first show the consistence of evaluation results by five human evaluators. Table 2 shows the numbers of fraud and non-fraud (i.e., numbers in brackets) labeled by five evaluators in the diagonal cells, and the number of over-lapping fraud and non-fraud (i.e., numbers in brackets) be-tween each pair of evaluators in Top Free 300 data set. Ta-ble 3 shows the same results of Top Paid 300 data set. As Figure 11: The N DCG @ K results of each approach. shown in the tables, these five evaluators are consistent in their judgements of both fraud and non-fraud labels. Specif-ically, all the five evaluators agree on 32 fraud sessions and 49 non-fraud sessions in Top Free 300 data set, which con-T able 4: The reported suspicious mobile Apps.
 st itute 71 . 1% of 114 evaluated sessions. Note that, 9 labeled fraud sessions among them are from the external reported suspicious Apps [3, 4], which validates the effectiveness of our human judgement. Similarly, they agree on 39 fraud sessions and 51 non-fraud sessions in Top Paid 300 data set, which constitute 70 . 3% of 128 evaluated sessions. Moreover, we compute the Cohen X  X  kappa coefficient [1] between each pair of evaluators to estimate the inter-evaluator agreement. Specifically, the values of Cohen X  X  kappa coefficient are be-tween 0 . 67 to 0 . 73 in Top Free 300 data set and between 0 . 70 to 0 . 73 in Top Paid 300 data set, which indicate the substantial agreement [10].

Figures 11 (a) and 11 (b) show the N DCG @ K results of each detection approach in two data sets. First, in these figures, we can see that EA-RFD consistently outperforms other baselines and the improvement is more significant for smaller K . Second, E-RFD outperforms Ranking-RFD and Rating-RFD slightly. This indicates that leveraging two kinds of evidences is more effective than only using one type of evidences, even if without evidence aggregation. Third, by comparing Ranking-RFD and Rating-RFD, we can ob-serve that the ranking based evidences are more effective than rating based evidences. This is because rating manip-ulation is only a supplementary to ranking manipulation. Finally, EA-RFD outperforms E-RFD, which validates the effectiveness of our evidence aggregation approach.
As introduced in Section 4, our approach can be used for evaluating the credibility of Apps by Equation 21. Here, we study the performance of evaluating App credibility based on the prior knowledge from existing reports. Specifically, as reported by IBTimes [4], there are eight free Apps which might involve in ranking fraud. In this paper, we use seven of them in our data set ( Tiny Pets , Social Girl , Fluff Friends , Crime City , VIP Poker , Sweet Shop , Top Girl ) for evalua-tion. Indeed, we try to study whether each approach can find these suspicious Apps with high rankings, since a good ranking fraud detection system should have the capability of capturing these suspicious Apps. Particularly, instead of setting a fixed fraud threshold  X  in Equation 21, we treat top 10% ranked leading sessions as suspicious sessions to compute the credibility of each App. Table 4 shows the top percentage position of each App in the ranked list returned by each approach. We can see that our approach EA-RFD can rank those suspicious Apps into higher positions than other baseline methods.

Figure 12 shows the ranking records of the above Apps (Limited by space, we only show two of them). In this fig-ure, we find all these Apps have clear ranking based fraud evidences, which validate the effectiveness of our approach. A learning process is required for evidence aggregation. After learning the aggregation model on a historical data set, Figure 12: The demonstration of the ranking records of two reported suspicious Apps. ( a) Top Free 300 data set (b ) Top Paid 300 data set
Figure 13: Robustness of the aggregation model. each new test App can reuse this model for detecting ranking fraud. Specifically, the learnt weight parameters (i.e., w in our approach EA-RFD are 0 . 24 (0 . 22), 0 . 30 (0 . 28), 0 . 19 Free 300 (Top Paid 300) data set, respectively. It indicates that ranking based evidences are more effective than rating based evidences. However, it is not clear how many learning data are required? To study this problem and validate the robustness of our approach, we first rank all leading sessions by modeling with weight parameters learnt from the entire data set. Then we also rank all leading sessions by modeling with weight parameters learnt from different segmentation of the entire data set (i.e., 10%,...,100%). Finally, we test the root mean squared error (RMSE) of the ranking of lead-ing sessions between different results. Figure 13 shows the results of robust test on two data sets. We can find that the aggregation model does not need a lot of learning data, thus the robustness of our approach is reasonable.
Generally speaking, the related works of this study can be grouped into three categories.
 The first category is about Web ranking spam detection. Specifically, the Web ranking spam refers to any deliber-ate actions which bring to selected Web pages an unjusti-fiable favorable relevance or importance [14]. For example, Ntoulas et al. [11] have studied various aspects of content-based spam on the Web and presented a number of heuristic methods for detecting content based spam. Zhou et al [14] have studied the problem of unsupervised Web ranking spam detection. Specifically, they proposed an efficient online link spam and term spam detection methods using spamicity. Recently, Spirin et al. [13] have reported a survey on Web spam detection, which comprehensively introduces the prin-ciples and algorithms in the literature. Indeed, the work of Web ranking spam detection is mainly based on the analysis o f ranking principles of search engines, such as PageRank and query term frequency. This is different from ranking fraud detection for mobile Apps.

The second category is focused on detecting online re-view spam. For example, Lim et al. [10] have identified sev-eral representative behaviors of review spammers and model these behaviors to detect the spammers. Wu et al. [15] have studied the problem of detecting hybrid shilling attacks on rating data. The proposed approach is based on the semi-supervised learning and can be used for trustworthy product recommendation. Xie et al. [16] have studied the problem of singleton review spam detection. Specifically, they solved this problem by detecting the co-anomaly patterns in mul-tiple review based time series. Although some of above ap-proaches can be used for anomaly detection from historical rating records, they are not able to extract fraud evidences for a given time period (i.e., leading session).
 Finally, the third category includes the studies on mobile App recommendation. For example, Yan et al. [17] devel-oped a mobile App recommender system, named Appjoy, which is based on user X  X  App usage records to build a pref-erence matrix instead of using explicit user ratings. Also, to solve the sparsity problem of App usage records, Shi et al. [12] studied several recommendation models and pro-posed a content based collaborative filtering model, named Eigenapp, for recommending Apps in their Web site Get-jar. In addition, some researchers studied the problem of exploiting enriched contextual information for mobile App recommendation. For example, Zhu et al. [19] proposed a uniform framework for personalized context-aware recom-mendation, which can integrate both context independency and dependency assumptions. However, to the best of our knowledge, none of previous works has studied the problem of ranking fraud detection for mobile Apps.
In this paper, we developed a ranking fraud detection system for mobile Apps. Specifically, we first showed that ranking fraud happened in leading sessions and provided a method for mining leading sessions for each App from its his-torical ranking records. Then, we identified ranking based evidences and rating based evidences for detecting ranking fraud. Moreover, we proposed an optimization based aggre-gation method to integrate all the evidences for evaluating the credibility of leading sessions from mobile Apps. An unique perspective of this approach is that all the evidences can be modeled by statistical hypothesis tests, thus it is easy to be extended with other evidences from domain knowledge to detect ranking fraud. Finally, we validate the proposed system with extensive experiments on real-world App data collected from the Apple X  X  App store. Experimental results showed the effectiveness of the proposed approach.
Acknowledgement. This work was supported in part by grants from Natural Science Foundation of China (NSFC, Grant No. 61073110 and 71028002), Research Fund for the Doctoral Program of Higher Education of China (Grant No. 20113402110024), the Key Program of National Natural Sci-ence Foundation of China (Grant No. 60933013), and Na-tional Key Technology Research and Development Program of the Ministry of Science and Technology of China (Grant No. 2012BAH17B03). The work was also partially sup-ported by grants from National Science Foundation (NSF) via grant numbers CCF-1018151 and IIS-1256016.
