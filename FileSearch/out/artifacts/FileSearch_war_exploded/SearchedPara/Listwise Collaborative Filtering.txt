 Recently, ranking-oriented collaborative filtering (CF) algo-rithms have achieved great success in recommender systems. They obtained state-of-the-art performances by estimat-ing a preference ranking of items for each user rather than estimating the absolute ratings on unrated items (as conventional rating-oriented CF algorithms do). In this paper, we propose a new ranking-oriented CF algorithm, called ListCF. Following the memory-based CF framework, ListCF directly predicts a total order of items for each user based on similar users X  probability distributions over permutations of the items, and thus differs from previous ranking-oriented memory-based CF algorithms that focus on predicting the pairwise preferences between items. One important advantage of ListCF lies in its ability of reducing the computational complexity of the training and prediction procedures while achieving the same or better ranking performances as compared to previous ranking-oriented memory-based CF algorithms. Extensive experiments on three benchmark datasets against several state-of-the-art baselines demonstrate the effectiveness of our proposal. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Information filtering Recommender systems; Collaborative filtering; Ranking-oriented collaborative filtering
In recent years, recommender system has become a key technology behind e-commerce to help customers find their Indicates equal contribution.
 c  X  desirable products [14, 22]. As one of the most successful recommendation techniques, collaborative filtering (CF) avoids the necessity of collecting extensive content infor-mation about items and users by fully utilizing the user-item rating matrix to make recommendations. As a result, CF can be easily applied to different recommender systems without requiring additional domain knowledge [15]. In the literature, many CF algorithms have been proposed, which roughly fall into two categories: rating-oriented CF and ranking-oriented CF.

Rating-oriented CF, such as user-based CF [1, 5], predicts a user X  X  potential ratings on unrated items based on the rating information from other similar users. The similarity between two users is calculated based on their rating scores on the set of commonly rated items. A popular similarity measure is the Pearson correlation coefficient [5]. Since this technique focuses on predicting each user X  X  rating on an unrated item, we refer to it as pointwise CF. Please note that pointwise CF might not be able to achieve the ultimate goal of recommender systems, which is to present a ranking or recommendation list to a user rather than predict the absolute value of the ratings [25]. In fact, it has been observed that improvements in pointwise error metrics such as RMSE (Root Mean Squared Error) do not necessarily lead to improved ranking effectiveness [4].

Different from rating-oriented CF, ranking-oriented CF algorithms [18] directly generate a preference ordering of items for each user without the intermediate step of rating prediction. Related works include memory-based algorithms such as EigenRank [15] and VSRank [27, 28], and model-based algorithms such as CoFiRank [29], ListRank-MF [25] and CCF [30]. In this paper, we focus on memory-based CF algorithms since they have demonstrated many advantages such as strong robustness, interpretability, and competitive performance [6].

Existing memory-based ranking-oriented CF algorithms [15, 27, 28] can also be referred to as pairwise ranking-oriented CF, since they predict pairwise ordering of items based on the correlation between users X  pairwise preferences of the items. A commonly used correlation measure is the Kendall X  X   X  correlation coefficient [17]. Although pairwise ranking-oriented CF has demonstrated significant improve-ment over previous algorithms in terms of ranking accuracy, it suffers from high computational complexity. Suppose there are N items, and each user has rated n items on average. The average number of commonly rated items for each pair of users is n , and the number of the neighbor users is l . Then the time complexity of computing the correlation between each pair of users is O n + n 2 the time complexity of the ranking prediction procedure is O l ( N  X  n ) 2 for each user. In most cases, the majority of the items are unrated for each user, resulting in a very large value of N  X  n . As a consequence, the computations of the pairwise ranking-oriented CF algorithms are usually much more expensive than the pointwise rating-oriented CF algorithms.

Beside the computational complexity, there could be problems with the accuracy of pairwise ranking-oriented CF algorithms too. First, Kendall X  X   X  correlation coefficient ignores the ranking positions of the items, which may lead to inaccurate similarities when discovering similar users. Second, Kendall X  X   X  correlation coefficient cannot handle the equal preferences tie, which may cause information loss in training and prediction. Third, pairwise ranking-oriented CF algorithms attempt to predict relative preferences be-tween pair of items rather than the total rankings, which may also result in accuracy loss due to rank aggregation. To tackle the aforementioned problems with the pairwise CF algorithms, we propose ListCF, a listwise memory-based ranking-oriented CF algorithm, which can reduce the time complexity of both the training and prediction phases while maintaining or even improving the prediction accuracy as compared to conventional pairwise ranking-oriented CF algorithms. ListCF directly predicts a ranking list of items for the target user based on the probability distributions over the permutations of items and thus avoids the accuracy loss caused by the prediction and aggregation of pairwise preferences.

In particular, ListCF utilizes the Plackett-Luce model [17], a widely used permutation probability model in the literature of learning to rank, to represent each user as a probability distribution over the permutations of rated items. In this way, both the ranking positions and equal preferences can be well considered. In ListCF, the similarity between two users is measured based on the Kullback X  Leibler (KL) divergence [11] between their probability dis-tributions over the set of commonly rated items. Then for each user, those users who have higher similarity scores are selected as the set of neighborhood users. Given a target user and the neighborhood users, ListCF infers predictions by minimizing the cross entropy loss between their probability distributions over permutations of items with gradient descent. Extensive experiments on three benchmark datasets in comparison with several state-of-the-art algorithms demonstrate the advantages of our approach.
The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 formulates the listwise collaborative filtering paradigm and presents the details of our algorithm. Section 4 analyzes the relationships between our listwise CF approach and conventional memory-based CF approaches. Section 5 reports the experimental results and analysis, and Section 6 concludes the paper.
In recent years, many recommendation algorithms have been proposed, which fall into content-based, collaborative filtering (CF) and hybrid algorithms. Content-based algo-rithms make recommendations based on similarities between the vector representing the user profile and the vectors representing the items, where the vectors for user profile and items are weighted explicit features [16]. CF avoids the necessity of collecting extensive content information about items and users by fully utilizing the user-item rating matrix to make recommendations. Thus, CF can be easily adopted in different recommender systems without requiring additional domain knowledge [15]. Given the effectiveness and convenience, many CF algorithms have been proposed, which are either rating-oriented CF or ranking-oriented CF. In addition, hybrid recommendation algorithms have also been proposed to combine two or more recommendation techniques to gain better performance [2].

In this paper, we focus on CF recommendation algorithms and propose a listwise CF algorithm which demonstrates advantages in recommendation efficiency and accuracy.
Traditional rating-oriented CF predicts the absolute value of ratings of individual users to unrated items, thus we refer to it as pointwise CF. Conventionally, pointwise rating-oriented CF can be either memory-based or model-based.
In memory-based rating-oriented algorithms, two rep-resentatives are user-based CF [5] and item-based CF [14, 23], which make predictions by utilizing historical ratings associated with similar users or similar items. The commonly used similarity metrics are Pearson correlation coefficient [5] and cosine similarity [1].

Model-based rating-oriented CF learns a model based on the observed ratings to make rating predictions. Latent factor models such as matrix factorization (MF) [9] that uses dimensionality reduction techniques have become important research directions in this area. A variety of adaptations of the MF model have demonstrated promising prediction performance, such as Probabilistic Matrix Factorization (PMF) [19], Non-negative Matrix Factorization (NMF) [12], and Max-Margin Matrix Factorization (MMMF) [21].
Different from rating-oriented CF, ranking-oriented CF [18] directly predicts a preference ranking of items for each user. Similarly, ranking-oriented CF algorithms can be also categorized into memory-based ranking-oriented CF [15, 27] and model-based ranking-oriented CF [10, 20, 25].
 EigenRank [15] is a classic memory-based ranking-oriented CF algorithm, which measures the similarity between users based on the correlation between their pairwise rankings of the co-rated items and ranks items by the preferences of similar users. VSRank [27] adopts the vector space model in representing user pairwise preferences and makes further improvement by considering the relative importance of each pairwise preference. Resulting from the pairwise preference based representations and predictions, we refer to EigenRank and VSRank as pairwise memory-based ranking-oriented CF algorithms. Although they achieve better per-formance in recommendation accuracy than rating-oriented CF, they suffer from serious efficiency problem. In this paper, we propose a listwise memory-based ranking-oriented CF algorithm to reduce the computational complexity while maintaining or even improving the ranking performance.
A variety of model-based ranking-oriented CF algorithms have been presented by optimizing ranking-oriented objec-tive functions, e.g., bayesian personalized ranking (BPR) [20], CLiMF [24], CoFiRank [29], and ListRank-MF [25]. BPR and CLiMF model the binary relevance data and optimize binary relevance metrics, i.e., Area Under the Curve (AUC) in BPR and Mean Reciprocal Rank (Mean) in CLiMF, which are not suited for graded relevance data. CoFiRank [29] minimizes a convex upper bound of the Normalized Discounted Cumulative Gain (NDCG) [7, 8] loss through matrix factorization while ListRank-MF [25] integrates the learning to rank technique into the matrix factorization model for top-N recommendation.
In this section, we propose ListCF, a listwise ranking-oriented collaborative filtering (CF) algorithm based on the memory-based CF framework. Generally, memory-based CF works in two phases [28]: The similarity calculation phase (Phase I) calculates the similarities between pair of users, based on which a set of most similar neighborhood users can be discovered for each user; Then the ranking prediction phase (Phase II) predicts the preference ordering of unrated items for each target user by aggregating preferences of neighborhood users for recommendation purpose.
In ListCF, we utilize Plackett-Luce model [17], a widely used permutation probability model, to represent each user as a probability distribution over the permutations of rated items. It is assumed that given a ranking function on a set of items, any permutation on the item set is possible. However, different permutations may have different proba-bilities, and more  X  X orrect X  permutations where items with larger ranking scores are ranked higher are assigned with larger probabilities.

Let I be a set of items. A permutation of I is a bijection from I to itself and it can be represented as an ordered list  X  =  X   X  1 , X  2 ,..., X  n  X  , where  X  i  X  I denotes the item at position i and  X  i 6 =  X  j if i 6 = j . The set of all the possible permutations of I is denoted as  X  I .

Definition 1. (Probability of permutation). Let  X  (  X  ) be an increasing and strictly positive function. Given a permutation  X  =  X   X  1 , X  2 ,..., X  n  X  and the rating scores of as follows:
In this paper, we use the exponential function  X  ( r ) = e For a set of n items, the number of different permutations is n !. Thus, it is too time-consuming to calculate the probabilities of all the permutations. To cope with this problem, an alternative solution is the top-k probability model [3] (1  X  k  X  n ), which only focuses on the permutations of items within the top-k positions.

Definition 2. (Top-k Permutation set). Given a set of items I , the top-k permutation set G k ( i 1 ,i 2 is a set containing all the permutations of I which share the same top-k items, i.e., i 1 ,i 2 ,...,i k where i 1 ,i Formally,
Two top-k permutation sets of I are different if the items in the top-k positions are different. Let G I k denote the set of different top-k permutation sets for I . Please note that the of G I k is n ! / ( n  X  k )!.

Example 1. Let I = { i 1 ,i 2 ,i 3 } . There are totally 3! = 6 possible permutations in  X  I :  X  i 1 ,i 2 ,i 3  X  i ,i 1 ,i 3  X  ,  X  i 2 ,i 3 ,i 1  X  ,  X  i 3 ,i 1 ,i 2  X  , and  X  i merely 3! / 2! = 3 top-1 permutation sets: G I 1 = {G 1 ( i G ( i 3 ) } , where G 1 ( i 1 ) = { X  i 1 ,i 2 ,i 3  X  ,  X  i { X  i 2 ,i 1 ,i 3  X  ,  X  i 2 ,i 3 ,i 1  X  X  , and G 1 ( i 3 ) = { X  i
Based on the Definition 2, the probability of the top-k probabilities of permutations in which the items i 1 ,i 2 are ranked in the top-k positions. To calculate all the proba-bilities of top-k permutation sets in G I k , we need consider n ! / ( n  X  k )! top-k permutation sets, each containing ( n  X  k )! permutations of I . Therefore we still have to calculate n ! / ( n  X  k )!  X  ( n  X  k )! = n ! probabilities of permutations in total. In [3], an efficient way to calculate the probability of the top-k permutation set P ( G k ( i 1 ,i 2 ,...,i k )) is shown in Lemma 1.
 Lemma 1. The probability of the top-k permutation set i ,i 2 ,...,i k are ranked in the top-k positions respectively. Formally, position l , l = 1 , 2 ,...,n .

Obviously, the probabilities of the top-k permutation sets in G I k form a probability distribution, which can be calculated much more efficiently than the probability dis-tribution over G I . In this study, we refer to the probability distribution over the top-k permutation sets as the top-k probability model , which uses the top-k permutation sets instead of the full permutations of all the items.

In recommender systems, for each pair of users u and v , let I u,v be the set of their commonly rated items. Then G k represents the set of top-k permutation sets of I We use P u and P v to denote the probability distributions of u and v over G I u,v k according to the top-k probability model, which can be calculated with Equation (1) based on their rating scores. Thus the similarity between user u and v can be estimated as the similarity between the probability distributions of P u and P v .

In this study, we use Kullback-Leibler (KL) divergence [11] based metric for similarity calculation, since it is a common measure of the difference between two probability distributions in probability theory and information theory. Given two users u and v , the KL divergence of P u from P is defined as follows: where P u ( g ) and P v ( g ) is the probability of the top-k permu-tation set g  X  G I u,v k for u and v respectively. D KL ( P can be calculated in the similar way. Obviously, the KL di-vergence is asymmetric, i.e., D KL ( P v k P u ) 6 = D KL In this study, we define a symmetric similarity metric based on the KL divergence, which is shown as follows:
The similarity metric in Equation (2) suffers from com-puting high similarity between users with few ratings in common. Like in [5], this can be alleviated by multiplying threshold. When the number of commonly rated items falls below this threshold, the similarity is scaled down.
Since D KL ( P v k P u ) ,D KL ( P u k P v )  X  0, according to Equa-tion (2), the similarity between each pair of users is no greater than 1. Formally,  X  u,v  X  U : s ( u,v )  X  1. The similarity between u and v reaches the maximum bound ( s ( u,v ) = 1) only if their probability distributions P P v over the top-k permutation sets are the same. We also note that s ( u,v ) may be negative and has no lower bound. However, this is not an important issue in our model because we are just interested in discovery of limited number of users with high similarities to the target user as the neighborhood users.
For a target user u , let N u be the set of neighborhood users and T u = { t 1 ,t 2 ,...,t p } be the set of items to be predicted. The ultimate goal of ranking-oriented CF is to predict a preference ranking of items in T u for u .
Let  X  P u be the probability distribution over the set of top-k permutation sets G T u k to be predicted for u . In order to guarantee that the sum of the probabilities in  X  P u is equal to 1, for each top-k permutation set  X  g  X  X  T u k , the probability  X  P ( g ) is denoted as follows: where {  X  u,g | X  g  X  G T u k } are unknown variables to be predicted for user u and P g  X  X  T u
In this study, following the same assumption of memory-based CF, we make predictions based on a set of neighbor-hood users with similar ranking preferences to the target user. If users have similar ranking preferences in the past, they are most likely to have similar ranking preferences in the future, which means that  X  P u should be as close as possible to the probability distributions of the neighborhood users over their sets of top-k permutation sets.

We use the cross entropy loss function to make predic-tions, as it is widely-used for optimizing similarity/distance between probability distributions in machine learning, and it has been successfully used in the text mining [3], multimedia retrieval [13], and pattern recognition [26] fields.
Given a target user u with a set of unrated items T u and a neighborhood user v  X  N u , let I v be the set of items that the neighborhood user v has rated, and T u,v = T u  X  I v . Thus G k is the set of top-k permutation sets of T u,v . Let and P 0 v be the respective probability distributions of u and v over G T u,v k . The cross entropy between  X  P 0 u and P calculated as follows: Please note that in the above equation, P 0 v is different from P v that is used in similarity calculation. As they are based on different sets of items, where P 0 v is based on T u,v P v is based on the commonly rated items I u,v = I u  X  I v of user u and v . Obviously P
ListCF attempts to make predictions by minimizing the weighted sum of the cross entropy loss between the proba-bility distributions of the target user and his neighborhood users over the set of the top-k permutation sets of T u,v a target user u and the set of unrated items T u , the cross entropy-based loss function is:
According to Equation (2) and Equation (4), the objective function can be specified as follows:
F (  X  u ) = X =  X  X =  X  X =
X
We optimize Equation (6) with the gradient descent method. The derivative of F with respect to  X  u,g is: = X = X
Given a learning rate  X  , the update formula of the gradient descent method is: Algorithm 1: The ListCF Algorithm Input : An item set I , a user set U , and a rating matrix
Output : A ranking  X   X  u of items for each user u  X  U . 1 for u  X  U do 2 for v  X  U and u 6 = v do 3 P u ,P v  X  TopKProDist( I u ,I v ,R ) /* Eq.1 */ 4 sim ( u,v )  X  Similarity( P u ,P v ) /* Eq.2 */ 5 end 6 N u  X  SelectNeighbors( { sim ( u,v ) } v  X  U/u ) 7 end 8 for u  X  U do 9 t = 1 10 repeat 11  X  = 0 12 Initialize(  X  0 u ) 13 for g  X  X  T u k do 14  X  t u,g  X  Update( N u ,sim,R ) /* Eq.8 */ 16 end 17 t  X  t + 1 18 until t &gt; maxIteration or  X  &lt; ; 19 for t  X  T u do 20 P ( t )  X  Aggregation( {  X  u,g } g  X  X  T u 21 end 22  X   X  u  X  Ordering( { P ( t ) } t  X  T 23 end
After updating  X  u,g , we can easily get the probabilities  X  P ( g ) according to Equation (3), which meets the constraint
In order to obtain an ultimate ranking of T u for each user u , the probabilities of the top-k permutation sets need to be aggregated. One intuitive way is to get the probabilities of the top-1 permutation sets and rank the items in T according to the descending order of the corresponding probabilities. The probability of the top-1 permutation set P ( G T u 1 ( t j )) actually equals to the sum of the probabilities of the top-k ( k &gt; 1) permutation sets with the item t ranked in the top-1 position. Here gives an example.
Example 2. Suppose user u has items I = { i 1 ,i 2 ,i 3 } to be recommended. We have probability distribution { 0 . 2 , 0 . 3 , G 2 = {G 2 ( i 1 ,i 2 ) , G 2 ( i 1 ,i 3 ) , G 2 ( i 2 ,i 1 G ( i 3 ,i 2 ) } . We can obtain the probabilities of that each item 0 . 5 ,P ( G 1 ( i 2 )) = 0 . 1 + 0 . 1 = 0 . 2 , and P ( G 0 . 2 = 0 . 3 . Since P ( G 1 ( i 1 )) &gt; P ( G 1 ( i recommendation list is ordered as  X  i 1 ,i 3 ,i 2  X  . The pseudocode of the ListCF algorithm is shown in Algorithm 1, where lines 1-7 calculate the similarities between users and discover the neighbor users for each target user, and lines 8-23 predict the ranking of items to make recommendations. When k &gt; 1 in the top-k probability model, lines 19-21 aggregate the probabilities of the top-k permutation sets to obtain the probabilities of the top-1 permutation sets, that are the probabilities of each item t  X  T u ranked in the first position.

In the proposed ListCF model, we use the top-k ( k  X  1) probability model [3] in the similarity calculation and ranking prediction phases. ListCF with different values of k may achieve different recommendation performances. On one hand, generally the accuracy will be higher when a larger k value is used. On the other hand, the computational complexity of similarity calculation and ranking prediction procedures will be exponentially increased. In particular, for a user u associated with n items, the number of permutation sets in the top-k probability model is n ! / ( n  X  k )!, and specially this number is merely n when k = 1. Thus the time complexity of ListCF based on the top-k ( k &gt; 1) probability model is ( n  X  1)! / ( n  X  k )! times higher than that of ListCF based on the top-1 probability model.
 Table 1: Accuracy and efficiency comparison of ListCF with different k .
For illustration purpose, we randomly selected 1000 users from the MovieLens-1M dataset and conducted experiments to compare the ranking accuracy as well as the runtime of the similarity calculation and ranking prediction phases (Phase I and II respectively) of ListCF when k = 1 , 2 and 3. The size of neighborhood users was set to be 50, and 50 unrated items were predicted for each user in Phase II. The results are reported in Table 1.

From the table we can see that, when k &gt; 1 in ListCF, the improvement in recommendation accuracy is very subtle, but the degeneration of efficiency is extremely significant. For example, ListCF with k = 3 achieves about 0.02 improvement in NDCG@5 over ListCF with k = 1, but it takes 433 and 488 times longer in Phase (I) and Phase (II) respectively. Obviously, the loss outweighs the gain. Thus we set k = 1 when compare ListCF with other recommendation algorithms in the experimental section.
Although ListCF, as a listwise memory-based CF algo-rithm, is completely different from pointwise and pairwise memory-based CF algorithms in similarity calculation and ranking prediction, these three categories of algorithms inherently share similar prediction strategy.

Theorem 1. ListCF predicts the probability of a top-k permutation set as the weighted average of the neighbors X  probabilities of the corresponding top-k permutation sets, if  X  v  X  N u : T u,v  X  T u holds. Formally, for  X  g  X  X  T u k
Proof. If  X  v  X  N u : T u,v  X  T u , then  X  v  X  N u : G T u,v G k . For the optimization problem in Equation (5), set the derivative of F (Equation 7) to be 0: Thus and
Theorem 1 presents the prediction formula of ListCF in a special case when all the neighbor users have rated the items in test set T u of user u , i.e.,  X  v  X  N u : T T u holds. In this case, for  X  g  X  G T u k , the probability  X  P ( g ) can be predicted as the weighted average of the neighbors X  corresponding probabilities, which is the same as the pairwise preference prediction formula in pairwise memory-based CF [15, 27, 28] as well as the rating prediction formula in the pointwise memory-based CF after performing the normalization of the ratings, i.e., the average rating of the user u has been deducted from each rating r u,i
Now we explain why we cannot obtain such a result without the constraint of  X  v  X  N u : T u,v  X  T u in ListCF while we can with the constraint. In pointwise CF and pairwise CF, the ratings and the pairwise preferences of users are constant. However, in listwise CF, the probability distributions of each pair of users u and v in the ranking prediction phase vary with respect to the set of items T u,v (1) Given a target user u , for any two different neighborhood the length of the probability distribution  X  P u of user u could change and the values in  X  P u could be different; (2) Given two different target users u 1 and u 2 who have a same neighborhood user v , the probability distribution of P v for user v could also be different. Thus we cannot simply make predictions with a weighted average formula as pointwise and pairwise CF do. However, with the constraint of  X  v  X  N u : T u,v  X  T u , G T u,v k = G T u k , the probabilities and P 0 v ( g ) (  X  g  X  X  T u k ) in the ranking prediction formula are invariable, which is the same as those in pointwise CF and pairwise CF. In this case, we can obtain the same result shown in Theorem 1. In comparison with conventional pairwise ranking-oriented CF, listwise CF demonstrates many advantages in efficiency (when k = 1) and accuracy on account of the listwise representation.

On the efficiency side, let M and N be the number of users and items. Suppose each user has rated n items on average, and the number of commonly rated items for each pair of users is n . In the similarity calculation phase, the time complexity of ListCF is O M 2 ( n + n ) . Recall that the complexity of pairwise CF is O M 2 ( n + n 2 ) , which is ( n + n 2 ) / ( n + n ) times higher than that of ListCF. Suppose each user has l neighbor users on average, and each prediction process maximally performs d iterations for gradient descent optimization. In the ranking prediction phase, the time complexity of ListCF is O ( M ( N  X  n ) ld ), where N  X  n is the number of items to recommend. Recall that the complexity of pairwise CF is O M ( N  X  n ) which is ( N  X  n ) /d times higher than that of ListCF, and d N  X  n in our experiments. Since d is a constant, the ratio of the complexity of pairwise CF to that of ListCF increases linearly with the growth of the number of items to be predicted in the ranking prediction phase.

On the accuracy side, firstly, permutation probability based similarity metric considers the ranking position infor-mation of items, which can discover more accurate neighbor users with similar preferences. Secondly, permutation probability based representation can easily handle the the case of equal preferences on items, i.e., assigns equal probability to the permutations in which the top items have equal ranking scores. Thirdly, ListCF attempts to make top-n recommendation by optimizing listwise loss function, which is more natural and accurate. The following example demonstrates the accuracy loss of the Kendall X  X   X  correlation coefficient because of not considering the ranking position information of items.

Example 3. Suppose there are three users U = { u 1 ,u 2 ,u and three items I = { i 1 ,i 2 ,i 3 } , where u 1 assigned ratings of { 5 , 3 , 4 } to items, u 2 assigned { 5 , 4 , 3 } , and u { 4 , 3 , 5 } .

In pointwise CF, the similarity (Pearson correlation co-efficient) between u 1 and u 2 and the similarity between u and u 3 are  X  ( u 1 ,u 2 ) =  X  ( u 1 ,u 3 ) = 0 . 5 .
In pairwise CF, the similarity (Kendall X  X   X  correlation coefficient) between u 1 and u 2 and the similarity between u 1 and u 3 are  X  ( u 1 ,u 2 ) =  X  ( u 1 ,u 3 ) = 0 . 333 .
In ListCF, according to Equation (1) , the top-1 probability 0 . 245) , P u 2 = (0 . 665 , 0 . 245 , 0 . 090) , and P u s ( u 1 ,u 3 ) = 0 . 395 , and thus s ( u 1 ,u 2 ) &gt; s ( u
Since recommender systems aim to recommend the items that users may be most interested in, we should emphasize the items with highest scores in the similarity calculation and ranking prediction phases, and thus u 2 is more similar to u than u 3 in Example 3. Obviously, ListCF can successfully recognize it while pointwise CF and pairwise CF cannot. #users 6,040 36,656 429,584 #items 3,952 1,623 17,770 #ratings 1,000,209 2,580,222 99,884,940 #ratings/user 165.6 70.4 232.5 #ratings/item 253.1 1589.8 5621.0 sparsity 93.7% 95.7% 98.7%
We conducted a series of experiments on three real-world rating datasets: Movielens-1M, EachMovie, and Netflix. All of these datasets are commonly used benchmark datasets in evaluating recommendation performance. The ratings in Movielens-1M and Netflix are on a scale from 1 to 5, while the EachMovie rating scale is from 1 to 6. In order to guarantee that there are adequate number of ratings per user for training and testing, we removed those users who have rated less than 20 items from these datasets. Several statistics on the resulting datasets are presented in Table 2, where the sparsity levels in the last row are calculated as follows [23]:
In our experiments, we mainly compared the performance of ListCF with three memory-based CF algorithms of PointCF[1], EigenRank [15] and VSRank [27, 28], a classic pointwise rating-oriented algorithm and two state-of-the-art pairwise ranking-oriented algorithms respectively. In particular, PointCF uses the Pearson correlation coefficient to calculate the similarity between users, and ranks the items according to the predicted ratings for each user. In EigenRank and VSRank, the greedy aggregation method is used to aggregate the predicted pairwise preferences of items into total ranks. In the memory-based CF algorithms, the number of neighborhood users was set to be 100 in Movielens-1M and EachMovie, and 300 in Netflix. Since ListCF is also a memory-based CF algorithm, a direct comparison of them will provide valuable and irreplaceable insights.

Besides, we also chose CoFiRank [29] and ListRank-MF [25], two state-of-the-art model-based ranking-oriented CF algorithms for comparison to further demonstrate the pro-mising performance of ListCF. Like [29], the dimensionality of latent features was set be to 10. In order to evaluate the efficiency of our model, we chose PointCF and EigenRank as our comparison partners, as they share the same memory-based CF framework with ListCF. Since CofiRank 1 and ListRank-MF 2 are two model-based CF algorithms and the implementation of them is based on the publicly available software packages written in different program languages from us, we did not include them in this section.

We conducted experiments to compare their runtime of the similarity calculation phase and ranking prediction phase on the datasets. Due to the huge size of the Netflix data, we selected users from Netflix who had rated no more than 50 items to form an extremely sparse dataset containing about 100,000 users. As analyzed in Section 4.2, when the number of items each user has rated increases the difference between EigenRank and ListCF would be more distinct, and we omit the result here. Experiments were run on a computer with 4 core 2 GHz CPU and 64 GB RAM.
 Figure 1: Runtime comparison of the similarity calculation phase.

Figure 1 demonstrates the respective runtime of the simi-larity calculation phase of PointCF, ListCF and EigenRank on three datasets. Note that the units of the runtime are Second (s), Minute (min) and Hour (h) for the datasets of Movielens-1M, EachMovie and Netflix respectively. From Figure 1, we can observe that in the similarity calculation phase:
Figure 2 shows the runtime comparison of the ranking prediction phase of the three CF algorithms, where the horizontal axis in these four sub-figures is the number of items to be predicted for each user. The vertical axis in Figure 2(a), 2(b) and 2(c) is the runtime of the algorithms, and the vertical axis in Figure 2(d) is the runtime ratio of EigenRank to ListCF. From the figures we can see that, in the ranking prediction phase ListCF demonstrates sig-nificant improvement in prediction efficiency in comparison with EigenRank. http://www.CoFiRank.org/ http://mmc.tudelft.nl/users/yue-shi (c) Runtime on Netflix
To demonstrate the recommendation accuracy of our model, we compare ListCF with all of the state-of-the-art recommendation algorithms mentioned in Section 5.1.2 on three datasets of Movielens-1M, EachMovie and Netflix. For each dataset, we randomly select each user X  X  10 rated items and their corresponding ratings to form test set, and the remaining ratings are used to form train set.
For rating-based CF algorithms, the standard evaluation criteria are the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). Both criteria measure the error between true ratings and predicted ratings. Since our study focuses on improving item rankings instead of rating prediction, we employ the Normalized Discounted Cumulative Gain (NDCG) [7, 8] as the evaluation metric, which is the most popular accuracy metric in information retrieval for evaluating ranking of retrieved documents with multi-level relevance scores.

In the context of collaborative filtering, item ratings assigned by users can naturally serve as relevance judgments. The NDCG metric is evaluated over some number of the top items on the ranked item list. Let U be the set of users and r u be the rating score assigned by user u to the item at the p th position of the ranked list. The NDCG value at the n th position with respect to the given user u is defined as follows: where Z u is a normalization factor calculated so that the NDCG value of the optimal ranking is 1. NDCG @ n takes the mean of the NDCG u @ n over the set of users U , which is defined as follows: where | U | is the cardinality of the set of users. Figure 3: Ranking performance comparison on Movielens-1M and EachMovie.

Figure 3 shows the performance comparison results in terms of NDCG@1, 3 and 5 on the Movielens-1M and EachMovie datasets. From the figure we can observe that: improvements of ListCF over the corresponding approaches.
In previous experiments, we show that ListCF can gen-erate superior performance in top-n recommendation on Movielens-1M and EachMovie datasets compared with other recommendation algorithms. In order to compare our model with the other algorithms thoroughly, distinct from previous validations focusing on comparing global quality of recom-mendations over all users, here we are particularly interested in testing the performance of respective algorithms in regard to different groups of users. We first select user groups based on the number of observed ratings in the training set. We use group  X 20-50 X ,  X 50-100 X  and  X 100-200 X  as experimental subjects, and each user group contains about 90 thousand users. User group  X 20-50 X  denotes users who have rated more than 20 and less than 50 items in the training set. User groups represent different datasets with different sparsities. The sparsity of user group  X 20-50 X  is 99.87%, and it is the sparsest dataset among three datasets.
 The detailed experimental results are reported in Table 3. From the table we can see that:
In this paper, we propose ListCF, a listwise ranking-oriented model for collaborative filtering. ListCF mea-sures the user-user similarity based on the Kullback-Leibler divergence between users X  probability distributions over permutations of commonly rated items. It predicts item rankings for each user by minimizing the cross entropy loss between the target user and his neighbors with weighted similarities. Experimental results on three benchmark datasets show that ListCF demonstrates many advantages in recommendation efficiency and ranking accuracy. In addition, we prove that ListCF shares similar prediction strategy with pointwise and pairwise memory-based CF under certain constraints.

In the future, we plan to explore other possible listwise loss functions in ListCF. In addition, it is also interesting to investigate model-based listwise CF algorithms based on the top-k probability representations.
This work was supported by the Natural Science Foun-dation of China (61272240, 61103151, 71402083, 71171122), Humanity and Social Science Foundation of Ministry of Edu-cation of China (12YJC630211), the Natural Science founda-tion of Shandong province (ZR2012FM037, BS2012DX012), Academy of Finland (268078), the Doctoral Fund of Min-istry of Education of China (20110131110028).
