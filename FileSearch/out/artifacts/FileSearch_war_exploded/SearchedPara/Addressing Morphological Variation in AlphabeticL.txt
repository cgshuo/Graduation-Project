 The selection of indexing terms for representing documents is a key decision that limits how effective subsequent re-trieval can be. Often stemming algorithms are used to nor-malize surface forms, and thereby address the problem of not finding documents that contain words related to query terms through inflectional or derivational morphology. How-ever, rule-based stemmers are not available for every lan-guage and it is unclear which methods for coping with mor-phology are most effective. In this paper we investigate an assortment of techniques for representing text and compare these approaches using data sets in eighteen languages and five different writing systems.

We find character n-gram tokenization to be highly effec-tive. In half of the languages examined n-grams outperform unnormalized words by more than 25%; in highly inflective languages relative improvements over 50% are obtained. In languages with less morphological richness the choice of to-kenization is not as critical and rule-based stemming can be an attractive option, if available. We also conducted an experiment to uncover the source of n-gram power and a causal relationship between the morphological complexity of a language and n-gram effectiveness was demonstrated. H.3.1 [ Information Systems ]: Content Analysis and In-dexing X  linguistic processing, indexing ; H.3.3 [ Information Systems ]: Information Search and Retrieval Experimentation Tokenization, Stemming, Morphology, Character N-grams, CLIR
Failure to normalize morphologically related words ( e.g., swimmer, swam, swimming ), can prevent matches in full-text retrieval. The conventional approach is to apply a stem-ming algorithm to each word to transform document repre-sentations from bags of surface forms to bags of stemmed forms. Stemming is an approximation to morpheme identi-fication. It is usually considered to be a performance enhanc-ing technique, despite the fact that all stemming algorithms suffer from errors of over-and/or under-conflation. Though they are sometimes difficult to distinguish from one another, three broad classes of morphological processes result in sur-face forms that impair effective retrieval: inflection , deriva-tion , and word formation .

Inflectional morphemes add information to root morphemes such as number ( e.g., dog/dog+s ; fox/fox+es ) and gender ( e.g., act+or/act+ress , though English does not often in-flect for gender). Other functions such as negation ( e.g., un+happy ) and comparison ( e.g., fast/fast+er/fast+est ) can be indicated with inflectional (or grammatical) morphemes, though sometimes these are expressed through function words ( e.g., not happy ). The process of adding inflectional mor-phemes by attaching them to root morphemes is called ag-glutination. Some languages separate each morpheme into distinct words ( e.g., Chinese and Vietnamese), and these languages are termed isolating . However, affixation, the use of prefixes and suffixes to attach morphemes is extremely common. Languages that do this extensively are termed ag-glutinative . Languages vary in the degree of inflection and lie somewhere on the spectrum from isolating to strongly ag-glutinative. For example, English nouns only have two cases (singular and plural), but in Finnish, a highly agglutinative language, nouns can have fifteen different cases.

Derivational morphology transforms words from one syn-tactic class into another. For example compute (verb) can produce computer (noun); or boy (noun) can become an ad-jective through addition of the suffix -ish .

There are a variety of other methods for producing new words in a language, including: Some of these processes are more harmful than others. Com-pounding, because it is so pervasive, is often given special treatment.

There are other linguistic phenomena that complicate in-formation retrieval, including the classic problems of poly-semy, where a word can have multiple meanings, and syn-onymy, where the same concept can be expressed with dif-ferent word choices. Dialectical and spelling variants (and errors) have deleterious effect; however, the aim of this paper is to focus on issues of morphology in alphabetic languages. Towards that end we consider a variety of ways to represent text and compare them to each other while keeping other aspects of the process fixed.

The textual representations we investigated are described in Section 2. In Section 3 our data and methods are de-scribed and in Section 4 we present experimental results. Section 5 examines language variability and presents addi-tional experiments that provide insight into why character n-grams are an effective technique. We summarize our find-ings in Section 6.
There are a number of operations in processing text, which while they are deserving of study, are so commonplace that they were adopted for all of the conditions examined. These include case folding, punctuation removal, and truncation of long numeric quantities to at most six digits. In this paper we examine 18 tokenization alternatives. Each is de-scribed below. Examples of indexing terms generated from the phrase medical doctors are given in Table 1 for each of the tokenization methods.
Our baseline condition, words , is formed from tokens de-limited by spaces. Plain words are commonly used in web search where efficiency is paramount and precision is val-ued over recall. Words are also well justified in languages with little morphological complexity. Unlike stemming algo-rithms there are no errors of over-conflation, but words do suffer from polysemy.

Rule-based stemming is based on linguistically-inspired transformations. Snowball is a stemming algorithm compiler developed by Porter 1 . Given a language-specific ruleset the compiler can produce source code that transforms surface forms into stems. For about half of the languages studied we were able to run Snowball ( snow ).

The motivation behind statistical stemmers is that they are language-neutral and thus universally applicable. We used the Morfessor algorithm 2 [4] ( morf ), which is designed to accomodate languages with concatenative morphology. The algorithm does not restrict the number of morphemes
Available at http://snowball.tartarus.org/ Available at http://www.cis.hut.fi/projects/morpho/
Table 1: Examples of indexing term formation. words medical, doctors snow medic, doctor morf medical, doctor, s devowel m.d.c.l, d.ct.rs soundex 5030204, 3023062 lfs4 edic, doct lfs5 medic, docto trun4 medi, doct trun5 medic, docto 3-grams me, med, edi, dic, ica, cal, al , l d, do, ... 4-grams med, medi, edic, dica, ical, cal , al d, l do, 5-grams medi, medic, edica, dical, ical , cal d, al do, 6-grams medic, medica, edical, dical , ical d, cal do, 7-grams medica, medical, edical , dical d, ical do, sk41 regular 4-grams in addition to m.dic, me.ic, wisk41 word-internal 4-grams in addition to m.dic, win4 med, medi, edic, dica, ical, cal , doc, doct, win5 medi, medic, edica, dical, ical , doct, docto, that can be present in a single word and it possesses no ex-plicit knowledge about a language; it only requires a list of words in the language. The algorithm is based on the min-imum description length principle and it optimizes a cost function that measures how well the model represents the observed data and the combined lengths of the segments that make up the model X  X  vocabulary. The output is a segmen-tation for each word. For example, seabirds is represented as sea+bird+s . In our experiments all of a word X  X  segments were included in the inverted file.
Several phenomena create orthographic variants that could be normalized. Examples include dialect ( e.g., color / colour ), alternate transliterations ( e.g., Gorbachev / Gorbachyov ), spelling variation ( e.g., judgement / judgment ), and spelling mistakes ( e.g., congratulations / congradulations ). Addi-tionally inflectional variation, particularly conjugation, is of-ten achieved through vowel substitution ( e.g., write / wrote , throw / threw ).

We considered two forms of normalization, though the number of possibilities here is enormous. Vowel transforma-tion ( devowel ) was accomplished by replacing all vowels with a unique symbol such as mapping  X  X  X ,  X  X  X , etc. with  X . X  (dot), and by collapsing adjacent vowels to a single symbol. The other transformation was based on the Soundex algorithm ( soundex ) [14]. Unlike the traditional algorithm all letters were replaced with numerals, entire words were transformed ( i.e., the resulting string was not limited to 4 characters), and characters outside the English alphabet were replaced with the digit  X 7 X . This method was only applied to lan-guages written in the Latin script.
Selection of a single substring from a word has the poten-tial to provide morphological normalization if the root mor-pheme can be identified. Approaches based on fixed-length prefixes [1] and substrings [17] have been proposed and both methods are considered here. Truncation of words to at most the first 4 ( trun4 ) or 5 ( trun5 ) characters is motivated by the observation that many languages rely heavily on suffixa-tion. However, this approach is not likely to help much in the presence of prefixation and compounding. Frequency-based selection of a substring is based on the principle that affixes are likely to be common substrings, so corpus-informed se-lection of the least frequent substring of length 4 ( lfs4 ) or 5 ( lfs5 ) might coincide with the root morpheme.
Fixed-length character n-grams have been noted for their language-independence and they have been used with good effect in European languages [19] and ideographic languages of Asia [2, 5, 22]. The technique provides significant redun-dancy in the representation of a word  X  the representation for a text is based on the number of letters it contains, not the number of words. This redundancy has the advantage of not requiring precise identification of root morphemes because a sliding window of length n will be sure to overlap mor-phemes. The main drawback of the approach is in increased disk space and run-time costs associated with significantly larger indexing representations.

Lengths of n = 3 to n = 7 were considered using word-spanning substrings, which may have a benefit in capturing some phrasal cues. Lengths of n = 4 and n = 5 have been reported to be the most effective [19]. Additionally word internal n-grams of lengths 4 &amp; 5 were studied ( win4, win5 ).
Character skipgrams, n-grams with skipped letters, have been proposed for fuzzy name matching [25], for normaliza-tion in Arabic [21], a language with root-and-template mor-phology, and as a means of finding translations in related languages. J  X  arvelin et al. [11] examined skip bigrams (two characters with a single skip); however, multiple, and even non-adjacent skips are possible. Here 4-grams are combined with strings of length 5 that retain 4 letters and replace a single internal letter with a dot symbol. Skipgrams were created from both word-spanning ( sk41 ) and word-internal ( wisk41 ) n-grams. Large multilingual collections from TREC 3 , CLEF 4 , and FIRE 5 were used in our experiments. The test sets are gen-erally comprised of newswire articles and information about the collections is given in Table 2.

Each evaluation typically created 50 or so queries per year and sometimes the document collections for a given lan-guage increased in size as additional corpora became avail-able. Pooling was performed to efficiently create relevance judgments for the topic sets, however post-hoc use of these benchmarks for comparative evaluation is believed to be re-liable. http://trec.nist.gov/ http://www.clef-campaign.org/ http://www.isical.ac.in/~fire/ AR Arabic 75 383,872 TREC  X 01- X 02 BG Bulgarian 149 85,427 CLEF  X 05- X 07 BN Bengali 50 123,040 FIRE  X 08 CS Czech 50 81,735 CLEF  X 07 DE German 192 294,805 CLEF  X 00- X 03 EN English 367 87,653 CLEF  X 00- X 07 ES Spanish 156 454,041 CLEF  X 01- X 03 FA Farsi 50 166,774 CLEF  X 08 FI Finnish 120 55,344 CLEF  X 02- X 04 FR French 333 177,450 CLEF  X 00- X 06 HI Hindi 45 95,213 FIRE  X 08 HU Hungarian 148 49,530 CLEF  X 05- X 07 IT Italian 181 157,558 CLEF  X 00- X 03 MR Marathi 49 99,359 FIRE  X 08 NL Dutch 156 190,605 CLEF  X 01- X 03 PT Portuguese 146 210,734 CLEF  X 04- X 06 RU Russian 62 16,715 CLEF  X 03- X 04
SV Swedish 102 142,819 CLEF  X 02- X 03
Queries were formed using title and description fields from the topic statements. As our focus is on comparing different tokenization techniques we wanted the experiment to reflect only changes in the indexing representation for documents (and queries). Therefore we elected not to employ relevance feedback in our experiments because it might conflate issues of expansion methods and term weighting with the selection of indexing terms.

Performance was measured using mean average precision (MAP) based on the number of queries shown in Table 2. Queries with no known relevant documents did not affect the calculation. Significance testing was performed with the paired t -test [3] using the available queries from multiple years to increase the sensitivity of the experiments.
A statistical language modeling approach [8, 20] was used for document ranking and smoothing was accomplished us-ing linear interpolation using a constant of  X  = 0 . 5 in all conditions: Relative document term frequency was used to estimate P ( t | D ). P ( t | C ) was based on the mean relative document term frequency from documents in the collection.
Table 3 presents mean average precision for the indexing variants described in Section 2. Two averages across the languages are given. The first one listed is based on the eight languages supported by the Snowball stemmer. 6 The second is based on the full set of 18 languages.

When only the Snowball languages are considered we ob-serve that snow has an 11.5% relative advantage over unnor-malized words . Morfessor segments and the least frequent
Dutch, English, Finnish, French, German, Italian, Por-tuguese, and Spanish. N +33.0% 0.2356 M +14.7% 0.2731 N +33.0% N +43.5% 0.2820 N +30.3% 0.3105 N +43.5% M +23.5% 0.3173 M +20.6% 0.3247 M +23.5% N +45.1% 0.3223 N +42.0% 0.3329 N +46.7% N +24.1% 0.4201 N +27.2% 0.4201 N +27.2% M +10.2% 0.3821 +5.6% 0.3986 M +10.2% N +46.5% 0.5078 N +49.1% 0.5078 N +49.1% M +5.7% 0.3930 N +8.0% 0.4019 N +10.5% N +36.1% 0.3271 N +34.7% 0.3305 N +36.1% N +89.6% 0.3624 N +83.4% 0.3746 N +89.6% N +60.0% 0.3739 N +45.4% 0.4164 N +61.8% N +10.6% 0.4243 N +11.3% 0.4243 N +11.3% N +27.5% 0.3330 M +24.7% 0.3739 N +40.0%
N +25.1% 0.4271 N +26.1% 0.4271 N +26.1% substring methods have modest gains of about 5%, but the n-grams of lengths n = 4 , 5 score the highest. The 5-grams at 15.9% edge out the 4-grams at 13.3%.

Interestingly a seemingly disruptive transformation such as discarding information about specific vowels ( devowel ) causes little harm (-2.6%), but the modified soundex drops performance markedly (-22.0%).

When all of the languages are examined direct compar-isons to Snowball cannot be made, but we see the 4-and 5-grams grow to a 22-23% advantage over words. 5-grams attain higher MAP in all 18 languages compared to plain words. 4-grams are marginally higher on average, but score lower than words in English and Italian. The word-spanning n-grams perform slightly better than the word-internal and skipgram forms. 3-grams are clearly too short to be effec-tive, but the longer lengths of n = 6 and n = 7 are still improvements over words.

The best non n-gram approach is trun5 (+16.2%), which retains at most the first five letters of a word. This gives two-thirds of the benefit of 4-grams but requires storage of only one posting entry per word. Performance is only slightly behind Snowball, but the method improves on words for all 18 languages.

In Table 4 we examine the top performing methods in greater detail. Mean average precision is given along with the relative improvement over the words baseline. Signifi-cant improvements with p &lt; 0 . 01 are indicated with solid triangles ( N ); open triangles indicate improvements with p &lt; 0 . 05 ( M ). Gains with 5-grams are statistically significant in 16/18 cases; 4-grams and trun5 each lead to significant improvements for 14 of the 18 languages.

Differences between n-grams and Snowball tend to be sig-nificant. 4-grams and 5-grams are statistically better in Ger-man, Finnish, and Swedish. Snowball is significantly better in English and Spanish (both), and in French and Italian (4-grams).

In Table 5 disk space usage and mean query response times are given for each of the the methods in Table 4. N-gram indexing can consume 6 times as much storage and queries can take 8 times as long to execute.
 Table 5: Storage requirements and execution speed using the CLEF 2003 English collection.
 words 4.7 60.0 0.51 snow 3.5 -26% 50.4 -16% 0.65 +27% trun5 1.6 -66% 47.0 -22% 0.90 +77% 4-grams 1.8 -62% 232 +287% 4.08 +700% 5-grams 10.9 +131% 391 +552% 4.42 +767%
The results on the Hungarian and Marathi collections are of particular note: n-grams were a better choice than words for 3 &lt; = n &lt; = 7. In Hungarian 4-and 5-grams were 80% more effective than words; for Marathi 4-grams yielded a 60% improvement.

N-grams were able to provide at least a 25% relative im-provement in Arabic, Bulgarian, Czech, German, Finnish, Hindi, Hungarian, Marathi, Russian, and Swedish. Notably absent from this list are any of the Romance languages. In fact, n-grams ( e.g., 5-grams) have the least advantage in English and in French, Italian, and Spanish.

Linguistic typology appears to affect the success of n-gram tokenization. One hypothesis that would account for this is that n-gram effectiveness is tied to morphological com-plexity. Though such methods are not without controversy among linguists, there have been studies that attempted to quantify morphological complexity using principles from in-formation theory.

Juola [12] examined translations of the Bible and erased morphology from each in the following way. Each word (or type) in a text is replaced with a unique symbol, a randomly selected integer. After this has been done to the entire text the words that normally exhibit morphological regularity, such as jump , jumped , jumping , no longer bear an obvious relationship to one another any more than do the numbers 18, 5429, and 1641. Juola then compared languages based Table 6: 5-gram effectiveness and linguistic com-plexity in European languages.
 on the ratio of the compressibility of the original text to the compressibility of the morphologically degraded text; the program gzip was used as a way of approximating the Kolmogorov complexity of the texts. Kettunen et al. [13] followed the approach described by Juola and performed a similar analysis using translations of the European Union Constitution in 21 languages and the program bzip2 .
In Table 6 data is presented that shows for each language: (1) the mean word length, by token; (2) the ratio that Juola computed to indicate morphological complexity (larger indi-cates greater complexity), if available; (3) Kettunen et al.  X  X  corresponding ratio for the language, if available 7 ; and (4) the relative improvement observed with 5-gram tokeniza-tion. The three estimates of morphological complexity can be used to rank languages by inferred complexity. Simi-larly the relative gains attained using 5-grams instead of words can also be used to order languages from those that gain much ( e.g., Hungarian and Finnish) down to those that gain little ( e.g., English and Spanish). The table also gives Spearman rank correlation coefficients, which show moder-ate to large correlations between each of the three estimates of morphological complexity and the gains attainable with 5-grams.
There are a number of factors that could be the underlying cause of the 20+% improvement with n-grams. The gains observed with n-grams could be due to: Spelling normalization can certainly provide gains, however this is somewhat difficult to quantify. We can say pretty con-
Kettunen et al.  X  X  ratios tend to be slightly higher, but for languages in common the agreement in rankings is good. Table 7: Example permutations using words from the CLEF 2000 English corpus.
 fidently that misspellings alone would not explain the large gains observed  X  spelling errors and variations are just not that common. Word-spanning and word-internal n-grams can be directly compared. From Table 3 we see that 4-grams and win4 achieve very similar performance. While word-spanning 5-grams are a bit more effective than their word-internal counterparts, it appears that the limited phrasal information from word-crossing n-grams would not explain what is occurring.

In an effort to establish whether or not coping with mor-phological processes such as inflection, derivation, and com-pounding is the prime reason behind n-gram X  X  monolingual effectiveness, we can attempt to remove morphology from language and see what changes occur. Inspired by Juola X  X  work in degrading morphology [12] a method of altering ev-ery word in the lexicon will be performed and retrieval exper-iments can be run against indexes created using word-based or n-gram-based tokenization on the transformed words. If the relative advantage of character n-grams disappears this will be support that it is by addressing morphology that n-grams improve on word-based indexing.

To remove morphological regularity we randomly shuffle the order of the characters in each word. Each word is thus transformed in a way that preserves its length, but makes it very hard to observe similarity between related lexemes (see Table 7). Short words and those with many repeated characters ( e.g., lull ) will bear a strong resemblance to their original forms even after scrambling the letters, but the ma-jority of surface forms will be considerably transformed. The effect on word-based indexing should be minimal, although some increase in polysemy is possible due to manufactured conflations in the transformed representations. This might happen because anagrams, such as team and meat , could become cognates through shuffling if each was converted to eamt . This method of removing morphology will not distin-guish between morphological processes such as inflection and compounding; some types of morphology may have a more significant impact on retrieval than others, but this exper-iment will not explain the relative contribution of different morphological processes in a language.

The first group in Table 7 illustrates that additional con-flations will occur as both ate and tea are transformed to aet , which has a document frequency near the sum of the num-ber of documents that the original terms appeared in. Ana-grams team and meat remain separate in the transformed space. No constraint was imposed to ensure that shuffled not, been scrambled throughout the corpus. forms differed from their original strings; while this is un-likely with longer terms, the word lull , which only has four possible forms depending on where the letter  X  X  X  is posi-tioned, is an example of a word left unaltered. Finally, the last grouping in the table demonstrates how related forms of the lexeme golf lack any resemblance in their scrambled representations.

We examined whether the relative effectiveness of n-grams changes when the letters of words are randomly scrambled. Figure 1 plots the percent change in performance for each language when character 4-grams are used instead of ordi-nary words. The vertical axis measures the relative gain (or loss) compared to words that is attributable to 4-gram in-dexing. The 0% threshold marks parity between the n-grams and words. Languages are ordered left-to-right by decreas-ing n-gram performance. The triangles indicate convention-ally produced 4-grams and circles are used for 4-grams that are generated from documents with permuted words.

We found that:
These results give strong evidence that it is the ability of overlapping character n-grams to capture regularity across morphologically related words forms that gives them their primary advantage.
This is not visible in Figure 1, but for words MAP was unaffected by the letter scrambling.

If it is the isolation of the root morpheme (or in com-pounds, roots) that is key, then these findings also suggest why longer length n-grams such as n = 6 and n = 7 are less effective than n = 4 and n = 5: longer sequences of characters are not focused on morphemes and fail to match some inflected allomorphs.

This also gives hope that the computational expense in-curred with n-gram indexing can be reduced through aggres-sive pruning based on detecting morphological roots. On early English collections (e.g., Cranfield, Medlars, and CACM) Harman found little advantage in stemming [7]. Hull reported average improvements of 1 to 3% [10]; how-ever, Krovetz found larger differences using CACM, NPL, TIME, and WEST [15] which he ascribed to addressing deriviational morphology. These foundational studies were based on the only available test collections of the time, which were in English. Since the number of English inflectional forms is low, it is not surprising that the observed differ-ences were not large.

The CLEF data sets have facilitated investigation of tok-enization methods in multiple European languages. Hollink et al. compared words, stems, lemmas, and some combi-nations such as n-grams over stems and lemmas [9]. Their results are consistent with those in this study. In particu-lar they found that stemming worked well in Romance lan-guages and for only one of eight languages did they find a technique that outperformed 4-grams significantly. More recently, Savoy has shown  X  X ight stemmers X , ones that only attempt to remove inflectional affixes, can be very effective [23]. He reported relative gains from 8% to 42%.

Corpus-based approaches to stemming have been stud-ied for over 30 years, beginning with methods based on successor varieties [6]. Xu and Croft examined word co-occurence statistics to enhance an existing stemmer or to induce one[24].
Recently the Morpho Challenge competition investigated the use of unsupervised morphological analysis for informa-tion retrieval, using English, Finnish, and German test sets from CLEF. Nearly all of the analyses produced by compet-ing systems outperformed the baseline condition, which left surface forms unaltered [16].

McNamee has conducted related tokenization experiments in European languages as well as experiments into full-text character skip-gram indexing and the use of automated rel-evance feedback with n-grams [18].
We compared a number of tokenization variants on test sets in diverse languages. The top performing method was word-spanning character n-gram indexing using n = 4 or n = 5, which is consistent with earlier reports in the liter-ature. Other n-gram variants performed well, but did not do as well as the nearly identically performing 4-grams and 5-grams. Another top performer was truncation of words to at most 5 letters. This method achieved much of the benefit of the n-grams yet it does not incur any run-time or disk space disadvantage. Both of these approaches are language-independent.

We noted differences based on language family. Rule-based stemming using the Snowball rulesets performed well in English and the Romance family, and in those languages it tended to outshine n-grams. In highly complex languages it proved essential to control for morphology to obtain the best results. In several languages relative improvements of 40% to 80% compared to words could be obtained using n-grams. We also showed strong correlations between 5-gram effectiveness and language complexity and conducted an ex-periment that showed that n-grams lose their power when morphology is removed from a language.

With the recent availability of test collections in diverse language families, we can now say that controlling for mor-phology is not optional, but is vital for effective multilingual IR. Furthermore, n-gram indexing is a strong default method that other approaches should be measured against. [1] P. Ahlgren and J. Kek  X  al  X  ainen. Indexing strategies for [2] A. Chen, J. He, L. Xu, Gey, F. C., and J. Meggs. [3] G. V. Cormack and T. R. Lynam. Validity and power [4] M. Creutz and K. Lagus. Unsupervised discovery of [5] S. Foo and H. Li. Chinese word segmentation and its [6] M. A. Hafer and S. F. Weiss. Word segmentation by [7] D. Harman. How effective is stemming? JASIS , [8] D. Hiemstra. Using Language Models for Information [9] V. Hollink, J. Kamps, C. Monz, and M. de Rijke. [10] D. A. Hull. Stemming algorithms: A case study for [11] A. J  X  arvelin, A. J  X  arvelin, and K. J  X  arvelin. S-grams: [12] P. Juola. Measuring linguistic complexity: the [13] K. Kettunen, M. Sadeniemi, T. Lindh-Knuutila, and [14] D. E. Knuth. Art of Computer Programming, Volume [15] R. Krovetz. Viewing morphology as an inference [16] M. Kurimo, M. Creutz, and V. Turunen. Overview of [17] J. Mayfield and P. McNamee. Single n-gram stemming. [18] P. McNamee. Textual Representations for [19] P. McNamee and J. Mayfield. Character n-gram [20] D. R. H. Miller, T. Leek, and R. M. Schwartz. A [21] S. H. Mustafa. Character contiguity in n-gram based [22] Y. Ogawa and T. Matsuda. Overlapping statistical [23] J. Savoy. Light stemming approaches for the French, [24] J. Xu and W. B. Croft. Corpus-based stemming using [25] J. Zobel and P. Dart. Finding approximate matches in
