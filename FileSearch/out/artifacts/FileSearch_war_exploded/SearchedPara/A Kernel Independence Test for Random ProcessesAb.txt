 University College London, Computer Science Department University College London, Gatsby Computational Neuroscience Unit Measures of statistical dependence between pairs of ran-dom variables ( X,Y ) are well established, and have been applied in a wide variety of areas, including fitting causal networks (Pearl, 2000), discovering features which have significant dependence on a label set (Song et al., 2012), and independent component analysis (Hyv  X  arinen et al., 2004). Where pairs of observations are independent and identically distributed, a number of non-parametric tests of independence have been developed (Feuerverger, 1993; 2010), which determine whether the dependence measure value is statistically significant. These non-parametric tests are consistent against any fixed alternative -they make no assumptions as to the nature of the dependence.
 For many data analysis tasks, however, the observations be-ing tested are drawn from a time series: each observation is dependent on its past values. Examples include audio signals, financial data, and brain activity. Given two such random processes, we propose a hypothesis test of instan-taneous dependence, of whether the two signals are depen-dent at a particular time t . Our test satisfies two important properties: it is consistent against any fixed alternatives, and it is non-parametric -we do not assume the depen-dence takes a particular form (such as linear correlation), nor do we require parametric models of the time series. We further avoid making use of a density estimate as an inter-mediate step, so as to avoid the assumption that the distri-butions have densities (for instance, when dealing with text or other structured data).
 We use as our test statistic the Hilbert-Schmidt Indepen-dence Criterion (HSIC) (Gretton et al., 2005; 2007), which can be interpreted as the distance between embeddings of the joint distribution and the product of the marginals in a reproducing kernel Hilbert space (RKHS) (Gretton et al., 2012, Section 7). When characteristic RKHSs are used, the HSIC is zero iff the variables are independent (Sriperum-budur et al., 2010). Under the null hypothesis of indepen-dence, P XY = P X P Y , the minimum variance estimate of HSIC is a degenerate U-statistic. The distribution of the empirical HSIC under the null is an infinite sum of inde-pendent  X  2 variables (Gretton et al., 2007), which follows directly from e.g. (Serfling, 2009, Ch. 5). In practice, given a sample ( x i ,y i ) n i =1 of pairs of variables drawn from P the null distribution is approximated by a bootstrap proce-dure, where a histogram is obtained by computing the test statistic on many different permutations { x i ,y  X  ( i ) decouple X and Y .
 In the case where the samples Z t = ( X t ,Y t ) are drawn from a random process, the analysis of the asymptotic be-haviour of HSIC requires substantially more effort than in the i.i.d. case. As our main contribution, we obtain both the null and alternative distributions of HSIC for random pro-cesses, where the null distribution is defined as X t being independent of Y t at time t . Such a test may be used for re-jecting causal effects (i.e., whether one signal is not depen-dent on the values of another signal at a particular delay) or instant coupling (see our first experiment in Section 4.2). The null distribution is again an infinite weighted sum of  X  2 variables, however these are now correlated, rather than independent. Under the alternative hypothesis, the statistic has an asymptotically normal distribution.
 For the test to be used in practice, we require an empiri-cal estimate of the null distribution, which gives the correct test threshold when Z t = ( X t ,Y t ) is a random process. Evidently, the bootstrap procedure used in the i.i.d. case is incorrect, as the temporal dependence structure within the Y t will be removed. This turns out to cause severe prob-lems in practice, since the permutation procedure will give an increasing rate of false positives as the temporal depen-dence of the Y t increases (i.e., dependence will be detected between X t and Y t , even though none exists, this is also known as a Type I error). Instead, our null estimate is ob-tained by making shifts of one signal relative to the other, so as to retain the dependence structure within each signal. Consequently, we are able to keep the Type I error at the designed level  X  = 0 . 05 . In our experiments, we address three examples: one artificial case consisting of two signals which are dependent but have no correlation, and two real-world examples on forex data. HSIC for random processes reveals dependencies that classical approaches fail to de-tect. Moreover, our new approach gives the correct Type I error rate, whereas a bootstrap-based approach designed for i.i.d. signals returns too many false positives. Related work Prior work on testing independence in time series may be categorized in two branches: testing serial dependence within a single time series, and testing dependence between one time series and another. The case of serial dependence turns out to be relatively straight-forward, as under the null hypothesis, the samples be-come independent: thus, the analysis reduces to the i.i.d. case. Pinkse (1998); Diks &amp; Panchenko (2005) provide a quadratic forms function-based serial dependence test which employs the same statistic as HSIC. Due to the sim-ple form of the null hypothesis, the analysis of (Serfling, 2009, Ch. 5) applies. Further work in the context of the se-rial dependency testing includes simple approaches based on rank statistics e.g. Spearman X  X  correlation or Kendall X  X  tau, correlation integrals e.g. (Broock et al., 1996); criteria based on integrated squared distance between densities e.g (Rosenblatt &amp; Wahlen, 1992); KL-divergence based crite-ria e.g. (Robinson, 1991; Hong &amp; White, 2005); and gen-eralizations of KL-divergence to so called q -class entropies e.g. (Granger et al., 2004; Racine &amp; Maasoumi, 2007). In most of the tests of independence of two time se-ries, specific conditions have been enforced, e.g that pro-cesses follow a moving average specification or the depen-dence is linear. Prior work in the context of dependency tests of two time series includes cross covariance based tests e.g. (Haugh, 1976; Hong, 1996; Shao, 2009); and a Generalized Association Measure based criterion (Fad-lallah et al., 2012). Some work has been undertaken in the non-parametric case, however. A non-parametric mea-sure of independence for time series, based on the Hilbert-Schmidt Independence Criterion, was proposed by Zhang et al. (2008). While this work established the conver-gence in probability of the statistic to its population value, no asymptotic distributions were obtained, and the statis-tic was not used in hypothesis testing. To our knowledge, the only non-parametric independence test for pairs of time series is due to Besserve et al. (2013), which addresses the harder problem of testing independence across all time lags simultaneously. 2 The procedure is to compute the Hilbert-Schmidt norm of a cross-spectral density operator (the Fourier transform of the covariance operator at each time lag). The resulting statistic is a function of frequency, and must be zero at all frequencies for independence, so a correction for multiple hypothesis testing is required. It is not clear how the asymptotic analysis used in the present work would apply to this statistic, and this remains an in-teresting topic of future study.
 The remaining material is organized as follows. In Sec-tion 2 we provide a brief introduction to random processes and various mixing conditions, and an expression for our independence statistic, HSIC. In Section 3, we character-ize the asymptotic behaviour of HSIC for random variables with temporal dependence, under the null and alternative hypotheses, and establish the test consistency. We propose an empirical procedure for constructing a statistical test, and demonstrate that the earlier bootstrap approach will not work for our case. Section 4 provides experiments on syn-thetic and real data. In this section we introduce necessary definitions referring to random processes. We then go on to define a V-statistic estimate of the Hilbert-Schmidt Independence Criterion, which applies in the i.i.d. case.
 Random process. First, we introduce the probabilistic tools needed for pairs of time series. Let ( Z t , F t ) a strictly stationary sequence of random variables defined on a probability space  X  with a probability measure P and natural filtration F t . Assume that Z t denotes a pair of ran-dom variables i.e. Z t = ( X t ,Y t ) , where X t is defined on X , and Y t on Y . Each Z t takes values in a measurable Pol-ish space ( Z , B ( Z ) ,P Z ) . The space Z is a Cartesian prod-uct of two Polish spaces X and Y , endowed with a natural Borel sigma field and a probability measure.
 We introduce a sequence of independent copies of Z 0 , i.e., ( Z t ) t  X  N . Since Z t is stationary, Z between random variables X t and Y t , but breaks the tem-poral dependence.
 Next, we formalize a concept of memory of a process. A process is called absolutely regular (  X  -mixing) if  X  ( m )  X  0 , where  X  ( m ) = The second supremum in the  X  ( m ) definition is taken over all pairs of finite partitions { A 1 ,  X  X  X  ,A { B 1 ,  X  X  X  ,B J } of the sample space such that A i  X  X  n B j  X  A  X  n + m , and A c b is a sigma field spanned by a sub-sequence, A c b =  X  ( Z b ,Z b +1 ,...,Z c ) . A process is called uniform mixing (  X  -mixing) if  X  ( m )  X  0 , where Uniform mixing implies absolute regularity, i.e.  X  ( m )  X   X  ( m ) (Bradley et al., 2005). Under technical assumptions, Autoregressive Moving Average processes  X  or more gen-erally Markov Chains  X  are absolutely regular or uni-formly mixing (Doukhan, 1994).
 Hilbert-Schmidt Independence Criterion Let k , l be positive definite kernels associated with respective repro-ducing kernel Hilbert spaces H X on X , and H Y on Y . We assume that k and l are bounded and continuous. We associate to the random variable X a mean embedding  X  X ( x ) := E X k ( X,x ) , such that  X  f  X  H X ,  X  f, X  X  X  E
X ( f ( X )) (Berlinet &amp; Thomas-Agnan, 2004; Smola et al., 2007). We assume k , l are characteristic kernels, mean-ing the mappings  X  X and  X  Y ( y ) := E Y l ( Y,y ) are injec-tive embeddings of the probability measures to the corre-sponding RKHSs; i.e., distributions have unique embed-dings (Fukumizu et al., 2008; Sriperumbudur et al., 2010). We next recall a measure of statistical dependence, the Hilbert-Schmidt Independence Criterion (HSIC), which can be expressed in terms of expectations of RKHS kernels (Gretton et al., 2005; 2007). Denote a group of permuta-tions over 4 elements by S 4 , with  X  one of its elements, i.e., a permutation of four elements. We define h ( z 1 ,z 2 ,z 3 ,z 4 ) = Lemma 1. Let  X  be an expected value of the function h , to HSIC, computed using a function symmetric in its argu-ments. For k and l characteristic, continuous, translation invariant, and vanishing at infinity,  X  is equal to zero if and only if the null hypothesis holds (see (Lyons, 2013, Lemma 3.8), applying (Sriperumbudur et al., 2011, Proposition 2), and the note at the end of Section 5).
 The value of  X  corresponds to a distance between embed-product kernel  X  = k  X  l (Gretton et al., 2012, Section 7). A biased empirical estimate of the Hilbert-Schmidt Inde-pendence Criterion can be expressed as a V -statistic (the unbiased estimate is a U-statistic, however the difference will be accounted for when constructing a hypothesis test, through an appropriate null distribution).
 V statistics. A V -statistic of a k -argument, symmetric function f is written Gretton et al. (2005) show that the biased estimator of  X  is V ( h,Z ) . The asymptotic behaviour of this statis-tic depends on the degeneracy of the function that defines it. We say that a k -argument, symmetric function f is j -degenerate ( j &lt; k ) if for each z 1 ,  X  X  X  ,z j  X  Z , If j = k  X  1 we say that the function is canonical. We refer to a normalized V statistic as a V -statistic multiplied by the sample size, n  X  V . In this section we construct the Hilbert-Schmidt Inde-pendence Criterion for random processes, and define its asymptotic behaviour. We then introduce an independence testing procedure for time series.
 We introduce two hypotheses: the null hypothesis H 0 that X t and Y t are independent, and the alternative hypothesis H 1 that they are dependent. To build a statistical test based on n  X  V ( h,Z ) we need two main results. First, if null hy-pothesis holds, we show n  X  V ( h,Z ) converges to a random variable. Second, if the null hypothesis does not hold, the n  X  V ( h,Z ) estimator diverges to infinity. Following these results, the Type I error (the probability of mistakenly re-jecting the null hypothesis) will stabilize at the design pa-rameter  X  , and the Type II error (the probability of mistak-enly accepting the null hypothesis when the variables are dependent) will drop to zero, as the sample size increases. We begin by introducing an auxiliary kernel function s , and characterize the normalized V -statistic distribution of s us-ing a CLT introduced by (Borisov &amp; Volodko, 2008). We then show that the normalized V -statistic associated with the function s has the same asymptotic distribution as the n  X  V ( h,Z ) distribution.
 Let s be an auxiliary function s ( z 1 ,z 2 ) =  X  k ( x 1 ,x 2 )  X  l ( y 1 ,y 2 ) , where and  X  l is defined similarly.
 Both  X  k and  X  l are kernels, meaning that they are dot prod-ucts between features centred in their respective RKHSs (Berlinet &amp; Thomas-Agnan, 2004). Therefore s =  X  k  X  defines a kernel on a product space of pairs Z t . Using Mer-cer X  X  Theorem we obtain an expansion for s .
 Statement 1. By Steinwart &amp; Scovel (2012) Corollary 3.5, the bounded, continuous kernel s has a representation 3 where ( e i ) i  X  N + denotes an orthonormal basis of L ( Z , B ( Z ) ,P Z ) . The series ( P N i =1  X  i e i ( z converges absolutely and uniformly. e i are eigenfunctions of s and  X  i are eigenvalues of s .
 We will henceforth assume that for every collection of pairwise distinct subscripts ( t 1 ,t 2 ) , the distribution of ( Z t 1 ,Z t 2 ) is absolutely continuous with respect to the ( Z currence of degenerate cases, such that all Z t being the same. The following results are proved in Section 5.1. Lemma 2. Let the process Z t have a mixing coefficient smaller than m  X  3 (  X  ( m ) , X  ( m )  X  m  X  3 ) and satisfy either of the following conditions: A Z t is  X  -mixing.
 B Z t is  X  -mixing. For some &gt; 0 and for an even number If the null hypothesis holds, then s is a canonical function and a kernel. What is more, where  X  j is a centred Gaussian sequence with the covari-ance matrix
E  X  a  X  b = E e a ( Z 1 ) e b ( Z 1 )+ We now characterize the asymptotics of V ( h,Z ) . Theorem 1. Under assumptions of Lemma 2, if H 0 holds, then the asymptotic distribution of the empirical HSIC (with scaling n ) is the same as that of n  X  V ( s,Z ) , Theorem 2. Under assumptions of the Lemma 2, if H 1 holds, then  X  &gt; 0 and normal distribution with mean zero and finite variance. Consequently, if the null hypothesis does not hold then P ( n  X  V ( h,Z ) &gt; C ) = P ( V ( h,Z ) &gt; C n )  X  1 for any fixed C . Finally, we show that the  X  estimator is easy to compute. According to Gretton et al. (2007, equation 4), V ( h,Z ) = n  X  2 trHKHL, where K ab = k ( X a ,X b ) , L ab = l ( Y a ,Y b ) , H ij =  X  ij  X  n  X  1 and n is a sample size. Testing procedure We begin by showing that the H 0 dis-tribution of the  X  estimator obtained via the bootstrap ap-proach of (Diks &amp; Panchenko, 2005; Gretton et al., 2007) gives an incorrect p-value estimate when used with inde-pendent random processes. In fact, the null hypothesis ob-tained by permutation corresponds to the processes being both i.i.d. and independent from each other. Recall the covariance structure of the  X  estimator from Theorem 1,
E  X  a  X  b = E e a ( Z 1 ) e b ( Z 1 )+ We can represent e a and e b as e a ( z ) = e X u ( x ) e e ( z ) = e X i ( x ) e Y p ( y ) , as a decomposition of the Z basis into bases of X and Y , respectively. Consider a partial sum T n of series from the above equation (3), with X t replaced with its permutation X  X  ( t ) ,
T Using covariance inequalities from (Doukhan, 1994, Section 1.2.2) we conclude that E e Y o ( Y 1 ) e Y p  X  (1) | ) 1 2 ) where  X  is an appropriate mixing coefficient (  X  or  X  ). Recall that 0 &lt;  X ( j ) &lt; Cj  X  3 .
 We can therefore reduce the problem to the convergence of a random variable where  X  is a random permutation drawn from the uniform distribution over the set of n -element permutations. In the supplementary material we show that this sum converges in probability to zero.
 Since S n &gt; T n &gt; 0 , then T n converges to zero in proba-bility, and consequently the covariance matrix entry E  X  a converges to unity for a = b , and to zero otherwise. Indeed, the expected value E e a (( X  X  (1) ,Y 1 )) e b (( X  X  (1) a 6 = b and is equal to one otherwise. Note that this is the covariance matrix described by Gretton et al. (2007). A correct approach to approximating the asymptotic null distribution of n  X  V ( h,Z ) under H 0 is by shifting of one time series relative to the other. Define the shifted process S t = Y t + c mod n for an integer c , 0  X  c  X  n and 0  X  t  X  n . If we let c vary over 0  X  A  X  B  X  n for A such that the dependence between Y t + A and X t is negligible, then we can approximate the null distribution with an empirical distribution calculated on points ( V ( h,Z k )) A  X  k  X  B Z t = ( X t ,S process S c t retains most of the dependence, since it does not scramble the time index. 4 We call this method Shift HSIC. In the supplementary material we show that Shift HSIC samples from the correct null distribution. In the experiments we compare Shift HSIC with the Boot-strap HSIC of Gretton et al. (2007). We investigate three cases: an artificial dataset, where two time series are cou-pled non-linearly; and two forex datasets, where in one case we seek residual dependence after one time series has been used to linearly predict another, and in the other case, we reveal strong dependencies between signals that are not seen via linear correlation. 4.1. Artificial data Non-linear dependence. We investigate two dependent, autoregressive random processes X t , Y t , specified by with an autoregressive component a . The coupling of the processes is a result of the dependence in the innovations Algorithm 1 Generate innovations
Input: extinction rate 0  X  p  X  1 , radius r . repeat until true , X  t . These t , X  t are drawn from an Extinct Gaussian dis-tribution, defined in Algorithm 1. The parameter p (called extinction rate) controls how often a point drawn form a ball B (0 ,r ) dies off. According to Algorithm 1, the prob-ability of seeing a point inside the ball B (0 ,r ) is differ-ent than for a two dimensional Gaussian N ( 0 ,Id ) . On the other hand, as p goes to zero, the Extinct Gaussian con-verges in distribution to N ( 0 ,Id ) . Figure 1 illustrates the ure 1 presents X t and Y t generated with an extinction rate of 50% , while the right hand plot is generated with an ex-tinction rate of 99 . 87% . Processes used in this experiment had an autoregressive component of 0 . 2 , and the radius of the innovation process was 1 .
 Figure 2 compares the power of the Shift HSIC test and the correlation test. The X axis represents an extinction rate, while the Y axis shows the true positive rate. Shift HSIC is capable of detecting non-linear dependence between X t and Y t , which is missed by linear correlation. The red star depicts performance of the KCSD algorithm developed by Besserve et al. (2013), with parameters tuned by its authors: note that this result required using four times as many data points as HSIC.
 False positive rates. We next investigate the rate of false positives for Shift HSIC and Bootstrap HSIC on indepen-dent copies of the AR (1) processes used in the previous experiment. To generate independent processes, we first (6), and then constructed Z by taking X from the first pair and Y from the second, i.e., Z t = ( X t ,Y 0 t ) . We set an extinction rate to 50% . 5 The AR component a in the model (6) controls the memory of a processes -the larger this component, the longer the memory. We performed the Shift HSIC and the Bootstrap HSIC tests on Z t generated under H 0 with different AR components. Figure 3 illus-trates the results of this experiment. The X axis is indexed by the AR component and Y axis shows the FP rate. As the temporal dependence increases, the Bootstrap HSIC incor-rectly gives an increasing number of false positives: thus, it cannot be relied on to detect dependence in time series. The Shift HSIC false positive rate remains at the targeted 5% p-value level. 4.2. Forex data We use Foreign Exchange Market quotes to evaluate Shift HSIC performance on the real life data. Practitioners point out that forex time series are noisy and hard to handle, especially at low granulations (smaller then 15 minutes). We decided to work with forex time series to show that Shift HSIC can detect dependence even on such a difficult dataset. The forex time series were granulated to obtain two minute sampling (the granulation function returned the last price in the two minute window). Using the test of Diks &amp; Panchenko (2005), we checked that serial dependence of the differentiated time series decays fast enough to satisfy the assumed mixing conditions (by a differentiated time se-ries, we refer to ( X t  X  X t  X  1 ) t  X  N ). The choice of the pairs and trading day (21st January 2013) were arbitrary. Instantaneous coupling and causal effect. Having one Australian dollar we may obtain a quantity of Yen in two ways, either by using AUD/JPY exchange rate explicitly or by buying Canadian dollars and then selling them at the CAD/JPY rate. Let X t be a differentiated AUD/JPY ex-change rate and Y t be a differentiated product of exchange rates AUD/CAD  X  CAD/JPY. We will investigate the rela-tion between these two. Common sense dictates that Y should behave similarly to X t . After examining the cross-correlation of X t and Y t , we propose a simple regression model to describe the interaction between the signals, We fit the model and see that a 0 = 0 . 97 , and the remain-ing coefficients are not bigger then 0 . 06 in absolute value. This suggest that most of the dependence is explained by an instantaneous coupling. We further investigate the cross-correlation between residuals R t = Y t  X   X  Y t and X t . We observe no significant correlations in the first 30 lags. Next we investigate dependence of residuals with lagged values of the explanatory variables, i.e., R t with X t  X  k k  X  (0 ,  X  X  X  , 30) . After calculating p-values using the Boot-strap HSIC and the Shift HSIC, we discover dependence only at lags 4 , 5 , 9 , 13 and 29 , as presented in the Figure 4. Lack of the dependence at lag zero suggests that the linear model for coupling is reasonable. However, both the Boot-strap HSIC and the Shift HSIC support the hypothesis that there is a strong relation at lag 5 , which is not explained well by the linear model.
 The questions remains whether test statistics at lags 4 , 9 , 13 and 29 indicate further model misspecification. Under H 0 at a significance level 94% , we expect 1.8 out of 30 statis-tics to be higher than the 94th percentile. Excluding the statistic at lag 5 , the Shift HSIC test reports two statistics above this percentile, while Bootstrap HSIC reports four. Should the statistics at the different lags be independent from each other, the probabilities of seeing two and four statistics above the percentile are respectively 25% and 6% . Shift HSIC indicates that the model fits the data well, while the Bootstrap HSIC suggests that some non-linear depen-dencies remain unexplained.
 Dependence structure. The data are five currency pairs. A correlation based independence test, and the Shift HSIC test, were performed on each pair of currencies. The de-pendencies revealed by these tests are depicted in Figure 5 -nodes represent the time series and edges represent de-pendence. Shift HSIC reveals a strong coupling between EUR/RUB and USD/JPY, HKD/JPY and XAU/USD that was not found by simple correlation. All edges revealed by Shift HSIC have p-values at most at level 0 . 03 -clearly, the Shift HSIC managed to find a strong non-linear depen-dence. Note that the obtained graphs are cliques. A U -statistic of a k -argument, symmetric function f , is written A decomposition due to Hoeffding allows us to decompose this U-statistic into a sum of U -statistics of canonical func-tions, U ( h,Z ) = P l k =1 l k U ( h k ,Z ) , where h k ( z are components of the decomposition. According to Ser-fling (2009, section 5.1.5), each of h 1 , h 2 , h 3 , h 4 ric and canonical. Note that h k is defined using indepen-dent samples Z  X  -this is because the CLT or LLN state that U-statistics or V-statistics of mixing processes converge to their expected value taken with respect to independent copies, i.e., Z  X  . Under H 0 , h 1 is equal to zero everywhere and h 2 = 1 6 s , where these results were obtained by Gretton et al. (2007). 6 See supplementary material for details. In order to characterize U ( h,Z ) , we show that under null hypothesis U ( h 2 ,Z ) converges to a random variable, and both U ( h 3 ,Z ) , U ( h 4 ,Z ) converge to zero in a probability (the latter proof can be found in the supplementary mate-rial). Bellow we characterise U ( h 2 ,Z ) convergence. Lemma 3. Under assumptions of Lemma 2, Proof. First recall that under null hypothesis h 2 = 1 6 s . We will check the conditions of (Borisov &amp; Volodko, 2008, Theorem 1) (also available in the supplementary).
 First, from Mercer X  X  Theorem (Steinwart &amp; Scovel, 2012, Corollary 3.5), we deduce that the h 2 coefficients in L ( Z , B Z ,P Z ) are absolutely summable. In the supple-mentary material we show that E e i ( Z  X  1 ) = 0 . Recall the assumptions of Lemma 2. If A holds then P is an orthonormal eigenfunction). Finally, if B holds then the process Z t is  X  -mixing. The remaining assumptions concerning uniform mixing in Borisov &amp; Volodko (2008) are exactly the same as in this lemma. 5.1. Main body proofs Proof. (Lemma 2) We use the fact that h 2 is equal to s up to scaling ( 6 U ( h 2 ,Z ) = U ( s,Z ) ), and Lemma 3, to see that nU ( s,Z ) D  X  P  X  i  X  i (  X  2 i  X  1) . Since E s ( Z processes, We use a relationship between U and V statistics, lim Proof. (Theorem 1) We operate under the null hypothe-sis. Recall that U ( h,Z ) can be decomposed as U ( h,Z ) = P mentary material that U ( h 3 ,Z ) and U ( h 4 ,Z ) tend to zero in probability. From Lemma 3, lim We define an auxiliary symmetric function w , It is obvious that E w ( Z  X  1 ,Z  X  2 ,Z  X  3 ) = 6 E h ( Z We consider the difference between the unnormalized V and U statistics, S tions of m distinct elements { i 1 ,  X  X  X  ,i m } from { 1 ,  X  X  X  ,n } . The difference is equal to the sum over 4 -tuples with at least one pair of equal elements. We can choose such tuples in 2 = 6 ways. Observe that w covers the choice of all these six tuples. Since for any z 1 ,z 2  X  Z , h ( z 1 ,z 1 ,z then w is zero whenever more than two indices are equal. Therefore we can sum w over distinct indices z 1 ,z 2 ,z 3 We see that S n is almost a U -statistic ( U ( w,Z ) ). By the CLT for U -statistics from Denker &amp; Keller (1983), Theo-rem 1(c), we obtain On the other hand, via the relation h 2 = 1 6 s and the h inition, we get E s ( Z  X  1 ,Z  X  1 ) = 6 E h ( Z  X  1 ,Z therefore lim Finally, we rewrite S n as lim We substitute (9) and (8) on the right hand side, and use equation (7) from Lemma 2 to replace lim Proof. (Theorem 2) If the null hypothesis does not hold, then  X  &gt; 0 (Gretton et al., 2005). In this case h is nonde-generate, and we can use Denker &amp; Keller (1983, Theorem 1(c)) to see that finite (see the note below Theorem 1 of (Denker &amp; Keller, 1983), stating that in case (c)  X  2 is finite, and the note above Theorem 1 stating that  X  2 = lim n  X  X  X  n  X  1  X  2 n ). Proof. (Lemma 1) We use Lemma 1 and Theorem 4 from Gretton et al. (2005) to show that E h ( Z  X  1 ,Z  X  2 ,Z  X  iff ( X  X  1 ,Y  X  1 ) has a product distribution. Since Z and Z t D = Z 1 , we infer that X t is independent from Y E h ( Z  X  1 ,Z  X  2 ,Z  X  3 ,Z  X  4 ) = 0 .
 Acknowledgements The authors thank the reviewers and
