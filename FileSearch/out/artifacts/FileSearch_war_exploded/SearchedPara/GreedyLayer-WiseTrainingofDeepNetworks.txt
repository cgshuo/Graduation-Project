 Recent analyses (Bengio, Delalleau, &amp; Le Roux, 2006; Bengio &amp; Le Cun, 2007) of modern non-parametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limita-tions of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is  X local X  (e.g., the Gaussian kernel), i.e., K ( x; y ) con verges to a constant when jj x y jj increases. These analyses point to the dif culty of learning  X highly-varying functions X  , i.e., functions that have a lar ge number of  X variations X  in the domain of interest, e.g., the y would require a lar ge number of pieces to be well represented by a piece wise-linear approximation. Since the number of pieces can be made to gro w exponentially with the number of factors of variations in the input, this is connected with the well-kno wn curse of dimensionality for classical non-parametric learning algorithms (for regres-sion, classication and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly . Ho we ver, if these shapes are related and can be predicted from each other ,  X non-local X  learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in comple x domains such as Articial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallo w architectur e , i.e., only two levels of data-dependent computational elements. This is also true of feedforw ard neural netw orks with a single hidden layer (which can become SVMs when the number of hidden units becomes lar ge (Bengio, Le Roux, Vincent, Delalleau, &amp; Marcotte, 2006)). A serious problem with shallo w architectures is that the y can be very inef cient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio &amp; Le Cun, 2007). One way to represent a highly-v arying function compactly (with few parameters) is through the composition of man y non-linearities, i.e., with a deep architectur e . For example, the parity function with d inputs requires O (2 d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O ( d 2 ) parameters for a one-hidden-layer neural netw ork, O ( d ) parameters and units for a multi-layer netw ork with O (log boolean functions (such as the function that computes the multiplication of two numbers from their d -bit representation) expressible by O (log d ) layers of combinatorial logic with O ( d ) elements in each layer may require O (2 d ) elements when expressed with only 2 layers (Utgof f &amp; Stracuzzi, 2002; Bengio &amp; Le Cun, 2007). When the representation of a concept requires an exponential number of elements, e.g., with a shallo w circuit, the number of training examples required to learn the concept may also be impractical. Formal analyses of the computational comple xity of shallo w circuits can be found in (Hastad, 1987) or (Allender , 1996). The y point in the same direction: shallo w circuits are much less expressi ve than deep ones.
 cally , deep netw orks were generally found to be not better , and often worse, than neural netw orks with one or two hidden layers (Tesauro, 1992). As this is a negati ve result, it has not been much reported in the machine learning literature. A reasonable explanation is that gradient-based optimization starting from random initialization may get stuck near poor solutions. An approach that has been explored with some success in the past is based on constructively adding layers. This was pre viously done using a supervised criterion at each stage (Fahlman &amp; Lebiere, 1990; Lengell  X  e &amp; Denoeux, 1996). Hinton, Osindero, and Teh (2006) recently introduced a greedy layer -wise unsupervised learning algorithm for Deep Belief Netw orks (DBN), a generati ve model with man y layers of hidden causal variables. The training strate gy for such netw orks may hold great promise as a principle to help address the problem of training deep netw orks. Upper layers of a DBN are supposed to represent more  X abstract X  concepts that explain the input observ ation x , whereas lower layers extract  X lo w-le vel features X  from x . The y learn simpler concepts rst, and build on them to learn more abstract concepts. This strate gy, studied in detail here, has not yet been much exploited in machine learning. We hypothesize that three aspects of this strate gy are particularly important: rst, pre-training one layer at a time in a greedy way; sec-ond, using unsupervised learning at each layer in order to preserv e information from the input; and nally , ne-tuning the whole netw ork with respect to the ultimate criterion of interest.
 We rst extend DBNs and their component layers, Restricted Boltzmann Machines (RBM), so that the y can more naturally handle continuous values in input. Second, we perform experiments to better understand the adv antage brought by the greedy layer -wise unsupervised learning. The basic question to answer is whether or not this approach helps to solv e a dif cult optimization problem. In DBNs, RBMs are used as building blocks, but applying this same strate gy using auto-encoders yielded similar results. Finally , we discuss a problem that occurs with the layer -wise greedy unsupervised procedure given the input variable. We evaluate a simple and successful solution to this problem. Let x be the input, and g i the hidden variables at layer i , with joint distrib ution tation of probability and sampling are easy . In Hinton et al. (2006) one considers the hidden layer g i a binary random vector with n i elements g i where sigm( t ) = 1 = (1 + e t ) , the b i layer i . If we denote g 0 = x , the generati ve model for the rst layer P ( x j g 1 ) also follo ws (1). 2.1 Restricted Boltzmann machines The top-le vel prior P ( g ` 1 ; g ` ) is a Restricted Boltzmann Machine (RBM) between layer ` 1 and layer ` . To lighten notation, consider a generic RBM with input layer acti vations v (for visi-ble units) and hidden layer acti vations h (for hidden units). It has the follo wing joint distrib ution: tor of biases for visible units, c is the vector of biases for the hidden units, and W is the weight matrix for the layer . Minus the argument of the exponential is called the ener gy function , We denote the RBM parameters together with = ( W; b; c ) . We denote Q ( h j v ) and P ( v j h ) the layer -to-layer conditional distrib utions associated with the abo ve RBM joint distrib ution. The layer -to-layer conditionals associated with the RBM factorize lik e in (1) and give rise to P ( v k = 1 j h ) = sigm( b k + 2.2 Gib bs Mark ov chain and log-lik elihood gradient in an RBM To obtain an estimator of the gradient on the log-lik elihood of an RBM, we consider a Gibbs Mark ov chain on the (visible units, hidden units) pair of variables. Gibbs sampling from an RBM proceeds by sampling h given v , then v given h , etc. Denote v t = 0 with v 0 , the  X input observ ation X  for the RBM. Therefore, ( v k ; h k ) for k !1 is a sample from the joint P ( v ; h ) . The log-lik elihood of a value v and its gradient with respect to = ( W; b; c ) is for k ! 1 . An unbiased sample is @ energy ( v 0 ; h 0 ) where h tion can be easily computed thanks to P ( h algorithm (Hinton, 2002) is to tak e k small (typically k = 1 ). A pseudo-code for Contrasti ve Di-vergence training (with k = 1 ) of an RBM with binomial input and hidden units is presented in the Appendix (Algorithm RBMupdate( x; ; W; b; c ) ). This procedure is called repeatedly with v sampled from the training distrib ution for the RBM. To decide when to stop one may use a proxy for the training criterion, such as the reconstruction error log P ( v 2.3 Gr eedy lay er-wise training of a DBN A greedy layer -wise training algorithm was proposed (Hinton et al., 2006) to train a DBN one layer at a time. One rst trains an RBM that tak es the empirical data as input and models it. Denote Q ( g 1 j g 0 ) the posterior over g 1 associated with that trained RBM (we recall that g 0 = x with x the observ ed input). This gives rise to an  X empirical X  distrib ution b p 1 over the rst layer g 1 , when g 0 is sampled from the data empirical distrib ution b p : we have b p 1 ( g 1 ) = Note that a 1-le vel DBN is an RBM. The basic idea of the greedy layer -wise strate gy is that after training the top-le vel RBM of a ` -le vel DBN, one changes the interpretation of the RBM parameters to insert them in a ( ` + 1 )-le vel DBN: the distrib ution P ( g ` 1 j g ` ) from the RBM associated with layers ` 1 and ` is kept as part of the DBN generati ve model. In the RBM between layers ` 1 and ` , P ( g ` ) is dened in terms on the parameters of that RBM, whereas in the DBN P ( g ` ) is dened in terms of the parameters of the upper layers. Consequently , Q ( g ` j g ` 1 ) of the RBM does not correspond to P ( g ` j g ` 1 ) in the DBN, except when that RBM is the top layer of the DBN. Ho we ver, we use Q ( g ` j g ` 1 ) of the RBM as an approximation of the posterior P ( g ` j g ` 1 ) for the DBN. with distrib ution b p ` through b p ` ( g ` ) = P resented explicitly it is easy to sample unbiasedly from it: pick a training example and propagate it stochastically through the Q ( g i j g i 1 ) at each level. As a nice side benet, one obtains an approxi-mation of the posterior for all the hidden variables in the DBN, at all levels, given an input g 0 = x . Mean-eld propagation (see belo w) gives a fast deterministic approximation of posteriors P ( g ` j x ) : Note that if we consider all the layers of a DBN from level i to the top, we have a smaller DBN, which generates the mar ginal distrib ution P ( g i ) for the complete DBN. The moti vation for the greedy procedure is that a partial DBN with ` i levels starting abo ve level i may pro vide a better model for P ( g i ) than does the RBM initially associated with level i itself.
 The abo ve greedy procedure is justied using a variational bound (Hinton et al., 2006). As a con-sequence of that bound, when inserting an additional layer , if it is initialized appropriately and has enough units, one can guarantee that initial impro vements on the training criterion for the next layer (tting b p ` ) will yield impro vement on the training criterion for the pre vious layer (lik elihood with respect to b p ` 1 ). The greedy layer -wise training algorithm for DBNs is quite simple, as illustrated by the pseudo-code in Algorithm TrainUnsupervisedDBN of the Appendix. 2.4 Super vised ne-tuning As a last training stage, it is possible to ne-tune the parameters of all the layers together . For exam-ple Hinton et al. (2006) propose to use the wake-sleep algorithm (Hinton, Dayan, Fre y, &amp; Neal, 1995) to continue unsupervised training. Hinton et al. (2006) also propose to optionally use a mean-eld ap-proximation of the posteriors P ( g i j g 0 ) , by replacing the samples g i 1 mean-eld expected value i 1 rules, the whole netw ork now deterministically computes internal representations as functions of the netw ork input g 0 = x . After unsupervised pre-training of the layers of a DBN follo wing Algorithm TrainUnsupervisedDBN (see Appendix) the whole netw ork can be further optimized by gradient descent with respect to any deterministically computable training criterion that depends on these rep-resentations. For example, this can be used (Hinton &amp; Salakhutdino v, 2006) to ne-tune a very deep auto-encoder , minimizing a reconstruction error . It is also possible to use this as initialization of all except the last layer of a traditional multi-layer neural netw ork, using gradient descent to ne-tune the whole netw ork with respect to a supervised training criterion.
 Algorithm DBNSupervisedFineTuning in the appendix contains pseudo-code for supervised ne-tuning, as part of the global supervised learning algorithm TrainSupervisedDBN . Note that better results were obtained when using a 20-fold lar ger learning rate with the supervised criterion (here, squared error or cross-entrop y) updates than in the contrasti ve divergence updates. With the binary units introduced for RBMs and DBNs in Hinton et al. (2006) one can  X cheat X  and handle continuous-v alued inputs by scaling them to the (0,1) interv al and considering each input con-tinuous value as the probability for a binary random variable to tak e the value 1. This has work ed well for pix el gray levels, but it may be inappropriate for other kinds of input variables. Pre vious work on continuous-v alued input in RBMs include (Chen &amp; Murray , 2003), in which noise is added to sigmoidal units, and the RBM forms a special form of Dif fusion Netw ork (Mo vellan, Mineiro, &amp; Williams, 2002). We concentrate here on simple extensions of the RBM frame work in which only the ener gy function and the allo wed range of values are changed.
 Linear ener gy: exponential or truncated exponential Consider a unit with value y of an RBM, connected to units z of the other layer . p ( y j z ) can be obtained from the terms in the exponential that contain y , which can be grouped in ya ( z ) for linear connecting unit y to units z . If we allo w y to tak e any value in interv al I , the conditional density of y becomes p ( y j z ) = Computing the density , computing the expected value ( = 1 =a ( z ) ) and sampling would all be easy . Alternati vely , if I is a closed interv al (as in man y applications of interest), or if we would lik e to use such a unit as a hidden unit with non-linear expected value , the abo ve density is a truncated exponential . For simplicity we consider the case I = [0 ; 1] here, for which the normalizing inte gral, it has a sigmoidal-lik e saturating and monotone non-linearity: E [ y j z ] = 1 sampling from the truncated exponential is easily obtained from a uniform sample U , using the inverse and not truncated cases, the Contrasti ve Divergence updates have the same form as for binomial units (input value times output value), since the updates only depend on the deri vative of the ener gy with respect to the parameters. Only sampling is changed, according to the unit' s conditional density . Quadratic ener gy: Gaussian units To obtain Gaussian-distrib uted units, one adds quadratic terms to the ener gy. Adding P rise to a diagonal covariance matrix between units of the same layer , where y of a Gaussian unit and d 2 1. Deep Netw ork with no pre-training 4.23 4.43 4.2 45.2% 42.9% 43.0% 2. Logistic regression 44.0% 42.6% 45.0% 3. DBN, binomial inputs, unsupervised 4.59 4.60 4.47 44.0% 42.6% 45.0% 4. DBN, binomial inputs, partially supervised 4.39 4.45 4.28 43.3% 41.1% 43.7% 5. DBN, Gaussian inputs, unsupervised 4.25 4.42 4.19 35.7% 34.9% 35.8% 6. DBN, Gaussian inputs, partially supervised 4.23 4.43 4.18 27.5% 28.4% 31.4% Table 1: Mean squared prediction error on Abalone task and classication error on Cotton task, sho wing impro vement with Gaussian units. this case the variance is unconditional, whereas the mean depends on the inputs of the unit: for a unit y with inputs z and inverse variance d 2 , E [ y j z ] = a ( z ) The Contrasti ve Divergence updates are easily obtained by computing the deri vative of the ener gy with respect to the parameters. For the parameters in the linear terms of the ener gy function (e.g., b and w abo ve), the deri vatives have the same form (input unit value times output unit value) as for the case of binomial units. For quadratic parameter d &gt; 0 , the deri vative is simply 2 dy 2 . Gaussian units were pre viously used as hidden units of an RBM (with binomial or multinomial inputs) applied to an information retrie val task (W elling, Rosen-Zvi, &amp; Hinton, 2005). Our interest here is to use them for continuous-v alued inputs.
 Using continuous-v alued hidden units Although we have introduced RBM units with continuous values to better deal with the representa-tion of input variables, the y could also be considered for use in the hidden layers, in replacement or complementing the binomial units which have been used in the past. Ho we ver, Gaussian and expo-nential hidden units have a weakness: the mean-eld propagation through a Gaussian unit gives rise to a purely linear transformation. Hence if we have only such linear hidden units in a multi-layered netw ork, the mean-eld propagation function that maps inputs to internal representations would be completely linear . In addition, in a DBN containing only Gaussian units, one would only be able to model Gaussian data. On the other hand, combining Gaussian with other types of units could be interesting. In contrast with Gaussian or exponential units, remark that the conditional expectation of truncated exponential units is non-linear , and in fact involv es a sigmoidal form of non-linearity applied to the weighted sum of its inputs.
 Experiment 1 This experiment was performed on two data sets: the UCI repository Abalone data set (split in 2177 training examples, 1000 validation examples, 1000 test examples) and a nancial data set. The latter has real-v alued input variables representing averages of returns and squared returns for which the bino-mial approximation would seem inappropriate. The tar get variable is next month' s return of a Cotton futures contract. There are 13 continuous input variables, that are averages of returns over dif ferent time-windo ws up to 504 days. There are 3135 training examples, 1000 validation examples, and 1000 test examples. The dataset is publicly available at http://www.iro.umontreal.ca/ X lisa/ fin_data/ . In Table 1 (ro ws 3 and 5), we sho w impro vements brought by DBNs with Gaussian inputs over DBNs with binomial inputs (with binomial hidden units in both cases). The netw orks have two hidden layers. All hyper -parameters are selected based on validation set performance. A reasonable explanation for the apparent success of the layer -wise training strate gy for DBNs is that unsupervised pre-training helps to mitigate the dif cult optimization problem of deep netw orks by better initializing the weights of all layers. Here we present experiments that support and clarify this. Training each lay er as an auto-encoder We want to verify that the layer -wise greedy unsupervised pre-training principle can be applied when using an auto-encoder instead of the RBM as a layer building block. Let x be the input vector with x biases column vector c , the reconstruction probability for bit i is p bilities p ( x ) = sigm( c + W sigm( b + W 0 x )) : The training criterion for the layer is the average of negati ve log-lik elihoods for predicting x from p ( x ) . For example, if x is interpreted either as a sequence of bits or a sequence of bit probabilities, we minimize the reconstruction cross-entrop y: R = training criterion for each layer , in comparison to the contrasti ve divergence algorithm for an RBM. Pseudo-code for a deep netw ork obtained by training each layer as an auto-encoder is given in Ap-pendix (Algorithm TrainGreedyAutoEncodingDeepNet ).
 One question that arises with auto-encoders in comparison with RBMs is whether the auto-encoders will fail to learn a useful representation when the number of units is not strictly decreasing from one layer to the next (since the netw orks could theoretically just learn to be the identity and perfectly min-imize the reconstruction error). Ho we ver, our experiments suggest that netw orks with non-decreasing layer sizes generalize well. This might be due to weight decay and stochastic gradient descent, pre vent-ing lar ge weights: optimization falls in a local minimum which corresponds to a good transformation of the input (that pro vides a good initialization for supervised training of the whole net). Gr eedy lay er-wise super vised training A reasonable question to ask is whether the fact that each layer is trained in an unsupervised way is critical or not. An alternati ve algorithm is supervised, greedy and layer -wise: train each new hidden layer as the hidden layer of a one-hidden layer supervised neural netw ork NN (taking as input the output of the last of pre viously trained layers), and then thro w away the output layer of NN and use the parameters of the hidden layer of NN as pre-training initialization of the new top layer of the deep net, to map the output of the pre vious layers to a hopefully better representation. Pseudo-code for a deep netw ork obtained by training each layer as the hidden layer of a supervised one-hidden-layer neural netw ork is given in Appendix (Algorithm TrainGreedySupervisedDeepNet ).
 Experiment 2 .
 We compared the performance on the MNIST digit classication task obtained with ve algorithms: (a) DBN, (b) deep netw ork whose layers are initialized as auto-encoders, (c) abo ve described su-pervised greedy layer -wise algorithm to pre-train each layer , (d) deep netw ork with no pre-training (random initialization), (e) shallo w netw ork (1 hidden layer) with no pre-training.
 The nal ne-tuning is done by adding a logistic regression layer on top of the netw ork and train-ing the whole netw ork by stochastic gradient descent on the cross-entrop y with respect to the tar get classication. The netw orks have the follo wing architecture: 784 inputs, 10 outputs, 3 hidden layers with variable number of hidden units, selected by validation set performance (typically selected layer sizes are between 500 and 1000). The shallo w netw ork has a single hidden layer . An L2 weight decay hyper -parameter is also optimized. The DBN was slo wer to train and less experiments were performed, so that longer training and more appropriately chosen sizes of layers and learning rates could yield better results (Hinton 2006, unpublished, reports 1.15% error on the MNIST test set). Table 2: Classication error on MNIST training, validation, and test sets, with the best hyper -parameters according to validation error , with and without pre-training, using purely supervised or purely unsupervised pre-training. In experiment 3, the size of the top hidden layer was set to 20. On MNIST , dif ferences of more than .1% are statistically signicant. The results in Table 2 suggest that the auto-encoding criterion can yield performance comparable to the DBN when the layers are nally tuned in a supervised fashion. The y also clearly sho w that the greedy unsupervised layer -wise pre-training gives much better results than the standard way to train a deep netw ork (with no greedy pre-training) or a shallo w netw ork, and that, without pre-training, deep netw orks tend to perform worse than shallo w netw orks. The results also suggest that unsupervised greedy layer -wise pre-tr aining can perform signicantly better than pur ely supervised greedy layer -wise pre-tr aining . A possible expla-nation is that the greedy supervised procedure is too greedy : in the learned hidden units representation it may discard some of the information about the tar get, information that cannot be captured easily by a one-hidden-layer neural netw ork but could be captured by composing more hidden layers. Experiment 3 Ho we ver, there is something troubling in the Experiment 2 results (Table 2): all the netw orks, even those without greedy layer -wise pre-training, perform almost perfectly on the training set , which would appear to contradict the hypothesis that the main effect of the layer -wise greedy strate gy is to help the optimization (with poor optimization one would expect poor training error). A possible explanation coherent with our initial hypothesis and with the abo ve results is captured by the follo wing hypothesis . Without pre-training, the lower layers are initialized poorly , but still allo wing the top two layers to learn the training set almost perfectly , because the output layer and the last hidden layer form a standard shallo w but fat neural netw ork. Consider the top two layers of the deep netw ork with pre-tr aining : it presumably tak es as input a better repr esentation , one that allo ws for better generalization. Instead, the netw ork without pre-tr aining sees a  X random X  transformation of the input, one that preserv es enough information about the input to t the training set, but that does not help to generalize. To test that hypothesis, we performed a second series of experiments in which we constrain the top hidden layer to be small (20 hidden units). The Experiment 3 results (Table 2) clearly conrm our hypothesis. With no pre-training, training error degrades signicantly when there are only 20 hidden units in the top hidden layer . In addition, the results obtained without pre-training were found to have extremely lar ge variance indicating high sensiti vity to initial conditions. Ov erall, the results in the tables and in Figure 1 are consistent with the hypothesis that the greedy layer -wise procedure essentially helps to better optimize the deep netw orks, probably by initializing the hidden layer s so that Continuous training of all lay ers of a DBN With the layer -wise training algorithm for DBNs ( TrainUnsupervisedDBN in Appendix), one element that we would lik e to dispense with is having to decide the number of training iterations for each layer . It would be good if we did not have to explicitly add layers one at a time, i.e., if we could train all layers simultaneously , but keeping the  X greedy X  idea that eac h layer is pre-tr ained to model its input, ignoring the effect of higher layer s . To achie ve this it is suf cient to insert a line in
TrainUnsupervisedDBN , so that RBMupdate is called on all the layers and the stochastic hidden values are propagated all the way up. Experiments with this variant demonstrated that it works at least as well as the original algorithm. The adv antage is that we can now have a single stopping criterion (for the whole netw ork). Computation time is slightly greater , since we do more computations initially (on the upper layers), which might be wasted (before the lower layers con verge to a decent representation), but time is saved on optimizing hyper -parameters. This variant may be more appealing for on-line training on very lar ge data-sets, where one would never cycle back on the training data. In classication problems such as MNIST where classes are well separated, the structure of the input distrib ution p ( x ) naturally contains much information about the tar get variable y . Imagine a super -vised learning task in which the input distrib ution is mostly unrelated with y . In regression problems, which we are interested in studying here, this problem could be much more pre valent. For example imagine a task in which x p ( x ) and the tar get y = f ( x )+ noise (e.g., p is Gaussian and f = sinus ) with no particular relation between p and f . In such settings we cannot expect the unsupervised greedy layer -wise pre-training procedure to help in training deep supervised netw orks. To deal with such uncooperati ve input distrib utions, we propose to train each layer with a mix ed training criterion that combines the unsupervised objecti ve (modeling or reconstructing the input) and a supervised ob-jecti ve (helping to predict the tar get). A simple algorithm thus adds the updates on the hidden layer weights from the unsupervised algorithm (Contrasti ve Divergence or reconstruction error gradient) with the updates from the gradient on a supervised prediction error , using a temporary output layer , as with the greedy layer -wise supervised training algorithm. In our experiments it appeared suf cient to perform that partial supervision with the rst lay er only , since once the predicti ve information about the tar get is  X forced X  into the representation of the rst layer , it tends to stay in the upper layers. The results in Figure 1 and Table 1 clearly sho w the adv antage of this partially supervised greedy training algorithm , in the case of the nancial dataset. Pseudo-code for partially supervising the rst (or later layer) is given in Algorithm TrainPartiallySupervisedLayer (in the Appendix). This paper is moti vated by the need to develop good training algorithms for deep architectures, since these can be much more representationally efcient than shallo w ones such as SVMs and one-hidden-layer neural nets. We study Deep Belief Netw orks applied to supervised learning tasks, and the prin-ciples that could explain the good performance the y have yielded. The three principal contrib utions of this paper are the follo wing. First we extended RBMs and DBNs in new ways to naturally handle continuous-v alued inputs, sho wing examples where much better predicti ve models can thus be ob-tained. Second, we performed experiments which support the hypothesis that the greedy unsupervised layer -wise training strate gy helps to optimize deep networks , but suggest that better generalization is level abstr actions . These experiments suggest a general principle that can be applied beyond DBNs, and we obtained similar results when each layer is initialized as an auto-associator instead of as an RBM. Finally , although we found that it is important to have an unsupervised component to train each layer (a fully supervised greedy layer -wise strate gy performed worse), we studied supervised tasks in which the structure of the input distrib ution is not revealing enough of the conditional density of y given x . In that case the DBN unsupervised greedy layer -wise strate gy appears inadequate and we proposed a simple x based on partial supervision, that can yield signicant impro vements. Allender , E. (1996). Circuit comple xity before the dawn of the new millennium. In 16th Annual Confer ence Chen, H., &amp; Murray , A. (2003). A continuous restricted boltzmann machine with an implementable training Hastad, J. T. (1987). Computational Limitations for Small Depth Cir cuits . MIT Press, Cambridge, MA. Hinton, G. (2002). Training products of experts by minimizing contrasti ve divergence. Neur al Computation , Tesauro, G. (1992). Practical issues in temporal dif ference learning. Mac hine Learning , 8 , 257 X 277. Utgof f, P., &amp; Stracuzzi, D. (2002). Man y-layered learning. Neur al Computation , 14 , 2497 X 2539. Welling, M., Rosen-Zvi, M., &amp; Hinton, G. E. (2005). Exponential family harmoniums with an application to
