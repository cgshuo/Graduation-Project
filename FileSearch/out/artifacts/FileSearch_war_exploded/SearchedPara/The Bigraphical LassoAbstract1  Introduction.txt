 Alfredo Kalaitzis a.kalaitzis@ucl.ac.uk John Lafferty lafferty@galton.uchicago.edu Neil D. Lawrence N.Lawrence@sheffield.ac.uk Shuheng Zhou shuhengz@umich.edu When fitting Gaussian models to data, an indepen-dence assumption is usually made across data points and the covariance matrix is fit by penalized likeli-hood. The number of parameters in the covariance matrix can be reduced by low rank constraints such as factor analysis (see e.g. Tipping &amp; Bishop, 1999) or by constraining the inverse covariance (or precision) to be sparse (e.g. Banerjee et al., 2008). A sparse precision matrix defines a Gaussian Markov random field relationship which is conveniently represented by a weighted undirected graph (Lauritzen, 1996). Nodes that are not neighbors in the graph are conditionally independent given all other nodes. Models specified in this way encode conditional independence structures between features.
 An alternative Gaussian modeling approach was intro-duced by Lawrence (2005) where the i.i.d. assumption is across data features . Lawrence (2012) showed that spectral dimensionality reduction methods have an in-terpretation as sparse graphical models over the in-stances (rows), with the parameters of the covariance fit by maximum likelihood (or in the case of local lin-ear embeddings (Roweis &amp; Saul, 2000) by maximizing a pseudolikelihood).
 Feature independence or data point independence is a model choice issue. Both are special cases of a more general framework that models conditional indepen-dence relationships between features and data points together. This is the type of model that we study in this paper. Specifically, we are concerned with esti-mating a sparse graph that interrelates both features and data points. In video data, for instance, both the frames of the data and the image variables (pix-els) are of course correlated. In Section 5 we illustrate our modeling approach by estimating the conditional independence structure for simple video in the COIL data set. In gene expression data, as another example, it may be of interest to extract a gene network from the expression values and also estimate ancestral re-lationships among the samples in a separate network. Such problems motivate the models and estimation al-gorithms studied in this paper. 1.1. Graphical Lasso and the Matrix-Variate The graphical lasso (GLasso, Friedman et al., 2008; Banerjee et al., 2008) is a computationally efficient pe-nalized likelihood algorithm for learning sparse Gaus-sian Markov random fields (GMRF) over features of i.i.d. vector-variate Gaussian samples.
 The matrix variate normal (Dawid, 1981; Gupta &amp; Na-gar, 1999) is a Gaussian density which can be applied to a matrix through first taking a vectorized ( vec ) rep-resentation 1 of the matrix samples X  X  R n  X  p and as-suming the covariance matrix has the form of a Kro-necker product (KP) between two covariance matri-ces, separately associated with the rows and columns of the data matrix. The KP assumption for the co-variance implies that the precision matrix is also a KP, which is formed from the KP of the precision matrices associated with the rows and columns (  X   X   X  ). One approach to sparse graphical modeling of matrix data is to combine the KP-structured matrix normal with the graphical Lasso. Dutilleul (MLE, 1999) used a flip-flop approach for maximum likelihood estima-tion of the parameters of the matrix-normal, and much later Zhang &amp; Schneider (2010) used it for MAP es-timation with sparsity penalties on the precision ma-trices. More recently, Leng &amp; Tang (2012) applied the SCAD penalty (Fan &amp; Li, 2001) as well as the Lasso penalty to the matrix normal. Tsiligkaridis et al. (2013) analyzed the asymptotic convergence of Kro-necker GLasso and carried out simulations showing significant convergence speedups over GLasso.
 While the KP structure arises naturally when consid-ering matrix-normals, it results in relatively dense de-pendencies between the rows. More precisely, if  X  in  X   X   X  is non-zero (for example, corresponding to an edge between samples i and j in the design matrix X ) then many edges between features of sample i and sample j (as many as in  X  ) will also be active. A sparser structure would benefit situations where the connection between a feature of some sample and a different feature of any other sample is of no interest or redundant. For instance in a video, it is reasonable to assume that the neighbors of pixel ( i,j ) in frame k are conditionally independent of the neighbors of pixel ( i,j ) in frame k + 1, conditioned on pixels ( i,j ) of both frames. 1.2. The Bigraphical Lasso We propose the bigraphical lasso (BiGLasso), a model for matrix-variate data that preserves their col-umn/row structure and, like the KP-structured ma-trix normal, simultaneously learns two graphs, one over rows and one over columns of the matrix sam-ples. The model is trained in a flip-flop fashion, so the number of lasso regressions reduces to O ( n + p ). However, the model preserves the matrix structure by using a novel Kronecker sum (KS) structure for the precision matrix, (  X   X  I ) + ( I  X   X  ) instead of the KP. This structure enjoys enhanced sparsity in comparison to the conventional KP structure of matrix normals. Graph Cartesian Product When operating on ad-jacency matrices, the KS is also known in algebraic graph theory as the Cartesian product of graphs and is arguably the most prominent of graph products (Sabidussi, 1959; Chung, 1996; Imrich et al., 2008). This endows the output of the BiGLasso with a more intuitive and interpretable graph decomposition of the induced GMRF, see figure 1 for an example.
 Enhanced Sparsity For a matrix density  X   X  [0 , 1] of both precision matrices the KS has O (  X np ( n + p )) non-zeros, whereas the KP has O (  X n 2 p 2 ) non-zeros. Better Information Transfer in GPs Kronecker product (KP) forms have a known weakness, referred to in the Gaussian process (GP) literature as the can-cellation of inter-task transfer . Zellner (1962), Bink-ley &amp; Nelson (1988) pointed out how the considera-tion of correlations between regression equations leads to a gain in efficiency. A KP covariance function can compromise potentially useful dependencies between the responses of a multi-output GP. In particular, Bonilla et al. (2008,  X  2.3) showed the following regard-ing multi-output GPs: if a noise-free covariance is KP-structured 2 and the same inputs are conditioned for all outputs 3 , then the predictive mean uncouples the dif-ferent tasks, that is, the posterior (GP conditioned on inputs) factorizes across the outputs such that they are independently computable. The property does not apply under the presence of additive noise, hence the outputs remain coupled. This result first arose in geo-statistics under the name of autokrigeability (Wacker-nagel, 2003) and is also discussed for covariance func-tions in (O X  X agan, 1998). On the contrary, due to its additive form a KS-structured noise-free covariance enables inter-task transfer. The autokrigeability result relies on the factorisable property of the inverse of a KP, (  X   X   X  )  X  1 =  X   X  1  X   X   X  1 . This property enables a simple flip-flop approach to fitting but also forbids the exploitation of correla-tion between different outputs. On the other hand, by coupling the outputs with additive noise on the KP, flip-flop is no longer straightforward. Stegle et al. (2011) addressed this issue by adding i.i.d. noise to a KP covariance  X  a low-rank factor for confounding effects and a sparse-inverse factor for inter-sample de-pendencies  X  and exploiting identities of the vec(.) notation for efficient computation within the matrix normal model.
 To summarize our contributions, in contrast to exist-ing approaches that use the KP structure, the KS pre-serves the inter-task transfer. Our algorithm main-tains the simplicity of the flip-flop with a simple trick of transposing the matrix-variate (samples become fea-tures and vice versa). At the same time, the induced Cartesian factorization of graphs provides a more par-simonious interpretation of the induced Markov net-work.
 The rest of this paper is structured as follows. We describe the matrix normal model with the KS-structured inverse-covariance in  X  2. In  X  3, we present the BiGLasso algorithm for learning the parameters of the KS inverse-covariance. We present some simula-tions in comparison to a recent KP-structured matrix normal model of Leng &amp; Tang (SMGM, 2012) in  X  4 and an application to an example from the COIL dataset in  X  5. We conclude in  X  6. To motivate our KS-structured model, first we con-sider the standard case where matrix-variate data Y are sampled i.i.d. from a matrix-normal distribution (or matrix Gaussian). This is a generalization of the Gaussian distribution towards higher-order tensor sup-port 4 . The matrix normal can be reparametrized such that the support is now over vectorised representa-tions of random matrices and it naturally follows a KP-structured covariance, 2.1. The KP-based SMGM Under the sparsity assumption of the KP-structured precision matrix  X  n  X   X  p , the SMGM estimator (Sparse Matrix Graphical Model) of Leng &amp; Tang (2012) is an extension of GLasso that iteratively mini-mizing the ` 1 -penalized negative likelihood function of (  X  n ,  X  p ) for the KP-structured matrix normal: min n log |  X  n where Y i is the i -th matrix sample, N is the sample size and  X  1 , X  2 the regularization parameters. Complexity and Biconvexity The SMGM objec-tive is a non-convex problem as the trace produces interaction terms between the two precision matri-ces. However, it is biconvex as convexity holds only with respect to either precision matrix individually but not jointly. The flip-flop approach to minimization proceeds by fixing one of the precision matrices (say, the columns-precision matrix  X  p ), thus reducing the above to a convex GLasso problem on  X  n with a pro-jected covariance ( P i Y i  X  p Y &gt; i ). Similarly, another GLasso step fits the the columns-precision matrix  X  p with a fixed rows-precision  X  n . As such its complex-ity on each iteration is that of two GLasso problems or O ( n + p ) lasso regressions. Note that each GLasso step involves an additive O ( N ) time-complexity term as the summation depends on a new estimate of the precision matrix, so in total the complexity is O ( n + p + N ). 2.2. The KS-based BiGLasso Let Y  X  R n  X  p be a random matrix. If its rows are generated as i.i.d. samples from N ( 0 ,  X  p ), then the sampling distribution of the sufficient statistic Y &gt; Y is Wishart ( n,  X  p ) with n degrees of freedom and scale matrix  X  p . Similarly, if the columns are generated as i.i.d. samples from N ( 0 ,  X  n ), then the sampling distribution is Wishart ( p,  X  n ).
 From a maximum entropy viewpoint we can constraint these second-order moments in a model both for the features and the datapoints of a design matrix. To do so, we combine these sufficient statistics in a model for the entire matrix Y as p ( Y )  X  exp  X  tr  X  n YY &gt;  X  tr  X  p Y &gt; Y , (3) where  X  n  X  R n  X  n and  X  p  X  R p  X  p are positive defi-nite matrices. This is equivalent to a joint factorized Gaussian distribution (see  X  3 in supplementary mate-rial) for the n  X  p entries of Y , with a precision matrix of the form where  X  is the Kronecker-product and  X  the Kronecker-sum operator. Thus, for i,k  X  { 1 ,...,n } and j,l  X  { 1 ,...,p } . As an immediate benefit of this parameterization, while the full covariance matrix has O ( n 2 p 2 ) entries, these are governed in our model by only O ( n 2 + p 2 ) parameters. Given data in the form of some design matrix Y , the BiGLasso estimates the sparse KS-structured inverse-covariance of a matrix normal through the ` 1 -penalized negative likelihood function of (  X  n ,  X  p ): are empirical covariances across the datapoints (rows) and features (columns) respectively.
 Complexity and Convexity The BiGLasso objec-tive is a convex problem because the negative log-determinant is convex and the KS is an affine op-eration. Therefore any coordinate descent algorithm for (6) converges to the global minimum. A solu-tion simultaneously estimates two graphs  X  one over the columns of Y , corresponding to the sparsity pat-tern of  X  p , and another over the rows of Y , corre-sponding to the sparsity pattern of  X  n . Note that (6) does not require a summation over the datapoints in each step as was the case in (2). Also note that since  X  ii,jj =  X  ii +  X  jj , the diagonals of  X  p and  X  n are not identifiable (though we could restrict the inverses to correlation matrices). However, this does not affect the estimation of the graph structure (locations of ze-ros). Similarly, the BiGLasso time complexity is that of two GLasso problems or O ( n + p ) lasso regressions. Note that a naive GLasso approach on matrix data would be O ( np ) but both the BiGLasso and SMGM exploit the special KS or KP precision structures that they respectively assume. A note on notation If M is an np  X  np matrix written in terms of p  X  p blocks, as then tr p ( M ) is the n  X  n matrix of traces of such blocks 5 : We alternate between optimizing over  X  n while hold-ing  X  p fixed and optimizing over  X  p while holding  X  n fixed. First we consider the case where there is no regular-ization. From (6), the first step of the optimization problem is reduced to Section 2 in the supplementary material shows how to take the gradient of (8) with respect to  X  n . Com-bining (5) and (6) of the supplementary material we obtain the stationary point: where we define W , (  X  n  X   X  p )  X  1 . We partition V , 1 p tr p ( W ) as where v 1 \ 1 is a vector of size n  X  1 and V \ 1 \ 1 ( n  X  1)  X  ( n  X  1) matrix. Despite the complex form of the stationarity condition, only the lower-left block of its partition will be of use: Similarly, we partition W into blocks: where W 11 is a p  X  p matrix and W 1 \ 1 is a p ( n  X  1)  X  p matrix. Then from the bottom-left block of W X  = = I n  X  I p , we get with 0 n  X  1 as the vector of n  X  1 zeros. According to the stationary point in (11), taking the blockwise trace tr p ( . ) of both sides, gives the equation: A By imposing an ` 1 penalty on  X  1 \ 1 , this problem re-duces to a Lasso regression: min After estimating  X  1 \ 1 , we compute W 1 \ 1 by substi-tuting into (13). It remains to compute W 11 . This follows from (12), which gives This algorithm iteratively estimates columns of  X  n and W in this manner. The procedure for estimat-ing  X  p , for fixed  X  n , becomes directly parallel to the above simply by transposing the design matrix (sam-ples become features and vice-versa) and applying the algorithm. Algorithm 1 outlines the BiGLasso. In our Algorithm 1 BiGLasso Input: Y , X , X  and initial estimates of  X  n and  X  p repeat until (6) converges or maximum iterations reached. experiments we treat  X  and  X  as the same parameter and the precision matrices  X  n and  X  p are initialized as identity matrices. The empirical mean matrix is sub-tracted from each dataset. Although cross-validation can be used to tune the regularization parameters, it typically overselects. To empirically assess the efficiency of BiGLasso, we generate the datasets described below from cen-tered Gaussians with Kronecker-product (KP) and Kronecker-sum (KS) precision matrices. We run the BiGLasso and SMGM using the ` 1 penalty. The  X  p and  X  n precision matrices in both cases are generated in accordance to (  X  4, Leng &amp; Tang, 2012); namely, as either of the following d  X  d blocks ( d being either p or n ) of increasing density: 1. A 1 : Inverse AR(1) (auto-regressive process) such 2. A 2 : AR(4) with A ij = I ( | i  X  j | = 0) + 0 . 4 I ( | i  X  3. A 3 = B +  X  I , where for each B ij = B ji ,i 6 = j , (or true-positive rate ) and precision = cations to assess the b  X  estimates under various setups.
 Each box shows a particular setup that varies in block combination ( A 1 , A 2 , A 3 ), in block sizes ( n,p ), in sam-ple size N generated from the matrix-normal and by the structure used (KS or KP) to generate the sample. Each curve in a box is the solution-path of a replication in precision-recall space for a range of regularization settings  X  = 5 x , for x  X  [  X  6 ,  X  2] interpolated 10 times. The blocks are arranged such that the overall density of the structured precision matrices increases from left to right.
 We note that since blocks A1,A2 have a fixed form, for such combinations each curve is a different sample from the same graph structure. Only A3 is random so in combinations involving A3, each box has a differ-ent random A3 and consequently generates a set of 50 replicates from a different graph. At a glance this has little effect.
 Figures 2 and 3 also compare against the results of SMGM (using the Lasso penalty) on data sim-ulated from the matrix-normal with KS structures. Leng &amp; Tang (2012) had also ran comparisons against the MLE method of Dutilleul (1999) (an unpenal-ized variant of SMGM), ridge-SMGM (SMGM with an ` 2 penalty instead of ` 1 ) and the GLasso of Friedman et al. (2008) (on vectorized samples from N 0 ,  X   X  1 n  X   X   X  1 p , i.e. ignoring the matrix struc-ture). They consistently outperformed all of these methods, so for brevity we compare only against the SMGM. Similarly, figure 3 visualizes the simulations under KP structures.
 By the empirical distributions of these solution-paths (50 for each model in each box), it is no surprise that the intrinsically denser SMGM tends to have low pre-cision (many false-positives) for smaller values of  X  . On the contrary, BiGLasso tends to have low recall (many false-negatives) due to its intrinsically sparser structure.
 Block A3 is the only randomized sparse structure whereas A1 and A2 are more  X  X rtificial X  as they re-spectively model an inverse-AR(1) and AR(4) and they yield banded precision matrices. Of interest is the observation that the largest effect of the increase in sample size (10  X  100) seems to occur on the A3/A3 combination (right end column of boxes). More pre-cisely in Figure 2, we note the difference from box (1,6) to (2,6) and from (3,6) to (4,6). The sample size is very effective: with sufficiently large sample size N, BiGLasso starts to recover exactly and SMGM occu-pies lower regions in general.
 In Figure 3, since the data generation process uses Kronecker-product structures, the SMGM is expected to outperform our method. Indeed for lower-density structure, the recovery rate of the SMGM seems con-sistently better than BiGLasso. and recovery can be almost exact for the SMGM for combination A1/A1. However, as the overall density increases, the perfor-mance of BiGLasso is balanced. Again, for combi-nations involving A3, larger sample sizes benefit Bi-GLasso more.
 In summary, KP-simulated data proved harder to tackle for both methods than KS-generated data. These simulations have shown that the BigLasso con-sistently outperforms the SMGM on KS-simulations, with the possibility of exact recovery on large sample sizes. On KP-simulations the comparison is less clear, but the BiGLasso proves more practical for denser Kronecker-product structures and the SMGM more practical for sparser structures. In this section we carry out a simple video analysis of a rotating rubber duck from the COIL dataset 6 . The video consists of gray-scaled images, see Figure 4. The goal is on two fronts: to recover the conditional dependency structure over the frames and the struc-ture over the pixels. For simplicity, we reduced the resolution of each frame and sub-sampled the frames (at a ratio 1:2). After vectorizing the frames (stack-ing their columns into 81  X  1 vectors) and arranging them into a design matrix Y , the resulting single  X  X at-apoint X  that BiGLasso has to learn from is 36  X  81 (#frames  X  vectorized frame length). Unlike our pre-vious simulations where we had many matrix-samples, here the challenge is to learn from this single matrix ( N = 1).
 Despite the big loss in resolution, the principal com-ponent (PCA) subspace of the rotating duck seems to remain smooth, see Figure 5. Being a time-series, the video is expected to resemble a 1D manifold,  X  X omeo-morphic X  to the one recovered by PCA shown in figure 5, so we applied the BiGLasso on the reduced images. Indeed, the left panel of figure 6 shows the row-precision parameter of BiGLasso capturing a manifold-like structure where the first and last frames join, as expected of a 360  X  rotation. The model recovered the temporal manifold structure, or in other words, we could use it to connect the dots in Figure 5 in case the true order of the frames was unknown (or randomly given to us).
 The right panel of Figure 6 shows the conditional de-pendency structure over the pixels. This figure shows strong dependencies at intervals of 9  X  that is, roughly in line with the size of a frame (due to the column-wise ordering of the pixels). This is expected, as neighbor-ing pixels are more likely to be conditionally depen-dent.
 A more intuitive picture of the induced Markov net-work is shown in Figure 7. A Gaussian graphical model can be naturally interpreted as a system of springs, where the off-diagonal entries of the inverse-covariance represent the negative stiffness of the springs. There-fore by the colorbar, a negative-color represents an  X  X ttracting X  spring between those two pixels and a positive-colour represents a  X  X epulsing X  spring. Nat-urally, in the frames network almost all non-zero ele-ments are negative.
 In this paper we proposed new techniques for mod-eling conditional dependencies, as encoded by the inverse-covariance of a matrix normal density. In high-dimensional cases the Markov network structures in-duced by a graph could be approximated by factorisa-tions such as the tensor-product (Kronecker-product of precision matrices). In this work, we motivated a novel application of the Cartesian factorization of graphs (Kronecker-sum of precision matrices), as a more par-simonious and interpretable structure for inter-sample and inter-feature conditional dependencies. We intro-duced the bigraphical Lasso, an algorithm for the si-multaneous point-estimation of the structures of two Gaussian graphical models: one over the rows of a matrix-sample and the other over its columns. This was demonstrated to good effect through simulations as well as a small example from the COIL dataset. For future research, the Kronecker sum structure may be of interest in both Gaussian processes and mod-eling higher order tensors. In multi-output GPs, a Kronecker-product noise-free covariance decouples the outputs when a block design is used. The additive form is an appealing feature of the Kronecker-sum for the preservation of inter-task transfer, thereby leading to potential applications on Kronecker-sums of kernels for multi-output Gaussian processes. The associativ-ity of the Kronecker sum may also yield an approach to the modeling of datasets organized into 3 or higher-dimensional arrays (amounting to GMRFs over higher-order tensors) with dependencies across any subset of the array dimensions.
 Software and Data Related source code for reproducing the experiments will appear on github.com/alkalait/biglasso .
 Acknowledgments AK would like to thank Amos Storkey, Eleni Vasilaki, Ricardo Silva and Martin Wainwright for useful dis-cussions. The authors would like to thank the review-ers for their useful comments. Research supported in part by NSF Grant IIS-1116730 and ONR grant N000141210762.
 Banerjee, O., El Ghaoui, L., and d X  X spremont, A.
Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data.
The Journal of Machine Learning Research , 9:485 X  516, June 2008.
 Binkley, J. K. and Nelson, C. H. A note on the effi-ciency of seemingly unrelated regression. The Amer-ican Statistician , 42(2):137 X 139, 1988.
 Bonilla, E. V., Chai, K. M. A., and Williams, C. K. I. Multi-task Gaussian process prediction. In Platt, J.C., Koller, D., Singer, Y., and Roweis, S. (eds.),
Advances in Neural Information Processing Systems 20 , pp. 153 X 160, Cambridge, MA, 2008. MIT Press, Cambridge, MA.
 Chung, F. R. K. Spectral Graph Theory (CBMS Re-gional Conference Series in Mathematics, No. 92) . American Mathematical Society, December 1996.
ISBN 0821803158. URL http://www.worldcat. org/isbn/0821803158 .
 Dawid, A. P. Some matrix-variate distribution the-ory: notational considerations and a Bayesian ap-plication. Biometrika , 68(1):265 X 274, 1981.
 Dutilleul, P. The mle algorithm for the matrix normal distribution. Journal of statistical computation and simulation , 64(2):105 X 123, 1999.
 Fan, J. and Li, R. Variable selection via nonconcave penalized likelihood and its oracle properties. Jour-nal of the American Statistical Association , 96(456): 1348 X 1360, 2001.
 Friedman, J., Hastie, T., and Tibshirani, R. Sparse in-verse covariance estimation with the graphical lasso. Biostatistics , 9(3):432 X 441, Jul 2008.
 Gupta, A. K. and Nagar, D. K. Matrix variate distri-butions . Chapman Hill, 1999.
 Imrich, W., Klavzar, S., and Rall, D. F. Topics in
Graph Theory: Graphs and Their Cartesian Prod-uct . AK Peters Ltd, 2008. ISBN 1568814291, 9781568814292.
 Lauritzen, S. L. Graphical models , volume 17. Oxford University Press, USA, 1996.
 Lawrence, N. D. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Re-search , 6:1783 X 1816, November 2005.
 Lawrence, N. D. A unifying probabilistic perspective for spectral dimensionality reduction: Insights and new models. Journal of Machine Learning Research , 13:1609 X 1638, 2012.
 Leng, C. and Tang, C. Y. Sparse matrix graphical models. Journal of the American Statistical Associ-ation , 107(499):1187 X 1200, 2012.
 O X  X agan, A. A Markov property for covariance struc-tures. Statistics Research Report 98-13, Nottingham University , 1998.
 Roweis, S. T. and Saul, L. K. Nonlinear dimensionality reduction by locally linear embedding. Science , 290 (5500):2323 X 2326, 2000. doi: 10.1126/science.290. 5500.2323.
 Sabidussi, G. Graph multiplication. Mathe-matische Zeitschrift , 72:446 X 457, 1959. ISSN 0025-5874. URL http://dx.doi.org/10.1007/ BF01162967 . 10.1007/BF01162967.
 Stegle, O., Lippert, C., Mooij, J., Lawrence, N. D., and Borgwardt, K. Efficient inference in matrix-variate
Gaussian models with iid observation noise. Ad-vances in Neural Information Processing Systems , 24:443, 2011.
 Tipping, Michael E. and Bishop, Christopher M. Prob-abilistic principal component analysis. Journal of the Royal Statistical Society, B , 6(3):611 X 622, 1999. doi: doi:10.1111/1467-9868.00196.
 Tsiligkaridis, T., Hero, A., and Zhou, S. On con-vergence of Kronecker graphical lasso algorithms.
Signal Processing, IEEE Transactions on , PP(99): 1, 2013. ISSN 1053-587X. doi: 10.1109/TSP.2013. 2240157.
 Wackernagel, H. Multivariate geostatistics . Springer, 2003.
 Zellner, A. An efficient method of estimating seem-ingly unrelated regressions and tests for aggregation bias. Journal of the American statistical Associa-tion , 57(298):348 X 368, 1962.
 Zhang, Y. and Schneider, J. Learning multiple tasks with a sparse matrix-normal penalty. Advances in
Neural Information Processing Systems , 23:2550 X 
