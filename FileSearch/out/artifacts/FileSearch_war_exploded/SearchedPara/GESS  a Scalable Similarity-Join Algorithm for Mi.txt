 during the join-phase due to a tremendous number of distance computations and therefore, MSJ is not com-petitive to a simple hash-based method [20]. memory required by MSJ to run efficiently (without any swapping) is very high (46% of the input-relation for typical situations). MSJ is therefore not scalable for large data sets. 1.0 tO 0.4 .."' " 0.0 10 20 30 40 50 60 70 80 90 100 Figure 1: Length ~ of a hypercube in one dimension as a function of d (total number of dimensions) results (x,x) from the response set. 
For an arbitrary similarity join, we employ the join based on the Loo metric as a filter step. This join is equivalent to an intersection join using sets of hypercubes H(x) '.= H~,4(x) where c refers to the length of the hypercube in the paper. 
The most serious problem of processing high-dimensional intersection joins is the so-called curse of dimensionality, a problem that has been known in statistics for long [22]. Under the assumption that data is uniformly distributed, it seems to be difficult to design efficient join algorithms. In different volumes (V) of the hypercubes. The results are ob-tained under the uniformity and independence assumption. Fortunately, these assumptions do not hold for real data sets where strong correlations between different dimensions can be observed. Results obtained from different experiments with real data [12] have given a strong indication that the fractal dimension is an excellent measure for the true di-mensionality of a data set. The analytical results obtained under the assumption of uniformity and independence are applicable and accurate for correlated data sets when d is simply replaced by the fractal dimension [2]. 
We are mainly interested in methods for processing simi-indexes. These methods can be classified into three cate-gories: 10 Stack_S.insert (H(x)); 11 } 12 } The code defined above returns a string with digits from partitioning we employ the lexieographical orderin 9 on the precede in the order. 
A recursive partitioning P(D) provides a skeleton where a hypercube H(x) represented by its feature vector x is as-signed to one or multiple disjoint subspaces of P(D). The spaces. The following join methods differ in their assignment strategies. 2.4.1 Review of Orenstein's Algorithm 
In this subsection, we briefly review the join-algorithm proposed by Orenstein [18][19] (termed ORE in the follow-ing). The method is based on a binary reeursive partitioning (n = 2), see Definition 2, where the binary code represents the so-called Z-ordering. 
ORE assigns each hypercube of the input relations to dis-joint subspaces of the recursive partitioning whose union entirely covers the hypercube. ORE sorts the two sets of hypercubes derived from the input relations (including the possible replicates) w.r.t, the lexicographical ordering of its binary code. After that, the relations are merged using two main-memory stacks Stack_R and Stack_S, see Figure 2. It is guaranteed that for two adjacent hypercubes in the stack, the prefix property is satisfied for their associated codes. This property is ensured in the third line of the algorithm. Therefore, only those hypercubes are joined (line 5 and 9) that have the same prefix code. 
A deficiency of ORE is that the different assignment strate-gies examined in [19] cause substantial replication rates. This results in an increase of the problem space and hence, sorting will be very expensive. Furthermore, ORE has not addressed the problem of eliminating duplicates in the result set. 2.4.2 Review of MSJ MSJ performs similar to ORE with two main differences: First, replication is not allowed and second, an I/O strategy based on so-eailed level-files is employed. Moreover, an n-ary recursive partitioning is used where n = 2 d (quadtree-partitioning). 
Let us first discuss for MSJ how a hypercube is assigned to a subspaee of the recursive partitioning. Among the sub-spaces which cover the hypercube we choose the minimum one. This guarantees that one and only one subspace is 10 Else If ( split_is-Allowed(H(x), P(D) ) ) { 11 replicate(H(x) N DO, DO ); 12 replicate(H(x) N D1, D1 ); :3 } 14 Else { 15 return (H(x), CodeD(D)); 16 } :7 } 
For sake of simplicity, let us first assume a binary re-cursive partitioning (n = 2). Later we will then change to a quadtree partitioning which is actually used in our implementation. The replication algorithm takes a hyper-cube H(x) and a subspace D as its input. Initially, it is called with the subspace representing the entire data space. The algorithm checks whether H(x) C Do Y H(x) C_ D1 with the enclosing direct subspace. Otherwise (i.e. H(x) Do A H(x) ~ D1) the algorithm calls the user-defined pred-icate split_is_Allowed. If the predicate is satisfied the hy-percube H(x) is split into two replicates H(x) N Do and H(x) N D1. The algorithm is then invoked recursively for process stops and the code determining the actual subspace is returned. If a hypercube is replicated, multiple codes are returned by the algorithm. Figure 5 reports the algorithm in pseudo-code. 
Our replication algorithm depends on a user-defined pred-icate split_is_Allowed that determines the replication strat-egy. The strategy is not fixed and can be changed dynami-cally. 
Assume a quadtree-partitioning for the rest of this paper, (P(D) = {D} U U0&lt;~&lt;2d P(D~)). We put our focus on the following important-strategies: STRATEGY 1 (MAXIMUM NUMBER OF SPLIT-LINES). Splitting a hypercube H(x) is allowed if not more than k hyperplanes are hit by H (x) at the current quadtree-level. 
STRATEGY 2 (MAXIMUM SPLIT LEVEL). Splitting a maximum split level. 
STRATEGY 3 (COMPOSITION-STRATEGY). Splitting a hypercube H(x) is allowed if Strategy 1 and Strategy 2 hold. 
Important for the Merge Algorithm of 0renstein (see Fig-ure 2) is the lexicographical ordering (see Section 2.4). A PROOF. Let P(D) be an n-ary recursive partitioning of H(y) are the corresponding hypercubes. The replication algorithm of GESS computes a set of disjoint subspaces, say Rep(x) C P(D) and Rep(y) C_ P(D) for each hypercube H(x) and H(y). The elements of Rep(x) and Rep(y) provide a conservative approximation for H(x) and H(y). Since V A,B E Rep(x),A ~ B,A n B = 0 and V A,B E Rep(y),A ~ B, AMB = 0, the reference point can only be in one of the subspaces of Rep(x) and Rep(y). It follows that for which holds. Let us assume A C_ B. Then, 
In the following we consider the CPU cost of the intersec-tion join for MSJ and GESS. 
Both of the methods partition the input relations R and S R(S) for GESS. The CPU cost of the methods is dearly dominated by the number of distance computations required in the join phase. LEMMA 2. The average number of distance computations The proof of the lemma is based on the fact that every hy-percube of P~ has to be checked against those hypercubes of S (including the replicated ones) which have the same prefix. In order to compute the number of distance computations DC we need to compute how many hypercubes will be on the different partitioning levels. We assume in our analyti-cal model that the hypercubes are entirely in the unit cube problem yet. However, through a simple linear mapping of inal join problem into an equivalent one that provides the desired properties. We transform each component x~ of a feature vector x as follows: Furthermore, we modify the join predicate of the intersec-tion join to 5l 
In order to compute the probability of a hypercube being conditional probability. For sake of simplicity, let us first assume that replication does not increase the number of ob-jects in the lower levels. The conditional probability to hit more than k hyperplanes at level l is given by Since the events on the different levels are independent, the conditional probability on the right side of equation 6 can simply be expressed by a product. Overall, we obtain the following equation: P(X~ &gt; k) = z-1 i=o The expected number of hypercubes at level I is then given by Next, we extend our model in such a way that the num-ber of replicated objects axe considered on the lower levels. In general, it is difficult to obtain an exact formula when ent levels are not independent anymore. Therefore, we are interested in a simple and accurate estimation. We make the following two simplifying assumption: Firat, we assume that the replicated objects follow the same distribution of the original objects. Second, we assume that the replicated objects axe of the same size as the original ones. 
Let rl be our estimation of the number of hypercubes at level I. Furthermore, let P&gt;~ be our estimation of number of hypercubes that passed level I. By setting ~-l = IR[, our estimations Pz and ~&gt;~ are recursively defined by the following formulas: 6.1 we will provide results from a simulation to show that our estimations are sufficiently accurate. In this section, we first examine the I/O cost of MSJ and GESS where R and S are the input relations. The I/O cost is expressed in the number of pages transferred between disk and main memory. We assume that at most M pages are available in memory. We charge one I/O for a page trans-fer. The basic idea of MSJ is that .r/and S are partitioned into level files Ro, R1,... and So, S1,..., respectively. For in the base relations r/and S, respectively. Figure 7: Maximum size of data sets if no interme-diate merges are used Figure 8: Improvement of GESS over MSJ in terms of I/O can be sorted without using intermediate merges. Figure 7 depicts maxd~ as a function of M (Msorter R = M]3, page size = 4K). The graph shows that even for small values of M very large data sets can be sorted without the need of intermediate merges, e.g. data sets as large as 690 MB can be processed using only 2 MB memory. We conclude that in most practical cases GESS does not need any intermediate merges. For these cases, the I/O cost of GESS is independent from the fact whether replication occurs or not. Figure 8 depicts the improvement of GESS over MSJ. GESS performs at least two times better than MSJ for all buffer sizes. If 10% buffer is available GESS is 2.5 times better. For smaller buffer sizes the improvement is 2.7 and better. 
In order to confirm the analytical results we have imple-mented a simulator in Java based on the random engine of the COLT library [10]. Our simulator generates uniformly distributed hypercubes and computes the codes and the cor-responding levels for each hypercube. 
The results depicted in Figure 9 show an excellent agree-ment of the result obtained from our simulation and the ones of the analytical model (d = 10, 6 = 10-4). The graphs provide the relative occupation of hypercubes among the data set II description I ~ points I d[ size I CAD of CAD-parts 1,312,173 16 160 MB CAD10 sample of CAD data set 131,217 16 16 MB Table 1: Description of the data sets used in the experiments Table 2: Execution times in seconds for GESS and MSJ for a self-join 
Figure llb shows the results of the same experiment for msl = 0. The memory requirements in the graph first de-crease exponentially and then remain constant. The latter ues of k level 0 is almost empty. 
In this section, we present results of an experimental eval-uation of GESS and MSJ. The experiments were performed on an AthlonTB 700 with 256 MB main memory and 40 GB hard disk. All algorithms were implemented in Java 1.2 on top of the XXL-library [5, 4] using Suns just-in-time com-piler. The buffer available to the algorithms was set to 10 MB. We run the experiments on real world data sets that have already been used in other experiments [8]. Table 1 describes important properties of the data sets. The charts in Table 2 show the execution time for GESS. Our new algorithm takes 772 seconds to compute the join for e = 0.001 and 275 seconds for e = 0.0001. For MS J, however, the corresponding runtimes were 3,821 seconds and 35,093 seconds ~ 10 hours, respectively. This means that the speedup of GESS is 14 and 45 for e = 0.0001 and created a sample of the CAD data set where we only consid-ered every tenth hypercube of the original data set for the join. Table 2 depicts the result where GESS performs the forms the join on the sample in 56 seconds. For e = 0.001, our algorithm takes 46 seconds, whereas the runtime of MSJ rises to 398 seconds. Overall, GESS is 8 times faster than MSJ for that experiment. 
The number of results for a self-join on CAD for e = 0.001 is already 4.1 million which is surprisingly large. Under the uniformity and independent assumption, the number of a strong indication that the true dimensionality [12] of the data is much lower. 
In order to illustrate the dramatic performance improve-ments of GESS, we varied the replication strategy of our algorithm. Figure 12 shows the runtime as a function of the parameter k. Recall that hypercubes are split when at most k hyperplanes are hit. For k &gt; 3 the curve remains almost constant (770 seconds). For k = 2 however the curve rises to 1,010 seconds. For k = 1 the execution time was i000 100 4,427 seconds. This demonstrates that a modest kind of replication dramatically improves the performance of the al-gorithm. Overall, the replication rate was always less than 14%. 
The overhead of the reference point method was irrelevant in most experiments. In the worst case of the experiments, 15% of the total runtime was spent for duplicate removal. 
In this paper, we discussed different methods for process-ing similarity joins in high-dimensional spaces. We pre-sented a new method (GESS) which employs a modest de-gree of replication to improve its overall runtime. The per-formance of GESS is compared to MS J, analytically and periments we observed that GESS clearly outperforms MSJ. 
Our experiments show the tremendous potential of our approach. As we have seen, the replication strategy used for GESS plays an important role to achieve a good trade-off between replication and the improvement factor IF. For practical purposes it is useful to find an 'optimal' strategy that determines the replication strategy dynamically. We are currently investigating dynamic and adaptable replica-[16] N. Koudas and K. Sevcik. High Dimensional [17] M.-L. Lo and C. V. 1%avishankax. Spatial Hash-Joins. [18] J. Orenstein. Spatial Query Processing in an [19] J. Orenstein. An Algorithm for Computing the [20] J. Patel and D. DeWitt. Partition Based [21] H. Samet. The Design and Analysis of Spatial Data [22] D. Scott. Multivariate Density Estimation. [23] K. Shim, 1%. Srikant, and 1%. Agrawal. [24] R.. Weber, H.-J. Schek, and S. Blott. A Quantitative d U msl mL R,S M DC 
IF improvement factor for GESS over MSJ in 
