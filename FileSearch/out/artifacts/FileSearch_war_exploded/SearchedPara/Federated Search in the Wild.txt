 Federated search has the potential of improving web search: the user becomes less dependent on a single search provider and parts of the deep web become available through a uni-fied interface, leading to a wider variety in the retrieved search results. However, a publicly available dataset for fed-erated search reflecting an actual web environment has been absent. As a result, it has been difficult to assess whether proposed systems are suitable for the web setting. We in-troduce a new test collection containing the results from more than a hundred actual search engines, ranging from large general web search engines such as Google and Bing to small domain-specific engines. We discuss the design and analyze the effect of several sampling methods. For a set of test queries, we collected relevance judgements for the top 10 results of each search engine. The dataset is publicly available and is useful for researchers interested in resource selection for web search collections, result merging and size estimation of uncooperative resources.
 H.3.3 [ Information Search and Retrieval ]: Miscella-neous Federated search, distributed information retrieval, evalua-tion, dataset, test collection, web search
Web search has become the most popular way for finding information on the web. The general web search engines use crawlers to populate their indices. However, a large part of the web, also called the hidden or deep web , is not easily crawlable [11]. Usually these pages are dynamic and only accessible through a search engine. A solution to this prob-lem is federated search , also called distributed information retrieval . Queries are directly issued to search interfaces of collections, taking away the need to crawl these collections. Given a query, a broker selects suitable search engines. The query is then forwarded to these search engines. The broker gathers the results and creates a single ranked list. Exam-ples of federated search on the web include vertical search, peer-to-peer networks and metasearch engines [12].

While federated search has been studied for many years and has many applications for the web, an appropriate dataset reflecting an actual web environment has been absent. So far, people have created artificial collections by dividing TREC collections [12], for example by topic or source. These collections are very different from actual search engines we find on the web, which have different retrieval methods, skewed sizes and heterogeneous content types (images, text, video etc.). As a result, it is not clear to what extent the findings so far in federated search hold in a web setting.
In this paper, we introduce a new dataset for federated search that we have made publicly available. The dataset contains result pages from 108 actual web search engines (such as Google, Yahoo, YouTube and Wikipedia). For each search engine, several query based samplings have been pro-vided for resource selection. We also provide the responses and relevance judgements of their results for a set of queries. Results are annotated in two ways, by judging the snip-pet created by the engine and by judging the actual doc-ument. This dataset reflects an uncontrolled environment that can often be found in real federated search applications on the web. For example, the actual sizes of the resources, as well as the used retrieval algorithms are unknown. For researchers, the dataset is useful to evaluate resource selec-tion, resource merging, and to experiment with size estima-tion of uncooperative resources. The dataset is available at http://www.snipdex.org/datasets.

We first discuss related work. We then describe the data collection and present analyses of the dataset. We conclude with a summary.
A federated search system presents three challenges: re-source description , obtaining a representation of the resource, resource selection , selecting suitable collections for a query and resource merging , creating a single ranked list from the returned results [2]. In this section we will focus on resource description, because this is highly related to the construc-tion of our dataset. In addition, we review existing test collections for federated search. Resource description The broker maintains a representation for each collection to facilitate the selection of suitable collections. In coopera-tive environments, in which each collection is willing to pro-vide broker meta-information about its contents, these rep-resentations could contain the complete term statistics. In uncooperative environments, where collections do not pro-vide meta-information about its contents or when this infor-mation cannot be trusted, the term statistics are typically approximated by sampling documents from the collection. Query-based sampling [3] involves selecting queries, sending them to the collection, and downloading the top n docu-ments for each query. These queries are usually single words sampled from the current representation. Another approach involves selecting queries from an external resource. This ap-proach can give more representative samples, however it is less efficient since some queries might not return any results. It has been suggested that 300-500 documents are sufficient for sampling a collection [3]. Recently, snippets were used instead of documents, eliminating the need to download doc-uments [16].
 Test collections The performance of resource selection methods was found to vary between test collections (e.g. [20], [14], [10]). Test collections for federated search have been constructed by reusing existing TREC datasets. For example, TREC disks 1 to 3 were divided into 100 collections based on source and publication date, and TREC4 was clustered using k-means into 100 collections. Recent datasets containing web pages were constructed by reusing the GOV2 and WT2G datasets and partitioning the pages by hosts. To simulate a more real-istic environment, the datasets were modified. For example, to reflect collections with skewed sizes or relevant document distributions collections where collapsed together. To reflect collections with overlapping documents, sliding windows and grouping by queries were used [13]. Zhou et al. [19] created a test collection for aggregated search using the ClueWeb09, ImageCLEF and TRECVID collections. Thomas and Hawk-ing [15] created a heterogeneous dataset for meta search, however the number of collections was small and each col-lection (regardless of size) had an equal amount of relevant documents.

Lack of a dataset reflecting a realistic web environment might have caused that only little research has been done on federated search for the web. In addition, the research done so far used data missing some important properties, making it unclear whether these results hold in a more realistic web environment. An exception is the work done by Arguello et al. [1]. However, their data was from a commercial search engine and not publicly available. In addition, they were in-terested in vertical search and therefore assumed a cooper-ative environment. Monroe et al. [8] examined query based sampling for web collections, by creating collections using pages in DMOZ. They found query based sampling to be ef-fective for web collections. Craswell et al. [5] experimented with resource selection for the web. To simulate a web en-vironment where search engines have different retrieval al-gorithms, resources used different retrieval methods, such as BM25 and Boolean ranking. Hawking and Thomas [6] evaluated resource selection methods in the GOV domain and proposed a hybrid approach combining distributed and central IR techniques. Although the work just discussed used actual web pages, the collections were relatively small compared to collections found on the web. For example, the largest collections were around 10 or 20 thousand documents [8, 6], or a couple of thousands [5]. Ipeirotis and Gravano [7] presented a new method for resource description and se-lection. One of their evaluations used 50 actual web search engines, however these were only shortly described and to our knowledge this dataset is not publicly available.
In this section, we discuss collection of the dataset. We will motivate the new dataset, and describe how the resource sampling and annotation were performed.
As outlined in the related work, most existing federated search collections were created using TREC collections. How-ever, these do not sufficiently reflect real search engines on the web. We believe a dataset for federated web search should have the following properties:
To accommodate the requirements we selected 108 real search engines on the web for our dataset. A diverse se-lection was made, with varying sizes (from very large ones such as Google to small ones such as the website of a com-pany), media types (web pages, images, videos) and domains (from general web search to specific domains like job search). The selection was made from search engines that are acces-sible through the OpenSearch 1 interface. To also include the major search engines that were not accessible through OpenSearch, an additional tool was used to scrape results [17]. A manual categorization of the resources is provided in Table 1. http://www.opensearch.org/
We used query based sampling [3] to obtain resource de-scriptions. The query-based samples and search results were collected between December 21, 2011 and January 21, 2012. We experimented with three methods to select queries for sampling: Random , Top , and Zipf . Random selects single terms randomly from the documents sampled so far. Top uses the most popular queries from a web search engine X  X  query log [9]. Zipf uses single term queries taken evenly from the binned term distribution in ClueWeb09, where terms were binned on a log-scale of their document frequency ( df ) to ensure that there are queries from the complete frequency distribution. For each query, the top 10 results from each engine were retrieved. Sampling was done by collecting the snippets of 197 queries and downloading the actual docu-ments of the first 40 queries. Example queries for Top and Zipf , which use the same queries for each resource, are pre-sented in Table 2. Random uses different queries per re-source.
As test queries, we used 50 topics (Topics 51-100) from the 2010 TREC Web Track Ad Hoc task [4]. Every query was sent to each resource and the top 10 results were collected. An ID, URL, title, date, snippet and corresponding HTML page were stored. The results were stored in a standard format: &lt;snippet id="DT01-0115-1037-06"&gt; &lt;origin pid="2e692a94a01e8c5dd3ec9cbb581798c6"/&gt; &lt;location cached="0115/random/1037-06.html"&gt; &lt;/location&gt; &lt;title&gt;Asian elephant&lt;/title&gt; &lt;found&gt;2012-01-04 10:29:12&lt;/found&gt; &lt;summary&gt; &lt;/summary&gt; &lt;/snippet&gt;
The evaluation set was created using 10 judges, includ-ing IR researchers and outsiders, with the goal of obtaining relevance assessments both for the snippets and, indepen-dently, for the pages of the returned results. Many of the topics in the 2010 TREC Web Track are highly ambiguous. They come with a general information need (description) and several subtopics. All judgements were based only on the general information need (description) of a query. For example, for the ambiguous query raffles , the accompanied description is Find the homepage of Raffles Hotel in Singa-pore .
 Snippet judgements From potentially 54,000 results to be judged (if each search engine would have returned 10 results for all queries), there were in practice only just over 35,000 results. First, all snip-pets were judged. The snippets for one query were all judged by the same person. Most queries were judged by a single judge, but a few were judged by several, such that in to-tal about 53,000 judgements were recorded. Per query, all gathered snippets were shown one by one in a random order, displaying their title, summary, preview (if present), and the page URL.

Given the snippet and information need for a query, the judges annotated the snippet with: Junk , Non , and Unlikely , Maybe , Sure , or Answered . The last category, where the par-ticular information need had been answered by the snippet itself, appeared not applicable for the considered topics. Page judgements The process of judging pages turned out to be significantly slower than snippets, and therefore the page annotation task was organized as follows. We assume that snippets labeled Junk or Non do not represent relevant pages. Although we might miss some relevant documents due to this assumption, an actual user would not have clicked on the snippet any-way. Therefore, only pages for which the snippet was rated Unlikely , Maybe , or higher by at least one of the judges, were judged. That reduced the amount of required page judge-ments to only 28% of the total number of snippets.

Six levels of relevance were used, denoted in increasing order of relevance as Junk , Non , Rel , HRel , Key , and Nav , corresponding with the relevance levels from the Web TREC 2010 ad-hoc task [4]. The judgements were based on a snapshot from the page, taken simultaneously with the snip-pets. Additionally, the HTML data from the page had been crawled and was available to the judges. For each query, all pages for which a judgement was required, were shown and judged in a random order and by the same judge. The other pages were rated Non , by default 2 .

Often, the same website appeared in the top 10 results from different search engines. Hence, in order to determine the reference judgement for each result, the URLs were first normalized to a standard form (e.g., omitting search engine specific additions, like Sponsored , or the query terms) and all judgements for the same normalized URL were considered together. The number of unique URLs amounts to 90.5% of the number of snippets. Moreover, for 11 out of 50 topics, we had all pages (and snippets) judged by at least two people. The different judgements corresponding to a specific URL were processed as follows. Each judgement received a score s , with value 0 ( Junk or Non ), 1 ( Rel ), 2 ( HRel ), 3 ( Key ), or 4 ( Nav ). The average  X  s of these scores was determined. In this paper, we only use binary page relevance levels for evaluation. A page is considered relevant if the page is on average rated Rel or higher, otherwise non-relevant. An alternative, more demanding, criterion for relevance would be on average HRel or higher.

An extensive discussion of the reliability of test collections is found in [18], in which it is shown that the overall evalu-ation of retrieval systems is not very sensitive with respect to different annotators. The overlap between two different sets of relevance assessments, defined as the size of the inter-section of the relevant document sets divided by the union of the relevant document sets for both judges, was found to lay between 0.42 and 0.49 for the TREC-4 collection used in [18].

For the current test collection, using the annotations by two different judges for 11 of the test topics, the average overlap is 0.43, quite similar to [18], confirming that this col-lection is equally apt for comparing retrieval systems. Note that, as in [18], the overlap varies significantly among the queries. This is due to the following reasons. First, we have instructed the judges to interpret the information need in a more general ( multimedia ) way, leading to different opin-ions on relevance for the results to some topics (e.g., when judging pictures). In addition, the background and inter-ests of the judges have probably influenced their judgement. The worst case (overlap = 0.18) is topic 97, south africa , a highly ambiguous query, whereas the best annotator agree-ment (overlap = 0.69) was found for topic 74, kiwi (general information need:  X  X ind information on kiwi fruit X  ), being simple, hence less prone to subjective judgement.
As a summary, the dataset contains the following:
From those snippets that had been judged Non by one judge, but higher by at least one other judge, only 3% of the pages were judged HRel . The pages from those snippets are however expected to be more often relevant than the ones from snippets judged Non by each assessor. Therefore, it is expected that on average, less than 3% of the pages that were not judged, would correspond to a highly relevant page, which confirms that our assumption is acceptable.
Note that future systems will not be disadvantaged when being evaluated on this dataset, because we provide rele-vance judgements for all resources without pooling based on runs. We obtained search results over many billions of docu-ments, undoubtedly much bigger than ClueWeb09. But, we do not need all actual documents to test federated search.
We now discuss the characteristics of the dataset. All analyses are based on the relevance judgements using pages, assuming the relevance criterion Rel or higher.
We first discuss the dataset in the context of the desired properties outlined in the previous section.
 Heterogeneity As shown in Table 1 the collections span a wide range of topics as well as content types. For example, there are 21 collections that focus on multimedia (e.g. video, sound, images, files in general), and the text genres range from news to blogs to Q &amp; A.
 Different retrieval algorithms We do not need to sim-ulate retrieval algorithms, but use the existing ones on the web. Most of them are (probably) specifically tailored to the specific collection they are searching.
 Relevance distribution For each resource, we calculate the proportion of queries the resource has at least one rele-vant result for. A histogram is presented in Figure 1. The distribution is highly skewed, with many resources not con-taining any relevant document for most of the queries. The 10 top resources according to the proportion of queries the resources have at least one relevant result for, are exactly the 10 general web search engines as defined before. We ob-serve the same trend in Figure 1b, which shows the average number of relevant documents per query. No resource has on average more than 6.1 (out of the first 10) relevant doc-uments per query, suggesting that a good resource selection algorithm would be useful.
 Document overlap Since we are dealing with actual search engines, we can only estimate the amount of overlap be-tween them. We find that 34 collections had URL domains not shared with other collections in our samplings. These were collections that returned results from within their do-main, such as CERN documents, CiteULike, Viddler and Wikispecies. Search engines with relatively high overlap are the ones in the category General web search (see Table 1). Skewed sizes Although the actual collection sizes are un-known, it is clear that the collection sizes vary widely, with very larges ones such as Google, Bing and Mamma.com, and very small ones that are dedicated to a very specific topic. (a) Avg fraction rele-vant/query
In this section we analyze the query selection methods for query based sampling.
 Average number of results In Figure 2, histograms are shown of the average number of results returned for a query per resource. Using the ran-dom method, which selects queries depending on the re-source, most queries have many results. However, for the top and Zipf methods, which select queries from an external resource, some of the resources return on average almost no results, resulting in a small amount of sampling documents from those resources.
 Figure 2: Histograms average number of results Overlap between sampling queries and TREC queries We found no overlap in the Zipf and top queries with the test queries (Web TREC 2010). However, there were a to-tal of 10 queries in random that matched exactly with one of the test queries such as rice (3x) or iron (2x). In addi-tion there were more partial matches, for example wall ( the wall ), sun ( the sun ) etc. However, since the random queries are different for each resource, the total number of queries that matched is negligible (10 out of about 21,000 queries).
In this paper we presented a new test collection for feder-ated search on the web, containing search results of over a hundred actual search engines including Google, YouTube, Wikipedia, Bing and many others. The dataset is publicly available and is useful for researchers interested in resource selection for web search collections, result merging and size estimation of uncooperative resources.

The construction of a second version of the data set is planned, with more samplings and more queries targeting specific collections or collection categories. This research was partly supported by the Netherlands Organization for Scientific Research, NWO, grant 639.022.809 and the Folktales as Classifiable Texts (FACT) project, part of the CATCH programme, and partly by the IBBT (Inter-disciplinary Institute for Broadband Technology) in Flan-ders.
