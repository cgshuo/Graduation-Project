 and the choice of a kernel in multiple kernel learning (see Se ction 2). kernel learning, for moderate values of the sample size. 2.1 Framework and notation Let us assume that one observes where  X  measurable function X 7 X  R and x is made on the set X . The goal is to reconstruct the signal F = ( f ( x estimator b F  X  R n , depending only on ( x n ( Y 1 , . . . , Y n )  X  R n , that is, b F = AY x quantities X  X sually regularization parameter or kernel com bination weights. 2.2 Examples of linear estimators such as the ones defined below.
 Hilbert space (RKHS) F , with norm k k K ab = k ( x a , x b ) kernels [9] X  X s obtained by minimizing with respect to f  X  F [2]: The unique solution is equal to b f = P n smoothing matrix A kernels k J ( f 1 , . . . , f p )= 1 n Note that when  X  -norm and thus the regular Lasso [12].
 Using a 1 / 2 = min 2 ( f 1 , . . . , f p ) min where I form parameterized by the regularization parameter  X   X  R amenable to gradient descent procedures X  X hich only find loca l optima. matrices A , and are not entirely covered by our theoretical results. the notion of minimal penalty. 3.1 Unbiased risk estimation heuristics of matrices ( A ( b
F quadratic risk of b F The best choice would be the oracle : data-driven b  X  satisfying an oracle inequality with large probability, where the leading constant C the remainder term R minimizes a criterion crit(  X  ) such that the empirical risk and a penalty term, i.e., using a criterio n of the form: (deterministic) penalty When b F where  X  = Y  X  F  X  R n and  X  t, u  X  R n , h t, u i = P n matrix  X  2 I up to the term  X  E [ n  X  1 k  X  k 2 Note that df(  X  ) = tr( A selection.
 lows X  C mator c  X  2 . The estimator of  X  2 usually used with C some  X  on  X  ( A for most A Massart [6] and further studied in [7]. 3.2 Minimal and optimal penalties We deduce from Eq. (3) the bias-variance decomposition of the risk: and from Eq. (4) the expectation of the empirical risk: instance, they are equal when A symmetric with a spectrum Sp( A the present informal reasoning.
 First, as proved in [20], the bias n  X  1 k ( A as classical fact in model selection.
 negative variance term which is the opposite of As suggested by the notation pen If then, up to concentration inequalities that are detailed in Section 4.2, b  X  mizer of g (  X  ) = E Therefore, two main cases can be distinguished: As a conclusion, pen a penalized criterion is not clearly overfitting.
 [21, 7, 22], we now propose to use that pen A projection estimators [6, 7]) to general linear estimators for which pen 4.1 Algorithm then considers the ideal penalty in Eq. (5) for selecting  X  .
Input:  X  a finite set with Card( X )  X  Kn  X  for some K,  X   X  0 , and matrices A possible in all cases to find a C such that df( b  X  sive values of df( b  X  4.2 Oracle inequality some K,  X   X  0 . Assume that  X   X   X   X  , A Gaussian with variance  X  2 &gt; 0 , and that  X   X  df(  X  1 )  X  Then, a numerical constant C every n  X  C Furthermore, if then, a constant C 4.3 Discussion of the assumptions of Theorem 1 Symmetry. The assumption that matrices A is only used for deriving from Eq. (13) a concentration inequ ality for h A [0 , 1] barely is an assumption since it means that A  X  actually shrinks Y . Assumptions ( A (
A Assumption ( A Theorem 1, but it is only needed for Eq. (11). According to Eq. (6), ( A A (
A general, ( A associated with a model of dimension df(  X  ) = tr( A When ( A this is not the case. Left: single kernel case, Right: multip le kernel case. Nevertheless, an oracle inequality can still be proved with out ( A slightly and adding a small fraction of  X  2 n  X  1 tr( A Enlarging b C is necessary in general: If tr( A  X  is very close to 2  X  2 n  X  1 tr( A underestimates  X  2 , even by a very small amount. 4.4 Main consequences of Theorem 1 and comparison with previo us results of such as the one usually used with Mallows X  C Eq. (11): The risk of the selected estimator b F [23, 19], C the effective dimensionality of b  X  a criterion proportional to the empirical risk. x as kernel, right: multiple kernels.
 Jump. In Figure 2 (left), we consider data x ways of optimizing over  X   X   X  = R 2 gradient descent X  X  situation not covered by Theorem 1.
 right part (multiple kernel), we compare to the performance of Mallows X  C all A [6, 7]. Indeed, Theorem 1 shows that for general linear estim ators, hence mostly when A but always relying on the concept of minimal penalty.
 almost equivalent to taking  X  = R minimal penalty algorithm and Theorem 1 certainly have to be modified. [2] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . MIT Press, 2001. [9] G. Wahba. Spline Models for Observational Data . SIAM, 1990. [17] C. L. Mallows. Some comments on C [23] K.-C. Li. Asymptotic optimality for C
