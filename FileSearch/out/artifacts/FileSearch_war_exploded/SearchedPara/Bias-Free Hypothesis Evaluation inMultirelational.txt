 In machine learning one typically assumes that the true classification of an ob-ject depends only on the object itself and, given the object, is independent of the classification of other objects. In this case, the observed sample error on a suffi-ciently large and randomly chosen independent test set is an unbiased estimator of true error. However, many applications rely on relational data where the label of an object may probabilistically depend on the labels and/or attributes of other related objects or shared parts of objects ( X  X utocorrelation X ). As pointed out by [1], whenever there is autocorrelation , the observed error on a randomly chosen test set may not be an unbiased estimator anymore. In [1] this issue is addressed using subgraph sampling , which however completely eliminates the dependency between training and test sets and thus is applicable only to problem settings where future data never link to previously seen data.

In this paper we therefore propose generalized subgraph sampling (GSS), a sampling procedure based on bin packing, which ensures that test sets are prop-erly chosen to match the probability p kn S of reencountering pr eviously seen ob-jects and which includes subgraph sampling as a special case for p kn S =0.Inthe following section we first introduce the i ssues associated with autocorrelation in more detail and then present the GSS algorithm in Section 3. We experimen-tally compare two variants of our sampling algorithm with random sampling in Section 4. The paper concludes with a summary and further challenges for error estimation using multirelational data.
 In multirelational domains the assumption of independent instances cannot be taken for granted. Let us consider an example that clearly shows the dependen-cies between objects due to linkage and aut ocorrelation [1]. The Internet Movie Database 1 (IMDb) stores information on over 450,000 movies, including actors, producers, studios and box office receipts. We regard the learning task to predict whether a movie has box office receipts of more than $2 million given information about the studio that made the movie. More formally, the application consists of two kinds of objects, movies X and studios A . Figure 1 shows the relevant structure of the movie data set. We will refer to a studio a  X  A as a neighbor of amovie x  X  X if the studio produced that movi e. The set of movies sharing the same studio a forms the neighborhood of a .The degree  X  a specifies the number of movies produced by studio a .

If we proceed as usual to estimate the error of a hypothesis and divide the movie data into a training and test set randomly (according to some split pro-portion), we will very likely assign movies of the same studio to both resulting sets. A dependency between the training and test set arises as the sets share some of their neighbors. In fact, when using random splits the relative frequency of common neighbors increases with the ch osen split ratio between training and test set [2]. How does this dependency influence the error estimate? Since the la-bels of movies produced by the same studio are correlated (it is plausible that big studios will make many movies with big b ox office receipts), any learner capable of exploiting relations (in particular probabilistic relational learners [3, 4, 5]) will form a hypothesis that exploits the known objects, and thus will make fewer errors predicting the label of movies from known studios than of movies from unknown studios. Thus, the more objects with known neighbors are in the test set, the lower the estimated error will be even though the hypothesis is the same. This has led [1] to postulate that the dependency between the training and test set should be removed. They present a procedure, subgraph sampling, which en-sures that any information (in this case st udios) shared between different objects is included in either the training or the test set and thus eliminates the above mentioned bias in error estimation.
This approach, however, considers only the application setting in which the studios of all future movies have never been seen before. Yet, in many applica-tions future objects that we need to cla ssify with our induced hypothesis will actually have known neighbors. In the movie domain it is quite likely that a new movie will be produced by one of the already existing studios. Therefore, the er-ror will be overestimated if the links be tween training and test set are removed completely. Instead, the test set should reflect the probability that a randomly drawn (future) object is linked to known neighbors. We call this probability the known neighbor probability .
 Definition 1 (Known Neighbor Probability). For an instance space X ,a distribution D X ,asample S ,aset A of neighbors and a function nb : X  X  A assigningtoeachinstance x  X  X its neighbor a  X  A , the known neighbor probability for an instance x randomly drawn according to D X is defined as p Given a sample S , the known neighbor probability p kn S is a domain property. In a transductive learning setting or a context where the distribution over the complete instance space were known, its computation would be straightforward. In most cases, however, the known neighbor probability must be supplied by the user based on application considerations. In order to arrive at an unbiased estimate, the fraction of objects in the test set with neighbors also present in the training set should match the known neighbor probability. How can this goal be achieved ? Above we already remarked that ran-dom sampling is incapable of establishing the known neighbor frequency, as the amount of related objects in the test set varies with the chosen training/test set split proportion 2 . Therefore, we propose generalized subgraph sampling (GSS), which is a sampling procedure based on the known neighbor probability. It en-sures that for a given data sample S , known neighbor probability p kn S and a chosen split proportion p train the resulting test set contains the same propor-tion of objects with known neighbors wit h respect to the training set as specified by the known neighbor probability. GSS includes subgraph sampling as proposed by [1] as a special case for p kn S =0.

The task to install the known neighbor probability into the test set can be considered as a bin packing problem. In g eneral, bin packing requires to pack a set of items into a number of bins such that their total weight does not exceed some maximum value. More specific, GSS needs to fill three bins. The first bin, S train , contains the training instances. The second and third bin, S test,ind , contain the test instances which are either rel ated to or ind ependent of instances in the training set respectivel y. We designed two versions of GSS. The first version (Simple) prefers neighbors with a small degree in order to sustain the specified bin sizes and is allowed to adjust the chosen split proportion if necessary. The second version (Modified) chooses all objects randomly, yet may discard data tuples to preserve the known neighbor probability as well as the chosen split ratio. Algorithm 1 depicts Part 1 and 2 of GSS Simple, which satisfy the specification if the sample contains a sufficient number of neighbors with degree  X  a &gt; 1. We indicate a subset by adding a subscript to the name of the originating set, e.g. S test denotes the test set created from sample S and S a denotes the neighborhood of neighbor a . The sizes of S , S test,rel and S test,ind are denoted by n , n test,rel and n test,ind respectively. For further details see [2]. We evaluated both variants of our algorithm on data from the IMDb and com-pared their performance against random sampling. Table 1 on top shows the achieved known neighbor probabilities for three chosen split proportions. As can be seen, both versions of our algorithm ar e successful in ensuring the required known neighbor probability of 0.45 regardless of the split proportion. As ex-pected, the known neighbor probability obtained by random sampling varies as the chosen split proportion changes. The bottom of Table 1 shows that both al-gorithms produce exactly the required sizes at a split proportion of 0.7 and 0.9. For a chosen split proportion of 0.5 both algorithms yield significantly enlarged training sets, which results from an unexpected large number of neighbors with  X  a =1. In relational domains it is well known that high linkage and autocorrelation cause a bias in test procedures. Therefore, s ampling procedures must be adjusted to provide for an unbiased error estimate. Present approaches only address the spe-cial case where no further dependencies between the data sample and randomly drawn future objects are expected. We p ropose a sampling procedure that con-trols the amount of dependent objects in the test set. Our evaluation shows that GSS is an effective sampling procedure that guarantees to partition a sample according to a given known neighbor probability.

So far our procedure relies on the user to provide the known neighbor prob-ability. It is a topic of future research to investigate whether certain conditions allow to estimate the known neighbor probability directly from the data sample.
