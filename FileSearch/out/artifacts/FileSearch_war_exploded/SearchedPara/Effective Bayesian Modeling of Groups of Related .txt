 Most classical time series forecasting models such as ex-ponential smoothing (Hyndman et al., 2008) and ARIMA models (Box et al., 2008) assume that observations are real-valued and can take on both positive and negative values. In addition, the majority of classical approaches provide nor-mal predictive distributions, if they do so at all. However, large segments of the practice of forecasting X  X or instance in supply chain planning X  X eal with time series that depart significantly from these assumptions: series, for example, that may consist only of non-negative integer observations, contain a large fraction of zeros, or are further character-ized by long runs of zeros interspersed by some large non-zero values. In other words, the classical assumptions of conditional normality are grossly violated. Moreover, if multiple contemporaneous series are considered, common models either treat them completely independently, or X  X s in vector autoregressive models (Box et al., 2008) X  X ttempt a more complex multivariate modeling that captures short-range cross-correlations but becomes unwieldy when man-aging hundreds of series; the common scenario of  X  X eak coupling X  between related series (e.g. consumer demand for a seasonal product at several stores of the same chain in a given city, which could share seasonal behavior but not strong cross-correlations) is not easily handled in classical modeling frameworks.
 1.1. Motivating Applications The starting point for the present work lies in the intermit-tent demand series that frequently occur in supply chain op-erations: these arise, for example, in the demand for spare parts in aviation, or in the number of  X  X low-moving X  items sold in retail stores (Altay &amp; Litteral, 2011). In addition to the non-negativity, integrality, skewness and high frac-tion of zeros attributes already outlined, these time series are commonly quite short: many weekly and monthly de-mand series encountered in practice may consist of some 30 to 100 observations. This makes it crucial to allow some information sharing across related series to more reliably capture posited common effects such as seasonalities and the impact of causal determinants (such as the market re-sponse to promotions or supply chain disruptions). These are the modeling challenges that we address in this paper. 1.2. Related Work Most of the literature on intermittent demand forecasting relies on relatively simple techniques, typically variants of Croston X  X  (1972) method which computes the expected de-mand in the next period as the ratio of the expected non-zero demand to the expected non-zero-demand time in-terval, both estimated by simple exponential smoothing. Croston X  X  method produces point forecasts only; Shenstone &amp; Hyndman (2005) studied variants with a proper stochas-tic foundation that can produce predictive intervals, al-though with no attempt to capture some known stylized facts of intermittent demand patterns such as heavy tails (Kwan, 1991). It is only with the recent work of Snyder et al. (2012) that a reasonably modern formulation was pro-posed in terms of a state-space model and distributional forecasts. This model still tracks the expected demand through an exponential-smoothing update, but emits pre-dictive distributions that belong to the negative binomial family (described in the next section); model parameters are estimated by maximum likelihood (ML). Despite the evidence of improved accuracy against common bench-marks, this approach still exhibits a number of shortcom-ings: it is fundamentally univariate, does not easily allow explanatory variables, and the ML estimation framework does not reflect model parameter uncertainty that arises with the very short series that are common in practice. 1.3. Contributions This paper makes the following three contributions: ( i ) introducing a hierarchical probabilistic state-space model that is a good match to commonly-seen types of count data, allowing for explanatory variables and permitting informa-troducing an effective inference algorithm for computing posterior distributions over latent variables and predictive distributions (  X  3); ( iii ) assessing the proposed approach X  X  performance via a thorough experimental evaluation (  X  4). We introduce the proposed model in several stages, start-ing with the basic state-space structure and integer-valued observations (  X  2.1), introducing explanatory variables and structural zeros (  X  2.2), and finishing with the hierarchical structure allowing information sharing across series (  X  2.3). 2.1. Core Model For a single non-negative time series, the model is ex-pressed in state-space form (e.g., Durbin &amp; Koopman, 2012), where the latent state  X  t at period t = 1 ,...,T represents the log-expected value of the non-negative in-teger observation y t . Of those, we shall assume that the first T  X  h ( h  X  1 ) are observed, and the last h consti-tute the future values over which we would like to forecast. Representation in log-space enforces the constraint that the process mean can never become negative. The state space structure makes all observations independent of each other conditionally on the latent state; here we assume that ob-servation y t is drawn from a negative binomial (NB) dis-tribution with mean exp  X  t and size parameter  X  (which is independent of t ), The negative binomial distribution (parametrized by the mean  X  instead of the more usual probability of success in a trial; e.g. Hilbe 2011) is given by
P NB ( y |  X , X  ) = The negative binomial is appropriate for count data that is overdispersed with respect to a Poisson distribution (with the variance greater than the mean); the size parameter  X  &gt; 0 governs the level of overdispersion. The limiting case  X   X  X  X  converges to a Poisson distribution.  X  The dynamics of the process log-mean  X  t depend on the properties of the time series being modeled. For a station-ary series, a mean-reverting autoregressive process is a sen-sible and tractable choice. In a supply chain context, mean reversion intuitively means that the long-run expected de-mand for an item, when projected far in the future, should fall back to a constant level in spite of any past transient disturbances. We express latent dynamics as an AR(1) pro-cess with normal innovations, with where  X   X  R is the long-run level of mean reversion,  X  1 &lt;  X  &lt; 1 is the speed of mean reversion,  X  &gt; 0 is the precision of the process innovations, and  X  0 &gt; 0 allows for additional variance in the initial period. All t are assumed mutually independent. The model structure is depicted in graphical form in Fig. 1. Forecasting in this model concep-tually proceeds in three steps: ( i ) from the observed values of the time series, we carry out inference over all unob-served model variables (the clear nodes in Fig. 1); ( ii ) us-ing the inferred process parameters (  X , X , X  ), we project the latent dynamics into the future to obtain a distribution over future values of the latent state (  X  5 and  X  6 in the figure); and ( iii ) obtaining a predictive distribution over future ob-servations ( y 5 and y 6 in the figure). This description can easily be extended to accommodate multivariate observa-tions and latent states; the latent states would then follow a vector autoregressive (VAR) process. 2.2. Explanatory Variables and Structural Zeros Explanatory variables (which can include seasonal terms, as well as factors that causally impact the observed time series) can be incorporated by viewing them as local forc-ing terms that temporarily shift the location of the latent process mean. This is illustrated in Fig. 2. We assume that explanatory variables at period t , x t  X  R N , are always ob-served, non-stochastic and known ahead of time (so that we know the future values of { x t } over the forecasting horizon). They are linearly combined through regression coefficients  X  to additively shift the latent  X  t , yielding an effective log-mean  X   X  t , where the {  X  t } follows the same AR(1) process as previ-ously. In latent (log) space, the addition operation corre-sponds to a multiplicative impact of explanatory variables on the process mean in observation space, which is often a good fit to the underlying data generating process (e.g. seasonalities, or consumer response to promotions or spe-cial events). It also makes the regression coefficients  X  rel-atively independent of the scale of the series, and makes it easier to share information across multiple time series as described in  X  2.3.
 In many real-world series, one observes an excess of zero values compared to the probability under the NB (e.g., Lambert, 1992): this can arise for structural reasons in the underlying process (e.g., out-of-stock items or supply chain disruptions in a retail store context, both of which would override the natural consumer demand modeled by the NB). For these reasons, we add extra unconditional mass at zero, yielding so-called zero-inflated NB observa-tions, where  X  0 represents unit probability mass at zero and z  X  [0 , 1] is the probability of structural zero. We assume a Beta( 1 2 , 1 2 ) prior for z . This is our final observation model. 2.3. Sharing Information Across Multiple Time Series Finally, we allow for a group of L related time series to share information, particularly in the form of a shared  X   X  hyperprior over regression coefficients and latent process characteristics. In the spirit of hierarchical models studied in statistics (Gelman &amp; Hill, 2007) and machine learning (Teh &amp; Jordan, 2010; Fox et al., 2010), we let those pa-rameters (for all time series `  X  { 1 ,...,L } that belong to the group being modeled simultaneously) share common parents, as illustrated on Fig. 3. A new plate iterates over the series-level parameters, which inherit as follows from  X  X lobal X  parameters shared across all time series: where  X ( a,b ) represents the gamma distribution with shape parameter a and scale parameter b , and Beta( a,b ) is the beta distribution with shape parameters a and b . The series-level parameters (variables in plate ` on Fig. 3) all have the same meaning as previously, except that the ` index makes them dependent on a specific time series. The hyperpriors that are used for the global parameters are given in the sup-plementary material. The latent dynamics of {  X  `,t } and the observation model are the same as in the previous sections. For convenience, we shall denote all  X  X lobal X  variables in Fig. 3 (except  X   X  , for reasons to be made clear shortly) all series-` -level variables (except  X  ` ) by  X  { z l , X  ` , X  ` , X  0 ,` , X  ` ,  X  ` , X   X ,` } , and all latents over which we should do inference by  X  =  X  G  X  X   X  ` } X  X   X   X  , X  ` , X  `,t In the remainder of this paper, we call this model the hier-archical negative-binomial state space ( H-NBSS ) model. It must be stressed that this model assumes that all time series in the group are conditionally independent given the series-level parameters. In particular, the model does not allow expressing observation-level cross-correlations across different time series ` i 6 = ` j , except through com-mon effects coming from explanatory variables. 1 Due to the non-conjugacy between the zero-inflated negative-binomial likelihood and the normal latent log-mean process prior (and the general difficulty of finding useful conjugate priors for negative-binomial likelihoods), inference in the H-NBSS model does not have an analyti-cally tractable solution. One must resort to approximation techniques, which fall, broadly speaking, into two families: deterministic and stochastic methods (Barber et al., 2011). Of the deterministic approaches, early examples include as-sumed density filtering (ADF, Maybeck, 1979), which is a sequential projection approach, as well as numerical inte-gration schemes such as the piecewise approximation of Kitagawa (1987). More recently, the expectation propaga-tion (EP) algorithm of Minka (2001), a generalization of ADF, has proved successful in a number of non-linear fil-tering and smoothing problems (Heskes &amp; Zoeter, 2002; Yu et al., 2006; Deisenroth &amp; Mohamed, 2012). As to stochas-tic approaches, they can take the form of variants of Gibbs sampling, such as the recursive forward-filtering backward-sampling (FFBS) algorithm (Robert et al., 1999; Scott, 2002) as well as sequential Monte Carlo techniques such as particle filtering (reviewed by Doucet et al., 2001). Durbin &amp; Koopman (2000) present an alternative approach based on importance sampling and antithetic variables. Static hi-erarchical regression models have been widely studied in the statistics literature (Gelman &amp; Hill, 2007), where typ-ical inference techniques rely on block Gibbs sampling. Dynamic hierarchical models have been less commonly studied, with the notable exception of the nonparametric Bayesian model of Fox et al. (2010), who use an efficient form of the Metropolis-Hastings algorithm for inference It is imperative to contrast the benefits of a proposed algo-rithm to the requirements of forecasting practice: for in-stance, in a supply chain context, it is routine business to process tens to hundreds of millions of time series on a daily or weekly basis. 2 Despite their inadequacies, prac-titioners still rely on very computationally simple methods such as exponential smoothing for the vast majority of their tasks. Needless to say, for a forecasting approach to have an impact in practice, its accuracy benefits must justify its computational cost. This seems to rule out all stochastic algorithms, as well as many deterministic ones such as EP. We shall argue that for the H-NBSS model, a Gaussian ap-proximation of the latent variables at their posterior mode, known as the Laplace approximation (Bishop, 2006), yields near-optimal performance at extremely attractive computa-tional cost compared to the alternatives. One reason to ex-pect good performance is that most of the important (for forecasting) latent variables in the model ( {  X   X  , X  have a conditionally normal prior; their posterior is nearly always close to normality despite the non-linear likelihood. 3.1. Posterior Calculation The Laplace approximation requires to calculate the log-posterior probability up to an additive constant, log P (  X  | Y ) = log P (  X  ) + log P ( Y |  X  ) + C, (5) where Y = { y `,t } T  X  h t =1 is the set of all observed series values in all groups and C is an unknown (and for the Laplace approximation, unimportant) constant. The log-likelihood term log P ( Y |  X  ) is derived straightforwardly from the observation model (4) along with the negative bi-nomial probability distribution (1). The first term X  X he log-prior X  X ecomposes into global-, series-and observation-level terms, log P (  X  ) = log P (  X  G ) + The second line of this equation expresses a prior over jointly normally-distributed variables with a highly struc-tured (and very sparse) precision matrix, a Gaussian Markov random field (GMRF, see Rue &amp; Held, 2005), to which we now turn. 3.2. GMRF Prior For the single time series process described in (2), assum-ing that the initial  X  1 has the long-run process distribution with precision of 1 / (  X  (1  X   X  2 )) , 3 the joint prior over the la-tent process {  X  t } along with normally-distributed long-run mean  X  (having prior precision  X   X  ) is normally distributed with a tridiagonal precision matrix Q , except for the last row and column (corresponding to  X  ), that follows the pat-tern
Q = where  X   X   X   X   X  1 and  X  T  X  T  X  2( T  X  1)  X  + ( T  X  2)  X  2 , where T is the number of observations. The determinant of this matrix is  X  T  X   X  (1  X   X  2 ) . 4 The sparsity of Q makes it extremely fast to compute the process prior term.
 When considering the hierarchical model of section 2.3, a similar sparsity pattern holds: the joint precision ma-trix across all variables that belong to the GMRF prior has block-diagonal structure, with one sub-matrix like Q for each time series, and a final row/column linking the series means {  X  ` } to the global mean  X   X  . Details are given in the supplementary material. 3.3. Optimization and Predictive Distribution Maximization of (5) over  X  can be carried out efficiently using Quasi-Newton methods such as L-BFGS (Nocedal &amp; Wright, 1999). 5 Let  X   X  be the maximizing value and  X   X  the inverse of H  X   X  , the Hessian matrix of (5) evaluated at  X 
 X  . The Laplace approximation posits that the posterior distribution over  X  is jointly normal with mean  X   X  and co-variance matrix  X   X  . Due to the structure of the GMRF prior, matrix H  X   X  is nearly block-diagonal except for the variables that belong to  X  G . This makes it efficient to compute  X  sparse matrix solvers (e.g., Davis, 2006).
 From the graphical model structure (Fig. 3) and the obser-vation model (4), the predictive distribution over a future value y `,t ,t  X  T  X  h + 1 depends only on the distribu-tainty over z ` and  X  ` is small and can be neglected. From the observation model, the posterior distribution over y `,t can be obtained by integrating out  X   X  `,t , P ( y `,t | Y ) = where a key use of the well-known summation property of the negative binomial is made, wherein for IID variables X i  X  NB(  X  i , X  ) , we have P i X i  X  NB P i  X  i , X  , and we assume that the summation converges to an integral in the limit. Hence, only the posterior expectation of exp  X   X  is needed, which is readily obtained as
E exp  X   X  `,t | Y = exp E [  X   X  `,t | Y ] + 1 2 Var [  X   X  since  X   X  `,t has a normal posterior under the Laplace approx-imation. From (3), where the conditional posteriors for  X  `,t and  X  ` are directly available in  X   X  . Similarly, the posterior variance for  X   X 
Var [  X   X  `,t | Y ] = Var[  X  `,t | Y ]+ where the variances and covariances on the right-hand side are from  X   X  . 3.4. Accuracy Compared to MCMC Ultimately, the validity of approximate inference is predi-cated on its empirical performance, which is evaluated in the next section. Here, we graphically contrast on Fig. 4 the inference results for a single series between the Laplace approximation outlined previously and an equivalent model computed with Markov chain Monte Carlo (MCMC). The latter is implemented in the Stan modeling language (Stan Development Team, 2013), which uses the  X  X o-U-turn X  variant (Hoffman &amp; Gelman, 2013) of Hamiltonian Monte Carlo (HMC). We combined the results of four indepen-dent chains, each run with 1500 burn-in iterations followed by 18500 sampling iterations. Overall, we note the sim-ilarity of the posterior distributions between the two ap-proaches, although the Laplace approximation slightly un-derestimates the posterior variance in  X  t over the forecast horizon (the region denoted  X  X cast X  in the plots) compared with MCMC. Should additional accuracy be required in the Laplace approximation, one could turn to a numerical inte-gration technique for hyperparameters of models equipped with GMRF priors, the so-called integrated nested Laplace approximation (INLA, Rue et al., 2009).
 Of significant importance for practical applications, how-ever, is computational time. For the results illustrated in Fig. 4, whereas our implementation of the Laplace approx-imation (coded in the interpreted language R) converges in a few seconds, a roughly equivalent run of the MCMC sampler takes 30 times longer (and Stan is a very efficient engine, compiling the model into C++ code with analytical gradient computation for HMC). As will be clear from the experimental results, the additional computational cost of MCMC does not translate into a performance advantage in forecasting. We evaluate model performance on three datasets obtained from supply chain operations. The first one (RAID) is the sales of bug spray at 26 locations of a major US retailer. The second one (GLUE) is the sales of gluestick at 2033 US retail locations. The third (PARTS) is the demand for spare parts of a major European IT firm, previously studied by Syntetos et al. (2012). The first two datasets illustrate variability at the location level for the same item (SKU), whereas the third one illustrates variability at the item level. All time series are non-negative integers, some with a large fraction of zeros, and all series of a given dataset covers the same date range; Table 1 shows some summary statistics. The  X  X ean non-zero value X  is the mean series value, con-ditional on the value being positive; the  X  X ean non-zero inter-period X  is the number of periods between non-zero observations; and the  X  X ean sq. coef. of variation X  is the square of the coefficient of variation ( CV 2 ), which is  X  /m 2 for a series with mean m and standard deviation  X  . Model performance is evaluated by the out-of-sample fore-casting accuracy over the horizon h (where h varies from 1 to 12 periods), measured according to the negative log-likelihood (NLL) per period, relative mean squared error (MSE) and relative mean absolute error (MAE). To reduce the scale dependence of the MSE and MAE, the MSE is normalized by the in-sample variance and the MAE is nor-malized by the in-sample mean absolute deviation from the in-sample mean. We evaluate performance by a sequential re-training procedure that alternates between model train-ing and testing, moving at each iteration the first observa-tion of the (previous) test set to the end of the (new) train-ing set. This simulates the action of a decision-maker act-ing in real-time, retraining models as new information be-comes available. All reported results are averages of out-of-sample performance under this procedure. The initial training set durations for each dataset are given in Table 1. 4.1. Benchmark Models We compare the forecasting performance of the pro-posed H-NBSS model against the following benchmarks: ( i ) Croston X  X  (1972) method, ( ii ) simple exponential smoothing (E-S) with additive errors and an automatically-adjusted smoothing constant, and ( iii ) the damped dynamic model with negative binomial observations of Snyder et al. (2012). The first two approaches are as implemented by the corresponding functions in the R forecast package (Hyndman et al., 2013). Since they provide point forecasts only, we evaluate predictive distributions under two alter-natives: a Gaussian distribution, with variance given by the variance of training residuals, or a Poisson distribution. In both cases, the mean is given by the point forecast. 4.2. Performance Results Seasonalities are significant on the RAID dataset, which are incorporated into the H-NBSS through explanatory variables; for this dataset, we report H-NBSS results both without and with seasonalities, for both the Laplace and MCMC approximate inference. Seasonalities are omitted from GLUE and PARTS for space reasons since they yield very similar performance to the model without seasonal effects. Moreover, H-NBSS results in this section con-sider each dataset series independently of the others (i.e. groups of size 1). The benefits of hierarchy are exam-ined in the next section. Out-of-sample performance results for all datasets and models at selected forecasting horizons are given in Table 2. NLL results at all horizons appear in Fig. 5. We observe that on the NLL measure (which measures predictive distributional accuracy) the H-NBSS model consistently yields the best performance, with the Laplace approximation slightly beating MCMC (both ap-proximations are very close). The MSE and MAE mea-sures tell a consistent story, the only exception being the PARTS dataset where Croston very slightly bests the other approaches in the forecast of the mean (MSE measure). This can be explained by high proportion of  X  X umpy X  se-ries in PARTS ( cf. Table 1), which exhibit high demand variability, and hence low predictability. 4.3. Benefits of Hierarchy We close this section by outlining the benefits of the hi-erarchical structure in H-NBSS. The major advantage of sharing information across several series in a group lies in the ability to increase  X  X tatistical strength X , in particular for series for which little history is available. We illustrate this in Fig. 6, which shows that when groups of time series can share information, useful patterns can be learned even for series with very short histories (here, four observations). In contrast, with no sharing, the model cannot do much better than fit a constant.
 These results translate quantitatively in Table 3. On 20 of the 24 series of the RAID dataset, we provided only the four observations before the forecasting horizon; for the other 4 series, we provided the full history. The table con-trasts the performance of a H-NBSS model trained sepa-rately for each series ( X  X ndependent models X ), versus a sin-gle H-NBSS model grouping the 24 series together ( X  X i-erarchical model X ) and reports average performance only on the 20 masked series. There is a dramatic gain in per-formance attributable to sharing in the hierarchical model: seasonalities learned on the four complete series transfer to the incomplete ones. This paper introduced a modeling methodology for groups of related time series of counts, such as the small-integer series frequently encountered in supply chain operations. We outlined the sizable accuracy gains possible through jointly modeling several time series in a hierarchical Bayesian framework and presented an effective approxi-mate inference algorithm to make the H-NBSS model use-ful in practice. Future work should investigate other trade-offs on the accuracy X  X omputational cost spectrum, such as the INLA approach (Rue et al., 2009), recently revisited by Han et al. (2013). Beyond its increased accuracy, the H-NBSS model can provide real-world benefits in applica-tions where count data dominate, for instance by supplying useful forecasts for new stores with very little (or no) his-tory and improving the efficiency of inventory management policies with better distributions of future demand.
