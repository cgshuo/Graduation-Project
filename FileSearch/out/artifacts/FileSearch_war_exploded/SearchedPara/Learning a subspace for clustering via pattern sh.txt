 1. Introduction searching for an interested web can also be improved considerably.

Due to the importance of clustering, a lot of researches have been emerged to address this problem. Existing clustering
Breunig, Kriegel, &amp; Sander, 1999; Kaufman &amp; Rousseeuw, 1990 ). These approaches have been widely used and achieved the dimensionality of data is very high. For example, for an image of d 2007 ).

To cluster high dimensional data, a lot of researches are proposed to first reduce the dimensionality of original data and then to cluster the computed low dimensional embeddings. This is mainly due to the rapid increase of researches about manifold learning, whose aim is to learn a low dimensional representation for high dimensional data. Among these  X  the subspace learning methods.
 Among the approaches that aim to learn a subspace for clustering, the most successful one is to employ the traditional 2007 ). Essentially, both LdaKm and DisKmeans compute the clustering results and learn the subspace alternatively. Some other subspace learning approaches, e.g., Locality preserving embedding (LPP), have also been used before clustering ( He 2009 ).

These approaches perform well in many applications. However, their performance can also be improved since (1) some of these approaches, such as PcaKm, separate the procedures of dimensionality reduction and clustering. In other words, the dimensionality reduction approaches are not particularly designed for clustering. (2) Some of these subspace learning ap-proaches, such as LdaKm, adaptive approach and SMMC, use linear projection for dimensionality reduction. They have not taken fully considerations about the nonlinear structure of data points.

In this paper, we aim to find a linear subspace which can incorporate both the linear and nonlinear structures of original posed. Different from previous subspace learning methods that take the original data as the input, we replace the original data by their shrunk patterns. This replacement not only integrates both the linear and nonlinear manifold structures, but also considers the requirements of subsequent clustering procedure. In the following, we name the proposed approach as Subspace Learning via Pattern Shrinking (SLPS).
 the small circle could move outside of the big circle step by step. See more details in Section 3 .
This paper is structured as follows. Section 2 presents brief introductions about related works. We describe the proposed clusions in Section 5 . 2. Related work
In this section, we will review some related works. The first is about two typical subspace learning approaches and the second is mainly about a framework of semi-supervised learning. 2.1. PcaKm and LdaKm
Assume that each data is represented by a r -dimensional vector and there are totally n samples for clustering, i.e., { x ... , x n }, where x i 2 R r for i =1,2, ... , n . In subspace clustering, we aim to compute a transformation matrix P 2 R is dimensionality of the subspace. Then, each sample is projected into a low dimensional subspace by z embedding of x i . Denote X =[ x 1 , x 2 , ... , x n ], Z =[ z some other clustering techniques, such as Kmeans, in the subspace for clustering.
 sume F 2 R n c is the cluster indicator matrix. F ij  X  1 = the number of clusters, n j is the number of samples in the j th cluster.
In PcaKm, it first employs the PCA technique to reduce the dimensionality. In other words, it tries to find the projection matrix P , which is the solution to the following problem. where 1 is a n -dimensional column vector with all ones.

Then, PcaKm employs the Kmeans approach to compute the clusters, i.e., it aims to minimize the following equation. where C j is a set formed by the data in j th cluster and m
Recall the definition of F and the equation Z = P T X , we rewrite the objective function of Kmeans in Eq. (2) as follows.
In summary, PcaKm first solves the problem in Eq. (1) to compute the optimal projection matrix P and then achieves the clustering results by solving the problem in Eq. (3) .
 and then uses LDA to compute the projection matrix. After projecting each sample, it employs Kmeans to cluster the sample in the low dimensional space and then applies LDA again. After repeating several times, we can compute the clusters and the projection matrix P simultaneously. More concretely, its objective function can be formulated in the following form. To solve this problem, LdaKm uses the alternative optimization strategy. It fixes one parameter and optimizes the other. More interestingly, this optimization procedure is equivalent to applying LDA and Kmeans alternatively.
One point should be mentioned here. In traditional LDA, the dimensionality of subspace is at most c 1, where c is the
The regularized LdaKm algorithm has the following formulation. where k is a regularization parameter. 2.2. Semi-supervised learning
Since our method is inspired by a framework of some semi-supervised learning methods, we would like to provide a brief introduction about this framework. As shown in Zhou et al. (2003) , some semi-supervised learning approaches employ a nonnegative weight W ij to measure the similarity between x tent with the given labels and (2) f should be smooth with respect to the data manifold. This will result in the following objective function.
 Here G is a loss function to measure the consistency between computed labels and given labels. X  X  S X  f  X  is the smooth constraint, which punishes the smoothness of f and usually has the form larization framework ( Belkin, Niyogi, &amp; Sindhwani, 2006 ).
 shrink the original pattern, not to predict the labels.
 3. Subspace learning via pattern shrinking
In this section, we will introduce some notations and formulate the subspace learning model by our pattern shrinking technique. After that, we show how to derive the approximated solution in a quick way. Finally, some preliminary discus-sions are provided. 3.1. Problem formulation
Let us introduce some notations. Assume the shrunk patterns of these samples are defined by { y y sional subspace by z i = P T y i . Here z i is the embedding of y Z =[ z 1 , z 2 , ... , z n ]. Therefore, Z = P T Y . In summary, we would like to list the notations in Table 1 .
We begin to show our algorithm step by step. Evoked by the intuition of semi-supervised learning framework in Eq. (5) , we try to optimize the following three objective functions. (1) Recall the requirement and basic assumption of clustering, i.e., nearby points are more likely to belong to the same (2) The shrunk pattern should keep the consistency with the original data. More concretely, the shrunk pattern and the (3) After learning the shrunk pattern, we expect to learn a subspace in which the projections of shrunk patterns should We now explain the details of each step.

The first step of our approach is to characterize the manifold structure of original data points, i.e., { x struct a k -nearest neighborhood graph, whose nodes are the points and whose edges are constructed by connecting every point to its k nearest neighbors. Denote that N X  i  X  is the index set of the points, who are k nearest neighbors of x matrix W , associated with this k -nearest neighborhood graph, is computed by the following equation. where r is the width parameter of the Gaussian function. Obviously, the weight matrix W satisfies the requirement of our Kelly &amp; Hancock, 2004 ), we choose the above strategy since it is simple and widely used ( Belkin et al., 2006 ).
Considering the definition of W , we keep the local similarity by minimizing the following equation.
For the second objective, we aim to keep the consistency between shrunk pattern and original data. Intuitively, the shrunk pattern should not be far way from the original data. Besides, we should also keep the data dissimilarity in function directly.
For the third objective, we compute the direction in which the variance of projection is as large as possible. In other words, we plan to optimize the following objective. As in most cases, we constrain that the transformation matrix is orthogonal, i.e. P balance the effects of three terms. In other words, we formulate SLPS as follows.
Here, a and b are two balance parameters and I is an identity matrix. We can see that the first two terms aim to derive the shrunk pattern and the last term projects the shrunk pattern to a low dimensional space.

Equivalently, the problem in Eq. (11) can be formulated, with respect to P and Y , as follows.
We now use the toy experimental results in Fig. 1 to explain why the pattern could shrink along the manifold and SLPS can characterize the nonlinear structure of data and considers the requirements of following clustering by learning shrunk pattern, i.e., by minimizing the first two objectives. We can derive a subspace by the joint effects of three terms. (1) The minimization of the first term in Eq. (11) tends to compress the representations of nearby samples as close as pos-(2) The second term aims to keep consistency between shrunk pattern and the original data. The third term is for project-(3) Considering the effects of the first two terms in Eq. (11) , we can compute the shrunk pattern by optimizing them (4) We employ parameters a and b to balance the effects of three terms. As mentioned above, by tuning a , we balance the (5) As seen from Fig. 1 , when b = 0, the smaller a is, the more dominating the first term becomes, resulting more patterns 3.2. Solution
In this subsection, we will solve the formulated problem. For convenience, denote L = D W , where D is a diagonal matrix et al., 2006 ). The objective function in Eq. (12) , denoted by L X  P ; Y  X  , becomes where H  X  I 1 n 11 T is the centralization matrix.
 apply the alternative optimization strategy to find the approximated solutions. More concretely, we fix one parameter and optimize the other, alternatively. 3.2.1. Fixing P and optimizing Y
We first fix P and set the derivative of L X  P ; Y  X  with respect to Y to zero, where M = PP T .

This problem cannot be directly solved by the spectral decomposition technique as in other subspace learning methods, for example, PCA, LDA and LPP. Nevertheless, it is a linear problem with respect to all the elements of Y , i.e., Y cretely, we expand X and Y by connecting each row of X and Y and define the expanded vector as follows.
By reformulating Eq. (14) in terms of ^ x and ^ y , we have the following equation. where represents the Kronecker product of two terms.

Considering the problem in Eq. (15) , the optimal solutions can be directly derived as follows. 3.2.2. Fixing Y and optimizing P
When we fix Y and optimize P , Eq. (12) becomes This is equivalent to the objective function of PCA. Thus, it can be effectively solved by spectral decomposition of YHY
In summary, we have proposed an approach to solve the optimization problem of SLPS in an alternative way. Comparing needs several times of iterations to obtain the approximated solutions. To reduce the computational complexity, we will show a property of SLPS and propose one-shot SLPS , which can derive an approximated solution without iteration. osition shows a property when b ? 0.
 Proposition 1. When b ! 0 ; b Y  X  a X  X  X  1 a  X  L  X  a I 1
Proof. when b ! 0 ; ^ y  X  a f X  X  1 a  X  L I  X  a I g 1 ^ x . Equivalently,
As seen from above derivation, we can get the same result by solving the problem in Eq. (14) directly. Although this prop-the following optimization problems in sequence.

Note that, the one-shot SLPS is first to compute the shrunk pattern and then apply PCA on the shrunk pattern to derive the final embedding Z . Although the computing and projecting of shrunk pattern are separated in one-shot SLPS , our method it is reasonable to use PCA on the shrunk patterns. More importantly, compared with solving the problems in Eqs. (15) and for several iterations. Thus, in the following experiments, we all use one-shot SLPS for comparison.
In summary, the basic procedure of SLPS is summarized in Table 2 . 3.3. Performance analysis We will analyze the proposed approach in several different aspects.

First, we would like to discuss the problem of parameter selection. There are four important parameters: the dimension-how well the constructed graph can model the low dimensional manifold. As in many related approaches, we apply the tech-parameter r , a local scaling parameter r i is calculated for each data point x k th neighbors. See more details in Zelnik-manor and Perona (2004) .

Fig. 1 . Among various kinds of methods, grid search is probably the simplest and most widely used one for unsupervised is omitted, we need not to determine this parameter.

The second concern is about the computational complexity since it is important for real applications. Considering the pro-cedure in previous subsection, we know that the shrunk pattern Y and transformation matrix P can be directly computed by solving the problem in Eqs. (18) and (19) , without iteration. The computational complexity is O ((max ( N , r )) spectral image processing. Another possible way is to conduct K-means for clustering at first and then use the centroid of clusters as the representative data to reduce the sample size.

Finally, the pattern shrinking technique can be regarded as a preprocessing technique. Other clustering methods are re-quired to compute the final results. As shown in the toy example, since SLPS can effectively shrink the pattern, it will be For brief, they are named as PsKm and PsNcut respectively.

One point should also be highlighted here is that Shi et al. have also proposed another  X  X  X hrinking X  X  based approach ( Shi subspace learning.

In addition, there are several subspace segmentation approaches which are also related to our research. Vidal et al. have an unknown number of subspaces of unknown and varying dimensions. It is mainly used to solve computer vision problems. tering a set of images of 3-D objects. Moreover, Tipping and Bishop have also formulated PCA in a probabilistic framework and computed several subspaces by EM algorithms ( Tipping &amp; Bishop, 1999 ). Comparing with our approach, these methods mainly focus on segmenting subspaces. Our research, however, learns only one subspace. They have different goals, although they all focus on subspace learning. 4. Experiments
In the following experiments, we have employed five different kinds of data sets, including the face image set: Orl, handwritten digit set: Mnist, 2 the object image set: Coil20,
The character of each data is as follows. We select 100 samples per class from the Mnist data set. Thus, there are totally hockey . Since the dimensionality is very high, we only select 200 samples per category. Some typical images from the first three data are shown in Fig. 2 .

For illustration, we compare our algorithm (We employ One-shot SLPS and take  X  X LPS X  for brief) with state of the art sub-space learning approaches: PCA, LdaKm and LPP. Since Kmeans is commonly used and Spectral Clustering also employs the similar graph to characterize data structure, we compare PsKm and PsNcut with these approaches (Kmeans and Ncut).
Although SLPS is evoked by semi-supervised learning, we have not compared with them since they focus on classification but we try to cluster data. Besides, the parameter a is determined by grid search in the following experiments. Since parameter determination is still an open problem, other parameters, such as r and k in Eq. (7) , are determined by the same concretely, r is computed by local scaling and k = 7 in all experiments.

There are mainly three groups of experiments. The first group contains experimental results on toy examples. We also vary the parameter b and compare SLPS with one-shot SLPS intuitively. In the second group, we compare our methods with other approaches quantitatively. Finally, we vary the parameter a and show its influence on the clustering performance.
The first experiment is to show the effectiveness of our SLPS approach on a toy example shown in Fig 3 . It contains 80 examples that is sampled from two distinct Gaussian distribution and can be separated linearly. We employ PCA, LdaKm,
LPP and SLPS to determine the projecting directions (the blue
As seen from this figure, it is obvious that SLPS could shrink the pattern and project the data in a separate way. Other methods fail to discover the right projection directions.
Additionally, since one-shot SLPS is obtained when beta = 0, we choose this parameter from 0, 0.01, 0.06, 0.11, ... , 0.41, and draw the corresponding projection in Fig. 4 intuitively The other setting are the same as in previous experiments. As seen from above result, we can draw following conclusion. (1) One-shot SLPS ( b = 0) could approximate the solution of large, it will tend to derive the similar projection of PCA.
 The second group of experiments is to compare our methods, i.e., PsKm and PsNcut, with other approaches quantitatively.
For brief, we name PCA + Kmeans as PcaKm, LPP + Kmeans as LppKm, PCA + Ncut as PcaNcut. We also combine original SLPS clustering results on five data sets are shown in Figs. 5 X 9 respectively.

The main observations from these figures include: (1) Since our method could fully characterize the low dimensional in Figs. 5 X 9 are better than their corresponding clustering methods and other subspace learning methods in most cases. (2)
When the data have distinct manifold structures, for example, the Coil20 data, the improvements are more significant. It means that our method could characterize the manifold structure effectively. (3) By comparing the results of PsKm and PsN-cut with Kmeans and Ncut, we know that pattern shrinking is an effective preprocessing technique. For example, on the
Newsgroup data, the clustering accuracy is much higher when we reduce the dimensionality. (4) For different dimensionality ods also have different performances. Nevertheless, dimensionality reduction is helpful in most cases.
In the third group, we aim to show the influence of parameter a . We have fixed d and employed PsKm on these sets. With experiments. The Acc and NMI results are shown in Figs. 10 and 11 .
 There are mainly three observations from Figs. 10 and 11 . (1) The performance of PsKm depends on the selection of a .
When a is suitably selected, PsKm could achieve very high accuracy. (2) With different a , the performance of PsKm is also smaller than that for Newsgroup. (3) For different data, we should select different a to achieve the highest accuracy. Take
Coil20 as an example, we should choose the small a . For some data, such as Newsgroup, the larger a is better. 5. Conclusion
In this paper, we proposed a new subspace learning model for clustering. It is mainly based on our proposed pattern
Compared with the state-of-the-art subspace clustering methods, it performs better. Moreover, it can also be regarded as an effective preprocessing technique. In other words, we can improve the performance of other clustering algorithms by simply adding our proposed pattern shrinking technique. Future researches mainly include the accelerating issue of our approach. Acknowledgements This work is supported by the 973 Program of China (No. 2013CB329503), NSFC China (No. 61005003, 60975038) and Zhejiang Provincial Natural Science Foundation of China (No. Y1110661).
 References
