 flaws, implementation bias and data bias (defined in detail below). Because of these flaws, much of the work has very little generalizability to real world problems. 
In particular, we claim that many of the contributions made (speed in the case of indexing, accuracy in the case of classification and clustering, model accuracy in the case of segmentation) offer an amount of "improvement" that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets, or the variance that would have been observed by changing minor (unstated) implementation details. 
In order to support our claim we have conducted the most exhaustive set of time series experiments ever attempted, re-implementing the.' contribution of more than 25 papers and testing them on 50 real word datasets. Our results strongly support our contention. of the data mining community. We note that several papers by the current first author are among the worst offenders in terms of weak experimental evaluation. While preparing the survey we read more than 340 data mining papers and we were struck by the originality and diversity of approaches that researchers have used to attack very difficult problems. Our goal is simply to demonstrate that empirical evaluations in the past have often been inadequate, and we hope this work will encourage more extensive experimental evaluations in the future. 
For concreteness we begin by defining the various tasks that occupy the attention of most time series data mining research.  X  Indexing (Query by Content): Given a query time series Q, and some similarity/dissimilarity measure D(Q,C), find the nearest matching time series in database DB.  X  Clustering: Find natural groupings of the time series in database DB under some similarity/dissimilarity measure 
Classification: Given an unlabeled time series Q, assign it to one of two or more predefined classes.  X  Segmentation: Given a time series Q containing n datapoints, construct a model ~, from K piecewise segments (K &lt;&lt; n) such that Q closely approximates Q. 102 In presenting the results of the survey, we echo the caution of Prechelt, that "while high numbers resulting from such counting cannot prove that the evaluation has high quality, low numbers (suggesO that the quality is low" [47]. The results are quite surprising; the median size of the test database was only 10,000 objects. Approximately 89% of the test databases could comfortably fit on a 1.44 Mb floppy disk. Another surprising finding of the survey is the relative paucity of rival methods to which the contribution of the paper is compared. The median number is 1 (The average is 0.91), but this number includes very unrealistic strawman. For example many papers (including one by the current first author [31]) compare times for an indexing method to sequential scan where both are preformed in main memory. However, it is well understood sequential scan enjoys a tenfold speed up when performed on disk because any indexing technique must perform costly random access, whereas sequential scan can take advantage of an optimized linear traverse of the disk [32]. The limited number of rival methods is particularly troubling for papers that introduce a novel similarity measure. Although 29 of the papers surveyed introduce a novel similarity measure, only 12 of them compare the new measure to any strawman. The average number of rival similarity measures considered is only 0.97. Although the small sizes of the test databases and the relatively scarcity of comparisons with rival methods is by itself troublesome, the most interesting finding concerns the number of datasets used in the experimental evaluation. On average, each contribution is tested on 1.85 datasets (1.26 real and 0.59 synthetic). This numbers are astonishingly low when you consider that new machine learning algorithms are typically evaluated on at least a dozen datasets [12, 33]. In fact, we feel that the numbers above are optimistic. Of the 30 papers that use two or more datasets, a very significant fraction (64%), use both stock market data and random walk data. However, we strongly believe these really should be counted as the same dataset. It is well known that random walk data can perfectly model stock market data is terms of all statistical properties, including variance, autocorrelation, stationarity etc [17, 53]. Work by the late Julian L. Simon suggested that humans find it impossible to differentiate between the two [53]. To confirm this finding we asked 12 professors at UCRs Anderson Graduate School of Management to look at Figure 1 and determine which three sequences are random walk, and which three are real S&amp;P500 stocks. The confusion matrix is show in Table 1. choice of representation, it is important to compare techniques in a manner that is free of implementation bias. 
Definition: Implementation bias is the conscious or unconscious disparity in the quality of implementation of a proposed approach, vs. the quality of implementation of the completing approaches. Implementing fairly complex indexing techniques allows many opportunities for implementation bias. For example, suppose you hope to demonstrate that DWT is superior to DFT. With shift-normalized data [11, 28] the first DWT coefficient is zero so you could take advantage of that fact by indexing the 2 na to N+I th coefficients, rather than the I st to Nthcoefficients. However, you might neglect doing a similar optimization for DFT, whose first real coefficient is also zero for normalized data. Another possibility is that you might use the simple O(n 2) DFT algorithm rather than spend the time to code the more complex O(nLogn) radix 2 algorithm [32]. In both these cases DFT's performance would be artificially deflated relative to DWT. One possible solution to the problem of implementation bias is extremely conscientious implementations of all approaches, combined with diligent explanations of the experimental process. Another possibility, which we explain below, is to design experiments that are free from the possibility of implementation bias. Since all the exact indexing techniques use the same basic framework, the efficiency of indexing depends only on how well the dimensionality-reduced approximation can model the distances between the original objects. We can measure this by calculating the tightness of the lower bounds for any given representation. 
Definition: The tightness of the lower bound (denoted T ) for any given representation is the ratio of the estimated distance between two sequences under that representation, over the true distance between the same two sequences. Note that T is in the range [0,1 ]. A value of 1 would allow a constant time search algorithm, and a value of 0 would force the indexing structure to degrade to sequence scan. In fact, because whereas any indexing scheme must make wasteful random disk accesses, it is well understood that T must be significantly greater 32]. Since one can always create artificial data for any estimated for a particular dataset by random sampling. Note that the value of T for any given dimensionality reduction technique depends only on the data and is independent of any implementation choices such as page size, buffer size, computer language, hardware platform, seek time etc. A handful of papers in the survey already make use of a similar measure to compare the quality of representations [ 10, 32]. This idea of an implementation free evaluation of performance is by no means new. In artificial intelligence, researchers often compare search algorithms by reporting the number of nodes expanded, rather than the CPU times [33]. The problem of implementation bias is also well understood in other computer science domains, including parallel processing [5]. 104 Figure 2. Experiments on the Powerplant, Infrasound and Attas datasets "demonstrate" that DFT outperforms DWT-Haar for indexing time series In contrast if we worked with the Network, ERPdata and Fetal 
EEG datasets we could conclude that there is no real difference between DFT and Haar, as suggested by Figure 3. Figure 3. Experiments on the Network, EPRdata and Fetal EEG datasets "demonstrate" that DFT and DWT-Haar have the same performance for indexing time series 
Finally had we had chosen the Chaotic, Earthquake and Wind datasets we could use the graphs in Figure 4 to demonstrate "convincingly" that Haar is superior to DFT. Figure 4. Experiments on the Chaotic, Earthquake and 
Wind datasets "demonstrate" that DWT-Haar outperforms DFT for indexing time series 
Although we used the value of T to demonstrate the problem, we also confirmed the findings on an implemented system, using an 
Figure 5. The affect of minor implementation details on the performance of sequential scan, for increasing large databases more difficult to know the scale of the problem for implementation bias. By its very nature, it is almost impossible to know what fraction of a claimed improvement should be credited to the proposed approach, and what fraction may be due to implementation bias. However, there are a handful of examples where this is clear. For example, one paper included in the survey finds a significant performance gap between the indexing abilities of Haar wavelets and Piecewise Aggregate Approximation (PAA) [45]. However it was proved by two independent groups of researchers that these two approaches have exactly the same tightness of lower bounds when the number of dimensions is a power of two (and very little difference when the number of dimensions is not a power of two) [32, 61]. We empirically confirmed this fact 4,000,000 times during the,experiments in 
Section 3.3.1. While there may be small differences in the CPU time to deal with the two representations, the order in which the original sequence.,; are retrieved from disk by the index structure should be the same for both approaches, and disk time completely dominates CPU for time series indexing under Euclidean distance. 
We strongly suspect the spurious result reported above was the result of implementation bias, so we conducted an experiment to demonstrate how a simple implementation detail could produce an effect which is larger than the approximately 11% difference claimed. 
We began our experiment by performing a fair comparison of the tightness of lower bounds for Haar and PAA on each of our 50 datasets, with a query length of 256 and 8 dimensions. Rather than estimate Twith 100,000 random samples as in Section 3.1.1, we averaged over 100 samples as in the paper in question. 
We repeated the experiment once more; this time neglecting to take advantage of the fact the first Haar coefficient is zero for normalized data. In other words, we wastefully index a value that is a constant zero. Once again we estimated T by averaging over 100 samples for each dataset. For each dataset we calculated the ratio of the correct implementation's value of T to the poor implementation's value of T. The 50 results are plotted as a histogram in Figure 6. 106 several time series from the domain of interest [30]. Euclidean Dynamic Time Warping 
Autocorrelation 
Figure 7. Dendograms can be used to visually assess the usefulness of a similarity measure. Above a dataset of 8 objects is clustered using the single linkage method, with 4 different distance measures. Euclidean distance and 
Dynamic Time Warping are decade old strawmen. The other two approaches have recently been proposed in data mining papers [57, 29] 
A large fraction of the papers in the survey either introduce a segmentation alg, orithm as their main contribution, or utilize a segmentation algorithm as a subroutine. Although the segments created could be polynomials of an arbitrary degree, the most common representation of the segments are linear functions. 
Intuitively a Piecewise Linear Representation (PLR) refers to the approximation of a time series Q, of length n, with K straight lines. Figure 8 contains an example. Figure 8. An example of a time series with its piecewise linear representation 
Because K is typically much smaller that n, this representation makes the storage, transmission and computation of the data more efficient. Specifically, in the context of data mining, pieeewise linear representation has been used to:  X  Support novel distance measures for time series, including "fuzzy queries" [52], weighted queries [30], multiresolution queries [39, 48], dynamic time warping [42, 46], autocorrelation queries [57] and relevance feedback [30].  X  Support concurrent mining of text and time series [37].  X  Support novel clustering and classification algorithms [30].  X  Support change point detection [20, 23]. 
Surprisingly, in spite of the ubiquity of this representation, with the exception of [52], there has been little attempt to understand and compare the algorithms that produce it. 
Although appearing under different names and with slightly different implementation details, most time series segmentation algorithms can be grouped into one of the following three categories.  X  Sliding-Wiindows (SW): A segment is grown until it exceeds some error bound. The process repeats with the next data point not included in the newly approximated segment.  X  Top-Down (TD): The time series is recursively partitioned until some stopping criteria is met.  X  Bottom-Up (BU): Starting from the finest possible approximation, segments are merged until some stopping criteria is met. 
We can measure the quality of a segmentation algorithm in several ways, the most obvious of which is to measure the reconstruction error for a fixed number of segments. The reconstruction error is simply the Euclidian distance between the original data and the segmented representation.  X  Algorithms should be tested on a wide range of datasets, unless the utility of the approach is only been claimed for a particular type of data. If possible, one subset of the datasets should be used to fine tune the approach, then a different subset of the datasets should be used to do that the actual testing. This methodology is widely used in the machine learning community to help prevent implementation and data bias [12].  X  Where possible, experiments should be designed to be free of the possibility of implementation bias. Note that this does not preclude the addition of extensive implementation testing.  X  Novel similarity measures should be compared to simple strawmen, such as Euclidian distance or Dynamic Time 
Warping. Some subjective visualization, or objective experiments should justify their introduction.  X  Where possible, all data and code used in the experiments should be made freely available to allow independent duplication of findings [6]. The authors would like to thank Michael Pazzani, Pedro 
Domingos, Dimitrios Gunopulos and the anonymous reviewers for their valuable suggestions and comments. We also thank the many donors of test data. [1] Agrawal, R., Faloutsos, C. &amp; Swami, A. (1993). Efficient similarity search in sequence databases. In proceedings of the 4/h Int'l Conference on Foundations of Data Organization and 
Algorithms. Chicago, IL, Oct 13-15. pp 69-84. similarity search in the presence of noise, scaling, and translation in time-series databases. In proceedings of the 21 st Int7 Conference on Very Large Databases. Zurich, Switzerland, 
Sept. pp 490-50. [3] Agrawal, R., Psaila, G., Wimmers, E. L. &amp; Zait, M. (1995). Querying shapes of histories. In proceedings of the 21 st lnt7 
Conference on Very Large Databases. Zurich, Switzerland, Sept 11-15. pp 502-514. 
Andr6-J6nsson, H. &amp; Badal. D. (1997). Using signature files for querying time-series data. In proceedings of Principles of Data Mining and Knowledge Discovery, I st European Symposium. Trondheim, Norway, Jun 24-27. pp 211-220. 
Bailey, D. (1991). Twelve ways to fool the masses when giving performance results on parallel computers. Supercomputing Review, Aug. 1991, pp. 54-55. 
Bay, S. (1999). UCI Repository of Kdd databases [http://kdd.ics.uci.edu/]. Irvine, CA: University of California, Department of Information and Computer Science 
Bemdt, D. J. &amp; Clifford, J. (1996). Finding patterns in time series: a dynamic programming approach. Advances in Knowledge Discovery and Data Mining. AAAI/MIT Press, 
Menlo Park, CA. pp 229-248. Goldin, D. &amp; Kanellakis, P. (1995) On similarity queries for time-series data: constraint specification and implementation. In proceedings of the 1 st Int'l Conference on the Principles and Practice of Constraint Programming. Cassis, France, Sept 19-22. pp 137-153. series data. In proceedings of the 5t~ A CM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining. San Diego, CA, Aug 15-18. pp 33-42. time-series data. In proceedings of the 5 lnt'l Conference on Knowledge Discovery and Data Mining. San Diego, CA, Aug 15-18. pp 282-286. similarities in aligned time series using wavelets. Data Mining and Knowledge Discovery: Theory, Tools, and Technology, 
SPIE Proceedings Series, Vol. 3695. Orlando, FL, Apr. pp 150-160. 
Indyk, P., Koudas, N. &amp; Muthukrishnan, S. (2000). Identifying representative trends in massive time series data sets using sketches. In proceedings of the 26 ~h lnt'l Conference on Very Large Data Bases. Cairo, Egypt, Sept 10-14. pp 363-372. series data. In proceedings of the 17 'h Int'l Conference on Data 
Engineering. Heidelberg, Germany, Apr 2-6. pp 273-282. structure for shift and scale invadant search of multi-attribute time sequences. In proceedings of the 18 'h lnt'l Conference on 
Data Engineering. San Jose, CA, Feb 26-Mar 1. to appear. 
Kalpakis, K., Gada, D. &amp; Puttagunta, V. (2001). Distance measures for effective clustering of ARIMA time-series. In proceedings of the 1EEE Int'l Conference on Data Mining. San 
Jose, CA, Nov 29-Dec 2. pp 273-280. time series which allows fast and accurate classification, Conference on Knowledge Discovery and Data Mining. New 
York, NY, Aug 27-31. pp 239-241. pattern matching in time series databases. In proceedings of the 3 rd lnt'l Conference on Knowledge Discovery and Data Mining. 
Newport Beach, CA, Aug 14-17. pp 24-20. Keogh, E., Chakrabarti, K., Pazzani, M. &amp; Mehrotra, S. (2001). 
Locally adaptive dimensionality reduction for indexing large time series databases. In proceedings of A CM SIGMOD 
Conference on Management of Data. Santa Barbara, CA, May 21-24. pp 15;1-162. 
Kibler, D., &amp; Langley, P. (1988). Machine learning as an experimental science. In Proceedings of the 3 "a European 
Working Session on Learning. pp. 81-92 intelligent matching for time series data. In proceedings of Data Warehousing and Knowledge Discovery, 2 ~a lnt'l Conference. 
London, UK, Sep 4-6. pp 347-357. supporting ad hoc queries in large datasets of time sequences. In 
Rafiei, D. &amp; Mendelzon, A. O. (1998). Efficient retrieval of similar time sequences using DFT. In proceedings of the 5 th Int7 Conference on Foundations of Data Organization and 
Algorithms. Kobe, Japan, Nov 12-13. data. In proceedings of the 15 sh IEEE Int'l Conference on Data 
Engineering. Sydney, Australia, Mar 23-26. pp 410-417. 
Shahabi, C., Tian, X. &amp; Zhao, W. (2000). TSA-tree: a wavelet based approach to improve the efficiency of multi-level surprise and trend queries. In proceedings of the 12 th lnt'l Conference on Scientific and Statistical Database Management. Berlin, 
Germany, Ju126-28. pp 55-68. representations for large data sequences. In proceedings of the 12 th IEEE Int'l Conference on Data Engineering. New Orleans, 
LA, Feb 26-Mar 1. pp 536-545. the theory of simulation and the use of resampling. The 
American Statistician, Vol. 48(4). Nov. pp 1-4. the time series similarity paradigm. In proceedings of Principles of Data Mining and Knowledge Discovery, ga European 
Conference. Prague, Czech Republic, Sept 15-18. pp 12-22. by radioactive decay, www.fourrnilab.ch/hotbits/ dimensional nearest neighbor search. In proceedings of ACM SIGMOD Workshop on Research lssues in Data Mining and 
Knowledge Discovery. Dallas, TX, May 14. pp 37-43. [57] Wang, C. &amp; Wang, X. S. (2000). Supporting content-based searches on time series via approximation. In proceedings of the 12 rh lnt'l Conference on Scientific and Statistical Database 
Management. Berlin, Germany, Ju126-28. pp 69-81. [58] Wang, C. &amp; Wang, X. S. (2000). Supporting subsefies nearest neighbor search via approximation. In proceedings of the 9 ~h A CM CIKMInt'I Conference on Information and Knowledge 
Management. McLean, VA, Nov 6-11. lap 314-321. 
FALCON: feedback adaptive loop for content-based retrieval. In proceedings of the 26 ~ lnt'l Conference on Very Large Data 
Bases. Cairo, Egypt, Sept 10-14. pp 297-306. DFT and DWT based similarity search in time-series databases. In proceedings of the 9 th ACM CIKM Int'I Conference on 
Information and Knowledge Management. McLean, VA, Nov 6-11. pp 488-495. 
Yi, B. &amp; Faloutsos, C. (2000). Fast time sequence indexing for arbitrary lp norms. In proceedings of the 26 rh Int'l Conference on 
Very Large Databases. Cairo, Egypt, Sept 10-14. pp 385-394. 
Yi, B., Jagadish, H. &amp; Faloutsos, C. (1998). Efficient retrieval of similar time sequences under time warping. In proceedings of the 14 th Int'l Conference on Data Engineering. Orlando, FL, Feb 23-27. pp 201-20. 
