 The parallel corpora ar e resources of great importance for many natural language processing tasks , especially for statistical machine translation (SMT) , while parallel corpora with high quality is expensive. In order to alleviate the lack of parallel data, many researchers have turned to mine the large amount of available comparable data on Internet . I n mining parallel text from comparable data, a great challenge is that the search space is quite vast, which makes it difficult to obtain good parallel resource and the process is too slow to be applied in the practical application.

To reduce the search space, much work [ 2, 8 , 9 , 11 , 1 3 , 1 4 ] utili ze s some heuristic information in which they step document alignment first and then only inspect the sentences in the aligned document pairs. However, in many situations, the simple heuristic information such as URL and title are not available which makes document alignment hard to actualize.

In this paper our purpose is to reduce the search space without document -level alignment. We believe it will be much helpful to narrow the search space if we can pre -select part of comparable sentence pairs that most possibly contain parallel text because the size of comparable data is huge. Inspired by this motivation, we resort to the cross -language information retrieval (CLIR) framework to pre -filter the cand i-dates, which indexes the target corpus directly at sentence -level and adopts a search engine to find the sentences in target corpus that are the most likely translations given a source sentence. The best CLIR -returned sentences are kept as candidates.

Some research work [10 , 1 5 ] has been done on how to use CLIR method for selec t-ing candidate sentences. Among these work,  X tef X nescu et al. [ 15 ] uses the dictio n ary -based CLIR fram e work to select candidates. However, a word usually has several translations, for e x ample, the word  X  reaction  X  has 6 meanings as shown in Table 1, and some transl a tions are not right in some specific contexts. W e believe i t will add extraneous term s to the query a nd thus will degrade the quality of candidate sentence collection if we just simply use all the dictionary translations. This will definitely affect the parallel text extraction procedure afterwards.

Based on the analysis above, we know that it is inappropriate to use CLIR without an y modification. Therefore, this paper presents two query expansion methods for the translation of queries from source language into target language. One is word -level b ilingual dictionary is utilized to translate the content words in the source sentence. Unlike the work of  X tef X nescu et al. [ 15 ] that uses all the dictionary translations, we pr o pose a word disambiguation algorithm using beam -search that only uses the mo n o-li n gual target corpus to select the better word sequence to form the query. In phrase -level translation, a simplified translation model which only uses phrase and lexical translation probability is proposed for the translation. Experiments show that our pr o-posed query expansion methods can not only help to reduce the space efficiently, but also can achieve a better collection of candidate sentence pairs.

The remainder of this paper is organized as follows: Section 2 introduces the rela t-ed work. Section 3 gives the CLIR framework for candidate sentence generation and describes our two query expansion methods in detail. Section 4 presents the exper i-ments. F inally, we conclude the paper in Section 5. Mining parallel text from comparable data h as attracted many researchers and much research work has been done on this task. However, it present s many difficulties and one of the greatest obs tacles is the vast search space. M uch work has been do ne to prune the search space and all these method s can be classified into two categories : ( 1 ) document level pre -filtering, and (2) sentence level pre -filtering.

T he general way for document level pre -filtering is to perform document alignment first and then to inspect the sentences in the aligned docume nt pairs only. T his road has been taken by many researchers. [2, 3] adopt a bilingual dictionary to compute document similarity for document alignment. [ 8 ] uses dictionary -based cross -lingu al information retrieval method to get more pre cise article pairs. [1 3 ] implements a hash -based algorithm to directly compute the cross -lingual pairwise similarity to find article pa i rs. A ll these methods need to calculate pairwise similarities across the huge bilingual corpora and it  X  s quite computationally intensive. To reduce the comput a-tional complexity, most studies fall back to heuristic s. [ 8 ] just compares news articles published close in time. [1 1 ] exploits  X  inter -wiki  X  links in Wikipedia to align doc u-ments. After document alignment, we can mine parallel resourc es from the aligned documents.

Since high -quality document -level alignment is difficult to acquire in many situ a-tions, some work has tried to pre -select candidate sentence pairs at sentence -level. [1 2 ] adopts a beam -search algorithm to extract parallel sen tences directly at sentence -level without document alignment. [ 5, 10 ] employ a SMT system to translate the source part of comparable corpus and then use the translations as queries to conduct info r-mation retrieval to find candidate sentences. However, there are not enough resources between many language pairs to build a SMT system. [1 5 ] uses the dictionary -based CLIR framework to generate candidates. B u t as mentioned before , the serious amb i-guity problem existed in a dictionary will affect the performance seriously. The pipeline of using CLIR framework to generate candidate sentence pairs is shown in Fig. 1. From the figure we can see, all of our processing is directly at the sentence -level. The procedure can be divided into three steps: (1) Building index for the target corpus; (2) Translating the source sentence into target language to form queries; (3) Search for candidate sentence pairs. 3.1 Indexing Target Sentences To implement our framework , w e need to build index for the target corpus first. S pli t-ting the target corpus into sentences and performing a series of basic operations that are similar with the process of Chinese word segmentation or English tokenization and stemming. W e use the Java implementation of Lucene 1 to index the target se n-tences as Lucene documents.

W e also compute the length of each sentence. We believe the length information lengths of two sentences that are translations of each other must be within a certain range. Therefore, for each Lucene document, we introduce the following two searc h-able fields: (a) A field storing the target sentence; (b) A field storin g the length of the sentence.

Then we build full -text indexing on these two fields for target sentences. T he index structure is extensible and we can add other useful field information easily. 3.2 Generating Query from Source Sentence After finishing indexing target sentences, our next step is considering how to translate source sentences into target language to generate queries. In our model, f or word -level transformatio n, we adopt a machine -readable bilingual dictionary to translate source sentence into target word by word. A nd to solve the ambiguity problem me n-tioned before , we present a beam -search algorithm using nothing more than the mon o-lingual target corpus to sele ct the best translation sequence . For phrase -level tran s-formation, we use a small collection of bilingual corpus to train a simplified transl a-tion model for the translation. 3.2.1 Word -level Transformation To map the information from source sentences into target , the most simple and co n-venient way is to utilize the bilingual machine readable dictionaries. However, the dictionary translations are usually ambiguous and thus will affect the retrieval results. Much efforts have been done to solve the problem of disambiguation [4, 6 , 7 ] and most of the existing approaches exploit the word co -occurrence patterns and then use a greedy algorithm to select an optimal translation set.

Instead, we deal with this problem from a dif ferent aspect. First, we see the co -occurrence of possible translation terms as a graph and the Mutual Information (MI) values are the weight between two words. Fig . 2 gives an example of such a graph. The square node s under , and represent the translations of the three words respectively and the li nk s indicate the MI values are available for the pairs.
MI can be used to evaluate the significance of word co -occurrence associations and is defined as the following formula (1) [1] :
Here,  X   X  , p x y is the co -occurrence probability of x and y within a window size 2 . px ,  X   X  py and  X   X  , p x y are the maximu m likelihood estim ates of the corresponding probabilities.

Given a source sentence S with a set of n content words  X   X  12 , , , n e e e and a set need to select the best translation for each content word to form the final translation set. However, to find such an optimal set will cost large amount of computation. I n-stead, we treat the problem as the optimal path selection and use a beam search alg o-rithm to search for the B -best path.
 Here, we define the path as a set of words  X   X  12 , , , n t t t , where  X   X  ii t T e  X  .
For example, in Fig . 2,
Our optimal algorithm to select path relies on the MI values. The MI values are based on the assumption that the words co -oc cur in the same query, and their proper corresponding translations are likely to co -occur in the same documents. We think the MI values can reflect the semantic association between words in some degree. On the basis of this idea, our evaluation function of  X   X  12 Path , , , n t t t is shown below (2): The best Path * Path is defined below (3):
W e use the beam -search algorithm for the best word translation sequence with the highest path score. T he algorithm is given in Fig. 3, where T is a set of translations as mentioned before, and the variable candidate represents one o f the current path kept in the candidates . T  X  is the translation candidates of one word and c  X  X  is one translation in T  X  . The agenda is a sequential list, used to keep all the paths generated at each stage, ordered by the score which is calculated using fo rmula (2). The variable candidates is the set of paths that can be used to generate new path, that is the B -best path from previous stage. A nd B is the number of paths retrained at each stage. Its value is an important factor to the performance of the algorithm and we set B to 128 in our application.

CLEAR empties the agenda and removes all the items from agenda. ADD refers to an operation that add a new translation node to expand the path and TOP -B returns the highest B scoring paths from the agenda. 3.2.2 Phrase -level Transformation In word -level transformation, the basic unit of translation is a single word. Though our proposed beam -search method to select word sequence can help to discard some error translations, the improvement of performance is limited since we only use the co -occurrence information in the target corpus and the context of the word in the source sentence is not available. However, the context of the word to be translated is helpful for the selection of translation. Thus, we can enlarge the translation unit from word -level to phrase -level to use the context information.

At present, phrase -based SMT model usually consists of three factors: the phrase translation table, the reordering model, and the language model. But in fact, even if the sentence is not smooth or the order of the words is not proper, we can retrieve a satisfied result set if the words given to the search engine are correct. In order to r e-duce the computational complexity, we propose to use a simplified phrase translation model that only uses the phrase translation table to translate source sentence into ta r-get language. Here, four features are used to choose the translation: phrase translation probability  X   X  | fe  X  ,  X   X  | ef  X  and lexical weighting  X   X  lex | fe ,  X   X  lex | ef . A small bilingual parallel corpus is used to train the translation model. 3.3 Searching for Candidate Sentences After translating the source sentence into target language, we can obtain a set of target words. A boolean model is used to retrieve the documents and each of the target words is added as a disjunctive query term (the OR operator). In order to narrow the makes the se arch engine only search the subset of sentences in which the ratio of se n-tence and the source sentence is within a certain range. In our experiments, the range engine t o get the best h hits. 4.1 Experiments setup W e want to measure the performance of using our query translation methods for the search engine to find translation candidates. W e conduct our experiments on manually created English -Chinese data set. The target corpus, denoted as FBIS&amp;NIST, consists of the Chinese side of FBIS corpus, NIST MT 2003 and NIST MT 2005. W e use Lucene to index FBIS&amp;NIST. T he English side of NIST MT 2003 (denoted as EN -03) and NIST MT 2005 (denoted a s EN -05) are used as the source corpus. Table 2 shows the relevant statistics of the target corpus and source corpus.

E ach sentence in the source corpus is used as query and the best k hits (target se n-tences) returned by the search engine are kept as candidates. Ideally, we would like to see the real translations in the candidates so that we can have a chance to extract th em lows: Let G denote the total sentence number of the source corpus. Each sentence will obtain a set of target sentences as translation candidates and then we will have G such retrieval sets. Let R denote the number of retrieval sets that contain the real transl a-tion s. Then The higher recall means more retrieval sets contain the real parallel text which is very important to the following steps. 4.2 Evaluation W e first design experiments to evaluate the efficiency of our proposed two query translation methods. I n word -level translation, we use a common English -Chinese dictionary containing 41,814 entries. I n phrase -level translation, we adopt the Moses toolkit and FBIS corpus with 235,670 sentence pairs to train the SMT system.

Results are shown in Table 3. Here, in baseline system [1 5 ] , we use the bilingual dictionary to translate the source sentence and using all the translations as query ter ms to search for target sentences. Method_1 refers to use the beam -search word sequence selection method and Method_2 refers to use the simplified translation model d e-scribed in sub -section 3 .2.1 and sub -section 3 .2.2 respectively.
 As Table 3 shows, the recall increases with the k increasing. Both on EN -03 and EN -05, our proposed two query translation methods improve the performance signif i-cantly compared to the baseline , which proves the effectiveness of our methods . O n one hand, our methods can help to achieve a much better quality of candidates which is important to the following parallel text mining steps. O n the other hand, even if we only keep the best 10 hits but the baseline keeps the best 50 hits for each sentence, we can obtain the comparable recall. This means the search space can be reduced about 10 times.

W e can see that the performance of using the simplified translation model is better than that of using the beam -search word sequence selection method. T his is mainly because the beam -search method to select word sequence only uses the co -occurrence information in the target corpus while the translat ion model uses the context of the word to be translated.

As we know t he performance of the translation model is domain -sensitive. In order to make a more comprehensive comparison of every method, we also conduct a cross -domain experiment . W e have 1,500 sentence pairs from computer science domain and add the Chinese side into the former built index. T he English side, denoted as COM, is used as source corpus to retrieve candidates. The results are shown in Table 4.
From Table 4 we can see that our beam -search method improves the performance significantly while the performance of using the translation model decreases a lot. This is because the bilingual dictionary we used is a general dictionary. We believe that the performance can be further promoted if we enlarge the dictionary. However, the translation model is trained on a news domain data and it performs poorly on the other domain which declines the retrieval performance. Comparing the performance of using diffe rent query translation method shown in Table 3 and Table 4, we can conclude that both our proposed beam -search method to select word sequence and the simplified translation model are very helpful to tran s-form source sentence into target language to obtain a better quality of candidate se n-tence pairs. Better yet, these two methods can be applied to translate queries in diffe r-ent situations according to the resources at hand. I f we have in -domain parallel corpus, a translation model can be trained to translat e the source sentence into target language. And if we only have bilingual dictionaries, the beam -search method also can help to select a set of good translations to form a better query. 4.3 Further Experiments To further verify the effectiveness of our method in the real comparable data, we fu r-ther conduct the experiment on the English -Chinese Wikipedia data. W e process the English and Chinese Wikipedia data separate ly and index the Chinese side using the method described in Section 3 .1. F or each English sentence, the beam -search method is used to translate it into target and here we only keep the best -1 retrieval result. A nd then we use the coverage matching score with bilingual dictionary to simply extract the sentence pairs higher than a threshold within the retrieval result set. T he statistics of sentence pairs over a certain threshold is shown as Table 5 . Here H i means diffe r-ent experiments under different thresholds.
 We use the retained sentence pairs as training data to train the SMT system. GIZA++ and grow -dia g -and for word alignment, the Moses toolkit with default se t-tings are used to train the System. NIST MT 2003 and NIST MT 2005 are adopted as development set and test set respectively. The SMT evaluation results using different tr aining data are given in Table 6 .

As Table 6 shows, the BLEU point of using training data H 1 is obviously below than the points of the other three. This is mainly because the threshold of H 1 is 0.3 and it will introduce much noise into the training data. However, the SMT evaluation results can prove the effectiveness of our method from another as pect. Using the CLIR framework, we can build a SMT system from the large comparable corpora. In this paper, we propose two simple and effective query translation methods for the CLIR based candidate sentence pre -selection framework: one is the beam -search word sequence selection method and the other one is the phrase -based simplified translation model. In beam -search word sequence selection method, only bilingual dictionary and monolingual target corpus is needed and a beam -search algorithm is adopted to select a set of word translations. In phrase -based translation model, a si m-plified translation model is trained to translate the source sentence into target la n-guage. Experimental results show that our method can help obtain candidate sentence pairs of high quality which is quite important to the following parallel sentence and fragments X  extraction. What  X  s more, o ur methods can contribute current SMT for two folds: (1) It can help build a SMT system from nothing but a small size of bilingual dictionary; (2) It can help enhance the current SMT performance with additional mined translation resources.

In the future work, we will try to study on the method of how to mining parallel text including both sentences and fragments from the obtained candidate sentence pairs.
 Acknowledgement. The research work has been partially funded by the Natural Sc i-ence Foundation of China under Grant No. 61333018 and the Hi -Tech Research and Development Program ( X 863 X  Program) of China under Grant N o. 2012AA011101, and also the High New Technology Research and Development Program of Xinjiang Uyghur Autonomous Region under Grant No. 201312103 as well.

