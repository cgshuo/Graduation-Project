 Cross-language information retrieval (CLIR) today is dominated by techniques that use token-to-token mappings from bilingual dic-tionaries. Yet, state-of-the-art statistical translation models (e.g., using Synchronous Context-Free Grammars) are far richer, cap-turing multi-term phrases, term dependencies, and contextual con-straints on translation choice. We present a novel CLIR framework that is able to reach inside the translation  X  X lack box X  and exploit these sources of evidence. Experiments on the TREC-5/6 English-Chinese test collection show this approach to be promising. Categories and Subject Descriptors : H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Keywords : machine translation, context
Query translation approaches for cross-language information re-trieval (CLIR) can be pursued either by applying a machine transla-tion (MT) system or by using a token-to-token bilingual mapping. These approaches have complementary strengths: MT makes good use of context but at the cost of producing only one-best results, while token-to-token mappings can produce n -best token transla-tions but without leveraging available contextual clues. This has led to a small cottage industry of what we might refer to as  X  X ontext recovery X  in which postprocessing techniques are used to select or reweight translation alternatives, usually based on evidence from term co-occurrence in a comparable collection.

We argue that this false choice results from thinking of MT sys-tems as black boxes [5]. Inside an MT system we find not alter-native translations for individual tokens, but rather for entire sen-tences. In state-of-the-art MT systems, these alternative  X  X eadings X  are based on Synchronous Context-Free Grammars (SCFG) and pieced together from units of varying size with complex hierar-chical dependencies. Reducing these to context independent token translation probabilities discards potentially useful contextual con-straints. An elegant solution, which we explore in this work, is to perform translation in context using a full SCFG MT system and then to reconstruct context-sensitive n -best token translation prob-abilities by tokenizing each reading and accumulating translation likelihood evidence, which can then be renormalized as estimates of probabilities. This technique is now routinely used in speech retrieval [7], but we are not aware of its prior use for CLIR.
These context-sensitive token translation probabilities can then be used in the same way as context-independent probabilities. We to consider the n candidates with highest likelihood, for some n&gt; 1 . In this case, the score of document D would be a weighted average of scores with respect to each candidate translation: where Pr cdec is the normalized likelihood value.

In order to compute tf and df statistics for tokens, we start by of source query s , we use word alignments in the grammar rules to determine which target tokens it is associated with. By doing this, we are constructing a probability distribution of possible transla-tions of s j based on the n query translations. Specifically, if source token s j is aligned to (i.e., translated as) t i in the k th best transla-tion, it receives a weight equal to Pr cdec ( t ( k ) | s ) . 1 As a result, we can map tf and df statistics by replacing Pr bitext with Pr nbest (see below,  X  is the normalization factor) in Equations 2 and 3.
This new probability distribution (i.e., Pr nbest ) is based only on the n translations that the decoder scores highest for the source query. Therefore, the distribution is informed by the query context and its derivation by the translation model. From this we would expect the distribution to be better biased in favor of appropriate translations, but perhaps at the cost of some reduction in variety due to overfitting. Finally, we can combine the two probability estimates to mitigate overfitting using simple linear interpolation: Pr
We evaluated our system on the TREC-5/6 CLIR task, using a corpus of 164,778 Chinese documen ts and titles of the 54 English topics as queries. The evaluation metric is Mean Average Preci-sion (MAP). The English-to-Chinese translation model was trained using the FBIS parallel text collection, which contains 1.6 million parallel sentences. The Chinese collection was tokenized using the Stanford segmenter for Chinese, the Porter stemmer was used for English, and alignment was performed using GIZA++ [6]. A SCFG was extracted from these alignments using a suffix array [4]. A Chinese token 3-gram model serves as the LM.

Results are summarized in Figure 1. At the left edge of the graph, at  X  =0 , we have the approach in equation (2) with context-independent translation probabilities (call this A). 2 At the right edge of the graph, at  X  =1 . 0 , we rely exclusively on context-sensitive probabilities (call this B). Effectiveness peaks at  X  =0 . 78 (call this C). 3 For reference, the horizontal line represents simply taking the one-best translation from the MT system (call this D). We also tried one-best context-independent translation for each token, and the MAP score was 0.2431. A randomization test shows that this is significantly below A, B, C and D ( p&lt; 0 . 05 ). On the other hand, A, B, C, and D are statistically indistinguishable on this test collec-tion. Yet, results are still promising: when C is compared to A and D, the p -value is approximately 0.15 and 0.18, respectively. Also, a
