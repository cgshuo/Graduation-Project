 Jason Chuang jcchuang@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Sonal Gupta sonal@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Christopher D. Manning manning@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Jeffrey Heer jheer@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Supplementary Figure 2 shows an enlarged version of Figure 1 in the main paper with additional details in the caption.
 Supplementary Figure 3 shows additional data points for Figure 7 in the main paper. We conducted a survey asking ten experienced infor-mation visualization (InfoVis) researchers to identify what they consider to be significant and coherent areas of research in their field. Participants were asked to label each area, and describe it with lists of exemplary terms and documents.
 We focused on InfoVis research due to relevance, scope and familiarity. Analysis of academic publications is one of the common real-world uses of topic modeling (Griffiths &amp; Steyvers, 2004). Our familiarity with the InfoVis community allowed us to contact experts ca-pable of exhaustively enumerating its research areas. InfoVis has a single primary conference, simplifying the construction and analysis of its publications. Survey recruitment was by invitation only. We con-tacted 23 researchers (12 past chairs of the IEEE In-formation Visualization Conference, six faculty mem-bers, two senior industry researchers, and three PhD students within a year of graduation) on a rolling ba-enable accurate data collection, and (4) balance be-tween maximizing the value of available expert time and preventing participant exhaustion.
 To avoid artificially limiting what they consider to be the scope of InfoVis, the participants were instructed to consider work published anywhere when creating the research topics. Participants were provided with multiple blank boxes, and asked to enumerate all ar-eas they consider to be significant. The webpage con-tained twenty boxes by default, but subjects could add additional boxes if desired.
 In pilot studies, the single most prominent issue was re-call. Exhaustively identifying all concepts in a domain purely from memory can be difficult. In response, we added a panel on the right that contains a list of all 442 papers published at the IEEE Information Visualiza-tion Conference (1995 to 2011), grouped by year. As InfoVis is a single track conference, we group papers within each year by session, so the ordering of sessions and papers are consistent with the actual conference program. Participants could browse through the pro-ceedings or search for specific papers by title, author, or abstract.
 The most scarce resource in conducting the survey was acquiring available expert time. To maximize the value of their responses, we chose exemplary words and doc-uments as the means to express a concept. Labels are widely used in cognitive psychology (Rosch et al., 1976) for identifying topics. Based on pilot studies, the two chosen properties  X  freeform typing of a list ing their responses within a maximum of five sessions. The amount of editing time suggests that the survey taxed the experts attention and available contiguous time. the probability of observe a positive outcome for the i -th event, let P k definitive ( n ) represent the probability that we observe exactly n positive outcomes among Similarly, given a Bernoulli process { x i }  X  i =1 , noise where x i is the probability of observe a positive outcome for the i -th event, let P k noise ( n ) be the probability that we observe exactly n positive outcomes among the follow-Suppose we construct a new series of Bernoulli pro-domly drawing from the two processes { x i } definitive and { x i } noise . Suppose we draw k events from { x i } definitive and m  X  k events from { x i } noise .
 Let P combined ( n ) be the probability that we observe exactly n positive outcomes among its m events. I 3.1. Sampling from Two Bernoulli Processes Since events in a Bernoulli process are considered inde-pendent, we can re-arrange the order of events without affecting the expected number of positive outcomes. 3.2. Sampling from Definitive vs. Noise Charts When computing the expected number of positive out-comes for P combined , the combined definitive and noise charts, we re-arrange the series { x i } combined so that the k definitive events occur first and the m  X  k noise events later. 3.3. Convolution Let { x i } be a Bernoulli event where x i is the proba-bility of observing a positive outcome for event i . We construct a 2-vector X i = [1  X  x i ,x i ] T . Let P k be the multinomial distribution represent-ing the observed cumulative outcome of the first k events where P k ( n ) is the probability that we observed exactly n positive outcomes for the first k events. We represent P k as an ( k + 1)-vector with entries [ P k (0) ,P k (1) ,  X  X  X  ,P k ( k )] T .
 subject to The above is an optimization involving both equal-ity and inequality constraints. We apply sequential quadratic programming to solve to P 00 using three mathematical components: barrier method to remove inequality constraints, first-order trust region to solve for equality-constrained minimizations, and heuristics to obtain a good initial solution. 5.2. Outer Iteration: Barrier Method We apply barrier method to remove the inequality con-straints, in order to reduce complexity and speed up computation. We modify the objective function as the following.
 P We perform 50 iterations and gradually increase  X  from 500 to 50000. 5.3. Inner Iteration: Trust Region Within each iteration of the barrier method, we ap-plied first-order trust region solve for an optimal solu-tion P 00 . 5.4. Initial Solution To ensure better convergence, we solve the linear sys-tem of equations AP 00 = P , to obtain an initial so-lution P 00 (0) . We clamp the values of P 00 (0) to within [0 , 1] and L 1 normalize the vector to ensure it X  X  a valid probability distribution. We use the resulting vector as the initial solution for the aforementioned barrier method/trust region solver.

 Jason Chuang jcchuang@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Sonal Gupta sonal@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Christopher D. Manning manning@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Jeffrey Heer jheer@cs.stanford.edu Stanford University, 353 Serra Mall, Stanford, CA 94305 USA Data analysts often apply probabilistic topic models to analyze document collections too large for any one person to read. In many real-word applications, latent topics need to be verified by experts to ensure they are semantically meaningful within the domain of analysis (Talley et al., 2011; Hall et al., 2008). Human-in-the-loop supervision may involve inspecting individual la-tent topics, comparing multiple models, or re-training using different parameter settings. As a result, manual validation can dominate the time and cost of building high-quality topic models.
 Intrinsic evaluation, based on statistical (Blei et al., 2003) or coherence (Newman et al., 2010b) measures, can be problematic in these contexts because these measures do not account for domain relevance. We also currently lack tools that provide diagnostic feed-back on how latent topics differ from users X  organiza-tion of domain concepts during the construction of a topic model. Analysts often resort to spot-checking topics in an ad hoc manner after the model is created. In response, we introduce a framework to support large-scale assessment of topical relevance . We first quantify the topical alignment between a set of latent topics and a set of reference concepts. We say a topic resolves to a concept if a one-to-one corre-spondence exists between the two, and recognize four types of misalignment: when models produce junk or fused topics or when reference concepts are missing or repeated among the latent topics.
 We then introduce a process to automate the calcula-tion of topical alignment, so that analysts can com-pare any number of models to known domain con-cepts and examine the deviations. Taking a human-centered approach, we estimate the likelihood of topic-concept pairs being considered equivalent by human judges. Using 1,000 ratings collected on Amazon Me-chanical Turk, we find that a rescaled dot product out-performs KL-divergence, cosine, and rank-based mea-sures in predicting user-identified topic matches. We estimate and remove topical correspondences that can be attributed to random chance via a generative prob-abilistic process. Finally, we visualize the results in a correspondence chart (Figure 1) to provide detailed diagnostic information.
 Our framework is sufficiently general to support the comparison of latent topics to any type of reference concepts, including model-to-model comparisons by treating one model X  X  outputs as the reference. For this work, we demonstrate our approach using expert-generated concepts. We asked ten experienced re-searchers in information visualization to exhaustively identify domain concepts in their field, and compiled a set of high-quality references.
 We show that, in addition to supporting model eval-uation, our framework can also provide insights into the following aspects of topic modeling research. Model Exploration. We construct latent Dirichlet allocation (LDA) models (Blei et al., 2003) using over 10,000 parameter and hyperparameter settings, and compare the resulting 569,000 latent topics to the ex-pert concepts. We observe that a small change in term smoothing prior can significantly alter the ratio of re-solved and fused topics. In many cases, increasing the number of latent topics leads to more junk and fused topics with a corresponding reduction in resolved top-ics. About 10% of the concepts in our dataset are only uncovered within a narrow range of settings.
 Evaluation of Inference Algorithms. We examine the effectiveness of parameter optimization and semi-supervised learning. We find that hyperparameter op-timization (Wallach et al., 2009a) is generally effective for LDA. Author-topic models (Rosen-Zvi et al., 2004) achieve lower coverage than optimized LDA but favor resolved over fused topics. Partially labeled LDA mod-els (Ramage et al., 2011) also achieve lower coverage but uncover a subset of concepts not resolved by LDA. Evaluation of Intrinsic Measures. Automatic evaluation is desirable when reference concepts are not available. We examine the ability of ten intrinsic mea-sures (Newman et al., 2010a; Alsumait et al., 2009; Mimno et al., 2011) to identify topical misalignments. We find little correlation between these measures and topics that are identified as junk. While some mea-sures can distinguish junk topics comprised of function words, they are unable to separate junk topics com-prised of incoherent content words from useful topics. In sum, we provide a new visualization tool and tech-niques for effective human-in-the-loop construction, diagnosis, and repair of domain-relevant topic models. Latent Dirichlet allocation (Blei et al., 2003) and vari-ants (Blei et al., 2004; Blei &amp; Lafferty, 2006; Ramage et al., 2009; Steyvers et al., 2004; Wang &amp; McCallum, 2006) have been applied in numerous domains (Talley et al., 2011; Newman et al., 2006; Ramage et al., 2010). While topic models can improve the performance of task-based systems (Wei &amp; Croft, 2006; Titov &amp; Mc-Donald, 2008), they are most frequently used in ex-ploratory text mining and typically evaluated based on statistical measures such as perplexity (Stevens et al., 2012) or held-out likelihood (Wallach et al., 2009b). Such measures, however, do not always correlate with human judgment of topical quality (Budiu et al., 2007) nor capture concepts that people consider to be rele-vant and interpretable (Chang et al., 2009). Chuang et al. (2012b) emphasize the importance of interpreta-tion and trust in model-driven data analysis.
 More recently, Chang et al. (2009) introduced hu-man validation of topical coherence via intrusion tests, but the process requires manual verification of every model built. Automatic measures of topical coherence have been proposed using word co-occurrence within the corpus (Mimno et al., 2011), Wikipedia articles or Google search results (Newman et al., 2010a), or WordNet (Musat et al., 2011). Alsumait et al. (2009) introduced heuristic measures of topical significance. Researchers have also explored the use of visualizations for interactive inspections of topic models. The Topic Browser (Chaney &amp; Blei, 2012) and Termite (Chuang et al., 2012a) focus on the exploration of a single topic model while TopicNets (Gretarsson et al., 2012) al-lows users to adaptively generate new models. How-ever, none have the ability to measure deviations from user-defined reference concepts, nor provide diagnostic information on why models may be under-performing. We present our method for aligning latent topics with reference concepts, where each topic or concept is a multinomial distribution over words. At the heart of our method is the calculation of matching likelihoods for topic-concept pairs: the probability that a human judge will consider a latent topic and a reference con-cept to be equivalent. Based on human-subjects data, we examine how well various similarity measures pre-dict topic matches and describe how we transform sim-ilarity scores into matching likelihoods. To improve ro-bustness when making a large number of comparisons, we introduce a method to account for correspondences that occur due to random chance. We also introduce the correspondence chart which visualizes the align-ment between latent topics and reference concepts. 3.1. Correspondence Chart and Misalignments The correspondence chart is an n  X  m matrix of all possible pairings among n reference concepts and m la-tent topics. We treat each entry p s,t as an independent Bernoulli random variable representing the matching likelihood that a user examining the word distributions associated with concept s and topic t would respond that the two are equivalent.
 We consider a correspondence optimal when every la-tent topic maps one-to-one to a reference concept. De-viations from an optimal arrangement lead to four types of misalignment, as shown in Figure 2. We treat entries { p i,t } n i =1 corresponding to topic t as a Bernoulli-like process: a series of independent events that can take on different probabilities. In this frame-work,  X  P t ( k ) is the likelihood that a user responds with exactly k matches after comparing topic t to all n ref-erence concepts. Similarly,  X  P s ( k ) is the likelihood of observing exactly k positive outcomes after compar-ing concept s to all m latent topics. The junk score for topic t is the probability  X  P t (0); the topic has no matching concept. The fused score for topic t is the likelihood P m k =2  X  P t ( k ); the topic matches two or more concepts. Similarly, the missing score for concept s is  X 
P s (0), and the repeated score is P n k =2  X  P s ( k ). 3.2. Human Judgment of Topic Matches We conducted a study to acquire data on when top-ics (probability distributions over terms) are consid-ered matching by people. We trained two LDA topic models on a corpus of information visualization pub-lications and sampled pairs of topics, one from each model. The texts were chosen to be consistent with the corpus of the expert-generated concepts that we col-lected (details in  X  4). Preliminary analysis suggested that the corpus contained about 28 domain concepts, and thus we trained the two models with 40 and 50 latent topics using priors  X  = 0 . 01 and  X  = 0 . 01. We presented study subjects with topical pairs, one at a time in a webpage. Each topic was displayed as a list of words, sorted by frequency, where the height of each word was scaled proportional to its frequency in the topic X  X  distribution. We asked the subjects whether the two topics match ( X  represent the same meaningful concept  X ), partially match, or do not match ( X  repre-sent different concepts or meaningless concepts  X ). We conducted our study using Amazon Mechanical Turk. We included five topical pairs in each task, posted 200 tasks with a US$0.25 reward per task in December 2012, and received 1,000 ratings for 167 topical pairs. 3.3. Evaluating Topical Similarity Measures We evaluated how well similarity measures predict hu-man judgment in terms of precision and recall. For each topical pair, we assign it a rating of { 1 , 0 . 5 , 0 } for each { match, partial match, no match } response, and consider a pair as matching if it has an average rating above 0.5. We computed the similarity between topics using four measures. Cosine, Spearman rank coeffi-cient, and KL-divergence are three common similarity measures. We also introduce a rescaled dot product to improve upon cosine (Table 1).
 Precision-recall scores in Figure 3 compare user-identified matches to the ordering of topical pairs in-duced by the similarity measures. Our rescaled dot product achieved the highest AUC, F1, F0.5, and F2 scores. We found that KL-divergence did a poor job of predicting human judgment; topical pairs ranked in the 90th percentile (among the 10% of most divergent pairs) still contained matches. Spearman rank correla-tion was concentrated in a narrow range (  X  0 . 04 , 0 . 16) for 96% of our data points. We observed that L 2 nor-malization in the cosine calculation was largely inef-fective when applied to ( L 1 normalized) probability distributions. Instead, given two word distributions we rescaled their dot product to the range of mini-mum and maximum possible similarities, and found that this outperformed the other measures. 3.4. Mapping Similarity Scores to Likelihoods While the rescaled dot product is predictive of human judgment, the actual similarity values deviate from our definition of matching likelihood. Figure 4 plots pre-cision against the similarity score at which that pre-cision is achieved. By definition, topical pairs ranked above a precision of 0.5 are considered matching by human judges over 50% of the time. For the rescaled dot product, this threshold occurs at 0.1485 instead of the desired value of 0.5.
 Linear transformation in log-ratio likelihood space per-forms well for correcting this deviation. We con-vert both similarity scores and precision values to log-ratio likelihoods, and apply linear regression to deter-mine optimal mapping coefficients (Table 2). For the rescaled dot product, the transformed scores deviate from average user ratings by 0.0650. Transformed co-sine angles deviate from user ratings by 0.1036. Pro-vided with sets of reference concepts and latent topics, we can now populate entries of a correspondence chart using the transformed rescaled dot product scores. 3.5. Estimating Random Chance of Matching Matching likelihoods determined from human judg-ments are rarely exactly zero. As a topic model may contain hundreds of latent topics, even a small chance probability of matching can accumulate and bias mis-alignment scores toward a high number of repeated concepts or fused topics. To ensure our framework is robust for large-scale comparisons, we introduce a method to estimate and remove topical correspon-dences that can be attributed to random chance. Given a correspondence matrix, we treat it as a linear combination of two sources: a definitive matrix whose entries are either 0 or 1; and a noise matrix represent-ing some chance probability. We assume that match-ing likelihoods are randomly drawn from the definitive matrix (1  X   X  ) of the time and from the noise matrix  X  of the time, where  X  is a noise factor between [0 , 1]. Without explicitly specifying the values of the en-tries in the definitive matrix, we can still construct P definitive if we know it contains k non-zero values. We compute the average row and column matching likeli-hoods, and create a noise matrix whose entries equal  X  p s,t = 0 . 5 P of sampling from the two source charts produces a is the convolution operator; mathematical derivations are provided in the supplementary materials. We com-pute  X  by solving the convex optimization: The optimal  X  value represents the estimated amount of matches that can be attributed to noise. We then estimate the most likely distribution of topical matches P denoised without the chance matches, by solving the following constrained optimization: subject to P denoised being a proper probability distri-bution whose entries sum to 1 and are in the range [0 , 1]. We apply the above process to each row and col-umn in the correspondence matrix, to obtain  X  P denoised and  X  P denoised from which we estimate topical misalign-ment scores as described previously. Reference concepts in our diagnostic framework can be determined from various sources: elicited from domain experts, derived from available metadata, or based on the outputs of other topic models. For this work, we fo-cus on the construction and use of high-quality expert-authored concepts to demonstrate our framework. We conducted a survey in which we asked ten expe-rienced researchers in information visualization (Info-Vis) to exhaustively enumerate research topics in their field. Human topical organization can depend on fac-tors such as expertise (Johnson &amp; Mervis, 1997) where prior research finds that experts attend to more sub-tle features and recognize more functionally important concepts. To ensure quality of our reference concepts, we designed our survey to meet the following three cri-teria: (1) admitting only expert participants, (2) elic-iting an exhaustive instead of partial list of concepts from each participant, and (3) collecting reference con-cepts from multiple subjects instead of a single source. Survey responses consisted of manually-constructed topics comprising a title, a set of descriptive key-phrases, and a set of exemplary documents. Respon-dents authored these topical descriptions using a web-based interface with a searchable index of all 442 pa-pers published at IEEE Information Visualization (de-tails in supplementary materials). We received a total of 202 reference concepts from the experts. We map survey responses to reference concepts as fol-lows. For each expert-authored topic, we construct two term frequency counts: one consisting of provided title and keyphrase terms, and another consisting of the terms found in the representative documents. We perform TF.IDF weighting, normalize, and average the two distributions to produce a reference concept. We chose InfoVis because of our familiarity with the community, which allowed us to contact experts capa-ble of exhaustively enumerating research topics. The survey responses, though specific to a domain, consti-tute a rare and large-scale collection of manual topical categorization. The dataset provides us with a con-crete baseline for assessing how machine-generated la-tent topics correspond to trusted concepts identified by experts, and enables comparisons with future stud-ies. We are currently collecting topical categorizations in other domains and for larger corpora. All results in this section are based on the InfoVis corpus. All models are built using Mallet (McCallum, 2013) unless otherwise stated. N denotes the number of latent topics in a topic model;  X  and  X  denote topic and term smoothing hyperparameters, respectively. 5.1. Exploration of Topic Models We experiment with an exploratory process of topic model construction, in which users specify reference concepts a priori and utilize alignment scores to ana-lyze the parameter space of models. We first examine the effects of varying N  X  [1 , 80], and then perform an exhaustive grid search over N ,  X  , and  X  .
 Talley et al. (2011) found that N affects concept res-olution and the number of poor quality topics. They arrived at this conclusion only after building a large number of models and performing an extensive manual review. In contrast, our framework allows users to map a large number of models onto predefined concepts and immediately inspect model qualities. In Figure 5, our misalignment measures indicate that the number of re-solved topics peaks at N = 18. While the ratio of fused topics dips at N = 20, the proportion of fused topics increases again for N  X  30. Trends in Figure 6 suggest that for a different set of hyperparameters, increasing N produces only more junk topics.
 In Figure 7, we extend the space of models to over 10,000 parameter settings by searching 13 values of  X  and  X  . The set of hyperparameter values are cho-sen so they center at the default setting (  X  = 50 /N and  X  = 0 . 01) and cover a range across 4 orders of magnitude. We observe additional qualitative changes in topic composition, such as the transition between fused and resolved concepts around  X  = 0 . 25. 5.2. Evaluation of Inference Algorithms We analyze three categories of models to examine the effectiveness of hyperparameter optimization (Wallach et al., 2009a) for LDA, and the inclusion of metadata for author-topic models (Steyvers et al., 2004) and par-tially labeled LDA (PLDA) (Ramage et al., 2011). We built 176 LDA models with hyperparameter op-timization using Mallet (McCallum, 2013) for a grid of 11 values of N  X  [5 , 80] and 4 initial values for each of  X  and  X  . We manually resolved all author names in all papers in the InfoVis corpus, and built 10 author-topic models using the Matlab Topic Mod-eling Toolbox (Steyvers &amp; Griffiths, 2013) by varying N  X  [5 , 80]. Finally, we built 11 PLDA models using the Stanford Topic Modeling Toolbox (Ramage, 2013), without hyperparameter optimization to isolate the ef-fects of learning from metadata. We trained 10 models corresponding to concepts identified by each of the 10 experts. We then manually identified 28 concepts pro-vided by at least 3 experts, and built an additional PLDA model containing these 28 concepts.
 Figure 9 shows the number of resolved concepts for the best performing model from each category. The graph also includes the best performing LDA models for five values of  X  . We find that hyperparameter optimiza-tion is often able to select a  X  value comparable to the best-performing LDA model among our set of 10,000 from Section 5.1. Both author-topic models and PLDA without optimization uncover fewer resolved topics. Qualitatively, we note that author-topic models gen-erally exhibit a higher proportion of resolved topics than fused topics. Examining individual concepts, we find that approximately 10% are uncovered by LDA only within narrow range of N, X , X  values. An exam-ple of such a topic is toolkit (Figure 8), provided by eight of our experts. We find that PLDA was able to consistently recover the toolkit concept. 5.3. Evaluation of Intrinsic Measures We apply our measures of junk and resolved topics to assess existing intrinsic measures for topical quality. We first describe the intrinsic measures under consid-eration, and then present our comparison results. Alsumait et al. (2009) proposed three classes of top-ical significance measures. The authors describe a latent topic as uninformative if it consists of a uni-form word distribution ( UniformW ) , a word distribu-tion matching the empirical term frequencies in the corpus ( Vacuous ) , or uniform weights across documents ( Background ) . The significance of a topic is its distance from one of these uninformative attributes; the exact definition of a  X  X istance X  was left open by the authors. For this work, we evaluated six significance measures based on KL-divergence ( UniformW-KL , Vacuous-KL , Background-KL ) and cosine dissimilarity ( UniformW-Cos , Vacuous-Cos , Background-Cos ) . We also examined Pearson rank correlation ( Vacuous-Cor ) . 1 Newman et al. (2010a) measured topical coherence based on word co-occurrence in WordNet, Wikipedia articles, or Google search results. We examined their two top performing measures: word co-occurrence in the titles of search engine results 2 ( BingTitles-10 ) and pointwise mutual information in Wikipedia text ( WikiPMI-10 ) . Mimno et al. (2011) measured topical coherence based on word co-occurrence in the docu-ment corpus ( MimnoCo-10 ) . These three coherence scores examine only the top k most probable words belonging to a topic. We experimented with various values up to k  X  30, but report only k = 10 which is representative of the overall results.
 We computed the topical significance and coherence scores for each of the 176 LDA models with hyperpa-rameter optimization built in Section 5.2. Figure 10 shows the correlation between the Vacuous-KL score and our junk measure. We observe that a small set of topics (bottom left) are marked as problematic by both measures. We also find, however, a large number of discrepancies (bottom right): junk topics without a meaningful corresponding expert-identified concept but marked as significant by Vacuous-KL .
 Figure 11 repeats the graph for all ten intrinsic mea-sures. As a whole, we observe little correlation across the graphs. Background-KL and Background-Cos exhibit a similar pattern as Vacuous-KL with some shared junk topics but a large number of discrepancies. WikiPMI-10 performs poorly because many domain-specific terms do not co-occur in Wikipedia articles. BingTitles-10 can separate topics comprising of func-tional words but otherwise lacks discriminative power. We also examined the ranking of topics within each model. We computed the Spearman rank correlation between the ranking of topics by the intrinsic scores and by our junk measure. The median values are shown in the chart titles in Figure 11. We find low levels of correlations, indicating discrepancies between topics considered meaningful by experts and those marked significant/coherent by existing measures. For many domain-specific tasks, applying topic mod-eling requires intensive manual processing. In this paper, we introduce a framework in which analysts can express their domain knowledge and assess topic diagnostics. We quantify four types of topical mis-alignment, and introduce a process to automate the calculation of topical correspondences. Our technique enables large-scale assessment of models. Our applica-tions suggest that diagnostic information can provide useful insights to both end-users and researchers. Our long-term research goal is to support a human-in-the-loop modeling workflow. While recent work has contributed learning techniques for incorporating user inputs to aid the construction of domain-specific mod-els (Hu et al., 2011; Ramage et al., 2011), we believe empirical studies of human topical organization and the design of user-facing tools can be equally impor-tant in supporting effective interactive topic modeling. For this work, we elicited high-quality expert-authored concepts for evaluating topic models. In various other domains, reference concepts may exist but can be of differing levels of quality or coverage. An open re-search question is how semi-supervised learning al-gorithms and automatic measures of topical quality would perform under noisy or incomplete user inputs. Our dataset and results provide a benchmark and a point of comparison for future research in these areas. Another research question is how topical misalignment affects a user X  X  ability to interpret and work with topic models. Our results suggest that different models pro-duce different types of misalignment. A better under-standing may lead to improved detection of problem-atic latent topics and more informed decisions on how to match models to analysis tasks. Certain misalign-ments may be more easily remedied by people; we can then design interactive diagnostic tools to elicit cor-rective actions from the users accordingly.

