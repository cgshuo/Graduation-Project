 Markus Wienecke, Gernot A. Fink, Gerhard Sagerer Abstract. The increasing popularity of whiteboards as a popular tool in meeting rooms has been accompanied by a growing interest in making use of the whiteboard as a user interface for human-computer interaction. There-fore, systems based on electronic whiteboards have been developed in order to serve as meeting assistants for, e.g., collaborative working. However, as special pens and erasers are required, natural interaction is restricted. In order to render this communication method more natu-ral, it was proposed to retain ordinary whiteboard and pens and to visually observe the writing process using a video camera [22, 25]. In this paper a prototype system for automatic video-based whiteboard reading is pre-sented. The system is designed for recognizing uncon-strained handwritten text and is further characterized by an incremental processing strategy in order to facil-itate recognizing portions of text as soon as they have been written on the board. We will present the meth-ods employed for extracting text regions, preprocessing, feature extraction, and statistical modeling and recogni-tion. Evaluation results on a writer-independent uncon-strained handwriting recognition task demonstrate the feasibility of the proposed approach. 1 Introduction In the field of human-computer interaction the ultimate goal is to make interaction with computing machinery possible without the need for special input devices such as keyboard or mouse. Therefore, more natural input modalities of communication such as speech, gesture, and handwriting are extensively studied. Besides investi-gating the use of handwriting for interacting with PDAs, there is also a growing interest in systems for recognizing handwriting on a whiteboard. This is mainly due to the increasing popularity of whiteboards not only for pre-sentations and educational purposes but also in meeting rooms for the exchange of ideas during group discussions, for project planning, system design, etc. Therefore, sys-tems that can serve as automatic meeting assistant to support collaborative working are highly desirable. terface for human-computer interaction, systems based on electronic whiteboards have been developed. Like dig-itizing tablets, these systems employ electronic pens and erasers allowing their position in the plane to be sensed and tracked during the writing process. Using a com-puter the movements of the pen and the eraser can be transformed in order to construct an electronic version of the document image on the whiteboard (cf., e.g., [7]). Additionally, the pen trajectory can be interpreted by an online recognition module to automatically recognize what was written on the board.
 tages. As special pens and erasers are necessary, natu-ral interaction is restricted. For example, an electronic whiteboard does not notice the erasing of parts of the written document if the writer uses a finger or paper towel instead of the eraser provided. Furthermore, graph-ical symbols or text written on Post-It notes that are then affixed to the board cannot be recognized. There-fore, a promising alternative might be to retain ordinary whiteboard and pens and to observe the writing process using a video camera.
 preferable position of the video camera is several me-ters in front of the board, either mounted to the ceiling or fixed on a tripod. With this setup, the gestures of the user writing on the board could also be considered. Furthermore, an active camera together with mosaicing techniques could be used to enlarge the observable area. Unfortunately, this setup also has a severe disadvantage. As the user usually stands in front of the board to have a clear view during the writing process, the pen and portions of text are very often occluded by the writer and therefore not visible in the image sequence. In order to circumvent this drawback, a kind of activity analysis could be employed to decide whether the captured image is suitable for further processing. An alternative method is to extract only the visible portions of the handwritten text and to incrementally integrate the partial results to the overall recognition result.
 whiteboard reading is presented. In contrast to the ap-proaches proposed in [22, 25], which only permit the recognition of a limited set of symbols, our system is de-signed for recognizing unconstrained handwritten text. As the pen is rarely visible in the image, and thus online recognition based on the pen trajectory is not feasible, the proposed system is characterized by an incremental offline recognition approach. Thus, the writing process is continuously observed, and recognition starts automati-cally as soon as a region of handwritten text is visible in the image. Besides saving processing time as only small portions of text are recognized each time, this approach also allows obtaining a qualitative estimation of the time structure of the handwriting process. This is an impor-tant prerequisite for systems used for meeting assistance and e-learning as the handwriting can be related to the gestures or the speech of the user.
 related work. The architecture of the proposed white-board reading system is presented in Sect. 3. Afterwards we describe in subsequent sections the methods for text extraction, preprocessing, feature extraction, and statis-tical modeling and recognition. Evaluation results will be presented in Sect. 9 in order to demonstrate the ef-fectiveness of the proposed approach. 2 Related work In the following discussion a short review of important work related to the task of video-based whiteboard read-ing is given. The first section presents some relevant sys-tems for catching and recognizing a person X  X  handwriting that are based on visual input. The next section deals with more general aspects of unconstrained offline hand-written text recognition. 2.1 Video-based systems In order to circumvent the drawbacks related to the use of specialized hardware for whiteboard reading, it was proposed to retain the ordinary whiteboard and to use a video camera for observing the board. In contrast to electronic whiteboards, where the pen trajectory is di-rectly recorded, systems based on visual input first have to detect pen movements or relevant image regions in the image sequence.
 that has a distinctive color. By tracking the pen a tem-poral trajectory is obtained that can be recognized using online methods. In [1] a system based on visual input is described that allows the user to control the computer through simple gestures. It uses a color histogram tracker to obtain the trajectory of a special marker pen. The re-sulting trajectory is then classified by a kind of particle-filtering algorithm in order to recognize the gestures. A video-based system capable of tracking an ordinary pen in image sequences was proposed by Munich and Per-ona [19]. The tracking process is based on a template-matching approach, i.e., the position of the pen tip is found by maximizing the correlation of the pen template and the image. Additionally, a kinematic motion model is applied to constrain the search space for template match-ing to a relatively small image region. In [11,27] the tra-jectories obtained from the template-matching approach described above are used for online handwriting recogni-tion. A similar handwriting recognition system based on visual input was proposed by Bunke et al. [2]. Here, the pen trajectory is obtained by analyzing the difference image of two consecutive frames. These online systems can be successfully employed in scenarios where the pen is always visible in the image, as this is a necessary pre-condition of such systems. However, they can hardly be applied to whiteboard reading, where the pen is very of-ten occluded by the writer, who usually stands between the board and the camera.
 board reading is to extract and analyze the relevant im-age regions after the writing process has completed. For example, the video-based BrightBoard system described in [25] continuously observes the whiteboard and grabs a suitable image when the movement of the writer has completed. The image is then analyzed to find and rec-ognize graphical marks that correspond to commands allowing the user to control the computer. A similar camera-based whiteboard scanner is the so-called Zom-bieBoard system described in [22], which has been in use in conference rooms for over 5 years. The system moni-tors activity in front of the board and watches the users to draw graphical marks indicating commands and asso-ciated parameters. In order to facilitate high-resolution whiteboard imaging, the ZombieBoard system applies an active camera combined with a mosaicing algorithm. 2.2 Handwritten text reco gnition As the system presented in this paper is not restricted to a small set of commands but is designed for recog-nizing unconstrained handwritten text, the approach is closely related not only to the task of locating text in image sequences but also to offline handwriting recog-nition. Whereas the problem of text detection in image sequences has so far been studied mostly for machine-printed text [5, 13, 18], much work has been done in the field of offline handwriting recognition. See, e.g., [21, 26] for an extensive survey.
 lexicon achieve high recognition rates and are therefore successfully employed for the task of postal address and legal amount reading[CE1]. In contrast to these tasks, the recognition of unconstrained handwritten texts us-ing a large or even unlimited vocabulary is much more difficult. This is mainly caused by the absence of context knowledge and word segmentation information.
 strained handwritten text recognition have been de-veloped. Earlier approaches applied segmentation-based methods in combination with sophisticated classification techniques (for a survey see, e.g., [3]). In more recent work, however, segmentation-free methods were pursued in order to avoid errors introduced by segmenting text into words or even characters at an early stage. Here, hidden Markov models (HMMs) have been successfully applied and have gained increasing interest in the re-search community. In [14] a segmentation-free method based on HMMs was proposed where a whole line of text is fed into the recognition module. The system is tested in single writer mode and achieves promising recognition results by incorporating statistical language models. Advanced systems for writer-independent un-constrained text recognition that are also tested on a large database [15,17] produced by several hundred writ-ers can be found in [16, 28]. 3 System architecture The system presented in this paper is characterized by an incremental processing strategy. The writing process is continuously observed, and the recognition process is activated as soon as a handwritten text region is visi-ble in the image. Thus the text regions are classified in their order of appearance and integrated into the overall recognition result. An example of the recognition process is shown in Fig. 2.
 in Fig. 3. After the image is captured, all text regions currently visible are extracted. In order to avoid recog-nizing the same text region multiple times in the image sequence, we employ a region memory containing all the different text regions extracted so far. If a new, not-yet-memorized text region is found, several preprocessing steps are applied to compensate for the highly varying background intensity and to normalize the handwriting. After that, features are extracted using a sliding window approach that are finally fed into a statistical recognition module based on hidden Markov models (HMMs). 4 Image aquisition The image sequences required for whiteboard reading are captured using a standard video camera (Sony EVI-D31) mounted on a tripod positioned approximately 3 to 4 m in front of the whiteboard (Fig. 4). The observed writing space covers an area of approximately 70  X  50 cm of the board. The camera is working in interlaced PAL mode grabbing about 5 images per second at a size of 756  X  576 pixels. From this it follows that the effective resolution is less than 30 dpi, which is about ten times smaller com-pared to scanned documents used mostly for handwrit-ing recognition. The observed writing space could be en-larged using an active camera and mosaicing techniques (cf., e.g. [18, 22]), but as the construction of mosaics is a time-consuming process, we use a fixed camera in order to achieve short response times. 5 Text detection 5.1 Requirements As the extraction of the handwritten text regions in the image sequence is an essential module for further pro-cessing, some requirements have to be met. First, the extraction of the text regions has to be robust with re-gard to noisy images, i.e., nontext regions that belong to the writer or are caused by nonuniform lighting should be suppressed. Additionally, the text extraction has to avoid splitting up words in order to facilitate lexicon-based recognition. Furthermore, graphical marks as lines, ar-rows, or circles have to be detected as they are typically used in tables and diagrams or to emphasize words. An-other important requirement is a short processing time that keeps the delays between writing and recognition as short as possible. In order to satisfy these conditions a two-step approach for robust and fast extraction of handwritten text regions is used (Fig. 5a X  X ). 5.2 Discrimination of text, background, and noise In the first step of the text extraction process the grayscale image is divided into overlapping blocks of equal size (40  X  40 pixels). On each of these blocks a three-dimensional feature vector is calculated for dis-criminating text from noise and pure background re-gions. It is assumed that text blocks can be identified using the following characteristics: They contain edge pixels caused by pen strokes, show an average pixel inten-sity similar to the empty whiteboard, and, compared to noise regions, remain relatively stable over time. There-fore, the feature vector consists of the following three components: 1. The average pixel intensity of the block, 2. The average difference of pixel intensity between two 3. The number of edge pixels per block (determined us-Empirically determined thresholds are then applied to the components of those feature vectors in order to de-cide whether the image block is part of a text region, an  X  X mpty X  whiteboard region, or noise. In order to adapt to changes of illumination, the average intensity of the empty whiteboard is updated when no noise regions are present in the image. 5.3 Aggregation of text components After the coarse discrimination of text, background, and noise regions are carried out, the image blocks, which are assumed to contain text and are sufficiently far away from noise regions as well, are binarized. Here, the Otsu method [20] using one global threshold per block is em-ployed to achieve a fast binarization. Subsequently, the connected components associated to ink strokes (the black pixels) are calculated. The connected components of all text blocks are then clustered based on the dis-tances between their bounding boxes in order to obtain regions corresponding to handwritten words or text lines. If adjacent text lines are touching, i.e., they are sharing at least one connected component, the horizontal pro-jection profile of the image region is searched for a mini-mum in order to split the lines. After having determined the bounding boxes of the aggregated text regions, the corresponding region is cut out of the original image for further processing.
 peatedly in consecutive images, a region memory is em-ployed that contains all regions recognized so far. Thus, in each timestep the extracted regions are compared with the regions stored in the memory. If the similarity (based on the pixelwise difference of the connected component representation) exceeds a threshold, it is assumed that the newly extracted region has already been recognized and, therefore, will not be recognized again. The ex-tracted text regions that cannot be found in the memory are then used for further preprocessing.
 memory permits the handling of corrections  X  an impor-tant feature for human-computer interaction. The han-dling of corrections is also accomplished by permanently comparing the region memory with the current image. If the writer wipes out a portion of text, the associated region is obviously no longer visible in the image. Con-sequently, the image region corresponding to the region previously stored in the memory cannot be found and the latter will be deleted from the memory. Therefore, the region memory always reflects the current configuration of the whiteboard. 5.4 Detection of graphical marks For the detection of graphical marks, a couple of heuris-tics are used based on the connected component repre-sentation of the ink strokes. We first calculate the size of the bounding box and the lineness of a connected component, i.e., the relation of the number of ink pix-els to the length of the diagonal of the bounding box. Obviously, the lineness is small for straight lines and in-creases with the curvature of the ink stroke. The line-ness feature in conjunction with size and the distance to adjacent components is, therefore, well suited to detect isolated straight lines as they are used in diagrams and tables or for underlining words. The connected compo-nent representation of an example input image is shown in Fig. 6a. The components that are assumed to corre-spond to graphical marks are emphasized. The resulting aggregated components are shown in Fig. 6b. Note that the component  X  X  X  of the word teacher is correctly ag-gregated to the text region because of the proximity to adjacent text components. 6 Preprocessing It is a well-known fact that the normalization of hand-writing has a great impact on recognition accuracy. This particularly applies to the task of video-based whiteboard reading, which is much harder than read-ing scanned documents. One reason is that illumination conditions often cause severe difficulties. As no special-ized lighting is employed, the background intensity of the image is highly varying so that no global threshold exists for discriminating between the foreground and the background. As the blockwise Otsu binarization that we chose for discriminating between text, background, and noise blocks in some cases led to missing text fragments, we decided to use a more time-consuming but accurate pixelwise method for binarization of the text regions. In our experiments the modified version of Niblack X  X  bi-narization method presented in [29] gave good results. Here, the calculation of the local threshold t ( x, y based on the average pixel intensity  X  ( x, y ) and the standard deviation  X  ( x, y ) of pixel intensity in the lo-cal neighborhood: where R denotes the dynamic range of the pixel inten-sities. The parameter k depends on the application. We discovered empirically that a value of 0.06 for k is well suited for our application.
 of whiteboard reading is the geometrical distortion of the handwriting. The lack of any reference lines, together with the circumstance that subjects are often not very familiar with writing on boards, results in distorted pat-terns of handwriting. Typically, long lines of text often show drifts of the baseline (Fig. 7a). Motivated by this observation, the vertical position, skew, and slant of each text region are corrected locally. Therefore, each line is split into hypothesized words by searching for white spaces between the segments of handwriting (Fig. 7b). A splitting position is hypothesized if a whitespace is found that spans at least ten pixels. In order to avoid segments that are too short for calculating reliable normalization factors, we discard those splitting hypotheses that would result in segments shorter than 100 pixels. These thresh-olds depend on writing size and camera settings and were determined by experiment.
 segments of the handwritten line (Fig. 7c). The follow-ing three-step procedure is applied to achieve a robust baseline estimation. First, the horizontal projection his-togram is calculated in order to coarsely estimate the position of the body of the writing, i.e., the area be-tween the upper and lower baseline. In the second step, the local contour minima of the writing are extracted for computing a straight-line approximation using linear re-gression. A distance threshold is applied to discard those minima that are too far from the body of the writing. In the last step, the estimated position of the baseline is improved by discarding further outlier minima. Here, a minimum is assumed to be an outlier if its distance to the baseline is at least two times larger than the av-erage distance. The remaining minima are then used for estimating the final position of the baseline. After the lo-cal baselines have been determined, the orientation and vertical displacement of the segments are corrected by a rotational and translational transformation in order to align the local baselines to a global horizontal line (Fig. 7d).
 are used for local slant normalization. The method for calculating the local slant angle is based on the edge ori-entation information of the respective segment, similar to the approach described in [24]. First, horizontal runs of ink pixels are eliminated as they do not account for the slant of the writing. Then, the Canny edge detector is applied in order to obtain the edge orientation data accumulated in an angle histogram. The mean of the histogram is used as slant angle, which can then be em-ployed in a shear transformation to normalize the hand-writing.
 count the number of local extrema of each handwritten line and put this number in relation to the width of the line. The scaling factor is based linearly on this relation because the larger this relation, the narrower the writing style. 7 Feature extraction The preprocessed images are used as input data for the feature-extraction step. A sliding window technique like the one described in [16] is applied. In our case, a window of the image X  X  height and the width of its four columns is moved with an overlap of two columns from left to right over the image, and several geometrical features are extracted.
 feature for reading (cf., e.g., [23]), the average distance of the lower baseline to the upper contour as well as to the lower contour is calculated (Fig. 8a,b). Additionally, the distance of the center of gravity of the ink pixels to the baseline is computed (Fig. 8c). These features are then normalized by the core size, i.e., the distance be-tween upper and lower baseline, in order to increase the robustness against variations in writing size. upper contour as well as the gradient of the mean value of the pixel distribution, we additionally calcu-late three directional features. Therefore, we estimate (a) (b) (d) (g) straight lines by linear regression through the four lower contour points, upper contour points, and mean values within the sliding window. The line orientations with re-gard to the baseline are then used as features (Fig. 8d X  X ). black-to-white transitions per column, the average num-ber of ink pixels per column, and the average num-ber of ink pixels between the upper and lower contour (Fig. 8g X  X ).
 tionally compute an approximate horizontal derivative for each component of the feature vector, so that an 18-dimensional feature vector is obtained (9 features per windo w + 9 derivatives).
 improve the class separability, we integrate linear dis-criminant analysis (LDA) in the training and recogni-tion phase (cf. [6]). The original feature representation is optimized by applying a global linear transformation A , which is obtained by solving an eigenvalue problem using the intraclass scatter matrix S w and the interclass scatter matrix S b of the training data. Since to com-pute these scatter matrices each feature vector has to be labeled with an HMM state, we at first carry out an ordinary training followed by a state-based alignment of the training data. When the scatter matrices are known, the LDA transformation is computed by solving the fol-lowing eigenvalue problem: where  X  i and ( A T  X  ) i are the eigenvalues and eigenvec-tors of S  X  1 w S b . A reduction of the dimensionality can be obtained by taking into account only m eigenvectors be-longing to the m largest eigenvalues. However, in our ex-periments the best results were achieved by keeping the full dimensionality, i.e., all eigenvectors ( m = 18) were taken into account for computing the transformation. 1 After LDA transforming all feature vectors, a completely new HMM training is carried out. 8 Statistical modeling and recognition A successful statistical recognition system for handwrit-ing or spoken language consists of two modeling com-ponents, one that describes the realization of individual segments, e.g., words or characters, and another that describes the restrictions on the expected segment se-quences. The first component is usually realized by hid-den Markov models (HMMs) that model the probability density p ( x | w ) of observing a certain sequence of fea-ture vectors x given a sequence of words or characters w . The restriction of these sequences to plausible ones is achieved by defining a probability distribution P ( w ) for all possible sequences, which can be realized by a Markov chain or n -gram model. The goal of the recognition pro-cess is then to find the word or character sequence  X  w that maximizes the probability of the combined statisti-cal model given the observed data x according to In analogy to the terminology used in spoken language processing the HMM, p ( x | w ) could be termed the writ-ing model and the n -gram model P ( w ) thought of as equivalent to the so-called language model . In order to adjust the contributions of both modeling components within the combined recognition model, in practice a weighting factor  X  is used. The value of this constant, which is sometimes called the linguistic matching fac-tor , has to be determined empirically on cross-validation data. 8.1 Corpora For the design of statistical recognition systems, the availability of a sufficiently large database of training samples is an important prerequisite. Ideally, for a video-based system it would be desirable to obtain a large amount of image data recorded while observing a sub-ject writing on the whiteboard. However, recording and labeling of such video data require a substantial manual effort. Therefore, we decided to use the IAM database of scanned documents [17] for training and cross vali-dation. The database provides a large amount of hand-written text documents that were produced by several hundred subjects based on prompts taken from the LOB corpus [12]. The documents are divided into categories according to the different topics covered.
 writer IDs for the handwritten samples. However, writers never provided samples for different categories. There-fore, we defined the training data to comprise categories A to D and the cross-validation data categories E and F. This partitioning corresponds to the training and test sets used in [28] and ensures all experiments are truly writer independent.
 image sequences of texts written on a whiteboard. In or-der to be able to compare the performance of the video-based system with our offline recognizer [28], we asked ten subjects to write portions from the offline cross-validation texts on the whiteboard, namely, from cate-gory F01. No constraints with respect to the writing style were given. In contrast to the training patterns result-ing from scanned forms, where rulers on a second sheet placed below were used to align the baseline horizon-tally, the video-based data often showed baseline drifts and variations of the corpus height.
 pora used is given in Table 1. Figure 9 shows examples of a scanned document used for training and the final version of a video document from the test data. Addi-tionally, the results of the incremental text detection are shown, which, for the example given, produces an ad-ditional segment for the third text line of the original document. 8.2 Writing model The configuration and parameter estimation for the HMMs defining the writing model as well as for the lan-guage models used is carried out in the framework of the ESMERALDA development environment [8].
 with a shared codebook of approximately 2000 Gaus-sian mixtures with diagonal covariance matrices. A to-tal of 75 HMMs are created for modeling 52 letters, 10 numbers, 12 punctuation marks and brackets, and white space. The white space consists of three variants account-ing for different lengths in blank space between words or characters. All these models use the Bakis -type topology, i.e., they are basically linear models that, in addition to loops and forward state transitions, permit the skipping of states in the sequence. Thus, the models can cope with a wider range of lengths in the character patterns described.
 mode by applying the k -means algorithm to the train-ing data. Then the initial HMM parameters can be de-termined on labeled initialization data. Afterwards, we apply several iterations of the Baum X  X elch parameter reestimation to the models.
 thus obtained models for arbitrary words of some given lexicon can be constructed easily by concatenating the appropriate character models. 8.3 Language model For estimating character-based language models the transcriptions of the training data and for word-based models[CE2], the original text prompts were used. The raw n -gram probability distributions were smoothed by applying absolute discounting (discounting factor  X  =1) and backing off (cf., e.g., [4]).
 based language model in our configuration of training and test data arises from the fact that the texts be-long to different categories covering widely differing top-ics. From the total of 2534 word forms appearing in the text prompts of the cross-validation data (categories E and F), more than 48% never appeared in the training texts (categories A X  X ). Additionally, writers sometimes used varying hyphenation, which introduces unseen word fragments. In the whiteboard data 316 different word forms are used, more than 26% of which are not covered by the training set. Therefore, we decided to include in addition to the lexicon of the training data all those word forms in the overall recognition lexicon that are neces-sary to describe the text prompts from which cross vali-dation and test set were generated. From this word list a small number of entries were eliminated that contained characters not present in the training material. The re-sulting recognition lexicon consists of 7485 entries in-cluding punctuation and word fragments resulting from hyphenation. The percentage of out-of-vocabulary words for both cross validation and test data is approximately 0.5%. 8.4 Model decoding If no statistical restrictions on the possible sequences of words or characters are imposed, i.e., if no language model is used, decoding of the HMMs can be achieved by standard Viterbi beam search. However, the com-bined use of a writing and a language model requires additional effort during the recognition process. Oth-erwise, the search might not be able to find the solu-tions, which truly maximize the combined HMM and n -gram score. Therefore, when using a language model in the recognition process we apply an enhanced ver-sion of the time-synchronous recognizer proposed in [10]. The search spaces for HMM states and recognition hy-potheses are established at different levels of abstraction. Only at the level of word or character hypotheses are the HMM scores combined with the n -gram probabilities provided by the language model. Thus it can be assured that long-span restrictions as represented by, e.g., 5-gram models can also be combined correctly with the writing model and their predictive power can be fully exploited. This method for decoding a combined HMM and n -gram model is roughly equivalent to a time-synchronous rescoring of the HMM-based hypotheses with the lan-guage model [9] 9 Results and discussion In order to evaluate the proposed methods for video-based whiteboard reading, we carried out several ex-periments on the test set described in Sect. 8.1. When-ever possible, the results obtained are compared to those achieved by an offline recognition system on the cross-validation data. 9.1 Text detection The precondition for whiteboard reading is to robustly detect the image regions of the handwriting. Therefore, we at first investigated the effectiveness of the method for text detection described in Sect. 5. Using the 20 image sequences for testing consisting of 152 handwritten lines of text, it turned out that a total of 188 image regions were detected, and 173 of these regions were correctly detected text regions. In only 15 cases did errors due to noise or line segmentation errors occur as a result of touching or heavily overlapping lines. The discrepancy of the total number of originally written lines (152) and the overall number of correctly detected text regions (173) is caused by the incremental processing strategy. Thus, we observed that in 21 cases portions of text lines were detected repeatedly (see, e.g., Fig. 9).
 detected regions corresponded to the chronological order in which the text lines were written on the board. We discovered that in 9 cases, out of a total of 173 text regions, the chronological order was not correct. 9.2 Lexicon-based reco gnition For lexicon-based recognition of whiteboard texts we used a carefully defined lexicon containing 7485 word forms (Sect. 8.3). The results achieved are summarized in Table 2. Without the use of any restrictions on the possible word sequences we obtain a word error rate of 47.8%. Clearly, such a figure would not be acceptable for an automatic transcription system. However, with some limited knowledge about the expected texts repre-sented as a bigram language model this figure could be improved to 28.9%. This corresponds to a reduction of the error rate of approximately 40%. Due to the widely differing lexicons of training and test data, the bigram model has a very high perplexity on both test and cross-validation sets. For a well-trained language model that could be estimated on text data matching , the topics of the final application  X  i.e., the test text s  X  a substantially lower perplexity can be expected. Therefore, word-based recognition results on whiteboard data could easily be improved further for better matching training and test conditions. 9.3 Lexicon-free reco gnition Ultimately, any handwriting recognition system should be able to recognize text independently from a prede-fined list of possible words. For such lexicon-free recog-nition at least some expectation on the possible se-quence of characters is required. Therefore, we estimated character-based language models with n -gram lengths ranging from two to five (Sect. 8.3). These models were then used in conjunction with the context-independent character HMMs during the recognition process. The re-sults obtained are shown in Table 3. Without the re-striction of a language model a character error rate of 31.0% is obtained, i.e., roughly every third character, including white, space is misrecognized. However, when using the statistical restrictions on possible character se-quences as represented by the character-based language models, this figure can be improved significantly. With a 5-gram model a character error rate as low as 19.0% can be achieved on the whiteboard data.
 and test data is a severe limitation for word-based recog-nition, it has an advantage for the judgment of the lexicon-free results. In principle, long-span n -gram mod-els could learn the training lexicon, and, therefore, re-sults obtained with such a model might not be truly lexicon free. In our configuration, however, learning of the word forms found in the training texts has very limited effect on the cross-validation and test data (see also Sect. 8.3). Therefore, the low character error rates achieved impressively demonstrate the capability of the n -gram models to capture more general characteristics of the character sequences. 9.4 Video vs. offline reco gnition The comparison of the recognition results obtained on the whiteboard data and on the scanned documents used for cross validation clearly shows better performance on the latter ones (Tables 2 and 3). However, the difference in recognition quality is relatively small considering the widely different nature of the documents used. This ev-idence makes it obvious that the methods used for text detection, preprocessing, and feature extraction are ca-pable of compensating for the majority of distortion ef-fects found in the video data. 10 Conclusion We presented a system for automatic whiteboard reading based on visual input. It is characterized by an incremen-tal processing strategy, i.e., text lines are extracted as soon as they are visible in the image. The preprocessing and feature-extraction methods applied generate a data representation that is, to a certain extent, robust against variations in writing style and in the reduced quality of the video-based data. Evaluation results on a writer-independent task were presented for both lexicon-based and lexicon-free recognition of unconstrained handwrit-ing. When using a 7.5K lexicon and a bigram model, a word error rate of only 28.9% was possible. Without an explicit lexicon and the use of only a character 5-gram model a character error rate as low as 19.0% was reached. These results clearly demonstrate the effectiveness of the proposed methods of text detection, preprocessing, fea-ture extraction, and statistical modeling and recognition and their successful combination in a complete system for automatic video-based whiteboard reading.
 References
