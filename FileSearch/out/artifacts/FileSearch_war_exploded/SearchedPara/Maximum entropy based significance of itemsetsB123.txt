 Nikolaj Tatti Abstract We consider the problem of defining the significance of an itemset. We say that the itemset is significant if we are surprised by its frequency when compared to the the frequencies of its sub-itemsets and compute the deviation between the real value and the estimate. For the estimation we use Maximum Entropy and for measuring the deviation we use Kullback X  X eibler divergence. A major advantage compared to the previous methods is that we are able to use richer models whereas the previous approaches only measure the deviation from the independence model. We show that our measure of significance goes to applying more flexible models leads to good results.
 Keywords Binary data mining  X  Itemsets  X  Maximum entropy 1 Introduction data mining. The major drawback is that, given a dataset, there are exponential number of itemsets. Hence, we need to rank itemsets in order to prune the uninteresting ones.
Traditionally, the frequency of an itemset is used as a rank measure. The higher the frequency, the more significant is the itemset. Frequency has many virtues: It is easy to itemset may be insignificant: An itemset AB may be frequent just because itemsets A and B are frequent. Second, an infrequent itemset may be significant: If itemsets A and B are frequent, the infrequency of AB is interesting information.
 the frequency of an itemset to an estimate obtained from the independence model. That is, the more the itemset deviates from the independence model, the more surprising, and thus the more significant, the itemset is.

Our proposal for ranking itemsets resembles the aforementioned approaches. We esti-mate the frequency of a given itemset from the frequencies of some selected sub-itemsets. Namely, we use Maximum Entropy for the estimation. This approach is more flexible than the independence model, since the independence model uses only the margins (the frequen-tools, no similar framework has been suggested previously.

Unlike the frequency, our measure is not decreasing with respect to set inclusion. Hence a statistical test, thus providing a clear interpretation for the measure.
The rest of the paper is organized as follows: Preliminaries are given in Sect. 2 .The definition and the properties of the measure are given in Sect. 3 .Wepresentrelatedworkin 2 Preliminaries and notation be used later on.
 A binary dataset D is a collection of M binary vectors, transactions , having length K . A to a real number between 0 and 1 such that  X   X  p ( X ) = 1. Given an itemset X ,a X . We denote this by
A family of itemsets F is called anti-monotonic or downward closed if every subset of is, itemsets having frequency larger than some given threshold  X  , is downward closed. We are interested in three particular families:  X  I , the family containing only itemsets of size 1.  X  C , the family containing itemsets of size 1 and 2.  X  A , the family containing all itemsets.

A negative border negbord ( F ) of the downward closed family F is the set of itemsets Y  X  X such that Y /  X  F .

Given a dataset D , we say that an itemset X is derivable if by knowing the frequencies (calculated from D ) of each proper subset of X we can deduce the frequency of X .For example, if some subset of X has a frequency 0, then we know that X must also have non-derivable . A family of all non-derivable itemsets is downward closed [ 8 ]. 3 Maximum entropy ranking fundamental idea behind our approach is to measure how surprising an itemset is compared done using maximum entropy method and the comparison is done using Kullback X  X eibler divergence. 3.1 Definition D G by keeping only the attributes included in G .
 distribution q G : G  X  [ 0 , 1 ] to be
Our goal is to compare the distribution q G to a distribution obtained by using maximum entropy [ 23 ], a method that we will describe next.

Assume now that we are given a family of itemsets F  X  A and let  X  X be the frequency we are only interested in subsets of G . Hence we define a projected family F G to be Note that F G may contain 2 | G |  X  2 itemsets, at maximum. This is the case if F = A . X  X  F G and its frequency  X  X we have q G  X  P . We select the distribution from P maximizing the entropy these variables from the notation for the sake of clarity.
 We omit D from the notation when the dataset is clear from the context. Example 1 Assume the simplest case where G = a is an itemset of size 1. Let  X  G be the frequency of G . Note that F G = X  , hence there are no constraints on selecting p  X  .This measure is obtaining its minimum when  X  G = 1 / 2 and is at its maximum when  X  G = 0or  X  G = 1. independence model.
 the non-zero entries of p  X  3.2 Properties Theorem 2 Let G be a derivable itemset. Then derive the frequency of G from A G , it follows that P = { q G } , and hence p  X  = q G .
We can reformulate the previous theorem in a stronger form by pointing out that we need to know only non-derivable itemsets.
 Theorem 3 Let F be a family of all non-derivable itemsets. Let G be outside of F .Then r ( G ; F ) = 0 .
 Proof Since all unknown sub-itemsets of G are derivable from F G , the argument of Theorem 2 holds.
 we can use r ( G ) as a statistical test.
 Theorem 4 Let G be a non-derivable itemset. Under the 0-hypothesis that G is distributed 1 of freedom.
 Theorem 4 is a special case of the following more general statement.
 Theorem 5 Let G be a non-derivable itemset and let F be an itemset family. Define H to be asymptotically as  X  2 with degree | H | = 2 | G |  X  1  X  | F G | of freedom. is provided in Appendix A.

Theorem 5 motivates us to define the normalised rank measure to be the one-sided  X  2 test, that is, 2 in Table 1 .
 ting the measures.
 Lemma 6 Let p  X  be the maximum entropy distribution for itemsets F and the corresponding frequencies  X  . Let q be a distribution satisfying the itemsets F . Then we have Corollary 7 Let F be the family of itemsets. We have that where p  X  is the maximum entropy distribution and q G is the empirical distribution. that q
G is the empirical distribution. Corollary 9 Let F , H be the families of itemsets such that H  X  F . We have that 3.3 Flexible models rank.

Our first rank measure is the optimal tree model. A tree model can be described as a tree from G and the itemsets of size 2 corresponding to the edges of the tree. sponding family of itemsets is T = { a , b , c , d , e , ab , ac , ad , de } .
We can show that the Maximum Entropy distribution for T has the form minimises the rank, that is, the rank measure as term is equivalent to finding maximum spanning tree in the mutual information graph. This can be done in polynomial time [ 9 ].
 in [ 19 ]. We can rewrite, by applying Corollary 7 , the rank as joint distribution cannot be explained even by the best tree model.

Our second model involves in finding a downward closed family F of itemsets that pro-duces the smallest normalised rank. Note that Corollary 9 implies that the rank decreases when we increase the number of known itemsets. However, this does not hold for the norma-the optimal downward closed family. Hence, we suggest a simple greedy approach. We start denote the resulting family.
 Algorithm 1 Greedy algorithm for finding the optimal downward closed family of item sets. that produces low rank for the itemset 3.4 Computing rank Corollary 7 allows us to rewrite the rank as a difference of two entropies positive entries at maximum, hence the term H ( q G ) can be computed efficiently.
The challenge in calculating the measure is to solve the Maximum Entropy distribution p  X  and calculate its entropy. This can be done in polynomial time for the independence reasonable size. The summary for evaluation times is provided in Table 1 . 3.5 The effect of pruning itemsets change the prediction as the following example demonstrates.
 Example 11 Assume that we have 3 attributes, a , b ,and c . Our known itemsets are F = { a Entropy distribution is the uniform distribution. The empirical distribution is we would have concluded that a = b and that the Maximum Entropy distribution is equal to the empirical distribution, hence the rank would have been 0.
 from the Gaussian model. 4 Related work measures that resemble the support are studied in [ 26 ].

Our work resembles approach of [ 6 ] in which the authors defined the significance of an model, the rank measure is but in addition they use Bayes screening to smooth the values. Also, in [ 1 ] the authors related to the independence model. Then the measure is This measure obtains small values when data obeys the independence model. In a related many measures has been suggested for ranking association rules [ 2 , 7 , 20 , 29 ].
The authors in [ 28 ] showed empirically that Maximum entropy model provides excellent estimation algorithm and a loss function. In [ 25 ] the authors use information component analysis to find patterns in a drug safety database. 5 Experiments different measures are related to each other, and the monotonicity of the ranks. 5.1 Synthetic datasets For the testing purposes we created two synthetic datasets. Each dataset contained 100 gen-copy , each column was a copy of the previous column corrupted by the symmetric white noise. The amount of noise, that is the probability generated by a coin flip. Our expectations are that in gen-ind the itemsets of size 1 are significant and that in gen-copy the itemsets of size 2 are significant. 5.2 Real datasets In our experiments we used the following real-world datasets. Data in Accidents 1 were obtained from the Belgian  X  X nalysis Form for Traffic Accidents X  forms that is filled out by a police officer for each traffic accident that occurs with injured or deadly wounded months worth of click-stream data from two e-commerce web sites. Kosarak 5 consists of basketdatasuppliedbyananonymousBelgianretailsupermarketstore[ 5 ].Thedataset Pa l e o 7 preprocessed as in [ 16 ]. 5.3 Setup for the experiments In this section we will describe how we conducted our experiments. We reduced the largest datasets by selecting the first 10 , 000 rows and 200 most frequent attributes. From each dataset we computed all almost non-derivable itemsets. By almost non-derivable we mean that the difference between the upper bound and the lower bound of a given itemset, say of G , then we cannot predict the frequency of G within n transactions. If n = 0, then an other reason is that we want to study how the measure behaves for infrequent itemsets.
To keep the sizes of the obtained families within reasonable bounds we used different
For each itemset from the obtained itemsets we queried the following measures:  X  Frequency.
The evaluation times and the sizes of the query families are given in Table 2 . 5.4 Significant itemsets Our first experiment is to study how many of the itemsets are significant. We did this by We also provide a typical example of box plots in Fig. 2 .
 Let us first study gen-ind , a synthetic dataset with independent columns. We see from distribution is not an independent model (although close to one). ranks, however, these ranks tend to be slightly larger than the ranks of nr ( G ; T  X  ) .
We turn our attention to real datasets. We see that for these datasets the independence change drastically, when we use richer models. According to nr ( G ; A ) only about 5 X 50% nr ( G ; A ) tends to produce higher values than nr ( G ; C ) but not in POS . 5.5 The effect of the known itemsets nr between the rank measures. The results are given in Tables 6 and 7 .
From the results we see that all correlations are positive. For the real datasets the nr supports the behaviour we have seen in Sect. 5.4 .
 than the correlation between nr ( G ; F  X  ) and nr ( G ; I ) . 5.6 Flexible models with many attributes.
 modelsoutperform nr ( G ; I ) ,however,theperformanceagainstothermeasuredependsonthe optimal family F  X  .

We studied the sizes of itemsets occurring in F  X  , the family of known itemsets in nr ( G ; F  X  ) .Tobemoreprecise,let F  X  L be the size of itemsets we are interested in. We define the ratio r L to be frequently used, however, the itemsets of larger size are rarely used. 5.7 Rank versus other methods relationships by plotting our measures as functions of the aforementioned approaches and such examples are given in Fig. 3 .
 nr r (
G ) and the rest of the measures although this correlation is much weaker compared to nr ( G ; I ) .
 5.8 Monotonicity of rank nr that is, decreasing w.r.t. set inclusion.
 anti-monotonic in more queries than nr ( G ; C ) . 6 Conclusions We have given a definition of a measure for ranking itemsets. The idea is to predict the between the actual frequency and the prediction. The more the itemset deviates from the can be solved by finding the optimal spanning tree in the mutual information matrix. For solving r ( G ; F  X  ) we proposed a simple greedy approach.

A clear advantage of our approach to the previous methods is that the previous soluti-ons calculate the deviation from the independence model whereas we are able to use the Our empirical results for real data show that the independence is too strict assumption: information.
 and r ( G ; A ) are anti-monotonic for a significant portion of itemsets. Appendix A. Asymptotic behaviour of the divergence By asymptotic behaviour we mean the following: We assume that we have an ensemble of frequencies of F G are all equal.
 Define N = | D | and M = | H | .Let P be the set of distributions satisfying the itemsets F that  X  i = p ( H i = 1 ) .Let be the set of all possible frequency vectors. The set is a closed polytope X  X he vectors located on the boundary of corresponds to the distributions in which at least one entry is 0.
 Let  X   X  be a frequency vector corresponding to the Maximum Entropy distribution p  X  . p  X  ( X ) = 0forsome  X  .Weknowthatthisimpliesthat p ( X ) = 0forall p  X  P [ 11 ,Theorem3.1]. (see [ 8 ]) implies that for each p  X  P making G derivable and contradicting the statement.
 Since  X   X  is an inner point of ,let B  X  be an open ball around  X   X  . Assume that  X   X  B . arrive to where  X  =  X   X   X   X  and  X  is a vector lying between  X  and  X   X  ,and H is the Hessian matrix of log p ( X  ;  X ) .

Let  X  N be the frequencies of H obtained from a dataset containing N points. According to 0-hypothesis we have  X  N  X   X  and matrix, If  X  N  X  B ,welet  X  N correspond to  X  in the Taylor expansion, otherwise we set  X  N = 0. We can show that  X  N  X   X  [ 33 , Theorem 2.7]. Consider a function [ 33 , Theorem 2.3] to obtain that  X   X  1 [ 23 , Lemma 4.11]. Theorem follows since X T  X  1 X is distributed as  X  2 with M degrees of freedom [ 33 , Lemma 17.1]. References Author Biography
