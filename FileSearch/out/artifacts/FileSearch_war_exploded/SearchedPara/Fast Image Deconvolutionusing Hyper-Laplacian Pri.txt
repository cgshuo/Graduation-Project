 Natural image statistics are a powerful tool in image proces sing, computer vision and computational photography. Denoising [14], deblurring [3], transparenc y separation [11] and super-resolution [20], problems to yield high-quality results. However, digital c ameras now have sensors that record im-ages with tens of megapixels (MP), e.g. the latest Canon DSLR s have over 20MP. Solving the above to existing algorithms. In this paper we focus on one particu lar problem: non-blind deconvolution, and propose an algorithm that is practical for very large ima ges while still yielding high quality results.
 Numerous deconvolution approaches exist, varying greatly in their speed and sophistication. Simple filtering operations are very fast but typically yield poor r esults. Most of the best-performing ap-outputs to match those of uncorrupted images, which act as a p rior to regularize the problem. For these methods, a trade-off exists between accurately model ing the image statistics and being able to solve the ensuing optimization problem efficiently. If the m arginal distributions are assumed to be Gaussian, a closed-form solution exists in the frequency do main and FFTs can be used to recover the image very quickly. However, real-world images typically h ave marginals that are non-Gaussian, as shown in Fig. 1, and thus the output is often of mediocre quali ty. A common approach is to assume the marginals have a Laplacian distribution. This allows a n umber of fast  X  methods [17, 22] to be deployed, which give good results in a r easonable time. However, studies Figure 1: A hyper-Laplacian with exponent  X  = 2 / 3 is a better model of image gradients than a Laplacian or a Gaussian. Left: A typical real-world scene. Right: The empirical distribution particularly in the tails. of real-world images have shown the marginal distributions have significantly heavier tails than a Laplacian, being well modeled by a hyper-Laplacian [4, 10, 1 8]. Although such priors give the best quality results, they are typically far slower than methods that use either Gaussian or Laplacian pri-ors. This is a direct consequence of the problem becoming non -convex for hyper-Laplacians with optimization methods such as conjugate gradient (CG) must b e used. One variant that works well squares problems with CG, each one an  X  point. In both cases, typically hundreds of CG iterations ar e needed, each involving an expensive convolution of the blur kernel with the current image estima te.
 In this paper we introduce an efficient scheme for non-blind d econvolution of images using a hyper-Laplacian image prior for 0 &lt;  X   X  1 . Our algorithm uses an alternating minimization scheme whe re the non-convex part of the problem is solved in one phase, fol lowed by a quadratic phase which can be efficiently solved in the frequency domain using FFTs. We f ocus on the first phase where at each pixel we are required to solve a non-convex separable minimi zation. We present two approaches to solving this sub-problem. The first uses a lookup table (LUT) ; the second is an analytic approach specific to two values of  X  . For  X  = 1 / 2 the global minima can be determined by finding the roots of a cubic polynomial analytically. In the  X  = 2 / 3 case, the polynomial is a quartic whose roots can also be found efficiently in closed-form. Both IRLS and our approach solve a series of approximations to the original problem. However, in our met hod each approximation is solved by alternating between the two phases above a few times, thus av oiding the expensive CG descent used by IRLS. This allows our scheme to operate several orders of m agnitude faster. Although we focus on the problem of non-blind deconvolution, it would be strai ghtforward to adapt our algorithm to other related problems, such as denoising or super-resolut ion. 1.1 Related Work Hyper-Laplacian image priors have been used in a range of set tings: super-resolution [20], trans-and Joshi et al. [7] have applied them to non-blind deconvolution problems u sing IRLS to solve for the deblurred image. Other types of sparse image prior inclu de: Gaussian Scale Mixtures (GSM) [21], which have been used for image deblurring [3] and denoi sing [14] and student-T distributions for denoising [25, 16]. With the exception of [14], these met hods use CG and thus are slow. The alternating minimization that we adopt is a common techn ique, known as half-quadratic split-ting, originally proposed by Geman and colleagues [5, 6]. Re cently, Wang et al. [22] showed how it could be used with a total-variation (TV) norm to deconvolve images. Our approach is closely re-lated to this work: we also use a half-quadratic minimizatio n, but the per-pixel sub-problem is quite different. With the TV norm it can be solved with a straightfo rward shrinkage operation. In our work, as a consequence of using a sparse prior, the problem is non-convex and solving it efficiently is one of the main contributions of this paper.
 Chartrand [1, 2] has introduced non-convex compressive sen sing, where the usual  X  signal to be recovered is replaced with a  X  splitting scheme is used, resulting in a non-convex per-pix el sub-problem. To solve this, a Huber approximation (see [1]) to the quasi-norm is used, allowing the derivation of a generalized shrinkage operator to solve the sub-problem efficiently. However, thi s approximates the original sub-problem, unlike our approach. We now introduce the non-blind deconvolution problem. x is the original uncorrupted linear grayscale image of N pixels; y is an image degraded by blur and/or noise, which we assume to be produced by convolving x with a blur kernel k and adding zero mean Gaussian noise. We as-sume that y and k are given and seek to reconstruct x . Given the ill-posed nature of the task, we regularize using a penalty function | . |  X  that acts on the output of a set of filters f to x . A weighting term  X  controls the strength of the regularization. From a probabi listic perspec-to minimizing the cost  X  log p ( x | y , k ) : where i is the pixel index, and  X  is the 2-dimensional convolution operator. For simplicity , we use two first-order derivative filters f easily be added (e.g. learned filters [13, 16], or higher orde r derivatives). For brevity, we denote F x  X  ( x  X  f j ) i for j = 1 , .., J .
 Using the half-quadratic penalty method [5, 6, 22], we now in troduce auxiliary variables w 1 (together denoted as w ) at each pixel that allow us to move the F j giving a new cost function: where  X  is a weight that we will vary during the optimization, as desc ribed in Section 2.3. As  X   X   X  , the solution of Eqn. 2 converges to that of Eqn. 1. Minimizin g Eqn. 2 for a fixed  X  can be performed by alternating between two steps, one where we s olve for x , given values of w and vice-versa. The novel part of our algorithm lies in the w sub-problem, but first we briefly describe the x sub-problem and its straightforward solution. 2.1 x sub-problem where K x  X  x  X  k . Assuming circular boundary conditions, we can apply 2D FFT  X  X  which diago-nalize the convolution matrices F 1 , F 2 , K , enabling us to find the optimal x directly: where  X  is the complex conjugate and  X  denotes component-wise multiplication. The division is al so performed component-wise. Solving Eqn. 4 requires only 3 FF T X  X  at each iteration since many of the terms can be precomputed. The form of this sub-problem is identical to that of [22]. 2.2 w sub-problem Given a fixed x , finding the optimal w consists of solving 2 N independent 1D problems of the form: where v  X  F j 2.2.1 Lookup table For a fixed value of  X  , w  X  in Eqn. 5 only depends on two variables,  X  and v , hence can easily be tabulated off-line to form a lookup table. We numerically so lve Eqn. 5 for 10 , 000 different values of v over the range encountered in our problem (  X  0 . 6  X  v  X  0 . 6 ). This is repeated for different  X  values, namely integer powers of  X  2 between 1 and 256 . Although the LUT gives an approximate solution, it allows the w sub-problem to be solved very quickly for any  X  &gt; 0 . 2.2.2 Analytic solution For  X  = 2 , the sub-problem is quadratic and thus easily solved. If  X  = 1 , Eqn. 5 reduces to a 1-D Here, we address the more challenging case of  X  &lt; 1 and we now describe a way to solve Eqn. 5 w to zero gives: For  X  = 1 / 2 , this becomes, with successive simplification: need only consider one of these as v is fixed and w  X  must lie between 0 and v . Hence we can replace sign ( w ) with sign ( v ) in Eqn. 9: For the case  X  = 2 / 3 , using a similar derivation, we arrive at: is either 0 or a root of the cubic polynomial in Eqn. 10 for  X  = 1 / 2 , or equivalently a root of the quartic polynomial in Eqn. 10 for  X  = 2 / 3 . Although it is tempting to try the same manipulation for  X  = 3 / 4 , this results in a 5 th order polynomial, which can only be solved numerically. Finding the roots of the cubic and quartic polynomials: Analytic formulae exist for the roots of cubic and quartic polynomials [23, 24] and they form the ba sis of our approach, as detailed in Algorithms 2 and 3. In both the cubic and quartic cases, the co mputational bottleneck is the cube root operation. An alternative way of finding the roots of the polynomials Eqn. 10 and Eqn. 11 is to use a numerical root-finder such as Newton-Raphson. In our experiments, we found Newton-Raphson to be slower and less accurate than either the analyt ic method or the LUT approach (see [8] for futher details).
 Selecting the correct roots: Given the roots of the polynomial, we need to determine which one corresponds to the global minima of Eqn. 5. When  X  = 1 / 2 , the resulting cubic equation can have: the | w |  X  term means Eqn. 5 has positive derivatives around 0 and the lack of real roots implies the derivative never becomes negative, thus w  X  = 0 . For (b), we need to compare the costs of the single real root and w = 0 , an operation that can be efficiently performed using Eqn. 13 below. In (c) we have 3 real roots. Examining Eqn. 7 and Eqn. 8, we see that the squari ng operation introduces a spurious root above v when v &gt; 0 , and below v when v &lt; 0 . This root can be ignored, since w  X  must lie between 0 and v . The cost function in Eqn. 5 has a local maximum near 0 and a local minimum between this local maximum and v . Hence of the 2 remaining roots, the one further from 0 will have a lower cost. Finally, we need to compare the cost of this root with that of w = 0 using Eqn. 13.
 We can use similar arguments for the  X  = 2 / 3 case. Here we can potentially have: (a) 4 imaginary (b), we pick the larger of the 2 real roots and compare the costs with w = 0 using Eqn. 13, similar was derived with a cubing operation from the analytic deriva tive. This introduces 2 spurious roots into the final solution, both of which are imaginary, thus onl y cases (a) and (b) are possible. In both the cubic and quartic cases, we need an efficient way to pick between w = 0 and a real root that is between 0 and v . We now describe a direct mechanism for doing this which does not involve Let r be the non-zero real root. 0 must be chosen if it has lower cost in Eqn. 5. This implies: from Eqn. 6 and Eqn. 12, yielding the condition: coded, e.g. lines 12 X 16 of Algorithm 2. Overall, the analyti c approach is slower than the LUT, but it gives an exact solution to the w sub-problem. 2.3 Summary of algorithm We now give the overall algorithm using a LUT for the w sub-problem. As outlined in Algorithm 1 below, we minimize Eqn. 2 by alternating the x and w sub-problems T times, before increasing the value of  X  and repeating. Starting with some small value  X  although more can sometimes be needed when  X  is small.
 Algorithm 1 Fast image deconvolution using hyper-Laplacian priors As with any non-convex optimization problem, it is difficult to derive any guarantees regarding the convergence of Algorithm 1. However, we can be sure that the g lobal optimum of each sub-problem will be found, given the fixed x and w from the previous iteration. Like other methods that use schedule. We find that the simple scheme shown in Algorithm 1 w orks well to minimize Eqn. 2 and its proxy Eqn. 1. The experiments in Section 3 show our scheme achieves very similar SNR levels to IRLS, but at a greatly lower computational cost. We evaluate the deconvolution performance of our algorithm on images, comparing them to numer-ous other methods: (i)  X  an  X  IRLS scheme, we used the implementation of [10] with default parameters, the only change being the removal of higher order derivative filters to enable a dir ect comparison with other approaches. Note that IRLS and  X  [22] minimize the cost in Eqn. 2, using T = 1 ,  X  we use  X  = 1 / 2 and  X  = 2 / 3 , and compare the performance of the LUT and analytic methods as well. All runs were performed with multithreading enabled ( over 4 CPU cores). We evaluate the algorithms using a set of blurry images, crea ted in the following way. 7 in-focus grayscale real-world images were downloaded from the web. T hey were then blurred by real-world camera shake kernels from [12]. 1% Gaussian noise was added, followed by quantization to 255 discrete values. In any practical deconvolution setting th e blur kernel is never perfectly known. Therefore, the kernel passed to the algorithms was a minor pe rturbation of the true kernel, to mimic kernel estimation errors. In experiments with non-perturb ed kernels (not shown), the results are similar to those in Tables 3 and 1 but with slightly higher SNR levels. See Fig. 2 for an example of a kernel from [12] and its perturbed version. Our evaluation m etric was the SNR between the original image  X  x and the deconvolved output x , defined as 10 log In Table 1 we compare the algorithms on 7 different images, al l blurred with the same 19  X  19 kernel. For each algorithm we exhaustively searched over different regularization weights  X  to find the value that gave the best SNR performance, as reported in the table. In Table 3 we evaluate the algorithms with the same 512  X  512 image blurred by 8 different kernels (from [12]) of varyi ng size. Again, the optimal value of  X  for each kernel/algorithm combination was chosen from a ran ge of values based on SNR performance. Table 2 shows the running time of se veral algorithms on images up to 3072  X  3072 pixels. Figure 2 shows a larger 27  X  27 blur being deconvolved from two example images, comparing the output of different methods.
 The tables and figures show our method with  X  = 2 / 3 and IRLS with  X  = 4 / 5 yielding higher quality results than other methods. However, our algorithm is around 70 to 350 times faster than IRLS depending on whether the analytic or LUT method is used. This speedup factor is independent of image size, as shown by Table 2. The  X  of comparable speed to ours but achieving lower SNR scores. T he SNR results for our method are almost the same whether we use LUTs or analytic approach. Hen ce, in practice, the LUT method is preferred, since it is approximately 5 times faster than the analytic method and can be used for any value of  X  .
 Table 1: Comparison of SNRs and running time of 9 different me thods for the deconvolution of 7 576  X  864 images, blurred with the same 19  X  19 kernel. L=Lookup table, A=Analytic. The best performing algorithm for each kernel is shown in bold. Our al gorithm with  X  = 2 / 3 beats IRLS with  X  = 4 / 5 , as well as being much faster. On average, both these methods outperform  X  strating the benefits of a sparse prior.
 Table 2: Run-times of different methods for a range of image s izes, using a 13  X  13 kernel. Our LUT algorithm is more than 100 times faster than the IRLS method of [10]. high quality results. Our algorithm takes a novel approach t o the non-convex optimization prob-Original Blurred SNR=7.31 Figure 2: Crops from two images (#1 &amp; #5) being deconvolved by 4 different algorithms, including ours using a 27  X  27 kernel (#7). In the bottom left inset, we show the original kernel from [12] (lower) and the perturbed version provided to the algorithm s (upper), to make the problem more realistic. This figure is best viewed on screen, rather than i n print. Table 3: Comparison of SNRs and running time of 9 different me thods for the deconvolution of a 512  X  512 image blurred by 7 different kernels. L=Lookup table, A= Analytic. Our algorithm beats our algorithm is far faster than IRLS, being comparable in sp eed to the  X  lem arising from the use of a hyper-Laplacian prior, by using a splitting approach that allows the non-convexity to become separable over pixels. Using a LUT t o solve this sub-problem allows for orders of magnitude speedup in the solution over existing me thods. Our Matlab implementation is available online at http://cs.nyu.edu/  X  dilip/wordpress/?page_id=122 .
 A potential drawback to our method, common to the TV and  X  frequency domain operations which assume circular boundar y conditions, something not present in real images. These give rise to boundary artifacts which can be overcome to some extend with edge tapering operations. However, our algorithm is suitable fo r very large images where the boundaries are a small fraction of the overall image.
 Although we focus on deconvolution, our scheme can be adapte d to a range of other problems which rely on natural image statistics. For example, by setting k = 1 the algorithm can be used to denoise, or if k is a defocus kernel it can be used for super-resolution. The s peed offered by our algorithm makes it practical to perform these operations on the multi-megapixel images from modern cameras. Algorithm 2: Solve Eqn. 5 for  X  = 1 / 2 [1] R. Chartrand. Fast algorithms for nonconvex compressiv e sensing: Mri reconstruction from [2] R. Chartrand and V. Staneva. Restricted isometry proper ties and nonconvex compressive sens-[3] R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. Fre eman. Removing camera shake [4] D. Field. What is the goal of sensory coding? Neural Computation , 6:559 X 601, 1994. [5] D. Geman and G. Reynolds. Constrained restoration and re covery of discontinuities. PAMI , [6] D. Geman and C. Yang. Nonlinear image recovery with half-quadratic regularization. PAMI , [7] N. Joshi, L. Zitnick, R. Szeliski, and D. Kriegman. Image deblurring and denoising using color [8] D. Krishnan and R. Fergus. Fast image deconvolution usin g hyper-laplacian priors, supple-[9] A. Levin. Blind motion deblurring using image statistic s. In NIPS , 2006. [10] A. Levin, R. Fergus, F. Durand, and W. Freeman. Image and depth from a conventional camera [11] A. Levin and Y. Weiss. User assisted separation of reflec tions from a single image using a [12] A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Underst anding and evaluating blind decon-[13] S. Osindero, M. Welling, and G. Hinton. Topographic pro duct models applied to natural scene [14] J. Portilla, V. Strela, M. J. Wainwright, and E. P. Simon celli. Image denoising using a scale [15] W. Richardson. Bayesian-based iterative method of ima ge restoration. 62:55 X 59, 1972. [16] S. Roth and M. J. Black. Fields of Experts: A Framework fo r Learning Image Priors. In CVPR , [17] L. Rudin, S. Osher, and E. Fatemi. Nonlinear total varia tion based noise removal algorithms. [18] E. Simoncelli and E. H. Adelson. Noise removal via bayes ian wavelet coring. In ICIP , pages [19] C. V. Stewart. Robust parameter estimation in computer vision. SIAM Reviews , 41(3):513 X 537, [20] M. F. Tappen, B. C. Russell, and W. T. Freeman. Exploitin g the sparse derivative prior for [21] M. Wainwright and S. Simoncelli. Scale mixtures of gaus sians and teh statistics of natural [22] Y. Wang, J. Yang, W. Yin, and Y. Zhang. A new alternating m inimization algorithm for total [23] E. W. Weisstein. Cubic formula. http://mathworld.wolfram.com/ [24] E. W. Weisstein. Quartic equation. http://mathworld.wolfram.com/ [25] M. Welling, G. Hinton, and S. Osindero. Learning sparse topographic representations with [26] S. Wright, R. Nowak, and M. Figueredo. Sparse reconstruc tion by separable approximation.
