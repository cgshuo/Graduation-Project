 Distance metric learning is an old problem that has been re-searched in the supervised learning  X eld for a very long time. In this paper, we consider the problem of learning a proper distance metric under the guidance of some weak supervi-sory information. Speci X cally, those information are in the form of pairwise constraints which specify whether a pair of data points are in the same class( must link constraints) or in the di X erent classes( cannot link constraints). Given those constraints, our algorithm aims to learn a distance metric under which the points with must link constraints are pushed as close as possible, while simultaneously the points with cannot link constraints are pulled away as far as possible. Finally the experimental results are presented to show the e X ectiveness of our method.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval| Clustering ; I.2.6 [ Arti X cial Intelli-gence ]: Learning Algorithms Metric Learning, Constraint Margin
Learning a good distance metric in feature space is crucial in many models and algorithms in machine learning, com-puter vision and data mining  X elds. Some typical examples include the k -Nearest Neighbor algorithm for supervised clas-si X cation and the kmeans algorithm for unsupervised cluster-ing . However, most of the existing algorithms use Euclidean distances for measuring the dissimilarities between pairs of data points, which may not be appropriate since the intrin-sic space that many data sets lie in are not Euclidean. That's the reason why in many cases those algorithms cannot give satisfactory results.

In this paper, we propose a novel distance metric learning algorithm from both the similarity constraints (which will be called must-links ), and dissimilarity constraints (which will be called cannot-links ). Like many traditional embed-ding based algorithms, our method seeks a linear subspace in which the constraint margin of the data set is maximized. Moreover, we also give a criterion to estimate the optimal dimensionality of such a linear space automatically. Finally the experimental results are represented to show the e X ec-tiveness of our algorithm.
In this section, we introduce our distance metric learning algorithm in detail. Assuming we are given a set of data points X = f x 1 ; : : : ; x N g ; 8 x i 2 R D together with a set of must-link constraints M and a set of cannot-link constraints C de X ned as follows: Our goal is to learn a Mahalanobis distance metric d (  X  ;  X  ) such that the distance between x i and x j can be computed from: where A is a D  X  D square matrix.

Before we go into the details of our algorithm,  X st let's introduce a preliminary concept called constraint margin .
Definition 1. ( Constraint Margin ) . Given the must/ cannot-link constraint sets M and C for data set X , we can de X ne the constraint margin on data set X as  X  = X where N M and N C represent the numbers of must links and cannot links respectively.
 Since our goal is to learn the parameter matrix A , which is symmetric and positive semi-de X nite , thus we can  X nd a non-square matrix W of size d  X  D , with d  X  D , such that Then the Mahalanobis distance between x i and x j induced by A becomes d ( x i ; x j ) = p ( y i  X  y j ) T ( y i  X  y W
T x i 2 R d . Thus W actually de X nes a mapping from the high-dimensional input space to a low-dimensional embed-ding. Euclidean distance in this low-dimensional embedding is equiv alent to the Mahalanobis distance in the original in-put space under a rank-de X cien t metric ( A has now rank at most d ). In this way,  X nding an optimal distance metric is equiv alent to  X nding an optimal projection matrix.
Now we can presen t our constraint margin maximization ( CMM ) criterion for learning distance metric. Com bining Eq.(3) and Eq.(2), we can get that
The goal of CMM is to seek for an optimal A which can induce a metric maximizing  X  . Equiv alently, CMM aims to  X nd an optimal projection matrix W such that in the pro-jected space, the constraint margin of X reaches its max-imum. We can also decomp ose  X  to be the sum of point constraint margin as in the previous subsection, then Figure 1 provides us an intuitiv e illustration of the CMM criterion.
We can further expand  X  as where tr (  X  ) denotes the trace of a matrix, and are called the constraint scatterness matrix and constraint compactness matrix respectiv ely. Therefore, based on the above presen tations, we can form ulate our algorithm as where we have add the constrain t W T W = I to restrict the scale of W . By applying the Ky Fan theorem, we know that (1) the solutions to Eq.(7) is the largest d eigen vectors of C  X  M ; (2) the optimal value of the objectiv e function is the sum of the largest d eigen values of C  X  M .

Another issue is that in practical applications, we usually only have a small number of constrain ts, and there are still a large amoun t of unlab eled data. It would be valuable if we can utilize those unlab eled data points to enhance the power of our metho d. Inspired by the work in [5], we may want the data in the embedded space to have the maxim um variance. Therefore we can de X ne the objectiv e function as: J = 1 where is the total scatterness matrix of the data set X . The scaling parameter  X  is use to balance the contribution of the vari-ance and the margin. In this case, our optimization problem
Figure 1: Experimen ts with di X eren t algorithms. becomes From the Ky Fan theorem we know that the solution to the above algorithm corresp onds to the largest d eigen vectors of S +  X  ( C  X  M ), and d is equiv alent to the number of positiv e eigen values of S +  X  ( C  X  M ).
In this section, we conduct a set of experimen ts to show the e X ectiv eness of the CMM . In our experimen ts, the con-strain ts were generated as follows: for each constrain t, we picked out one pair of data points randomly from the input data sets (the labels of which were available for evaluation purp ose but unavailable for clustering). If the labels of this pair of points were the same, then we generated a must link. If the labels were di X eren t, a cannot link was generated. The amoun ts of constrain ts were determined by the size of input data. In all the experimen ts, the balance parameter  X  was set to be 5 and the results were averaged over 50 trials to eliminate the di X erence caused by constrain ts.

In this set of experimen ts, we  X rst learn a distance metric by our linear CMM metho d, and then cluster data set by kmeans based on the learned metric. The clustering results were compared with three other algorithms: (1) K-Means using the Euclidean metric in the input data space; (2) K-Means using the Euclidean metric in the subspace obtained by PCA ; (3) Constrained K-Means [3] using the traditional K-Means but assigning the same labels to the points with must link while assigning di X eren t labels to the ones with cannot links. All the four algorithms were tested on the UCI data sets Balanc e and Iris [1], and the F-score [2] was used to evaluate the performance of each algorithm. The results are shown in Fig.1, from which we can clearly see the e X ectiv eness of our metho d.
