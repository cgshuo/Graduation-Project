
Several typical problems in natural language processing (NLP) can be seen as the task of assigning labels to words in a text sequence. It is quite difficult to obtain labeled training sentences with word-level annotations compared with document-level classification tasks (such as text catego-rization), because hand-labeling individual words and word boundaries is much harder than assigning article-level class labels [1]. Supervised techniques have yielded great success in the NLP community, even though they are restricted by the expense of annotating the corpus. Semi-supervised learn-ing has become one of the most natural forms of training for language processing tasks, since unlabeled language data is plentiful in this field.

Self-training [2], [3], [4], and co-training [5], [6] are pop-ular semi-supervised methods applied in natural language processing tasks. They utilize large sets of unlabeled corpora and try to improve over supervised methods by iteratively adding self-labeled examples predicted by the current model. However, they are vulnerable to the incestuous training bias problem [7], [8], i.e. examples may be consistently misla-beled making the model even worse on the next iteration. To combat this, several authors have proposed schemes for only adding examples that meet a selection criterion [4], [7], [9], but these heuristic choices still might yield unreliable results.

In this paper 1 we propose a simple and scalable semi-supervised strategy that works by providing semi-supervision at the level of feature attributes rather than examples . Under the assumption that words (and word attributes) carry rich label information, we learn to derive a group of features (which we name  X  X elf-learned features X  (SLF)) that are related to the distribution of target classes through a large unlabeled corpus. These distribution patterns are used as extra features to retrain our base supervised classifier in an iterative fashion. Basic SLF learns to model words X  probabilities to every possible target class. We also propose several extensions to our basic approach: (1) Bound-ary SLF learns to represent words X  class boundary patterns; (2) Attribute SLF tries to model target class distribution patterns for each value of the discrete word attributes (for instance, capitalization properties of words); (3) Clustered SLF clusters word according to SLF values and the derived cluster ID would be used as novel extra features.
We tested our approach on four classic sequence labeling tasks: two CoNLL-2003 [11] shared tasks (German and English Named Entity Recognition (NER)), one English Part-of-Speech tagging [12] and the GM BioCreativeII com-petition (gene name recognition) [13]. Two state-of-the-art natural language processing systems are used as baseline supervised methods to train SLF: (a) a neural network (NN) model [14] for unified NLP (b) a conditional random field (CRF) [15] which has shown success in many information extraction tasks. We observed improvements from using SLF in various setups: compared to the baseline classifiers, to semi-supervised auxillary task and to self-training, whenever we applied it. In particular we achieved a state-of-the art result of 75.72 F1 on the German NER task and token error rate 2.73% on the English POS task. Moreover, SLF models exhibit robust behavior since noisy self-labeled examples are not added (as in self-training). It is also highly scalable to large unlabeled corpora (for instance, English Wikipedia).
Unlike most popular semi-supervised approaches (details in Section IV), we propose to induce features from a large corpus of unannotated examples in a supervised fashion, and then use these features to augment the feature space of the labeled set. Since this is an orthogonal method for improving accuracy, it can be combined with many of the other semi-supervised methods (section IV).

SLF relies on the key observation that individual words carry significant label information, since they are fundamen-tal building blocks of NLP systems. In the following, we describe our basic model: word-level SLF first, and then go on to describe several extensions.

We consider the setting where one is given labeled training examples { ( x i , y i ) } i =1 ,...,L  X  X  X Y and a large unlabeled set of examples { x  X  i } i =1 ,...,U  X  X  where the unlabeled set is much larger than the labeled one ( U L ).

In particular for the sequence labeling tasks which we focus on in this paper, X is the set of all sequences composed of elements which take on a finite set of possible values, e.g. sequences of words (or in the general case this could include other discrete types of word attributes as well, e.g. capital types, POS tags, stem-ends, etc.). That is, we will assume an input sequence x = ( x 1 , . . . , x | x | ) , where | x | means the length of the sequence. Its j th item is x j  X  X  , with D representing a dictionary of size |D| , for instance a vocabulary of English words. The labels Y X  X  1 , . . . , K } are the K classes a sequence can be assigned to, e.g. NER system labels atomic elements in the sentence into categories such as  X  X ERSON X ,  X  X OMPANY X , or  X  X OCATION X .
 A. Basic method: Word SLF
We define the basic self-learned features for a given word w  X  X  as a feature vector SLF ( w )  X  R K which models the probability to each target class this word might be assigned with, where That is, the i th dimension measures the probability of label y = i being assigned given that word w is present in the input sequence x (Figure 1). This distribution is of course unknown but can be estimated from the training set or, critically, can be re-estimated using an unlabeled corpus by applying a trained classifier.
 We thus define the empirical SLF for a given word as: where f (  X  ) is a classifier trained to predict y  X  X  given x  X  X  . In equation 2 the numerator counts the number of sequences that have been classified as type i and include the word w . The denominator describes the total number of sequences including the word w in the unlabeled corpus. Essentially this distribution measures the proportion of text sequences assigned as class i given word w is present. This distribution could be smoothed by a Bayesian Beta Prior in extreme cases (e.g. infrequent words) [16].

We hence propose the following iterative semi-supervised training algorithm (pseudo-code in Table I): 1) Define the feature representation  X  ( w ) for a word w , 2) Train a classifier f (  X  ) on training examples ( x i , y 3) Augment the representation of words with their SLF 4) Iterate steps 2-3 until performance does not improve. B. Modified Word SLF in Window-based Sequence Labeling
Many NLP tasks can be treated as sequence segmentation or sequence labeling where prediction performance is mea-sured with word-level evaluations. A classical way to handle these tasks is to utilize a window-based approach where the tagging algorithm considers a window of a fixed size around each word we want to label (Figure 1).

To apply SLF learning for this case, we propose the mod-ified self-learned features for words where we are interested in class distributions only for the words to be labeled: where
Here we only count the sequences (sliding text windows) whose middle word (with the index m ) matches to the word w . However, we still augment all words in the window with SLF (  X  ) features. This actually gives a pattern of possible context (neighboring) tags for the middle word of interest. C. Extension (1): Attribute SLF
In the basic SLF, we learn to derive the class-distribution features for each word in the vocabulary from a large unlabeled corpus. Besides words, there exist a number of word attributes that are important for typical NLP problems, such as capitalization properties of words, stem-ends and prefix strings. These attributes are normally discrete and from a finite dictionary of values. For instance, the attribute  X  X apitalization X  could have values of  X  X ES X  (the first letter of the word is capital letter) or  X  X O X .

Similar to words, some important attributes describing words also carry rich class information, because they are also essential building blocks of natural languages. We could treat them the same way as words and learn their SLF from the unlabeled set: we call this  X  X ttribute SLF X  (ASLF) learning.
Suppose an input example x is a sequence of words, plus the  X  X apitalization X  of each word. Assuming c represents the  X  X apitalization X  of each word, each element in the sequence x has the format x i = ( w, c ) , and Equation 1 is modified to handle c : We then augment the representation of words with both the word SLF and attribute SLF compared to Equation 3 in the iterative algorithm (Table I): This simple extension could be applied to any attribute that is discrete and from a finite dictionary of values. D. Extension (2): Boundary SLF
For NLP tasks like named entity recognition or gene name recognition, rare words (words with very low frequency) are normally the hardest examples to label in the sequence since the training set could hardly cover them. In this case, we could consider to model those words which happen frequently before (or after) a certain class label. Later, when a rare word needs to be labeled, its neighborhood words might be able to provide strong indications of its target class types if these words are always in the boundary of a certain class. Table II lists several examplar words that are usually very close to named entities (person names or gene names). Clearly some words carry strong class boundary information.
Based on the above observation, we extend the basic SLF strategy to incorporate the class bounary distributions. Basic SLF (Equation 1) models the probability of label y = i being assigned given the word w present in the input sequence x . Boundary SLF (BSLF) models how likely the word w is right before or after a certain class (assuming class t):
SLF 00 ( w ) t, 1 = P ( y i = t | w  X  X  ( x i ) 1 , . . . , ( x
SLF 00 ( w ) t, 2 = P ( y i = t | w  X  X  ( x i ) m +1 , . . . , ( x where m is the middle index in a given  X  X indow X  x i (sequence), as before. Thus, similar to Equation 3, we have We show experimentally that this extension effectively im-proves the gene name recognition task which suffers a lot from the  X  X are word X  problem.
 E. Extension (3): Clustered SLF
For a given word w (or attribute), SLF defines a feature vector SLF ( w )  X  R K 0 . K 0 = K in basic SLF and its value depends on the SLF extension (Figure 1). Words exhibiting similar empirical class distribution patterns have similar SLF feature vectors. Thus grouping them together might give stronger indications of target sequence labels or class boundaries.

Here we explored a vector quantization (VQ) technique[17] to convert SLF distibution vectors to prototype vectors. VQ clusters (or quantizes) groups of values together instead of one at a time. The groups of values are called input vectors and the quantization levels are called code vectors . The set of all code vectors is called the codebook . Quantization techniques allow the modeling of SLF feature vectors by the distribution of codebook vectors.

Formally speaking, we have SLF feature vectors for every word w in the dictionary D , where SLF ( w i )  X  R K 0 . We use C to represent the codebook set which includes N codebook vectors, C = { C 1 , C 2 , ..., C N } .

Then VQ tries to optimize (minimize) the following objective function, to find the codebook C and to round off the input vectors into code vectors, For each input vector SLF ( w d ) , VQ tries to find the best code vector C k it could be quantized into (to optimize the above function). Each vector SLF ( w ) would be assigned to one of the code vectors in the codebook after VQ step. Thus we use the indices of this code vector as the new feature for the word w . Essentially the whole process clusters all the words in the dictionary into multiple ( N ) clusters and then uses the cluster ID as the feature representation instead (a binary vector with all zeros apart from the ID dimension which is set to 1). Since this is basically a k -means clustering, we call this extension  X  X lustered SLF X  (CSLF).
 F. Advantages of SLF
Like self-training and co-training our algorithm (i) iter-atively tries to improve its model; and (ii) is a wrapper approach that can use a supervised (or semi-supervised) classifier as a  X  X ase learner X . However, our algorithm also has the following benefits:  X  It has no incestuous bias from introducing new ex- X  It does not require tricky selection heuristics as in self- X  The supervised model can learn if the SLF features are  X  The constructed SLF features contain information about  X  In a sequence labeling task, the SLF features for  X  This algorithm is highly scalable (it adds a few features
In addition, SLF vectors give a robust abstraction of predicted class (or boundary) distributions for each word (or discrete attribute) in a large unlabeled corpus. This semi-supervsied strategy provides a SLF matrix (having size D X  K for basic SLF where D is the size of the word vocabulary and K is the number of possible values for the target label). This matrix can be trained before-hand and utilized later for NLP prediction as long as the basic words are available. For example, in the case of online NER prediction, efficiency and speed are critical for online systems. Some of the features, such as part-of-speech tags, are quite slow to extract online. In this situation, we could make NER predictions relying on only basic word features (that are fast to extract, such as low-case word and capitalzation information), plus the well-trained SLF patterns from better models that incorporate harder to compute features. In the Results section we show that this setting achieves much better performance compared to sequence tagging relying only on basic features.

In this paper, we focus on NLP sequence labeling prob-lems which can be treated as multi-class classification (as-signing labels to words). As a wrapper approach, our method could be applied on any baseline sequence tagger. We choose two state-of-the-art systems to test our approach: (1) a deep neural network (NN) framework for unified NLP processing [14]; (2) a conditional random field model (CRF) which has been proved successful in many NLP tagging tasks. A. Sequence Labeling with Deep Neural Network (NN)
The authors in [14] proposed a deep neural network (NN) framework for semantic role labeling in English. In this paper we adapt this framework on named entity recognition and part-of-speech tagging problems.

Traditional NLP approaches usually extract a rich set of hand-crafted features from the sentence. In contrast, this deep neural network framework provides a unified framework to handle NLP tasks and processes an input sentence by several layers of feature extraction. The features in all the layers of the network are automatically trained by backpropagation to be relevant for the target task. The first layer is a lookup-table layer which map raw words into real-valued vectors (which are learnt) for processing by subsequent layers towards the targets. The second layer extracts features from the sentence treating it as a sequence with local and global structure (a sliding window approach). The remaining layers are classical NN layers. We employ one hidden layer for both NER and POS tasks.
 Leveraging Unlabeled Data with an Auxillary Task: Language Model Training Semi-supervision was used in this deep NN framework through a so-called  X  X anguage modeling X  strategy. Relying on the abundant unlabeled text data freely available on the web, the authors [14] proposed the unsupervised LM task to model natural human language sequences (English in their case) (also deep NN based). The motivation is: for most NLP tagging tasks, words seman-tically similar can be usually exchanged with no impact on the labels. The proposed LM tried to force two natural sentences s 1 and s 2 with same semantic tags to have a close representation in the feature extraction layers. The features (embedding) learned by the lookup-table layer from this LM-NN essentially clusters semantically similar words. Then weights of this LM lookup-table are used to initialize the entries in word lookup-table for the target sequence labeling task, e.g. NER (which ends up similar to multi-tasking with the LM). Our experimental results show that this auxillary task is very useful for NLP sequence labeling and our SLF could improve over this semi-supervision as well. We trained two language models (one for German and the other for English, both with the Wikipedia corpus).

Viterbi Training: For sequence tagging such as NER or chunking, each entity normally involves more than one word. The unified deep NN framework described above utilizes a labeling-per-word strategy without exploring dependencies among targeted classes. For example, for each NER tag type, the popular IOBES tagging style could make totally 17 classes for sequence classification, with clear dependencies between certain tag types. In this paper, we handle the local dependency through a Viterbi-style structured pre-diction technique (dynamic programming), which increases the performance of this NN framework effectively. Our SLF models used without Viterbi have a somewhat similar function to the Viterbi, since the predicted class-distribution of the surrounding neighbor words should obey the local depencies as well, and can be used during predictions. In the section V, we compared the performance of adding SLF features when using Viterbi analysis or not, to measure how much these two techniques overlap.

SLF in Deep NN Framework: It is straighforward to apply the SLF learning in the above deep NN framework. Unlike normal word features (essentially, letter patterns), SLF vectors (except clustered SLF) could be used directly, since they are numerical values already. For all words (or discrete attributes) in the sliding text window centered at the word of interest, SLF distributions are concatenated, added to other attributes X  lookup-table outputs, and then fed into the second NN layer for sequential labeling.
 B. Conditional Random Field
Conditional random fields (CRFs) [15] achieve state-of-the-art performance across a broad spectrum of sequence labeling tasks, including the gene mention task in the BioCreativeII competition [13]. Linear-chain CRFs are dis-criminative probabilistic models over observation sequences x and label sequences y = ( y 1 , ..., y | y | ) , where | x | = | y | , and each label y i has K different possible discrete values (multi-class). The conditional probability is defined as, where F j ( x , y ) = P n i =1 f j ( x , y i , y i +1 , i ) and Z ( x ) = P y exp( P j  X  j F j ( x , y )) .

The model is trained by maximizing the log-likelihood of the training data by gradient methods, which is a convex optimization problem [15]. A dynamic algorithm (the for-ward/backward strategy) is used to compute all the required probabilities p  X  ( y i , y i +1 ) for calculating the gradient of the likelihood.

We used the CRF++ toolkit [18]. Since CRF++ accept discrete features, clustered SLF is tested as a wrapper for CRF in this case.
 A. Supervised Sequence Labeling for Natural Languages
NLP research aims to convert human language into repre-sentations that are easy for computers to manipulate. Several sub-tasks are identified as useful for end applications such as information extraction or machine translation. Typical prob-lems range from the syntactic, like part-of-speech tagging, chunking and parsing, to the semantic, such as semantic-role labeling and named entity extraction. Most of these sub-tasks are sequence segmenatation or sequence labeling problems. While early studies were mostly based on handcrafted rules, recent tagging systems use supervised machine learning techniques to automatically train labeling algorithms from a collection of training examples. Popular supervised tech-niques includes Hidden Markov Models (HMM), Maximum Entropy (ME) Models, Support Vector Machines (SVM), and Conditional Random Fields (CRF) (for NER review, see [19]).
 B. Semi-supervised Learning
As already mentioned, self-training [2] (also called  X  X oot-strapping X  in the traditional NLP field) and co-training [5] augment the training set with labeled examples from the unlabeled set which are predicted by the model itself. This can give improvements to a model, but care must be taken as the predictions are prone to noise. Many other semi-supervised learning algorithms exist, including Tranductive SVMs [20], [21], graph-based regularization [22], entropy regularization [23] and EM with generative mixture models [24], see [25] for a review.

Traditionally, generative models (with EM used on the unlabeled set) were dominant for structural learning tasks in NLP. Recently discriminative models ([26], [27]) have been shown to offer competitive performance over a variety of sequential and structured learning problems in NLP. Gener-ative models generally do not achieve the same accuracy as discriminatively trained models, and therefore it is preferable to focus on discriminative approaches [1] in this paper. A number of semi-supervised learning systems can bootstrap from small sets of labeled data using discriminative learners, including self-training, co-training, and transductive SVM [20]. None of them seems to outperform the others across different domains, and each has its prons and cons.
Self-training can be used in combination with any dis-criminative learning model, but the predictions are prone to noise.

Co-training [5] is a weakly supervised learning technique in which the redundancy of the task is captured by training two classifiers using separate views of the same data. Pierce et al. [28] examined the scalability behavior of co-training on NLP tasks, and found that co-training is effective for learning from small amounts of labeled data but care must be taken due to possible degradation in the quality of bootstrapped data.

Transductive SVMs [20], [21] might learn better max-margin hyperplanes with the use of unlabeled data, but their optimization procedure is non-trivial [21]. The authors of [21] proposed a practical procedure to adapt transductive SVM to large scale nonlinear cases. The largest scale tested in that paper includes just 1000 labeled examples and 60,000 unlabeled examples, which takes about 40 hours for training. Hence, apparently the scalability of this approach is limited, and impractical for the real natural language processing tasks.

Graph-based semi-supervised approaches make use of dependencies introduced between the labels of nearby ex-amples on a constructed graph [22], [29]. These models train to encourage nearby data points to have the same class labels. They can obtain impressive performance using a very small amount of labeled data. However, since they model pairwise similarities among instances, most of these techniques require joint inference over the whole data set of size L + U at test time ( O (( L + U ) 3 ) ), which is not practical for a real NLP corpus.

Entropy regularization framework [1] has recently been proposed on linear-chain Conditional Random Fields (which are popular in NLP community). In its formulation, the traditional label likelihood (on supervised data) is augmented with an additional term that encourages the model to predict low entropy label distributions on the unlabeled set. However this technique was pointed out to be quite fragile and the solution might assign all tokens the same label [30].
Most of the above semi-supervised methods adapt well to thousand of training examples (including both labeled and unlabeled data) only [31]. Except self-training and co-training, other semi-supervised algorithms have scalability problems for realistic language modeling tasks, which nor-mally involves hundreds of thousands labeled examples. Table III lists the size of both labeled and unlabeled corpora for the four NLP tasks we tested in this work. For instance, the two CoNLL-2003 [11] NER tasks have two hundred thousand training tokens and giga-word scale unlabeled corpora (Wikipedia). In this respect, our proposed SLF method is superior to many methods since it can make use of the extremely large amount of unlabeled data in the presence of a fairly large labeled set.

Finally, there are some methods that use auxiliary tasks on large unlabeled corpora for training sequence models (i.e. through multi-task learning). Ando and Zhang (2005) [32] proposed a method based on defining multiple tasks using unlabeled data that are multi-tasked with the task of interest, which they showed to perform very well on POS and NER tasks. The methods using auxiliary information can often find good solutions, however the selection of auxiliary problem requires significant insights. Similarly, the language model strategy proposed in Collobert et al. [14] is another type of auxillary task as above.
 C. Semi-supervised Learning with Labeled Features
Beyond the significant amount of work on semi-supervised learning with small amounts of fully labeled data, there has been a growing interest in the use of background domain knowledge to augument supervised learning.
This includes a number of methods that make use of human-provided associations of features to particular classes (for example, a human annotator might label the word  X  X ashback X  to the document category  X  X hopping X ). The prior class-bias of features (called  X  X abeled features X  in the follow-ing) could be used to generate pseudo labeled examples that are then used for augumenting standard supervised learning [33], [34], [35]. This trend of work is orthogonal to the traditional semi-supervised category.

Schapire et al [33] utilized a set of features annotated with their associated majority labels to label pseudo-examples for boosting a logistic regression model. Similarly, Wu and Srihari [34] assigned labels to unlabeled documents with  X  X abeled features X  and then use these pseudo-examples in conjunction with labeled examples to train a weighted margin Support Vector Machine with regularization. Later Haghighi and Klein [36] explored similar  X  X abeled features X  for a  X  X seudo-example X  strategy of training a generative MRF sequence model.

Unlike the above approaches, Druck et al. [35] utilized a generalization expectation (GE) criterion with soft constraint of the model X  X  predictions on unlabeled instances using  X  X abeled features X . Similarly, Mann et al. [30] introduced GE to linear-chain CRFs for targeting sequence modeling problems which makes use of  X  X abeled features X  rather than  X  X abeled instances X .

In contrast to all above methods, our semi-supervised method induces features, instead of examples , from a large corpus of unannotated sentences in a supervised fashion. Our SLF method is orthogonal to traditional semi-supervised techniques and also orthonal to those approaches relying on  X  X abeled features X  (we do not use human annotations). As a wrapper approach, it can be combined with many of the other supervised and semi-supervised methods.
 A. Datasets &amp; Settings We first test our approach on the English and German NER datasets provided in the CoNLL-2003 shared task [11]. For each language, four files are provided, including a training file, a development file (for parameter tuning), a test file and a large file with unannotated data. For both languages, more than 200,000 training tokens exist in the provided training files (size of training/testing/unlabeled sets in Table III). The ECI data file provided in CoNLL-2003 [11] is used as our unlabeled corpus for semi-supervised learning of German NER. The English Wikipedia web pages is used for the English NER case. We then test basic SLF on the English POS data set described in [12].
 For these three tasks, we used the unified deep Neural Network (NN) framework [14] as a base classifier for SLF. The first lookup-table layer maps words to 50-dimensional vectors (one vector for each word in the dictionary), the parameters of which are automatically trained during the learning process using backpropagation. The second layer is a classical layer of H hidden units (where H is optimized on the development set), and the final layer outputs probabilities of the class labels. The basic sequence labeling was handeled using a sliding window approach. We then extend this baseline with a Viterbi decoding of the entire sentence to incorporate local dependencies between target classes.
For the baseline deep NN model we tried various combi-nations of the following word features:  X  For English NER, we use (i) words in a 7-word-window  X  For German NER, we use (i) words in a 5-word- X  For English POS, we use (i) words in a 5-word-window
We then test SLF using conditional random fields (us-ing CRF++ tool) as a base classifier on the gene-name-recognition (GM) data set (the number of tokens in train-ing/testing/unlabeled sets in Table III) from the 2006 BioCreativeII BioNLP competition [13]. For this GM task, we utilized the following word features: (i) words in a 5-word-window surrounding current, (ii) capitalization fea-tures of current and surrounding words, (iii) prefix and suffix (up to length 4) of current and surrounding words, (iv) string patterns of current and surrounding words. (Note we did not use any gazetteer lists).

In the following, we compare basic and extended SLF models over various baselines, including supervision alone, using Viterbi decoding or semi-supervision via LM . B. Basic SLF: Comparison with Supervised Deep NN &amp; Semi-Supervision via LM
Table IV lists the test set performance of the German NER task using the F1 measure when applying basic SLF as a wrapper to various systems: using only word features (with and without a Viterbi decoding step), and using all features plus the language model (LM) based semi-supervised learn-ing. In all cases SLF improves over the baseline. Our best performance of 75.72 (using all features + SLF) beats the state-of-the-art German NER performance of 75.27 which was reported in [32]. The best result during the competition was 74.17 [37].
 We also considered taking our best model, and adding the SLF features predicted by it to a basic word-features only model (the last row of Table IV). This improved its accuracy from 50.61 to 64.1. Using the LM as well yields 72.45 (words+LM on their own are 69.05). This is interesting because these results do not require POS, chunk, stem or caps features any more, but are close to the state-of-the-art. As we mentioned in the section II-F, this shows that NER predictions relying on only basic word plus the well-trained SLF patterns, (from better models which include harder to compute features), could achieve much better performance and this setting could be a feasible choice for online NER systems.
 In addition, we also tried a transductive setting with basic SLF where we use the test set as the unlabeled corpus to build class-distribution feature vectors. The results for German NER either only help marginally, or not at all. For instance, when using  X  X ll Features+LM X , this setup achieves F1 72.04 which is even slightly lower than the supervised NN baseline (F1 72.44 in Table IV). Moreover, we tried counting words X  class in the train or validation sets. The resulting word-class distribution vectors could be used instead of SLF. Adding them to German NER achieves similar performance as the transductive SLF (results not shown). These results show that the semi-supervision from using the large unlabeled set in SLF is important. Table V provides results for the English NER task. Again, SLF improves over all baselines; our best result was 88.69. In contrast, the best performing method during the competition was 88.76, and [32] have since reported 89.31 using multi-task semi-supervision. Here, our slightly worse performance seems to be due to our weaker baseline method (before even applying SLF) compared to these approaches. C. Attribute SLF: Comparison with Supervised Deep NN &amp; Semi-Supervision via LM Table VI compared the performance of English POS with SLF applied to the deep NN, or not. Similar conclusions as with the two NER tasks were observed. The comparisons are under various settings (with different combinations of features added, and language model improvements). The best state-of-art [12] English POS system (as far as we know) achieved the token level error rate 2.76%. We could see that with only words, cap feature and stem-end feature, our SLF improves the deep-NN system to the state-of-the-art POS performance. The key word  X  X ttribute X  in the last column means that we added not only the basic SLF feature for word but also added the attribute SLF features for  X  X aps X  and  X  X tem-end X  attributes. Attribute SLF makes improvements on basic SLF over both supervised baseline alone and semi-supervision LM.
 D. Clustered &amp; Boundary SLF: Comparison with CRF
The BioCreativeII GM [13] data set involves a sequence tagging task which looks for gene/protein names in a bio-literature text corpus. Since almost all the top teams in BioCreativeII GM competition [13] utilized CRF, we tried to use CRF on this sequence tagging problem as well. We used CRF++ as our baseline and apply clustered SLF as a wrapper semi-supervised approach. Considering there are only two classes in this tagging task (gene name or not), we tested boundary SLF on this corpus as well.

Table VII uses F1 test score to compare GM performances under multiple setups. We could see that both clustered basic SLF and clustered boundary SLF improve over CRF super-vised baselines. Boundary SLF shows better improvements compared to basic SLF.

The best performance achieved here (F1 87.16 in the last row of Table VII) is slightly lower than the best team (87.21 F1 test) in the BioCreativeII GM competition. Here we used only basic word,  X  X aps X  features, string patterns and prefix/suffix attributes. All top teams in the competition used many other word attributes, such as POS, and they all utilized other complex techniques such as the bidirectional training of models and extensive domain lexicons. In sum-mary, SLF could improve CRF on this GM corpus effectively and we could achieve state-of-the-art performance using only basic word features.
 E. Comparison with Self-training
We applied self-training to the same baseline methods to compare to the performance of SLF. There are numerous variants of self-training. We adopt the following weighting scheme: given L training examples, we choose L/R ( R is a parameter to choose) unlabeled examples to add in the next round X  X  training. By varying R , we get a range of impacts from self-training.
 Table VIII and Table IX give results of self-training for the English and German NER tasks. Table X provides the results for English POS. For all three corpus, self-training only helped marginally, or not at all, depending on the parameters.
The above comparison indicates that SLF has better behavior than self-training with a random selection strategy. Since there exist many selection strategies for self-training, other selection techniques might bring improvements, see e.g. [9], [7] for other strategies. Still, these heuristic choices are difficult and need careful tuning [7]. In contrast, the proposed SLF method does not seem to suffer from these issues.

Further, the performance in multiple rounds of self-training might oscillate because of degradation by noisy labels (see e.g. [7], [8]). We observed that basic SLF X  X  iterative training gives stable results. Figure 2 shows the test F1 from the iterations (as a wrapper for the  X  X ll features + LM + Viterbi X  baseline) for the German NER set. It appears to converge in only a few iterations.

In this work we proposed a novel semi-supervised algo-rithm for learning features from a large unlabeled corpus in a supervised fashion. These features try to model class-distribution patterns for words or word attributes. We applied this method and several extensions to four classical labeling tasks, where we obtained improvements over the supervised and semi-supervised baselines tested. Our method is highly scalable, contains no difficult parameters to tune, and we found it to be empirically robust.

The proposed method can easily be extended to other cases or domains. For example, instead of calculating pre-dicted class distributions for each word, we could consider n -gram distributions instead. Moreover, one can generalize beyond word-level evaluation tasks. For instance in text cat-egorization problems (document classification or sentiment analysis) a word X  X  class distribution is the distribution of labels of documents that contained that word.

