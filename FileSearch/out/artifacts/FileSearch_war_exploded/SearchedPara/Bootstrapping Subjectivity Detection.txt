 We describe a method for automatically generating subjectivity clues for a specific topic and a set of (relevant) document, evaluating it on the task of classifying sentences w.r.t. subjectivity, with improve-ments over previous work.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic processing Experimentation, Measurement Subjectivity, sentiment retrieval We address the task of detecting on-topic subjectivity in text. Specifically, we want to (1) tell whether a textual document ex-presses an attitude (positive or negative) towards a specific topic, and moreover, (2) to find where exactly in the document it is ex-pressed (up to a phrase or at least a sentence). The first task is in the area of sentiment retrieval . The simplest approach here consist of two stages: first, we find texts that are on topic, then we filter out (or, rank low) those without attitude [3]. A more elaborate approach is based on the assumption that documents are mixtures of two generative components, one  X  X opical X  and one  X  X ubjective X  [4]. In practice, however, these components are not independent: a word that is neutral w.r.t. one topic can be a good subjectivity clue for another (e.g., compare hard copy and hard problem ). Noticing this, Na et al. [6] generate a topic-specific list of possible clues, based on top relevant documents, and use this list for subjectivity filtering (reranking). Furthermore, Jijkoun et al. [1] argue that such clues are specific not only to the topic, but to the exact target they refer to, e.g., when looking for opinions about a sportsman, solid is a good subjectivity clue in the phrase solid performance but not in solid color .

Jijkoun et al. [1] describe a method for learning such pairs (clue, target) for a given topic in an unsupervised manner, using syntactic dependencies between clues and targets. Kim et al. [2] also use syntactic relations to bootstrap a set of topic-specific clues and use them for detecting sentences containing on-topic sentiment. Note  X  Current affiliation: Textkernel BV, Amsterdam.
 that the methods in [1, 2] also address the second task introduced above: finding the exact location of sentiment in documents.
We go beyond the subjectivity lexicon generation methods from [1, 2], with the goal of improving subjectivity spotting. We extend the method of [1] using bootstrapping (similarly to [2]). Unlike [1], we directly evaluate the performance on the task of detecting on-topic subjectivity at the sentence level, not on sentiment retrieval with entire documents. Unlike [2], our method does not use a seed set for a given topic: we only need a general purpose subjectivity lexicon, a topic and a set of (presumably) relevant documents.
We start with a topic T (a textual description) and a set R = { d 1 ,...,d N } of documents deemed relevant to T . The method uses a general-purpose list of subjectivity clues L (in our exper-iments, the well-known MPQA lexicon [9]). We will also use a large background corpus BG of documents of a similar genre, cov-ering many topics beside T . We use the Stanford syntactic parser to extract dependency relations in all sentences in all documents. Our method outputs a set of triples { ( c i ,r i ,t i ) } , where c jective clue, t i a subjectivity target and r i a dependency relation between the two words. We interpret an occurrence of such a triple in a document as an indication of sentiment relevant to T , specifi-cally directed at t i .

Our method operationalizes a number of intuitions. First, we as-sume that a given topic can be associated with a number of related targets (e.g., opinions about a sportsman may cover such targets as performance, reaction, serve, etc.) and each target has a number of possible clues expressing attitude towards it (e.g., solid perfor-mance ). We assume that clues and targets are typically syntacti-cally related (e.g., the target serve can be a direct object of clue to like ), and every clue has syntactic relations connecting it to possi-ble targets (e.g., for to like only the direct object can be a target, but not the subject, a adverbial modifier, etc.).
 and every type of syntactic relation r that can originate from it in the background corpus, we compute a clue score s clue ( c,r ) as the entropy of words at the other endpoint of r in BG (normalized between 0 and 1 for all c and r ). The clue score gives an initial estimate of how well ( c,r ) may work as a subjectivity clue. Here, we follow the intuition of [1]: targets are more diverse than other syntactic neighbours of clues.
 Step 2: Target scoring. For every word t  X  R we determine its target score that tells us how likely t is an opinion target related to topic T . Our main intuition here is that targets are words that occur unusually often in subjective contexts in relevant documents. First, we compute C R ( t ) = P s clue ( c,r ) for all occurrences of the syntactic relation r between words c and t in corpus R . Sim-ilarly, we compute C BG ( t ) for the background corpus BG . We view C R (  X  ) and C BG (  X  ) as (weighted) counts, and compute a par-simonious language model p R (  X  ) using a simple EM algorithm [5]. We also compute a language model p B G (  X  ) from counts C by simple normalization. Finally, we define the target score of a word t as the likelihood that the occurrence of t in R comes from p (  X  ) rather than p B G (  X  ) : Step 3: Clue scoring. Mirroring Step 2, we now use target scores to compute better estimates for clue scores. Here, our intu-ition is that good subjectivity clues are those that occur unusually often near possible opinion targets for a given topic. The computa-tion is similar to Step 2, with s clue ( c,r ) and s t gt ( t ) interchanged: we compute weighted counts, a parsimonious model and, finally, the updated s clue ( c,r ) . Now, we iterate Step 2 and Step 3, each ues at the previous iteration. After K iterations we select N targets and M pairs (clue, relation) with the highest scores. We check which of the N targets co-occur with which of the M clues in R .
We evaluate different versions of our method on the following sentence classification task: for a given topic and a list of docu-ments relevant to the topic, we need to identify sentences that ex-press opinions relevant the topic. We compute precision, recall and F-score for detection of relevant opinionated sentences.
 In our experiments, we use the NTCIR-6 [7] and NTCIR-7 [8] Opinion Analysis datasets, containing judgements for 45 queries and 12,000 sentences.

In order to understand how the quality of relevant documents af-fects the performance of the method, we selected R to be (1) R top 100 document retrieved from the NTCIR-6/7 English collec-tion using Lucene, (2) R r : only documents with at least one rele-vant (not necessarily opinionated) sentence as identified by NTCIR annotators, and (3) R r +100 the union of (1) and (3).

We also ran the method with different numbers of iterations ( K ), different number of selected targets ( N ) and selected clues ( M ). In all settings, the overall performance stabilizes at K  X  5 . Table 3 shows the evaluation results:
As one might expect, we see that reducing the number of se-lected targets ( N ) improves precision but harms recall. Chang-ing the number of selected clues ( M ) has little effect on precision: since for detecting opinionatedness we combine clues with targets, noise in clues does not necessarily lead to drop in precision. Overall, we notice that with in the best setting ( K = 4 , N = 40 , M = 50 ) the method outperforms [1] (significantly, at p=0.05, us-ing t-test). Performance of the method varies substantially per topic ( F 1 between 0.13 and 0.48), but the optimal values for parameters are stable for high-performing topics (with F 1 &gt; 0 . 26 ).
We have described a method for automatically generating sub-jectivity clues for a specific topic and a set of (relevant) document, evaluating it on the task of classification of sentences w.r.t. subjec-tivity, demonstrating improvements over previous work. We plan to incorporate more complex syntactic patterns in our clues (going beyond word-word relations) and study the effect of user feedback (which extracted targets are correct? which clues are indeed sub-jective?) with the view of implementing an interactive system. This research was partially supported by the European Union X  X  ICT Policy Support Programme as part of the Competitiveness and In-novation Framework Programme, CIP ICT-PSP under grant agree-ment nr 250430, the PROMISE Network of Excellence co-funded by the 7th Framework Programme of the European Commission, grant agreement no. 258191, the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.-061.814, 612.061.815, 640.004.802, 380-70-011, the Center for Creation, Content and Technology (CCCT), the Hyperlocal Service Platform project funded by the Service Innovation &amp; ICT program, the WAHSP project funded by the CLARIN-nl program, and under COMMIT project Infiniti.
 [1] V. Jijkoun, M. de Rijke, and W. Weerkamp. Generating focused [2] Y. Kim, Y. Choi, and S.-H. Myaeng. Generating domain-[3] Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and J.-H. Lee. [4] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic sen-[5] E. Meij, W. Weerkamp, K. Balog, and M. de Rijke. Parsimo-[6] S.-H. Na, Y. Lee, S.-H. Nam, and J.-H. Lee. Improving opinion [7] Y. Seki, D. K. Evans, L.-W. Ku, H.-H. Chen, N. Kando, and C.-[8] Y. Seki, D. K. Evans, L.-W. Ku, L. Sun, H.-H. Chen, and [9] J. Wiebe, T. Wilson, and C. Cardie. Annotating expressions of
