 Previous papers in ad hoc IR reported that scoring functions should satisfy a set of heuristic retrieval constraints, provid-ing a mathematical justication for the normalizations his-torically applied to the term frequency (TF). In this paper, we propose a further level of abstraction, claiming that the successive normalizations are carried out through composi-tion. Thus we introduce a principled framework that fully explains BM25 as a variant of TF-IDF with an inverse or-der of function composition. Our experiments over standard datasets indicate that the respective orders of composition chosen in the original papers for both TF-IDF and BM25 are the most effective ones. Moreover, since the order is dif-ferent between the two models, they also demonstrated that the order is instrumental in the design of weighting models. In fact, while considering more complex scoring functions such as BM25+, we discovered a novel weighting model in terms of order of composition that consistently outperforms all the rest. Our contribution here is twofold: we provide a unifying mathematical framework for IR and a novel scoring function discovered using this framework.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Theory, Algorithms, Experimentation IR theory; scoring functions; TF normalizations; heuristic retrieval constraints; function composition
Fang et al. introduced in [3] a set of heuristic retrieval constraints that any scoring function used for ad hoc infor-mation retrieval (IR) should satisfy. In particular, these con-straints involve term frequency, term discrimination, docu-ment length and the interactions between them. For in-stance, they stated that a scoring function should favor document matching more distinct query terms. It is one of the earliest works that formally dened the properties that both the TF and the IDF components of any weighting model should possess. It is a unifying theory in IR that ap-plies to the vector space model (TF-IDF [13]), probabilistic (BM25 [10]), language modeling (Dirichlet prior [15]) and information-based (SPL [2]) approaches and the divergence from randomness framework (PL2 [1]).

The denition of these constraints contributed to the im-provement of the overall effectiveness of most modern scor-ing functions. Constraints on the term frequency result in successive normalizations on the raw TF, each one satisfy-ing one or more properties. In our work, we intended to go one step further and we propose the use of composition to explain how the normalizations are applied successively in the general TF IDF weighting scheme. In section 2, we de-scribe in details the mathematical framework we designed. In section 3, we present the experiments we conducted over standard datasets and the results obtained that indicate how important the order of composition is, along with a novel and effective weighting model, namely TF l  X   X   X  p IDF. Finally, in section 4, we conclude and mention future work.
In ad hoc IR, a scoring function associates a score to a term appearing both in a query and a document. This func-tion consists of three components supposedly independent of one another: one at the query level (QF), one at the docu-ment level (TF) and one at the collection level (IDF). These components are aggregated through multiplication to obtain a nal score for the term denoted hereinafter by TF IDF. We omit voluntarily to mention QF in the name since it is a function of the term frequency in the query and it has usually a smaller impact on the score, in particular for Web queries that tend to be short. We make here a difference be-tween the TF IDF general weighting scheme and TF-IDF, the pivoted normalization weighting dened in [13]. Note that because we rank documents, these term scores will be aggregated through sum to obtain a document score but this is beyond the scope of the current paper.
Since the early work of Luhn [4], term frequency (TF) has been claimed to play an important role in information retriev al and is at the center of all the weighting models. Intuitively, the more times a document contains a term of the query, the more relevant this document is for the query. Hence, it is commonly accepted that the scoring function must be an increasing function of the term frequency and the simplest TF component can be dened as follows: where tf ( t, d ) is the term frequency of the term t in the document d . However, as the use of the raw term frequency proved to be non-optimal in ad hoc IR, the research com-munity started normalizing it considering multiple criteria, mainly concavity and document length normalization . Later, these normalizations were explained as functions satisfying some heuristic retrieval constraints as aforementioned [3].
The marginal gain of seeing an additional occurrence of a term inside a document is not constant but rather decreas-ing. Indeed, the change in the score caused by increasing TF from 1 to 2 should be much larger than the one caused by increasing TF from 100 to 101. Mathematically, this cor-responds to applying a concave function on the raw TF. We prefer the term concave like in [2] to sublinear like in [5] since the positive homogeneity property is rarely respected (and actually not welcomed) and the subadditivity one, even though desirable, not sufficient enough to ensure a decreas-ing marginal gain.

There are mainly two concave functions used in practice: the one in TF-IDF [13] and the one in BM25 [10] that we re-spectively called log-concavity (TF l ) and k-concavity (TF where k 1 is a constant set by default to 1.2 corresponding to the asymptotical maximal gain achievable by multiple occurrences compared to a single occurrence.

When collections consist of documents of varying lengths (like web pages for the Web), longer documents will as a result of containing more terms have higher TF val-ues without necessary containing more information. For in-stance, a document twice as long and containing twice as more times a term should not get a score twice as large but rather a very similar score. As a consequence, it is com-monly accepted that the scoring function should be an in-verse function of the document length to compensate that effect. Early works in vector space model suggested to nor-malize the score by the norm of the vector, be it the L 1 norm (document length), the L 2 norm (Euclidian length) or the L 1 norm (maximum TF value in the document) [11]. These norms still mask some subtleties about longer docu-ments since they contain more terms, they tend to score higher anyway. Instead, the research community has been using a more complex normalization function known as piv-oted document length normalization and dened as follows: where b 2 [0 , 1] is the slope parameter, j d j the document length and avdl the average document length across the col-lection of documents as dened in [12].
Based on this set of properties and their associated func-tions, it seems natural to apply them to the raw TF succes-sively by composing them. In the literature, the document length normalization has usually been applied to either the overall term score or the document score like one would nor-mally normalize a vector. However, it was then hard to fully t BM25 in the TF IDF weighting scheme. With composi-tion, it is just a matter of ordering the functions. We present here the two compositions behind TF-IDF and BM25, re-spectively TF p  X  l (= TF p  X  TF l ) and TF k  X  p (= TF k where K = k 1 (1 b + b j d j av dl ) as dened in [10].
Note that under this form, it is not obvious that TF k  X  p really a composition of two functions with the same proper-ties as the one in TF p  X  l . We think this is the main reason why composition has never been considered before. By do-ing so, we provide not only a way to fully explain BM25 as a TF IDF weighting scheme but also a way to easily consider variants of a weighting model by simply changing the order of composition. As we will see in section 3, this led us to a new TF IDF weighting scheme that outperforms BM25.
While the higher the frequency of a term in a document is, the more salient this term is supposed to be, this is no longer true at the collection level. This is actually quite the inverse since these terms have a presumably lower discrimination power. Hence, the use of a function of the term specicity , namely the Inverse Document Frequency (IDF) as dened in [14] and expressed as follows: where N is the number of documents in the collection and df ( t ) the document frequency of the term t .
By TF-IDF, we refer to the TF IDF weighting model dened in [13], often called pivoted normalization weighting . The weighting model corresponds to TF p  X  l IDF:
T F -IDF ( t, d ) = 1 + ln[1 + ln[ tf ( t, d )]]
By BM25, we refer to the scoring function dened in [10], often called Okapi weighting . It corresponds to TF k  X  p when we omit QF (k-concavity of parameter k 3 for tf ( t, q )). Thus, it has an inverse order of composition between the conca vity and the document length normalization compared to TF-IDF. The within-document scoring function of BM25 is written as follows when using the IDF formula dened in subsection 2.3 to avoid negative values, following [3]:
Through composition, we also allow additional constraints for the TF component to be satised easily. Subadditiv-ity is for instance a desirable property if two documents have the same total occurrences of all query terms, a higher score should be given to the document covering more dis-tinct query terms. Here, it just happens that TF l and TF already satisfy it as noted in [3].

Recently, Lv and Zhai introduced in [6] two new con-straints to the work of Fang et al. to lower-bound the TF component. In particular, there should be a sufficiently large gap in the score between the presence and absence of a query term even for very long documents where TF p tends to 0 and a fortiori the overall score too. Mathematically, this corre-sponds to be composing with a third function TF  X  that is always composed after TF p since it compensates the poten-tial null limit introduced by TF p and it is dened as follows: where  X  is the gap, set to 0.5 if TF  X  is composed immediately after TF p and 1 if concavity is applied in-between. These are the two values dened in the original papers [6, 7] and we just interpreted their context of use in terms of order of composition. We did not change nor tune the values.
The weighting models Piv+ and BM25+ dened in [6] correspond respectively to TF  X   X  p  X  l IDF and TF  X   X  k  X  while BM25L dened in [7] to TF k  X   X   X  p IDF. We clearly see that the only difference between BM25+ and BM25L is the order of composition: this is one of the advantages of our framework easily represent and compute multiple variants of a same general weighting model. In the experiments, we considered all the possible orders of composition between TF k or TF l , TF p and TF  X  with the condition that TF p always precedes TF  X  as explained before.
 For instance, we will consider a novel model TF l  X   X   X  p dened as follows:
T F l  X   X   X  p IDF ( t, d ) = 1+ln[1+ln[ tf ( t, d ) where b is set to 0.20 and  X  to 0.5.
Following our mathematical framework that relies on com-position, we wondered why the order of composition was dif-ferent between two widely used scoring functions TF-IDF and BM25. In the original papers [11, 9], there was no men-tion of the difference in the order and this motivated us to investigate the matter. Our initial thought was that using an inverse order of composition in BM25 could improve it or vice-versa for TF-IDF. As a consequence, we tried ex-haustively the combinations among TF k , TF l and TF p and report the results. Thereafter, as mentioned in subsection 2.5, we followed the same procedure considering a third func-tion to compose with: TF  X  . Indeed, we wanted to explore the extensions considered by the research community [6, 7] in terms of composition. This led us to a novel weighting model that outperforms them (see subsection 3.4).
We used two TREC collections to carry out our exper-iments: Disks 4&amp;5 (minus the Congressional Record) and WT10G. Disks 4&amp;5 contains 528,155 news releases while WT10G consists of 1,692,096 crawled pages from a snap-shot of the Web in 1997. For each collection, we used a set of TREC topics (title only to mimic Web queries) and their associated relevance judgments: 301-450 and 601-700 for Disks 4&amp;5 (TREC 2004 Robust Track) and 451-550 for WT10G (TREC9-10 Web Tracks).

We evaluated the scoring functions in terms of Mean Av-erage Precision (MAP) and Precision at 10 (P@10) con-sidering only the top-ranked 1000 documents for each run. Our goal is to compare weighting models that use the same functions but with a different order of composition and se-lect the best ones on both metrics. For example, in Table 1, TF p  X  l IDF is compared with TF l  X  p IDF and TF k  X  p IDF with TF p  X  k IDF. The statistical signicance of improve-ment was assessed using the Student's paired t-test consid-ering p-values less than 0.01 to reject the null hypothesis.
We have been using Terrier version 3.5 [8] to index, re-trieve and evaluate over the TREC collections. For both datasets, the preprocessing steps involved Terrier's built-in stopword removal and Porter's stemming. We did not tune the slope parameter b of the pivoted document length nor-malization on each dataset. We set it to the default value suggested in the original papers: 0.20 when used with log-concavity [13] and 0.75 when used with k-concavity [10].
We report in Table 1 the results we obtained on the afore-mentioned datasets when considering concavity and pivoted document length normalization. To the best of our knowl-edge, experiments regarding the same functions (TF k , TF and TF p ) with a different order of composition have never been reported before. They indeed show that the original or-der chosen for both TF-IDF and BM25 is the most effective one: TF p  X  l IDF outperforms TF l  X  p IDF and TF k  X  p IDF outperforms TF p  X  k IDF. But since the order is different be-tween the two, this also indicates that the order does matter depending on which function is chosen for each property.
For these two models (TF-IDF and BM25), the use of a different concave function to meet the exact same con-straints requires the pivoted document length normalization to be applied before or after the function. The impact is even more signicant on the Web dataset (WT10G) that corre-sponds the most to contemporary collections of documents.
In Table 2, we considered in addition the lower-bounding normalization function TF  X  dened in subsection 2.5. The best-performing weighting model on both datasets is a novel one TF l  X   X   X  p IDF and it even outperforms BM25+ and BM25L (signicantly using the t-test and p &lt; 0.01). This model has never been considered before in the literature to the best of our knowledge. In fact, the results from Table 1 establish that TF l should apparently be applied before TF T able 1: TF-IDF vs. BM25: an inverse order of composition; bold indicates signicant performances lik e in TF-IDF. With lower-bounding normalization, it no longer holds. The formula for TF l  X   X   X  p IDF was given in equation 11. Without the use of our formal framework and composition, it would have been harder to detect and test these variants that can outperform state-of-the-art scoring functions when the order of composition is chosen carefully. Table 2: TF l  X   X   X  p IDF vs. BM25+ and BM25L; bold indicates signicant performances.

Scoring function design is a cornerstone issue in informa-tion retrieval. In this short paper, we intended to provide new insights on scoring functions for ad hoc IR. In partic-ular, we proposed a unifying mathematical framework that explains how weighting models articulate around a set of heuristic retrieval constraints introduced in related work.
Using composition to combine the successive normaliza-tions historically applied to the term frequency, we were able to fully explain BM25 as a TF IDF weighting scheme with just an inverse order of composition between the con-cavity and the document length normalization compared to TF-IDF. Besides, the framework also allowed us to discover and report a novel weighting model TF l  X   X   X  p IDF that consistently and signicantly outperformed BM25 and its extensions on two standard datasets in MAP and P@10.
Future work might involve the design of novel retrieval constraints and their compositions with existing ones. We are condent that rening the mathematical properties be-hind scoring functions will continue to improve the effective-ness of these models in ad hoc IR.
We thank the anonymous reviewers for their useful feed-backs. This material is based upon work supported by the French DIGITEO Chair grant LEVETONE. [1] G. Amati and C. J. Van Rijsbergen. Probabilistic [2] S. Clinchant and E. Gaussier. Information-based [3] H. Fang, T. Tao, and C. Zhai. A formal study of [4] H. P. Luhn. A statistical approach to mechanized [5] Y. Lv and C. Zhai. Adaptive term frequency [6] Y. Lv and C. Zhai. Lower-bounding term frequency [7] Y. Lv and C. Zhai. When documents are very long, [8] I. Ounis, G. Amati, V. Plachouras, B. He, [9] S. E. Robertson and S. Walker. Some simple effective [10] S. E. Robertson, S. Walker, K. Sp X rck Jones, [11] G. Salton and C. Buckley. Term-weighting approaches [12] A. Singhal, C. Buckley, and M. Mitra. Pivoted [13] A. Singhal, J. Choi, D. Hindle, D. Lewis, and [14] K. Sp X rck Jones. A statistical interpretation of term [15] C. Zhai and J. Lafferty. A study of smoothing
