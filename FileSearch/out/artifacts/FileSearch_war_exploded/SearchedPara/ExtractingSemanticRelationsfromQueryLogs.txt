 In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic re-lations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We rst propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then an-alyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.
 H.2.8 [ Information Systems ]: Database applications| Data mining ; H.3.5 [ Information Systems ]: Information Search and Retrieval| Query formulation Experimentation, Measurement Graph mining, query logs analysis, knowledge extraction.
One of the recurrent goals of mankind has been to recol-lect all human knowledge, as the wisdom of all the people is larger than any particular individual (e.g. the wisdom Work done while visiting Yahoo! Research Barcelona. Co pyrigh t200 7ACM 978-1-59593-609-7/07/000 8... $ 5.00. of crowds [16]). The Web can be seen as the largest in-tent to store all human knowledge, either explicitly (e.g. Wikipedia) or implicitly. Social media (also called Web 2.0) has allowed more people to be able to contribute content in the Web, something that a few years ago was not true. The collaborative tagging work on sites like Flickr has created non-hierarchical categories called folksonomies . Neverthe-less, still not all the people participate on social media, so a rst question is: can we tap on the knowledge of more peo-ple? Below we argue that the answer is on the positive side.
Queries submitted to search engines convey implicit knowl-edge if we assume that most of the time user actions are meaningful. Queries can be seen as tags associated to doc-uments clicked by the people making those queries, in a similar way that link anchor text is used as surrogate text of the linked Web pages. So we can look at queries as an implicit folksonomy. However queries contain more seman-tic relations: some are equivalent, other are more speci c, etc. In fact, those relations imply a pragmatic taxonomy, the taxonomy of the language that people use to search the Web (a kind of webslang ). Hence, the challenge is how to extract interesting relations from very large query logs and nd some structure for the webslang. This paper is one of the rst attempts towards this ambitious goal. One natural starting point is to infer a graph from the queries. One such graph is the bipartite graph of queries and URLs, where a query and a URL are connected if a user clicked in a URL that was an answer for a query. Another possibility, more frequent in previous research, is to de ne a similarity (or dis-tance) function between queries. This also implies a graph based on this function. One drawback of de ning a function is that it is more dicult to understand why two queries are similar and in some degree we add arti cial artifacts that can add noise to data that is already noisy.

In this paper we give two di erent contributions. First we present a natural distance among queries that is related to previous work and we analyze large graphs that are gener-ated by such a distance. The graphs are less sparse than in previous work, perhaps because before large data sets were usually not available. We analyze several characteristics of these graphs. This analysis provides information not only about how people query but also about how they behave af-ter a query and the content distribution of what they look at. We then attempt to extract knowledge using the previously mentioned bipartite graph as starting point. Basically two queries are connected if there is a path between them in the bipartite graph. We also de ne di erent edge colors based on the URLs that connect two queries, which imply possi-ble seman tic relations among queries. To explore our ideas we use more than twenty million queries from the Yaho o! searc h engine and we do a man ual and automatic evalua-tion of the results. Moreo ver, we sketch an application that emplo ys the graph to detect multitopical URLs, pro viding a promising approac h to tackle the noise issue. In Section 2 we discuss previous work in query similarit y and kno wledge extraction from queries. In section 3 we describ e a natural vector mo del to de ne similar queries and de ne seman tic re-lations among queries. In section 4 we analyze a large graph generated from a large query log and in section 5 we evaluate our results. We end with some conclusions and future work.
Most of the work on query similarit y is related to query expansion or query clustering. One early technique prop osed by Ragha van and Sev er [14] attempts to measure query sim-ilarit y using the di erences in the ordering of documen ts re-triev ed in the answ ers, whic h is not feasible in the curren t Web. Later, Fitzpatric k and Den t [11], measured query sim-ilarit y using the normalized set intersection of the top 200 documen ts in the answ ers for the queries. Again, this is not meaningful in the Web as the intersection for seman tically similar queries that use di eren t synon yms can and will be very small.

Wen et al [17] prop osed to cluster similar queries to rec-ommend URLs to frequen tly ask ed queries of a searc h en-gine. They used four notions of query distance based on: (1) keyw ords or phrases of the query; (2) string matc hing of keyw ords; (3) common clic ked URL's; and (4) the distance of the clic ked documen ts in some pre-de ned hierarc hy. Bef-ferman and Berger [4] also prop osed a query clustering tech-nique based on distance notion (3). As the average num ber of words in queries is small (ab out two) and the num ber of clic ks in the answ er pages is also small [1], notions (1) and (2) generate very sparse distance matrices. Notion (4) needs a concept taxonom y and the clic ked documen ts to be clas-si ed into the taxonom y, whic h cannot be done in a large scale. Also (3)is sparse, but this sparsit y can be diminished using large query logs, as in this pap er.

Fonseca et al [12] prop ose to disco ver related queries us-ing asso ciation rules. The query log is view ed as a set of transactions, with eac h transaction represen ting a session in whic h a single user submits a sequence of related queries in a time interv al. The metho d sho ws good results, but two problems arise: it is dicult to determine sessions of queries belonging to the same searc h pro cess; moreo ver the most in-teresting related queries, those submitted by di eren t users, cannot be disco vered, since the supp ort of a rule increases only if its queries app ear in the same query session (i.e. they are submitted by the same user.)
Baeza-Y ates et al. [2, 3] used the con ten t of clic ked Web pages to de ne a term-w eigh t vector mo del for a query . They consider terms in the URLs clic ked after a query . Eac h term is weigh ted according to the num ber of occurrences of the query and the num ber of clic ks of the documen ts in whic h the term app ears. Then the similarit y of two queries is equiv alen t to the similarit y of their vector represen tations, like the cosine distance function. This notion of query sim-ilarit y has sev eral adv antages. First, it is simple and easy to compute. On the other hand, it allo ws to relate queries that happ en to be worded di eren tly but stem from the same topic, hence capturing seman tic relationships among queries.Recen tly, Sahami and Heilman [15] used a query sim-ilarit y based on the snipp ets of the answ ers to the queries. However, they do not consider the feedbac k of the users (i.e. clic ked pages). Another related pap er de nes a query tax-onom y to cluster the answ ers [18], but query logs are not used, while in Cid et al. [5] they use query logs to main tain a taxonom y, but not to build one.

In the work by Chuang et al. [6, 7, 8, 13] query logs are used to build a query taxonom y, but no user feedbac k is used. This idea of building a taxonom y based on queries is extended in [9], but this is not the same as building a taxonom y of the queries, whic h is what we would call a query taxonom y. The closest work to ours is by Dupret and Mendoza [10] where relations among queries are de ned using the rank of clic ked URLS. Although with a di eren t goal, they do generate query relations that can be asso ciated to parts of a query taxonom y.
In this section we de ne a graph that naturally comes from user actions after a query . It is based on the notion (3) explained on the previous section. Before con tinuing, we de ne the main concepts used in the sequel of the pap er:
We are interested in the aggregation of equal queries (e.g. same set of words or same phrase) indep enden tly of other attributes of the query . Hence from now on, we will use query to denote the set of all equiv alen t queries (queries for whic h the set of words is the same), and we will use as URL cover the union of all covers for the equiv alen t queries. We now introduce a vectorial represen tation for the queries. Queries are represen ted as points in a high dimensional space, where eac h dimension corresp onds to a unique URL u . That is, a query is based on all the di eren t URLs in its URL cover. Giv en a query q , denote its represen tation with q . Eac h comp onen t of the vector q is assigned a weigh t equal to the frequency with whic h the corresp onding URL u has been clic ked for that query q . We now de ne a graph based on this vectorial represen tation.

Eac h query is a node of the graph. Tw o nodes (queries) are connected by an edge i they share at least one URL u (that is, their vectorial represen tations have one comp onen t that is positiv e in both queries). Hence we obtain a non-directed graph. Edges are weigh ted according to the cosine similarit y of the queries they connect. Hence, if e = f q; q and the URL space has D dimensions (total num ber of dif-feren t URLs), the weigh t of e is given by:
We de ne the weighte d degree of a node, the sum of the weigh ts of all its edges divided by its degree ( q ). That is, the weigh ted degree W ( q ) of a node q is:
This graph can be computed relativ ely fast. The edges can be computed by sorting the URLs clic ked by a query . The weigh ts of the edges can be computed in linear time in the worst case, much faster on average as queries do not share man y URLs. Let M be the maximal num ber of URLs between two queries. Hence the graph can be computed in max f M E; n log n g time, where E is the num ber of edges in the graph and n is the num ber of nodes. On average, M = O (1).
We now de ne di eren t edge types based on the set re-lations among URLs that connect two query nodes. We classify the edges in three types, suc h that there is an edge between q 1 and q 2 if: For most of the analysis of the graph that is carried out in the next section we do not consider this distinction between di eren t edges. On the other hand, for extracting kno wledge from the graph, we focus on the rst two types of edges.
These notions of set covers can be relaxed using a param-eter . That is, we say that a query q 1 is include d in q the Euclidean norm of the vector we obtain by pro jecting q on the URLs it shares with q 2 is at least . More formally , q 1 q 2 if and only if U C ( q 1 ; q 2 ) = U C q 1 \ U C q 2 .

We say that an edge e = f q 1 ; q 2 g is red if q 1 q 2 and q
We say that a directed edge e = ( q 1 ; q 2 ) is green if q
We use this relaxation as a way to lter the graph for noisy data, reducing also its size.
As we already said in the introduction, the main purp oses of this graph is to pro vide a tool for extracting seman tic re-lations between queries. One of the factors leading to edges represen ting relations of poor qualit y is the presence of Mul-titopic al URLs : URLs that cover either sev eral topics or a single very general topic. URLs related to e-commerce sites are examples of the former, and portals about music or travel of the latter. Even very di eren t queries can lead to clic ks on suc h sites, hence a shared clic k on suc h a URL is not very informativ e. If we were able to detect suc h URLs we could lter them out thus possibly pro viding relations of better qualit y. To this end, we prop ose a simple but e ectiv e ap-proac h. We start by noticing that low weigh t edges are more likely to represen t poor qualit y seman tic relations. Hence, URLs involved in suc h edges are not likely to be very spe-ci c. This leads us to this heuristic: consider low weigh t edges as voters and let the URLs be the candidates . Eac h edge votes for its URLs (that is, for the URLs its queries share.) Now we can sort URLs according to the num ber of votes they receiv ed: the more votes a URL gets, the more multitopical it is. We can then apply a threshold (either on the num ber of votes or of URLs we want to get rid o ) and compute a new graph without the selected URLs.
We have used sev eral query logs con taining up to fty mil-lion queries and the results are similar for all of them, so here we presen t data from only one log piece of 2005 coming from the Yaho o! searc h engine. For building this graph we have used all the queries in the log piece with at least one clic k. As this graph is very large, we have ltered both its edges and its nodes. Concerning the nodes, we have ltered the queries with few clic ks, while for the edges we have used a weigh t criterion. Intuitiv ely, when cutting low weigh t edges we are also reducing the noise, since those edges represen t weak relations between queries. Similarly , queries with few clic ks tend to be more noisy since we have less information at disp osal. The application that we have in mind for this graph (see section 5) leads us to consider also another way to lter our graph. This last technique, describ ed in the previous subsection, essen tially consists in detecting and l-tering out multitopical URLs. We rst describ e the data for the whole graph and its versions obtained by simpler ltering techniques. Data concerning the more re ned one are postp oned to subsection 5.2 were also an exp erimen tal evaluation of the technique is presen ted.

Table 1 gives the main characteristics of the di eren t ver-sions of the graph.F or all the generated graphs we have stud-ied di eren t parameters. In the follo wing eac h of them is dis-cussed, and the corresp onding plots are presen ted. Often, the observ ed beha vior clearly follo ws a power law. When Degree distribution, and Queries Frequencies vs. Node Degree. this is the case, the asso ciated law is also plotted. Moreo ver, in Table 2 the interested reader can nd the coecien ts of all the laws.

The left hand side of the rst row of Figure 1 sho ws the frequency of the queries in the whole log. Eac h entry in the query log is coun ted as a distinct query occurrence. Three curv es are sho wn, one coun ts the occurrences of all the queries, one coun ts the occurrences of the queries for whic h at least one result has been clic ked, and the last one the occurrences of the queries without any clic k. All the curv es exhibit a power law beha vior. Clic ked queries essen-tially sho w the same beha vior (and actually they follo w the same power law) of what is observ ed in whole log. On the other hand, queries without clic ks app ear to be more biased. The plot on the righ t hand side of the same row sho ws the query clic k distribution. Eac h point represen ts the num ber of queries ( y axis) with a given num ber of clic ks ( x axis). Again we can see that for the full graph there is a power law at work (and this holds also for its ltered versions, not sho wn in the plot.)
The plot on the left hand side of the second row sho ws the clic ks distribution from the URLs point of view. Eac h point represen ts the num ber of URLs with a given num ber of clic ks. Not very surprisingly , also this distribution clearly follo ws a power law. On the righ t hand side of the same row, the queries clic k distribution is plotted against the queries frequency . That is, eac h point represen ts the num ber of oc-currences and the num ber of clic ks for a given query . Both axis are normalized by dividing either by the total occur-rences ( x axis) or by the total num ber of clic ks ( y axis.) The bottom left part of the graph clearly sho ws much more variance than the rest. That is because less frequen t queries have more clic k variance while very frequen t queries tend to have a bit less than one clic k on average.

The plots on the last row of gure 1 are probably ones of the most interesting. On the left it is sho wn the node de-gree distribution (normalized so that eac h point sho ws the num ber of nodes with a given degree), while on the righ t one eac h node degree is plotted against the num ber of oc-currences of the corresp onding query . As we can see from the graph, the node degree distribution follo ws a power law. Moreo ver, this is true for all the graphs we have analyzed. Hence this prop erty seems to remain constan t even if you lter the graph, both on the nodes and on the edges. Thus, the cover graph we have generated could be seen as a kind of free-scale and autosimilar net work. We also underline that the curv e for the graph ltered according to the edge weigh t, sho ws even more clearly suc h a prop erty. Thus, low weigh t edges (weak seman tic relation between queries) are resp onsi-ble of the outliers and probably man y of them are just noise. Moreo ver, this kind of noise can be easily remo ved (by just ltering the low weigh t edges.)
On the rst row of gure 2, the nodes weigh t (left) and edges weigh t distributions are plotted. In both graphs, the y axis is normalized, sho wing the fraction of nodes (edges) with a given weigh t. The weigh t of a node is just its weigh ted degree, as previously de ned. The curv e for the graph that has been ltered according to the edges weigh t is obviously truncated at the threshold we have used. The same applies for the other graph.

The last plot, Figure 3 sho ws the connected comp onen ts distribution. Eac h points represen t the num ber of comp o-nen ts of a given size (expressed as a fraction of the total num ber of nodes). Also in this case we see that for all three graphs the distribution follo ws a power law. Interestingly enough, all graph exhibits a connected comp onen t that is a good fraction of the whole graph, resem bling someho w what is observ ed in the web graph. Moreo ver, the ratio between the size of the biggest connected comp onen t and the num-ber of nodes, varies according to whic h kind of ltering we apply . In particular, we can observ e that this ratio increases when the clic k lter is used but it gets smaller when the edge weigh t lter is used.

Tables 3 and 4 sho ws examples of type I (red) edges and type II (green) edges. The rst table has the webslang ex-amples at the end, detailing the type of equiv alence found. The second table sho ws path examples on similar queries. In gure 4 we sho w a small part of a cover graph, that in-cludes sev eral separate comp onen ts. Notice that edges with larger weigh ts are dark er, dashed (red) edges are iden tical covers and dotted (green) edges are complete covers. ltered
The weigh t asso ciated to an edge could be interpreted as a measure of the con dence we have in the fact that queries joined by that edge are related. Hence, it is interesting to study how the graph changes as only edges of increasing weigh t are left. Plots on the second row of gure 2 rep orts our nding to this resp ect. In particular we let the edge weigh t threshold vary between 0 (all edges are kept) and 0 : 9 (edges with weigh t lesser than 0 : 9 are discarded), and chec k how the node degree distribution, the num ber of iso-lated nodes and the size of the largest connected comp onen t change. The rst plot sho ws the node degree distribution: eac h point sho ws the degree of a node, with nodes being sorted in decreasing order with resp ect to their degree. In-terestingly enough, as we lter more and more edges, the beha viour of the distribution does not change much. How-ever, as the threshold increases, we can notice that long sequences of nodes with the same degree app ears: these are usually clique induced by a single URL. On the second plot, we can see that the percen tage of isolated nodes increases somewhat faster than linearly in the threshold value. As the threshold reac h 0 : 9, the graph gets almost completely disconnected. On the other hand, the size of the largest connected comp onen t does not decrease very fast: this in-dicates that to hea vily mo dify the structure of the graph man y edges have to be remo ved. Also similar observ ations hold for the connected comp onen t distribution, whic h we do not sho w here due to lack of space.
In this section we rep ort some exp erimen ts that we have carried out to assess how good in practice are the relation-ships represen ted by the edges of our graphs. Later, we pro vide an exp erimen tal evaluation of our heuristic for de-tecting multitopical URLs.
Evaluating the qualit y of seman tic relations is dicult, in particular for webslang, as there are no linguistic resources available. We mainly focus on the rst kind of edges, the ones induced by iden tical covers. From the user persp ectiv e queries in these edges are very related, since they give rise to clic ks on exactly the same set of URLs. We have done both a man ual evaluation by a human exp ert, and auto-matic evaluation, on di eren t samples of type I (red) edges. More precisely , for the evaluation we have used four di eren t samples of the edges, eac h of them con taining one thousand edges. The four samples di er in the num ber of clicks in-volved in their edges. Remem ber that an edge consists of two queries and that to eac h query is asso ciated the sets of URLs users clic ked after having submitted that query . Hence we can coun t, for eac h query , how man y times a user did clic k on one of the results (notice that of course this num ber is usually di eren t from the num ber of distinct URLs clic ked for a given query .) The rst sample is selected at random among edges that con tain queries with just one clic k as-sociated, that is, queries whic h, no matter of the num ber of times were submitted, gave rise to just one clic k. Our second sample was chosen by selecting (again at random) edges for whic h both queries have asso ciated at least four clic ks. The third sample is chosen by selecting, among all red(green) edges, those that maximize the s um of the clic ks asso ciated to the queries. The last sample is chosen by se-lecting those edges whic h maximize the minim um num ber of clic ks involved, that is edges that maximize the num ber of clic ks asso ciated to the query with less clic ks in the edge. The intuition behind varying the clic ks in the sample is that the more information (clic ks) we have about a query , the better the seman tic relation that we infer is going to be. For the automatic evaluation we have used data from the Op en Directory Pro ject 1 . As the reader may kno w, when a user types a query in ODP , besides site matc hes, we can also nd categories matc hes in the form of paths between direc-tories. Moreo ver, these categories are ordered by relev ance. For instance, the query \Spain" would pro vide (among oth-ers) the category \Regional/ Europ e/ Spain", while one of the results for \Barcelona" would be \Regional/ Europ e/ Spain/ Autonomous Comm unities/ Catalonia/ Barcelona". Hence, to measure how related two queries are, we can use a notion of similarit y between the corresp onding categories (as pro vided by ODP .) In particular, we measure the sim-ilarit y between two categories D and D 0 as the length of their longest common pre x P ( D; D 0 ) divided by the length of the longest path between D; D 0 . More precisely , denot-ing the length of a path with j D j , this similarit y is de-instance, the similarit y between the two queries above is 3 = 7 since they share the path \Regional/Europ e/Spain" and the longest one is made of sev en directories. We have evaluated the similarit y between two queries by measuring the similar-ity between the most similar categories of the two queries, among the top k answ ers pro vided by ODP , for various val-ues of k .

The rst row of plots in gure 5 sho ws the results con-cerning the red edges. On the y axis the similarit y is sho wn, while on the x axis we let the parameter k change. The results supp orts our intuition about using the clic ks as an indicator of the reliabilit y of the inferred relations. In fact, the samples for whic h we get the best results are the one that were chosen by maximizing the minim um num bers of clic ks in the pairs and the one in whic h both queries con tain at least four clic ks. That is because suc h edges are less noisy: for both queries we have a reasonable amoun t of information. The fact that we obtain good performances for the latter sample is also interesting from a applicativ e point of view: for practical purp oses this means that our technique can be applied to a large num ber of queries and not only to those with man y clic ks. On the righ t side we see that we can relax the graph a lot and still get good results (actually results in some cases even impro ve). On the other hand, we can see that by selecting edges that maximize the sum of the clic ks, we do even worse than if we select edges in whic h there are queries with one clic k. Intuitiv ely, that is because one noisy query (the one with few clic ks), is enough to mak e the edge not very informativ e. Moreo ver, the typical edge selected in this way con tains one query with man y clic ks and one with just one clic k, whic h mak es the rst query a good candidate for being a popular query and the second for being a rather obscure or not well form ulated query . Hence, having poor results for suc h edges is not really very surprising.
The same set of results concerning green edges are given in gure 6. Similarly to what happ ens with the red edges, here the samples that performs better are the one with at least four clic ks and the one that maximizes the minim um num ber of clic ks, for the same reasons discussed before. The sample that maximizes the sum of the clic ks performs even worse than the one in whic h there are queries with just one clic k. Actually , these edges seem not to be very interesting http://www.dmoz.org/
Table 5: Matc hes in ODP for both types of edges. since most of them con tains a very speci c query (with few clic ks, often just one, perhaps even a mistak e) and a very general one, like a searc h engine or a portal (with man y clic ks). Comparing gure 6 with gure 5, we can see that overall the similarit y in the rst plot is lower than in the second one. This is to be exp ected, since these edges ex-press weak er seman tic relations than the red ones, as they are mean t to describ e pairs of queries in whic h the rst one is more speci c than the second one. Hence a very high sim-ilarit y is nor exp ected neither desired. Actually , since the most obvious application of edges of this kind is for query recommendation/expansion/reform ulation, edges with per-fect similarit y are not very useful: for suc h edges you are likely to get the same set of results for both queries. Instead, we are more interested in queries whic h are similar, but not too much, since from suc h queries we can get di eren t re-sults. The second plot in the same gure sho ws for eac h connected comp onen t (except for the largest one) the per-cen tace of red and green edges. We can observ e that as long as the size of the comp onen t is relativ ely small, both comp o-nen ts where the vast ma jorit y of edges is red (or green) and comp onen ts where these kind of edges are a small percen t-age can be found. As the size of the comp onen t increases, the beha viour seems to somewhat stabilize.

On the second row of plots in gure 5, we can compare all three types of edges (for type I and II edges a sample with four clic ks was used as before). The rst plot sho ws that the kind of edges we are considering does mak e the di erence: red edges sho w a quite higher similarit y than green edges, whic h in turn sho w a much higher similarit y than partial cover edges. Moreo ver, as a sanit y tester, we also evaluated random edges between queries that in the graph do not share an edge: as the plot sho ws, this kind of edges sho w a very low similarit y. The second plot sho ws the ODP similarit y (x axis) versus the clic k similarit y (the weigh t) of the samples evaluated in the previous plot. As we can see, for all samples there is not a strong correlation between the two measures.
Table 5 sho ws for eac h of the samples that we studied, the percen tage of both type of edges that we were able to retriev e from ODP . We coun ted a positiv e matc h whenev er both queries had at least one category listed. We can notice that the sample for whic h we get the best results are also the ones for whic h there is a better coverage in ODP . This implies that the webslang relations could be up to more than 50% of all the extracted relations, and a high percen tage of them can be relev ant.
We exp erimen ted our approac h on the large graph and observ ed some interesting facts. Looking at the distribution of votes (plot omitted due to lack of space), whic h again follo ws a power law, we decided to cut the 400 URLs that got most votes. This brough t a very drastic change in the num ber of edges: it decreased from 361 to only 11 million. On the other hand and rather surprisingly , suc h a dramatic change concerning the edges, did not alter much the struc-
Table 6: Precision against votes for all samples. ture of the graph: the connected comp onen ts distribution re-mained almost unaltered, in particular the asso ciated power law exhibits exactly the same coecien ts. Also the num ber of isolated nodes remained essen tially unaltered. We regard these two facts as a good hin t that our approac h goes in the righ t direction for addressing the noise issue. In fact our results seem to indicate that we are able to remo ve edges without changing the underlying structure of the graph.
In order to evaluate the e ectiv eness of our classi cation, we have ordered the URLs by vote and selected the rst 2% among the URLs that receiv ed at least one vote. Among the original (ab out) ve million URLs, about 157K receiv ed at least one vote. We studied how the precision varies with the num ber of votes by man ually classifying three di eren t samples of 50 distinct URLs, tak en resp ectiv ely in the top, middle and bottom of the list. We man ually measured the precision by coun ting in eac h sample, how man y URLs were actually multitopical. For the evaluation, brok en URLs were remo ved and discarded. Table 6 sho ws how precision varies among the three samples. It also sho ws the votes range. As the table sho ws, we get quite good precision with the top of the list, while precision decreases as we go further down in the list. This implies that the num ber of votes and how multitopical a URL is, are correlated, pro viding exp erimen tal supp ort to our heuristic. Moreo ver, we also found this approac h to be, at least to some exten t successful with resp ect to our original goal: the ltered graph pro vided seman tic relations that gave an impro vemen t of up to 5% on the average similarit y.
Our results are really promising if we consider that the query log was actually small and over a short perio d of time. This implies that we can neither follo w patterns over time nor consider the num ber of di eren t users involved in the clic ks (more users, more wisdom). So an immediate exten-sion is to use larger logs and include information on unique users to classify the qualit y of results.

Also, we underline that the similarit y measure used in the exp erimen tal evaluation is quite strict: often queries that are not related according to ODP , are actually equiv alen t. For instance, the two queries "anc horage alask a newspap er" and "adn com", do not share any ODP category but they refer to the same thing: ( adn.c om is the site of the Anchor age Daily News .) Hence, the gures rep orted in the evaluation are more a lower bound on the performance we can achiev e, rather than an exact measure. In fact, we did a man ual evaluation of a random sample of 130 pairs from the rst two sample sets and we found that the precision was 63%, whic h is prett y close and larger than our results. From the relev ant pairs, we found that at least 40% were synon yms, 17% were site name-domain equiv alences and more than 5% were webslang (e.g. typos), whic h cannot be found normally in ODP .

Qualit y of results could also be impro ved by incorp orat-ing in the query represen tation information about the order of clic ked URLs. Actually , it is well kno wn that the clic ks distribution is biased by the order in whic h the results are presen ted.If we were able to remo ve the bias, and weigh t eac h clic k taking into accoun t a positional factor, then we could obtain a more faithful represen tation of the \informa-tion need" asso ciated with eac h query .

Further analysis of the structure of the graph can sur-face even more relations. For example, a graph clique often implies that there is at least one URL that was clic ked for all the queries in the clique. One poten tial reason could be again multi-topical URLs, so we can also use the graph to typify web con ten t.

The graph mining techniques prop osed in this pap er, ap-plied to larger query logs, can generate huge amoun ts of interesting relations: according to searc henginew atch.com, the num ber of queries of large searc h engines per day is of the order of hundred of millions. A one day graph would have then more than 10 billion edges. According to our re-sults, around 6% of them will be equiv alen t queries and 15% of them similar queries. Considering a 50% precision for the rst set and a 20% precision for the second, we have a total of 300 million poten tial interesting relations! That is, we are just seeing the tip of the iceb erg, because also the poten tial num ber of applications of query graphs is huge.
