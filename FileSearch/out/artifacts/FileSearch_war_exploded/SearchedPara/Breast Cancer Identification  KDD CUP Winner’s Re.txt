 We describe the ideas and methodologies that we developed in addressing the KDD Cup 2008 on early breast cancer detection, and discuss how they contributed to our success. The most important components of our solution were 1) the identification of predictive information in the patient identifier, 2) a linear SVM on the 117 provided features, and 3) a heuristic post-processing approach to optimize the evaluation criteria. The KDD Cup 2008 was organized by Siemens medical so-lutions and consisted of two prediction tasks in breast can-cer detection from images. The organizers provided data from 1712 patients for training; of these 118 had cancer. Siemens uses proprietary software to identify in each image (two views for each breast) suspect locations (called can-didates ), that are described by their coordinates and 117 features. No explanation of the features was given. Overall the training set includes 102,294 candidates, 623 of which are positive. A second dataset with similar properties was used as the test set for competition evaluation. The two modeling tasks were: Task 1: Rank the candidates by the likelihood of being can-cerous in decreasing order. The evaluation criterion for th is task was an area under the FROC curve, which measures how many of the actual patients with cancer are identified while limiting the number of candidate false alarms to a range between 0.2 and 0.3 per image. This was meant to reflect realistic requirements when the prediction model is used as an actual decision support tool for radiologists. Task 2: Suggest a maximal list of patients who are surely healthy. In this task, including any patient with cancer in the list will disqualify the entry. This was meant to be ap-propriate for a scenario where the model is used to save the radiologist work by ruling out patients who are definitely healthy , and thus the model was required to have no false negatives .
 Several aspects of the data and the tasks made this compe-tition interesting, including:  X  The presence of leakage, whereby patient IDs turned  X  Unique data properties, including the presence of ex- X  The unique FROC score, which treats patients as pos-We present our final submission briefly in Section 5. Leakage can be defined as the introduction of predictive in-formation about the target by the data generation, collec-tion, and preparation process. Such information leakage -while potentially highly predictive out-of-sample within the study -leads to limited generalization and model applica-bility, and to overestimation of the predictive performanc e. Two of the most common causes for leakage are: 1. Combination of data from multiple sources and/or mul-2. Accidental creation of artificial dependencies and addi-This year X  X  KDD Cup data suffered from leakage that was probably due to the first cause. The patient IDs in the com-petition data carried significant information towards iden -tifying patients with malignant candidates. This is best il -lustrated through a discretization of the patient ID range, as demonstrated in Figure 1. The patient IDs are natu-rally divided into three disjoint bins: between 0 and 20,000 (254 patients; 36% malignant); between 100,000 and 500,000 (414 patients; 1% malignant); and above 4,000,000 (1044 patients, of them 1.7% malignant). We can further observe that all 18 afflicted patients in the last bin have patient IDs in the range 4,000,000 to 4,870,000, and there are only 3 healthy patients in this range. This gives us a four-bin divi -sion of the data with great power to identify sick patients. This binning and its correlation with the patient X  X  health Figure 1: Distribution of malignant (black) and benign (gray) candidates depending on patient ID on the X-axis in log scale. The Y-axes is the score of a linear SVM model on the 117 features. Vertical lines show the boundaries of the identified ID bins. generalized to the test data. Our hypothesis is that this lea k-age reflects the compilation of the competition data from dif -ferent medical institutions and maybe different equipment, where the identity of the source is reflected in the ID range and is highly informative of the patient X  X  outcome. For ex-ample, one source might be a preventive care institution with only very low base rate of malignant patients and an-other could be a treatment-oriented institution with much higher cancer prevalence 1 .
 While it is clear that such leakage does not represent a usefu l pattern for real application, we consider its discovery and analysis an integral and important part of successful data analysis. Furthermore, we suggest that the problem in this dataset may actually run deeper and that all participants unknowingly benefited somewhat from a leakage problem and all reported performances are likely to be inflated. If the predictiveness of the identifiers is caused by the com-bination of data from different sources, there may be ad-ditional implicit leakages due to differences in data collec -tion settings (e.g., machine calibration). This would stil l be present even if the patient IDs had been removed. We test this hypothesis with the following experiment: If such a leakage exists (say the average grayscale is slightly differ ent), it should be possible to predict the data source (i.e., one of the four identifier bins) from negative candidates only. We cannot include positives because we already know that the cancer prevalence is correlated with the bins. Our analy-sis shows that both group 1 (ID below 20000) and group 4 (ID above 4,870,000) are easily identified by a logistic mode l from the 117 provided features with AUCs of 0.86 and 0.75 respectively.
 Given this result we feel confident to conclude that any rea-sonable model can infer the patient group to some extent from the 117 variables and thereby implicitly the cancer prevalence in that patient population. So all models built on this data set are likely to overestimate the true predicti ve performance of cancer detection when applied to an entirely different population.
 More generally, experience has shown that leakages occur in many modeling competitions, including KDD-Cup 2007 [4], where the organizers X  preparation of the data for one task exposed some information about the response for the other task; the INFORMS Data Mining Contest in 2008, were
The organizers later explained that in order to increase the number of positive examples, the dataset was comprised of examples from different time periods. it was possible to identify the partial removal of diagnosis codes used for the target construction; and KDD-Cup 2000 [1], where internal testing patterns that were left in the da ta by the organizers supplied a significant boost to those who were able to identify them.
 Exploratory data analysis seems to have become something of a lost art in the KDD community. In proper exploratory analysis, the modeler carefully examines the data with litt le preconception about what it contains, and allows patterns and phenomena to present themselves, only then analyzing them and questioning their origin and validity. We hope that our discovery of this leakage can serve as a reminder of the value of open-minded exploratory analysis. Given the obvious predictive value of the patient ID we in-corporated this information as a categorical variable for t he classification models with 4 possible bin numbers { 1,2,3,4 } . We also explored building 4 separate models, but this did not yield better results, presumably because for some of them the number of training examples is rather small. In order to investigate the generalization performance of d if-ferent methods, we created a stratified 50% training and test split by patient. We ensured that exactly half of the positiv e patients were assigned to each split. All results presented in Table 1 are based on our internal test set 2 .
 We explored the use of various learning algorithms for the underlying candidate classification problem including Neu -ral Networks, Logistic regression and several SVM variants using the SVMPerf package [2]. Ultimately, linear models (logistic regression or linear SVMs) yielded the most promi s-ing results. In this section we explore various directions f or improving the initial FROC results from the linear SVM of 0.0834 without and 0.0882 with a patient bin variable. Kernel selection: We compared linear SVMs, RBF kernels and polynomial kernels of degree 2 and 3. We found that linear kernels performed best, and have the added advantage of being extremely fast compared to the other approaches. The RBF kernels took the longest time to run and had a dis-mal performance of 0.0229. Given these results, we adopted linear SVMs for all following experiments.
 Loss function: Most work in the use of SVMs has fo-cused on minimizing the error rate or zero-one loss func-tion. In recent work, Joachims [2] presented efficient ways to train SVMs to maximize alternative multivariate perfor-mance measures, such as the area under the ROC curve (AUC). Given that the evaluation metric for Task 1 is re-lated to AUC, we trained an SVM to maximize AUC. We also compared maximizing Precision and Recall at k , which is the Precision/Recall of a classifier that predicts exactl y k instances as positive. In particular, given p positive in-stances in the training set, we used k = p/ 2 and k = 2 p for Precision and Recall respectively. Since AUC is most closel y related to the FROC metric, we find that maximizing it per-forms the best, improving the FROC from 0.0882 to 0.0893 but probably not significantly. The labeles of the true test set were never published. Table 1: Comparing FROC of different approaches on our 50% test set.
 Regularization and Bagging: As observed by Valentini and Dietterich [5], bagging can significantly improve clas-sification accuracy over a single SVM. Since bagging is a variance-reduction technique, they propose applying bag-ging to SVMs with low bias and high variance. In particular, for linear SVMs, they show that decreasing bias by increas-ing the regularization parameter and then applying bagging is very effective. We test the effectiveness of bagging in our setting for maximizing FROC. We applied 10 iterations of bagging linear SVMs with the regularization parameter c set to 20 and 500. We observed that with c = 20, bagging does improve the FROC to 0.090. For c = 500, however the FROC drops to 0.0873, so the accuracy results do not seem to carry over in our setting. The majority of the 117 features exhibit distributions with heavy tails and at times significant skew. We explore cut-ting outliers to improve our models estimation and to avoid extreme predictions caused by extreme x values on the sub-mission set. We calculated an upper and lower bound for each of the 117 features based on the inter-quartile range. We define Q 25 k as the 25th and Q 75 k as the 75th percentile of feature k . We replaced each value x k &lt; Q 25 k  X  3  X  ( Q with the lower bound Q 25 k  X  3  X  ( Q 75 k  X  Q 25 k ) and x 3  X  ( Q 75 k  X  Q 25 k ) with the upper bound Q 75 k + 3  X  ( Q This adjustment affected a non-negligible percentage of can -didates and reduced the linear SVM performance to 0.0858 and so we kept the extreme values unaltered. Candidate Location: We considered that the location of a candidate relative to nipple or breast boundary might be indicative of malignant candidates. Figure 2 shows the nor-malized and aligned locations of all candidates. It turned out that even sophisticated features derived from the loca-tion such as high-degree polynomials, functions of the dis-tance from the deduced boundaries etc. yielded no improve-ment. This indicates that either there are no  X  X igh risk X  areas in the breast, or that the 117 original features alread y contain the relevant location information of the candidate . Figure 2: Distribution of location of malignant (black dia-mond) and benign (gray dot).
 Number of candidates per patient: Another direction that could potentially lead to additional informative feat ures is the number of candidates belonging to a patient. Once again, all the attempts to construct features using this qua n-tity lead either to deterioration or non X  X ubstantial impro ve-ment of the performance on the test data. As suggested in one of the hints provided by the organizers, a cancerous lesion should be visible in both views v 1 and v a breast (although in extremely rare cases some lesions may only be visible in one view). It might therefore be possible to formulate meaningful constraints and exploit correlati ons in the classification decisions for candidates from the same region of a breast.
 Constraint Formulation: The data contains the coordi-nates of each candidate as well as the coordinates of the nipple in each image. Since the different views have dif-ferent directions, we cannot compare the coordinates di-rectly. Rather we identify pairs of candidates in terms of similar Euclidean distance to the nipple in both views for each breast. We define a match as a pair of candidates from different views with the difference of the Euclidean distance s less than a threshold. For a threshold of 20 this trans-lated into 29139 matched pairs { ( x k,v 1 , x k,v 2 ) } k  X  C C denotes the set of (29139) indexes. We would like can-didates belonging to a pair to have similar predicted labels , i.e. f ( x k,v 1 ) = f ( x k,v 2 ). We shall incorporate this condition into an optimization formula as a penalty term.
 Pairwise Constraint Kernel Logistic Regression: We use pairwise kernel logistic regression, which is able to pl ug in additional pairwise constraints together with labeled d ata to model the decision boundary directly [6]. Suppose we have a set of training examples { ( x i , y i ) } , and a set of pairs { ( x k,v 1 , x k,v 2 ) } k  X  C constructed from both labeled and unla-beled data. To make the optimization problem feasible to solve, we define a convex loss function via the logit loss as follows: O ( f ) = 1  X  n
X where the first term is the loss on labeled training examples, the second is the regularizer and third term is the loss as-sociated with the difference between the predicted labels of the example pairs. The pairwise constraint coefficient  X  is set to 1. For simplicity, we define f as a linear classifier, i.e. f ( x ) = w T x . Since the optimization function is convex, a gradient search algorithm can guarantee the finding of the global optimum. It is easy to derive the parameter estima-tion method using the interior-reflective Newton method, and we omit the detailed discussion. The constrained logis-tic regression unfortunately only yielded a FROC of 0.079. Task 1 uses the predictions from the classification model to order the candidates and computes the FROC metric based on this ordering. As observed in Section 3 attempting to maximize AUC improves ranking of candidates, which in turn often improves FROC. However, optimizing AUC is not guaranteed to optimize FROC. The AUC measures the area under the curve of true positive rate versus false positive rate, whereas the Y-axis of a FROC curve is the true posi-tive rate at a patient level. Contrary to AUC, a higher true positive rate at a candidate-level does not improve FROC unless the positive candidates are from different patients. For instance, it is better to have 2 correctly identified cand i-dates from different patients, instead of 5 correctly identi fied candidates from the same. So it might be possible to reorder candidates such that a larger variety of patients are repre-sented at the top of the list 3 .
 In order to do this, we create a pool of the top n candidates, as ordered by our model. We then select the candidates with the highest scores for each patient in this pool, and move these to the top of our list. We repeat this process iteratively with the remaining candidates in our pool until we have exhausted all candidates.
 We only do this for the top n candidates, since the FROC metric is based only on the area under the curve for a small range of false alarm rates at the beginning of the curve. We leave the ordering of the remaining candidates untouched. The only parameter this post-processing procedure require s is the choice of n for the number of top-ranked candidates we want to re-order. We can select this parameter based on an estimate of the maximum number of candidates that have to be classified as positive before we hit the upper bound of the false alarm rate used in the FROC metric. For the specific FROC metric used to evaluate Task 1, it can be shown that the optimum value for n is the number of positive candidates + 1.2  X  number of patients .
 Since the true number of positive candidates in the test set is not known, we estimate this from the positive rate in the training set. For our train-test split we used n = 1500, and report the results before and after post-processing in Table 1. This re-ordering of model scores had a significant impact on the resulting FROC, increasing the FROC of the bagged linear SVM model score from 0.09 to 0.093.
We have a theoretical result of an optimal reordering algo-rithm under the assumption that the model predictions are the correct probabilities of candidate malignancy. Howeve r, our attempts to calibrate the predictions failed to reach su f-ficient quality to take advantage of this optimality result. Task 1: For our final submission we used the bagged lin-ear SVM with the ID features, maximizing zero-one loss, c = 20 and heuristic post processing. This approach scored the winning result of 0.0933 on the test set.
 Task 2: While the ID leakage had some influence on our Task 1 model, it was clearly essential for our Task 2 submis-sion. It also explains the huge gap between our submission and the next best result. We investigated different mod-els with and without ID features after ranking increasingly by the maximum candidate score of each patient. Models without ID typically produce the first false positive patien t within about 200 patients of the training set (interestingl y they are the positive patients from bin 3 just at the bound-ary between the two bins as seen in Figure 1). Conversely, we were fairly confident that bin 4 alone (more than 1000 on the training and 970 on the submission set) had no positive patients at all. Models with ID features rank all patients of bin 4 first and next some patients from bin 2. The first false negatives occur typically around 1100. On this task -a logistic regression model performs slightly better than the linear SVM models due to the high sensitivity of likeli-hood to extreme errors. We finally submitted the 1020 first ranked patients from a logistic model that included the ID features in addition to the original 117 provided features. [1] A. Inger, N. Vatnik, S. Rosset, and E. Neumann. KDD-[2] T. Joachims. A support vector method for multivari-[3] J. Platt. Probabilistic outputs for support vector ma-[4] S. Rosset, C. Perlich, and Y. Liu. Making the most of [5] G. Valentini and T. G. Dietterich. Low bias bagged sup-[6] R. Yan, J. Zhang, J. Yang, and A. Hauptmann. A dis-
