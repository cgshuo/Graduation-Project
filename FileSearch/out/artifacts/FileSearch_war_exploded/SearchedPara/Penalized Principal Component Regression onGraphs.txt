 Simultaneous analysis of groups of system components with similar functions, or subsystems, has recently received considerable attention. This problem is of particular interest in high dimensional biological applications, where changes in individual components may not reveal the underlying biological phenomenon, whereas the combined effect of functionally related components could im-prove the efficiency and interpretability of results. This idea has motivated the method of gene set enrichment analysis (GSEA), along with a number of related methods [1, 2]. The main premise of this method is that by assessing the significance of sets rather than individual components (i.e. genes), interactions among them can be preserved, and more efficient inference methods can be developed. A different class of models (see e.g. [3, 4] and references therein) has focused on di-rectly incorporating the network information in order to achieve better efficiency in assessing the significance of individual components.
 These ideas have been combined in [5, 6], by introducing a model for incorporating the regulatory gene network, and developing an inference framework for analysis of subnetworks defined by bio-logical pathways. In this frameworks, called NetGSA, a global model is introduced with parameters for individual genes/proteins, and the parameters are then combined appropriately in order to assess the significance of biological pathways. However, the main challenge in applying NetGSA in real-world biological applications is the extensive computational time. In addition, the total number of parameters allowed in the model are limited by the available sample size n (see Section 5). In this paper, we propose a dimension reduction technique for networks, based on Laplacian eigen-maps, with the goal of providing an optimal low-dimensional projection for the space of random variables in each subnetwork. We then propose a general inference framework for analysis of sub-networks by reformulating the inference problem as a penalized principal regression problem on the graph. In Section 2, we review the Laplacian eigenmaps and establish their connection to principal component analysis (PCA) for random variables on a graph. Inference for significance of subnet-works is discussed in Section 3, where we introduce Laplacian eigenmaps with Neumann boundary conditions and present the group-penalized principal component regression framework for analy-sis of arbitrary subnetworks. Results of applying the new methodology to simulated and real data examples are presented in Section 4, and the results are summarized in Section 5. Consider p random variables X i , i = 1 ,..., p (e.g. expression values of genes) defined on nodes of an undirected (weighted) graph G = ( V , E ) . Here V is the set of nodes of G and E  X  V  X  V its edge set. Throughout this paper, we represent the edge set and the strength of associations among nodes through the adjacency matrix of the graph A . Specifically, A i j  X  0 and i and j are adjacent if the A i j (and hence A ji ) is non-zero. In this case we write i  X  j . Finally, we denote the observed values of the random variables by the n  X  p data matrix X .
 The subnetworks of interest are defined based on additional knowledge about their attributes and functions. In biological applications, these subnetworks are defined by common biological function, co-regulation or chromosomal location. The objective of the current paper is to develop dimension reduction methods on networks, in order to assess the significance of a priori defined subnetworks (e.g. biological pathways) with minimal information loss. 2.1 Graph Laplacian and Eigenmaps Laplacian eigenmaps are defined using the eigenfunctions of the graph Laplacian, which is com-monly used in spectral graph theory, computer science and image processing. Applications based on Laplacian eigenmaps include image segmentation and the normalized cut algorithm of [7], spec-tral clustering [8, 9] and collaborative filtering [10].
 The Laplacian matrix and its eigenvectors have also been used in biological applications. For exam-ple, in [11], the Laplacian matrix has been used to define a network-penalty for variable selection on graphs, and the interpretation of Laplacian eigenmaps as a Fourier basis was exploited in [12] to propose supervised and unsupervised classification methods.
 Different definitions and representations have been proposed for the spectrum of a graph, and the results may vary depending on the definition of the Laplacian matrix (see [13] for a review). Here, we follow the notation in [13], and consider the normalized Laplacian matrix of the graph. To that end, let D denote the diagonal degree matrix for A , i.e. D ii =  X  j A i j  X  d i , and define the Laplacian matrix of the graph by L = D  X  1 / 2 ( D  X  A ) D  X  1 / 2 , or alternatively It can be shown that [13] L is positive semidefinite with eigenvalues 0 =  X  0  X   X  1  X  ...  X   X  p  X  1  X  2. Its eigenfunctions are known as the spectrum of G , and optimize the Rayleigh quotient It can be seen from (1), that the 0-eigenvalue of L is g = D 1 / 2 1 , corresponding to the average over the graph G . The first non-zero eigenvalue  X  1 is the harmonic eigenfunction of L , which corresponds to the Laplace-Beltrami operator on Reimannian manifolds, and is given by More generally, denoting by C k  X  1 the projection to the subspace of the first k  X  1 eigenfunctions, 2.2 Principal Component Analysis on Graphs Previous applications of the graph Laplacian and its spectrum often focus on the properties of the graph; however, the connection to the probability distribution of the random variables on nodes of the graph has not been strongly emphasized. In graphical models, the undirected graph G among random variables corresponds naturally to a Markov random field [14]. The following result es-tablishes the relationship between the Laplacian eigenmaps and the principal components of the random variables defined on the nodes of the graph, in case of Gaussian observations. Lemma 1. Let X = ( X 1 ,..., X p ) be random variables defined on the nodes of graph G = ( V , E ) and denote by L and L + the Laplacian matrix of G and its Moore-Penrose generalized inverse. If X  X  N ( 0 ,  X  ) , then L and L + correspond to  X  and  X  , respectively (  X   X   X   X  1 ) . In addition, let  X  ,...,  X  p  X  1 denote the eigenfunctions corresponding to eigenvalues of L . Then  X  0 ,...,  X  p  X  1 are the principal components of X , with  X  0 corresponding to the leading principal component. Proof. For Gaussian random variables, the inverse covariance (or precision) matrix has the same non-zero pattern as the adjacency matrix of the graph, i.e. for i 6 = j ,  X  i j = 0 iff A i j = 0. Moreover,  X  autoregression (CAR) representation of Gaussian Markov random fields [16], we can write Therefore,  X  = T  X  1 ( I p  X  C ) and hence ( I p  X  C ) should be PD.
 Taking limit as  X   X  0, it follows that L and L + correspond to  X  and  X  , respectively. The second part follows directly from the above connection between  X  L  X  1 and  X  . In particular, suppose, without loss of generality, that  X  2 i = 1. Then, it is easily seen that the principal components of X are given by eigenfunctions of  X  L  X  1 , which are in turn equal to the eigenfunctions of  X  L with the ordering of the eigenvalues reversed. However, since eigenfunctions of L +  X  I p and L are equal, the principal components of X are obtained from eigenfunctions of L . Figure 1: Left: A simple subnetwork of interest, marked with the dotted circle. Right: Illustration of the Neumann random walk, the dotted curve indicates the boundary of the subnetwork. defined on graphs, can be given by assuming that the graph represents  X  X imilarities X  among random variables and using an optimal embedding of graph G in a lower dimensional Euclidean space 1 . In the case of one dimensional embedding, the goal is to find an embedding v = ( v 1 ,..., v p ) T that preserves the distances among the nodes of the graph. The objective function of the embedding by finding the eigenvector corresponding to the smallest eigenvalue of L .
 Lemma 1 provides an efficient dimension reduction framework that summarizes the information in the entire network into few feature vectors. Although the resulting dimension reduction method can be used efficiently in classification (as in [12]), the eigenfunctions of G do not provide any information about significance of arbitrary subnetworks, and therefore cannot be used to analyze the changes in subnetworks. In the next section, we introduce a restricted version of Laplacian eigenmaps, and discuss the problem of analysis of subnetworks. In [5], the authors argue that to analyze the effect of subnetworks, the test statistic needs to represent the pure effect of the subnetwork, without being influenced by external nodes, and propose an inference procedure based on mixed linear models to achieve this goal. However, in order to achieve dimension reduction, we need a method that only incorporates local information at the level of each subnetwork, and possibly its neighbors (see the left panel of Figure 1).
 Using the connection of the Laplace operator in Reimannian manifolds to heat flow (see e.g. [17]), the problem of analysis of arbitrary subnetworks can be reformulated as a heat equation with bound-ary conditions. It then follows that in order to assess the  X  X ffect X  of each subnetwork, the appropriate boundary conditions should block the flow of heat at the boundary of the set. This corresponds to insulating the boundary, also known as the Neumann boundary condition . For the general heat equation  X  ( v , x ) , this boundary condition is given by  X   X  the normal direction orthogonal to the tangent hyperplane at x .
 be any (connected) subnetwork of G , and denote by  X  S the boundary of S in G . The Neumann boundary condition states that for every x  X   X  S ,  X  y : { x , y } X   X  S ( f ( x )  X  f ( y )) = 0. The Neumann eigenfunctions of S are then the optimizers of the restricted Rayleigh quotient where C i  X  1 is the projection to the space of previous eigenfunctions. In [13], a connection between the Neumann boundary conditions and a reflected random walk on the graph is established, and it is shown that the Neumann eigenvectors can be alternatively calculated from the eigenvectors of the transition probability matrix of this reflected random walk, also known as the Neumann random walk (see [13] for additional details). Here, we generalize this idea to weighted adjacency matrices.
 Let  X  P and P denote the transition probability matrix of the reflected random walk, and the original random walk defined on G , respectively. Noting that P = D  X  1 A , we can extend the results in [13] as follows. For the general case of weighted graphs, define the transition probability matrix of the reflected random walk by given by  X  i = 1  X   X  i , where  X  i is the i th eigenvalue of  X  P .
 Remark 3 . The connection with the Neumann random walk also sheds light into the effect of the proposed boundary condition on the joint probability distribution of the random variables on the graph. To illustrate this, consider the simple graph in the right panel of Figure 1. For the moment, suppose that the random variables X 1 , X 2 , X 3 are Gaussian, and the edges from X 1 and X 2 to X 3 are directed. As discussed in [5], the joint probability distribution of the random variables on the graph is then given by linear structural equation models: Then, the conditional probability distribution of X 1 and X 2 given X 3 , is Gaussian, with the inverse covariance matrix given by A comparison between (3) and (4) then reveals that the proposed Neumann random walk corre-sponds to conditioning on the boundary variables, if the edges going from the set S to its boundary are directed. The reflected random walk, for the original problem, therefore corresponds to first setting all the influences from other nodes in the graph to nodes in the set S to zero (resulting in directed edges) and then conditioning on the boundary variables. Therefore, the proposed method offers a compromise compared to the full model of [5], based on local information at the level of each subnetwork. 3.1 Group-Penalized PCR on Graph Using the Neumann eigenvectors of subnetworks, we now define a principal component regression on graphs, which can be used to analyze the significance of subnetworks. Let N j denote the | S j | X  m j matrix of the m j smallest Neumann eigenfunctions for subgraph S j . Also, let X ( j ) be the n  X | S j | matrix of observations for the j -th subnetwork. An m j -dimensional projection of the original data the number of eigenfunctions m j for each subnetwork. A simple procedure determines a predefined threshold for the proportion of variance explained by each eigenfunction. These proportions can be determined by considering the reciprocal of Neumann eigenvalues (ignoring the 0-eigenvalue). To simplify the presentation, here we assume m j = m ,  X  j . The significance of subnetwork S j is a function of the combined effect of all the nodes, captured (MANOVA) model. Formally, let y be the mn  X  1 vector of observations obtained by stacking all the transformed data matrices  X  X ( j ) . Also, let X be the mn  X  Jmr design matrix corresponding to the experimental settings, where r is the number of parameters used to model experimental conditions, and  X  be the vector of regression coefficients. For simplicity, here we focus on the case of a two-class inference problem (e.g. treatment vs. control). Extensions to more general experimental settings follow naturally and are discussed in Section 5.
 To evaluate the combined effect of each subnetwork, we impose a group penalty on the coefficient of the regression of y on the design matrix X . In particular, using the group lasso penalty [18], we estimate the significance of the subnetwork by solving the following optimization problem 2 where J is the total number of subnetworks considered and X ( j ) and  X  ( j ) denote the columns of X , and entries of  X  corresponding to the subnetwork j , respectively.
 In equation (5),  X  is the tuning parameter and is usually determined by performing k-fold cross vali-dation or evaluation on independent data sets. However, since the goal of our analysis is to determine the significance of subnetworks,  X  should be determined so that the probability of false positives is controlled at a given significance level  X  . Here we adapt the approach in [20] and determine the optimal value of  X  so that the family-wise error rate (FWER) in repeated sampling with replacement (bootstrap) is controlled at the level  X  . Specifically, let q i  X  be the total number of subnetworks con-sidered significant based on the value of  X  in the i th bootstrap sample. Let  X  be the threshold for ficients corresponding to subnetwork j in the i th bootstrap sample, the subnetwork j is considered significant if max  X  P ( j ) i  X   X  . Using this method, we select  X  such that q i  X  = p ( 2  X   X  1 )  X  p . 3 The following result shows that the proposed methodology correctly selects the significant subnet-works, while controlling FWER at level  X  . We begin by introducing some additional notations and assumptions. We assume the columns of design matrix X are normalized so that n  X  1 X i T X i = 1, Throughout this paper, we consider the case where the total number of nodes in the graph p , and the number of design parameters r are allowed to diverge (the p n setting). In addition, let s be the total number of non-zero elements in the true regression vector  X  .
 Theorem 4. Suppose that m , n  X  1 and there exists  X   X  1 and t  X  s  X  1 such that n  X  1 X T X i j  X  ( 7 are independent. If the tuning parameter  X  is selected such that such that q  X  = p ( 2  X   X  1 )  X  r p, (i) there exists  X  =  X  ( n , p ) &gt; 0 such that  X   X  0 as n  X   X  and with probability at least 1  X   X  the significant subnetworks are correctly selected with high probability, (ii) the family-wise error rate is controlled at the level  X  .
 Outline of the Proof. First note that the MANOVA model presented above can be reformulated as a multi-task learning problem [21]. Upon establishing the fact that for the proposed tuning parameter  X   X  p log p / ( nm 3 / 2 ) , it follows from the results in [22] that for each bootstrap sample, there exists  X  model with hight probability. In particular, it can be shown that  X  =  X  { where B is the number of bootstrap samples and  X  is the cumulative normal distribution. This proves the first claim.
 network are orthogonal, imply that for each j ,  X  X ( j ) i , i = 1 ,..., m are independent. Moreover, the realizations of i.i.d standard normal random variables. On the other hand, the KarushKuhnTucker sgn (  X   X  ( j ) )  X  , where  X  x , y  X  denotes their inner product. It is hence clear that 1 [ Combining this with the first part of the theorem, the claim follows from Theorem 1 of [20]. Remark 5 . The main assumption of Theorem 4 is the independence of the variables in different sub-networks. Although this is not satisfied in general problems, it may be satisfied by the conditioning argument of Remark 3. It is possible to further relax this assumption using an argument similar to Theorem 2 of [20], but we do not pursue this here. We illustrate the performance of the proposed method using simulated data motivated by biological applications, as well as a real data application based on gene expression analysis. In the simulation, we generate a small network of 80 nodes (genes), with 8 subnetworks. The random variables (ex-pression levels of genes) are generated according to a normal distribution with mean  X  . Under the null hypothesis,  X  null = 1 and the association weight  X  for all edges of the network is set to 0.2. The setting of parameters under the alternative hypothesis are given in Table 1, where  X  alt = 3. These settings are illustrated in the left panel of Figure 2. Table 1 also includes the estimated powers of the tests for subnetworks based on 200 simulations with n = 50 observations. It can be seen that the proposed GPCR method offers improvements over GSEA [1], especially in case of subnetworks 3 and 6. However, it results in a less accurate inference compared to NetGSA [5].
 In [5], the pathways involved in Galactose utilization in yeast were analyzed based on the data from [23], and the performances of the NetGSA and GSEA methods were compared. The interactions among genes, along with significance of individual genes (based on single gene analysis) are given in the right panel of Figure 2, and the results of significance analysis based on NetGSA, GSEA analysis indicate that GPCR results in improved efficiency over GSEA, while failing to detect the significance of some of the pathways detected by NetGSA. We proposed a principal component regression method for graphs, called GPCR, using Laplacian eigenmaps with Neumann boundary conditions. The proposed method offers a systematic approach Table 1: Parameter settings under the alternative and estimated powers for the simulation study. Figure 2: Left: Setting of the simulation parameters under the alternative hypothesis. Right: Net-work of yeast genes involved in Galactose utilization. for dimension reduction in networks, with a priori defined subnetworks of interest. It can also incor-porate both weighted and unweighted adjacency matrices and can be easily extended to analyzing complex experimental conditions through the framework of linear models. This method can also be used in longitudinal and time-course studies.
 Our simulation studies, and the real data example indicate that the proposed GPCR method offers significant improvements over the methods of gene set enrichment analysis (GSEA). However, it does not achieve optimal powers in comparison to NetGSA. This difference in power may be at-tributable to the mechanism of incorporating the network information in the two methods: while NetGSA incorporates the full network information, GPCR only account for local network informa-tion, at the level of each subnetwork, and restricts the interactions with the rest of the network based on the Neumann boundary condition. However, the most computationally involved step in Net-GSA requires O ( p 3 ) operation, whereas the computational cost of GPCR is O ( m 3 ) . It is clear that since m p in most applications, GPCR could result in significant improvement in terms of com-putational time and memory requirements for analysis of high dimensional networks. In addition, NetGSA requires that r &lt; n , whilst the dimension reduction and the penalization of the proposed GPCR removes the need for any such restriction and facilitates the analysis of complex experiments in the settings with small sample sizes.
 Acknowledgments Funding for this work was provided by NIH grants 1RC1CA145444-0110 and 5R01LM010138-02.
