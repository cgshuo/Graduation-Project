 We are increasingly confronted with very high dimensional data in speech signals, images, gene-expression data, and other sources. Manifold Learning can be loosely defined to be a collection of methodologies that are motivated by the belief that this hypothesis (henceforth called the manifold hypothesis) is true. It includes a number of extensively used algorithms such as Locally Linear Embedding [17], ISOMAP [19], Laplacian Eigenmaps [4] and Hessian Eigenmaps [8]. The sample complexity of classification is known to be independent of the ambient dimension [15] under the manifold hypothesis, (assuming the decision boundary is a manifold as well,) thus obviating the curse of dimensionality. A recent empirical study [6] of a large number of 3  X  3 images, represented as points in R 9 revealed that they approximately lie on a two dimensional manifold known as the Klein bottle. On the other hand, knowledge that the manifold hypothesis is false with regard to certain data would give us reason to exercise caution in applying algorithms from manifold learning and would provide an incentive for further study.
 manifold. Our primary technical results are the following. One technical contribution of this paper is the use of dimensionality reduction via random projec-tions in the proof of Theorem 5 to bound the Fat-Shattering dimension of a function class, elements of which roughly correspond to the squared distance to a low dimensional manifold. The application of the probabilistic method involves a projection onto a low dimensional random subspace. This is then followed by arguments of a combinatorial nature involving the VC dimension of halfspaces, and the Sauer-Shelah Lemma applied with respect to the low dimensional subspace. While random projections have frequently been used in machine learning algorithms, for example in [2, 7], to our knowledge, they have not been used as a tool to bound the complexity of a function class. We il-lustrate the algorithmic utility of our uniform bound by devising an algorithm for k  X  means and a convex programming algorithm for fitting a piecewise linear curve of bounded length. For a fixed error threshold and length, the dependence on the ambient dimension is linear, which is optimal since this is the complexity of reading the input. In the context of curves, [10] proposed  X  X rincipal Curves X , where it was suggested that a natural was proposed by [12], where they attempted to find piecewise linear curves of bounded length which minimize the expected squared distance to a random point from a distribution. This paper studies dependence of the error rate on the ambient dimension and the bound on the length. We address this in a more general setup in Theorem 4, and obtain sample complexity bounds that are independent of the ambient dimension, and depend linearly on the bound on the length. There is a significant amount of recent research aimed at understanding topological aspects of data, such its homology [20, 16]. the sample complexity of Empirical Risk minimization on k  X  means applied to data in a unit ball of arbitrary dimension is tight. Here is the desired bound on the error and  X  is a bound on the Rademacher complexities. We improve this bound to O k 2 min k, log 4 k 2 + log 1  X  2 , using an argument that bounds the Fat-Shattering dimension of the appropriate function class using random principal manifolds in certain regularized settings have been studied in [18]. There, the sample complexity was related to the decay of eigenvalues of a Mercer kernel associated with the regularizer. complexity s that is independent of m and depends at most linearly on k , which also leads to an approximation algorithm with additive error, based on sub-sampling. If one allows a multiplicative error of 4 in addition to an additive error of , a statement of this nature has been proven by Ben-David (Theorem 7, [5]). In the remainder of the paper, C will always denote a universal constant which may differ across the paper. For any submanifold M contained in, and probability distribution P supported on the from P , a tolerance and a class of manifolds F , Empirical Risk Minimization (ERM) outputs a parameters , X  , and a rule A that outputs a manifold in F when provided with a set of samples, we define the sample complexity s = s ( , X , A ) to be the least number such that for any probability from P is N , then P [ L ( N , P ) &lt; inf M X  X  L ( M , P ) + ] &gt; 1  X   X . 3.1 Bounded intrinsic curvature Let M be a Riemannian manifold and let p  X  X  . Let  X  be a geodesic starting at p .
 Definition 1. The first point on  X  where  X  ceases to minimize distance is called the cut point of p along M . The cut locus of p is the set of cut points of M . The injectivity radius is the minimum taken over all points of the distance between the point and its cut locus. M is complete if it is complete as a metric space.
 Let G i = G i ( d,V, X , X  ) be the family of all isometrically embedded, complete Riemannian sub-manifolds of B having dimension less or equal to d , induced d  X  dimensional volume less or U Theorem 1. Let and  X  be error parameters. If and x = { x 1 ,...,x s } is a set of i.i.d points from P then, The proof of this theorem is deferred to Section 4. 3.2 Bounded extrinsic curvature We will consider submanifolds of B that have the property that around each of them, for any radius follows.
 Definition 2 (Condition Number) . Let M be a smooth d  X  dimensional submanifold of R m . We for any r &lt;  X  no two normals of length r that are incident on M have a common point unless it is on M .
 Let G e = G e ( d,V, X  ) be the family of Riemannian submanifolds of B having dimension  X  d , volume  X  V and condition number  X  1  X  . Let and  X  be error parameters. Let U ext ( 1 ,d, X  ) := Theorem 2. If and x = { x 1 ,...,x s } is a set of i.i.d points from P then, injectivity radius suffice to ensure that they can be covered by relatively few  X  balls. Let V M p be the volume of a ball of radius M centered around a point p . See ([9], page 51) for a proof of the following theorem.
 Theorem 3 (Bishop-G  X  unther Inequality) . Let M be a complete Riemannian manifold and assume that r is not greater than the injectivity radius  X  . Let K M denote the sectional curvature of M and let  X  &gt; 0 be a constant. Then, K M  X   X  implies V M p ( r )  X  2  X  Thus, if &lt; min(  X ,  X  X   X  1 2 2 ) , then, V M p ( ) &gt; Cd d .
 Proof of Theorem 1. As a consequence of Theorem 3, we obtain an upper bound of V Cd d on B there is no point p  X  X  such that min balls, S The proof of Theorem 2 is along the lines of that of Theorem 1, so it has been deferred to the journal version. In this section, we show that uniform bounds relating the empirical squares loss and the expected squared loss can be obtained for a class of manifolds whose covering number at a different scale has a specified upper bound. Let U : R +  X  Z + be any integer valued function. Let G be any family of subsets of B such that for all r &gt; 0 every element M  X  G can be covered using open Euclidean of k  X  tuples of points, U (1 /r ) can be taken to be the constant function equal to k and we recover the k  X  means question. A priori, it is unclear if variable (although if the set is countable this is true). However (2) is equal to Thus, for a fixed n , the quantity in the limits is a random variable. Since the limit as n  X   X  of a sequence of bounded random variables is a random variable as well, (2) is a random variable too. Theorem 4. Let and  X  be error parameters. If Then, that g is covered by the union of balls of radius / 16 centered at these points. Thus, for any point x  X  B , 2 c := { c 1 ,...,c k } and a point x  X  B , let f c ( x ) := d ( x, c ( g, )) 2 . Then, For any set of s samples x 1 ,...,x s , By Hoeffding X  X  inequality, which is less than  X  2 .
 By Theorem 5, P Therefore, P The core of the uniform bounds in Theorems 1 and 2 is the following uniform bound on the minimum of k linear functions on a ball in R m .
 Theorem 5. Let F be the set of all functions f from B := { x  X  R m : k x k X  1 } to R , such that for some k vectors v 1 ,...,v k  X  B , Independent of m , if then It has been open since 1997 [3], whether the known lower bound of C k 2 + 1 2 log 1  X  on the sample complexity s is tight. Theorem 5 in [14], uses Rademacher complexities to obtain an upper bound of (The scenarios in [3, 14] are that of k  X  means, but the argument in Theorem 4 reduces k  X  means to our setting.) Theorem 5 improves this to by putting together (11) with a bound of obtained using the Fat-Shattering dimension. Due to constraints on space, the details of the proof of Theorem 5 will appear in the journal version, but the essential ideas are summarized here. VC theory to bound u , but doing so directly leads to a linear dependence on the ambient dimension linear subspace and the image under an appropriately scaled orthogonal projection R of the points x  X   X  shattered. Using Vapnik-Chervonenkis theory and the Sauer-Shelah Lemma, we then show that to conclude that fat F ( 24 )  X  Ck 2 log 2 k . By a well-known theorem of [1], a bound of Ck 2 log 2 k on fat F ( 24 ) implies the bound in (13) on the sample complexity, which implies Theorem 5. Let K be a universal constant whose value will be fixed throughout this section. In this section, we will state lower bounds on the number of samples needed for the minimax decision rule for learning from high dimensional data, with high probability, a manifold with a squared loss that is within of the optimal. We will construct a carefully chosen prior on the space of probability distributions and use an argument that can either be viewed as an application of the probabilistic method or of the fact that the Minimax risk is at least the risk of a Bayes optimal manifold computed with respect to this prior. Let U be a K 2 d k  X  dimensional vector space containing the origin, spanned by the basis { e Let S 1 ,...,S K 2 d k denote spheres, such that for each i , S i := S  X  ( R . The following theorem shows that no algorithm can produce a nearly optimal manifold with high probability unless it uses a number of samples that depends linearly on volume, exponentially on intrinsic dimension and polynomially on the curvature.
 Theorem 6. Let F be equal to either G e ( d,V, X  ) or G i ( d,V, 1  X  2 , X  X  ) . Let k = b V be an arbitrary algorithm that takes as input a set of data points x = { x 1 ,...,x k } and outputs a manifold M A ( x ) in F . If + 2  X  &lt; 1 3 1 where P ranges over all distributions supported on B and x 1 ,...,x k are i.i.d draws from P . Proof. Observe from Lemma ?? and Theorem 3 that F is a class of a manifolds such that of the last sentence, one needs some leeway.) Let P 1 ,..., P ` be probability distributions that are uniform on {M 1 ,..., M ` } with respect to the induced Riemannian measure. Suppose A is an al-Let r be chosen uniformly at random from { 1 ,...,` } . Then, inf We first prove a lower bound on inf x E r [ L ( M A ( x ) , P r ) | x ] .
 We see that Conditioned on x , the probability of the event (say E dif ) that x k +1 does not belong to the same sphere as one of the x 1 ,...,x k is at least 1 2 .
 { S i } which contain at least one point among x 1 ,...,x k .
 y ,...,y However, it is easy to check that for any dimension m , the cardinality of the set S y of all S i that have a nonempty intersection with the balls of radius 1 K + 2  X  , because L ( M A ( x ) , P r ) is bounded above by 2 . 8.1 k  X  means Applying Theorem 4 to the case when P is a distribution supported equally on n specific points (that are part of an input) in a unit ball of R m , we see that in order to obtain an additive approximation for the k  X  means problem with probability 1  X   X  , it suffices to sample points uniformly at random (which would have a cost of O ( s log n ) if the cost of one random bit is O (1) ) and exhaustively solve k  X  means on the resulting subset. Supposing that a dot product between two vectors x i , x j can be computed using  X  m operations, the total cost of sampling and multiplicative (1 + ) approximation, the best running time known depends linearly on n [13]. If P is an unknown probability distribution, the above algorithm improves upon the best results in a natural statistical framework for clustering [5]. 8.2 Fitting piecewise linear curves In this subsection, we illustrate the algorithmic utility of the uniform bound in Theorem 4 by ob-taining an algorithm for fitting a curve of length no more than L , to data drawn from an unknown probability distribution P supported in B , whose sample complexity is independent of the ambient dimension. This curve, with probability 1  X   X  , achieves a mean squared error of less than more than the optimum. The proof of its correctness and analysis of its run-time have been deferred to the journal version. The algorithm is as follows: We are grateful to Stephen Boyd for several helpful conversations. [1] Noga Alon, Shai Ben-David, Nicol ` o Cesa-Bianchi, and David Haussler. Scale-sensitive di-[2] Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and [3] Peter Bartlett. The minimax distortion redundancy in empirical quantizer design. IEEE Trans-[4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data [5] Shai Ben-David. A framework for statistical clustering with constant time approximation al-[6] Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society , 46:255 X  [7] Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS , pages 634 X 644, 1999. [8] David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding [9] A. Gray. Tubes . Addison-Wesley, 1990. [10] Trevor J. Hastie and Werner Stuetzle. Principal curves. Journal of the American Statistical [11] William Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert [13] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A simple linear time (1+ )  X  approximation [14] Andreas Maurer and Massimiliano Pontil. Generalization bounds for k-dimensional coding [15] H. Narayanan and P. Niyogi. On the sample complexity of learning smooth cuts on a manifold. [16] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submani-[17] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear [18] Alexander J. Smola, Sebastian Mika, Bernhard Sch  X  olkopf, and Robert C. Williamson. Regu-[19] J. B. Tenenbaum, V. Silva, and J. C. Langford. A Global Geometric Framework for Nonlinear [20] Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete &amp; Compu-
