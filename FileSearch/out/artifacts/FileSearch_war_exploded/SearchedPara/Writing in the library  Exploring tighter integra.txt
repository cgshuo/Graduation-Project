 1. Introduction
Great progress has been made in providing people with online access to information. A combination of digitization activities, digital repositories, digital libraries, indexing, metadata provision and ever more sophis-ticated and integrated search and browse features are already having significant impacts on the way people search for information and integrate these practices into their lives. In this paper we aim to look at a range of activities and design opportunities made possible by these new technologies and resources. We want to consider the potential of a tighter integration of information searching, using information and writing. Would it help to be able to write while you search? If not in all circumstances, then when and why? Can we support more spiral-like approaches to searching, reading and writing than those inspired by more waterfall-like mod-els of the writing process? Typically the technologies to support searching, reading, organizing and writing are considered separately, leading to separate applications. But just because we have traditionally done it this way, does that mean it is the best way to do it? Pre-computer technologies for searching, reading and writing (books, paper, index cards, pens etc.) allowed far more integrated use. One can search, read and write inside a physical library: why not in a digital one?
In this exploratory work we restrict ourselves to a very limited but well known, well studied and easily accessible kind of information use: scholarly writing. Future work will need to consider the generalizability of the findings to wider contexts. Scholarly writing has conventionally been divided into a series of somewhat distinct activities: (1) Searching for the information (2) Organizing, analyzing, systematizing, synthesizing, obtaining insights, planning the report (3) Writing the report
These are often considered to be done in the order shown. Digital library (DL) systems and researchers in information retrieval have mostly looked to support 1 and parts of 2. Conventionally word processors and researchers in computers and writing have only looked to support 3 and parts of 2. Researchers examining information use, scholarly work, and the production of scholarship look at 1, 2 and 3, but mostly report on what is done, and rarely make recommendations for systems redesign. We want to explore the development of tools that explicitly support 1, 2 and 3, and enable people to do them in any order switching between them as they like. That is, we want to explore re-contextualizing information search into the process of writing ( McNee, Riedl, &amp; Konstan, 2006a, 2006b ). We acknowledge that this is not universally appropriate or desir-able, but we do believe there are circumstances when it might be very helpful. We agree with Hendry X  X  com-ments that:
Of course skilled writers have always used techniques to integrate searching, analyzing and writing more tightly. These can range from strewing papers around a desk and the floor to juggling multiple windows and applications on a PC. However these approaches can be hard to manage, meaning that some people cur-rently compartmentalize the actions rather than integrating and hence contextualizing them  X  just because of the constraints of the available tools. Consequently we want to support a contextualization that currently only happens relatively rarely.

Our aim is to sketch out a number of spaces and issues that will need to be addressed. We are not proposing a solution and evidence for its adoption. Rather our ongoing systems development and evaluation is intended to explore the design space and uncover the critical issues necessary for more systematic design. The prototype system reported here is just a single data point in this space, developed to enable us to explore the space in a more purposeful manner. As well as exploring the design space, we try to show the emergent research issues, the multidisciplinary challenge (and opportunities) arising from investigating a topic at the edge of a number of different research traditions, the potential of rapid prototyping approaches to investigating novel applica-tions, and the difficulties of evaluating a system that can have many kinds of use, many emergent as people appropriate it and innovate with it, changing their searching X  X eading X  X riting activities in the process.
In this paper we review existing studies on tools to integrate writing and searching. We then describe PIRA, our web-based tool for linking document authoring and searching. Section 4 describes the results of user trials with PIRA, and we then discuss the methodological implications of evaluating such systems. 2. Related work
A system to support integrated information search and use should be informed by research in a number of areas. As with all multidisciplinary research, this is both a great opportunity and a great burden. Given multiple literatures to consider, the following review is rather selective, but aims to sample from different lit-eratures to show the importance of integrating knowledge from multiple fields into the ongoing development process. 2.1. Studies of the writing process
Writing, particularly academic writing, can be a challenge for experts and novices alike. Digital libraries have greatly improved the ease with which we can search for information, even from our desktop. It is easier to integrate searching and writing activities when both are done in different windows on the same computer.
Nevertheless, the act of writing remains difficult. The very accessibility of so much information through ever more complete DLs with ever more sophisticated search functionalities can mean that searching and reading articles turns into something of a displacement activity, postponing the dreadful moment of starting work on the paper. Sadly this kind of problem is one more likely to be experienced by the more diligent, perfectionist student, a personality trait particularly evident at the graduate level.

There is a literature in writing studies that advocates for a tighter integration of writing and searching as a way to improve the quality of final papers. It is viewed as important to start writing at the early stages of one X  X  research because the writing process can be considered as such that stimulates learning. Emig (1977) , for exam-ple, studying various definitions of the learning process by some of the most influential psychologists of the 20th century, discovered clear correspondences between writing and learning. Nelson and Hayes (1988) found that more experienced writers were inclined to employ an issue-driven (writing down preliminary thoughts, looking for supportive sources, reading) rather than a content-driven (exhaustive information search, reading, and only then writing) approach. There is also some evidence that at least some successful students tightly integrate information search, reading and writing ( Fister, 1992 ). We refer to the  X  X ssue-driven X  approach as  X  X rite while you search X . We wondered if a tool that encouraged a focus on writing, but provided a back-ground, almost ambient search functionality would be helpful and encourage researchers to change their hab-its to the more productive  X  X rite while you search X  approach.

Furthermore,  X  X riting-to-learn X , and specifically the knowledge transforming model ( Bereiter &amp; Scardama-lia, 1987 ) has considerable theoretical and empirical backing in educational psychology, as a manifestation of constructivism and creating a variety of opportunities for computer supported collaborative learning. In this paper we focus on individual aspects of integrated writing processes, but there are clearly many opportunities for collaborative extensions. 2.2. Studies of scholarly practice
In studying how scholars actually work, a number of researchers have noted the interweaving of writing with the consulting of resources, and indeed searching activities. Brockman, Neumann, Palmer, and Tidline (2001) note that:
The difficulties of conventional searching lead to the accumulation of resources creating temporary and per-manent personal collections and notes distilling information. As online searching becomes easier, it becomes possible to do more  X  X ust in time X  searching, although such personal collections are unlikely to disappear.
After conducting an extensive review of various information seeking models as well as two influential mod-els of writing ( Hayes &amp; Flower, 1980; Sharples, 1996 ), Attfield (2004) came to the conclusion that: 2.3. Models of information use The lack of a theoretical foundation greatly contributes to the challenges of designing an integrated tool.
To the best of our knowledge, no one model explicitly describes information searching and writing processes within the same framework of the user X  X  information behavior. At first glance, Kuhlthau X  X  model of Informa-tional Seeking ( Kuhlthau, 2004 ) may seem to cover the integrated approach to searching and writing. How-ever, while this model provides some initial explanation to how task and information seeking behavior may cifically ( Elmborg &amp; Hook, 2005 ). On the other hand, those models that do attempt to consider a work task such as Vakkari X  X  task-based Information Retrieval model ( Vakkari, 2001 ) are too broad to cover each indi-vidual task like writing.

The classic berrypicking model of information search ( Bates, 1989 ) can act as a base in understanding longer term contextualized use. As the searcher evolves their goal the system X  X  main model of their progress, the berrybasket, is incrementally modified through the addition and deletion of collection items. When the task of writing is integrated with that of searching the main system representation becomes the evolving doc-ument. This partially-authored document is a rich source of detail about the user X  X  context but is also more complex and structured than a list of documents in the standard berrybasket approach. Although as a docu-ment can itself contain a list of references then it can subsume the notion of a berrybasket and potentially build richer basket structures. However, the lack of structure in most evolving documents means that the task of automating future queries is more complex than in structured berrybasket-like approaches such as Sense-
Maker ( Baldonado, 2000 ). 2.4. Considerations of digital library use in context
There has been a growing interest in the importance of studying digital library use in context (e.g. Covi, 1999; Blandford, 2006 ). For example, Carr et al. (2004) pointed out that DLs should not be just a static archive. Instead, DLs should be aware of user X  X  information need and context. The authors called this a  X  X roadening X  view of DLs. This is a natural progression from earlier concentrations on the mechanics of dig-itization and the collection and aggregation of online resources, the provision of useful search functionalities and the development of better user interfaces.

Some of these studies show that as users have available to them ever richer sources of information, both larger DLs and multiple DLs, their information searching activities change, revealing both more satisfying behaviors and evidence of difficulties (and reluctance) in using more complex and systematic search methods.
This suggests a need for support for a range of search and browsing functionalities more integrated with infor-mation use, and perhaps a need for additional lightweight or ambient functionalities to supplement focused systematic search.

Stelmaszewska and Blandford (2004) note the power of serendipity in physical libraries as an artifact of their design, raising an interesting challenge of how we might take steps to build this and other affordances of phys-of any resources to support this  X  that they would need to be situated and in use over extended time periods.
Additionally, other naturalistic studies highlight that not all contexts are conducive to document authoring as an integrated task and that users have diverse goals beyond those of authoring ( Adams &amp; Blandford, 2005 ). 2.5. Agent based systems: autonomous IR Thanks to the development of online search engines, it is now significantly easier to look for information. Yet despite their sophistication, it can be difficult to find relevant results by simply relying on a few words provided by users. In scholarly work that aspires to completeness (as opposed to the satisfying of most every-day search needs) this becomes even more problematic. However, with the very rapid return of results and accessible and interpretable display of those results, we do not need to rely just on a one-shot query and result.
Instead, users may employ a more iterative  X  X onversation X  with the search engine, refining their query (and even what they want) in the light of intermediate results. Unfortunately, in many cases, tweaking search que-ries is not a very effective search strategy ( Spink, Bateman, &amp; Jansen, 1999 ).

One way for users to  X  X  X reatly reduce the cost of searching for information X  X  ( Rhodes &amp; Maes, 2000, p. 693 ) is to use personalized autonomous Information Retrieval (IR) agents to conduct the search, also known as
Query-Free Proactive Retrieval (QFPR) agents or Just-in-Time IR (JITIR) agents. Instead of requiring the user to develop a query, and then enter it, the agents develop queries based on the user X  X  prior and ongoing activities, using these to try and infer possible information needs. Essentially these autonomous agents are pro-grams that attempt to perform the following six main functions: (1) Collect information about the user X  X  preferences and task; (2) Identify the user X  X  information needs from preferences and task; (3) Define what local and/or external data sources most likely will produce highly relevant results; (4) Conduct searches in the background; (5) Display highly relevant results to the user; (6) Record the user X  X  explicit or implicit relevance feedback to improve its relevance judgments.
Due to the complexity of the various tasks that a typical JITIR agent must accomplish, the task of building a successful proactive IR agent has proven to be very difficult. Some example of these autonomous IR agents includes Watson ( Budzik &amp; Hammond, 2000 ) and Remembrance Agent ( Rhodes &amp; Maes, 2000 ). For more examples of JITIR agents see Table 1 . Table 1 summarizes a number of different projects that applied this approach to support users in a range of different contexts, using various resources to inform query develop-ment and providing different kinds of results. The table uses  X  X  X nput X  X utput X  X  data characteristics proposed by
Henzinger, Bay-Wei, Milch, and Brin (2003) . Another closely related body of work in this area is that on rec-ommender systems, particularly those that combine various sources of information including explicit and implicit activity of the current user as well as activities of others via collaborative recommending (see Burke (2002) for a good overview of work in this area). 2.6. Challenges in evaluation
Three of the projects mentioned in the previous section included user studies that are of particular rele-vance, because they involve systems that are used in the context of writing.

In the first study, Rhodes and Maes (2000) used 27 subjects to test their Remembrance Agent (RA) ambient search agent with the Emacs text editor. One group used Emacs and RA while the other used only Emacs and a web browser to write a short factual article. At the end of the study, RA was evaluated based on the users X  personal ranking of the retuned results and its perceived overall usefulness to the users. They found that 79% of subjects preferred RA over a web browser.

Budzik, Hammond, and Birnbaum (2001) performed a similar experiment to evaluate suggestions produced by their own proprietary ambient search agent called Watson. Watson is integrated with various popular web browsers (e.g. Internet Explorer, Firefox) and Microsoft Office. The researchers started the experiment by recruiting six subjects to submit a sample of a paper that they have written in the past. The text from these papers was then input into Watson for processing; the output was a list of suggested sources. The researchers then asked the six subjects to review the list and rate whether these results would have been useful for their writing. On average, subjects found 2.5 of the documents suggested by Watson useful.
 Finally, Babaian, Grosz, and Shieber (2002) conducted a pilot study with 11 subjects to evaluate Writer X  X 
Aid, another JITIR agent integrated with the Emacs text editor. The researchers asked users to write  X  X  X  par-agraph or two ... in the area of their expertise involving citations X  X  (p. 13) using Writer X  X  Aid. After completing this task, the subjects were asked to fill out a questionnaire giving feedback on the system. In short, almost all (10) subjects found Writer X  X  Aid either  X  X  X ery useful X  X  or  X  X  X seful X  X .
The main evaluative measure for all of these studies is the users X  general perception of the system and of the retrieved results in particular. This type of user feedback can be very helpful for improving a particular version of a particular system. However, it provides little insight into what influence, if any, an ambient search agent might have on the overall writing process. In other words, while it is true that the number of relevant results is very important; what users actually do with the results is equally important. Thus, in addition to the common evaluation questions which tend to focus primarily on the evaluation of the produced results, we propose to also examine whether the use of an ambient search agent: may actually lead to better writing? may change the way that people write? How? And is that good or bad? may lead to more inspiration and greater creativity or diversity of thinking/citing?
Admittedly, there may be many different methods to address and answer these questions. One approach could be to evaluate the quality of the final draft of a paper that has been written with the aid of a JITIR agent. This could be done by surveying instructor(s) who gave an assignment and asking for their evaluation of the overall quality of their students X  papers. However, by exploring only a small part of a writer X  X  work, taking only a snapshot of his or her writing efforts, this approach may lead to the omission of important user-system interaction patterns. This is because as users proceed with their work and move from uncertainty to an ever increasing level of familiarity and certainty with their subject matter, their written ideas and citation list tend to constantly change, as do their relevance judgments ( Kuhlthau, 2004 ). As a result, what a writer believes is relevant at the beginning of their writing may or may not be as relevant at a later stage ( Vakkari, 2001 )  X  just as with the evolving queries in the berrypicking model ( Bates, 1989 ). To account for this, we believe that the use of any ambient search agent should be studied over a longer duration, ideally from the start of the work to the production of a declared final version of the writing.

It has proven to be difficult to systematically evaluate the effectiveness of JITIR agents. Currently the focus has been on examining a particular version of a particular program with an emphasis on the evaluation of the produced search results. We argue that traditional IR precision-like measures and/or HCI user satisfaction measures are not the only factors that should be considered in evaluating the effectiveness of a JITIR agent.
Thomas and Hawking (2006) demonstrated that a combination of methods, unobtrusive computer-aided observation and search log analysis may be used to achieve an effective evaluation of JITIR-like systems.
However, we believe that in order to adequately evaluate JITIR agents, we need to tightly integrate the eval-uation process with the user X  X  primary task. In our case, it is academic writing. In addition to the methods used by Thomas and Hawking, in our study we also relied on a pre-experiment questionnaire and conducted a con-tent analysis of different drafts for each paper written by a user. The questionnaire helped us to learn more about the current methods that the user is employing to perform his or her primary task, and the content anal-ysis helped us to understand how the user makes sense and use of the information retrieved by the system. (See Section 4 for more details).

We hope to open up a broader discussion on how to systematically evaluate the concept of ambient search as part of the overall design space. We will first lay out the challenges of doing such an evaluation and will explore possible research questions. Then we will also attempt to address some of these problems using data collected from our own autonomous IR agent. 3. PIRA description
In order to explore the issues outlined above, we developed an online writing system with a built-in ambient search agent, the personal information retrieval assistant, or PIRA. PIRA allows us to explore the design space by building several versions, exploiting the power of mashups and web services to gain the classic advan-tages of rapid prototyping. Using these prototypes we can log and study users X  information behavior in the context of academic writing. PIRA was developed to explore the consequences of more tightly integrating searching and writing through combining the basic features of a word processor with online search tools.
In so doing, it encourages a different way of writing  X  a more exploratory mode where the author discovers issues about the topic through the act of writing about it. We will refer to it as a  X  X rite while you search X  approach. This can be contrasted with conventional modes or at least the expectations of many students that they should first read everything that they intend to read on the topic, organize their thoughts and analysis, plan the paper and then write it.

PIRA has been built with an envisaged use context of a graduate student or a researcher, perhaps some-one who has struggled with conventional approaches to writing. Clearly there are many situations when
PIRA-like systems would not be useful, or even be undesirable. They could allow an unmotivated under-graduate to produce a stream of consciousness verbiage, adding in some vaguely plausible-looking references to create the illusion of a reasonable term paper. However, at this stage of the research, we choose not to consider all the potential ramifications of such a tool, and instead use it as a concept piece to help uncover interesting questions about the nature of the writing process and how technologies can, or should, help human creativity.

With PIRA, a user can cycle between writing and searching for relevant background information to read, analyze and reintegrate into their ongoing work. While it may sound unusual to start writing on a subject mat-ter before consulting and reading various resources on the matter, it can be a helpful way to get over the hurdle of actually starting a paper. As with any writing assignment, most of our intended users probably already know something basic about the subject matter. Having a tool that encourages users to write down what they know in the early stages of an assignment can help writers to understand their current thinking. As users read and write more, their understandings on the subject matter evolve. When users write in PIRA X  X  word proces-sor module, the search module automatically suggests related references ( Fig. 3 ). At any point while writing, the user is free to consider the suggested references, read one, indicate an interest in one, or reject one. Equally, the user may just ignore them and newer suggestions will periodically flow in. An evolving draft is a good source for any JITIR agent to gather information about the user X  X  task, information needs and background knowledge of a problem domain. 3.1. Rapid prototyping using a web mashup approach
Rapid prototyping is a well established technique in software development, particularly for exploring novel applications. Quickly building and equally quickly evaluating multiple low cost proofs of concept allow a more thorough exploration of a design space. Typically, very low cost prototypes sacrifice functionality for speed of development (as in the extreme case of paper prototyping), or if a working version, are typically highly fragile and unsuitable for more authentic evaluations.

Fortunately, there are now many open source software packages that can be downloaded and used as a basis for application development, requiring only the coding of additional features or variant interfaces. Fur-thermore, the rise of web services and web mashups means that prototypes can be created by combining exist-ing web services together in novel ways, and integrating them with existing or adapted open source applications ( Jones, 2007 ). We incorporated both open source software packages and publicly available but closed source web services into PIRA. In taking this route, we eliminated the need to develop certain basic but essential components of PIRA. For example, instead of developing our own text editor we integrated already existing text editors such as TinyMCE. To find possible search terms in users X  drafts, we used the
Yahoo! Term Extraction service. However, one of the major disadvantages of this web mashup style approach is that these external contents remain independent and unalterable within our own system. If they fail to deli-ver data or fail to deliver it in the form our system needs, our system as a whole also fails. To reduce this risk, we developed a set of server-side scripts (written in Python) responsible for communication with external sources. With a server-side script running, PIRA X  X  user interface does not directly communicate with external sources, but instead does this via our own scripts. This architecture allows us to (1) detect the failure of an external source, (2) substitute this failure with our own services/sources where possible, and if necessary (3) replace a failing module with another external sources of a similar nature or purpose. For instance, if the
Yahoo! Term Extraction service does not return a list of suggested search terms for whatever reason, we can immediately redirect this request to our own script which then extracts search keywords  X  less effective than the noun phrases that Yahoo! can usually supply, but in their absence, this allows some information retrieval to occur.

A general architecture of PIRA is presented in Fig. 1 . 3.2. The evolution of PIRA
The first fully working PIRA prototype was demonstrated at a departmental research showcase in March 2006. Attendees saw a demo of PIRA, tried it out themselves, and then something interesting happened.
Attendees started showing each other how PIRA worked and giving examples from their experience where, when and how a PIRA-like system could be used as a stand-alone application or as part of another web ser-vice. This event was very valuable to us. Quite unexpectedly we were able to get some very useful and action-able suggestions about how to improve the inner working of PIRA and its interface directly from potential end-users.

This informal incident, where a presentation of a research idea mutated into a data collection exercise rein-forced for us the critical importance of having a working prototype as a means of conveying a concept of novel functionality. In the absence of a demonstration, people could not really imagine what  X  X rite while searching X  might be; let alone whether it would be something that they might want to use. Once they saw a demonstra-tion, they seemed to immediately understand the intent and then move on to making suggestions or critiques of the functionality or interface; both features they thought interfered with use or might be useful. Although this is not at all new to proponents of rapid prototyping, we were surprised at the dramatic way that having a prototype confirmed these claims. This first, unintentional  X  X est X  of the PIRA interface showed that although far from optimal (as the following sections will describe) at least it was successful in illustrating what it was for to the extent that non-technical participants could explain it to each other and use it as a basis for talking about the design space.

For example, in the discussions arising in the session, participants noted that during writing, the number and nature of ideas and arguments in a draft is constantly changing. Every new idea or argument may stim-ulate a new information need(s). To make PIRA more receptive to the appearance of new ideas and arguments in one X  X  paper, we adopted a query term aging mechanism similar to the history algorithm used by Henzinger et al. (2003) . As a paper evolves and new ideas appear, each active term gradually ages and is replaced by a new one from the waiting list. The aging rate depends on how fast new concepts are introduced in the paper.
PIRA was next presented in an undergraduate class, on web technologies and techniques. Afterwards, stu-dents were invited to test the system at home and leave feedback via an online form. Thirteen students sent in feedback. They said they liked PIRA and felt that it was particularly useful for preliminary writing (e.g. note taking and brainstorming), but many questioned the relevance of some of the search terms suggested by
PIRA. To provide a diverse list of references, PIRA added suggested terms to the search query. These sug-gested terms were obtained by submitting terms from user X  X  draft to a web-based clustering engine (clu-sty.com). Upon close inspection, we realized that clusty.com was providing overly broad terms and that the students X  concerns were very perceptive. Consequently, we disabled this function in subsequent versions of PIRA. As we were using a web service rather than a carefully constructed module, it was relatively easy to decide to just ditch this option rather than to devote limited resources to tweaking it. The concept of sug-gested terms remains interesting but can be relegated to future work, ensuring that a reasonably working pro-totype is available early in the development process.

Another major change involved removing the  X  X eference archive X  element. The reference archive stored a list of all results that had been displayed to a user. The intent was that the ambient and ephemeral nature of sug-gested references might be rather disconcerting for people unused to this approach. What if you noticed some-thing of interest but it was replaced by new suggestions before you had got around to saving it or noting it as relevant? The archive was intended as a reassurance that ambient results would always be recoverable. How-ever, according to the students, as the number of displayed references grew, it became harder to find relevant records in the list. The volume of the archive rendered it unusable. As an interim fix, instead of the  X  X eference archive X , we introduced the archive of saved references that includes only relevant references manually selected by a user. The need to recover missed but potentially interesting results remains and we continue to work on this issue.

Finally, students wanted to get reference suggestions from different DLs, not only from CiteSeer ( http:// citeseer.ist.psu.edu ), a Computer Science-oriented DL. To address this concern, we added access to other open access archives/journals such as dLIST ( http://dlharvest.sir.arizona.edu ) and DOAJ ( http://www.doaj.org ).
However, adding multiple data sources to PIRA caused a new problem: the result list tended to be overpop-ulated with results from just one or two of the larger DLs. Since PIRA was sending the request to all the DLs at once, the larger DLs with huge collections of documents were usually returning more items per request and at a much more frequent rate. As a result, smaller size DLs never had an opportunity to be part of the refer-ence list window. To ensure that the user is presented with the most complete and diverse set of suggested ref-erences from all the available data sources, we placed all the available DLs into a loop. Each time PIRA is ready to submit a search request, it no longer sends this request to all DLs at once, but instead it sends the request to the first DL in the loop. As soon as the current search request is completed, that DL is placed at the end of the loop, and so on.
 The third public demonstration occurred at the workshop on Digital Libraries in the Context of Users X 
Broader Activities at the Joint Conference on Digital Libraries in June 2006, yielding yet more useful sugges-tions including providing PIRA with a bibliographic database management system and allowing users to orga-nize selected references into different subject folders. These recommendations were incorporated into the current version of PIRA.

The current version also lets users manually select favorite digital libraries from the available list, assign the number of retrieved references per request for each selected digital library, manage (e.g. restore, compare) dif-ferent drafts of the paper, and directly open any PDF files returned. We have extended the number of web browsers supported. PIRA has been successfully tested with Internet Explorer 5.0+, FireFox 1.0+, Mozilla 1.3+, Netscape 7+ and Safari 2.0. There is also an implicit feedback mechanism using the user X  X  real-time selected citations. As a user views and selects a suggested reference, the metadata from this record (e.g. title, abstract) is then used by PIRA to extract additional search terms. 3.3. Use scenario
To demonstrate PIRA in action, we present an example of our own interaction with the system while work-ing on this paper. After logging into PIRA ( http://pira.isrl.uiuc.edu ), first we selected three DLs from a list of six to be used as sources for our writing: two specialized scientific databases  X  CiteSeer ( http://cite-seer.ist.psu.edu ), the ACM DL ( http://portal.acm.org ), and one general purpose  X  Citebase ( http://www.cite-base.org )( Fig. 2 , marker 1). Next, we created a new document called  X  X  X IRA paper ideas X  X  by clicking on the  X  X  X reate X  X  button ( Fig. 2 , marker 2).

Next PIRA opened a web page composed of three parts: a text editor with an MS Word-like interface (to some recommendations from PIRA, we started typing into the text editor pane. After completing three short paragraphs describing how PIRA works, we notice that PIRA has already compiled a list of suggested articles related to our draft. Moving the cursor over a reference gives more detail, showing which database it came from, the keywords that were used to generate it and the abstract (if available)  X  as an attempt to answer the question:  X  X  X hy are you suggesting this item? X  X .

After a list of references is retrieved from a DL, PIRA does not display them all at once. Instead, each new reference is added to the list after 30 s delay. This is done purposely to ensure that the user is not overwhelmed with a constant stream of ever changing references. However, if a user wants to see the next newly retrieved references more quickly, he/she can do it by clicking on  X   X  X  X ee next reference X  X  button in the  X  X  X eference Box X  X  window ( Fig. 4 a).

The suggested articles tend to fall into four distinct groups or types: (1) Irrelevant; (2) Interesting but not sure if relevant; (3) Relevant but for later use; (4) Relevant and can be used right away.

If an article is irrelevant (Group 1), no action is necessary on the user X  X  part; it will gradually be pushed down by newer references and will eventually disappear. For Group 2 articles, a user can select them with the checkbox. This locks the citation in the pane. For citations from Groups 2, 3 or 4, there are three possible actions activated by clicking on one of three icons at the end of each citation:
Among the references suggested by PIRA, we spotted a couple of interesting articles. For example, one of the references  X  X  X rchitecture of a metasearch engine that supports user information needs X  X  looked very promising. We decided to save the citation for later use. After clicking on the save icon, we opted to asso-ciate this record with the current document called  X  X  X IRA paper ideas X  X  ( Fig. 4 b). By clicking on the  X  X  X aved X  X  button at the top of the search results window ( Fig. 4 c), we can confirm that the citation was successfully saved. The saved citation is now also accessible via the  X  X aved Citation X  section on the PIRA X  X  website ( Fig. 5 ).

Another article suggested by PIRA that we found potentially relevant to our research was  X  X  X uerying Het-erogeneous Information Sources ...  X  X . By hovering over this citation, we saw that it was retrieved by CiteSeer using keywords  X  X  X earch tools X  X . This time we decided to simply use a checkbox to lock the citation in order to come back to it later and to resume writing. Because we checked this reference, PIRA used the available meta-data for this citation (e.g. title) to extract additional search terms such as  X  X  X eterogeneous information sources X  X  ( Fig. 6 ).

If the user X  X  draft is changing slowly, and as a result there are not a lot of new concepts introduced in the text, PIRA may reuse previously extracted search terms to search for articles. This may result in the appear-ance of duplicate references. Although, at first this may seem a flaw, we believe that showing one references multiple times may actually have a positive effect. For example, a user may miss a highly relevant reference when it was displayed the first time. Second, repeating reference might be relevant to different parts of the user X  X  paper. On the other hand, there also could be instances when an irrelevant reference is displayed several times  X  this can be very annoying. In the future we will investigate this issue further, possibly by including an option to allow users to block unwanted references.

We chose to develop and use a system such as PIRA for an investigation of integrated and ambient search because it allows us to efficiently and effectively collect a vast amount of data on user interaction. PIRA also allows us to gather information in a non-intrusive manner from the user X  X  perspective; thus, eliminating the  X  X ehind-the-shoulder X  effect that have tainted the results of some user studies. Furthermore, being a web sys-tem, there is no need to install additional software. PIRA allows users to access their documents virtually from any computer with an Internet connection and a web browser. The cumulative effect of these characteristics of
PIRA has allowed us to perform a user study with a greater degree of authenticity than the previous JITIR studies described in section 2 , and at the same time at relatively low development cost. 4. Small scale pilot user study 4.1. Designing the study
As we continue to develop PIRA, we wanted a mechanism that could incorporate rapid lightweight eval-uation to complement the rapid development processes that had allowed us to develop PIRA to date. Unfor-tunately, while there are many rapid development techniques, and with the advent of web services and mashups, even more ways to generate working prototypes that can be used and shared online, most evaluation techniques are somewhat slower.

We invited 15 PhD students in Library and Information Science to participate in our first round of an ongoing study. The users were asked to use PIRA for their research-related writing projects. To help them get started with the system, we posted a brief online tutorial on PIRA X  X  web site ( http://pira.isrl.uiuc.edu ).
Before their first use of the system, each user was asked to complete an online preliminary survey. Users were not given any restriction on time, topic, nor by the number or size of papers that they can write using PIRA.
Users were able to leave the system and then resume their work at any time. After using the system for a few sessions, many subjects also gave us some additional feedback via emails.
 4.2. Results
All users X  interactions with the system (such as selection or view of certain citations) as well as users X  drafts were recorded for further analysis. This online approach allows a wealth of data to be collected, however it remains to be seen how to make the best use of this data, integrating it with richer but less complete qualitative data to help us understand not only what people did, but why, and what this can tell us about how we should refine the system. The findings presented here are necessarily only preliminary. They do however serve as an indication of the potential of using web-based applications to collect use data in more authentic but necessarily less controlled settings than conventional experiments ( Thomas &amp; Kellogg, 1989 ).

Over the first two weeks of the observations, we collected over 10,000 events in the user-system interaction log and over 3000  X  X rafts X  of papers (each draft is an automatically saved copy of the paper, saved every 3 min). To simplify this initial analysis we only considered those drafts differing by at least 100 characters  X  approximately 10 words ( Baldwin &amp; Tanaka, 2001 ). Note that this indicates the fine-grained nature of the data can be collected  X  this data was derived from only 15 users. The list of user-system interaction events that were recorded are presented in Table 2 .

To help with the analysis of the interaction log, we used a simple but effective visualization technique. The drafts and all their interaction events are shown in chronological order with the user X  X  changes to the text auto-matically highlighted ( Fig. 7 ). To make the interaction log more readable, the results are plotted on a simple graph ( Fig. 8 ). This shows the accumulated count of all interactions occurring between any two drafts (shaded areas at the bottom), the number of new and deleted characters in each subsequent draft (green at the top), and the total word count in each draft (red 3 the results easier to visualize, the values for character changes and total word count are plotted as normalized values, dividing the actual values by ten. As an example, consider a segment between draft 2 and draft 3 in the graph ( Fig. 8 , d2 and d3). In draft 3, the user added a total of 147 new characters. Since this is less than she added in draft 2 (249 characters), the green (upper) line in the graph went down from 24.9 to 14.7. The red (middle) line, which represents the total number of words in a draft, went up from 3.1 (31 words) to 5.0 (50 words). Between draft 2 and draft 3, PIRA registered 5 interaction events. This is shown in the (lower) shaded area. The graph shows that the shaded area went from 0 interactions right after draft 2 to 5 interac-tions (plotted un-normalized) at draft 3. These graphical representations of the user X  X  activities allow us to explore possible relationships between writing, relevance judgments, and reading  X  effectively supporting  X  X uman data mining X . Typical use is to glance over the graph, using the number of interactions (shaded) as a visual starting point and looking for potentially interesting interactions with the other two values. This can then be used to inform more detailed examination of subparts of the voluminous full log data.
To demonstrate how the diagram can be used together with the web analyzer, we walk through a detailed analysis of an interaction log ( Fig. 7 ). This log comes from a user who was writing about Computer-Supported Collaborative Learning .

To see whether or not suggested references actually influenced the user X  X  writing, we first have to examine the user X  X  activity diagram. By looking at this diagram, we are hoping to find instances when the user X  X  inter-actions with suggested references (e.g. save, open) are immediately followed by substantial changes to the user X  X  draft. An example of such an event or a candidate area of interest might be when the user initiates a copy of a quote from a suggested reference and then quickly followed by a paste into the draft. After a can-didate area of interest is located ( Fig. 9 ), we then began using the web analyzer to inspect the corresponding portion of the log. It appeared that the new text introduced to the draft ( Fig. 7 ) does in fact include a quote from some article. But without examining the user X  X  preceding interactions with the system, it was impossible to say whether or not the quote is from one of the references suggested by PIRA. Thus to clarify this point we then proceeded to an examination of the user X  X  interaction log.

The part of the log that we were particularly interested in is when the user stopped writing to evaluate sug-gestions compiled by PIRA. After checking for later review a reference titled  X  X  X romoting reflective interac-tions in a computer-supported collaborative learning environment X  X  [ checkref ( true )]; the user wanted to see more suggested references [ click ( SeeNextRef )].

Responding to the user X  X  request, PIRA displayed one more reference titled  X  X  X upporting online problem-solving communities with the semantic web X  X . This event is displayed in the following format: [ addref ( Porta-l.acm.org :  X  X  X eb Communities X  X  AND  X  X  X oun Phrases X  X  : Evaluation Computer-Supported Collaborative Learn-ing )] (the name of the event in the form of a hyperlink leading to the article; digital library used to find the article, and a set of keywords used in the search query).

The user seems to have rejected this reference as they requested the next one [ click ( SeeNextRef )]. This time, the newly appeared reference  X  X  X arvesting social knowledge from folksonomies X  X  caught the user X  X  attention, refhome ] and then after spending about 4 min reading the article, the user eventually saved the citation [ save-ref ]. To determine whether the quote came from this article, we also accessed its full-text. In the article X  X  abstract, we did find the quote used by the user.

From this interaction, we made two observations. First, this is the second time  X  X  X arvesting social knowl-
Fig. 7 ) This may mean the user either: (1) did not notice this reference the first time, (2) noticed it, but did not have time to react, or (3) noticed it, but considered it irrelevant at an earlier stage of writing.

Secondly, it would be interesting to know why the user decided to open this article in the first place. Its title clearly does not suggest a direct relevance to the user X  X  draft. There are several possible explanations of this decision, including: (1) the user has already read the article on another occasion; (2) the user might be aware of the author(s) but maybe from a different article; (3) the user was familiar with the research on Folksonomies (Social Classification), and already knew about (4) the user was just curious (as we were) as to why PIRA suggested this reference.

These issues of interpretation of user activity illustrate the importance of an integration of quantitative and qualitative data. The kinds of instrumentation, logging and visualization that is relatively easy to include in an exploratory system allows for a rich broad analysis of use over time and scales well, at least in ease of collec-tion and storage. However it needs to be supplemented by more detailed (and necessarily smaller scale) qual-itative analyses to more fully understand why people do the things that the logs record, along with what they fail to do, why they fail to do them, and what they would want to be able to do. In the next section, we report on our observations and analysis of the use of PIRA by three users. These users, referred to as Users A, B and
C, were selected as representatives of the various but distinct types of PIRA users and uses that were present in our study. We also chose these particular users because of the differences in their preferred writing/research technologies. With a sample of just 15 drawn from a rather specialized pool, we cannot claim to have a good coverage of all kinds of academic writing methods and potential uses of PIRA. What we can say is that even with such a small, somewhat homogeneous sample, a notable diversity of use emerged and that a system to support writing would do well to support, at the very least, such diversity. 4.3. User A
According to User A X  X  responses to our initial questionnaire, she  X  X  X ostly X  X  uses a word processor for writ-ing research papers. Along with a word processor, she uses Refworks, an online bibliographic database man-agement system to keep track of her citations. Typically she prefers to access and read articles in digital format over print alternatives. She says she  X  X  X lways X  X  accesses/search for additional sources while writing. The follow-ing is a concise description of her interaction with PIRA derived from the log analysis.

The topic of her paper was what and how the context of a user informational search can be used to improve the performance of Information Retrieval (IR) systems . She started her paper by stating a general topic for the paper. While freewriting (that is, just typing ideas into the word processor), she paid close attention to the sug-gestions (search terms and references) produced by PIRA. In Fig. 8 , one can see that the periods of writing (green line with stars) interleave with interactions with the emerging search results events (shaded areas). Over two separate sessions (in total over 30 min in length) user A produced 8 substantially different drafts. Her final draft was four paragraphs long consisting of 163 words. During the drafting stage, PIRA suggested a total of 49 references, 15 of which were manually checked as an item of potential interest for further review. Of these 15, nine were saved by A, ensuring that these particular references would be persistently available from session to session. Interestingly, A did not open any of the  X  X  X aved X  X  references to read during the writing stage.
A X  X  declared topic was on IR in Context, yet curiously, most of her manually checked and saved references were on machine learning . To us this was interesting but somewhat perplexing. A follow-up revealed that while the machine learning articles were not directly related to the paper that she was working on, they were relevant to her broader research interest. This insight into A X  X  behavior demonstrates a factor that traditional IR sys-tems fail to address: What type of user input/feedback should be given priority? What is considered to be a more accurate representation of user X  X  current information need and under what circumstances? For example, should we emphasize the content of the user X  X  drafts or the user X  X  relevance judgments? These big questions are not within the scope of this paper, but they definitely warrant further investigation, and necessarily have an impact on system design decisions.

One possible design implication from this observation is the realization that berrybaskets (from Bates X , 1989 berrypicking model) do not have any internal structure; they are essentially flat spaces. In this case, User
A knows that some documents are related to  X  X omething else X , but the system does not give her any mechanism for signaling this. If your berrybasket or document has explicit partitions (such as folders) then the user can explicitly indicate the presence of two topics  X  which presumably makes it easier for PIRA-like agents to pro-vide recommendations. This approach is similar to using sub-folders in the main bookmarks  X  X older X  of a web browser. Additionally, this activity seems to be an instance of information encountering where  X  X  X  user actively seeks information related to one problem and unexpectedly finds information related to some other problem X  X  ( Erdelez, 2004, p. 1015 ). We had not designed PIRA to support information encountering or even be an envi-ronment to study it, but having encountered it, we now are convinced it merits more careful subsequent study.
One might wonder why PIRA suggested so many articles on machine learning in the first place. Search terms primarily come from the user X  X  drafts and from the citations selected by the user. Based upon this user input, the PIRA algorithm provided a list of suggested references. In this case, PIRA used  X  X  X nformation retrieval X  X  (terms extracted from User A X  X  draft) to search for references available through www.citebase.org . Among references returned via this simple search, it happened that a high-ranked result from one database was  X  X  X earning Algorithms for Keyphrase Extraction X  X . As soon as the user expressed interested in this title by checking it more articles of a similar nature appeared including  X  X  X ining the Web for Lexical Knowledge to Improve Keyphrase Extraction: Learning from Labeled and Unlabeled Data X  X  and  X  X  X orpus-based Learning of Analogies and Semantic Relations X  X .

This episode illustrates that although PIRA is designed to support ambient search, using data extracted from the act of writing with the user required to do nothing directly to initiate or refine the search, if the user chooses to register an interest in a particular reference (by clicking), PIRA does take this into account in its relevance calculations. Effectively it provides the option for manual relevance feedback. In this instance that led to a kind of positive feedback loop as the topic veered away from what the user was currently writing about to a different but related topic of interest to the user. Although manual, the effort required (one click) is minimal and so we consider it legitimate to consider it as still residing somewhat in the design space of ambi-ent search.

During User A X  X  two sessions, PIRA extracted a total of 37 search terms by analyzing both the user X  X  text/ metadata and the user X  X  checked references. Additional search terms were manually added. A also manually removed about one third of the suggested terms. Most of the removed terms were names of authors from ref-erence articles that were extracted by the PIRA X  X  implicit feedback feature. We speculate that one possible reason why this occurred was because A was not familiar with those authors or was confused by the appear-ance of personal names as search terms. 4.4. User B
The second user in our study, User B, declared a preference to write in plain text editors and/or wiki-envi-ronments. B said that he typically used both print and digital materials. Because of a few unsatisfactory pre-vious experiences with bibliographic database management software, B currently does not use any. He also indicated in the survey that he  X  X  X lways X  X  consults external sources while writing.

User B was working on a paper about web mash-up tools for rapid prototyping. He already had some writ-ten text on the topic, including five references. His first action was to copy and paste this previously written draft consisting of 1176 words (62 sentences) into the PIRA text editor. He returned to PIRA on three separate occasions to work on this paper, spending a total of 134 min on it. Over those 134 min, PIRA displayed 146 unique references. Some of these references were displayed multiple times. B saved 31 references, about 12% of the total number shown (261). To make a relevance judgment, B accessed the full-text of 16 of the references. B manually blocked only three irrelevant search terms (e.g.  X  X ews station X ) that were automatically extracted from his original text by PIRA. By the end of the third session, the size of B X  X  document had increased by 357 words.
 Although the search results were generated from four different DLs, B saved only references from ACM
DL. This may suggest that if PIRA monitors user X  X  selections, it will be able to automatically detect a set of preferable DLs for a particular user for a particular paper. With this information, the system can then auto-matically increase the number of returned results from only those preferred DLs. 4.5. User C As with User A, User C also prefers to use a computer word processing program to write. And like User B,
User C likes to consult sources in both print and digital formats and does not use bibliographic database man-agement software. However, unlike both User A and B, User C  X  X  X lways X  X  collects and reads most of the needed literature sources before starting to write and only  X  X  X ometimes X  X  consults external sources while writ-ing. This last difference is very important to our study. One of the reasons for developing a system like PIRA is the notion that it may be better to  X  X  X rite while you search X  X . User C represents an ideal candidate for us to test this notion out.

User C X  X  topic was online support groups for the breast cancer patients. C started by freewriting instead of first doing an exhaustive search and reading up on the topic. During her 15 min long single session on PIRA, she composed two paragraphs consisting of 132 words. PIRA was only able to provide 6 references. And of those 6, C only found one that she deemed relevant. The low number of suggested references was a disappoint-ment to C and a surprise to the researchers. Upon closely examining C X  X  interaction log, it became evident that despite the fact that PIRA used relevant search terms such as  X  X reast cancer patients X  and  X  X nline support groups X , the suggested references were mostly about possible treatments for breast cancer or only about online support groups rather than both. For example, among titles retrieved by PIRA was  X  X  X  practical application of neural network analysis for predicting outcome of individual breast cancer patients X  X . This happened because the default digital libraries (Citeseer and dLIST) used by PIRA to produce recommendations tend to place an emphasis on Computer and Information Science publications. If User C had manually selected other digital libraries more appropriate to her research topic, the results could have been much better. To test this expectation, we first selected general purpose databases such as Citebase and then resubmitted C X  X  last drafts to PIRA. The revised suggested references were much more relevant to the user X  X  stated task. They included  X  X ealth Information and Community Outreach X ,  X  X ocial Support and Adjustment to Cancer X , and  X  X xperiences of Women with Breast Cancer: Exchanging Social Support over the CHESS Computer Network X .

This observation demonstrates that the user needs to know and select the appropriate databases in order to improve the likelihood of getting the correct type of results. To ensure that this will occur, in the future we plan to ask users to pre-select their preferred databases when they first sign-in to PIRA. This episode rein-forces the point that simple short keywords and concepts extracted or generated by PIRA and then used as query terms work far better in specialist digital libraries than in larger more generalist ones.
In many ways, User C X  X  experience with the system can be seen as something of a failure for the reasons noted above. The fact that she only used PIRA for 15 min is also significant  X  clearly she did not find it par-ticularly helpful. However, as a means to gain information for how to improve PIRA, we found C X  X  interac-tions invaluable. 5. Discussion
The experience of developing and iteratively refining PIRA and the most recent set of pilot studies outlined above have led to a richer understanding of the nature of the design space of technologies, functionalities and interfaces that integrate digital library use in the broader context of users X  activities. We outline some of those findings here.

Writing while searching shows promise, but there are many details to be ironed out. Novel applications have complex aspects of learnability, usability and use. Users have to learn how to use the software and then be able to use it productively; they have to be able to integrate it into their wider context of use (even wider than the widening contexts that the system explicitly tries to support). They also have to be able to understand what this new application is and what it is for. As noted, we find that getting across this concept in the absence of a working version can be very difficult. Fortunately in this particular case, a short exposure to the working system does seem to convey what it does extremely well. Additionally, a system is more likely to be successful if it can support a variety of different kinds of use, as illustrated in the different approaches of our pilot test users. Even better, a successful system is likely to be one that supports appropriation , where people use it in productive ways that the designers had never intended.

As with all software, particularly novel software, aspects of the interface can swamp the impact of the underlying functionality. Evaluation is highly complex. One has to distinguish between an evaluation of a par-ticular version of the software in a particular use context, a particular set of functionalities, and the underlying ideas about potential use that just happen to be manifested in a particular version. That is, to what extent do any findings of any evaluation, including future more systematic rigorous evaluations, tell us about the success of PIRA version 2.4, PIRA as a system, the utility of writing while searching or the concept of ambient search.
Although we have talked about PIRA as a system supporting ambient searching, it also has features that allow for and even encourage more direct feedback from users, especially on issues such as judgments about relevance. Thus although belonging to the category of ambient search tools, PIRA is not pure ambient search; it can allow varying levels of direct user engagement.

The search results returned by systems such as PIRA are likely to fare poorly on measures such as recall and precision. Given the difficulties of performing an ambient search, many of the results such systems return intentions can be worse than useless, creating considerable annoyance for users. The often violent reactions against Microsoft Office X  X  Clippy are a salutary reminder of the consequences of failure. To date we have found remarkable tolerance by our end users of the many  X  X ailed X  results of PIRA that are clearly not relevant.
This needs further investigation, but we do have some indications for why this might be so, that in turn can feed into design implications for future systems.

One possibility is that this is simply due to sampling bias  X  that the people we have asked to test out PIRA are mostly colleagues who might be being polite. The framing of PIRA as an experimental system may also have helped to encourage error tolerance. Given participants X  willingness to point out other flaws, we are inclined to discount that concern somewhat. The suggested keywords are merely intermediate information that users can ignore, but there is some evidence that users were much more irritated by incorrect or useless keywords than they were by the irrelevant results derived from those same keywords. In one sense that irri-tation was productive, provoking some people to delete unwanted keywords. However that moves the inter-action with the information search part of PIRA to a less ambient mode, forcing people to interact  X  if only out of annoyance. The Phrasier system ( Jones &amp; Staveley, 1999 ) also allowed users to be more active in guid-ing the direction of the search through explicit keyphrase selection in the interface. Although both the Phrasier and PIRA architectures allow for user input in different ways (e.g. through explicit text selection, keyword selection and reference selection) further user studies are clearly needed to understand the trade-offs between these different interaction styles.

The mystery remains of why the irrelevant search results were so much less irritating than the inappropriate keywords. We suspect that in part this is due to the managed lowered expectations of users. They were often initially skeptical that PIRA could offer anything useful, and so irrelevant results were to be expected. They were easy to ignore, as the main focus was typically on the word processing interface. When a relevant result appeared, it was treated as a bonus. Often the results were treated as serendipitous, offering a potential of a new line of research or indeed search inquiry. Also, of course that could instead be yet another complicating distraction. It seems that users tolerated many irrelevant results as (so they judged) an inevitable price to pay for serendipity. Additionally, by doing nothing, irrelevant results just went away as new results appeared.
Thus if PIRA is viewed more as a tool for encouraging creativity ( Shneiderman, 2002 ) that offers somewhat random results that may or may not be deemed useful, the irrelevant results are tolerated in a way that they may not be in a purely functional IR system. We believe this issue can be of great interest as we continue to investigate ambient, and indeed error-prone systems. The challenge of designing a system so that users will tolerate its inevitable and frequent partial failures is one that we believe has been under-studied.
A web-based system allows more authentic use by a larger number of people than is feasible in traditional controlled experiments. It also allows for the collection of large amounts of log data for any one use event.
This opportunity creates a need for tools to help cope with this wealth of data. As part of our study, we have designed and tested a set of tools that will help us to record and study the interaction between the user and
PIRA-like systems. The first tool, the  X  X nalyzer X  is a web based interface that captures and displays changes in the user X  X  text while simultaneously keeping track of the user-system events (user interaction) in a real time, running log ( Fig. 7 ). Essentially, it is an automated system for organizing, categorizing and presenting the information recorded by the log. The second is a  X  X ser model X  diagram that is a concise graphical representa-tion of the information that organized and categorized by the analyzer ( Figs. 8 and 10 ). When used together, they can save a researcher a lot of time. Instead of examining the entire event log to detect potential usage patterns, if any, the  X  X ser model X  diagrams allow researchers to quickly identify candidate intervals in the log that might warrant a more detailed analysis.

These tools can be used to speed up a qualitative analysis of the collected user data by finding candidate intervals in the log file to confirm whether a suggested reference from a JITIR system might have inspired the writer to write something or not. After candidate intervals are identified, researchers can then proceed to examine them in greater detail to confirm or reject the expectation, significantly speeding up the analysis of thousands of log events in a large scale user study. In the future, a system such as this can also be matched up with a machine learning algorithm to learn an evaluator X  X /researcher X  X  judgments. The approach developed relies on the discovery of a specific user information behavioral pattern, or marker. A marker is a point when a user attempts to access the full text of a saved reference between two consequential drafts. This usually signi-fies that the user read some article right before starting to edit the draft. We expect that in such a scenario there is a high chance that what the user read might have influenced his/her writing. In order to automatically find intervals that match this scenario, we use regular expressions involving PIRA X  X  log events. For example, a rule might be constructed that effectively says:  X  X  X ind all sequences in the log file where an article was opened right before changes were made in the text, given a maximum acceptable number of events that may occur before the user resumes typing that do not involve opening any other articles X  X . Additionally, we can put a minimum time requirement for how long a reference was actually opened. If more than one article was opened in between two drafts, then all these articles may be considered.

There is still much to be done to understand the issues in the design space of integrating writing and search-ing, and the particular niche of supporting more ambient aspects of search. Examples include the desirability of integrated searching and writing and concerns about it being inappropriate in many cases. Even if it is appropriate, in practice it might turn out to be highly distracting, irritating, or beguiling users into inefficient time-wasting practices creating the illusion of progress and rigor. How good do such systems have to be before they become useful? We know that inadequately complete digital libraries can be irritating to users, leading to people abandoning them and reverting to Google. Do we need more features for writing? More features for analysis and planning, reading and thinking? More features for searching multiple sources? More features for even wider integration  X  explicitly supporting incidental discoveries for other tasks, multitasking, interrup-tions, long term work on multiple tasks in parallel, interweaving tasks etc.? But will providing these features create a behemoth that is so bewildering in its features and the complexity of its interface that people refuse to use it?
Many of these issues in supporting  X  X rite while you search X  have arisen out of our ongoing prototyping activities using web services. This research approach might itself might be typifies as  X  X uild while you analyze X  and contrasted with the more typical approach of careful analysis prior to costly and time consuming building and evaluation phases. The opportunities afforded by rapid prototyping using web-based services mean that faster, more lightweight, integrated discovery methods become feasible for researching just as they do for writing. 6. Conclusion
The question of  X  X hat to automate X  is a perennial issue for information retrieval ( Bates, 1990 ), and there are many  X  X  X utstanding research questions over the appropriate balance of power and capability between user and system X  X  ( Blandford, 2006 ). The main contributions of this paper are:
A preliminary exploration of the design space of applications that more tightly integrate searching and writing.
 An exploration of ambient search as a resource for writing.
 An illustration of the use of web mashups as an approach to exploring design spaces.
 A description of PIRA, its initial design and testing with a small number of authentic user interactions.
Hendry (2006) comments that it is not easy to embed search interfaces into users X  workspaces. In this paper we have described a lightweight approach for embedding search in the writing process and analyzing the resulting interactions. Preliminary findings suggest the approach shows promise. They do however raise inter-esting questions of how to proceed in exploring the potential and utility of the design space. Traditional eval-uation methods may not be appropriate or just simply to be too slow to be useful in helping to inform the dozens of design decisions necessary in any prototype. Contextualized information search has more aspects of satisfying than optimizing, whereas traditional measures tend to work best with the latter. Serendipity can be valuable in encouraging creativity and breaking out of writer X  X  block, but at other times can become unpro-ductively distracting.

We believe that exploratory methods exploiting both rapid testing and rapid evaluation as a way to achieve multiple iterations is a promising approach, but that to be effective, we must uncover ways to ground these fast lightweight methods in the findings derived from slower but more rigorous methods of traditional scholarship. References
