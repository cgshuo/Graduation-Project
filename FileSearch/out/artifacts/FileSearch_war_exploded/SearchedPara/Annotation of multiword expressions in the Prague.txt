 Eduard Bejc  X  ek  X  Pavel Stran  X  a  X  k Abstract We describe annotation of multiword expressions (MWEs) in the Prague dependency treebank, using several automatic pre-annotation steps. We use subtrees of the tectogrammatical tree structures of the Prague dependency treebank to store representations of the MWEs in the dictionary and pre-annotate following occur-rences automatically. We also show a way to measure reliability of this type of annotation.
 Keywords Multiword expressions Treebanks Annotation Inter-annotator agreement Named entities 1 Motivation Various projects involving lexico-semantic annotation have been ongoing for many years. Among those there are the projects of word sense annotation, usually for creating training data for word sense disambiguation. However majority of these projects have only annotated very limited number of word senses (cf. Kilgarriff ( 1998 ). Even among those that aim towards  X  X  X ll words X  X  word-sense annotation, multiword expressions (MWEs) are not annotated adequately (see Mihalcea ( 1998 ) or Hajic  X  et al. ( 2004 )), because for their successful annotation a methodology allowing identification of new MWEs during annotation is required. Existing dictionaries that include MWEs concentrate only on the most frequent ones, but we argue that there are many more MWEs that can only be identified (and added to the dictionary) by annotation.

There are various projects for identification of named entities (for an overview named entities to be concerned with lexical meaning. At this place we just wish to recall that these projects only select some specific parts of text and provide information only for these. They do not aim for full lexico-semantic annotation of texts.

There is also another group of projects that have to tackle the problem of lexical meaning, namely treebanking projects that aim to develop a deeper layer of agreed to concern lexical meaning. To our best knowledge, the lexico-semantic annotations still deal with separate words, phrases are split and their parts are connected with some kind of dependency. Furthermore, only words with valency are involved in projects like NomBank (Meyers et al. 2004 ), PropBank (Palmer et al. 2005 ) or PDT. 1.1 Prague dependency treebank We work with the Prague dependency treebank (PDT; see Hajic  X  2005 ), which is a large corpus with rich annotation on three layers: it has in addition to the morphological and the surface syntactic layers also the tectogrammatical layer. (In fact, there is also one non-annotation layer, representing the  X  X  X aw-text X  X  segmented into documents, paragraphs, and tokens.) Annotation of a sentence on the morphological layer consists of attaching several attributes to the tokens of the w-layer, the most important of which are morphological lemma and tag. A sentence at the analytical layer is represented as a rooted ordered tree with labeled nodes. The dependency relation between two nodes is captured by an edge with a functional label. The tectogrammatical layer has been construed as the layer of the (literal) meaning of the sentence and thus should be composed of monosemic lexemes and the relations between their occurrences. 1
On the tectogrammatical layer only the autosemantic words form nodes in a tree (t-nodes). Synsemantic (function) words are represented by various attributes of t-nodes. Each t-node has a lemma: an attribute whose value is the node X  X  basic lexical form. Currently t-nodes, and consequently their t-lemmas, are still visibly derived from the morphological division of text into tokens. This preliminary handling has always been considered unsatisfactory in FGD. 2 There is a clear goal to distinguish t-lemmas through their senses, but this process has not been completed so far (see Sect. 3 ).
 Figure 1 shows the relations between the neighboring layers of PDT.

Our project aims at improving the current state of t-lemmas. Our goal is to assign each t-node a t-lemma that would correspond to a monosemic lexeme, i.e. that would really distinguish the t-node X  X  lexical meanings. To achieve this goal, in the first phase of the project, which we report on in this paper, we identify MWEs and create a lexicon of the corresponding monosemic lexemes . A simple view of Sect. 4.2 . 2 Introduction In our project we annotate all occurrences of MWEs (including named entities, see below) in PDT 2.0. When we speak of MWEs we mean  X  X  X diosyncratic interpretations that cross word boundaries X  X  (Sag et al. 2002 ). We do not inspect various types of MWEs, because we are not concerned in their grammatical attributes. We only want to identify them. Once there will be a lexicon with them and their occurrences annotated in corpora, the description and sorting of MWEs will take place. We hope that annotation of a treebank will help X  X WEs with fixed syntactic form will be easily distinguished from the others that can be modified by added words.

We distinguish a special type of MWEs, for which we are mainly interested in its type, during the annotation: named entities ( NE ). 3 Treatment of NEs together with other MWEs is important, because syntactic functions are more or less arbitrary inside a NE (consider an address with phone numbers, etc.) and so is the assignment of semantic roles. That is why we need each NE to be combined into a single node, just like we do it with MWEs in general.
 For the purpose of annotation we have built a repository of MWEs, which we call SemLex. We have built it using entries from some existing dictionaries and it is being enriched during the annotation in order to contain every MWEs that was annotated. We explain this in detail in Sect. 4.1 . 3 Current state of MWEs in PDT 2.0 During the annotation of valency, which is a part of the tectogrammatical layer of PDT 2.0, the t-lemmas, have been basically identified for all the verbs and some nouns and adjectives. The resulting valency lexicon is called PDT-VALLEX, adjectives and nouns in PDT that have valency. 4
This is a starting point for having t-nodes corresponding to lexemes. However in MWEs are not joined into one node. Parts of frames marked as idiomatic are still represented by separate formally dependent t-nodes in a tectogrammatical tree (e.g. consisting of copulla  X  X  by  X  t  X  X  (to be) and a noun or adjective are also split into two nodes, where the nominal part is governed by the verb. Idioms that do not contain any morphological verb have either been annotated and assigned their own valency frames just like the above described verbal idioms (in case of idioms containing nouns derived from verbs by suffixes -n X   X  or -t X   X  ), or (in case of the idioms consisting of only one t-node) have not been annotated at all in the current PDT. For detailed description see Sect. 6.8. of Mikulova  X  et al. 2006 ).

In Figs. 3 , 4 , and 5 we give several examples of t-trees in PDT 2.0, that include idioms, light verb constructions and named entities: 4 Methodology 4.1 Building SemLex Each entry we add into SemLex is considered to be a monosemic MWEs. We have also added nine special entries to identify NE types, so we do not need to add all the expressions themselves. These types are derived from the NE classification by S  X  evc  X   X   X  kova  X  et al. 2007. Some frequent names of persons, institutions or other objects (e.g. film titles) are being added into SemLex during annotation (while keeping the information about their NE type), because this allows for their following occurrences to be pre-annotated automatically (see Sect. 5 ). For others, like addresses or bibliographic entries, it makes but little sense, because they most probably will not reappear during the annotation.

Currently (for the first stage of lexico-semantic annotation of PDT) SemLex contains only MWEs. Its base has been composed of MWEs extracted from Czech WordNet (Smrz  X  2003 ), Eurovoc (Eurovoc 2007 ) and Dictionary of Czech Phraseology and Idiomatics (C  X  erma  X  k et al. 1994 ). Currently there are over 30,000 MWEs in SemLex and more are being added during annotations.

The entries added by annotators must have defined their  X  X  X ense X  X . Annotators define it informally (as well as possible) and we extract an example of usage and the basic form from the annotation automatically. The  X  X  X ense X  X  information will be revised by a lexicographer, based on annotated occurrences. 4.2 Annotation PDT 2.0 uses PML (Pajas and S  X  te  X  pa  X  nek 2005 ), which is an application of XML that utilises a stand-off annotation scheme. We have extended the PDT-PML with a new schema for so-called s-files. We use these files to store all of our annotation without altering the PDT itself. These s-files are very simple: basically each of them corresponds to one file of PDT and consists of a list of s-nodes. Each s-node corresponds to an occurrence of a MWEs and is composed of a link to an entry in SemLex and a list of identifiers of t-nodes that correspond to this s-node. Figure 6 shows a relation of s-layer to PDT layers and SemLex. 5
Our annotation program reads in a tectogrammatical representation (t-file) and tectogrammatical representation) is presented to the annotator. While the annotator marks MWEs already present in SemLex or adds new MWEs into SemLex, tree representations of these MWEs extracted from underlying t-trees are added into their SemLex entries via TrEd scripts. 5 Pre-annotation Because MWEs tend to occur repeatedly in a text, we have decided to test pre-annotation both for speed improvement and for improving the consistency of annotations. On the assumption that all occurrences of a MWEs share the same tree structure , while there are no restrictions on the surface word order other than those imposed by the tree structure itself we have decided to employ four types of pre-annotation: (A) External pre-annotation provided by our colleague (see Hna  X  tkova  X  2002 ). With (B) Our one-time pre-annotation with those MWEs from SemLex that have been (C) Dynamic pre-annotation as in (B), only with the SemLex entries that have (D) When an annotator tags an occurrence of a MWEs in the text, other
Pre-annotation (A) was executed once for all of the PDT. (B) is performed each time we merge MWEs added by annotators into the main SemLex. We carry out this annotation in one batch for all PDT files remaining to annotate. (C) is done for each file while it is being opened in the annotation environment. (D) happens each time the annotator adds a new MWEs into SemLex and uses it to annotate an occurrence in the text. In subsequent files instances of this MWEs are already annotated in step (C), and later even in (B).

After the pilot annotation without pre-annotation (D) we have compared instances of the same tags and found that 10.5% of repeated MWEs happened to have two different tree representations. Below we analyse the most important sources of these inconsistent t-trees and possible improvements:  X  Occasional lemmatisation errors . They are not very frequent, but there is no  X  Annotator X  X  mistake (not marking correct words) . When an annotator makes an  X  Gender opposites, diminutives and augmentatives . These are currently repre- X  Newly established t-nodes corresponding to elided parts of MWEs in coordi-
Up to now we have not found any MWEs such that its structure cannot be represented by a single tectogrammatical tree. 1.1% of all occurrences were not connected graphs, but this happened due to errors in data and to our incorrect handling of coordinations with newly established t-nodes (see above). This corroborates our assumption that (disregarding errors) all occurrences of a MWEs share the same tree structure. As a result, we started storing the tree structures in the SemLex entries and employ them in pre-annotation (D). This also allows us to use pre-annotations (B) and (C), but we have decided not to use them at the moment, in order to be able to evaluate each pre-annotation step separately. Thus the following section reports on the experiments that employ pre-annotations (A) and (D). 6 Analysis of annotations Two annotators have started to use (and test) the tool we have developed. They both have got the same texts. The text is generated from the t-trees and presented as a plain text with pre-annotated words marked by colour labels. Annotators add their tags in the form of different colour labels and they can delete the pre-annotated tags. In this experiment the data consists of approx. 310,000 tokens, which correspond to 250,000 t-nodes. Both annotators have marked about 37,000 t-nodes ( &amp; 15%) as parts of MWEs and grouped them into 17,000 MWEs. So the average length of a MWEs is 2.2 t-nodes.

The ratio of general named entities versus SemLex entries was 50:50 for annotator A and 52:48 in the case of annotator B . Annotator A used SemLex more frequently (than she used named entities and also than annotator B used SemLex), but did not utilise as many lexicon items as annotator B . This and some other comparison is given in Table 1 .
 Both annotators also needed to add missing entries to the originally compiled SemLex or to edit existing entries. Annotator A added 1,361 entries while annotator B added 2,302. They modified 1,307 and 2,127 existing entries, respectively. 6.1 Measuring inter-annotator agreement In this section our primary goal is to assess whether with our current methodology we produce a reliable annotation of MWEs. To that end we measure the amount of inter-annotator agreement that is above chance. Our attempt exploits weighted kappa measure j w (Cohen 1968 ).

The reason for using a weighted measure is essential: we do not know which parts of sentences are MWEs and which are not. Therefore annotators work with all words and even if they do not agree on the type of a particular MWEs, it is still an agreement on the fact that this t-node is a part of some MWEs and thus should be tagged. This means we have to allow for partial agreement on a tag.

There are, however, a few sources of complications in measuring agreement of our task even by j w :  X  Each tag of a MWEs identifies a subtree of a tectogrammatical tree (represented  X  There is no clear upper bound as to how many (and how long) MWEs there are  X  There is not a clear and simple way to estimate the amount of agreement by
Since we want to keep our agreement calculation as simple as possible but we also need to take into account the issues above, we have decided to start from Cohen X  X  j w [quoted from (Artstein and Poesio 2007 )]: (further explained in Eq. 3 ) and to make a few adjustments to allow for an agreement on non-annotation and an estimated upper bound. We explain these adjustments in following paragraphs.

Because we do not know how many MWEs there are in our texts, we need to calculate the agreement over all t-nodes , rather than just the t-nodes that  X  X  X hould be annotated X  X . This also means that the theoretical maximal agreement (upper bound) U cannot be 1 (as in the denominator of Eq. 1 ). If it were 1, it would be saying that all nodes are part of MWEs.

Since we know that U &lt; 1 but we do not know its exact value, i.e. we do not know the  X  X orrect X  ratio of MWEs and NEs in a text, we use the estimated upper bound b U (see Eq. 2 ). Because we calculate b U over all t-nodes, we need to account not only for agreement on tagging a t-node, but also for agreement on a t-node not being a part of a MWEs, i.e. not tagged at all. This allows us to positively discriminate the cases where annotators agree that a t-node is not a part of a MWEs from the cases where one annotator annotates a t-node and the other one does not, which is evidently worse.

If N is the number of all t-nodes in our data and n A [ B is the number of t-nodes annotated by at least one annotator, then we estimate b U as follows:
The weight 0.051 used for scoring the t-nodes that were not annotated is explained below (class c = 4). Because there is some disagreement of the anno-believe that the real upper bound U lies somewhat below it and the agreement value 0.213 is not something that should (or could) be achieved. It is however important to note that this is based on the assumption that the data we have not yet seen have similar proportion of MWEs as the data we have used for the upper bound estimate. Since the PDT is composed of only news articles, the assumption seems reasonable.
To account for partial agreement we divide the t-nodes into 5 classes c and assign each class a weight w c as follows: c = 1 If the annotators agree on the exact tag from SemLex, we get maximum c = 2 If they agree that the t-node is a part of a NE or they agree that it is a part of c = 3 If they agree that the t-node is a part of a MWEs, but disagree whether a c = 4 If they agree that the t-node is not a part of a MWEs, w 4 = 0.051. This low We can see that even two ideal annotators who agree on all their assignments could not reach agreement U = 1, since they naturally leave some t-nodes without annotation and even if they are the same t-nodes for both of them, this agreement is weighted by w 4 . Now we can look back at Eq. 2 and see that b U is exactly the agreement which two ideal annotators reach. c = 5 If the annotators do not agree whether to annotate a t-node or not, w 5 =0.
It should be explained why the upper bound does not need to be corrected in other weighted measures like j w . There are weights for some types of disagreement in j w to distinguish  X  X  X etter X  X  disagreement from  X  X  X orse X  X  one, too. But it is still a disagreement and annotators could agree completely. In our task, on the contrary, this class c = 4 represents an agreement of its kind. The reason, why we do not count it as an agreement, is the biased resulting measure, if we do so: The less they would annotate the higher the agreement would be (if they annotated nothing, j w would equal 1).
We have also measured standard j without weights. All partial (dis)-agreements had to be treated as full disagreements, because of lack of a weight function. In j 1 we counted every non-annotated t-node as a disagreement, too; in j 2 we think of non-annotation as a new category, so it is counted as an agreement. The difference is quite clear ( j 1 = 0.04 and j 2 = 0.68). j 2 might seem as a usable measure, even though over-generalising. However we can see that agreement on not-annotating is again counted as equally valuable as full agreement on a MWEs. Thus it has the same problem as j w without the class c = 4, as explained above.
 The numbers of t-nodes n c and weights w per class c are given in Table 2 .
Now that we have estimated the upper bound of agreement b U and the weights w for all t-nodes we can calculate our generalised version of weighted j w :
A o is the observed agreement of annotators and A e is the agreement expected by chance (which is similar to a baseline). j U w is thus a simple ratio of our observed agreement above chance and maximum agreement above chance. In equivalent (and often used) definition, D o and D e are observed and expected disagreements. Weights w come into account in calculation of A o and A e .

We calculate A o by multiplying the number of t-nodes in each category c by that category X  X  weight w c (see Table 2 ), summing these five weighted sums and dividing this sum of all the observed agreement in the data by the total number of t-nodes:
A e is the probability of agreement expected by chance over all t-nodes. This means it is the sum of the weighted probabilities of all the combinations of all the tags that can be obtained by a pair of annotators. Every possible combination of tags (including not tagging a t-node) falls into one of the categories c and thus gets the appropriate weight w . (Let us say a combination of tags i and j has a probability p ij and is weighted by w ij .) We estimated these probabilities from annotated data where n q i A is the number of lexicon entry q i in annotated data from annotator A and N
A is the amount of t-nodes given to annotator A . Here, the non-annotation is treated like any other label assigned to a t-node.

The resulting j U w is then 6.2 Interpretation of inter-annotator agreement When we analyse disagreement and partial agreement we find that mostly it has to do with SemLex entries rather than NEs. This is due to the problems in the dictionary and its size (annotators cannot explore each of almost 30,000 SemLex entries), but also our current methodology that relies too much on searching the SemLex. This should, however, improve by employing pre-annotations (B) and (C).
One more source of disagreement are the sentences for which non-trivial knowledge of the world is needed:  X  X  X ang Di Pertuan Agong Sultan Azlan Shah, the sultan of the state of Perak, [...] flew back to Perak. X  X  Is  X  X  X ultan Azlan Shah X  X  still a part of the name or is it (or is a part of it) a title?
The last important cause of disagreement is simple: both annotators identify the same part of text as MWEs instances, but while searching the SemLex they choose different entry as the tags. This can be rectified by:  X  Removing duplicate entries from SemLex (currently there are many almost  X  Employing improved pre-annotation B and C, as mentioned above.

We introduced generalised j U w measure, which is Cohen X  X  weighted kappa with the upper bound U B 1, and we argue why such generalisation is essential for annotation project of this kind. 7 We also explain why and how the estimation of the upper bound of annotations should account for a difference between (agreement on) not annotating a unit (a t-node) and disagreement on annotation.

The main problem with interpretation of our results is that we don X  X  know of any direct comparison. We were not able to find any published results of inter-annotator agreement on a task like ours, i.e. task with no exact upper bound on a number of tags, and a possibility of partial agreement on the size and type of tags. Until our results are compared to other such projects, the informative value of our numbers is limited. 7 Conclusion We have annotated multi-word lexemes and named entities on a part of PDT 2.0. We use tectogrammatical tree structures of MWEs for automatic pre-annotation. In Sect. 5 we show that the richer the tectogrammatical annotation the better the possibilities for automatic pre-annotation that minimises human errors. In the analysis of inter-annotator agreement we show that a weighted measure that accounts for partial agreement as well as estimation of maximal agreement is needed.

The resulting j U w  X  0 : 644 should gradually improve as we clean up the annotation lexicon, more entries are pre-annotated automatically, and further types of pre-annotation are employed.
 References
