 Computer architects utilize simulation tools to evaluate the merits of a new design feature. The time needed to ade-quately evaluate the tradeoffs associated with adding any new feature has become a critical issue. Recent work has found that by identifying execution phases present in com-mon workloads used in simulation studies, we can apply clus-tering algorithms to significantly reduce the amount of time needed to complete the simulation. Our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies. We also look to im-prove upon prior work by applying more appropriate clus-tering algorithms to identify phases, and to further reduce simulation time.

We find that the phase clustering in computer architecture simulation has many similarities to text clustering. In prior work on clustering techniques to reduce simulation time, K-means clustering was used to identify representative pro-gram phases. In this paper we apply a mixture of multino-mials to the clustering problem and show its advantages over using K-means on simulation data. We have implemented these two clustering algorithms and evaluate how well they can characterize program behavior. By adopting a mixture of multinomials model, we find that we can maintain simula-tion result fidelity, while greatly reducing overall simulation time. We report results for a range of applications taken from the SPEC2000 benchmark suite.
 C.1 [ Processor Architectures ]: Single Data Stream Ar-chitectures; I.5.3 [ Pattern Recognition ]: Clustering Program Phase, simulation, clustering, EM, K-means, mix-ture of multinomials
In the field of Computer Architecture, it is typical to eval-uate new microprocessor designs using quantitative simu-lation studies. A software-based simulation model of the proposed architecture is developed, and then benchmark programs are executed on the model to evaluate the mer-its of new microprocessor design features (i.e., incremental changes to the base microprocessor design that are made to increase performance, increase reliability, or lower power). Examples of performance metrics commonly used to evalu-ate the merit of a new design feature include instructions executed per cycle (IPC), cache miss rates, and branch pre-diction accuracy.

As the complexity of a microprocessor architecture design grows, the time needed to evaluate a single design feature has become prohibitive [1]. A number of techniques have been proposed to reduce this simulation time [2, 3, 4, 5, 6, 7]. One approach that has been shown to be particularly effective exploits the distinct phases that are present in pro-gram execution [3].

A phase in a program is defined as a time interval or exe-cution slice where processor resources are utilized uniformly. Each program phase is comprised of on the order of 10-100 million instructions, as compared to the full program execu-tion, which can include many trillions of instructions. Dur-ing the simulation of a microprocessor design, if we could use a representative mix of these program phases to drive our simulation, we should be able to reduce the time to perform these needed evaluations, without sacrificing simulation ac-curacy.

Once we can identify the distinct phases of a program, clustering algorithms can be applied to identify similarities between phases. The fundamental observation is that the behavior of industry standard benchmark programs can be effectively characterized by a rather small number of rep-resentative phases [3, 8]. By running simulations on just those representative phases, and then weighting the individ-ual results by the frequency that the phase occurs in the full application, we can obtain very similar simulation results as compared to running a simulation using the entire program. This approach can lead to significant reductions in overall simulation time.

Previous work on clustering phases [3] applies random projection, followed by K-means clustering, and uses the Euclidean distance as the measure for dissimilarity. To cap-ture characteristic program behavior, phases are represented as a vector of frequency counts. The count represents the number of times a particular basic block has been executed. Basic block execution profiles possess similar characteristics as text data. Thus, we expect that models more appro-priate for handling frequency vectors, such as a mixture of multinomials, should outperform K-means. In this paper, we propose to first use random projection, followed by clus-tering using a mixture of multinomials. Our experimental results confirm that a mixture of multinomials can reduce the error introduced by sampling by almost 50%.

This paper is organized as follows. In section 2, we de-fine the terminology used in this problem, and describe our methodology. In section 3, we motivate our choice of using a mixture of multinomials for clustering. In this section, we also explain how we apply feature reduction techniques before performing clustering. In section 4, we describe our experimental setup and discuss results. Finally, we conclude the paper in section 6, and discuss directions for future work.
A basic block is defined as a program section compris-ing of a set of instructions which have only one entry and one exit point. To be more precise, there are no branch in-structions within a basic block. There have been different program characteristics proposed in the literature to iden-tify the distinct phases of complex programs [9, 10]. Of all these techniques, basic blocks frequencies have been shown to be the most effective in capturing phase transitions. Ba-sic block vectors (BBVs) (a BBV contains the execution frequency of each basic block during a program phase) ex-hibit a high degree of stability within a phase [9]. Therefore, in this study we use the number of times each basic block has been executed (i.e., BBV) as the feature to identify the distinct phases of a program.
As defined earlier, a phase is an interval during program execution where the program behaves uniformly in terms of utilizing processor resources. The full execution of a pro-gram can be characterized by a set of distinct phases in which a particular phase can repeat several times during program execution. The representative phase representing each distinct phase is called a  X  X imulation point X  or  X  X im-point [10]. X  By simulating a program using only the Sim-points instead of the entire program, we expect to obtain reasonable simulation accuracy, while significantly reducing simulation runtime.

Industry standard benchmarks are commonly used in the evaluation of microprocessor performance. Presently, the most commonly used benchmark suite is SPEC2000. The number of basic blocks present in individual benchmarks in this suite range from 1000 to 30 , 000. The full execution of just a single benchmark program can surpass one trillion instructions.
The phases found in benchmark programs look similar to the  X  X ag-of-words X  representation of text documents. Basic
A basic block is an instruction execution sequence with one entry point and one exit point. blocks present in a phase are analogous to the words present in a paragraph. We can represent a program phase/document based on the frequency of basic blocks/words. To our best knowledge, in the computer architecture literature, only the K-means algorithm has been applied [3] to this problem. This is in contrast to the amount of previous work done on clustering text, where several data mining techniques have been examined [11, 12, 13]. It has been shown that when K-means is applied to cluster text data using Euclidean dis-tance, the results are marginal [14]. The similarity between program phases and text inspires us to investigate cluster-ing techniques that have been shown to be effective for text applications. In particular, we investigate the mixture of multinomials [15, 12].

Next, we describe mixture of multinomial models, along with the feature reduction algorithm that we apply.
Clustering, using finite mixture models, is a well-known generative method [16]. When applying this method, one assumes that data y is generated from a mixture of K com-ponent density functions, in which the component density function p ( y |  X  j ) represents cluster j for all j s ,where  X  the parameter (to be estimated) for cluster j . The proba-bility density of data y , is expressed by: where the  X  j s are the mixing proportions of the components (subject to:  X  j  X  0and of the N observed data points is then given by: It is difficult to directly optimize (2), so we can use the Expectation-Maximization (EM) [17] algorithm to find a (lo-cal) maximum likelihood or maximum a posteriori (MAP) estimation of the parameters for the given data set. The EM algorithm iterates between an E-step and a M-step un-til convergence which are defined in section 3.2.
Selecting the number of clusters in a mixture model is a difficult task, and remains an open problem. A number of methods for selecting the number of clusters are discussed in [16]. We cannot simply apply the maximum likelihood criterion to determine the number of clusters, because this will lead to a clustering where each data point is a cluster. Some form of penalty for model complexity is needed. Here, we utilize the popular Bayesian information criterion (BIC) approach [18].

The optimal number of components K is selected by: where m isthenumberofparametersforthemodel.
In this paper, we make a naive Bayes assumption which has gained popularity in text classification and clustering due to its simplicity and good performance [19, 20, 21]. The assumption is that each feature (i.e., the number of occur-rence of basic blocks in our case) is independent of any other feature, given the class label. Given this assumption, a data point (i.e., a program phase in our case) is generated ac-cording to a multinomial probability distribution, given the j  X  X : P ( y i |  X  j )= of the feature l occurring in cluster j , and hence is subject to of feature l appearing in data y i ,and D isthenumberof features. To avoid zero values in P j ( b l ), we utilize the MAP estimate with a Dirichlet prior. The use of this type of prior is also known as Laplace smoothing [21]. The updated equa-tions for computing the MAP estimator are as follows: E-step: Ez ij , the posterior probability of data y i belonging to clus-ter j is given by: M-step: where,  X  j = 1+
When we compute the BIC for a mixture of multinomial models using 3, the number of parameters m is K  X  1+ K ( D  X  1).
A phase is typically comprised of 2 , 000 to 30 , 000 features (i.e., the number of basic blocks). Due to this high dimen-sionality (which may impact the performance of the clus-tering [22]), we apply a random projection method, since it has been shown that random projection can obtain reason-able performance [23], and is much faster than other feature reduction methods (e.g., faster than principal component analysis) [24, 25].

In random projection, the original D -dimensional data is projected to a q -dimensional ( q&lt;D ) subspace, using a random D by q matrix R . More specifically, let X D  X  N stands for the original set of N D -dimensional observations. The projected data on the q dimensional subspace is given by X N  X  q = X N  X  D R D  X  q . The rationale behind this com-putationally efficient method is the Johnson-Lindenstrauss lemma [26]. There are many choices for selecting the distri-bution of R ij (the elements of the random projection ma-trix). We could choose to select a Gaussian or uniform distri-bution[27]. We choose to use the equation described below, that was proposed by [23], because it is simple and compu-tationally efficient. Moreover, this equation has shown to perform well on text data [24].
 In our implementation, we omit the it will not affect the separation between data points.
To evaluate the merit of using different clustering algo-rithms, we compare the instructions per cycle (IPC) ob-tained by running a full simulation with using the simula-tion points identified by the different clustering algorithms. The clustering approaches we compare are: 1. K-means algorithm combined with random projection 2. EM applied to a mixture of multinomials, combined The random matrix is generated as described in section 3.3. Since the EM and K-means algorithms may result in local maxima, we apply ten random restarts to initialize all the clustering algorithms in our experiments for each K .To find K , we ran a mixture of multinomials for K =1to kmax = 15. We then picked the clustering result with the largest BIC score. The number of Simpoints obtained for each data is shown in figure 2 and will be discussed in sec-tion 5. The original dimensions were reduced by random projection. For datasets with an original dimension in the range of 1000  X  2000, we reduced them to 15 dimensions. For datasets with dimensions greater than 20000, we reduced them to 100 dimensions. Several techniques have been pro-posed to determine the number of retained dimensions, with Principal Component Analysis (PCA) being one of them. But PCA is computationally expensive to compute for data sets possessing high dimensionality. Therefore, we utilized the heuristics described above to set the number of dimen-sions for random projection. In the following subsections, we describe how an actual simulation is conducted.
The simulations are performed on a popular cycle accurate simulator, the Simplescalar toolset [28]. Simplescalar con-sists of various toolsets, of which sim-fast and sim-outorder are used for this work. A modified version of the sim-fast Simplescalar simulator available from [10] is used to generate basic block vectors.
 The results are evaluated for a set of industry standard SPEC2000 benchmark programs. The programs in the SPEC 2000 suite are frequently used to drive performance evalua-tions of new and enhanced microprocessor architectures in industry and in research. The SPEC2000 programs consist of a set of floating-point and integer programs. Our exper-iments are conducted using both floating-point and integer benchmark programs. The integer benchmark programs ex-hibit less deterministic program behavior as compared to floating point benchmark programs. Therefore, it is more difficult to capture the behavior of integer programs (as we will discuss in section 5). The programs selected in our study exhibit the most non-deterministic behavior in the entire suite.
The BBVs are generated with the help of the basic block vector generation tool provided by [10]. Every phase of 100 million instructions forms a vector, which contains the fre-quency of each basic block executed within that phase. The program is run to completion to generate a matrix, where each row represents a fixed interval of size 100 million in-structions, and each column captures the frequency of each basic block executed during the phase. Thus the phases rep-resent a point in the entire execution space, and each basic block adds a dimension to it. A different phase size could also be used, though a phase size of 100 million instructions reasonably captures the phase transitions in a program. It would also be interesting to have a variable-length phase, but we have restricted our study to fixed-length phases. The dataset is generated for nine programs, with some using dif-ferent input data sets, to form eleven benchmark programs.
The dataset generated is the input to our clustering algo-rithm. After clustering the dataset, we obtain the mean of each cluster. We then select the program phase which has the shortest Euclidean distance to the mean of each clus-ter. The selected point becomes a Simpoint .Eachofthe Simpoints obtained is weighted by the priority of the corre-sponding class. This ensures that the results obtained for each Simpoint is weighted in proportion to its contribution to the overall program performance.

Having obtained the Simpoints and their corresponding weights, the Simpoints are tested by fast-forwarding (i.e., ex-ecuting the program without performing any cycle-accurate simulation, as described in [3]) up to the simulation point, and then running a cycle accurate simulation for 100 mil-lion instructions. The sim-outorder tool provides a conve-nient way to test programs in the manner described above. Fast-forwarding a program implies only a functional simula-tion and avoids any time consuming detailed cycle accurate measurements. The IPC (instructions executed per cycle) is recorded for each Simpoint. The IPC metrics are weighted based on the cluster contributions of each individual Sim-point (i.e., cluster frequency).
Table 1 shows the multiple Simpoints (i.e., cluster repre-sentative) for the eleven benchmark programs used in this study. The numbers in the multiple Simpoints column show the start of the simulation point where a cycle accurate sim-ulation should be performed for a length of 100 million in-structions. To reach each of the Simpoints, the program needs to be fast forwarded to 100 million times the numeric value of the Simpoint and then start the accurate simula-tion run for 100 million instructions. It should be noted here that running the full simulation for the benchmark programs chosen takes about 4  X  5 days, whereas with the methodol-ogy described here, it would take less than 5 hours for any program. As was discussed previously, the full execution of a benchmark program results in up to 1000 phases with each phase of size 100 million instructions. Now, if we simulate for just 2  X  5 phases versus 1000 phases, then we can re-duce simulation time to less than 0 . 5% of the original time. Therefore, simulation time is proportional to the number of Simpoints obtained for a particular program.

Figure 1 shows the results obtained in terms of the per-cent error in Instructions per Cycle (IPC) for the 11 pro-grams, when considering the two clustering techniques. As we can see, our method performs better in 9 out of the 11 programs chosen. This is mainly due to the fact that we model the dataset more appropriately. By using a more ap-propriate clustering algorithm (random projection followed by mixture of multinomials), we reduce the percentage er-ror to 1 . 65% as compared to 3 . 24% obtained by previous techniques (random projection, followed by K-means) for the programs that we tested. The average error reported in [3] is 3%, though this was for all the SPEC2000 bench-mark programs, whereas we are comparing only a subset of the programs that are more interesting. Also, as is shown in Figure 2, we are able to reduce the number of Simpoints by as much as 45% as compared to the Simpoints obtained by [3]. This suggests that we will need fewer points to sim-ulate, and as the simulation time becomes proportional to the number of Simpoints, it consequently reduces the sim-ulation time further. We thereby show that by incorporat-ing our clustering algorithm within the Simpoint tool [3], we can obtain better results. There are other programs in the SPEC2000 benchmark suite, but we focused our study to the 11 most challenging programs to characterize. Basi-cally, the programs we have chosen have comparatively more phase transitions and therefore it is more difficult to capture the phase behavior of the program.
The cost to evaluate the merit of a new design feature continues to grow rapidly with each new generation of mi-croprocessor. To reduce this cost, a sampled version of a benchmark program X  X  execution can be used to drive the simulation study.

In prior work on this problem, K-means clustering was used to identify representative program phases. We ob-served that the workload characterization problem has many similarities to text clustering. In this paper, we have il-lustrated the benefits of utilizing an alternative clustering scheme (specifically, using a mixture of multinomials).
To summarize, this paper makes the following contribu-tions: 1. We are able to accurately capture the distinct phases 2. Our algorithm results in a smaller number of clusters 3. Our toolset is the current state-of-the-art, as compared
We wish to express our sincere thanks to Shi Zhong who provided us with a MATLAB version of EM on a mixture of multinomials model. This research was supported by NSF Grant No. IIS-0347532. [1] S. Mukherjee, S. Adve, T. Austin, J. Emer, and [2] T. Lafage and A. Seznec. Choosing representative each Simpoint. The value in parentheses indicates the weight (i.e.,  X  Number of Simpoints art X 110 [3] T. Sherwood, E. Perelman, G. Hamerly, and [4] S. Girbal, G. Mouchard, A. Cohen, and O. Temam. [5] T. Conte, M. Hirsch, and K. Menezes. Reducing state [6] L. Eeckhout, H. Vandierendonck, and [7] R. Wunderlich, T. Wenisch, B. Falsafi, and J. Hoe. [8] T. Sherwood, S. Sair, and B. Calder. Phase tracking [9] A. Dhodapkar and J. Smith. Comparing program [10] T. Sherwood, E. Perelman, and B. Calder. Basic block [11] M. Steinbach, G. Karypis, and V. Kumar. A [12] M. Meil and D. Heckerman. An experimental [13] S. Zhong and J. Ghosh. Generative model-based [14] A. Strehl, J. Ghosh, and R. Mooney. Impact of [15] S. Vaithyanathan and B. Dom. Model-based [16] G. McLachlan and D. Peel. Finite Mixture Models . [17] A. Dempster, N. Laird, and D. Rubin. Maximum [18] G. Schwarz. Estimating the dimension of a model. The [19] D. Lewis. Naive (Bayes) at forty: The independence [20] T. Mitchell. Machine Learning . McGraw-Hill, 1997. [21] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. [22] J. Dy and C. Brodley. Feature selection for [23] D. Achlioptas. Database-friendly random projections. [24] E. Bingham and H. Mannila. Random projection in [25] D. Fradkin and D. Madigan. Experiments with [26] W. Johnson and J. Lindenstrauss. Extensions of [27] S. Dasgupta. Experiments with random projection. In [28] C. Burger and T. Austin. The simplescalar tool set,
