 The motivation of the present work lies in the growing interest of the machine learning commu-nity for learning tasks that are richer than now well-studied classification and regression. Among those, we especially have in mind the task of ranking , where one is interested in learning a ranking function capable of predicting an accurate ordering of objects according to some attached relevance information. Tackling such problems generally implies the use of loss functions other than the 0-1 misclassification loss such as, for example, a misranking loss [6] or a surrogate thereof. For ( x ,y ) and ( x 0 ,y 0 ) two pairs from some space Z := X  X Y (e.g., X = R d and Y = R ) the misranking loss ` rank and a surrogate convex loss ` sur may be defined for a scoring function f  X  X  X as: Given such losses or, more generally, a loss ` : Y X  X Z  X Z  X  R , and a training sample Z n = { ( X i ,Y i ) } n i =1 of independent copies of some random variable Z := ( X,Y ) distributed according to D , the learning task is to derive a function f  X  X  Y such that the expected risk R ` ( f ) of f is as small as possible. In practice, this naturally brings up the empirical estimate  X  R ` ( f,Z n ) which is a U-statistic [6, 10].
 An important question is to precisely characterize how  X  R ` ( f,Z n ) is related to R ` ( f ) and, more and other quantities such as a measure of the capacity of the class of functions f belongs to and the size n of Z n  X  in other words, we may talk about generalization bounds [4]. Pivotal tools to perform several independent variables to deviate from its expectation; of course, the sharper the concentration inequalities the more accurate the characterization of the relation between the empirical estimate and it is just as important that these inequalities rely as much as possible on empirical quantities. Here, we propose new empirical Bernstein inequalities for U-statistics. As indicated by the name (i) our results are Bernstein-type inequalities and therefore make use of information on the variance of the variables under consideration, (ii) instead of resting on some assumed knowledge about this variance, they only rely on empirical related quantities and (iii) they apply to U-statistics. Our new inequalities generalize those of [3] and [13], which also feature points (i) and (ii) (but not (iii)), while based on simple arguments. To the best of our knowledge, these are the first results that fulfill (i), (ii) and (iii); they may give rise to a few applications, of which we describe two in the sequel. The paper is organized as follows. Section 2 introduces the notations and briefly recalls the basics of U-statistics as well as tail inequalities our results are based upon. Our empirical Bernstein inequali-ties are presented in Section 3; we also provide an efficient way of computing the empirical variance when the U-statistics considered are based on the misranking loss ` rank of (1). Section 4 discusses two applications of our new results: test set bounds for bipartite ranking and online ranking. 2.1 Notation The following notation will hold from here on. Z is a random variable of distribution D taking n denotes the set A Finally, a function q : Z m  X  R is said to be symmetric if the value of q ( z ) = q ( z 1 ,...,z m ) is independent of the order of the z i  X  X  in z . 2.2 U-statistics and Tail Inequalities Definition 1 (U-statistic, Hoeffding [10]) . The random variable  X  U q ( Z n ) defined as is a U-statistic of order m with kernel q , when q : Z m  X  R is a measurable function on Z m . Remark 1 . Obviously, E Z variance estimate of E Z Remark 2 . Two peculiarities of U-statistics that entail a special care are the following: (i) they are sums of identically distributed but dependent variables: special tools need be resorted to in order algorithmic point of view, their direct computations may be expensive, as it scales as O ( n m ) ; in Section 3, we show for the special case of bipartite ranking how this complexity can be reduced. Figure 1: First two plots: values of the right-hand size of (5) and (6), for D uni and kernel q m for m = 2 and m = 10 (see Example 1) as functions of n . Last two plots: same for D Ber (0 . 15) . We now recall three tail inequalities (Eq. (5), (6), (7)) that hold for U-statistics with symmetric and bounded kernels q . Normally, these inequalities make explicit use of the length q max  X  q min of the has range [0 , 1] (an easy way of retrieving the results for bounded q is to consider q/ k q k  X  ). the integer part of the ratio n/m  X  this quantity might be thought of as the effective number of data. To simplify the notation, we will assume that n is a multiple of m and, therefore, b n/m c = ( n/m ) . Theorem 1 (First order tail inequality for  X  U q , [11].) . Hoeffding proved the following: Hence  X   X   X  (0 , 1] , with probability at least 1  X   X  over the random draw of Z n : To go from the tail inequality (4) to the bound version (5), it suffices to make use of the elementary inequality reversal lemma (Lemma 1) provided in section 3, used also for the bounds given below. Theorem 2 (Bernstein Inequalities for  X  U q , [2, 11]) . Hoeffding [11] and, later, Arcones [2] refined the previous result in the form of Bernstein-type inequalities of the form Hence,  X   X   X  (0 , 1] , with probability at least 1  X   X  : With a slight abuse, we will now refer to Eq. (5), (6) and (7) as tail inequalities. In essence, these are confidence intervals at level 1  X   X  for E Z Remark 3 . Eq. (7) is based on the so-called Hoeffding decomposition of U-statistics [11]. It provides a more accurate Bernstein-type inequality than that of Eq. (6), as m X  2 q is known to be smaller than  X  q (see [16]). However, for moderate values of n/m (e.g. n/m &lt; 10 (e.g.  X  = 0 . 05 ), the influence of the log terms might be such that the advantage of (7) over (6) goes unnoticed. Thus, we detail our results focusing on an empirical version of (6).
 Example 1 . To illustrate how the use of the variance information provides smaller confidence inter-p  X  [0 , 1] , for which  X  2 = p m (1  X  p m ) . Figure 1 shows the behaviors of (6) and (5) for various values of m as functions of n . Observe that the variance information renders the bound smaller. This section presents the main results of the paper. We first introduce the inequality reversal lemma, which allows to transform tail inequalities into upper bounds (or confidence intervals), as in (5)-(7). Lemma 1 (Inequality Reversal lemma) . Let X be a random variable and a,b&gt; 0 ,c,d  X  0 such that then, with probability at least 1  X   X  Proof. Solving for  X  such that the right hand side of (8) is equal to  X  gives: Using 3.1 Empirical Bernstein Inequalities Let us now define the empirical variances we will use in our main result.
 Definition 2. Let  X   X  2 q be the U-statistic of order 2 m defined as: and  X   X  2 q be the U-statistic of order 2 m  X  1 defined as: It is straightforward to see that (cf. the definitions of  X  2 q in (6) and  X  2 q in (7)) We have the following main result.
 Theorem 3 (Empirical Bernstein Inequalities/Bounds) . With probability at least 1  X   X  over Z n , And, also, with probability at least 1  X   X  , ( b m is the same as in (7) ) Proof. We provide the proof of (12) for the upper bound of the confidence interval; the same rea-soning carries over to prove the lower bound. The proof of (13) is very similar.
 First, let us call Q the kernel of  X   X  2 q : Q is of order 2 m , has range [0 , 1] but it is not necessarily symmetric. An equivalent symmetric kernel for  X   X  2 q is Q sym : where P m is the set of all the permutations over { 1 ,...,m } . This kernel is symmetric (and has range [0 , 1] ) and Theorem 2 can be applied to bound  X  2 as follows: with prob. at least 1  X   X  where V ( Q sym ) is the variance of Q sym . As Q sym has range [0 , 1] , and therefore (To establish (13) we additionally use  X   X  2 q ( Z n )  X   X  2 q ).
 Following the approach of [13], we introduce and taking the square root of both side, using 1 + p 7 / 3 &lt; 3 and We now apply Theorem 2 to bound | E Z 0  X  to  X / 2 so the obtained inequality still holds with probability 1  X   X  . Bounding appropriate constants gives the desired result.
 Remark 4 . In addition to providing an empirical Bernstein bound for U-statistics based on arbitrary bounded kernels, our result differs from that of Maurer and Pontil [13] by the way we derive it. Here, we apply the same tail inequality twice, taking advantage of the fact that estimates for the variances we are interested in are also U-statistics. Maurer and Pontil use a tail inequality on self bounded random variables and do not explicitly take advantage of the estimates they use being U-statistics. 3.2 Efficient Computation of the Variance Estimate for Bipartite Ranking estimates that enter into play in the presented results are U-statistics with kernels of order 2 m (or 2 m  X  1 ), meaning that a direct approach to practically compute them would scale as O ( n 2 m ) (or O ( n 2 m  X  1 ) ). This scaling might be prohibitive as soon as n gets large.
  X   X  ) in the special case where Y = { X  1 , +1 } and the kernel q f induces the misranking loss (1): which is a symmetric kernel of order m = 2 with range [0 , 1] . In other words, we address the bipartite ranking problem. We have the following result.
 Proposition 1 (Efficient computation of  X   X  2 q can be performed in O ( n ln n ) . Proof. We simply provide an algorithmic way to compute  X   X  2 q the use of  X  instead of = in the first line below). We have The first term of the last line is proportional to the well-known Wilcoxon-Mann-Whitney statistic We show how to deal with the second term, using sorting arguments as well. Note that i = l , j = k , j = l ; using the symmetry of q f then provides the second term (together with the factor The only term that now requires special care is the last one (which is proportional to  X   X  2 q Recalling that q f ( z i ,z j ) = 1 { ( y Let us define E + ( i ) and E  X  ( i ) as and their sizes  X  + i := |E + ( i ) | , and  X   X  i := |E  X  ( i ) | .
 For i such that y i = 1 ,  X  + i is the number of negative instances that have been scored higher than x i by f . From (15), we see that the contribution of i to the last term of (14) corresponds to the number  X  i (  X  A simple way to compute the first sum (on i such that y i = +1 ) is to sort and visit the data by descending order of scores and then to incrementally compute the  X  + i  X  X  and the corresponding sum: when a negative instance is encountered,  X  + i is incremented by 1 and when a positive instance is visited,  X  + i (  X  + i  X  1) is added to the current sum. An identical reasoning works for the second sum. The cost of computing  X   X  q f is therefore that of sorting the scores, which has cost O ( n ln n ) . Here, we mention potential applications of the new empirical inequalities we have just presented. Figure 2: Left: UCI banana dataset, data labelled +1 (  X  1 ) in red (green). Right: half the confidence interval of the Hoeffding bound and that of the empirical Bernstein bound as functions of n test . 4.1 Test Set Bounds A direct use of the empirical Bernstein inequalities is to draw test set bounds. In this scenario, a sample Z n is split into a training set Z train := Z 1: n Z U-statistic inducing loss (such as in (1) or (2)) and Z test is used to compute a confidence interval on the corresponding kernel is q f ( Z,Z 0 ) = ` rank ( f,Z,Z 0 ) , and, with probability at least 1  X   X  where  X   X  2 q Figure 2 displays the behavior of such test set bounds as n test grows for the UCI banana dataset. To produce this plot, we have learned a linear scoring function f (  X  ) =  X  w ,  X  X  by minimizing for  X  = 1 . 0 . Of course, a purely linear scoring function would not make it possible to achieve good ranking accuracy so we in fact work in the reproducing kernel hilbert space associated with the points and evaluate the test set bound on n test = 100 , 500 , 1000 , 5000 , 10000 data points. Figure 2 (right) reports the size of half the confidence interval of the Hoeffding bound (5) and that of the empirical Bernstein bound given in (16). Just as in the situation described in Example 1, the use of variance information gives rise to smaller confidence intervals, even for moderate sizes of test sets. 4.2 Online Ranking and Empirical Racing Algorithms Another application that we would like to describe is online bipartite ranking. Due to space limita-tion, we only provide the main ideas on how we think our empirical tail inequalities and the efficient computation of the variance estimates we propose might be particularly useful in this scenario. First, let us precise what we mean by online bipartite ranking. Obviously, this means that Y = { Z hypotheses f 1 ,f 2 ,...,f T . As ` rank entails a kernel of order m = 2 , we assume that n = 2 T and more clever ways to handle the data but this goes out of the scope of the present paper). We do not specify any learning algorithm but we may imagine trying to minimize a penalized empirical risk like k w k 2 is used then the optimization problem to solve is of the same form as in the batch case: but is solved incrementally here. Rank-1 update formulas for inverses of matrices easily provide means to incrementally solve this problem as new data arrive (this is the main reason why we have mentioned this surrogate function).
 As evoked by [5], a nice feature of online learning is that the expected risk of hypothesis f t can be estimated on the n  X  2 t examples of Z it was not trained on. Namely, when 2  X  data have been processed, there exist  X  hypotheses f 1 ,...,f  X  and, for t &lt;  X  , with probability at least 1  X   X  : If one wants to have these confidence intervals to simultaneously hold for all t and all  X  with prob-ability 1  X   X  , basic computations to calculate the number of pairs ( t, X  ) , with 1  X  t &lt;  X   X  n show We would like to draw the attention of the reader on two features: one has to do with statistical considerations and the other with algorithmic ones. First, if the confidence intervals simultaneously corresponds to a racing algorithm as described in [12]. Theoretically analyzing the relevance of such a race can be easily done with the results of [14], which deal with empirical Bernstein racing, but for non-U-statistics. This full analysis will be provided in a long version of the present paper. Second, it is algorithmically possible to preserve some efficiency in computing the various variance estimates through the online learning process: these computations rely on sorting arguments, and it is possible to take advantage of structures like binary search trees such as AVL trees, that are precisely designed to efficiently maintain and update sorted lists of numbers. The remaining question is whether it is possible to have shared such structures to summarize the sorted lists of scores for various hypotheses (recall that the scores are computed on the same data). This will be the subject of further research. We have proposed new empirical Bernstein inequalities designed for U-statistics. They generalize the empirical inequalities of [13] and [3] while they merely result from two applications of the same non-empirical tail inequality for U-statistics. We also show how, in the bipartite ranking situation, the empirical variance can be efficiently computed. We mention potential applications, with illustra-extensions discussed in the previous section, we wonder whether it is possible to draw similar empir-plan to work on establishing generalization bounds derived from the new concentration inequalities presented. This would require to carefully define a sound notion of capacity for U-statistic-based classes of functions (inspired, for example, from localized Rademacher complexities). Such new bounds would be compared with those proposed in [1, 6, 7, 15] for the bipartite ranking and/or pair-wise classification problems. Finally, we also plan to carry out intensive simulations  X  X n particular for the task of online ranking X  to get even more insights on the relevance of our contribution. Acknowledgments This work is partially supported by the IST Program of the EC, under the FP7 Pascal 2 Network of Excellence, ICT-216886-NOE. LR is partially supported by the ANR project ASAP. [1] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization Bounds for [3] J.-Y. Audibert, R. Munos, and C. Szepesv  X  ari. Tuning bandit algorithms in stochastic environ-[4] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classification : A survey of some recent [5] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of online learning [6] S. Cl  X  emenc  X on, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of u -statistics. [7] Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. An efficient boosting algorithm for combining [8] J. H  X  ajek and Z. Sid  X  ak. Theory of Rank Tests . Academic Press, 1967. [9] J. A. Hanley and B. J. Mcneil. The meaning and use of the area under a receiver operating [10] W. Hoeffding. A Class of Statistics with Asymptotically Normal Distribution. Annals of [11] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the [12] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classifica-[13] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In [15] C. Rudin and R. E. Schapire. Margin-based ranking and an equivalence between AdaBoost [16] R. J. Serfling. Approximation theorems of mathematical statistics . J. Wiley &amp; Sons, 1980.
