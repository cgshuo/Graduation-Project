 A methodology based on  X  X nformation nuggets X  has recently emerged as the de facto standard by which answers to com-plex questions are evaluated. After several implementations in the TREC question answering tracks, the community has gained a better understanding of its many characteristics. This paper focuses on one particular aspect of the evalua-tion: the human assignment of nuggets to answer strings, which serves as the basis of the F-score computation. As a byproduct of the TREC 2006 ciQA task, identical answer strings were independently evaluated twice, which allowed us to assess the consistency of human judgments. Based on these results, we explored simulations of assessor behavior that provide a method to quantify scoring variations. Un-derstanding these variations in turn lets researchers be more confident in their comparisons of systems.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation Measurement, Experimentation TREC, complex information needs, human judgments
Quantitative evaluation has always played an important role in information retrieval research, a tradition that dates back to the Cranfield experiments in the 60 X  X  [4]. The re-liance on controlled, reproducible experiments to guide pro-gress in the field naturally places evaluation methodologies atthecenterofmuchattention. Forthe ad hoc retrieval task, significant work has focused on validating and refining Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. the laboratory tools that researchers use (e.g., [13, 2, 10, 3], just to name a few). These studies provide a degree of confi-dence in the meaningfulness of results from IR experiments.
In the past decade or so, question answering has emerged as an active area of research. By combining term-based information retrieval techniques with deeper linguistic pro-cessing technologies, QA systems return answers instead of hits . A departure from the ranked list has necessitated the development of new evaluation methodologies to assess the quality of system output. So-called  X  X actoid X  questions such as  X  X hen was Sputnik launched? X  presented interest-ing evaluation challenges and have been explored in many studies [15, 6, 9]. In contrast, the nugget-based methodol-ogy for evaluating answers to complex questions X  X he focus of this paper X  X as received comparatively little attention.
After having made substantial headway in factoid QA, researchers have turned their attention to more complex in-formation needs that cannot be answered by simply extract-ing named entities (persons, organization, locations, dates, etc.) from documents. These questions might, for example, involve inferencing or synthesizing information from mul-tiple documents. The definition and  X  X ther X  questions in the NIST-sponsored TREC QA evaluations exemplify this shift to more complex information needs [12]. The complex, interactive question answering task (ciQA) X  X ntroduced by NIST in TREC 2006 and the focus of our paper X  X s another instance of this trend. For these tasks, NIST has developed an evaluation methodology based on  X  X nformation nuggets X , in which humans are called upon to establish the presence of important facts in system responses.

We tackle the following question: are human-based nugget assignments stable? In other words, can assessors reliably determine the nugget content of answer strings? The an-swer holds important implications for the reliability of sys-tem scores and the degree of confidence researchers can have in system comparisons. As a byproduct of the ciQA task in TREC 2006, submissions with identical answer strings were independently judged twice by NIST assessors (with-out their explicit knowledge). This fortunate circumstance provided a unique opportunity to answer this research ques-tion. Analysis shows that, on the whole, NIST assessors are sufficiently consistent such that human variability does not have a large effect on system scores. Through simulation studies, we are able to quantify these scoring variations. We believe that insights gained through analysis of ciQA results can be broadly applied to other complex QA evaluations that employ the same methodology.

This paper is organized as follows: The nugget-based eval-uation methodology and the ciQA task are described in Sec-tions 2 and 3, respectively. A rough analysis of assessor inconsistencies is presented in Section 4 and refined in Sec-tion 5. We describe a simulation-based method for quanti-fying the effects of inconsistent judgments in Section 6 and discuss our model in Section 7. Future extensions are out-lined in Section 8 before the conclusion.
To date, NIST has conducted several formal evaluations of complex question answering: definition questions at TREC 2003,  X  X ther X  questions at TREC 2004 X 2006, and ciQA at TREC 2006 (see next section). All of them have employed the nugget-based evaluation methodology, which has evolved into the de facto standard for complex QA evaluation over the past few years. Given the central role of evaluation in guiding research, aspects of the process warrant closer examination. This section provides a brief overview of the evaluation process; the reader is advised to consult related work for more details [11, 12, 7, 8].

Answers to complex questions are comprised of an un-ordered set of [document-id, answer string] pairs (we use answer string and response interchangeably throughout this paper). Although no explicit limit is placed on the length of each answer string and how many answer strings there are, verbosity is penalized. To evaluate system output, NIST pools answer strings from all systems, removes their asso-ciation with the systems that produced them, and presents them to a human assessor. Using these responses and re-search performed during the original development of the question, the assessor creates an  X  X nswer key X  X  X  list of  X  X n-formation nuggets X  about the target. An information nugget is defined as a fact for which the assessor can make a binary decision as to whether a response contains that nugget [11]. The assessor also manually classifies each nugget as either vital or okay . Vital nuggets represent concepts that must be present in a  X  X ood X  answer, in the opinion of the assessor; on the other hand, okay nuggets contribute worthwhile in-formation but are not essential (they are neither explicitly penalized nor rewarded). Under the pyramid extension [8], multiple assessors are called upon to supply vital/okay judg-ments (after the creation of the answer key), which are then aggregated to produce a nugget importance weight.
After the nugget answer key is created, the assessor then manually scores each run. A single assessor is responsible for each question (over all submitted runs), so she will always use her own answer key. The nugget descriptions themselves vary in quality, ranging from sentence extracts to shorthand notes primarily meant as memory aids. For each answer string, the assessor decides whether or not each nugget is present. Assessors do not simply perform string matches in this decision process X  X ather, the matching occurs at the conceptual level, abstracting away from issues such as vo-cabulary differences, syntactic divergences, paraphrases, etc. In this paper, we study the nugget assignment process using detailed records provided by NIST. These files indicate the location of nuggets in system output, as determined by the assessor. Note that the textual descriptions of the nuggets are themselves byproducts of the evaluation and have no  X  X fficial status X . However, researchers have leveraged these text fragments for automatic evaluation of answers to com-plex questions [7].

Once nugget assignments have been made, the final F-score is straightforwardly derived. Nugget recall is com-puted with respect to vital nuggets only, or alternatively using nugget weights in the pyramid method [8]. Nugget precision is approximated by a length allowance. Both com-ponents are combined into an F-score with  X  =3,which gives recall three times more weight than precision. The final score of a run is an average over the F-scores of indi-vidual questions. As the exact formula is not material to this work, the reader is invited to consult [11, 8] for details.
The most salient aspect of the nugget-based evaluation methodology from the viewpoint of stability and reliability is the complex mapping between useful fundamental units of information (i.e., nuggets) as determined by assessors and the presence of those units in system output. In ad hoc retrieval, this correspondence is straightforward, as docids from assessors X  relevance judgments can be directly matched to hits in the ranked lists returned by systems. The same cannot be said for information nuggets, which represent con-cepts that may manifest in a variety of natural language ex-pressions. As previously discussed, it is the role of NIST as-sessors to establish semantic-level correspondences between system output and answer nuggets. Whether this can be done consistently remains an open question X  X ne that we explore in this paper. The issue of assignment consistency has been raised in the past: for example, by Hildebrandt et al. [5]. They illustrated the general issue with anecdotal evidence, but fell short of a detailed analysis. To our knowl-edge, we are the first to systematically study this question.
The complex, interactive question answering (ciQA) task was started in TREC 2006 as a secondary QA task to push the community simultaneously in two directions: towards more complex information needs (beyond factoids) and to-wards richer models of interaction (beyond today X  X  batch-style evaluation framework).

The ciQA task extended and refined the so-called  X  X e-lationship X  questions piloted in TREC 2005 [14]. A rela-tionship is defined as the ability of one entity to influence another, including both the means to influence and the mo-tivation for doing so. Eight  X  X pheres of influence X  were noted in a previous study funded by the AQUAINT pro-gram: financial, movement of goods, family ties, communi-cation pathways, organizational ties, co-location, common interests, and temporal. Evidence for both the existence or absence of ties is relevant. The particular relationships of interest naturally depend on the context.

A relationship question in the ciQA task X  X  topic, in standard TREC parlance X  X s composed of two parts (see type AAAAAMM AAAA # responses per topic: initial 40 40 37 37 41 42 4 28 28 28 28 32 # responses per topic: final 37 37 37 38 42 36 4 24 24 20 28 30 # identical responses 814 623 631 682 482 1001 133 457 643 374 647 590 avg length per response 148 150 153 151 172 162 288 133 133 174 173 167 complete example in Figure 1). The question template is a stylized information need that has a fixed structure and free slots whose instantiation varies across different topics (represented by items in square brackets). The narrative is free-form natural language text that elaborates on the infor-mation need, providing, for example, user context, a more articulated statement of interest, focus on particular topical aspects, etc. For TREC 2006, five templates were devel-oped and the test set included six instantiations of each, for a total of 30 topics.

In order for large-scale evaluation of interactive question answering to be practical, user X  X ystem interactions in ciQA were encapsulate in HTML pages called interaction forms X  similar to clarification forms in the TREC 2005 HARD track, which focused on single-iteration clarification dialogues [1]. Instead of arbitrarily complex interface controls, interactions were limited to elements that could appear on an HTML form X  X heckboxes, radio buttons, text input boxes, and the like. Each topic is associated with its own individual inter-action form, which can have anything on it so long it satisfies technical restrictions imposed by NIST.

The ciQA evaluation followed a multi-step procedure. In the first round, participants submitted initial answers along with the interaction forms. NIST assessors then interacted with these pages and the results of user input (i.e., CGI bindings) were sent back to the participants. In the second round, participants submitted final answers based on user feedback. NIST then evaluated both the initial and final submissions together. By comparing the two, it was possible to quantify the effects of the interactions.

An interesting byproduct of the ciQA task setup was that, in many cases, the same answer string was independently evaluatedtwice,sincequiteoftenthesameresponseap-peared in both the initial and final submissions. Since all runs were evaluated together, the NIST assessors were not explicitly aware of any repetition. Thus, we had a naturally-occurring experiment that provided a unique look into as-sessor behavior and the nugget assignment process.
The ciQA task in TREC 2006 drew participation from six groups. NIST received ten initial and eleven final runs, which yielded 11 pairs of corresponding initial X  X inal submis-sions. In the spirit of TREC, we have anonymized runtags and simply refer to them by number. However, we do note that 6 and 7 are manual runs, a difference that will have sig-nificant implications later on. To be clear, when we mention a run by number, we are actually referring to the pair of cor-responding initial and final submissions. In instances where the distinction is important, the initial or final submission will be explicitly referenced.

As previously described, submissions to NIST consisted of a set of responses (answer strings). For ciQA, guidelines explicitly stated that they should be ordered in terms of the likelihood that the answer string contains an information nugget X  X n essence, a ranked list. 1 Since most participants employed sentence-level extraction techniques (or variants thereof), we can roughly equate an answer string with a sen-tence extracted from the collection. However, since manual runs were allowed, answer strings could contain arbitrary text not found in any document (e.g., higher-level abstrac-tive summaries written by humans).

All run pairs were processed to retain answer strings that were exactly the same in the initial and final submissions. We employed a strict string comparison, thus yielding a set of responses that were identical in both submissions. 2 Due to the ciQA setup, these answer strings were independently assessed twice. Therefore, by comparing human judgments on both occasions, we can gain insights into the nugget assignment process. We did not examine identical answer strings in runs submitted by different participants.
Descriptive statistics for the submitted runs can be found in Table 1. Lengths are measured in non-whitespace char-acters, the standard for TREC QA evaluations. In total, the test data included thirty topics, six each for five dis-tinct templates. Overall, the final submissions decreased slightly in length, both in terms of number of responses and total length. Run 7, a manual run, was unlike any of the others X  X oth the initial and final submissions (which appear to be identical) contained far fewer responses and were much shorter overall. Otherwise, the remaining runs were not par-ticularly notable based on these statistics alone. Table 1 also shows the number of responses in each run and the amount of overlap between initial and final submissions. These val-ues are important because they tell us how representative the overlapping responses are of the complete system out-put. Note in particular that initial and final submissions differ quite a lot in runs 2, 3, and 5.

Our initial analysis focused on inconsistencies in nugget assignments between the same response in the initial and final submissions X  X e call these  X  X ugget flips X . Note that the notion of precedence is irrelevant for the purposes of case 1 Y  X  Y 65 68 74 78 61 132 65 41 54 82 114 76 case 2 N  X  N 731 529 517 572 374 771 55 403 566 263 502 480 case 3 Y  X  N 5 9 151519436 7 111717 15 case 4 N  X  Y 13 17 25 17 28 55 7 6 12 12 14 19 this study, since all runs were evaluated at roughly the same time. We only refer to  X  X nitial answer string X  and  X  X inal an-swer string X  for convenience. There are four possible cases that could arise when an identical answer string is indepen-dently evaluated twice, enumerated below:
Although in principle it is possible for an assessor to assign more than one nugget to an answer string (i.e., a response containing multiple information nuggets), this seldom hap-pens. For all practical purposes and without affecting subse-quent analyses, we can safely treat assessors X  decision as bi-nary: an answer string either contains a nugget or it doesn X  X . For now, let us assume that nugget assignment on each an-swer string happens independently X  X his is not true, as we discuss in Section 5.

Instance counts and the frequency of the four cases de-scribed above, across all 11 run pairs, are listed in Table 2. How consistent are these binary nugget assignments? An analysis of the raw data is presented in Table 3. From this, we can see that in instances where a nugget was assigned to an answer string in the initial submission, a nugget was also assigned to the same answer string in the final submission on average 83.6% of the time (the first row). In cases where no nugget was assigned to the answer string in the initial sub-mission, the same decision was made in the final submission 96.2% of the time (the second row). The second figure is not surprising, since it is easy to recognize irrelevant responses, but the first figure suggests that binary decisions about the presence or absence of nuggets are relatively consistent.
We note that the distribution of the four cases appear sim-ilar across all submitted runs, with the exception of run 7, a manual run. This particular submission was much shorter in length and much more precise, thus skewing the relative proportion of cases 1 and 2. Also, in both manual runs, cases 3 and 4 occur more frequently X  X his holds implications for our simulation studies, as we shall see later.

What is the net effect of these  X  X lips X ? When indepen-dently evaluating the same answer string twice, an assessor is likely to miss some nuggets (compared to before) and pick up some other nuggets (not previously found). Table 4 quan-tifies this by showing the total number of nuggets  X  X ained X  (i.e., nuggets assigned in the final submission but not in the initial submission) and the total number of nuggets  X  X ost X  (i.e., nuggets assigned in the initial submission but not in the final submission). Overall, the net differences are small, but note the large numbers of nuggets gained and lost for run 6, a manual run. It is not possible to directly translate these numbers into F-score differences since some nuggets are more important than others (either based on the vi-tal/okay distinction or the pyramid nugget weight). The relationship between figures in Table 2 and Table 4 will be-come apparent in the next section.

For reference, the official NIST answer key contains a to-tal of 447 nuggets for 30 topics (an average of 16.2 nuggets per topic). From Table 2, we see that the overlapping an-swer strings contain an average of 76 nuggets per run across all topics (the Y  X  Y case). Although the figures in Table 4 exhibit some variance, the values are relatively small when compared to the total number of nugget assignments made. This appears to support the claim that assessors are suffi-ciently consistent as to not impact final F-scores by much. The question of how much willbetakenupinSection6.
In this section, we revisit the assumption that nugget as-signments are made independently for each answer string, which was adopted in the previous analysis for convenience. This is a simplification, because NIST assessors take the complete system output into consideration during the eval-uation process. For the ciQA task, they were instructed to assign a nugget to the first answer string that contains the information. Thus, nugget assignments were affected by the ordering of responses in a particular system X  X  output, which may have changed between the initial and final submissions. How does this consideration affect our previous analysis? It did not capture the fact that, in many cases, the assessor assigned the same nugget to different answer strings X  X ue to different orderings of the responses or other idiosyncrasies. This does not have an impact on the overall score, but makes assessors seem more inconsistent than they really are. If we take into account order-related dependencies and other details in the nugget assignment process, we arrive at several subcases of the four basic types of nugget assignments shown in Table 2, discussed below.

Just because a nugget was assigned to the same answer string in the initial and final submissions doesn X  X  necessar-ily mean that the assessor found the same nugget. Case 1 (Y  X  Y) actually breaks down into two scenarios:
There are no subcases for case 2. For case 3 (Y  X  N) and 4(N  X  Y), there are two subcases each:
In fact, the already assigned subcases cannot be consid-ered inconsistencies, since they were the correct decisions given the evaluation guidelines (a nugget cannot be assigned twice). The other subcases represent situations where asses-sors X  behavior can not be readily explained.

Table 5 shows counts for all these subcases, which breaks down the figures from Table 2 in greater detail. Case 2 is repeated for convenience. The nuggets  X  X ost X  and  X  X ained X  in Table 4 correspond exactly to the cases 3b and 4b, re-spectively.

Considering this more detailed analysis, we see that  X  X n-consistent X  judgments are represented by cases 1b, 3b, and 4b. We cannot rule out ordering effects as the cause for cases 3a and 4a, since the assessor appears to be adhering to the evaluation guidelines in those situations. So how of-ten are assessors X  judgments actually inconsistent? This is shown in Table 6. Considering all judgments, NIST asses-sors are only inconsistent on average 4.5% of the time for each answer string. This figure appears low since it includes judgments about irrelevant answer strings (and there are many of those). The final row of Table 6 shows the same figures, except with instances of case 2 (no nuggets assigned) removed.
What are the effects of inconsistencies in the nugget as-signment process? If the same run were assessed multiple times by the same human, we would expect variations in the F-score, but how big would these variations be? This answer is critical because it affects how researchers make comparisons between different techniques: in order to con-clude that one system is  X  X etter X  than another, the element of assessor variability must be accounted for. Confidence in making such comparisons is the key to rapid advances in the state of the art.

We have developed a simulation framework that attempts to answer this question. The analyses performed in the previous sections yield a characterization of assessor behav-ior, which we incorporate into simulations. As an approx-imation, we propose an incremental linear model of assess-ment, in which each individual answer string is considered in turn. Based on the known outcome (assignments made by NIST assessors), we can randomly perturb the  X  X fficial X  judgments, thereby simulating the effect of independently evaluating the answer string multiple times. In this simple stochastic model, the two relevant probabilities are:
From this, we can simulate the effects of evaluating sys-tem output, and by repeating the simulation many times, we can quantify the range of scoring variations. Informally, this allows us to  X  X raw error bars X  around system performance, thus facilitating more meaningful comparison between dif-ferent QA techniques. We describe this in more detail below.
Based on our simple model of assessor behavior, we can simulate outcomes of the same run evaluated multiple times. We accomplish this by iterating over all responses in a sub-mission. At each point, we make the following decision:
For simplicity, we ignore the actual content of the answer strings (a decision we discuss in Section 8). Also, we do not consider the case where a different nugget is assigned to the same answer string (corresponding to case 1b in Table 5).
Iterating through the answer strings in a particular sub-mission, we arrive at a simulated set of nugget assignments based on the model outlined above. From these assignments, we compute a new F-score for the submission. Repeating this experiment multiple times, we get a range of simulated scores representing how the submission would have fared had it been independently assessed many times.

We then compute the standard deviation of all these sim-ulated F-scores. Assuming the values are normally dis-tributed, we know that approximately 95% of the scores will fall within  X  2  X  of the mean. This range, in essence, provides the confidence interval that quantifies variations that can be attributed to assessor inconsistencies in nugget assignment (at least according to our simulation model). Thus, if a sub-mission were independently evaluated multiple times, we can claim with 95% confidence that the F-score will fall within this range. The upshot is that we have now quantified the effects of judgment variation. Obviously, these computa-tions are dependent on the fidelity of the simulation and the realism of our assessor model. We first present experimental results and defer a detailed discussion to Section 7.
The assessor model described above was applied to all fi-nal submissions from the TREC 2006 ciQA task. For the  X  X lip X  probabilities, we employed figures from Table 3. Val-ues in the first row were selected for P ( nug | y ) and one minus the values in the second row were used for P ( nug | n ). We ran two separate sets of experiments: in the first, individual probabilities were employed for each run (the values in the eleven different columns); in the second, the average prob-abilities were used for simulating all runs. For convenience, we refer to these as the individual and overall conditions, respectively. Since this method for determining evaluation reliability is untested, we opted to start with the simpler breakdown of assessor inconsistencies, as opposed to the more detailed analysis presented in Table 5. Possible ex-tensions are discussed in Section 8.

For each run, we simulated the scoring process 100 times, and then computed statistics (mean and standard deviation) over these values. Following the official TREC 2006 ciQA task guidelines, we computed pyramid F-scores, which in-corporate nugget weights into the recall calculation [8].
Results of the simulations with individual probabilities are shown on the left in Figure 2. Equivalent results from the simulation with overall probabilities are shown on the right in the same figure. In both bar graphs, the bars are sorted in decreasing order of the mean of the simulation results. The error bars indicate the range  X  2  X  (around the mean), which defines the 95% confidence interval for judgment vari-ability (as previously described). The solid diamonds show the official NIST F-scores of each run.

The two different sets of experiments produce different results. For the individual probabilities case, official NIST scores fall squarely with our  X  2  X  interval for all the final submissions except for those from runs 2, 3, and 5. From Table 1, we see that these are exactly the runs where the initial and final submissions had little overlap X  X hich meant that less data were available for parameter estimation. Oth-erwise, for many of the submissions, the simulated means were very close to the official F-scores.

The right bar graph in Figure 2 shows results from the overall probabilities condition, in which model parameters were derived from averaging across all runs. This seems to yield a simulation with greater fidelity, as there is a better correlation between the ranking generated by official NIST scores and the ranking generated by the simulation (sorted by the means). However, we note that for the final sub-missions in runs 6 and 7, simulated scores are very different (much lower) than the official scores. It is no coincidence that both of these are manual runs X  X  point we will take up in the next section.
The importance of meta-evaluation in IR research is well established, since progress in the field hinges on accurate and reliable measurements of system performance. This work represents a contribution to question answering evaluation because it is the first in-depth exploration of nugget assign-and overall probabilities (right). ment inconsistencies that we are aware of. Voorhees [11] mentioned scoring variations attributed to assessors, but did not undertake a detailed analysis. In the TREC 2003 QA track, duplicate runs were also independently assessed twice (but for different reasons). For identical runs, she noted a maximum F-score difference of 0.043, with an average of 0.013. Our detailed analysis of assessor behavior and the re-sults of the simulation experiments help researchers better understand the characteristics of nugget-based evaluations. We believe that many insights gained in this study can be broadly applied to other tasks that employ the same evalua-tion methodology, even though the actual information needs may be different.

The differences between the bar graphs in Figure 2 sug-gest that it is more desirable to construct a general model of assessor behavior using aggregate data from all submis-sions. The system ranking generated with overall probabil-ities correlates better with the official NIST ranking. This makes sense, as we expect a uniform distribution of assessor errors in general. This assumption, however, appears to be violated in some situations, as we explain.

Most complex QA systems today employ some variant of passage or sentence retrieval. Since they are fundamentally extractive, the quality of the answer strings is relatively uni-form. Put another way, if one were to build language models of responses, they would be expected to have relatively low cross-entropy. Thus, we should observe the same types of assessor inconsistencies across all submissions. This intu-ition is borne out by our analyses, as the distribution of the  X  X ugget flip X  cases is fairly similar across automatic runs. Our observations are also confirmed by the simulation ex-periments, where official scores fall within the confidence interval derived from our model of assessor inconsistencies. However, human involvement in manual runs might make them qualitatively different from primarily extractive runs X  thus, we might expect different types of inconsistencies.
In particular, estimation of P ( nug | n ) is sensitive to two factors: the underlying performance of the run and the length of the response. One would expect runs with human involve-ment to be better. And the better the system output is to begin with, the greater the chance that a response will ac-tually contain a nugget. With respect to the second point, P ( nug | n ) is derived from overlapping responses between ini-tial and final submissions, and the number of overlapping responses is at least indirectly related to the length of each submission. Implicit in the parameter is the assumption of equal length across all submissions X  X therwise, longer runs would be unfairly rewarded because they generate more ran-dom events by the simulation (i.e., N  X  Y flips) and shorter runs would be unfairly penalized for the opposite reason.
We believe that these observations explain why the sim-ulation model does not agree with official NIST scores for the manual runs 6 and 7. The first contained manually-selected sentences  X  X added X  with results from a sentence retrieval algorithm. Therefore, some of the responses were inherently better than others. In addition, we see from Ta-ble 2 that run 6 had a proportionally larger number of Y  X  and N  X  Y flips, probably caused by human involvement in preparing the submission. Due to these factors, our simula-tion model does not appear to accurately capture assessor behavior. The explanation for why our mean simulation score diverges from the actual NIST score for run 7 ap-pears simple: the submission was much shorter in length compared to the others, thus reducing the likelihood of gen-erating N  X  Yflips.

In general, differences in run length also explain the re-lationship between the mean simulation F-scores and the official NIST F-scores (Figure 2, right). Considering results from the overall probabilities condition, our experiments ap-pear to have underestimated the performance of runs 11 and 10, which are shorter than average. Similarly, our experi-ments may have overestimated the performance of run 5, the longest.
Simulations have been previously employed as a general method to quantify evaluation stability and reliability X  X or example, Zobel X  X   X  X ake one out X  experiments [16] demon-strated that ad hoc test collections built from pooling are reusable; Lin [6] applied similar methods to examine re-sources for factoid QA evaluation. Since it is impractical to manually assess system output repeatedly, simulations offer an attractive alternative.

Our approach, however, is different in trying to better understand assessor behavior; this knowledge is then incor-porated into models of the assessment process. Although the current implementation is relatively crude, the general approach can certainly be extended with more faithful rep-resentations. In this section, we discuss a number of possible refinements and extensions.

First, our model of the evaluation process does not ac-tually take content into account X  X hat is, the probabilities of nugget assignment are not conditioned on any features of the answer string. Intuitively, one would expect this to be an important factor, since the semantic distance be-tween an answer string and an answer nugget might affect its  X  X onfusability X . One might conceivably incorporate some content-based metric, for example, nugget match according to
Pourpre [7]. However, it would be very difficult to real-istically model this effect, and the extra degree of freedom in estimating parameters might cause data sparseness to be-come an issue.

Second, we assume that each answer string is indepen-dently assessed in sequential order. In reality, assessors as-sign nuggets in a more  X  X olistic X  fashion X  X hey consider the responses and the answer nuggets simultaneously. The as-sessor is unlikely to read system output in a strict linear fashion, and most likely scans the answer strings repeatedly, focusing on regions of interest. In addition, effects such as assessor fatigue may be significant for long submissions X  X or excessively lengthy outputs, an assessor might at some point simply stop reading. None of these considerations are incor-porated into the static probabilities in our assessor model. Specifically, the issue of answer length is of concern, since our experiments have revealed it to be an important factor in simulation fidelity.

Third, it might be insightful to model inconsistencies for individual NIST assessors, since they might be prone to dif-ferent types of errors. Although such information is not presently available to researchers, to our knowledge NIST does keep records of the mapping between assessors and topics. The setup of the evaluation ensures that one single assessor examines all runs. Thus, inter-assessor differences might be more pertinent than inter-run differences.
Obviously, the validity of any simulation result hinges on the fidelity of the underlying model. A more faithful rep-resentation will yield a better characterization of scoring variations. A better understanding of this  X  X oise X  will help researchers more accurately diagnose system performance. Thus, explorations of evaluation methodology must keep pace with system development, suggesting a constant need for careful meta-evaluation.

Although we have no doubt that our simulation-based method can be applied to analyze other instances of complex QA evaluation (for example,  X  X ther X  questions in TREC), it remains to be seen if model parameters generalize across different tasks. In other words, do the values P ( nug | y )and P ( nug | n ) quantify intrinsic human variability, or are they task specific? This remains an open question, and one we are keen on exploring.
Due to fortunate circumstances, the TREC 2006 ciQA task provided a unique look into the nugget-based evalua-tion methodology that is commonly employed in complex QA. Analysis of run data yielded statistics about assessor inconsistencies that were then used to develop a model of the evaluation process. Simulation experiments using these models allowed us, for the first time in QA evaluation, to quantify the effects of assessor inconsistencies in nugget as-signment. These results help us make more meaningful com-parisons between systems, isolating actual differences from human-attributable scoring variations. This ability provides the community with a more accurate compass for charting research progress.
This work has been supported in part by DARPA con-tract HR0011-06-2-0001 (GALE). We are grateful to Ellen Voorhees, Hoa Dang, and all the NIST assessors for making TREC possible; also, to Diane Kelly for her role in making the ciQA task possible. The first author would like to thank Kiri and Esther for their kind support. [1] J. Allan. HARD track overview in TREC 2005: High [2] C. Buckley and E. Voorhees. Retrieval evaluation with [3] B. Carterette, J. Allan, and R. Sitaraman. Minimal [4] C. Cleverdon, J. Mills, and E. Keen. Factors [5] W. Hildebrandt, B. Katz, and J. Lin. Answering [6] J. Lin. Evaluation of resources for question answering [7] J. Lin and D. Demner-Fushman. Automatically [8] J. Lin and D. Demner-Fushman. Will pyramids built [9] J. Lin and B. Katz. Building a reusable test collection [10] M. Sanderson and J. Zobel. Information retrieval [11] E. Voorhees. Overview of the TREC 2003 question [12] E. Voorhees. Overview of the TREC 2004 question [13] E. Voorhees. Variations in relevance judgments and [14] E. Voorhees and H. Dang. Overview of the TREC [15] E. Voorhees and D. Tice. Building a question [16] J. Zobel. How reliable are the results of large-scale
