 Density estimation is a key element of statistician X  X  tool-box, yet it remains a challenging problem. A popular class of methods relies on mixture models, such as Parzen windows (Parzen, 1962; Silverman, 1986) or mixtures of Gaussians or other basis functions (McLachlan &amp; Basford, 1988). These models are normally learned using the likeli-hood. However, density estimation is often not the ultimate goal but rather an intermediate step in solving another prob-lem. For instance, we may ultimately want to compute the expectation of a random variable or functions thereof. In this case it is not clear whether likelihood is the ideal ob-jective, especially when the training sample size is small. A second class of density estimators employ exponen-tial family models and are based on the duality between maximum entropy and maximum likelihood estimation (Barndorff-Nielsen, 1978; Dud  X   X k et al., 2004; Altun &amp; Smola, 2006). These methods match the moments of the estimators to those of the data, which helps focus the mod-els on certain aspects of the data for particular applications. However, these parametric moment based methods can be too limited in terms of the class of distributions. Further-more, exponential families tend to require highly nontriv-ial integration of high-dimensional distributions to ensure proper normalization. We desire to overcome these draw-backs and extend this technique to a larger class of models. In this paper, we generalize moment matching to nonpara-metric mixture models. Our major aim is to tailor these density estimators for a particular function class, and pro-vide uniform convergence guarantees for approximating the function expectations. The key idea is if we have good knowledge of the function class, we can tightly couple the density estimation with this knowledge. Rather than per-forming a full density estimation where we leave the func-tion class and subsequent operations arbitrary, we restrict our attention to a smaller set of functions and the expec-tation operator. By exploiting this kind of domain knowl-edge, we make the hard density estimation problem easier. Our approach is motivated by the fact that distributions can be represented as points in the marginal polytope in re-producing kernel Hilbert spaces (RKHS) (Wainwright &amp; Jordan, 2003; Smola et al., 2007). By projecting data and density estimators into RKHS via kernel mean maps, we match them in that space (also referred to as the feature space). Choosing the kernel determines how much infor-mation about the density is retained by the kernel mean map, and thus which aspects ( e.g. , moments) of a den-sity are considered important in the matching process. The matching process, and thus our density estimation proce-dure, amounts to the solution of a convex quadratic pro-gram. We demonstrate the application of our approach in experiments, and show that it can lead to improvements in more complicated applications such as particle filtering and image processing. Let X be a compact domain and X = { x 1 , . . . , x m } be a sample of size m drawn independently and identically distributed (iid.) from a distribution p over X . We aim to find an approximation  X  p of p based on the sample X . Let H be a reproducing kernel Hilbert space (RKHS) on X with kernel k : X  X  X 7 X  R and associated feature map  X  : X 7 X  H such that k ( x, x 0 ) =  X   X  ( x ) ,  X  ( x By design H has the reproducing property, that is, for any f  X  H we have f ( x ) =  X  f, k ( x,  X  )  X  H . A kernel k is called universal if H is dense in the space of bounded continu-ous functions C 0 ( X ) on the compact domain X in the L  X  norm (Steinwart, 2002). Examples of such kernels include Gaussian kernel exp(  X  X  x  X  x 0 k 2 / 2  X  2 ) and Laplace ker-nel exp(  X  X  x  X  x 0 k / 2  X  2 ) .
 The marginal polytope is the range of the expectation of the feature map  X  under all distributions in a set P , i.e. , M := 2003). The map  X  : P 7 X  X  associates a distribution to an element in the RKHS. For universal kernels, the elements of the marginal polytope uniquely determine distributions: Theorem 1 (Gretton et al. (2007)) Let k be universal and P denote the set of Borel probability measures p on X with E x  X  p [ k ( x, x )] &lt;  X  . Then the map  X  is injective. Given a finite sample X from p ,  X  [ p ] can be approximated by the empirical mean map  X  [ X ] := 1 m P m i =1  X  ( x i suggests that a good estimate  X  p of p should be chosen such that  X  [ X  p ] matches  X  [ X ] : this is the key idea of the paper. The flow of reasoning works as follows: The first line of this reasoning was established in (Al-tun &amp; Smola, 2006, Theorem 15). Let R m ( H , p ) be the Rademacher average (Bartlett &amp; Mendelson, 2002) associ-ated with p and H via where  X   X  X  X  1 } is uniformly random. We use it to bound the deviation between empirical means and expectations: Theorem 2 (Altun &amp; Smola (2006)) Assume k f k  X   X  R for all f  X  H with k f k H  X  1 . Then for &gt; 0 with probability at least 1  X  exp(  X  2 mR  X  2 / 2) we have k  X  [ p ]  X   X  [ X ] k H  X  2 R m ( H , p ) + .
 This ensures that  X  [ X ] is a good proxy for  X  [ p ] . To carry out the last step of (1) we assume the density estimator  X  p is a mixture of a set of candidate densities p i (or prototypes): where 1 is a vector of all ones. Here the goal is to obtain good estimates for the coefficients  X  i and to obtain perfor-mance guarantees which specify how well  X  p is capable of estimating p . This can be cast as an optimization problem: To prevent overfitting, we add a regularizer  X [  X  ] , such as k  X  k 2 , and weight it by a regularization constant  X  &gt; 0 . Using the expansion of  X  p in (2) we obtain a quadratic pro-gram (QP) for  X  min where I is the identity matrix. Q  X  R n  X  n and l  X  R n are given by Q l j =  X   X  [ X ] ,  X  [ p j ]  X  H = 1 m X m By construction Q 0 is positive semidefinite, hence the program (4) is convex. We will discuss examples of kernels k and prototypes p i where Q and l have closed form in Sec-tion 5. In many cases, the prototypes p i also contain tun-able parameters. We can also optimize them via gradient methods. Before doing so, we first explain our theoretical basis for tailoring density estimators. Given functions f  X  H , a key question is to bound how well the expectations of f with respect to p can be approx-imated by  X  p . We have the following lemma: Lemma 3 Let &gt; 0 and 0 := k  X  [ X ]  X   X  [ X  p ] k H . Under the assumptions of Theorem 2 we have with probability at least 1  X  exp(  X  2 mR  X  2 / 2) sup Proof In the RKHS, we have E x  X  p [ f ( x )] =  X  f,  X  [ p ]  X  and E x  X   X  p [ f ( x )] =  X  f,  X  [ X  p ]  X  H . Hence the LHS of the bound equates to sup k f k assumption on  X  [ X  p ] and Theorem 2 completes the proof. This means that we have good control over the behavior of the expectations, as long as the function class is  X  X mooth X  on X in terms of the Rademacher average. It also means that k  X  [ X ]  X   X  [ X  p ] k H is a sensible objective to minimize if we are only interested in approximating well the expecta-tions over functions f .
 This bound also provides the basis for tailoring density es-timators. Essentially, if we have good knowledge of the function class used in an application, we can choose the corresponding RKHS or the mean map. This is equivalent to filtering the data and extracting only certain moments. Then the density estimator  X  p can focus on matching p only up to these moments. We now give concrete examples of density estimation. A number of existing methods are special cases of our setting. Discrete Prototype or Discrete Kernel The simplest case is to represent p by a convex combination of Dirac measures p i ( x ) =  X  x i . Particle filters (Doucet et al., 2001) use this choice when approximating distributions. For in-stance, we could choose x i to be the set of training points. In this case Q defined in (5) equals the kernel matrix and l is the vector of empirical kernel averages: The key difference between an unweighted set as used in particle filtering and our setting is that our expansion is specifically optimized towards good estimates with respect to functions drawn from H .
 The problem of data squashing (DuMouchel et al., 1999) can likewise be seen as a special case of kernel mean matching. Here one aims to approximate a potentially large set X by a smaller set X 0 = { ( x 1 ,  X  1 ) , . . . , ( x weighted observations. We want to discard X and only re-tain X 0 for all further processing. If k  X  [ X ]  X   X  [ X small, we expect X 0 to be a good proxy for X .
 Instead of using generic kernels k and discrete measures  X  i as prototypes for density estimation, we may reverse their roles. That is, we may pick generic densities p i and a Dirac kernel k ( x, x 0 ) =  X  ( x = x 0 ) . Note this is only well defined for discrete domains X . 1 In this case the mean op-erator simply maps a distribution into itself and we obtain
Q Gaussian Prototype In general we will neither pick dis-crete prototypes nor discrete kernels for density estimation. We now give explicit expressions for Gaussian prototypes where d is the dimension of the data,  X  i 0 is a covariance matrix, and k x  X  x 0 k 2  X  squared Mahalanobis distance. When used in conjunction with different kernels, we have the expansions in Table 1. Other Prototypes and Kernels Other combinations of kernels and prototypes also lead to closed form expansions. For instance, similar expressions also holds for a Laplacian kernel. However, this involves more tedious integrals of the ple is to use indicator functions on unit intervals centered at x i as p i and a Gaussian RBF kernel. In this case, both Q and l can be expressed using the error function (erf). Furthermore, Jebara et al. (2004) introduced kernels on probability functions which effectively used definition (5). While they were not motivated by the connection between kernels and density estimation, their results for rich classes of densities, such as HMMs, can be used directly to com-pute our Q and l . Our work is related to the density estimators of Vapnik &amp; Mukherjee (2000) and Shawe-Taylor &amp; Dolia (2007). The main difference lies in the function space chosen to mea-sure the approximation properties. The former uses the Banach space of functions of bounded variation, while the latter uses the space L 1 ( q ) , where q denotes a distribution over test functions. For spherically invariant distributions over test functions our approach and the latter approach are identical, with a key difference (to our advantage) that our optimization is a simple QP which does not require con-straint sampling to make the optimization feasible. Support Vector Density Estimation The model of Vap-nik &amp; Mukherjee (2000) can be summarized as follows: let F [ X  p ] be the cumulative distribution function of  X  p and let F [ X ] be its empirical counterpart. Assume  X  p is given by (2), and that we have a regularizer  X [  X  ] as previously dis-cussed. In this case the Support Vector Density Estimation problem can be written as That is, we minimize the ` 1 distance between the empir-ical and estimated cumulative distribution functions when evaluated on the set of observations X .
 To integrate this into our framework we need to extend our setting from Hilbert spaces to Banach spaces. De-note by B a Banach space, let X be a domain furnished with probability measures p, p 0 , and let  X  : X 7 X  B be a feature map into B . Analogously, we define the mean map  X  : P 7 X  B as  X  [ p ] := E x  X  p ( x ) [  X  ( x )] . More-over, we define a distance between distributions p and p 0  X  tor function, and use the ` m 1 norm on  X  we recover SV den-sity estimation as a special case.
 Expected Deviation Estimation Shawe-Taylor &amp; Dolia (2007) defined a distance between distributions as follows: let H be a set of functions on X and q be a probability distribution over F . Then the distance between two distri-butions p and p 0 is given by That is, we compute the average distance between p and p 0 with respect to a distribution of test functions. Lemma 4 Let H be a reproducing kernel Hilbert space, f  X  H , and assume q ( f ) = q ( k f k H ) with finite E f  X  q [ k f k H ] . Then D ( p, p 0 ) = C k  X  [ p ]  X   X  [ p some constant C which depends only on H and q .
 Proof Note that by definition E x  X  p [ f ( x )] =  X   X  [ p ] , f  X  Using linearity of the inner product, Equation (11) equals = k  X  [ p ]  X   X  [ p 0 ] k H where the integral is independent of p, p 0 . To see this, note be turned into, say, the first canonical basis vector by a ro-tation which leaves the integral invariant, bearing in mind that q is rotation invariant.
 The above result covers a large number of interesting func-tion classes. To go beyond Hilbert spaces, let  X  : X 7 X  B be the transformation from x into f ( x ) for all f  X  H and also be written as k  X  [ p ]  X   X  [ p 0 ] k B , where  X  is the mean map into Banach spaces. Its main drawback is the nontriv-ial computation for constraint sampling (de Farias &amp; Roy, 2004) and the additional uniform convergence reasoning required. In Hilbert spaces no such operations are needed. We focus on two aspects: first, our method performs well as a density estimator per se ; and second, it can be tailored towards the expectation over a particular function class. 7.1. Methods for Comparison Gaussian Mixture Model (GMM) 2 The density was represented as a convex sum of Gaussians. GMM was ini-tialized with k -means clustering. The centers, covariances and mixing proportions of the Gaussians were optimized using the EM algorithm. We used diagonal covariances in all our experiments. We always employed 50 random restarts for k -means, and returned the results from the best restart.
 Parzen Windows (PZ) The density was represented as an average of a set of normalized RBF functions, with each centered on a data point. The bandwidths of the RBF functions were identical and tuned via the likelihood using leave-one-out cross validation.
 Reduced Set Density Estimation (RSDE) 3 Girolami &amp; He (2003) compressed a Parzen window estimator using RBF functions of larger bandwidths. The reduced represen-tation was produced by minimizing an integrated squared distance between the two densities.
 Kernel Moment Matching (KMM) In applying our method, we used Gaussians with diagonal covariances as our prototypes p i . The regularization parameter  X  in our algorithm was fixed at 10  X  10 throughout. Since KMM may be tailored for different RKHS, we instantiated it with the four different kernels in Table 1. We denote them as LIN, POL2, POL3 and RBF, respectively. Our choice of kernel corresponded to the function class where we evaluated the expectations. The initialization of the prototypes will be further discussed below. 7.2. Evaluation Criterion We compared various density estimators in terms of two criteria: negative log-likelihood and discrepancy between function expectations on test data. Since different algo-rithms are optimized using different criteria, we expect that each will win with respect to the criterion it employs. The benefit of our density estimator is that we can explicitly tailor for different classes of functions. For this reason, we will focus on the second criterion.
 Given a function f , the discrepancy between function ex-pectations is computed as follows: ( i ) Evaluate function expectation using test points, i.e. , 1 m P m i =1 f ( x i uate function expectation using estimated density  X  p , i.e. , E x  X   X  p [ f ( x )] . ( iii ) Calculate 1 m P and normalize it by 1 m P m i =1 f ( x i ) .
 We will compare various methods either by repeated ran-dom instantiation or random split of the data (which we will make clear in context). For both cases, we will per-form paired sign tests at the significance level of 0.01 on the results obtained from the randomizations. We will always present the median of the results in subsequent tables, and highlight in boldface those statistically equivalent methods that are significantly better than the rest. 7.3. Synthetic Dataset In this experiment, we use synthetic datasets to compare various methods as the sample size changes. We also show that KMM leads to sparse solutions.
 Data Generation We generated 2 dimensional mixtures and covariances 0 . 8 2 I , 1 . 2 2 I and I respectively. The mix-ing proportions were 0 . 2 , 0 . 3 and 0 . 5 respectively. We var-ied the training sample size while always testing on sam-ples of size 1000 . For each fixed training size, we randomly instantiated the experiments 100 times.
 Experimental Protocol All training data points were used as prototypes for RSDE and KMM. Their initial co-variances were set to be identical, and were initialized in both cases using the approach of RSDE. We used the RBF instance of KMM and set the bandwidth  X  of the kernel to be the same as that for the prototypes. GMM used 3 cen-ters.
 Negative Log Likelihood The results are plotted in Fig-ure 1. GMM performs best in general, while KMM is su-perior for small sample sizes. This is not surprising since we used a correct generative model of the data for GMM. When the sample size is small (less than 30 data points for each cluster), GMM is susceptible to local maxima and does not result in good estimates.
 Sparsity of the Solution KMM also leads to sparse so-lutions ( e.g. , Figure 1). When using all data points as can-didate prototypes, KMM automatically prunes away most of them and results in a much reduced representation of the data. In terms of both likelihood and sparsity, KMM is su-perior to other reduction method such as RSDE (Table 2). 7.4. UCI Dataset We used 15 UCI datasets to compare various methods based on the discrepancies between function expectations. Data Description We only included real-valued dimen-sions of the data, and normalized each dimension of the data separately to zero mean and unit variance. For each dataset, we randomly shuffled the data points for 50 times. In each shuffle, we used the first half of the data for training and the remaining data for testing. In each shuffle, we ran-domly generated 100 functions f to evaluate the discrep-ancy criterion, i.e. , f = P m 0 i =1 w i k ( x i ,  X  ) where m was uniformly random in [1 , m ] , w i  X  R was uniformly random in [  X  1 , 1] , and the x i were uniformly sampled from the test points. Thus, each method resulted in 5000 num-bers for each dataset.
 Experimental Protocol Both GMM and KMM used 10 prototypes and diagonal covariances, and both were initial-ized using k -means clustering. We used all four instances of KMM, namely LIN, POL2, POL3 and RBF, for the ex-periments, depending on the function class where we eval-uated the expectations. When we used the RBF instance of KMM, we set the bandwidth  X  of the kernel to the median of the distances between data points. Besides optimizing the mixing proportions of the prototypes of KMM, we also used conjugate gradient descent to optimize the center po-sitions and covariances of the prototypes.
 Negative Log Likelihood Kernel moment matching can be a very different objective from the likelihood (Table 3). Except for the LIN instance, KMM results in much larger negative log-likelihood. This suggests that if the purpose of density estimation is to approximate the function expecta-tions, likelihood is no longer a good criterion. We confirm this observation in our next experiment.
 Discrepancy between Function Expectations We used four classes of functions corresponding to the RKHS of the LIN, POL2, POL3 and RBF instances of KMM. For non-linear functions KMM clearly outperforms other density estimators, while for linear functions KMM has equivalent performance to PZ and GMM (Table 4). These results are not surprising, since KMM is explicitly optimized for ap-proximating the function expectations well. Note that PZ is the second best for polynomial functions. This is rea-sonable since PZ retains all training points in the density, and should perform better than compressed representations such as GMM and RSDE. We also applied this new exper-imental protocol to the synthetic mixture of 3 Gaussians from the last section. We instantiate the synthetic data with 3 different sample sizes: 100, 500 and 1000. The results are shown in the last three rows of Table 3 and 4, which are consistent with those for UCI data. A closer view of the difference between GMM and KMM using  X  X overtype X  dataset is shown in Figure 2. We chose to compare GMM and KMM because they are initialized similarly.
 As an aside, we remark that PZ and GMM also match the empirical mean 1 m P m j =1 x j . This is obvious for PZ. For GMM, in each EM iteration, the centers  X  i and weights  X  i of each p i are updated via  X  i  X  P and  X  i  X  1 m P m j =1  X  i j . Here  X  i j is the probability of x being generated by p i . It follows that E p [ x ] = P i  X  In this section, we employ KMM for two different applica-tions: message compression in graphical models, and im-age processing. The common feature of these two applica-tions is that they involve density estimation for computing the expectation of a function, which is the relevant setting for KMM. 8.1. Message Compression We use density estimation to compress messages in graph-ical models. This is of particular interest for applications such as distributed inference in sensor networks. It is our desire to compress the messages to cater for limited power supply and communication bandwidth. We will use a par-ticle filtering example to compare GMM and KMM only, since they performed best in our earlier experiments. We model a one dimensional time series y t ( t = 1 . . . 100 ) as being conditionally independent given an unobserved state s t , which is itself Markovian. This system evolves as follows: The random variables  X  and  X  represent process and mea-surement noise, respectively, and are modeled as mixtures of Gaussians, Throughout this experiment, we fix  X  to 0 . 2 and choose  X  i to be { X  1 . 5 ,  X  0 . 5 , 0 . 5 , 1 . 5 , 2 } . We initialize s Note that our setting is a modification of de Freitas X  X  demo where we only change the process noise from a unimodal gamma distribution to a more complicated mixture model. The task of particle filtering (Doucet et al., 2001) is to in-fer the hidden state given past and current observations. This can be carried out by estimating the filtering density cedure. First, the current filtering density p ( s t | Y t agated into the future via the transition density p ( s t +1 to produce the prediction density p ( s t +1 | Y t ) , i.e. , E
Second, p ( s t | Y t ) is updated via Bayes X  law, The integral in (15) is usually intractable since the filter-ing density p ( s t | Y t ) can take a complicated form. There-fore, p ( s t | Y t ) is often approximated with a set of samples called particles. For distributed inference, it is these sam-ples that need to be passed around. We want to compress the samples using density estimation such that we still do well in computing E s ple, p ( s t +1 | s t ) takes the form using this kernel, and compress messages by targeting a good approximation of E s We use 5 centers for both GMM and KMM to compress the messages. We compare the filtering results with the true states. The error is measured as the root mean square of the deviations. The results for compressing different num-bers of particles are reported in Table 5. We find that fil-tering results after compression even outperform those ob-tained from the full set of particles (PF). In particular, the results for KMM are slightly better than those for GMM. By compression, we have extracted the information most essential to statistical inference, and actually made the in-ference more robust. If the compression is targeted to E ply get better results.
 The shortcomings of general purpose density estimation also arise in the more general settings of message passing and belief propagation. This is due to the way messages are constructed: given a clique, the incoming messages are multiplied by the clique potential and all variables not in the receiver are integrated out. In most cases, this makes the outgoing messages very complicated, causing signif-icant computational problems. Popular methods include particle filtering, which uses a discrete approximation of the messages, and expectation propagation, which uses a single Gaussian approximation of the messages (Minka, 2001). We plan to further investigate KMM in these gen-eral settings. Our key benefit is that we can customize the approximation properties for a particular graphical model. 8.2. Image Retrieval and Categorization Following the work of (Rubner et al., 2000; Greenspan et al., 2002), we use density estimation as an intermediate step for image retrieval and categorization. Image retrieval is the task of finding from a given database the set of images similar to a given query image. An im-age is normally characterized by the distribution over fea-tures ( e.g. , color, texture) of pixels, patches, etc. It is thus helpful to compress the distribution by density estimation into more compact forms ( e.g. , mixtures of Gaussians), on which the query is based. In particular, the advantage is that density estimation can be computed offline before the query takes place, thus offering computational and storage savings.
 Method Greenspan et al. (2002) used GMM for den-sity estimation; we propose KMM as an alternative. After density estimation, the dissimilarity between two distribu-tions needs to be measured and the Earth Mover X  X  Distance (EMD) is a state-of-the-art measure. Given two distribu-tions represented by sets of weighted prototypes, EMD re-gards one collection as mass of earth spread in the fea-ture space, while the other is a collection of holes. The EMD is defined as the least amount of work needed to fill the holes with earth. A unit of work corresponds to the ground distance between two prototypes. If we rep-resent the distributions by mixtures of Gaussians, then a sensible ground distance D ( p i , p 0 j ) between two Gaussians p tance used in (Greenspan et al., 2002), D Based on D ( p i , p 0 j ) , if p = P i  X  i p i where p i and  X  i is its weight, and similarly p 0 = P j  X  0 j p 0 j EMD between p and p 0 is where  X  ij  X  0 is the flow between p i and q j . Feasibility means P i  X  ij  X   X  0 j and P j  X  ij  X   X  i for all i and j . Settings In this experiment, the distance measure is fixed to EMD. We plug the densities estimated by GMM and KMM into EMD 5 , and compare the retrieval results. Para-meters for KMM and GMM were chosen in the same way as in Section 7.4. Here KMM used POL3. For each image, we sampled 10 3 pixels and each pixel X  X  feature vector was the CIE-Lab value of a 5  X  5 window centered on it. Results We collected L = 10537 images from various sources including FIRE and CIRES 6 . The dataset included 10 labeled categories like horse, beach, and each cate-gory has 100 images. For each image I c ( i ) from class c ( c  X  { 1 , ..., 10 } , i  X  { 1 , ..., 100 } ), we retrieved r ( r  X  { 1 , ..., L } ) closest images (in terms of EMD) from the whole database and counted how many among them are also from class c , denoted as g c ( i, r ) for GMM and k for KMM. For each c and r , we performed a paired sign test between { g c ( i, r ) } 100 i =1 and { k c ( i, r ) } 100 always in (0,1], we report in Figure 3 the log p -value if the erwise, we plot the negative log p -value. Negative values are in favor of KMM. In Figure 3, performance of KMM is superior to or competitive with GMM in 8 categories and for most values of r (number of retrieved images). It is important to note that the Fr  X  echet distance is not in the class of functions used in KMM, and KMM still performs reasonably well. In the next section, we learn an image classifier using the same kernel as used in KMM. A closely related but different task is learning to cate-gorize images using multi-class classification, particularly by SVM. Here all we need is a kernel between pairs of image densities p and q , which is readily given by  X   X  [ p ] ,  X  [ q ]  X  H . The SVM classifier takes the form f ( p P i  X  i  X   X  [ p i ] ,  X  [ p j ]  X  = E x  X  p j [ P i  X  i  X  [ p  X  i  X  R . Since P i  X  i  X  [ p i ]  X  H , KMM ensures that p j estimated such that this expectation is well approximated. Our 10-class classification used 1000 images from the 10 categories. We randomly divided each category into 70 im-ages for training and 30 images for testing. We used Lib-SVM to train a multi-class SVM with one-against-one cri-terion on the combined 700 training images. The loss and regularization tradeoff parameter was determined by an in-ner loop 10-fold cross validation on the training data. Fi-nally we test the accuracy of the learned model on the 300 test images. The whole process is repeated for 1500 times. We use POL3 for both KMM and SVM, because for both GMM and KMM, POL3 significantly outperforms POL2 and RBF in practice 7 . By using paired sign test, KMM yields lower error rate than GMM at significance level 0 . 01 . Figure 4 shows the scatter plot of the resulting error rates. Acknowledgements NICTA is funded by the Australian
