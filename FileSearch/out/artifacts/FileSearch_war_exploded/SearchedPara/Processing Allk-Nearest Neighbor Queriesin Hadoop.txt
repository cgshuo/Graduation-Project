 An all k -nearest neighbor query (an A k NN query )isavariationofa k -nearest neighbor query and determines the k -NNs for each point in the given dataset in one query process. Although efficient algorithms for A k NN queries are available for centralized databases [2,4,11], we n eed to consider to supp ort distributed en-vironments where the target data is managed in multiple servers in a distributed way. Especially, MapReduce , which is a fundamental framework for processing large-scaled data in distributed and parallel environments, is promising for en-abling scalable data processing. In our work, we focus on the use of Hadoop [5] since it is quite popular software for MapReduce-based data processing.
In this paper, we propose a method for efficiently processing A k NN queries in Hadoop. The basic idea is to decompose the target space into smaller cells. At the first phase, we scan the entire dataset and get the summary of the point distribution. According to the information, we determine an appropriate cell decomposition. Then we determine k -NN objects for each data points by con-sidering the maximal range in which possible k -NN objects are located. In this work, we assume the use of the distributed and parallel computing frame-work Hadoop [5,9]. MapReduce [3] is the foundation of Hadoop data processing. Due to the page length limitation, we omit these details. If you are interested in the framework, please refer to textbooks like [9].

An A k NN query is regarded as a kind of a self-join query. Join processing in the MapReduce framework has been st udied intensively recent years [1,6], but generally speaking, MapReduce only supports equi-joins; development of query processing methods for non-equi joins is one of the interesting topics on the MapReduce technology [8]. For A k NN queries, there are proposals that use R-trees and space-filling curves [2,4,11], but they are limited for the use in a centralized environment. For processing A k NNqueriesinHadoopinanefficient manner, we need to develop a query processing method that effectively uses the MapReduce framework. 3.1 Basic Idea We now describe the basics of our A k NN query processing method. We consider two dimensional points with x and y axes. Basically, we decompose the target space into 2 n  X  2 n small cells. The constant n determines the granularity of the decomposition. Since the k -nearest neighbor points for a data point is usually located in the nearby area of the point, we can expect that most of the k -NN objects are found in the nearby cells. A simple idea is to classify data points into the corresponding cells and compute candidate k -NN points for each point. The process can be parallelized easily and is suited to the MapReduce framework.
However, we may not be able to determine k -NN points at one step; we need to perform an additional step for such a case. Data points in other nearby cells may belong to the k -nearest neighbors. To illustrate this problem, consider Fig. 1, whereweareprocessinganA k NN query for k = 2. We can find 2-NN points for A by only investigating the inside of cell 1 since the circle centered at A and tightly covers 2-NN objects (we call such a circle a boundary circle )doesnot overlap the boundary of cell 0 . In contrast, the boundary circle for B overlaps with cells 1 , 2 ,and 3 . In this case, there is a possibility that we can find 2-NN objects in the three cells. Therefore, an additional investigation is necessary.
The idea is simple but there is a problem; we may not be able to draw the boundary circle for a point. Consider point C in Fig. 1. For this point, there is only one (less than k ) point in cell 1 . Thus, we cannot draw the boundary circle. We solve the problem in the following subsection. 3.2 Merging Cells Using Data Distribution Information We solve the problem described above by prohibiting the situation that there are not enough points in each cell. We first check the number of points within each cell. If we find a cell with less number of points, we merge the cell with the neighboring cells to assure that the number of points in the merged cell is greater than or equal to k . After that, we can draw the boundary circle.
The outline of the idea is illustrated in Fig. 2, where 4  X  4 decomposition is performed. At the first step, we count the number of points in each cell. Then, we merge the cells with less number of objects with the neighboring cells. In our method, we employ the hierarchical space decomposition used in quadtrees [7]. When we perform merging, we merge four neighboring cells which correspond to the same node at the parent level.
The problem of this approach is that we need to perform an additional count-ing phase before the nearest neighbor computation. However, it can simplify the following steps. The distribution information is useful in other ways. If we can know there is no points in a cell beforeh and, we do not need to consider the cell in the following processes. As shown in the experiments, the cost of the counting is relatively small compared to the total cost. The query process consists of four MapReduce steps: 1. MapReduce1: Data distribution information is obtained and cell merging is 2. MapReduce2: We collect input records for each cell and then compute can-3. MapReduce3: It updates k -NN points for each point. We use the idea de-4. MapReduce4: Integrating multiple k -NN lists for each point and get the Due to the page length limitation, we only show examples. Please refer to [10] for the detail of the algorithms,
We give an example of MapReduce1 to 3 steps that finds A k NN points. Fig-ure 3 shows the distribution of points and we focus on points A , B ,and C .The three points are the representative example patterns:  X  A (the bounding circle does not overlap with other cells): k -NN points are  X  B (the bounding circle overlaps with one cell): We can determine k -NN points  X  C (the bounding circle overlaps with multiple cells): We investigate additional
Figure 5 illustrates the execution steps of the entire MapReduce steps. We can see that the k -NN lists for points A , B ,and C are incrementally updated and finally fixed. We have implemented the proposed method. In this section, we evaluate the performance of the MapReduce program running in a Hadoop environment. 5.1 Datasets and Experimental Environment The experiments are performed using t wo synthetic datasets: the datasets 1M and 10M consist of 1,000,000 and 10,000,000 points in the target space, respectively. Their file sizes are 34MB and 350MB 1 .
 We use three nodes of Linux 3.0.0-14-server (Ubuntu 11.10) with Intel Xeon CPU (E5620 @ 2.40GHz). Since each CPU has 4  X  2cores,wehave24cores in total. The system has 500GB storage and the servers are connected by 1G bit Ethernet. We run Hadoop version 0.20.203.0 in the system. The number of replicas is set to 1 since we do not care failures in this experiment. The number of max number of Map tasks and Reduce tasks are set to 8. 5.2 Summary of Experiments As an example, Fig. 4 shows an experimental result for Experiment 2, where the granularity parameter is set to n = 8, the number of Reduce tasks is 24, and the dataset is 10M . As the figure illustrates, the processing time increases as the k value increases. Especially, the in creases of the cost for MapReduce3 and MapReduce4 are large. The reason is that a large k value results in a large size of each intermediate record, and it re sults in the increase of data processing time. In addition to that, since the radius of a boundary circle becomes large, we need to investigate more data points in MapReduce3 and MapReduce4 steps. We summarize the experimental results . Please refer to [10] for the details. Based on the experiments, we observed that the proposed A k NN query process-ing method can reduce the processing time by parallel processing, especially for a large dataset (Experiment 1). In addition, it was observed that the increase of the k value results in the total pro cessing cost (Experiment 2).

In our method, we incorporated a preparation step to obtain the overall distribution of the points in the target space. As shown in the experimental re-sults, this process, including the cell mer ging cost, is quite efficient compared to other processes of the algorithm. The observation is more clear especially when the dataset size is large. The preproce ssing can simplify the algorithm because the strategy based on a boundary circle becomes simpler. Thus, the benefit of the first phase is larger than the cost of the process. In this paper, we have proposed an A k NN query processing method in the MapReduce framework. By using cell decomposition, the method adapts the dis-tributed and parallel query framework of MapReduce. Since k -NN points may be located outside of the target cell, we may need additional steps. We solved the problem by considering a boundary circle for the target point. In addition, to simplify the algorithm, we proposed to collect distribution statistics beforehand and the statistics is used for cell merging. In the experiments, we have investi-gated the behaviors of the algorithm for different parallelization parameters and different k values.

The future work includes how to estimate the appropriate number of cell decomposition granularity ( n ) by using statistics of the underlying data. In ad-dition, the number of parallel processes is also an important tuning factor. As shown in the experiments, too many parallel processes may result in the increase of the total processing cost due to the overhead. If we can estimate these pa-rameters accurately and adaptively, we would be able to achieve nearly optimal processing for any environments.
 Acknowledgments. This research is partly supported by the Grant-in-Aid for Scientific Research (22300034) and DIAS (Data Integration &amp; Analysis System) Program, Japan.

