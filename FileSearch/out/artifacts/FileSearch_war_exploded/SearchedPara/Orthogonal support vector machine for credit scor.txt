 1. Introduction
Credit risk based on the characteristics of the debtor is often divided into sovereign, corporate, retail, etc. Retail debt is cen-tered on customer credit, which includes short-term and intermediate-term credit to finance the purchase of commodities and services for consumption or to refinance debt incurred for such purposes. Retail credit is characterized by three points: first, large amounts with small scale. At present in China, retail loans can account for a quarter of the total debt, with a speed of growth approaching 10%; second, the potential risk is high but the information is scattered and complicated. In the loan application form there are thousands of variables to describe and, even worse, is that different organizations always use different variables; and third, the efficiency of business processing requires highly devel-oped decision-making techniques as competition is getting more and more intense. These characteristics determine the banks need to implement risk management evaluation methods based on quantitative analysis. A good credit risk evaluation tool can help to grant credit to more creditworthy applicants and thus increases profit. Moreover, it can deny credit for the noncredit worthy applicants and thus decreases losses.

Currently, credit scoring has become the primary method to develop a credit risk assessment tool. It is a method to evaluate the credit risk of loan applicants with their corresponding credit score that is obtained from a credit scoring model ( Altman, 1998 ).
A credit score is a number that can represent the creditworthiness of an applicant and it is based on the analysis of an applicant X  X  characteristics from the application file using the credit scoring model. The credit scoring model ( Thomas et al., 2002 ) is devel-oped on the basis of historical data about the applicant X  X  perfor-mance on previously made loans with the use of some quantitative techniques, such as statistics analysis, mathematical programming, artificial intelligence and data mining. A well-designed model should have higher classification accuracy to classify the new applicants or existing customers as good or bad and the model is the core of credit scoring.

The most popular methods adopted in credit scoring are statistical methods. The statistical principle discriminating differ-ent groups in a population can be traced back to 1936 in Fisher (1936 ) publication which used a linear model to calculate the distance between two classes as the decision factor. It is known as the Fisher X  X  discrimination model. In 1977, Martin (1977 ) first introduced the logistic regression method to the bank crisis early warning classification. Martin chose to use data between 1970 and 1976, with 105 bankrupt companies and 2058 non-bankrupt companies in the matching sample, and analyzed the bankruptcy probability interval distribution, with two types of errors and the relationship between the split points, he then found that size, capital structure, and performance were key indexes for the judgment. Martin determined that the accuracy rate of the overall classification could reach 96.12%. Logistic regression analysis had significant improvements over discriminant analysis with respect to the problem of classification. Martin also noted that logistic regression could overcome many of the issues with discriminant analysis, including the assumption that variables must be nor-mally distributed. Wiginton (1980 ), was one of the first research-ers to report credit scoring results with the logistic regression model. Although the result was not very impressive, the model was simple and could be illustrated easily. Then, at that point the logistic regression model had become the main approach for the practical credit scoring application. In 1997, Hand and Henley (1997 ) summarized statistical methods in credit scoring. These methods are relatively easy to implement and are able to generate straightforward results that can be readily interpreted. None-theless, as commonly known, there are also quite a few limita-tions associated with the applications of these statistical methods.
First of all, they have the fatal problem called  X  X urse of dimension X  which suggests that if there are numerous variables to apply, because of multicollinearity between variables, the results are always erroneous and misleading. Therefore, before applying statistical methods, the process entailed tremendous data pre-processing efforts through variable selection. This strategy usually requires domain expert knowledge and an in-depth understand-ing of the data. In addition, all the statistical models are based on a hypothesis condition. In a real world application, a hypothesis such as that the dependent variable should follow logic normal distribution and so on, may not hold. Most importantly, based on these algorithms, these statistical models have difficulty in the automation of modeling processes and lack robustness. When environmental or population changes occur, the static models usually fail to adapt and need to be rebuilt again.
 loans applications, researchers discovered the application of the support vector machine (SVM). The support vector machines (SVM) approach was first proposed by Cortes and Vapnik (1995 ). The main idea of SVM is to minimize the upper bound of the generalization error. SVM usually maps the input variables into a high-dimensional feature space through some nonlinear mapping. In that space, an optimal separating hyper plane, which is one that separates the data with the maximal margin, is constructed by solving a constrained quadratic optimization problem. Suykens et al. (2002) constructed the least squares support vector machine(LS-SVM) and used it for the credit rating of banks and reported the experimental results compared with ordinary least squares (OLS), ordinary logistic regression (OLR) and the multilayer perceptron (MLP). The result showed that the accuracy of the LS-SVM classifier was better than the other three methods. Schebesch and Stecking (2005) used a type of standard
SVM proposed by Vanik with a linear and radial basis function (RBF) kernel for dividing credit applicants into subsets of  X  X ypical X  and  X  X ritical X  patterns which can be used for rejecting applicants.
Schebesch and Stecking concluded these types of SVM should be widely used because of their performance. Gestel et al. (2003) discussed a benchmark study of seventeen different classification techniques on eight different real-life credit datasets. They used
SVM and LS-SVM with linear and RBF kernels and adopted a grid search mechanism to tune the hyper parameters in their study.
The experimental results indicated that six different methods were the best in terms of classification accuracy among the eight datasets  X  linear regression, logistic regression, linear program-ming, classification tree, neural networks and SVM. In addition, the experiments showed that the SVM classifiers can overall yield the best performance. Yang (2007 ) experimented with several kernel learning methods to apply adaptive credit scoring, and found that the results can be very impressive when using the
SVM. Nevertheless the existing research findings have all focused on batch learning and the selection of parameters, as seen in the work of Yu et al. (2006,2008 ) which shows SVM X  X  advantages in solving high dimensional problems. However, there are two obvious drawbacks to SVM ( Min and Lee, 2005 ). One is that when the variables are not  X  X eaningful X  and  X  X uge X , SVM requires a long time to train and the hyper plane is not accurate, which we also define as curse of dimension. The drawback is a fatal flaw, although this method has good robustness and can always achieve higher accuracy, when applied to samples, SVM lacks the capability to explain its results. That is, the results obtained from SVM classifiers are not intuitive to humans and are difficult to illustrate comparing with logistic regression. This is a common problem that all machine learning methods are facing. Though the results with these methods have strong advantages in accuracy, the non-parameter results often lack of statistical theory, and so which cannot be directly corresponding to the realistic economic significance. Just as in regression analysis, regression coefficient directly represents the influence of independent variable acting on dependent variable, but in support vector machine (SVM), the relationship between independent variable and dependent vari-able cannot be explained directly. So this limits these methods in practical application, and at the same time this also is a cause for over fitting phenomenon.

Dimension curse ( Anderson, 1962 ) can be defined as this phenomenon: as the number of variables increase, more and more variables will have multicollinearity, which can be described as when the correlation coefficient gets large, and is in a high dimensional space, the distribution of the sample points will become sparse. Statistical methods will prove to be erroneous with multicollinearity, and SVM will need a large amount of support vectors to construct hyper plane. Now, to solve the curse of dimensionality, researchers often use two methods to reduce variables. One method is feature selection, another is feature extraction. Feature selection is to select important variables closely related with the target in order to reduce the model X  X  dimensions; feature extraction is to construct new variables which are not linearly dependent through structure transforma-tion. The drawback of feature selection is in reducing information and the advantage is that it is easy to explain. Feature extraction is just the opposite. Many scholars have performed a lot of work to reduce dimensions. Sugiyama (2007 ) tried feature selection to reduce dimensions in Fisher discriminant analysis. Bellman (1961 ) is the first to note the curse of dimension in kernel classifiers. He stipulated that owing to the large amounts of data from public financial statements that can be used for bankruptcy predictions, the large scale of input data makes Kernel classifiers infeasible due to the curse of dimensionality. Consequently, one needs to transform the input data space to a suitable low dimensional subspace that optimally represents the data struc-ture. In the studies of Huang (2009 ), he discussed the use of a nonlinear graph as a type of method for feature selection to reduce dimension. Han and Han (2010 ) have tried logistic regression to select meaningful variables for neural networks. The other methods regarding dimensionality reduction, linear algorithms such as principal component analysis and linear discriminant analysis, are the two most widely used methods, which can be found in the works of Gutierrez et al. (2010) and Hua et al. (2007) .

Just based on the studies above, we want to improve the accuracy of credit scoring through dimension reduction. Our novel contribution is that we give these researchers in the field of application using logistic regression and support vector machine a new way to address dimension curse that we defined as  X  X rthogonal dimension reduction X  (ORD). Based on the experi-ence of statistics, we compare the traditional way to address dimension curse  X  hybridizing with logistic regression (HLR) ( Fukunaga, 1990 ) on behalf of feature selection and principal component analysis (PCA) ( Jolliffe, 2002 ) on behalf of feature extraction. Then, we fit these helpful features chosen by ORD, HLR and PCA in logistic regression and support vector machine to evaluate the accuracy of credit scoring. Furthermore, we compare these results with the original methods without reducing dimen-sion. Finally, we acquire cross-validation to make the results sensitive.

The structure of the rest in this paper is as follows: the next section puts forward the prior research of logistic regression and support vector machine. Section 3 briefly summarizes the pre-vious methods of reducing dimension. Section 4 describes our method of orthogonal dimension reduction and its main princi-ples in detail. Section 5 is about experiment design, including data and variable description, data pre-processing, evolutionary learn-ing for SVM, features selection with HLR, features extraction with PCA and ODR, cross-validation design, and accuracy criterion. Experimental studies using the original methods and the methods hybridizing dimension reduction are presented in Section 6 . The Final section discusses the interesting results and gives some remarks. 2. Prior research
Let X  X  x 1 , x 2 , x n  X  X  T be a set of n random variables which describe the information from a customer X  X  application form and credit reference bureau. The actual value of the variables for a particular applicant i is denoted by X i  X  x 1 i , x 2 i , x samples, X i is the attribute vector of the ith customer, and y corresponding observed result of timely repayment. If the custo-mer is good, y i  X  1, else y i  X  1. Let I  X  { i 9 y i  X  1, i behalf of good customers, J  X  { i 9 y i  X  1, i A N ,( x i of bad ones.

Though in practice a credit scoring result needs the score of each applicant, in fact our greatest concern is the accuracy of the distinction between categories. Thus, the credit scoring problem can be described simply as making a classification of good or bad for a certain customer using the attribute characteristics of a certain customer. That is, using the attribute vector X k judge the credit status. The typical credit risk modeling techni-ques, which were tested in this paper, are briefly described below. 2.1. Logistic regression
Just as linear regression, logistic regression assumes that the sum of the weighted input variables is linearly correlated to the natural log of the odds that the outcome event will happen. It can be described as (1) : log p = 1 p  X  b 1 x 1  X  b 2 x 2  X  ...  X  b k x k  X  e  X  b T model, the maximum likelihood method can be applied to compute the estimate of b i i  X  1 , 2 k . We refer to p /(1 p ) as odds-ratio and assume the regression model in (1) is obtained, the estimated probability of no default is as follows: p  X  e b
Linear regression is based on the idea of using vector X to explain y logistic regression is the same, using X to explain natural log of the odds, so just like linear regression, it has good interpretations in statistical sense. But logistic regression can overcome the flaw of linear regression, which is that the right side of the model could take any value from N to  X  N but the left side can only take values between 0 and 1.

The method has two shortcomings: one is that it can only explain the intrinsically linear relationship, and cannot address non-linear effects in practice. Researchers always explain any non-linear effects with variable combinations and this requires several repetitions of a trial-and-error process. In addition, the method is sensitive to redundancy or collinearity in the input variables to guarantee the basic assumption of e , which is e
NID 0 , s 2 , which therefore requires that y obey logic normal distribution. If this condition is not satisfied, this method will give erroneous estimates of the coefficients and is not valid for statistical interpretation. 2.2. Support vector machine
The main idea of support vector machine is to minimize the upper bound of the generalization error not the empirical error.
Without loss of generality, in a two-dimensional space, if these scoring samples are linear separable, The upper bound can be constructed by ( wx )  X  b  X  1 and ( wx )  X  b  X  1, so a decision func-tion can be created to specify whether a given application belongs
While the vector w defines the boundary, in order to get the two upper bound separated as far as possible, the optimal hyperplane can be obtained as a solution to the optimization problem: s : t : y w
U x i  X  X  X  b  X  Z 1 i  X  1 , 2 , n which could be written as (4) : 2 J w J s : t : y w
U x i  X  X  X  b  X  Z 1 i  X  1 , 2 , n
So an optimal separating hyperplane which is one that separates the data with the maximal margin is constructed by solving a constrained quadratic optimization problem whose solution has an expansion in terms of a subset of training patterns that lie closet to the boundary, and this subset of patterns are called as support vector (SV). Fig. 1 shows such a hyperplane that separate two classes to the boundary.

In many practical situation, the training samples cannot be linear separable. There is a need to use soft margin and C penalty parameters, this is formulized as the following constraint opti-mization problem: min Jw , b , x k  X  X   X  1 2 J w J 2  X  C s : t : y w
U x i  X  X  X  b  X  Z 1 i  X  1 , 2 , ... n x k Z 0 where C is the corresponding penalty parameters indicating a tradeoff between large margin and a small number of margin failures: few errors are permitted for high C , while low C allows a higher proportion of errors in the solution. The solution to this optimization problem can be given by the saddle point of the
Lagrange function with Lagrange multipliers a i , and then the problem can be transformed into its dual form: max J a  X  X  X  1 2 P k P l a k a l y k y l / x k U x l S  X  P s : t : able to separate the two classes accurately, a hyperplane is created that allows linear separation in the higher dimension by the use of transformation function j ( U ) which can map the input space into a higher dimensional feature space ( z -space), then the objective function can be rewritten as: max J a  X  X   X  1 2 tation-intensive. And we can find j ( U ) only uses for inner product, therefore a kernel can be used to perform this transformation and then inner product can be replaced by kernel function which is given by Mercer X  X  theorem. The kernel function is defined as (8) : Kx , y  X  X  X  j  X  x  X  j  X  y  X  X  8  X  (1) Linear kernel function: Kx k , x  X  X  X  x T k x (2) Polynomial function: Kx k , x  X  X  X  x T k x  X  1 d (3) Gaussian function: Kx k , x  X  X  X  exp J x k x J 2 = s 2 (4) Radial basis kernel: Kx k , x  X  X  X  exp J x k x J 2 = 2 s max J a  X  X  X  1 2 P k P l a k a l y k y l Kx k , x l  X  X  X  P k s : t : neural networks are similar; this can be illustrated in the following
Fig. 2 . SVM can be seen as a type of networks which uses the kernel function as an activation function, and the optimization program as a threshold function. So to control the generalization capability of SVM, there are a few parameters such as C and kernel parameters that need to be trained. But there is not a most effective method up to now, a number of approaches have been presented including Genetic algo-rithms ( Jack and Nandi, 2002 ), Artificial Immunization Algorithm ( Yuan and Chu, 2007 ), Particle Swarm Optimization ( Samanta and Nataraj, 2009 ) and so on. In our work, the parameters choice is not the focus of our research, we give a general method in Section 4 . 3. Reducing dimension 3.1. Feature selection
The task of feature selection is to examine characteristics which are contained in the input variables, and then delete those that are irrelevant to the target variables. There are many statistical methods such as variance analysis (ANOVA) and correlation analysis. However, they are all based on the conditions of the experimental data, for credit scoring, the most widely used method is hybridizing with logistic regression to do feature selection.

Hybridizing with logistic regression is based on the statistics X  meaning. In these regression models, through the variance analysis, we can find the variable which can give the largest contribution to the variation of target variables, and we think these variables have the closest relationship to the target variable. As (1) , ^ y i is the regression result, y is the mean of observation, y an observation. The total fluctuations of data can be described as S  X  S y i y 2 , S R  X  S ^ y i y 2 measures explanatory power of E ( y ) decided by X , and S e  X  S y i ^ y i 2 measures the difference between result of regression and observation. So S T  X  S R  X  S e and it can be proved S e = s 2 w 2 n 2  X  X  ;if E ( y ) decided by X is true, S and S e is independent with S R . Therefore, through ANOVA we can get the useful variable. 3.2. Feature extraction
The most widely used method for feature extraction is princi-pal component analysis. The principle of component analysis is to keep as much information as possible of the original variables, and to achieve dimension reduction, through the use of compre-hensive new variables.
 Let u  X  E ( X ), S  X  cov  X  X  X  X  Exx 0  X  X  is the covariance matrix. To Apply linear transformation with X , we can construct the new variables Z as follow: ...... 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :
PCA tries to sequentially find the projection u 1 , u 2 , (where J u i J  X  1) such that the variance of the projected data z ( i  X  1,2, y , n ) is maximized: var z i  X  X  X  cov u 0 i X  X  u 0 i S u i  X  11  X 
If S is estimated by its MLE, which is the sample covariance matrix S defined as (12) : S  X  1 N
Let l 1 Z l 2 Z Z l n is eigenvalue of S and r 1 , r 2 , r corresponding eigenvectors, based on spectrum decomposition, the ith principal component can be written as (13) , with var z i  X  X  X  r 0 i S r i  X  l i and cov z i , z j  X  r 0 i S r z  X  r 1 i x 1  X  r 2 i x 2  X  X  r ni x n i  X  1 , 2 , ... , n  X  X   X  13  X  4. Orthogonal dimension reduction In this section we give a new method to do feature extraction. The step of transform is described as (14) : z  X  x 1 z z z  X  x s Theorem 1. : Any group of vectors x 1 , , x s  X  X  can transform into orthogonal vectors z 1 , , z s  X  X  by the process as (14) . The process in two dimensions space can be explained as Fig. 3 . The proof can be found in the Ref. Jain and Gunawardena (2003 ).
From Theorem 1 , we know that for any group of variables with the rank s r n to apply the orthogonal transform, and we get z , z 2 , , z n . Among them, there must be s variables orthogonal with each other, and the other ( p  X  s ) variables are zero vector. Therefore, through this transform, it normally reduces the dimen-sions of the original high dimensional system.
 Theorem 2. : If the variables x 1 , x 2 , , x n are standardized, after orthogonal transform, the variance z 1 , z 2 , , z s can get is Var z k  X  X  X  Var x k  X  X  S k 1 j  X  1 r 2 z j , x k Proof. _ var x k  X  X  X  1  X 
J x k J 2  X  n _ x k  X  z k  X  / z k U z j S  X  0  X  var z k  X  X  X  var x k  X  X 
Information in data collection can be measured by the total variance of variables. From Theorem 2 , we know that the reduc-tions of these variables are redundancy information that can be measured by the correlation coefficient.

In short, through the orthogonal transform process we can reduce dimensions to realize feature extraction from two aspects: firstly, through the transform, one will find the orthogonal basis of a high dimensional space, which will help find whether a variable is a linear into 0 vectors. Thus, it is helpful to reduce the number of variables; furthermore, to those which are not 0 vectors, because of the subtraction of the correlation coefficient, the new feature can obtain thecoreoriginalvariables.

The step to apply orthogonal dimension reduction is summar-ized below: Step 1. Standardize the group of x 1 , x 2 , , x n
Step 2. Choose z 1 , which has the maximum correlation coefficient squares with the other variables, might as well written as z 1  X  x 1 find the one has the largest variance var z 2  X  X  X  max var  X  z j  X  2 , , n as z 2 , written as z 2  X  z 1 2
Step 4. Let z 2 j  X  x j  X  X  x 0 j z 1  X  =  X  z 0 1 z 1  X  X  z 3 , 4 , ... , m , choose z 3 with the with the vectors which are not 0 vectors and var z 3  X  X  X  maxvar  X  z 2 j  X  ,  X  2 , , m
Step 5. Repeat the step until TNP  X  X  S s i  X  1 Var  X  z Var  X  z j  X  X  Z d , d is settled with situations.

Step 6. End. The first s vectors are the features extracted from orthogonal dimension reduction (ODR). 5. Experiments design
This section is structured in six subsections. Firstly, we have a brief description of the dataset used in the experiments. And then we discuss the process of data pre-processing for modeling. In the third part, we discuss the methods for training parameters in SVM. The methods reducing dimension which are introduced in
Sections 3 and 4 will be tested in the next subsections. Then, to test the robustness of the models, we design two cross-validation methodologies that are illustrated in the fifth subsection. The final subsection defines evaluation criteria. 5.1. Dataset description
The credit dataset used in these experiments is German credit dataset, which is provided by Professor Dr. Hans Hofmann of the University of Hamburg and is obtained from UCI Machine Learning
Repository ( http://www.ics.uci.edu/ mlearn/databases/statlog/ german/ ). The total number of instances is 1000 including 700 creditworthy cases and 300 default cases. There is no missing data.
For each applicant, 20 kinds of attri bute are available; the variable names of these attributes used in the models are listed below with short names in brackets. There are 13 categorical attributes including status of existing checking account (checking), credit history (his-tory), purpose for the credit (purpose), saving account (savings), present employment since (emplo yed), personal status and sex (marital), other debtors/guarant ors (coapp), property style (prop-status (job), telephone status (te lephone), foreign worker or not (foreign) and 7 numerical attrib utes including duration in month (duration), credit amount (amount) , installment rate in percentage of disposable income (installp), present residence since (resident), age in years (age), number of existing credits at this bank (existcr), number of people being liable to provide maintenance for (depends).
Tables 1 and 2 below show the basic statistics and information of these attributes. Table 3 shows correlation matrix.

From these statistics of these attributes, we can see some attributes have relatively concentrated distribution, for example foreignandcoapp,themodesgetmorethan90%.Withthe numerical attributes, the variable amount is more  X  X ig X  than others in the amount level. And because of co ncentrated level of categorical they always have relevance with others which can be found in
Table 3 . So there may need some pre-processing before modeling. 5.2. Data pre-processing them with their levels as dummy variables. Then, it will reduce relevance.Inourmodelsweusethesamemethod,soallofthe categorical attributes are used with their levels. For example, foreign has two levels, so there will be two variables in the model  X  one is foreigh_0, and another is foreign_1. Other variables are the same.
However, it can be seen that the way t o address categorical attributes will cause the number of total variables to increase significantly. normalization with the logistic regression model ( Liu et al., 2012 ).
Though the variable coefficient estimates b i vary because of the unit, the correlation coefficient estimate R 2 and the model X  X  results are not influenced, therefore it has no effect on the choice of variables. Because there is no proof as to the necessity of normal-ization with SVM, we cannot give a logistic reason for normal-ization, but in practice, we always normalize the numerical attributes in the models so that the results cannot be affected by the unit. To better demonstrate we also try the experiments with dimensionless numerical attributes and the ones which are not normalized, respectively, to compare in the models of SVM. The results will be illustrated in the fifth section. The method we use for normalization is ( x i mean )/ std , this method can transform any distribution of the variable into a standard normal distribution, which well achieves the variable distribution that the statistical models require. Fig. 4 shows the distributions of two main numerical variables in the data set: amount and age. It can be found that they are very different from normal distribution. 5.3. Select parameters for SVM powerful learning method for classification problems, its performance is sensitive not only to the algorithm that solves the quadratic programming problem, but also to the parameters set in the SVM. In the process of using SVM, the first issue is how to discover the best parameter of SVM for a specified problem.
An easy and reliable approach is to determine a parameter range, and then make an exhaustive gri d search over the parameter space to find the best setting ( Rojas and Nandi 2006 ). In the grid search method, each point in the grid is defined by the grid range [(C_min, sigma_min), (C_max, sigma_max)] and a unit grid size (_C, _sigma) is evaluated by the objective function F. The point with the smallest F value corresponds to the optima l parameters. In the experiments, we used 750 samples and gird search to select t C and kernel parameters of SVM. To address the nonlinear effect, we choose a linear kernel function to compare with the logistic regression. 5.4. Reduce dimension
These categorical attributes in the previous section are all con-tained in dummy variables which avoid multi-linearity. However, the number of variables will be increasing. To reduce these types of variables, they must fit the feature selection. We apply logistic regression to this problem and set 15% as the significance level. In this paper, we refer to this method a s HLG for short. Because logistic regression can give the model more meaningful interpretation for three reasons. First, logistic reg ression selects the most relevant variables to the target variables int o the model. Second, in statistics logistic regression can use the W ald test to decide whether adding variables improves the unconstrained model. Third, by fitting variable selection with logistic regression we can exclude the multiple correlations among variables. Then, we follow these steps: firstly, we use all the dummy variables as i nputs, which have been prepared with the methods introduced in Section 5.2 . And secondly we address the variable selection process in logistic regression. The steps of selecting variables using the forward logistic regression are summar-ized in Table 4 .

For these numerical attributes, after standardization, we experiment with PCA and ORD to compare.
 The process of PCA is introduced in Section 3.2 . The results of PCA are shown in Tables 5 and 6 .

From Table 5 , the first three components have contained 60% variance. In order to make comparable with ODR, we can reduce seven dimensions to three what can be obtained from the eigenvec-tors of correlation matrix. For example, z 1  X  0.66 du  X  0.72 am 0.22 in  X  0.06 re  X  0.01 age  X  0.02 ex  X  0.03 de .

For ODR, the correlation matrix of numerical attributes is shown in Table 7 below. Max S 7 i  X  1 r 2 x j , x i  X  S 7 choose z 1  X  x 2 , and then apply ODR with the other six variables, the variables have realized orthogonalization.

Calculate net information percentage NP  X  S i Var ( z i )/ S total net information percentage TNP  X  S s i  X  1 Var z s  X  X  = S the results listed in Table 9 and the original variables are labeled in brackets. Fig. 5 shows the accumulative contribution of z has 66% variance which is smaller than the sum of our z i means by ODR we can only choose the orthogonal variables z , z 2 , z 3 instead of the seven numerical attributes. 5.5. Cross-validation
As the best model is tailored to fit one sub-sample, the model often estimates the true error rate too optimistically. Therefore, to get a true estimate of the error rate, we applied two types of cross-validation methodologies which were suggested by Zhang et al. (1999) . First, as these typically did, the cross-validation methodology is employed to test the effect of sampling variation on the model performance. To test the robustness of the models, one should apply a simple validation technique, by dividing the data set into a training sample and a validating sample, with a small scale which evaluates the predictive effectiveness of the fitted model. Second, to study the overall predictive capability of the classification models for unknown populations, one should use the whole data set as a large test set, if the data set for unknown population is not available.
 divide the data sample into four mutually exclusive equal sub-samples. Each sub-sample has the same rate for the bad custo-mers and the good ones. We train the logistic regression and the
SVM with three sub-samples, and validate the models with the fourth remaining sub-sample. Therefore, out-of-train prediction of validation gives us a relative true classification rate of all the observations in the data set with its averages. Secondly, to test the overall predictive capability of the unknown population compre-hensively, we use the entire data sample, by using the entire data set as the test sample; we can reduce the sampling variation in the test design. Finally, we apply statistical tests to test these models for accuracy. We use a paired-t test to test the difference between the means of the original method and the method hybridizing reduction of dimension in experimental and illustrate the application in Section 6 . 5.6. The evaluation criteria
The problem of credit scoring mainly focuses on the accuracy of classification. As the criterion of accuracy, the goal is to judge the good ones from the bad. Let the number of creditworthy cases classified as good be GG and classified as bad be GB; denote the number of default cases classified as good with BG and as bad with BB. Then, the evaluation criteria measure the accuracy of the classification, which is defined as follows: Good credit accuracy GCA  X  X  X  GG GG  X  GB 100% Bad credit accuracy BCA  X  X  X  BB BG  X  BB 100% Overall accuracy OA  X  X  X  GG  X  BB GG  X  GB  X  BG  X  BB 100%  X  15  X 
Defined by these three indicators, one can see GCA is the specificity, which determines the ability to identify good clients; BCA is the sensitivity for the model that shows the ability to identify bad customers. At the same time, OA gives the total efficiency of the model and reflects prediction accuracy of the model and can compare with others.

In our study, Type I error occurs when a bad credit is classified as good credit, which equals 1-BCA. And Type II error occurs when a good credit is classified as a bad credit, which equals 1-GCA. For credit scoring, Type I error is more critical than Type II error. Note that all these measures with Type I error and Type II error are mostly obtained using a 0.5 probability threshold for the classifica-tion. However, the use of arbitrary cut-off probabilities makes the computed error rates difficult to interpret and the use of a relevant pay-off function and prior probabilities to determine the optional model could lead to some types of bias on the results. Bradley (1997 ) gave a way to judge the efficiency with ROC. The receiver operating characteristics (ROC) graph is useful for organizing classifiers and visualizing their performance. The ROC graph is a two-dimensional graph in which the BCA (true positives) rate is plotted on the Y axis and the 1-GCA (false positives) rate is plotted on the X axis. The ROC graph depicts relative trade-offs between benefits (true positives) and costs (false positives).
Most classifiers naturally yield an instance probability or score, a numeric value that represents the degree to which an instance is a member of a class. Such a ranking or scoring classifier can be used with a threshold to produce a discrete (binary) classifier. Each threshold value produces a different point in ROC space, and if we join all these points we obtain an ROC curve. Additionally, as the production process of ROC, it can be seen that the diagonal line y  X  x represents the strategy of randomly guessing a class. To compare classifiers we want to reduce ROC performance into a sin gle scalar value representing expected performance. A common method is to calculate the area under the ROC curve, abbreviated as AUC. Because the AUC is a portion of the area of the unit square, its value will always be between 0 and 1. However, because random guessing produces the diagonal line between (0, 0) and (1, 1), which has an area of 0.5, no realistic classifier should have an AUC less than 0.5. In Fawcett X  X  (2006 ) study, he compared popular machine learning algorithms using AUC and found that AUC is a highly effective way to measure the results of models, and it can exhibit several desirable properties compared to accuracy.
For example, AUC has increased sensitivity in analysis of variance tests, is independent to the decision threshold, and is invariant to a priori class probability distributions. Moreover, theAUCmeasureismoresensitivetotheerrorsonthepositive class, because it has important statistical meaning: it is equiva-lent to the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen nega-tive instance, considering also all possible thresholds. It is very meaningful because in the credit scoring problem, cost will increase if one judges a bad credit as a worthy one.

Because of these reasons, the AUC has been selected as the main evaluation criteria in this paper, which can be seen in the
ROC pictures, and the BCA and GCA are also be used for checking the accuracy of these models as a reference. 6. Experiments results
The experimental results presented in this paper are struc-tured in three subsections. The first subsection describes the performance difference of original methods between the one with attributes normalization and the one without. The next section shows performance of original methods and methods hybridizing with reducing dimension. Finally, the comparison of accuracy and
ROC is shown in details in the third subsection. The process of experiments is shown in Fig. 6 as below, data set is described by round said, variables are with elliptic sections, and the methods and models are with rectangular. 6.1. Performance difference with attributes normalization
In this section, we focus on the original methods to discuss the effect of attributes normalization. We use the test data set (1000 observations and 20 variables) for this experiment. The method for normalization is introduced in Section 5 .
 without normalization. From the results, we can see that the models with normalization have the same results as the models without normalization in LG. However, there is a very interesting thing with SVM. Without normalization the results using SVM are very bad and the main error occurs in the prediction of the good credit. We also perform some similar experiments with numerical variables reduced by PCA and ODR to test the results and these results are the same: SVM cannot do the right job without normalization in the good prediction. We propose that this phenomenon may be caused by dimension curse, because differ-ent dimension units can get different variance scales, so in the high space, the distribution of data will be sparse, and the plane will not be sensitive to some specific points. Until now, however, we have found no relevant information introduced, so we define it as  X  X imensional Interference X . Thus, in the following experiments, we use only the models with normalization. 6.2. Performance difference with reducing dimensions 4.D and just as the introduction of cross-validation design above in Section 5.5 , we have four times experiments with the train data set of 750 observations tagged as D1, and then apply four times validation with the validation data set of 250 observations tagged as D2 left by the train data set. Each of these data sets uses equal proportions of the bad ones. The entire data set is used as the test data set tagged as D3. The results are the average of these data sets for four experiments. The cross-validation results of these models with normalization are summarized in Table 10 . is that with these models, without reducing dimensions, we can see all of these models do not get satisfactory accuracy in the prediction for the bad ones, though OA is really high with the train data set. Second, the SVM model is a little better and with the expense of the good customer forecast. There is an over-fitting problem with logistic regression and SVM. Because logistic regression is the method based on the variance of variables, it lacks robustness, from Table 10 , we can find it is a very good model for the train data set, but it is not the same good to the validation data set and the test data set. Also there is the same problem with SVM. Furthermore, with reducing dimensions, it can be found that there are improvements with SVM in all the data sets. 6.3. Comparison of models
To conduct a comparison of these models, we design four groups for comparative test.

First, compare the performance between LG and SVM. We conduct experiments separately, and the results are shown in the following Tables 12 X 14 .

From these tables, we can find the performance of SVM is better than logistic regression, especially for these reducing dimension models. This point is consistent with recent research.
Second, analyze the effect of dimension reduction for the logistic regression model. We use the logistic regression model and the other two reducing dimension logistic regression models to apply comparative analysis. The results of the comparison can be found in Tables 15 and 16 .

As shown in Tables 15 and 16 , on average, the overall performance of reducing dimension models for prediction is not better than the original models at a 5% rejection level. This may be due to the limited variables in the model, so that the linear relationship is not obvious. In these models, it is still difficult to say that the model with reducing dimension is significantly improved.

Furthermore, we compare these results between each model to analyze the effect of dimension reduction SVM. A comparison of results is listed in Table 17 .

From Table 17 , we can find that some models have the same accuracy, such as HLG X  X VM and ODR X  X VM; these are in italics. From the accuracy of BCA, we can determine that the HLG X  X DR X  SVM is the best one. HLG X  X VM and PCA X  X VM are the same at the accuracy of BCA, and what is more is that HLG X  X VM has more accuracy than ODR X  X VM and it is statistically significant. For GCA, there is not much obvious improvement with these models. We still can find that PCA X  X VM is not a good model for the prediction accuracy because other models, except HLG X  X CA X  X VM, can out-perform it at the 5% significance level. For OA, there is an increase for SVM and PCA X  X VM, so we can infer that dimension reduction caused the overall increase in accuracy, which mainly comes from the increase in BCA.

This boost in BCA is mainly dues to reduction of redundant variables and getting the major characteristic which increased the model X  X  predictive power; while due to the reduction of variables, the amount of information about customers is also reduced, so it leads to some difficulty in upgrading the good ones.

Finally, find the best model for credit scoring. Through the first three discussions on the comparison, we know that SVM can outperform LG and that reducing dimension does not improve LG much but improves SVM a lot. Therefore, to further compare the accuracy of the model with any distribution, we provide the ROC graphs with these reducing dimension SVMs (PCA X  X VM, ODR X  SVM, HLG X  X VM, HLG X  X CA X  X VM, HLG X  X DR X  X VM) in Fig. 7 , which describes the ROC of the validation dataset and in Fig. 8 ,which is the ROC of the test dataset. Therefore, AUG can be seen clearly and we can choose the best model more easily.

With the ROC graph, we can see that no model can be completely superior to other models in any case. The AUGs between PCA X  X VM, ODR X  X VM and HLG X  X CA X  X VM are almost the same, a little weaker for HLG X  X VM and a little stronger in HLG X  X DR X  X VM. Upon the above comparison, therefore, HLG X  ODR X  X VM is most effective method in this credit scoring problem.

We can summarize the advantages of orthogonal dimension reduction models. First of all, reducing dimension has better accuracy in BCA, which is the key index for credit scoring. Then, ODR reduces the less correlated variables so enables rapidity of convergence of the SVM models. Lastly, using the features extracted by orthogonal dimension reduction hybridizing logistic regression gives the modeler better explanations. 7. Conclusions
Most recently, researchers have found support vector machine can provide better performance in the prediction of credit scoring. However, support vector machine is a black-box method and lacks rules for selecting good input variables. Similar to other artificial intelligence methods, they face the problem of  X  X arbage in, garbage out X . Thus SVM is usually troubled with dimension curse. Focusing on this, we introduce orthogonal feature extraction techniques with logistic regres-sion and support vector machine, which has better interpret-ability for the input variables, reduces the dimension and accelerates convergence.

Our research follows the next processes. Because SVM is sensitive to initial condition and training algorithms, we use a grid search to select parameters for the SVM. Then, we design the process of reducing dimension with PCA, ODR and HLG to reduce the redundant variables, thus this resolves the problem of high multicollinearity to a certain degree. Finally, we experiment on these methods using the filtered features with the same training methods to test the effectiveness.

We have also found out in our experiments that SVM cannot work well with the numerical variables which are not normal-ized. We call this phenomenon as  X  X imensional interference X , as described in Section 6.1 in detail. That may be caused by the distribution of variables, which stops the kernel function effectively transform it to the high dimension. So in the high dimension space, it still cannot be separated by line. However, there are few references that have discussed this. It still needs more theoretical study.

To assess the performance of th ese experiments, we experi-ment with two types of cross-validation  X  asmalldatasetanda complete data set, and use a paired-t test to test the means for different accuracy between original models and the ones with reducing dimension. This way can validate the robustness of the models and give statistical significance for the improve-ment in accuracy. Because Type I error is more serious in credit scoring and cannot be arbitrarily cut off, we also use the area under the ROC graph to judge the effectiveness between different models. We find that no model can be completely superior to other models in any case, but SVMs with orthogonal feature extraction techniques have improved the prediction of BCA and OA. Obviously, PCA is not a good choice for SVMs reducing dimension and in total the AUG shows there is a little success with the orthogonal featur e extraction SVM hybridizing logistic regression.

Based on those assessments there is also an interesting finding that the lift in SVMs mostly comes from the lift in BCA. For a more elaborate model, such as HLG X  X CA X  X VM and HLG X  X DR X  X VM, there always is a decline in GCA, though it is not statistically significant, which is understandable because of the reduction of the variable: the model has a power to better distinguish core features, which may give more results for the bad ones, but it will lose some information and lower the specificity for a degree, which decreases the accuracy for some good cases. Fortunately, GCA, of all the models, is quite impressive.

Finally, to summarize, this study provides a new way  X  orthogonal dimension reduction  X  to address dimension curse, and it has an impressive effect in SVM for credit scoring, which is quite distinct from these methods based on techniques improve-ment. We discuss the related properties of this method in detail and test other common statistical approaches  X  principal compo-nent analysis and hybridizing logistic regression  X  to better solve and evaluate the problem. Moreover, in our opinion, for other applications such as pattern recognition, this method can also be used. The results potentially need further discussion, but we propose that it will prove highly helpful because reducing dimension methods can lead better interpretations for the vari-ables selection at a minimum.
 Acknowledgment R.B.G. thanks Project supported by the National Natural Science Foundation of China (Grant nos. 70831001, 70821061 and 71232003).
 References
