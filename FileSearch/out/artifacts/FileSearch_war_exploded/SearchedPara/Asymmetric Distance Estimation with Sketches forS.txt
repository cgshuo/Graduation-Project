 Efficient similarity search in high-dimensional spaces is im-portant to content-based retrieval systems. Recent stud-distance in high-dimensional spaces, and that filtering with sketches can speed up similarity search by an order of magni-tude. It is a challenge to further reduce the size of sketches, which are already compact, without compromising accuracy of distance estimation.

This paper presents an efficient sketch algorithm for simi-larity search with L 2 distances and a novel asymmetric dis-tance estimation technique. Our new asymmetric estimator takes advantage of the original feature vector of the query to boost the distance estimation accuracy. We also apply this asymmetric method to existing sketches for cosine simi-larity and L 1 distance. Evaluations with datasets extracted from images and telephone records show that our L 2 sketch outperforms existing methods, and the asymmetric estima-tors consistently improve the accuracy of different sketch methods. To achieve the same search quality, asymmetric estimators can reduce the sketch size by 10% to 40%. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance similarity search, sketch, asymmetric distance estimation
In this paper, we consider similarity search in high-dimensional spaces, a central problem in content-based retrieval sys-tems: given a collection of data objects represented by high-dimensional feature vectors, the objective is to preprocess them so that we can quickly find data objects similar to queries issued at run time. A query is also represented by a high-dimensional feature vector and the problem is to find the k nearest neighbors to the query point.

Sketch construction is an effective way to approximate feature vectors for similarity search. Sketches are compact bit vectors that can be used instead of the original feature vectors to estimate distances. In a search system, sketches are scanned upon a query to quickly generate a small set of candidates which can be ranked later with original feature vectors to obtain the search result. Such a process is often called filtering. As reported by previous work [10, 11, 15], sketches are typically an order of magnitude smaller than their feature vectors, and can significantly improve the space requirement and search speed. A key challenge for sketch construction is to achieve a high ratio of distance estimation accuracy to sketch size.

The traditional method of using sketches to estimate dis-tances, like L 1 distance [10] and cosine similarity [2], is to construct a bit vector for each feature vector where each bit is determined by the position of the feature vector with re-spect to a random hyperplane. The Hamming distance of such sketches will then be used to approximate the origi-nal distance measure. Since computing Hamming distance (counting the number of bits that are different) is simple, the filtering process can be an order of magnitude faster than scanning the original feature vectors. However, the distance estimation with such sketches is a crude approximation; ac-curacy can be low when sketches are very compact. A chal-lenging question is whether the accuracy of distance estima-tion can be substantially improved using the same sketches.
Distance estimation using sketches is typically symmetric in that two sketches are compared to produce an estimate of the distance. In this paper, we propose a novel asymmetric approach to estimate the distance between the feature vector of a query and the sketch of a data object. As the distance estimation is done in two different spaces, we call such algo-rithms asymmetric estimators . Asymmetric estimators do not impose additional space requirements because they use the same sketches for all data points and there is only one query point. These methods can achieve higher accuracy because they take advantage of the position information of the query point in the feature vector space. Another way to look at the effect of the proposed approach is that it allows thesketchestobemorecompacttoachievethesameac-curacy. Asymmetric estimators achieve these improvements at the cost of more computation than computing Hamming distance (by a constant factor). In other words, they provide a way to trade CPU time for accuracy or sketch compaction. However, computation is less of an issue today given the ex-panding gaps between CPU processing power, memory size and disk bandwidth.
 sketch construction and its asymmetric estimator. Our new method has an interesting property particularly desirable for similarity search: it is only sensitive to a small distance range covering most k -nearest neighbors. We then provide general guidelines on designing asymmetric estimators and show how to design asymmetric estimators for two existing sketch constructions for L 1 distance [10] and cosine similar-ity [7, 2].

We evaluate all three sketch constructions with symmetric and asymmetric estimators using real-life datasets extracted from images and telephone records, and demonstrate the significant space saving (10% to 40%) of the asymmetric estimators, as well as the superiority of our new L 2 sketch over the existing methods.
Similarity search in high-dimensional spaces is a long-studied problem so far without a general solution. It is known that when dimensionality is high, existing tree-based index structures degenerate to brute-force scan [16]. The re-cently developed methods, like VA-file [16] and LSH [6], usu-ally involve scanning a certain portion of the whole dataset. Filtering with sketches can potentially be used to reduce the candidates to be scanned.

Sketches were originally used to answer aggregate queries over data streams [1], and were recently used as a filter to accelerate similarity search [10, 11, 15]. In existing methods for all these applications, however, the estimating opera-tion is carried out completely in the sketch space. Simi-larity search, as studied in this paper, is a typical scenario where additional information other than sketches can be ex-ploited to improve estimation accuracy. We believe our idea of asymmetric distance estimation will find other applica-tionsaswell.

A sketch algorithm for L 1 distance was proposed in [10], and later applied to other datasets [11]. The random hyper-plane technique, first proposed in [7] for solving the max-cut problem, and later used in [2] for locality sensitive hash-ing, implicitly gives a sketch algorithm for cosine similarity, which is essentially equivalent to L 2 distance. However, its performance in similarity search has not been experimen-tally studied. In this paper, we propose asymmetric esti-mators for both methods, and experimentally evaluate all distance.

An analytical performance model of similarity search us-ing sketches was recently given in [15]. We use the same query-processing algorithm, so the performance model can be easily adapted to work with our method.

Dimension reduction is an active research area with many applications, and various methods exist. [5] is a comprehen-sive survey of popular methods. Though related in concept, sketches are not merely new dimension reduction methods. They are task-specific (similarity search in our case), and the emphasis is more on reducing the size rather than keep-ing high precision. PCA and random projection are used in this paper as performance baselines. Figure 1: Illustration of L 2 Sketch. (a) The space is partitioned randomly into gray and white stripes, inducing a 0/1 sketch function. (b) The left half of the curve shows a near-linear relationship, meeting the requirement of similarity search. distance, which we call L 2 Sketch, and show how to exploit the raw feature vectors for asymmetric distance estimation.
The goal of sketches for similarity search is different from that for simple distance estimation. First, we only need to know the distance relationships (smaller or larger), rather than exact values. Second, we only need high accuracy for small distances instead of uniform accuracy/inaccuracy across the whole range of distances. These two relaxations potentially allow data reduction more aggressive than tradi-tional dimension reduction methods.
 We achieve both requirements by a striping technique. Figure 1(a) gives an illustration of the scheme in R 2 space, and the idea is exactly the same for high-dimensional spaces. We randomly partition the space into interleaving white and gray stripes of equal width, and use stripe colors to deter-mine the sketch values of the points: gray for 0 and white for 1. Points closer to each other are more likely to fall into the same stripe, and subsequently, the XOR of their sketch values has a smaller expectation. We do the random strip-ing multiple times to produce a sequence of bits for each point, and use the Hamming distance of the bit vectors as a proximity estimator. Specifically,
L 2 Sketch 1. For a point p  X  R n ,its L 2 sketch is a bit vector  X  ( p )  X  X  0 , 1 } m ,witheachbit  X  i ( p ) produced by where A i  X  R n is a random vector with each dimension sam-pled independently from the standard Gaussian distribution N (0 , 1) ,and b i  X  R is sampled from the uniform distribu-tion U [0 ,W ) . W is called the window size . For two points p, q  X  R n , their sketch distance is defined as where d H is Hamming distance.

The above algorithm specifies how to produce a random and W together determine the width of the stripes, and the random shift b i determines the  X  X hase X . Figure 1(b) shows the relationship between the expectation of symmetric sketch distance and L 2 distance (see Section 3.3 for formu-las). The monotonicity of the curve satisfies our ranking purpose. For small L 2 distances, we can see a near-linear relationship, and when L 2 distance grows large, sketch dis-tance quickly converges to the limit 0.5. The window size W determines the sensitive distance range.
Figure 1(a) explains how the original query point is useful are data points, with only sketches available, and q is the query point, with both sketch and feature vector available. We know the precise location of q , but only know that p 1 and and q are in stripes of different colors, the distance between p 2 and q is lower-bounded by the minimum distance from q to a stripe boundary. This lower bound (marked  X  in the figure) replaces the Hamming distance 1 in asymmetric distance estimation. On the other hand, p 1 and p 3 are in stripes of the same color as q , and the value 0 is used. The asymmetric distance estimator is defined below.

L 2 Sketch 2. Under the setting of L 2 Sketch 1, for data point p and query point q ,bothin R n , the asymmetric sketch distance is defined as where  X  i ( q ) = min { h i ( q )  X  h i ( q ) ,h i ( q )
The asymmetric sketch distance is a weighted Hamming distance of the sketches. The weight  X  i ( q ) is the distance from q to the closest stripe boundary.
The following lemma gives the relationship between sym-metric and asymmetric sketch distances and original L 2 dis-tance.
 Lemma 1. Under the setting of L 2 Sketch, for any p, q  X  R n ,let d = p  X  q 2 be the L 2 distance, d  X  = d  X  ( p, q ) be the symmetric sketch distance and d  X   X  = d  X   X  ( p, q ) be the asym-metric sketch distance, then for any k  X  R , where f k ( t )= and  X  (  X  ) is the probability density function of the standard Gaussian distribution N (0 , 1) .

Proof. The m bits in the sketch are identically distributed, so we only consider the case when m = 1 and omit subscript i as in A i ,b i and h i .For m = 1, the expectation remains the same, and variance is scaled by 1 /m .

We shift the real line by h ( p ) ,sothat h ( p )= y  X  [0 ,W ), following uniform distribution. Let j  X  Z be an arbitrary integer. When h ( q )  X  [2 jW, (2 j +1) W ), d  X  = d  X   X  Thus only the case when h ( q )  X  [(2 j +1) W, (2 j +2) W )is interesting. In this case, we have have d  X  =1and d  X   X  = min { x, W  X  x } ,where x  X  [0 ,W ) is the distance from h ( q ) to the left window boundary (2 j +1) W . The probability for this to happen is
The last step is due to the 2-stability of Gaussian dis-tribution [3], which states that for any p  X  R n , A  X  p p 2  X  N (0 , 1).

The expectations of d  X  and d  X   X  are obtained by averaging the corresponding values weighted by the above probability across all x, y  X  [0 ,W ).

The variances of the estimators can be obtained in a sim-ilar way, and we do not show them here due to the page limit. Instead, we use experimental evaluation in Section 5 to compare the performance of these estimators.

Choosing a proper window size W is a practical issue. For k -nearest neighbor search, twice the typical distance of the k th nearest neighbor is a good starting point for tuning.
In this section we describe a general framework for design-ing asymmetric estimators and use this to devise asymmetric estimators for two existing sketch methods for cosine simi-larity and L 1 distance.
The asymmetric estimator proposed in the previous sec-tion can be generalized into the following framework
First, a sketch algorithm  X  for a metric space X, d spec-ifies a family  X  = {  X  i | i  X  I } of bipartitions of the space, mapping an arbitrary point p  X  X into a bit sequence  X  ( p )= X is the expectation of XOR of the corresponding bits in sketch algorithm should establish a predictable relationship between the sketch distance d  X  and original distance d .A closely related concept is locality sensitive hashing (LSH) [8]. Actually, a bipartition  X  i is a 0/1 valued LSH function.
Second, an asymmetric distance estimator specifies a dis-tance function d  X  (not necessarily the original d )between a point and a partition. In the case of L 2 Sketch, d  X  the L 2 distance measure under a random projection. If asymmetric sketch distance is simply the average distance from query point q to the part that the data point p is in: d ( p, q )= E i  X  I { d  X  ([  X  i ( p )]  X  i ,q ) } .Because d lower bound of the crude XOR, the asymmetric estimator potentially provides higher accuracy.

Given a sketch algorithm, the challenge then lies in finding a meaningful asymmetric distance function d  X  . Though the sketch algorithm and the original distance measure usually Figure 2: The space partitioning schemes of Cosine Sketch and L 1 Sketch, both based on random hyper-plane method. give useful clues, such an estimator is not always obvious and does not always exist. This is one limitation of the asymmetric distance estimation method. It is possible that an asymmetric estimator results in a distance measure dif-ferent from the original one, but is still helpful in improving the search quality. We later discuss such an estimator for the L 1 case. It is also possible that more than one asym-metric estimator might exist for a single sketch algorithm, each having its own advantage.

Another limitation of the asymmetric method is the accu-racy improvement. The error in distance estimation comes from uncertainty in the positions of the data point and the query point. Even if the asymmetric method fully eliminates the error introduced by the query point, it eliminates only half the source of error. In practice (mainly due to the ran-domization in sketch algorithms), this 50% upper bound is usually far from tight, even though the asymmetric estima-tors do make a significant difference in many cases.
In the following two subsections, we restate two existing sketch algorithms in our framework, and devise asymmet-ric estimators for them. We call these algorithms Cosine Sketch and L 1 Sketch based on the distance measure they approximate.
Cosine similarity is not a strict distance measure, but is still important to many applications like text document re-trieval. For any p, q  X  R n , the cosine similarity is defined by Thus, L 2 distance and cosine similarity have the same rank-ing effect for normalized datasets, and for unnormalized datasets, given the 2-norms of the points, a sketch algo-distance.

The following sketch algorithm for cosine distance is im-plicitly given in [7, 2]. For simplicity of presentation, we assume all the points are normalized to unit vectors.
Cosine Sketch 1. For a point p  X  S n  X  1 = { r  X  R n | r 2 = 1 } , its cosine sketch is a bit vector  X  ( p )  X  X  0 , 1 } m ,witheach bit  X  i ( p ) produced by where  X  i for each i is sampled uniformly at random from the unit hypersphere S n  X  1 . The symmetric sketch distance be-
The idea of the random hyperplane method is illustrated in Figure 2(a). The random vector  X   X  S n  X  1 determines ahyperplane l (the orthogonal complement of  X  ) which bi-partitions the sphere. A bit in the sketch records whether or not the point falls on the same side of the hyperplane as  X  . To design an asymmetric distance estimator, we need to assess the distance from the query point q to its neighbor-ing half space (the gray one). We find the distance from q to the hyperplane l , indicated by  X  in the figure, a natural choice. Simple enough,  X  is exactly |  X   X  q | .Thus,wehave the following asymmetric estimator.

Cosine Sketch 2. Under the setting of Cosine Sketch 1, for data point p and query point q ,bothin S n  X  1 , the asym-metric sketch distance is defined as d  X   X  ( p, q )= 1
The following lemma gives the relationship between sym-metric and asymmetric sketch distances and the original co-sine similarity.

Lemma 2. Under the setting of Cosine Sketch, for any p, q  X  S n  X  1 ,let  X  = b pq be the angle between p and q , d =cos(  X  ) be the cosine similarity, d  X  = d  X  ( p, q ) be the symmetric sketch distance and d  X   X  = d  X   X  ( p, q ) be the asym-metric sketch distance, then the following relations hold: where B (  X  ,  X  ) is the Beta function.

Proof. (a) See [7, 2]. (b) Given the angle  X  between p and q , the probability that the sketches have different values of the i th bit is  X / X  .Thus, the probability that the sketches have Hamming distance k is given by the binomial distribution B ( m,  X / X  ). The expec-tation of cos  X d  X  can be obtained by taking the average for k from 0 to m . (c) By linearity of expectation, we only need to consider the case m = 1, and we drop the subscript i .Weusehyper-spherical coordinate system, where each point is represented with one radial coordinate r and n  X  1 angular coordinates simply omit this coordinate. Moreover, for particular p and q , we rotate the coordinate system so that Figure 3: Mean squared error of Cosine Sketch esti-mators vs. cosine similarity ( n = 128 ,m = 256 ). The asymmetric curve has its left half higher than the symmetric one. We use a trick (described in Sec-tion 4.2) to lower it down to the dotted curve, which mirrors the right half of the asymmetric curve.
 The corresponding Cartesian coordinates are p = 0 ,..., 1 , 0 and q = 0 ,..., cos(  X  ) , sin(  X  ) . Let the hyper-spherical coor-dinates of the random vector  X   X  S n  X  1 be  X  1 ,..., X  D  X  1 . We thus have Only the hyperplanes that separate p and q make a non-zero contribution to the asymmetric distance. These hyperplanes correspond to  X   X   X  1  X   X  2 ,where  X  1 = {  X  |  X   X  p&lt; 0 , X   X  q&gt; 0 } = {  X  | 1  X  2 = {  X  |  X   X  p&gt; 0 , X   X  q&lt; 0 } = {  X  | 3 Thus E [ d  X   X  ( p, q )] = (d) Similar to (c).

The symmetric estimator actually estimates the angle  X  instead of the cosine similarity. In practice we use cos(  X d  X  ) as a biased estimator of cosine similarity.

Figure 3 shows the mean squared error of both symmet-ric and asymmetric estimators when n = 128 and m = 256 based on Lemma 2. We can see that the asymmetric esti-mator works well when cosine similarity is close to 1, but degrades badly when smaller than 0. For similarity search under cosine similarity, this works well, for only similarity close to 1 is interesting. For estimating L 2 distance, how-ever, we actually want low error across the whole range. We use the following trick to circumvent this flaw.

For arbitrary p, q  X  S n  X  1 ,wehave d cos (  X  p, q )=  X  d cos ( p, q ); and is estimated more accurate ly. Furthermore, the sketch  X  (  X  p )of  X  p is exactly the binary complement of that of p , and is thus available at query time. As a result, we always compute an estimation for both p and  X  p , and use the better one to produce the final estimation. [10] introduced a sketch algorithm for weighted L 1 dis-tance. To simplify the presentation, we only consider the unweighted case here. Our asymmetric estimator can be easily adapted to work with the weighted case. Following is the unweighted version of the L 1 sketch algorithm proposed in [10]. Note that the algorithm works only for limited space, and requires the range of each dimension as input.
L 1 Sketch 1. Givenaclosedspace X =[ l 1 ,u 1 ]  X  X  X  X  X  [ l n ,u n ]  X  R n , for a point p  X  X ,its L 1 sketch is a bit vector  X  ( p )  X  X  0 , 1 } m ,eachbit  X  i ( p ) produced by Each bit in the sketch is the XOR of b independently gen-erated bits, and b is a parameter specified by the user. For each i, j pair, the index s i,j and threshold t i,j are generated in the following two steps: 2. Pick t i,j uniformly at random from [ l s i,j ,u s i,j The symmetric sketch distance between p, q  X  S n  X  1 is de-fined as d  X  ( p, q )= 1 m d H [  X  ( p ) , X  ( q )] .
It can be proved that if b =1,theexpectationofsketch distance is proportional to L 1 distance. XORing b bits has the effect of increasing the sensitivity to small distance range, similar to what is shown in Figure 1(b). Though the theoretical properties of this XOR idea are interesting, no intuitive explanation is given in [10, 11, 15]. Here we give a geometrical view of the XOR method, which is rather straightforward.

Figure 2(b) illustrates a bipartition of a 2D space accord-ing to the sketch algorithm, with b = 3. The three random dimension-threshold pairs partition the space into six rect-angular regions, and the XOR scheme induces a gray-white coloring of the regions such that touching regions are always colored differently. Note that regions of the same color, as in L 2 Sketch, are considered as one partition.

Given the geometrical view of the bipartition scheme, one asymmetric estimator is straightforward. We take the dis-tance from the query point q to the closest boundary of the region, which is t 3 in the figure, as the asymmetric distance function d  X  . This distance is indicated in the figure by  X  . Following is the algorithm of the asymmetric estimator.
L 1 Sketch 2. Under the setting of L 1 Sketch 1, for data point p and query point q ,bothin X , the asymmetric sketch distance is defined as where  X  i ( q ) = min {| q s i,j  X  t i,j || j =1 , 2 ,...,b
The drawback of the above asymmetric estimator is that it is not a proper estimator of L 1 distance. It can be shown that when b = 1, the asymmetric distance is in fact propor-tional to the L 2 distance. Nevertheless, as the experimental results in Section 5 show, it does improve search quality in practice.
In this section, we conduct experiments with two real-life datasets to study the performance of the methods described in the previous two sections . We first measure the accuracy of different methods to demonstrate the advantage of sketch methods over traditional dimension reduction methods and the improvement of asymmetric estimators over the basic symmetric ones. We then plug our methods into a similarity search algorithm and evaluate the space reduction achievable by the new asymmetric estimators.

WehaveshowninSection4.2that L 2 distance and cosine similarity are essentially equivalent. Due to the page limit, we evaluate both L 2 Sketch and cosine sketch under L 2 mea-sure by using the Cosine Sketch to estimate L 2 distance.
We use two high-dimensional datasets, i.e. images and audio, in our evaluation. These datasets are reasonably large and reflect real-life usage. Table 1 is a summary of these datasets.
 Image: The image dataset is extracted from the Caltech 101 [4] object recognition benchmark. This dataset con-tains 101 object categories and a background clutter cate-gory, with 9144 images in total. We convert the images into grayscale PGM format and use the SIFT [9, 12] algorithm with default parameters to extract feature vectors, obtain-ing nearly 4.5 million 128-dimensional feature vectors. We treat feature vectors extracted from the same image as in-dependent objects, as we only consider similarity search on the feature vector level.
 Audio: The audio dataset is the LDC-SWITCHBOARD-1 [13] collection, which is a collection of about 2,400 two-side telephone conversations among 543 speakers from all areas of the United States. The conversations are segmented into individual words based on human transcription. We use the Marsyas library [14] to extrac t feature vectors from the word segments. For each word segment, we take a 512-sample sliding window with variable stride to obtain 32 windows, and from each window extra ct the first six MFCC param-eters, resulting in a 192-dimensional feature vector. About 2.5 million feature vectors are extracted.
For L 2 distance, we compare the following six methods. Figure 4: Accuracy of various L 2 distance estima-tion methods on different distance values of the im-age dataset. Note that only the small distances are interesting to similarity search.
For L 1 distance, we simply compare the symmetric and asymmetric estimators of L 1 Sketch.

We always allocate sketches in full bytes to simplify im-plementation. Actually, varying sketch size by less than one byte does not make a practically significant difference in per-formance. For Cosine Sketch, we need the 2-norms of the vectors to estimate L 2 distance, and count 4 extra bytes into sketch size. For PCA and RP, we use 32-bit floating-point representation, and thus count each dimension as four bytes.
We implement asymmetric sketch distances in the follow-ing way. Assume an m -bit sketch length, we pre-calculate for each query point an m  X  2matrix M , M [ i ][ j ] being the value to be added if bit-i of the data sketch is j . Therefore, eval-uating the asymmetric sketch distance for each data point involves m floating-point additions.
Because our asymmetric estimator of L 1 Sketch does not produce a proper estimation of L 1 distance, we do not con-sider L 1 distance here. We only evaluate the L 2 distance re-lated methods. Also, we only use the image dataset, which is sufficient to demonstrate the behavior of different methods.
First, we measure the mean squared error of various meth-ods at different distance values. We randomly sample 100,000 pairs of points from the image dataset, create sketches for them, estimate the distance for each pair with sketches, and then compare the estimations with real distances. We then bin the squared error values according to the correspond-ing real distances and take the average of each bin. For asymmetric estimators, we use the raw feature vector of one arbitrary point from each pair. The results are plotted in Figure 4. We also attach the distribution of real L 2 distance in the bottom of the figure to show the relative importance of different distance values. Figure 5: Mean squared error of different methods vs. sketch size, image dataset.

Figure 4 gives a clear view of how different methods be-have for different distance values. The L 2 Sketch curves are most interesting, for the errors are low when distance is small, but beyond 400, the errors increase dramatically as distance grows. Note that the turning point is tunable in practice via the window size W . The curves of the other methods are relatively flat. Especially, the Cosine Sketch estimators are pretty consistent across the whole distance range, and are the first choice i f one is interested in estimat-ing distances for arbitrary points rather than for the nearest neighbors only.

We then consider the overall accuracy vs. sketch size. For the purpose of nearest neighbor search, it does not make sense to cover the whole distance range from zero to infinite. Instead, we only consider the distance range from 0 to 300, which is around one standard deviation (  X  60) beyond the average distance of the 100th nearest neighbor (  X  250). We sample from the image dataset 1000 pairs of points that are within this distance range, and take the mean squared error of different methods at different sketch sizes. The results are shown in Figure 5.

The curves in Figure 5 can be grouped into three cate-gories, from low to high in accuracy: dimension reduction methods, sketches with symmetric estimators and sketches with asymmetric estimators. The relative performances of these methods are mostly consistent across the whole figure. The more than 50% error reduction of the sketch methods over PCA and RP clearly shows their superiority. The figure also shows that the improvement of an asymmetric estima-tor (from  X  X ym cos X  to  X  X sym cos X ) is larger than using a better sketch scheme (from  X  X ym cos X  to  X  X ym L 2  X ) . In the above two figures, RP consistently out-performs PCA in terms of mean squared error. This is mainly be-cause PCA has a systematic bias that makes it always lower-estimate actual distance. This bias does not affect similarity search and we will see that PCA is actually better at simi-larity search.
With the accuracy improvements of our new methods con-firmed, in this subsection we evaluate the methods in the sce-nario of similarity search, and see how they improve search quality, and equivalently, reduce the space requirement to achieve a fixed quality.

Here is how we create the evaluation benchmark: for each dataset, we pick 100 points at random as query points, and use the rest of the points as the data points indexed. For each query point, we sequentially scan all the data points for k nearest neighbors under each distance measure concerned, and use these results as the ground truth. We use k = 100 in our experiments.

We measure search quality by recall ,whichistheper-centage of the true nearest neighbors found by the query algorithm. We do not consider precision here, but use the equivalent filter ratios t and t as input parameters to the query algorithm explained below.

We use the same query algorithm as modeled in [15]. The query algorithm with symmetric estimators proceeds in two steps: first, scan the sketches for a candidate set of t points with smallest sketch distances to the query point; second, scan the raw feature vectors of the candidate set, and find the top k within them as the final result. We use t = 20 in all our experiments.

Asymmetric estimators take more computation than the symmetric ones, and filtering all the sketches with asymmet-ric estimators is not always affordable. As a work-around, we extend the query algorithm with an extra filtering step. We first scan the sketches with a symmetric estimator for t  X  t  X  k candidate points, and then use an asymmetric esti-mator to rank them and choose the top t  X  k candidates for final re-ranking with raw feature vectors. Our experience shows that t = 10, which we use throughout the following experiments, provides nearly identical recall as using asym-metric estimators to scan all the sketches.

The relationships between t , k and recall are thoroughly studied in [15] and are not our focus in this paper. Instead, we focus on the recall/size performance of different sketch distance, and Figure 7 shows those for L 1 distance. L 2 sym 32 40 56 32 42 60 asym 22 29 39 23 32 48 Reduction 31% 28% 30% 28% 24% 20% Cos sym 40 54 73 41 54 73 asym 26 32 42 25 32 46 Reduction 35% 41% 43% 30% 41% 37% L 1 sym 51 66 92 64 80 120 asym 43 59 83 47 64 106 Reduction 16% 11% 10% 27% 20% 12% Table 2: Space requirement (in bytes) of various methods to achieve specific recalls. Here L 2 sketch and Cosine sketch are evaluated with L 2 distance, and L 1 sketch is evaluated with L 1 distance.

Figure 6 shows that for each sketch method, the asym-metric estimator always out-performs the symmetric one. The improvement is especially significant when the sketch is small. But even at a baseline recall of 0.90, the asymmet-ric methods still achieve a 0.05 improvement in most cases. The results for L 1 distance, as shown in Figure 7, are not as good; however, we do see a consistent improvement.
The performance of PCA on the audio dataset is pretty interesting, as the recall grows very quickly when the size is smaller than 32, or 8 dimensions, reaching one of the sketch methods at 8 dimensions, and then slows down sig-nificantly. This suggests that the intrinsic dimension of the audio dataset might be around 10.

Table 2 shows the minimal sketch sizes of symmetric and distance, the asymmetric estimators achieve sketch size re-duction from 20% to 43%. For L 1 distance, the reduction is smaller, from 10% to 27%.
In this paper, we proposed the idea of asymmetric distance estimators to exploit the raw query points, which are not used by traditional methods when estimating distances with sketches. We apply the idea to three sketch algorithms, one of them newly proposed in this paper. Our experimental results confirm the precision improvement of the asymmetric estimators, and show that to achieve the same search quality, the asymmetric estimators can reduce the space requirement by 10% to 40%.

The asymmetric estimators designed in this paper are a proof of concept, and the potential performance limit of asymmetric estimators is an open question. Designing bet-ter asymmetric estimators, especially for L 1 distance, is an interesting direction for future research.
 This work is supported in part by NSF grants EIA-0101247, CCR-0205594, CCR-0237113, CNS-0509447, DMS-0528414 and by research grants from Google, Intel, Microsoft, and Yahoo!. Wei Dong is supported by Gordon Wu Fellowship.
