 Peer review provides learning opportunities for stu-dents in their roles as both author and reviewer, and is a promising approach for helping students im-prove their writing (Lundstrom and Baker, 2009). However, one limitation of peer review is that stu-dent reviewers are generally novices in their disci-plines and typically inexperienced in constructing helpful textual reviews (Cho and Schunn, 2007). Re-search in the learning sciences has identified prop-erties of helpful comments in textual reviews, e.g., localizing where problems occur in a paper and sug-gesting solutions to problems (Nelson and Schunn, 2009), or providing review justifications such as ex-planations of judgments (Gielen et al., 2010). Re-search in computer science, in turn, has used nat-ural language processing and machine learning to build models for automatically identifying helpful review properties, including localization and solu-tion (Xiong and Litman, 2010; Nguyen and Lit-man, 2013; Xiong et al., 2012; Nguyen and Litman, 2014), as well as quality and tone (Ramachandran and Gehringer, 2015). While such prediction mod-els have been evaluated intrinsically (i.e., with re-spect to predicting gold-standard labels), few have actually been incorporated into working peer review systems and evaluated extrinsically (Ramachandran and Gehringer, 2013; Nguyen et al., 2014).
 active research threads for improving the utility of an existing web-based peer review system. Our re-search in the SWoRD project aims at building instant feedback components for improving the quality of textual peer reviews. Our initial work focused on improving review localization (Nguyen et al., 2014). Here we focus on increasing the presence of solu-tions in reviews. When students submit reviews, natural language processing is used to automatically predict whether a solution is present in each peer review comment (Figure 1). If not enough critical comments are predicted to contain explicit solutions for how to make the paper better, students are taken from the original review interface to a new instant feedback interface which scaffolds them in produc-tively revising the original peer reviews (Figure 2).
Sections 2 and 3 describe the Instant-feedback workflow, and the supporting natural language pro-cessing techniques. Section 4 demonstrates the promise of our system in supporting student review revision in a recent system deployment. ciprocal peer review, especially in large classes in-volving writing in the disciplines where writing and revision are hard to support due to lack of resources. A typical peer review exercise in SWoRD involves three main phases: (1) student authors submit pa-pers to SWoRD, (2) student reviewers download pa-pers assigned to them and submit peer reviews of the papers, and (3) student authors submit paper re-visions that address the peer reviews they received. To further enhance the utility of SWoRD, we have developed Instant-feedback SWoRD , with the goal of helping student reviewers increase the presence of solutions in the peer review comments produced during Phase 2 of the typical peer review exercise.
Figures 2 and 1 illustrate technical details of Instant-feedback SWoRD. As in the original SWoRD, student reviewers create a new review ses-sion by opening the review interface (Figure 2, left). Now, however, whenever the S UBMIT button is clicked, the  X  X ext review input X  is passed to the  X  X ubmission order check X  (Figure 1, diamond #1). times a review will be processed for instant feedback (e.g., 0 means no instant feedback, 1 means only the original comments are analyzed, 2 means revised comments are also analyzed, etc.). If the threshold is not reached, each comment in the review is an-alyzed by the  X  X omment-level Solution Prediction Component X  (see Section 3) and classified as a Solu-tion , Problem-only , or Non-criticism . Problem-only comments point out problems without providing so-lutions, while Non-criticisms such as summaries or praise do not require solutions. To measure how many problem comments have solutions, we define S-R ATIO as number of solution comments over the sum of solution and problem-only comments. If the predicted S-R ATIO is less than or equal to a thresh-gered to scaffold students in revising problem-only comments. Otherwise the review is deemed accept-able and stored for later use by Phase 3.

When instant feedback is triggered, the instant feedback interface (Figure 2, right) displays a mes-sage at the top suggesting that comments may need to be revised to include solutions, followed by but-tons representing the 3 possible reviewer responses: revise the review and resubmit (left), view some pre-defined example comments with solutions before re-sponding (center), or submit the review without revi-sion (right). To call the reviewer X  X  attention to com-ments that might need revision, the interface turns text boxes around predicted problem-only comments to red (Figure 2, middle right). For these comments, the system also generates option buttons that ask re-viewers to provide feedback on the prediction. We hypothesized that asking students to reason about the absence of solutions in their own comments would promote review revision. Their feedback on the system X  X  predictions also provides new anno-tated examples for future re-training of the predic-tion model (described in Section 3). Conversely, the interface highlights predicted solution comments in green (Figure 2, bottom right) along with displaying a thumbs-up icon. This highlighting was designed to draw reviewer attention to examples of solutions in their own comments. Finally, for reviews that are revised and resubmitted, Instant-feedback SWoRD increases the submission order and re-checks the threshold (diamond #1 in Figure 1). Unrevised re-views are instead stored for Phase 3 of SWoRD. To support the instant feedback interface described in the prior section, we developed a 3-way classi-fication model for predicting a review comment X  X  feedback type: Solution , Problem-only , or Non-criticism . Challenges emerge from the fact that SWoRD serves a wide range of classes ranging from high school to graduate school and from STEM to language arts. Consequently, our prediction model has to process peer review comments that greatly differ in style and vocabulary. We thus focused on modeling how students suggested solutions by de-veloping the following feature sets that abstracted over specific lexicons and paper topics:
Our solution prediction model was trained with logistic regression using annotated peer review com-ments from two university classes (Computer Sci-ence, History) and a high-school class (Literature). During learning, we used a cost matrix to favor in-stant feedback precision over recall by penalizing relevant error types. We thought it would be better to miss some feedback opportunities than to incor-rectly trigger instant feedback (e.g., asking students to revise reviews where all comments already con-tained solutions) or to incorrectly display comments as red or green in the feedback interface. In Spring 2015, SWoRD with instant-feedback was deployed in 9 high-school Advanced Placement (AP) classes. We conducted preliminary evalua-tions to answer two research questions: (1) How precisely does the system predict peer review solu-tion and trigger the instant feedback? (2) How does the instant feedback impact review revisions? We collected peer review submissions which were inter-vened by Instant-feedback SWoRD (i.e., triggered instant feedback), and their immediately subsequent resubmissions (if any), then had an expert manu-ally code the collected comments for their feed-back types: solution, problem-only, non-criticism (double-coded data had inter-rater  X  0.87).
Only intervened reviews were used to evaluate model performance because subsequent resubmis-sions were not predicted. In our deployment, 134 of 1428 reviews were intervened, containing 891 comments: 223 Solution , 340 Problem-only , and 328 Non-criticism . Table 1 shows that our deployed model outperforms a Bag-of-Words (BoW) base-was never used for model training, the obtained per-formance is promising and encourages us to improve the model with more data.

Regarding instant feedback precision, we calcu-lated the true S-R ATIO for each intervened review (using gold standard labels). Table 2 shows that given the 0.7 threshold used for this deployment, Instant-feedback SWoRD incorrectly triggered in-stant feedback for 24 submissions (column 3) out of 134, yielding a precision 0.82. Because Instant-feedback SWoRD does not let student reviewers know the S-R ATIO threshold, students should only think that the instant feedback was incorrect when they provided solutions for all mentioned problems (true S-R ATIO = 1). From this student perspective, Instant-feedback SWoRD had 16 incorrect triggers (column 4), achieving a precision 0.88.

Finally, to evaluate the impact of instant feedback on review revision, we considered the 74 subsequent resubmissions. We collected comments that were re-vised or newly-added to the resubmissions (no com-ment was deleted), and obtained 115 comments. Pairing 111 revised comments with their original versions, we observed that 73 (66%) comments were fixed from problem-only to solution, 3 (3%) from non-criticism to solution, only 1 comment (0.9%) was edited from solution to non-criticism, and none from solution to problem-only. All of the 4 newly-added comments mentioned problems and provided solutions. These results suggest that Instant-feedback SWoRD does indeed help review-ers revise their comments to include more solutions. This paper presented Instant-feedback SWoRD, which was designed to increase the presence of so-lutions in peer reviews. Evaluation results showed that Instant-feedback SWoRD achieved high perfor-mance in predicting solution in review comments and in triggering instant feedback. Moreover, for reviewers who revised their reviews after receiving instant feedback, the number of comments with so-lution increased. In future work, we plan to use more data from a wider range of classes to re-train the currently deployed prediction model. Also, a com-prehensive comparison of our approach to studies of similar tasks would give us insight into features and algorithms for performance improvement.
 Acknowledgments This research is supported by NSF/1122504, and IES/R305A120370. The larger research project (co-led by Professors Kevin Ashley, Amanda Godley, Diane Litman and Chris Schunn) is a collaboration with the Learning Research &amp; Development Center and the School of Education. We are grateful to our colleagues in the project.
