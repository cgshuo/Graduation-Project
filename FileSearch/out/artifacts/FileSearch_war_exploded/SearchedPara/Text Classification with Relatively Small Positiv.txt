 This paper addresses the problem of dealing with a collec-tion of negative training documents which is suitable for relatively small number of positive documents, and presents a method for eliminating the need for manually collecting negative training documents based on supervised machine learning techniques. We applied an error correction tech-nique to the results of negative training data obtained by the Positive Example Based Learning (PEBL). Moreover, we used a boosting technique to learn a set of negative data to train classifiers. The results using Japanese newspaper doc-uments showed that the method contributes for reducing the cost of manual collection of negative training documents. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.5.2 [ Design Methodology ]: Pat-tern Analysis Experimentation Text Classification, Small Positive Documents and Unla-beled Data
As the volume of online documents has drastically in-creased, text classification based on larger corpora has be-come more important, and a growing number of ML tech-niques have been applied to the task [1]. However, the in-creasing number of pages and categories often hampers the development of practical classification systems, mainly due to the quantity and quality of the manual annotation of the corpus, especially annotating negative training documents. For instance, let documents related to  X  X ccident X  be posi-tive training documents and documents related to  X  X ports X  be negative training documents. A document concerning to  X  X gyptAir-990 crash X  is judged as positive. On the other hand, let documents related to  X  X ccident X  be positive and documents related to  X  X ir crash X  be negative documents. In this case, the same document,  X  X gyptAir-990 crash X  should be judged as negative. These observations show that neg-ative training documents affect the classification accuracy as manually collected negative training documents could be biased because of humans X  unintentional prejudice.
This paper present a text classification method with rel-atively small positive documents and unlabeled data. Our goal is to achieve classification accuracy from positive and unlabeled data as high as that from labeled positive and la-beled negative data. Our method is based on an algorithm called positive example based learning (PEBL) developed by Yu et al. [7]. The PEBL learns from additional positive data and the given unlabeled data without requiring labeled data. It consists of two steps: Mapping and Convergence (M-C) procedures that learn from positive and unlabeled data as accurately as a traditional SVM that learns from labeled data. However, once SVM incorrectly classified pos-itive document collected from unlabeled data into negative, the document is set to negative. As a result, the test data is not classified correctly by using the final classifier. Moreover, if the M-C selects weak negative data from unlabeled data, the accuracy is decreased. In order to solve these problems, we applied an error correction technique for the results of SVM classification. Moreover, we used boosting technique [6] to learn a set of negative data to train classifiers.
Figure 1 illustrates our system, PEBL with Error Correc-tion and Boosting (hereafter, referred to as PEBL ECB). The first step is the mapping procedure that selects the strongest negative training data from the training data U . The strongest negative training data is a data not having any of the positive features/words in the documents. We used  X  2 feature selection technique to select positive features. We selected the strongest negative N 1 , each of which did not in-clude any positive features. The remaining data U \ N 1 is set to R 1 . The second step is convergence procedure consisting of correction of errors and boosting. For each N i (1  X  i n ), where n is the number of iteration, we applied an error detection technique. Moreover, for the results obtained by the correction procedure, we applied boosting. These pro-cedures are repeated until there are no training documents judged to be negative. Finally, the test data is classified by using the final classifiers.
As an error candidate, our method focuses on support vec-tors extracted from the training documents by SVM. Train-ing SVM is to find the optimal hyperplane consisting of sup-port vectors, and only the support vectors affect the perfor-mance. Thus, if some training document deteriorates the overall performance of text classification because of an out-lier, we can assume that the document is a support vector. The method uses a loss function which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner X  X  prediction. We applied it to the extracted support vectors, and corrected annotation errors. Figure 2 illustrates an overview of correc-tion procedure consisting of extraction of error candidates, estimation of error reduction and correction of errors. 1. Extraction of error candidates
Let D be a set consisting of P and N ,and x k  X  X  x 1 , x 2  X  X  X  , x m } be support vectors of negative samples. We remove support vectors { x 1 , x 2 ,  X  X  X  , x m } from the training samples D . The resulting D 1 is used for training Naive Bayes (NB), leading to a classification model. This classification model is tested on each support vector, x k and assigns positive or negative label. If the label is positive, we declare x k an error candidate. 2. Estimation of error reduction
Roy et al. proposed a method of active learning that di-rectly optimizes expected future error by log-loss, using the entropy of the posterior class distribution on a sample of the unlabeled documents [4]. The active learning aims to select x , such that when the sample is given label y and added to the training set, the learner trained on the resulting set ( D +( x , y )) has lower error rate than any other x .We applied their technique to the extracted error candidates. A loss function is defined by (1).
 D 2 denotes the training documents D except for the error prediction. X denotes the total number of test document x . If the value of Eq. (1) is sufficiently small, the learner X  X  prediction is close to the true distribution. Like Roy et al X  X  method, we use bagging to reduce variance of the true out-3. Correction of errors
We used Eq. (1) for each error candidate in order to de-termine the label assigned to x k is either positive or nega-tive. More formally, x k be an error candidate. We calculate E to a label of support vector, i.e. , negative, and y k new resulting category label estimated by the NB classifier, i.e. , positive. If the value of E  X  P its true label is positive.
We applied boosting for each result obtained by the cor-rection procedure. We recall that once the positive training data is incorrectly classified into negative, then the positive training data is included into the negative training data. This causes the result of not only the iteration is not termi-nated but also the classification accuracy is decreased. We focused on the problem and applied a boosting technique [6] to each iteration of the convergence procedure. Boosting is a general technique for improving the accuracies of machine learning algorithms.
 Figure 3 illustrates our method incorporating boosting. At each round t , it increases the weights of positive and negative training documents, P and N i which are incor-rectly classified by SVM, and the model created so far. As a result, the learning at the next round will be focused on the creation of a weak hypothesis. Once a hypothesis is built at each round by linearly combining the weak hypoth-esis constructed so far, the test data is classified by using a vote of each classifier. The procedures, error corrections and boosting are repeated until there are no training documents judged to be negative.
We had an experiment to evaluate our method. We used the RWCP corpus labeled with UDC codes selected from 1994 Japanese Mainichi newspaper consists of 28,203 doc-uments organized into a fine-grained categories, i.e. , 9,951 categories with a seven-level hierarchy [5]. All documents were tagged by using a morphological analyzer Chasen [3]. We selected noun and verbs and used them as a feature of a vector applying SVM, boosting and NB. We used Joachim X  X  SVM-Light package [2] for training and testing. We applied linear kernel and set all parameters to their default values. Throughout the experiments, we set the number of itera-tions to 100 times. In the mapping procedure, we used  X  2 method 1 . We divided the RWCP corpus into two: 22,203 training documents and 6,000 test documents. The average number of categories per document was 2.7, and the largest number of categories per document was 9. UDC code is a hierarchical structure. We used four, eight, and twenty cat-egories from the top, second, and third levels of a hierarchy having more than 50 documents, respectively. We estimated the true output distribution P ( y | x ) shown in Eq. (1). More precisely, for each category, we selected 10 documents from training documents and used them as test data. For bagging , we split the remaining training documents into two sets, and create a new classifier from each set. This procedure is re-peated 10 times. We compared our method, PEBL ECB with four baselines: (1) SVM, (2) PEBL, (3) PEBL + error correction, and (4) PEBL + boosting. The number of posi-tive and negative training data in each method is as follows: 1. SVM: For each category, we randomly choose 50 doc-2. PEBL, Error correction, boosting, and PEBL ECB:
We tested 1-DNF,  X  2 method, mutual information and in-formation gain to select the strongest negative training data. We reported only the result obtained by  X  2 method as it was the best results among them. Figure 4: F-score against the # of negative doc
Table 1 shows classification performance.  X  X rain X  and  X  X est X  refer to the number of training and test documents, respectively.  X   X   X  indicates that our method, PEBL ECB shows statistical significance of t-test compared with the marked method.  X  X VM 50 X  X nd  X  X VM 250 X  X how the results obtained by using 50 for each positive and negative docu-ments, and 50 positive and 250 negative documents, respec-tively. Parentheses indicates the number of iterations until the number of negative documents becomes zero in conver-gence procedure. We can see from Table 1 that the F-score obtained by PEBL ECB was better than those obtained by other methods. The results of each hierarchy level obtained by the PEBL ECB are statistically significant except for the result of second level with applying boosting (0.588). More-over, the number of iterations obtained by our method was smaller than those obtained by other methods. These obser-vations show that combining error correction and boosting techniques is effective for classification. Table 1 shows that our method is competitive to the SVM 250 as there is no significant difference between the F-score between them.
Figure 4 illustrates F-score and the number of negative documents obtained by PEBL ECB. The left Y-axis refers to F-score and the right indicates the # of negative docu-ments. We can see from Figure 4 that when the number of iterations was twelve times, the number of negative train-ing documents was 2,112 and the F-score by PEBL ECB (0.680) exceeded SVM (0.664). Moreover, when the number of iterations was 15 times, the number of negative train-ing data was 2,192 and subsequently there are no significant difference between both F-scores. We recall that in SVM classification, we used 50 positive and 250 negative labeled documents for each category, while PEBL ECB was only 50 labeled documents. As can be seen clearly from Figure 4, the PEBL ECB correctly choose negative training data. This shows that our method contributes for reducing the cost of manual tagging.
The PEBL ECB uses error correction results. Therefore, it is important to examine by hand whether the corrected * denotes statistical significance t-test, P-value  X  0.05
Figure 5: The # of negative and mislabeled docs errors are true errors or not. Figure 5 illustrates the number of negative training documents judged by the PEBL ECB and the number of mislabeled documents including it. The results are average number of documents in the third level of a hierarchy. The evaluation is made by three humans. The classification is determined to be correct if the majority of three human judges agrees.

As shown in Figure 5, the ratio of mislabeled documents divided by the total number of negative documents is around 10% when the number of iterations is smaller than 10. How-ever, the number of iterations is larger than 10, the ratio of mislabeled documents becomes large. This is not surprising because the larger the number of round is, the judgement whether the label is positive or not is more difficult. There are two causes of miss correction, (1) it could not be cor-rectly detected, and (2) it could be correctly detected but could not be correctly corrected. We manually examined documents that are not correctly corrected, and found that most of the documents are included in (1). For instance, there are fourteen documents which should be corrected at 14 rounds. Of these, eight documents are included in (1), and two documents are included in (2) 2 .WeusedNBto
The remaining four documents are correctly corrected. detect mislabeled documents because of its efficient com-putation. However, it was not produce better performance for a small number of training documents. Applying other learning techniques to improve correction accuracy is still ongoing research.
We have developed an approach to a collection of nega-tive training documents which is suitable for relatively small number of positive documents for text classification. The results showed that combining error correction and boost-ing techniques contributes for reducing the cost of manual tagging. Future work will include: (i) examining learning techniques other than NB to improve correction accuracy, and (ii) extension of the method to collect positive training documentsaswellasnegativedocuments. [1] S. Gopal and Y. Yang. Multilabel Classification with [2] T. Joachims. SVM Light Support Vector Machine. In [3] Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, [4] N. Roy and A. K. McCallum. Toward Optimal Active [5] RWCP. RWC Text Database . In Real World Computing [6] R. E. Schapire and Y. Singer. BoosTexter: A [7] H. Yu, H. Han, and K. C.-C. Chang. PEBL: Positive
