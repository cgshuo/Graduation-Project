 which characterize a class of hypotheses, independently of any algorithm. on the same day or of the same stock on different days may be dep endent. Vector Regression (SVR) [15] and Kernel Ridge Regression [1 3]. scenarios.
 then briefly discuss the learning scenarios in the non-i.i.d . case. 2.1 Non-i.i.d. Definitions Definition 1. A sequence of random variables Z = { Z and non-negative integers m and k , the random vectors ( Z have the same distribution.
 Thus, the index t or time, does not affect the distribution of a variable Z equal Pr[ Z random variables Z quantity, we are adopting here that of [17].
 Definition 2. Let Z = { Z are defined as Z algebraically  X  -mixing ( algebraically  X  -mixing ) if there exist real numbers  X  and r &gt; 0 such that  X  ( k )  X   X  exist real numbers  X   X  ( k )  X   X  0 exp(  X   X  1 k r ) ) for all k .
 also be estimated in such cases. the same size. The following lemma is a special case of Coroll ary 2.7 from [17]. absolute value bounded by M , on a product probability space Q s on (  X  Let  X  ( Q ) = sup  X  ( Q ) =  X  ( k  X  ) , where k  X  = min i ( k i ) is the smallest gap between blocks. 2.2 Learning Scenarios of For a fixed learning algorithm, we denote by h S R by b R ( h ) the empirical error of a hypothesis h for a training sample S = ( z In the standard machine learning scenario, the sample pairs z samples. But, here, we must distinguish two versions of this problem. ization error or true error of the hypothesis h conditioned on the sample S : trained on S is then: studies cannot be applied to the more general setting.
 processes only.
 any two training samples S and S  X  that differ by a single point satisfy Section 3.3).
 b these in this non-i.i.d. scenario. 3.1 Lipschitz Condition We denote by R ( h E [ c ( h S S sequence. The block S of the same size in S .
 by b  X  X  0 , . . . , m } , the following holds: Proof. The  X   X  -stability of the learning algorithm implies that The application of Lemma 1 yields We can now prove a Lipschitz bound for the function  X  . Lemma 3. Let S = ( z  X  -mixing stationary process that differ only in point i  X  [1 , m ] , and let h the following inequality holds: one that disagrees with M yields | b
R ( h S )  X  b R ( h S i ) | = The lemma X  X  statement is obtained by combining inequalitie s 11 and 12. 3.2 Bound on E[ X ] As mentioned earlier, to make the bound useful, we also need t o bound E analyzing independent blocks using Lemma 1.
 Lemma 4. Let h Proof. We first analyze the term E after point z e z is independent of the two others. By the  X   X  -stability of the algorithm, Applying Lemma 1 to the first term of the right-hand side yield s Combining the independent block sequences associated to b R ( h lemma in a way similar to the i.i.d. case treated in [3]. Let S consider the sequence S as four blocks. As before, we will consider a sequence e S the same distribution as the corresponding blocks in S learning algorithm, the following holds: the right-hand side leads to Since e z and e z we can replace e z E [ X ( S )]  X  E where e S i  X   X  can be shown following the same steps. 3.3 Main Results following theorem which extends McDiarmid X  X  inequality to  X  -mixing distributions. the following holds for all  X  &gt; 0 : where ||  X  Theorem 2 (General Non-i.i.d. Stability Bound) . Let h  X  &gt; 0 , the following generalization bound holds then we can solve for the value of b to optimize the bound. Theorem 3 (Non-i.i.d. Stability Bound for Algebraically Mixing Sequ ences) . Let h  X  -mixing stationary distribution,  X  ( k ) =  X  holds where  X  ( b ) =  X  satisfies  X   X b = rM  X  ( b ) , which gives b =  X   X  following term can be bounded as of the theorem.
 Theorem 3,  X   X   X  O (1 /m ) implies  X  ( b ) =  X  illustrates the application of Theorem 3 to several general classes of algorithms. regularized objective function based on the norm kk K is a positive definite symmetric kernel: and Kernel Ridge Regression [13], for which c ( h, z ) = ( h ( z )  X  y ) 2 . for all x for some  X  &gt; 0 . Let h least 1  X   X  , the following generalization bounds hold for  X   X   X  2  X  2 B 2 / (  X m ) and M &lt;  X  using the lower bound on r , r &gt; 1 , yield the statement of the corollary. stability bounds for SVR and KRR is tight. Weaker notions of stability might help further improve or re fine them.
