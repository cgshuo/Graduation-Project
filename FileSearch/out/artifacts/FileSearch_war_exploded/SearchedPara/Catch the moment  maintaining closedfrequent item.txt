 REGULAR PAPER Yu n C h i  X  Haixun Wang  X  Philip S. Yu  X  Richard R. Muntz Abstract This paper considers the problem of mining closed frequent itemsets over a data stream sliding window using limited memory space. We design a syn-opsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and mem-ory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets will make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of itemsets over a sliding window. The selected itemsets contain a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of min-ing closed frequent itemsets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than representa-tive algorithms for the sate-of-the-art approaches.
 Keywords Data streams  X  Sliding window  X  Closed frequent itemset  X  Incremental learning 1 Introduction 1.1 Motivation Data streams arise with the introduction of new application areas, including ubiq-uitous computing and electronic commerce. Mining data streams for knowledge discovery is important to many applications, such as fraud detection, intrusion detection, trend learning, etc. In this paper, we consider the problem of mining closed frequent itemsets on data streams. Mining frequent itemset on static data sets has been studied extensively. However, in many applications, mining frequent itemsets on data streams is needed. For example, for a commercial web site, web pages that are usually visited together by different users reveal important infor-contained in large-volume, unbounded user click streams. As another example, frequent itemsets can help traffic measurement and intrusion detection in high-speed, real-time network traffic data. Data streams have posed new challenges. First, data streams are continuous, high-speed, and unbounded. Archiving every-thing from streams is impossible, not to mention mining association rules from them using algorithms that require multiple scans. Second, the data distribution in streams are usually changing with time, and very often people are interested in the most recent patterns.
 approach is to always focus on frequent itemsets in the most recent window. A similar effect can be achieved by exponentially discounting old itemsets. For the window-based approach, we can immediately come up with two naive methods: 1. Regenerate frequent itemsets from the entire window whenever a new transac-2. Store every itemset, frequent or not, in a traditional data structure such as the sonable, and the concept drifts in the stream is not too dramatic, most itemsets do not change their status (from frequent to non-frequent or from non-frequent to frequent) often. Thus, instead of regenerating all frequent itemsets every time from the entire window, we shall adopt an incremental approach.
 in practice. The prefix tree [ 1 ] is often used for mining association rules on static data sets. In a prefix tree, each node n I represents an itemset I and each child node of n I represents an itemset obtained by adding a new item to I . The total number of possible nodes is exponential. Due to memory constraints, we cannot keep a prefix tree in memory, and disk-based structures will make real time update costly.
 that are (i) informative enough to answer at any time queries such as  X  X hat are the (closed) frequent itemsets in the current window X , and at the same time, (ii) small enough so that they can be easily maintained in memory and updated in real time. reduce memory usage, we are tempted to select, for example, nothing but frequent (or even closed frequent) itemsets. However, if the frequency of a non-frequent itemset is not monitored, we will never know when it becomes frequent. A naive approach is to monitor all itemsets whose support is above a reduced threshold minsup  X  , so that we will not miss itemsets whose current support is within of minsup when they become frequent. This approach is apparently not general enough.
 between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the data structure. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. The problem of mining an infinite amount of data is thus converted to mine data that can potentially change the boundary in the current model. Because most of the itemsets do not often change status, which means the boundary is relatively stable, and even if some does, the boundary movement is local, the cost of mining closed frequent itemsets is dramatically reduced. 1.2 Our contribution This paper makes the following contributions: (1) We introduce a novel algorithm, Moment, 1 to mine closed frequent itemsets over data stream sliding windows. To the best of our knowledge, our algorithm is the first one for mining closed fre-quent itemsets in data streams. (2) We present an in-memory data structure, the closed enumeration tree (CET), which monitors closed frequent itemsets as well as itemsets that form the boundary between the closed frequent itemsets and the rest of the itemsets. We show that (i) a status change of any itemset (e.g., from non-frequent to frequent) must come through the boundary itemsets, which means we do not have to monitor itemsets beyond the boundary, and (ii) the boundary is rel-atively stable, which means the update cost is minimum. (3) We introduce a novel algorithm to maintain the CET in an efficient way. (4) We have done extensive experimental studies to evaluate the performance of the proposed new algorithm. Experiments show that for mining closed frequent itemsets in data streams, Mo-ment has significant performance advantage over representative algorithms for the state-of-the-art approaches.
 background in frequent itemset mining. In Sect. 3 , we describe in detail our Mo-ment algorithm. In Sect. 4 , we have some discussion on the Moment algorithm. In Sect. 5 , we introduce related work. In Sect. 6 , we give experimental results. We give conclusion in Sect. 7 . 2 Problem statement 2.1 Preliminaries Given a set of items , a database D wherein each transaction is a subset of , and a threshold f called the minimum frequency ,0 &lt; f  X  1, the frequent itemset mining problem is to find all itemsets that occur in at least f | D | transactions. For an itemset I , we call the number of transactions in which I occurs the support of I . In addition, we define the minimum support ( minsup ) s as s = f | D | . use X  X  Y to denote that item X is lexicographically smaller than item Y .Further-more, an itemset can be represented by a sequence, wherein items are lexicograph-ically ordered. For instance, { A , B , C } is represented by ABC ,given A  X  B  X  C . We also abuse notation by using  X  to denote the lexicographical order between two itemsets. For instance, AB  X  ABC  X  CD .
 s = 2, then the frequent itemsets are In
F , each frequent itemset is associated with its support in database D . 2.2 Combinatorial explosion According to the a priori property, any subset of a frequent itemset is also frequent. Thus, algorithms that mine all frequent itemsets often suffer from the problem of combinatorial explosion.
 (e.g., [ 4 , 10 ]), only maximal frequent itemsets are discovered. A frequent itemset is maximal if none of its proper supersets is frequent. The total number of maximal frequent itemsets M is usually much smaller than that of frequent itemsets F , and we can derive each frequent itemset from M .However, M does not contain information of the support of each frequent itemset unless the itemset is a maximal frequent itemset. Thus, mining only maximal frequent itemsets loses information. ered. An itemset is closed if none of its proper supersets has the same support as it has. Usually, the total number of closed frequent itemsets C is still much smaller than that of frequent itemsets F . Furthermore, we can derive F from C , because a frequent itemset I must be a subset of one (or more) closed frequent itemset, contain I .
 maximal frequent itemsets for the above examples are itemsets, in this paper, we focus on mining the closed frequent itemsets because they maintain sufficient information to determine all the frequent itemsets as well as their support.
 2.3 Problem statement The problem is to mine closed frequent itemsets in the most recent N transactions (or the most recent N samples ) in a data stream. Each transaction has a time stamp, which is used as the tid (transaction id) of the transaction. Figure 1 is an example with ={ A , B , C , D } and window size N = 4. We use this example throughout the paper with minimum support s = 2.
 models the current frequent itemsets. We update the data structure incrementally. The combinatorial explosion problem of mining frequent itemsets becomes even more serious in the streaming environment. As a result, on the one hand, we cannot afford keeping track of all itemsets or even all frequent itemsets, because of time and space constraints. On the other hand, any omission (for instance, maintaining only M , C ,or F instead of all itemsets) may prevent us from discovering future frequent itemsets. Thus, the challenge lies in designing a compact data structure which does not lose information of any frequent itemset over a sliding window. 3 The Moment algorithm We propose the Moment algorithm and an in-memory data structure, the closed enable us to answer the query  X  X hat are the current closed frequent itemsets? X  at any time. 3.1 The closed enumeration tree Similar to a prefix tree, each node n I in a closed enumeration tree (CET) repre-sents an itemset I . A child node, n J , is obtained by adding a new item to I such that I  X  J . However, unlike a prefix tree, which maintains all itemsets, a CET only maintains a dynamically selected set of itemsets, which include (i) closed frequent itemsets, and (ii) itemsets that form a boundary between closed frequent itemsets and the rest of the itemsets.
 stream are not too dramatic, most itemsets do not change their status (from fre-quent to non-frequent or from non-frequent to frequent). In other words, the effects of transactions moving in and out of a window offset each other and usually do not cause change of status of many involved nodes.
 increasing or decreasing the counts of the involved itemsets. If it does change its status, then, as we will show, the change must come through the boundary nodes, which means the changes to the entire tree structure is still limited. respond to the boundary between frequent and non-frequent itemsets, and the boundary between closed and non-closed itemsets, respectively. Itemsets within the boundary also have two categories, namely the closed nodes, and other in-termediary nodes that have closed nodes as descendants. For each category, we define specific actions to be taken in order to maintain a shifting boundary when there are concept drifts in data streams (Sect. 3.3 ). The four types of itemsets are listed below. 1. Infrequent gateway nodes A node n I is an infrequent gateway node if (i) I is 2. Unpromising gateway nodes A node n I is an unpromising gateway node if (i) 3. Intermediate nodes A node n I is an intermediate node if (i) I is a frequent 4. Closed nodes These nodes represent closed frequent itemsets in the current 3.2 Node properties We prove the following properties for the nodes in the CET. Properties 1 and 2 enable us to prune a large amount of itemsets from the CET, while Property 3 makes sure certain itemsets are not pruned. Together, they enable us to mine closed frequent itemsets over a sliding window using an efficient and compact synopsis data structure.
 Property 1 If n I is an infrequent gateway node, then any node n J where J  X  I represents an infrequent itemset.
 Proof Property 1 is derived from the apriori property.
 It prunes the descendants of n I and the descendants of n I  X  X  siblings nodes that subsume I . However, the CET  X  X emembers X  the boundary where such pruning occurs, so that it knows where to start exploring when n I is no longer an infrequent gateway node. An infrequent gateway node marks such a boundary. In particular, infrequent gateway nodes are leaf nodes in a CET. For example, in Fig. 2 ,after knowing that D is infrequent, we do not explore the subtree under D . Furthermore, we do not join A with D to generate A  X  X  child nodes. As a result, a large amount of the itemsets are pruned.
 Property 2 If n I is an unpromising gateway node, then n I is not closed, and none of n I  X  X  descendents is closed.
 Proof Based on the definition of unpromising gateway nodes, there exists an item-set J such that (i) J  X  I , and (ii) J  X  I and support ( J ) = support ( I ) . From (ii), we know n I is not closed. Let i max be the lexicographically largest item in I .Since J  X  I and J  X  I , there must exist an item j  X  J \ I such that j  X  i cause support ( J ) = support ( I ) , itemset J \ I must appear in every transaction I appears, which means support ( n I ) = support ( n { j } X  I ) ,so I is not closed. nodes can be found there, and the CET  X  X emembers X  the boundary where such pruning occurs by recording the unpromising gateway nodes.
 Property 3 If n I is an intermediate node, then n I is not closed and n I has closed descendants.
 Proof Based on the definition of intermediate nodes, n I is not closed. Thus, there must exists a closed node n J such that J  X  I and support ( J ) = support ( I ) .If J  X  I ,then n intermediate node. So we have I  X  J .However,if I  X  J ,then n J must be n I  X  X  descendant because J  X  I .
 3.3 Building the closed enumeration tree For each node n I in a CET, we store the following information: (i) the itemset I occurs, and (iv) tid sum :the sum of the tids of the transactions in which I occurs. The purpose of having tid sum is because we use a hash table to maintain closed itemsets. 3.3.1 The hash table We frequently check whether or not a certain node is an unpromising gateway node, which means we need to know whether there is a closed frequent node that has the same support as the current node.
 an unpromising gateway node, by definition, we check if there is a closed frequent itemset J such that J  X  I , J  X  I ,and support ( J ) = support ( I ) . ate frequent hash collisions. We know if support ( I ) = support ( J ) and I  X  J , then I and J must occur in the same set of transactions. Thus, a better choice is the set of tid s. However, the set of tid s take too much space, so we instead use ( support , tid sum ) asthekey.Notethat tid sum of an itemset can be incremen-tally updated. To check if n I is an unpromising gateway node, we hash on the ( support , tid sum ) of n I , fetch the list of closed frequent itemsets in the corre-sponding entry of the hash table, and check if there is a J in the list such that J  X  I , J  X  I ,and support ( J ) = support ( I ) .
 store the pointers pointing to the corresponding nodes in the CET. 3.3.2 FP-tree for transactions We store the transactions in the sliding window in an FP-tree, in order to reduce the memory footprint and to speed up exploration of the transactions. FP-tree was first introduced by Han et al. for mining frequent itemsets without candidate generation [ 11 ]. In an FP-tree, each transaction is stored along a root-path; when transactions have a common prefix, the common part only needs to be stored once; a counter is used to record the number of times the common part is repeated. As demonstrated by Han et al., an FP-tree is a compact data structure that stores all necessary in-formation for frequent itemsets mining and it is usually much smaller than the database itself. Figure 3 shows the FP-tree for the first sliding window. Note that the items are stored in an inverse lexicographical order among the root-path. This arrangement makes it easy to explore the FP-tree.
 use the FP-tree to store all the transactions in the sliding window, so we do not prune infrequent items. Second, in addition to the head table in traditional FP-trees (which is used to record the starting pointers to each item), we also maintain another table, the tid table. In the tid table, for each tid (transaction id), there is a pointer pointing to a node in the FP-tree, which we call the node the tail of the transaction; the path from the tail to the root of the FP-tree gives us the itemset corresponding to the given tid. By using the FP-tree with the tid table, we do not need the transactions anymore.
 in the FP-tree and insert a new entry at the end of the tid table, where the pointer in the new entry points to the tail of the new transaction in the FP-tree; to delete a transaction from the sliding window, we pop an entry from the front of the tid be deleted. We then follow the path from the tail to the root of the FP-tree, and update the counters along the path. Notice that although the size of the tid table is the same as that of the sliding window ( N ), if we follow a first-in-first-out rule for updating the sliding window, most part of the tid table can be stored in disk, because we only update the front and the end of the tid table. 3.3.3 CET construction To build a CET, first we create a root node n  X  . Second, we create | | child nodes for n  X  (i.e., each i  X  corresponds to a child node n { i } ), and then we call Explore on each child node n { i } . Pseudo code for the Explore algorithm is given in Fig. 4 . order. For an itemset I , Explore consults the FP-tree to determine the support and tid sum of I . In lines 1 and 2 of Fig. 4 , if a node is found to be infrequent, then it is marked as an infrequent gateway node, and we do not explore it further (Property 1). However, the support and tid sum of an infrequent gateway node have to be stored because they will provide important information during a CET update when an infrequent itemset can potentially become frequent.
 other lexicographically smaller itemset, then n I is an unpromising gateway node. Based on Property 2, we do not explore n I  X  X  descendants, which does not contain any closed frequent itemsets. However, n I  X  X  support and tid sum must be stored, because during a CET update, n I may become promising. up the hash table to see if there exists a previously discovered closed itemset that has the same support as n I and which also subsumes I , and if so, it returns true (in this case n I is an unpromising gateway node); otherwise, it returns false (in this case n I is a promising node).
 its descendants (lines 6 X 10). After that, we can determine if n I is an intermediate node or a closed node (lines 11 X 15) according to Property 3.
 Complexity : The time complexity of the Explore algorithm depends on the size of the sliding window N , the minimum support, and the number of nodes in the CET. However, because Explore only visits those nodes that are necessary for discovering closed frequent itemsets, so Explore should have the same asymptotic time complexity as any closed frequent itemset mining algorithm that is based on traversing the enumeration tree. 3.4 Updating the CET New transactions are inserted into the window, as old transactions are deleted from the window. We discuss the maintenance of the CET for the two operations: addition and deletion. 3 3.4.1 Adding a transaction In Fig. 5 , a new transaction T ( tid 5) is added to the sliding window. We traverse the parts of the CET that are related to transaction T . For each related node n I , we update its support , tid sum , and possibly its node type. n  X  X  support and tid sum , and the cost is minimum. In the following, we discuss cases where the new transaction T causes n I to change its node type. n
I was an infrequent gateway node. If n I becomes frequent (e.g., from node D in Fig. 2 to node D in Fig. 5 ), two types of updates must be made. First, for each of n I  X  X  left siblings it must be checked if new children should be created. Second, the originally pruned branch (under n I ) must be re-explored by calling Explore . a frequent node, node A and C must be updated by adding new children ( AD and CD , respectively). Some of these new children will become new infrequent gateway nodes (e.g., node AD ), and others may become other types of nodes (e.g., node CD becomes a closed node). In addition, this update may propagate down more than one level. n
I was an unpromising gateway node. Node n I may become promising (e.g., from node AC in Fig. 2 to node AC in Fig. 5 ) for the following reason. Originally,  X  ( j  X  i max and j  X  I ) s.t. j occurs in each transaction that I occurs. However, if T contains I but not any of such j  X  X , then the above condition does not hold anymore. If this happens, the originally pruned branch (under n I )mustbe explored by calling Explore . n
I was a closed node. Based on the following property, n I will remain a closed node.
 Property 4 Adding a new transaction will not change a node from closed to non-sliding window.
 Proof Originally,  X  J  X  I , support ( J )&lt; support ( I ) ; after adding the new trans-action T ,  X  J  X  I ,if J  X  T then I  X  T . Therefore if J  X  X  support is increased by still holds after adding the new transaction T . However, if a closed node n I is vis-ited during an addition, its entry in the hash table will be updated. Its support is increased by 1 and its tid sum is increased by adding the tid of the new transaction. n
I was an intermediate node. An intermediate node, such as node A in Fig. 2 , can possibly become a closed node after adding a new transaction T . Originally, n
I was an intermediate node because one of n I  X  X  children has the same support as n
I does; if T contains I but none of n I  X  X  children who have the same support as n I had before the addition, then n I becomes a closed node because its new support is higher than the support of any of its children. However, n I cannot change to an infrequent gateway node or an unpromising gateway node. First, n I  X  X  support will not decrease because of adding T , so it cannot become infrequent. Second, if in each transaction that I occurs; this statement will not change after we add T . Therefore, leftcheck ( n I ) = false after the addition.
 transaction to the sliding window will trigger a call of Addition on n  X  , the root of the CET.
 we can easily derive the following property of Addition : Property 5 The Addition algorithm will not decrease the number of nodes in a CET. 3.4.2 Deleting a transaction In Fig. 7 , an old transaction T ( tid 1) is deleted from the sliding window. To delete a transaction, we also traverse the parts of the CET that is related to the deleted transaction. Most likely, n I  X  X  node type will not change, in which case, we simply update n I  X  X  support and tid sum , and the cost is minimum. In the following, we discuss the impact of deletion in detail.
 n  X  X  node type. If n I was an unpromising gateway node, deletion may change n I to infrequent but will not change n I to promising, for the following reason. For an unpromising gateway node n I , if before deletion, leftcheck ( n I ) = true , then  X  ( j  X  i max and j  X  I ) s.t. j occurs in each transaction that I occurs; this statement remains true when we delete a transaction.
 of its support, in which case, all n I  X  X  descendants are pruned and n I becomes an infrequent gateway node. In addition, all of n I  X  X  left siblings are updated by removing children obtained from joining with n I . For example in Fig. 7 ,when transaction T ( tid 1) is removed from the window, node D becomes infrequent. We prune all descendants of node D ,aswellas AD and CD , which were obtained by joining A and C with D , respectively.
 tion, for the following reason. If before the deletion,  X  ( j  X  i max and j  X  I ) s.t. j occurs in each transaction that I occurs, except only for the transaction to be deleted, then after deleting the transaction, I becomes unpromising. This happens to node C in Fig. 7 . Therefore, if originally n I was neither infrequent nor un-promising, then we have to do the leftcheck on n I . From the above discussion we can also see that for a node n I to change to unpromising because of a deletion, n I must be contained in the deleted transaction. Therefore n I will be visited by the traversal and we will not miss it.
 delete another transaction T ( tid 2) from the sliding window. Figure 8 shows this example where previously closed node n I (e.g. A and AB ) become non-closed because of the deletion. This can be determined by looking at the supports of the children of n I after visiting them. If a previously closed node that is included in the deleted transaction remains closed after the deletion, we still need to update its entry in the hash table: its support is decreased by 1 and its tid sum is decreased by subtracting the tid of the deleted transaction.
 operation on a CET.
 Property 6 Deleting an old transaction will not change a node in the CET from non-closed to closed, and therefore it will not increase the number of closed item-sets in the sliding window.
 Proof If an itemset I was originally non-closed, then before the deletion,  X  j  X  I s . t . j occurs in each transaction that I occurs. Obviously, this fact will not be changed due to deleting a transaction. So I will still be non-closed after the deletion.
 are skipped in the description. For example, when pruning a branch from the CET, all the closed frequent itemsets in the branch should be removed from the hash table.
 we can easily derive the following property of Deletion : Property 7 The Deletion algorithm will not increase the number of nodes in a CET.
 4 Discussion on the Moment algorithm In this section, we discuss some properties and extensions of the Moment algo-rithm. 4.1 Discussion on CET updates In the addition algorithm, Explore is the most time consuming operation, because it scans the transactions stored in the FP-tree. However, as will be demonstrated in the experiments, the number of such invocations is very small, as most inser-tions will not change node types. In addition, the new branches grown by calling Explore are usually very small subsets of the whole CET, therefore such incre-mental growing takes much less time than regenerating the whole CET. On the other hand, deletion only involves related nodes in the CET, and does not scan transactions stored in the FP-tree. Therefore, its time complexity is at most linear to the number of nodes. Usually it is faster to perform a deletion than an addition. promising/unpromising), then I is in the added or deleted transaction and there-fore n I is guaranteed to be visited during the update. Consequently, our algorithm will correctly maintain the current close frequent itemsets after any of the two operations. Furthermore, if n I remains closed after an addition or a deletion and I is contained in the added/deleted transaction, then its position in the hash ta-ble is changed because its support and tid sum are changed. To make the update, we delete the itemset from the hash table and re-insert it back to the hash table based on the new key value. However, such an update has amortized constant time complexity. 4.2 Variable sliding window size In our discussion so far, we used sliding windows of fixed size. However, the two operations  X  addition and deletion  X  are independent of each other. Therefore, if needed, the size for the sliding window can grow or shrink without affecting the correctness of our algorithm. However, there is a subtle issue when the size of the sliding window is not fixed. For the discussion so far, we have used an absolute support , which is the number of transactions in which an itemset occurs. In most applications, relative support , which is the fraction of transactions in which an itemset occurs (i.e., the f in Sect. 2.1 ), is more commonly used. We have chosen to use absolute support in the Moment algorithm for a practical reason: when up-dating the CET, instead of all the CET nodes, only those nodes related to the added or deleted transactions are visited, and therefore, Moment can achieve quick re-sponse time. When the size of the sliding window is fixed, the Moment algorithm works for both the absolute and relative supports because they are equivalent. However, when the size of the sliding window varies, if we chose to use the rel-ative support, Moment will lose its advantage because an itemset may change its status (in terms of frequent/infrequent and closed/not closed) even if it is not con-tained in the added or deleted transaction. How to handle relative support under variable sliding window size is among our future work on improving the Moment algorithm. 4.3 Approximate algorithms The Moment algorithm is deterministic. However, our algorithm does not restrict a deletion to happen at the end of the window: at any given time, any transac-tion in the sliding window can be removed. As a result, with minor changes to the Moment algorithm, we can implement different approximate algorithms. For one example, when a new transaction arrives, we can use random sampling with a reservoir [ 19 ] to decide whether to insert the new transaction into the sliding window and if so, which old transaction to remove from the sliding window. As a consequence, the sliding window always contains a set of uniformly selected sam-ples from the data seen so far. As another example, if when removing a transac-tion, the transaction to be removed is picked with the following random scheme: the newer transactions have lower probability of being removed than the older ones, then our algorithm can implement a sliding window with soft boundary, i.e., the more recent the transaction, the higher chance it will remain in the sliding window. 4.4 Streaming output From the description of the Moment algorithm (Figs. 6 and 9 ) we can see that when adding or deleting a transaction, only those CET nodes related to the update will be visited and changed. One consequence of this design, as we mentioned before, is the efficiency of the algorithm. In addition, another consequence is that the algorithm can directly output those itemsets that have changed status due to the update. For example, the Moment algorithm can output the itemsets that have become newly closed frequent and the itemsets that have ceased to have that prop-erty due to the most recent update. These itemsets can be output as data streams and be sent to other data analysis processes. 4.5 Lazy pruning In the Moment algorithm, to reduce memory usage, all descendants of an infre-quent gateway node or an unpromising gateway node are pruned from the CET. In some cases, however, this design is not time efficient. For example, if the newly added transaction is exactly the same as the just deleted transaction, then all the CET nodes pruned due to the previous deletion must be regrown because of the addition, and we know growing nodes (by calling explor() ) is time consuming. A possible method to alleviate this situation is to use lazy pruning. That is, instead of physically pruning a node from CET, we prune it logically by using a flag to indicate that the node is no longer a part of the CET. If this node has to be regrown later, we can simply change the flag instead of calling explor() . Of course, such a solution has its own problems. On the one hand, because of combinatoric explo-sion, it is impossible to maintain all infrequent nodes, and therefore we have to make a decision on which nodes are to be physically pruned and which nodes are to be logically pruned. On the other hand, for a logically pruned node to be useful in the future, the information on its support must be maintained. That is, during an update, in addition to the CET nodes that are related to the added/deleted trans-action, we have to visit the logically deleted nodes related to the transaction to update their supports. Currently, we are investigating on how to add this extension to the Moment algorithm. 4.6 Batch updates So far our algorithm only handles one transaction in one update. There are sit-uations in which data are bursty and multiple transactions need to be added and deleted during one update. However, it is not difficult to adapt our algorithm to handle multiple transactions in one update. Originally, for an addition or a dele-tion, we traverse the CET with the single added or deleted transaction; if an update contains a batch of transactions, we can still traverse the CET in the same fashion using the batch of transactions and project out unrelated transactions along the traversal. 5 Related work Incrementally mining frequent itemsets has been investigated by many re-searchers. Manku et al. [ 13 ] developed a randomized algorithm, the Sticky Sam-pling Algorithm, and a deterministic algorithm, the Lossy Counting Algorithm, for maintaining frequent items over a data stream where for a given time t ,the frequent items are defined over the entire data stream up to t . The algorithms guarantee no false negative and a bound on the error of estimated frequency (the guarantees are in a probabilistic sense for the randomized algorithm). The Lossy Counting Algorithm is extended to handle frequent itemsets ,whereatrieisusedto maintain all frequent itemsets and the trie is updated by batches of transactions in the data stream. The main differences between Manku X  X  algorithms and our Mo-ment algorithm are (1) in Manku X  X  algorithms, the frequent items (and itemsets) are defined over the whole data stream while in Moment the frequent itemsets are defined over a sliding window to reflect the most recent trends, (2) Manku X  X  algorithms are approximate algorithms where the support of an itemset is only guaranteed to be within a range while in Moment, the support is exact, and (3) Manku X  X  algorithms strive for a tunable compromise between memory usage and error bounds while in Moment, quick response time is the main goal.
 updating frequent itemsets. Thomas et al. [ 17 ] presented a similar algorithm. Both Cheung X  X  and Thomas X  X  algorithms assume batch updates and take advantage of the relationship between the original database ( DB ) and the incrementally changed transactions ( db ). FUP is similar to the well-known Apriori Algorithm, which is a multiple-step algorithm. The key observation of FUP is that by adding db to DB , some previously frequent itemsets will remain frequent and some previously infrequent itemsets will become frequent (these itemsets are called winners ); at the same time, some previously frequent itemsets will become infrequent (these itemsets are called losers ). The key technique of FUP is to use information in db to filter out some winners and losers, and therefore reduce the size of candidate set in the Apriori algorithm. Because the performance of the Apriori algorithm re-lies heavily on the size of candidate set, FUP improves the performance of Apriori greatly. FUP 2 extended FUP by allowing deleting old transactions from a database as well. The algorithm proposed by Thomas et al. is similar to FUP 2 except that in addition to frequent itemsets, a negative border [ 14 ] is maintained. In the algo-rithm, the frequent itemsets in db are mined first. At the same time, the counts of frequent itemsets (and itemsets on the negative border) in DB are updated. Then based on the change of the frequent itemsets in DB , the negative border in DB ,and the frequent itemsets in db , the frequent itemsets in the updated database are com-puted with a possible scan of the updated database. Because the updated database is scanned atmost once, Thomas X  X  algorithm has a very good performance rate. Cheung X  X  and Thomas X  X  algorithms are different from our Moment algorithm in several ways. First, in Cheung X  X  and Thomas X  X  algorithms, the update is assumed to be a batch file db and if the size of db is too small, the algorithm will not work because in such a case, almost all itemsets in db will be frequent (as an example consider the case when | db |= 2and minsup = 50%). We argue that in data stream applications, updates are high speed and users are often interested in the new frequent itemsets in real time. Accumulating a batch file db before running the algorithm will prolong the response time and is not appropriate for data stream applications. Second, in both Cheung X  X  and Thomas X  X  algorithms, for one updates, all frequent itemsets need to be updated: in Cheung X  X  algorithms, although some candidates are pruned by the algorithms, the Apriori algorithm is run once for each update; in Thomas X  X  algorithm, for each update the whole negative border needs to be regenerated from scratch. In contrast, in our Moment algorithm, only the part of the boundary that is related to the change needs to be updated. sets in evolving databases. Later, Otey et al. [ 15 ] extended ZIGZAG into parallel and distributed algorithms. ZIGZAG is similar to Cheung X  X  and Thomas X  X  algo-rithms in that it achieves its speedup by using the relationship between DB and db . However, ZIGZAG has many distinct features. First, ZIGZAG mainly used db to speedup the support counting of frequent itemsets in the updated database and it does not discover the frequent itemsets in db itself. As a result, for a given mini-mum support, ZIGZAG can handle batch update with arbitrary block size. Second, ZIGZAG adapts the techniques proposed in the GENMAX algorithm [ 10 ]andin each update only maintains maximal frequent itemsets. Because the information on maximal frequent itemsets and their supports is not enough to generate as-sociation rules (because the support information of some non-maximal frequent itemsets may be missing), a second step is used in ZIGZAG in which the updated database is scanned to discover all frequent itemsets and their supports. In our ex-perimental study, we will compare the performance of our algorithm with that of ZIGZAG.
 items whose frequencies satisfy a threshold with high probabilities. Teng et al. [ 16 ] presented an algorithm, FTP-DS, that mines frequent temporal patterns from mines recent frequent itemsets where the frequency is defined by an aging func-tion. Giannella et al. [ 9 ] proposed an approximate algorithm for mining frequent itemsets in data streams during arbitrary time intervals. An in-memory data struc-ture, FP-stream , is used to store and update historic information about frequent itemsets and their frequency over time and an aging function is used to update the entries so that more recent entries are weighted more. Asai et al. [ 3 ] presented an online algorithm, StreamT , for mining frequent rooted ordered trees. To reduce the number of subtrees to be maintained, an update policy that is similar to that in online association rule mining [ 12 ] was used and therefore the results are inexact. In all these studies, approximate algorithms were used. In contrast, our algorithm is an exact one. On the other hand, we can also assume that an approximation step has been implemented through the sampling scheme and our exact algorithm works on a sliding window containing the random samples (which are a synopsis of the data stream).
 algorithm is that it only mines and maintains closed frequent itemsets while all above algorithms focused on mining all frequent itemsets. The large number of frequent itemsets makes it impractical to maintain information about all frequent itemsets using in-memory data structures. As demonstrated by extensive experi-mental studies, e.g., [ 21 ], there are usually much fewer closed frequent itemsets compared to the total number of frequent itemsets. As a consequence, our algo-rithm has better performance in terms of memory usage and running time, as is demonstrated in the experimental studies. 6 Experimental results We performed extensive experiments to evaluate the performance of Moment al-gorithm. We use Charm, a state-of-the-art algorithm proposed by Zaki et al. [ 21 ], as the baseline algorithm to generate closed frequent itemsets without using in-cremental updates. We have used the latest version of Charm. As demonstrated in many studies (e.g., [ 21 , 22 ]), among the algorithms that mine closed frequent itemsets, Charm has best performance for various data sets. We also compare the performance of Moment with that of ZIGZAG, a recently proposed algorithm on incrementally mining frequent itemsets [ 18 ]. All our experiments were done on a 2 GHz Intel Pentium IV PC with 2 GB main memory, running RedHat Linux 7.3 operating system.
 world data sets. The synthetic data sets are generated using the IBM synthetic data generator for association rules. The first three real-world data sets were used for KDDCUP 2000 [ 22 ]. Among them, the first two, BMS-WebView-1 and BMS-WebView-2, record several months of clickstream data from two e-commerce web sites; the third one, BMS-POS, contains several years of point-of-sale data from a large electronics retailer [ 22 ]. The fourth real-world data set, Mushroom, has been used extensively in the AI area. The last two real-world data sets, WCup and WPortal, are kindly provided by Matthew Eric Otey [ 15 ]. Among them, WCup is derived from the click-stream data of the official 1998 World Soccer Cup web site, and WPortal is derived from a large web portal in Brazil. The data characteristics for all the data sets are summarized in Table 1 . 6.1 Synthetic data sets The synthetic data sets are generated using the synthetic data generator developed byAgrawaletal.[ 2 ]. Data from this generator mimics transactions from retail stores. Here are some of the parameters that we have controlled: the size of the maximal potentially frequent itemsets I . 6.1.1 Performance under different sliding window sizes In the first experiment, we compare Moment and Charm under different slid-T20I4N10 K X 100 K, we have set the parameters as T = 20, I = 4; in the second one, T40I10N10 K X 100 K, we have set the parameters as T = 40, I = 10. In both data sets, we let the sliding window size N grow from 10 K to 100 K. Be-cause in Moment, one update consists of adding a new transaction to and deleting an old transaction from a sliding window, each sliding window differs from the previous one by exactly one transaction. That is, for example when the sliding window size is 10,000, the first sliding window contains transactions 1 X 10,000, the second sliding window contains transactions 2 X 10,001, and so on. In the ex-perimental results, for both algorithms, we report the average running time over 100 consecutive sliding windows.
 all closed frequent itemsets for Charm grows in a linear fashion. In contrast, the running time of Moment does not change too much with the sliding window size. This result demonstrates an advantage of the Moment algorithm: because of its incremental updating fashion, it is not sensitive to the sliding window size.
 mentally, the information about the first sliding window should be available. In Fig. 11 a we compare the time for Moment to bulk-load the first sliding win-dow (by calling Explore() ) and the time for Charm to mine the closed frequent itemsets in the first sliding window. As can be seen from the figure, for getting the results in the first sliding window, Charm is faster by 5 X 10 times. There are which is heavily optimized for large set operations (e.g., by using the diffset tech-niques); second, Moment has extra data structures to maintain (e.g., creating the CET nodes, update their support and tid sum, etc.). However, we argue that this comparison is not fair  X  Moment is an incremental algorithm and the bulk-loading should not be used at all. That is, because data streams are unbounded, there is cial scenarios in which bulk-loading is needed. For example, after a web server had been shut down (e.g., for the purpose of database maintenance), it has just started to accept new customer requests. In such a scenario, we have to accumu-late enough transactions to fill in the first sliding window before the algorithm can start working incrementally. In such a case, Moment can actually output cur-rent frequent itemsets as soon as new customer requests arrive, even before the the following experiment: originally, the sliding window is empty, then transac-tions are added one by one until the sliding window is full. As new transactions added, Moment outputs the current closed frequent itemsets even though the slid-ing window is not completely full. We have done this experiment under different sliding window sizes (10 X 100K), and in Fig. 11 b we report the average time for adding each new transaction, where the time includes the time for updating the FP-tree and that for updating the CET. As we can see from the figure, the aver-age time per transaction is very small and it is not very sensitive to the window size. 6.1.2 Performance under different minimum support In the second experiment, we compare the performance of Moment and Charm under different minimum supports. The data set we have used, T 20 I 10 N 100 K , has the following parameters: T = 20, I = 10, N = 100K. We let the minimum support decrease from 1 to 0.1%.
 consecutive sliding windows and reported the average performance over these 100 sliding windows. Figure 12 shows the average running time for Moment and for Charm under different minimum supports for two different sliding window sizes, 2K and 100K. As can be seen from the figure, as minimum support decreases, because the number of closed frequent itemsets increases, the running time for both algorithms grows. However, the running time of Moment is faster than that of Charm by more than an order of magnitude under all the minimum supports. ports with the sliding window size of 100K. In addition, in the table we show some static and dynamic statistics about the CET data structure. All reported data are average values taken over the 100 sliding windows. The first three columns show the minimum support, the number of closed itemsets, and the number of nodes in the CET. From the table we can see that as the minimum support decreases, the number of closed itemsets grows rapidly. So does the number of nodes in the CET. However, the ratio between the number of nodes in the CET and the number of closed itemsets (which is shown in Fig. 13 ) actually decreases as the minimum support decreases. This implies that as the sizes of the CET grows larger, it be-comes more efficient and the size of the CET is bounded by the number of closed itemsets times a constant number.
 study how many nodes change their status from infrequent/unpromising to fre-quent/promising (column 4) and how many new nodes are created due to the ad-dition (column 5). From the data we can see that during an addition, the average number of nodes that change from infrequent to frequent or from unpromising to promising in the CET is very small relative to the total number of nodes in the CET. Similarly, the number of new nodes created due to an addition is also very small. These results verify the postulation behind our algorithm: that an update usually only affects the status of a very small portion of the CET and the new branches grown because of an update is usually a very small subset of the CET. In addition, we have reported the average number of CET nodes deleted due to a deletion (column 6). It can be seen that this number is in about the same scale as that of added nodes. However, because a deletion does not query the FP-tree and does not grow the CET, it is a relatively inexpensive operation and therefore will not affect the performance too much. 6.2 Real-world data sets We have used four real-world data sets to compare the performance of Mo-ment with that of Charm. The first three data sets are BMS-WebView-1, BMS-WebView-2, and BMS-POS. Our forth real-world data set is the Mushroom data set used by Zaki et al. [ 21 ] and it belongs to the family of  X  X ense X  data, where there exists strong correlation among transactions. The data characteristics for the 4 data sets are summarized in Table 1 .
 small one and a large one. We set the small window size to be 2K for all data sets. The large window-size is different for different data sets, because the number of available transactions in each data set is different. (Other than these two sliding-window sizes, we have also tested various window sizes in between and got similar results.) Figures 14  X  17 show the average running time of Moment and Charm for the four data sets, under different minimum supports. From the figure we can see that Moment outperforms Charm by one or two orders of magnitude for all the data sets under a range of values for minimum support and sliding-window size. These results show that Moment has a good performance rate in real-world data sets of various characteristics (sparse or dense data, large or small sliding-window size, large or small minimum support). 6.3 Comparison with ZIGZAG In this section, we compare the performance of Moment with that of an incre-mental frequent itemset mining algorithm. Among all the incremental frequent itemset mining algorithms introduced in the section of related work, we were only able to obtain a version of the ZIGZAG algorithm that supports only addition (no deletion). Even so, we believe this version of ZIGZAG is representative for the following reasons. First, ZIGZAG is a relatively new algorithm with good performance. For example, in [ 18 ], the authors showed that ZIGZAG significantly outperformed Thomas X  X  algorithm. In addition, Thomas X  X  algorithm has similar performance to Cheung X  X  FUP and FUP 2 algorithms. Second, an update in ZIGZAG can be considered as an addition followed by a deletion. So the running time of the addition-only version of ZIGZAG can be considered as a lower bound of that of the general version of ZIGZAG.
 comparable with Moment, because Moment strives for quick response time for updating one transaction. Assume for example, that Moment were 100 times faster than a non-incremental frequent itemset mining algorithm in terms of time to up-date one transaction. In such a case, if users do not mind some delay, they can simply wait until 100 new transactions have been obtained and then apply the non-incremental algorithm to the updated database, which will give them running time similar to that of Moment. Even better, they can apply incremental frequent itemset mining algorithm such as ZIGZAG to get better performance. Therefore, in this section we mainly compare Moment with ZIGZAG in terms of updating one transaction.
 provided by Matthew Eric Otey. These two data sets were used by Otey et al. [ 15 ] in the performance study of ZIGZAG. The characteristics of the two data sets are described in Table 1 .Table 3 gives the running time (in seconds) for Moment and ZIGZAG for the WCup and the WPortal data sets. For each data set, we tested two sliding window sizes, 2K and 100K. For WCup, we have set the minimum support to 0.5%, and for WPortal, 0.1% (for lower minimum support, ZIGZAG will exhaust all available memory). As can be seen from Table 3 , for an update consisting of only one transaction (the bold line in the table), Moment outper-forms ZIGZAG by orders of magnitude. However, as the batch update size (i.e., the number of transactions in a batch update) grows larger, the running time of ZIGZAG does not change very much. For example, for the WCup data set with the slide window size to be 2K, on average the total time for Moment to make 100 consecutive updates (each consisting of one transaction) is 2.67 s, while the time for ZIGZAG to make one batch update (consisting of 100 transactions) is 1.10 s. These results show that Moment has its limitation: when users are mainly concerned about quick response time for the update of each transaction, Moment is beneficial; however, for a batch update with large batch size, an incremental mining algorithm that can directly handle batch updates is a better choice. 6.4 The number of CET nodes One design consideration for Moment is to maintain in CET only information related to closed frequent itemsets, instead of all frequent itemsets. In this section, we use real data sets to justify this consideration.
 itemsets, and the number of CET nodes for two data sets. Figure 18 ashowsthese numbers for the BMS-WebView-1 data set and Fig. 18 b shows these numbers for the Mushroom data set under different minimum supports. As can be seen from Fig. 18 a, because BMS-WebView-1 is a relatively sparse data set, under high minimum supports, the number of closed frequent itemsets and that of all frequent itemsets do not have much difference; however, when the minimum sup-port decreases further, as some large itemsets become frequent, the total number of frequent itemsets blows up dramatically; in contrast, the number of CET nodes The Mushroom data set, in comparison, is relatively dense, and therefore, even at high minimum support, there are much more frequent itemsets than closed frequent itemsets. As shown in Fig. 18 b, although the number of CET nodes is about one order of magnitude more than the number of closed frequent itemsets, itemsets. 7Conclusion In this paper we propose a novel algorithm, Moment, to discover and maintain samples in a data stream. In the Moment algorithm, an efficient in-memory data structure, the closed enumeration tree (CET), is used to record all closed frequent itemsets in the current sliding window. In addition, CET also monitors the item-sets that form the boundary between closed frequent itemsets and the rest of the itemsets. We have also developed efficient algorithms to incrementally update the CET when newly-arrived transactions change the content of the sliding window. Experimental studies show that the running time of the Moment algorithm is not sensitive to the sliding window size and Moment outperforms a state-of-the-art al-gorithm that mines closed frequent itemsets without using incremental updates. In addition, the number of CET nodes is shown to be proportional to that of closed frequent itemsets. Under low minimum supports or when applied to dense data sets, CET has much fewer number of nodes than the total number of frequent itemsets.
 References
