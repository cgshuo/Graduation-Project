 to the use of machine learning and adaptive signal processin g techniques (see [9] and references therein) and novel dry electrodes [18]. Initial BCI systems were based on operant conditioning and could easily require months of training on the subject si de before it was possible to use them [1, 10]. Second generation BCI systems require to record a br ief calibration session during which a subject assumes a fixed number of brain states, say, movemen t imagination and after which the subject-specific spatio-temporal filters (e.g. [6]) are inf erred along with individualized classifiers [9]. Recently, first steps to transfer a BCI user X  X  filters and classifiers between sessions was studied [14] and a further online-study confirmed that indeed such tr ansfer is possible without significant performance loss [16]. In the present paper we even will go on e step further in this spirit and propose a subject-independent zero-training BCI that enables both experienced and novice BCI subjects to use BCI immediately without calibration.
 in order to optimally construct such one-size-fits-all clas sifiers from a vast number of redundant features, here a large filter bank available from 83 BCI users . The use of sparsifying techniques specifically tell us what are the interesting aspects in EEG t hat are predictive to future BCI users. As expected, we find that a distribution of different alpha band features in combination with a number of characteristic common spatial patterns (CSPs) is highly pr edictive for all users. What is found as the outcome of a machine learning experiment can also be viewed a s a compact quantitative description best subjects that characterize the variance necessary for a subject independent algorithm, rather the spread over existing physiology is to be represented concis ely. Clearly, our proceedure may also be of use appart from BCI in other scientific fields, where comple x characteristic features need to be homogenized into one overall inference model.
 outlined, consisting of the procedure for building the filte rs, the classifiers and the gating function, where we apply various machine learning methods. Interesti ngly we are able to successfully classify trials of novel subjects with zero training suffering only a small loss in performance. Finally we put our results into perspective. We used 83 BCI datasets (sessions), each consisting of 150 tr ials from 83 individual subjects. Each trial consists of one of two predefined movement imagination s, being left and right hand, i.e. data was chosen such that it relies only on these 2 classes, althou gh originally three classes were cued dur-ing the calibration session, being left hand (L), right hand (R) and foot (F). 45 EEG channels, which are in accordance with the 10-20 system, were identified to be common in all sessions considered. The data were recorded while subjects were immobile, seated on a comfortable chair with arm rests. The cues for performing a movement imagination were given by visual stimuli, and occurred every 4.5-6 seconds in random order. Each trial was referenced by a 3 second long time-window starting at 500 msec after the presentation of the cue. Individual exp eriments consisted of three different training paradigms. The first two training paradigms consis ted of visual cues in form of a letter or an arrow, respectively. In the third training paradigm the s ubject was instructed to follow a moving target on the screen. Within this target the edges lit up to in dicate the type of movement imagination required. The experimental proceedure was designed to clos ely follow [3]. Electromyogram (EMG) on both forearms and the foot were recorded as well as electro oculogram (EOG) to ensure there were no real movements of the arms and that the movements of th e eyes were not correlated to the required mental tasks. The ensemble consists of a large redundant set of subject-de pendent common spatial pattern fil-ters (CSP cf. [6]) and their matching classifiers (LDA). Each dataset is first preprocessed by 18 predefined temporal filters (i.e. band-pass filters) in paral lel (see upper panel of Figure 1). A cor-responding spatial filter and linear classifier is obtained f or every dataset and temporal filter. Each resulting CSP-LDA couple can be interpreted as a potential b asis function. Finding an appropriate weighting for the classifier outputs of these basis function s is of paramount importance for the ac-curate prediction. We employed different forms of regressi on and classification in order to find an optimal weighting for predicting the movement imagination data of unseen subjects[2, 4]. This pro-cessing was done by leave-one-subject-out cross-validati on, i.e. the session of a particular subject was removed, the algorithm trained on the remaining trials ( of the other subjects) and then applied to this subject X  X  data (see lower panel of Figure 1). 3.1 Temporal Filters The  X  -rhythm (9-14 Hz) and synchronized components in the  X  -band (16-22 Hz) are macroscopic idle rhythms that prevail over the postcentral somatosenso ry cortex and precentral motor cortex, when a given subject is at rest. Imaginations of movements as well as actual movements are known to suppress these idle rhythms contralaterally. However, t here are not only subject-specific differ-ences of the most discriminative frequency range of the ment ioned idle-rhythms, but also session differences thereof.
 We identified 18 neurophysiologically relevant temporal fil ters, of which 12 lie within the  X  -band, 3 in the  X  -band, two in between  X  -and  X  -band and one broadband 7  X  30 Hz. In all following performance related tables we used the percentage of miscla ssified trials, or 0-1 loss. 3.2 Spatial Filters and Classifiers )synchronization (ERD/ERS), and is considered to be the gol d-standard of ERD-based BCI systems [13, 19, 6]. The CSP algorithm maximizes the variance of righ t hand trials, while simultaneously minimizing the variance for left hand trials. Given the two c ovariance matrices  X  channels x concatenated timepoints , the CSP algorithm returns the matrices W and D . W is a ma-Figure 1: 2 Flowcharts of the ensemble method. The red patche s in the top panel illustrate the inactive nodes of the ensemble after sparsification. trix of projections, where the i-th row has a relative varian ce of d variance of 1  X  d the number of channels: Best discrimination is provided by filters with very high (em phazising one class) or very low eigen-values (emphazising the other class), we therefore chose to only include projections with the highest 2 and corresponding lowest 2 eigenvalues for our analysis. W e use Linear Discriminant Analysis (LDA) [5], each time filtered session corresponds to a CSP set and to a matched LDA. 3.3 Final gating function The final gating function combines the outputs of the individ ual ensemble members to a single one. This can be realized in many ways. For a number of ensemble met hods the mean has proven to be a surprisingly good choice [17]. As a baseline for our ensemb le we simply averaged all outputs of our individual classifiers. This result is given as mean in Table 2.
 Classification We employ various classification methods such as k Nearest Ne ighbor (kNN), Linear Discriminant Analysis (LDA), Support Vector Machine (SVM) and a Linear Programming Machine (LPM) [12].
 Quadratic regression with ` where c session j by applying the bandpass filter i , B is the number of frequency bands, S the complete set Figure 2: Feature selection during cross-validation: whit e dashes mark the features kept after regu-larization for the prediction of the data of each subject. Th e numbers on the vertical axis represent the subject index as well as the Error Rate (%). The red line de picts the baseline error of individual subjects (classical auto-band CSP). Features as well as bas eline errors are sorted by the error mag-nitude of the self-prediction. Note that some of the feature s are useful in predicting the data of most other subjects, while some are rarely or never used. of sessions, X the complete data set, S k , The hyperparameter  X  in equation (2) was varied on a logarithmic scale and multipl ied by a dataset scaling factor which accounted for fluctuations in voting po pulation distribution and size for each subject. The dataset scaling factor is computed using c efficiency reasons the hyperparameter was tuned on a small ra ndom subset of subjects whose labels are to be predicted from data obtained from other subjects su ch that the resulting test/train error ratio was minimal, which in turn affected the choice (leave i n/out) of classifiers among the 83x18 candidates. The ` results (in terms of feature sparsification) shown in Figure 2. In fact the exemplary CSP patterns shown in the lower part of the Figure exhibit neurophysiolog ically meaningful activation in motor-cortical areas. The most predictive subjects show smooth mo nopolar patterns, while subjects with a higher self-prediction loss slowly move from dipolar to rat her ragged maps. From the point of view of approximation even the latter make sense for capturing th e overall ensemble variance. The implementation of the regressions were performed using CVX , a package for specifying and solving convex programs [11]. We coupled an ` scheme ensemble.
 Least Squares Regression Is a special case of equation (2), with  X  = 0 . 3.4 Validation The subject-specific CSP-based classification methods with automatically, subject-dependent tuned temporal filters (termed reference methods) are validated b y an 8-fold cross-validation, splitting the data chronologically. The chronological splitting for cro ss-validation is a common practice in EEG classification, since the non-stationarity of the data is th us preserved [9].
 To validate the quality of the ensemble learning we employed a leave-one-subject out cross-validation (LOSO-CV) procedure, i.e. for predicting the la bels of a particular subject we only use data from other subjects. Overall performance of the reference methods, other baseli ne methods and of the ensemble method is presented in Table 2. Reference method performances of su bject-specific CSP-based classification are presented with heuristically tuned frequency bands [6] . Furthermore we considered much sim-pler (zero-training) methods as a control. Laplacian stand s for the power difference in two Laplace filtered channels (C3 vs. C4) and simple band-power stands fo r the power difference of the same two approach machine learning classical method mean kNN LDA LPM SVM LSR LSR-` # &lt; 25% 31 30 18 14 29 19 36 24 11 39 25%-tile 17.3 17.3 27.3 26.7 18.7 26.0 16.0 22.0 31.3 11.9 median 30.7 31.3 36.0 37.3 28.7 36.7 29.3 34.7 38.7 25.9 75%-tile 41.3 42.0 43.3 44.0 41.3 44.0 40.7 45.3 45.3 41.4 channels without any spatial filtering. For the simple zero-training methods we chose a broad-band level. The bias b in equation (3) can be tuned broadly for all sessions or corre cted individually by session, and implemented for online experiments in multipl e ways [16, 20, 15]. In our case we chose to adapt b without label information, but operating under the assumpt ion that class frequency is bal-anced. We therefore simply subtracted the mean over all tria ls of a given session. Table 1 shows a comparison of the various classification schemes. We evalu ate the performance on a given per-centage of the training data in order to observe information gain as a function of datapoints. Clearly the two best ML techniques are on par with subject-dependent CSP classifiers and outperform the simple zero-training methods (not shown in Table 1 but in Tab le 2) by far. While SVM scores the best median loss over all subjects (see Table 1), L1 regulari zed regression scored better results for well performing BCI subjects (Figure 3 column 1, row 3). In Fi gure 3 and Table 2 we furthermore show the results of the L1 regularized regression and SVM ver sus the auto-band reference method (zero-training versus subject-dependent training) as wel l as vs. the simple zero-training methods Laplace and band-power. Figure 4 shows all individual tempo ral filters used to generate the ensem-ble, where their color codes for the frequency they were used to predict labels of previously unseen data. As to be expected mostly  X  -band related temporal filters were selected. Contrary to wh at one may expect, features that generalize well to other subjects  X  data do not exclusively come from BCI subjects with low self-prediction errors (see white dashes in Figure 2), in fact there are some features of weak performing subjects that are necessary to capture al l variance of the ensemble. However there is a strong correlation between subjects with a low sel f-prediction loss and the generalizability of their features to predicting other subjects, as can be see n on the right part of Figure 4. 4.1 Focusing on a particular subject subject. We chose to use the subject with the lowest referenc e method cross-validation error (10%). Given the non-linearity in the band-power estimation (see F igure 1) it is impossible to picture the resulting ensemble spatial filter exactly. However, by aver aging the chosen CSP filters with the weightings, obtained by the ensemble and multiplying them b y their LDA classifier weight, we get an approximation: where w and (3), W (B in Figure 5). For the case of classical auto-band CSP this s imply reduces to P Figure 5). Figure 3: Compares the two best-scoring machine learning me thods ` Support Vector Machine to subject-dependent CSP and other s imple zero-training approaches. The axis show the classification loss in percent. Figure 4: On the left: The used temporal filters and in color-c ode their contribution to the final L1 regularized regression classification (the scale is normal ized from 0 to 1). Cleary  X  -band temporal filters between 10  X  13 Hz are most predictive. On the right: Number of features used vs. self-predicted cross-validation. A high self-prediction can be seen to yield a large number of features that are predictable for the whole ensemble. Another way to exemplify the ensemble performance is to refe r to a transfer function. By inject-ing a sinusoid with a frequency within the corresponding ban d-pass filter into a given channel and processing it by the four CSP filters, estimating the bandpow er of the resulting signal and finally combining the four outputs by the LDA classifier, we obtain a r esponse for the particular channel, where the sinusoid was injected. Repeating this procedure f or each channel results in a response matrix. This procedure can be applied for a single CSP/LDA pa ir, however we may also repeat the given method for as many times as features were chosen for a given subject by the ensemble and hence obtain an accurate description of how the ensemble processes the given EEG data. The resulting response matrices are displayed in panel C of Figu re 5. While the subject-specific pattern (classical) looks less focused and more diverse the general pattern matches the one obtained by the ensemble. A third way of visualizing how the ensemble works, we show the primary projections of the CSP filters that were given the 6 highest weights by the e nsemble on the left panel (F) and the distribution of all weights in panel D. The spatial posit ions of highest channel weightings differ slightly for each of the CSP filters given, however the maxima of the projection matrices are clearly positioned around the primary motor cortex. On the path of bringing BCI technology from the lab to a more pr actical every day situation, it be-comes indispensable to reduce the setup time which is nowada ys more than one hour towards less than one minute. While dry electrodes provide a first step to a void the time for placing the cap, cali-bration still remained and it is here where we contribute by d ispensing with calibration sessions. Our present study is an offline analysis providing a positive ans wer to the question whether a subject in-dependent classifier could become reality for a BCI-na  X  X ve user. We hav e taken great care in this work to exclude data from a given subject when predicting his/her performance by using the previously described LOSOCV. In contrast with previous work on ensembl e approaches to BCI classification based on simple majority voting and Adaboost [21, 8] that hav e utilized only a limited dataset, we have profitted greatly by a large body of high quality experim ental data accumulated over the years. This has enabled us to choose by means of machine learning tec hnology a very sparse set of voting classifiers which performed as well as standard, state-of-t he-art subject calibrated methods. ` larized regression in this case performed better than other methods (such as majority voting) which we have also tested. Note that, interestingly, the chosen fe atures (see Figure 2), do not exclusively come from the best performing subjects, in fact some average performer was also selected. How-ever most white dashes are present in the left half, i.e. most subjects with high auto-band reference method performance were selected. Interestingly some subj ects with very high BCI performance are subject X  X  data. No single frequency band dominated classifi cation accuracy  X  see Figure 4. There-fore, the regularization must have selected diverse featur es. Nevertheless, as can be seen in panel G of Figure 5 there is significant redundancy between classifi ers in the ensemble. Our approach of finding a sparse solution reduces the dimensionality of the c hosen features significantly. For very able subjects our zero-training method exhibits a slight pe rformance decrease, which however will not prevent them from performing successfully in BCI.
 The sparsification of classifiers, in this case, also leads to potential insight into neurophysiological processes. It identifies relevant cortical locations and fr equency bands of neuronal population activ-ity which are in agreement with general neuroscientific know ledge. While this work concentrated on zero training classification and not brain activity inter pretation, a much closer look is warranted. Movement imagination detection is not only determined by th e cortical representation of the limb whose control is being imagined (in this case the arm) but als o by differentially located cortical regions involved in movement planning (frontal), executio n (fronto-parietal) and sensory feedback (occipito-parietal). Patterns relevant to BCI detection a ppear in all these areas and while dominant discriminant frequencies are in the  X  range, higher frequencies appear in our ensemble, albeit in combination with less focused patterns. So what we have foun d from our machine learning algo-rithm should be interpreted as representing the characteri stic neurophysiological variation a large subject group, which in itself is a highly relevant topic tha t goes beyond the scope of this technical study.
 Future online studies will be needed to add further experime ntal evidence in support of our find-ings. We plan to adopt the ensemble approach in combination w ith a recently developed EEG cap having dry electrodes [18] and thus to be able to reduce the re quired preparation time for setting up Figure 5: A: primary projections for classical auto-band CS P. B: linearly averaged CSP X  X  from the ensemble. C: transfer function for classical auto-band and ensemble CSP X  X . D: weightings of 28 ensemble members, the six highest components are shown in F. E: linear average ensemble temporal filter (red), heuristic (blue). F: primary projections of th e 6 ensemble members that received highest classifiers are applied to each trial of one subject. The top r ow (broad) gives the label, the second row (broad) gives the output of the classical auto-band CSP, and each of the following rows (thin) gives the outputs of the individual classifiers of other subj ects. The individual classifier outputs are by true labels with primary key and by mean ensemble output as a secondary key. The row at the bottom gives the sign of the average ensemble output. a running BCI system to essentially zero. The generic ensemb le classifier derived here is also an excellent starting point for a subsequent coadaptive learn ing procedure in the spirit of [7].
