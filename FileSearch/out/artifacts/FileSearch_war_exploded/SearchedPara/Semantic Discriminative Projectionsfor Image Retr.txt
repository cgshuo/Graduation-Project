 With the development of digital imaging technology and the popularity of World Wide Web, Gigabytes of images are generated every day. It is a challenge that manage effectively images visual content. Content Based Image Retrieval (CBIR) is receiving research interest for this pur pose [1,2,3,4]. However, there are still many open issues to be solved. Firstly, t he visual content such as color, shape, texture, is extracted from an image as feature vectors. The dimensionality of feature space is usually very high. It ranges from tens to hundreds of thousands in most cases. Traditional machine learning approaches fail to learn in such a high-dimensional feature space. This is the well-known curse of dimensionality. Secondly, the low-level image features used in CBIR are often visual charac-terized, but it doesn X  X  exist the directly connection with high-level semantic concepts, i.e. so-called semantic gap.

To alleviate the open issues, more and more attention has been drawn on the dimensionality reduction techniques. ISOMAP [5], Locally Linear Embedding (LLE) [6] and Laplacian eigenmaps [7] usher in manifold learning, these algo-rithms discover the intrinsic structure and preserve the local or global property of training data. Tenenbaum et al. [5] uses geodesic distance instead of Euclidean distance to estimate distance between points. Multidimensional Scaling [8] is ap-plied to discover the embedding space. Saul et al. [6] assumes there are smooth local patches that could be approximately linear, meanwhile a point in the train-ing space could be approximated by linear combination of its neighbors. The projected space minimizes the reconstruc tion error using neighborhood correla-tion. Laplacian eigenmaps preserves locality information and makes neighbors close to each other in the projected spa ce. These algorithms are unsupervised and limited to a nonlinear map. It is hard to map entire data space to low-dimensional space.

Locality Preserving Projections (LPP) [9], and Local Discriminant Embed-ding (LDE) [10] are proposed to extend the nonlinear learning approaches. These algorithms are all motivated by Laplacian eigenmaps. He et al. [9] uses a neigh-borhood graph to characterize locality preserving property that nearest neigh-bors in the original space should be near est neighbors in the projected space. LDE constructs two neighborhood graphs, one prevents neighbors from different category and another preserves the locality through the affinity matrix using neighborhood information. LPP and LDE are effectively used to map data in entire image space. But only one neighborhood graph is used to discover the in-trinsic structure, and LLE doesn X  X  utilize the label information. LDE only keeps the neighborhood images from different classes away. LPP and LDE need to compute the inverse matrix, suffering from the singularity problem.

Bridge low-level visual feature to the high-level semantic is a great challenge in CBIR. We use Laplacian to learn the images semantic subspace in order to achieve more discriminative image representation for CBIR. In our work, both vi-sual similarity and semantic dissimilarity are applied to construct neighborhood graph since they not only contain the des criptive information of the unlabeled images but also the discriminative information of the labeled images utilized in learning. We introduce a penalty  X  to formulate a constrained optimization problem in the difference form, so that the optimal projection can be found by eigenvalue decomposition. Information of conjunctive graphs is represented by a affinity matrix, and it is much more computationally efficient in time and storage than LPP and LDE. On the other hand,the learnt subspace can preserve both local geometry and relevance information. Previous works often neglect the singularity problem and the optimal dimensionality, but we will determine the optimal dimensionality and avoid the singularity problem simultaneously.
This paper is organized as follows. In section 2, we introduce the SDP ap-proach, kernel trick is used to the nonlinear learning approach in section 3, fol-lowed by the experiment results are discussed in section 4, and lastly we conclude our paper in section 5. In this section, we introduce our learning approach for image retrieval which respects the local descriptive and discriminative information of the original image space.
Suppose n training images, { x i } n i =1  X  R D . we can construct two graph. G S denotes the semantic similarity via semantic label and G V denotes the visual resemblance by exploring the neighborhood of each image in the geometric space. W S and W V denotes the affinity matrix of G S and G V respectively. W S and W V are computed as follows: Where k  X  NN(  X  ) denotes the k -nearest-neighbors.
 We integrate two kinds of information: Where  X   X   X  denotes the Meet of tw o zero-one matrices.
 We utilize the penalized difference form to formulate following constrained op-timization problem.
 where  X  is a penalized coefficient, the constraint P T P = I avoids trivial solution, and I is the d  X  d identity matrix, d is the reduced dimensionality.
The above formulation exhibits the implication that local neighbors with se-mantic dissimilarity should separate each other and different semantic classes are far away from each other; the images with similar semantic and visual content will be clustered together, preserving the intrinsic structure.
 We rewrite (4) in the form of trace, and get the following formulation: Where L  X  = D  X   X  W  X  ,and D  X  is a diagonal matrix with D  X  ii = j W  X  ij .Anal-ogously, L + = D +  X  W + with D + ii = j W + ij . Thus the optimization problem can be formulated as: We take no dimensionality reduction as the baseline, therefore (5) could be zero. We could get a positive scalar  X  when the dimensionality is reduced [11]. Then we have: It is obviously that X ( L  X   X   X L + ) X T is a D  X  D , sparse, symmetric and positive semidefinite matrix. According to the result of Rayleigh quotient, the optimiza-tion problem can be calculated by eigenvalue decomposition.
 eigenvectors corresponding to the d largest eigenvalues of X ( L  X   X   X L + ) X T .  X  i is the optimal value of the above optimization problem, where  X  i ( i =1 ,  X  X  X  ,d ) are the d largest eigenvalues. d is the number of positive eigenvalues and J reaches the maximum.

We can see that the singularity problem in LPP, LDE does not exist in our approach, meanwhile we find the optimal dimensionality.
 To get returns for the query in image retrieval, we project any test image x  X  R D to R d via y= P T x and will find the nearest neighbors of Euclidean distances. Those images corresponding to the nearest neighbors will be the top-ranking returns. As the kernel trick [12] successfully app lied to Kernel LPP [13], Kernel LDE [10], we generalize SDP to kernel SDP, in which kernel transformation is applied to handle nonlinear data.
 Denote  X  : R D  X  X  is a nonlinear mapping, so the image feature vectors in the high-dimensionality feature space is denoted as  X  (x i ) , ( i =1 ,  X  X  X  ,n ). The inner product in F can be computed by the kernel function. we specify the RBF kernel k assume v i is the linear combination of  X  (x i ) in the projected space F : we have: is kernel matrix. Replacing X with  X  (X), we rewrite (5), and consider the kernel-based optimiza-tion problem: where the constraint A T A = I avoids trivial solution, I is the d  X  d identity matrix.

Analogously according to the result of Rayleigh quotient, the optimization problem can be calculated by eigenvalue decomposition. We select the d largest eigenvalues of K ( L  X   X   X L + ) K ,where d is the number of positive eigenvalues. Our approach doesn X  X  suffer from the singularity problem, and get the optimal dimensionality.

To get returns for the query in image retrieval, we map any test image xtoby y= V T xwiththe i th dimensionality computed by y i =v T i x= n i =1  X  ij k (x j , x), and find the nearest neighbors of Euclidean distances. Those images correspond-ing to the nearest neighbors will be the top-ranking returns. In this section, we experimentally evaluate the performance of SDP on COREL dataset and compare with LPP, LDE in order to demonstrate effec-tiveness of our approach for image retrieval. The COREL dataset is widely used in many CBIR systems [18]. In our experiments, the dataset consists of 2500 color images, including 25 categories, each category contains 100 samples. Those im-ages in the same category share the same semantic concept, but they have their individual varieties. Images from the same category are considered relevant, and otherwise irrelevant.

In our experiments, we only consider these queries which don X  X  exist in the training images. Five-fold cross validation is used to evaluate the retrieval per-formance. We pick one set as the testing images, and leave the other four sets as the training images. Table 1 shows the features of which the dimensionality is 145.

Precision-Recall curve (PRC) is widely used as a performance evaluation met-rics for image retrieval [19]. In many cases, PRC is overlapped in high recall, moreover, it is unreasonable to calcula te the mean average precision among dif-ferent categories. We alternatively adopt the precision-category. Given a specified number N , we define the precision as following: Users are usually interested in the first screen results just like Google TM Image. We have N =20 in our experiments. 4.1 Comparisons with LPP, LDE Model Selection is very important in many subspace learning algorithms. In our experiments, it is not very sensitive to tuning parameter k .weset k = 10. We adopt Gaussian heat kernel to compute the W ij , W ij = exp (  X  x i  X  x j 2 /c ), where c is a positive scalar. the aforementioned W ij is superior to 0/1 and is not sensitive to c .

Inthisexperiment,wecompareSDPwithLPP,LDE.LPPandLDEhave the limitation of singularity problem due to compute inverse matrix. Both LPP and LDE adopt PCA to overcome the limitation, retaining the 98% principal components [13]. The optimal dimensionality of SDP is 64, as shown in Fig. 1. SDP achieves much better retrieval performance than other approaches. As a matter of fact, we gain much more discriminating image representation by SDP. Next, we give the experiment result of Kernel SDP in Fig. 2, the optimal di-mensionality of Kernel SDP is 145, Except for the category 15, Kernel LDE performs marginally better than Kernel SDP. We can conclude that Kernel SDP outperforms other approaches. 4.2 Reduced Dimensionality Even though our approach can determine the optimal dimensionality, the di-mensionality of reduced space is importa nt tradeoff between retrieval precision and the computational complexity. In this experiment, we investigate the rela-tion between dimension and precision. T he precision of SDP reaches its peak at 10 dimensions in Fig. 3. The precision of Kernel SDP converges rapidly from 2 to 10 dimensions, and then achieves relia ble results from 10 to 64 dimensions. As shown in Fig. 4, The precision reaches its peak at 30 dimensions, converges rapidly from 2 to 30 dimensions, and then achieves smooth results from 30 to 145 dimensions. As shown in Fig. 3 and Fig. 4, we can gain the lower dimensionality while the precision is stable, even relatively higher. We have introduced a subspace learning approach SDP for image retrieval. The image structure is approximated by the adj acent graph integr ating descriptive and discriminative information of the original image space. SDP focuses on the improvement of the discriminative performance of image representation. As pre-vious work neglect the singularity problem and optimal dimensionality, SDP avoid the singularity problem and determine the optimal dimensionality simul-taneously. Moreover, we extend our approach and present kernel SDP. Experi-mental results have revealed the effectiveness of our approach.

Owing to the effectiveness of SDP, fur ther work will be investigated as following: 1. Feature selection is an open issue in CBIR. In this work, we only adopt 2. Representing an image as a matrix intrinsically, and extending the subspace 3. Utilizing the user X  X  interaction, A possible extension of our work is to incor-This work was supported in part by the National Natural Science Foundation of China (Grant No. 60572078) and by Guangdong Natu ral Science Foundation (Grant No. 05006349).

