 There X  X  now an emerging trend in web information service that people answer others X  questions ([1]), such as Google Answers (http://answers.google.com). As time goes by, these sites possess huge number of question and answer pairs which cover various zhidao.baidu.com), a popular Chinese web site providing this kind of service, has solved up to 11 million questions submitted by users till now (2007-1). Large archives of questions and answers are collected over time like FAQ lists. When people submit a question that has been solved already, corresponding answer can be returned immediately without waiting for manual response. Thereby, these archives can be no doubtfully ideal resources for a general Question Answering system if only similar questions could be found accurately without lag. [1] has made a first attempt to establish such a system for Korean based on translation model and declared an encouraging performance. 
Performance of such systems depends heavily on the measures of sentence semantic similarity i.e. how to quantify the semantic similarity between sentences. People of different backgrounds are likely to express the same meaning with different wordings. This leads to the phenomena that two questions may have the same meaning but differ lexically. Therefore, a rigid lexical-based measure of sentence semantic similarity reduces the overall recall whereas a loose one reduces the precision. 
What X  X  more, sentence should be recognized as a sequence of words that are organized in specified structure. However, most of previous work on sentence similarity determination turns out to be bag-of-words approaches without taking the structure information of sentence into account ([2], [3], [4]). In this paper, we X  X l propose an approach to quantify the degree of similarity between sentences combined with syntactic knowledge, and incorporate it into a Question Answering system based on archives of questions and answers collected from Baidu Zhidao. An examination will be carried out on the impact of syntactic information on the measurement. We X  X l also show that our approach outperforms other known retrieval models that have been implemented in the Lemur Toolkit (http://www.lemurproject.org). 
The rest of the paper is organized as follows. Section 2 illustrates our approach in details. In section 3, we describe the dataset used for our experiments and evaluation. analysis on the results in section 4. Section 5 gives the conclusions of our work. As mentioned above, most previous traditional measures of sentence (text) semantic similarity take advantage of word oriented measures just in a bag-of-words way. This kind of approach ignores lots of important information hiding in sentence structure, such as syntactic roles played by different words in the sentence ([3]). Words of different syntactic elements in a sentence make different contributions to sentence semantic similarity measurement. For example, two sentences containing same or similar subjects are more likely to be semantic similar than those only containing contribute more in measuring the similarity of sentences than those as attribute. Therefore, an appropriate weighting strategy is needed to give different weights to corresponding syntactic components in the sentence to reflect the effect that the component makes to the overall measurement. 
Our quantified formula for the measure is in spired by the work in [3] that used the inverse document frequency (IDF) to quantify the importance of a word. Given two sentences S 1 and S 2 of length m and n respectively, semantic similarity between them is formulated as follows: according to the syntactic element of the wo rd and guaranteed to be a value between 0.1 and 0.9. Within this formula, two critical issues have to be re-examined in details: the measure of word similarity and the weighting strategy for syntactic elements. 2.1 Measure Word Similarity Knowledge-based methodology is adopted in our work to quantify semantic similarity between words. We take use of tongyicicilin (http://www.ir-lab.org/), a Chinese thesaurus containing 77343 terms, as the semantic network to describe word relationships. In this thesaurus, all words are first classified hierarchically into different categories according to the degree of semantic similarity: coarse-degree, medium-degree and fine-degree. Furtherm ore, words are categorized again into clusters and atom-clusters to gain a more fine-grained hierarchical classification. To facilitate calculation, words in the thesaurus are all encoded using numbers and letters similarity of two words as five levels from synonymic to unrelated by counting the overlaps of their corresponding codes. 
Table 1 gives an overview of the relationships between words, explained by examples. To quantify these different levels of word similarity, corresponding values are set for each level. 2.2 Weighting Strategy Substantial efforts have been made on syntactic parsing of natural languages, and many sophisticated parsing grammars have been proposed to describe different aspects of linguistic characteristic, such as Phrase Structure Grammar ([6]) and Dependence Grammar ([7]). It is believed that Dependence Grammar is more suitable for Chinese natural language processing ([5]). In Dependence Grammar, individual words in a sentence are considered to be linked together over dependency relations instead of being combined mechanically. The main idea of Dependence Grammar is that roles played by words of different grammar elements in a sentence are not same to each other, saying that, some words depend on others while some words govern others. In other words, relationship between words is governing or being governed. This theory is quite similar to our weigh ting strategy that gives weights according to the different importance of syntactic elements. 
We define totally 23 dependency elements in our Dependence Grammar parsing, makes to the similarity measurement of whole sentence. Obviously, subject, predicate and object are the skeleton of a sentence and represent the main meaning of the sentence. Therefore, if two sentences are of the same subject, predicate, or object, they tend to express the similar thing. Similarity in other grammar elements also augments whole similarity between sentences, but makes less contribution than the three ones. Contributions are then quantified based on the training corpus as two parameters (alpha and beta) in our approach. These two parameters sum to 1 and are estimated empirically. 3.1 Raw Data Set Zhidao is one of the leading question answ ering service providers in China and has collected up to 11 millions question and answer pairs in Chinese till now (2007-1). answers will always be gained in the web page, waiting for being judged. Experts of Zhidao will timely check whether there is correct answer and mark the question as special tags. Therefore, a straightforward web extraction technique will be capable of digging out questions and correct answers from web pages. Although answers of questions are not used during current experiments, they will be vital in building up the QA system. 
Experiments conducted in our paper are based on the data set obtained from this large archive. The raw data set consists of more than 77,000 question and answer pairs over all categories and is divided into two collections: collections A and B. Collection A contains 50,000 pairs and is used to find the optimal parameter values for our approach in parameter estimation phase. Remainder pairs of the whole raw performance of our approach with other retrieval models. 3.2 Collections with Judgment Information Both parameter estimation and result evaluation require collections with judgment information. Therefore, raw collections A and B have to be refined for the estimation and the comparison of the performance of our proposed approach with other retrieval models. In our work, such two sets of questions with judgment information, named J A and J B , are constructed proportionally from collections A and B. 100 questions are selected randomly from collection A while 50 ones from collection B. These questions are regarded as queries to different retrieval models (approaches). To gather semantically relevant questions to these queries, we employ the pooling technique that is used in the TREC (http://trec.nist.gov) conference series. We utilize the five retrieval models implemented in Lemur Toolkit to retrieve results to queries from collection A and B respectively. Only top 10 results from each retrieval model are kept as candidates. Then judgments of semantic relevancy are done among the total 50 results for one query manually, ignoring overlaps. Result (question) is considered to be semantically relevant with the query as long as they can be categorized into the same question type an d are semantically identical or similar to each other. 
Table 2 shows the 7 question types defined in our system. In addition, to get rid of partiality generated by one judger, we employ 2 judgers to make the judgment separately and then integrate their results. Finally, we X  X e got 335 semantically relevant questions for the 100 queries in J A from collection A and 160 ones for the 50 queries in J B from collection B. 4.1 Parameter Estimation The 23 dependence grammar elements are classified into two categories: skeleton subject, predicate and object while the rest grammar elements compose the latter. Thereby, in the parameter estimation phase of our work, two parameters alpha and beta have to be estimated from J A and J B , which are used to quantify the importance beta sum to 1, we in fact only have to estimate one parameter. 
Figure 1 show the mean average precisions (MAPs) and average recalls evaluated results demonstrate the impact of syntactic information on the measurement of sentence semantic similarity. Specifically, our approach, when equipped with parameter alpha equal to beta (i.e. alpha=be ta=0.5), can be regarded as one without considering the impact of syntactic evidence because it ignores the differentia between the importance of syntactic elements. In the graph, MAP and recall in this situation fail to reach the summits of the curves. This result indicates the necessity of considering the impact of s yntactic evidence. What X  X  more, as we can see from the curves, situations with parameter alpha larger than beta work better in average than those with alpha smaller than beta both in MAP and average recall. This verifies our assumption that syntactic elements of E skeleton play a more important role in measuring curves descend as alpha increases when alpha keeps larger than 0.6. It shows that as similarity measurement. Neglect of these elements degrades the overall performance. Therefore, we have to make a balance between the values of alpha and beta to give prominence to elements of E skeleton while still maintaining the contributions of 4.2 Results and Evaluation Evaluation of our approach with other models is carried out in collection J B . Parameters alpha and beta are estimated in the phase above that are optimal in with queries in descending order. Only top 5 results are used to evaluate the performance of the approach. Then, we compare our results with the top 5 results returned by the five baseline models implemented in the Lemur Toolkit. This toolkit is designed to facilitate researches in language modeling and information retrieval. In our experiments, the five baseline retrieval models include: the TFIDF retrieval model (TFIDF), the Okapi BM25 retrieval function (Okapi) and the KL-divergence language model based retrieval method with three different smooth algorithms (Jelinek-Mercer (KL-jm), Dirichlet prior (KL-dir) and Absolute discounting (KL-abs)). 
For each of the five baseline models, their parameters are set to the optimal values in J A . Table 3 demonstrates evaluation results of our approach (SynSim) with the five baseline models. Although both MAP and average recall are still far from being satisfied for a practical QA system, our approach outperforms other models. The low MAP and average recall are mostly caused by the lack of an adequate data collection which reduces the probability of encountering an existing question that is semantic similar to the query. We believe that an improvement in MAP and recall will be gained as long as a large data collection is available. 4.3 Example and Analysis One typical example is showed in Table 4 to explain why our approach works better than other five models. 
Question 1 When was Du Fu born (  X   X   X  X  X  X  X  X 
Question 2 Which dynasty do the four famous 
In the example, Question 1 is considered to be semantic similar to the query while However, these two words are of different grammar elements in the query.  X  X u Fu X  query and question 2 (All these grammar elements are derived when query and questions are in Chinese). According to our weighting strategy, we regard  X  X u Fu X  more important than  X  X ynasty X  in measuring the sentence similarity. As a result, we finally get question 1 as a result at rank 3 while ranking question 2 at 10. However, in the other 5 models, they fail to get the point and return both of these two questions in the top 5 results. Our work is conducted for an archive-based QA system in which measures of sentence semantic similarity dominate the overall performance when given a large archive. In this paper, we propose such a method that incorporated with syntactic information. Words in a sentence are parsed using Dependence Grammar. The syntactic elements are categorized into two classes according to their contributions to the similarity measurement of whole sentence. A weighting strategy is introduced to quantify the contributions. 
During the parameter estimation phase in our experiments, the results demonstrate the impact of syntactic information with different weights on similarity measurement and show that the consideration of syntactic information enhances the performance of a similarity measure. We also compare the performance of our approach with five baseline retrieval models implemented in the Lemur Toolkit. Experiments show that our approach outperforms other models in finding semantic similar sentences. 
However, in order to incorporate our sent ence similarity measure into a practical archive-based QA system, we plan to build up a larger corpus to improve the performance of our approach. We also plan to add in the information of question types to help to measure the similarity in the future. This research was supported in part by the National Basic Research Program of China (973 Program) under Grant No. 2006CB303000. 
