 In recent years, topic modeling has em erged as a relatively new approach to analyze text data. Lots of topic models, such as Probabilistic Latent Seman-tic Indexing(PLSI)[10], Latent Dirichlet Allocation(LDA)[3] and Hierarchical Dirichlet Processes(HDP) [18] have been proposed to model documents with-out tags.

Meanwhile, more and more pages in many websites, especially social media websites, are born with tags, or labels. These tags are usually assigned by users, and carry a lot of information about the pages. For example, a twitter user posts a tweet, and associates it with some hashtags, which produces a tagged document. Also, generally, category names of news pages can also be regarded as tags of pages.

Therefore, a number of topic models have been proposed to model labeled documents. Labeled LDA is one of them proposed by Ramage et al. in 2009, and has many extensions including PLDA[15], hLLDA[11] and so on. In Labeled LDA, a topic is defined as a word distribution, and each topic corresponds to a label. Thus, topics of each labeled doc ument are restricted within its labels. Also, through selecting the topic with highest probability for each word, La-beled LDA assigns the most appropriate label to each individual word. With this ability, it can solve many text analysis problems, such as credit attribu-tion, tagged document visualization, snippet extraction and multi-labeled text classification[12][16]. For example, in tagged document visualization, annotating important words with their labels provides users a short semantic description of documents; In snippet extraction, by finding out snippet that contains most specific label, appropriate s nippet can be extracted.

Traditional inference algorithms like Gibbs sampling and variational inference require multiple passes through the whole corpus. These algorithms are often referred to as off-line or batch algorithm. Obviously, it X  X  difficult to apply batch-LLDA for large scale data. Considering the task of clustering all tagged web pages in del.icio.us , a popular social bookmarking website, it can be accomplished by using the topic distributions learned by Labeled LDA. Since it contains massive amounts of pages, and we have to run through all pages for many times, it will be time consuming and infeasible. Also, when documents arrive in a stream, batch algorithms do not work. For example, twitter users update status whenever they like to, so it will generate a flow of tweets. If we use batch algorithm to analyze these tweets, once a new tweet is available, we have to run the algorithm again over all tweets including previously seen ones to obtain the newest model parameters. That will be slower and slower as the number of tweets increases, which is unbearable in the real world. Thus, it X  X  necessary to develop an efficient algorithm to solve the problem.

To the best of our knowledge, there is no existing work to obtain param-eters of Labeled LDA by using an online style algorithm. Aiming at solving this problem, we propose an online algorithm, called online-LLDA, which incre-mentally updates model parameters when a document arrives. In this case, the algorithm do not need to visit the previous documents that have been processed. Consequently, these documents can be abandoned to save memory. Differently from batch Gibbs sampling, we use another sampling-based framework, particle filter[5], to make the algorithm work in online mode. We evaluate our method on two datasets, and show that online-LLDA performs better than batch-LLDA in time, while achieving equivalent effect.
 The rest of the paper is organized as follows. We conclude the related work in Section 2. Our approach is introduced in S ection 3. We demonstrate effectiveness and efficiency of our algorithm in section 4. The final conclusion and future work will be discussed in section 5. Most existing researches on LDA-like model utilize various inference algorithms, such as variational Bayesian[3][17], Gibbs sampling[7][8], expectation propagation [13] and belief propagation[21] to obtain the parameters. Unfortunately, most of them are batch algorithms.

Thus, a host of online learning methods have been developed for topic mod-els. Some of them focus on modeling large amount of documents efficiently based on LDA. Typical researches include TM-LDA[19], On-Line LDA[1] and so on. By minimizing the error between predicted topic distribution and the real distribu-tion generated by LDA, TM-LDA captures the latent topic transitions in temporal documents. On-Line LDA uses topics learned from previous documents by LDA as the prior of following topics. It was designed by Alsumait et al.(2008) to detect and track topics in an online fashion.

Some other work try to improve inference algorithms themselves. Banerjee et al.(2007)presented online variants of vMF, EDCM and LDA. Their experiments illustrated faster speed and better performance on real-world streaming text[2]. Hoffmanetal.(2010)developedanonlinevariationalBayesianalgorithm[9]forLDA based on online stochastic optimization. In their approach, they thought of LDA as a probabilistic factorization of the matrix of word counts into a matrix of topic weights and a dictionary of topics. Thus, they used online matrix factorization techniques to obtain an online fashion schema of LDA. Canini et al.(2009) also pro-posed an online version algorithm[4] using Gibbs sampling. Yao et al.(2009) com-pared several batch methods mentioned above, and introduced a new algorithm, SparseLDA[20] to accelerate learning pro cess and consequently achieved nearly 20 times faster speed than traditional LDA.

To conclude, a large number of prior work have made great efforts on designing appropriate online algorithms for LDA. However, less work focus on improving the efficiency for Labeled LDA. In this paper, we propose a novel online learning method for Labeled LDA, called online-LLDA. In this section, we first review the Lab eled LDA model along with its original batch algorithm. Then, we analyze the feasibility of applying particle filter on Labeled LDA in the following part. Finally, we present online-LLDA in detail. 3.1 Preliminary Labeled LDA is a generative model for labeled documents that extends LDA. With the assumption that topics of labeled document are relevant with its labels, it adds a constraint on topic generation. Formally, for each document d in the collection of documents D , it can only pick up topics associated with its labels. As the graphical model shows in Figure 1, topics  X  are determined by both topic prior and labels. Let K be the number of unique labels in all current observed documents, which equals the number of topics. Each document d has a label set L in document d . For each element  X  ( d ) j in  X  d ,wehave  X  ( d ) j = where I is the indicator function.
 The original inference approach of Labeled LDA is a batch algorithm(batch-LLDA)[14], that iteratively samples topics of all words using the conditional probability of latent variable z . For a word i , it is sampled according to the following equation: where w i is a word in document d , V is the size of vocabulary, K is the number w i from all documents. n is the count of words in document d with topic assignment k , excluding w i .And n d,  X  i is the corresponding summation over topics.  X  and  X  are prior parameters.
With Equation (1), batch Gibbs sampler can constantly sample topic for each word. After the burn-in period, topic assignments z are available for estimation. To ensure the convergence, the number of iterations is usually set as big as enough. Hence, it will take too much time for batch-LLDA to reach the burn-in point, which leads to inefficiency.
 3.2 Feasibility Analysis Labeled LDA can be viewed as a state space model, if we regard the latent variable topic z as a state variable, and the word w as an observation variable. Particle filter is a kind of efficient Markov-Chain Monte Carlo(MCMC) sampling method for estimating parameters of sta te space model. Differently from Gibbs sampling, it X  X  an online algorithm.

Particle filter can be applied to solve the inference problem of Labeled LDA. (1) Suppose x is the hidden state variable and y is the observation variable in particle filter. The objective of particle filter is to estimate the values of hidden state variables given observations, that is P ( x | y ). Coincidently, what we want to acquire in Labeled LDA is the joint distribution P ( z | w ), which is also the probability of state variables given observations. (2) Particle filter assumes that observations are conditionally independent, and observation y k is only determined by x k . Obviously, based on the bag-of-words assumption, Labeled LDA fulfills this requirement.

In this paper, we use Rao-Blackwellised particle filter(RBPF), an enhanced version of particle filter as our approximation method. RBPF integrates out the latent variables, and thus makes the solution simpler. In our algorithm, we have P particles, and each particle p represents an answer that we desire, namely the posterior distributions of topics. In our implementation, a particle p stores all topic assignments of words, together with an importance weight  X  ( p ) indicating the importance of particle. What X  X  more, a reassignment process is added to enhance the quality of samples. 3.3 Online Labeled LDA In this section, we introduce an online learning method by using particle filter. As demonstrated in Algorithm 1, the overall algorithm consists of two phases: initialization phase and online phase . Initialization phase accomplishes the task of launching the online phase, while online phase continually processes every word w i in a newly arrived document d and generates ne w parameter set  X  d after the entire document has been processed.
 In initialization phase (line 1 to line 4), for each particle, we apply batch-LLDA on an initial corpus E that contains a small fraction of documents. After running over, we get initial topic assignments of all initial words, along with sufficient statistics. These values are sto red into each particle, which are useful in the online phase.

In online phase (line 5 to line 17), we first initialize particle weights with equal values and then process documents in a t ext stream one afte r another. Here, N d represents the length of document d . Model parameters will be updated every time a document is processed. A new sampling equation is used as shown in Equation (2). In this equation,  X  i has different meaning from Equation (1). In Equation (1), it represen ts all words or topics except i , but here it aims at excluding i from currently observed words. Thus, i  X  i represents first i  X  1words, i  X  j represents first i words except j . This difference is essential between batch-LLDA and online-LLDA. Also notice that when i =1, i  X  i represents nothing, that is another reason why we need an initialization phase.

Since we use all currently observed words including words in initial corpus, the word count in Equation (2) includes two parts. The first part is the word count of initial collection of documen ts, and the second part is the word count of documents coming in a stream in the online phase. The superscript p in all notations indicates the particle index. e ( p ) k,w assigned to topic k in initial corpus, and e ( p ) k,  X  is the summation. n is similar with those in Equation (1). T d is the number of unique labels when document d is available, namely the current topic number. All sampled topics should be restricted within its document label set.
 Algorithm 1. Online Labeled LDA in topic z ( p ) i , which is sampled in line 9. It is calculated using all currently sampled topic assignments, as the equation tells. Next, effective sample size  X  N ef f is calculated as:
It is an estimation of N ef f , that measures the efficiency of the method and controls the algorithm to avoid degeneracy[6]. A sampling importance resample procedure (line 14) will be run if  X  N ef f is no more than threshold N thresh . P particles are resampled with replacement according to the importance weights. Then old particles are replaced with the n ew ones. After this process, particles with small weight have high possibility to be eliminated. This process reflects the  X  X urvival of the fittest X  law,  X  X xcellent X  solutions should be inherited. Intuitively, N thresh decides the frequency of resample, and thus influences the effectiveness and speed of the algorithm.

In addition, we add a reassignment period (line 15) after resample to improve the quality of samples. Since words coming in online phase are only sampled once, the result might be inaccurate. We solve this problem by picking up some words randomly, and reassigning topics to them. R ( i ) is an index set, containing a fixed number of indexes that no more than i . These indexes are randomly selected from { 1 , ..., i } , and represent the words r eached earlier than word i including word i itself. For each element r in R ( i ), we sample a new topic according to Equation (2). Obviously, when | R ( i ) | is big enough, online-LLDA will degenerate to a batch algorithm, since previous words will be reassigned constantly.
Generally, our final objective, the joint distribution P ( z | w ) as we mentioned in section 3.2 is approximated as below: where I ( z , z ( p ) ) is an indicator function, In other words, particles with same vector z ,havesame P ( z | w ). And it equals the sum of weights of these particles. Then, topic assignments z  X  for parameter estimation are calculated as follows: However, in reality, we found that it cost too much time to check two particles whether they share the same z ,since z is a vector whose length is the size of all words. We also found that there were seldom same particles. Therefore, we modify this procedure to choose the particle with biggest weight for estimation. The modified z  X  is: With z  X  , we can compute word count and estimate parameters we need. After document d is processed we can get  X  d . It contains word distributions, or topics for all labels. We evaluated both efficiency and effectiven ess of our method on two real datasets through three experiments. The first one compares the interpretability of two algorithms. The second one shows the eff ectiveness of batch-LLDA and online-LLDA, by using perplexity as metric. An d the last one demonstrates the power of our online-LLDA in fast processing document. 4.1 Experiment Setting The first dataset is a collection of p apers crawled from the ACM website 1 , which consists of 1712 full papers of four conferences (CIKM,SIGIR,SIGKDD and WWW) from the year 2011 to the y ear 2013. We refer to this corpus as Conf in the rest of this paper. We used keywords in the papers as labels. The second dataset, called Twitter , is a corpus of tweets downloaded from Twitter, which contains about 2 million tweets. We used hashtags as labels in this corpus. The detailed information of datasets is listed in Table 1. All experiments were run on a server with an Intel Xeon E3-1230 3.3 GHz CPU and 32GB memory.
In all of our experiments,  X  and  X  are fixed at 0.1, and burn-in iteration time is 1000 for batch-LLDA. Since | R ( i ) | , N thresh and P influences the effectiveness and runtime of online-LLDA, we should make a tradeoff. In our experiments, | and P is 10. 4.2 Topic Visualization In this experiment, we ran both batch-LLDA and online-LLDA over two entire corpora. Then, we picked up top ten words of each topic learned from each algorithm to represent a topic. Each topic is associated with its corresponding label. Table 2 compares the results of two algorithms on Conf corpus, while Table 3 shows the Twitter  X  X .
 Most topics generated by our online-LLDA are as interpretable as batch-LLDA. And these topics are highly relevant with corresponding labels. For example, in Conf, the top words  X  X opic X , X  X opics X  and  X  X odeling X  explain the la-bel  X  X opic modeling X  well;  X  X ser X ,  X  X tem X  and  X  X ecommendation X  are frequently used to describe  X  X ecommender systems X ; In Twitter, when talking about  X  X ol-itics X , people are most likely to discuss  X  X ound X ,  X  X bama X  and  X  X ealth X . 4.3 Document Modeling In computational linguistics, perplexity is a widely used metric that measures how well a language model predicts words in the test corpus. Lower perplexity indicates higher likelihood of the test c orpus, or better generation performance of the model. Generally, it is computed as Equation (8). Since topics in Labeled LDA are relevant with labels, when a label in the test corpus do not appear in the training corpus, the algorithm will not assign any topics that relates with this label on words. Thus, only prior parameter  X  can influence the word distribution  X  in this case. It means that, in this topic, all words share the same probability to be generated, which leads to low ability of predicting words in unseen documents and high perplexity. In this experiment, we simulated the situation that documents are coming in a stream. In our online-LLDA, model parameters are updated every time a new document arrives. We use 200 documents in Conf and 5% tweets in Twitter for initialization. For the sake of fairness, we excluded topics that were sampled in initialization phase when generating the model. We computed perplexity of the held-out test dataset at some points using the current model learned by online-LLDA. As for batch-LLDA, we computed perplexity at the same points, however each run has to use all of the d ocuments previously seen.

Figure (2) shows that, in both corpora, perplexity is much higher at the start, and then declines as the seen documents number increases. It is reasonable since the seen documents used for training are not enough at first, and lack most of labels in test corpus. As the training dataset grows, the learned model fits the test corpus better, and perplexity converges. In Conf, we achieved better effect than batch-LLDA. In Twitter, the two curves are very close, and online-LLDA X  X  is lower than batch-LLDA X  X  as we can see in the zoomed fragment. It X  X  not surprise since online-LLDA incorporate s a resample process and a reassignment period. Also, better particles will remain according to the algorithm, which leads to better effectiveness. Since the number of tweets is large, and a single tweet is short, the result in Twitter is satisfactory. Notice that the document numbers in Twitter are in logarithmic scale. 4.4 Efficiency In this experiment, we evaluated the efficiency for each algorithm, which is the main purpose of online-LLDA. We used thesamepointsaswedescribedin section 4.3, and recorded the training t ime using current observed documents.
When new documents arrive, batch-LLDA has to run over all documents for many iterations again. Since the time for each iteration grows with the number of documents, the total time for batch-LLDA grows fast. However, online-LLDA does not need to do this, it should only process the new coming document and update parameters to get a new model. As Figure (3) shows, batch-LLDA costs much more time to get the new para meters compared with online-LLDA, especially when the number of observed documents is large. When running over the Conf corpus, the training time of online-LLDA is less than 400 seconds, while batch-LLDA takes more than 3,000 seconds, which is about 8 times longer. In twitter, batch-LLDA takes more than 10,000 seconds, while oline-LLDA takes less than 6,000 seconds.
 In this paper, we proposed an online method for Labeled LDA inference, called online-LLDA. We analyzed the feasibility of applying particle filter, a MCMC method on Labeled LDA and demonstrated that it X  X  a feasible choice for Labeled LDA. We presented our algorithm in detail and conducted several experiments on two real datasets. Our experiment r esults clearly show that online-LLDA performs as good as batch-LLDA, and costs much less time.

In the future, we will explore the following directions. (1) We will speed up this algorithm through optimizing the data structure of particle and word count. (2) We will try other types of method, including online version of variational Bayesian, expectation propagation, belief propagation, and so on.
 Acknowledgment. The work was supported by National Natural Science Foun-dation of China (Grant No. 61402036 and No. 3070021501109), and 863 Program of China (Grant No. 2015AA015404).

