 As an important matrix factorization model, Nonnegative Matrix Factorization (NMF) has been widely used in infor-mation retrieval and data mining research. Standard Non-negative Matrix Factorization is known to use the Frobenius norm to calculate the residual, making it sensitive to noises and outliers. It is desirable to use robust NMF models for practical applications, in which usually there are many data outliers. It has been studied that the ` 2 , 1 -norm or ` can be used for robust NMF formulations to deal with data outliers. However, these alternatives still suffer from the extreme data outliers. In this paper, we present a novel ro-bust capped norm orthogonal Nonnegative Matrix Factor-ization model, which utilizes the capped norm for the objec-tive to handle these extreme outliers. Meanwhile, we derive a new efficient optimization algorithm to solve the proposed non-convex non-smooth objective. Extensive experiments on both synthetic and real datasets show our proposed new robust NMF method consistently outperforms related ap-proaches.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms  X  Corresponding author.
 c  X  Capped Norm; Robust Nonnegative Matrix Factorization; Robust Clustering
Nonnegative Matrix Factorization (NMF) is an important matrix factorization algorithm which deals with a specific class of the nonnegative matrix. It is first used by G. Golub X  X  notes, and further discussed by Paatero [18]. The work of [16] made this matrix factorization algorithm well-known in data mining and information retrieval areas. The original motivation of NMF is to make the factors coherent with the original nonnegative data, making the factorization easy to interpret, as opposed to singular value decomposition (SVD) which has both negative values and positive ones.

Later work [4] showed the connection between NMF and clustering methods. After that, the NMF methods have been widely used as one of the most important clustering methods, and have been applied to a lot of different appli-cations, e.g. multimedia data analysis [1], text and docu-ment mining [26], protein interaction prediction [23], trans-fer learning [21], and multi-relational clustering [22, 24].
Driven by such huge amounts of different useful appli-cations, a lot of research has been done to further improve NMF algorithm and its performance. [3] improved the NMF algorithm by adding an orthogonal constraint, proving the equivalence between orthogonal NMF and the relaxed K -Means clustering [13], and also extending it to Nonnegative Matrix Tri-Factorization (NMTF) for co-clustering. [10] ex-plicitly incorporated the notion of sparseness to improve the found decompositions. [25] proposed a large-scale NMF and NMTF algorithms using clustering indicators.

However, most NMF models use the Frobenius norm based objectives, such that these models are sensitive to data out-liers. Recently, several robust NMF models have been pro-posed. [5] proposed to use the correntropy induced metric to make it insensitive to outliers. [9] adopted a hypersurface cost function to make the NMF robust to the outliers. [27] proposed to subtract a sparse outlier matrix from the data matrix to reduce the effect of the outliers. In the other work [14], the authors adopted ` 2 , 1 -norm based objective function to weaken the effect of data outliers. [11] further introduced robust NMF formulation with manifold regularization. All of these methods can reduce the effects of data outliers on the NMF calculation. However, they can not remove the ef-fect of the outliers. Therefore, these methods still suffer from the extreme data outliers, which can dominate the residue calculation even with the ` 1 -norm distance.

To address this problem, in this paper, we propose novel robust capped norm NMF formulations, which can remove the effect of extreme data outliers via the capped norm based residual calculation. Meanwhile, we derive a new efficient optimization algorithm to solve the proposed non-smooth and non-convex objective with rigorous theoretical analysis. Our new algorithm is as efficient as the traditional NMF and ` 2 , 1 -norm based NMF. Extensive experiments on both toy data and benchmark datasets show our proposed new robust NMF model X  X  correctness and effectiveness.

Notations: We use &lt; m  X  n to denote m  X  n matrices. For matrix M , its j -th column is denoted by m j . || M || F the Frobenius norm.
In this section, we will revisit the traditional NMF as well as robust ` 2 , 1 -norm NMF, giving the motivation for propos-ing the capped ` 2 , 1 -norm NMF models.
The traditional Nonnegative Matrix Factorization (NMF) is defined as the following: where X  X  &lt; d  X  n , d is the dimension of features, n is the number of data points. When using NMF to do the cluster-ing, F  X  &lt; d  X  k denotes the k basis factors, where k is the number of classes. G  X &lt; n  X  k represents the class indicators, indicating the final clustering result.

However, the traditional NMF uses the Frobenius norm to calculate the residual, known to be sensitive to data out-liers in practical problems. To address the robustness issue, several robust NMF models have been proposed. The main idea is to use the ` 2 , 1 -norm or ` 1 -norm based objective func-tions. For example, [14] proposed to use ` 2 , 1 -norm to replace the Frobenius norm in objective and solved: Let X = [ x 1 ,x 2 ,...,x n ], G T = [ g 1 ,g 2 ,...,g n ]. The ` based loss function can be reformulated as following:
The above loss function uses the ` 2 -norm within a data point and the ` 1 -norm among data points, such that the ` 2 , 1 -norm NMF is robust to data outliers (the non-squared mining and information retrieval applications, we usually consider the data outliers exist and trust the feature genera-tion. For example, we trust the instrument which measures the features of data points, but the data outliers are often created during the data collection process. If the noise exists in both data and feature dimensions, we can use the ` 1 -norm NMF model, which uses the ` 1 -norm for both data points and features. But, in terms of optimization, there is no big difference between these two types of models.

Although the ` 2 , 1 -norm NMF is a robust formulation, it still suffers from the extreme odd points. For example, there exists an extreme odd point x k whose residual || x k  X  Fg extremely large, which could affect the clustering result sig-nificantly. The reason is that the ` 2 , 1 -norm based objective just weakens the effect of the outliers, but it still cannot re-move the effects of the outliers. Therefore, we hope to design a better robust NMF model to deal with such situation.
In order to get a more robust clustering model, we propose a novel capped norm NMF method with solving: where  X  &gt; 0 is a thresholding parameter for choosing the extreme data outliers.

In this new objective, if the residual of a data point || x Fg i || 2 &gt;  X  , we consider the data point x i as extreme outlier and its residual is also capped as  X  such that its effect to the whole model is fixed (if we don X  X  cap the residual and x i extremely large, it will affect the final result dramatically). For other data points with residual || x j  X  Fg j || 2  X   X  , our ` -norm residual minimization, which is robust to regular data outliers. Thus, the proposed capped norm NMF is a more robust model than previous ` 2 , 1 -norm NMF.
Note that, in our new model, the extreme data outliers are not heuristically decided based on their magnitude. They are indeed chosen during the nonnegative matrix factoriza-tion process, i.e. they are the extreme data outliers for the clustering task. When we iteratively optimize the capped norm NMF objective, the selected extreme data outliers can be distinct in different iterations (with the same threshold-ing parameter  X  ). When the solution converges, we will find the correct extreme data outliers.

To enhance the performance of the capped norm NMF, we enforce the orthogonal constraint to G , which was used in previous Orthogonal Nonnegative Matrix Factorization (ONMF) method [3]. Thus, our new capped norm orthogo-nal NMF is to solve:
The main advantage of orthogonal constraint is that the orthogonality of G guarantees the uniqueness of the solution and makes the clustering results easily to be interpreted. Thus, the capped norm ONMF method is a more robust clustering model. Our new objective function (5) is diffi-cult to solve. In next section, we will propose an efficient optimization algorithm to solve it.
In this section, we will propose an efficient optimization algorithm to solve the new objective in Eq. (5), and prove the convergence of our proposed algorithm.
We can easily verify that the derivative of Eq. (5) is equiv-alent to the derivative of the following objective: where
We can reformulate Eq. (6) as the following: where D is a diagonal matrix with the j -th diagonal element D
Such a problem can be solved via the iterative re-weighted optimization strategy. When D is fixed, the objective func-tion in Eq. (8) can be expanded as: Therefore, the updating rules for F and G are: When F and G are fixed, the updating rule for D is as following: The detail of our algorithm is summarized in Algorithm 1.
In this section, we will prove the convergence of our pro-posed algorithm.

Theorem 1. In each iteration, the updating rule for ma-trix G while fixing F will monotonically decrease the objec-tive value in Eq. (5).

To make Eq. (5) more compact, we separate all the data points as two subsets and re-write Eq. (5) as: where A = { x j ||| x j  X  Fg j || 2  X   X  } .
 Algorithm 1 Algorithm to solve the problem in Eq. (5). Initialize F and G using the traditional k -means method,
F = F k where F k is the clustering centroid obtained by k -means method, G = G k + 0 . 2, where G k is the k -means clustering result. Initialize matrix D as the identity ma-trix. repeat until Converges
We first prove that the updating rule for matrix G while fixing F will monotonically decrease the value of the above J ( F,G ). In the following proof, we will not count the || x Fg j || 2 when its value is greater than  X  . Thus, Eq. (12) is equivalent to the following: We will prove the following inequality after updating G : where G T G = I . To prove this inequality, we need the following lemma.

Lemma 1. Under the updating rule for matrix G in Eq. (10) while fixing matrix F, the following inequality holds: where d j = 1 / (2 || x j  X  Fg t j || 2 ) , G T G = I .
Proof. To prove this constrained problem, we need prove the Lagrangian function J ( G ) = P j  X  X  d j || x j  X  Fg Tr ( X ( G T G  X  I )) is monotonically decreasing. Similar to the proof in [16], we use the auxiliary function approach, where if a function A ( G,G 0 ) satisfies Let X  X  denote A ( G,G 0 ) as the auxiliary function of J ( G ). We further define G t +1 = arg min A ( G,G t ). As a result, we can which proves that J ( G t ) decreases monotonically. Thus, the key point is to find an auxiliary function J ( G ). Note that the objective function J ( G ) can be written as:
J ( G ) = Tr ( XDX T  X  2 XDGF T ) + Tr ( G T DGF T F ) + Tr ( X ( G T G  X  I )) , An appropriate auxiliary function is as the following:
A ( G,G 0 ) = Tr ( XDX T  X   X )
To prove the auxiliary function (17) satisfies equation (15), we need the inequality, z  X  1 + log ( z ) for all z &gt; 0, and the following inequality derived in [2]: where A and B are nonnegative symmetric matrices, H is nonnegative matrix, and the equality holds when H = H 0 . Therefore, we have the following inequality:
Consequently, the auxiliary function defined in Eq. (17) is a valid auxiliary function. In the next step, we should find the stationary point of A ( G,G 0 ). Let f ( G ) = A ( G,G taking the derivative of f ( G ) with respect to G:  X  X  ( G ) Setting it to zero and substituting  X  =  X  G 0 T DG G 0 T DX T F , we can get the stationary point: To prove the stationary point is the minimum of f ( G ), we should check the Hessian matrix whether is a positive semidef-inite matrix. Taking the second derivative with respect to G , we get:  X  2 f ( G ) Obviously, the Hessian matrix is a positive semidefinite ma-trix, which means f ( G ) is a convex function, thus the sta-tionary point in Eq. (21) is the unique global minima of f ( G ). Note that if replacing G = G ( t +1) and G 0 = G Eq. (21), we will get the updating rule in Eq. (10). Conse-quently, we complete the proof of lemma 1.

Now we go back to prove the Theorem 1. We can easily have the following inequality: where u and u t are nonzero vectors. We replace the u and u with x j  X  Fg t +1 j and x j  X  Fg t j , respectively, and have: which is further equivalent to: Summing all the j together, we will get: Fg j || 2 2 . Therefore, the inequality in Eq. (13) is proved, i.e. J ( F,G t +1 , A )  X  J ( F,G t , A ).

Theorem 2. In each iteration, the updating rule for ma-trix F while fixing G will monotonically decrease the objec-tive value in Eq. (5).
 The proof of Theorem 2 need prove that the following in-equality holds: Like the proof of Theorem 1, we need the following lemma:
Lemma 2. Under the updating rule for matrix G in Eq. (10) while fixing matrix F, the following inequality holds: where d j = 1 / (2 || x j  X  F t g j || 2 ) .
 Proof. We can get the same function for J ( F ) as Eq. (16): J ( F ) = Tr ( XDX T  X  2 XDGF T ) + Tr ( F T FG T DG ) (27) Also, we can get an auxiliary function as Eq. (17):
A ( F,F 0 ) = Tr ( XDX T  X  2 XDGF T ) + Similar to prove Lemma 1, we can prove Lemma 2.
 The rest proof of Theorem 2 is also similar to the proof of Theorem 1. We skip them due to limited space.

We should notice that the set A will be also updated to A 0 in our objective of Eq. (5), after F and G are up-dated. Based on the definition of our objective, we can eas-the above Theorems 1 and 2, we know: J ( F t +1 ,G t +1 , A )  X  J ( F t ,G t +1 , A )  X  J ( F t ,G t , A ). Thus, J ( F t +1 J ( F t ,G t , A ), i.e. our algorithm monotonically decreases the objective function.

After proving the monotonic decrease of the objective function, we need to prove the limit point ( F  X  ,G  X  ) of the sequence { F k ,G k } generated under the updating rule in Eq. (10) is a stationary point, satisfying the KKT condi-tion.
 Theorem 3. The limit solution G  X  satisfies the Karush-Kohn-Tucker (KKT) condition of the constrained optimiza-tion theory.

Proof. The objective function for G while fixing F is: According to the standard theory of constrained optimiza-tion, we introduce the Lagrangian multipliers C and  X , and minimize the Lagrangian function: L ( G,C,  X ) = X According to the KKT condition  X  X  ( G,C,  X )  X  X  = 0 and C jk 0, we can get the following equation: In the next, we should prove Eq. (31) holds for the limit points G  X  . Note that where D jj = 1 / (2 || x j  X  Fg j || 2 ). Then Eq. (31) becomes: According to [3], Eq. (33) has the same fixed point condition with the following equation: Following [3], when G converges, it satisfies the following equation according to the updating rule (10): which exactly satisfies the Eq. (34). Thus, the limit solution G  X  satisfies the KKT condition.
 Theorem 4. The limit solution F  X  satisfies the Karush-Kohn-Tucker (KKT) condition of the constrained optimiza-tion theory.

Proof. The objective function for F while fixing G is: Similar to the proof for Theorem 3, we also introduce the Lagrangian multiplier  X  which is a matrix and minimize the Lagrangian function: According to the KKT condition, we can get  X  X  ( F,  X )  X  X  = 0,  X  ik F ik = 0 and the following equation: Specifically, the derivation for F ik is as the following: Therefore, Eq. (38) becomes the following: Furthermore, the updating rule for the limit solution F  X  which is identical to Eq. (42). Thus, the limit solution F satisfies the KKT condition.
The G-orthogonal nonnegative matrix factorization is equiv-alent to relaxed K -means clustering method [3]. Thus, we can represent the objective function of the K -means with clustering indicator as the following: where W  X  &lt; d  X  k is the cluster centroid matrix, and H  X  &lt; k  X  n is the cluster indicator matrix and each column of H is coded as 1-of-K scheme. It satisfies HH T = I .
Similarly, we can use capped norm based objective to make K -means robust to outliers as our proposed capped norm orthogonal NMF. This new objective function is de-fined as following: where  X  &gt; 0 is a thresholding parameter. Apparently, be-cause HH T = I and W  X  0, hence the relaxed capped norm K -means method is equivalent to our proposed capped norm orthogonal NMF model.
In this section, we use both toy data and real-world datasets to show the performance of our proposed capped norm ONMF method.
Toy Data Setup The toy data is generated according to the equation X = FG T , where the feature dimension of X is 2, the number of samples is 20, and we set k = 1. Because Figure 1: The fitting result from both regular ONMF and our proposed capped norm ONMF in fitting 20 data points with 3 outliers. we want to show the robustness of our proposed method, three of the generated data samples are corrupted to be the outliers.

Implementation We use the ONMF proposed in [3] to be our comparative. The initialization of F and G for both ONMF and our proposed capped norm ONMF is generated randomly. For each data point, we use Fg j to fit the original data point. The fitting values from both ONMF and our proposed method are shown in Fig. 1. From Fig. 1 we can see that our method can fit the original data points with much higher accuracy, while the fitting curve of the traditional ONMF deviates a lot from the original one because of the existence of the outliers. Therefore, our proposed method is robust to the outliers.
In this section, we use 10 benchmark real world datasets, including 3 face datasets, 2 handwritten datasets, 1 object image dataset and 4 document datasets, to evaluate our pro-posed capped norm ONMF.

Data Description The following is the brief description of each dataset, and the detailed information about these 10 datasets are summarized in Table 1. Dataset Number of Instances Dimensions Classes Yale 165 1024 15 ORL 400 644 40 UMIST 575 644 20 MNIST 150 784 10 USPS 9298 256 10 COIL-20 1440 1024 20 BBCNews 2225 9635 5 BBCSport 737 4613 5
WebKB 4199 7770 4 20News 18774 61188 20
Experiment Setup For the image dataset, we normalize each data point as a vector with unit length. For the docu-ment dataset, we apply TF-IDF term weight normalisation to each data point and normalize the TF-IDF vector to unit length.

To evaluate the clustering performance, we adopt the widely used clustering accuracy and normalized mutual informa-tion as our metric. The details about the two metrics are described as the following: http://www.cs.cmu.edu/~WebKB/ http://people.csail.mit.edu/jrennie/20Newsgroups/ Figure 2: The randomly selected sample images and the outliers found by our method on Yale dataset.
 The ratio of the outliers is set as 0.05. Most of the outliers have bad illumination.
We compare our proposed method with K -Means, stan-dard NMF, traditional ONMF and ` 2 , 1 -norm NMF [14]. We run K -Means method for 50 times and choose the best clus-tering result, corresponding to the lowest objective value, as the K -Means final result. For fairness, we use such best K -Means result to initialize all the NMF methods as de-scribed in Algorithm 1. For each dataset, the value k is set to the real number of classes. In our experiment, the value of  X  is set according to the ratio of the outliers. For all the datasets, we set the ratio of outliers between 0.03 and 0.05.
Result Analysis The clustering accuracy and normalized mutual information results are shown in Table 2 and Table 3. In Table 2 we can see that our proposed method is very competitive, always better than the other 4 methods. From Table 3 we can also conclude that our proposed method results are better than other 4 methods in all datasets in terms of the normalized mutual information.

To illustrate the effectiveness of our method on outliers, we show the outliers of the Yale face dataset as an example in Table 4: The 10 most representative terms of each topic found by our method on BBCSport dataset athletics cricket football rugby tennis indoor test leagu robinson open kenteri seri unit nation 76 thanou bowl liverpool franc australian Fig. 2. In this dataset, each subject is taken pictures under different conditions. In our experiment, we set the ratio of the outliers as 5 percentage. As shown in the Fig. 2, all the outliers have a bad illumination comparing with the other sample images. They are odd compared with the other faces, therefore they are selected as the outliers by our proposed method automatically.

To show the correctness of our method on document dataset, we show the 10 most representative terms of each topic found by our method on BBCSport dataset in Table 4. In this and tennis . In the NMF method, each column of the basis matrix F corresponds to a topic, and the elements in such a column are the representative terms for the corresponding topic. We select the 10 most representative terms for each topic. From Table 4 we can see that the representative terms found by our method can describe the corresponding topic to some extent.
To further illustrate the effectiveness our proposed method, we construct a real word dataset with outliers. That is to add two images, which are randomly extracted from Cal-tech101 [6] dataset, to each class of ORL dataset, just as shown in Fig. 3. Due to the limitation of space, we only show the first ten subjects.
 We perform standard NMF, traditional ONMF, ` 2 , 1 -norm NMF and our method on this dataset. We reshape the basis factors f k contained in the matrix F  X  &lt; d  X  k to show the reconstructed confusion face images, just as shown in Fig. 4.
We can conclude that confusion face images produced by our proposed method are more clear than those produced by the other three NMF methods. As examples are the groups in the red rectangle in Fig. 4, the base faces produced by our method is disturbed by the noises less than the other three methods apparently. Specifically, our method can success-fully remove the effect from the outlier images, generating the face images rather than the outlier images.

In addition, we show the relationship between the ratio of outliers (determines the value of  X  ) and the clustering performance in Fig. 5. We perform this experiment on our constructed ORL dataset with outliers as shown in Fig. 3. shown in Fig. 5, when we set the ratio of outliers as a small value, some outliers will be remained, impairing the clus-Figure 3: The ORL data set with noise images from Caltech101. Each class has two noise images. tering performance. When we set it with a larger value, the performance is improved. Specifically, before the setting ratio approaches to 1 / 6, the performance is improved dra-performance is improved slightly due to the removement of the outliers.

Based on the experiment result of all the datasets, we can conclude that it is reasonable to use the more robust capped norm orthogonal NMF to deal with practical problems with outliers and noises.
In Fig. 6, we show the convergence of our proposed algo-rithm. As we have talked about in the previous part, we normalize each data point to unit length. In addition, the stop criteria for all the NMF methods is defined as following: where f t is the objective value in the t -th iteration. We im-plement our experiment with Matlab 2014 on an ordinary computer, which is configured with 4 Intel i5-4210U (up to 2.60GHz) cores, 8GB RAM and Windows 8.1 operating sys-tem.

Due to the limitation of the space, we only show the result of two datasets, ORL and UMIST. In Fig. 6, to make the difference among these four NMF methods clear, we only erations and time that each method needs to converge to the optimal solution. We can find that our method needs the least iterations to be converged. Comparing with the other two methods, ` 2 , 1 -norm based NMF and our method consume more time due to the re-weight strategy. Figure 4: The basis face images obtained by per-forming standard NMF, ONMF, ` 2 , 1 -norm NMF and our method on noise dataset. Four rows constitute a group where each group corresponds to a face. In each group, the first row is standard NMF, the second is ONMF, the third is the ` 2 , 1 -norm NMF, and the fourth is our proposed method. Obviously, in the images within the red rectangles, the out-liers effect the standard NMF, ONMF and ` 2 , 1 -norm NMF calculation. But our method can successfully remove the effect from these outlier images.
In this paper, we propose a new robust NMF method, which is called capped norm orthogonal NMF, to make the NMF method more robust to the outliers and noises. Al-Figure 5: The relationship between the setting ratio of outliers (determines the value of  X  ) and clustering performance on our constructed ORL dataset with 1 / 6 outliers. Figure 6: The convergence of the four NMF meth-ods on ORL and UMIST datasets.
 Table 5: The computational time and iterations of different methods to reach convergence.
 Dataset Metric NMF ONMF ` 2 , 1 NMF Ours though our proposed objective function is difficult to solve, we use the re-weighted strategy to solve it successfully. Fur-thermore, we prove the convergence and correctness of our proposed method. Extensive experiments for both synthetic and real world data demonstrate the correctness and effec-tiveness of our method in dealing with the clustering prob-lem. This work was supported in part by Australian Research Council (ARC) grants and U.S. NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628. [1] M. Cooper and J. Foote. Summarizing video using [2] C. Ding, T. Li, and M. I. Jordan. Convex and [3] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [4] C. H. Ding, X. He, and H. D. Simon. On the [5] L. Du, X. Li, and Y.-D. Shen. Robust nonnegative [6] L. Fei-Fei, R. Fergus, and P. Perona. Learning [7] D. B. Graham and N. M. Allinson. Characterising [8] D. Greene and P. Cunningham. Practical solutions to [9] A. B. Hamza and D. J. Brady. Reconstruction of [10] P. O. Hoyer. Non-negative matrix factorization with [11] J. Huang, F. Nie, H. Huang, and C. Ding. Robust [12] J. J. Hull. A database for handwritten text recognition [13] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. [14] D. Kong, C. Ding, and H. Huang. Robust nonnegative [15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [16] D. D. Lee and H. S. Seung. Algorithms for [17] S. A. Nene, S. K. Nayar, H. Murase, et al. Columbia [18] P. Paatero and U. Tapper. Positive matrix [19] F. S. Samaria and A. C. Harter. Parameterisation of a [20] T. Sim, S. Baker, and M. Bsat. The cmu pose, [21] H. Wang, H. Huang, and C. Ding. Cross-language web [22] H. Wang, H. Huang, and C. Ding. Simultaneous [23] H. Wang, H. Huang, C. Ding, and F. Nie. Predicting [24] H. Wang, H. Huang, F. Nie, and C. Ding. Nonnegative [25] H. Wang, F. Nie, H. Huang, and F. Makedon. Fast [26] W. Xu, X. Liu, and Y. Gong. Document clustering [27] L. Zhang, Z. Chen, M. Zheng, and X. He. Robust
