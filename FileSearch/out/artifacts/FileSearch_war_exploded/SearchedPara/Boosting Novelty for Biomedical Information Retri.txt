 In information retrieval, we are interested in the information that is not only relevant but also novel. In this paper, we study how to boost novelty for biomedical information retrieval through prob-abilistic latent semantic analysis. We conduct the study based on TREC Genomics Track data. In TREC Genomics Track, each topic is considered to have an arbitrary number of aspects, and the nov-elty of a piece of information retrieved, called a passage , is assessed based on the amount of new aspects it contains. In particular, the aspect performance of a ranked list is rewarded by the number of new aspects reached at each rank and penalized by the amount of irrelevant passages that are rated higher than the novel ones. There-fore, to improve aspect performance, we should reach as many as-pects as possible and as early as possible. In this paper, we make a preliminary study on how probabilistic latent semantic analysis can help capture different aspects of a ranked list, and improve its per-formance by re-ranking. Experiments indicate that the proposed approach can greatly improve the aspect-level performance over baseline algorithm Okapi BM25.
 H.3.3 [ Information Systems ]: Information Search and Retrieval Algorithms, Experimentation Genomics IR, passage retrieval, aspect search
Information retrieval (IR) in the context of biomedical databases is characterized by the frequent use of abundant acronyms, homonyms and synonyms. How to deal with the tremendous variants of the same term has been a challenging task in biomedical IR. The Ge-nomics track of Text REtrieval Conference (TREC) provided a com-mon platform to evaluate the methods and techniques proposed by various research groups for biomedical IR. In its last two years (2006 &amp; 2007), the Genomics track focused on the passage re-trieval for question answering, where a passage is a piece of contin-uous text ranging from a phrase up to a paragraph of a document. One of the performances concerned for passage retrieval was the aspect-based mean average precision (MAP) [8]. To evaluate the performance of a ranked list, in 2006, the judges of the competition fi rst identi fi ed all relevant passages for each topic from all submis-sions, and then, based on the content of such relevant passages, assigned a set of Medical Subject Headings (MeSH) terms to each topic as their representative  X  X spects X . In 2007, instead of MeSH terms, the judges picked and assigned terms from the pool of nom-inated passages deemed relevant to each topic as their  X  X spects X . That is, the  X  X spects X  of a topic in Genomics Track are represented by a set of terms. A passage for a topic is novel if it contains as-pect terms assigned to the topic which has not appeared in the pas-sages ranked higher. The novelty of a ranked list is rewarded by the amount of relevant aspects reached at each rank and penalized by the amount of irrelevant passages ranked higher than novel ones. The aspect-based MAP is an average re fl ection of novelty retrieval performance on all topics.

For the aspect-level evaluation, the search should reach as many relevant aspects as possible and rank their containing passages as high as possible.  X  X spects X  are assigned to each topic by the judges only after the submission by all groups, and such aspects are picked only from the nominated passages. At competition, nobody knows how many aspects there exist for each topic in the literature, and what they are. Therefore,  X  X spects X  of each topic in this problem are latent, and it is also not an easy problem to fi gure out the as-pects covered by a passage from its  X  X ag-of-words X  representation. However, it is well known that a topic model can represent a doc-ument as a mixture of latent aspects. That is, a topic model can convert a document from its  X  X ag-of-words X  space to its latent se-mantic space of a reduced dimensionality. In this paper, we study whether the latent semantic representation would help capture dif-ferent  X  X spects X  of a passage and further improve the performance of a ranked list by re-ranking. There exist a list of topic models such as Latent Semantic Analysis (LSA)[5], Probabilistic Latent Seman-tic Analysis (PLSA) [9], and Latent Dirichlet Allocation (LDA) [2]. In this preliminary study, we focus on PLSA. In the future, we would study the problem with both LSA and LDA included.

To the best of our knowledge, this is the fi rst investigation about how well a topic model such as PLSA can help capture hidden as-pects in novelty information retrieval. In the investigation, we also examine the hyperparameter settings for PLSA such as initial con-ditional probabilities and zero estimate smoothing in the context of our problem. Besides standard PLSA model [9], we also examine its variants, e.g. instead of word frequencies, tf-idf weighting is used.
In information retrieval, ranking based on pure relevance may not be suf fi cient when the potential relevant documents are huge and highly redundant with each other. In [3, 16, 14, 15, 18], differ-ent ways representing and optimizing novelty and diversity of the retrieved documents are studied. The objective is to fi nd the docu-ments that cover as many different aspects (subtopics) as possible while maintaining minimal redundancy. One problem with the nov-elty (diversity, aspect, subtopic)-based retrieval is how to evaluate the ranking quality. In [14], 3 metrics are introduced: the subtopic recall measures the percentage of subtopics covered as a function of rank, the subtopic precision measures the precision of the re-trieved documents as a function of the minimal rank for a certain subtopic recall, and the weighted subtopic precision measures the precision with redundancy penalized based on ranking cost. In [4], a cumulative gain-based metric is proposed to measure the novelty and redundancy, which is also a function of rank.

Most existing methods [3, 16, 14, 15] improve novelty in IR by penalizing redundancy, but they seem not to work well in Genomics aspect search. In the 2006 TREC Genomics track, University of Wisconsin at Madison failed to promote novelty by penalizing re-dundancy based on a clustering-based approach [7]. In the 2007 TREC Genomics track, most teams simply submitted their relevant passage retrieval results for aspect evaluation such as National Li-brary of Medicine (NLM) [6] and University of Illinois at Chicago [17]. In [10] and [12], a Bayesian learning approach is proposed to fi nd potential aspects for different topics. In [13], a survival model approach is applied to biomedical search diversi fi cation with Wikipedia.
It is well known that PLSA can help to reveal semantic rela-tions between entities of interest in a principled way [9]. In this paper, we consider each retrieved passage d i ( 1  X  i  X  N D = { d 1 , ..., d N } as being generated under the in fl uence of a num-ber of hidden aspect factors Z = { z 1 , ..., z K } with words from a vocabulary W = { w 1 , ..., w M } . Therefore, all passages retrieved initially can be described as an N  X  M matrix T =(( c ( d where c ( d i ,w j ) is the number of times w j appears in passage Each row in D is then a frequency vector that corresponds to a passage. Assume given a hidden aspect factor z , a passage dependent of the word w . Then by Bayes X  rule, the joint probability P ( d, w ) can be obtained as follows: To explain the observed frequencies in matrix T , we need to fi nd P ( z ) ,P ( d | z ) , and P ( w | z ) that maximize the following likelihood function: It can be shown that the solution can be achieved by EM algorithm iteratively through the following two alternating steps. 1. By E-step, we calculate the posterior probabilities of the hid-2. By M-step, we update parameters to maximize the complete After its convergence, we can calculate the probability of hidden aspect factor z given passage d by P ( z | d )= P ( d Hence, we can summarize the aspect trend of each passage d a normalized factor vector ( P ( z i | d )) K i =1 . By this way, we trans-form the passage representation from the  X  X ag-of-words" space to a lower latent semantic space. We expect this representation would capture the aspect trend of each passage in a better way. All pas-sages can then be clustered based on this vector representation or simply based on their most probable hidden aspect factor With latter, we may sort all passages in each group based on the probability P ( z d | d ) in descending order. By either way, we can al-ways re-rank retrieved passages by repetitively picking one passage from the top of each group until none is left. We test our method on a set of runs obtained by the improved Okapi retrieval system [11] for TREC Genomics Track 2007 topics. The set of runs are acquired under different conditions as shown in Tables1to4,where k1 and b are tuning constants of the weighting function BM25. Indexing on database could be paragraph-based (where each piece of indexed information is a paragraph from doc-uments) or word-based (where each piece of indexed information has a limited number of words), and topic expansion is applied once based on uni fi ed medical language system (UMLS). To enhance the performance of these runs, feedback analysis is performed by the Okapi retrieval system. In feedback analysis, the system retrieves ten passages that are deemed most relevant for a particular topic, and forms a list of the most recurring words from those passages. Each topic is expanded by these words, and then relevant passages for the extended topic is retrieved. Each feedback term is assigned a weight by Okapi. In our experiments, feedback weight is set to 0.25.

To get their vector representation, we apply both Porter stem-ming and a stoplist with general stopwords to passages. After Porter stemming and stoplist application, around 4000 words are left for each topic. All passages nominated for each topic are then represented with these words weighted by tf-idf (Better performance is observed with tf-idf instead of frequency used in the standard PLSA model as described in Section 3). We try to use princi-pal component analysis (PCA) to reduce vector dimensionality. It seems PCA is not very helpful in reducing vector dimensionality without hurting performance in this problem. It might be because of the sparsity of data, no obvious dimensions are much more im-portant than others, and every word has some contribution in rep-resenting passages nominated for a topic.

Topic models like PLSA typically operate in extremely high di-mensional spaces. As a consequence, the  X  X urse of dimensionality X  is lurking around the corner, and thus the hyperparameters (such as initial conditional probabilities and smoothing parameters) set-tings have the potential to signi fi cantly affect the results [1]. In the experiments, we fi nd that we cannot start PLSA model with a uniform distribution for P ( z ) ,P ( d | z ) ,and P ( w | convergence will happen immediately in the fi rst iteration due to the sparsity of data. Instead, we start with a normalized random distribution for all these conditional probabilities (the results re-ported in this paper are the average of a few runs). Due to the large dimensionality, there are a lot of zero probabilities in each passage vector representation. Zero estimates could cause signi fi cant prob-lems such as zeroing-out the impact of some other useful parame-ters in multiplication. Zero estimates could also cause computation problems such as  X  X ivision by zero X . In our experiments, we apply Laplace smoothing to avoid zero probability estimates. We add a small value 2  X  52 to all probabilities before normalization. In the future, more smoothing techniques would be studied.
In the experiment, we examine two ways of clustering passages in latent semantic space: one is centroid-based clustering with dif-ferent distance functions (squared Euclidean, cosine, and cityblock) and the other is based on their most probable aspect factor. It is found that our problem is not so sensitive to either way of cluster-ing, and for the former, not so much sensitive to the change of dis-tance functions. We believe that this is also caused by the sparsity of data. Our experiment results reported here are from centroid-based clustering with cityblock distance function. In the future, we would explore other clustering algorithms that might be more suitable to our problem such as hierarchical clustering and density-based clustering.
In the experiments, we change the number of hidden aspects K from 1 to 10 continuously for all runs. When the number of hid-den aspects is set to 1, there is no re-ranking and hence the perfor-mances are the same as the original runs. It turns out for all other 9 different hidden aspect numbers, all runs get positive performance improvements by re-ranking as shown in Tables 1 to 4. To illustrate the re-ranking performance graphically, we plot the data in Figures 1 to 4, respectively, where y-axis stands for the aspect-level per-formance MAP. It can be observed that on all 9 different number of hidden factors, the re-ranked results are all better than the orig-inal ones. Over all runs, the maximum improvement is 46.97% when K =5 for run2, the minimum improvement is 1.47% when K =7 for run4, and the average improvement is 20.06%. This is illustrated in Figure 5.

It should be noted that the hidden aspect factors in PLSA mod-els are not necessarily the same as the aspects of Genomics Track. In PLSA models, the number of hidden aspect factors is a tuning variable, while the aspects of Genomics Track topics are constants once the corpus and topics are determined. The hidden aspect fac-tors in PLSA models are statistically identi fi ed from data while the aspects of Genomics Track topics are assigned by the judges but not results of statistical analyses. Since PLSA models are good in semantic analysis and synonym and concept recognition [9], we use the hidden aspect factors identi fi ed by PLSA models to clas-sify passages and then use this classi fi cation information to re-rank ranked lists in the hope that the hidden aspect factors do have some correlation with topic aspects in some way. Our experiment results highly support the hope.
In this paper, we conducted a preliminary study on using PLSA models to capture hidden aspects of retrieved passages. The hidden aspects caught are used to improve the performance of a ranked list by re-ranking. It turned out all runs on all 9 continuous hidden aspect numbers got positive improvements. This indicates PLSA models are very promising in fi nding diverse aspects in retrieved passages. By contrast, it was indicated [7] a clustering-based method always failed to improve the aspect performance over baseline al-gorithms.

In the future, more experiments will be conducted to further in-vestigate the proposed method. We will extend the method to more runs, and will study whether there exist a range of hidden aspect numbers that can always be safely used in re-ranking to improve performance. In addition, we will investigate how to set different hidden aspect numbers for different topics. We will also examine other topic models such as LDA and LSA on this matter.

This research is supported by the research grant from the Natu-ral Sciences &amp; Engineering Research Council (NSERC) of Canada. We thank anonymous reviewers for their thorough review com-ments on this paper.
