 Commercial search engines use query associations in a variety of ways, including the recommendation of related queries in Bing,  X  X omething different X  in Google, and  X  X lso try X  and related concepts in Ya-hoo. Mining techniques to extract such query asso-ciations generally fall into four categories: (a) clus-tering queries by their co-clicked url patterns (Wen et al., 2001; Baeza-Yates et al., 2004); (b) leveraging co-occurrences of sequential queries in web search query sessions (Zhang and Nasraoui, 2006; Boldi et al., 2009); (c) pattern-based extraction over lexico-syntactic structures of individual queries (Pas  X ca and Durme, 2008; Jain and Pantel, 2009); and (d) distri-butional similarity techniques over news or web cor-pora (Agirre et al., 2009; Pantel et al., 2009). These techniques operate at the surface level, associating one surface context (e.g., queries) to another.
In this paper, we focus instead on associating sur-face contexts with entities that refer to a particu-lar entry in a knowledge base such as Freebase, IMDB, Amazon X  X  product catalog, or The Library of Congress. Whereas the former models might as-sociate the string  X  Ronaldinho  X  with the strings  X  AC Milan  X  or  X  Lionel Messi  X , our goal is to associate  X  Ronaldinho  X  with, for example, the Wikipedia en-tity page  X  wiki/AC Milan  X  or the Freebase entity  X  en/lionel mess  X . Or for the query string  X  ice fish-ing  X , we aim to recommend products in a commer-cial catalog, such as jigs or lures.
 The benefits and potential applications are large. By knowing the entity identifiers associated with a query (instead of strings), one can greatly improve both the presentation of search results as well as the click-through experience. For example, consider when the associated entity is a product. Not only can we present the product name to the web user, but we can also display the image, price, and re-views associated with the entity identifier. Once the entity is clicked, instead of issuing a simple web search query, we can now directly show a product page for the exact product; or we can even perform actions directly on the entity, such as buying the en-tity on Amazon.com, retrieving the product X  X  oper-ating manual, or even polling your social network for friends that own the product. This is a big step towards a richer semantic search experience.
In this paper, we define the association between a query string q and an entity id e as the probability that e is relevant given the query q , P ( e | q ) . Fol-lowing Baeza-Yates et al. (2004), we model rele-vance as the likelihood that a user would click on e given q , events which can be observed in large query-click graphs. Due to the extreme sparsity of query click graphs (Baeza-Yates, 2004), we pro-pose several smoothing models that extend the click graph with query synonyms and then use the syn-onym click probabilities as a background model. We demonstrate the effectiveness of our smoothing models, via a large-scale empirical study over real-world data, which significantly reduce model errors. We further apply our models to the task of query-product recommendation. Queries in session logs are annotated using our association probabilities and recommendations are obtained by modeling session-level query-product co-occurrences in the annotated sessions. Finally, we demonstrate that our models affect 9% of general web queries with 94% recom-mendation precision. We introduce a novel application of significant com-mercial value: entity recommendations for general Web queries. This is different from the vast body of work on query suggestions (Baeza-Yates et al., 2004; Fuxman et al., 2008; Mei et al., 2008b; Zhang and Nasraoui, 2006; Craswell and Szummer, 2007; Jagabathula et al., 2011), because our suggestions are actual entities (as opposed to queries or docu-ments). There is also a rich literature on recom-mendation systems (Sarwar et al., 2001), including successful commercial systems such as the Ama-zon product recommendation system (Linden et al., 2003) and the Netflix movie recommendation sys-tem (Bell et al., 2007). However, these are entity-to-entity recommendations systems. For example, Netflix recommends movies based on previously seen movies (i.e., entities). Furthermore, these sys-tems have access to previous transactions (i.e., ac-tual movie rentals or product purchases), whereas our recommendation system leverages a different re-source, namely query sessions.

In principle, one could consider vertical search engines (Nie et al., 2007) as a mechanism for as-sociating queries to entities. For example, if we type the query  X  X anon eos digital camera X  on a commerce search engine such as Bing Shopping or Google Products, we get a listing of digital camera entities that satisfy our query. However, vertical search en-gines are essentially rankers that given a query, re-turn a sorted list of (pointers to) entities that are re-lated to the query. That is, they do not expose actual association scores, which is a key contribution of our work, nor do they operate on general search queries.
Our smoothing methods for estimating associ-ation probabilities are related to techniques de-veloped by the NLP and speech communities to smooth n -gram probabilities in language model-ing. The simplest are discounting methods, such as additive smoothing (Lidstone, 1920) and Good-Turing (Good, 1953). Other methods leverage lower-order background models for low-frequency events, such as Katz X  backoff smoothing (Katz, 1987), Witten-Bell discounting (Witten and Bell, 1991), Jelinek-Mercer interpolation (Jelinek and Mercer, 1980), and Kneser-Ney (Kneser and Ney, 1995).
 In the information retrieval community, Ponte and Croft (1998) are credited for accelerating the use of language models. Initial proposals were based on learning global smoothing models, where the smoothing of a word would be independent of the document that the word belongs to (Zhai and Laf-ferty, 2001). More recently, a number of local smoothing models have been proposed (Liu and Croft, 2004; Kurland and Lee, 2004; Tao et al., 2006). Unlike global models, local models leverage relationships between documents in a corpus. In par-ticular, they rely on a graph structure that represents document similarity. Intuitively, the smoothing of a word in a document is influenced by the smoothing of the word in similar documents. For a complete survey of these methods and a general optimization framework that encompasses all previous proposals, please see the work of Mei, Zhang et al. (2008a). All the work on local smoothing models has been applied to the prediction of priors for words in docu-ments. To the best of our knowledge, we are the first to establish that query-click graphs can be used to create accurate models of query-entity associations. Task Definition: Consider a collection of entities E . Given a search query q , our task is to compute P ( e | q ) , the probability that an entity e is relevant to q , for all e  X  E .

We limit our model to sets of entities that can be accessed through urls on the web, such as Ama-zon.com products, IMDB movies, Wikipedia enti-ties, and Yelp points of interest.

Following Baeza-Yates et al. (2004), we model relevance as the click probability of an entity given a query, which we can observe from click logs of vertical search engines, i.e., domain-specific search engines such as the product search engine at Ama-zon, the local search engine at Yelp, or the travel search engine at Bing Travel. Clicked results in a vertical search engine are edges between queries and entities e in the vertical X  X  knowledge base. General search query click logs, which capture direct user intent signals, have shown significant improvements when used for web search ranking (Agichtein et al., 2006). Unlike for general search engines, vertical search engines have typically much less traffic re-sulting in extremely sparse click logs.

In this section, we define a graph structure for recording click information and we propose several models for estimating P ( e | q ) using the graph. 3.1 Query Entity Click Graph We define a query entity click graph , QEC ( Q  X  U  X  E,C u  X  C e ) , as a tripartite graph consisting of a set of query nodes Q , url nodes U , entity nodes E , and weighted edges C u exclusively between nodes of Q and nodes of U , as well as weighted edges C e ex-clusively between nodes of Q and nodes of E . Each edge in C u and C e represents the number of clicks observed between query-url pairs and query-entity pairs, respectively. Let w u ( q,u ) be the click weight of the edges in C u , and w e ( q,e ) be the click weight of the edges in C e .

If C e is very large, then we can model the associa-tion probability, P ( e | q ) , as the maximum likelihood estimation (MLE) of observing clicks on e given the query q :
Figure 1 illustrates an example query entity graph linking general web queries to entities in a large commercial product catalog. Figure 1a illus-trates eight queries in Q with their observed clicks (solid lines) with products in E 1 . Some probabil-ity estimates, assigned by Equation 3.1, include:  X  P mle ( panfish jigs ,e 1 ) = 0 ,
Even for the largest search engines, query click logs are extremely sparse, and smoothing techniques are necessary (Craswell and Szummer, 2007; Gao et al., 2009). By considering only C e , those clicked urls that map to our entity collection E , the sparsity situation is even more dire. The sparsity of the graph comes in two forms: a) there are many queries for which an entity is relevant that will never be seen in the click logs (e.g.,  X  X anfish jig X  in Figure 1a); and b) the query-click distribution is Zipfian and most observed edges will have very low click counts yielding unreliable statistics. In the following sub-sections, we present a method to expand QEC with unseen queries that are associated with entities in E . Then we propose smoothing methods for leveraging a background model over the expanded click graph.
Throughout our models, we make the simplifying assumption that the knowledge base E is complete. 3.2 Graph Expansion Following Gao et al. (2009), we address the spar-sity of edges in C e by inferring new edges through traversing the query-url click subgraph, UC ( Q  X  U,C u ) , which contains many more edges than C e . If two queries q i and q j are synonyms or near syn-onyms 2 , then we expect their click patterns to be similar.

We define the synonymy similarity, s ( q i ,q j ) as the cosine of the angle between q i and q j , the click pattern vectors of q i and q j , respectively: where q is an n u dimensional vector consisting of the pointwise mutual information between q and each url u in U , pmi( q,u ) : PMI is known to be biased towards infrequent events. We apply the discounting factor,  X  ( q,u ) , proposed in (Pantel and Lin, 2002): Enrichment: We enrich the original QEC graph by creating a new edge { q 0 , e } , where q 0  X  Q and e  X  E , if there exists a query q where s ( q,q 0 ) &gt;  X  and w e ( q,e ) &gt; 0 .  X  is set experimentally, as described in Section 5.2.

Figure 1b illustrates similarity edges created be-tween query  X  X ce auger X  and both  X  X ower auger X  and  X  X  rock X . Since  X  X ce auger X  was connected to entities e 3 and e 4 in the original QEC , our expan-sion model creates new edges in C e between { power auger, e 3 } , { power auger, e 4 } , and { d rock, e 3 } .
For each newly added edge { q , e } ,  X  P mle = 0 ac-cording to our model from Equation 3.1 since we have never observed any clicks between q and e . In-stead, we define a new model that uses  X  P mle when clicks are observed and otherwise assigns uniform probability mass, as: where  X  ( q,e ) is an indicator variable which is 1 if there is an edge between { q,e } in C e .

This model does not leverage the local synonymy graph in order to transfer edge weight to unseen edges. In the next section, we investigate smooth-ing techniques for achieving this. 3.3 Smoothing Smoothing techniques can be useful to alleviate data sparsity problems common in statistical models. In practice, methods that leverage a background model (e.g., a lower-order n -gram model) have shown most promise (Katz, 1987; Witten and Bell, 1991; Je-linek and Mercer, 1980; Kneser and Ney, 1995). In this section, we present two smoothing methods, de-rived from Jelinek-Mercer interpolation (Jelinek and Mercer, 1980), for estimating the target association probability P ( e | q ) .

Figure 1c highlights two edges, illustrated with dashed lines, inserted into C e during the graph ex-pansion phase of Section 3.2.  X  w e ( q,e ) represents the weight of our background model, which can be viewed as smoothed click counts, and are obtained by propagating clicks to unseen edges using the syn-onymy model as follows: where N s q = P q 0  X  Q s ( q,q 0 ) . By normalizing the smoothed weights, we obtain our background model,  X  P bsim : Below we propose two models for interpolating our foreground model from Equation 3.1 with the back-ground model from Equation 3.5.
 Basic Interpolation: This smoothing model,  X  P intu ( e | q ) , linearly combines our foreground and background models using a model parameter  X  : Bucket Interpolation: Intuitively, edges { q,e }  X  C e with higher observed clicks, w e ( q,e ) , should be trusted more than those with low or no clicks. A ground and background models in the same way ir-respective of the observed foreground clicks. Our final model,  X  P intp ( e | q ) parameterizes the interpola-tion by the number of observed clicks:
In practice, we bucket the observed click parame-ter, w e ( q,e ) , into eleven buckets: { 1-click, 2-clicks, ..., 10-clicks, more than 10 clicks } .

Section 5.2 outlines our procedure for learn-ing the model parameters for both  X  P intu ( e | q ) and  X  P intp ( e | q ) . 3.4 Summary Table 1 summarizes the association models pre-sented in this section as well as a strawman that as-signs uniform probability to all edges in QEC :
In the following section, we apply these models to the task of extracting product recommendations for general web search queries. A large-scale exper-imental study is presented in Section 5 supporting the effectiveness of our models. Query recommendations are pervasive in commer-cial search engines. Many systems extract recom-mendations by mining temporal query chains from search sessions and clickthrough patterns (Zhang and Nasraoui, 2006). We adopt a similar strategy, except instead of mining query-query associations, we propose to mine query-entity associations, where entities come from an entity database as described in Section 1. Our technical challenge lies in annotating sessions with entities that are relevant to the session. 4.1 Product Entity Domain Although our model generalizes to any entity do-main, we focus now on a product domain. Specifi-cally, our universe of entities, E , consists of the enti-ties in a large commercial product catalog, for which we observe query-click-product clicks, C e , from the vertical search logs. Our QEC graph is completed by extracting query-click-urls from a search engine X  X  general search logs, C u . These datasets are de-scribed in Section 5.1. 4.2 Recommendation Algorithm We hypothesize that if an entity is relevant to a query, then it is relevant to all other queries co-occurring in the same session. Key to our method are the models from Section 3.
 Step 1  X  Query Annotation: For each query q in a session s , we annotate it with a set E q , consisting of every pair { e,  X  P ( e | q ) } , where e  X  E such that there exists an edge { q,e }  X  C e with probability  X  P ( e | q ) . Note that E q will be empty for many queries. Step 2  X  Session Analysis: We build a query-entity frequency co-occurrence matrix, A , consist-ing of n | Q | rows and n | E | columns, where each row corresponds to a query and each column to an entity. The value of the cell A qe is the sum over each ses-sion s , of the maximum edge weight between any query q 0  X  s and e 3 : where S consists of all observed search sessions and:  X  ( s,e ) = arg max Step 3  X  Ranking: We compute ranking scores between each query q and entity e using pointwise mutual information over the frequencies in A , simi-larly to Eq. 3.2.

The final recommendations for a query q are ob-tained by returning the top-k entities e according to Step 3. Filters may be applied on: f the frequency A qe ; and p the pointwise mutual information rank-ing score between q and e . 5.1 Datasets We instantiate our models from Sections 3 and 4 us-ing search query logs and a large catalog of prod-ucts from a commercial search engine. We form our QEC graphs by first collecting in C e aggregate query-click-entity counts observed over two years in a commerce vertical search engine. Similarly, C u is formed by collecting aggregate query-click-url counts observed over six months in a web search en-gine, where each query must have frequency at least 10. Three final QEC graphs are sampled by taking various snapshots of the above graph as follows: a) T
RAIN consists of 50% of the graph; b) T EST con-sists of 25% of the graph; c) D EV consists of 25% of the graph. 5.2 Association Models 5.2.1 Model Parameters We tune the  X  parameters for  X  P intu and  X  P intp against the D EV QEC graph. There are twelve parameters to be tuned:  X  for  X  P intu and  X  (1) ,  X  (2) , ...,  X  (10) ,  X  ( &gt; 10) for  X  P intp , where  X  ( x ) is the observed click bucket as described in Section 3.3. For each, we choose the parameter value that minimizes the mean-squared error (MSE) of the D EV set, where model probabilities are computed using the T RAIN QEC graph. Figure 2 illustrates the MSE ranging over [0, 0.05, 0.1, ..., 1].

We trained the query synonym model of Sec-tion 3.2 on the D EV set and hand-annotated 100 ran-dom synonymy pairs according to whether or not the pairs were synonyms 2 . Setting  X  = 0 . 4 results in a precision &gt; 0.9. 5.2.2 Analysis We evaluate the quality of our models in Table 1 by evaluating their mean-squared error (MSE) against the target P ( e | q ) computed on the T EST set: where C T e are the edges in the T EST QEC graph with weight w T e ( q,e ) , P T ( e | q ) is the target proba-bility computed over the T EST QEC graph, and  X  P is one of our models trained on the T RAIN QEC graph. MSE measures against each edge type, which makes it sensitive to the long tail of the click graph. Conversely, MSE W measures against each edge instance, which makes it a good mea-sure against the head of the click graph. We expect our smoothing models to have much more impact on MSE (i.e., the tail) than on MSE W since head queries do not suffer from data sparsity.

Table 2 lists the MSE and MSE W results for each model. We consider  X  P unif as a strawman and  X  P mle as a strong baseline (i.e., without any graph expansion nor any smoothing against a background model).  X  P unif performs generally very poorly, how-ever  X  P mle is much better, with an expected estima-tion error of 0.16 accounting for an MSE of 0.0261. As expected, our smoothing models have little im-provement on the head-sensitive metric ( MSE W ) relative to  X  P mle . In particular,  X  P hybr performs nearly identically to  X  P mle on the head. On the tail, all three smoothing models significantly outperform  X  P mle with  X  P intp reducing the error by 18.4%. Table 3 lists query-product associations for five randomly sam-pled products along with their model scores from  X  P
Figure 3 provides an intrinsic view into MSE as a function of the number of observed clicks in the T
EST set. As expected, for larger observed click counts ( &gt; 4), all models perform roughly the same, indicating that smoothing is not necessary. However, for low click counts, which in our dataset accounts for over 20% of the overall click instances, we see a large reduction in MSE with  X  P intp outperforming  X  P intu , which in turn outperforms forms very poorly. The reason it does worse as the observed click count rises is that head queries tend to result in more distinct urls with high-variance clicks, which in turn makes a uniform model susceptible to more error.

Figure 3 illustrates that the benefit of the smooth-ing models is in the tail of the click graph, which supports the larger error reductions seen in MSE in Table 2. For associations only observed once,  X  P intp reduces the error by 29% relative to  X  P mle .
We also performed an editorial evaluation of the query-entity associations obtained with bucket inter-polation. We created two samples from the TEST dataset: one randomly sampled by taking click weights into account, and the other sampled uni-formly at random. Each set contains results for 100 queries. The former consists of 203 query-product associations, and the latter of 159 associa-tions. The evaluation was done using Amazon Me-chanical Turk 4 . We created a Mechanical Turk HIT 5 where we show to the Mechanical Turk workers the query and the actual Web page in a Product search engine. For each query-entity association, we gath-ered seven labels and considered an association to be correct if five Mechanical Turk workers gave a pos-itive label. An association was considered to be in-correct if at least five workers gave a negative label. Borderline cases where no label got five votes were discarded (14% of items were borderline for the uni-form sample; 11% for the weighted sample). To en-sure the quality of the results, we introduced 30% of incorrect associations as honeypots. We blocked workers who responded incorrectly on the honey-pots so that the precision on honeypots is 1. The result of the evaluation is that the precision of the as-sociations is 0.88 on the weighted sample and 0.90 on the uniform sample. 5.3 Related Product Recommendation We now present an experimental evaluation of our product recommendation system using the baseline model  X  P mle and our best-performing model  X  P intp . The goals of this evaluation are to (1) determine the quality of our product recommendations; and (2) assess the impact of our association models on the product recommendations. 5.3.1 Experimental Setup We instantiate our recommendation algorithm from Section 4.2 using session co-occurrence frequencies from a one-month snapshot of user query sessions at a Web search engine, where session boundaries oc-cur when 60 seconds elapse in between user queries. We experiment with the recommendation parame-ters defined at the end of Section 4.2 as follows: k = 10, f ranging from 10 to 100, and p ranging from 3 to 10.

For each configuration, we report coverage as the total number of queries in the output (i.e., the queries for which there is some recommendation) divided by the total number of queries in the log. For our per-formance metrics, we sampled two sets of queries: (a) Query Set Sample : uniform random sam-ple of 100 queries from the unique queries in the one-month log; and (b) Query Bag Sample : weighted random sample, by query frequency, of 100 queries from the query instances in the one-month log. For each sample query, we pooled to-gether and randomly shuffled all recommendations by our algorithm using both  X  P mle and  X  P intp on each parameter configuration. We then manually anno-tated each { query, product } pair as relevant , mildly relevant or non-relevant . In total, 1127 pairs were annotated. Interannotator agreement between two judges on this task yielded a Cohen X  X  Kappa (Cohen, 1960) of 0.56. We therefore collapsed the mildly relevant and non-relevant classes yielding two final classes: relevant and non-relevant . Cohen X  X  Kappa on this binary classification is 0.71.

Let C M be the number of relevant (i.e., correct) suggestions recommended by a configuration M and let | M | be the number of recommendations returned by M . Then we define the (micro-) precision of M as: P M = C M C . We define relative recall (Pantel et al., 2004) between two configurations M 1 and M 2 as R M 5.3.2 Results Table 4 summarizes our results for some configura-tions (others omitted for lack of space). Most re-markable is the { f = 10 , p = 10 } configuration where the  X  P intp model affected 9.4% of all query instances posed by the millions of users of a major search engine , with a precision of 94%. Although this model covers 0.8% of the unique queries, the fact that it covers many head queries such as wal-mart and iphone accounts for the large query in-stance coverage. Also since there may be many gen-eral web queries for which there is no appropriate product in the database, a coverage of 100% is not attainable (nor desirable); in fact the upper bound for the coverage is likely to be much lower.
Turning to the impact of the association models on product recommendations, we note that precision is stable in our  X  P intp model relative to our baseline  X  P mle model. However, a large lift in relative recall is observed, up to a 19% increase for the { f = 100 , p = 10 } configuration. These results are consistent with those of Section 5.2, which compared the asso-ciation models independently of the application and showed that  X  P intp outperforms  X  P mle .

Table 5 shows sample product recommendations discovered by our  X  P intp model. Manual inspection revealed two main sources of errors. First, ambiguity is introduced both by the click model and the graph expansion algorithm of Section 3.2. In many cases, the ambiguity is resolved by user click patterns (i.e., users disambiguate queries through their browsing behavior), but one such error was seen for the query  X  shark attack videos  X  where several Shark -branded vacuum cleaners are recommended. This is because of the ambiguous query  X  shark  X  that is found in the click logs and in query sessions co-occurring with the query  X  shark attack videos  X . The second source of errors is caused by systematic user errors com-monly found in session logs such as a user acciden-tally submitting a query while typing. An example session is: {  X  speedo  X ,  X  speedometer  X  } where the in-tended session was just the second query and the un-intended first query is associated with products such as Speedo swimsuits . This ultimately causes our sys-tem to recommend various swimsuits for the query  X  speedometer  X . Learning associations between web queries and entities has many possible applications, including query-entity recommendation, personalization by associating entity vectors to users, and direct adver-tising. Although many techniques have been devel-oped for associating queries to queries or queries to documents, to the best of our knowledge this is the first that aims to associate queries to entities by leveraging click graphs from both general search logs and vertical search logs.

We developed several models for estimating the probability that an entity is relevant given a user query. The sparsity of query entity graphs is ad-dressed by first expanding the graph with query synonyms, and then smoothing query-entity click counts over these unseen queries. Our best per-forming model, which interpolates between a fore-ground click model and a smoothed background model, significantly reduces testing error when com-pared against a strong baseline, by 18%. On associ-ations observed only once in our test collection, the modeling error is reduced by 29% over the baseline.
We applied our best performing model to the task of query-entity recommendation, by analyz-ing session co-occurrences between queries and an-notated entities. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.

