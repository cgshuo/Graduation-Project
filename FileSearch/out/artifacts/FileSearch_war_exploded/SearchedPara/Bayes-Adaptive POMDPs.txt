 sross12@cs.mcgill.ca unknown parameters.
 finite subspace necessary for  X  -optimal solutions.
 to learn good POMDP models and improve its return as it learns better model estimate. probabilities { T sas  X  } probabilities { O saz } starting from an initial belief b lief after performing action a and observation z and  X   X  [0 , 1) is the discount factor. knowledge of the POMDP model, which is a strong assumption in practice. Some approaches do and do not address the exploration-exploitation tradeoff. but the model can easily be generalised to learn the reward fu nction as well. Given  X  follow a Dirichlet distribution, i.e. ( p ( p 1 ,...,p k ) probability density function is defined by: f ( p, X  ) = 1 beta function. The expected value of p 3.1 The BAPOMDP Model represented by experience counts:  X  a curred, similarly  X  a a the count vectors  X  and  X  , the expected transition probability for T sas  X  is: T sas  X  the BAPOMDP. Thus, the state space S  X  of the BAPOMDP is defined as S  X  = S  X T X O , as  X  for the count  X  a distributions, which turn out to be their expectations. Hen ce, we define T  X  and O  X  to be: most likely models, given the prior and experience so far. If b unknown POMDP, and the count vectors  X  POMDP, then the initial belief of the BAPOMDP is: b  X  0 mixtures of Dirichlet distributions (i.e. mixtures of coun t vectors). ( S,A,Z,T,O,R, X  ) . If S is finite, then at any time t , the set S  X  b  X  size | S  X  Proof. Proof available in [11]. Proceeds by induction from b  X  The proof of this theorem suggests that it is sufficient to ite rate over S and S  X  the belief state b  X  3.1 can be used to update the belief state. 3.2 Exact Solution for BAPOMDP in Finite Horizons dynamic programming (see [5] for more details):  X  a 1 = {  X  a |  X  a ( s, X , X  ) = R ( s,a ) } , Note here that the definition of  X  a,z T  X   X  =  X  +  X  a practice, it will be impossible to compute  X  a,z techniques. Various methods for belief tracking in the infin ite model are also presented. 4.1 Approximate Finite Model and N sa Theorem 4.1. Given any  X , X   X   X  X  ,  X , X   X   X  X  , and  X   X  (0 , 1) , then for all t : sup Proof. Proof available in [11] finds a bound on a 1-step backup and sol ves the recurrence.  X  Theorem 4.2. Given any  X  &gt; 0 and ( s, X , X  )  X  S  X  such that  X  a  X  A,s  X   X  S , N s  X  a |  X  t ( s, X , X  )  X   X  t ( s, X   X  , X   X  ) | &lt;  X  Proof. Proof available in [11].
 the space of Dirichlet parameters to count vectors  X   X   X  T finite, we can define a finite approximate BAPOMDP as the tuple (  X  S  X  S  X  = S  X  within the finite space. To achieve, this we define a projectio n operator P projects every state in S  X  to their closest state in  X  S Definition 4.1. Let d : S  X   X  S  X   X  R be defined such that: d ( s, X , X ,s  X  , X   X  , X   X  ) = Definition 4.2. Let P P P (  X  ) =  X  . Using P  X  , the transition and observation function are defined as foll ows:  X 
T  X  (( s, X , X  ) ,a, ( s  X  , X   X  , X   X  )) = T sas projection to make sure that the incremented count vectors s tays in  X  S  X  R the  X  -vector computed with the original model.
 Theorem 4.3. Given any  X  &gt; 0 , ( s, X , X  )  X  S  X  and  X  Let  X   X  BAPOMDP (  X  S favor a faster online solution approach, as described below . 4.2 Approximate Belief Monitoring different particle-based approximations that allow polyn omial-time belief tracking. b s then a next state s  X   X  S is sampled from the normalized distribution T sa is added directly to b  X  ( s  X  , X  +  X  a keep the K most probable states in the new belief b  X  and renormalize b  X  . 4.3 Online planning the planning horizon and C models in a belief state b : 5.1 Tiger but the observation probabilities are not. Hence, there are four unknown parameters: O O observation count vector  X  = (  X  over to the next episode.
 Figures 1 and 2 show how the average return and model accuracy evolve over the 100 episodes Figure 1: Return with different belief approximations. introduced when using fewer samples. 5.2 Follow robot and the person can perform five motion actions { NoAction,North,East,South,West } . tive to the robot: { Same,North,East,South,West,Unseen } . The robot perceives the correct cells away from the robot, also causing a reward of -20. We use a discount factor of 0.9. (  X  for person 1 and  X  2 2-step lookahead search for planning in the BAPOMDP.
 time efficient for the performance it provides in complex env ironment. Figure 4: Return with different belief approximations. (4) planning action sequences which optimally trade-off ex ploration and exploitation. We proposed a new model, the Bayes-Adaptive POMDP, and showe d that it can be approximated to we be able to reason and plan under parameter uncertainty.

