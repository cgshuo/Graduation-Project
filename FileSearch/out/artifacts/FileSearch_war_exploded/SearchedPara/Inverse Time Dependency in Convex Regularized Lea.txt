 objective rather than the empirical objective [12] [1]. The generalization objective, also known as the stochastic objective , is given with respect to a linear predictor  X  X  X  X  , where  X  X  X  X   X  is the domain of  X  :  X  ( X   X , X   X  ; X  ) is a mapping that calculate s the cost or regret by on a random selection of the sample  X  over the entire sample distribution  X  X  X  X  X  X  . generality. As an example,  X  can be in the form of ( X , X ) adapting (1) to classifica tions. The loss function  X  can be for example the SVM hinge loss problem becomes to minimize the empirical objective  X   X  training samples is used to approximate the generalization loss. prediction set is strongly associated with equation (1). This (2) as the step 1, and effectiv ely solve (2) as the step 2. unregularized objectives and di vided the tradeoff into three optimization tradeoff. For regularized learning, Karthik of 1/ X  to  X   X  (  X  ) for strongly convex objectives. gradient descent (SGD) algorith m to solve large scale linear prediction problems. It proves that a constant learning rate will numerically achieve some good accuracy, and states the correlation between SGD and online learning. In 2006, Hazan et al [3] introduced a framework with logarithmic regret to solve online strongly convex problems, which is the tightest known regret bound for online optimization. Utilizing this result, Shai Shalev-Shwartz et al [10] proposed an  X   X  -norm linear SVM algorithm called P EGASOS . et al [11] presented a surprising result for P assuming the endurable accuracy is given and fixed, the training time has an inverse dependency on the size of the program runs to achieve this given accuracy. He claimed that, for example, if we get a predictor with accuracy 95% by training one thousand samples, we can use the extra nine thousand samples to train and get a predictor also with accuracy of 95% , but in less time. SVM problem, partially because the  X   X  -norm is naturally a strongly convex function and the hinge loss in SVM is easy to be handled. However, applying this inverse dependency property into more general problems, like  X   X  -norm, other loss functions, or other machine learning algorithms, is very desirable, but it is an under-explored research problem. (PGS), which employs the following regularizer: where the coefficient of 12( X  X  X 1)  X  is to maintain the strong convexity of  X ( X ) . At the same time, we consider the arbitrary Lipschitz continuous and convex loss function Gradient Solver algorithm can achieve the inverse time dependency on the training data size. This conclusion is also verified in the experiments. We summarize the contributions of this paper as below: first provide mathematical backgrounds on convex optimization theory in Section II. Next in Section III, we propose our main result by introducing our Primal Gradient Solver, and analyzing its inverse dependency property. We further demonstrate our solver in SVM, logistic regression and regularized least square in Section IV, and present experimental results in Section V to substantiate our findings. We provide the theoretical proofs of our main theorems in Section VI. We then raise some discussions in Section VII, and conclude the paper in Section VIII. where  X  X  X  [ 1, X  )  X  {  X  } . We also summarize the notations used in this paper in TABLE I. Considering the boundedness of some vector  X  , we will stick to the expression  X   X   X   X  bounded X  instead of  X   X  is bounded X  for some explicit  X  . 2 Next in this section, we introduce some definitions frequently used in convex optimization, and a proposition to be used later. Definition 1 : A function  X : X  X  X  X  is called  X  -Lipschitz continuous w.r.t a norm  X  .  X  if  X  X   X  , X   X   X  X  X , Definition 2 : A function  X : X  X  X  X  is called  X  -strongly convex w.r.t a norm  X  .  X  if  X  X   X  , X   X   X  X  X , X  X  X  [ 0,1 ] , Definition 3 : The Fenchel conjugate of a function  X : X  X  X  X  is defined as: Example 1 : When  X = X   X  , for  X  X  X  ( 1,2 ] , the function  X  (  X  ) =  X  and its Fenchel conjugate  X   X  (  X  ) =  X  convexity does not hold for  X &gt;2 . Definition 4 : The dual norm of the  X   X  -norm  X   X   X  ( X  X   X  |  X   X  )  X / X  is the  X   X  -norm  X   X   X   X  = ( X  X   X  |  X   X  )  X / X  if 1/p+ 1/q=1 . As a special case,  X   X   X   X  =  X  X   X   X  |  X   X   X   X   X  =max  X  |  X   X  | .  X  if for all  X   X   X  X  X  we have  X  (  X   X  )  X  X  X  (  X  )  X  X  X  X   X   X  X  X , X  X  . The differential set of  X  at  X  consists of all the sub-gradients  X  X  X  (  X  ) contains exactly one element  X  X  X  (  X  ) = {  X  X  X  (  X  )} . Proposition 1 : If a function  X : X  X  X  X  is L-Lipschitz at  X  is bounded:  X   X   X   X   X  X  X , X  X  X  X  X  X ( X ) , where 1/p+ 1/q=1 . Proof : By the definition of differential set and Lipschitz continuity, we have for any  X   X   X  X  X  , By the knowledge of H X lder inequality there exists a  X   X   X  X  X  such that  X   X   X   X  X  X , X   X  =  X   X   X   X  X  X   X   X   X   X   X   X  , and combining the above two we arrive at  X   X   X   X   X  X  X  .  X  and state the requirements for the regularizer and the loss function; we then use two theorems to reveal the inverse time dependency, that is, the requi red running time decreases as the number of samples increases, when achieving a fixed generalization error. A. Primal Gradient Solver regularized convex optimization problem, assuming  X  X  X  ( 1,2 ] . By substituting the regularizer (3) into (2), we have: satisfies the following two assumptions: gradient  X   X   X  ( X   X , X   X  ; X  ) is bounded w.r.t.  X  .  X   X  will be used later. Primal Gradient Solver (Figure 1). We take four parameters: number of iterations  X , and a given positive integer  X  . Initially we set  X   X  =0 and a working vector  X =0 . At iteration t we randomly choose a set  X   X   X  X , |  X  consider a temporal loss function  X   X  ( X ) to approximate the empirical loss  X   X  ( X ) :  X   X  X  X   X  (  X   X  X  X  X  ) , and subtract it from  X  by  X  X  X  X  X  X  X  Fenchel conjugate (see Section II for definition): we write the explicit formula of Equation (9) for the two cases  X = X   X  and  X = X  X  X :  X   X   X   X   X  X  X  X  . We will show that these two cases cover most of the circumstances in the applications. features per sample is  X  . If the sub-gradient  X  X  X  computed efficiently in  X ( X  X  X ) , the time complexity for the Primal Gradient Solver is  X  X  X  X  (  X  X  X + X  )  X  since calculating the gradient of  X   X  costs  X ( X ) , as shown in in Figure 2. the special case of  X =2 , since the term  X   X  ... degenerates to 1: we can use a variable to stor e the coefficient in front of  X  and  X  (  X  X  X  X  ) . B. Inverse Dependency on Training Data Size Gradient Solver for  X   X  regularized convex optimization, and iterations. Now we state the correlation between the optimization error and the number of iterations  X  , which will give us a running time in terms of the optimization error (see Figure 3). Theorem 1 (To be proved in Section VI.A) : If  X  (  X  ) satisfies the convexity and Lipschitz continuity, then  X  X   X  ( 0,1 ) , with probability of at least 1 X  X  X  over the choices of  X   X  ,... X   X  and the index  X  , we have: optimization error is  X   X  X  X  X  , and satisfies  X   X   X   X   X  X  X  X  , the algorithm needs  X = X  logarithmic factors. predictor, optimized by our Primal Gradient Solver, the most immediate reflection of its accuracy is the generalization error  X  . In some other words,  X  X   X   X  X  X  ,  X  (  X   X  )  X  X  X  (  X  The following theorem actua lly bases on Theorem 1 to further give us a correlation between the generalization error and the number of iterations. Theorem 2 (To be proved in Section VI.B) : Suppose  X   X  is the predictor optimized by the Primal Gradient Solver. If the desired error rate  X  obeys  X  (  X   X  )  X  X  X  (  X   X  ) + X  , X  X  the required number of iterations satisfies: of the Primal Gradient Solver, we conclude that: from above, decreases as the sample count  X  increases. This is called the property of inverse time dependency on the training data size. This conclusion confirms the theoretical result in [11] which proves the inverse dependency in the special case of  X =2 with the SVM hinge loss. of the following two: when the number of training samples increases three specific loss functions. We first consider the binary classification problem with instance-label pairs  X =( X , X ) where  X  X  X  {  X 1,1 } , we have the following two famous demonstrations of the loss functions. value pairs  X =( X , X ) where  X  X  X  X  , we have verified mathematically , w.r.t the entire space  X = X   X  . Now we consider the Lipschitz continuity of the Least Square loss. space  X = X   X  , but we may constrain the space to  X =  X  X  X :  X   X   X   X   X  X  X  X  . For any  X   X  , X   X   X  X  X  , using H X lder's inequality we deduce that  X  ( X   X   X  , X   X  ; X  )  X  X  X  ( X   X   X  , X   X  ; X  ) =  X   X   X   X  X  X   X  , X   X ( X   X   X  , X   X  +  X   X   X  , X   X   X 2 X  )  X   X   X   X   X  X  X   X   X   X   X   X   X   X   X 2 X   X   X   X   X  +2 |  X  |  X  X   X   X   X   X  X  X  the last inequality holds for the reason that the sample space is fixed and thus  X   X   X   X  and |  X  | are naturally bounded. All we left to do is to further verify the empirical optimum solution  X  the upper bound for |  X  | . write down  X   X  : 
Therefore, our solver can be pr operly adapted to these three loss functions. Note that the Lipschitz continuity of the loss function is an important requirement in the deduction (see restrict  X  to some bounded sphere just like we did for the Least Square loss. We emphasize that the introduction of bounded  X  enables more kinds of convex and continuous functions to be included as loss functions. 1/  X   X  } as an example, our solv er immediately gives the algorithm called P EGASOS [10]. In that paper the proof of the accuracy bound depends on the boundedness of  X  . However, we used a slightly different Lemma 1 which tells us that even in  X = X   X  case our algorithm can still run efficiently. It answers the question in footnote 2 of [11] on why the projection step can be skipped. experimental results. We test our solver in three regularizer-loss pairs:  X   X  -Logistic,  X   X . X  -Logistic and  X   X  -LeastSquare. We has been well-tested in [11]. All the following works are conducted on a computer with 2.4 GHz AMD Opteron Processor 852 and 32G RAM. We first introduce the dataset in the experiments: maximal achievable accuracy on th e testing set, and then re-run the program to retrieve the required running time to obtain some benchmark accuracies , like 93%, 94%, etc. We remark here that choosing a best  X  according to the test data is not scientific [4]. See further discussion in Section VII for details. Solver (PGS) for  X   X  and  X   X . X  -regularized Logistic Regression (LR) against the L -BFGS Quasi-Newton (QN) method [7] for LR. The latter has been proved to be superior in training large-scale  X   X  -regularized Logistic Regression by [6]. In PGS, we choose  X =1 for  X =2 and  X  =300 for  X =1.8 X  ( 1,2 ) . The reason for this selection is discussed in Section VII. Newton algorithm, the running time of our Primal Gradient Solver does not increase as the sample size  X  increases, for both  X =2 and  X =1.8 X  ( 1,2 ) . Although QN can achieve an accuracy of the same level as PGS, namely, higher than 94.5% , its running time is above 600 seconds and we ignore that, in the experiment of QN, we also discover the number of iterations inversely dependent on  X  . However, because each iteration in QN has a time complexity related to  X  , the total running time of QN still increases. On the contrary, PGS is profited by its stochastic behavior. Not only its number of iterations inversely dependent on  X  , the time complexity of a single iteration in PGS is also independent on  X  . It is the combination of these two properties that contributes to the final inverse time dependency. dependency for different sets of regularizer-loss pairs against both CCAT data and our toy data. We run our program against a set of distinct sample sizes and record the number of seconds required to reach each accuracy benchmark. Due to the randomness of our Primal Gradient 
Solver we test our program at least 20 times and choose the median. Notice that although Equation (12) theoretically studies an upper bound in th e training time, the decreasing of upper bound does not directly suggest the real-time inverse dependency. Nevertheless, the experimental results in Figure 6, Figure 7, Figure 8 and Figure 9 all confirm the property in (12). and for  X   X . X  -norm  X  =300 . The median of 20 runs are used for Figure 6, Figure 7 and Figure 8. Figure 9 demonstrates the number of iterations required for PGS of  X   X 
Regression to train our toy data to achieve an error  X   X   X   X   X  of 0.05. The median of 150 runs are used. We state that independent on the number of training samples  X  , so we use the number of iterations to be the y-axis for a better illustration in Figure 9. dataset against the optimal solution generated by Quasi-
Newton algorithm. We run the QN program with sufficient number of iterations to reach the convergent solution that minimizes the objective (it takes more than 2 hours). Results in TABLE II. indicate that our Primal Gradient Solver can obtain the accuracy on the same level as Quasi-Newton, while the training time is within 1 minute for  X  ones, and within 10 minutes for the  X   X . X  regularized one. two theorems in Section III.B, using the best known logarithmic regret [4] for online convex optimization [15], and the Oracle inequality in decomposing generalization loss [12]. A. Proof of Theorem 1 given by convex (Example 1) and a  X   X  ( X ) is convex according to our based on the additivity in [8]. online convex optimization, introduced by [15]. In such problem, the ultimate purpose is to minimize the regret Theorem 2 in [4]. functions over some convex domain  X  w.r.t the some norm  X  .  X   X  . Assume  X  .  X   X  is the dual norm of  X  .  X  algorithm defined in Figure 1 satisfies: Primal Gradient Solver, the above regret is further bounded Proof : This boundedness is ensured if  X   X   X   X   X   X  constant. Recall the Lipschitz continuity for  X  ( X   X , X   X  ; X  ) , which infers the Lipschitz continuity for  X   X  . Based on arriving at our conclusion.  X  based on the i.i.d. selection of subsets  X   X  ,... X   X  and the  X  Line 9 of Figure 1. where the empirical optimum  X   X  =argmin  X  X  X S  X   X   X  (  X  ) and substituting them into (16) and using the result of Lemma 1 we have the proof of theorem 1. Proof of Theorem 1 : The random variable  X   X  X  X  X  = X   X   X  (  X   X   X  (  X   X  )  X 0 is non-negative, and we have  X  [  X   X  X  X  X  then using the Markov inequality statement.  X  B. Proof of Theorem 2 Proof of Theorem 2 : Following [12], we decompose the generalization loss into four parts: population optimum  X   X  =argmin  X  X  X  X   X   X  ( X ) , and generalization loss  X  (  X  ) = X   X  X  X  [  X  ( X   X , X   X  ; X  )] . positive, while the first term, the generalization objective difference, can be further bounded by the empirical objective difference according to the main result in [12]. Combining the results along with the op timization accuracy studied in the previous section (Theorem 1), we arrive at the following inequality hand side is bounded as following: immediately arrive at Theorem 2.  X  our Primal Gradient Solver and propose some enhancements. We also discuss some problems raised in the previous sections. Why p-norm? In the Primal Gradient Solver, the strong convexity is a core requisite to ensure the convergence rate of  X / X  . However, few strongly convex functions found up to now are also suitable to be regularizers. In this paper we examined the squared  X  X  X  (  X , X  ] norms, and experimental results show that  X = X . X  does slightly better than others (for instance, TABLE II. ). The reason is still unknown and example: multiple-re gularizer learning. reason that 1-norm itself has a poor convexity. However, 1-norm has the often desired property of reducing the number of active features. In [8] [9], Shai proposed a substitute also works well in Primal Gradient Solver with advantages in feature selection. The adoption of Kernel. All the works above are verified under the assumption that  X  is a linear predictor. When  X =2 , a common technique is to construct a mapping  X  that maps from the feature space to the Reproducing Kernel Hilbert Space (RKHS) space, availing us a non-linear separator. We emphasize that our Primal Gradient Solver can be slightly adjusted to cater for this assumption, as the calculation of  X   X , X  (  X  ) X  needs a traverse on the support complexity cost for this inner product, the inverse time dependency property no longer holds. In a counterpart of this paper [14] we studied the performance of such kernel PGS, and the result shows that even without the inverse time dependency, the algorithm overwhelms the state-of-the-art in both efficiency and accuracy. Incorporate with a biased term . In our algorithm defined above, we have ignored the biased term in the general loss  X  . The most efficient way to compensate for it is to add this and at the same time modify the regularizer to regularizer, but runs into a different way as the normal regularizer without this bias term. If we consistently ignore this term in the regularizer, the convergence rate of our solver will reduce to  X  X  X 1/  X   X   X  like a generalized convex optimization problem [4]. The selection of parameter  X  . From the above discussion we can see the number of selected samples  X = |  X   X  | is never use the Chernoff bound to boost the confidence and give a better bound than  X = X   X   X   X  experimental results show that in the  X =2 case it is worthless to set  X &gt;1 ; as an alternative, we may choose one single sample each iteration and do  X  X  X  X  iterations in total while the time complexity remains the same and the accuracy is raised. in Section III.A, if the complexity of Primal Gradient Solver is  X  X  X  X  (  X  X  X + X  )  X  , we had better choose  X = X ( X / X ) which keeps the complexity unchanged but boosts the confidence significantly. For the RCV1 dataset where  X =47,236 and  X  X  X 40 , we may choose  X  =300 , which greatly reduces the number of required iterations. Experimental results in Section V have confirmed this analysis and we will investigate the influence of  X  more theoretically in the future. running time depends on an unknown vector  X   X  that is the depends on  X   X  and we never have such a priori knowledge on how to choose it. A valida tion set does not work because we are optimizing the running speed and not until we actually know  X  we cannot run the program at all. Due to this reason, we are currently working on a modified version of the Primal Gradient Solver that will make  X  self-adapted. the  X   X  -norm regularized convex learning problems that can deal with any loss satisfying the convexity and Lipschitz continuity, including the famous SVM loss, Logistic loss and Least Square loss. For all of th em the expected running time is proved to be  X ( X / X   X  X  X  X   X  X  X ) for  X =2 and  X ( X / X   X  X  X  X  regularization parameter,  X  is the average number of non-feature space, and  X   X  X  X  X  is the desired optimization error. Volume 1 (RCV1) show that our Primal Gradient Solver, for all of the three loss functions, approaches an accuracy of 94% within 10 seconds for  X =2 , and 20 seconds for  X =1.8 , while the L -BFGS Quasi-Newton method needs 600 seconds to obtain the same accuracy. based on this Primal Gradient Solver, we proved it is not only more efficient than the traditional algorithms, but also endowed with inverse time dependency property on the number of training sample s, for a fixed accuracy. some algorithm, like our Primal Gradient Solver, whose time complexity is independent on the number of samples  X  , and even inversely dependent on  X  . Hebrew University for his valuable discussions. The authors also acknowledge Matt Callcut and all four anonymous reviewers for their fruitful comments. Lemma 2 : Let  X  be a closed and stro ngly convex function over  X  X  X  X   X  with respect to norm  X  .  X  , then  X  differentiable and Its proof can be seen from Lemma 6 of [9]. Theorem 3 : If  X = X  X  X :  X   X   X   X   X  X  X  X  and  X  (  X  ) =  X   X   X  (  X  ) = X  and Proof : For any given  X  , using the H X lder's inequality we by  X  and substituting the definition of  X   X  , i.e. Eq. (6): H X lder's inequality. For a given norm  X =min{ X ,  X   X   X  we construct  X   X  by letting  X   X   X  =  X  checked that  X   X   X   X   X  = X  and  X   X   X  , X   X  =  X   X   X   X   X  the latter holds because ((  X   X   X  )  X  ,... (  X   X   X  )  X  ) are linear dependent. We remark here that we use  X   X  for the equality of Eq. (26), i.e. Eq. (23).  X  X  X   X  (  X  ) =argmin  X  X  X S  X   X   X   X   X   X   X   X   X   X   X   X  = X   X  .  X  Corollary 2 : If  X = X  X  X :  X   X   X   X   X  X  X  X  and  X  (  X  ) = and Proof : Assume  X  (  X  ) =  X  according to Theorem 3. Now we calculate  X   X  using  X   X  (23) and noticing that  X  X  X 1=  X  we make the calculation: where the third equality is according to Eq . (24). This completes the proof.  X  Corollary 3 : If  X = X   X  and  X  (  X  ) =  X   X   X  (  X  ) =  X 
