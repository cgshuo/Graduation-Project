
We study the problem of clustering uncertain objects whose locations are described by probability density func-tions (pdf). We show that the UK-means algorithm, which generalises the k-means algorithm to handle uncertain ob-jects, is very inefficient. The inefficiency comes from the fa ct that UK-means computes expected distances (ED) between objects and cluster representatives. For arbitrary pdf X  X , ex-pected distances are computed by numerical integrations, which are costly operations. We propose pruning tech-niques that are based on Voronoi diagrams to reduce the number of expected distance calculation. These techniques are analytically proven to be more effective than the basic bounding-box-based technique previous known in the liter-ature. We conduct experiments to evaluate the effectivenes s of our pruning techniques and to show that our techniques significantly outperform previous methods.
Clustering is a technique that has been widely studied and applied to many real-life applications. Many efficient algorithms, including the well known and widely applied k-means algorithm, have been devised to solve the cluster-ing problem efficiently. Traditionally, clustering algori thms deal with a set of objects whose positions are accurately known. The goal is to find a way to divide objects into clus-ters so that the total distance of the objects to their assign ed cluster centres is minimised.

Although simple, the problem model does not address situations where object locations are uncertain. Data un-certainty, however, arises naturally and often inherently in many applications. For example, physical measurements can never be 100% precise in theory (due to Heisenberg X  X  Uncertainty Principle ). Limitations of measuring devices thus induce uncertainty to the measured values in practice.
As another example, consider the application of cluster-ing a set of mobile devices. By grouping mobile devices into clusters, a leader can be elected for each cluster, whic h can then coordinate the work within its cluster. For exam-ple, a cluster leader may collect data from its cluster X  X  mem -bers, process the data, and send the data to a central server via an access point in batch. In this way, local communi-cation within a cluster only requires short-ranged signals , for which a higher bandwidth is available. Long-ranged communication between the leaders and the mobile network only takes place in the form of batch communication. This results in better bandwidth utilisation and energy conserv a-tion.

We remark that device locations are uncertain in prac-tice. A mobile device may deduce and report its location by comparing the strengths of radio signals from mobile ac-cess points. Unfortunately, such deductions are susceptib le to noise. Furthermore, locations are reported periodicall y. Between two sampling time instances, a location value is unknown and can only be estimated by considering a last reported value and an uncertainty model. Typically, such an uncertainty model considers factors such as the speed of the moving devices and other geometrical constraints (such as road network, etc.). In this paper we consider the problem of clustering uncertain objects whose locations are speci-fied by uncertainty regions over which arbitrary probabilit y density functions (pdf X  X ) are defined.

Traditional clustering methods were designed to handle point-valued data and thus cannot cope with data uncer-tainty. One possible way to handle data uncertainty is to firs t transform uncertain data into point-valued data by selecti ng a representative point for each object before applying a tra -ditional clustering algorithm. For example, the centroid o f an object X  X  pdf can be used as such a representative point. However, in [4], it is shown that considering object pdf X  X  gives better clustering results than the centroid method.
In this paper we concentrate on the problem of clustering objects with location uncertainty. Rather than a single poi nt in space, an object is represented by a probability density function (pdf) over the space R m being studied. We as-sume that each object is confined in a finite region, so that the probability density outside the region is zero. Each ob-ject can thus be bounded by a finite bounding box. This assumption is realistic because in practice the probabilit y density of an object is high only within a very small region of concentration. The probability density is negligible ou t-side the region. (For example, the uncertainty region of a mobile device can be limited by the maximum speed of the device.) Given a set of such objects, our goal is to divide them into k clusters, minimising the total expected distance (ED) [4] from the objects to their cluster centres.
The problem of clustering uncertain objects was first de-scribed in [4], in which the UK-means algorithm was pro-posed. UK-means is a generalisation of the traditional k-means algorithm to handle objects with uncertain locations . The major computational cost of UK-means is the evalua-tion of EDs, which involves numerical integration using a large number of sample points for each pdf. To improve ef-ficiency, [19] introduced some pruning techniques to avoid many ED computations. The pruning techniques make use of bounding boxes over objects as well as the triangle in-equality to establish lower-and upper-bounds of the EDs. Using these bounds, some candidate clusters are eliminated from consideration when UK-means determines the cluster assignment of an object. The corresponding computation of expected distances from the object to the pruned clusters are thus not necessary and are avoided.

The contribution of this paper is the introduction of a new set of pruning techniques for the UK-means algo-rithm that are based on Voronoi diagrams [10]. These new pruning techniques take into consideration the spatial relationship among the cluster representatives. We prove that Voronoi-diagram-based technique is strictly more ef-fective than the basic bounding-box-based technique. An-other technique we investigate is the partial ED evaluation method, which can be shown to further save the compu-tation costs of UK-means. Since our pruning techniques are orthogonal to the ones proposed in [19], we study a hybrid algorithm that integrates the various pruning tech-niques. Our empirical study shows that the hybrid algorithm achieves significant performance improvement.

The rest of the paper is organised as follows. We mention a few related works in Section 2. In Section 3, we formally define the problem. In Section 4, we first briefly describe the UK-means algorithm and the bounding-box-basedprun-ing techniques. After that, we discuss our Voronoi-diagram -based pruning techniques. We present experiment results in Section 5, comparing our new pruning techniques with ex-isting ones. Finally, Section 6 concludes the paper.
Data uncertainty has been broadly classified into exis-tential uncertainty and value uncertainty. Existential un cer-tainty appears when it is uncertain whether an object or a data tuple exists. For example, a data tuple in a relational database could be associated with a probability that repre-sents the confidence of its presence [9, 2]. Value uncer-tainty, on the other hand, appears when a tuple is known to exist, but its values are not known precisely. A data item with value uncertainty is usually represented by a pdf over a finite and bounded region of possible values [6, 7, 5, 4]. In this paper, we study the problem of clustering objects with value (e.g., location) uncertainty.

One well-studied topic on value uncertainty is  X  X mpre-cise query processing. X  An answer to such a query is associated with a probabilistic guarantee on its correct-ness. Some example studies on imprecise data include in-dexing structures for range query processing [6], nearest neighbour query processing [7], and imprecise location-dependent query processing [5].

Depending on the application, the result of cluster anal-ysis can be used to identify the (locally) most probable val-ues of model parameters [11] (e.g., means of Gaussian mix-tures), to identify high-density connected regions [15] (e .g., areas with high population density), or to minimise an ob-jective function (e.g., the total within-cluster squared d is-tance to centroids [18]). For model parameters learning, by viewing uncertain data as samples from distributions with hidden parameters, the standard Expectation-Maximisatio n (EM) framework [11] can be used to handle data uncertainty [13].

There has been growing interest in uncertain data min-ing. In [4], the well-known k-means clustering algorithm is extended to the UK-means algorithm for clustering uncer-tain data. In that study, it is empirically shown that cluste r-ing results are improved if data uncertainty is taken into ac -count during the clustering process. As we have explained, data uncertainty is usually captured by pdf X  X , which are gen -erally represented by sets of sample values. Mining uncer-tain data is therefore computationally costly due to infor-mation explosion (sets of samples vs. singular values). To improve the performance of UK-means, CK-means [17] in-troduced a novel method for computing the EDs efficiently. However, that method only works for a specific form of dis-tance function. For general distance functions, [19] takes the approach of pruning, and proposed pruning techniques such as min-max-dist pruning. In this paper we take this lat-ter approach and propose new pruning techniques that are significantly more powerful than those proposed in [19].
Apart from studies in partition-based uncertain data clus-tering, other directions in uncertain data mining include density-based clustering (e.g., FDBSCAN [15]), frequent itemset mining [8] and density-based classification [1].
For density-based clustering, two well-known algo-rithms, namely, DBSCAN and OPTICS have been extended to handle uncertain data. The corresponding algorithms are called FDBSCAN [15] and FOPTICS [16], respectively. In DBSCAN, the concepts of core objects and reachability are defined. Clusters are then formed based on these concepts. In FDBSCAN, the concepts are re-defined to handle uncer-tain data. For example, under FDBSCAN, an object o is a core object if the probability that there is a  X  X ood number X  of other objects that are close to o exceeds a certain prob-ability threshold. Also, whether an object y is  X  X eachable X  from another object x depends on both the probability of y being close to x and the probability that x is a core object. FOPTICS takes a similar approach of using probabilities to modify the OPTICS algorithm to cluster uncertain data.
Clustering of uncertain data is also related to fuzzy clus-tering, which has long been studied in fuzzy logic [20]. In fuzzy clustering, a cluster is represented by a fuzzy subset of objects. Each object has a  X  X egree of belongingness X  with respect to each cluster. The fuzzy c-means algorithm is one of the most widely used fuzzy clustering methods [12, 3]. Different fuzzy clustering methods have been applied on normal or fuzzy data to produce fuzzy clusters [21, 23]. A major difference between the clustering problem studied in this paper and fuzzy clustering is that we focus on hard clus-tering, for which each object belongs to exactly one cluster . Our formulation targets for applications such as mobile de-vice clustering, in which each device should report its loca -tion to exactly one cluster leader.

Voronoi diagram is a well-known geometric structure in computational geometry. It has also been applied to clus-tering. For example, Voronoi trees [10] have been proposed to answer Reverse Nearest Neighbour (RNN) queries [22]. Given a set of data points and a query point q , the RNN problem [14] is to find all the data points whose nearest neighbour is q . The TPL algorithms proposed in [24] uses more advanced pruning techniques to solve this problem ef-ficiently.
Consider a set of objects O = { o 1 , . . . , o n } in an m -dimensional space R m with a distance function d : R m  X  R m  X  R giving the distance d ( x, y )  X  0 between any points x, y  X  R m . Associated with each object is a pdf f : R m  X  R , which gives the probability density of o i at each point x  X  R m . By the definition of pdf, we have (for all i = 1 , . . . , n ) Further, we assume that the probability density of o i is confined in a finite region A i , so that f i ( x ) = 0 for all x  X  R m \ A i .

We define the expected distance between an object o i and any point y  X  R m : Now, given an integer constant k , the problem of cluster-ing uncertain data is to find a set of cluster representative { 1 , . . . , k } so that the total expected distance is minimised.

To facilitate our discussion on bounding-box-basedalgo-rithms, we use MBR i to denote the minimum bounding box of object o i . MBR i is the smallest box, with faces perpen-dicular to the principal axes of R m , that encloses A i . Note that Equation (1) still holds if we replace  X  x  X  A i  X  with  X  x  X  MBR i  X . This fact can be exploited for optimisation when computing ED.
We first give a short description of the UK-means algo-rithm [4] and existing pruning techniques [19] that improve UK-means. Then, we present our new pruning techniques that are based on Voronoi diagrams.
UK-means is an adaptation of the well known k-means algorithm to handle data objects with uncertain locations. 1: Choose k arbitrary points as c j ( j = 1 , . . . , k ) 2: repeat 3: for all o i  X  O do /*assign objects to clusters*/ 4: for all c j  X  C do 5: Compute ED ( o i , c j ) . 6: h ( i )  X  j  X  where j  X  minimises ED ( o i , c j ) (among 7: for all j = 1 , . . . , k do /*readjust cluster represen-8: c j  X  centroid of { o i  X  O | h ( i ) = j } 9: until C and h become stable
Initially, k arbitrary points c 1 , . . . , c k are chosen as the cluster representatives. Then, UK-means repeats the fol-lowing steps until the result converges. First, for each obj ect o , ED ( o i , c j ) is computed for all c j  X  C . Object o assigned to cluster c j  X  that minimises ED, i.e., h ( i )  X  j  X  . Next, each cluster representative c j is recomputed as the centroid of all o i  X  X  that are assigned to cluster j . The two steps are repeated until the solution C = { c 1 , . . . , c h ( ) converge.
 The UK-means algorithm is inefficient. This is because UK-means computes ED for each object-cluster pair in each iteration. So, given n objects and k clusters, UK-means computes nk EDs in each iteration. The computation of an ED involves numerically integrating a function that involv es an object X  X  pdf. In practice, a pdf is represented by a prob-ability distribution matrix, with each element of the matri x representing a sample point in an MBR. To accurately rep-resent a pdf, a large number of sample points are needed. The computation cost of an integration is thus high.
To improve the performance of UK-means, we need to reduce the time spent on ED calculations. To incorporate pruning into UK-means, we replace lines 4 X 6 in UK-means with the following: 1: Q i  X  C /*candidate clusters*/ 2: Apply a pruning technique 3: if only one candidate remains in Q i then 4: h ( i )  X  j where c j  X  Q i . 5: else 6: for all c j  X  Q i do /*remaining candidates*/ 7: Compute ED ( o i , c j ) . 8: h ( i )  X  j  X  where j  X  minimises ED ( o i , c j ) (among For a given object o i , the set Q i stores the set of candidate cluster representatives that are potentially the closest t o o Initially, Q i = C , the set of all cluster representatives. In line 2, a pruning algorithm is applied to prune candidate representatives from Q i that are guaranteed to be not the closest to object o i . If all but one candidate cluster remains in Q i , object o i is assigned the that cluster. Otherwise, we compute the expected distances between o i and each cluster in Q i . Object o i is then assigned to the cluster that gives the smallest expected distance. We describe a few pruning methods in the following sections. Several pruning techniques that are based on bounds on ED have been proposed in [19]. In the MinMax approach, for an object o i and a cluster representative c j , certain points in MBR i are geometrically determined. The distances from those points to c j are computed to establish bounds on ED. Formally, we define It should be obvious that MinD ( o i , c j )  X  ED ( o i , c MaxD ( o i , c j ) . Then, if MinD ( o i , c p ) &gt; MaxD ( o some cluster representatives c p and c q , we can deduce that ues of the EDs. So, object o i will not be assigned to cluster p (since there is another cluster q that gives a smaller ex-pected distance from object o i ). We can thus prune away cluster p without having to compute ED ( o i , c p ) . As an op-timisation, we can prune away cluster p if MinD ( o i , c MinMaxD ( o i ) . This gives rise to the following BB (bound-ing box) pruning algorithm. 1: for all c j  X  C do /*for a fixed object o i */ 2: Compute MinD ( o i , c j ) and MaxD ( o i , c j ) . 3: Compute MinMaxD ( o i ) . 4: for all c j  X  C do 5: if MinD ( o i , c j ) &gt; MinMaxD ( o i ) then 6: Remove c j from Q i
We call this pruning algorithm MinMax-BB. Depending on data distribution, the pruning condition MinD ( o i , c MinMaxD ( o i ) potentially removes many clusters from con-sideration in line 6. This avoids many ED computations at the expense of computing MinD and MaxD. We remark that computing MinD and MaxD requires us to consider only a few points on the perimeter of an MBR, instead of all points in an object X  X  pdf. Thus, computing MinD and MaxD is much simpler than computing ED and it does not involve evaluating an integral. They can be computed much faster than ED.

Another pruning technique proposed in [19] makes use of the inequalities: for any point y  X  R m . (Equation (2) is indeed the triangle inequality.) These inequalities give bounds on ED ( o i , c based on ED ( o i , y ) . If we can compute the latter efficiently, then we can find the bounds efficiently. One possibility is to choose (for each object) certain fixed points as y , and pre-compute ED ( o i , y ) . Then, evaluating the bounds using the inequalities involves only an addition, a subtraction, and an evaluation of distance d ( y, c j ) , which are relatively cheap. Note that y is fixed for each object while c j , a cluster repre-sentative, changes across different iterations in UK-mean s. So, for an object o i , by computing one expected distance ED ( o i , y ) , we are able to obtain bounds for many EDs that involve o i and any cluster representative c j .

Another pruning method proposed in [19] is called  X  X luster-shift X  (CS). Consider a cluster j whose represen-tatives in two consecutive iterations are c  X  j and c j in that order. If ED ( o i , c  X  j ) has been calculated, then we can use as y in Equations (2) and (3) to bound ED ( o i , c j ) . An appealing aspect of CS is that in the later iterations, as the solution converges, d ( c  X  j , c j ) decreases rapidly, making the bounds very tight. It is shown in [19] that the cluster-shift method is very effective in pruning. Also, it does not re-quire any pre-determined fixed points y and hence no pre-computation of ED ( o i , y ) is needed. In our following dis-cussion, we consider the cluster-shift method instead of th e fixed-point method.

Now, the MinMax-BB algorithm can be augmented with the cluster-shift technique to improve pruning power. The bounds computed using Equations (2) and (3) allow us to refine the bounds on ED to tighter values rather than using MinD and MaxD alone. With tighter bounds, we are able to prune more candidates at little additional cost. We call thi s algorithm MinMax-SHIFT.
MinMax-based pruning techniques improve the perfor-mance of UK-means significantly by making use of effi-ciently evaluable bounds on ED to avoid many ED compu-tations. However, these techniques do not consider the ge-ometric structure of R m or the spatial relationships among the cluster representatives. The major innovation in this p a-per is the introduction of Voronoi diagrams [10] as a method to exploit the spatial relationships among the cluster repr e-sentatives to achieve a very effective pruning. We will show in this section that our Voronoi-diagram-based pruning technique is theoretically strictly stronger than MinMax-BB. Also, we will discuss how our Voronoi-diagram-based method can be combined with the cluster-shift method to achieve the most efficient pruning algorithm.

We start with a definition of Voronoi diagram and a brief discussion of its properties. Given a set of points C = { c 1 , . . . , c k } , the Voronoi diagram divides the space R m into k cells V ( c j ) with the following property: The boundary of a cell V ( c p ) and its adjacent cell V ( c consists of points on the perpendicular bisector , denoted c | c q between the points c p and c q . The bisector is the hy-perplane that is perpendicular to the line segment joining c and c q that passes through the mid-point of the line segment. This hyperplane divides the space R m into two halves. We denote the half containing c p (but excluding the hyperplane itself) as H p/q . Thus, H p/q , H q/p and c p | c q form a partition of the space R m . Further, we have the following properties:  X  distinct c p , c q  X  C ,
Here is how we use Voronoi diagram for pruning in UK-means: In each iteration, we first construct the Voronoi diagram from the k cluster representative points, C = { c 1 , . . . , c k } . The Voronoi diagram leads to two pruning methods: The first one is Voronoi-cell pruning. For each c | c 3 object o i , we check if MBR i lies completely inside any Voronoi cell V ( c j ) . If so, then object o i is assigned to clus-ter c j . This is because it follows from Equations (1) and (4) that: Note that in this case, no ED is computed. All clusters ex-cept c j are pruned. An example is illustrated in Figure 1, in which V ( c j ) is adjacent to V ( c 1 ) , V ( c 2 ) and V ( c MBR i lies completely in V ( c j ) , all points belonging to o lie closer to c j than any other c q . It follows that ED ( o is strictly smaller than ED ( o i , c q ) for all c q 6 = c Voronoi-cell pruning method can be summarised by the fol-lowing pseudo code: 1: Compute the Voronoi diagram for C = { c 1 , . . . , c k 2: for all c j  X  C do 3: if MBR i  X  V ( c j ) then
The other pruning method is bisector pruning. Bisectors are the side-products of Voronoi diagram construction, and thus they are available at little extra cost. Given an object o , we consider every pair of distinct cluster representative s c , c q from C . We then check if MBR i lies completely in H p/q . If it does, then by Equation (5), we can deduce that ED ( o i , c p ) &lt; ED ( o i , c q ) , and c q is pruned from Q expected distance ED ( o i , c q ) is not computed. The bisector-pruning method is summarised below: 1: for all distinct c p , c q  X  C do 2: if MBR i  X  H p/q then 3: remove c q from Q i
In the following theorem, we show that bisector pruning is strictly stronger than MinMax-BB in terms of pruning effectiveness.
 Theorem 1 For any object o i  X  O and cluster j ( j = 1 , . . . , k ), if bisector pruning does not prune away candi-date cluster j , then neither does MinMax-BB.
Figure 2. Illustration of the proof of Theo-rem 1 Proof: Let c r be the cluster representative that gives the smallest MaxD with object o i , i.e., MaxD ( o i , c r MinMaxD ( o i ) . We consider two cases: Case 1: r = j . Then,
MinD ( o i , c j )  X  MaxD ( o i , c j ) by definition Since MinMax-BB prunes cluster j only when MinD ( o i , c ) &gt; MinMaxD ( o i ) , we conclude that MinMax-BB does not prune away cluster j in this case. The theorem thus holds in this case.

Case 2: r 6 = j . The bisector c j | c r is well defined and the space R m can be partitioned into { H r/j , c j | c r , H consider 2 subcases:
Case 2a: MBR i lies completely in H r/j . In this case, cluster j will be pruned by bisector pruning. So, the theo-rem holds for this case because the antecedent is not satis-fied.

Case 2b: MBR i overlaps with H j/r  X  ( c j | c r ) . Now, con-Figure 2. We have:
MinD ( o i , c j )  X  d ( x, c j ) since x  X  MBR i Again, the pruning criterion of MinMax-BB is not satisfied and MinMax-BB cannot prune away cluster j . The theorem thus holds.

Hence, we conclude that if bisector pruning does not prune away cluster j , neither does MinMax-BB. Q. E. D. The converse of the theorem, however, does not hold. That is, there are cases in which MinMax-BB fails to prune a cluster while bisector pruning can. Figure 3 shows such an example in R 2 with 2 clusters. Suppose c 1 = (  X  2 , 0) and c 2 = (2 , 0) . Then, c 1 | c 2 is the line x = 0 , i.e., the y -axis. Now, consider an object o 1 with MBR 1 bounded by the lines x = 1 , x = 3 , y =  X  1 , y = 3 . Since MBR 1 lies com-pletely in H 2 / 1 , bisector pruning can prune away cluster 1 . How about MinMax-BB? Note that MinD ( o 1 , c 1 ) = 3 ; MaxD ( o 1 , c 1 ) = have MinMaxD ( o 1 ) = the pruning condition of MinMax-BB is not satisfied. So, MinMax-BB cannot prune away cluster 1 .

We have thus shown that bisector pruning is strictly stronger than MinMax-BB in terms of pruning effective-ness. Note that in implementation, bisectors are a side-product of Voronoi diagram computation. It is therefore ad-vantageous to perform both Voronoi-cell pruning and bisec-tor pruning together. As the Voronoi diagram and bisectors depend only on the cluster representatives c j ( j = 1 , . . . , k ), we can move the computation of the Voronoi diagram to the outermost loop in the UK-means algorithm as a further optimisation. We call the resulting algorithm VDBi (for Voronoi Diagram with Bisector pruning).
Given two cluster representatives c p and c q and an object o , bisector pruning prunes cluster q if MBR i  X  H p/q . If no bisector that involves cluster q can be found to prune clus-ter q , the expected distance ED ( o i , q ) may have to be com-puted. Interestingly, it is not necessary that we compute th e complete integral of ED ( o i , q ) . Our next pruning technique attempts to prune a cluster by computing ED partially. Again, consider two clusters p and q and an object o i . If MBR i intersects the bisector c p | c q , we partition MBR two parts X and Y ( X  X  Y = MBR i and X  X  Y =  X  ) such that X  X  V ( c p ) . The expected distance ED ( o i , c can then be computed by two  X  X maller X  integrals: Similarly, we have ED ( o i , c q ) = ED X ( o i , c q ) + ED c ) .

Now, since X  X  V ( c p ) , by Equation (4), we know grals ED Y ( o i , c p ) and ED Y ( o i , c q ) and if ED c ) . Cluster q can thus be pruned. Otherwise, if q can-not be pruned, we have to compute ED ( o i , c q ) later, but we need not do so from scratch. We only need to com-pute ED X ( o i , c q ) and then add it to the already computed spent on computing ED Y ( o i , c q ) can be reused for comput-ing complete ED later if necessary. Thus, the partial com-putation of ED ( o i , c q ) involves little overhead. We incor-porate the above idea of partial ED computation into VDBi to improve the pruning power of the algorithm. We call the resulting algorithm VDBiP.
Our Voronoi-diagram-based pruning methods are based on a different principle than the MinMax-based methods. They are thus orthogonal and can be combined to achieve a better performance. For example, we can combine VDBi with the cluster shift technique, i.e., we attempt to prune candidate cluster representatives using bisector pruning and if that fails, we apply cluster-shift pruning. Similarly, w e can combine VDBiP with the cluster shift method. We call these hybrid pruning algorithms VDBi-SHIFT and VDBiP-SHIFT. Note that there is no need to combine MinMax-BB with VDBi, as we have shown in Theorem 1 that VDBi prunes a superset of what MinMax-BB prunes.
We have performed a series of experiments to com-pare the performance of our Voronoi-diagram-based prun-ing methods with MinMax-based pruning methods [19]. We compare the algorithms VDBi, VDBiP, MinMax-BB, VDBi-SHIFT, VDBiP-SHIFT and MinMax-SHIFT. All the algorithms are implemented in Visual C++. Experiments are carried out on a PC with a Pentium-4 2GHz CPU and 768MB of main memory.
Following [19], we generated many sets of data for ex-periments. For each dataset, a set of n MBRs are generated in the 2D space [0 , 100]  X  [0 , 100] . Each MBR X  X  side length is generated randomly, but bounded above by d . The MBR is then divided into a of s grid cells, each corresponding to a sample point. Each sample point is associated with a randomly generated prob-ability value, normalised so that the sum of probabilities o f the MBR is equal to 1. These probability values give a dis-cretised representation of the pdf f i of the corresponding object.

In each set of experiments, we generate such a data set as well as k random points to serve as the initial cluster centres. The data set and initial cluster centres are then fed to the si x algorithms. The clustering results from all algorithms are compared to ensure that they are the same. For each set of parameters, 10 sets of experiments are run and the average values are taken and reported.

The parameters used for the experiments are summarised in Table 1. The rightmost column of the table shows the baseline values of the various parameters.
We carried out the first set of experiments using the pa-rameters shown in Table 1. The results are shown in Table 2. The execution times taken by the algorithms are given in the second column. For algorithms that employ Voronoi di-agrams, the column labelled t V shows the amount of time spent on the computation of Voronoi diagrams. The column N
ED shows the number of ED calculations per object per iteration.
 Note that if we had performed the same experiment with UK-means, the number N ED for UK-means would be k [19]. This is because in each iteration, UK-means computes for each object all k expected distances from the object to the k cluster representatives. In our baseline setting, k = 49 and therefore N ED for UK-means would be 49. From Ta-ble 2, we see that the pruning algorithms are very effec-Execution time (seconds) tive. (The smaller the value of N ED , the more effective the pruning is.) All of the pruning algorithms reduce N ED from k = 49 (UK-means) to below 1.6. That is a reduction of more than 96.7%. We can see that VDBi is more effective than MinMax-BB, confirming Theorem 1. The optimisation further introduced by partial ED computation (Section 4.4) is also significant. Finally, the pruning effects are even be t-ter when we consider the hybrid algorithms that combine any of MinMax-BB, VDBi, VDBiP with the cluster-shift technique (Section 4.5).

It should be noted that the computation of Voronoi di-agrams took less than 1.7% of the execution time of the relevant algorithms. This effort is certainly paid off by the amount of ED calculation saved. As bisector pruning is strictly stronger than MinMax-BB pruning as proved in Theorem 1, we can conclude that Voronoi-diagram-based pruning is a more practical and effective pruning technique than MinMax-BB.
Next, we varied s , the number of samples used to rep-resent an object X  X  pdf. In this experiment, s is varied from 16 to 900. The execution times taken by the algorithms are plotted in Figure 4.

From the figure, we see that the execution times of the al-gorithms generally increase as s increases. This is because the time to compute an ED grows linearly with s . We ob-serve from the figure that when s is large (e.g., s  X  196 ), the relative performance of the six algorithms is mostly con -sistent with that observed in our baseline experiment (Ta-ble 2). When s is smaller (e.g., s  X  121 ), however, we see that MinMax-SHIFT is not faster than MinMax-BB. This is because for small values of s , the cost of computing an ex-pected distance is small. The extra overhead that MinMax-SHIFT spends on performing pruning test cannot be paid off by the amount of time saved by reducing the number of ED computation. Similarly, we observe that the advan-tage of VDBi-SHIFT and VDBiP-SHIFT over VDBi and VDBiP is not very significant for small values of s . The cluster-shift technique is thus more useful when pdf X  X  are represented by a large number of sample points. We would like to emphasise that pruning techniques that are based on Voronoi diagrams still perform better than MinMax-based pruning techniques, even for small values of s .

Note that the value of s only affects the amount of time taken to compute an ED, not the number of ED computa-tions. In the following experiments, we will concentrate on measuring the pruning effectiveness of the algorithms. Therefore, in the following experiments, we will omit exe-cution times and report N ED only. Since N ED is unaffected by the value of s , we have reduced the value of s down to 16 in the experiments so that the experiments could be com-pleted faster. Again, we have already established that for large values of s , the relative efficiency of the algorithms is similar to that reported in Table 2. We have also con-firmed that the Voronoi-diagram-based algorithms outper-form MinMax-BB for all ranges of s studied.
In our next set of experiments, we vary the number of uncertain objects, n , from 4000 to 80000. Other parameters are given their baseline values (Table 1). The resulting val -ues of N ED are plotted against n in Figure 5. Apparently, the effectiveness of the pruning algorithms is insensitive to the number of uncertain objects.
In another experiment, we vary the number of clusters, k , from 4 and 144. The other parameters are kept at their baseline values. Figure 6 shows the results. We see from the graph that N ED increases with k . This is because with a larger number of clusters, cluster representatives are gen er-ally less spread out. It is therefore less likely that the pru n-ing algorithms will be able to prune all but one cluster for a given object. Hence, more ED will have to be computed to determine the cluster assignment. For example, under Voronoi-cell pruning, a larger number of clusters implies N N smaller Voronoi cells. It is thus less likely that an object is found to be enclosed entirely within a particular Voronoi cell so that all but one cluster representative are pruned.
From Figure 6, we see that the N ED curves are always significantly lower than k . Recall that UK-means performs k ED computations per object per iteration, Figure 6 thus show that all six pruning algorithms are very effective for a wide range of values of k . To better illustrate the algo-rithms X  pruning effectiveness with respect to the basic UK-means algorithm, we plot N ED /k against k in Figure 7. The figure thus shows the fraction of expected distances com-puted by the various algorithms compared with UK-means.
From the figure, we see that the values of N ED /k are very small. The pruning algorithms are thus very effective. For example, when k = 4 , MinMax-BB and VDBiP-SHIFT computed 6.34% and 2.38% of the EDs computed by UK-means, respectively. These translate into a pruning effec-tiveness of 93.66% and 97.62%, respectively. The best-performing algorithm VDBiP-SHIFT thus computes 62.5% fewer EDs than MinMax-BB. Also, we see that N ED /k de-N /k N creases as k increases for all six pruning algorithms. In other words, the fraction of ED pruned by the algorithms increases when there are more clusters. The pruning ef-fectiveness of VDBi is seen to be consistently better than MinMax-BB over the whole range of k value. By achieving additional pruning using partial ED computation, VDBiP performs even better than VDBi. Furthermore, all hybrid algorithms are more effective than their non-hybrid coun-terparts. The results also show that, consistently, VDBiP-SHIFT is more effective than VDBi-SHIFT, which in turns, performs better than MinMax-SHIFT.
To study the effect of the extent of uncertainty on the algorithms X  performance, we vary d , the maximum side length of an object X  X  MBR, from 1 . 0 to 25 . Other param-eters are kept at their baseline values. Essentially, a larg er MBR implies a larger uncertainty region and so an object X  X  location is more uncertain . The results are shown in Fig-ure 8.

We can see from the graph that N ED increases as the size of the MBRs increases. This is because as the size of the MBRs increases, it is more likely that the MBRs overlap among one another or with multiple Voronoi cells. The former causes MinMax-based pruning to fail and the lat-ter causes Voronoi-cell pruning and bisector pruning to fai l. Even though the pruning effectiveness decreases when d be-comes large, Figure 8 shows that the overall pruning effec-tiveness is still very impressive (recall that N ED for the basic UK-means algorithm is 49). Comparing MinMax-BB and VDBiP-SHIFT, the latter prunes over two thirds of the EDs computed by the former. Our Voronoi-diagram-based prun-ing algorithms are thus seen to outperform the correspond-ing MinMax-based algorithms by a wide margin.
In this paper we have studied the problem of cluster-ing uncertain objects whose locations are represented by probability density functions. We have discussed the UK-means algorithm [4], which was the first algorithm to solve the problem. We have explained that the computation of expected distances dominates the clustering process, espe -cially when the number of samples used in representing ob-jects X  pdfs is large. We have mentioned an existing pruning technique MinMax-BB and its improved variant MinMax-SHIFT [19]. Although these techniques can improve the efficiency of UK-means, they do not consider the spatial re-lationship among cluster representatives.

To further improve the performance of UK-means, we have devised new pruning techniques that are based on Voronoi diagrams. The VDBi algorithm achieves effective pruning by two pruning methods: Voronoi-cell pruning and bisector pruning. We have proved theoretically that bisect or pruning is strictly stronger than MinMax-BB. Furthermore, we have proposed the idea of pruning by partial ED calcula-tions and have incorporated the method in VDBiP. We have also noticed that the different pruning techniques, employ -ing different pruning criteria, can be combined. This leads to two hybrid algorithms VDBi-SHIFT and VDBiP-SHIFT that are highly effective.

We have conducted extensive experiments to evaluate the relative performance of the various pruning algorithms. Th e results show that our new pruning techniques outperform MinMax-BB consistently over a wide range of experimen-tal parameters. The overhead of computing Voronoi dia-grams for our Voronoi-diagram-based technique is paid off by the large number of ED calculations saved. The exper-iments also consistently demonstrated that the hybrid algo -rithms can prune more effectively than the other algorithms . Therefore, we conclude that our innovative pruning tech-niques based on Voronoi diagrams are effective and practi-cal.

