 Atypical behaviours are the basis of a valuable knowledge in domains related to security ( e.g. fraud detection for credit card [1 ], cyber security [4] or safety of critical systems [6]). Atypicity generally depends on the isolation level of a (set of) records, compared to the dataset. One possible method for finding atypic records aims to perform two steps. The first step is a clustering (grouping the records by similarity) and the second step is the identification of clusters that do not correspond to a satisfying number of records. Actually, atypical events (or outliers) might be indicative of suspicious data such as skewed or erroneous values, entry mistakes or malicious behaviours. A malicious behaviour can be detected as an outlier in datasets such as transactions in a credit card database or records of usage on a web site. To the best of our knowledge, outlier detection always relies on a parameter, given by the end-user and standing for a  X  X egree of outlyingness X  above which records are considered as atypical. For instance, in [10], a distance-based outlier is an object such that a user-defined fraction of dataset objects have a distance of more than a user-defined minimum distance from that object. In [5], the authors propose a nonparametric clustering process and the detection of outliers requires a user defined value k corresponding to the top-k desired outliers.

In this paper we propose Mrab (Multi-Resolution detection of Atipycal Be-haviours), a parameterless method intending to automatically extract outliers from a clustering result. In contrast to previous work, our goal is to find the best division of a distribution and to automatically separate values into two sets corresponding to clusters on the one hand and outliers on the other hand. Our method fits any clustering result dep ending on any characteristic such as distances between objects [10], objects X  de nsity [2,11] or clusters X  size [7,13]. Our framework involves clustering-based outlier detection in data streams. This framework will allow us to illustrate our proposal with one of the possible char-acteristics observed for building a distribution of objects ( i.e. clusters X  size). The choice of data streams is motivated by the specific constraints of this domain. In a data stream environment, data are generated at a very high rate and it is not possible to perform blocking operations. In this context, requesting a parameter such as k ,fortop-k outliers, or p , a percentage of small clusters, should be pro-hibited. First, because the user doesn X  X  have enough time to try different values of these parameters for each period of analysis on the stream. Secondly, because a permanent value may be adapted to one period of the stream but it is highly likely to be wrong on the next periods. Actually, from one batch to another, the clustering results will evolve and the data distribution will change, as well as the number or percentage of outliers. For th ese reasons, detect ing outliers should not depend on any parameter and should be adaptive in order to keep the best accuracy all alo ng the stream.

Section 2 gives the details of Mrab and its principle for separating outliers from clusters. Section 3 shows the advantages of Mrab through a set of experi-ments on real Web usage data and S ection 4 gives our conclusion. Clustering is the problem of finding a partition of a data set so that similar objects are in the same part of the partition and different objects are in different parts. A data stream S = { S 1 , ..., S i , ..., S n } is a series of batches S i ,readin increasing order of the indices i . Each batch contains a set of objects O = { o 1 , ..., o m events. This is the core of this paper, and our principle (based on a multi-resolution analysis) for this parameterl ess detection is presented in Section 2.
Most previous work in outlier detection requires a parameter [8,15,12,9], such as a percent of small clusters that should be considered as outliers or the top-n outliers. Generally, their key idea is to sort the clusters by size and/or tight-ness. We consider that our clusters will be as tight as possible, according to our clustering algorithm, and we aim to extract outliers by sorting the clusters by size. The problem is to separate  X  X ig X  and  X  X mall X  clusters without any apriori knowledge about what is big or small. Our solution is based on an analysis of the clusters X  distribution, once th ey are sorted by size. The key idea of Mrab is to use a wavelet transform to cut down such a distribution. With a prior knowl-edge on the number of plateaux (we want two plateaux, the first one standing for small groups, or outliers, and the second one standing for big groups, or clusters) we can cut the distribution in a very effective manner. Actually, each cluster having size lower than (or equal to) the first plateau will be considered as an outlier.

The wavelet transform is a tool that cuts up data or functions or operators into different frequency components, and then studies each component with a resolution matched to its scale [3]. In other words, wavelet theory represents se-ries of values by breaking them down in to many interrelated component pieces; when the pieces are scaled and translated wavelets, this breaking down process is termed wavelet decomposition or wavelet transform. Wavelet reconstructions or inverse wavelet transforms involve putting the wavelet pieces back together to re-trieve the original object [14]. Mathematically, the continuous wavelet transform is defined by: where z  X  denotes the complex conjugate of z ,  X   X  ( x ) is the analyzing wavelet, a ( &gt; 0) is the scale parameter and b is the translation parameter. This transform is a linear transformation and it is co-variant under translations and dilations. This expression can be equally interpreted as a signal projection on a function family analyzing  X  a,b constructed from a mother function in accordance with the that are localized in time and frequency and are obtained by translations and dilations from a single function  X  ( t ), called the mother wavelet. For some very special choices of a, b, and  X  ,  X  a,b is an orthonormal basis for L 2 ( R ). Any signal can be decomposed by projecting it on the co rresponding wavelet basis function. To understand the mechanism of wavelet transform, we must understand the multiresolution analysis (MRA). A multiresolution analysis of the space L 2 ( R ) consists of a sequence of nested subspaces such as: ...  X  V 2  X  V 1  X  V 0  X  V  X  1 ...  X  V j +1  X  V j ...  X  j  X  Z if f ( x )  X  V j  X  X  X  f (2  X  1 x )  X  V j  X  k  X  Z if f ( x )  X  V
There is a function  X  ( x )  X  L 2 ( R ), called scaling function, which by dilation and translation generates an orthonormal basis of V j . Basis functions are con-structed according to the following relation :  X  j,n ( x )=2  X  j 2  X  (2  X  j x  X  n ) ,n  X  Z , and the basis is orthonormated if +  X   X  X  X   X  ( x )  X   X  ( x + n ) dx =  X  ( n ) ,n  X  Z .For each V j , its orthogonal complement W j in V j  X  1 can be defined as follows: V j  X  1 = V orthogonal to W j ,so  X  j, k = j then W j  X  W k .

There is a function  X  ( x )  X  R , called wavelet, which by dilations and transla-tions generates an orthonormal basis of W j ,andsoof L 2 ( R ). The basis functions is decomposed into an infinite sequence of wavelet spaces, i.e. L 2 ( R )= j  X  Z W j . To summarize the wavelet decomposition: given a f n function in V n , f n is de-composed into two parts, one part in V n  X  1 and the other in W n  X  1 .Atnextstep, the part in V n  X  1 continues to be decomposed into two parts, one part in V n  X  2 and the other in W n  X  2 and so on.

A direct application of multiresolution analysis is the fast discrete wavelet transform algorithm. The idea is to iteratively smooth data and keep the details all along the way. More formal proofs about wavelets can be found in [3]. The wavelet transform provides a tool for time-frequency localization and are gen-erally used to summarize data and to capture the trend in numerical functions. In practice, the majority of wavelets coefficients are small or insignificant, so to capture the trend only a few significant coefficients are needed. We use the Haar wavelets to illustrate our outlier detection method. Let us consider the following by the following table:
Then, we keep only the most two significa nt coefficients. In our series of coeffi-the following step, the inverse operation is calculated and we obtain an approx-The set of outliers contains all the clusters having size smaller than the first plateau( e.g. 2 . 25). In our example, o = { 1 , 1 , 2 } gives the sizes of outliers ( i.e. clusters having size  X  2).

More generally, advantages of this method, for our problem, are illustrated in figure 1. In figure 1, the y axis stands for the size of the clusters, whereas their index in the sorted list is represented on x , and the two plateaux allow separating small and big clusters. Depending on the distribution, wavelets will give differ-ent indexes (where to cut). For instance, in our usage data at anonymized lab , there is a variation between night and day in the usage of PHP scripts. This variation results into two main shapes. Figure 1 gives an illustration of two dif-ferent distributions, similar to the ones we found out in our experiments. Let us consider the 10% filter on this distribution, which aims to isolate outliers corre-sponding to 10% of the global shape. If one uses the 10% percent filter in order to detect outliers, he will obtain a relevant outlier detection for the first distri-bution (corresponding to usage of scripts at 1 am). However, with the second distribution (calculated from the usages at 5 pm), this filter will give a very high value and return clusters that should not be considered as outliers. On the other hand, our wavelet based filter will adjust to the distribution variation and the threshold for outlier detection will only slightly increase, taking into account the new distribution shape.

Applying the wavelet transform on the series allows us to obtain a good data compression and, meanwhile, according t o different trends, a good separation. Knowing that outliers are infrequent objects, they will always be grouped into small clusters. Choosing a good level of outlyingness in a streaming environment is highly diffi-cult given the short time available to take a decision. In this context, an atypical event detection method which does not depend on a parameter such as k ,forthe top-k outliers, or a percentage p of small clusters, should be much appreciated. On the other hand, such a parameterless method also has to guarantee good results. This method should be able to provide the end-user with an accurate separation of the clusteing results into small and big clusters. It should also be able to fit any kind of distribution shape (exponential, logarithmic, linear, etc.). The most important characteristic is to be able to automatically adjust to the number of clusters and to their size from one batch to the other. Our claim is that Mrab matches all these requirements and we illustrate these features in this section.

For these experiments we used real data, coming from the Web Log usage of anonymized lab from January 2006 to April 2007. The original files have a total size of 18 Gb and they correspond to a total of 11 millions navigations that have been split into batches of 8500 requests each (in average). In these experiments, we focus on 16 batches, since they are representative of the global results and they illustrate the variation of distribution. The first 8 batches have been selected among PHP request occuring between 1 and 2 am, and the 8 former have been selected among requests occuring between 3 and 4 pm.
 Figure 2 shows the behaviour of filters top-k and p % on those 16 batches. The first surface (left) shows the size of clusters selected by a top-k filter. The principle of this filter is to select only the first k clusters after sorting them by size. An obvious disadvantage of this filter is to select either too much or not enough clusters. Let us consider, for instance, batch 13 in Figure 2. With k =50 the maximum outliers size is 4, whereas with k =90thissizeis67(whichisthe maximum size of a cluster in this batch, which contains only 84 clusters).
We have also implemented a filter based on p %, a percentage of clusters, for outlier selection. The number of outliers selected by this filter with different values of p ( i.e. from 0 . 01 to 0 . 09) is given in figure 2 (right surface). The principle is to consider p  X  [0 .. 1], a percentage given by the end-user, d = maxV al  X  minV al the range of cluster sizes and y =( p  X  d )+ minV al . Then, the filter aims to select only clusters having size t , such that t  X  y . For instance, with s = { 1 , 3 , 10 , 11 , 15 , 20 , 55 , 100 } a series of sizes and p =0 . 1wehave d = 100  X  1 = 99, y =1+(0 . 1  X  99) = 10 and the set of outliers will be o = { 1 , 3 , 10 } . In our experiments, this filter is generally better than a top-k filter. Actually, we can notice homogeneous results from Figure 2. For instance, with batch 13 wecanseeanumberofoutliersrangingfrom44(1%)to78(9%),which corresponds to the results of top-40 to top-70.

Figure 3 gives a comparison of Mrab (applied to the same data) with top-k and percentage filtering. In the left part of Figure 3, we compare Mrab with a top-10 and a top-70 filter. For the first 8 batches, top-10 and Mrab give the best result. Unfortunately, for batches 9 to 16, top-10 returns too few outliers (having maximum size 1). Therefore this filter cannot be used for the whole stream. The best top-k results for batches 9 to 16 are given by a top-70. Unfortunately, its results for batches 1 to 8 are bad (values are too high, with outlier having size up to 28). Therefore, no value of k in this filter can be considered as a reference. The end-user would have to modify the value of k from one batch to another. This result is thus not acceptable and shows that top-k is unable to adjust to changes in the distribution of cluster sizes. On the other hand, thanks to its wavelet feature, Mrab is able to automatically adjust and will always select a correct maximum size to detect atypical events.

In the right part of Figure 3, we focus on two percentage filters ( i.e. 1% and 5%) and we compare them to Mrab . Our observation is that Mrab and 1% would give similar results. For instance, with batch 7, we know that Mrab labels clusters having size less than or equal to 3 as outliers. That filtering gives a total of 9 clusters (where filter 1 % gives 8 outliers). We also can observe that most of the values given by filter 1 % on the first 8 batches are low. Filter 5 % gives better values for the first 8 batches (very similar to Mrab ) but it has bad results on the next 8 batches (up to 138 outliers). This is due to the variation of the distribution, as illustrated by Figure 1. Therefore, the advantage of Mrab over the percentage filter is double: 1. Mrab does not require any parameter tuning. It adjusts automatically, 2. Mrab gives an optimal separation between small and big values . Let us In this paper we have presented Mrab , an outlier detection method that does not require any manual tuning. Our principle is first based on a distribution of clusters according to some c haracteristics such as their size, tightness, density or any other characteristic. Thanks to its wavelet feature, Mrab is able to cut down this distribution into two sets, corresponding to clusters and outliers. The advantages of Mrab are i) automatic adjustment to distribution shape variations and ii) relevant and accurate detection of outliers with very natural results. Our experiments, performed on real data, confirm this separation feature of Mrab compared to well-known outlier detection principles such as the top-k outliers or the percentage filter.

