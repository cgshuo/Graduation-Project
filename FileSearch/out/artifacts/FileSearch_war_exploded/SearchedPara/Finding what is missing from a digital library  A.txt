 1. Introduction
On-line access to the full text of cataloged documents is an important requirement for satisfying the needs and expecta-those built by aggregating metadata from heterogeneous sources, not all metadata records have a direct pointer (e.g., a URL) to the corresponding full text. This situation is also common when the DL makes available information about citations and references but there is no direct access to the referenced items.

Even the presence of a direct pointer may not be useful for the user, for instance, in the cases in which the access to the full text requires the payment of a fee or the pointer became invalid due to the Web dynamics. In these cases, a service that retrieves the respective missing full texts from other Web sources would be of great value to the DL X  X  users.
In this article, we propose a process to provide such a service. It explores general purpose and specialized Web search engines to retrieve the URLs of full-text documents for which metadata records exist in a DL catalog but a full-text pointer is not available. The idea is to explore the potentiality of the existing search engines and to study how they behave in this specific task. For this, we experimented with records of documents from DLs in the Computer Science field. We also study the potential of the proposed process in finding other related documents and information that may be useful for the user, such as other publications (e.g., a thesis or dissertation) authored by one of the authors of the searched article and additional meta-data that complements the information already in the DL (e.g., references). In our experiments, we consider content freely Web site).

We investigate how to specify, for each considered search engine, the most effective queries for the task at hand and which search engine is the best given distinct user requirement levels. We also explore search engine combination strategies to improve the overall effectiveness of our process. Finally, we analyze the capability of the proposed process to find the de-sired content in the considered scenarios.

In our evaluation, we use metadata records from the BDBComp  X  Brazilian Digital Library of Computing (Laender, Gon X alves, &amp; Roberto, 2004 ) complemented with a set of records extracted from DBLP  X  The DBLP Computer Science Bibliography 2 corresponding to conference papers authored by Brazilian researchers but not present in the first collection.
We randomly sampled a set of records from these two collections and used them to build queries which we submitted to three general purpose search engines (Google, Yahoo!, and MSN) and two specialized ones (Scholar and CiteSeer) aiming at retrieving the corresponding full texts or other relevant but missing information. Our experimental results demonstrate that our proposed responding URL is missing. They have also shown that, among the five tested search engines, Scholar is the most effective one for this task and that, when combined with Google, significant gains are achieved for all considered scenarios.
It is important to note that, despite the study presented in this article has been carried out with metadata records of doc-uments from Computer Science, a field very well represented on the Web, recent studies show that search engines, such as
Scholar, also cover in a reasonable manner the content of other fields (Walters, 2007 ). Since the proposed process does not rely on intrinsic characteristics of any academic field, such as publication patterns, standards or preferences, but depends only on the metadata cataloged in DLs, we believe that it can be applied to other fields with similar results.
In summary, the main contributions of this article are: (1) The proposal of a process for finding the URL of the corresponding full text, or of any relevant related material, for (2) A comprehensive study of this process using different query strategies applied to different search engines and consid-(3) The proposal of a strategy for combining the results coming from specific search engines and re-ranking them, which (4) An analysis of the existent trade-off between the efficiency and the effectiveness of the proposed process.
The remainder of this article is organized as follows. Section 2 addresses related work. Section 3 describes the process proposed for retrieving the missing URLs. Section 4 discusses our experimental environment. Section 5 describes the meth-odology and the metrics used to compile the experimental results. Section 6 presents and discusses these results. Section 7 analyzes the impact of several factors in the likelihood of finding the missing URLs. Section 8 presents the conclusions and future work. 2. Related work Current approaches to find documents missing from DLs rely mostly on focused crawlers (Chakrabarti, van den Berg, &amp;
Dom, 1999 ). For instance, in Zhuang, Wagle, and Giles (2005) , the authors investigate the feasibility of using publication metadata to guide the crawler towards author X  X  homepages to harvest documents that are missing from a DL collection.
However, relying on focused crawlers to maintain collections of scientific documents requires the construction of a complex software infrastructure. Therefore, in this work we advocate taking advantage of the current content already indexed by existing search engines but having just a small effort of formulating appropriate queries to such search engines.
The use of the infrastructure provided by search engines has been beneficial in many situations. In Qin, Zhou, and Chau (2004) , for example, the authors discuss limitations of traditional focused crawling algorithms and argue that the use of meta-search can help overcome such deficiencies. They also propose that answers of queries submitted to search engines can be used to make more diverse the search space of such algorithms, which are normally limited to the content located close to the seeds selected as initial points for the crawling process. In Harrison and Nelson (2006) , the authors describe strategies for finding information related to pages missing from Web sites. Cached versions of the missing pages retrieved from search engines are used for generating a lexical signature of a set of terms that captures the essential information pre-sented in the page which is then used to find similar documents or alternative copies of the original document. This strategy is the basis of a framework that aims at preserving the information available on the Web.

Some systems that provide searching or crawling services for scientific articles have been reported in the literature, such as HPSearch and Mops (Hoff &amp; Mundhenk, 2001 ), and Paper Search Engine (PaSE) (On &amp; Dongwon, 2004 ). However, these works focus on searching for scientific articles in general. In our work, we restrict our investigation to articles for which metadata records exist in a DL but a full-text pointer is not available. Thus, we evaluate the effectiveness of existing search engines to accomplish this task.

Finally, comparative studies evaluating the effectiveness of generic search engines to satisfy general information needs are very common (Bharat &amp; Broder, 1998; Chu &amp; Rosenthal, 1996; Gordon &amp; Pathak, 1999; Lawrence &amp; Giles, 1998, 1999). However, we have been unable to find any work comparing the use of generic and specialized search engines for the specific task described here. 3. Proposed process We envision a service aimed at helping users to find DL missing content on the Web. The proposed process is depicted in request the service to search for the missing information. By using the Article Metadata record m matically generates and submits queries to one or more search engines requesting the missing information. Candidate URLs are extracted from the resulting pages as an ordered list C same search engine have their relative positions preserved.

Then, the Filter removes from C d those URLs with no or little interest to the user, contributing to reduce the processing costs of the next steps. For this, we adopt a simple procedure that mimics the behavior of typical users who usually examine t &amp; Yang, 1975 ). As a result, the Filter generates a list F
Next, the Ranker ranks the F d list and generates a new ranked list R returned document and the source of the result. The Ranker works by trying to put, on top of the ranked list, those documents with higher chance of satisfying the user X  X  needs, as we shall see later. The R face , which shows it to the user. 4. Experimental environment
In order to find the best configuration for the proposed service when searching for the full text of documents whose metadata records have been taken from the two Computer Science collections described next, we first investigated the effectiveness of individual search engines for this specific task. We tested five popular search engines available on the Web:
Google, 4 Yahoo!, 5 MSN Search, 6 Google Scholar, 7 and CiteSeer. among the ones with the largest audience on the Web. The last two are specialized search engines that index scientific publi-cations, being CiteSeer focused on the Computer Science field.
 Our experimental environment is a simplified version of the service architecture described in Fig. 1 , where the Sample
Catalog contains metadata records of Computer Science conference papers for which a URL is missing from the DL and each query q d , generated for a metadata record m d , is submitted to a single search engine in order to find a relevant URL for m each search engine, except CiteSeer, we developed a specific Query Interface that submits the queries directly to the respec-the CiteSeer domain. This was due to the fact that CiteSeer was constantly unavailable at the time of our experiments. We have adopted such an alternative based on the results of a previous experiment in which we randomly selected 1060 records from the CiteSeer metadata catalog 9 and then submitted queries to Google requesting for related content. For 98 1 % of these metadata record.

To create the Sample Catalog we carried out a stratified random sampling of two collections: (a) a set of 3969 metadata records obtained from the complete catalog of BDBComp, a collection of papers published in proceedings of major Brazilian
Computer Science conferences, and (b) a set of 3181 records extracted from DBLP, which we call DBLP-Br, corresponding to papers published by Brazilian researchers in proceedings of international conferences. Note that no records from the second collection belong to the first one. The percentage of articles without a full-text URL is about 66% in BDBComp and 36% in DBLP-Br. The resulting Sample Catalog comprises 200 metadata records with missing URLs.

Each metadata record m d was then used to generate the queries to be used for requesting the full text of the respective papers. We have tested seven types of query, as illustrated by the following examples, derived from the article Data Extrac-tion By Example whose authors, as cataloged at DBLP, are Laender, Ribeiro-Neto and da Silva: AS : surnames of all cataloged authors (e.g., Laender Ribeiro-Neto Silva ).
 UT : unquoted title (e.g., DEByE  X  Data Extraction By Example ).
 UT + FS : unquoted title followed by the surname of the first cataloged author (e.g., DEByE  X  Data Extraction By Example Laender ).
 UT + AS : unquoted title followed by the surnames of all cataloged authors (e.g., DEByE  X  Data Extraction By Example Laender Ribeiro-Neto Silva ).
 QT : quoted title (e.g.,  X  X  DEByE  X  Data Extraction By Example  X ).
 QT + FS : quoted title followed by the surname of the first cataloged author (e.g.,  X  X  DEByE  X  Data Extraction By Example X  Laender).
 QT + AS : quoted title followed by the surnames of all cataloged authors (e.g.,  X  X  X EByE  X  Data Extraction By Example  X  Laender Ribeiro-Neto Silva ).

For each query q d , the Query Interface generated a list C the Filter removed elements whose titles lead to Jaccard coefficients lower than the threshold value list F d of filtered URLs. 5. Evaluation
After submitting the queries to the five search engines, we combined the obtained result lists into a single one. This pro-cific paper a . To classify these pairs according to their usefulness, we first defined the different user scenarios we are interested.

Different users of a DL may have different interests and needs in different circumstances. For instance, a user looking for for this content, she might prefer to have access only to those articles whose full texts are freely accessible. Later on, when complementing some bibliographic references, this same user may become interested in missing metadata such as page rial can be classified according to its content and accessibility, as described below.

With respect to content, we have considered the items in our result list as belonging to the following six categories: (1) Full text : the URL u points to the full text (or to a document containing a pointer to it) of the article described by m (3) Useful metadata : the URL u does not belong to any of the above categories and points to a document containing meta-(4) Similar metadata : the URL u does not belong to any of the above categories and points to a document containing meta-(5) Redundant metadata : the URL u does not belong to any of the above categories and points to a document describing (6) Others : the URL u does not belong to any of the above categories.

Regarding accessibility, we have considered the items in our result list as belonging to the following two categories: (1) Restricted : the URL u provides access to the full text (or related document) by means of some sort of payment or (2) Free : the URL u provides free access to the full text (or related document).

Based on the previously described categories, we derived eight scenarios that model users with different interests, as Restricted covers, for example, users interested only in  X  X  X fficial X  documents coming from trusted sources. In the At Least
Metadata scenario, additionally to all relevant documents considered in the Highly Flexible scenario, users are also interested in documents that contain either redundant metadata describing a searched article or metadata describing another docu-ment similar to the desired one, no matter how they can be accessed. Finally, in the No Requirements scenario, users have no requirements. Any kind of content related to a searched article is considered relevant, no matter also how it can be ac-cessed. The last three scenarios are defined only for the purpose of analyzing coverage, as discussed in Section 7.
We then asked 27 subjects, members of our research group, to classify the resulting 3676 pairs as useful or not according to the previously described usage scenarios. For instance, a page containing additional metadata related about a paper but not its full text could be considered useful in the At Least Metadata scenario, but not useful in the Strict scenario.
To evaluate our results, we use three metrics: average precision at seen relevant documents ( P ( MAP ), and mean reciprocal rank ( MRR ).

A good ranked list should maximize the placement of relevant content near to the top positions since these are the most likely positions to be inspected by the users. To accomplish this, the evaluation metric should take into consideration the number of relevant results and the order in which they appear. An example of such metric is P non-interpolated average precision , a measure commonly used in TREC evaluations ( Harman, 1996 ). It is defined as where m is the number of documents returned as result for a query q , n is the total number of relevant documents for q , and r is 1 if the i th returned document is relevant or 0 otherwise. This metric intuitively weights higher relevant documents that appear on top positions of the ranked list.

Since P q provides only one evaluation per query, and we need a metric to evaluate a whole set of queries submitted to a search engine, we use MAP to provide a global estimate to a set of queries. MAP is the mean of the average precisions ( P calculated over a set of queries and defined as where k is the total of queries. This is exactly the mean of the average precisions obtained for the metadata records in the Sample Catalog . Finally, MRR (Voorhees, 1999 ) estimates how close a document is from the top of the ranking. It is defined as where i is the position of the first relevant document observed as result for query q . This metric assumes that users are usu-mitted to find it.

For all MAP and MRR results, we report value intervals with a 95% confidence level. We assume c , the central value of an interval c e as being its representative value. For all reported comparisons, we tested the statistical significance using the pairwise t -test ( Jain, 1991 ). We consider statistically significant the results with, at least, 95% confidence level. 6. Results and discussion
In this section, we present the results obtained, considering the five first scenarios described in Table 1 . In Section 6.1,we analyze seven types of query for the task of retrieving the full text of an article. In Section 6.2, we compare the five search engines considering, for each one, the best query type in each scenario. In Section 6.3, we present strategies to improve the overall performance of our proposed process. 6.1. Query type analysis
Tables 2 and 3 present the MAP and MRR value intervals achieved by the seven types of query for each tested search en-values shown are the ones that, for each query type, achieved the highest MAP value for records of the Sample Catalog .
Note that the values for each query type are not comparable among different search engines, since these values have been computed considering relevance information in a pool of results obtained by the union of the results of all seven query types for the same search engine. These values can only be used to compare the relative performance of each query type for a same search engine. Global comparisons among the tested search engines are presented in Section 6.2.

Table 2 presents the MAP results for the seven types of query submitted to the five tested search engine. As we can see, in nificantly better than the other ones for Google, Scholar, and CiteSeer. For Yahoo!, the apparently best performance of un-quoted queries over the other types was not statistically significant. In general, AS queries presented a very poor performance, except for CiteSeer. For MSN, the number of papers for which we have been able to retrieve relevant results was only 10% of the total, which makes it hard to draw any significant conclusion. Despite this, we note in MSN a very poor performance for UT queries and a general advantage of quoted queries. As we can see from Table 3 , much of the observation drawn for MAP results holds when we analyze the MRR values.

Although intuitively quoted title queries were expected to return better results, it seems that quoted terms exclude many relevant results mainly due differences between the document title cataloged in the DL and the title of results returned by the search engines. These differences may happen for a number of reasons such as typographical errors at cataloguing time and conversion errors of PDF documents, particularly when the document titles include formulas and subscripts/ superscripts. 6.2. Comparison among search engines
The comparison among search engines has been carried out by using the most effective query type for each tested search engine in each scenario. For example, in scenario Strict , we use query types QT + FS for MSN and UT + FS for the other search for all search engines in a specific scenario. The analysis of the results is shown next. Note that MAP and MRR values are comparable only within a specific scenario.

Table 4 shows the effectiveness of the search engines according to the MAP metric. For each search engine, we also show gains over the search engine immediately below. Bold values correspond to statistically significant gains. Scholar clearly out-ible regarding which she considers a relevant material. Google and Yahoo! present similar performances, being Google significantly better than Yahoo! only when restricted content is considered relevant (scenarios Flexible and Highly Flexible ).
Yahoo! slightly outperforms Google in scenario Strict &amp; Free probably due to the great amount of restricted content indexed by Google. MSN and CiteSeer have not shown to be good alternatives in general.

As shown in Table 5 , results for MRR are similar to those obtained with MAP but with gains of smaller magnitude. Addi-tionally, in flexible scenarios, Scholar and Google present similar performance. Thus, we can say that users do not notice much difference between Scholar and Google when they are only interested in one relevant result that is close to the top of the ranking.
 6.3. Improvements
By analyzing our results, we observed that the tested search engines have different coverages of the Web. This is consis-sources that, in general, provide no free access to their content. This ranking is also heavily influenced by citation count.
Based on these observations, in this section, we present strategies to improve our best results. In particular, in Section 6.3.1, we describe how to combine rankings to improve content coverage. In Section 6.3.2, we study re-ranking strategies aimed at improving the quality of the retrieval, including strategies to promote free content. As seen in previous sections, conclusions drawn from MRR and MAP results are very similar, therefore, in the next sections we only analyze MAP results. 6.3.1. Combination strategies
Our main aim here is to analyze some possible strategies for combining the results returned by Scholar and Google, the two search engines with the best performance. 11 Note that it is not our intent to produce the  X  X  X est X  combination strategy but only to show that even simple combinations can improve results for this specific task. In particular, we test two strategies, to Google only if Scholar returns no answer.

As we can see in Table 6 , our combination strategies yielded significant gains in all scenarios, except SGC 00 in the case of can be explained by the larger coverage of this strategy. 6.3.2. Re-ranking strategies
Having considered the above combination strategies, in order to better assess their retrieval effectiveness we then com-pare Scholar and Google rankings with an ideal one, i.e., a ranking where all relevant entries appear on top of the irrelevant &amp; Free , and Flexible &amp; Free . With respect to Google, the room for gains is more limited in all scenarios.
The larger room for gains with Scholar is due to the fact that its ranking is strongly influenced by citation count, which in some cases becomes more important than the actual similarity between the query and the document. Additionally, Scholar  X  X  X refers X  URLs coming from DLs of well-known publishers such as ACM, IEEE, Springer and Elsevier in detriment of free ones.
This makes sense since the  X  X  X fficial X  URL is probably the most reliable one. However, such a preference obviously does not work in scenarios in which restricted content is considered irrelevant.

In order to improve Scholar X  X  ranking, we reorder document positions of the result list as follows. Given two documents in the result list, f i and f j , we change their positions if the title of f positions if the URL frequency of the domain 12 of f j is greater than that of f quency of an document full-text source the smaller the likelihood that it will supply free content.

To calculate the title similarity, we used the cosine distance weighting terms according to the traditional TF-IDF weight-ing scheme (Baeza-Yates &amp; Ribeiro-Neto, 1999 ). To calculate the domain frequencies, we sampled 200 metadata records of missing URL papers from our test collection. From these records, we built queries and submitted them to Scholar. We then calculated the domain frequencies from the resulting lists in a pre-processing batch mode based on the number of times a are similar, the one coming from the most infrequent domain (e.g., personal or research group lab pages) have a higher prob-ability of providing free access to the full text of documents than those coming from a highly frequent domain (e.g., large publishers or commercial DLs).
 We applied the described re-ranking strategy to Scholar obtaining gains of about 10% in scenario Strict , 16% in scenario ing resulted in a worse performance, which means that Google X  X  ranking is already a good one. We also applied the described 7.

As expected, gains for the re-ranking strategies are higher in scenarios that require free accessibility. Note, however, that there is no meaning on re-ranking results for the cases in which URLs are only provided by either a free source or by a re-stricted one. In Table 8 , we show results of experiments that apply the proposed combination and re-ranking method only to can see, our gains in these situations are even higher. Similarly, by applying the re-ranking strategy without considering the frequency of the URLs, we obtained gains smaller and not significant.
 compensatory.

Table 9 shows the gains ( G ) of RSGC 0 over RSGC 00 for all considered scenarios and the percentage of these gains coming from compensatory gains (CG). We can see that the compensatory gains were inferior to 15% of the overall gain in all sce-lar X  X oogle combination). Further, if processing time is a critical problem, we can always reduce the maximum number of candidates to be extracted. For instance, Fig. 2 shows for the scenario Strict the impact of the number of extracted ranking results on effectiveness of the Scholar X  X oogle combination. As we can see, reducing the number of extracted results to half (from 40 to 20) led to just a slightly decreasing in effectiveness. 7. Coverage analysis In this section, we analyze some factors that may affect the likelihood of finding the URL of a specific full-text document.
For this, we use the notion of coverage, that is, the proportion of documents for which a search engine finds relevant infor-mation. More formally, we define coverage as where n is the total number of documents for which a query q is submitted and m is the number of documents for which at least one relevant document is returned by search engine s as result of q in scenario c .

Since the cost of computing C qcs can be prohibitive due to the need of a manual inspection of the results, we estimate the coverage as a variation interval using a random sampling. Thus, we now compare the effectiveness of the search engines according to their coverage over the resulting sample. Variation intervals are estimated with a 95% confidence level. Intervals without intersection correspond to significantly different coverages.

The results reported in this section are based on the Scholar X  X oogle combination considering the best query types for these two search engines in the scenario Strict . We start by analyzing the influence of the users X  requirement on the results.
Table 10 shows the coverage intervals for the scenarios that we have studied so far considering the sample of 200 metadata records we have used in our experiments. For completeness, we include all the scenarios described in Table 1 .
In Table 10 , we show minimum, mean, and maximum values for the collection coverage interval in each scenario ordered scenarios can be significantly improved with respect to the more restrictive ones. We also note that the percentage of doc-8. Conclusions
We have proposed in this article a process that uses results from queries submitted to search engines for finding the URL of the corresponding full text (or of any relevant related material) for those documents cataloged in a DL but for which this information is missing. This process can be used in the implementation of a service for users in the case a DL does not offer that a DL contain are broken or point only to restricted items, by means of payment, and the user is not willing to complete the transaction. Finally, such a service would also be useful to obtain additional metadata about an item or to discover re-lated material.

We have presented a comprehensive study of this process for conference papers in the Computer Science field by inves-tigating different query strategies applied to several search engines and user scenarios. According to our experimental re-sults, we have concluded that Scholar is the best alternative for this task. Google can be considered as the second alternative, but achieving a performance equivalent to Yahoo! in scenarios where users consider only documents that can be freely accessed. For these three search engines, queries that include the unquoted title along with the surname of the first author (UT + FS) are more effective than those that include quoted titles (QT + FS). CiteSeer and MSN have not shown to be good alternatives for this particular task.

Further, we have shown that, by using a combination of Google and Scholar along with a re-ranking strategy, the overall quality of the process is significantly improved. We have also shown that some reduction in the processing costs can be achieved with low impact on the overall effectiveness. In addition, we have noticed that the percentage of papers in the test collections for which is possible to obtain free access to the full text is significantly higher than for those with restricted access.

Finally, given the fact that search engines such as Scholar also have a good coverage of other fields (Walters, 2007 ), an interesting future work would be to experiment with fields such as Health and Medical Sciences or Physics, which have a large body of knowledge published on the Web. We also intend to study how digital libraries can be enriched by the process discussed here. For this, an experimental service based on the process proposed in this article has been deployed to BDB-Comp ( Santos, Silva, Santos, Laender, &amp; Gon X alves, 2007 ) and is under evaluation.
 Acknowledgements This research is partially supported by the MCT/CNPq/CT-INFO projects 5S-VQ (Grant Number 551013/2005-2) and Info-Web (Grant Number 550874/2007-0), and by the authors X  scholarships and individual research grants from CAPES and CNPq. References
