 Jiashi Feng a0066331@nus.edu.sg Huan Xu mpexuh@nus.edu.sg Shuicheng Yan eleyans@nus.edu.sg This paper is about robust p rincipal c omponent a nalysis (PCA) for high-dimensional data, a topic that has drawn surging attention in recent years. PCA is one of the most widely used data analysis meth-ods (Pearson, 1901). It constructs a low-dimensional subspace based on a set of principal components (PCs) to approximate the observations in the least-square sense. Standard PCA computes PCs as eigenvectors of the sample covariance matrix. Due to the quadratic error criterion, PCA is notoriously sensitive and frag-ile, and the quality of its output can suffer severely in the face of even few corrupted samples. Therefore, it is not surprising that many works have been dedi-cated to robustifying PCA (Hubert et al., 2005; Croux &amp; Ruiz-Gazen, 2005; Candes et al., 2009).
 Analyzing high dimensional data  X  data sets where the dimensionality of each observation is comparable to or even larger than the number of observations  X  has be-come a critical task in modern statistics and machine learning (Donoho, 2000). Practical high dimensional data, such as DNA microarray data, financial data, consumer data, and climate data, easily have dimen-sionality ranging from thousand to billions. Partly due to the fact that extending traditional statistical tools (designed for the low dimensional case) into this high-dimensional regime are often unsuccessful, tremendous research efforts have been made to design fresh statisti-cal tools to cope with such  X  X imensionality explosion X . The work of Xu et al. (2010a) is among the first to an-alyze robust PCA algorithms in the high-dimensional setup. They identified three pitfalls, namely diminish-ing breakdown point, noise explosion and algorithmic intractability, where previous robust PCA algorithms stumble. They then proposed the high-dimensional robust PCA (HR-PCA) algorithm that can effectively overcome these problems, and showed that HR-PCA is tractable, provably robust and easily kernelizable. In particular, in contrast to standard PCA and existing robust PCA algorithms, HR-PCA is able to robustly estimate the PCs in the high-dimensional regime even in the face of a constant fraction of outliers and ex-tremely low Signal Noise Ratio (SNR)  X  the break-down point of HR-PCA is 50%, 1 which is the highest breakdown point can ever be achieved, whereas other existing methods all have breakdown points diminish-ing to zero. Indeed, to the best of our knowledge, HR-PCA appears to be the only algorithm having these properties in the high-dimensional regime.
 Briefly speaking, HR-PCA is an iterative method which in each iteration performs standard PCA, and then randomly remove one point in a way that outliers are more likely to be removed, so that the algorithm converges to a good output. Because in each iteration, only one point is removed, the number of iterations required to find a good solution is at least as much as the number of outliers. This, combined with the fact that PCA is computationally expensive itself, prevents HR-PCA from effectively handling large-scale data-sets with many outliers. In addition, the performance of HR-PCA depends on the ability of the built-in ran-dom removal to eliminate outliers correctly, which is only guaranteed in a probabilistic manner.
 To address these two issues, we propose a determin-istic high dimensional robust PCA algorithm (DHR-PCA). Specifically, instead of removing one point, the proposed algorithm decreases the weights of all ob-servations in each iteration, in a way that the total weight of the outliers will decrease faster than that of the true samples. We show that DHR-PCA inherits all desirable theoretical properties of HR-PCA, including tractability, kernelizability, the maximal breakdown point, provable performance guarantee and asymptot-ical optimality. Moreover, DHR-PCA can be much more computationally efficient than (randomized) HR-PCA. As we show below, the number of iterations for DHR-PCA to converge is nearly constant, in sharp contrast to HR-PCA whose number of iterations re-quired increases linearly with the number of outliers. Simulations in Section 4 show that for any fixed num-ber of iterations, the solution to DHR-PCA is at least as good as HR-PCA, and is significantly better when the number of iterations is small. This is very ap-pealing in practice, as both algorithms are  X  X ny-time X  algorithms, i.e., one can terminate the algorithms at any time and obtain the best solution so-far. Besides HR-PCA, there have been abundant works on robust PCA, which we briefly discuss in this section. Robust PCA algorithms focusing on the low-dimensional setup (e.g., Rousseeuw, 1984; Croux &amp; Ruiz-Gazen, 2005; Hubert et al., 2005) can be roughly categorized into two groups. The first group of algorithms pursue robust estimation of the covariance matrix, e.g. , M -estimator (Maronna, 1976), S -estimator (Rousseeuw &amp; Leroy, 1987), and Minimum Covariance Determinant (MCD) estima-tor (Rousseeuw, 1984). These algorithms generally provide more robust results, but their applicability is severely limited to small or moderate dimensions, as there are not enough observations to robustly esti-mate a high-dimensional covariance matrix. The sec-ond group of algorithms directly maximize certain ro-bust estimation of univariate variance for the projected observations and then obtain maximizers as the can-didate principal components (Li &amp; Chen, 1985; Croux &amp; Ruiz-Gazen, 1996; 2005; Hubert et al., 2002). These algorithms inherit the robustness characteristics of the adopted estimators and are qualitatively robust. How-ever, all of these algorithms run into unsolvable issues in the high dimensional regime incurred by the curse of dimensionality as stated in the followings. The targeted high-dimensional regime poses three main challenges to existing robust PCA methods. First, some robust PCA algorithms have breakdown point inversely proportional to the dimensionality, e.g. , M -estimator (Maronna, 1976), in the high-dimensional regime their breakdown points will dimin-ish and the results will be arbitrarily bad in presence of even few outliers. Second, widely used outlyingness indicators, including Mahalanobis distance and Stahel-Donoho outlyingness (Donoho, 1982) are no longer valid, due to a phenomenon termed  X   X  X oise explo-sion X  (Xu et al., 2010a). This causes the algorithms relying on such outlyingness measures (Hubert et al., 2005) to collapse. The third problem is that the di-mensionality may be larger than the number of data points and thus some robust estimators including Min-imum Volume Ellipsoid (MVE) and Minimum Covari-ance Determinant (MCD) (Rousseeuw, 1984) become degenerated. Furthermore, the extremely high compu-tational complexity of these estimators and projection pursuit methods for high dimensional data prevents them from being tractable.
 Finally, we discuss recent works addressing robust PCA using low-rank technique. (Candes et al., 2009) developed a framework to perform robust PCA using low-rank matrix decomposition. Yet, their method fo-cuses on the scenario that random entries of the ob-servation matrix are arbitrarily corrupted, which dif-fers from our setup where one corrupted data point may change the whole column of the observation ma-trix. The later setup is then investigated in Xu et al. (2010b). While their proposed method performs well under a small fraction of outliers, it breaks down for larger fraction of outliers  X  in particular, the break-down point is far from 50%. Moreover, the perfor-mance scales unfavorably with the magnitude of noise, which makes it not suitable for the high-dimensional setup, due to  X  X oise-explosion X .
 In this section, we first formally state the problem setup of the high dimensional robust PCA. Then we provide the details of the proposed DHR-PCA algo-rithm and finally present the main theoretic results on the performance guarantees of the algorithm. 3.1. Problem Setup In this subsection, we present the formal problem de-scription of PCA for the high dimensional data with contamination. Our setup, detailed below for com-pleteness, largely follows that of Xu et al. (2010a). Given n observations, there are t observations not cor-rupted, called authentic samples. The authentic sam-ples z i  X  R m are generated through a linear mapping: z = A x i + n i . Here, noise n i is sampled from normal distribution N ( 0 ,I m ); and the signal x i  X  R d are i.i.d. samples of a random variable x with mean zero and variance I d . The matrix A  X  R m  X  d and the distribu-tion  X  of x are unknown. We assume  X  is absolutely continuous w.r.t. the Borel measure and spherically symmetric. And  X  has light tails, i.e., there exist con-stants K,C &gt; 0 such that Pr( k x k X  x )  X  K exp(  X  Cx ) for all x  X  0. We are interested in the case where n  X  m d , i.e., the dimensionality of observations is much larger than that of signals and of the same order as the number of observations.
 The outliers (the corrupted data) are denoted as o ,..., o n  X  t  X  R m and they are with arbitrary val-ues. We only require that n  X  t  X  t , i.e., the number of outliers are not more than that of authentic samples. Let  X  , ( n  X  t ) /n be the fraction of corrupted points. We observe the contaminated dataset
Y , { y 1 ,..., y n } = { z 1 ,..., z t } [ { o 1 ,..., o and aim to recover the principal components of A , i.e. , the top eigenvectors  X  w 1 ,...,  X  w d of AA T . That is, we seek a collection of orthogonal vectors w 1 ,..., w d , that maximize the following performance metric called the Expressed Variance (E.V.): The E.V. represents the portion of signal A x being expressed by w 1 ,..., w d . Thus, 1  X  E . V . is the recon-struction error of the signal. The E.V. is a commonly used evaluation metric for the PCA algorithms (Xu et al., 2010a; d X  X spremont et al., 2008). It is always less than one, with equality achieved by a perfect re-covery, i.e., the vectors w 1 ,..., w d have the same span as the true principal components {  X  w 1 ,...,  X  w d } . The distribution  X  affects the performance of the algo-rithms through its tail. We hence adapt the following tail weight function V : [0 , 1]  X  [0 , 1] from Xu et al. (2010a), which essentially represents how the tail of  X   X  contributes to its variance, where  X   X  is the one-dimensional margin of  X  and c  X  0 , V (1) = 1, and V (  X  ) is continuous. 3.2. Deterministic HR-PCA Algorithm Our main algorithm is given in Algorithm 1. Here, a Robust Variance Estimator (RVE)  X  V  X  t (  X  ) is adopted to identify the candidate principal components. For w  X  S m , the RVE is defined as where the subscript (  X  ) denotes a non-decreasing or-der of the variables. And it can be seen that the RVE stands for the following statistics: project y i onto the direction w , replace the furthest n  X   X  t samples by 0, and then compute the variance. If the variance is large, it is likely that a correct principal component direction is found. Otherwise, a number of points with largest variance may be corrupted. Notice that the RVE is always performed on the original observed set Y . We find that RVE coincides with the robust L-estimator, which is defined as a linear combination of order statis-tics: T n = P n i =1 a ni h ( x ( i ) ) for some function h . We now explain our innovation compared to HR-PCA, and its intuition. In HR-PCA, steps 4 and 5 are re-placed by a random removal  X  the probability  X  y i being removed is proportional to P d j =1 w T j  X  y i 2 . It has been shown in Xu et al. (2010a) that in expectation (and in probability) , either the number of outliers will decrease faster, or the algorithm will find a good solution. Since in each iteration, only one point is removed, the num-ber of iterations required to find a satisfactory output depends linearly on the number of outliers.
 Instead of resorting to a random mechanism, DHR-PCA deterministically reduce the effect of corrupted data points. In particular, Moreover, DHR-PCA op-erates on all the data points in each iteration, which decouples the dependence of the computational cost on the number of outliers and enhances the efficiency significantly compared with HR-PCA. We consider an artificial example to illustrate this: assume both HR-PCA and DHR-PCA requires M iterations for a data-set Y 0 . Now suppose a new data-set Y contains J identical copies of data-set Y 0 . Then the number of iterations for DHR-PCA remains unchanged, while HR-PCA requires JM iterations. Simulation results Algorithm 1 DHR-PCA.

Input: Contaminated sample set Y = { y 1 ,..., y n } X  R m , parameters d,  X  t .
 Output: Recovered PCs: w  X  1 ,..., w  X  d .

Initialize  X  y i := y i , X  i = 1 ,  X  i = 1 ,...,n ; Opt := 0. repeat until Convergence for more realistic setups, reported in Section 4, also demonstrate that the deterministic algorithm provides higher efficiency than HR-PCA.
 Theorem 1 and Theorem 2 below show that the pro-posed algorithm achieves the same performance guar-antees as HR-PCA. The proofs are shown in Section 5. Theorem 1. (Finite Sample Performance) Let the Al-gorithm 1 output { w 1 ,..., w d } . Fix a  X  &gt; 0 , and let  X  = max( m/n, 1) . There exists a universal con-stant c 0 and a constant C which can possibly depend on  X  t/t, X ,d, X  and  X  , such that for any  X  &lt; 1 , if n/ log 4 n  X  log 6 (1 / X  ) , then with probability 1  X   X  the following holds  X   X   X  We also consider the asymptotic performance of the proposed algorithm when the dimension and the number of data points grow together to in-finity. Our asymptotic setting is similar to (Xu et al., 2010a). Suppose there exists a sequence of sample sets {Y ( j ) } = {Y (1) , Y (2) ,... } , where sponding values of the quantities defined above, the following must hold for some positive constants c 1 ,c 2 : While trace A ( j ) T A ( j )  X   X  , if it scales slowly than p m ( j ), the SNR will asymptotically decrease to zero. The last three terms in Theorem 1 go to zero as the dimension and number of points scale to infinity, i.e., as n and m  X  X  X  . Therefore, we immediately obtain: Theorem 2. (Asymptotic Performance) Given a se-quence of {Y ( j ) } , if the asymptotic scaling in Expres-sion (1) holds, and lim sup  X  ( j )  X   X   X  , then the follow-ing holds in probability when j  X   X  (i.e., when n and m  X  X  X  ), Observe that when  X   X  = 0, i.e., the number of outliers scales sublinearly, the right-hand-side converges to 1 by taking  X  ( j ) = p  X  ( j ), implying that the algorithm is asymptotically optimal. On the other hand, for any  X  &lt; 0 . 5, the right hand side is strictly positive (picking  X  large enough), implying that the breakdown point converges to 50%.
 For small  X  , we can make use of the light tail condi-tion on  X   X  , to establish the following bound that sim-plifies (2). The proof is deferred to the supplementary material.
 Corollary 1. Under the settings of the above theorem, the following holds in probability when j  X   X  (i.e., when n,p  X  X  X  ), lim inf Before concluding this section, we remark that DHR-PCA is easily kernelizable. Specifically, given a map-ping function  X  (  X  ) : R m  X  X  and kernel function k (  X  ,  X  ) satisfying k ( x , y ) =  X   X  ( x ) , X  ( y )  X  for all x , y  X  we can perform dimension reduction without requiring the explicit form of  X  (  X  ) in the kernel PCA (Sch  X olkopf et al., 1997). In particular, for the centered mapped represented as And the feature projection can be calculated by where a ( q ) is the q th eigenvector of the kernel ma-trix. Note that Algorithm 1 only involves calculating  X  w q , X  ( y i )  X  (in RVE evaluation) and  X  w q , X  ( (in decreasing values of  X  i  X  X ). Since the kernelization of both these two steps are obtained, the DHR-PCA algorithm can be kernelized easily. We devote this section to experimentally comparing the proposed DHR-PCA with HR-PCA. Since HR-PCA has shown superior robustness (against the di-mensionality and number of outliers) over several ro-bust PCA algorithms and standard PCA (Xu et al., 2010a), we skip simulations for them here.
 The numerical study is aimed to illustrate that DHR-PCA is much more efficient than HR-PCA, and mean-while it achieves competitive performance. Here, we report the results for d = 1. We follow the data gener-ation method in (Xu et al., 2010a) to randomly gener-ate an m  X  1 matrix and then scale its leading singular value to  X  . A  X  fraction of outliers are generated on a line with a uniform distribution over [  X   X   X  mag , X   X  mag]. Thus,  X  X ag X  represents the ratio between the magni-tude of the outliers and that of the signal A x i and is fixed as 10. The value of  X  t is set as (1  X   X  ) n , if  X  is known exactly. Otherwise,  X  t can be simply set as 0 . 5 n . For each parameter setup, we report the average result of 20 tests and standard deviation.
 Figure 4 shows the results for m = 100, 1000 and 10000 cases respectively with  X  = 5. From the figure, we can make following observations. Firstly, DHR-PCA con-verges much faster than HR-PCA, especially for a large number of outliers. For example, when m = 10000 and  X  = 0 . 4, the proposed algorithm converges using less than 2 iterations in average while HR-PCA needs more than 4000 iterations to converge. Secondly, the computational time for DHR-PCA in each iteration is always in the same order as HR-PCA. These results well demonstrate that DHR-PCA is much more effi-cient than HR-PCA.
 As for the performance, i.e., the E.V. of the recovered PCs, Figure 4 shows that DHR-PCA performs com-petitively to HR-PCA. For all the cases, the E.V. of fi-nal solution of DHR-PCA is always larger than that of HR-PCA. Moreover, if we terminate both algorithms at any early iteration, DHR-PCA always perform bet-ter than HR-PCA. This is appealing in practice, as we can terminate DHR-PCA at any time and obtain a satisfactory result in practical implementation. In ad-dition, both DHR-PCA and HR-PCA perform quite well even in presence of varying number of outliers (  X  = 0 . 05 to 0 . 4) and small signal magnitude (  X  = 5), which coincides with the results in (Xu et al., 2010a). We then investigate the relationship between the num-ber of iterations before convergence and the number of outliers for the two methods. As shown in Figure 2, the number of iterations taken by HR-PCA is ap-proximately proportional to the number of corrupted points. This is not surprising, since in each iteration HR-PCA removes at most one outlier. In a stark con-trast, the number of required iterations of DHR-PCA remains nearly constant, shown by the flat curve in the figures. This demonstrates that DHR-PCA has good scalability and can potentially be applied to large real applications. We provide more simulations under nu-merous settings in the appendix. In this section, we sketch the proof of Theorem 1. In what follows, we let d,m/n, X ,  X  t/t , and  X  be fixed. We can fix a  X   X  (0 , 0 . 5) w.l.o.g. due to the fact that if a result is shown to hold for  X  , then it holds for  X  0 &lt;  X  . The letter c is used to represent a constant, and is a constant that decreases to zero as n and m increase to infinity. Let w 1 ( s ) ,..., w d ( s ) be the candidate so-lution at stage s . Let Z and O be the sets of indices of authentic samples and corrupted samples respec-tively. We let B d , { w  X  R d |k w k X  1 } , and S d be its boundary. Here Theorems 3 and 4 are directly adapted from (Xu et al., 2010a). 5.1. Validity of the Robust Variance Estimator We first show that the following condition holds with high probability. The detailed proof can be found in (Xu et al., 2010a).
 Condition 1. There exists 1 , 2 ,  X  c such that Theorem 3. Fix any  X  &lt; 1 . With probability at least 1  X  3  X  , Condition 1 holds uniformly for all t 0  X   X t , with  X  c = c X  (1 + log(1 / X  ) n ) , 2 = c log 2 n log 3 (1 / X  ) / constant c possibly depends on d ,  X  and  X  .
 Under Condition 1, RVE is a good estimator.
 Theorem 4. Let t 0  X  t . Suppose Condition 1 holds. Then for all w  X  X  m the following holds:  X   X  (1 + 1 ) k w T A k 2 V From the above theorem, we can immediately obtain the following corollary.
 Corollary 2. Let t 0  X  t . Suppose Condition 1 holds. Then for all any w 1 ,  X  X  X  , w d  X  X  m the following holds:  X   X  (1 + 1 ) V and where H ( w ) , P d j =1 k w T j A k 2 . 5.2. Finite Steps for a Good Solution In this step, we show that the algorithm finds a good solution in a small number of steps. Proving this in-volves showing that at any given step, either the al-gorithm finds a good solution, or the weight adjusting step decreases weights of corrupted points more than the authentic points. Let  X  ( s ) i denote the weight of the i th data point in the s th stage. These points are a good solution if the variance of the points projected onto their span is mainly due to the authentic sam-ples rather than the corrupted points. We denote this  X  X ood output event at step s  X  by E ( s ), defined as: where the variance v i ( s ) = P d j =1 w j ( s ) T y i intuition is that there cannot be too many steps with-out finding a good solution, since too many weights of the corrupted points will have been decreased to zero. Theorem 5. The event E ( s ) is true for some 1  X  s  X  s The proof of the above theorem is provided in the sup-plementary material. We compare Theorem 5 with its randomized counterpart, Theorem 9 of Xu et al. (2010a). The latter states that for HR-PCA, E ( s ) suc-ceeds with high probability for some s  X  (1 + )(1 +  X  )  X n/ X  , where depends on  X  and  X  , and decreases to 0 when n  X   X  (for fixed  X  and  X  ). Thus, the advan-tage of Theorem 5 is two-fold: it is deterministic as opposed to probabilistic, and it does not require the decreasing . 5.3. Bounds on the Solution Performance Let  X  w 1 ,...,  X  w d be the eigenvectors corresponding to the d largest eigenvalues of AA T , namely the opti-mal solution, w  X  1 ,..., w  X  d be the output of the Al-gorithm 1 and w 1 ( s ) ,..., w d ( s ) be the candidate solution at stage s . We define H ( w 1 ,..., w d ) , P  X  H , H (  X  w 1 ,...,  X  w d ), H s , H ( w 1 ( s ) ,..., w H  X  , H ( w  X  The statement of the finite-sample and asymptotic theorems (Theorem 1 and Theorem 2, respectively) lower bound the expressed variance, E.V., which is the ratio H  X  /  X  H . The final part of the proof accomplishes this in two main steps. First, we lower bound H s in terms of  X  H where s is some step for which E ( s ) is true, i.e., the principal components found by the s th step of the algorithm are  X  X ood X . By Theorem 5, we know that there is a  X  X mall X  such s . Based on the true E ( s ) and the algorithm definition, we can conclude the bound via some algebraic manipulations. The final output of the algorithm, however, is only guaranteed to have a high value of the robust variance estimator,  X  V -that is, even if there is a  X  X ood X  solution at some intermediate step s , we do not necessarily have a way of identifying it. Thus, the next step lower bounds the value of H  X  in terms of the value H of any out-put w 0 1 ,..., w 0 d that has a smaller value of the robust variance estimator. The details of these two steps are deferred to the supplementary material. Combining the results of above two steps, we can obtain the fol-lowing theorem providing a lower bound of the ratio H  X  /  X  H , i.e., the expressed variance.
 Theorem 6. If S s 0 s =1 E ( s ) is true, and there exist 1 &lt; 1 , 2 ,  X  c such that Condition 1 holds, then H  X 
H where D 1 = (2  X  + 4)(1  X  1 ) V  X  t t  X   X  1  X   X  and D 2 = 4(1 + 2 )(1 +  X  ) .
 By bounding all diminishing terms in the right hand side of (3), it reduces to Theorem 1. And Theorem 2 follows immediately. The proofs of Theorem 6 and Theorem 1 are similar to those in (Xu et al., 2010a) and we omit it here. In this work, we proposed a deterministic robust PCA algorithm for high-dimensional data corrupted by ar-bitrary outliers. The algorithm alternates between a classical PCA and decrease of weight coefficients on all the data points. Theoretical analysis showed that the proposed algorithm is tractable, robust to corrupt points, easily kernelizable, asymptotic consistent and achieving maximal breakdown point of 50%  X  to the best of our knowledge, the first deterministic algorithm that achieves these properties in the high-dimensional setup. More importantly, simulation results demon-strated that the proposed algorithm improves com-putational efficiency over its randomized counterpart HR-PCA  X  indeed, the number of iterations required to find a satisfactory solution appears to approximate constant, in sharp contrast to HR-PCA whose num-ber of iterations required increases linearly with the number of outliers.
 H. Xu is partially supported by National University of Singapore startup grant R-265-000-384-133. S. Yan is partially supported by Singapore Ministry of Educa-tion under research Grant MOE2010-T2-1-087.

