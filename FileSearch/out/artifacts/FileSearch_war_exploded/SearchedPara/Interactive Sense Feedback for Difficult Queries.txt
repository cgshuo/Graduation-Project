 Ambiguity of query terms is a common cause of inaccu-rate retrieval results. Existing work has mostly focused on studying how to improve retrieval accuracy by automati-cally resolving word sense ambiguity. However, fully au-tomatic sense identification and disambiguation is a very challenging task. In this work, we propose to involve a user in the process of disambiguation through interactive sense feedback and study the potential effectiveness of this novel feedback strategy. We propose several general methods to automatically identify the major senses of query terms based on global analysis of document collection and generate con-cise representations of the discovered senses to the users. This feedback strategy does not rely on initial retrieval re-sults, and thus can be especially useful for improving the results of difficult queries. We evaluated the effectiveness of the proposed methods for sense identification and presenta-tion through simulation experiments and user studies, which both indicate that sense feedback strategy is a promising al-ternative to the existing interactive feedback techniques such as relevance feedback and term feedback.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  search process, query formula-tion, relevance feedback Algorithms, Experimentation Query Analysis, Query Reformulation, Interactive Feedback, Sense Disambiguation
Ambiguity is a fundamental property of natural language, which negatively affects the quality of retrieval results by decreasing precision. Generally, an ambiguous query can be defined as any query which contains one or several polyse-mous terms. The difficulty of lexical ambiguity resolution (or sense disambiguation) varies greatly depending on sev-eral factors. When a query is sufficiently long, other terms in the query may serve as effective disambiguation clues due to the collocation effects [12]. In such cases, a search system may attempt to resolve ambiguity in an unsupervised way or by leveraging external resources, such as on-line dictio-naries [13] or thesauri (e.g., WordNet [34] [30] [15]). Auto-matic disambiguation, however, proved to be very challeng-ing, particularly because queries are usually very short and even humans cannot perform it with perfect accuracy.
The problem of ambiguity is exacerbated when a user X  X  information need corresponds to a minority (non-popular) sense of an ambiguous query term in the collection. In such a case, the initial retrieval results would most likely be dom-inated by a large number of non-relevant documents cov-ering the popular, but distracting senses of an ambiguous query term, while the relevant documents covering the non-popular sense that the user is interested in may be ranked so far down in the ranked list that even diversification of search results would not be very helpful. Clearly, for such difficult queries, any feedback techniques that rely on the assumption that there is some relevant information in the top ranked results (e.g., pseudo feedback, document-level relevance feedback, top results-based term feedback) would not work well either. Consequently, designing an effective feedback method for such difficult queries is a theoretically and practically important problem, particularly in those do-mains, where short and ambiguous queries prevail, such as Web search.

In this work, we propose interactive sense feedback (ISF), a new method for interactive query disambiguation and re-formulation, which, unlike the previously proposed methods for interactive relevance feedback [21], such as explicit [9] and term feedback [10] [31], does not rely on the assump-tion that the initial retrieval results contain relevant docu-ments. Because of its independence of the initial retrieval results, ISF can leverage user interaction both during the early stages of the search process or after it is complete.
At the high level, the proposed ISF is similar to query spelling correction, a popular and widely used feature of all major search engines. When a user submits a misspelled query, she may not be aware (at least immediately) of the reason the search results are of poor quality. A search sys-tem, however, can detect the problem, step in and try to improve the results by asking a user if she accidentally mis-spelled the query. Similarly, when users submit ambiguous queries, they are likely to spend some time and effort perus-ing search results, not realizing that the sense of a polyse-mous term that they had in mind is not the most common sense in the collection being searched. Similar to spelling correction, along with presenting the initial search results, a search system can provide sense suggestions to narrow down the scope of the query. Ideally, sense suggestions can be pre-sented as clarification questions (e.g.,  X  X id you mean &lt; biguous query term &gt; as &lt; sense label &gt; ? X  ), where the sense label can be either a single term or multiple terms.
Our approach is aiming to only signal and reveal the am-biguity of one or several query terms, leaving the final deci-sion whether to disambiguate the query or not to the user. In some sense, our approach takes the best of both worlds: search systems can leverage the vastness of the data and their processing capabilities to infer the col lection-specific senses of query terms and signal potential problems early on, while the users can leverage their intelligence and world knowledge to interpret the signals from the system and make the final decision. If the users are satisfied with search re-sults, they may simply disregard sense suggestions. How-ever, if the quality of search results is poor and a user can easily identify the desired sense of an ambiguous query term, she may indicate that sense and rely on the search system to update the results, according to the provided feedback.
We illustrate the idea of interactive sense feedback with the following example scenario. Suppose a user submits an ambiguous short query like  X  X iracy X  and is looking for doc-uments about instances of copyright law violations as op-posed to armed ship hijackings. In a collection of recent news documents, the intended sense of  X  X iracy X  corresponds to a minority sense, and one would expect the top-ranked retrieved documents to be non-relevant. Instead of having a user go through the search results and locate the relevant documents, a search system can instead find all the contexts, in which the query term occurred in the collection, indicate that the query term likely has two distinct collection-specific senses and ask the user  X  X id you mean piracy as copyright infringement? X  or  X  X id you mean piracy as ship hijacking? X  .
From the above discussion, it follows that interactive sense feedback needs to address the following two major problems. The first problem is designing an efficient algorithm for auto-matic off-line identification of discriminative senses of query terms through the global analysis of document collection. We emphasize the global analysis because a local analysis method such as pseudo-feedback cannot discover minority senses when the initial search results are poor, a scenario which we focus on in this work. The second problem is how to generate a representation of the discovered senses in such a way that each sense is easily interpretable and the best sense (i.e. the sense that results in the best retrieval perfor-mance) is easily identifiable by the users.

To solve the first problem, we propose and study several different algorithms for discovering the query term senses based on the global analysis of the collection. We compare these algorithms based on their upper bound retrieval per-formance and select the best performing one.

To solve the second problem, we propose several alter-native methods for concise representation of the discovered senses and conducted a user study to evaluate the effective-ness of each method with the actual retrieval performance of user sense selections.

The rest of the paper is organized as follows. In the next section, we review the related work. In Section 3, we in-troduce the general idea and formally define the concept of interactive sense feedback. In Sections 4 and 5, we provide a detailed description of the methods used for sense detection and representation, respectively. In Section 6, we experi-mentally determine the potential and actual retrieval effec-tiveness of sense feedback and report the results of our user study. Finally, section 7 concludes the work and provides directions for future research.
Methods to improve the qualit y of retrieval results by re-ducing the negative impact of lexical ambiguity have been studied for many years and this research direction proved to be very challenging. Below we briefly overview three major lines of related previous work.

The first line is aimed at understanding the nature of lex-ical ambiguity in IR. This direction has been started by the work of Krovetz and Croft [12], who conducted a series of experiments in order to examine the quantitative aspects of lexical ambiguity in IR test collections and determined its influence on retrieval performance. They concluded that achieving benefits from disambiguation methods is depen-dent on how successful a sense-aware IR system is in discrim-inatingly applying them, however an effective and robust automatic disambiguation method is still an open problem in IR. Their results were later enhanced by a series of works by Sanderson [26, 23, 24, 22]. In [22], Sanderson concluded that improvements in IR effectiveness from using automatic disambiguation methods can be observed only if those meth-ods can provide the accuracy close to that of humans and wrong disambiguation decisions can dramatically hurt the retrieval performance.

The second line aims at performing automatic sense dis-ambiguation during retrieval by using external resources (such as machine-readable dictionaries [13], thesauri (e.g., Word-Net) or supervised methods. Voorhees [34] conducted the first large scale study of a retrieval system which featured au-tomatic word sense disambiguation based on WordNet and concluded that automatic sense disambiguation did not im-prove the performance. Mandala et al. [17] proposed a method to combine three different thesaurus types for query expansion: manually constructed (WordNet), automatically constructed based on document co-occurrence relations and automatically constructed based on head-modifier relations and found out that improvements in retrieval performance can be achieved by combining all three types of resources. Liu et al. [15] proposed several heuristics for disambiguat-ing the query terms that used adjacent query terms and WordNet. Kim [11] proposed an approach for coarse-grained disambiguation of nouns by mapping them into 25 unique terms associated with the root synsets of each of the noun hierarchies in WordNet. Gonzalo et al. [8] showed that the performance of vector space retrieval model can be im-proved if WordNet synsets are chosen as the indexing space. In general, approaches relying on external resources share the common problems of coverage (a query term may have a specialized sense in a particular domain, which may not be covered by a generic lexical resource) and domain mis-match (some of the dictionary senses may not occur in the collection being searched). Automatic disambiguation has also been addressed in the context of vector space retrieval methods. Sch  X  utze et al. [27] proposed a method to learn the senses from a vector space representation of the term con-texts during training and classify the senses during testing. They achieved the best experimental results by allowing a word to be tagged with up to three senses and combining term and sense ranking. Stokoe et al. [29] used state-of-the-art disambiguation algorithm based on supervised ma-chine learning that was trained on an external corpus to perform retrieval experiments on the TREC WT10G data set and concluded that sense based vector space retrieval consistently outperformed traditional vector space models even if the accuracy of the disambiguation algorithm is be-low 90%. The query expansion method proposed by Qiu and Frei [20] for generalized vector-space retrieval models used global term co-occurrence data to select the best expansion terms by ranking them according to the vector-space based similarity score of a term to the entire query. Fonseca et al. [7] represented the query concepts as a set of related past queries from the search logs and proposed an interac-tive query expansion technique for web queries.

The third line aims at addressing the problem of am-biguity indirectly by improving the initial retrieval results through various types of relevance feedback. In the con-text of pseudo-relevance feedback (PRF), the problems of minority sense and query drift have been addressed through clustering [32] [14] [33]. In particular, Liu and Croft [16] pro-posed to cluster the initially retrieved documents and used the discovered clusters to smooth the document language model. Pu and He [19] went one step further and proposed to use independent component analysis as a dimensionality reduction technique and cluster the top retrieved documents in the latent semantic space. Xu et al. [36] used a com-bination of query-specific clustering and external resource (Wikipedia) for query expansion. However, both the tradi-tional and clustering-based PRF can be effective only when there are some relevant documents in the top results, which is generally not the case for difficult queries. In the context of interactive term feedback, an alternative to the document-based relevance feedback, Anick and Tipireni [3] proposed a method for creating lexical hierarchies of expansion terms, based on the linguistically-aware processing of the document collection. A similar method, but based on using simple co-occurrence statistics has been proposed by Sanderson and Croft [25]. Carmel et al. [5] proposed to use lexical affinities to automatically select the expansion terms in such a way that the information gain of the retrieved document set is maximized. Tan et al. [31] proposed a method for interac-tive term feedback based on clustering the initial retrieval results. They reported that users were having difficulties in selecting the good expansion terms, primarily because term clustering generally lacks semantic coherence. We believe that term feedback has two major limitations. First, similar to PRF, it uses the initially retrieved documents for gen-erating feedback terms, which makes it ineffective for dif-ficult queries. Second, since term feedback does not take into account the relationships between individual terms, it cannot capture the semantics of the feedback terms well. Wang et al. [35] proposed the concept of negative feedback, when only negative signals are used for improving difficult queries, however, to the best of our knowledge, there has been no prior work on improving difficult queries through interactive relevance feedback. Therefore, the primary mo-tivation behind interactive sense feedback is to overcome the limitations of existing relevance feedback methods for diffi-cult queries, when poor search results are caused by query ambiguity.
Despite years of research, there is still no consensus within the AI and IR research communities about what kind of in-formation is the most useful for sense disambiguation. De-pending on the definition of a word sense, there are two ma-jor ways to approach word sense disambiguation. Within the first view, the sense of a word is defined as its intrin-sic property, which correspo nds to the high-level concepts denoted by the word lexeme. This view assumes that the correct and comprehensive specification of the word sense requires complete knowledge about the world and can only be provided in the form of a manually created knowledge base. The second view assumes that the sense of a word, rather than being its predefined property, is determined by various contextual clues, such as its syntactic role and the nearby context.

This work adopts the latter view and is based on the as-sumption that the senses of a query term can be differen-tiated by grouping and analyzing all the contexts, in which it appears in the collection . Consequently, a sense-aware retrieval model should consider not only individual terms, but also all the contexts, in which those terms appear in the collection. It uses two types of contexts: the local con-text , which corresponds to an individual co-occurrence of a term with other terms within a certain unit of text (such as a text window or the entire document) and the global context , which aggregates all the local contexts associated with a word. Such aggregation allows to eliminate noise and identify strong, collection-wide contextual co-occurrence re-lations of a given term with other terms in the vocabulary of a collection. The global context for a particular term can then be analyzed to identify the subsets of terms, which con-sistently appear in the global contexts of each other. We con-sider such subsets of terms as the col lection-specific senses of a query term.

Algorithm-wise, sense feedback works as follows: 1. First, a document collection is preprocessed to construct a contextual similarity matrix, which includes all the terms in the vocabulary of a collection using one of the methods in Section 4.1; the contextual similarity matrix is a sparse matrix, in which the rows corresponds to the global contexts of each term in the vocabulary of a collection. 2. Given a query, the retrieval system first constructs a query term similarity graph for each query term, which in-cludes all the terms appearing in the global context of the given query term and the contextual co-occurrence relations between them. Next the system identifies clusters of terms in the query term similarity graph. Each of those clusters is then converted into a language model, which takes into account the strength of relations between the terms in the contextual similarity matrix and represents the col lection-specific sense of a query term. 3. For each of the identified senses, the system generates a concise representation using the method described in Section 5, which can be presented to a user. If a user recognizes the intended sense among those presented by the system, the language model of the original query is updated with the language model of the selected sense. The updated query language model can then be used to retrieve a new set of documents reflecting user feedback and focused on the spe-cific sense of the initially ambiguous query term.
The interactive sense feedback approach has several ad-vantages over the existing feedback methods. Firstly, sense feedback does not rely on the initial retrieval results and can be used either on-line or off-line. Secondly, only those senses that actually occur in the collection would be pre-sented to the users. Finally, sense feedback does not rely on any external resources, and hence is completely general.
We study interactive sense feedback with the language modeling approach to IR, specifically the KL-divergence re-trieval model [37], according to which the retrieval task in-volves estimating a query language model,  X  q for a given term-based query q and document language models  X  D i for each document D i in the document collection C = { D 1 ,...,D .
 The documents in the collection are scored and ranked ac-cording to the Kullback-Leibler divergence: Within the KL-divergence retrieval model, relevance feed-back is considered as the process of updating the query lan-guage model  X  q , given the feedback obtained after the initial retrieval results are presented to the users. Such feedback may be explicitly provided by the user or implicitly derived from the retrieved results. According to this view, sense feedback can be treated as the process of updating  X  q with the sense of an ambiguous query term identified by the user as relevant to her information need.

By following the language modeling approach, given a term-based query q = { q 1 ,...,q n } , a particular sense the query term q i is represented as a sense language model  X   X 
Definition 1. Sense Language Model  X   X  s t for a par-ticular sense s of term t  X  V is a probability distribution p ( w |  X   X  s t ) over a subset of words S  X  V ,where V is a vocab-ulary of a particular document collection C .

Given that a user selects a particular sense s for the query term q i , the language model  X   X  s q i associated with the selected sense can be naturally used for updating the original query language model  X  q through linear interpolation: where  X  is the interpolation coefficient between the sense language model and the original query model.

Definition 2. Contextual Term Similarity Matrix is a sparse matrix S of size n  X  n where n = | V | . Each row S i corresponds to a word w i  X  V and represents a prob-ability distribution over all other words w in the vocabu-lary V , such that the probability mass would be concentrated on the terms, which are strongly semantically related to w Each element S ij of the matrix corresponds to a probability p ( w j | w i ) , which indicates the strength of semantic related-ness of the words w i and w j in a document collection C .
Definition 3. Term Similarity Graph G w i =( V w i ,E w i for a term w i is a graph, in which  X  j  X  V w i , S ij =0 and  X  u, v , such that ( u, v )  X  E w
Having formally defined the concept of a sense, in the fol-lowing sections we discuss the proposed approaches to sense detection and presentation in more detail.
Our sense detection method has two components: con-structing the contextual similarity matrix and clustering the query term similarity graph. We discuss each below.
Constructing the contextual similarity matrix for a docu-ment collection requires a method to calculate the strength of relations between the terms in the vocabulary. In this work, we experiment with two such methods: mutual infor-mation (MI) and HAL scores.
Given two words w and v , the mutual information between them is calculated by comparing the probability of observing w and v together with the probabilities of observing them independently, according to the following formula: where X w and X v are binary variables indicating whether or v are present or absent in a document. The probabilities are estimated as follows: where c ( X w =1)and c ( X v = 1) are the numbers of doc-uments containing the words w and v , respectively, and c (
X w =1 ,X v = 1) is the number of documents that contain both w and v . Mutual information measures the strength of association between the two words and can be considered as a measure of their semantic relatedness. The higher the mutual information between the two words, the more often they tend to occur in the same document, and hence, the more semantically related they are. For each term in the vo-cabulary of the collection, we identify the top k terms that have the highest mutual information scores with the given term and use them as a global similarity context of a term in the contextual similarity matrix of the collection.
Hyperspace Analogue to Language (or HAL) [4] is a rep-resentational model for high dimensional concept spaces. It was created based on the studies of human cognition. Previ-ous work [28] has demonstrated that HAL can be effectively applied to IR. Constructing the HAL space for an n -term Table 1: HAL space matrix for the sentence  X  X he effects of pollution on the population X  vocabulary involves traversing a sliding window of width w over each word in the corpus, ignoring punctuation, sentence and paragraph boundaries. All words within a sliding win-dow are considered as the local context of the term, over which the sliding window is centered. Each word in the lo-cal context receives a score according to its distance from the center of the sliding window (words that are closer to the center receive higher score). After traversing the entire corpus, an n  X  n HAL space matrix H , which aggregates the local contexts for all the terms in the vocabulary is produced. In this matrix, the row vectors encode the preceding word order and the column vectors encode the posterior word or-der. An example HAL space matrix for the sentence  X  X he effects of pollution on the population X  constructed using the sliding window of size 10 (5 words before and after the center word) is shown in Table 1.

In the HAL-based approach, the global co-occurrence ma-trix is first produced by merging the row and column cor-responding to each term in the HAL space matrix. Each term t corresponds to a row in the global co-occurrence ma-of co-occurrences of the term t with all other terms in the vocabulary. After the merge, each row H t in the global co-occurrence matrix is normalized to obtain the contextual term similarity matrix for the collection: Unlike mutual information, HAL uses the contextual win-dows of sizes smaller than the entire document to create the local contexts, which presumably would produce less noisy sets of semantically related terms.
Algorithm 1 is a high-level representation of a method to detect the senses of a given query term q i .
 Algorithm 1 Sense detection for a query term q i 1. forall j : S ij =0 2. forall ( u, v ):( u, v )  X  V q i  X  V q i 3.

C  X  cluster ( G q for k =1to | C | 4. p ( t |  X   X  k q i )=
The algorithm works as follows:
The hypothesis that the query term similarity graphs con-tain inherent clustering structure is based on the observation that they are likely to be small world graphs. Small world graphs correspond to a subset of graphs, in which most pairs of nodes are connected with very short paths. Small world graphs are known to contain inherent community or cluster structure. In this work, we experiment with two methods for finding this structure: Clauset-Newman-Moore community clustering algorithm [6] and clustering by committee [18].
In the proposed sense feedback approach, a sense is rep-resented as a sense language model. Although such a repre-sentation is effective for retrieval, it may not be suitable for presenting the senses to the users, since interpreting it may place a significant cognitive burden on them. Therefore, we need to generate a concise representation of a sense that can be easily interpreted. We explore two sense presentation methods: using the top k terms with the highest probability in the sense language model and selecting a small number of the most representative terms from the sense language model as a sense label. The latter approach uses a subgraph of the query term similarity graph, from which the sense lan-guage model was created to find a subset of terms that cover the subgraph in such a way that the sum of the weights of the vertices in the cover is maximized. This is known as the Dominating Set Problem, which is NP-complete.
 Algorithm 2 Generate a set of labels L for a sense language model  X   X  s q L  X   X  C  X   X 
W  X   X  forall t : t  X   X   X  s q 1. W t  X  W t  X 
W  X  sort ( W ) 2. forall t : t  X  W t 3. L  X  L  X  t
Therefore, we employ a greed y Algorithm 2, which works as follows:
In this section, we present the results for experimental evaluation of sense feedback. We first describe our experi-mental setup and two experimental settings used to study the upper-bound and actual retrieval effectiveness of sense feedback. In the first setting, in order to determine the upper bound for potential retrieval effectiveness of sense feedback on several standard TREC datasets, we simulated the optimal user behavior by measuring the retrieval perfor-mance of all the senses discovered by each sense detection method and saving only the results of the optimal (best per-forming) sense. We also determined the optimal parameter settings for each sense detection method through simulation experiments and compared the upper-bound effectiveness of each method with the baselines. In the second setting, in order to find out whether the users can recognize the query term senses discovered by the best sense detection method and effectively use them to improve the quality of retrieval results, we conducted a user study by asking the users to pick one sense for each query based on different sense pre-sentation methods. We then determined the best method for sense presentation and the actual performance of sense feedback, using different sense presentation methods.
All experiments were conducted on three standard TREC collections: AP (Associated Press), which was used for var-ious Ad Hoc tracks; ROBUST04, which was used for the 2004 Robust track [1] and AQUAINT, which was used for the 2005 HARD [2] and Robust tracks. Various statistics for the experimental datasets are summarized in Table 2. Table 2: Statistics of the experimental datasets
The TREC topics 51-150 for the AP88-89 collection are long, sentence-like queries, including on average more than 3 query terms. The TREC topics 301-450 and 601-700 for the ROBUST04 collection are mostly 2-3 term queries with a small number of highly ambiguous one term queries (e.g, metabolism, robotics, tourism, creativity). The AQUAINT topics include 50 topics, known to be hard (i.e. resulting in very low retrieval performance) from the previous Robust tacks. All documents and queries have been preprocessed by stemming with the Porter stemmer and removing the stop words. For each of the test collections, we precomputed the contextual term matrices using both the mutual information and HAL scores as measures of similarity. We did not in-clude very rare terms (the ones that occur less than 5 times in the entire collection) or very popular ones (the ones that occur in more than 10% of documents) in the contextual term similarity matrices. A maximum of 100 most contex-tually similar terms according to the particular similarity measure have been stored for each term in the contextual term similarity matrix. For the construction of the query term similarity graphs, we selected only those terms with a similarity value greater than 0.001.
We experimentally determine the upper bound for re-trieval performance of sense feedback and compare it with the baseline feedback method on all three test collections. As a baseline, we use the model-based feedback method pro-posed in [38] with the suggested parameter settings: mixture noise was set to 0.95 and feedback coefficient to 0.9. Note that since the proposed sense feedback method is meant to be a complementary, rather than a competing method to pseudo feedback (any pseudo feedback method can be eas-ily combined with sense feedback), we only include pseudo feedback as a reference baseline and are not interested in comparing with the best performing pseudo feedback meth-ods.

The upper bound for the retrieval performance of sense feedback is determined through simulation of a user who is always able to select the optimal sense for each query. Specifically, we first determine all possible senses for each query term and use each sense to update the initial query model, perform retrieval, and estimate its effectiveness us-ing the relevance judgments. The sense that maximizes the average precision of the retrieved results is then chosen as the best sense for a given query. For model-based pseudo-relevance feedback we used the top 10 retrieved documents. For initial retrieval, we used the KL divergence retrieval method with a Dirichlet smoothing prior set to 2000. Before comparing different sense detection methods to the baseline in the following section we determine the optimal configu-ration for each of them. We use the AP88-89 dataset for parameter tuning.
First, we set the interpolation coefficient  X  to 0.9 and em-pirically determined the optimal size of the sliding window, used for construction of the HAL-based contextual similar-ity matrix. Figure 1 shows the performance of Community Clustering (CC) and Clustering By Committee (CBC) with respect to MAP on the HAL-based contextual similarity ma-trix by varying the size of the sliding window used for its construction. Figure 1: Performance of sense detection methods by varying the size of the HAL sliding window
Two important conclusions can be made based on the analysis of Figure 1. First, community clustering consis-tently outperforms clustering by committee for all sizes of the HAL window. Second, the optimal size of the HAL win-dow for both sense detection methods is 20 (10 words to the left and 10 words to the right from the target word). Next, we determine the optimal value of the interpolation coefficient  X  for different combinations of methods for con-struction of the contextual term similarity matrix and sense detection. In these experiments, we set the size of the HAL window to its optimal value of 20. Figure 2: Performance of sense detection methods by varying the interpolation parameter  X  (the name of the sense detection method is before the hyphen and the similarity measure is after the hyphen).

From Figure 2, it follows that the combination of com-munity clustering and HAL-based similarity matrix outper-forms all other sense detection methods. The best configu-rationforeachsensedetectionmethodisasfollows: w =20 and  X  =0 . 5 for CC-HAL; w =20and  X  =0 . 7 for CBC-HAL;  X  =0 . 6 for CC-MI and  X  =0 . 7 for CBC-MI. Having determined the optimal parameter setting for each sense de-tection method, in the next section we determine the best sense feedback method with respect to the upper-bound re-trieval performance and compare it to the baselines.
The upper-bound performance of different combinations of methods for construction of contextual similarity matrix and sense detection on all three experimental datasets is summarized and compared with the baselines in Table 3. For these experiments, we used the best configuration for each sense detection method empirically determined in the previous section. All feedback methods are evaluated based on their ranking of the top 1000 documents with respect to the mean average (non-interpolated) precision (MAP), precision at top 5 and 20 documents (Pr@5 and Pr@20) and the total number of relevant documents retrieved (RR). We also report the retrieval performance of the initial KL-divergence based retrieval (KL), which is used for model-based pseudo-feedback (KL-PF). As explained earlier, we include pseudo feedback only as a reference baseline since sense feedback is meant to be complementary with pseudo feedback, and they can be easily combined.

From the analysis of Table 3, we can make the following conclusions: 1. The combination of community clustering and HAL-2. Community clustering generally outperforms cluster-3. Sense feedback is effective for both short AQUAINT
Next, we compared the upper-bound effectiveness of sense feedback to the baselines in case of difficult queries (i.e., queries whose initial retrieval results have a MAP value less than 0.1). The results are presented in Table 4. As fol-lows from Table 4, sense feedback effectively improves per-formance of difficult queries and outperforms both baselines, particularly improving the ranking of the top results, as in-dicated by significant improvement in Pr@5 and Pr@10, whereas for the AQUAINT dataset, pseudo-feedback de-creased the retrieval performance.
 Table 4: Comparison of the upper-bound per-formance of sense feedback with KL-divergence retrieval model (KL) and model-based pseudo-relevance feedback (KL-PF) on difficult topics. * indicates statistically significant difference relative to KL (95% confidence level), according to the Wilcoxon signed-rank test.  X  indicates statistically significant difference relative to KL-PF (95% confi-dence level), according to the Wilcoxon signed-rank test.

The absolute numbers of difficult and normal topics im-proved by pseudo-feedback and sense feedback on different datasets are shown in Table 5.
 Table 5: Number of difficult (D) and normal (N) topics improved by pseudo-feedback (KL-PF) and sense feedback (SF) in different datasets.

As follows from Table 5, sense feedback improved the re-trieval performance of a significantly larger number of both difficult and normal queries than pseudo-feedback in each dataset.
Although it is clear from the simulation experiments that automatically identified senses have the potential to im-prove the quality of retrieval, the next important question is whether the users can recognize and select the optimal sense from retrieval perspective. In order to answer this question, we conducted a user study. For the user study we selected the AQUAINT topics, since those topics were used in 2005 TREC HARD track, which was created to explore topics and all collections. the methods for improving the accuracy of retrieval systems through  X  X ighly focused, short-duration interaction with the searcher X . In the study, we asked the six participants to as-sume that they are typing a provided TREC query into the search engine box and the search engine asks to clarify the meaning of a query by first selecting a query term and one of its senses that best fits the description of the query and makes the entire query less ambiguous.

We used the best performing combination of community clustering and HAL scores to generate the candidate senses of the query terms for the user study and presented the dis-covered senses using one-term labels, two-term labels, three-term labels, top-three terms from the sense language model and the top 10 words from the sense language model. We then compared query term and sense selections made by the users with the query term and sense selections resulting in the best upper-bound retrieval performance determined through simulation. Table 6 shows the accuracy of sense selection by the users as the fraction (in percentages) of the users, who select both the optimal term and the optimal sense for feedback (in boldface) and the optimal term only (in parenthesis), regardless of whether the selected sense of the term is optimal.
 Table 6: Percentage of users selecting the opti-mal sense of the optimal term for sense feedback (in boldface) and the optimal term, but suboptimal sense (in parenthesis).

As follows from Table 6, on average for most labeling methods the users were able to select the best term for sense feedback at least for half of the queries in the study, which indicates that users are able to identify the potentially am-biguous query terms that can benefit most from sense feed-back. The percentages of times that the users can select both the best term for sense feedback and the best sense of that term are less, achieving the maximum of 36%. The following interesting conclusions can also be made from the analysis of Table 6: 1. Users do not tend to select the best sense more of-2. 3-term labeling and choosing the top 3 terms from the
In order to determine the practical utility of interactive sense feedback, we generated and evaluated the retrieval re-sults based on the actual user sense selections. First we tuned  X  , the parameter for interpolating the sense language model into the original language model. Using sense selec-tions of users for the best sense representation method (we used top 10 terms with the highest weights in the sense lan-guage model for parameter tuning and evaluation, since it is the best sense representation method, according to Ta-ble 6), we varied the value of the interpolation coefficient  X  and plotted the resulting performance on all AQUAINT queries with respect to MAP in Figure 3. Figure 3: Retrieval performance of user sense selec-tions for all queries in terms of MAP, depending on the value of interpolation parameter  X  .

From Figure 3 it follows that when  X  =0 . 8 sense feed-back is consistently most effective in terms of MAP for all the users. Setting  X  to its optimal value, we determined the retrieval performance of user sense selections on difficult topics, according to different sense presentation methods. TheresultsarepresentedinTable7.
 Table 7: Retrieval performance of user sense selec-tions on difficult topics with respect to MAP, de-pending on the sense presentation method. Perfor-mance of the baselines is shown in the first two rows of the table.

As follows from Table 7, although the user sense selections do not achieve the upper bound performance, we can con-clude that interactive sense feedback can effectively improve the retrieval performance of difficult queries.
To gain some insight at what the senses presented to the user look like, in Tables 8 and 9, we show some sample senses discovered by using the community clustering algo-rithm in combination with the HAL scores for the query term  X  X tealth X  of the AP topic #132  X  X tealth aircraft X  and for the query term  X  X ancer X  of the AQUAINT topic # 310  X  X a-dio waves and brain cancer X . Inferring the meaning behind each sense from the top representative terms is not hard, but sometimes requires certain background knowledge. For example Sense 2 of the query term  X  X tealth X  clearly corre-sponds to aircrafts with low radar visibility.

In case of the term  X  X ancer X , senses are less distinguish-able, but nevertheless correspond to semantically coherent aspects of the query topic. For example, sense 1 most likely corresponds to cancer research, sense 2 is about different types of cancer, sense 3 is about cancer treatment and sense 4 is likely to correspond to cancer statistics in the US.
It is important to note that most TREC queries consist of at least 2-3 terms and are generally not highly ambigu-ous. Therefore, several collection-based senses of a query term may have comparable retri eval performance to the best sense and users often select these senses instead of the best performing sense. For example, for the query #625  X  X r-rests bombing WTC X  the best sense is the sense labeled as  X  X olice X  for the query term  X  X ombing X . However, all the users who participated in the study selected the sense la-beled as  X  X rrest X  for the query term  X  X TC X . Similarly, for the query #639  X  X onsumer on-line shopping X  most users se-lected the sense labeled as  X  X eb X  for the query term  X  X on-sumer X , whereas the best sense is the sense labeled  X  X nline X  for the query term  X  X hopping X .
In this work, we presented a novel idea of interactive sense feedback. Sense feedback can automatically discover collec-tion specific senses of query terms, present those senses to the users and update the queries based on user sense se-lections. Because the senses are discovered from the entire collection, this feedback strategy is not biased to focus on the popular senses covered in the top-ranked results, and thus is especially useful for improving performance for diffi-cult queries.

We experimentally determined the upper bound for the retrieval performance of all possible combinations of several different methods for automatic sense discovery and measur-ing the strength of semantic relatedness between the terms. Experimental results show that the combination of Commu-nity Clustering and Hyperspace Analog to Language (HAL) results in the best overall retrieval performance and can also significantly improve the retrieval accuracy for difficult queries. We also proposed different presentation methods for the discovered senses and evaluated the effectiveness of user sense selections when senses are concisely represented. The results show that users are able to select the best senses in most cases, leading to improvement of average retrieval accuracy for difficult queries. Therefore, sense feedback has all the potential to be used as an alternative or supplemen-tal technique to the existing interactive feedback methods, such as term, relevance and pseudo-feedback, particularly for difficult queries.

Our work can be extended in several ways. First, we can explore other methods for automatic sense detection and compare them with the ones proposed in this work. Second, we can investigate alternative ways of effectively presenting senses to the users. Finally, it would be very interesting to experiment with sense feedback on real ambiguous Web-style queries and incorporate sense feedback into search engine infrastructure as a complimentary strategy to search results diversification. We envision that sense feedback will show its full real potential in this case.
This material is based upon work supported by the Na-tional Science Fo undation under Grant Numbers IIS-0713581, CNS-0834709, and CNS 1028381, by a Sloan Research Fel-lowship, and by a Microsoft gift grant. [1] J. Allan. Overview of the trec 2004 robust retrieval [2] J. Allan. Hard track overview in trec 2005 -high [3] P. G. Anick and S. Tipirneni. The paraphrase search [4] C. Burgess, K. Livesay, and K. Lund. Explorations in [5] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. [6] A. Clauset, M. E. J. Newman, and C. Moore. Finding [7] B.M.Fonseca,P.Golgher,B.P X  ossas, B. Ribero-Neto, [8] J.Gonzalo,F.Verdejo,I.Chugur,andJ.Cigarran.
 [9] M. Iwayama. Relevance feedback with a small number [10] D. Kelly and X. Fu. Elicitation of term relevance [11] S.-B. Kim, H.-C. Seo, and H.-C. Rim. Information [12] R. Krovetz and W. B. Croft. Lexical ambiguity and [13] M. Lesk. Automatic sense disambiguation using [14] A. Leuski. Evaluating document clustering for [15] S. Liu, F. Liu, C. Yu, and W. Meng. An effective [16] X. Liu and W. B. Croft. Cluster-based retrieval using [17] R. Mandala, T. Tokunaga, and H. Tanaka. Combining [18] P. Pantel and D. Lin. Discovering word senses from [19] Q. Pu and D. He. Pseudo relevance feedback using [20] Y. Qiu and H.-P. Frei. Concept based query [21] I. Ruthven and M. Lalmas. A survey on the use of [22] M. Sanderson. Word sense disambiguation and [23] M. Sanderson. Retrieving with good sense.
 [24] M. Sanderson. Ambiguous queries: Test collections [25] M. Sanderson and W. B. Croft. Deriving concept [26] M. Sanderson and C. van Rijsbergen. The impact on [27] H. Sch  X  utze and J. O. Pedersen. Information retrieval [28] D. Song and P. Bruza. Towards context sensitive [29] C. Stokoe, M. P. Oakes, and J. Tait. Word sense [30] M. Sussna. Word sense disambiguation for free-text [31] B. Tan, A. Velivelli, H. Fang, and C. Zhai. Term [32] A. Tombros, R. Villa, and C. J. van Rijsbergen. The [33] R. Udupa, A. Bhole, and P. Bhattacharyya.  X  atermis [34] E. M. Voorhees. Using wordnet to disambiguate word [35] X. Wang, H. Fang, and C. Zhai. A study of methods [36] Y. Xu, G. J. Jones, and B. Wang. Query dependent [37] C. Zhai and J. Lafferty. Document language models, [38] C. Zhai and J. Lafferty. Model-based feedback in the
