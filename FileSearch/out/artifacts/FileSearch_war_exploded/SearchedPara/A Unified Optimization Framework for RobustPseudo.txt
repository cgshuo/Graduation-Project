 We present a flexible new optimization framework for find-ing effective, reliable pseudo-relevance feedback models that unifies existing complementary approaches in a principled way. The result is an algorithmic approach that not only brings together different benefits of previous methods, such as parameter self-tuning and risk reduction from term de-pendency modeling, but also allows a rich new space of model search strategies to be investigated. We compare the effectiveness of a unified algorithm to existing methods by examining iterative performance and risk-reward tradeoffs. We also discuss extensions for generating new algorithms within our framework.
 Categories and Subject Descriptors: H.3.3 [ Informa-tion Retrieval ]: Retrieval Models General Terms: Algorithms, Experimentation Keywords: Query expansion, optimization A relatively new advance in IR is the development of risk-aware algorithms that not only attempt to perform well on average across queries, but which seek to dynamically adjust their behavior from query to query to reduce their variance or instability  X  especially to avoid serious errors. As one example of such a task, it is well known that the effective-ness of pseudo-relevance feedback can be highly sensitive to a number of parameters, such as the number of terms, or number of top-ranked documents chosen. Thus, robust al-gorithms seek to reduce instability by finding reliable values for these parameters automatically, removing the need to commit to a single operational setting for all queries.
In one example, Collins-Thompson [4] introduced a novel view of query expansion as a portfolio optimization problem,  X  Work partially conducted during an internship at Microsoft Research.
 resulting in a constrained quadratic program (which we call the CT algorithm) that finds a reliable, effective set of feed-back terms by combining term covariance information with a set of task-specific constraints that prune out bad expansion models. Their approach operates as a post-process on a set of candidate terms and assumes little about the underlying retrieval model. It operates only in the space of expansion terms and and may return the empty set of expansion terms if expansion is deemed too risky for a particular query.
On the other hand, Tao and Zhai [20] introduced a robust pseudo-relevance model (which we call the TZ algorithm) based on the language modeling approach that jointly solves for both term and document weights. Unlike the CT algo-rithm, it includes document weights in the model, but does not model term dependencies or have the ability to prune a sparse subset of expansion terms. While the CT objec-tive is convex and uses  X  X ard X  constraints, the TZ algorithm uses a non-convex likelihood objective, finding local maxima with regularized EM, with a soft, successively relaxed initial penalty on models far from the initial query.

In this work we unify these two seemingly unrelated ap-proaches in a principled way to produce a pseudo-relevance feedback algorithm that jointly determines both the optimal term subset and the optimal document subset to use for a given query, while also allowing a rich set of new potential constraints and improved objective structure, so that term and document dependencies, sparsity, and so on, is easily added. Our evaluation includes standard evaluation met-rics, iterative analysis, and a parameter space visualization computed on a high-performance computing cluster show-ing regimes of mean and variance of performance attained across parameter sweeps.

Making progress on the robust pseudo-relevance feedback problem is important not only for potentially improved re-sult quality, but also because increasingly available context data in Web search engines need a principled framework for exploiting them to model the underlying information need. In addition, improving a query representation has other ap-plications, such as broad matching of search advertisements with web pages. Finally, pseudo-relevance feedback can be seen as an instance of a broader feature selection problem under uncertainty, so better techniques for pseudo-relevance feedback may lead to better, more generally applicable fea-ture selection methods in other areas of information retrieval or machine learning that must deal with limited, noisy train-ing examples and uncertainty in parameter estimation. We derive our optimization model in three steps. First, we describe a generative model introduced by Tao and Zhai that conditions queries and documents on latent variables. Second, we give some background on the TZ algorithm for estimating the parameters of the model using EM and a regularized maximum likelihood objective. Third, we show how to merge the non-convex likelihood of the Tao-Zhai (TZ) algorithm with the convex risk-reward optimization of the Collins-Thompson (CT) algorithm in a principled way by invoking a general optimization family called a Convex-Concave (CCCP) program. Finally, we discuss how our CCCP analysis gives an easy way to generate new algorithms that fix limitations of the TZ algorithm while also allowing new domain-specific constraints and objectives. The overall result is an extremely flexible optimization framework for constructing effective searches for high-quality query repre-sentations that can account for problem structure. Figure 1: The Tao-Zhai (TZ) generative procedure for a sin-gle document in the feedback set. This model posits docu-ments are a two-part mixture of multinomials (on a per-word basis). L denotes the length of the document, i.e., number of multinomial samplings, and  X  its relevant/nonrelevant mixture. Documents are assumed independent, given the parameters. Figure 2: Graphical model depiction of (above) generative procedure (for n documents). Nodes are random variables, with shaded nodes being observed (user X  X  query  X  q ,fixed corpus background model  X  B , word counts c dw ). Plates in-dicate conditionally independent replications.
 Tao &amp; Zhai [20] introduced the simple generative model dw Multinomial random variable indicating number
Z dw Binomial random variable indicating term w of  X 
T Parameters for K -dimensional topic model T (im- X  d Document relevance parameters ( F -dimensional)
F k ( q ) Top-k feedback documents associated with q from
V Vocabulary; with a slight abuse in notation, words  X  q Multinomial parameters for query q , i.e.  X  qw =  X 
B Multinomial parameters for background model B ,
 X  Vector of parameters, typically  X  = [  X  X  T ].  X  Effective sampling size for query, i.e.  X  T  X  Dir (1+ and optimization algorithm for pseudo-relevance feedback 1 shown in Figures 1 and 2, in which feedback documents F ( q ) are generated from a mixture of two multinomial lan-guage models: a background model  X  B and a  X  X elevant topic X  model  X  T . We assume these models use a vocabulary V of dimension K . The TZ model assumes that the topic model  X 
T has a fixed Dirichlet prior, Dir (1 +  X  X  q ). One distinctive feature of their model is that it jointly optimizes both query and document weights simultaneously: the feedback docu-ments are endowed with  X  X elevance X  weights  X  d which are to be learned jointly with the word probabilities of  X  T .
From this assumed generative algorithm, we note that the complete-data log-likelihood (for a single document) is with model prior, log Pr( X  |  X ,  X  q )  X  log Pr(  X  T |  X ,  X  q )=  X  where we have disregarded all normalization terms as they are constant in  X .

We note the similarity of this model to LDA, viz., a two topic LDA with one topic held fixed, and non-informative or constant priors. Since this is a two-topic model, the con-ditional Pr( z dw | c dw ,  X ) is binomial, leaving the incomplete-data log-likelihood as a multinomial with parameters  X  X  T (1  X   X  )  X  B . Learning this model is made difficult by the fact that this log-sum is not linear.
Our notation mostly follows that used in Tao &amp; Zhai, with some extensions. Omitting the subscript indicates the ma-trix/vector, as opposed to the specific element, e.g.  X  is a vector of  X  i and [  X  B ] w =  X  Bw . Also, the value of a variable, e.g.  X  ,after k iterations is denoted by  X  ( k ) .
Several approaches exist to cope with this problem. Tao &amp; Zhai employ standard EM manipulations which yield the following two-step iterative procedure: E-step: M-step: with Lagrangian  X  the appropriate normalization of  X  ( k +1)
Typically the E-and M-steps are repeated until conver-gence, however TZ deviate from this practice by imposing a schedule of decay on parameter  X  , i.e.,  X   X   X  X  k where  X   X  (0 , 1). Convergence is heuristically defined by the cross-over point between the current value of  X  and the expected number of relevant words in the feedback set using the model at iteration k . By forcing the training procedure to pursue only the most significant parameter updates, this approach is intended to cope with high term variance without incur-ring additional modeling overhead. It plays a role somewhat analogous to a prior and helps to prevent overfitting.
However, this approach lacks a certain flexibility. The ability to access fine control over the query prior is inextrica-bly coupled to the rate of convergence of the EM algorithm. This convergence is in turn tied to complicated term-term covariances and document length variability. By reformulat-ing the updates in such a way that accounts for the rate of change in updates, we obtain both a clearer interpretation of  X  as well as finer control across iterations and within (by accounting for parameter covariation). The above EM algorithm seeks a parametrization that maxi-mizes the posterior likelihood under the query-driven Dirich-let prior. Here, we seek an alternative procedure based on the same generative model, but in which we may gain addi-tional flexibility: first by exploring alterative regularizations to improve stability, and secondly by formulating additional objectives or constraints in the space of latent variables ,in-stead of parameter space. The motivation for the latter is that task-specific knowledge can often be expressed by func-tions of expectations over the latent variables that have an intuitive interpretation and give more control over the pa-rameter estimation process. For example, if we have feed-back observations about the utility of specific words or doc-uments, or even specific words in specific documents, we can easily incorporate these as constraints in the latent variable space. To do this, we study generalizations of EM in which the basic closed-form E-step on latent variables is replaced by a more general convex optimization problem. The general idea of EM is to increase the incomplete-data likelihood ( X ; C ) through maximizing some function of the complete-data likelihood; it is assumed that this object, ( X ; C, Z ), is easier to manipulate. Such a formulation can easily be understood through developing a lower bound us-ing the information theoretic functionals, cross-entropy H ( p, q ), entropy H ( p )= H ( p, p ), and KL-divergence D ( p || q )= H ( p, q ) H ( p ). For details, see chapter 2 of [7]. ( X ; C )= The last inequality follows from the non-negativity of KL-divergence, a result commonly known as Gibbs X  inequality. We note that this bound is valid for any distribution q ( Z ).
Standard EM derivations typically choose to exploit the (assumed) simplicity of the conditional and the fact that the entropy portion of F becomes constant for  X . Hence, maximizing (6) reduces to minimization of cross-entropies, i H Pr  X  ( Z i | C i ) , Pr  X  ( C i ,Z i ) . By parameter-izing q in this way, the search path taken toward the MLE is dictated by the precise characteristics of the complete-data model. If we were to allow the minimization to converge, this fact would be of lesser concern and it would be prudent to let computational efficiency constrain the choice of dis-tribution family for q . However, since the TZ likelihood has an adaptive component, i.e.,  X  X ecaying prior, X  the precise nature of the search path becomes more important. Ac-cordingly, we aim to recast the optimization in a way that allows a larger space of potential distributions over Z ,and thus, a richer set of search path strategies.

We begin by restating arguments of [16, 24] who note that an equivalent formulation of EM is,
E-step: q ( t +1)  X  argmin M-step:  X  ( t +1)  X  argmin Ignoring issues like local extrema, this formulation reveals that EM is equivalent to the joint minimizing of F n ( q,  X ) with respect to q and  X . Hence where also yields MLE solutions for the incomplete-data likelihood.
To simplify computation of (10) we treat all Z ij as in-dependent binomials with parameters ( p ij ,c ij ). Although simple, this class is still a richer set of distributions than Pr( Z ij | c ij ,  X ). The entropy of q ( Z | p ) conveniently decom-poses into a term-count weighted combination of binary en-tropies, namely H ( q )= n i =1 V j =1 c ij H ( p ij )where p resents the probability that word j of document i is relevant, i.e., Pr( Z ij = 1). Similar results apply to the cross-entropy term as well. The revised objective p =argmin enjoys several properties which we develop subsequently. Most notably, J ( p ) is a difference of convex functions, i.e., negative entropy ( u ) plus the sum of a log-prior and nega-tive cross-entropy as a function of  X   X  ( q )( v ) [24]. This is the final ingredient which allows us to recast the TZ likelihood into the more general optimization framework of Convex-Concave programs, which we now describe. Now that we have J written as the difference of convex func-tions u and v , we can minimize J using a generalization of EM known as the Convex-Concave Procedure . Yuille and Rangarajan [24] describe the Convex-Concave optimization procedure (CCCP) with the following recurrence: where u , v ,and c i are real-valued convex functions, d j an affine function, v is differentiable, and all are defined on . More information on the convergence properties of the CCCP optimization family are given by Sriperumbudur and Lanckriet [18].

The advantage of the CCCP framework is that it is very general, and in fact includes all EM algorithms and some variational algorithms as special cases [24]. While other EM generalizations exist, CCCP also provides a recipe for de-riving new algorithms for a very wide class of optimization problems, since almost any function can be expressed as a sum of convex and concave functions. Other techniques are not usually so broadly applicable.

The term x T  X  v ( x ( k ) ) is a linear approximation to v at x k ) and is known as a majorizor of v ( x ) since it a tight upper bound for v at the point x ( k ) . Substituting u from Eq.12andthederivative  X  v of Eq. 13 into Eq. 14 gives the convex optimization problem where w =  X  v ( p ( k ) ). This is an unconstrained maximum en-tropy problem that has a simple analytical solution, namely the matrix  X  p with individual sigmoid entries where g ij ( X )=exp(  X  X ij /C ij ). Deriving the specific form of w =  X  v (  X  ) involves only basic calculus but due to space constraints we do not derive it here. We now have a unifying view of the TZ and CT algorithms in one framework. Recall that the CT algorithm X  X  objective is a mean-variance tradeoff inspired by portfolio theory: Comparing this to the CCCP objective for the TZ likeli-hood in Eq. 15, both algorithms use the same bi-criterion form of objective with a linear function of p and a regular-ization function R ( p ), which is an entropy term R TZ ( p )=  X  H ( p ) for TZ and a quadratic term R CT ( p )= x T  X  x for the CT algorithm. Both algorithms estimate expected values p ij  X  [0 , 1] of latent variables: in the CT case p is a single V  X  1 vector, and in the TZ-CCCP case is a V  X  N matrix with one column for each document. Unlike TZ, the CT algorithm was run as a single-step post-process on an initial p 0 produced by a black-box feedback algorithm. Also, the CCCP prior term log Pr( X   X  ( p )) in v ( p ) (Eq. 13) can be seen as a soft constraint corresponding to the CT algorithm X  X  hard query support constraint forcing query term p i values to stay close to 1 in the solution.

In the default TZ model, interactions between terms in  X  T are not explicitly modeled, whereas the CT captures term dependencies using the matrix  X  in the quadratic term. The regularization term R ( p ) in the CCCP objective, however, gives us a place to add dependencies between p ij .Wecan either use a single  X  for all documents, or estimate a matrix  X  d individually for each document. In the next section, we describe how translation kernels can be used to effectively estimate  X . One effective method of estimating semantic term depen-dency is to define a statistical translation process between terms using translation kernels [9]. We create a translation kernel by first computing a similarity graph between all pairs of terms. For vertices u and v ,theedgeweight e ( u, v )isde-fined as a function of f u ( w ), the co-occurrence frequency of term u with term w in the top-ranked documents, giving a matrix E with entries where the sum is taken over all words w in the vocabulary V . The graph heat kernel is computed via the matrix expo-nential of the normalized graph Laplacian where D is a diagonal matrix with D ii = j e ij . The matrix exponential H = exp (  X  t L ) models the flow of heat across the graph as a function of time parameter t ,whichcontrols the amount of translation. For small t , H  X  I and for large t , H is approximately uniform. Finally, we interpolate the submatrix T of  X  by computing  X  T =(1  X   X  ) T +  X H .
In the CT model, p is a 1 xV vector with entries p p ( w i |  X  R )foreachword w i in vocabulary V and relevance model  X  R . X isa V  X  V positive definite term dependency matrix. The parameter  X  specifies the mean-variance trade-off. The weights c were derived from a Relevance Model estimated from top-ranked documents.
Note that the matrix  X  is applied at each step and thus affects the relative change in p . Thus, the role of a low weight in entry  X  vw is to penalize or restrict changes to the model in directions where v and w covary. For example, terms that have high translation probability from the query will be treated almost as conservatively as the query itself: the optimization is reluctant to make large-magnitude changes away from not only query terms but closely related ones. Thus, we retain the conservative strategy of staying close to the initial query, but with the advantage of a flexible, more semantically rich definition of distance. Rather than modify the CCCP objective function of Eq. 15 directly to add additional factors like term dependency into the objective, we can consider a modular approach that is also applicable to standard EM formulations: solve for  X  p analytically (either from the CCCP step or the default EM E-step), and then find the closest latent variable matrix X to  X  p ( k ) but subject to additional conditions. By  X  X losest X , for this paper we use the Frobenius norm, a standard distance measure for matrices. We refer to this as the constrained E-step method. We now gives examples showing how new constraints and objectives can be added to this framework.
Constraints can play an important role in helping to elim-inate low-quality models from consideration by encoding their properties (or rather, the opposite of them) to define a  X  X ard X  feasible set of the problem. In many cases, linear con-straints are sufficient to encode a rich variety of conditions. Note that because these are iterative algorithms, the con-straints can in theory also be dynamic, changing with each iteration to reflect important local factors such as feature confidence. We now give two examples of how extensions can be fit into the constrained E-step framework. Diversity constraints In the search for reliable solutions, we may consider that relying too heavily on only a small number of uncertain latent variables is too risky. Instead, we could implement a diversity constraint over the words in each document, so that no more than  X  w -percent of the total probability mass can be allocated to the top r w terms. It turns out that this can be expressed as a linear con-straint ([1], p.279) via auxiliary vectors u j of size | V scalar variable t j for each document d j , along the columns of X k ) . This diversity constraint appears superficially similar to standard smoothing methods, in that it acts to redis-tribute probability mass from higher-probability events to lower-probability ones. However, unlike standard smooth-ing, relative changes in latent variable mass can change sig-nificantly from iteration to interation, due to the nature of the top-k criterion and hard upper-bound on the mass 3 .We also note that the aspect balance constraint over query mod-els in the Collins-Thompson query expansion approach is a type of linear diversity constraint [4].
 Term dependencies We can also make use of the transla-tion kernels described in Section 2.4. As before, we create
We note that a similar form of diversity constraint could be applied to documents by constraining the rows of X ( k which hold the latent variables for occurrences of a single word w across all documents. We might prefer states in which there must be stronger evidence across multiple doc-uments instead of relying on a single source. Variance-based Markowitz-type diversity [4] is another possibility. These are topics for future work. subject to  X  i X ij = X  i  X  p ij Doc mass invariant (21) Figure 4: The basic constrained E-step for finding the closest matrix X to the default E-step matrix  X  p , while respecting diversity constraints over a document X  X  latent variables for words, and using a translation kernel  X  T in the objective. Here, j =1 ...F over the set of F feedback documents. a translation kernel  X  T = exp (  X  t L ) with time parameter t , which controls the amount of translation. We use a trans-lation strength parameter  X  to combine  X  T with p using ( I +  X   X  T ).

An optimization step that brings all these together is shown in Figure 4. To use it in a standard EM algorithm, we simply replace the normal E-step with our convex pro-gram, and use the optimal solution matrix  X  X instead of the default matrix  X  p . To use it with the CCCP program, we can apply it to the  X  p solution from Eq. 15. In Section 4.5 we do a basic evaluation of the effects of these generalizations on the risk-reward tradeoff of the query model estimation algorithm. We leave further exploration of objectives and constraints for future work. The most relevant previous studies are of the two algorithms by Collins-Thompson [3] and Tao &amp; Zhai [20] described ear-lier. Xu and Akella [22] replaced the TZ two-mixture genera-tive model with a Dirichlet Compound Multinomial, using a different latent variable model and closed-form E-step based on simulated annealing. It would be interesting to explore the use of the latter X  X  more sophisticated generative model within our CCCP optimization framework.

CCCP and related algorithms have seen increased use re-cently for machine learning problems. For example, Yu and Joachims gave a CCCP for learning structural Support Vec-tor Machines with latent variables [23]. CCCPs themselves are connected to a broad class of majorization-minorization algorithms, in which EM is a special case. Such frameworks have been introduced and motivated by problems in areas like image restoration [10], but we have not seen much ap-plication yet to information retrieval problems.

Previous work on regularization schemes can be divided into two types: term score smoothing, and document score smoothing. In the document score domain, Diaz [8] intro-duced the use of regularization that smoothed over the graph of document-document similarities. It would be interesting to investigate this type of smoothing in the parameter ma-trix of our model, in addition to the term-term smoothing that our translation model does. In the term domain, Mei and Zhai [15] described smoothing language models over graph structures. Unlike these previous approaches, our framework can model structure between terms and docu-ments, not just between entities of the same type. In mo-tivating the importance of modeling term dependencies, we note a recent study by Udapa et al. [21] that confirmed the importance of accounting for term dependencies and set-level properties in finding higher-quality expansion sets, compared to searching for expansion terms individually.
The use of heat-transfer kernels for query expansion is an-other contribution of this paper. The closest previous work on random walk models for query expansion [6] also used a term dependency graph in which word co-occurrence was one of several dependency types, with combined transition edge weights estimated using logistic regression. The heat-transfer approach has the advantage of giving a more inter-pretable, statistically principled derivation of term transi-tion probabilities.

Finally, we have drawn inspiration from a key reference work by Graca et al. [11], who proposed modifying EM using a constrained E-step in order to model posterior constraints. They also gave theoretical results that give a penalized maxi-mum likelihood interpretations to their framework, and gave examples of several natural-language applications, including statistical translation. Here we confirm the utility of the basic CCCP formulation and include preliminary analysis of the effect of term depen-dency and diversity constraints in latent variable space on performance.

We use two standard test collections: TREC 1&amp;2 (topics 51-200, TREC disks 1&amp;2) and Robust 2004 (topics 301 X  450 and 601 X 700, TREC disks 4&amp;5). We chose these partly because topics 301 X 450 (the TREC678 topic set) overlap with those used in the TZ study, while also adding 100 new queries that typically are more challenging for query expan-sion. Also, the RIA workshop [12] made available an ex-tensive failure analysis over the TREC678 topic set. Index-ing and retrieval were performed using the Indri 2.8 system in the Lemur toolkit [14]. We used the title fields of the TREC topics and phrases were not used. We also did not use stopping or stemming since we believe this removes po-tentially valuable word evidence, and that a principled ap-proach should be able to make stopword and stemming deci-sions automatically as part of the estimation process. Doc-ument scoring was performed using query likelihood with the top 1000 documents retrieved and using Dirichlet query smoothing with  X  = 2000.

We also compute a Relevance Model expansion baseline [13] by first selecting, using the top 50 ranked documents, the top 1000 terms based on their Ponte [17] log-odds score for use as the vocabulary space V for  X  T .Thetop20expan-sion terms based on their Relevance Model probability were then selected as expansion terms. Note that the TZ study performed expansion by computing an EM solution over a large vocabulary and then truncating at the top 100 expan-sion terms. We first give a basic comparison between the basic CCCP iterative algorithm of Eq. 15 and the TZ algorithm. Fig-ure 5 compares the gains that the CCCP and TZ algorithms achieve on the TREC 1&amp;2 and Robust 2004 topics and collections, for both Mean Average Precision (MAP) and Precision-at-20 (P20). Each curve captures the gain or loss, compared to the initial (unexpanded) query, of a particular topic model  X  T model computed at each iteration and used as the query model for retrieval. In general, the range of the performance statistics is in accord with previous studies on the same topics and collections (e.g., in [5]).

Both the TZ and CCCP algorithms achieved their peak performance at around 40 to 50 iterations for both MAP and P20. Not coincidentally, this is the point at which the influ-ence of the query prior starts to disappear when  X  = 30000 and  X  =0 . 9. The CCCP algorithm achieved significantly higher peak MAP gain of 41.4% for TREC 1&amp; 2, compared to the TZ peak MAP of 31.4%. Both algorithms outper-formed the Relevance Model baseline gain of 29.1%. The relative performance of the algorithms was the same for the Robust 2004 collection, with the CCCP peak MAP gain of 13.3% being slightly better than the TZ peak MAP gain of 12.9%. For comparison, the Relevance Model MAP gain was only 1.6% on this collection.

Beyond 50 iterations, the two algorithms behaved very differently. As the TZ algorithm ran toward convergence, it suffered from serious overfitting, resulting in rapidly dete-riorating retrieval performance with each step. The CCCP algorithm, on the other hand, never experienced such a de-cline from the peak performance value and converged to a reasonable stable point within another 20 to 30 iterations. In practice, the TZ algorithm requires the use of an early stopping heuristic to avoid this overfitting problem, while the stability of the CCCP algorithm makes early stopping much less critical. Overall, the CCCP performance curves consistently dominated those of the TZ algorithm.
Table 1 summarizes performance using standard retrieval measures for the algorithms over both collections. The key parameters to be initialized are  X  0 , the starting weight of  X  i for all documents, and  X  0 and  X  for the query prior. The choice of  X  0 can be seen as representing our initial belief in the likely quality of the feedback set for a typical query. Our experience with setting  X  and  X  matches that found by Tao &amp; Zhai: as long as  X  0 is  X  X arge X  and  X  is close to 1.0, the feedback performance is not much affected by varying those parameters.

The choice of  X  0 had some effect on the overall perfor-mance of the CCCP algorithm, although peak improvements to MAP were strong across a fairly wide range of initial val-ues. In particular, we found that a single operational setting of  X  0 =0 . 15 worked well and gave close to optimal perfor-mance for both collections. Figure 6 shows the effect on Robust 2004 topics of varying  X  0 for the CCCP algorithm, as a function of the number of iterations. For the curve with  X  0 =0 . 05, the peak MAP value is 8.5%; this increases to 13.3% for  X  0 =0 . 10, remains stable at 13.6% for  X  0 =0 . 15, and then declines to 9.9% for  X  =0 . 20. In all cases, how-ever, peak MAP occurs after approximately 50 iterations. In the long term, when the CCCP algorithm is allowed to run to convergence (roughly 100 iterations or more), the differ-ences are more dramatic, ranging from a MAP gain of 6.2% at  X  0 =0 . 05%, to a small MAP loss -1.8% for  X  0 =0 . 20. We also observed that the TZ algorithm was somewhat sen-sitive to the choice of  X  0 , within a much smaller operational range, i.e. between 1 e  X  5 and 1 e  X  7 , with values closer to zero typically giving slightly better performance. generative model, both TZ and CCCP use the same initial values of  X  query performance. (All numbers multiplied by 100.) Figure 7: Sample query models for Tao-Zhai expansion (left) and CCCP method (right) for the TREC topic 447,  X  X tirling engine X . The top 10 expansion terms are shown for the op-timal model found by each method. We looked at the expansion terms and convergence of a par-ticular query: TREC topic 447  X  X tirling engine X . We chose this topic because it was featured in the individual analysis of the original Tao-Zhai paper [20]. Note that our com-puted TZ term weights may be slightly different from those reported by [20]. We were careful to use the same EM pa-rameters such as  X  and  X  . However, for efficiency the CCCP method works with a vocabulary of 100 candidate expansion terms. For this query, we experimented with limiting the TZ algorithm to the same 100-word candidate vocabulary, and performance dropped slightly to an AP of 0.3424, versus AP of 0.3549 for a full 50,000 word vocabulary.

After expansion, the TZ method obtained a maximum performance of AP 0.7989 after 28 iterations, with P5 of 1.0 and P20 of 0.60. Our CCCP unified method obtained an optimal AP of 0.9786 after 14 iterations, with P5 of 1.0 and P20 of 0.80. Both methods assign roughly equal total mass to the original query terms (49.6% for TZ vs 45.3% for CCCP), but assign it very differently: TZ gave most mass to the rarer term  X  X tirling X  and had rapidly diminishing, almost sparse expansion weights, while the CCCP framework main-tained a more even distribution over terms. We attribute this partly to the introduction of the maximum entropy term in the objective. For TREC query 312 ( X  X ydroponics X ) also given in the Tao-Zhai paper, performance of TZ was slightly better, with an AP of 0.206 vs 0.1909 for CCCP. We also compared the TZ, CT, and hybrid algorithms in the space of achievable risk-reward tradeoffs [2] over a range of possible parameters, by using a massive parameter sweep on a large-scale computing cluster. For this experiment we used a proprietary internal Web corpus of 1.2 million documents and 400 queries with in-house relevance judgments. We used the Indri engine [19] in the Lemur toolkit [14] to retrieve the initial top-ranked documents for each query. Then for each query expansion algorithm, we sampled 5000 different com-binations of parameters in that algorithm X  X  high-dimensional  X  X ox X  of potential settings. Figure 8 shows the results as a risk-reward plot [3]. Here, the x -axis (risk) represents the percentage MAP loss averaged over queries that were hurt by expansion; the reward y -axis gives the percentage MAP gain/loss over all queries. Each of the 5000 points in the figure represents an experiment over 400 queries for a par-ticular operational setting. Not surprisingly, since the TZ method has few parameters and does not model term co-variance, it is able to achieve a restricted subset of potential risk-reward tradeoffs. Similarly, for this corpus the Jaccard-based CT expansion method has limited losses, but no real gains. However, the unified method with heat-kernel trans-lation exhibits a broad regime of potential tradeoffs, includ-ing a range of gains superior to either baseline algorithm. We provide a brief analysis of the example latent variable optimization program described in Sec. 2.5. We evaluated the effect of adding a translation kernel to the objective, and the effect of the linear diversity constraints.

For this summary evaluation, we present results for wt10g Figure 8: Massive parameter space exploration of the risk-reward tradeoffs achievable in the parameter space of three different query expansion methods: TZ (dense green, left), TZ+CT hybrid with Jaccard translation model (dense red, right) and TZ+CT hybrid with heat-kernel translation model (large scattered blue). (TREC topics 451-550), using the same setup described in Section 4. We limited the maximum number of iterations for both TZ and Constrained EM to 50, since both algo-rithms have generally converged by then. We used the 50 top-ranked documents and 20 expansion terms.

Again, we use risk-reward curves to show an algorithm X  X  achievable tradeoff between average precision gain and the loss due to expansion failures. One important difference is that our risk-reward curves are generated as a function of the number of iterations of each algorithm instead of functions of a feedback interpolation parameter  X  .

We set the dilation factor  X  2 and time parameter t of the translation kernel to 0 . 75 and 5 respectively. In practice, because the initial z dw are very close to zero, for numerical reasons we first run a small number L of iterations (in these experiments L = 3) of the standard EM algorithm before switching to the Constrained EM version. Because the di-versity constraint is a hard constraint, the convex program is occasionally infeasible at some iteration 4 . When this hap-pens, we simply terminate the search and use the last known good solution.

Figure 9 shows two effects. In a), increasing the amount of translation from  X  =0to  X  = 5 gives a dramatic im-provement in the risk-reward tradeoff: with the translation kernel on this collection, the algorithm never hurts query performance on average, giving its maximum MAP at con-vergence. The baseline TZ algorithm, on the other hand, deteriorates quickly after about 20 iterations. In b), making the diversity constraint more strict by decreasing the allow-able  X  percentage available to the top-ranked words acts to flatten the risk-reward curve slightly. Future analysis work includes sensitivity of the results to parameter changes, and
Problems such as infeasibility and failure to converge are detected automatically by the solver and reported with a status code. Figure 9: Risk-reward tradeoff curves show the effect of in-creasing the word translation factor  X  with fixed diversity constraint  X  W =0 . 90, compared to Tao-Zhai baseline (TZ). Curves are a function of iteration, with each iteration as a dot and every 10th iteration numbered. Because the ini-tial query is the starting point and the y -axis shows relative MAP gain, curves will start at the origin and trace out a risk-reward tradeoff with each iteration. Tradeoff curves that are higher and to the left are better. measuring interaction effects between diversity and transla-tion or other components. The expansions found by the TZ and constrained EM are quite different, so improved per-formance is not due to simply slower convergence toward a similar solution. The superior performance of the CCCP algorithm compared to its EM counterpart is interesting considering that both methods are based on the same generative model and are designed to compute a Maximum A Posteriori likelihood so-lution. Unlike most learning scenarios, however, it appears that for the problem of query expansion, the nature of the path on the way to the likelihood objective is much more important than the goal itself. The slow, coordinate-wise ascent approach of EM, and more controlled search path of CCCP approaches, turn into an advantage in such cases. Our CCCP method generalizes the EM approach so that we have finer control over the nature of the steps. It also uses a tighter lower bound than the closed-form EM version that includes an extra entropy term to be maximized. While a bit more involved to optimize, this extra regularization penalty over the distribution of Z appears to help stabilize the so-lution. We did not do extensive parameter tuning in our model, so further gains may be possible.

While most computational effort for modeling query in-tent is appropriately spent training large-scale models of-fline, Web search engines must operate an increasingly com-plex decision environment in which some evidence, such as user interaction feedback, is only visible at query time. Thus, we forsee that an on-line learning component that solves query-specific optimization problems in real time will be a powerful complement to off-line training. Applications in-clude time-or context-sensitive noise reduction, real-time feature selection on predictor inputs, and  X  X ourse correction X  via posterior constraints on predictor outputs. Because of their generality and simplicity we believe that CCCP-type frameworks are a good starting point for future exploration of useful constraints and objectives in such problems. In gen-eral, we believe that the exploration of effective optimization frameworks opens up a new area of research in information retrieval in which tradeoffs and principled decision-making under uncertainty can be greatly improved. We have made both algorithmic and empirical contributions to the problem of searching for an optimal query model that is both effective and reliable. We used the Convex-Concave procedure as a way to reveal more of the implicit structure, objectives and constraints of an existing feedback algorithm, to unify two complementary approaches, and to generate new algorithms in a principled way. On the empirical side, we performed an iterative performance analysis as well as a risk-reward parameter-space analysis on a high-performance computing cluster to gain new insights into the space of computational tradeoffs achievable with different types of algorithms. A general trend in software systems is that sim-pler or more powerful algorithms are eventually preferred over methods designed for efficiency in special cases. We believe the advantages of effective optimization frameworks for use in information retrieval systems will soon outweigh their moderate computational costs.
 We thank Guy Lebanon for his valuable feedback, Tao Tao for helpful discussions of the TZ model, and several anony-mous reviewers for their comments.
