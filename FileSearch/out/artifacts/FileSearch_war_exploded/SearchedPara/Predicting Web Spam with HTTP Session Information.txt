 Web spam is a widely-recognized threat to the quality and security of the Web. Web spam pages pollute search en-gine indexes, burden Web crawlers and Web mining ser-vices, and expose users to dangerous Web-borne malware. To defend against Web spam, most previous research ana-lyzes the contents of Web pages and the link structure of the Web graph. Unfortunately, these heavyweight approaches require full downloads of both legitimate and spam pages to be effective, making real-time deployment of these tech-niques infeasible for Web browsers, high-performance Web crawlers, and real-time Web applications. In this paper, we present a lightweight, predictive approach to Web spam classification that relies exclusively on HTTP session infor-mation (i.e., hosting IP addresses and HTTP session head-ers). Concretely, we built an HTTP session classifier based on our predictive technique, and by incorporating this clas-sifier into HTTP retrieval operations, we are able to de-tect Web spam pages before the actual content transfer. As a result, our approach protects Web users from Web-propagated malware, and it generates significant bandwidth and storage savings. By applying our predictive technique to a corpus of almost 350,000 Web spam instances and almost 400,000 legitimate instances, we were able to successfully detect 88.2% of the Web spam pages with a false positive rate of only 0.4%. These classification results are superior to previous evaluation results obtained with traditional link-based and content-based techniques. Additionally, our ex-periments show that our approach saves an average of 15.4 KB of bandwidth and storage resources for every success-fully identified Web spam page, while only adding an aver-age of 101  X  s to each HTTP retrieval operation. Therefore, our predictive technique can be successfully deployed in ap-plications that demand real-time spam detection.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.5.2 [ Pattern Recognition ]: Design Methodology X  classifier design and evaluation ; K.6.5 [ Management of Computing and In-formation Systems ]: Security and Protection Algorithms, Experimentation, Measurement, Security Web spam, HTTP session information, classification
The use of malicious Web pages to influence human users and attack browser client machines has grown significantly despite continued research and industry efforts. Examples of malicious Web pages (commonly called Web spam) include pages that subvert search engine ranking algorithms (ac-counting for between 13.8% and 22.1% of all Web pages [4, 22]) as well as pages designed to propagate malware by host-ing Web-borne spyware and browser exploits [20, 21, 23, 25]. Thus, for both performance and security reasons, the identification of Web spam pages has become increasingly important.

Most previous and current research efforts on Web spam defenses have focused on protecting human users by identi-fying Web pages that skew search engine rankings through link or content manipulations. Representative examples of this research include link-based [2, 3, 5, 6, 7, 16, 29] and content-based [11, 22, 27] analysis techniques. Although these analysis techniques effectively identify certain types of Web spam, the techniques need to download and inspect the content associated with suspect Web pages for the analysis algorithms to work. This dependence on Web spam con-tent makes these previous approaches practically defense-less against Web-propagated malware, and it severely limits their impact on the resource burdens imposed by Web spam.
Notwithstanding the common assumption that Web spam content is necessary for identifying Web spam, the main hypothesis of this paper is that HTTP session information (i.e., hosting IP addresses and HTTP session headers) pro-vides sufficient evidence for the successful identification of many Web spam pages. This hypothesis is supported by our previous work [27], which shows that very distinct pat-terns emerge within the HTTP session information associ-ated with Web spam pages. In this paper, we leverage that observation by using HTTP session information to train clas-sification algorithms to distinguish between spam and legit-imate Web pages.
The first contribution of this paper is a new approach for retrieving Web pages using HTTP. Specifically, we insert an HTTP session classifier into the HTTP retrieval process, which predicts whether a Web page is spam based on the page X  X  HTTP session information (completely ignoring its content). This predictive approach protects Web users from Web-propagated malware, and it generates significant band-width and storage savings because it avoids downloading and storing useless Web spam pages. Our approach is particu-larly useful for search engines because it enables more effi-cient and effective Web crawlers, which will generate indexes with higher quality content.

The second contribution of this paper is a large-scale ex-perimental evaluation of Web spam classification using HTTP session information. As part of this evaluation, we investi-gate various classification algorithms and discover that many classifiers are capable of successfully detecting almost 90% of the Web spam in our corpora. Then, we evaluate the ef-fects of varying class distributions and observe that our best classifiers are very robust under a number of environmental circumstances. Finally, we investigate the computational costs and resource savings associated with our approach. Based on our experiments, we observe that our most effec-tive classifier only adds an average of 101  X  stoeachWeb page retrieval, while saving an average of 15.4 KB of band-width and storage resources for every successfully identified Web spam page.

The remainder of the paper is organized as follows. Sec-tion 2 reviews previous research on Web spam identifica-tion and classification. Section 3 provides background in-formation about HTTP operations and describes our new approach for retrieving Web pages. Section 4 describes the experimental setup we used to evaluate our new approach, and Sections 5 and 6 present our experimental results us-ing various corpora of spam and legitimate Web pages. We conclude the paper in Section 7.
Previous Web spam research can be categorized broadly into three groups: link-based, content-based, and hybrid analysis techniques. Link-based analysis techniques attempt to identify Web spam pages based on their hyperlink connec-tions to other pages. Davison [8] was the first to investigate link-based Web spam, building decision trees to successfully identify  X  X epotistic links. X  Becchetti et al. [2] revisited the use of decision trees on a newer collection of Web data and were able to successfully identify 80.4% of the 840 Web spam examples in their sample with a false positive rate of 1.1%. To help mitigate the effects of link manipulations on link-based ranking algorithms, Gy  X  ongyi et al. [16] developed an algorithm called TrustRank that can be used to propagate trust from a seed set of Web pages to the successors of those pages. Wu and Davison [29] and Benczur et al. [3] also proposed solutions for improving the resiliency of ranking algorithms. However, instead of propagating trust to good pages, they propagated distrust to bad pages. Finally, in our previous research, we proposed a spam resilient rank-ing approach called Spam-Resilient SourceRank [7]. This approach collects Web pages into logical groupings called sources, and it provides parameters that can be tuned to throttle the ranking influence of various sources. Addition-ally, we presented a credibility-based Web ranking algorithm called CredibleRank [6]. This algorithm assesses the link credibility of Web pages, and then, it incorporates that cred-ibility information into the rankings of each page on the Web.

Content-based analysis techniques focus on identifying fun-damental differences between the content of spam and legit-imate Web pages. Fetterly et al. [11] statistically analyzed two data sets of Web pages (DS1 and DS2) using properties such as page content, content duplication, and page evolu-tion. They found that many of the outliers in the statis-tical distributions of these properties were Web spam, and they manually identified 98 out of 1,286 Web pages as spam. Ntoulas et al. [22] extended the work by using additional content-based features (e.g., fraction of visible content, com-pressibility, independent n -gram likelihoods, etc.) to build decision trees, which were able to successfully classify 86.2% of their collection of 2,364 spam pages with a false posi-tive rate of 1.3%. Anderson et al. [1] developed a technique called spamscatter, which extracts URLs from spam email messages, obtains the corresponding Web content, and clus-ters that Web content based on an image shingling tech-nique. Using their approach on a one-week collection of email, they were able to identify 2,334 unique  X  X cams X  being hosted on 7,029 unique IP addresses. Finally, in our previ-ous work [27], we performed a large-scale characterization of 348,878 Web spam pages, which focused on various content-based components of Web spam (e.g., content duplication, content categorization, redirection, etc.).

Hybrid analysis techniques combine content-based and link-based techniques to identify Web spam. Gy  X  ongyi and Garcia-Molina [15] proposed a taxonomy that categorizes various techniques used by Web spammers to manipulate search en-gine results. This taxonomy showcases a variety of content-based and link-based spamming techniques. Drost and Schef-fer [9] trained a support vector machines (SVM) classifier using content-based features (e.g., number of page tokens, number of URL characters, etc.) as well as link-based fea-tures (e.g., contextual similarities between a page, its prede-cessor, and its successors). Then, using a private collection of 1,285 Web pages (431 spam and 854 legitimate), their classifier consistently obtained area under the ROC curve (AUC) values greater than 90%. Finally, Castillo et al. [5] created a cost-sensitive decision tree using the content-based features described in [22] and link-based features described in [2]. Their decision tree was able to successfully identify 88.4% of their 675 Web spam examples with a false positive rate of 6.3%.
In this section, we review the underlying protocols respon-sible for Web browsing and Web crawling. First, we discuss the HTTP operations available for retrieving a Web page from a Web server. Then, we propose a new approach for retrieving Web pages that leverages HTTP session classifi-cation to avoid downloading Web spam content.
The HyperText Transfer Protocol (HTTP) [12] is a re-quest/response protocol that allows clients (or user agents) to exchange information with servers (or origin servers) lo-cated around the world. In the Web context, user agents are typically Web browsers and Web crawlers, and origin servers are typically Web servers that store various forms of Web data.
The information exchange between user agents and origin servers is accomplished by transmitting MIME-like messages (as specified in RFC 2616 [12]) back and forth between the user agents and origin servers. First, the user agent initi-ates a request by sending a request message to the origin server. A request message contains a request line, zero or more session headers, an empty line that denotes the end of the headers, and an optional message body. The request line includes a request method, a URL on which to apply the request method, and the user agent X  X  supported HTTP version (HTTP/1.1 is the most widely supported version). An annotated example of a simple  X  X ET X  request message isshowninFigure1.

After the origin server receives a request message, the server performs the requested method on the provided URL and returns the result in a response message .Figure2shows the annotated response message that corresponds to the  X  X ET X  request message shown in Figure 1. A response mes-sage contains a status line, zero or more headers, an empty line that denotes the end of the headers, and an optional message body. The status line includes the origin server X  X  supported HTTP version, a three digit status code (e.g.,  X 200 X ) that indicates whether or not the request was suc-cessful, and a textual description of the status code (e.g.,  X  X K X ).
To improve the Web browsing experience for users and to enhance the quality of information obtained by Web crawlers, we propose a new approach for retrieving Web pages with HTTP. Our proposed approach does not affect the underly-ing HTTP operations; however, it does change the manner in which user agents respond to the results of those opera-tions. Specifically, we insert an HTTP session classifier into the retrieval process to dramatically reduce the amount of Web spam that is retrieved by user agents. First, the user agent initiates a  X  X ET X  request using the approach described above in Section 3.1. Upon receiving the request message, the origin server performs the requested method and returns the result in a response message.

In the traditional approach, the user agent blindly accepts the response (and its corresponding Web page), continuing to its next task (i.e., crawling the contents of the next URL, requesting images or other embedded objects that are found in the received Web page, etc.). However, in our proposed approach, the user agent only reads the response line and HTTP session headers from the response message (i.e., it reads up to the empty line). Then, the user agent employs a classifier to evaluate the headers and classify them as spam or legitimate. If the headers are classified as spam, the user agent closes its connection with the origin server, ignoring the remainder of the response message and saving valuable bandwidth and storage resources. Alternatively, if the head-ers are classified as legitimate, the user agent finishes reading the response message and continues its normal operation. Our new approach offers at least three benefits. First, the approach is broadly applicable to any URL on the Web, re-gardless of where that URL is obtained (e.g., in an email, in a search engine result, in a social networking community, etc.). Since we integrate our predictive technique into the HTTP retrieval process, we are able to protect all of the page requests made by a user agent. Second, by making the classification decision at the HTTP level, we avoid down-loading the content of a page unless we predict the page is legitimate. Thus, our approach enables more efficient and effective Web crawlers by providing significant bandwidth and storage savings. Additionally, by restricting content downloads, we can protect Web users against exposure to Web-borne spyware and browser exploits, which are becom-ing increasingly prevalent on Web spam pages [20, 23, 25]. Third, our approach is complementary to existing link-based and content-based analysis techniques. Hence, the approach can be combined with existing techniques to create a multi-layered defense against Web spam.
The success of our predictive approach is contingent upon the performance of HTTP session classification. Therefore, we performed extensive evaluations to determine the effec-tiveness of classifying Web spam using HTTP session infor-mation. In this section, we discuss the fundamental princi-ples used to perform our experimental evaluations. Then, in Sections 5 and 6, we provide experimental evidence that HTTP session classification is an effective and efficient solu-tion for automatically predicting Web spam pages.
Previous research [2, 5, 8, 9, 22] has shown that Web spam detection can be modeled as a binary classification problem, where the two classes are spam and legitimate (or non-spam). In binary classification problems, a model (or classifier) is built and evaluated in two separate phases: the training phase and the classification phase (or testing phase). During the training phase, a set of labeled instances (i.e., the training data) is used to train a classifier, establishing a mapping between instance features and the classes. Dur-ing the classification phase, the trained classifier is used to classify a separate set of labeled instances (i.e., the testing data), and the resulting classifications are presented in the form of a confusion matrix (or contingency table):
In the above confusion matrix, a represents the number of correctly classified legitimate Web pages, b represents the number of legitimate Web pages that were misclassified as spam, c represents the number of Web spam pages that were misclassified as legitimate, and d represents the number of correctly classified Web spam pages. Once a confusion ma-trix is generated, we compute various performance metrics to evaluate the effectiveness of a classifier: true positive (TP) rate, false positive (FP) rate, F-measure, and Accuracy. The TP rate is the same as recall R ,whichis d c + d ,andtheFP rate is b a + b . F-measure is defined as 2 PR P + R ,where P is and Accuracy is defined as a + d a + b + c + d . All of these metrics are well known and widely cited throughout previous machine learning and information retrieval research.

To improve the reliability of our classifier evaluations, each evaluation was performed using stratified tenfold cross-validation [18]. In stratified tenfold cross-validation, a fixed sample of data is randomly divided into ten equally-sized folds (or partitions), and each fold is comprised of approx-imately the same class distribution as the original sample. After the data is split, each fold is evaluated with a classifier that was trained with the other nine folds, and a confusion matrix is generated by averaging the results of these ten evaluations. Finally, the performance metrics are calculated using this confusion matrix.
In our previous research [26], we developed an automatic technique for obtaining Web spam examples that leverages the presence of URLs in email spam messages. Specifically, we extracted almost 1.2 million unique URLs from more than 1.4 million email spam messages. Then, we built a crawler to obtain the Web pages that corresponded to those URLs. Our crawler obtained two types of information for every successfully accessed URL: the HTML content of the page identified by the URL and the HTTP session informa-tion associated with the page request transaction. After our crawling process was complete, we had 348,878 Web spam pages, which are referred to as the Webb Spam Corpus 1 . This corpus is currently the largest publicly available collec-tion of Web spam, and it served as our primary source of spam instances. For a more detailed description of our col-lection methodology and the format of the files in the Webb Spam Corpus, please consult [26].

In addition to the Webb Spam Corpus, we also used the labeled spam instances in the WEBSPAM-UK2006 corpus 2 as a source of Web spam pages. The WEBSPAM-UK2006 corpus [4] is a publicly available Web spam collection that is based on a May 2006 crawl of the .uk domain. This corpus is much smaller and far less diverse than the Webb Spam Cor-pus, but since the WEBSPAM-UK2006 corpus has appeared in recent Web spam classification research [2, 5], we used it for comparative purposes. Unfortunately, this corpus does not contain HTTP session information, and as a result, we were forced to recrawl its spam URLs and obtain their cor-responding HTTP session information. We performed this crawl in July 2007 (i.e., a little over a year after the original corpus was created), and we were only able to obtain 1,338 out of the 1,924 spam pages (70%) in the corpus because the missing pages were no longer available.

Since the Webb Spam Corpus was collected at the end of December 2005, we needed legitimate Web data from a similar time period to establish a fair comparison between spam and legitimate HTTP session information. To ob-tain this temporally similar data, we used the December 2005 crawl from Stanford X  X  pub licly available WebBase Web Page Repository [17]. This crawl is comprised of 20.7 mil-lion pages, which are stored along with the HTTP session information that was returned when the pages were crawled. We extracted a stratified random sample of 392,664 legiti-mate pages from this crawl, maintaining the same relative distribution of hosts in our sample.

In addition to our WebBase data, we also used the labeled legitimate instances in the WEBSPAM-UK2006 corpus as a source of legitimate Web data. As mentioned above, this corpus does not contain HTTP session information; thus, we recrawled the legitimate URLs from the corpus to obtain that data. We performed this crawl in July 2007, and we were only able to obtain 3,542 out of the 5,549 legitimate pages (64%) in the corpus because the missing pages were no longer available.

Table 1 presents a summary of our corpora. For each corpus, the table lists its abbreviated name, its number of instances, and the date it was created.
Classification algorithms require their inputs (or data in-stances) to be represented in a consistent format. In our ex-periments, we adopted the traditional vector space model [24] (or  X  X ag of words X  model), which has been quite effective in previous information retrieval and machine learning re-search. In the vector space model, each data instance is rep-resented as a feature vector f of n features: &lt;f 1 ,f 2 All of our features are Boolean; hence, if f i = 1, the feature is present in a given instance; otherwise, the feature is ab-sent.

Unlike previous Web spam classification research, which has relied on link-based and content-based analysis tech-niques, our work focuses exclusively on HTTP session infor-WEBSPAM-UK2006 corpus.
 mation. One of the most important pieces of HTTP session information is the IP address of the Web server that hosts a given Web page  X  the hosting IP address .Figure3(a)com-pares the distributions of the hosting IP addresses for the Webb Spam Corpus and WebBase data. The figure clearly shows that each distribution is quite distinct. The hosting IP addresses in the Webb Spam Corpus are concentrated around a few IP address ranges. Specifically, the 63.*  X  69.* and 204.*  X  216.* IP address ranges account for 45.4% and 38.6% of the addresses associated with that corpus, respec-tively (84%, collectively). On the other hand, only 50.6% of the WebBase data X  X  hosting IP addresses are in those ranges (21.8% for 63.*  X  69.* and 28.8% for 204.*  X  216.*). Simi-larly, 25.6% of the hosting IP addresses associated with the WebBase data are in the 128.*  X  171.* range, whereas only 2.3% of the addresses in the Webb Spam Corpus fall within that range. Figure 3(b) compares the distributions of the hosting IP addresses for the spam and legitimate instances in the WEBSPAM-UK2006 corpus, showing distinct trends that are similar to those described for the Webb Spam Cor-pus and WebBase data. Since the distributions of spam and legitimate hosting IP addresses are quite distinct, we used those addresses as features in our classification experiments to help distinguish between spam and legitimate Web pages.
In addition to hosting IP addresses, we also derived a num-ber of features from HTTP session header values. Table 2 shows a list of the most popular HTTP session headers in our corpora. For each corpus, the table lists the number of pages associated with each header and the number of unique val-ues each header has in the corpus. The table also presents a sample value for each of the headers. Two important obser-vations emerge from this table. First, the relative popularity of the various HTTP session headers varies greatly among spam and legitimate Web pages. For example, 60% of the spam instances in the Webb Spam Corpus possess an  X  X -Powered-By X  header, whereas only 29.1% of the legitimate instances in the WebBase data contain that header. This disparity is even larger for the  X  X ink X  header, which is found in 40.9% of the spam instances and only 0.02% of the legiti-mate instances. The table also shows a similar phenomenon for the spam and legitimate instances in the WEBSPAM-UK2006 corpus.

The other important observation from Table 2 is that dis-tinct distributions exist for the HTTP session header values of spam and legitimate Web pages. For example, of the 59,023 spam instances in the Webb Spam Corpus that pos-sess a  X  X 3P X  header value, only 34.5% of them have a value that is also possessed by at least one legitimate instance in the WebBase data (i.e., 65.5% of those spam instances are uniquely identified by their  X  X 3P X  header value). Similarly, 17.8% of all spam instances in the Webb Spam Corpus are uniquely identified by their  X  X ontent-Type X  header value, and 24.8% of the spam instances with a  X  X erver X  header value are uniquely identified by those values. Similar dis-tinctions also exist for the HTTP session header values of spam and legitimate Web pages in the WEBSPAM-UK2006 corpus.

To derive classification features from HTTP session head-ers, we created three representations (phrases, n -grams, and tokens) for each unique header value. First, we stored the header value as an uninterrupted phrase. Then, we tok-enized the header value using whitespace and punctuation characters as delimiters. Next, we stored the resulting to-kens, along with the unique 2-grams and 3-grams. Finally, we prepended the header name to each of our generated feature values to disambiguate the values for distinct head-ers. For example, we prepended all  X  X erver X  values with  X  X erver  X  to avoid potential collisions with the values of other headers. To illustrate our feature generation process, Ta-ble 3 shows the features that were derived from the sample  X  X erver X  header shown in Table 2 ( X  X pache/2.0.52 (fedora) X ).
Text data is often characterized as having an extremely high dimensionality due to the vast number of features that can occur in the feature vector. Our corpora exhibit a partic-ularly high dimensionality because they contain thousands of unique headers with millions of unique values. Addition-ally, we expand the feature space even further with host-ing IP addresses and the three feature representations (i.e., phrases, n -grams, and tokens) that we apply to each HTTP header value.

Many of these features are critical for establishing accu-rate class boundaries and creating effective classifiers. How-ever, not all of these features ar e relevant to the distinction between spam and legitimate instances. Irrelevant features actually introduce noise into the classification process, wast-ing valuable computational and storage resources during the training phase and all subsequent uses of the trained classi-fier. To alleviate the problems associated with high dimen-sionality, we select a smaller number of the  X  X est X  features from our vast feature space  X  a process known as dimension-ality reduction (or feature selection). In our experiments, we use a well-known information theoretic measure called In-formation Gain [13, 30] to accomplish this feature selection process. Information Gain is defined as follows: where f i is a feature in the feature vector, p ( f ) is the proba-bility that f occurs in the training set, c j is one of the classes (i.e., spam or legitimate), p ( c ) is the probability that c oc-curs in the training set, and p ( f,c ) is the joint probability that f and c occur in the training set.

Intuitively, Information Gain quantifies how much the knowl-edge of feature f i helps to correctly identify class c i feature f 0 has a higher Information Gain value than feature f ,wesaythat f 0 has more predictive power than f 1 .For our experiments, we calculate the Information Gain for each feature in the feature space of a given collection of corpora. Then, a user-specified number n of tokens with the highest Information Gain scores are selected (or retained) and used to train the classifiers.
Unlike previous Web spam classification research [2, 5, 8, 22], which has relied almost exclusively on decision trees (e.g., C4.5), we performed our HTTP session classification experiments with a variety of classification algorithms. Specif-ically, we performed an extensive evaluation with 40 classi-fiers that are implemented in the Weka toolkit [28]. These classifiers include decision trees (e.g., C4.5, random forest, etc.), rule generators (e.g., RIPPER, PART, etc.), boosting algorithms (e.g., AdaBoost, LogitBoost, etc.), logistic re-gression, radial basis function (RBF) networks, HyperPipes, multilayer perceptrons, support vector machines (SVM), and na  X   X ve bayes. The algorithmic details of these classifiers are beyond the scope of this paper. Additional information about the classifiers is available in most standard machine learning texts (e.g., [19]).
In this section, we present our experimental HTTP session classification results using spam instances from the Webb Spam Corpus and legitimate instances from our WebBase sample. The Webb Spam Corpus is currently the largest publicly available collection of Web spam; consequently, the results presented in this section are a truly representative measure of the effectiveness of our approach.
Our first experiments explored the impact of feature set size and corpus sample size on the effectiveness of our clas-sifiers. As part of these experiments, we varied the feature set size between 100 and 10,000 (incrementing the variations on a log scale), and we varied the corpus sample size across the same range with the same variations. As a result, we evaluated close to 400 unique feature set size and corpus sample size combinations. After performing this evaluation, we found that the majority of our classifiers consistently exhibited their best performance with 5,000 retained fea-tures (the 10 most effective features for these corpora are summarized in Table 4) and a corpus sample consisting of 10,000 data instances. Therefore, those settings were used Table 4: Top 10 features for Webb Spam and Web-Base.
 Table 5: Classifier performance results for Webb Spam and WebBase.
 for the remainder of our experiments involving the Webb Spam Corpus and WebBase data.

Once we identified an appropriate feature set size and cor-pus sample size, we used those settings to evaluate the per-formance of the 40 classifiers we described above in Sec-tion 4.5. This evaluation was performed using a random corpus sample with an equal distribution of spam and le-gitimate instances (i.e., 5,000 instances of each class). We chose to use an equal class distribution to minimize the class-specific contributions and focus on each classifier X  X  ability to correctly classify instances from each class. We revisit this decision in Section 5.2 by investigating the impact of a corpus sample X  X  class distribution on the performance of a classifier.

The performance metrics for the 5 most effective classi-fiers from our evaluation are presented in Table 5. The table clearly shows that all of our classifiers were quite successful; however, HyperPipes was consistently the best performer. SVM and C4.5 both exhibit a slightly higher TP rate (i.e., spam detection rate) than HyperPipes, but HyperPipes of-fers a substantially lower FP rate than all of the other clas-sifiers. The HyperPipes classifier operates as follows. Dur-ing the training phase, an n -dimensional parallel-pipe (or HyperPipe) is constructed for each class. Each HyperPipe contains all of the feature values associated with its class, which generates feature boundaries for that class. During the testing phase, each test instance is classified according to the class that most contains that instance (i.e., the class with feature boundaries that overlap the most with the test instance).

To further investigate the performance of HyperPipes (and the other classifiers), we generated a Receiver Operating Characteristics (ROC) graph. ROC graphs display the TP rate on the Y axis and the FP rate on the X axis, depict-Figure 4: ROC curves for the top 5 classifiers using Webb Spam and WebBase. ing the relative tradeoffs between benefits (true positives) and costs (false positives). In the machine learning commu-nity, these graphs are used as another method for comparing the performance of classifiers [10]. Figure 4 shows the ROC graph for our classifiers. As the graph shows, the Hyper-Pipes classifier offers one of the best tradeoffs between TP and FP performance. Since us ers are more concerned with reducing false positives (i.e., legitimate Web pages that are misclassified as spam) than false negatives (i.e., Web spam pages that are misclassified as legitimate), the HyperPipes classifier offers the best overall performance, and we will rely on it for future evaluations.

To help put our results in their proper context, Table 5 also shows the best results from a previously studied content-based Web spam classifier [22]. Since the results from this previous study were obtained using a private data set, we were unable to generate directly comparable results. How-ever, an indirect comparison shows that all of our top 5 classifiers were able to detect a higher percentage of Web spam pages than the content-based classifier (i.e., our clas-sifiers exhibited higher TP rates), and two of our classifiers (HyperPipes and RBFNetwork) generated lower FP rates. Based on these results, we believe that our approach for predicting Web spam with HTTP session information per-forms as well as (if not better than) content-based Web spam classification efforts.
BasedontheempiricalgrowthpatternsexhibitedbyWeb spam [14], we expect the percentage of Web spam on the Web to continue growing for the foreseeable future. As a re-sult, any proposed solution for Web spam must be resilient against a constantly evolving distribution of spam and le-gitimate Web pages. With this in mind, we performed an experiment to evaluate the effect of a corpus sample X  X  class distribution on the performance of a HyperPipes classifier. First, we created 9 corpus sa mples, each with a different class distribution. The percentage of spam instances in each sample was varied from 10% to 90%, in increments of 10%. Then, for each sample, we selected the 5,000 most infor-mative features and evaluated a HyperPipes classifier using stratified tenfold cross-validation. The results of this eval-uation are summarized in Table 6. As the table shows, the Table 6: Class distribution results for Webb Spam and WebBase.
 classifier X  X  performance declines as the sample X  X  spam per-centage increases. However, the classifier X  X  overall perfor-mance is incredibly robust. The FP rate remains relatively constant until the sample is composed of 60% spam, and the lowest TP rate, F-Measure, and Accuracy values are all within about 10% of their best values.
Up to this point, our evaluations have focused primarily on the effectiveness of HTTP session classification. How-ever, another important consideration for our proposed Web spam solution is the computational cost of HTTP session classification. Ultimately, even the most effective classifier is useless if its timing requirements inhibit Web browsers and Web crawlers from performing their intended tasks.
To evaluate the computational requirements of HTTP ses-sion classification, we performed timing experiments using the 5 classifiers presented in Section 5.1. For each classifier, we computed the training time and per instance classifica-tion time necessary to perform a stratified tenfold cross-validation evaluation. Our timing experiments were con-ducted on a dual processor (Intel Xeon 2.8 GHz) system with 4 GB of memory, and the results for the 3 most effi-cient classifiers are shown in Table 7. As the table shows, C4.5 and HyperPipes are both extremely efficient when clas-sifying data instances, requiring an average of only 9  X  sand 101  X  s, respectively. RBFNetwork is more than two orders of magnitude slower than HyperPipes, requiring an average of 14.32ms to classify each instance. The table also shows that HyperPipes is the most efficient classifier in terms of train-ing time (408.8ms). The training times of C4.5 (5,389.1s) and RBFNetwork (734.99s) are both more than three or-ders of magnitude larger than the corresponding training time for HyperPipes. However, it is important to note that these training times are only incurred when the classifiers are trained (or retrained), which is an infrequent occurrence. Regardless, our HyperPipes classifier provides the most ef-ficient HTTP session classification solution, adding a mere 101  X  s to each HTTP request operation. This overhead will be completely unnoticeable by Web users, and it should also have almost no effect on the operations of Web crawlers.
One of the primary benefits of our predictive approach is that it identifies Web spam without downloading the con-tents of Web spam pages. By eliminating these downloads, we generate significant bandwidth and storage space savings for Web browsers and Web crawlers.
 Table 7: Classifier training and per instance classi-fication times using Webb Spam and WebBase.
 Table 8: Top 10 features for WEBSPAM-UK2006.

To help quantify the resource savings associated with our approach, we calculated size information for our spam in-stances. Specifically, we determined that the average size of a Web spam response message in the Webb Spam Corpus is about 16 KB (15.4 KB for the message body and 0.6 KB for the headers). Hence, if our approach successfully de-tected 100% of the Web spam pages in the corpus, we would save about 15.4 KB in bandwidth and storage costs every time we encountered a spam URL. Based on the actual de-tection rate (88.2%) exhibited by our HyperPipes classifier, we expect to save an average of about 13.6 KB for every encountered spam URL.
In Section 5, we showcased the effectiveness of our pre-dictive approach to Web spam classification on large, di-verse corpora. The Webb Spam Corpus and WebBase data were drawn from a variety of domains (e.g., .com, .net, .info, etc.), and each collection contains more than 300,000 pages. In this section, we focus on the smaller, more focused WEBSPAM-UK2006 corpus. This collection has the advan-tage of being more recent than our other corpora (July 2007 versus December 2005), but it only contains a few thousand pages that were all drawn from the .uk domain. Therefore, the corpus was used primarily to investigate the robustness of our predictive method in such a different setting.
To evaluate the effectiveness of HTTP session classifiers on the WEBSPAM-UK2006 corpus, we used the same method-ology that we described in Section 5.1. First, we explored the impact of the feature set size, and again, we found that the majority of the classifiers generated their best results with 5,000 retained features (the 10 most effective features for the WEBSPAM-UK2006 corpus are summarized in Ta-ble 8). Due to the limited size of this corpus, we did not evaluate the impact of corpus sample size on the classifiers. Table 9: Classifier performance results for WEBSPAM-UK2006.
 Figure 5: ROC curves for the top 5 classifiers using WEBSPAM-UK2006.
 Instead, we chose to use a sample size of 1,486 because that is the largest sample size, given the available data, that would allow us to reproduce the class distribution evaluation de-scribed in Section 5.2. Both of these settings were used for the remainder of our experiments involving the WEBSPAM-UK2006 corpus.

After we determined an appropriate feature set size and corpus sample size, we used a random corpus sample with an equal distribution of spam and legitimate instances (i.e., 743 instances of each class) to evaluate the performance of the 40 classification algorithms described in Section 4.5. Table 9 summarizes the results of the 5 most effective classifiers from this evaluation. As the table illustrates, the TP rates for all of the classifiers (except RBFNetwork) declined compared to their performance in Table 5, and the FP rates for all of the classifiers increased. A similar performance degradation is also evident in the ROC graph shown in Figure 5.
At least two explanations exist for the performance degra-dation of the classifiers on this corpus. First, our corpus sample size (1,486) for this evaluation was almost an order of magnitude smaller than the corpus sample size used for the evaluation in Section 5.1. Unfortunately, this was un-avoidable due to the limited size of the WEBSPAM-UK2006 corpus. Second, previous research [5] noted several times that the content of the spam and legitimate pages in this corpus is more similar than in other corpora. Thus, it is not unreasonable to expect the HTTP session information associated with the spam and legitimate pages in this cor-pus to be equally similar, making it much more difficult to correctly identify the class boundaries.
 Table 9 also shows the best results from a previously stud-Table 10: Class distribution results for WEBSPAM-UK2006.
 ied Web spam classifier that was trained on this corpus using both content-based and link-based features (i.e., a hybrid technique) [5]. It is important to note that these previous results were generated with a highly optimized decision tree algorithm that utilized multiple passes of stacked graphical learning. Our classifiers, on the other hand, were built us-ing default settings and without the benefit of meta-learning optimizations, and even without these optimizations, they generated comparable results. In fact, our HyperPipes clas-sifier actually produced a significantly lower FP rate, which prompted us to use it for the remaining evaluations.
As mentioned above in Section 5.2, effective Web spam solutions must be able to handle an evolving distribution of spam and legitimate Web pages. Therefore, after we iden-tified HyperPipes as the best classifier for this corpus, we evaluated its resiliency against various class distributions. Using the same approach described in Section 5.2, we eval-uated the classifier on 9 corpus samples with class distribu-tions varying from 10% spam to 90% spam, in increments of 10%. The results of this evaluation are shown in Table 10. The table shows somewhat erratic TP rates, FP rates, and F-Measure values as the percentage of spam increases. Un-like Table 6, which shows a slow decline for these metrics as the spam percentage increases, the results in Table 10 exhibit distinct patterns of increasing and decreasing per-formance. When the spam percentage is between 30% and 70%, the classifier X  X  performance is relatively consistent, ex-hibiting TP rates between 62.7% and 73.1% and FP rates between 1.0% and 2.2%. However, the classifier X  X  perfor-mance varies widely for the most extreme class distributions (i.e., spam percentages of 10%, 20%, 80%, and 90%).
The most logical explanation for the classifier X  X  perfor-mance variations is the small size of this corpus. When the spam percentage was 10%, the sample only contained 149 spam instances. This small spam sample made it extremely difficult for the classifier to learn distinguishing spam fea-tures, and the classifier X  X  TP rate decreased dramatically. Similarly, when the spam percentage was 90%, the sample only contained 149 legitimate instances. This small num-ber of legitimate instances made it quite challenging for the classifier to learn distinguishing legitimate features; conse-quently, the classifier X  X  FP rate increased. If the WEBSPAM-UK2006 corpus was larger and we could derive a sample size comparable to the one used in Section 5.2, we believe these performance fluctuations would be reduced. Table 11: Classifier training and per instance classi-fication times using WEBSPAM-UK2006.

To evaluate the computational costs of HTTP session clas-sification on the WEBSPAM-UK2006 corpus, we performed timing experiments with the 5 classifiers presented in Sec-tion 6.1. Using the same approach described in Section 5.3, we computed the training time and per instance classifica-tion time necessary for each classifier to perform a stratified tenfold cross-validation evaluation. Table 11 shows the re-sults for the 3 most efficient classifiers. The relative perfor-mance of the classifiers shown in the table is the same as in Table 7. However, the training times in Table 11 are all much lower because the corpus sample size for this evalua-tion was almost an order of magnitude smaller. Addition-ally, the per classification times for C4.5 and HyperPipes actually increased slightly, whereas the corresponding time for RBFNetwork decreased. We believe these slight varia-tions are another artifact of the small sample size, which is reaffirmed by the higher standard deviation values shown in Table 11. As a result, we believe the timings obtained on the larger corpora in Section 5.3 are more indicative of our approach X  X  efficiency.
By avoiding the costly downloads associated with Web spam pages, our approach saves valuable bandwidth and storage resources for Web browsers and Web crawlers. To help quantify the magnitude of these savings, we investi-gated the size characteristics of the Web spam pages in the WEBSPAM-UK2006 corpus. The average size of a Web spam response message in the corpus is about 24 KB (23.3 KB for the message body and 0.7 KB for the headers). Hence, if our approach successfully detected 100% of the Web spam pages, we would save about 23.3 KB in band-width and storage costs every time we encountered a spam URL. Based on the actual detection rate (71.9%) exhibited by our HyperPipes classifier, we expect to save an average of about 16.8 KB for every encountered spam URL.
In this paper, we have presented a predictive approach to Web spam classification that relies exclusively on HTTP session information (i.e., hosting IP addresses and HTTP session headers), and we used this predictive approach to build an HTTP session classifier. Using a corpus of almost 350,000 Web spam instances and almost 400,000 legitimate instances, our HTTP session classifier effectively detected 88.2% of the Web spam pages with a false positive rate of only 0.4%. Additionally, by incorporating this classifier into the HTTP retrieval process, our approach was capable of saving an average of 15.4 KB of bandwidth and storage resources for every successfully identified Web spam page, while only adding an average of 101  X  s to each HTTP re-trieval operation.
 Our predictive approach is complementary to previous Web spam research focused on link-based and content-based analysis techniques. An interesting direction for future re-search involves combining our HTTP session classification process with these existing analysis techniques to create a multi-layered defense against Web spam.
