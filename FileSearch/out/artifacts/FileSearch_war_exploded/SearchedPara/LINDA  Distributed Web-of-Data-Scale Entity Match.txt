 Linked Data has emerged as a powerful way of intercon-necting structured data on the Web. However, the cross-linkage between Linked Data sources is not as extensive as one would hope for. In this paper, we formalize the task of automatically creating  X  X ameAs X  links across data sources in a globally consistent manner. Our algorithm, presented in a multi-core as well as a distributed version, achieves this link generation by accounting for joint evidence of a match. Experiments confirm that our system scales beyond 100 mil-lion entities and delivers highly accurate results despite the vast heterogeneity and daunting scale.
 I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods; H.4 [ Information Systems Ap-plications ]: Miscellaneous Algorithms
Linked Open Data (LOD) is emerging as a way of in-terconnecting structured-data sources on the Internet and creating a  X  X eb of Data X . In total, the Web of Data cur-rently contains about 30 billion triples. The key point of LOD is to provide extensive cross-linkage between sources at the level of entities. For example, two sameAs links suffice to connect data about the director David Lynch in DBpedia, Freebase, and the BBC. This way, we can answer join queries that require biographic data from DBpedia as well as data about compositions from the BBC. Clearly, this has enor-mous potential. Unfortunately, this cross-linkage between LOD sources is not nearly as extensive as one would hope. Although exact numbers are not known, an estimate for the number of sameAs links is in the order of 500 million. A large number of trivial links exist between major knowledge bases like DBpedia, Freebase and Yago, which are derived from Wikipedia and can use article titles as a common de-nominator. Our aim is to develop fully automated domain-independent methods that discover high-quality links, while scaling to the immense proportions of the Web of Data.
At first glance, this seems to be identical to the classical record linkage task, also known as entity resolution . The problem has received much attention in the literature [1, 8]. A closer look, however, reveals fundamental differences between the typical record-linkage setting and our prob-lem of entity matching in LOD. First, the fine-grained and loose-schema nature of RDF triples makes it much harder to identify similarity features. Second, in contrast to the traditional setting, we are faced with the vast heterogeneity of LOD sources, spanning many different domains. Third, we deal with a much larger number of sources. This is an additional challenge, but also an opportunity, since we can potentially exploit sameAs transitivity over many sources. Finally, many billions of triples is a daunting scale that has not yet been tackled in a joint approach. Recent work geared towards LOD [5, 6, 9, 14] has either only been applied to small datasets or has partitioned the entity matching into separate jobs without accounting for their interactions.
Our approach is based on an optimization model that cap-tures the joint evidence for entities in one source matching entities in other sources. The evidence is based on neighbor-ing nodes of an entity and those of another entity to which a sameAs link could possibly exist. Consider the example of the entity David Lynch on LinkedMDB and a candidate en-tity in Freebase. For inferring equivalence, we consider both sides X  attributes such as birthplace, awards and relationships to movies, compositions, etc.
 This situation motivates our joint-reasoning approach: After matching David Lynch correctly, we have more evi-dence to also choose the right Mulholland Drive in Free-base, i.e., the movie and not the street in the Los Angeles area. This reasoning proceeds recursively, i.e., a decision about equivalence of neighboring entities may in turn affect further neighboring entities.

This paper makes the following technical contributions: (1) A new optimization model for joint evidence and con-sistency of sameAs linkage between LOD entities. (2) An efficient and scalable algorithm for computing an approx-imate solution of the optimization problem, with a multi-core as well as a distributed MapReduce-based version. (3) Large-scale experiments that demonstrate the viability of our approach on graphs with &gt; 100 millions of entities. Goal. Given Linked Data RDF triples, we would like to match entity identifiers that come from different sources and represent the same real-world entity.
 Input. Our input is a set of RDF triples of the form ( s,p,o ), i.e., with subject, predicate, and object. These are cast into an entity graph G , with nodes corresponding to entity URIs, and edge labels representing triple predicates.
 Desired Output. Our output is a square 0-1 matrix X , stating whether any two URIs represent the same entity or not. The output is subject to certain constraints. We re-quire the sameAs relationship to be symmetric and transi-tive, which corresponds to establishing equivalence classes of entities. We do not aim to discover sameAs links within a single data source, as single sources are usually much better maintained than links across sources. Our entity matching algorithm makes joint decisions for multiple URI pairs when producing sameAs links. It is initialized with a prior similar-ity between entities, based on the immediate neighborhoods of the URIs. The algorithm iteratively considers similari-ties between entities to reinforce or invalidate the match-ing between two URIs. Output matchings are stored in an assignment matrix X , while the dynamically re-computed similarity values are tracked in a similarity matrix Y . Entity Graph and Assignment Matrix. For the graph, we consider only triples ( s,p,o ) where s , p , and o are URIs. Literals are compiled into a set of values L ( s ). The data source from which a URI a originates is denoted as S ( a ). Definition 1. An entity graph is a directed multigraph G = ( V,E ) with a set of nodes V representing URIs and labeled edges in E capturing their relationships. Given a set T of triples, we set V = { s | ( s,p,o )  X  T } X  X  o | ( s,p,o )  X  T } , and E = { ( s,p,o )  X  T } .

Definition 2. Given an entity graph G = ( V,E ), an as-signment matrix X is a symmetric n  X  n matrix with n = | V | and x a,b  X  { 0 , 1 } . An entry x a,b states whether our algo-rithm outputs  X  a sameAs b  X .

Definition 3. An assignment matrix X is consistent if it satisfies the following constraints: 1. Reflexivity  X  a  X  V : x a,a = 1 2. Symmetry  X  a,b  X  V : x a,b = x b,a 3. Transitivity  X  a,b,c  X  V : x a,b  X  x b,c  X  x a,c 4. optional: Unique mapping per data source The entity graph is the data structure that captures our in-put. The output is gradually built into an assignment matrix subject to constraints. The optional last constraint states that an entity a cannot simultaneously match two entities from the same second source.This constraint allows us to fo-cus on cross-source links and, in practice, picking only the best match within a dataset avoids many false positives. Objective Function. Let sim( a,b,G, X ) be a similarity function between two entities a and b that may depend on the entity graph G as well as the current X . For constant G and X , sim should be a semimetric and return scores in [  X  X  X  , +  X  ]: positive scores for likely entity matches and neg-ative scores for likely non-matches. To quantify the quality of the output X , we define the following objective.
Definition 4. Given an entity graph G = ( V,E ) and a similarity function sim, the maximum consistent assignment problem consists in choosing a consistent assignment matrix X with values x a,b  X  X  0 , 1 } that maximizes The sim function often becomes negative and deliberately depends on the assignment matrix X : For instance, in our previous example, the similarity of two Mulholland Drive URIs depends on whether two other (related) URIs both represent David Lynch . By reduction from the CLIQUE problem one can show that this problem is NP-hard .
We now present an algorithm that computes a consistent assignment matrix X while attempting to maximize the ob-jective score. This first algorithm can benefit from multiple cores during operation on a single machine. For scalability, our algorithm iteratively adjusts the values in X with care-fully chosen, greedy improvements in the objective function. X is initialized with 1s on the diagonal and 0s otherwise. In addition, we use a similarity matrix Y with pairwise, real-valued similarity values. Y is initialized with values that reflect the pair-wise similarity of URIs (see Sec. 4). Given the previous iteration X  X  values of X , we compute new simi-larity values for Y , which is maintained in a priority queue and used to adjust elements of X . Both X and Y are sparse symmetric matrices that do not reside entirely in memory. Y entries need to be materialized only on demand for the priority queue; negative scores do not need to be retained. So the Y matrix is a purely conceptual construct only.
Algorithm 1 describes the method more formally. In line 3, the updatable priority queue Q stores the initial similarity scores y a,b = sim( a,b,G, X ). In practice, sim yields negative scores for most entity pairs. With MapReduce, we can ef-ficiently determine entity pairs with positive scores without computing a quadratic number of similarities (see Sec. 4).
The algorithm then repeatedly dequeues the entity pair a,b with the highest similarity score from Q and sets x a,b 1. To ensure transitivity, it further considers all equivalents E ( a, X ) = { a 0 | x a,a 0 = 1 } of a and E ( b, X ) = { b 1 } of b already identified with them. The for-loop (line 7) essentially merges the equivalence classes of a and b .
Then, the algorithm determines pairs of entities which may need updating (line 9) due to modifying X . For the similarity function we define later in Sec. 4, these are the candidate pairs involving entities in E ( a, X ) as well as en-tity graph neighbors of entities in E ( a, X ). The algorithm adjusts the scores of these pairs in Y in parallel, making use of multiple cores. The new similarity scores y a 0 ,b puted in line 12 reflect the new X with E ( a, X ) = E ( b, X ), highlighting the joint mapping strategy.

For space reasons, we omit the proofs that our algorithm produces a consistent matrix X and is guaranteed to con-verge for well-behaved similarity functions. Convergence to local maxima can be overcome with simple randomization or stochastic optimizations.
 Assignment with MapReduce. The previously de-scribed Algorithm 1 is bounded by the number of CPU cores available for parallelism and by memory available to store X and (parts of) the priority queue Q . Algorithm 1 Multi-Core Assignment Algorithm 1: procedure linda ( G ) 3: Q  X  initial similarities . with MapReduce 4: while Q non-empty do 16: return X
To overcome these bounds, we report on a MapReduce-based version of the assignment algorithm that allows scal-ing our solution to immense amounts of data, since today X  X  large cluster sizes range in the thousands. With MapRe-duce, a problem is divided into independent map tasks that consume and emit key-value-pairs as well as reduce tasks that aggregate intermediate output. Fig. 1 illustrates the workflow of our MapReduce-based approach.

Consider an input entity graph G (Fig. 1, left) distributed across a cluster. Each node holds a portion Q i of the queue Q and a respective partition G i of G . An entity graph parti-tion G i comprises all information for vertices in queue parti-tion Q i . Partitions are stored as sorted lists, which enables fast merge-join-like access instead of the graph and the queue being shuffled across the cluster. The only information sent from mappers to reducers are messages about which pairs of vertices require similarity score recomputations.

Each mapper reads Q i and stores the top K entries in a buffer B . We refer to K as the acceptance rate . Mappers also forward messages from previous phases. After mapper completion, a procedure accepts all pairs a 0 ,b 0 of equivalents for entries of B for the resulting X (step (1) in Fig. 1).
Remember that the acceptance of a sameAs edge between vertices a and b induces a series of score adjustments. In our setup, reducers handle specific graph partitions. Hence, if t is the number of edge hops to nodes whose sim scores may change, then the algorithm uses t reducer steps to trigger the respective score recomputations. For our sim function (see Sec. 4), a new sameAs edge induces two sorts of recom-putations, i.e., (1) for the candidate pairs involving entities in E ( a, X ) or E ( b, X ) and (2) for the entity graph neighbors of entities in E ( a, X ) or E ( b, X ). It thus requires two steps: one to reach the equivalents ( notify step in Fig. 1), another to reach their neighbors in the graph ( update step).
After accepting pairs a 0 ,b 0  X  E ( a, X )  X  E ( b, X ), the map-per emits two sorts of notification messages (step (2) in Fig. 1): notification triggers similarity score updates for entity graph neighbors of a 0 and b 0 ; updateTargets triggers candidate pair updates for a 0 and b 0 .
 A reducer reacts according to the message it receives. Given notification , it emits update messages for respec-tive neighbors. It additionally emits update s for all affected Q i entries. Given an updateTargets message, it emits up-date s for all affected Q i entries on other compute nodes. This is step (3) in Fig. 1. The actual update of queue entries is triggered when a reducer receives such an update message. Then, the reducer checks whether the unique mapping con-straint still holds and computes the new y 0 value (step (4) in Fig. 1).

Note that the round-trip of the effect of a new entry in X takes two iterations (since we do two edge traversals). Therefore, the unique mapping constraint may not be ful-filled when accepting sameAs edges while queue updates for decisions from previous mappers are still pending.
The LINDA (LINked Data Alignment) system implements the these two assignment algorithm variants in combination with judiciously designed similarity functions. LINDA com-putes two kinds of similarities between entities a , b : (i) a prior similarity sim 0 based on literals and constraints, which is computed once in advance and used to initialize the sim-ilarity matrix Y and later as a smoothing prior, and (ii) a contextual similarity sim C , which is recomputed in each it-eration and considers the current state of the assignment matrix X . The two similarity measures are combined to an overall similarity score sim( a,b,G, X ). Given an entity graph G = ( V,E ), an assignment matrix X , and two pa-rameters  X  ,  X  , the similarity score for entities a,b  X  V is: C ( a ) and C ( b ) denote the contexts of a , b and are defined later. The parameter  X  controls the contextual influence and  X  is used for renormalization to values around 0  X  positive scores should reflect likely mappings and negative scores im-ply dissimilarities as required by Def. 4. We experimentally found  X  = 1 . 0 to perform well.
 Prior Similarities. Prior similarities sim 0 ( a,b ) reflect the direct evidence of two entities matching. Given two entities a and b and their sets of normalized literal n-grams N a = N ( L ( a )), N b = N ( L ( b )), the prior similarity sim to  X  X  X  if they stem from the same data source, and to 0 if they have no common n-grams. In all other cases, sim 0 ( a,b ) is computed as Contextual Similarities Given an entity graph G = ( V,E ), the context C ( a ) of an entity a is a set of context tuples ( r,n,w ), where r is a predicate (edge label in the entity graph), n is a neighboring entity of a , and w is a nu-meric weight. The context of an entity a includes objects n of triples with a as subject and subjects n of triples with a as object. The weights w = 1 log freq( r,n ) are higher for less frequent and thus more discriminative context tuples. Given an entity graph G , an assignment matrix X , and two enti-ties a , b with contexts C a = C ( a ), C b = C ( b ), the contextual similarity sim C ( C a ,C b ,G, X ) is defined as  X   X   X   X   X   X   X   X   X  Intuitively, this function finds matching pairs of context tuples and sums up their similarity values. It determines the smaller set C s and then sums up weighted similari-ties for the best matching context tuples for each tuple ( r,n,w )  X  C s . The similarity of individual context tuples ( r a ,n a ,w a ), ( r b ,n b ,w b ) depends on the current state of the entity matching and on predicate similarities sim( r a ,r
All input data is first processed with MapReduce to cre-ate and store the entity graph, prior similarities as well as context weights and predicate similarities. The algorithm itself uses a sparse matrix representation to store X , and a priority queue to keep relevant entries in Y . The multi-core version uses a thread pool for parallelization and was run on a single 80-core server. The distributed version of the algorithm operated on a 1+9-node-cluster.
 Precision and Recall. We compared LINDA with tra-ditional small-scale instance matching systems using the OAEI 2010 benchmarks. Table 1 compares LINDA (  X  = 0 . 2) with all systems that participated for all four datasets ac-cording to the official website ( oaei.ontologymatching.org ). The DI benchmark unfortunately could not be tested, as its reference alignments included URIs missing in the supplied input data. Overall, our system delivers very competitive results although it uses the same set of parameters across all datasets. Competing systems like RiMOM use different sets of highly tuned methods and settings for each of the datasets. LINDA favors precision over recall to ensure high quality results, but a trade-off is possible by lowering the  X  parameter or by choosing other prior similarity functions. Large-Scale Experiment. Next, we tested our system on very large volumes of data by creating an augmented Billion Triple Challenge Dataset ( BTC+ ). The original BTC 2010 dataset ( http://km.aifb.kit.edu/projects/btc-2010 ) contains  X  3.1 billion RDF quadruples crawled from  X  2,500 differ-ent sources (624 GB of data). We increased the size of the dataset by including all triples from the DBpedia dataset. For this very large dataset, building the database with prior similarities took 1 day. Removing provenance information, duplicate triples, RDF blank nodes as well as reification statements resulted in  X  350 million unique triples describ-ing  X  95 million different URIs. Computing initial similar-ities for the algorithm X  X  queue took 1h, and after that the algorithm ran for 7.5h using the same server setup as above.
At  X  = 1 . 0, LINDA delivered a total of 2,601,392 sameAs links, not including identity links, with a sampled precision of 0 . 83  X  0 . 06. At  X  = 0 . 25, LINDA delivered 12.3 million links, but the accuracy drops to 66% due to the particularly noisy nature of this dataset. We observed in particular that the joint inference strategy of our algorithm allowed us to infer many new mappings that initially had negative similar-ity scores. A few initial matches suffice to subsequently find other matches, and so on. The greedy nature of our algo-rithm works well in conjunction with our unique mappings per data source constraint, as it allows our algorithm to se-lect the most likely mappings, while ignoring other mappings into the same data source that only coincidentally have pos-itive similarity scores. Inaccurate mappings often seemed to result from the noisiness of the BTC dataset, which includes a lot of metadata entities that all look the same. Note that we cannot compare our results with those of existing sys-tems: To the best of our knowledge no existing joint match-ing system is able to process the same amount of data. Distributed Web-Scale Experiment. We evaluated the MapReduce algorithm for LINDA on a web-scale dataset BTC++, which comprised all triples from the 2011 BTC data, DBpedia (version 3.6), Freebase, Yago, and Geon-ames.org (as of mid 2011), in total 566.2 million unique triples describing 115.5 million URIs. The initial queue com-prises 154.5 million entity pairs, i.e., 4.07GB of binary data in HDFS. The BTC++ entity graph amounts to 39.17 GB in HDFS (including stored overlaps). The degrees of the graph vertices are very high, sometimes above 100,000.

We conducted experiments for a varying number of com-pute nodes n as well as varying acceptance rates K . We measured the time and the number of accepted links per iter-ation. Also, we kept track of the number of update messages and queue updates registered. For the number of messages sent across the cluster, we observed large numbers (dozens of millions) during early iterations. The message traffic lev-els out after 10-20 iterations (depending on n and K ). As for the actual updates, we observed thousands of updates during early iterations, also leveling out later on. Next, we compare output sizes with respect to runtime. Figure 2 shows the cumulative runtime (solid lines) and the number of links (dashed lines) computed by our distributed approach ( K = 100). Clearly, given a specific iteration i as well as fixed n and K , the output size grows, since i  X  n  X  K is its lower bound. However, as the acceptance of a single map-ping induces the merge of the respective equivalence classes, the output size increases beyond this. Note that at some point (iteration 27 for n = 60), this increase is steeper than the runtime X  X  increase  X  essentially leading to more links per time unit. For n = 40 and n = 60 ( K = 100) we ran the algorithm until the our output comprised five million links for the BTC++ data. This takes 500 or 325 iterations, re-spectively. Record Linkage. Classical record linkage typically re-lies on similarity measures between strings and between records [8, 1]. Smart blocking techniques are often employed to reduce similarity comparisons [9, 11, 15]. More recently, research has shifted to reasoning on entire groups of inter-related records [4]. The purest approaches employ collective relational learning, based on probabilistic graphical models [12, 16]. These work well for highly structured data but are not yet geared towards web-scale datasets.
 Semantic Web. A highly related task to our problem is ontology alignment (see, e.g., [2]), which focuses on aligning classes rather than large numbers of instances of classes. Systems typically rely on ontological constraints that are often unavailable for Linked Data. Existing systems for connecting Linked Data sources require humans to specify data-specific rules or training data and have not been ap-plied jointly across a broad range of datasets [14, 3, 6, 13]. The sameas.org service provides sameAs links that have been manually collected. In contrast, our approach aims at auto-matic unsupervised discovery of links across the LOD cloud. Distributed Entity Matching. MapReduce paralleliza-tion has been applied to standard blocking techniques [7]. A recent proposal for scaling up joint inference partitions the input into neighborhoods, and then parallel computa-tions per neighborhood are periodically reconciled by mes-sage passing [10]. This line of work is viable for highly structured and clearly typed records but does not consider densely connected Linked Data triples with high heterogene-ity at Web scale. Recently, MapReduce has been investi-gated for matching in the Linked Data world [14, 5]. How-ever, these systems simply distribute independently made matching decisions. This is different to our joint model, where decisions in one partition can affect decisions in other partitions. To the best of our knowledge, there are no re-ports on joint mapping experiments at the scale of our input data with more than 110 million unique entity URIs.
We have presented the first fully automatic system for joint entity matching in the Web of Data that is able to scale beyond the size of the Billion Triples data. Unlike previous approaches, it is designed to operate on all LOD sources to-gether, without source-specific customization. Our experi-ments demonstrate that the LINDA system successfully har-nesses joint evidence and constraints. In our experiments, we did not use pre-existing sameAs links as additional evi-dence, but when deploying LINDA in practice it would be straightforward to exploit such knowledge for higher-quality output. We anticipate that joint linking approaches that consider the entire LOD cloud when making decisions will be crucial as the Web of Data keeps growing. [1] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios. [2] J. Euzenat and P. Shvaiko. Ontology matching . [3] O. Hassanzadeh, A. Kementsietsidis, L. Lim, R. J. [4] M. Herschel, F. Naumann, S. Szott, and M. Taubert. [5] A. Hogan et al. Scalable and distributed methods for [6] W. Hu, J. Chen, and Y. Qu. A self-training approach [7] L. Kolb, A. Thor, and E. Rahm. Block-based Load [8] H. K  X  opcke and E. Rahm. Frameworks for entity [9] G. Papadakis et al. Beyond 100 million entities: [10] V. Rastogi, N. Dalvi, and M. Garofalakis. Large-scale [11] L. Shu, A. Chen, M. Xiong, and W. Meng. Efficient [12] P. Singla and P. Domingos. Entity resolution with [13] F. M. Suchanek, S. Abiteboul, and P. Senellart. [14] J. Volz et al. Discovering and maintaining links on the [15] S. E. Whang and H. Garcia-Molina. Joint entity [16] M. L. Wick, A. Culotta, K. Rohanimanesh, and
