 the case of standard classification and regression. The need for such approaches arises in several output space. These methods have been successfully applied in areas like computational biology, natural language processing and information retrieval. On the other hand there has been a recent series of work which generalizes regression with multivariate output to the case where the output space is a Riemannian manifold, see [4, 5, 6, 7], with applications in signal processing, computer vision, computer graphics and robotics. One can also see this branch as structured output learning if one thinks of a Riemannian manifold as isometrically embedded in a Euclidean space. Then the restriction that the output has to lie on the manifold can be interpreted as constrained regression in Euclidean space, where the constraints couple several output features together.
 In this paper we propose a family of kernel estimators for regression with metric-space valued input and output motivated by estimators proposed in [6, 8] for manifold-valued regression. We discuss loss functions and the corresponding Bayesian decision theory for this general regression problem. cases for certain choices of the output space and its metric. However, our main emphasis lies on the problem of regression with manifold-valued input and output which includes the multivariate Euclidean case. In particular, we show for all our proposed estimators their pointwise and Bayes consistency, that is in the limit as the sample size goes to infinity the estimated mapping converges to the Bayes optimal mapping. This includes estimators implementing several robust loss functions like the L 1 -loss, Huber loss or the  X  -insensitive loss. This generality is possible since our proof considers directly the functional which is minimized instead of its minimizer as it is usually done in consistency proofs of the Nadaraya-Watson estimator. Finally, we conclude with a toy experiment illustrating the robustness properties and difference of the estimators. We consider the structured output learning problem where the task is to learn a mapping  X  : M  X  N between two metric spaces M and N , where d M denotes the metric of M and d N the metric of N . We assume that both metric spaces M and N are separable 1 . In general, we are in a statistical setting where the given input/output pairs ( X i ,Y i ) are i.i.d. samples from a probability measure P on M  X  N .
 In order to prove later on consistency of our metric-space valued estimator we first have to define the Bayes optimal mapping  X   X  : M  X  N in the case where M and N are general metric spaces which depends on the employed loss function. In multivariate regression the most common loss function where Med denotes the median of P( Y | X = x ) which is a robust location measure. Several general-izations of the median for multivariate output have been proposed, see e.g. [9]. In this paper we refer to the minimizer of the loss function L ( y,f ( x )) = k y  X  f ( x ) k as the (generalized) median, since this seems to be the only generalization of the univariate me-dian which has a straightforward extension to metric spaces. In analogy to Euclidean case, we will therefore use loss functions penalizing the distance between predicted output and desired output: where  X  : R +  X  R + . We will later on restrict  X  to a certain family of functions. The associated risk (or expected loss) is: R  X  (  X  ) = E [ L ( Y, X  ( X ))] and its Bayes optimal mapping  X   X   X  : M  X  N can then be determined by In the second step we used a result of [10] which states that a joint probability measure on the product of two separable metric spaces can always be factorized into a conditional probability measure and the marginal. In order that the risk is well-defined, we assume that there exists a measurable mapping  X  : M  X  N so that E [ X  d N ( Y, X  ( X )) ] &lt;  X  . This holds always once N has bounded diameter. Apart from the global risk R  X  (  X  ) we analyze for each x  X  M the pointwise risk R 0  X  ( x, X  ( x )) , which measures the loss suffered by predicting  X  ( x ) for the input x  X  M . The total loss R  X  (  X  ) of to find the Bayes optimal mapping  X   X  pointwise,  X  ( x ) = arg min where d X  x is the conditional probability of Y conditioned on X = x . Later on we prove consistency functions.
 Definition 1 A convex function  X  : R +  X  R + is said to be (  X ,s ) -bounded if Several functions  X  corresponding to standard loss functions in regression are (  X ,s ) -bounded: While uniqueness of the minimizer of the pointwise loss functional R 0  X  ( x,  X  ) cannot be guaranteed anymore in the case of metric space valued output, the following lemma shows that R 0  X  ( x,  X  ) has reasonable properties (all longer proofs can be found in Section 7 or in the supplementary material). It generalizes a result provided in [11] for  X ( x ) = x 2 to all (  X ,s ) -bounded losses. Lemma 1 Let N be a complete and separable metric space such that d ( x,y ) &lt;  X  for all x,y  X  N and every closed and bounded set is compact. If  X  is (  X ,s ) -bounded and R 0  X  ( x,q ) &lt;  X  for some q  X  N , then pointwise risk, is called the Frech  X  et mean 2 or Karcher mean in the case where N is a manifold. It is the generaliza-tion of a mean in Euclidean space to a general metric space. Unfortunately, it needs to be no longer unique as in the Euclidean case. A simple example is the sphere as the output space together with a uniform probability measure on it. In this case every point p on the sphere attains the same value F ( p ) and thus the global minimum is non-unique. We refer to [12, 13, 11] for more information under which conditions one can prove uniqueness of the global minimizer if N is a Riemannian manifold. The generalization of the median to Riemannian manifolds, that is  X ( x ) = x , is discussed in [9, 4, 8]. For a discussion of the computation of the median in general metric spaces see [14]. In the following we provide the definition of the kernel estimator with metric-space valued out-put motivated by the two estimators proposed in [6, 8] for manifold-valued output. We use in the following the notation k h ( x ) = 1 h m k ( x/h ) .
 Definition 2 Let ( X i ,Y i ) l i =1 be the sample with X i  X  M and Y i  X  N . The metric-space-valued kernel estimator  X  l : M  X  N from metric space M to metric space N is defined for all x  X  M as where  X  : R +  X  R + is (  X ,s ) -bounded and k : R +  X  R + .
 If the data contains a large fraction of outliers one should use a robust loss function  X  , see Sec-tion 6. Usually the kernel function should be monotonically decreasing since the interpretation of k h d M ( x,X i ) is to measure the similarity between x and X i in M which should decrease as the distance increases. The computational complexity to determine  X  l ( x ) is quite high as for each test point one has to solve an optimization problem but comparable to structured output learning (see discussion below) where one maximizes for each test point the score function over the output space. For manifold-valued output we will describe in the next section a simple gradient-descent type opti-mization scheme in order to determine  X  l ( x ) .
 It is interesting to see that several well-known nonparametric estimators for classification and re-gression can be seen as special cases of this estimator (or a slightly more general form) for different choices of the output space, its metric and the loss function. In particular, the approach shows a cer-tain analogy of a generalization of regression into a continuous space (manifold-valued regression) and regression into a discrete space (structured output learning). Multiclass classification: Let N = { 1 ,...,K } where K denotes the number of classes K . If 0 else leads for any  X  to the standard multiclass classification scheme using a majority vote. Cost-class q by class q 0 . Since general costs can generally not be modeled by a metric, it should be noted that the estimator can be modified using a similarity function, s : N  X  N  X  R , The consistency result below can be generalized to this case given that N has finite cardinality. Multivariate regression: Let N = R n and M be a metric space. Then for  X ( x ) = x 2 , one gets estimator, see [15, 16], on a metric space. In [17] a related estimator is discussed when M is a closed Riemannian manifold and [18] discusses the Nadaraya-Watson estimator when M is a metric space. Manifold-valued regression: In [6] the estimator  X  l ( x ) has been proposed for the case where N is a Riemannian manifold and  X ( x ) = x 2 , in particular with the emphasis on N being the manifold of shapes. The discussion of a robust median-type estimator, that is  X ( x ) = x , has been done recently outperforms the estimator  X  l ( x ) , it is a well working baseline with a simple implementation, see Section 4.
 Structured output: Structured output learning, see [1, 2, 3] and references therein, can be formu-supposed to measure jointly the similarity and thus can capture non-trivial dependencies between input and output. Using such kernels [1, 2, 3] learn a score function s : M  X  N  X  R , with being the final prediction for x  X  M . The similarity to our estimator  X  l ( x ) in (2) becomes more obvious when we use that in the framework of [1] the learned score function can be written as where  X   X  R l is the learned coefficient vector. Apart from the coefficient vector  X  this has almost the form of the previously discussed estimator in Equation (3), using a joint similarity function on input and output space. Clearly, a structured output method where the coefficients  X  have been optimized, should perform better than  X  i = const. In cases where training time is prohibitive the M and N , and k N ( q,q ) = const., then one can rewrite the problem in (4) as, where d N is the induced (semi)-metric 3 of k N . Apart from the learned coefficients this is basically equivalent to  X  l ( x ) in (2) for  X ( x ) = x 2 .
 In the following we restrict ourselves to the case where M and N are Riemannian manifolds. In this case the optimization to obtain  X  l ( x ) can still be done very efficiently as the next section shows. For fixed x  X  M , the functional F ( q ) for q  X  N which is optimized in the kernel estimator  X  l ( x ) can be rewritten with w i = k h ( d M ( x,X i )) as, a tangent vector at q with k v i k T from Y i to q (pointing  X  X way X  from Y i ). Denoting by exp q : T q N  X  N the exponential map at q , the simple gradient descent based optimization scheme can be written as As stopping criterion we use either the norm of the gradient or a threshold on the change of F . For the experiments in Section 6 we get convergence in 5 to 40 steps. In this section we show the pointwise and Bayes consistency of the kernel estimator  X  l in the case where M and N are Riemannian manifolds. This case already subsumes several of the interesting applications discussed in [6, 8]. The proof of consistency of the general metric-space valued kernel estimator (for a restricted class of metric spaces including all Riemannian manifolds) requires high technical overload which is interesting in itself but which would make the paper hard accessible. The consistency of  X  l will be proven under the following assumptions: Assumptions (A1): Note, that existence of a density is not necessary for consistency. However, in order to keep the proofs simple, we restrict ourselves to this setting. In the following dV = natural volume element of a Riemannian manifold with metric g , vol( S ) and diam( N ) are the volume and diameter of the set S . For the proof of our main theorem we need the following two propositions. The first one summarizes two results from [20].
 Proposition 1 Let M be a compact m -dimensional Riemannian manifold. Then, there exists r 0 &gt; 0 Moreover, the cardinality K of a  X  -covering of M is upper bounded as, K  X  vol( N ) S Moreover, we need a result about convolutions on manifolds.
 Proposition 2 Let the assumptions A1 hold, then if f is continuous we get for any x  X  M \  X  X  , Lipschitz constant L , then there exists a h 0 &gt; 0 such that for all h &lt; h 0 ( x ) , The following main theorem proves the almost sure pointwise convergence of the manifold-valued kernel estimator for all (  X ,s ) -bounded loss functions  X  .
 Theorem 1 Suppose the assumptions in A1 hold. Let  X  l ( x ) be the estimate of the kernel estimator for sample size l . If h  X  0 and lh m / log l  X  X  X  , then for any x  X  M \  X  X  , If additionally p (  X  ,y ) is Lipschitz-continuous for any y  X  N , then The optimal rate is given by h = O (log l/l ) 1 2+ m so that Note, that the condition lh m / log l  X   X  for convergence is the same as for the Nadaraya-Watson estimator on a m -dimensional Euclidean space. This had to be expected as this condition still holds output is not more  X  X ifficult X  than standard regression with multivariate output.
 Next, we show Bayes consistency of the manifold-valued kernel estimator.
 Theorem 2 Let the assumptions A1 hold. If h  X  0 and lh m / log l  X  X  X  , then E theorem proven by Glick, see [21], provides the result. We illustrate the differences of median and mean type estimator on a synthetic dataset with the task of estimating a curve on the sphere, that is M = [0 , 1] and N = S 1 . The kernel used had the form, k | x  X  y | /h = 1  X  X  x  X  y | /h . The parameter h was found by 5-fold cross validation from the set of van-Mises noise (note that the parameter k is inverse to the variance of the distribution) in Table 1. As expected the the L 1 -loss and the Huber loss as robust loss functions outperform the L 2 -loss in the presence of outliers, whereas the L 2 -loss outperforms the robust versions when no outliers are present. Note, that the Huber loss as a hybrid version between L 1 -and L 2 -loss is even slightly dataset it makes sense not only to do cross-validation of the parameter h of the kernel function but also over different loss functions in order to adapt to possible outliers in the data. Figure 1: Regression problem on the sphere with 1000 training points (black points). The blue points are the ground truth disturbed by van Mises noise with parameter k = 100 and 20% (outliers) with k = 3 . The estimated curves are shown in green. Left: Result of L 1 -loss, mean error (ME) 0 . 256 , mean squared error (MSE) 0 . 165 . Middle: Result of L 2 -loss: ME = 0 . 265 , MSE = 0 . 169 . Right: Result of Huber loss with  X  = 0 . 1 : ME = 0 . 255 , MSE = 0 . 165 . In particular, the curves found using L 1 and Huber loss are very close to the ground truth.
 Table 1: Mean squared error (unit 10  X  1 ) for regression on the sphere -for different noise levels k , number of labeled points, without and with outliers. Results are averaged over 10 runs. Lemma 2 Let  X  : R +  X  R be convex, differentiable and monotonically increasing. Then arg min the pointwise estimate, R
 X  ( x, X  l ( x ))  X  min q  X  N R Moreover, we assume to have a  X  -covering of N with centers N  X  = { q  X  } K  X  =1 where using Lemma we have to control four terms, where we have used Lemma 2 and the fact that E holds. Then, there exists a constant C such that which can be shown using Bernstein X  X  inequality for 1 l P l i =1 W i  X  E [ W i ] where W i = where we used Proposition 1 to lower bound vol( B ( x,hr 1 )) for small enough h . Third, we get for the third term using again Lemma 2, tion on the joint density p ( x,y ) we can use Proposition 2. For every x  X  M \  X  X  we get, lim where C x &gt; 0 . Thus with we get for every x  X  M \  X  X  , where we have used g h  X  aS 1 r 1 p min &gt; 0 and g = C x p ( x ) &gt; 0 . Moreover, using results from the proof of Proposition 2 one can show f h &lt; C for some constant C . Thus f h /g h &lt; C for some constant and f h /g h  X  f/g as h  X  0 . Using the dominated convergence theorem we thus get lim R  X  ( x,q ) + O ( h ) .
 In total, there exist constants A,B,C,D 1 ,D 2 , such that for sufficiently small h one has with prob-With  X  = l  X  s for some s &gt; 0 one gets convergence if lh m log l  X   X  together with y  X  N we have R E  X  ( x,q ) = R 0  X  ( x,q ) + O ( h ) and can choose s large enough so that the bound from the approximation error dominates the one of the covering. Under the condition lh m log l  X  X  X  the probabilistic bound is summable in l which yields almost sure convergence by the Borel-Cantelli-Lemma. The optimal rate in the Lipschitz continuous case is then determined by fixing h such that both terms of the bound are of the same order.
 Acknowledgments We thank Florian Steinke for helpful discussions about relations between generalized kernel esti-mators and structured output learning. This work has been partially supported by the Cluster of Excellence MMCI at Saarland University. [1] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured [2] J. Weston, G. BakIr, O. Bousquet, B. Sch  X  olkopf, T. Mann, and W. S. Noble. Joint kernel maps. [3] E. Ricci, T. De Bie, and N. Cristianini. Magic moments for structured output prediction. JMLR , [4] K.V. Mardia and P.E. Jupp. Directional statistics . Wiley New York, 2000. [5] Inam Ur Rahman, Iddo Drori, Victoria C. Stodden, David L. Donoho, and Peter Schroder. [6] B. C. Davis, P. T. Fletcher, E. Bullitt, and S. Joshi. Population shape regression from random [7] F. Steinke and M. Hein. Non-parametric regression between Riemannian manifolds. In Ad-[8] P. T. Fletcher, S. Venkatasubramanian, and S. Joshi. The geometric median on Riemannian [9] C. G. Small. A survey of multidimensional medians. International Statistical Review , 58:263 X  [10] D. Blackwell and M. Maitra. Factorization of probability measures and absolutely measurable [11] R. Bhattacharya and V. Patrangenaru. Large sample theory of intrinsic and extrinsic sample [12] H. Karcher. Riemannian center of mass and mollifier smoothing. Communications on Pure [13] W. Kendall. Probability, convexity, and harmonic maps with small image. I. Uniqueness and [14] P. Indyk. Sublinear time algorithms for metric space problems. In Proceedings of the 31st [15] L. Gy  X  orfi, M. Kohler, A. Krzy  X  zak, and H. Walk. A Distribution-Free Theory of Nonparametric [16] W. Greblicki and M. Pawlak. Nonparametric System Identification . Cambridge University [17] B. Pelletier. Nonparametric regression estimation on closed Riemannian manifolds. J. of [18] S. Dabo-Niang and N. Rhomari. Estimation non parametrique de la regression avec variable [19] D. P. Bertsekas. Nonlinear Programming . Athena Scientific, Belmont, Mass., 1999. [20] M. Hein. Uniform convergence of adaptive graph-based regularization. In G. Lugosi and [21] N. Glick. Consistency conditions for probability estimators and integrals of density estimators.
