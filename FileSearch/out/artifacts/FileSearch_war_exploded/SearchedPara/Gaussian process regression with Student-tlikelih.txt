 A commonly used observation model in the Gaussian process (GP) regression is the Normal distri-bution. This is convenient since the inference is analytically tractable up to the covariance function parameters. However, a known limitation with the Gaussian observation model is its non-robustness, and replacing the normal distribution with a heavy-tailed one, such as the Student-t distribution, can be useful in problems with outlying observations.
 the information sources. Thus, outlying observations may significantly reduce the accuracy of the inference. For example, a single corrupted observation may pull the posterior expectation of the unknown function value considerably far from the level described by the other observations (see Figure 1). A robust, or outlier-prone, observation model would, however, weight down the outlying observations the more, the further away they are from the other observations and prior mean. The idea of robust regression is not new. Outlier rejection was described already by De Finetti [1] and theoretical results were given by Dawid [2], and O X  X agan [3]. Student-t observation model with linear regression was studied already by West [4] and Geweke [5], and Neal [6] introduced it for GP regression. Other robust observation models include, for example, mixtures of Gaussians, Laplace Figure 1: An example of regression with outliers by Neal [6]. On the left Gaussian and on the right the Student-t observation model. The real function is plotted with black line. distribution and input dependent observation models [7 X 10]. The challenge with the Student-t model is the inference, which is analytically intractable. A common approach has been to use the scale-mixture representation of the Student-t distribution [5], which enables Gibbs sampling [5, 6], and a factorized variational approximation (VB) for the posterior inference [7, 11].
 Laplace approximation for the approximate inference. We discuss the known weaknesses of the approximation scheme and show that in practice it works very well and quickly. We use several different data sets to compare it to both a full MCMC and a factorial VB, which utilize the scale mixture equivalent of the Student-t distribution. We show that the predictive performances are sim-ilar and that the Laplace X  X  method approximates the posterior covariance somewhat better than VB. We also point out some of the similarities between these two methods and discuss their differences. Consider a regression problem, where the data comprise observations y i = f ( x i ) + i at input locations X = { x i } n i =1 , where the observation errors 1 ,..., n are zero-mean exchangeable random variables. The object of inference is the latent function f , which is given a Gaussian process prior. distribution. In particular, at the observed input locations X the latent variables have a distribution where K f , f is the covariance matrix and  X  the mean function. For the notational simplicity, we will use a zero-mean Gaussian process. Each element in the covariance matrix is a realization of covari-latent function (for a detailed introduction on GP regression see [12]). The covariance function used where  X  2 se is the scaling parameter and l d are the length-scales.
 A formal definition of robustness is given, for example, in terms of an outlier-prone observation model. The observation model is defined to be outlier-prone of order n , if p ( f | y 1 ,...,y n +1 )  X  p ( f | y 1 ,...,y n ) as y n +1  X   X  [3, 4]. That is, the effect of a single conflicting observation to the posterior becomes asymptotically negligible as the observation approaches infinity. This contrasts heavily with the Gaussian observation model where each observation influences the posterior no matter how far it is from the others. The zero-mean Student-t distribution where  X  is the degrees of freedom and  X  the scale parameter [13], is outlier prone of order 1, and it can reject up to m outliers if there are at least 2 m observations in all [3]. From this on we will collect all the hyperparameters into  X  = {  X  2 se ,l 1 ,...,l D , X , X  } . 3.1 The conditional posterior of the latent variables Our approach is motivated by the Laplace approximation in GP classification [14]. A similar ap-proximation has been considered by West [4] in the case of robust linear regression and by Rue et al. [15] in their integrated nested Laplace approximation (INLA). Below we follow the notation of Rasmussen and Williams [12].
 A second order Taylor expansion of log p ( f | y , X  ) around the mode, gives a Gaussian approximation where  X  f = arg max f p ( f | y , X  ) and  X   X  1 is the Hessian of the negative log conditional posterior at the mode  X  f [12, 13]: where r = ( y i  X  f i ) , and W ji = 0 if i 6 = j . 3.2 The maximum a posterior estimate of the hyperparameters To find a maximum a posterior estimate (MAP) for the hyperparameters, we write p (  X  | y )  X  p ( y |  X  ) p (  X  ) , where is the marginal likelihood. To find an approximation, q ( y |  X  ) , for the marginal likelihood one can utilize the Laplace method second time [12]. A Taylor expansion of the logarithm of the integrand in (5) around  X  f gives a Gaussian integral over f multiplied by a constant, giving The hyperparameters can then be optimized by maximizing the approximate log marginal posterior, of gradient based optimization to find  X   X  = arg max  X  q (  X  | y ) [12]. 3.3 Making predictions The approximate posterior distribution of a latent variable f  X  at a new input location x  X  is also Gaussian, and therefore defined by its mean and variance [12] distribution of f  X  which can be evaluated, for example, with a Gaussian quadrature integration. 3.4 Properties of the Laplace approximation The Student-t distribution is not log-concave, and therefore the posterior distribution may be mul-timodal. The immediate concern from this is that a unimodal Laplace approximation may give a poor estimate for the posterior. This is, however, a problem for all unimodal approximations, Figure 2: A comparison of the Laplace and VB approximation for p ( f |  X ,y ) in the case of a single observation with the Student-t likelihood and a Gaussian prior. The likelihood is centered at zero and the prior mean is altered. The upper plots show the probability density functions and the lower plots the variance of the true posterior and its approximations as a function of the posterior mean.  X   X  1 =  X  X  X  X  log p ( f | y , X  ) | Hessian  X   X  1 is always positive definite and in practice approximates the truth rather well according to our experiments. If the optimization for f ends up in a saddle point or the mode is very flat,  X   X  1 may be close to singular, which leads to problems in the implementation. In this section, we will discuss these issues with simple examples and address the implementation in the section 4. Consider a single observation y i = 0 from a Student-t distribution with a Gaussian prior for its mean, f i . The behavior of the true posterior, the Laplace approximation, and VB as a function of prior mean are illustrated in the upper plots of the Figure 2. The dotted lines represent the situation, where the observation is a clear outlier in which case the posterior is very close to the prior (cf. section 2). The solid lines represent a situation where the prior and data agree, and the dashed lines represent a situation where the prior and data conflict moderately.
 the prior variance and W ( f i ) is the Hessian of the negative log likelihood at f i (see equations (3) and (4)). With  X  and  X  fixed, W ( f i ) reaches its (negative) minimum at | y i  X  f i | =  X   X  i  X  (  X  + 1) / (8  X  X  2 ) . Therefore, the posterior distribution is unimodal if  X  most problematic situation for the Laplace approximation is when the prior is much wider than the likelihood. Then in the case of a moderate conflict ( | y i  X   X  f i | is close to multimodal (see the Figure 2(a)), meaning that it is unclear whether the observation is an outlier or not. In this case, W ( f i ) is negative and  X   X  1 may be close to zero, which reflects uncertainty on the location. In the implementation this may lead to numerical problems but in practice, the problem becomes concrete only seldom as described in the section 4.
 The negative values of W relate to a decrease in the posterior precision compared to the prior preci-sion. As long as the total precision remains positive it approximates the behavior of the true posterior rather well. The Student-t likelihood leads to a decrease in the variance from prior to posterior only if the prior mean and the observation are consistent with each other as shown in the Figure 2. This behavior is not captured with the factorized VB approximation [7], where W in q ( f |  X , y ) is replaced with a strictly positive diagonal that always increases the precision as illustrated in the Figure 2. 4.1 Posterior mode of the latent variables The mode of the latent variables,  X  f , can be found with general optimization methods such as the scaled conjugate gradients. The most robust and efficient method, however, proved to be the expec-tation maximization (EM) algorithm that utilizes the scale mixture representation of the Student-t distribution where each observation has its own noise variance V i that is Inv- X  2 distributed. Following Gelman et al. [13], p. 456 the E-step of the algorithm consists of evaluating the expectation after which the latent variables are updated in the M-step as where V  X  1 is a diagonal matrix of the expectations in (12). In practice, we do not invert K f , f and, thus,  X  f is updated using the Woodbury-Sherman-Morrison [e.g. 16] lemma where matrix B = I + V  X  1 / 2 K f , f V  X  1 / 2 . This is numerically more stable than directly inverting the covariance matrix, and gives as an intermediate result the vector a = K -1 f , f  X  f for later use. 4.2 Approximate marginal likelihood Rasmussen and Williams [12] discuss a numerically stable formulation to evaluate the approximate marginal likelihood and its gradients with a classification model. Their approach relies on W being non-negative, for which reason it requires some modification for our setting. With the Student-t likelihood, we found the most stable formulation for (6) is where R and L are the Cholesky decomposition of K f , f and  X  = ( K -1 f , f + W )  X  1 , and a is obtained from the EM algorithm. The only problematic term is the last one, which is numerically unstable many cases even worse than the direct evaluation, since W  X  1 might have arbitrary large negative values. For this reason, we evaluate LL T =  X  using a rank one Cholesky updates in a specific order. After L is found it can also be used in the predictive variance (8) and in the gradients of (6) with only minor modification to equations given in [12]. We write first the posterior covariance as where e i is the i th unit vector. The terms e i e T i W ii are added iteratively and the Cholesky decompo-sition of  X  is updated accordingly. At the beginning L = chol ( K f , f ) , and at iteration step i +1 we use the rank one Cholesky update to find which increases the covariance. The increase may be arbitrary large if (  X  ( i ) ii )  X  1  X   X  W ii , but in resulting Cholesky downdate is not positive definite. This should not happen if  X  f is at local maxima, but in practice it may be in a saddle point or this happens because of numerical instability or the iterative framework to update the Cholesky decomposition. The problem is prevented by adding the diagonals in a decreasing order, that is, first the  X  X ormal X  observations and last the outliers. A single Cholesky update is analogous to the discussion in section 3.4 in that the posterior covariance is updated using the result of the previous iteration as a prior. If we added the negative W values large and lead to problems in the later iterations (compare to the dashed black line in the Figure 2(a)). Adding first the largest W we reduce  X  so that negative values of W are less problematic (compare to the dashed black line in the Figure 2(b)), and the updates are numerically more stable. During the Cholesky updates, we cross-check with the condition (  X  ( i ) ii )  X  1 + W ii  X  0 that everything This ensures that the Cholesky update will remain positive definite and doubles the marginal vari-ance instead. However, in practice we never encountered any warnings in our experiments if the hyperparameters were initialized sensibly so that the prior was tight compared to the likelihood. Neal [6] implemented the Student-t model for the Gaussian process via Markov chain Monte Carlo utilizing the scale mixture representation. However, the most similar approaches to the Laplace approximation are the VB approximation [7, 11] and the one in INLA [15]. Here we will shortly summarize them.
 The difference between INLA and GP framework is that INLA utilizes Gaussian Markov random fields (GMRF) in place of the Gaussian process. The Gaussian approximation for p ( f | y , X  ) in INLA is the same as the Laplace approximation here with the covariance function replaced by a precision matrix. Rue et al. [15] derive the approximation for the log marginal posterior, log p (  X  | y ) , from The proportionality sign is due to the fact that the normalization constant for p ( f , X  | y ) is unknown. This is exactly the same as the approximation derived in the section 3.2. Taking the logarithm of (18) we end up in log q (  X  | y )  X  log q ( y |  X  ) + log p (  X  ) , where log q ( y |  X  ) is given in (6). In the variational approximation [7], the joint posterior of the latent variables and the scale param-eters in the scale mixture representation (10)-(11) is approximated with a factorizing distribution where  X   X  = { m , A ,  X   X ,  X   X  2 } are the parameters of the variational approximation. The approximate distributions and the hyperparameters are updated in turns so that  X   X  are updated with current esti-mate for  X  and after that  X  is updated with fixed  X   X  .
 the EM algorithm described in section 4 except that the update of E V  X  1 ii in (12) is replaced with are very similar. In practice, the posterior mode, m , is very close to the mode  X  f , and the main difference between the approximations is in the covariance and the hyperparameter estimates. In the variational approximation  X   X  is searched by maximizing the variational lower bound where we have made visible the implicit dependence of the approximations q ( f ) and q ( V ) to the data and hyperparameters, and included prior for  X  . The variational lower bound is similar to the ap-G 0.393 0.324 0.324 0.230 0.254 0.227 1.249 0.0642 T-lapl 0.028 0.220 0.289 0.231 -2.181 -0.16 0.080 -0.116 T-vb 0.029 0.220 0.294 0.212 -2.228 -0.049 0.091 -0.132
T-mcmc 0.055 0.253 0.287 0.197 -1.907 -0.106 0.029 -0.241 proximate log marginal posterior (18). Only the point estimate  X  f is replaced with averaging over the approximating distribution q ( f , V | y , X  ) . The other difference is that in the Laplace approximation the scale parameters V are marginalized out and it approximates directly p ( f | y , X  ) . We studied four data sets: 1) Neal data [6] with 100 data points and one input shown in Figure 1. 2) Friedman data with a nonlinear function of 10 inputs, from which we generated 10 data sets with 100 training points including 10 randomly selected outliers as described by Kuss [7], p. 83. 3) The Boston housing data that summarize median house prices in Boston metropolitan area for 506 data points and 13 input variables [7]. 4) Concrete data that summarize the quality of concrete casting as a function of 27 variables for 215 measurements [17]. In earlier experiments, the Student-t model has worked better than the Gaussian observation model in all of these data sets.
 The predictive performance is measured with a root mean squared error (RMSE) and a negative log predictive density (NLP). With simulated data these are evaluated for a test set of 1000 latent variables. With real data we use 10-fold cross-validation. The compared observation models are Gaussian (G) and Student-t (T). The Student-t model is inferred using the Laplace approximation (lapl), VB (vb) [7] and full MCMC (mcmc) [6]. The Gaussian observation model, the Laplace approximation and VB are evaluated at  X   X  , and in MCMC we sample  X  . INLA is excluded from the experiments since GMRF model can not be constructed naturally for these non-regularly dis-tributed data sets. The results are summarized in the Table 1. The significance of the differences in performance is approximated using a Gaussian approximation for the distribution of the NLP and RMSE statistics [17]. The Student-t model is significantly better than the Gaussian with higher than 95% probability in all other tests but in the RMSE with the concrete data. There is no significant difference between the Laplace approximation, VB and MCMC.
 The inference time was the shortest with Gaussian observation model and the longest with the Student-t model utilizing full MCMC. The Laplace approximation for the Student-t likelihood took in average 50% more time than the Gaussian model, and VB was in average 8-10 times slower than the Laplace approximation. The reason for this is that in VB two sets of parameters,  X  and  X   X  , are updated in turns, which slows down the convergence of hyperparameters. In the Laplace approx-imation we have to optimize only  X  . Figure 3 shows the mean and the variance of p ( f |  X   X , y ) for MCMC versus the Laplace approximation and VB. The mean of the Laplace approximation and VB match equally well the mean of the MCMC solution, but VB underestimates the variance more than the Laplace approximation (see also the figure 2). In the housing data, both approximations under-estimate the variance remarkably for few data points (40 of 506) that were located as clusters at places where inputs, x are truncated along one or more dimension. At these locations, the marginal posteriors were slightly skew and their tails were rather heavy, and thus a Gaussian approximation presumably underestimates the variance.
 The degrees of freedom of the Student-t likelihood were optimized only in Neal data and Boston housing data using the Laplace approximation. In other data sets, there was not enough information to infer  X  and it was set to 4. Optimizing  X  was more problematic for VB than for the Laplace approximation probably because the factorized approximation makes it harder to identify  X  . The MAP estimates  X   X  found by the Laplace approximation and VB were slightly different. This is reasonable since the optimized functions (18) and (19) are also different. Figure 3: Scatter plot of the posterior mean and variance of the latent variables. Upper row consists means, and lower row variances. In each figure, left plot is for MCMC (x-axis) vs the Laplace approximation (y-axis) and the right plot is MCMC (x-axis) vs. VB (y-axis). In our experiments we found that the predictive performance of both the Laplace approximation and the factorial VB is similar with the full MCMC. Compared to the MCMC the Laplace approximation and VB estimate the posterior mean E [ f |  X   X , y ] similarly but VB underestimates the posterior variance Var [ f |  X   X , y ] more than the Laplace approximation. Optimizing the hyperparameters is clearly faster with the Laplace approximation than with VB.
 Both the Laplace and the VB approximation estimate the posterior precision as a sum of a prior pre-cision and a diagonal matrix. In VB the diagonal is strictly positive, whereas in the Laplace approx-imation the diagonal elements corresponding to outlying observations are negative. The Laplace ap-proximation is closer to the reality in that respect since the outlying observations have a negative ef-which requires that the q ( f , V ) must be close to zero whenever p ( f , V ) is (see for example [18]). Since a posteriori f and V are correlated, the marginal q ( f ) underestimates the effect of marginal-izing over the scale parameters. The Laplace approximation, on the other hand, tries to estimate directly the posterior p ( f ) of the latent variables. Recently, Opper and Archambeau [19] discussed the relation between the Laplace approximation and VB, and proposed a variational approximation directly for the latent variables and tried it with a Cauchy likelihood (they did not perform extensive experiments though). Presumably their implementation would give better estimate for p ( f ) than the factorized approximation. However, experiments on that respect are left for future.
 whereas the Laplace approximation (18) is not. However, the marginal posteriors p ( f | y , X  ) in our experiments (inferred with MCMC) were so close to Gaussian that the Laplace approximation to the truth (see also justifications in [15]).
 In recent years the expectation propagation (EP) algorithm [20] has been demonstrated to be very ac-curate and efficient method for approximate inference in many models with factorizing likelihoods. However, the Student-t likelihood is problematic for EP since it is not log-concave, for which rea-son EPs estimate for the posterior covariance may become singular during the site updates [21]. The reason for this is that the variance parameters of the site approximations may become negative. As demonstrated with Laplace approximation here, this reflects the behavior of the true posterior. We assume that the problem can be overcome, but we are not aware of any work that would have solved this problem.
 Acknowledgments This research was funded by the Academy of Finland, and the Graduate School in Electronics and Telecommunications and Automation (GETA). The first and second author thank also the Finnish Foundation for Economic and Technology Sciences -KAUTE, Finnish Cultural Foundation, Emil Aaltonen Foundation, and Finnish Foundation for Technology Promotion for supporting their post graduate studies. [1] Bruno De Finetti. The Bayesian approach to the rejection of outliers. In Proceedings of [2] A. Philip Dawid. Posterior expectations for large observations. Biometrika , 60(3):664 X 667, [3] Anthony O X  X agan. On outlier rejection phenomena in Bayes inference. Royal Statistical [4] Mike West. Outlier models and prior distributions in Bayesian linear regression. Journal of [5] John Geweke. Bayesian treatment of the independent Student-t linear model. Journal of [6] Radford M. Neal. Monte Carlo Implementation of Gaussian Process Models for Bayesian Re-[7] Malte Kuss. Gaussian Process Models for Robust Regression, Classification, and Reinforce-[8] Paul W. Goldberg, Christopher K.I. Williams, and Christopher M. Bishop. Regression with [9] Andrew Naish-Guzman and Sean Holden. Robust regression with twinned gaussian processes. [10] Oliver Stegle, Sebastian V. Fallert, David J. C. MacKay, and S X ren Brage. Gaussian process [11] Michael E. Tipping and Neil D. Lawrence. Variational inference for Student-t models: Robust [12] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine [13] Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis . [14] Christopher K. I. Williams and David Barber. Bayesian classification with Gaussian processes. [15] H  X  avard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent [16] David A. Harville. Matrix Algebra From a Statistician X  X  Perspective . Springer-Verlag, 1997. [17] Aki Vehtari and Jouko Lampinen. Bayesian model assessment and comparison using cross-[18] Christopher M. Bishop. Pattern Recognition and Machine Learning . Springer Science +Busi-[19] Manfred Opper and C  X  edric Archambeau. The variational Gaussian approximation revisited. [20] Thomas Minka. A family of algorithms for approximate Bayesian inference . PhD thesis, [21] Matthias Seeger. Bayesian inference and optimal design for the sparse linear model. Journal
