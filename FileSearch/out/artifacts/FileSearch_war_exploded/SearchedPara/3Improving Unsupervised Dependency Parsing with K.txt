 XIUMING QIAO, HAILONG CAO, and TIEJUN ZHAO , Harbin Institute of Technology Dependency parsing is the task of analyzing dependency relations ( head  X  dependent ) between words in one sentence. It is widely applied in machine translation [Quirk et al. 2005; Shen et al. 2010], information extraction [Culotta and Sorensen 2004], question answering [Cui et al. 2005; Wang et al. 2007], and so on. Dependency parser can be ei-ther trained in a supervised, semi-supervised, or unsupervised fashion. Supervised de-pendency parsing [McDonald and Pereira 2006; Nivre et al. 2007] and semi-supervised dependency parsing [Koo et al. 2008; Chen et al. 2013] can achieve high performance, but they rely too much on treebank annotations. Treebanks are difficult and expensive to build. What X  X  more, most of the existing treebanks are concentrated on the news domain [Yu et al. 2013]. Therefore, more and more people tend to research unsuper-vised dependency parsing.

Unsupervised dependency parsing is appealing since it does not need annotated tree-bank and can adapt to any domain. However, its accuracy is very low due to lack of sufficient knowledge. Recent researches show that additional knowledge such as punc-tuation [Spitkovsky et al. 2011], word cluster [Spitkovsky et al. 2011], lexical [Headden 2012], reducibility [Mare  X  cek and Straka 2013], and bilingual information [Liu et al. 2013; Ma and Xia 2014] are very effective to improve unsupervised dependency pars-ing. Alternatively, in this article, we propose a simple yet very effective approach to improve unsupervised dependency parsing by making use of query logs, which are available for many languages, even on a large scale. Queries are input by users when interacting with search engines. Although each query is short and contains only few words, the words in a query are not independent with each other. For example, if some-one wants to browse international news, he/she usually enters  X  /international ing to our manual evaluation on the query logs (see Section 3.1), we find that about 76% of the queries contain syntactic structures. These syntactic structures could be natu-rally used as human annotated data to disambiguate for unsupervised dependency parsing. By using the dependency structure  X  /international /news X  as a (soft) constraint, one can easily imagine that  X  /international X  should not be attached into  X  /is X  but be attached into  X  /news X  when parsing the sentence  X  /above /is
In this article, we employ two steps to put the above idea into practice. First, we automatically acquire dependency structures from query logs and define a syntactic relation score model over a pair of dependent words based on their occurrence. Given such a model, even though the exact kind of dependency structures in a query is unclear, we can still know how probable one word depends on another. We make three main contributions in this article:  X  X e publish manually annotated query log data 1 that may be useful for other natural language processing (NLP) tasks such as semantic parsing (Section 3.1).  X  X e show an approach to acquire syntactic knowledge from query logs for dependency parsing (Section 3.2).  X  X e propose Query-Augmented Dependency Model with Valence (QA-DMV) (Section 4), which obtains substantial improvements over the standard dependency model with valence on both the Chinese Penn Treebank and the CoNLL 2007 English task (Section 5). One of the most successful unsupervised dependency parsing models is the DMV, which uses only part-of-speech (POS) tags [Klein and Manning 2004]. It is a generative model in which a dependency tree T is generated from a given sentence S by maximizing the conditional probability P ( T | S ): Since P ( S ) is a constant, our maximization problem can be regarded as a joint inference of P ( T , S ). Moreover, our problem can be reduced to the maximization problem of P ( T ), given P ( T , S ) = P ( T ) P ( S | T ), and S is the leaf string of T .

First, the root of T is selected according to the probability of selecting a position in the rightward direction. Then, its children are generated to the left of the root until we make a decision to stop. Similarly, the children to the right of root are generated until we decide to stop. The process is recursively performed for each generated word until we have generated all the leaves. Whether to generate a child (  X  stop )ornot( stop ) P whether h already has a child in the direction of dir ,and f is the list of children of h in the direction of dir . If stop, then no more children of h in the direction dir will be generated. If not, then a child d is chosen according to the attach-probability P
We denote a subtree of T as D . D has left children deps ( h , l ) and right children computed as follows: The probability of a single tree P tree ( T ) is computed as follows: We assume that queries are not merely short plain texts but contain latent syntactic structures. Our goal is to mine such syntactic knowledge implicated in each query. We use a score function score ( x , y ) to measure the relation between two words x and y occurring in queries. A query is manually input and can reflect a human X  X  consciousness using a few words. Indeed, our preliminary studies indicate that syntactic structures are preserved in many queries, such as predicate-object, subject-predicate, or noun-modifier. For ex-ample,  X  X tex download X  is a predicate-object structure,  X  X ackson dance X  is a subject-predicate structure, and  X  X irthday cakes X  is a noun-modifier structure. These structures may be substructures of many sentences.

We annotate the syntactic structures of 300 queries, and Table I shows the ratio of each relation, where  X  X thers X  denotes those queries do not have syntactic relations. For example, there seem to be no syntactic relations among the words in the query of  X  X ieber twitter. X  Table I indicates that queries that contain syntactic structures account for about 76% of all 300 queries. Query log is rich with syntactic knowledge.
Though we cannot distinguish which dependency relation within queries in unsu-pervised setting, we can get the strength of the syntactic relation between two words through computing their occurrence, and the  X  X trength X  means the size of the proba-bility that one word depends on another word. Let score ( x , y ) denote the strength of the syntactic relation between two words x and y acquired from query logs. count ( x ) denotes the number of times word x occurs in query logs, and count ( xy ) denotes the number 3 of times x and y co-occur in query logs. The score function should satisfy these three constraints: (1) Its value is between 0 ( x and y never co-occur) and 1 ( x and y always co-occur); (2) It is symmetrical for each word; Our definition of score ( x , y ) is similar to pointwise mutual information [Church and Hanks 1990], except that its value is between 0 and 1. The second constraint is set because we find that words in queries are always unordered, especially for the queries that contain only two words. In this extreme case, we can easily predict that two words are more likely a single head-dependent pair, but still we cannot tell which one is a and count ( y ) together.

Under the above constraints, the score of x and y can be computed as follows: Now we will introduce our model, QA-DMV, a query-augmented DMV, and an extension of DMV using the syntactic relationships acquired from query logs. In the original DMV framework, the model incorporates only POS tags and completely ignores lexical features. Our QA-DMV indirectly augments DMV with lexical features using the head-dependent relationship score estimated from query logs through linear interpolation that is similar to that in Headden [2012].
 h , In QA-DMV, the stop-probability is computed based on the POS tags, not on the surface word, as follows [Mare  X  cek and Straka 2013]: where c h is the POS tag of h ,and c f is the corresponding POS tag list of f . frequency of selecting c f as dependents for c h in the direction dir . We smooth it with the parameter 2 / 3 following Mare  X  cek and Straka [2013]. If h has no dependent in the current direction dir ,then adj is 1. Otherwise, if h has one or more dependents, then adj is 0.
 dir, and it is computed as follows in the original framework [Mare  X  cek and Straka 2013]: where count ( c d , c h , dir ) denotes the frequency of choosing c d as a dependent of c h in direction dir ,and count ( c h , dir ) is the frequency of c h being a head in direction dir . Following the suggestions by Mare  X  cek and Straka [2013], the attachment-probability is smoothed by  X  c and | C | where | C | is the number of POS tag categories in the whole corpus, and  X  c is empirically set to 50.

In order to get the relative attach probability model estimated from query logs, we normalize the syntactic relation score in Equation (4) as follows: The model is linearly interpolated with the attachment model in Equation (7) and we where Q is the set of words found in query logs. Note that the syntactic relation model of Equation (8) is selectively interpolated with the original attachment model of Equation (7) using a constant  X  (0  X   X   X  1) in order to avoid over penalties when the words h and d are never observed in Q .When  X  = 0, we uncover the baseline model that does not consider the syntactic relations estimated from query logs. Similarly, when  X  = 1, only the syntactic relations are employed to assign the attachment probabilities when two words are found in Q . The probability for the subtree D ( h ) is computed as follows: P tree ( D ( h )) Finally, P tree ( T ), the probability of the whole tree T is computed in the same way as Equation (3). The probability of a treebank P treebank is computed as the product of the probabilities of all trees in the treebank: Our parser is a variation of the parser in Mare  X  cek and Straka [2013], which differs in the computation methods of stop probability and attach probability. The inference steps of our parser are as follows:  X  X nitialization: A random projective dependency structure is given to each sentence.  X  X ampling: We use Gibbs sampling to sample a dependency tree for each sentence, according to other annotated sentences. We believe that the corpus is large enough to ignore the impact of edges within the same sentence.  X  X ampling is done iteratively until the convergence of P treebank . After the burn-in period (first 500 iterations), we count every edge e in sentences from the 501th iteration to the 1000th iteration and save it in the format of count ( e ), following Mare  X  cek and
Straka [2013].  X  X ecoding: Chu-Liu/Edmond X  X  algorithm [Chu and Liu 1965] is used to decode every sentence.

The count of each edge gained in sampling is used as its weight, that is, count ( e ). The maximum spanning tree we need is the tree that maximize the sum of weights for all e  X  T .
 We use SogouQ query logs (Version 2008) 4 and Baidu query logs (part of queries in a month of 2010) 5 as our Chinese knowledge source. The SogouQ contains 44 million (M) queries of March 2007 [Liu et al. 2011], and the Baidu query logs contain 108M queries.

The above Chinese queries are simplified Chinese texts, so we evaluate our parser on the Penn Chinese Treebank 5 (CTB5). We adopt the data split of Li et al. [2014] and we use Penn2Malt 6 to convert the original constituency trees into dependency trees with its default head rules. Table II shows the data statistics. The coverage of words in Sogou and Baidu queries over CTB5 is 57.48%.

We also conduct experiments on English. AOL query logs 7 are used as English knowl-edge source. This query set contains 36M queries.

CoNLL07 English is the dependency data used in CoNLL 2007 shared task on de-pendency parsing. Its training set is WSJ sections 2-11 of Penn Treebank and the test set is a subset of section 23. The data used in many state-of-the-art related works are CoNLL07 English. To better compare with other works, we use CoNLL07 English to evaluate our parser. The coverage of words in AOL query logs over CoNLL 2007 English is 43.79%.
We process the query logs as follows: Extract user queries and remove other in-formation such as ID, URL and so on; remove queries containing non-Chinese (or non-English) characters; segment queries by Stanford segmenter 8 ; and keep queries that only contain two words. If we have more query logs in other languages, then we will experiment in other datasets of CoNLL 2007 in future work. Our baseline parser is a unsupervised dependency parser 9 with a pure DMV [Mare  X  cek and Straka 2013]. The parser uses the original stop probabilities and the attachment probabilities as described in Equation (6) and Equation (7). This parser uses Gibbs sampling as the training method and samples in many iterations until the convergence of P treebank . But the Gibbs sampler does not always convergent on a similar grammar, so we run each inference 50 times and take the run with the highest P treebank for the evaluation, following Mare  X  cek and Straka [2013]. We evaluate the parser by unlabeled attachment score (UAS): the percentage of words that have correct heads, excluding punctuations. During tuning  X  in Equation (9), we use SogouQ and Baidu query logs as Chinese knowledge source and test on the dev set of CTB5. Word pairs from the preprocessed parser performs best on development data, as shown in Table III. Then we adopt this setting in following evaluations. We evaluate our parser on the test set of CTB5, and the UAS is 46.31%, achieving improvement of 4.1% from the baseline system (DMV). We use syntactic knowledge from AOL queries to improve dependency parsing on English. When we evaluate our parser on the test set of CoNLL07 English, the UAS is 44.38%, winning the baseline system by 8.07%.

Table IV shows the comparison between performance of our parser and previous work on the Chinese Penn Treebank. Our parser is the QA-DMV with the best setting on development set. Klein and Manning [2004] implement an original DMV parser. Liu et al. [2013] use a bilingually guided parsing model. From Table IV, we can see that our parser performs better than other three parsers.

Table V shows the comparison between performance of our parser and previous work on CoNLL07 English test data. Mare  X  cek and Straka [2013] estimate stop probabilities from Wikipedia articles. Mare  X  cek and  X  Zabokrtsk  X  y [2012a] computes reducibility scores from Wikipedia articles. Spitkovsky et al. [2012] use different boundaries to help un-supervised dependency parsing.
 Our parser does not perform better than Mare  X  cek and  X  Zabokrtsk  X  y [2012a] and Mare  X  cek and Straka [2013], which use POS tag reducibility gained from W2C cor-pus of Wikipedia articles [Majli  X  sand  X  Zabokrtsk  X  y 2012]. However, the word coverage of AOL queries on CoNLL07 English is 43.79% and our performance will increase with the wider lexical coverage ratio, as discussed in Section 5.7.

We just use lexical surface features gained from queries rather than POS tags or cluster features, because Barr et al. [2008] points that about 70% of queries are noun phrases. And our experiments, which are not shown due to limited space, prove that the noise of POS tags and cluster information in queries are very big. As an error analysis, we show the parsing result by the baseline parser and our proposed parser in Figures 2 and 3, respectively, for the sentence in Figure 1. Although both results share the same left subtree of  X  /is, X  they differ considerably in the right subtree, denoted by D 1 for the baseline and by D 2 for our proposed method. Our syntactic relation model from the SogouQ and Baidu query logs indicates that P attach ( /international| /is) that is, the word  X  /international X  has a stronger relation with the word  X  /news X  than with the word  X  /is. X  During sampling, the right subtree of  X  /is X  is more likely to be sampled as D 2 rather than D 1 . As a result, our parser tends to get the gold tree. the running time of the baseline parser and our system with the same settings and corpus. We use AOL queries as a knowledge source and CoNLL 2007 English (  X  10) as a test corpus. The number of iterations is set at 1000. The data of CoNLL 2007 English (  X  10) has 18,916 words. The average run time per word of baseline parser and our parser is shown in Table VI. Our proposed method incurs an additional 0.0027s per word to the baseline parser, which is not extremely high. Moreover, we measure the impact of query size on parsing performance. Figure 4 shows the lexical coverage ratio of different size of queries on CoNLL 2007 English (  X  10) data. Each dataset is a subset of larger sets. Figure 5 presents the experimental results on varying the AOL query log data size tested on CoNLL 2007 English (  X  10) data. The plot clearly shows that more query logs are helpful for better unsupervised parsing performance. Previous work on unsupervised dependency parsing extends DMV by adding additional knowledge. Spitkovsky et al. [2010b] acquires natural annotations from web structure data (anchors, bold, italics, and underlines) and applies this information into decod-ing. The reducibility feature of one word [Mare  X  cek and  X  Zabokrtsk  X  y 2012b] or a word sequence [Mare  X  cek and  X  Zabokrtsk  X  y 2012a; Mare  X  cek and Straka 2013] is used in DMV and leads to many improvements on many languages. We propose to augment DMV with the syntactic knowledge from query logs. We assume that the words in each query are not independent with each other but contain latent structures that could be useful as a clue to find head-dependent relationship among texts.

Multilingual information [Cohen et al. 2011; S X gaard 2011] also works in unsu-pervised dependency parsing. Naseem et al. [2012] uses annotations from a diverse set of source languages, performing well in multi-source transfer-based dependency parsing. Liu et al. [2013] utilize information from both sides of bilingual corpus, out-performing previous bilingual-guided unsupervised models. Ma and Xia [2014] train parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich languages with entropy regularization. The resources they use are limited, but the amount of query logs we can use is huge, because users will input more than 1 million queries everyday through the Sogou search engine. And there are many other search engines. Then we can cover more words and have a more accurate parser.
Much more information can be used to improve unsupervised dependency parsing, such as cluster information [Spitkovsky et al. 2011], lexicals [Headden 2012], punctu-ations [Spitkovsky et al. 2011], and sentence boundaries [Spitkovsky et al. 2012]. Moreover, many researchers are devoted to improving the training method in DMV. Klein and Manning [2004] use an inside-outside re-estimation method to learn the grammar without any smoothing. Headden et al. [2009] adds smoothing into DMV with rich context information. Spitkovsky et al. [2010] combines  X  X aby Steps X  and  X  X ess is More. X  Spitkovsky et al. [2010a] uses Viterbi EM to learn grammar, performing better in long sentences than classic EM.

Though simple, query logs contain many kinds of knowledge. Tannebaum and Rauber [2012] acquire lexical knowledge from query logs to help query expansion in patent searching. Li [2010] improves query understanding using its lexical features, syntactic features, and semantic features. Sekine and Suzuki [2007] extract ontological knowl-edge using search query logs. Tur et al. [2011] exploit user queries mined from search engine query click logs to bootstrap or improve slot filling models for spoken language understanding. To the best of our knowledge, our method is the first to incorporate syntactic knowledge from query logs into unsupervised dependency parsing. In this article, we extracted syntactic knowledge from query logs to help estimate the attach probability in DMV. We presented significant improvements on Chinese and English unsupervised parsing task and also demonstrated that more queries can lead to better performance.

In the future, we will apply knowledge from query logs in other formats to further improve dependency parsing. Moreover, dependency parsing on queries is very impor-tant for information retrieval and its accuracy is rather low now. We will pay much more attention to query-dependency parsing.

