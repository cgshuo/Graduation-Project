 Discriminative machine learning methods have led to better solutions for many problems in natu-ral language processing (NLP), such as various kinds of sequence labeling. However, only limited advances have been made on NLP problems involving tree-structured prediction. State of the art methods for both parsing and translation use discriminative methods, but they are still limited by their reliance on generative models that can be estimated relatively cheaply. For example, some parsers and translators use a generative model to generate a list of candidates, and then rerank them using a discriminative reranker (e.g., Henderson, 2004; Charniak &amp; Johnson, 2005; Cowan et al., 2006). Others use a generative model as a feature in a discriminative framework, because other-wise training is impractically slow (Collins &amp; Roark, 2004; Taskar et al., 2004; Riezler &amp; Maxwell, 2006). Similarly, the best machine translation (MT) systems use discriminative methods only to cal-ibrate the weights of a handful of di ff erent knowledge sources, which are either enumerated by hand or learned automatically but not discriminatively (e.g., Chiang, 2005). The problem with generative models is that they are typically not regularized in a principled way, and it is di ffi cult to make up for their unregularized risk post-hoc. It is also di ffi cult to come up with a generative model for certain kinds of data, especially the kind used to train MT systems, so approaches that rely on generative models are hard to adapt.
 This paper proposes a discriminative learning method that can scale up to large structured prediction problems, without using generative models in any way. The proposed method employs the traditional AI technique of predicting a structure by searching over possible sequences of inferences, where each inference predicts a part of the eventual structure. However, unlike most approaches employed in NLP, the proposed method makes no independence assumptions: The function that evaluates each inference can use arbitrary information not only from the input, but also from all previous inferences. Let us define some terms to help explain how our algorithm predicts a tree. An item is a node in the tree. Every state in the search space consists of a set of items, representing nodes that have been inference is a (state, item) pair, i.e. a state and an item to be added to it. Each inference represents a transition from one state to another. A state is correct if it is possible to infer zero or more items to obtain the final state that corresponds to the training data tree. Similarly, an inference is correct if it leads to a correct state.
 Given input s , the inference engine searches the possible complete trees T ( s ) for the tree  X  t  X  T ( s ) that has minimum cost C  X  ( t ) under model  X  : i . The number of states in the search space is exponential in the size of the input. The freedom to compute c  X  ( i ) using arbitrary non-local information from anywhere in inference i  X  X  state precludes exact solutions by ordinary dynamic programming. We know of two e ff ective ways to approach such large search problems. The first, which we use for our parsing experiments, is to severely restrict the order in which items can be inferred. The second, which we use for translation, is to make the simplifying assumption that the cost of adding a given item to a state is the same for all states. Under this assumption, the fraction of any state X  X  cost due to a particular item can be computed just once per item, instead of once per state. However, in contrast to traditional context-free parsing algorithms, that computation can involve context-sensitive features.
 set. Given the typically very large number of possible features, the learning method must satisfy two criteria. First, it must be able to learn e ff ectively even if the number of irrelevant features is exponential in the number of examples. It is too time-consuming to manually figure out the right feature set for such problems. Second, the learned function must be sparse. Otherwise, it would be too large for the memory of an ordinary computer, and therefore impractical.
 Section 2 presents an algorithm that satisfies these criteria. This algorithm is in the family that has been shown to converge to an ` 1 -optimal separating hyperplane, which maximizes the minimum ` -margin on separable training data (Rosset et al., 2004). Sections 3 and 4 present experiments on parsing and translation, respectively, illustrating the advantages of this algorithm. For lack of space, the experiments are described tersely; for details see Turian and Melamed (2006a) and Wellington et al. (2006). Also, Turian and Melamed (2006b) show how to reduce training time. 2.1 The Training Set gold-standard trees are used to generate training examples, each of which is a candidate inference: Starting at the initial state, we randomly choose a sequence of correct inferences that lead to the (gold-standard) final state. All the candidate inferences that can possibly follow each state in this sequence become part of the training set. The vast majority of these inferences will lead to incorrect states, which makes them negative examples. An advantage of this method of generating training examples is that it does not require a working inference engine and can be run prior to any training. A disadvantage of this approach is that it does not teach the model to recover from mistakes. We conjecture this this approach is not subject to label bias because states can  X  X ampen X  the mass they receive, as recommended by La ff erty et al. (2001).
 might be more important than others, so each is given a bias b ( i )  X  R + . By default, all b ( i ) = 1. A priori , we define only a set A of simple atomic features (described later). The learner then induces compound features, each of which is a conjunction of possibly negated atomic features. Each atomic feature can have one of three values (yes / no / don X  X  care), so the size of the compound feature space is 3 | A | , exponential in the number of atomic features. In our experiments, it was also exponential in the number of training examples, because | A |  X  | I | . For this reason, we expect that the number of irrelevant (compound) features is exponential in the number of training examples. 2.2 Objective Function The training method induces a real-valued inference evaluation function h  X  ( i ). In the present work, h
 X  is a linear model parameterized by a real vector  X  , which has one entry for each feature f : The sign of h  X  ( i ) predicts the y -value of i and the magnitude gives the confidence in this prediction. The training procedure adjusts  X  to minimize the expected risk R  X  over training set I . R  X  is the objective function, which is the sum of loss function L  X  and regularization term  X   X  . We use the log-loss and ` 1 regularization, so we have  X  of example i . The tree cost C  X  (Equation 1) is obtained by computing the objective function with y ( i ) =+ 1 and b ( i ) = 1 for every inference in the tree, and treating the penalty term  X   X  as constant. I.e., c  X  ( i ) = ln(1 + exp(  X  h  X  ( i ))).
 This choice of objective function was motivated by Ng (2004), who showed that it is possible to achieve sample complexity that is logarithmic in the number of irrelevant features by minimizing the ` 1 -regularized log-loss. On the other hand, Ng showed that most other discriminative learning algorithms used for structured prediction in NLP will overfit in this setting, including: the perceptron algorithm, unregularized logistic regression, logistic regression with an ` 2 penalty (a Gaussian prior), SVMs using most kernels, and neural nets trained by back-propagation. 2.3 Boosting ` 1 -Regularized Decision Trees We use an ensemble of confidence-rated decision trees (Schapire &amp; Singer, 1999) to represent h  X  . 3 Each internal node is split on an atomic feature. The path from the root to each node n in a decision tree corresponds to a compound feature f , and we write  X  ( n ) = f . An inference i percolates down to i using a decision tree, we percolate the inference down to a leaf n and return confidence  X   X  ( n ) . The score h  X  ( i ) given to an inference i by the whole ensemble is the sum of the confidences returned by all trees in the ensemble.
 Listing 1 Outline of training algorithm. procedure T  X  X  X  X  X  X  ( I ) Listing 1 presents our training algorithm. At the beginning of training, the ensemble is empty,  X = 0 , and  X  is set to  X  . We grow the ensemble until the objective cannot be further reduced for the current choice of  X  . We then relax the regularization penalty by decreasing  X  and continue training. In this way, instead of choosing the best  X  heuristically, we can optimize it during a single training run. Each invocation of M  X  X  X  X  T  X  X  X  X  has several steps. First, we choose some compound features that will allow us to decrease the objective function. We do this by building a decision tree, whose leaf node paths represent the chosen compound features. Second, we confidence-rate each leaf to minimize the objective over the examples that percolate down to that leaf. Finally, we append the decision tree to the ensemble and update parameter vector  X  accordingly. In this manner, compound feature selection is performed incrementally during training, as opposed to a priori .
 Our strategy for feature selection is a variant of steepest descent (Perkins et al., 2003), extended to work over the compound feature space. The construction of each decision tree begins with a root node, which corresponds to a dummy  X  X lways true X  feature. To avoid the discontinuity at  X  f = 0 of the gradient of the regularization term in the objective (Equation 3), we define the gain of feature f as: The gain function indicates how the polyhedral structure of the ` 1 norm tends to keep the model exceeds the penalty term  X  , the gain is zero and the objective cannot be reduced by adjusting param-eter  X  f away from zero. However, if the gain is non-zero, G  X  ( I ; f ) is the magnitude of the gradient of the objective as we adjust  X  f in the direction that reduces R  X  . Let us define the weight of an example i under the current model as the rate at which loss decreases as the margin of i increases: Now, to compute the gain (Equation 4), we note that:  X  L  X  ( I ) We recursively split leaf nodes by choosing the best atomic splitting feature that will allow us to increase the gain. Specifically, we consider splitting each leaf node n using atomic feature  X  a , where split node n using  X  a only if the total gain of these two children exceeds the gain of the unsplit node, i.e. if: Otherwise, n remains a leaf node of the decision tree, and  X   X  ( n ) becomes one of the values to be optimized during the parameter update step.
 Parameter update is done sequentially on only the most recently added compound features, which correspond to the leaves of the new decision tree. After the entire tree is built, we percolate each example down to its appropriate leaf node. A convenient property of decision trees is that the leaves X  compound features are mutually exclusive, so their parameters can be directly optimized indepen-dently of each other. We use a line search to choose for each leaf node n the parameter  X   X  ( n ) that minimizes the objective over the examples in n . The parsing algorithm starts from an initial state that contains one terminal item per input word, labeled with a part-of-speech (POS) tag by the method of Ratnaparkhi (1996). For simplicity and e ffi ciency, we impose a (deterministic) bottom-up right-to-left order for adding items to a state. The resulting search space is still exponential, and one might worry about search errors. However, in our experiments, the inference evaluation function was learned accurately enough to guide the parser to the optimal parse reasonably quickly without pruning, and thus without search errors.
 Following Taskar et al. (2004), we trained and tested a parser using the algorithm in Section 2 on  X  15 word sentences from the English Penn Treebank (Taylor et al., 2003). We used sections 02 X 21 Table 1 Accuracy on the English Penn Treebank, training and testing on sentences of  X  15 words. for training, section 22 for development, and section 23 for testing. There were 40 million training inferences. Turian and Melamed (2005) observed that uniform example biases b ( i ) produced lower accuracy as training progressed, because the model minimized the error per example . To minimize the error per state , we assigned every training state equal value and shared half the value uniformly among negative examples generated from that state and gave the other half to the positive examples. Our atomic feature set A contained features of the form  X  X s there an item in group J whose la-bel / headword / headtag / headtagclass is X? X . Possible values of X for each predicate were collected from the training data. Some examples of possible values for J are the last n child items, the first n left-context items, all right-context items, and the terminal items dominated by the non-head child items. These feature templates gave rise to 1.1 million di ff erent atomic features. Significantly smaller feature sets lowered accuracy on the development set.
 To situate our results in the literature, we compared them to those reported by Taskar et al. (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested conditions, including the use of POS tags from Ratnaparkhi (1996). The comparison was in terms of the standard PARSEVAL measures (Black et al., 1991): labeled precision, labeled recall, and labeled F-measure, which are based on the number of non-terminal items in the parser X  X  output that match those in the gold-standard parse. Table 1 shows the results of these four parsers on the test set. The accuracy of our parser is at least as high as that of comparable parsers in the literature. An advantage of our choice of loss function is that each of the binary classifiers can be learned independently of the others. We parallelized training by inducing 26 separate classifiers, one for each non-terminal label in the Penn Treebank. It took less than five CPU-days to build each of the ensembles used at test time by the final parser. By comparison, it took several CPU-months to train the parser of Taskar et al. (2004) (Dan Klein, p.c.). The experiments in this section employed the tree transduction approach to translation, which is used by today X  X  best MT systems (Marcu et al., 2006). To translate by tree transduction, we assume that the input sentence has already been parsed by a parser like the one described in Section 3. The transduction algorithm performs a sequence of inferences to transform this input parse tree into an output parse tree, which has words of the target language in its leaves, often in a di ff erent order than the corresponding words in the source tree. The words are then read o ff the target tree and outputted; the rest of the tree is discarded. Inferences are ordered by their cost, just like in ordinary parsing, and tree transduction stops when each source node has been transduced.
 The data for our experiments came from the English and French components of the EuroParl corpus (Koehn, 2005). From this corpus, we extracted sentence pairs where both sentences had between 5 and 40 words, and where the ratio of their lengths was no more than 2:1. We then extracted dis-joint training, tuning, development, and test sets. The tuning, development, and test sets were 1000 sentence pairs each. Typical MT systems in the literature are trained on hundreds of thousands of sentence pairs, so our main experiment used 100K sentence pairs of training data. Where noted, preliminary experiments were performed using 10K sentence pairs of training data. We computed parse trees for all the English sentences in all data sets. For each of our two training sets, we induced word alignments using the default configuration of GIZA ++ (Och &amp; Ney, 2003). The training set word alignments and English parse trees were fed into the default French-English hierarchical align-ment algorithm distributed with the GenPar system (Burbank et al., 2005) to produce binarized tree alignments. Tree alignments are the ideal form of training data for tree transducers, because they fully specify the relation between nodes in the source tree and nodes in the target tree. We experimented with a simplistic tree transducer that involves only two types of inferences. The first type transduces words at the leaves of the source tree; the second type transduces internal nodes. To transduce a word w at the leaf, the transducer replaces it with a single word v that is a translation of w . v can be empty ( X  X ULL X ). Leaves that are transduced to NULL are deterministically erased. Internal nodes are transduced merely by permuting the order of their children, where one of the pos-sible permutations is to retain the original order. E.g., for a node with two children, the permutation classifier predicts either (1,2) or (2,1). This transducer is grossly inadequate for modeling real trans-lations (Galley et al., 2004): It cannot account for many kinds of noise nor for many real translingual phenomena, such as head-switching and discontinuous constituents, which are important for accu-rate MT. It cannot even capture common  X  X hrasal X  translations such as English there is to French il y a . However, it is su ffi cient for controlled comparison of learning methods. One could apply the same learning methods to more sophisticated tree transducers.
 When inducing leaf transducers using 10K training sentence pairs, there were 819K training infer-ences and 80.9K tuning inferences. For 100K training sentence pairs, there were 36.8M and 375K, respectively. And for inducing internal node transducers using 100K training sentence pairs, there were 1.0M and 9.2K, respectively. 362K leaf transduction inferences were used for development. We parallelized training of the word transducers according to the source and target word pair ( w , v ). filtering, we induced 11.6K di ff erent word transducers over 10K training sentence pairs, and 41.3K over 100K sentence pairs.
 We used several kinds of features to evaluate leaf transductions.  X  X indow X  features included the source words and part-of-speech (POS) tags within a 2-word window around the word in the leaf (the  X  X ocus X  word), along with their relative positions (from -2 to + 2).  X  X o-occurrence X  features included all words and POS tags from the whole source sentence, without position information.  X  X ependency X  features were compiled from the automatically generated English parse trees. The literature on monolingual parsing gives a standard procedure for annotating each node in an English parse tree with its  X  X exical head word. X  The dependency features of each word were the label of its and lexical head of all dependents of the maximal projection, and all the labels of all head-children, recursively, of the maximal projection. The features used to evaluate transductions of internal nodes included all those listed for leaf transduction above, where the focus words were the head words of the children of the internal node. Using these features, we applied the method of Section 2 to induce confidence-rating binary classifiers for each word pair in the lexicon, and additional binary classifiers for predicting the permutations of the children of internal tree nodes.
 Before attempting the whole transduction task, we compared the model of Section 2 with the model of Vickrey et al. (2005), which learned word transduction classifiers using logistic regression with ` regularization. The ` 2 parameters were optimized using the conjugate gradient implementation of Daum  X  e (2004). We induced word transduction classifiers over the 10K training data using this model and our own, and tested them on the development set. The accuracy of the two models was statistically indistinguishable (about 54%). However, the models were vastly di ff erent in their size. The boosted decision trees had a total of about 38.7K non-zero compound features over an even smaller number of atomic features. In contrast, the ` 2 -regularized model had about 6.5 million non-zero features X  X n increase of more than two orders of magnitude.
 We estimated that, to scale up to training data sizes typically used by modern statistical MT systems, the ` 2 classifiers would not fit in memory. To make them fit, we set all but the heaviest feature weights to zero. The number of features allowed to remain active in the ` 2 classifier was the number of active features in the ` 1 classifier. With the playing field leveled, the accuracy of the ` 2 classifiers was only Table 2 Accuracy of tree transducers using 100K sentence pairs of training data. 45%, even worse than the baseline accuracy of 48% obtained by always predicting the most common translation.
 In the main experiment, we compared two models of the inference cost function c  X   X  X ne generative and one discriminative. The generative model was a top-down tree transducer (Comon et al., 1997), which stochastically generates the target tree top-down given the source tree. Under this model, the loss of an inference i is the negative log-probability of the node n ( i ) that it infers. We estimated the parameters of this transducer using the Viterbi approximation to the inside-outside algorithm described by Graehl and Knight (2004). We lexicalized the nodes so that their probabilities could capture bilexical dependencies. Our hypothesis was that the discriminative approach would be more accurate than the generative model, because its evaluation of each inference could take into account a greater variety of information in the tree, including its entire yield (string), not just the information in nearby nodes.
 We used the second search technique described in Section 1 to find the minimum cost target tree. For e ffi ciency, we used a chart to keep track of item costs, and pruned items whose cost was more whenever the number of items in the same cell exceeded 40. Our entire tree transduction algorithm was equivalent to bottom-up synchronous parsing (Melamed, 2004) where the source side of the output bi-tree is constrained by the input (source) tree.
 We compared the generative and discriminative models by reading out the string encoded in their predicted trees, and computing the F-measure between that string and the reference target sentence in the test corpus. Turian et al. (2003) show how to compute precision, recall, and the F-measure over pairs of strings without double-counting. Their family of measures is parameterized by an ex-ponent. With the exponent set to 1.0, the F-measure is essentially the unigram overlap ratio. With the exponent set to 2.0, the F-measure rewards longer n -gram matches without double-counting. The generative transducer achieved its highest F-measure when the input parse trees were computed by the generative parser of Bikel (2004). The discriminatively trained transducer was most accurate when the source trees were computed by the parser in Section 3. Table 2 shows the results X  X he dis-criminatively trained transducer was much more accurate on all measures, at a statistical significance level of 0.001 using the Wilcoxon signed ranks test.
 We have demonstrated how to predict tree structures using binary classifiers. These classifiers are discriminatively induced by boosting confidence-rated decision trees to minimize the ` 1 -regularized log-loss. For large problems in tree-structured prediction, such as natural language parsing and trans-lation, this learning algorithm has several attractive properties. It learned a purely discriminative machine over 40 million training examples and 1.1 million atomic features, using no generative model of any kind. The method did not require a great deal of feature engineering a priori , because it performed feature selection over a compound feature space as it learned. To our knowledge, this is the first purely discriminatively trained constituent parser that surpasses a generative baseline, as well as the first published method for purely discriminative training of a syntax-driven MT system that makes no use of generative translation models, either in training or translation. In future work, we plan to integrate the parsing and translation methods described in our experiments, to reduce compounded error.
 The authors would like to thank L  X  eon Bottou, Patrick Ha ff ner, Fernando Pereira, Cynthia Rudin, and the anonymous reviewers for their helpful comments and constructive criticism. This research was sponsored by NSF grants #0238406 and #0415933.

