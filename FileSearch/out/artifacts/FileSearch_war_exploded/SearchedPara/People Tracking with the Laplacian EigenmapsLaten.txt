 Recent research in reconstructing articulated human motio n has focused on methods that can exploit available prior knowledge on typical human poses or motions in an attempt to build more reliable algorithms. The high-dimensionality of human ambient pose space X  X etween 30-60 joint angles or joint positions depending on the desired accuracy level, makes exhaustive search prohibitively expensive. This has negative impact on existing trackers, w hich are often not sufficiently reliable at reconstructing human-like poses, self-initializing or re covering from failure. Such difficulties have stimulated research in algorithms and models that reduce th e effective working space, either us-ing generic search focusing methods (annealing, state spac e decomposition, covariance scaling) or by exploiting specific problem structure (e.g. kinematic ju mps). Experience with these procedures has nevertheless shown that any search strategy, no matter h ow effective, can be made significantly more reliable if restricted to low-dimensional state space s. This permits a more thorough explo-ration of the typical solution space, for a given, comparati vely similar computational effort as a high-dimensional method. The argument correlates well wit h the belief that the human pose space, although high-dimensional in its natural ambient paramete rization, has a significantly lower percep-tual (latent or intrinsic) dimensionality, at least in a pra ctical sense X  X any poses that are possible are so improbable in many real-world situations that it pays off to encode them with low accuracy. A perceptual representation has to be powerful enough to cap ture the diversity of human poses in a sufficiently broad domain of applicability (the task domain ), yet compact and analytically tractable for search and optimization. This justifies the use of models that are nonlinear and low-dimensional (able to unfold highly nonlinear manifolds with low distort ion), yet probabilistically motivated and globally continuous for efficient optimization. Reducing d imensionality is not the only goal: per-ceptual representations have to preserve critical propert ies of the ambient space. Reliable tracking needs locality: nearby regions in ambient space have to be mapped to nearby regions in latent space. in latent space in order to follow smooth trajectories in the joint angle ambient space. In this paper we propose to model priors for articulated moti on using a recently introduced proba-bilistic dimensionality reduction method, the Laplacian E igenmaps Latent Variable Model (LELVM) [1]. Section 1 discusses the requirements of priors for arti culated motion in the context of proba-bilistic and spectral methods for manifold learning, and se ction 2 describes LELVM and shows how it combines both types of methods in a principled way. Sectio n 3 describes our tracking frame-work (using a particle filter) and section 4 shows experiment s with synthetic and real human motion sequences using LELVM priors learned from motion-capture d ata.
 Related work: There is significant work in human tracking, using both gener ative and discrimina-tive methods. Due to space limitations, we will focus on the m ore restricted class of 3D generative pact prior representations for tracking people or other art iculated objects is an active research field, steadily growing with the increased availability of human m otion capture data. Howe et al. and Sidenbladh et al. [2] propose Gaussian mixture representat ions of short human motion fragments (snippets) and integrate them in a Bayesian MAP estimation f ramework that uses 2D human joint measurements, independently tracked by scaled prismatic m odels [3]. Brand [4] models the human pose manifold using a Gaussian mixture and uses an HMM to infe r the mixture component index based on a temporal sequence of human silhouettes. Sidenbla dh et al. [5] use similar dynamic priors and exploit ideas in texture synthesis X  X fficient nearest-ne ighbor search for similar motion frag-ments at runtime X  X n order to build a particle-filter tracker w ith observation model based on contour and image intensity measurements. Sminchisescu and Jepson [6] propose a low-dimensional proba-bilistic model based on fitting a parametric reconstruction mapping (sparse radial basis function) and a parametric latent density (Gaussian mixture) to the embed ding produced with a spectral method. They track humans walking and involved in conversations usi ng a Bayesian multiple hypotheses framework that fuses contour and intensity measurements. U rtasun et al. [7] use a dynamic MAP estimation framework based on a GPLVM and 2D human joint corr espondences obtained from an independent image-based tracker. Li et al. [8] use a coordin ated mixture of factor analyzers within a particle filtering framework, in order to reconstruct human motion in multiple views using chamfer matching to score different configuration. Wang et al. [9] le arn a latent space with associated dy-namics where both the dynamics and observation mapping are G aussian processes, and Urtasun et al. [10] use it for tracking. Taylor et al. [11] also learn a bi nary latent space with dynamics (using an energy-based model) but apply it to synthesis, not tracki ng. Our work learns a static, generative low-dimensional model of poses and integrates it into a part icle filter for tracking. We show its ability to work with real or partially missing data and to tra ck multiple activities. We consider the problem of learning a probabilistic low-dim ensional model of human articulated motion. Call y  X  R D the representation in ambient space of the articulated pose of a person. In this paper, y contains the 3D locations of anywhere between 10 and 60 marke rs located on the person X  X  normalised for translation and rotation in order to remove r igid motion and leave only the articulated motion (see section 3 for how we track the rigid motion). While y is high-dimensional, the motion pattern lives in a low-dimensional manifold because most va lues of y yield poses that violate body constraints or are simply atypical for the motion type consi dered. Thus we want to model y in terms of a small number of latent variables x given a collection of poses { y with motion-capture technology). The model should satisfy the following: (1) It should define a probability density for x and y , to be able to deal with noise (in the image or marker measurem ents) and uncertainty (from missing data due to occlusion or marke rs that drop), and to allow integration in a sequential Bayesian estimation framework. The density model should also be flexible enough to represent multimodal densities. (2) It should define mapp ings for dimensionality reduction F : y  X  x and reconstruction f : x  X  y that apply to any value of x and y (not just those in the training set); and such mappings should be defined on a global coordinate system, be continuous (to avoid physically impossible discontinuities) and diff erentiable (to allow efficient optimisation when tracking), yet flexible enough to represent the highly n onlinear manifold of articulated poses. (LVMs) do; for example, factor analysis defines linear mappi ngs and Gaussian densities, while the generative topographic mapping (GTM; [12]) defines nonline ar mappings and a Gaussian-mixture density in ambient space. However, factor analysis is too li mited to be of practical use, and GTM X  while flexible X  X as two important practical problems: (1) the latent space must be discretised to allow tractable learning and inference, which limits it to v ery low (2 X 3) latent dimensions; (2) the parameter estimation is prone to bad local optima that resul t in highly distorted mappings. Another dimensionality reduction method recently introdu ced, GPLVM [13], which uses a Gauss-ian process mapping f ( x ) , partly improves this situation by defining a tunable parame ter x each data point y for { x GPLVM for tracking human motion [7]. However, GPLVM has some disadvantages: its training is very costly (each step of the gradient iteration is cubic on t he number of training points N , though approximations based on using few points exist); unlike tru e LVMs, it defines neither a posterior representation it obtains is not ideal. For example, for per iodic motions such as running or walking, repeated periods (identical up to small noise) can be mapped apart from each other in latent space because nothing constrains x There exists a different type of dimensionality reduction m ethods, spectral methods (such as Isomap, LLE or Laplacian eigenmaps [14]), that have advantages and d isadvantages complementary to those of LVMs. They define neither mappings nor densities but just a correspondence ( x points in latent space x value problem) and has no local optima, and often yields a cor respondence that successfully models highly nonlinear, convoluted manifolds such as the Swiss ro ll. While these attractive properties have spurred recent research in spectral methods, their lack of m appings and densities has limited their applicability in people tracking. However, a new model that combines the advantages of LVMs and spectral methods in a principled way has been recently propo sed [1], which we briefly describe next. LELVM is based on a natural way of defining an out-of-sample ma pping for Laplacian eigenmaps (LE) which, in addition, results in a density model. In LE, ty pically we first define a k -nearest-neighbour graph on the sample data { y function K ( y where we define the matrix X gree matrix D = diag ( P N The constraints eliminate the two trivial solutions X = 0 (by fixing an arbitrary scale) and x 1 = = x N The solution is given by the leading u lated, rotated or uniformly scaled X is equally valid).
 Following [1], we now define an out-of-sample mapping F ( y ) = x for a new point y as a semi-supervised learning problem, by recomputing the embedding as in (1) (i.e., augmenting the graph Laplacian with the new point), but keeping the old embedding fixed: where K the Gaussian affinity (applied only to the k nearest neighbours of y , i.e., K This is one natural way of adding a new point to the embedding b y keeping existing embedded points fixed. We need not use the constraints from (1) because they would trivially determine x , and the uninteresting solutions X = 0 and X = constant were already removed in the old embedding anyway. The solution yields an out-of-sample dimensionali ty reduction mapping x = F ( y ) : applicable to any point y (new or old). This mapping is formally identical to a Nadaray a-Watson estimator (kernel regression; [15]) using as data { ( x a step further by defining a LVM that has as joint distribution a kernel density estimate (KDE): where K the marginals in observed and latent space are also KDEs, and the dimensionality reduction and We allow the bandwidths to be different in the latent and ambi ent spaces: K x ( x , x n )  X  exp (  X  1 2 k ( x  X  x n ) / X  x k may be tuned to control the smoothness of the mappings and den sities [1].
 Thus, LELVM naturally extends a LE embedding (efficiently ob tained as a sparse eigenvalue prob-lem with a cost O ( N 2 ) ) to global, continuous, differentiable mappings (NW estim ators) and po-tentially multimodal densities having the form of a Gaussia n KDE. This allows easy computation of posterior probabilities such as p ( x | y ) (unlike GPLVM). It can use a continuous latent space of arbitrary dimension L (unlike GTM) by simply choosing L eigenvectors in the LE embedding. It has no local optima since it is based on the LE embedding. LELV M can learn convoluted mappings (e.g. the Swiss roll) and define maps and densities for them [1 ]. The only parameters to set are the graph parameters (number of neighbours k , affinity width  X  ) and the smoothing bandwidths  X  We follow the sequential Bayesian estimation framework, wh ere for state variables s and observation variables z we have the recursive prediction and correction equations: We define the state variables as s = ( x , d ) where x  X  R L is the low-dim. latent space (for pose) the orientation of the body, but for simplicity here we descr ibe only the translation). The observed variables z consist of image features or the perspective projection of t he markers on the camera plane. The mapping from state to observations is (for the mar kers X  case, assuming M markers): where f is the LELVM reconstruction mapping (learnt from mocap data );  X  shifts each 3D marker by d ; and P is the perspective projection (pinhole camera), applied to each 3D point separately. Here we use a simple observation model p ( z and isotropic covariance (set by the user to control the influ ence of measurements in the tracking). We assume known correspondences and observations that are o btained either from the 3D markers (for tracking synthetic data) or 2D tracks obtained from a 2D tracker. Our dynamics model is where both dynamics models for d and x are random walks: Gaussians centred at the previous step value d influence of dynamics in the tracking); and p ( x predicts states that are both near the previous state and yie ld feasible poses. Of course, more complex dynamics models could be used if e.g. the speed and direction of movement are known.
 As tracker we use the Gaussian mixture Sigma-point particle filter (GMSPPF) [16]. This is a par-ticle filter that uses a Gaussian mixture representation for the posterior distribution in state space and updates it with a Sigma-point Kalman filter. This Gaussia n mixture will be used as proposal distribution to draw the particles. As in other particle filt er implementations, the prediction step is carried out by approximating the integral (5) with partic les and updating the particles X  weights. Then, a new Gaussian mixture is fitted with a weighted EM algor ithm to these particles. This re-places the resampling stage needed by many particle filters a nd mitigates the problem of sample depletion while also preventing the number of components in the Gaussian mixture from growing over time. The choice of this particular tracker is not criti cal; we use it to illustrate the fact that LELVM can be introduced in any probabilistic tracker for non linear, nongaussian models. Given the corrected distribution p ( s possible to choose instead the mode closest to the state at t  X  1 , which could be found by mean-shift or Newton algorithms [17] since we are using a Gaussian-mixt ure representation in state space. We demonstrate our low-dimensional tracker on image sequen ces of people walking and running, both synthetic (fig. 1) and real (fig. 2 X 3). Fig. 1 shows the mod el copes well with persistent partial occlusion and severely subsampled training data ( A , B ), and quantitatively evaluates temporal recon-struction ( C ). For all our experiments, the LELVM parameters (number of n eighbors k , Gaussian affinity  X  , and bandwidths  X  (for pose, plus 6D for rigid motion), which were expressive e nough for our experiments. More complex, higher-dimensional models are straightforward t o construct. The initial state distribution p ( s 0 ) was chosen a broad Gaussian, the dynamics and observation co variance were set manually to control the tracking smoothness, and the GMSPPF tracker use d a 5-component Gaussian mixture in latent space (and in the state space of rigid motion) and a s mall set of 500 particles. The 3D representation we use is a 102-D vector obtained by concaten ating the 3D markers coordinates of all the body joints. These would be highly unconstrained if esti mated independently, but we only use them as intermediate representation; tracking actually oc curs in the latent space, tightly controlled using the LELVM prior. For the synthetic experiments and som e of the real experiments (figs. 2 X 3) the camera parameters and the body proportions were known (f or the latter, we used the 2D outputs of [6]). For the CMU mocap video (fig. 2 B ) we roughly guessed. We used mocap data from several sources (CMU, OSU). As observations we always use 2D marker p ositions, which, depending on the analyzed sequence were either known (the synthetic case ), or provided by an existing tracker [6] or specified manually (fig. 2 B ). Alternatively 2D point trackers similar to the ones of [7] can be used. The forward generative model is obtained by combining the latent to ambient space mapping (this provides the position of the 3D markers) with a perspec tive projection transformation. The observation model is a product of Gaussians, each measuring the probability of a particular marker position given its corresponding image point track.
 Experiments with synthetic data: we analyze the performance of our tracker in controlled cond i-tions (noise perturbed synthetically generated image trac ks) both under regular circumstances (rea-sonable sampling of training data) and more severe conditio ns with subsampled training points and persistent partial occlusion (the man running behind a fenc e, with many of the 2D marker tracks obstructed). Fig. 1 B , C shows both the posterior (filtered) latent space distributi on obtained from our tracker, and its mean (we do not show the distribution of t he global rigid body motion; in all experiments this is tracked with good accuracy). In the late nt space plot shown in fig. 1 B , the onset of running (two cycles were used) appears as a separate regio n external to the main loop. It does not appear in the subsampled training set in fig. 1 B , where only one running cycle was used for training and the onset of running was removed. In each case, one can see that the model is able to track quite competently, with a modest decrease in its temporal accurac y, shown in fig. 1 C , where the averages are computed per 3D joint (normalised wrt body height). Subs ampling causes some ambiguity in the estimate, e.g. see the bimodality in the right plot in fig. 1 C . In another set of experiments (not shown) we also tracked using different subsets of 3D markers . The estimates were accurate even when about 30% of the markers were dropped.
 Experiments with real images: this shows our tracker X  X  ability to work with real motions of differ-ent people, with different body proportions, not in its latent variable model training set (figs. 2 X 3). We study walking, running and turns. In all cases, tracking a nd 3D reconstruction are reasonably ac-curate. We have also run comparisons against low-dimension al models based on PCA and GPLVM (fig. 3). It is important to note that, for LELVM, errors in the pose estimates are primarily caused by mismatches between the mocap data used to learn the LELVM p rior and the body proportions of the person in the video. For example, the body proportions of the OSU motion captured walker are quite different from those of the image in fig. 2 X 3 (e.g. note h ow the legs of the stick man are shorter relative to the trunk). Likewise, the style of the runner fro m the OSU data (e.g. the swinging of the arms) is quite different from that of the video. Finally, the interest points tracked by the 2D tracker do not entirely correspond either in number or location to th e motion capture markers, and are noisy and sometimes missing. In future work, we plan to include an o ptimization step to also estimate the body proportions. This would be complicated for a general, u nconstrained model because the di-mensions of the body couple with the pose, so either one or the other can be changed to improve the tracking error (the observation likelihood can also become singular). But for dedicated prior pose models like ours these difficulties should be significantly r educed. The model simply cannot assume highly unlikely stances X  X hese are either not representable at all, or have reduced probability X  X nd thus avoids compensatory, unrealistic body proportion est imates. Figure 1: A B C (with added noise) and for tracking. Row 1 : tracking in the 2D latent space. The contours (very tight in this sequence) are the posterior probability. Row 2 : perspective-projection-based observations with occlusions. Row 3 : each quadruplet ( a, a  X  , b, b  X  ) show the true pose of the running man from use the first running cycle for training LELVM and the second c ycle for tracking. C : RMSE errors for each frame, for the tracking of A ( left plot ) and B ( middle plot ), normalised so that 1 equals the height of the stick man. RMSE ( n ) = 1 markers, i.e., comparison of reconstructed stick man  X  y multimodal posterior distribution in pose space for the mod el of A (frame 42).
 Comparison with PCA and GPLVM (fig. 3): for these models, the tracker uses the same GMSPPF setting as for LELVM (number of particles, initialisation, random-walk dynamics, etc.) but with the mapping y = f ( x ) provided by GPLVM or PCA, and with a uniform prior p ( x ) in latent space (since neither GPLVM nor the non-probabilistic PCA provide one). The LELVM-tracker uses both its f ( x ) and latent space prior p ( x ) , as discussed. All methods use a 2D latent space. We ensured the best possible training of GPLVM by model selection based on multiple runs. For PCA, the latent space looks deceptively good, showing non-intersec ting loops. However, (1) individual loops do not collect together as they should (for LELVM they do); (2 ) worse still, the mapping from 2D to pose space yields a poor observation model. The reason is t hat the loop in 102-D pose space is nonlinearly bent and a plane can at best intersect it at a fe w points, so the tracker often stays put at one of those (typically an  X  X verage X  standing positio n), since leaving it would increase the error a lot. Using more latent dimensions would improve this , but as LELVM shows, this is not necessary. For GPLVM, we found high sensitivity to filter ini tialisation: the estimates have high variance across runs and are inaccurate  X  80% of the time. When it fails, the GPLVM tracker often freezes in latent space, like PCA. When it does succeed, it pro duces results that are comparable with LELVM, although somewhat less accurate visually. Howe ver, even then GPLVM X  X  latent space consists of continuous chunks spread apart and offset from e ach other; GPLVM has no incentive to place nearby two x s mapping to the same y . This effect, combined with the lack of a data-sensitive, realistic latent space density p ( x ) , makes GPLVM jump erratically from chunk to chunk, in contra st with LELVM, which smoothly follows the 1D loop. Some GPLVM pr oblems might be alleviated using higher-order dynamics, but our experiments suggest t hat such modeling sophistication is less Figure 2: A B cycles) for training LELVM. Row 1 : tracking in the 2D latent space. The contours are the estima ted posterior probability. Row 2 : tracking based on markers. The red dots are the 2D tracks and the green stick man is the 3D reconstruction obtained using our m odel. Row 3 : our 3D reconstruction from a different viewpoint. B : tracking of a person running straight towards the camera. N otice the scale changes and possible forward-backward ambiguities i n the 3D estimates. We train the LELVM using 180 datapoints (2.5 running cycles); 2D tracks were ob tained by manually marking the video. In both A  X  B the mocap training data was for a person different from the vi deo X  X  (with different body proportions and motions), and no ground-truth estimate was available for favourable initialisation. crucial if locality constraints are correctly modeled (as i n LELVM). We conclude that, compared to LELVM, GPLVM is significantly less robust for tracking, has m uch higher training overhead and lacks some operations (e.g. computing latent conditionals based on partly missing ambient data). We have proposed the use of priors based on the Laplacian Eige nmaps Latent Variable Model (LELVM) for people tracking. LELVM is a probabilistic dim. r ed. method that combines the advan-tages of latent variable models and spectral manifold learn ing algorithms: a multimodal probability density over latent and ambient variables, globally differ entiable nonlinear mappings for reconstruc-tion and dimensionality reduction, no local optima, abilit y to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. L ELVM is computationally efficient, sim-ple to learn from sparse training data, and compatible with s tandard probabilistic trackers such as particle filters. Our results using a LELVM-based probabili stic sigma point mixture tracker with sev-eral real and synthetic human motion sequences show that LEL VM provides sufficient constraints for robust operation in the presence of missing, noisy and am biguous image measurements. Com-parisons with PCA and GPLVM show LELVM is superior in terms of accuracy, robustness and computation time. The objective of this paper was to demonst rate the ability of the LELVM prior in a simple setting using 2D tracks obtained automatically o r manually, and single-type motions (running, walking). Future work will explore more complex o bservation models such as silhouettes; the combination of different motion types in the same latent space (whose dimension will exceed 2); and the exploration of multimodal posterior distributions in latent space caused by ambiguities. Acknowledgments This work was partially supported by NSF CAREER award IIS X 05 46857 (MACP), NSF IIS X 0535140 and EC MCEXT X 025481 (CS). CMU data: http://mocap.cs.cmu.edu (created with fund-ing from NSF EIA X 0196217). OSU data: http://accad.osu.edu/research/mocap/mocap data.htm .
 References
