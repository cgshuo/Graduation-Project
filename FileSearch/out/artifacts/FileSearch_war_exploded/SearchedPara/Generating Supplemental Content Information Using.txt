 We describe a hybrid recommendation system at LinkedIn that seeks to optimally extract relevant information pertain-ing to items to be recommended. By extending the notion of an item profile, we propose the concept of a  X  X irtual profile X  that augments the content of the item with rich set of fea-tures inherited from members who have already shown ex-plicit interest in it. Unlike item-based collaborative filtering, we focus on discovering the characteristic descriptors that underlie the item-user association. Such information is used as supplemental features in a content-based filtering system. The main objective of virtual profiles is to provide a means to tap into rich-content information from one type of entity and propagate features extracted from which to other affili-ated entities that may suffer from relative data scarcity. We empirically evaluate the proposed method on a real-world community recommendation problem at LinkedIn. The re-sult shows that the virtual profiles outperform a collabora-tive filtering based approach (user who likes this also likes that). In particular, the improvement is more significant for new users with only limited connections, demonstrating the capability of the method to address the cold-start problem in pure collaborative filtering systems.
 H.2.8 [ Database Management ]: Data Mining hybrid recommender systems; feature generation and extrac-tion; model-based recommendation; virtural profiles
Large scale recommender systems, in the era of internet scale data deluge, contribute significantly to mitigating in-formation overload by unveiling relevant and interesting ob-jects to users. Rather than hoping for serendipitous encoun-ters, recommender systems bring forth the notion of per-sonalized information discovery by presenting to the user a smaller pool of relevant objects. Collaborative filtering, a popular mechanism for recommendation, is vulnerable to  X  X old start problems, X  which has led to the exploration of hybrid recommenders. Hybrid recommenders combine in-formation obtained from different sources and techniques to achieve better outcome. Typically a hybrid recommender system incorporates information from a myriad of sources e.g., content meta data, global popularity, social network and social interaction information, and so on. Each of these information sources offers different level of relevance guar-antee at varying computation overhead. Hence, how these information sources are computed and how they are com-bined play a vital role in the final outcome.

As of today LinkedIn has more than 225 million users. As the largest and most popular professional networking site, LinkedIn presents some unique opportunities and challenges for content discovery and recommendation. It is imperative for the members to be able to discover and subscribe to companies and groups (referred to as community henceforth) that might be relevant to them from a professional context.
In this paper, we describe a hybrid community recom-mendation system at LinkedIn that optimally combines in-formation from multiple sources. In order to extract more relevant information pertaining to the community to be rec-ommended, i.e., to further extend the notion of content meta data, we have proposed the concept of a X  X irtual profile X  X hat augments the content meta data with rich set of features in-herited from the set of members who have already shown explicit interest in it. In general the notion of virtual profile answers:  X  X hat are the most dominant features pertain-ing to the members who have shown interest in a particular community? X  This question essentially maps an object into the same feature space as that of the subscribers X . Content meta data, extended with this inferred information provides additional warranty against cold start problem. LinkedIn data presents a unique opportunity to extend the content features with extracted features since there is no dearth of rich set of information about the subscribers in the data set, which essentially renders the synergy immensely valuable.
The contributions presented in this paper are as follows: 1. A generic content meta data extension method i.e., the 2. A scalable and generic recommendation computation 3. A seamless integration of multiple, heterogeneous data
The rest or the paper is organized as follows. In Section 2 we briefly review previous effort in hybrid recommender sys-tems where multiple sources of information are utilized. In Section 3 we describe details of the virtual profile gener-ationprocessandthewayitisinterfacedwithageneric recommender system at LinkedIn. In Section 4 we present experiment results from both online and offline evaluations. The utility of virtual profiles are demonstrated by compar-ing its performance with various baseline and alternatives. Finally, we conclude the paper in Section 5 and discuss pos-sible future directions.
There has been a flurry of research in the domain of rec-ommender systems with the objective of improving person-alization [1]. Most traditional recommenders are powered by collaborative filtering [10, 21], content-based predictors [9, 18] and knowledge based filtering techniques [13]. Each indi-vidual techniques have their own strengths and weaknesses e.g. while collaborative filtering techniques suffer from data sparsity and cold start problems [19], content-based tech-niques are prone to skewed recommendation [18]. Hybrid recommenders combine the best of both worlds, making the recommenders more robust in practice. Much work has been done to combine multiple recommenders in an effective way to outperform any single one. In [6] Burke depicts a taxon-omy of recommender systems, w here multiple recommenders are arranged to allow execution in a parallel or cascaded topology. A system described in [5] combines multiple col-laborative filtering approaches using a linear combination of static weights learned via linear regression. STREAM [3], which combines multi-tier predictors, uses dynamically gen-erated metrics to learn the next level of predictors. In [15], a hybrid movie recommender system is proposed that uses content based predictors to boost user data which drives the ensuing collaborative filtering based recommendation. The content information is obtained from IMDB and a Naive Bayes classifier is used for building user item profiles. Fi-nally a user-based collaborative filtering is employed to ob-tain the final recommendation. However, this approach suf-fers from scalability issues. Pazzani [17] proposed a hybrid recommender system where the content based user profiles are used to group similar users which is subsequently used to predict user preferences.

Previous work has explored ways to propagate informa-tion to users from communities they belong to. The intu-ition is that users with common attributes are more likely to be friends and often form dense communities. For instance, systems described in [16, 14, 2] propose methods to infer in-complete/undisclosed user attributes from their contacts or communities in social networks. These methods work bet-ter in cases where more information is available for recom-mended item than for users. On the other hand, for applica-tions with rich information available about users, we advo-cate that items to be recommended can be augmented with features from users who have had interaction with them. In other words, these items can be represented as an objects in the same feature space as that of the users. These rep-resentations could be thought of  X  X irtual user profiles X  or  X  X irtual profiles. X  This could potentially add another layer of information source to guide the recommendation process. In our approach, we describe a large scale recommender sys-tem that combines data from heterogeneous sources includ-ing virtual profiles and social network information to serve real time recommendation requests.
We adhere to building a hybrid recommender system on top of content-based filtering since we have an abundant access to rich-content entities, such as user profiles, which enables a straightforward means for feature extraction, in-dexing and matching. Target entities (those the client wants recommendations of) are feature extracted and put into an inverted index, and source entities (those the client wants recommendations for) are converted into complex queries against the index. This provides a form of content-based recommendation score where the match is determined by the degree of similarity between the source and target entity features, with different fields weighted by a set of parameters determined by an offline learning-to-rank process. Figure 1: A brief architecture for the recommender system with virtual profiles.

Figure 1 illustrates a brief architecture of the system. On the left side of the dashed line is the core content-based fil-tering system. The feature space is composed of features extracted from source (e.g., user) and target (e.g., commu-nitiy) entities into standardized fields. Some features are further transformed into pairwise features representing sim-ilarity between source and target fields, which are obtained by issuing field-based queries against the inverted index. Ex-tensions to the feature space can be made in the system by including information emanated from various heterogeneous sources, such as virtual profiles (detailed below), network proximity, etc., as additional features to appropriate entities, as is shown on the right side of the figure. These features are fed into an offline regularized logistic regression learning process, and the trained model is subsequently ported online to serve real-time recommendation requests.

We view every entity as being characterized by two set of content features: one extracted from explicit information associated with the entity which we name the  X  X rimary pro-file, X  and the other inferred from the entity X  X  behavior and association with other entities, which we name the  X  X irtual profile. X  The main objective of virtual profiles is to provide a means to tap into rich-content information from one type of entity and propagate features extracted from which to other affiliated entities that may suffer from relative data scarcity. Essentially, a virtual profile of an entity is an aggregation of statistically relevant features from primary profiles of af-filiated entities, in which way it introduces a collaborative filtering aspect in our content-based filtering system. For example, a virtual profile of a Linkedin group constitutes distinctive features from its participants so that the group can be most effectively distinguished from others.
To first extract features from entities to generate primary profiles, we utilize a feature extractor layer, a standalone service that accumulates underlying entity database change events and identifies various fields in the document. Various types of fields that could be feature extracted include rich text fields, such as job summary, member position summary etc., and specialized fields, such as Geo entities including region, country, city, coordinates, etc.

The presented content-based filtering system can be ex-tended to consider other collaborative filtering aspects, for example, by including network proximity as a feature while computing relevance scores. We describe a method based on the user-community co-affiliation along this line as a com-parison in Section 4. As a general platform, every appli-cation consuming recommendations from this system can easily build its own logic for reranking/reordering of results based on custom filtering criteria.
The virtual profile generation process for an entity aims at selecting, from a total of n features of its affiliated entities, a subset with k&lt;n features that is  X  X aximally informa-tive X  about the entity. From a classification point of view, the entity that we generate a virtual profile for represents a target class for a set of documents (primary profiles). We need a measure to evaluate the X  X nformation content X  X f each individual feature with regard to the target class. We pro-pose to use mutual information for this purpose. Mutual information measures arbitrary dependencies between ran-dom variables. And the fact that the mutual information is independent of the coordinates chosen permits a robust estimation, which makes it suitable for assessing the  X  X nfor-mation content X  of features in complex classification tasks.
In accordance with Shannon X  X  information theory, the un-certainty of a document class C as a random variable can be measured as: In this case, a document class is a virtual profile of a target entity, and documents are primary profiles of entities that have or have not been affiliated with the target entity. Af-ter knowing the feature vector F , the conditional entropy H ( C | F ) measures the remaining uncertainty about C : And after having observed the feature vector F ,themutual information, i.e., the amount of decreased class uncertainty is defined as:
I ( C ; F )= H ( C )  X  H ( C | F )= where P ( c, f ) is the joint probability of class c and feature f .

Therefore, to generate virtual profiles, the goal is to find the optimal feature subset, S  X  F , such that I ( C ; S )ismax-imized. From an information theoretic perspective, select-ing features that maximize I ( C ; S ) translates into selecting those features that contain the maximum information about class C . However, locating the optimal subset requires an exhaustive combinatorial search over the feature space, re-quiring a number of runs equals to n k ,where n is the size of the original feature set and k is that of the desired sub-set. Besides, an exact solution also demands large training sample sizes to estimate the higher order joint probability distribution in I ( C ; S ). For example, Fraser X  X  method [7], a computationally efficient algorithm for calculating the opti-mal I ( C ; S ), requires for its convergence a number of sam-ples  X  X n the millions X  when the number of features in the input vector is larger than 3 or 4.

Given these difficulties, most of the existing approaches approximate I ( C ; S ) based on the assumption of lower-order dependencies between features. For example, a second-order feature dependence assumption is proposed by Battiti [4] to approximate I ( C ; S ) by a greedy incremental selection scheme with a heuristic to account for correlations between features: given a set of already selected features, the algo-rithm chooses the next feature as the one that maximizes the information about the class corrected by subtracting a quan-tity proportional to the average mutual information with the selected features.

Unfortunately, the calculation of pairwise feature correla-tion I ( f,f ) is impractical in our case because the feature di-mension is extremely high given the bag-of-words extracted from textual contents. Therefore, we make a first-order class dependence assumption that each feature independently in-fluences the class variable, which means to select the m th feature, f m , is independent from the ( m  X  1) already selected features, i.e., P ( f m | f 1 ,...,f m  X  1 ,C )= P ( f m | C sults in a straightforward greedy algorithm to generate the virtual profile for an entity c , which consists of the following steps: 1) gather features from all primary profiles associ-ated with entities that have an affiliation with c ,2)calcu-late mutual information, I ( f ; c ), between each feature and e ,and3)selecttop k features with highest I ( f ; c )intothe virtual profile. More specifically, the mutual information between a specific term (feature value) and target virtual profile, I ( f ; c ), can be calculated as follows.
I ( f ; c )= where f is a random variable that takes values e f = 1 (the entity primary profile contains feature f )and e f = 0 (the entity primary profile does not contain feature f ), and c a random variable that takes values e c = 1 (the entity is affiliated with c )and e c = 0 (the entity is not affiliated with c ). The probabilities in Equation 1 can be derived using maximum likelihood estimation.
Our goal is to test if virtual profiles are a useful source of features to improve the recommendation performance. In designing experiments, we want to verify the heuristic as-sumption that virtual profiles can be made of features greed-ily selected by mutual information. We also want to compare the performance of virtual profiles with other classic collab-orative filtering methods and study their tradeoffs. Further-more, by experimenting with di fferent parameter settings to generate virtual profiles, we want to provide a general guidance on how virtual profiles can be best implemented in practice.
We choose a community recommendation problem at LinkedIn as the test application. We gather various feedbacks from the interaction between users and the recommender system. For example, successful recommendations would result in users following certain communities, while users are also pre-sented the choice to opt-out communities at any later point.
We extract three kinds of features from entities (users and communities) in this application domain: 1. content features X  X eatures from users X  and communi-2. virtual profile X  X s described in Section 3, a set of fea-3. co-follows X  X  collaborative feature representing  X  X sers The co-follow (CF for short hereafter) feature captures a notion of similarity between communities that is driven by users X  preference. To generate CF for a community, from all other communities that it shares followers with, we choose top50onesrankedbyTF/IDF.Andthenforeachuser, we take the closure of communities she has already followed with respect to CF, and select top 50 ones weighted by their TF/IDF scores normalized over the number of communities followed. Communities selected in this way can essentially be seen as recommendations by collaborative filtering. We treat them as an additional feature to be combined with users X  content features to generate search queries, which would lead to a larger set of communities to be returned as Figure 2: Kernel density of number of selected terms. matches. And the weight of this feature, similar to others, can be determined in an offline learning process.
The content features extracted for communities contains only three fields (i.e., name, description, and tags). They represent a bare minimum amount of information that is required for a content-based filtering recommender system to function, and are therefore considered as a baseline in the experiment. Co-follows, on the other hand, are designed as an alternative to virtual profiles for comparison, given that they both take into account the interaction among entities.
As for model fitting, we use a training set including 3.4 million positive and 2.2 million negative examples gathered from both explicit and implicit user feedbacks (e.g., fol-low/unfollow or lack of action to recommendations). We apply an L2-regularized logistic regression with various com-bination of the above mentioned features. The best model under each configuration is selected by optimizing the area under the ROC curve (AUC-ROC). Performances of differ-ent models are evaluated both offline and online. The results are presented in the next sections.
First of all, to qualitatively illustrate the difference from including virtual profiles into content features, Figure 2 shows two kernel densities, comparing the distribution of the num-ber of terms (i.e., feature values that are words extracted from text) for target communites with and without virtual profiles (vp). The size of the virtual profile is capped at 50. It is evident that the original feature space without vir-tual profiles is a mixture of two underlying distributions: One with a tall spike near zero represents the group of com-munites that have very limited content information, most Figure 3: ROC curves for virtual profiles with dif-ferent number of terms. likely only names available. And the other group are those that have richer content. The effect of adding virtual pro-files is shifting the feature number distribution rightwards, increasing the number of terms extracted for both groups by 50 on average. The benefit is especially notable for the first group as they turned from unlikely to be matched to recommendation-worthy with the additional features, as is further demonstrated later in the study of the coverage of recommendations.

Next, we carry out a pilot experiment to empirically de-termine the optimal size k of virtual profiles. We set k to 25, 50, 100, and 200 and generated virtual profiles accordingly. Figure 3 shows ROC curves of models trained with these various sized virtual profiles. We can see that the trained model performs the best when k = 50. This result is inline with a phenomenon known as Hughes effect [12]: As the number of dimensionality increases the predictive power of classifier fails to keep up given a fixed number of training examples. The correlation between features are also part of the reason why including more features does not improve the model performance. We use k = 50 for all subsequent experiments discussed below.

Now we proceed to compare models obtained by train-ing with four different feature configurations, namely, (A) content features only, (B) content features plus virtual pro-files (vp), (C) content features plus co-follows (cf), and (D) content features plus both vp and cf. It can be seen from Figure 4 that, the ROC curve of model A completely domi-nates that of model B (with AUCs 0.72 vs. 0.60), and both of them dominate that of model C (AUC 0.44). It can be also seen that the ROC curve of model D is almost identi-cal to that of model C, despite having both the vp and cf features. The same performance pattern is also exhibited in the precision-recall curve, as shown in Figure 5.
Besides classification performance, another important mea-sure that can be evaluated offline is the coverage, which refers to the degree to which recommendations cover the set of available items (item space coverage) [8, 11]. Owing to a distributed algorithm developed at LinkedIn, we are able to calculate recommendations offline for all our 225 million users. Using each of the trained model described above, we calculate a different set of recommendations for each user, with the size of each capped at 50. We counted numbers of times unique communities appeared in recommendations (frequencies) under different models. Figure 6 shows a log-arithmic scale of the frequencies sorted in descending order plotted against their ranks.

It is not surprising to see that the baseline curve from the content-features-only model is the lowest since features extracted for communities in this case contains the least amount of information. And the distribution of the recom-mendation frequency simply reflects the distribution of the amount of textual content of each community (see Figure 2: a large number of communities have only a small number of terms extracted). On the other hand, the curve from the model with the addition of CF visibly bulges outwards from the baseline for about two thirds of points, indicating that those points are getting higher frequencies showing up in recommendations, hence more coverage. Most remarkably, the model with the addition of virtual profiles significantly increased the frequencies for almost all points on the curve except for cases where original baseline frequencies are ex-tremely high or low.

The reason why CF slightly boost the coverage for some communities is because those communities bear little con-tent information yet having followers already. Having follow-Figure 5: Precision-recall curves for different mod-els. ers makes them eligible to be potentially included in other communities X  CF, and thus leads to a higher chance of being recommended to users. However, for users not having fol-lowed any communities at all, CF becomes an empty feature, which is the reason why for about a third of communities, there sees no increase in coverage from CF compared with the baseline. This phenomenon is also illustrated in Fig-ure 7, in which the recommendation frequencies of unique companies are only counted for new users (i.e., users who have not started following communities yet). We observe that the model with CF produces an identical curve to the baseline, while the model with virtual profiles exerts a con-sistent boost. This shows that CF, as a feature of a collab-orative filtering aspect, fails to address cold start, while vir-tual profiles provides a well-rounded improvement in terms of both coverage and predictive power.
To further evaluate models with various feature configu-rations (i.e., content features with vp, content features with bm, content features with both vp and bm, and content fea-tures only), we deployed them to serve realtime online rec-ommendation requests and compare performances through a bucket test. We assign a distinct bucket of 2.5% randomly selected users from the whole user base to each model. The bucket with the model based only on content features is the control, while others are variants.

The duration of the test is determined according to Wheeler [22], where a conservative estimation of sample size to achieve an 80% power (the probability of correctly rejecting the null hypothesis when it is indeed false) is given by Equation 2. Figure 6: number of recommendations per unique companies. where n is the minimum number of samples (impressions to be delivered) for each equal-sized variant, r is the number of variants,  X  2 is the variance of the OEC (Overall Evalu-ation Criterion [20], a quantitative measure of the exper-iment X  X  objective), and  X  is the sensitivity, or the desired amount of change. The OEC in this test is the Click-through rate (CTR) of recommendations. Assume each click-through event is a Bernoulli trial with probability p = ctr 0 (con-trol CTR, which is estimated from historical data), then  X  2 = p (1  X  p ). Applying Equation 2 and knowing the ap-proximate recommendation impressions per day, we derive the length of the test to be 7 days.

Figure 8 presents the results of the test by showing the percentage change in CTR of variant models relative to the control, on each individual day of the test. Overall, the model with virtual profiles outperforms the control by 91.2%. Surprisingly, however, we do not observe any im-provement from the model with CF. The model with both virtual profiles and CF increased the CTR by 84.4%. The difference between the two best performing model is not sig-nificant ( p value 0.062), which is similar to the offline eval-uation result. The reason why CF fails to increase overall CTR may be attributed to the fact that only one third of users have followed communities in this particular applica-tion, meaning the cold start effect is much pronounced. Vir-tual profiles, on the other hand, is not vulnerable to this problem since it is content-based and does not rely on pre-existing user-item affiliations, as is demonstrated in this ex-periment.
We presented a generic content meta data extension method called virtual profiles. We also introduced how it is uti-Figure 7: number of recommendations for new users per unique companies. lized in a scalable and generic content-based hybrid recom-mender system that powers multiple real-time recommenda-tion products at LinkedIn. The goal of virtual profiles is to provide a means to tap into rich-content information from one type of entity and propagate features extracted from which to other affiliated entities that may suffer from rela-tive data scarcity. It brings a collaborative filtering aspect in the form of a supplement feature to contents in the recom-mender system. It is shown to outperform a method that directly incorporate network proximity from collaborative filtering.

Experiments supported that our first-order class depen-dence assumption and the greedy algorithm in calculating the mutual information are reasonable approximations. In future work, we will investigate scalable ways to account for dependencies among features. We plan to explore more term weighting methods besides mutual information, includ-ing other classic information theoretic quantities such as the Kullback-Leibler divergence, or TF/IDF. We would also like to experiment with feature transformation techniques to ad-dress the feature correlation and dimensionality problems while preserving the capability for field-based search.
We are grateful for the contribution of Christian Posse and Alexis Pribula who have helped in formulating the idea and provided valuable insights. [1] G. Adomavicius and A. Tuzhilin. Toward the next Figure 8: Relative model Click-through rates com-pared with the baseline.
 [2] C. G. Akcora, B. Carminati, and E. Ferrari. Network [3] X. Bao, L. Bergman, and R. Thompson. Stacking [4] R. Battiti. Using mutual information for selecting [5] R. M. Bell, Y. Koren, and C. Volinsky. The BellKor [6] R. Burke. Hybrid recommender systems: Survey and [7] A. M. Fraser and H. L. Swinney. Independent [8] M. Ge, C. Delgado-Battenfeld, and D. Jannach. [9] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. [10] J. L. Herlocker, J. A. Konstan, and J. Riedl. [11] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [12] G. Hughes. On the mean accuracy of statistical [13] P. B. Kantor. Recommender systems handbook . [14] J. Lindamood, R. Heatherly, M. Kantarcioglu, and [15] P. Melville, R. J. Mooney, and R. Nagarajan. [16] A. Mislove, B. Viswanath, K. P. Gummadi, and [17] M. J. Pazzani. A framework for collaborative, [18] M. J. Pazzani and D. Billsus. The adaptive web. [19] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [20] R. K. Roy. Design of experiments using the taguchi [21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [22] R. E. Wheller. Portable power. Technometrics ,
