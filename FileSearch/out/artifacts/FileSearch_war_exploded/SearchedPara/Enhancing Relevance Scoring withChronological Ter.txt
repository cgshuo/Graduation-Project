 We introduce a new relevance scoring technique that en-hances existing relevance scoring schemes with term posi-tion information. This technique uses chronological term rank (CTR) which captures the positions of terms as they occur in the sequence of words in a document. CTR is both conceptually and computationally simple when compared to other approaches that use document structure information, such as term proximity, term order and document features. CTR works well when paired with Okapi BM25. We eval-uate the performance of various combinations of CTR with Okapi BM25 in order to identify the most effective formula. We then compare the performance of the selected approach against the performance of existing methods such as Okapi BM25, pivoted length normalization and language models. Significant improvements are seen consistently across a va-riety of TREC data and topic sets, measured by the major retrieval performance metrics. This seems to be the first use of this statistic for relevance scoring. There is likely to be greater retrieval improvements possible using chronological term rank enhanced methods in future work.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods ; H.3.3 [ Informa-tion Storage and Retrieval ]: Information Search and Re-trieval X  Retrieval models,Search process ; H.3.4 [ Informat-ion Storage and Retrieval ]: Systems and Software X  Per-formance evaluation Algorithms, Experimentation, Performance Relevance ranking, term position, similarity scoring, docu-ment structure, chronological term rank, term weighting Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
In the beginning, there was term frequency. As many have pointed out [4, 7], the notion of utilizing term frequencies in order to estimate term significance was asserted by Luhn as early as the late 1950s [11]:
Following the first element of Luhn X  X  proposal, term fre-quency based methods have since become the benchmarks by which new work in relevance scoring is judged, including the work presented here. Two term frequency statistics are most commonly used. The first, referred to simply as term frequency , is the number of occurrences of a term in a partic-ular document, denoted here as tf . The second, the number of documents in the collection containing the term of interest is generally referred to as document frequency and denoted as df . Document frequency is most commonly used in term weights as inverse document frequency ( idf ), as proposed by Jones in 1972 [9].

Two ( tf  X  idf )-based relevance estimation techniques have become particularly dominant: Okapi BM25 [15] and piv-oted length normalization [16]. A popular formulation of the Okapi BM25 model is shown in Eq. 1.

In addition to tf and df as defined above, t is a term in the query q and document d , N is the total number of documents in the collection, dl is the length of d and avdl is the average length of all documents in a collection. One version of the pivoted normalization scheme is shown in Eq. 2, where qtf is the number of occurrences of t in q .

Three common features of these techniques are worth not-ing. First, term frequency is used as the core indicator of document relevance. Second, the importance of a term is in-versely related to its commonality: rare terms are more use-ful indicators of relevance. Finally, document length is uti-lized to correct for the greater likelihood of retrieving longer documents simply because they contain a greater number of words.

While term frequency is certainly one of the most use-ful indicators of relevance, relying solely on term frequency amounts to viewing a document simply as a bag-of-words and ignores much of its structure. With this in mind, researchers have developed techniques incorporating document struc-ture information.

The second element of Luhn X  X  proposal, that term posi-tion is also important, has historically seen only intermittent interest. With the growing difficulty of achieving further re-trieval improvements using only term frequencies, there is an increasing interest in information derived from document structure.

Term order is an example of structural information that some recent approaches utilize. Such approaches assign higher relevance scores to documents in which query terms appear in the same order as they appear in the query.

Document-feature based approaches take into account the occurrence of query terms in locations of varying promi-nence, such as title, header text, or body text. The most prominent document-feature based approach is BM25F, built upon Okapi BM25 [14, 19]. Document feature techniques are most commonly used in web search as features can be easily identified due to the presence of HTML tags.

Term proximity, which is most closely related to the work presented here, is an idea which has seen recent interest. In this context, term proximity refers to the lexical distance [7] between query terms, calculated as the number of words separating query terms in a document. Keen carried out some of the earliest investigations of using term proximity in the 90s [10]. Hawking and Thistlewaite provided a frame-work for formal discussion of proximity based methods [7]. Rasolofo and Savoy combined proximity information with Okapi and found improved retrieval performance, partic-ularly among the top scoring documents [13]. Beigbeder and Mercier achieved improved precision through the use of fuzzy proximity statistics [4]. B  X uttcher, Clarke and Lush-man also added proximity information to Okapi BM25, with positive results [5]. Many research groups now use prox-imity enhanced approaches as part of their TREC submis-sions [18]. Modern experimental retrieval systems such Indri support proximity query operators [17].

In this work, we examine the use of a relatively neglected term position statistic: chronological term rank (CTR). We were not able to locate any similar work in the literature. Chronological term rank is the rank obtained from the po-sition of the term in the sequence of words in the original document. Throughout this paper the phrase  X  X erm rank X  refers to chronological term rank unless otherwise indicated. Intuition indicates that this statistic may be useful; terms most strongly related to the main content of a document are likely to occur near the beginning. We augment Okapi BM25 with CTR, resulting in a hybrid scoring method that improves retrieval performance across a variety of TREC data and topic sets, measured by major retrieval perfor-mance metrics.

In the next section we introduce chronological term rank, our chronological term rank model and discuss some func-tional considerations for its use. In Section 3 we experimen-tally evaluate these considerations in order to identify an optimal approach. In Section 4 we compare the optimized approach with several current benchmark relevance estima-tion methods on a variety of data and topic sets. We end with some concluding remarks.
Just as document-structure based methods move beyond the traditional bag-of-words approaches to relevance estima-tion, so does our method, utilizing chronological term rank.
The chronological rank of a term (CTR) within a docu-ment is defined as the rank of the term in the sequence of words in the original document. We refer to this statistic as  X  X hronological X  to emphasize its correspondence to the se-quential occurrence of the terms within the document from the beginning to end. We also use this name to differentiate it from the many other uses of  X  X ank X  in the literature, such as Anh and Moffat X  X  frequency-based term rank [2].
Intuitively, CTR is the sequence of words as they are en-countered as the document is read from start to finish. This intuition is very simple; perhaps even more clear and suc-cint than that behind term proximity. Authors tend to state the main ideas and topics of a piece as early as possible to quickly convey core ideas and results and grasp attention. This practice is explicitly reflected in titles, the use of ab-stracts in scientific papers and the inverted-pyramid style of writing favored by journalists [6]. Journalism students are taught to place the most important and relevant content at the beginning of an article while progressively including less important and smaller details as the article continues. As such, important terms may be placed at the beginning of a document and may not be explicitly mentioned again, poten-tially spoiling the correlation between term frequency and the importance of the term. Terms mentioned many times, but only at the end of a document, may not be as important. Chronological term rank systematically and elegantly incor-porates these ideas by utilizing one of the simplest possible structural observations.

The CTR values of each term in the query are independent of one another. This differs from other structural approaches such as term proximity or term order, making CTR easier to incorporate into existing systems. Indexing systems need only to store one additional integer along with each term frequency if document-term impact scores are not computed at indexing time. Fully determining order or proximity be-tween all occurrences of query terms within a document or using document features can be expensive in both storage space and computation time. Existing term frequency based systems would likely need to undergo significant modifica-tions to utilize structural information other than CTR.
The CTR model is defined as follows: let D = ( t 1 , . . . , t be a document where t i are terms (words) ordered according to their sequence in the original document. Let tr t := i , where the chronological rank tr of term t is assigned as the subscript i of the earliest occurrence of t in D .
How should CRT be utilized? In this subsection we de-scribe some of the approaches we studied. This is by no means exhaustive. Many other uses of CTR may be pos-sible and researchers may develop more effective schemes incorporating CTR in the future. As with other success-ful structural approaches, we use Okapi BM25 as the base for our ranking functions, adding an additional term which incorporates CTR. We use BM25 as formulated in Eq. 1. In our preliminary testing, the pivoted length normalization formula did not integrate with term rank successfully, at least for the approaches we attempted. Thus we will focus on the integration of CTR with BM25, with the following considerations.
In this section we experimentally evaluate the functional considerations discussed in the previous section. Our goal is to construct the most effective relevance ranking formula using chronological term rank. In addition we will be able to identify the most effective range of parameters for the constructed function. Again, these experiments are by no means exhaustive. We are simply exploring some readily available avenues towards the effective use of chronological term rank.

Evaluation of each of the above considerations cannot be done completely independently. A complete term rank en-hanced relevance function cannot be constructed without making some choices for each consideration. With that said, we attempt to keep the experiments as independent as pos-sible.
 In each of the following experiments the dataset used is WSJ2 , which consists of the Wall Street Journal (1990 X 1992) portion of disk two of the TREC corpus. The topic set is the title fields from TREC topics 51 X 200. This data and topic set should be fairly good as an indicator of potential effectiveness for other data and topic sets; others have used it for similar purposes [2]. We focus on mean average precision (MAP), generally viewed as the most useful single measure of retrieval effectiveness.

We used our own retrieval system for this work, following standard guidelines in implementation [3]. As benchmarks, our implementations of BM25 and pivoted length normal-ization achieve MAP of 0.1478 and 0.1506 respectively for the selected data and topic sets. These results are consistent with other published results.

Formulae evaluated in the following subsections are com-piled in Table 1 for easy reference. Specifically, a complete formula is constructed by selecting either base formula A or B and one of the CTR formulae, R , a X  X  . For each formula to be evaluated, we measure MAP across the effective range of the parameter associated with the particular feature being examined.
 Here we evaluate whether chronological term rank is more effective as a multiplicative or additive supplement to BM25. This will determine whether the term rank statistic should be a scaler on the tf term or whether it is more effective as an independent term. Multiplicative use is shown in Eq. 3 where R is the CTR term (formula A in Table 1). Likewise, additive use is shown in Eq. 4 (formula B in Table 1).
We evaluated a total of three functions. An additive func-tion, specifically Eq. 4 where R is equal to C  X  (1  X  tr  X  1 (formula [B,a] ), essentially a rank percentage scaled by a constant, C . Two multiplicative functions were evaluated. Both are built upon Eq. 3 using different R terms. The first, referred to as Multiplicative1 below, where R = 1 + ( C  X  (1  X  tr  X  1 dl )) scales the score for each term by a value between 1 and 1+ C , according to the percent rank (formula [A,b] ). The second, referred to as Multiplicative2 , where R = (1  X  C ) + ( C  X  (1  X  tr  X  1 dl )), scales a portion of the term score according to the term rank percentage (formula [A,c] ), for a final value between 1  X  C and 1. Figure 1: Comparison of MAP for additive and mul-tiplicative term rank methods ( [B,a],[A,b],[A,c] ).
The MAP of these functions over varying C values is shown in Fig. 1. It is clear that the additive formula, with peak MAP around C = 0 . 2, is more effective than the mul-tiplicative formulae. The multiplicative functions provide only a slight boost in MAP. Throughout the rest of these experiments, Eq. 4 is used as the base formula.
 We next determined whether absolute rank or rank per-centage is more effective. In particular, for the percentage method we used the same formula as the additive function in the previous subsection (formula [B,a] ). For absolute rank functions we use Eq. 4 as the base with R equal to C  X  1 tr (formula [B,d] ) and C  X  1 log ( tr ) (formula [B,e] ). Figure 2: Comparison of MAP for percentage and absolute term rank methods ( [B,a],[B,e],[B,d] ).
MAP for each of these three formulae over varying C val-ues is shown in Fig. 2. The percentage formula handily outperforms the two inverse absolute formulae. The non-log inverse rank formula provides virtually no improvement at all. The percentage formula also has the advantage of being less temperamental over different values of C .
 Perhaps a term occurring at the end of a short document is more indicative of relevance than a term occurring at the end of a long document. If this is the case, individual rank percentages may not be optimal. In order to evaluate this hypothesis, we again use formula [B,a] as the benchmark compared to base formula B with R = C  X  (1  X  tr  X  1 maxdl maxdl is the length of the longest document in the collection (formula [B,f] ). This will give shorter documents somewhat of a boost in score.

The comparison of MAP for these two formulae over vary-ing C values are shown in Fig. 3. The maximum document length formula outperforms the individual length formula with its maximum MAP at about C = 0 . 3, though for larger values of C the individual formula performs slightly better. Figure 3: Comparison of MAP for term rank per-centage methods using individual document length and max document length ( [B,a],[B,f] ).
 Logarithms of term rank statistics may improve effective-ness. For instance, differences in term ranks may be most important in the beginning portions of documents. In order to evaluate this, we compare formulae [B,g] , [B,h] and [B,i] which use logarithms of term rank and document length statistics. Formula [B,g] uses the logarithm of both term rank and document length. Formula [B,h] uses the loga-rithm of scaled term rank and scaled document length, with the scaler being 1 D . The scaler helps the rank statistics to better fit into the useful range of the logarithm. Formula [B,i] uses the maximum document length of the collection rather than individual document length. These log values do not change the magnitude of the rank term, rather as part of a percentage they change the shape of the function over the range of term ranks. Figure 4: Comparison of MAP for term rank meth-ods using log values ( [B,a],[B,g],[B,h],[B,i] ).
Figure 4 compares the MAP of theses formulae using vari-ous constants. NoLog is, again, formula [B,a] . Log is formula [B,g] , Log/10, Log/30, Log/50 are formula [B,h] with D equal to 10, 30 and 50 respectively. Log/30max is formula [B,i] with D equal to 30. Log/30 achieves the highest MAP. Interestingly, Log/30max , which uses maxdl , did not perform as well as would be expected based on the performance of the MaxDocLength method in the previous subsection. Overall, the log statistics provided significant improvement in MAP. Unintentionally, the way the logarithms used here gives sim-ilar rank percentages higher scores as if they occur in longer documents. This also likely contributed to the performance. This indicates that CTR may be a more useful statistic for determining relevance of longer documents.
 Next, we determine whether retrieval can be improved by limiting the range of C over which CTR has an effect. The way the logarithms are used above, for example, limits the effective range of the term rank percentage. This may be partly the cause of the improved performance of those func-tions. Here we will examine that notion further. This is done by using formula [B,j] where D determines the per-centage of C which is affected by the term rank percentage, for a final term value between C  X  (1  X  D ) and C where D is some value from 0 X 1. Figure 5: Comparison of MAP for term rank meth-ods with limited C range variance ( [B,j] ).

The comparison of the MAP of this formula using several values of C and D is shown in Fig. 5. Overall, we see a dramatic improvement using this formula. In particular we see the maximum MAP at C = 0 . 6 and D = 0 . 6, though the peak MAP for each of the C values is not dramatically different.
 Finally, we evaluate whether the inclusion or exclusion of stopwords in CTR calculation has any effect on retrieval per-formance. Specifically, when encountering a stopword while indexing, should the rank count be incremented or not? The stopword list we use throughout this work is stoplist.orig from Zobel 1 which contains 600 common words. Formula [B,a] was used with and without stopwords in CTR calcula-tion. Figure 6: Comparison of MAP for term rank method with stopwords included and not included in rank calculation ( [B,a] ).

The comparison of MAP for formula [B,a] with and with-out stopwords as part of the CTR calculation is shown in Fig. 6 over varying C values. The exclusion of stopwords in rank calculations consistently outperforms rank that in-cludes stopwords. The exclusion of stopwords likely pro-duces a more truthful relative ordering of terms in the doc-uments.

Based on the results of these experiments, we discovered that the formula with the highest mean average precision uses base formula Eq. 4 with R equal to Eq. 5 below, with stopwords ignored in chronological term rank calculations.
Intuition can provide clues as to why these functional choices result in top performance. The optimal performance of additive usage suggests that term rank is an independent relevance bearing statistic, rather than a modifier of term frequency. The superiority of rank percentage with individ-ual document lengths indicates that relative rank is more useful than absolute rank. Log values put more empha-sis on the differences of CTR in the early portion of the documents with those in the latter being more similar to one another, with a greater emphasis on ranks in longer documents. The limited range variance provides a small base value to the rank term. Computing rank without stop-words likely provides a more accurate chronological ordering of terms. Interestingly, Eq. 5 alone along with idf weights from BM25 produces MAP of 0.1199, a decent score, on the selected data and topic sets, which is suggestive of the utility of chronological term rank as a relevance indicator. http://goanna.cs.rmit.edu.au/  X  jz/resources/ stopping.zip !
In this section we compare the performance of our chrono-logical rank method against established benchmark ranking formulae on larger data and topic sets. In particular, we compare against the performance of pivoted length normal-ization [16] and Okapi BM25 [15] as implemented in our sys-tem as well as the published performance of language model, title language model of Jin, Hauptmann, and Zhai [8], and the Global-By-Value and Local-By-Rank methods of Anh and Moffat [1, 2]. Variables such as stemming, stopwords and parsing were identical for all the methods implemented in our system. We used the Porter stemmer [12] and the stopword list indicated in Section 3. Performance of the BM25 and pivoted normalization formulae as implemented in our system is consistent, though not identically, to pub-lished performance of the same algorithms [2, 8]. The spe-cific CTR formula used for this comparative study is Eq. 4 with R equal to Eq. 6, below. Stopwords were ignored in term rank calculations. This differs from Eq. 5 in the statis-tic scaler, which is 20 rather than 30. This difference makes the formula more effective when stopwords are ignored in term rank computation.

Two datasets and four topic sets were used for the first set of comparisons. The TREC12 dataset consists of disks one and two of the TREC corpus. TREC45-CR consists of disks four and five of the TREC collection excluding the congressional report documents. Title queries from TREC topics 51 X 200 were run against the TREC12 dataset. Three topic sets were run against TREC45-CR : the title fields from topics 401 X 450 (TREC-8 ad hoc track), title fields from top-ics 351 X 450 (TREC-7 and TREC-8 ad hoc tracks), and the title fields from topics 301 X 450 and 601 X 700 (TREC 2004 robust track). These data and topic sets should be of suffi-cient size and contain sufficient variety of queries, along with other comparisons below, to provide a conclusive evaluation of our CTR approach.

A summary of the performance of CTR, pivoted length normalization and BM25 on each of the data and topic sets above is shown in Table 2. Three metrics are shown: mean average precision (MAP), precision after 10 documents re-trieved (P@10) and reciprocal rank (R. Rank). The col-umn which is labeled CTR is the score of the chronologi-cal term rank method for the indicated metric. The next three columns contain the scores of pivoted length normal-ization, percent improvement of our method over pivoted length normalization and whether the difference is signifi-cant according to the Wilcoxon sign-rank test at 95% confi-dence. The next three columns are similar but refer to Okapi BM25. The amount of improvement of CTR over both piv-oted length normalization and BM25 across the data and topic sets for the three retrieval metrics is striking. The differences are significant in all cases.

We next compared the performance of CTR with the pub-lished results of several other scoring schemes. First we com-pared against the recent performance of Anh and Moffat X  X  Local-By-Rank and Global-By-Value scoring schemes [2, 1]. Datasets used for this comparison are TREC12 and TREC45-CR as described previously. Title fields from topics 51 X 200 were run against TREC12 while the title fields of 351-450 were run against TREC45-CR . Summary of results for these runs is shown in Table 3. Again, statistics include mean average precision (MAP), precision after 10 documents re-trieved (P@10) and reciprocal rank (R. Rank). The score for each compared method is shown, as well as the percent of improvement made by CTR. Significance tests were not possible due to the lack of availability of individual query results. The improvements made here are again striking, though in some cases less so than the previous comparisons. MAP improvements are the most consistent. It should be kept in mind that these are not perfect comparisons; some performance differences may be due to unknown differences in parsing or stemming, for instance.

Finally, we compared our approach with the published performance of BM25, traditional language model (LM), and title language model (TLM) from Jin et al. [8]. For these comparisons four smaller TREC datasets were used: the As-sociated Press portions of disks two and three ( AP2,AP3 ), San Jose Mercury News from disk three ( SJM ) and Wall Street Journal from disk two ( WSJ2 ). The topic set for these comparisons was the description fields from TREC topics 201 X 250. This topic set is somewhat different than the ones used previously. In particular, the queries are much longer and consist of a complete sentence rather than a few words. The MAP of runs of the topic set against each dataset with each scoring method is shown in Table 4.

Here again we see dramatic improvement using CTR. The title language model performs most similarly to CTR, with CTR only performing slightly better on the SJM dataset, for instance. As with the previous comparisons, these are not perfect due to unknown retrieval system differences. Overall, we can conclude from these comparisons that CTR provides significant improvements in retrieval perfor-mance compared to several traditional and more recent bench-mark methods on a variety of established data and topic sets.
In this work we introduced enhanced relevance scoring with chronological term rank. Chronological term rank is the rank of a term obtained from the sequential ordering of terms in the original document. In particular, we enhanced Okapi BM25 with an additive chronological rank percentage term. The new chronological term rank method produces significant improvements in the major retrieval metrics when compared to the performance of existing relevance scoring formulae on a variety of TREC data and topic sets. Meth-ods compared include Okapi BM25 [15], pivoted length nor-malization [16], traditional language model, title language model of Jin et al. [8], and Global-By-Value and Local-By-Rank models of Anh and Moffat [1, 2]. The CTR approach presented here has also been successfully implemented as part of our vertical digital library prototype 2 , to appear in the future.

Chronological term rank goes beyond the prevailing bag-of-words , term frequency approaches to relevance scoring by incorporating fine-grained information regarding document structure in the relevance estimation process. The chrono-logical term rank method has the advantage of being concep-tually and computationally simple when compared to other document structure approaches such as those incorporating term proximity, term order or document features. Chrono-logical scores of terms are independent of one another, and can be calculated by a single counter at indexing time. For these reasons, CTR can be more easily incorporated into many existing retrieval systems.

Further improvements in retrieval performance using CTR are likely possible. We experimentally evaluated many con-siderations in the use of CTR in order to identify an optimal strategy. These experiments provide a good foundation for future work in the development of other approaches incor-porating chronological term rank. [1] V. N. Anh and A. Moffat. Impact transformation: [2] V. N. Anh and A. Moffat. Simplified similarity scoring http://memsworldonline.case.edu Global-By-Value methods of Anh and Moffat.
 model (LM) and title language model (TLM) from Jin et al. [3] R. Baeza-Yates and B. Ribeiro-Neto. Modern [4] M. Beigbeder and A. Mercier. An information [5] S. B  X uttcher, C. L. A. Clarke, and B. Lushman. Term [6] J. R. Dominick. The Dynamics of Mass [7] D. Hawking and P. Thistlewaite. Relevance weighting [8] R. Jin, A. G. Hauptmann, and C. X. Zhai. Title [9] K. S. Jones. A statistical interpretation of term [10] E. M. Keen. Term position ranking: some new test [11] H. P. Luhn. The automatic creation of literature [12] M. Porter. An algorithm for suffix stripping. Program , [13] Y. Rasolofo and J. Savoy. Term proximity scoring for [14] S. Robertson, H. Zaragoza, and M. Taylor. Simple [15] S. E. Robertson, S. Walker, and M. Beaulieu. Okapi at [16] A. Singhal, C. Buckley, and M. Mitra. Pivoted [17] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. [18] E. M. Voorhees and L. P. Buckland, editors. [19] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and
