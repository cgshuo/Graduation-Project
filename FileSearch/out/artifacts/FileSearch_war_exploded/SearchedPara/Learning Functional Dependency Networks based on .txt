
Bayesian Network (BN) is a powerful network model, which represents a set of variables in the domain and pro-vides the probabilistic relationships among them. But BN can handle discrete values only; it cannot handle contin-uous, interval and ordinal ones, which must be converted to discrete values and the order information is lost. Thus, BN tends to have higher network complexity and lower un-derstandability. In this paper, we present a novel depen-dency network which can handle discrete, continuous, in-terval and ordinal values through functions; it has lower network complexity and stronger expressive power; it can represent any kind of relationships; and it can incorporate a-priori knowledge though user-defined functions. We also propose a novel Genetic Programming (GP) to learn depen-dency networks. The novel GP does not use any knowledge-guided nor application-oriented operator, thus it is robust and easy to replicate. The experimental results demonstrate that the novel GP can successfully discover the target novel dependency networks, which have the highest accuracy and the lowest network complexity.
BN has been applied to different areas [8]. A good BN should balance the accuracy and the network complexity, which is defined in term of the total number of entries in the conditional tables [9]. Variables can be classified into dis-crete, continuous, interval and ordinal variables. Since BN can only handle discrete values, continuous, interval and or-dinal values must be discretized and thus the order informa-tion is ignored [12]. The disability to handle continuous, in-terval and ordinal values increases the network complexity and the relationships represented by BN become less under-standable.

Consider a university programme selection problem that has two ordinal variables. Suppose a high school student
Figure 1. The Bayesian Network for the pro-gramme selection problem. would choose a science programme if he/she got a better grade in biology; he/she would study an art programme if he/she done better in history; otherwise, he/she would study anyone randomly. Figure 1 shows the BN representing the problem. Since BN cannot compare the course grades di-rectly, it enumerates all instances of the combination of the subject grades and calculates the corresponding probabili-ties. Although there are only two subjects, the conditional tables are large and have a lot of entries, i.e. the network complexity is high and the meanings of the relationships are unclear and incomprehensible. The BN shows there are some relationships, but it cannot illustrate that the grade comparison result affects the programme selection. The un-derstandability of the relationships is reduced because there are several entries in the conditional tables.
 Continuous BN is proposed to handle continuous values [5]. They rely on the assumption of parametric or semi-parametric families, like Gaussian distributions. But contin-uous BN cannot handle discrete values. Mixed BN tries to handle both of them, but it cannot represent discrete nodes with continuous parents [3].

We propose a novel dependency network, Functional De-pendency Network (FDN) to incorporate functions into BN. Besides of the relationships among variables, it can also represent the ones among functions of variables. Any kind of functions can be used and different relationships can be represented. They can handle discrete, continuous, inter-val and ordinal values, thus the order information is pre-served. They could be simply a value comparison among variables; they could be any kind of calculations among variables; they could represent any kind of special rela-tionships among variables; and they could incorporate a-priori knowledge as user-defined functions. The functions increase the network X  X  understandability and strengthen its expressive power.

Figure 2 shows the FDN for the programme selection problem. The FDN has a functional node, history &gt; biology . It represents the grade comparison between the two subjects and its conditional table has only one entry specifying the function &gt; . &gt; returns 1 if the first argu-ment is greater than the second one; if the two arguments are equal, it returns 0; otherwise, -1 is returned. With the functional node, the FDN has reduced the number of en-tries in the conditional tables from 6+6+6 2  X  2 , i.e. 84 6+6+3  X  2+1 , i.e. 19 , the network complexity is signifi-cantly reduced. By realizing the meaning of &gt; , it is easy to understand and interpret the meanings of the relationships; if necessary, the relationships can be expressed in rule for-mat easily. For instance, the rules could be merged into an expert system and the number of rules is proportional to the number of entries in the conditional tables.

Wong and Leung proposed an evolutionary program-ming (EP) to learn BN [15]. Myers et al. used a genetic algorithm (GA) to learn BN from incomplete data [11]. However, these methods are not flexible enough to repre-sent functions and thus they cannot learn FDNs.

GP is a branch of evolutionary computation (EC). It has greater representation power than EP and GA. GP has been applied to different areas, like shortest path finding and clas-sification [4]. Wong and Leung proposed a grammar-based GP to handle the closure problem and incorporate a-priori knowledge [14].
 In this paper, we present a novel GP, MDL Genetic Programming (MDLGP) to learn FDNs. Unlike other EC algorithms for BN learning, MDLGP does not use any knowledge-guided nor application-oriented operator, thus it is robust and easy to replicate. The paper is organized as follows. We introduce the FDN in next section, followed by a description of the MDLGP. The experimental results
Figure 2. The Functional Dependency Net-work for the programme selection problem. are presented in Section 3. A conclusion is given in the last section.
BN is a directed acyclic network. Figure 1 shows an ex-ample of BN. A node denotes a variable in the domain and a directed link, N i  X  N j represents the dependency relation-ships between the child N j and the parent N i . Each vari-able node has a conditional table specifying the probability of each particular value of the variable node given an in-stantiation of the parents. In other words, for each variable node N i with parents  X  N i , there is a conditional table spec-ifying the conditional probability distribution P ( N i |  X  for each N i with no parent, the conditional table specify-ing the priori probability distribution P ( N i ) . BN encodes the joint probability distribution of the domain variables, U = { N 1 , ..., N
The FDN is a directed acyclic network. Figure 2 shows an example of the FDN. A new type of node is introduced, functional node, it represents functions of variables. The functions can have any number of arguments and any num-ber of nesting levels. Their conditional tables have only one entry, which specifies the functions producing the value of the functional node given an instantiation of its parents.
Like GP, the function set is defined by the user. Different types of functions operate on different types of variables. For example, it is prohibited to apply the continuous func-tion log to a discrete variable.

Similar to BN, a variable node can handle discrete values only. If a continuous functional node has a variable node as its child, discretization is needed.

A continuous variable node can either have another vari-able node or a continuous functional node or both as its chil-dren. If the child is a variable node, discretized values are produced; if the child is a continuous functional node, con-tinuous values are generated according to a Gaussian dis-tribution function. Each entry in the conditional table rep-resents an interval. To generate a continuous value, 1) se-lect an entry according to the probability, 2) according to Gaussian distribution function, pick up a continuous value randomly given the mean and the standard deviation of the interval.

Although the functions may slightly increase the com-putation complexity, they could reduce the network com-plexity and save the time in searching the large conditional tables. In other words, the overall performance of the FDN is similar to (or even smaller than) that of BN. Moreover, the relationships described by the functions are also more understandable than those represented by a number of en-tries in the conditional tables. 2.2.1 The Population We propose a novel GP, the MDLGP to learn the FDN. It has a population of individuals and each individual represents one FDN. Individuals are of the form, ( &lt; parents &gt;  X  &lt;child&gt; ) 1 ..... ( &lt; parents &gt; ) where y  X  Z + . We call each of the ( &lt; parents &gt;  X  child &gt; ) as a fragment, which represents the relation-ships between the &lt; parents &gt; and the &lt;child&gt; . &lt; parents &gt; and &lt;child&gt; are in the prefix form. &lt; parents &gt; denotes one or more parents and a par-ent can either be a variable node or a functional node. &lt;child&gt; is a variable node. Different fragments can have the same &lt;child&gt; . The fragments containing functional nodes also represent the variables in the functions. For in-stance, the individual representing the FDN in Figure 2 is (( &gt; history biology )  X  art or science ) The MDLGP has great representation power. A FDN can be represented by more than one instantia-tions of individual. For example, both of the indi-viduals ( variable A, variable B  X  variable C ) and ( variable A  X  variable C )( variable B  X  variable C ) represent the same FDN. Any number of nesting levels is also possible.

The MDLGP uses a grammar to prevent the closure problem, i.e. the type mismatching issue among functions and variables. Table 1 shows the grammar.

The MDLGP translates an individual into a network fragment by fragment. Individuals may carry invalid frag-ments, which would create cycles in the network. The MDLGP validates them one by one, starting from the left-most one. If the fragment is valid, it would be translated as links and nodes into the network. If it would create cycle in the network, it would be simply ignored. The advantages of simply ignoring the invalid fragments, instead of repair-ing or preventing them are, 1) a fragment may be invalid in the current instantiation of individual, but may be valid and contributive while it is replicated to the other through ge-netic operators; 2) the crossover can be used. It is an impor-tant genetic operator which can promote the convergence of EC; 3) no repairing nor preventing heuristic is required. Different applications may need different heuristics and it is hard to define a good one. A bad heuristic would heavily degrade the performance; 4) the diversity of the population is promoted. If the invalid fragments are repaired or pre-vented, it may reduce the diversity of the population, which may in turn degrade the performance of the MDLGP.
The population has a fixed number of individuals and the initial population is generated randomly.
 2.2.2 Extended MDL Calculation There are many different scoring schemes to evaluate BN. They measure the fitness of BN, in terms of the accuracy and the network complexity. They are either derived from Bayesian statistics, information theory or MDL principle [10].

The MDLGP uses MDL to evaluate the fitness of indi-viduals. MDL balances between the network X  X  accuracy and simplicity; and it is a combination of the network de-scription length and the data description length. The smaller MDL score is the better.
 We extend MDL to incorporate the functional nodes. Let G be a network, U = { N 1 , ..., N n } be the variables in the domain, V = { F 1 , ..., F f } be the functional nodes in G , be the parents of the variable node or the functional node  X  , s  X  be the number of possible states of the variable node or the functional node  X  and d is the number of bits required to store a numerical value. The MDL score of G is the sum of the MDL scores of U and V .
 a variable node or a functional node  X   X  X  MDL score is given by where  X  is either N i or F j
The network description length measures the number of bits encoding the network. To represent a particular FDN, the following information is necessary and sufficient:  X 
A list of the parents of each variable node and the number of bits required is |  X  N i | log 2 ( n + f ) .  X 
The set of conditional probabilities associated with each variable node and the number of bits required is d ( s N i 1)  X 
The functional description associated with each functional node. For instance, the functional description of the func-tional node ( &gt;variable 1 variable 3) is &gt;v 1 v 3 and the length is 5  X  8 , i.e. 40 bits.

The network description length of a node, ND (  X ,  X   X  ) is where fdl () is the length of the functional description and is measured in term of bits
The data description length measures the number of bits encoding the data set given the network, using Huffman code and the probabilities of occurrences of each instanti-ation in the data set. Closer the probabilities represented by the network to the correct ones smaller the data descrip-tion length, i.e. the data description length reflects the net-work accuracy. Functional nodes X  data description lengths are counted as 0, because 1) they simply apply the func-tions on the parents X  instantiations and the relationships are absolutely certain (according to the MDL principle, the data description length would be calculated as 0 if the relation-ship is certain); 2) the calculation results are not actually encoded in the data set.
 The data description length of a node, DD () is where M () is the number of data items that match a partic-ular instantiation in the data set and e is the total number of data items in the data set (the log 2 function will be M ( X  Sine the MDL calculation is time consuming, the MDLGP uses a multi-level hash table to store the calcu-lation results, MDL (  X ,  X   X  ) . When it encounters the same  X  and  X   X  , it simply retrieves the stored result, instead of performing the calculation again. 2.2.3 Selection and Reproduction The MDLGP selects individuals for reproduction through tournament competition. Each individual competes with a number of randomly chosen individuals. According to the number of winning competitions, fitter individuals are se-lected.

The MDLGP has four genetic operators, they are the mu-tation, the crossover, the insertion and the deletion. The mu-tation and the crossover are the canonical ones of GP. The insertion and the deletion are novel genetic operators, the insertion inserts a randomly generated fragment or a parent into a random position of the selected individual; the dele-tion deletes a randomly chosen fragment or a parent from the selected individual.

After the reproduction, the total number of individuals and offspring is double. To keep the population size remain constant, the worst half of them are destroyed.

Then, the extinction is used to further promote the di-versity of the population [6]. Fitter individuals have higher chance to reproduce more offspring and their offspring also have higher chance to survive. As the learning process goes on, the individuals in the population become similar with each other, i.e. the diversity is decreased. The extinction promotes the diversity by replacing the worst portion of the population with randomly generated individuals.

Once the maximum number of generations is met, the learning process is stopped and the fittest individual in the population is chosen as the final solution.
 Table 2. The pseudocode used to generate
Data Set 1.
We have evaluated the MDLGP and the FDN on one synthetic and two real-life data sets. Table 2 shows the pseudocode used to generate the synthetic data set, Data set 1, that consists of continuous, ordinal and discrete variables. The data set has 7 variables ( history test 1 history test 2 , biology test 1 , biology test 2 , history , biology and art or science ) and 1000 data items. It simu-lates a real-life situation in a school. Suppose there are two subjects, history and biology. Each subject has two tests and the sum of the marks determines the course grade, from A to F. The grade comparison result between the subjects determines if the student would choose a science or an art programme in university.

Data set 2 is a real-life data set that models the psycho-logical experiments reported by Siegler [13]. It is available from UCI machine learning repository [7]. It has 5 con-tinuous variables and 625 data items. Each data item is classified as having the balance scale tip to the right, tip to the left, or be balanced. The variables are left distance , left weight , right distance and right weight . The cor-rect way to find the class is the greater of ( left distance left weight ) and ( right distance  X  right weight ) . If they are equal, it is balanced.

Data set 3 is the Monk data set from UCI machine learning repository [7]. It has 7 nominal variables and 556 data items. The variables are variable 0 , variable 1 variable 2 , variable 3 , variable 4 , variable 5 and variable 6 . The relationships, variable 0= variable 1 and variable 4=1 determine the values of variable 6 . In the experiments, the MDLGP has learnt both of the FDN and BN. They are compared with Belief Network PowerConstructor (PC), WinMine (WM) and B-Course (BC) [1, 2, 12]. The PC and the WM are deterministic algo-rithms. The experimental results are evaluated in terms of,  X 
MDL, the average and the best MDL scores of the final solutions, the smaller the better,  X 
EN, the average and the best total number of entries in the conditional tables of the final solutions, the smaller the lower network complexity,  X 
DDL, the average and the best data description length of the final solutions, the smaller the more accurate,  X  the average and the best structural differences of variable nodes, which are measured in terms of the number of links inserted (LI), the number of links omitted (LO) and the number of links reversed (LR) between the original struc-ture and the final solutions,  X  the average and the best structural differences of func-tional nodes, which are measured in terms of the number of functional nodes inserted (FI) and the number of functional nodes omitted (FO) between the original structure and the final solutions.

The experimental results are obtained from 10 indepen-dent runs.
The values of the maximum number of generations, the tournament competition size, the number of individuals, the extinction portion, the number of discretization levels, the crossover rate, the mutation rate, the insertion rate and the
Figure 3. The network learnt by the MDLGP with the FDN for Data Set 1. deletion rate are 1000, 7, 50, 0.33, 5, 0.3, 0.3, 0.3 and 0.1 respectively.

Table 3 shows the results for Data Set 1. The values in parenthesis are the best results of the runs. Since the PC and the WM are deterministic algorithms, they always got the same results. The MDLGP with the FDN and the MDLGP with BN always converged. Figures 3 and 4 show the net-works learnt by the MDLGP with the FDN and the WM re-spectively. The MDLGP with the FDN has got the smallest values of MDL, EN and DDL. It has learnt the network cor-rectly, which has the highest accuracy and the smallest num-ber of entries in the conditional tables. Among the other al-gorithms, the WM has learnt the most similar and accurate network without functional nodes, but it has got much more entries in the conditional tables. More entries reduce the un-Figure 4. The network learnt by the WM for Data Set 1.
 derstandability of the relationships. Although the WM has found the dependency relationships, it cannot illustrate their meanings. With the functional nodes, the MDLGP with the FDN has reduced the number of entries from 320 to 79 and the FDN provided the meanings of the relationships as well.
The values of the maximum number of generations, the tournament competition size, the number of individuals, the extinction portion, the number of discretization levels, the crossover rate, the mutation rate, the insertion rate and the deletion rate are 1000, 7, 50, 0.5, 5, 0.3, 0.1, 0.3 and 0.3 respectively.

Table 4 shows the results for Data Set 2. Figures 5 and 6 show the networks learnt by the MDLGP with the FDN and the PC respectively. The best network from the MDLGP with the FDN has got the smallest values of MDL and EN. It represents Data set 2 accurately and has had the small-est number of entries in the conditional tables. Among the others, the PC has learnt the most similar network without functional nodes, it has had the smallest value of DDL. Al-though its network represents Data set 2 accurately, it has got a lot of entries in the conditional tables, thus the mean-
Figure 5. The network learnt by the MDLGP with the FDN for Data Set 2.
 ings of the relationships are unclear and incomprehensible. In contrast, the MDLGP with the FDN has significantly re-duced the number of entries from 1895 to 30 and the func-tional node also illustrated the meanings of the relation-ships.
The values of the maximum number of generations, the tournament competition size, the number of individuals, the extinction portion, the number of discretization levels, the crossover rate, the mutation rate, the insertion rate and the deletion rate are 1000, 7, 50, 0.5, 5, 0.2, 0.3, 0.2 and 0.2 respectively.

Table 5 shows the results for Data Set 3. Figures 7 and 8 show the networks learnt by the MDLGP with the FDN, the MDLGP with BN and the BC respectively. Both of the MDLGP with the FDN and the MDLGP with BN are always Figure 6. The network learnt by the PC for
Data Set 2. converged. The MDLGP with the FDN has got the small-est values of MDL and DDL. It has learnt the correct net-work which has the highest accuracy and the second small-est number of entries in the conditional tables. The MDLGP with BN and the BC have achieved the same result, they have got the second smallest values of MDL and DDL, but the worst in EN; they have learnt the most similar network without functional node. Their networks infer the values of variable 6 by enumerating the instances of the combi-nation of variable 0 , variable 1 and variable 4 , thus the conditional tables are large. In contrast, the MDLGP with the FDN has produced a concise and understandable net-work, which illustrated the values of variable 6 are deter-mined by the relationships variable 0= variable 1 and variable 4 . Although the WM has got the smallest value of EN, it has had the worst result in MDL and DDL, i.e. it has learnt the simplest but the most inaccurate network.
In this paper, we propose a novel dependency network with functions, the FDN. It can handle discrete, continuous, interval and ordinal values; it can represent any kind of rela-tionships; and it can incorporate a-priori knowledge as user-defined functions. The FDN has lower network complexity and stronger expressive power than BN. We also present a novel GP, the MDLGP to learn the FDN. The MDLGP with the FDN is evaluated and compared with several algorithms. The experimental results demonstrate that the MDLGP can successfully discover the target FDNs, which have the high-est accuracy and the lowest network complexity.

Figure 7. The network learnt by the MDLGP with the FDN for Data Set 3.
 This work is partially supported by the Earmarked Grant LU 3012/01E of Hong Kong SAR.
 [1] Jie Cheng. Belief network powerconstructor. [2] David Maxwell Chickering. The winmine toolkit. [3] D.E. Edwards. Hierarchical interaction models. J. [4] Alex A. Freitas, editor. Data Mining and Knowledge [5] N. Friedman and I. Nachman. Gaussian procces net-[6] G.W. Greewood, G.B. Fogel, and M. Ciobanu. Em-[7] S. Hettich, C.L. Blake, and C.J. Merz. Uci repository
Figure 8. The network learnt by the MDLGP with BN and the BC for Data Set 3. [8] Chrisman L. A roadmap to research on bayesian net-[9] Wai Lam. Bayesian network refinement via machine [10] Wai Lam and Fahiem Bacchus. Learning bayesian be-[11] J.W. Myers, K.B. Laskey, and T.S. Levitt. Learning [12] P. Myllymaki, T. Silander, H. Tirri, and P. Uronen. [13] R.S. Siegler. Three aspects of cognitive development. [14] Man-Leung Wong and Kwong-Sak Leung. Evolving [15] Man Leung Wong and Kwong Sak Leung. An effi-
