 Community Question Answering (CQA) sites have become valu-able platforms to create, share, and seek a massive volume of hu-man knowledge. How can we spot an insightful question that would inspire massive further discussions in CQA sites? How can we de-tect a valuable answer that benefits many users? The long-term impact (e.g., the size of the population a post benefits) of a ques-tion/answer post is the key quantity to answer these questions. In this paper, we aim to predict the long-term impact of questions/answers shortly after they are posted in the CQA sites. In particular, we pro-pose a family of algorithms for the prediction problem by modeling three key aspects, i.e., non-linearity, question/answer coupling, and dynamics. We analyze our algorithms in terms of optimality, cor-rectness, and complexity. We conduct extensive experimental eval-uations on two real CQA data sets to demonstrate the effectiveness and efficiency of our algorithms.
 H.2.8 [ Database Management ]: Database applications X  Data min-ing Question answering; long-term impact; impact correlation
Community Question Answering (CQA) sites, such as Stack Over-come valuable platforms to create, share, and seek a massive vol-ume of human knowledge. How can we spot an insightful question that would inspire massive further discussions in these CQA sites? How can we detect an valuable answer that benefits many users? The long-term impact of a question/answer post, which is the size of population it benefits in total, is the key qua ntity to answer these questions. How can we predict the L ong-term I mpact of a P ost (ei-ther a question or an answer) shortly after it is posted on the CQA site? This, which we refer to as the LIP problem in this paper, is an essential task for the prosperity and sustainability of the CQA ecosystem that benefits all types of its users, including the infor-mation producers, spreaders, and consumers.

Despite its importance, it is not an easy task to predict the long-term impact of question/answer posts. Consequently, there are very few dedicated, existing tools for this problem (see Section 6 for a review). We summarize two major challenges below.

The first challenge lies in the multi-aspect of the long-term im-pact of question/answer posts. The user rating/voting mechanism within most of the CQA sites often provides a good measure of the long-term impact of CQA posts. For example, the voting score in Stack Overflow directly tells how many site users find the cor-responding question/answer is beneficial to him/her. This naturally leads us to cast the LIP problem as a (supervised) data mining prob-lem. Nonetheless, this problem has its own characteristics, making any off-the-shelf data mining algorithm sub-optimal for this prob-lem. To be specific, while each of the following aspects might af-fect the long-term impact of question/answer posts, they require different treatments in the data mining algorithms. How can we build a comprehensive model to capture all these aspects to maxi-mally boost the prediction accuracy?
The second challenge is the computation . Each of the above aspects (non-linearity, the question-answer coupling, and the dy-namics) will add the extra complexity into the mining process. For example, while many machine learning algorithms (e.g., kernel re-gression, support vector regression, etc.) exist to capture the non-linearity between the features and outputs (long-term impact score in our case), they typically require at least O ( n 2 ) in time and space complexity, where n is the total number of the training examples. Moreover, when the new training examples arrive in a stream, ever-growing fashion, even an O ( n ) algorithm might be too expensive. How can we make our prediction algorithms scalable to millions of CQA posts and adaptive to the newly arrived training examples over time?
In this paper, we aim to address both challenges by proposing a family of algorithms for predicting the long-term impact of ques-tion/answer posts. Our algorithms enjoy three key advantages. First, they are comprehensive in the sense that our model naturally cap-tures all the above three key aspects (i.e., non-linearity, coupling, and dynamics) that matter with the long-term impact of a post. Sec-ond, they are flexible and general , being able to handle the special cases where only a fraction of these aspects are prominent. For example, in some CQA sites, we might be only interested in the prediction of answers posts (such as Yahoo! Answers), and/or the features may have a linear effect on the post impact, etc. Third, they are scalable and adaptive to the newly arrived examples. For ex-ample, one of our algorithms (LIP-KIMAA) has a sub-linear com-plexity in both time and space. On the Stack Overflow data set with more than 3 million posts, this LIP-KIMAA algorithm can build and update our model in seconds, while straightforward ap-proaches would take hours.

The main contributions of this paper are summarized as follows:
The rest of the paper is organized as follows. Section 2 describes the problem definition. Section 3 presents the proposed algorithms. Section 4 discusses some variants. Section 5 presents the experi-mental results. Section 6 reviews related work, and Section 7 con-cludes the paper.
In this section, we first define the LIP problem, and then present its solution space to illustrate the relati onship between our proposed algorithms and the existing work. Table 1 lists the main symbols we use throughout the paper. For convenience, we use bold capital letters for existing matri-ces/vectors at time t , and bold lower case letters for newly arrived matrices/vectors at time t + 1. We use superscript (i.e., q or a )to distinguish questions and answers, and use subscript (i.e., t , t etc.) to indicate time. For example, we use F q t to denote the feature matrix for questions at time t ,and f q t + 1 to denote the feature matrix of newly arrived questions at time t + 1. Each row of F q tains the feature vector for the corresponding question. Similarly, we use Y q t to denote the vector of impact scores at time t ,and y to denote the vector of impact scores from new questions at time t + 1. Following conventions, we use calligraphic letter K q K t to denote the kernel matrix for questions and answers at time t . We will omit the subscript when the meaning of matrices/vectors Symbol Definition and Description
F f + 1 , f a t + 1 the features of new questions/answers at time t k k
U
U
M t the row-normalized association matrix between existing m t + 1 the row-normalized association matrix between new
Y y + 1 , y a t + 1 the impact score for new questions/answers at time t n q , n a the number of existing questions/answers at time t , i a the number of new questions/answers at time t + 1 d the feature dimension r the rank of U q t ,  X  q t , U a t ,and  X  a t th the threshold for the filtering step is clear in the context. We use the row-normalized n q  X  n trix M t to denote the association between questions and answers at belongs to the i th question. Thus, the matrix M t is sparse since it contains only n a non-zero elements. We also use F q t ( i vector Y q t ,and( F q t ) to represent the transpose of F
Based on the above notations, we define the LIP problem in its static form as: P ROBLEM 1. Static LIP Problem Given: the question/answer feature matrix F q / F a , the question/answer Output: the impact of new questions and their answers.

In real CQA sites where questions and answers continuously ar-rive, we need to update the model to keep it up-to-date. To this end, we define the following dynamic form of the LIP problem: P ROBLEM 2. Dynamic LIP Problem Given: the question/answer feature matrix F q t / F a t and the newly Output: the impact of new questions and their answers.
Solution Space . Let us first define the solution space of the LIP problem, which is represented by a genealogy graph in Fig. 1. In this paper, we consider three key aspects that matter with the pre-diction performance, including (a) whether the predication mod-els are linear or non-linear; (b) whether we treat the prediction of the questions and the answers separately (single) or jointly (cou-pling); and (c) whether the prediction is static or dynamic. Based on these three aspects, we could have different variants of the pre-diction algorithms for the LIP problem, whose intrinsic relationship is also summarized in Fig. 1. In the figure, we use the letter K , M , and I to denote non-linearity, coupling, and dynamics, respectively. Figure 1: The solution space of LIP problem. Shaded boxes are proposed algorithms; and white boxes are existing work. For example, LIP-KIM means that our model is non-linear and dy-namic, and it jointly predict the long-term impact of questions and answers; LIP-K means that our prediction model is non-linear and static, and it treats questions and answers separately, etc.
In Fig. 1, each upward solid arrow makes the model more com-prehensive by modeling more aspects in the prediction algorithms, and each dashed arrow makes the algorithms more scalable. For example, starting with the ridge regression algorithm in the bottom layer, we have the kernel ridge regression (KRR) [25] by incorpo-rating non-linearity, and we have the recursive ridge regression [15] by incorporating dynamics. If we incorporate both non-linearity and dynamics, we have the recursive kernel ridge regression algo-rithm (RKRR) [11] in the third layer.

Preliminaries . In Fig. 1, we use shaded boxes to indicate the algorithms proposed in this paper, and while boxes to indicate ex-isting work. Before presenting the proposed algorithms in the next section, we first briefly review some existing work (e.g., white boxes), which serves as the building blocks of our proposed al-gorithms. (A) Linear Co-Prediction . In our tech report [29], we proposed a regularized optimization formulation to jointly predict the voting score of questions and answers min where parameters  X  and  X  are used to control regularization and the importance of the coupling between questions and answers, respec-tively. (B) Kernel Ridge Regression . In order to capture the non-linearity between the features and outputs, a natural choice is to user kernel-ized methods. Take question impact prediction as an example, the so-called kernel ridge regression [25] aims to estimate a coefficient as follows In this section, we propose our solutions for the LIP problem. We start with presenting two algorithms for Problem 1 (subsection 3.1) and Problem 2 (subsection 3.2), respectively; and then address the computational challenges (subsection 3.3-3.4).
Here, we address the static LIP problem (Problem 1). We pro-pose an algorithm (LIP-KM) to capture both the non-linearity and the coupling aspects.

For the non-linear aspect, a natural choice is to kernelize a linear prediction model (e.g., linear ridge regression). Recall that kernel method aims to produce non-linear versions of linear learning algo-rithms by mapping the data points into a high-dimensional Hilbert space H with a non-linear function  X  [4]. The key idea behind kernel methods is to use the kernel functions to replace the inner-product operations in the high-dimensional Hilbert space H such replacement can be ensured by Mercer X  X  Condition [6]. In of  X  ( F ( i , :)) and  X  ( F ( j , :)) in the Hilbert space H computed by a Mercer kernel  X  ( F ( i , :) , F ( j , :))  X  ( F ( i , :) , F ( j , :)) = &lt; X  ( F ( i , :)) , X  ( F ( j Eq. (3), we can derive the non-linear models without any explicit knowledge of either  X  or H . Common kernel functions include Gaussian kernel, polynomial kernel, cosine kernel, etc.
For the coupling aspect, LIP-KM imposes a so-called impact consistency on the prediction space by requiring the predicted im-pact of a question to be close to that of its answer (see our tech report [29] for the detailed explanations about its rationality).
Putting the non-linearity and couplin g aspects together, we have the following optimization formulation for Problem 1 min where  X  is a weight parameter to control the importance of cou-pling,  X  is a regularization parameter, and K q and K a are the kernel K q ( i , j ) =  X  ( F q ( i , :) , F q ( j , :)), and K a ( i
Eq. (4) can be solved by the closed-form solution where  X  = [  X  q ;  X  a ]. Once the coefficient vectors  X  q inferred from the above equation, the impact of questions/answers can then be predicted as where the kernel matrices on the test set F q test and F a puted as K q test ( i , j ) =  X  ( F q test ( i , :) , F q ( j F a ( j , :)).

Algorithm Analysis . Let us analyze the effectiveness and effi-ciency of the LIP-KM algorithm (i.e., Eq. (5)). We first summarize the optimality of LIP-KM in the following lemma, which states that LIP-KM finds (at least) a local minimum for the static LIP problem.
L EMMA 1. Optimality of LIP-KM .Eq. (5) finds a local min-imum for Eq. (4) .

P ROOF . Omitted for brevity. Algorithm 1 The LIP-KIM Algorithm.
 1: compute the new kernels k q t + 1 , h q t + 1 , k a t + 2: compute S 1 , S 2 , S 3 , D ,and E 1 as Eq. (9) -(13); 3: update S  X  1 t + 1 as E 1 S 4: update  X  t + 1 as E 1
Next, we summarize the time complexity and space complexity of LIP-KM in the following lemma, which basically states that LIP-KM requires O (( n q + n a ) 3 ) time and O (( n q + n a ) 2 ) space. L EMMA 2. Complexity of LIP-KM . The time complexity of Eq. (5) is O (( n q + n a ) 2 + ( n q + n a ) d ) .

P ROOF . Omitted for brevity.
Here, we address the dynamic LIP problem (Problem 2). We present the LIP-KIM algorithm to incrementally update the model in Eq. (5). The basic idea of LIP-KIM is to incorporate the dynamic aspect into LIP-KM, and therefore it is adaptive to newly arrived training examples.

When new questions and answers arrive at time t + 1, we first need to compute the new kernel matrices K q t + 1 and K a as follows where the kernel matrices involving the newly arrived examples can be computed as: k q t + 1 ( i , j ) =  X  ( f q t + 1 ( i  X  ( f  X 
To simplify the algorithm description, let us introduce the fol-lowing matrices
With these extra notations, we present our LIP-KIM algorithm for solving Problem 2, which is summarized in Alg. 1. As we can see from the algorithm, we re-use S  X  1 t and  X  t from previous compu-tations. Therefore, we also need to update S  X  1 t + 1 and iterations. After we compute the new kernels as well as the matri-ces (i.e., S 1 , S 2 , S 3 and D ) that are based on the new kernels, we can update the model in Steps 3-4. In these two steps, E 1 a permutation matrix to exchange the corresponding rows/columns at time t + 1, respectively.

Algorithm Analysis . The correctness of LIP-KIM is summarized in the following theorem, which states that LIP-KIM can find the same coefficients (i.e.,  X  t + 1 ) as if we apply the LIP-KM algorithm whenever we have new training examples.

T HEOREM 1. Correctness of LIP-KIM .Let  X   X  t + 1 be the output of Eq. (5) at time t + 1 , and  X  t + 1 be the output of Alg. 1 updated from time t to t + 1 , we have that  X  t + 1 =  X   X  t + 1 .
P ROOF . Notice that Eq. (5) can be re-written as Therefore, we need to first prove the update procedure for S Step3inAlg.1). S t + 1 can be written as where M t + 1 = [ M t , 0 ; 0 , m t + 1 ], and K q t + 1 and Eq. (7).

By exchanging the rows and columns of S t + 1 (i.e., moving all the features at time t into the upper-left corner), we have where S 1 , S 2 , S 3 and E 1 are specified in Eq. (9), Eq. (10), Eq. (11) and Eq. (13), respectively.

Applying Matrix Inversion Lemma [24, 14] to Eq. (14), we have that where D is specified in Eq. (12), and E  X  1 1 = E 1 .

Based on the updated S  X  1 t + 1 ,wehavethat which completes the proof.

The time complexity and space complexity of our LIP-KIM is summarized in the following lemma, which basically states that LIP-KIM requires O (( n q + n a ) 2 ) time and O (( n q + L EMMA 3. Complexity of LIP-KIM . The time complexity of Alg. 1 is O (( n q + n a ) 2 ( i q + i a ) + ( n q + n a )( i P ROOF . Omitted for brevity.
 feature dimension d is a fixed constant, the time and space com-plexity of LIP-KIM can be re-written as O (( n q + n a ) 2 ). Compared with LIP-KM which is cubic in time, LIP-KIM is much more ef-ficient. However, it is still quadratic wrt the total number of the training examples. In the next two subsections, we propose two approximate algorithms to further speed-up the computation.
The reason that LIP-KIM is quadratic is that we need to maintain two kernel matrices of the size n q  X  n q and n a  X  n a , respectively. In order to avoid quadratic cost for both time and space, we need an efficient way to approximate/compress the full kernel matrices and update them over time.
 Take the kernel matrix for questions as an example. Notice that t is symmetric and semi-positive definite; therefore, we can ap-proximate it by eigen-decomposition: K q t  X  U q t  X  q t ( U is an n q  X  r orthogonal matrix, and  X  q t is an r  X  r diagonal matrix whose entries are the largest r eigenvalues of K q t . By doing so, we reduce the space cost from O ( n 2 q )to O ( n q r ).

When new questions arrive at time t + 1, we have the new kernel matrix K q t + 1 as shown in Eq. (7). We approximate K q t method [10] as where we define X 1 as the ( n q + i q )  X  r matrix [ U q
To make the decomposition of K q t + 1 reusable for future updates, we need to find the eigen-decomposition form of K q t + 1 end, we first perform the Singular Value Decomposition (SVD) on X perform eigen-decomposition on an r  X  r matrix X 2 = X  q 1 that is, X 2 = V  X  q V .

Based on the above two steps, we have the approximate eigen-decomposition of the new kernel matrix K q t + 1 as follows where we define U q t + 1 = PV and  X  q t + 1 = X  q . Notice that orthogonal because both P and V are orthogonal.

We use the same approach to approximate and update the kernel matrix for answers: K a t + 1  X  U a t + 1  X  a t + 1 ( U a Algorithm 2 The LIP-KIMA Algorithm.
 1: compute the new kernels k q t + 1 and k a t + 1 in Eq. (7); 3: define U ,  X  and G as Eq. (17); 4: define A and B as [ U , X  GU ]and[  X  U ;  X  U ]; 5: update  X  t + 1 as 1  X  ( I  X  A (  X  I + BA )  X  1 B ) Y following notations to simplify the algorithm description
Then, we have the following approximation for the S t + 1 defined in Eq. (8) wherewedefine A = [ U , X  GU ]and B = [  X  U ;  X  U ].

Finally, applying Matrix Inversion Lemma to Eq. (18), we have the coefficients  X  t + 1 The complete algorithm of LIP-KIMA is summarized in Alg. 2. As we can see, in addition to  X  t + 1 , the only variables we need to we do not need to store S t ; instead, we only need to store the much smaller matrices of U q t ,  X  q t , U a t and  X  a t .

Algorithm Analysis .The effectiveness of LIP-KIMA is summa-rized in Lemma 4. According to Lemma 4, there are two possible places where we could introduce the approximation error in the LIP-KIMA algorithm, including (a) eigen-decomposition for the K t and (b) the Nystr X m method for do eigen-decomposition at t = 1, such approximation error might be accumulated and amplified over time. In practice, we could  X  X e-start X  the algorithm every few time ticks, that is, to re-compute (as opposed to approximate) the eigen-decomposition f or the current kernel matrix.

L EMMA 4. Effectiveness of LIP-KIMA .Let  X   X  t + 1 be the output of Eq. (5) at time t + 1 , and  X  t + 1 be the output of Alg. 2 updated from time t to t + 1 , we have  X  t + 1 =  X   X  t + 1 if K t h
P ROOF . Omitted for brevity.
The time complexity and space complexity of Alg. 2 is summa-rized in the following lemma. It basically says that the LIP-KIMA algorithm requires linear time and space wrt the total number of questions and answers.
 L EMMA 5. Complexity of LIP-KIMA . The time complexity of complexity of Alg. 2 is O (( n q + n a + i q + i a ) d + ( n P ROOF . Omitted for brevity.
 low rank r and feature dimension d are fixed constants, the time complexity and space complexity of Alg. 2 can be re-written as O ( n q + n a ) in terms of the total number of questions and answers.
Compared with LIP-KIM, LIP-KIMA is much more scalable, being linear in terms of both time and space complexity. However, if the new training examples arrive in a stream-like, ever-growing fashion, a linear algorithm might be still too expensive. To address this issue, we further present the LIP-KIMAA algorithm to reduce the complexity to be sub-linear .

Our LIP-KIMAA is built upon LIP-KIMA. The main difference between LIP-KIMAA and LIP-KIMA is that we add an additional filtering step between Step 1 and Step 2 in Alg. 2. That is, when new questions and answers arrive at time t + 1, we first treat them as test set and apply the existing model at time t on this test set. Based on the prediction results, we only add the questions and answers whose prediction error is larger than a given threshold th . Notice that the complexity of LIP-KIMA is linear wrt the number of ques-tions and answers; as a result, our LIP-KIMAA scales linearly wrt the number of remaining questions and answers after the filtering steps. Therefore, LIP-KIMAA scal es sub-linearly wrt to the total number of questions and answers in both time and space. We omit the detailed algorithm for brevity. The proposed LIP-KIM and its two approximate algorithms (LIP-KIMA and LIP-LIMAA) are comprehensive . In terms of the mod-eling power, they capture all the three aspects (non-linearity, cou-pling, and dynamics). In this section, we show that our algorithms are also flexible . That is, if only a subset of these three aspects mat-ter with the prediction performance for some applications, our al-gorithms can be naturally adapted to these special cases. We briefly discuss some of these variants, and then summarize the algorithms in Fig. 1.
If we only consider the coupling and dynamic aspects (i.e., ig-noring the non-linear aspect), our LIP-KIM can be simplified as the LIP-IM algorithm. Essentially, LIP-IM aims to efficiently update the solution of Eq. (1)  X  = (  X  + 1)( F q ) F q +  X  I  X   X  ( F q ) MF a  X   X  where  X  = [  X  q ;  X  a ].
 In this case, the S t matrix becomes
S
When new questions and answers arrive at time t + 1, we define the L matrix and the R matrix as L = (
As we can see, both L and R are only based on new observations f
Next, we further define matrix C = S  X  1 t L ( I + RS  X  1 can show that the model can be updated as follows
Remarks . Notice that compared with LIP-KIM, LIP-IM is much more efficient since it only needs to compute the inverse of a much C ). By ignoring the smaller terms (i.e., i q and i a ), the overall time complexity of LIP-IM is O ( d 2 )where d indicates feature dimen-sion; on the other hand, directly updating the model would require the proof for Theorem 1, we can show that LIP-IM finds the exact solution of Eq. (1).
If we only consider the non-linearity and dynamics aspects (i.e., ignoring the coupling aspect), our LIP-KIMA can be further simpli-fied. Take the prediction for questions as an example. With the ap-proximate eigen-decomposition by Eq. (16): K q t + 1  X  U q we can update the coefficients  X  q t + 1 as follows where  X  q 3 is still a diagona l matrix with  X  q 3 ( i , i ) Remarks . Although sharing the same linear time complexity as LIP-KIMA, this variant is even more efficient in practice since it does not need any matrix inversion at all.
Next, we discuss a special case of LIP-KIM when only new ques-tions arrive at time t + 1. Compared to Alg. 1, we can simplify the following notations Then the model can be updated as
Remarks . The time complexity of this variant is O (( n q of this variant can also be shown by following a similar procedure as the proof for Theorem 1. The right five columns are the proposed algorithms in this paper.
Finally, we make a comparison of different algorithms in Fig. 1 in terms of their modeling power and efficiency. The results are summarized in Table 2. In this table, we first check whether a given algorithm captures each of the three desired aspects (non-linearity, coupling, and dynamics). As we can see, only our LIP-KIM, LIP-KIMA, and LIP-KIMAA algorithms meet this require-ment. We also summarize the time complexity of the algorithm for updating the model at each time tick, where the smaller terms (e.g., the feature dimensionality d , the number of new training examples KRR [25], support vector regression (SVR) [9, 7], RKRR [11], LIP-KM and LIP-KIM, they need at least quadratic time for the model training because they typically need to maintain the kernel matrix. Such a complexity is unaffordable in many CQA sites. On the other hand, the time complexity of the proposed LIP-KIMA and LIP-KIMAA algorithms is linear and sub-linear wrt the total number of questions and answers, respectively.
In this section, we present the experimental evaluations. The experiments are designed to answer the following questions:
We use the data from two real CQA sites, i.e., Stack Overflow (SO) and Mathematics Stack Exchange (Math), to evaluate our al-gorithms. They are popular CQA sites for programming and math, respectively. The statistics of the two data sets are summarized in Table 3.

We use both content and contextual features. For content fea-tures, we adopt the  X  X ag of words X  model to extract content features after removing the infrequent words. This model is widely used in natural language processing where the frequency of each word is used as a feature for training. In addition, we adopt some com-monly used contextual features in the literature, including the ques-tioner X  X  reputation at question creation time, the answerer X  X  repu-tation at answer creation time, the length of the question/answer, the number of questioner X  X  previous questions at question creation time, and the number of answerer X  X  previous answers at answer creation time. For these features, we can extract them at the mo-ment when the question/answer is posted. For other contextual features, we need to choose a short time window by the end of which the voting score is predicted. With such a fixed time win-dow, we can include some time-related features such as the number of comments/answers received during the fixed time window. In this work, we fix this time window as 24 hours. Overall, we have 4,180 and 4,444 features for Math data and SO data, respectively (i.e., d = 4 , 180 for Math data and d = 4 , 444 for SO data). For the kernel matrix, we adopt the cosine kernel function due to the sparsity of our feature matrix.

For long-term impact, we restrict our attention to predict the im-pact of a question/answer after it is posted for six months. For each post in the data set, there are several choices to measure impact in-cluding the number of pageviews, the number of favorites, and the user voting score (which is the difference between the number of up-votes and the number of down-votes on the post). In this work, we choose the voting score for the following two reasons. First, we conduct a survey, asking different users about which is the best metric as the long-term impact measure of CQA posts; and most of the users (79.4%) choose the voting score. Second, to some ex-tent, the voting score of a question/answer resembles the number of the citations that a research paper receives in the scientific pub-lication domain. It reflects the net number of users who have a positive attitude toward it. In addition, we also measured the cor-relation between the three choices and found that all of them are strongly positive correlated. Thus, we expect that our algorithms could also be used to predict the other two metrics (i.e., pageviews and favorites). We normalize the voting scores into the range of [0 , 1].

To evaluate the dynamic aspect of our algorithms, we start with a small initial training set, and gradually add new examples into the training set in chronological order. For Math data, we start with 5% initial data, add 5% data for each update, and use the latest 10% data as the test set. For SO data whose size is much larger than the Math data, we start with 0.1% initial data, add 0.1% data for each update, and use the latest 0.1% data as the test set.

For evaluation metrics, we adopt the root mean square error (RMSE) between the real impact and the estimated impact for effectiveness, and the wall-clock time for efficiency. All the efficiency experi-ments were run on a machine with eight 3.4GHz Intel Cores and 24GB memory.

Repeatability of Experimental Results. Both data sets are of-make the code of the proposed algorithms as well as the extracted feature files publicly available. For all the results reported in this section, the specific parameter settings are as follows. We set  X  = 1 for Math data and  X  =  X  = 0 . 1 for SO data. For the low rank r in LIP-KIMA and LIP-KIMAA, we set it as 10. For LIP-KIMAA, we set th = 0 . 22 for Math data and th = 0 . 18 for SO data.
We first compare the effectiveness of the proposed algorithms with two state-of-the-art non-linear regression methods, i.e., kernel http://blog.stac koverflow.com/category/cc-wiki-dump/ sub-linearly (in the upper-right corner). ridge regression (KRR) [25] and support vector regression (SVR) [7]. The prediction results of questions and answers on the two data sets are shown in Fig. 2. On SO data, we only report the first few points because some of the algorithms (e.g., KRR) cannot finish training within 1 hour. We do not report the results by linear models (e.g., linear ridge regression) since their performance (RMSE) is much worse than SVR.
 We make several observations from Fig. 2. First, the proposed LIP-KIM algorithm performs the best in most of the cases. For ex-ample, when the size of training set increases to 90% on the Math data, LIP-KIM improves the SVR method by 5.7% for questions and 6.0% for answers. On SO data, LIP-KIM improves the KRR method by up to 35.8% for questions and 3.6% for answers. This indicates that the coupling aspect indeed helps in impact predic-tion. Second, the performance of the proposed LIP-KIMA algo-rithm is close to LIP-KIM. This result indicates that while it re-duces the time complexity from quadratic to linear, the approxima-tion method introduces little performance lo ss. Finally, although not as good as LIP-KIM and LIP-KIMA, the LIP-KIMAA algo-Table 4: Performance gain analysis. Smaller is better. All three aspects of non-linearity, coupling, and dynamics are helpful. rithm is still better than the compared methods for most of the cases.

To further show the effects of all the three aspects (i.e., non-linearity, coupling, and dynamics), we analyze the performance gain in Table 4. In the table, LIP-K incorporates non-linearity into ridge regression, LIP-KM incorporates coupling into LIP-K, and LIP-KIM incorporates dynamics into LIP-KM. As we can see, all three aspects are helpful to improve the prediction performance. Next, we compare the efficiency of the proposed algorithms with KRR and SVR in Fig. 3. Notice that the y-axis is in log scale. In Fig. 3, we also plot the results of LIP-KIMAA with y-axis in linear scale in the upper-right corner. We only report the results by LIP-KIMAA there because it is the only algorithm that can handle the entire SO data set.

As we can see from the figure, our LIP-KIMA and LIP-KIMAA are much faster than the other algorithms. In the upper-right corner, we can observe that the LIP-KIMAA scales sub-linearly wrt the to-tal number of questions and answers. For instance, it only requires about 60 seconds when there are more than 3,000,000 questions and answers. In contrast, KRR requires more than 2,000 seconds when the size of the training set is about 30,000.

Finally, we study the quality-speed balance-off of different al-gorithms in Fig. 4. In the figure, we show the answer prediction results only. Similar results are observed in question prediction, and we omit the results for brevity. In Fig. 4, we plot the RMSE on the y-axis and the wall-clock time on the x-axis. We also plot the results of the linear co-prediction method CoPs [29] and LIP-KM. Ideally, we want an algorithm sitting in the left-bottom corner. As we can see, both our LIP-KIMA and LIP-KIMAA are in the left-bottom corner. For example, for answer impact prediction on the SO data, compared with SVR, LIP-KIMAA is 70x faster in wall-clock time and 14.0% better in RMSE. Overall, we recommend LIP-KIMAA in practice. In this section, we briefly review related work including mining CQA sites and mining stream data.

Mining CQA Sites: There is a large body of existing work on mining CQA sites. For example, Li et al. [19] aim to predict ques-tion quality, which is defined as the combination of user attention, answer attempts and the arrival speed of the best answer. Jeon et al. [18] and Suryanto et al. [28] evaluate the usefulness of an-swer quality and incorporate it to improve retrieval performance. To predict the quality of both questions and answers, Agichtein et al. [2] develop a graph-based model to catch the relationships among users, Li et al. [20] adopt the co-training approach to employ both question features and answer features, and Bian et al. [5] pro-pose to propagate the labels through user-question-answer graph so as to tackle the sparsity problem where only a small number of questions/answers are labeled. Recently, Anderson et al. [3] propose to predict the long-lasting value (i.e., the pageviews) of a question and its answers. How to predict the answer that the questioner will probably choose as the accepted answer is also well studied [22, 26, 1]. Overall, our work differs from these existing work at the methodology level. While most of the existing work treats the prediction problem as a single, and/or linear, and/or static problem, we view the problem from a comprehensive perspective and propose to incorporate all these important aspects into the pre-diction models.

Mining Stream Data: From the dynamic aspect, our LIP prob-lem is related to stream mining [13] and time-series mining [12]. The main focus of existing stream/time-series mining work is on pattern discovery, clustering, and classification tasks. Chen et al. [8] and Ikonomovska et al. [17] study the regression problem in data streams; however, they still focus on a single and linear predic-tion problem. Several researchers also consider the non-linear and dynamic aspects in regression problem [11, 23]. Different from these existing work, we consider the coupling between questions and answers, and propose approximation methods to speed-up and scale-up the computation.

Other Related Work: There are several pieces of interesting work that are remotely related to our work. Liu et al. [21] propose the problem of CQA site searcher satisfaction, i.e., whether or not the answer in a CQA site satisfies the information searcher using the search engines. Shtok et al. [27] attempt to answer certain new questions by existing answers. The question routing problem (e.g., how to route the right question to the right answerer) is also an active research area [30, 16].
In this paper, we have proposed a family of algorithms to pre-dict the long-term impact of questions/answers in CQA sites. The proposed algorithms enjoy three key advantages. First, they are comprehensive in the sense that our model naturally captures three key aspects (i.e., non-linearity, coupling, and dynamics) that mat-ter with the long-term impact of a post. Second, they are flexible and general , being able to handle the special cases where only a fraction of these aspects are prominent. Third, they are scalable and adaptive to the newly arrived questions and answers. We an-alyze our algorithms in terms of optimality , correctness ,and com-plexity , and reveal the intrinsic relationship among different algo-rithms. We conduct extensive experimental evaluations on two real CQA data sets to demonstrate the effectiveness and efficiency of our approaches.
This work is supported by the National 863 Program of China (No. 2012AA011205), and the National Natural Science Founda-tion of China (No. 91318301, 61321491, 61100037). This material is partially supported by by the National Science Foundation un-der Grant No. IIS1017415, by the Army Research Laboratory un-der Cooperative Agreement Number W911NF-09-2-0053, by De-fense Advanced Research Projects Agency (DARPA) under Con-tract Number W911NF-11-C-0200 and W911NF-12-C-0028, and by Region II University Transportation Center under the project number 49997-33 25.

The content of the information in this document does not nec-essarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] L. Adamic, J. Zhang, E. Bakshy, and M. Ackerman.
 [2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [3] A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec. [4] N. Aronszajn. Theory of reproducing kernels. Transactions [5] J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. Learning [6] C. J. Burges. A tutorial on support vector machines for [7] C.-C. Chang and C.-J. Lin. Libsvm: a library for support [8] Y. Chen, G. Dong, J. Han, B. W. Wah, and J. Wang.
 [9] R. Collobert and S. Bengio. Svmtorch: Support vector [10] P. Drineas and M. W. Mahoney. On the nystr X m method for [11] Y. Engel, S. Mannor, and R. Meir. The kernel recursive [12] T.-c. Fu. A review on time series data mining. Engineering [13] M. M. Gaber, A. Zaslavsky, and S. Krishnaswamy. Mining [14] G. Golub and C. Van Loan. Matrix computations. 1996. [15] S. S. Haykin. Adaptive filter theory . 2005. [16] D. Horowitz and S. Kamvar. The anatomy of a large-scale [17] E. Ikonomovska, J. Gama, and S. D X eroski. Learning model [18] J. Jeon, W. Croft, J. Lee, and S. Park. A framework to predict [19] B. Li, T. Jin, M. R. Lyu, I. King, and B. Mak. Analyzing and [20] B. Li, Y. Liu, and E. Agichtein. Cocqa: co-training over [21] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, Y. Maarek, [22] Y. Liu, J. Bian, and E. Agichtein. Predicting information [23] B. Pan, J. J. Xia, P. Yuan, J. Gateno, H. H. Ip, Q. He, P. K. [24] W. W. Piegorsch and G. Casella. Inverting a sum of matrices. [25] C. Saunders, A. Gammerman, and V. Vovk. Ridge regression [26] C. Shah and J. Pomerantz. Evaluating and predicting answer [27] A. Shtok, G. Dror, Y. Maarek, and I. Szpektor. Learning [28] M. Suryanto, E. Lim, A. Sun, and R. Chiang. Quality-aware [29] Y. Yao, H. Tong, T. Xie, L. Akoglu, F. Xu, and J. Lu. Want a [30] Y. Zhou, G. Cong, B. Cui, C. Jensen, and J. Yao. Routing
