 Sentiment classification is an important problem in tweets mining. There lack labeled data and rating mechanism for generating them in Twitter service. And topics in Twit-ter are more diverse while sentiment classifiers always ded-icate themselves to a specific domain or topic. Thus it is a challenge to make sentiment classification adaptive to di-verse topics without sufficient labeled data. Therefore we formally propose an adaptive multiclass SVM model which transfers an initial common sentiment classifier to a topic-adaptive one. To tackle the tweet sparsity, non-text features are explored besides the conventional text features, which are intuitively split into two views. An iterative algorithm is proposed for solving this model by alternating among three steps: optimization, unlabeled data selection and adaptive feature expansion steps. The algorithm alternatively mini-mizes the margins of two independent objectives on different views to learn coefficient matrices, which are collaboratively used for unlabeled tweets selection from the topic that the algorithm is adapting to. And then topic-adaptive senti-ment words are expended based on the above selection, in turn to help the first two steps find more confident and un-labeled tweets and boost the final performance. Compar-ing with the well-known supervised sentiment classifiers and semi-supervised approaches, our algorithm achieves promis-ing increases in accuracy averagely on the 6 topics from pub-lic tweet corpus.
 I.2.7 [ Natural Language Processing ]: Text analysis. Algorithms, Design, Experimentation.
 co-training; sentiment classification; multiclass SVM; semi-supervised learning; tweet sentiment
Recent sentiment classification works show many interests in large scale tweets or blogs [10, 29, 34]. Unlike the product reviews usually companied with a scoring mechanism that quantifies the overall sentiment, tweets lack labeled data. It is a labor intensive task to manually label a large number of tweets. Emoticons were ever used as noisy labels for Twit-ter sentiment classification [9] to tackle this problem, which brought unexpected noise only relying on the emoticons to label tweets sentiment classes, and the neutral class could not be labeled this way. Semi-supervised approaches [23, 24, 39, 40] have been used for sentiment classification on other medium than Microblog. Semi-supervised Support Vector Machines [1, 7], a.k.a. S3VM, is one of the more promising candidates to utilize the unlabeled data combin-ing with a small amount of labeled ones, since SVM min-imizes the structural risk. S3VM was first introduced as Transductive Support Vector Machines (TSVM) by Vap-nik [38] and Joachims [16], originally designed for directly estimating labels of unlabeled points. And collaborative training (co-training) framework [3] is an alternative wrap-per, and achieves a good performance, which is often used in the scenarios whose features are easily split into differ-ent views. [30] demonstrated that algorithms that manufac-ture a feature split outperform algorithms not using a split, meanwhile algorithms leveraging a natural independent split of features performs better.

Various topics are discussed in timelines and trending top-ics of Twitter. A sentiment classifier is always dedicated to its training domain, since of the domain-dependent labeled data and features. Take a comment of  X  X ead the book X  as an example, it could be positive in a book review while nega-tive in a movie review. However, it is impossible to give pre-labeled tweets for all kinds of topics that will be discussed in the future, even labeling a small amount of tweets for semi-supervised learning. [27] studied on transferring sentiment classifier developed on a topic for one social medium to a different medium, which is also a topic-dependent approach for cross-medium sentiment classification. Cross-domain or cross-topic adaptation in the context of sentiment analy-sis was studied in [2, 22, 32], which explicitly borrowed a bridge to connect the topic-dependent features to a known or common features. However, such bridges are built assum-ing that the parallel sentiment words exists for each pair of topics, such as books, DVDs, electronics, kitchen appliances etc. In product reviews, which is not necessarily applica-ble to various topics in tweets, and mining the bridges relies on sufficient labeled data. Besides, all above studies focus on the review data set. None of them conduct the cross-domain sentiment classification for twitter data set, which is more difficult because of the sparse content, diverse top-ics, and lack of labeled data. Detecting and tracking topics from tweets is another research topic, and ad-hoc Microblog search task proposed in Text REtrieval Conference (TREC) 2011 [14] and 2012 [15] is a choice for people to query tweets on a given topic.

Therefore we focus on the cross-domain sentiment anal-ysis on twitter, and propose an adaptive multiclass SVM model which transfers an initial common sentiment classi-fier to a topic-adaptive one, by simultaneously augmenting the labeled set and expanding topic-adaptive features. To tackle the content sparsity of tweets, non-text features in-cluding biological clock, emoticons, and punctuations are explored besides the conventional text features. Because text and non-text features are naturally independent split, co-training framework is a good choice to wrap adaptive S3VM model for collaboratively transferring the sentiment classifier. An iterative algorithm is proposed for solving this model by alternating among optimization, unlabeled data selection, and adaptive feature expansion steps. Since the nonconvex of the problem, some heuristic policy is adopted in the iteration for its convergence. Comparing with the well-known supervised sentiment classifiers, including Na  X   X ve Bayes, Decision Tree, multiclass SVM, RF (Random Forest); and semi-supervised approaches, such as multiclass S3VM, and co-training multiclass SVM, our algorithm achieves sig-nificance improvements in accuracy on the 6 topics from the public tweet corpus. To better evaluate our algorithms, we test it with some different randomly sampling ratios.
The rest of paper is organized as follows. Section 2 inves-tigates the related work. In section 3, we describe the details of adaptive co-training SVM, and the iterative algorithm to solve it. Extracting the text and non-text features, and the candidates of adaptive features are discussed in section 4. Experiments are conducted in section 5. And section 6 con-cludes the whole paper. Sentiment analysis is a hot topic in the area of Natural Language Processing and text mining in recent years [25, 33]. One of the most important tasks is sentiment classifica-tion, which aims to classify opinion text into different senti-ment polarities, such as positive and negative [21, 37]. For example, Turney [37] presented an effective unsupervised learning algorithm, called semantic orientation, for classify-ing reviews as recommended or not recommended. A web-kernel based measurement was proposed as PMI-IR, which is independent to the corpus collection in hand. Another important task is opinion extraction, which aims to extract the sentiment words and targets from text [13, 20]. There are also other tasks, including rating prediction [21], aspects mining [28, 35] etc. However, these studies focus on the sen-timent of one domain, and did not consider the adaptiveness across topics.

Microblogs as social medias have attracted many studies on sentiment analysis [9, 18, 26, 36]. Go et al [9] intro-duced a distant supervised learning approach for automati-cally classifying the sentiment of tweets using emoticons as noisy labels for training data. Tumasjan et al [36] showed that Twitter can be considered as a valid indicator of po-litical opinion. Kouloumpis et al [18] leveraged the existing hashtags in tweets to build training data and demonstrated that part-of-speech features might not be useful for senti-ment analysis of tweets. Mehta et al [26] used the Twitter data as a corpus for sentiment analysis and tracking the in-fluence of a particular brand activity on the social network. In our work, we focus on the adaptive sentiment classifica-tion across different topics from tweets.

Among the existing semi-supervised learning methods, co-training proposed by Blum [3] in 1998, is attracting more and more attentions. In the original work, some experi-ments were conducted, and the results were rather encour-aging. Nigam and Ghani [30] performed extensive experi-ments comparing the performance of co-training and another popular algorithm that uses unlabeled data: Expectation-Maximization. These experiments show that co-training outperforms EM even on tasks where there is no natural split of features. And Nigam and Ghani [31] experimen-tally showed that co-training method can improve the super-vised classification performance even though the theoretical assumption cannot be satisfied. Kiritchenko [17] adopted the co-training methods to address the email classification problem. They trained the classifiers using SVM and Na  X  Bayes, and the results showed that SVM was a better choice. To tackle the lackness of labeled tweets, and adaptiveness to different topics, we build a formal multiclass S3VM model with adaptive feature variables that collaboratively trans-ferring a common classifier to a classifier on a brand new topic.

Cross-domain sentiment classification has been widely stud-ied in recent years. Blitzer [2] proposed an approach called structural correspondence learning (SCL) for domain adap-tation. It employed the pivot features as the bridge to help cross-domain classification. Pan [32] proposed a spectral feature alignment (SFA) algorithm to bridge the gap be-tween the domains with domain independent words. Li Fangtao [22] proposed the cross-domain sentiment lexicon extraction for sentiment classification. [12] and [8] conducted cross-domain sentiment classification on topic level. They employed probabilistic topic model to bridge different do-mains in semantic levels. However, all above studies focus on the review data set. None of them conduct the cross-domain sentiment classification for twitter data set. As de-scribed in introduction, twitter data set is much different from the review. It contains fewer words, which need to ex-tract more features. It also contains more diversified topics, which need more data to train a precise original classifier. We also notice that [27] conducted the experiments on cross-media sentiment analysis with news, blogs and twitter data sets. They also find twitter data set is very different from other resources. In this paper, we focus on the cross-domain sentiment analysis on twitter.
Since the sentiment classification requires telling among the objective i.e. neutral, positive and negative expressions, it is obviously a multiclass classification problem. The train-ing data is given in ( x i ,y i ), where x i is the feature vector with each element as the value of the corresponding fea-ture, and y i is the class that the data belongs to. Let K be the number of classes we need to classify to, and y  X  X  1 ,  X  X  X  ,K } . Each tweet belongs and only belongs to one class.

In this section, we firstly introduce multiclass SVM model based on X  X ne-versus-rest X  X trategy as preliminaries. And we then formally build semi-supervised multiclass SVM model adapting to unlabeled data on a specific topic, which only bi-nary classifier, SVMs, in semi-supervised setting was formu-lated in the existing work. Furthermore, the semi-supervised multiclass SVM model is extended to adapt to sentiment words on a specific topic as well. At last, both text and non-text features are extracted and naturally split to apply co-training scheme for sentiment classification on tweets, and an iterative procedure that alternates among three steps, i.e. optimization, unlabeled data selection, and adaptive feature expansion, is proposed to solve the model.
SVMs model is originally build for binary classification, thus we investigate the SVM-based classifiers for multiple classes. There are intuitive ways to solve multiclass with SVMs. The most common technique in practice has been to build K  X  X ne-versus-rest X  classifiers, and to choose the class which classifies the test data with greatest margin. Or we build K ( K  X  1) / 2 X  X ne-versus-one X  X lassifiers, and choose the class that is selected by the most classifiers. In our work, the  X  X n-versus-rest X  strategy is used for multiclass.
Based on  X  X ne-versus-rest X  strategy which optimizes the structural risk, multiclass SVM model can be formally build as follows. min where w is a matrix with each column w i as the coefficient vector corresponding to the features,  X  i is the non-negative slack variable for input i ,and C is a constant coefficient. To better capture how C scales with the training set, C is divided by the total number of labeled data. It is seen that model (1) is a structured SVM model which results in a single support vector machine, instead of multiple SVMs for binary classification.

Constraint (2) shows the  X  X ne-versus-rest X  strategy with greatest margin. And we call the w T y x i the confidence score of data t i belonging to class y . Thus in the prediction, the class label of data t i is
By introducing the maximum function, the constraints in multiclass SVMs model (1) can be written as equation (4).
And the multiclass SVMs model is rewritten as min
The sentiment classifiers trained using labeled tweets from one topic are not usually adaptive to another one. Thus we choose labeled data set L from an even mixture of various topics. The initial sentiment classifier is trained using such a labeled set L , which is viewed as a common classifier since of the small amount of mixed labeled data and common sentiment lexicon. To adapt to sentiment classification on atopic e , the unlabeled tweet set U of topic e is used for transferring without any more costs of manually labeling on topic e in a semi-supervised way.

With adding those unlabeled tweets as an optimization term in the optimization model (5), the semi-supervised multiclass SVM model adapting to unlabeled tweets on topic e is formed as follows. min where | L | and | U | indicate the number of elements in sets L and U separately. Furthermore, the selected unlabeled tweet t j is predicted to be class y j as equation (3). Thus the following equation satisfied.
We define the function submax { X } as the second largest value in set { X } . So the slack for unlabeled data in the third minimization term of model (6) is as follows.
It is the hinge loss h l (  X  )=max { 0 , 1  X  X  X  of the difference between the largest and the second largest confidence score of tweet t i among all the sentiment classes. In practice, not all the unlabeled tweets are picked for the adaptive training. Only the most confident classifying results are preferred to be added to avoid bringing much noise. We define the nor-malized confidence score S j of tweet t j with predicted class y as
Therefore given a confidence threshold  X  , the unlabeled tweets t j satisfying with S j  X   X  are selected. Thus the optimization model with confidence threshold is as follows. min where C is a constant coefficient for unlabeled optimization term,  X  is binary vector, and each value indicates whether the unlabeled tweet is selected for optimization. Function sign(  X  ) returns 1 if the variable is positive, 0 if it is zero, and  X  1ifitisnegative. |  X  | is the number of ones in binary vector  X  .
In the other hand, tweets on various topics may have quite different sentiment lexicons [32]. Thus we add the topic-adaptive sentiment words as variables in feature vector x of tweet t i , while keeping the common sentiment lexicons fixed as common feature values. Let x i with i =1 ,...,v are the fixed common feature values, and x i with i = v + 1 ,...,v + u are the topic-adaptive feature variables. Initially, x i =0 ,i = v +1 ,...,v + u , which guarantees a common classifier is trained. In the adaptive training, the feature values of topic-adaptive sentiment words are evaluated in the selected unlabeled tweets.

The weight of a topic-adaptive sentiment word  X  belong-ing to a class y is as follows. where f j (  X  ) is the term frequency of word  X  in tweet t equation shows that  X  y (  X  ) is the weighted summation of the term frequency of word  X  in the tweets t j with predicted class y j being y . The sentiment class that word  X  belongs to is
And the feature value x  X  =max y {  X  y (  X  ) } .Inthesame way, we do not hope all the topic-adaptive sentiment words are added. Thus given a threshold  X  , the selection vector  X  is defined in the following way. And we define the significance w  X  of sentiment word  X  be-longing to a predicted class as follows.
The normalized w  X  indicates the significance of sentiment word  X  in the class arg max y {  X  y (  X  ) } than that in other classes. Thus the values of topic-adaptive features in the objective function of model (7) are calculated as follows. Algorithm 1 Co-training for sentiment classification. Given: loop end loop return L , C 1 ,and C 2 .
In the conventional co-training framework, features are independently split into two views, denoted as  X  1 and  X  2 Two classifiers C 1 and C 2 are trained based on feature sets  X  1 and  X  2 separately using initial labeled data L . As Algo-rithm 1 shows, the unlabeled and confident data are selected to augment labeled data set L , which is used for the next iteration. And the final sentiment classification result is ob-tained by the classifier trained on the combining features  X  +  X  2 using augmented labeled data set L .

Since a tweet is extremely short, it is necessary to ex-tract more features besides the traditional sentiment words. Some intrinsic properties of tweets are explored as non-text features, which include post time, emoticons, and punctua-tions, which are naturally applicable across different topics. Therefore, it is inspiring to use the non-text features to find more topic-related unlabeled data for adaptive training, with the help of co-training framework.

In order to make adaptive multiclass S3VMs model (7) in a co-training scheme, we define another selection vector  X  of unlabeled data for another multiclass S3VMs.
And classification confidence S j is calculated with non-text features  X  j  X   X  2 as follows.

In practice, to avoid noises, we follow  X  X greement X  strat-egy [4], and only select the confident and unlabeled data that both classifiers agree most. Thus we replace  X  j with  X  j  X   X  j as the selection coefficient in the third optimization term of model (7) resulting in the objective of text view  X  as formula (11), and the objective of non-text view  X  2 has the same form as formula (11) by replacing coefficient ma-trix w and feature vector x with w and constant vector x corresponding to the feature values of  X  2 . min + C
In the adaptive co-training S3VM model for multiclass, there are coefficient matrices w and w separately for text Algorithm 2 Algorithm of adaptive co-training for senti-ment classification on topic e Given: loop end loop
Train multiclass SVM C  X  on the features consist of x and x using augmented L . return L, x, and C  X  . and non-text feature sets  X  1 and  X  2 , adaptive feature vari-ables x i ,i = v +1 ,...,v + u in set  X  1 , and selection vector  X  and  X  for unlabeled data to be estimated. Thus we can solve this model using an iterative procedure that alternates among the following three steps.

Optimization step for solving coefficient matrices w and w : by fixing selection vectors  X  and  X  , and feature vector x , we solve two minimization of multiclass SVM separately, since the feature sets  X  1 and  X  2 are designed conditionally independent.

Unlabeled data selection step for solving selection vectors  X  and  X  : by fixing matrices w , w and feature vector x , the unlabeled data are selected by comparing the confi-dence scores S j and S j with threshold  X  according to the constraint in model (7) and equation (10) separately.
Adaptive feature expansion step for solving adaptive feature variables x i ,i = v +1 ,...,v + u :byfixing w , w ,and selection vectors  X  and  X  , topic-adaptive sentiment words are expanded according to equation (8), and their corre-sponding feature variables x i are evaluated as equation (9).
In the details of the algorithm, unlabeled tweets and topic-adaptive sentiment words are added gradually to avoid mis-leading of the overwhelming unlabeled data and adaptive features at early iterations. As shown in Algorithm 2, it is given a common feature set split into independent views: text view  X  1 and non-text view  X  2 ,anevenmixtureofla-beled tweets from various topics, and a specific topic e with its unlabeled tweet set U . The candidates of topic-adaptive sentiment words are extracted from unlabeled set U ,which is described in section 4. In feature set  X  1 ,wesettheval-ues of all the candidate words to zeros, which only common features take effects initially. By fixing topic-adaptive fea-tures, optimization and unlabeled data selection steps al-ternates until the rest tweets t j  X  U such that S j &lt; X  or S j &lt; X  . In an iteration, at most l unlabeled tweets selected from each sentiment classes. The iterations between opti-mization and unlabeled data selection agree well with the co-training framework. Furthermore, adaptive feature ex-pansion is executed for expending at most c topic-adaptive sentiment words, and transferring the sentiment classifier in the aspect of features. Such a step is alternated with the above co-training iterations, until there are no adaptive words can be expanded or updated. Finally, at the end of the algorithm the augmented labeled set L , expended adap-tive features x and a combined sentiment classifier C  X  is output. Classifier C  X  is trained on the expanded adaptive features x i from text view  X  1 and x i from non-text view  X  using the augmented set L .
Features are split into two views, i.e. text feature set  X  of sentiment words, and non-text feature set  X  2 , including emoticons, temporal features, and punctuation.
 Text features:
Sentiment words consist of common ones and topic-adaptive ones. The common sentiment words are collected from Word-Net and public sentiment lexicon. The feature values are evaluated by Point-wise Mutual Information and Informa-tion Retrieval (PMI-IR) [37], which uses the web search en-gine as the kernel to get the convincing mutual information based on a huge amount of database. The PMI-IR value of sentiment word  X  w is calculated as follows.
 where hits (  X  ) is the number of records in the query results, e.g. hits ( X  X xcellent X ) indicates the number of records that containing  X  X xcellent X  in the database of web search engine.  X  X EAR X  is one of the commonly supporting keywords of search engines, which means the word on the left side co-occurs with the right side one and no more than a certain number of words away. And P is the common sentiment lexicon. So x  X   X  = PM -IR (  X   X  ) ,  X   X   X  P .
The topic-adaptive lexicon is built based on the part-of-speech (POS) tagging. We select the frequent adjectives, verbs, nouns and adverbs from the tagged tweets, and added them as candidates of topic-adaptive sentiment word set  X  with initial feature values x  X  =0 , X   X   X . And the whole vector space of text feature set  X  1 is v + u ,where v = | and u = |  X  | .
 Non-text features:
Temporal features. One of the most notable differences between tweets and traditional documents lies in their real-time; therefore, people a  X rs sentiments expressed in Twitter evolve dynamically over time. It is proved that the opin-ions expressed by users correlated well with their biological clock [19]. People tend to act differently in the morning and the noon, the beginning and ending of a week or month, spring and winter, etc. Thus we classify the post time into different hours, dates, day of week and months as tempo-ral features. As for tweets post in the different time zones, we can map the post time into their local periods without difficulties.

Emoticon feature. A set of emoticons from Wikipedia are :-(, :(, etc. They are labeled with positive (+1), neutral (0), or negative (  X  1) emoticons. The corresponding values of an emoticon in a tweet are summed up as its feature value.
Punctuation feature. Punctuation marks such as excla-mation mark (!), question mark (?), and their combinations and repeats, express the emotional intensity. Thus the term frequency of each punctuation mark in a tweet is counted for the feature.
In the experiment, we use three publicly available Cor-puses for evaluation. One is Sanders-Twitter Sentiment Corpus consisting of 5 , 513 manually labeled tweets. These tweets were collected with respect to one of the four differ-ent topics (Apple, Google, Microsoft, and Twitter). After removing the non-English and spam tweets, we have 3 , 727 tweets left. Another one is the 9 , 413 tweets mentioned  X  X aco Bell X  during January 24-31 , 2011. And the last one is the first 2008 Presidential debate corpus [6] with senti-ment judgments on 3 , 238 tweets. The detailed information of the corpus is shown in Table 1.
 Topics Positive Neutral Negative Total Apple 191 581 377 1149 Google 218 604 61 883 Microsoft 93 671 138 902 Twitter 68 647 78 793 Taco Bell 902 2099 596 3597 President Debate 1465 1019 729 3213 With some necessary preprocess of the tweets, we use the Stanford POS tagger to tag the tweets of each topic, and select the frequent adjectives, verbs, nouns and adverbs as the candidates for topic-adaptive sentiment words. In order to show how our algorithm performs with a small amount of labeled data, we randomly sample p labeled tweets from each of the three sentiment classes as training data L .And the rest (1  X  p ) tweets from each topic are used for testing.
We use the well-known supervised and semi-supervised approaches as the baselines. The baselines and our compar-ing algorithms are listed in the following.
As for the evaluation, we calculate the accuracy, precision, recall, and F-score. Let the number of correctly classified tweets be true positive tp y , and the number of incorrectly classified tweets be false positive fp y in all the tweets pre-dicted as class y . Let the number of tweets correctly clas-sified into other classes than class y be true negative tn and the number of tweets incorrectly classified into other classes than class y be false negative fn y , in all the tweets classified into other classes than class y . Thus the metrics are calculated as follows.

Considering that there are 3 sentiment classes, we average the precisions, recalls and F-scores for all classes as follows. the topic of Google
For shortness, we use  X  X cc X  to denote Accuracy defined as equation (12), and use  X  X -s. X  to denote F-score defined as equation (15) in the following tables.
To show the topic adaptive ability, we perform our ACoMS3VM algorithm with an initial training set L randomly sampled from all the 6 topics with ratios p . Table 2 illustrated the final sentiment classification results on the metrics of preci-sion and F-score for the 6 adaptive topics. The incensements of our algorithm compared to MSVM are also listed in the last two rows of Table 2, which are averaged on 6 topics. It is seen that we can get the increases of at most 7 . 08% and 27 . 44% on accuracy and F-score separately with totally 113 labeled tweets from 6 topics, which is 1% sample ratio. And it is reasonable that the increase becomes less with more la-beled training data, since the MSVM algorithm can already get enough supervised information to get a better solution.
To intuitively show the performances of our ACoMS3VM algorithm, we draw the accuracy curves with the size of aug-menting set L during iterations as illustrated in Figure 1, from the runs of sample ratios 1% and 10% on the topic of Google. It shows the increases of accuracy with each itera-tion of optimization, unlabeled data selection and adaptive feature expansion. And all the iterations finally converge into a better position.
We compare our adaptive algorithm ACoMS3VM with the 6 baseline algorithms of NB, DT, MSVM, RF, MS3VM and CoMS3VM with in columns 2-15 of Table 3. The accura-cies and F-scores are given separately for each baseline on all the 6 topics. The last  X  X verage X  row of the table av-erages the accuracies and F-scores of the 6 testing topics. And the increase percentages of our algorithm comparing to baselines are given in the last  X  X ncr X  row. It is indicates that our algorithm ACoMS3VM outperforms other baselines than CoMS3VM by 2 . 22% to 13 . 19% increase in accuracy. And even as for the semi-supervised co-training algorithm CoMS3VM with our extraction of features, our algorithm achieves 0 . 74% and 4 . 78% increases in accuracy and F-score separately. Although The ensemble learning method RF achieves the best F-score, ours achieves 13 . 19% accuracy im-provement than RF. Therefore, our algorithm ACoMS3VM achieves reliable performances than the baselines.
At last we illustrate some of the initial common sentiment words, and the expanded topic-adaptive sentiment words of each iteration in Table 4 on the topic of President Debate. And Table 5 shows the augmented tweets for different senti-ment classes on the topic of Apple. It is seen that the steps of adaptive feature expansion and unlabeled data selection pick reasonable topic-adaptive sentiment words and unla-beled tweets from positive, negative, and neutral classes in semi-supervised way.
Various topics are discussed in tweets of Microblog ser-vices. Sentiment classifications on tweets suffer from the problem of lack of labeled data, and adapting to some un-known topics in the future. We propose an adaptive multi-class SVM model which transfers an initial common senti-ment classifier into a topic-adaptive one, by simultaneously augmenting the labeled set and expanding topic-adaptive features. To solve this nonconvex model, an iterative algo-rithm alternates among three steps. Average Acc.  X  7.08  X  3.35  X  2.95  X  2.47  X  0.92
With the first two steps alternating to collaboratively aug-ment the labeled data, the third is executed to expand topic-adaptive sentiment words from those selected unlabeled tweets. And in turn to help the first two steps find more confident and unlabeled tweets to boost the final performance of sen-timent classification. Thanks a lot for the supply of corpuses from Mr. Niek Sanders of Sanders Analytics LLC, Nick Diakopoulos and Ayman Shamma, etc.

This paper is partially supported by National Natural Sci-ence Foundation of China (No. 61202213, 61202215, 61173008, 61232010), National Grand Fundamental Research 973 Pro-gram of China (No. 2013CB329602), Projects of Devel-opment Plan of the State High Technology Research (No. 2012AA011003) and Beijing nova program (No. Z121101002512063). [1] K. Bennett, A. Demiriz, et al. Semi-supervised [2] J. Blitzer, M. Dredze, and F. Pereira. Biographies, [3] A. Blum and T. Mitchell. Combining labeled and [4] M. Collins and Y. Singer. Unsupervised models for [5] K. Crammer and Y. Singer. On the algorithmic [6] N. A. Diakopoulos and D. A. Shamma. Characterizing tweets class [7] G. Fung and O. L. Mangasarian. Semi-superyised [8] S. Gao and H. Li. A cross-domain adaptation method [9] A. Go, R. Bhayani, and L. Huang. Twitter sentiment [10] N. Godbole, M. Srinivasaiah, and S. Skiena.
 [11] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [12] Y. He, C. Lin, and H. Alani. Automatically extracting [13] M. Hu and B. Liu. Mining opinion features in [14] O. I., M. C., L. J., and S. I. Overview of the TREC [15] S. I., O. I., and L. J. Overview of the TREC 2012 [16] T. Joachims. Transductive inference for text [17] S. Kiritchenko and S. Matwin. Email classification [18] E. Kouloumpis, T. Wilson, and J. Moore. Twitter [19] O. Kucuktunc, B. B. Cambazoglu, I. Weber, and [20] F. Li, C. Han, M. Huang, X. Zhu, Y.-J. Xia, S. Zhang, [21] F. Li, N. Liu, H. Jin, K. Zhao, Q. Yang, and X. Zhu. [22] F. Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu. [23] S. Li, C.-R. Huang, G. Zhou, and S. Y. M. Lee. [24] S. Li, Z. Wang, G. Zhou, and S. Y. M. Lee.
 [25] K.-L. Liu, W.-J. Li, and M. Guo. Emoticon smoothed [26] R. Mehta, D. Mehta, D. Chheda, C. Shah, and P. M. [27] Y. Mejova and P. Srinivasan. Crossing media streams [28] A. Mukherjee and B. Liu. Aspect extraction through [29] L. T. Nguyen, P. Wu, W. Chan, W. Peng, and [30] K. Nigam and R. Ghani. Analyzing the effectiveness [31] K. Nigam and R. Ghani. Understanding the behavior [32] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen. [33] B. Pang and L. Lee. Opinion mining and sentiment [34] M. Thelwall, K. Buckley, and G. Paltoglou. Sentiment [35] I. Titov and R. McDonald. A joint model of text and [36] A. Tumasjan, T. O. Sprenger, P. G. Sandner, and [37] P. D. Turney. Thumbs up or thumbs down?: semantic [38] V. N. Vapnik. An overview of statistical learning [39] X. Wan. Co-training for cross-lingual sentiment [40] N. Yu and S. K  X  ubler. Filling the gap: Semi-supervised
