 MINWOO JEONG and GARY GEUNBAE LEE Pohang University of Science and Technology 1. INTRODUCTION A spoken-language interface is often required in many appli cation environ-ments, such as mobile information retrieval, car navigatio n, and ubiquitous computing. However, the low performance of automatic speec h recognition (ASR) systems makes it difficult to extend their application to new fields. For a spoken-language system to provide practical interaction between human and machine, the major problem is how to recover the decreasing a pplication-level performance due to incomplete outputs in speech recognitio n. Especially, the accuracy of ASR is severely affected by recognition task cha nges. In a new recognition domain, the lexical, syntactic, or semantic ch aracteristics of the ut-terance have different distributions in comparison with th e training corpus. To recover this mismatch and improve the abilities of spoken-l anguage systems, the speech recognizer should be adapted to a specific recogni tion domain. answering (QA) is performance decrease due to the recogniti on errors in ASR systems. Erroneously recognized spoken queries drop the pr ecision and recall of IR and QA systems. Some authors investigated the relation of ASR errors and precision of IR [Barnett et al. 1997; Crestani 2000]. The y evaluated the effectiveness of the IR systems through various error rates using 35 queries of the Text Retrieval Conference (TREC). Their research sho ws that the in-creasing word error rate (WER) quickly decreases the precis ion of IR. Another group investigated the performance of spoken queries in NII Test Collection for IR Systems (NTCIR) collections [Fujii et al. 2002]. They evaluated a va-riety of speakers, and calculated the error rate with respec t to a query term, which is a key word used for the retrieval. They showed that th e WER of the query terms was generally higher than that of the general wor ds irrespective of the speakers. The performance of the serially connected s poken QA sys-tem, based on the QA system from text input that has 76% perfor mance and the output of the ASR that operated at a 30% WER, was only 7% [Ha rabagiu et al. 2002]. Harabagiu et al. [2002] exposed several fundam ental flaws of this simple combination of an ASR and QA system, including the imp ortance of named entity information, and the inadequacies of current s peech recognition technology based on n -gram language models.
 rion. Two main information sources are usually combined: an acoustic model and a language model. The language model may provide paramet er estimates that perform well on average when the testing and training co rpus are similar tasks. However, the resulting parameters generally cannot cope with signif-icantly changing conditions of the language involving pote ntial variations in vocabulary, syntax, content, and speaker style. The langua ge model should be adapted with specific knowledge of the recognition task to improve speech recognition accuracy.
 for overcoming speech recognition errors such as post-erro r correction and language model adaptation. ASR error correction can be one o f the domain adaptation techniques to improve the recognition accuracy , and the primary advantage of the error correction approach is its independe nce of the specific speech recognizer. If the speech recognizer can be regarded as a black box, then we can perform robust and flexible domain adaptation through the post-error correction process.

For language model adaptation, various adaptation approac hes have been extensively studied [Bellegarda 2004]. The most widesprea d approaches of adaptive language modeling use the n -best or lattice reranking (or rescoring) technique, which is an efficient method to adapt to new observ ed data in the recognition task. Typically, n -best rescoring assumes that the correct-answer sentence should always be included in the ASR hypotheses. Bu t if a speech recognizer cannot provide correct words or lattices due to e xtremely noisy en-vironments or abnormal characteristics of speakers, then e ven the n -best hy-potheses cannot cover the correct-answer sentence in many o f the cases.
To address these problems, we present an error-corrective r eranking ap-proach that can correct or regenerate the hypothesis space a nd then select the best hypothesis in an expanded n -best list or word lattice. Our method is a unified model to integrate error correction and high-level l inguistic knowledge incorporation in language model adaptation.

The remainder of this article is organized as follows. In the following section, we briefly present previous approaches and contrast them wit h our method. In Section 3, we review a general error correction technique an d propose our mod-ification. Moreover, we explain an error-corrective rerank ing approach with a semantic-oriented language model to rerank the generated hypotheses in Section 4. Then in Sections 5 and 6, we present descriptions o f target tasks and data sets and empirical results. Finally, we draw conclu sions in Section 7. 2. ERROR CORRECTION AND LANGUAGE MODEL ADAPTATION 2.1 Previous Works Researchers [Kaki et al. 1998; Ringger and Allen 1996] have i nvestigated post-error correction techniques to overcome speech recog nition errors. One approach in post-error correction, which is a straightforw ard and intuitive method to handle many kinds of recognition errors, was a rule -based approach [Kaki et al. 1998]. Kaki et al. collected many lexical error p atterns that oc-curred in a speech translation system in Japanese. They coul d correct any type of errors by matching the strings in the transcription w ith lexical er-ror patterns in the database. However, this approach has a di sadvantage in that the correction is only feasible to the trained (or colle cted) lexical error patterns. Another approach has been based on a statistical m ethod utiliz-ing the probabilistic information of words in a spoken dialo gue situation and the language models adapted to the train information servic e domain [Ring-ger and Allen 1996]. Ringger and Allen applied the noisy chan nel model to the correction of the errors in speech recognition. They sim plified a statisti-cal machine translation model called an IBM model [Brown et a l. 1994], and tried to construct a general postprocessor that can correct errors generated by any speech recognizer. They use the probability of mistak enly recognized words, the co-occurrence information extracted from the wo rds and their neigh-boring words, and the tagged word bi-grams, which are all lex ical clues in error strings.
 successful results, but they still have major drawbacks. Th e performance of such systems depends on the size and the quality of the speech recognition result or on the database of collected error strings since sy stems are directly dependent on lexical items. The available error patterns co nstructed are not enough, because it is expensive to collect them; thus in many cases, patterns fail to recover the original strings from the lexical-speci fic error patterns. Also, since the lexical-based correction systems are sensitive t o error patterns, they occasionally misidentify a correct word as an error word. To address this prob-lem, Jeong et al. [2004, 2005] attempted to incorporate ling uistic knowledge for ASR error correction. High-level linguistic information s uch as lexico-semantic information is useful to present better hypotheses correct ed by error correction procedure.
 els were successfully applied in adaptive language modelin g. Language model adaptation can include syntactic and semantic information as well. In Rosen-feld [1996], trigger pairs were encoded as features along wi th conventional n -gram and distance-2 n -gram. Khudanpur and Wu [2000] also used a condi-tional ME model to build a language model with topic and synta ctic informa-tion. Significant perplexity reduction over a baseline trig ram was observed in ME-based language models. Rosenfeld et al. [2001] proposed a whole-sentence ME model. To avoid huge computations and encode global sente nce phenom-ena, a whole-sentence exponential model adopts a  X  X ag of fea tures X  approach to each sentence or utterance, where features are arbitrary computable prop-erties of the entire sentence. 2.2 Our Method In traditional spoken-dialogue systems, the best ASR outpu t, which is reranked with a domain-adapted language model, is passed to spoken-language understanding (SLU) and dialogue manager (DM). In this approach, there are two disadvantages. First, it uses only adaptive (a nd domain-specific) language models to rescore the n -best list or word lattice generated by the base speech recognizer. Hence, these adaptation methods depend on the baseline recognizer, and the hypothesis space is fixed by the base resu lts of the n -best list or lattice. Second, the acoustic (or speaker) model and language model are adapted independently in previous adaptation approach es although, some-times, acoustic or speaker variation influences language mo dels.
 (E
CO -R) approach that can correct or regenerate the hypothesis s pace and then select the best hypothesis in an expanded n -best list or word lattice. Our method is depicted in Figure 1. In this framework, we can cons ider two models as intermediate levels, the E CO and R ERANK modules. The E CO generates new hypotheses whose errors are corrected by the correction mod els and bigram language models. The R ERANK reevaluates the generated hypotheses with semantic structure and domain-specific statistical langua ge models. Finally, we can obtain the best sentence and its corresponding semant ic structures. domain-specific text corpora. In addition to text corpora, o ur E CO -R requires correction data and semantic-annotated data to adapt a doma in-specific dia-logue system. The former consists of hypothesis and referen ce pairs of ASR, which is relevant to the E CO module. The other data is used for generating a linguistic feature corpus, which is relevant to the R ERANK . The data collection and training procedure is illustrated in Figure 2. In genera l, correction and semantic-annotated data are drawn from the same dialogue co rpus. 3. GENERATING CORRECTED N -BEST LISTS 3.1 Error Correction The E CO module models the relationship between ASR hypotheses ( h ) and correct references ( r ). The ASR hypotheses may be incorrect, may not include domain vocabulary, and may not have domain-affected or spea ker-dependent models. Therefore, we assume that the ASR hypotheses h should be adapted by the correct reference r . According to this assumption, we can model the E CO within the source-channel or noisy-channel paradigm.
 lems, such as speech recognition [Rabiner 1989], statistic al machine trans-lation [Brown et al. 1994], spell-correction [Brill and Moo re 2000], or speech error correction [Ringger and Allen 1996]. The key idea of th e noisy channel model is that we can model some channel properties by estimat ing the pos-terior probabilities. This model can be viewed either as a ma ximum a poste-riori (MAP) adaptation strategy with different parameteri zations of the prior distribution or as an optimized model approach to minimize t he error rate in postprocessing on the recognizer output. The E CO is based on the basic noisy-channel mode, but the main difference between our model and o ther related models is that E CO employs the n -best or lattice input and output. Moreover, these n -best or lattice results are reevaluated by the R ERANK model. cluding erroneous words and the adaptation procedure will fi nd the new word sequence with an adapted language model distribution. Thus , the problem of E for a given input sentence, then find the best corrected word s equence r  X  = ing Bayes X  rule and dropping the constant denominator, we ca n rewrite it as: At this point, we have a noisy channel model for E CO with two components, the correction model (or translation model) P ( h | r ) estimated by correction data and the language model P ( r ) integrating the background corpus and adaptation data. 3.2 Training To train the E CO model, we need the training data consisting of ( h , r ) pairs that are ASR outputs and their manually transcribed strings . Also, we align the pair based on minimizing the edit distance between r i and h i by dynamic programming. Once the alignment algorithm is executed, the ( h , r ) pairs has the same word sequence size, hence we can assume that n = m . Figure 3 shows the aligned ( h , r ) pairs. (This example is from Ringger and Allen [1996]). ASR environment. If we assume that the output word sequences produced by ASR are independent of one another, then we have the followin g formula: or merged errors, which frequently appear in an ASR output, s ince errors influence the surrounding words, and there is a context depen dency in the error word sequence. Figure 3, for example, shows a split or a merged error problem, which can be solved by fertility modeling [Brown et al. 1994; Ring-ger and Allen 1996]. We refer to the k -number of postchannel words h i pro-duced by a prechannel word r i as a fertility with probability P ( f : k | r i ). We can simplify the fertility model of IBM model-4, and allow th e fertility within Thus, the fertility model can deal with ( newwark , new york ) or ( to leave , toledo ) substitution in the example of Figure 3.
 Figure 4 shows a distribution of error phrase size from two ta sks (air travel and telebanking service domain; further details in Section 5). About 80% of errors are within 2 sizes of phrase. 1 In our model, a correct pair (non-error) and 5 types of error pairs exist (Figure 5). We can divide multipl e error phrases into the combinations of these 5 types of error patterns. If 4 words are mis-matched, then we can divide by two 2-to-2 patterns, or four 1-to-1 patterns. Furthermore, we generalize that the correction units ( h i , r i ) are phrases rather than words; that is, our model is a phrase-based correction ( phrase is limited by 2 words).
 dle models that are more complex than just one-to-one mappin g. Let L be a and fertility k  X  L . Then, we can obtain the following equation: tility P ( f : k | r i ) by maximum-likelihood estimation (MLE). Let N ( x ) be the number of times the event x occurred in the training corpus. Then, with the fertility k . 3.3 Smoothing Techniques If the correction data is not enough to estimate correction m odel parame-ters, then model parameter P ( h i | r i ) is overfitted in general. To overcome this problem, we apply well-known language model discounting te chniques to the correction model: absolute, Good-Turing, and modified Knes er-Ney discount-ing. The first is the absolute discounting method. Let d = N ( h i , r i ) and N d is the number of joint pairs that occur exactly d times in the training data. Ney et al. [1994] suggested a simple method to smooth a language m odel, where we can replace Equation 4 with where B is the size of vocabulary, and we can typically set Y = N 1
The second method is the Good-Turing estimate to build our co rrection model [Good 1953]. We can set the model probability as follow s:
The last discounting method is a modified version of Kneser-N ey smoothing [Chen and Goodman 1999]. In our modeling, it can be a novel var iation of absolute discounting, which has three different parameter s, D 1 , D 2 , and D 3+ , that are applied to channel pairs with one, two, and three or m ore counts, respectively. Hence, we take where For further details of these discounting methods, refer to C hen and Goodman [1999].

Learning the language model P ( r ) is slightly different from the correction model. The probability P ( r ) is given by the language model and plays a prior role. The distribution P ( r ) can be defined by using an n -gram, structured lan-guage model, or any other tool in statistical language model ing. In this article, we use a simple bigram language model in the E CO model to reduce the search space and R ERANK reevaluates the resulting hypotheses. E CO searches for the n -best hypotheses using the bigram, and R ERANK rescores the best one via more sophisticated language models. 3.4 Decoding The search procedure of Equation 1 can be efficiently calcula ted by a Viterbi search algorithm (with beam pruning). Output hypotheses ca n be error-corrected (or newly generated) n -best sentences. The outputs of E CO depend on two subcomponent models, a correction model P ( h | r ) and a language model P ( r ). Given the new ASR output sequence h , E CO first generates candidate phrases (1-to-1, 1-to-2, and null) for each word h i , and merged candidate phrases (2-to-1 and 2-to-2) from h i and h i +1 . Figure 6 shows an example lat-tice. Then we have a hypothesis space (lattice) to search for the best probable sequence. After generating the search space, we perform a mo dified Viterbi search algorithm where 2-to-N transitions skip a t + 1 time line. It is trivial to exactly search the best hypotheses because 2-to-N candid ates are merged from h t and h t +1 . This is an efficient and simple trick to deal with merged erro r cases. Figure 7 graphically compares 1-to-N and 2-to-N tran sitions.
Our correction model can be viewed as a special case of phrase -based trans-lation model [Koehn et al. 2003]. In our error correction alg orithm, we set the phrase size as within 2, which is reasonable for ASR error cor rection. In this setting, we have only 2 types of edge transitions, which lead to tractable and efficient decoding. To produce n -best outputs, our modified Viterbi algorithm keeps the vector where n -best traces are stored for each node. After our search process is performed, we have corrected n -best sentences. 4. ERROR-CORRECTIVE RERANKING The probabilistic reranking approach is widely used in natu ral language processing, including language modeling [Bellegarda 2004 ] and natural lan-guage parsing [Collins and Koo 2005]. The major benefit of the reranking method is more tractable computation rather than direct pro cessing, which can be adaptable to specific domains without remodeling the b ase ASR in the language modeling task. The E CO -R reevaluates the error-corrected n -best hypotheses using probabilistic reranking. An example of E CO -R for a telebanking service domain is depicted in Figure 8. The erro r word ( ee-ran , boldface in Figure 8) influences the probability of using a la nguage analyzer, and resulting semantic analysis is incorrect. From ASR 1-be st output, E CO -R generates n -best lists using the correction model. Our algorithm selec ts a best sentence with a high score of correction and semantic analys is. Intuitively, correct semantics have the potential to be selected. This ex ample shows the ef-fects of reranking with semantic information. In this secti on, we explain how to use domain-specific semantic knowledge in our reranking m odel. 4.1 Spoken-Language Understanding The goal of spoken-language understanding (SLU) is to extra ct meanings from the recognized user X  X  utterances and to infer dialogue acts or information goals based on the recognized semantic concepts and preceding dia logue context. A reference semantic frame (or template) is a well-formed str ucture of extracted information consisting of slot/value pairs. The examples f or air travel and telebanking services are shown in Figure 9.
 structed by extending each slot label by three additional sy mbols, B,I , and O [Ramshaw and Marcus 1995]. These symbols correspond to case s when a following example shows a BIO representation converted from the semantic concept frame on the air travel and telebanking domain. r  X  . Input word sequences r  X  are converted to feature vectors x , which are not only lexical strings, but also multiple linguistic feature s. To extract seman-tic frames from the user X  X  utterance inputs, we use a linear-chain conditional random fields (CRF) model, which is a model that assigns a join t probabil-ity distribution over labels that is conditional on the inpu t sequences, where the distribution respects the independent relations encod ed in a graph [Laf-ferty et al. 2001]. CRF has been previously applied to obtain promising results in various tasks such as table extraction, shallow parsing, and information extraction for research papers. Hence, we construct the SLU model with a semantic-annotated corpus, and exploit it to estimate sema ntic information for E CO -R. 4.2 Reranking with Semantic Information The aim of the E CO -R is to build a practical and robust spoken dialogue sys-tem. The spoken dialogue system has a pipeline structure tha t consists of speech recognizer and domain-specific language understand ing modules. In such dialogue applications, task-oriented SLU can be used f or our reranking language model.
 mantic class-based language (bigram) model motivated by SL U and a dialogue system. SLU-based LM (SLU-LM) is defined by the following equ ation. where s i is the semantic class to which the i -th word belongs. Learning SLU-LM is divided into two phases: (1) assigning semantic class s to a raw training corpus by the SLU model, and (2) estimating the two probabili ty functions from estimated from a corpus as follows.

There are two main differences between a class-based langua ge model (Class-LM) and SLU-LM: (1) a word belongs to a unique class in Class-LM but to multiple classes in SLU-LM, and (2) the model is learned by an Expectation-Maximization (EM) algorithm in Class-LM and by counting in S LU-LM. Nev-ertheless, SLU-LM requires a semantic-tagged corpus to tra in the SLU model, which is normal for practical spoken dialogue systems.

Although SLU-LM can incorporate semantic information from domain knowledge into language modeling, there is no direct way to r erank with an-other information source. While semantic classes are assig ned by a probabilis-tic classifier using multiple linguistic features, R ERANK cannot directly exploit this fluent information. Following the Markov assumption, S LU-LM depends on the previous semantic class. To cope with this problem, we combine the se-mantic information and other linguistic information (incl uding a long-distance dependency feature) without information loss.

To exploit multiple knowledge sources (e.g., part-of-spee ch tag, syntactic de-pendency, or semantic information), the adaptation data is used to extract specific linguistic information on different aspects of the mismatch between training and recognition conditions. The extracted inform ation is used as lin-guistic features to overcome a particular type of linguisti c mismatch in isola-tion. These features reflect linguistic variations such as s tyle, topic, or genre of the text.

A popular way to combine multiple knowledge sources is to use log-linear models. The conditional maximum entropy (ME) model was first suggested for language modeling tasks by Rosenfeld [1996] as a means of combining dif-ferent features from many sources. The underlying ME criter ion offers the theoretical advantage of incorporating arbitrary feature s while avoiding frag-mentation. In addition, the features considered in the cons traint specification are not restricted to functions of frequency count in langua ge modeling.
Let w i be a local history such as a bigram, trigram, or skip n -gram, and t i be syntactic information such as a part-of-speech tag, shal low parse bracket (chunk), or dependency head word. The semantic information s i and d cor-respond to the semantic class and sentence-level feature (e .g., dialogue act) extracted from the SLU component. If we consider all feature s as input ob-probability P ( r i | x i ) can be defined by using an exponential model of the form: where  X  j are the parameters, and the features f j ( r i , x i ) are arbitrary com-putable feature functions of the history pair. In this work, the feature functions include n -gram, part-of-speech tags, phrase chunk labels, lead word s, and se-mantic classes. In fact, the feature function is likely to be a binary test for nat- X  X enver, X  the previous word is  X  X rom, X  the semantic class la bel is  X  city name-b , X  ( x words r k i can be ignored for reranking models. To estimate the paramet er  X  j , we use the well-known Gaussian prior smoothing [Chen and Ros enfeld 2000] and L-BFGS numerical optimization algorithms [Malouf 2002 ]. 5. TASK DESCRIPTION 5.1 Data Sets We have evaluated our method by using two data sets: the CMU-Communicator travel corpus ( air travel ) and Korean telebanking dialogue corpus ( telebank ). Table I show a summary of the two data sets we used for experiments.

The airtravel set made by The University of Colorado is an over-the-telephone spoken dialogue corpus to develop a dialogue syst em for accessing live airline, hotel, and car rental information, which was c ollected in 461 days and consists of 2,211 dialogues or 38,408 utterances in tota l. We removed most of the single-word responses such as  X  X es, please X  or  X  X o X  wh ich were not very useful to evaluate SLU performance. In general, there are no domain-specific semantic concepts in such single words. After cleaning up, w e selected 13,983 utterances, and labeled semantic categories to the raw data . The semantic labels of airtravel were automatically generated by a Phoenix parser and were manually corrected. In the airtravel set, there are 31 unique NE classes in 13,983 utterances. The defined NEs are shown in Table II.
 The telebank is a Korean spoken dialogue corpus provided by Sogang University. The telebank set consists of seven different subscenarios and about 22,000 utterances in total. We selected subscenarios related to loan information and removed meaningless utterances, hence we o btained 2,239 ut-terances. For the semantic annotated data, we manually tagg ed the semantic frame to the raw text. In addition to word-level semantic lab eling, sentence-level dialogue acts are labeled in the telebank set (see details in Eun et al. [2005]). We defined 15 word-level NEs and 5/25 types of senten ce-level seman-tics (dialogue act and main goal). The defined NEs are in Table II, and the defined dialogue acts and main goals are in Table III.
 sets. The airtravel data set is 630K words with 29K entities. Roughly half of the entities are time-related information, a quarter of the entities are city names, a tenth are state and country names, and a fifth are airl ine and air-port names. The telebank data set is 17K words with 2K entities. Figure 10 (c) and (d) show relative frequencies of dialogue act and main ac tion for telebank data set. 5.2 Speech Recognition For the two data sets, we used two ASR systems. In both airtravel and telebank sets, we trained recognizers with only the domain-specific s peech corpus, and used no acoustic/speaker adaptation or noise filter, becaus e we wanted to eval-uate our method fairly without being fused with other techni ques. For airtravel , we used the open source speech recognizer Sphinx2 and the ope n source CMU-Communicator Feb-2000 acoustic model 2 , which was trained with a semicon-tinuous density model. The reported WER of airtravel data is about 15%, but we had 26.15% of WER in our experimental setup because we only us ed the sub-set data (13,983 out of 38,408) without tuning, and the sente nces of this subset are longer and more complex than those of the removed ones, mo st of which are single-word responses. For telebank , we made hidden Markov model toolkit (HTK)-based Korean speech recognizer (morpheme-based rec ognition system) using mel-frequency cepstrum coefficients (MFCC) 39-dimen sional feature vec-tors. Since the telebank set was collected by restricted scenario and vocabulary, the data is small and the WER of this data is lower than that of t he airtravel set (14.85%).
 articles containing 177K sentences for airtravel and MATEC 4 , which consists of 34K sentences from novel and news articles for telebank . These two back-ground corpora were used to build language models in ASR and E CO -R. 5.3 Experimental Setup We divided each test set into 5 parts and evaluated the result s using five-fold cross validation for all our experiments. To evaluate sever al experiments, we used 100-best lists of ASR output. Also, we built the correct ion model with ( h j , r ) j =1 ,, J , where h j is j -th best output of ASR.
The trigram model and class-based language model (Class-LM ) [Brown et al. 1992] have been successfully applied in ASR tasks, hence we c hose these two models for our baseline R ERANK model. We defined P ( r ) with the trigram as the following simple form.
 The Class-LM predicts a word sequence by the following equat ion.
 where c i is the word class to which the i -th word belongs. The word cluster from uniquely assigned word class sequences (refer to Brown et al. [1992]).
All language models were trained with modified Kneser-Ney di scounting [Chen and Goodman 1999]. The E CO model generated 10 bests for each utter-ance in the lists, hence about 1,000 best outputs were passed to the R ERANK . 6. EMPIRICAL RESULTS 6.1 Error Correction X  E CO Model First, we consider the E CO model. Figure 11 shows the effect of the E CO model (baseline E CO -R without reranking). The baseline speech recognizer X  X  WE R of the airtravel set is 26.15% and that of telebank is 20.81%. Since our HTK-based Korean speech recognizer just provides bigram n -best, we rescored the telebank data with a trigram language model, and the baseline WER was r e-duced to 14.85%. Our E CO model achieved the improvement of performance by increasing the training data. To reduce the overfitting prob lem, we used three different smoothing methods: (1) absolute, (2) Good-Turin g, and (3) modified Kneser-Ney smoothing. In Figure 11, the discounting method s work well in general, and we came to the conclusion that the best training strategy is to employ the modified version of Kneser-Ney method, which work s well even for a small number of data items ( telebank ).

We performed the standard National Institute of Standards a nd Technology (NIST) Matched Pairs Sentence Segment Word Error Test (MAPS SWE) as a statistical significance test [Pallett et al. 1990] between the baseline trigram and our E CO -R with the simple bigram to verify our result and found that:  X  X he WER improvement of the airtravel set with the E CO -R adaptation (E CO with bigram model) over the baseline trigram model is signifi cant ( p &lt; 0 . 001), and  X  X he WER improvement of the telebank set with E CO -R (E CO with bigram model) over the baseline trigram model is also significant ( p &lt; 0 . 001). E
CO model results via the NIST Speech Recognition Scoring Toolk it 5 . The airtravel set is marked by call ID rather than speaker ID, because anony mous data was collected. We assume that all calls are from differe nt speakers. These results can provide deeper analysis. As shown in the figures, E CO -R is signif-icant for improving the accuracy of the baseline speech reco gnition system for almost all speakers. 6.2 Improving Recognition and Understanding In the next experiment, we applied the reranking approach to our base-line E CO -R. The outputs of previous experiments, 1000-best lists ge nerated by E CO , were re-evaluated by the R ERANK model. We ran the second-pass reranking algorithm over both baseline ASR system and E CO outputs us-ing either trigram, Class-LM, or our SLU-LM. To verify that t he proposed model is helpful to both speech recognition and spoken-lang uage application, we evaluated not only the WER but also the performance of SLU. SLU eval-uation is measured in terms of F1-measure (F1) on a slot/valu e pair, which is the harmonic mean between the precision (Prec) 6 and recall (Rec) 7 for concept frame matching with the  X  X old standard X  frames, whi ch is defined as F 1 = (2  X  Prec  X  Rec ) / ( Prec + Rec ).
 duction [RR] rate) and SLU performance using E CO -R with various reranking language models (R ERANK model). Since the dialogue act and main goal are not available for the airtravel set, we show only the result of NE as precision, recall, and F1-score to evaluate the understanding perform ance for the air-travel set. In contrast, for the telebank set, we add the results of dialogue act and main goal classification. In both data sets, WER decrease d and perfor-mances of the semantic frame extraction increased by the R ERANK model over the baseline E CO -R, which is an E CO model. Thus, we can see that the E CO -R significantly improves the performance for both WER and SLU F 1 (all results of E CO -R pass the MAPSSWE test with p &lt; 0 . 001).
 Especially, the SLU-LM is better than both trigram and Class -LM; thus, it shows that semantic information is useful to a spoken-lan guage system whether the E CO model is used or not. The best model is the E CO -R model with SLU-LM: its results for both test sets were 9.7% and 13.5 % error reduc-tion over the ASR trigram.

Language-understanding performances of the gold-standar d transcripts (0% WER) are 89.76 and 88.67 for airtravel and telebank , respectively. The performance normally decreases linearly with errors of the speech recognizer. However, we also achieved an improvement of the frame extrac tion perfor-mance using E CO -R over the baseline F1 values of 70.04 and 83.40. In com-parison to airtravel , the telebank result is better, because the speech recognition accuracy of telebank is higher than that of airtravel . Similarly, dialogue act classification is affected by WER. Table V shows the comparis on of dialogue act and main goal in our SLU system [Eun et al. 2005]. The perfo rmance for SLU consists of a three-part result: (1) F1-measure for dial ogue act F1(act), (2) F1-measure for main goal F1(goal), and (3) F1-measure fo r NE recogni-tion F1(NE). The oracle performances are 97.72 and 95.07, wh ile the baseline F1-measure performances of the classification task are 95.9 4 and 91.69 in tele-bank evaluation. However, adaptation via E CO -R with R ERANK model can increase the performance of the dialogue act task as shown in Table V. This result shows that the E CO -R is a viable approach to improve the final SLU performance including dialog act classification as well as A SR accuracy. 6.3 Results of SLU-LM Figure 13 shows the performance of Class-LM versus SLU-LM re ranking. The free parameter of Class-LM reranking is the number of word cl usters used for EM training. We built the Class-LM with different cluster nu mbers. (We used the SRI Language Modeling (SRILM) toolkit to cluster words a nd train Class-LM. See details in Brown et al. [1992].) In contrast, SLU-LM d oes not require additional free parameters but still outperforms any class -based model. More concretely, SLU-LM can be seen as a special case of Class-LM. The key dif-ference is that the model X  X  parameters can be learned in a sup ervised manner. SLU-LM also requires predefined semantic classes, but they a re available from labeled data for the SLU model. SLU-LM can explicitly captur e the semantic representation to improve a dialogue system, while Class-L M captures only lexical and contextual similarity of words. Intuitively, s emantic information encoded in SLU-LM is useful to improve performance of both sp eech recogni-tion and understanding.

We experimented on integrating multiple types of linguisti c knowledge into our R ERANK model. In this experiment, we assumed that linguistic featu res are useful in selecting valid output of the ASR and semantic p rocessor. The fea-tures we used are described in Table VI. The syntactic featur es are helpful to capture long-distance dependency information, and give a b etter weight to pa-rameters in grammatically correct sentences than in gramma tically incorrect sentences. Using part-of-speech tagging and parsing, we ad ded syntactic fea-tures to combine syntax with lexical features ( model1 ). Then we added seman-tic knowledge to model1 incrementally one by one as follows. The word-level semantic labels, which are assigned by the SLU component, ar e integrated into the base model ( model2 ). Also, another type of information, two-level dialog acts, are used ( model3 ). Finally, we combined all types of the features in one unified model ( model4 ).
 els. The E CO -R with the ME-based reranking model outperforms all other models of the reranking without E CO -R. All performance differences are significant when tested by MAPSSWE. For the trigram versus ME -based mod-els, we found that: the WER improvement of the model1 over the trigram is significant at p = 0 . 001, and the WER improvement of the other three mod-els ( model2,3,4 ) are very significant at p &lt; 0 . 001. Moreover, for the ME-based models against E CO -R models, we found that: all E CO -R models (with model1,2,3,4) were better than the ASR models ( model1,2,3,4 without E CO -R) at p &lt; 0 . 001.
 recognition and a 96.83, 92.63, and 84.71 F1-measure for eac h act, goal classi-fication, and slot extraction (or semantic class labeling) u sing the E CO -R with model4 .
 available at http://home.postech.ac.kr/  X  stardust/research/eco/ . 7. CONCLUSION The main issues of practical spoken-language applications for providing in-terface between human and machine are how to overcome incomp leteness of speech recognition and how to guarantee the reasonable en d performance of spoken-language applications. Therefore, handling err oneously recognized outputs is a key to developing robust spoken-language syste ms.
 both domain environment characteristics and high-level li nguistic knowledge. First, our E CO -R expands the sentence hypotheses from a speech recognizer with noise channel properties. Next, we rerank the generate d hypotheses with semantic information combined with high-level linguistic knowledge. Using E
CO -R, we demonstrated the improvement of performance in both A SR and domain-specific SLU applications. In air-travel and teleba nking service do-mains, we showed an error reduction of up to 9.7% and 16.8% of W ER. The performance of NE recognition also increased from 70.04 to 7 1.67 and 83.40 to 84.55, respectively. In addition, for telebanking data, we achieved the im-proved results as 96.83 and 92.63 for dialogue act and main go al classification. introduction of the unified model combining error correctio n and a linguistic-driven adaptive language model. This can capture the charac teristics of the recognition environment (e.g., domain, style, or speaker v ariations) with lin-guistic variation. Therefore, our framework provides a sim ple way to incor-porate acoustic (or speaker) and language model adaptation . The second con-tribution is the attempt to integrate semantic information into the adaptive language model. The domain-specific language understandin g model is tightly coupled and contributes feedback to the language model.
 ASR and can be adapted by domain-specific text corpora and rec ognition re-sults to improve the accuracy of speech recognition systems . It can be com-bined with general statistical language modeling methods, which successfully integrate higher-level linguistic knowledge. In our E CO -R framework, the cor-rection model is tightly coupled with the SLU component usin g the ME-based semantic language model to rescore the corrected candidate sentences. More-over, the ME-based model design allows for further improvem ent via a more sophisticated feature induction algorithm, which is an aut omatic iterative pro-cedure for selecting features from a given candidate set [Pi etra et al. 1997]. this data scarcity problem, we are considering using active -learning methods to reduce the effort of human transcription [Cohn et al. 1994 ]. Our method can be integrated with the acoustic or speaker adaptation pr ocess which al-ready uses transcripts and acoustic signals pair data. Thus , active learning with speaker adaptation can be the subject of future intensi ve research on our E
CO -R. Moreover, signal-level features such as acoustic score s can be helpful in creating a better correction model, thus we are consideri ng these types of features to model channel properties.
 models to http://home.postech.ac.kr/  X  stardust/research/eco/ .
 We thank the anonymous reviewers for helpful comments.

