 Spatial scan statistics are used to determine hotspots in spatial data, and are widely used in epidemiology and bio-surveillance. In recent years, there has been much effort invested in designing efficient algorithms for finding such  X  X igh discrepancy X  regions, with methods ranging from fast heuristics for special cases, to general grid-based methods, and to efficient approximation algorithms with provable guar-antees on performance and quality.

In this paper, we make a number of contributions to the computational study of spatial scan statistics. First, we de-scribe a simple exact algorithm for finding the largest dis-crepancy region in a domain. Second, we propose a new approximation algorithm for a large class of discrepancy functions (including the Kulldorff scan statistic) that im-proves the approximation versus runtime trade-off of prior methods. Third, we extend our simple exact and our ap-proximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid. Fourth, we conduct a detailed experimental comparison of these methods with a number of known methods, demonstrating that our approx-imation algorithm has far superior performance in practice to prior methods, and exhibits a good performance-accuracy trade-off.

All extant methods (including those in this paper) are suitable for data sets that are modestly sized; if data sets are of the order of millions of data points, none of these methods scale well. For such massive data settings, it is nat-ural to examine whether small-space streaming algorithms might yield accurate answers. Here, we provide some neg-ative results, showing that any streaming algorithms that even provide approximately optimal answers to the discrep-ancy maximization problem must use space linear in the input.
 Categories and Subject Descriptors: G.3 Mathematics of Computing -Probability and Statistics Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. General Terms: Algorithms, Design, Performance Keywords: Enter your choices. Spatial Scan Statistics, Kulldorff Scan Statistic, Discrepancy
With the availability of massive data and cheap comput-ing power, the problem of detecting  X  X otspots X  has become ubiquitous and has received a lot of attention in data min-ing [7, 15, 14]. In particular, the special case of detecting  X  X umps X  or local clusters in spatial data has found numer-ous applications in epidemiology, bio-surveillance, astron-omy etc. While a wide range of methods have been proposed to test for spatial clustering (after adjusting for an inhomo-geneous background population), the spatial scan statistic is by far the most popular. The original method proposed by [11] computes the maximum discrepancy region obtained by scanning the spatial region under study with a set of circular regions of various radii. The discrepancy score for each region is based on a likelihood ratio test statistic con-structed to detect significant overdensity under the Poisson or Bernoulli model. The test was shown to satisfy the opti-mality property of being the individually most powerful test . Roughly speaking, this means if the model is correct and the main focus is to find the actual location of clusters rather than just detect an overall clustering effect, the spatial scan statistic is optimal. However, due to the dependencies intro-duced by considering overlapping regions, the analytical dis-tribution of the spatial scan statistic is often intractable and hence one takes recourse to randomization tests [5]. Such a test computes the distribution of the scan statistic by sim-ulating data under the null hypothesis (no clustering) and callibrating the observed value relative to this distribution (using a p-value) to determine the statistical significance of the most discrepant region. In general, 1000 simulations from the null distribution are enough to determine signifi-cance. In practice, when the null hypothesis holds, one may be able to conclude statistical insignificance with a lot fewer repetitions. Originally applied to small sample size prob-lems in epidemiology, the technique has attracted interest in the post 9/11 era for surveillance of massive geographic databases leading to growing interest in computationally ef-ficient algorithms. Recently, the scan statistic has also been used in other areas like bioinformatics [10] and for detecting chatter in massive social networks [17].

Friedman and Fisher [7] introduced an alternative ap-proach which greedily computes a high discrepancy rect-angle, but has no guarantees as to how it compares to the optimal. Their approach is quite general, and works in ar-bitrary dimensional spaces, but is not conservative: many regions will remain unexplored. A series of papers by Neill and Moore [15, 14, 16] developed a grid-based heuristic that uses pruning strategies to avoid enumerating all gular ranges; similar ideas work for square ranges as well. Most recently, Agarwal et al. [1] presented approximation algorithms that run in time O ( 1 n 2 log 2 n ) while guarantee-ing a solution that is at most less than the optimal solution. Their algorithm extends to any convex discrepancy function; the above bound is for the Kulldorff scan statistic.
In this paper we continue the investigation of efficient computational strategies for scan statistics. Our contribu-tions are as follows: Figure 1: Example of maximal discrepancy range on a data set. Xs are measured data and Os are baseline data.
Let P be a set of n points in the plane. Measurements and baseline measures over P will be represented by two functions, m : P  X  R and b : P  X  R . R denotes a range space over P .A discrepancy function is defined as d :( m, b, R )  X  R ,for R  X  X  . For instance, in an epidemiol-ogy application where the goal is to find disease clusters, the points in space could be a collection of counties. The mea-surement m associated with each county is the number of cases of some rare disease and the baseline measure b is the population at risk. If one assumes a Poisson distribution for the number of disease cases, the optimal discrepancy mea-sure d obtained in this scenario is the well known Kulldorff scan statistic.
 Let m R = P p  X  U m ( p ), B = all of P . We will assume that d canbewrittenasaconvex function of m R ,b R . All the discrepancy functions that we consider in this paper satisfy this condition; most discrep-ancy functions considered prior to this work are convex as well. We can write d ( m, b, R ) as a function d :[0 , 1] where d ( m, b, R )= d ( m R ,b R ). We will use d to refer to either function where the context is clear. With these no-tations, the Kulldorff scan statistic (ignoring constants) is given by d ( m R ,b R )= m R log if m R &gt;b R and 0 otherwise.

Linear discrepancy functions are a special class of discrep-ancy functions where d =  X   X  m R +  X   X  b R +  X  .Itiseasytosee that combinatorial (bichromatic) discrepancy , the difference between the number of red and blue points in a region, is a special case of a linear discrepancy function.
 The main problem we study in this paper is:
Problem 2.1 (Maximizing Discrepancy). Given a po-int set P with measurements m ,baselinemeasure b ,arange space R , and a convex discrepancy function d , find the range R  X  X  that (approximately) maximizes d .

An equivalent formulation, replacing the range R by the point r =( m R ,b R )is:
Problem 2.2. (Approximately) Maximize convex discrep-ancy function d over all points r =( m R ,b R ) ,wherethe range R  X  X  .

In this paper we will only consider range spaces consist-ing of axis-parallel rectangles. Two rectangles that contain the same set of points are equivalent for the purpose of dis-crepancy calculation. Therefore there are O ( n 4 )distinct axis-parallel rectangles.
 Boundary conditions . As is customary to avoid over-fitting, we remove from consideration any range that has very small support in either the baseline or measurements. Formally, we require that any range and its compliment has a measure of at least C , for some arbitrary constant C  X  1. In our mapping from ranges to points, this is equivalent to saying that the domain we maximize over is S n =[ C/M, 1  X  C/M ]  X  [ C/B, 1  X  C/B ]. Often we only care about ranges with proportionally more measured data than baseline data. These points are defined by S { ( m R ,b R )  X  S n | m R &gt;b R } .
 q u Grid algorithms . For some algorithms, the data is as-sumed to lie on a grid, or is accumulated onto a set of grid cells. For such algorithms, we will assume a grid of size g  X  g , with measurement and baseline values associated with each grid point as before. Note that in such a grid, the effective number of points is g 2 , and the number of dis-tinct axis-parallel rectangles is O (( g 2 ) 2 )= g 4 , which differs from the corresponding numbers n and O ( n 4 ) for points and axis-parallel rectangles in general position. It will be impor-tant to keep this distinction in mind when comparing grid algorithms with those taking inputs in general position.
In this section we present a simple algorithm running in time O ( n 4 ) that computes the maximum discrepancy rect-angle exactly. Even though there are O ( n 4 )rectanglesto be considered, a naive search strategy might end up taking linear time for each rectangle (to estimate m R ,b R ) yielding a net running time of O ( n 5 ). We use a simple sweep line technique and incremental updates to avoid this problem.
Any set of four points defines a unique bounding rect-angle, with one point defining each side. See Figure 2(a). Fix a pair of points p r ,p l  X  P , and consider the set of all rectangles whose left and right extremes are defined by this pair. Choose a third point p b  X  P in between these two; this point defines the bottom edge of the rectangle if it is below otherwise one of p r ,p l does. Now let a horizontal line seg-ment spanning the rectangle sweep the plane upwards start-ing from p b . Every time the sweep line encounters a point, we update m R ,b R in constant time and recompute the dis-crepancy, maintaining the largest value. Each sweep takes linear time, and there are O ( n 3 ) choices of triples ( p Thus, the algorithm runs in time O ( n 4 ). The details of this algorithm are presented in Algorithm 1.

If the points lie in a g  X  g grid, a similar argument yields an algorithm ( Exact-Grid ) that runs in time O ( g 4 ). This algorithm has the same asymptotic running time as the algo-rithm of Neill and Moore [15], has comparable performance in practice (see Section 7), and is much simpler.
The basis for the heuristic we present in this section is the following linearization lemma, proved in [1].
 Algorithm 1 Algorithm Exact maxd = -1
Sort all points by y-coordinate. for all pairs of points ( p l ,p r ) do
Lemma 4.1 ([1]). A discrepancy function of the form d ( m R ,b R )=  X m R +  X b R +  X  can be maximized over axis parallel rectangles in time O ( n 2 log n ) .
 One way of exploiting linearization is to represent the dis-crepancy function as the upper envelope of a collection of linear functions. The resulting piece-wise linear function closely approximates the true discrepancy, and thus any computation performed using the linear functions will yield an answer close to the true optimum. We refer to this as the Approx-Linear Algorithm. This was the approach used in [1].

However, a better approach exploits two facts: first, we only wish to approximate the value of the maximum discrep-ancy rectangle and second, the function being optimized is monotone in S + n . Recall that each range R  X  X  can be rep-resented as a point r =( m R ,b R )  X  [0 , 1] 2 . We wish to find r =argmax r  X  X  d ( r ), the maximum discrepancy range, or to find a range which closely approximates r  X  . To this end we will approximate the convex function with a set of t linear functions: L = { i } t i =1 ,whereeach i ( m R ,b R )=  X  i  X  b R +  X  i . By taking the largest point r i =argmax r  X  X  i foreachlinearfunction i and then returning r =argmax r i d ( r we can approximate the maximum discrepancy range on d .
Let C z = { ( x, y )  X  S + n | d ( x, y )= z } be the contour of f at value z . For optimal point r  X  , all points in C d also optimal, and any point r  X  C z such that d ( r  X  )  X   X  z  X  d ( r  X  )givesan -approximation to r  X  .
 Let n r be the normal direction to C d ( r ) at r . A linear (b) To approximate the function in its entirety we need many functions.
 function i defines a direction u i , and sorts all points along that direction.

Lemma 4.2. If u i = n r  X  ,then i correctly identifies r  X  arg max r  X  X  d ( r ) .

Proof. By definition of r  X  , there is no point r  X  X  such that d ( r ) &gt;d ( r  X  ). Since d is convex, any point p less than or equal to r  X  along direction u i = n r  X  .Thus r is the maximum point along u i .
 Thus, if we can find a direction u i such that n r  X  = u i a single invocation of the linearization lemma yields the op-timal solution. Figure 3 illustrates this idea, one dimension lower. The plots depicts a convex discrepancy function de-fined over a single input parameter. A straightfoward linear approximation of the function would require us to use mul-tiple linear functions, illustrated in Figure 3(b). However, the direction approach described above requires us only to preserve the ordering of points along this direction, and thus two linear functions suffice (Figure 3(a)).

However, we would like to bound the error caused by a u u = n r for every point r  X  X  .

Lemma 4.3. Consider the point r such that u i = n r and ( r )= i ( r  X  ) .If d ( r )  X  d ( r  X  )  X  ,then i identifies a point r  X  X  that gives an -approximation to r  X  .

Proof. Any point p  X  S + n such that d ( p )  X  d ( r )= d ( r will have i ( p ) &lt; i ( r ). See Figure 2(b). For any point q  X  S + identifies a point q  X  X  such that d ( q )  X  d ( r )  X  d ( r
Lemma 4.4. Consider a piece of a convex curve that is of arc length l and the angle of the normal to the curve changes by no more than  X &lt;  X  2 . This curve segment can be approximated by a line segment such that the maximum error is no more than l X / 2 .

Proof. Set the line so that it connects both of the end points of the curve. Since the curve is convex, its error can be maximized at the mid-point of the segment when the curve is two segments that bends an angle of  X   X   X  at its mid-point: see Figure 2(c). Let the height from the mid-point of the segment to the mid-point of the curve to be h .

We now have a right triangle with angle  X / 2, adjacent side length less than l/ 2, and opposite side length h .Thuswe know that tan(  X / 2) = h/ ( l/ 2). Thus  X / 2 = arctan(2 h/l ) h/l ,for  X  less than  X  .Thus h  X  l X / 2.

Now let r  X  be the maximal discrepancy range. It can lie anywhere in S + n . We want to consider the maximal error allowed by some linear function i .Let r  X  C d ( r  X  ) have n u .Alsolet X   X  ( r  X  ,i ) be the difference in angles between n and n r = u i .Let g ( r  X  ,r ) be the maximal gradient anywhere on C d ( r  X  ) between r  X  and r . Now we can bound the error incurred by approximating the maximum discrepancy range on d with i . since  X   X  ( r  X  ,i ) &lt; X / 2 and thus | r  X   X  r | &lt; 2 arclength of C d ( r  X  ) between r  X  and r . Thus, we need to place a set of linear functions to minimize this quantity for any placement of r  X  in S + n .
Using this intuition we describe a set of linear functions which exploits these properties. For each of t linear functions ( m R ,b R )=  X  i m R +  X  i b R +  X  i let where h i =( i  X  1)  X   X / (2 t  X  1). For t  X  2, set h 1 =  X / 4, as this single function often gives a very good approximation just by itself.

In summary, the Approx-Extents algorithm runs by cre-ating t linear functions according to (4.2) and then invok-ing the algorithm described by Lemma 4.1 in [1] on each of them. Now let the maximal range for each linear function be r i =argmax r  X  X  i ( r ). We return the maximum r i on d defined r =argmax r i d ( r i ). The details of this algorithm are presented in Algorithm 2.
 Algorithm 2 Algorithm Approx-Extents maxd =  X  1 for i =1to t do
The running time of Approx-Extents is O ( tn 2 log n )be-cause we invoke Lemma 4.1 t times.
As we mentioned earlier, algorithms like those presented in [15, 14] aggregate data to a regular g  X  g grid. Since such agridcontains g 2 points, one can run any of the above men-tioned algorithms, setting n = g 2 . However, this is very inef-ficient, and ignores the special structure of the grid. For ex-ample, algorithm Exact would then run in time O (( g 2 ) 4 O ( g 8 ). In this section, we present two algorithms that take advantage of grid structured data.
The first algorithm returns the maximum discrepancy rect-angle in time O ( g 4 ). It is quite similar to the algorithm of Section 3, using a sweep line to explore the space of rectan-gles. The basic idea is as follows. We maintain four sweep lines, two horizontal and two vertical. The two vertical sweep lines move from left to right. At any moment, one of them is at x position i , and the other at x position j&gt;i . As the second sweep line moves from i to the right most position, we maintain a count, for each row, of the total measured and baseline mass in this row between i and j . Thiscanbedoneintime O ( g ) for each move of the second sweep line. Once the two vertical sweep lines are fixed, two horizontal sweep lines move from bottom to top. Since we maintain counts of the total mass in each row, the discrep-ancy function for the range bounded by the four sweep lines can be computed in constant time every time the higher horizontal sweep line is moved. A detailed description is presented in Algorithm 3.
Our second algorithm is approximate, and builds upon theapproximateschemesdevelopedin[1]andinSection4.
 In all our approximate schemes, the main subroutine is an O ( n 2 log n ) time algorithm for maximizing a linear discrep-ancy function over the space of all axis-parallel rectangles. It is easy to extract from this algorithm an O ( n ) algorithm Linear1D for finding the interval in one dimension that maximizes any linear discrepancy function. Naively trans-ferring the algorithm over rectangles to the grid would yield an algorithm running in time O ( g 4 log g ). We improve this to O ( g 3 ). In brief, the O ( n 2 log n ) procedure [1] uses two horizontal sweep lines going from bottom to top. For any position of the two sweep lines, the maximum discrepancy Algorithm 3 Algorithm Exact-Grid : Input is g  X  g grid with values m ( i, j ) ,b ( i, j ) for i =1to g do { Left sweep line } rectangle among rectangles bounded by these lines can be found by projecting all points onto the lower sweep line and solving a one-dimensional problem (the resulting interval de-fines the x-extents of the optimal rectangle). In the modified grid variant, we maintain two arrays m [], b [], each of size g , such that m [ i ] stores the sum of all values m ( i, j ) between the lower and upper sweep lines. Note that this can be maintained in constant time per entry as the upper sweep line moves. For each such movement, we run Linear1D on the values of m [] and b []. The total running time is there-fore g positions of the bottom sweep line  X  g positions of the top sweep line  X  O ( g ) for updating counts and running Linear1D , for a net running time of O ( g 3 ).

We describe the algorithm in detail in two parts. First we give the O ( g 3 ) gridded algorithm for linear discrepancy functions on a grid: Algorithm 4.
 Algorithm 4 Algorithm Linear-Grid : Input is g  X  g grid with values m ( i, j ) ,b ( i, j ), and linear function maxd =  X  1 for i =1to g do { Left sweep line }
This algorithm is then used as the core subroutine in Al-gorithm 5.
 Algorithm 5 Algorithm Approx-Grid maxd =  X  1 for i =1to t do The runtime of Approx-Grid is O ( tg 3 ), since there are t calls of Linear-Grid which runs in O ( g 3 ). This algorithm could also use a family of linear functions as in Agarwal et al. [1]. Then it would give an -approximation to the maximum discrepancy range on the grid and would run in O ( 1 g 3 log g ). We use the former version because it is more efficient as is demonstrated in Section 7
In this section we consider algorithms for the data stream model [9, 6, 2]. Here the data points arrive in some, possibly adversarial, order. An algorithm in the streaming model has limited space, S , to catalog all the points in the stream. Unfortunately most of our results will be lower bounds.
As is typical for lower bounds in the stream model, our lower bounds are proved using reductions from communi-cation complexity problems [12]. We denote C  X  ( f )asthe  X  -error randomized communication complexity of function f . Also, let C 1-way  X  ( f )betheone-way  X  -error randomized communication complexity of function f .
 Definition 1 (Indexing). There are two player P 1 and P . P 1 has an n bit string x and P 2 has an index j  X  [ n ] . The indexing function returns index ( x, j )= x j .
Definition 2 (Multi-Pa rty Set Disjointness). There are t players P 1 ,...,P t . P i has an n bit string x i .The t -party set disjointness [3] function returns disj n,t ( x 1
Theorem 6.1. For any 0 &lt; X &lt; 1 / 4 , C 1 -way  X  ( index  X ( n ) . The result remains true for instances ( x, j ) where x has exactly n/ 2 entries which are 1.

Theorem 6.2 (Chakrabarti et al. [4]). For any 0 &lt;  X &lt; 1 / 4 , This result remains true for the following family F of in-stances ( x 1 ,...x t ) satisfying For a linear discrepancy function, we make the assumptions that m : P  X  N , b : P  X  N and that m  X  =max p  X  P m ( p )and b  X  =max p  X  P b ( p )are constant. As a preprocessing step to any algorithm, we con-struct two point sets P m and P b :foreach p  X  P place m ( p ) copies of the point in P m and b ( p ) copies of the point in P .Foreach p  X  P m let m ( p )=1and b ( p ) = 0. Similarly, for each p  X  P b let m ( p )=0and b ( p ) = 1. Henceforth we will refer to a point p being colored red if p  X  P m ,or blue if p  X  P b .Notethat | P m  X  P n | = O ( n ) and that this con-struction can be achieved in O ( n ) time. Finally note that discrepancy for any R  X  X  is the same with respect to P as it is to P m  X  P b .

We will also consider the problem of maximizing numeri-cal discrepancy . Here we assume that the P points are drawn from some universe U . For all p  X  P , m ( p )=1. Thenthe numerical discrepancy is,
Theorem 6.3. Any P pass streaming algorithm return-ing a t relative approximation to the numerical discrepency with probability at least 3 / 4 requires  X ( n/ ( t 6 P log t )) space. Alternatively, any P pass streaming algorithm returning an additive approximation with probability at least 3 / 4 requires  X (1 / ( P )) space.

Proof. Let ( x 1 ,...,x t )  X  X  be an instance of disj n ,t where n = n/ (3 t 2 )and t =3 t 2 . We will show how to trans-form ( x 1 ,...,x t )intoasize n t = n instance of the numeri-cal discrepancy problem such that t -approximating the max-imum numerical discrepancy problem determines the value of
The stream we define consists of n t elements E where elements will come from a universe [ n ( t + 1)]. We partition the universe into regions R 1 ,...,R n where R i =[( i  X  1)( t + 1) + 1 ,i ( t +1)]. Each player P i determines a size n subset of the elements, Note that every region contains t elements from E .Wenext show how the maximum discrepancy of the set depends on the value of disj n ,t . 1. If disj n ,t = 1 then the maximum numerical discrep-2. If disj n ,t = 0 then each element occurs at most once. Hence, if an algorithm disambiguates between the maximum numerical discrepancy being greater than t / ( n ( t +1)) or less than 2 / ( n ( t + 1)) then the value of disj n ,t is also determined. Therefore, a relative approximation better than p t / 2 &gt;t determines disj n ,t .

Assume that there exists a P pass algorithm A that re-turns a t relative approximation to the maximum numerical discrepancy of n points (with probability at least 3 / 4) and uses at most S ( n, t ) bits of memory. This algorithm gives rise to a communication protocol for disj n ,t as follows. Let the stream be ordered as E 1 ,E 2 ,...,E t .Let m i,j be the memory state of A after the last elements from E i has gone past in the j pass. Each player P i constructs E i from x runs A on E 1 and sends the memory state m 1 , 1 to P 2 . P tializes A with memory state m 1 , 1 , runs A on E 2 and sends the memory state, m 1 , 2 ,to P 3 . They continue in this way where m i,j is the ( i + t ( j  X  1))th message sent. The memory state m t , P determines a t approximation to the maximum discrepancy and, therefore, the value of disj n ,t .Eachmes-sage is at most S ( n, t ) bits long and there are at most t messages. Hence the total communication is O ( t S ( n, t ) bits. By appealing to Theorem 6.2, we deduce that, The second lower bound uses a similar reduction to the first except that t =3 ,n =1 / (8 ) and every point in the above construction is replaced by 8 n/ 3 identical points. Note that the above result also applies to approximating the maximum linear discrepancy where  X  =  X   X  =1. This is because their may be exactly 1 baseline point at every location in the discretized universe. Although the data in this lower bound lies on the grid, it applies when the data need not lie on a grid; shifting each point slightly gives the same discrepancy values.

Corollary 6.1. Any P pass streaming algorithm return-ing a t relative approximation to the maximum linear dis-crepency with probability at least 3 / 4 requires  X ( n/ ( t space. Alternatively, any P pass streaming algorithm return-ing an additive approximation with probability at least 3 / 4 requires  X (1 / ( P )) space.
 The next lower bound gives a dependence on  X  when ap-proximating the maximum linear discrepancy.

Theorem 6.4. Any one pass streaming algorithm that additively approximates the maximum linear discrepancy with probability at least 3 / 4 requires  X ( |  X  | / ) space.
Proof. Consider an instance ( x, j )of index |  X  | / .Let w = |  X  | / (2 ) be the number of 1 X  X  in x . We will show how to transform ( x, j )intoasize n + 1 instance of the linear discrepancy problem such that an additive -approximation of the maximum linear discrepancy problem determines the value of index |  X  | / ( x, j ).

The stream starts with elements determined by P 1 :for each i  X  [ |  X  | / ] such that x i =1therearetwobluepoints with value i . The stream ends with one red point j .Note that the maximum value of  X m R +  X b R +  X  is  X  +  X  if index |  X  | / ( x, j )=0andis  X   X  2 +  X  if index |  X  | / ( x, j )=1.
Then, by appealing to Theorem 6.1, we deduce that the space required is  X ( |  X  | / ).
We now present an algorithm that finds an additive ap-proximation to the maximum linear discrepancy. It is based upon a sampling approach related to the construction of -nets and -approximations [8].
 Theorem 6.5. Consider a set of points S in the plane. Let R be a set of axis-aligned rectangles. An -approximation is a subset A of S such that, for any R  X  X  ,  X  With probability at least 1  X   X  , a random subset of size, is an -approximation.

Theorem 6.6. Let  X  =max(  X , |  X  | ) . There exists an al-gorithm running in time that returns an estimate E such that with probability at least 1  X   X  , | E  X  max R  X  X  d ( R ) | X  .

Proof. We first preprocess the point set as described above. This takes O ( n ) time. We randomly construct a sam-ple P of points as follows: Randomly select a size O ((  X / ) (log(  X / (  X  )))) random subset A of P m . Similarly, construct B ,asize O (( |  X  | / ) 2 log( |  X  | / (  X  ))) random subset of P P = A  X  B .Estimate d ( R )by  X  d ( R )=  X  | A  X  R | | A  X  . Then, by Theorem 6.5, with probability at least 1  X   X  , for all R  X  X  , Hence with probability at least 1  X   X  , | d ( R )  X   X  d ( R ) all R  X  X  . We then appeal to Lemma 4.1.
 It should be noted that a related algorithm can be used for numerical discrepancy or when the data is known to lie on a grid. Further, observe that this algorithm (when used in conjunction with the reservoir sampling technique [18]) can be viewed as a streaming algorithm that uses O space.
We now present a performance study of the schemes de-scribed in this paper, and compare them to prior work. Algorithms . We implemented the simple exact algorithms Exact and Exact-Grid , as well as the approximation algo-rithms Approx-Extents and Approx-Grid .Wecompare these to two algorithms from prior work; the grid algorithm NM-Grid of Neill and Moore [15], and the approximation algorithm Approx-Linear of Agarwal et al. [1].
 Code for NM-Grid was provided by the authors [13]. Their code is a black box that solves the maximum discrep-ancy problem and then runs N randomization tests. It only returns a range if it is statist ically significant. In order to compare their algorithm we set N =0andusethediscrep-ancy generated by Exact-Grid : both solve the problem ex-actly on the grid. The code for NM-Grid has an additional parameter allowing it to find an approximate solution. Neill and Moore [15] demonstrate this giving 5  X  to 20  X  speedups while only misidentifying &lt; 10% of the regions. We did not investigate this additional parameter due to difficulties in extracting the discrepancy values. The other algorithms were implemented by us. All experiments were run on a ma-chine with 3GHz Pentium IV processor and 1Gb SD-RAM running CentOS.

It should be noted that given the maximum discrepancy range for a given set of data, the problem remains to deter-mine whether it is statistically significant. This is tradition-ally done by running about 1000 rando mization tests, where the experiment is repeated on randomly generated examples from a null hypothesis. Only if the maximum discrepancy range is larger than 95% of the maximum discrepancy ranges from the randomization tests is it deemed significant. Thus the problem we describe in this paper, is repeatedly solved on the order of 1000 times in practical applications, making an efficient solution paramount for massive data sets. Here we study solving the maximum discrepancy problem once. Data sets . We used a combination of real and synthetic data to evaluate the algorithms. We start with the example-city-in data file provided with the code for NM-Grid which con-tains 61291 data points of the form ( x, y, b, m )where( x, y ) lies on a 175  X  203 integer grid distributed according to a relevant population, and where b, m  X  X  0 , 1 } . The popu-lation data represents the emergency patients X  home loca-tions in Western Pennsylvania from 1999 through 2002, and the measured data are the locations corresponding to pa-tients from a two month span. We generate data sets of size n = { 256 , 512 , 1024 } first by sampling ( x, y )coordi-nates from example-city-in .Wethenlet x = x + u 1 and y = y + u 2 where u 1 ,u 2 are drawn uniformly at random from [0 , 1], in order to take the points off the grid. We next generate b using an exponential distribution to model the population: we set b = exp(6 u ) where u is uniform ran-dom in [0 , 1]. We then generate a random rectangle R of size 7  X  9somewhereinthe( x, y ) domain. Finally we gen-erate m = P oisson ( b  X  f 2 )(where f 2 = . 005) if the point is in R and m = P oisson ( b  X  f 1 )(where f 1 = . 001) if the point is outside R . The sample in R should indicate a larger discrepancy range. We tested various sizes of R and various values for f 1 and f 2 , but these do not significantly affect the runtimes or accuracies of our algorithms.

Both the gridded algorithms and the approximation algo-rithms can tradeoff their accuracy for runtime. For the grid-ded algorithms we use a g  X  g grid where g = { 64 , 128 , 256 , 512 } . For the approximation algorithms we set ,themaxi-mum error, to = { . 01 ,. 1 , 1 , 5 , 10 , 20 , 40 , 100 } Linear and the number of linear functions t = { 16 , 8 , 4 , 2 , 1 for Approx-Extents and Approx-Grid .
Some of the algorithms we evaluate are approximate, and others are defined on a grid (which incurs its own error). To compare these algorithms, we compared their performance versus error curves. In other words, we looked at, for a fixed error bound (percentage discrepancy), how fast each algorithm ran, and for a fixed budget in time, what error was obtained by the algorithms.

For each data set we use the returned maximum discrep-ancy value d Exact of Exact as ground truth. We measure error as the percentage discrepancy for an algorithm A by E
A = d A /d Exact where d A is its returned maximum dis-crepancy value of that specific data set. We used 30 data sets for size n = { 256 , 512 , 1024 } which are generated as described above. We do not test larger values of n in this manor because Exact becomes prohibitively slow. For each algorithm at each parameter setting we average the E A val-ues and the runtime values over all 30 data sets of each size. Figure 4 presents the results, for all three values of n . Approximate versus grid algorithms . The first obser-vation we can make is that the approximation algorithms are consistently faster than the grid-based algorithms, if one wishes to get within roughly 20% of the true maximum. Approx-Extents performs the best overall, in all cases.
As the desired accuracy increases, the approximation al-Figure 4: Running time (in seconds) vs error (as a percentage of the true answer) for all the algorithms. gorithms Approx-Extents and Approx-Linear scale bet-ter, and thus the disparity between their performance and that of the grid algorithms increases. At the 90% accuracy level, approximation algorithms are 3400  X  ,94  X  ,and31  X  faster than the gridded algorithms, for point sets of size 256, 512, 1024. No gridded algorithm has an expected 95% accu-racy, even for a 512  X  512 grid, while the Approx-Extents algorithmcanreturnthisexpectedapproximationwith1 linear function. This is further demonstrated in Figure 5. As n increases, this disparity decreases: the approximate algorithms degrade in performance; however, their behaviour remains superior to the grid-based schemes.
 Variationinsolutions . ThevaluesplottedinFigure4 represent averages over 30 trials each of the algorithms. For both NM-Grid and Exact-Grid , the standard deviation of run times are about 25% of the total time, where as for Approx-Extents and Approx-Linear the standard devi-ations are only about 2% of the total time. This is likely because the true asymptotic behavior might be governed by the fact that NM-Grid and Exact-Grid vary in time de-pending on how they scan over the data and how many times they do the updates (if statements), whereas the approxima-tion algorithms perform the same operations independent of the positioning of the data.
From the above graphs, we also note that Exact-Grid , although usually worse than NM-Grid , is comparable in speed. This is noteworthy since NM-Grid is a fairly com-plicated algorithm that uses a recursive prune and search to determine the optimal solution. On the other hand, Exact-Grid can be written in a few lines of code. Moreover, the code provided for NM-Grid cannot be modified easily if one wishes to use it for other functions.

As the size of the point sets increase the algorithms that are not on a grid become slower at a faster pace than those on a grid. We demonstrate this further by generating points sets with n =10 , 000 in the same way as before and com-paring the gridded algorithms: Exact-Grid , NM-Grid , and Approx-Grid .Wealsorun Approx-Extents with t = { 1 , 8 } . Again we plot, in Figure 6, the curves demon-strating the tradeoff between percentage discrepancy and time. However, since it takes too long to run Exact ,we use the maximum discrepancy returned by any algorithm in place of the ground truth for each data set. Figure 6: Running time (in seconds) vs error (as percentage of best known answer) for 10000 points. Note that NM-Grid still perform better than Exact-Grid . However, the difference is small, and might often be outweighed by the simplicity of the code for Exact-Grid .Also Approx-Grid is now working much faster than Exact-Grid and NM-Grid . This is probably because the true asymptotic behavior is somehow governed by the num-ber of grid cells that have nonzero values, and Approx-Grid is faster by a factor of g/t asymptotically. So when g is large and t small this pays off.

Also notice how Approx-Extents continues to perform faster than NM-Grid and Exact-Grid for g = 512, and has much less error. Hence, for large data sets, Approx-Extents is better if minimizing error is important. How-ever, if performance is critical, and error bounds on the order of 20% are tolerable, then gridded algorithms are superior, and Approx-Grid is the best algorithm to use.
Both Approx-Linear and Approx-Extents approximate the convex discrepancy function with a set of linear func-tions. Approx-Extents requires far fewer linear functions Figure 7: Number of linear functions needed by Approx-Linear and Approx-Extents to achieve and expected error. to get the same expected erro r. We verify this by plotting the percentage discrepancy (averaged over the data sets used above with n = { 256 , 512 , 1024 } ) versus the number of linear functions used to approximate d (see Figure 7).

Approx-Linear requires about 80 linear functions to get an expected error of about 1%, where as Approx-Extents only needs 8: a 10  X  speedup. Also Approx-Extents never drops below 93% expected percentage discrepancy even with 1 linear function whereas Approx-Linear drops below 90% with less than 5 linear functions.
To assess the effect of our approximation parameters on the statistical power of the resultant scan statistic, we ana-lyzed the variation in the distribution of percentage discrep-ancy. We note that high variation in this distribution would lead to erroneous p-values and invalidate our approximation algorithms. We find consistently small variance estimates both for Approx-Extents and Approx-Linear .Infact, Approx-Extents consistently provides small variance for t = { 8 , 16 } ,asdoes Approx-Linear for = { . 01 ,. 1 } .The coefficient of variation drops off to zero rapidly with increase in E A (see Figure 8). Also, the coefficient of variation tends to get smaller with increase in the number of points. A direct evaluation of the power curves involving large scale simulation experiments will be reported in future work. We thank the Anomaly Detection working group at SAMSI and Daniel Neill for comments and observations and Neill et al. for providing their code [13] for comparison.
