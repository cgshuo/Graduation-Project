 Chris Ding chqding@lbl.gov Xiaofeng He xhe@lbl.gov In recent years spectral clustering emerges as solid ap-proach for data clustering. Spectral clustering includes a class of clustering methods (Bach &amp; Jordan, 2003; Chan et al., 1994; Ding et al., 2002; Hagen &amp; Kahng, 1992; Meila &amp; Xu, 2003; Ng et al., 2001; Shi &amp; Ma-lik, 2000; Yu &amp; Shi, 2003) that use eigenvectors of the Laplacian of the symmetric matrix W = ( w ij ) con-taining the pairwise similarity between data objects i, j . Spectral clustering has well-motivated clustering objective functions and many interesting and useful properties can be proved.
 Spectral clustering is most conveniently applied to 2-way clustering problem using a single eigenvector. When applying to multi-way ( K -way) clustering, there are two main approaches: (1) the 2-way spectral clus-tering is recursively applied or (2) an embedding to spectral space using several eigenvectors is first done and some other methods, such as K -means (Ng et al., 2001; Zha et al., 2002; Bach &amp; Jordan, 2003), are used to cluster the points. These cluster assignment meth-ods are indirect (except perhaps a recent study (Yu &amp; Shi, 2003)). Here we propose and study a direct K -way cluster as-signment method. The method transforms the prob-lem to one of finding valleys and peaks of a 1-D quan-tity called cluster crossing, which measures the clus-ter overlap across a cut point along linear ordering of data objects. In other words, the method linearizes the clustering assignment problem.
 The linearized assignment algorithm depends crucially on an algorithm for ordering objects based on a pair-wise similarity metric. The ordering is such that ad-jacent objects are similar while objects far away along the ordering are dis-similar. We show that for such an ordering objective function the inverse index permu-tation has a continuous (relaxed) solution which is the eigenvector of the Laplacian of the similarity matrix. This spectral ordering approach has been previously considered for reduction of the envelope of a sparse symmetric matrix. (Barnard et al., 1993). Our con-tributions are (1) providing a clear ordering objective function and a new derivation, and (2) introducing a modification that significantly improves the ordering. This is discussed in  X  3.
 The actual linearization is performed via the cluster crossing, the sum of similarities symmetrically across a cut point along the linear ordering. Computation-ally, this is the sum along anti-diagonal direction on W within a pre-specified bandwidth. Details are dis-cussed in  X  4.
 If the clusters in a dataset are well-separated, i.e., the similarity matrix W is nearly disconnected, the clus-tering crossing along the spectral ordering can easily detect the clusters. For datasets where clusters mod-erately or strongly overlap, cluster crossing directly computed from the similarity matrix W provides weak signals for revealing cluster structure. The connectiv-ity matrix (Ding et al., 2002) provides sharper cluster structure and is adoped in the linearized assignment algorithm. This is briefly discussed in  X  5.
 In summary, the linearized assignment algorithm de-pends on three techniques: (i) an ordering of the data objects, (ii) cluster crossing, (iii) the connectivity ma-trix. These are discussed in the following sections. Given n objects and the similarities between them W = ( w ij ), the objective of ordering is to insure that (i) adjacent objects are similar (ii) the larger the dis-tance between the objects, the less similar the two ob-jects are.
 The ordering is defined by the index permuta-x = ( x 1 ,  X   X   X  , x n ) T , the permuted vector is  X  ( x ) = ( x (  X W  X  T ) ij = w  X  resent the pairwise similarities between objects with fixed distance  X  on the permuted order. We define the global ordering objective as min Here larger distance similarities are minimized more heavily than smaller distances, to ensure that the larger the distance between a pair of objects, the less similar these two objects.
 Let us compute the optimal  X  . First, let j = i +  X  or  X  = | i  X  j | , J (  X  ) can be rewritten as
J (  X  ) = Replacing  X  i by i in the summation and noting that index i is permuted to  X   X  1 i , where  X   X  1 is the inverse permutation, we obtain = For simplicity, we define the shifted and rescaled in-verse index permutation q which satisfies where q is further scaled by q i  X  ( n 3 / 12  X  n/ 3)  X  1 / 2 which does not change the permutation. Note that X where D is a diagonal matrix with each diagonal el-ement being the sum of the corresponding row ( d i = P j w ij ). Therefore, we need to minimize q T ( D  X  W ) q for q i taking those discrete values of Eq.(2), subject to the constraints in Eq.(3). Using a Lagrangian multi-plier for the second constraint in Eq.(3), minimization of J (  X  ) becomes Finding the optimal solution for the discrete values of q is a combinatorial optimization problem, and is likely to have no polynomial-time optimal algorithms. How-ever a continuous solution for q can be computed. We relax the restriction that q i must take discrete values of Eq.(2) in [  X  1 , 1], and let q i take continuous values in [  X  1 , 1]. With this,  X  J 1 can be minimized by solving an eigenvalue problem. It is well-known that q is an eigenvector of the equation Clearly q 0 = 1 = (1 ,  X   X   X  , 1) T is an eigenvector with  X  = 0. All other eigenvector are orthogonal to q 0 , i.e., the first constraint in Eq.(3) is also satisfied. Therefore q 1 is the desired continuous solution of the distance sensitive ordering.
 We note that earlier work on sparse matrix envelope reduction (Barnard et al., 1993) based on different mo-tivation, reaches the same eigenvector solution. Our contribution here is to introduce the distance sensi-tive objective function J (  X  ), and provide the detailed derivation using shifted inverse permutation vector  X   X  1 to show that the solution is q 1 .
 Now we make a crucial modification on the above solu-tion which (a) improves the quality of the solution, and (b) makes a direct connection to the scaled PCA and connectivity matrix in  X  5. The modification is made on the constraints in Eq.(3); we weight each point i with the degree d i , the column sum of the similarity matrix W . In graph theory (Chung, 1997), d i is called the volume of node i . The new constraints are (more discussion later). With these constraints, the minimization problem of J (  X  ) becomes Relaxing q i to continuous values in (  X  1 , 1). the solu-tion for q satisfies the eigenvalue equation Let q = D  X  1 / 2 z . Substituting it into Eq.(8), we obtain This is a standard eigenvalue equation. Thus the eigenvectors z k and q k have the orthogonality relation The trivial eigenvector is q 0 = e with  X  0 = 0. Thus the constraints in Eq.(6) are automatically satisfied. Since ( D  X  W ) is semipositive definite, we have In distance-sensitive ordering, we seek q k with the smallest  X  k , or the largest  X  k . The desired solution for the permutation  X   X  1 i is q 1 ( i ) (the i th element of q subject to the rescaling and a constant shift condition according to Eq.(2). Note that Thus  X   X  1 can be uniquely recovered from q 1 . A simple implementation to recover the permutations is to sort the elements of q 1 in increasing order. This sorting induces the desired index permutation  X  . In Figure 1, we show a matrix where  X  J 1 ordering is compared to  X  J 2 ordering. Clearly,  X  J 2 ordering provides a better distance-sensitive ordering. The values of the initial ordering objective J (  X  ) are where h J i = ( mean value for J (  X  ). Another way to measure the ef-fects of ordering is to use the bandwidth and envelope of a symmetric sparse matrix C . The bandwidth b ( i ) at row i is the largest distance between the diagonal element and any nonzero element in row i . The band-width of the entire matrix is the largest of b ( i ) and the envelope is the sum of b ( i ). For the  X  J 1 ordering of C , bandwidth = 495 , envelope = 156 , 240 . For the  X  J 2 ordering of C , bandwidth = 320 , envelope = 48 , 650 . Clearly,  X  J 2 ordering is better.  X  J 2 ordering uses the weighted constraints of Eq.(6) while  X  J 1 uses the unweighted constraints of Eq.(3). To understand why the weighting leads to better or-dering, first observe that objects with large d i will get smaller | q i | to balance out the equation. Now we re-write Eq.(2) as  X   X  1 i = q i  X  ( n +1) / 2 ignoring the over-is in the middle. Smaller | q i | indicates  X   X  1 i is near the middle, thus objects with large d i are more likely to be permuted towards the middle using the weighted constraints of Eq.(6). This is favorable, since objects with large d i are more likely to have more edges; and moving these objects towards middle decreases the dis-tances among these similar objects, therefore improves J (  X  ).
 Connection to spectral clustering Note that eigenvector of Eq.(5) is used in the Ratio cut spectral clustering (Hagen &amp; Kahng, 1992) and eigenvector of Eq.(8) is used in the normalized cut (Shi &amp; Malik, 2000) and min-max cut (Ding et al., 2001) spectral clustering.
 In deriving 2-way spectral clustering, only the signs of the cluster indicator vector are useful and all objects in a cluster have the same magnitude. This indica-tor vector is then relaxed into the eigenvector. In our derivation of spectral ordering, both the sign and mag-nitude of the scaled and shifted permutation vector are useful, see Eq.(2). Since an eigenvector has both sign and magnitude, the relaxation of the permutation vec-tor is therefore better quality approximation than the relaxation of cluster indicator.
 From this analysis, we believe the better reason for the success of spectral clustering is due to the order-ing, instead of relaxing the discrete cluster indicators. In fact, this ordering perspective is used in actual im-plementation of spectral clustering (Hagen &amp; Kahng, 1992; Shi &amp; Malik, 2000): one first sort q 1 to provide a linear ordering, then along this ordering, search for the cut that optimizes the cluster objective function. Thus our ordering analysis provides a deeper understanding of spectral clustering.
 Our results indicates that  X  J 2 ordering is better than ordering. This is due to the weighting of d i , the node degree, in Eq.(6). Similar motivation is used in nor-malized cut . Let s 12 be the cut between two subgraphs C , C 2 . All three graph clustering objective function can be written as J = s 12 /a 1 + s 12 /a 2 . For Ratio cut , a This weighting of the subgraph volume improves upon the simple weighting of the subgraph size in ratio cut. For min-max cut, a k = weights inside C k .
 That  X  J 2 ordering is better than  X  J 1 ordering implies that normalized cut and min-max cut in general pro-vides a better clustering than ratio cut. This fact is ob-served in experiments (Shi &amp; Malik, 2000; Ding et al., 2001).
 It is sometimes happens that there is a symmetry among several nodes in the graph, i.e., G ( W ) G T = W , where G is a permutation specifying an element in the invariant symmetry group. In this case, an eigen-nodes with the same value. If this happens, neither the ordering nor clustering problems can be uniquely determined. This is not necessarily a weakness of the spectral methods, although it become obvious from the perspective of the eigenvector. In practice, this happens rarely for weighted graphs. We start with cluster overlap. Given two clusters C k , C l , the cluster overlap can be defined as the sum of pairwise associations between two clusters, In spectral clustering, s kl are minimized.
 Cluster overlap involves all | C k |  X  | C l | pairwise similar-ities. We define cluster crossing as the sum of a small fraction of the pairwise similarities. This is aided by linear ordering data points. Given a linear order o of all objects, at each site i of the order, we can sum over w ij within a window size 2 m + 1 across the site i , This corresponds to sum along the anti-diagonal di-rections in the similarity matrix W with a bandwidth m . There are 2 n  X  1 anti-diagonals in a matrix, among them  X  ( i ) are n full-step anti-diagonals. We also utilize the other n  X  1 half-step anti-diagonals, i.e., The final crossing is the weighted average (When i is close to the two ends, i.e., n  X  i  X  m or i  X  m , the sum should be properly weighted to reflect the fact that the number of similarities in the sum is less than the normal case.) Clearly, cluster crossing  X   X  ( i ) should have a minimum at the cluster boundary between C k , C l . As i moves away from the boundary,  X   X  ( i ) increases. This form the basis of the linearized cluster assignment. This approach works for K &gt; 2 as well as for K = 2; In essence, it reformulate a problem of K -way cluster-ing with pairwise similarities into a K -way clustering problem in 1-dimension. To illustrates the basic ideas in this approach. we com-pute the crossing of the matrix C shown in Figure 1(bottom). The crossing curves are shown in Figure 2. For matrix C using  X  J 2 ordering, the crossing (mid-dle panel in Figure 2) exhibit clearly the five-cluster structure.
 In cluster crossing curve, the valleys are more impor-tant than the peaks. This is because between two consecutive valleys, there could be several overlapping clusters so that the peaks are not as pronounced as the valleys. Using the valleys, we can clearly separate sets of clusters (composite cluster).
 This suggest a divide-and-conquer approach, i.e., re-cursively apply the algorithm on each set of clusters, until the total number of cluster reach the pre-specified K , or some other criteria are met (like the top-down divisive clustering approach). For example in Figure 1 (middle), the 2nd and 3rd clusters overlap slightly, and the corresponding valley point in the crossing curve (middle panel in Figure 2) is not as low as others (although unambiguously clear). We might consider these two cluster as one composite cluster. Thus we cut the crossing into 4 clusters at present round and cut the composite cluster in next round. Given a symmetric similarity matrix W , one can ob-tain a spectral decomposition to get the principal com-ponent analysis (PCA), a widely used technique in multivariate statistics. In scaled PCA proposed in (Ding et al., 2002), one performs the following spectral decomposition here the spectral decomposition is performed on the scaled matrix c W = D  X  1 / 2 W D  X  1 / 2 . Clearly, the eigen-vectors z k are governed by Eq.(9). The magnitudes of all eigenvalues are less than 1, as in Eq.(11). The con-nectivity matrix C is obtained by truncating the PCA expansion at K terms and setting the eigenvalues to unity,  X  k = 1, as where q k = D  X  1 / 2 z k is called scaled principal com-ponents due to its similarity to the usual PCA. q is governed by Eq.(8), and is closely related to spectral clustering. It is shown via a perturbation analysis that C has a so-called self-aggregation property that connectivities (matrix elements in C ) between differ-ent clusters are suppressed while connectivities within clusters are enhanced. Thus C is useful for revealing cluster structure.
 Connectivity matrix approach involves a noise reduc-tion procedure. The probability that two objects i, j belong to the same cluster is p ij = C ij /C 1 / 2 ii C 1 / 2 reduce noise one set where  X  = 0 . 8. For a range of problems,  X  = 0 . 5  X  0 . 9 leads to very similar results.
 As an illustration, the original similarity matrix (based on 5 newsgroups in  X  6) is shown in Figure 1(bottom) using  X  J 2 ordering, where cluster structure is not ap-parent. Connectivity matrix (shown in Figure 1) con-structed based on this similarity matrix has clear clus-ter structure. Prespecify K as the numnber of clusters, and set band-width m = n/K (or the expected largest cluster size). The complete algorithm is as follows: (1) Compute connective matrix C ; (2) Compute the  X  J 2 ordering of C ; (3) Compute the crossing of C based on  X  J 2 ordering. (4) Locate valley points in the crossing curve. Assign each region sandwiched between two valley points or ends to one composite cluster. (5) If the total number of current composite cluster is less than K , recursively apply the algorithm to the largest to further split it. Note that this recursive clustering algorithm differs from the usual recursive 2-way clustering in that, a current composite cluster is partitioned into several clusters depending on the crossing curve, not restricted to 2 clusters. It is possible that all K clusters are iden-tified using one crossing curve as in  X  4. Thus the total number of recursion is less than or equal to K -1, which is required by the usual recursive 2-way clustering. In step (5), the choice of next cluster to split is based on the largest (size) cluster. This simple choice is more oriented towards cluster balance. More refined choices for cluster split is discussed in (Ding &amp; He, 2002). The main advantage of this approach is that clusters are formed consistently. In 2-way recursive clustering, each current cluster is formed via a certain clustering objective function which is correctly motivated for only true clusters, not for composite clusters. For example in normalized cut, the cluster objective function for K -way clustering can not be recursively constructed from the 2-way clustering objective. Therefore the 2-way recursive procedure is only a heuristic for K -way clustering. In our linearized assignment, cluster are assign based on the criteria that the connectivity be-tween clusters are small, which is valid for both true clusters and composite clusters. The linearized cluster assignment method is applied to Internet newsgroup articles. A 20-newsgroup dataset is from www.cs.cmu.edu/afs/cs/project/theo-11/www/naive-bayes.html. Word-document matrix is first constructed. 1000 words are selected according to the mutual information between words and docu-ments in unsupervised manner. Standard tf.idf term weighting is used. Each document is normalized to 1. We focus on two sets of 5-newsgroup combinations. The choice of K = 5 is to have some variety in the recursive steps (we avoid K = 4 , 8). These two news-group combinations are listed below: Datasets with well separated clusters are easy to han-dle; we are interested in clustering medium and large overlapping clusters. To measure cluster separation, we compute s kl and define the symmetrically scaled cluster overlap as the cluster separation index between clusters C k , C l as  X  kl = s kl / aration is defined as Clearly 0  X   X   X  1 and  X  kk = 1. For a complete graph  X  kl = 1 and  X  = 1. Cluster overlap s kl and separation index  X  kl can be conveniently stored in a matrix S , where S (upper-right triangle including diagonals) = s kl and S (lower-left triangle)=  X  kl . For datasets A, B , their overlap-separations are Their average separation are  X  ( A ) = 0 . 695 ,  X  ( B ) = 0 . 755 . These information are useful. For example, for dataset B, NG18 (mideast) is a coherent clus-ter because s 55 = 996 is relatively large, whereas NG13(electronics) is less coherent because s 44 = 472 is relatively small. The overlap between NG2 (graphics) and NG3(windows OS) is relatively large:  X  12 = 0 . 578 while the overlap between NG8 (auto) and NG13 (elec-tronics) is also large:  X  34 = 0 . 483. Overall, dataset A is moderately overlapping and dataset B is strongly overlapping.
 The above is based on a random sample of documents from the newsgroups. Each cluster has 100 documents. To accumulate sufficient statistics, for each newsgroup combination, we generate 5 samples and average their performance.
 Dataset A The cosine similarity matrix W among documents shown in Figure 1(bottom). The cluster structure is not clear from the similarity matrix W . The connec-tivity matrix C is displayed in Figure 1(top) which exhibits the cluster structure.
 Clustering crossings based on W and C are shown in Figure 2. The crossing for C based on  X  J 2 ordering (middle panel) shows clear cluster structure, whereas crossing for C based on  X  J 1 ordering (bottom pannel) shows less clear cluster structure. Crossing for W (top pannel) show even less cluster structure.
 Based on the crossing of C using  X  J 2 ordering, local minima in the valleys are identified using a simple smoothing procedure, where the new smoothed value on each point is the average of old values on 5 near-est points. This smoothing procedure overcomes the local abrupt changes and automatically compute the more stable or consensus valley points, although the difference with un-smoothed one is often small. Data points between two valleys or ends are assigned to one cluster. Thus all points are assigned into 5 clusters in one shot.
 Each of newsgroup article X  X  cluster label is known (al-though not necessarily perfect). Using this, the con-fusion matrix T = ( t kl ) for the clustering results are computed, where t kl = number of points belonging to cluster k but clustered to cluster l . Based on the lin-earized cluster assignment results, T is computed as For this results, the clustering accuracy, Q = P k t kk /N = 90 . 5% .
 The clustering experiment is repeated for 5 different random samples. The accuracy for this linearized or-dering approach is listed in Table 1. For the same dataset, the cluster accuracy using recursive 2-way spectral clustering and standard K -means are also listed in Table 1. One see that the linearized assign-ment outperform slightly over the recursive 2-way clus-tering and significantly over the K -means .
 Dataset B The cosine-similarity matrix W is shown in Figure 3 (top panel, using  X  J 2 ordering). The connectivity ma-trix C of this dataset is shown in Figure 4. The over-lap between NG2 (computer graphics) and NG3 (Win-dows OS) is large; the overlap between NG8 (autos) and NG13 (electronics) is large as well. These are expected from the cluster separation indexes S ( B ), and also can be confirmed by inspecting the cosine-similarity W shown in Figure 3 (bottom panel), using  X  J 2 ordering based on C .
 The crossing based on C is shown in Figure 5 (top panel). The lower panels show the crossing curves after four successive applications of smoothing. Based on this crossing, we can identify three composite clusters by two clear and low-lying valley points. The two large composite clusters are further clustered using the same linearized algorithm.
 Repeating the experiments on 5 random samples from dataset B, the clustering accuracy is listed in Table 1. The linearized assignment outperforms the recursive 2-way spectral clustering and the standard K -means . In summary, we propose and study a direct K -way cluster assignment method that linearize the cluster-ing problem into 1-D clustering crossing curve. The method depends on an effective linear ordering pro-vided by the spectral ordering. We prove a clear derivation of the distance sensitive ordering and show the shifted and scaled index permutation vector is re-laxed into eigenvectors of the Laplacian of the similar-ity matrix. Our results provides a deeper insights to spectral clustering as well.
 This work is supported by U.S. Department of Energy, Office of Science, Office of Laboratory Policy and In-frastructure, through an LBNL LDRD, under contract DE-AC03-76SF00098.

