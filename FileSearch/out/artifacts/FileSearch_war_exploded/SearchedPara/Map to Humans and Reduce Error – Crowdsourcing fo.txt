 Detecting duplicate entities, usually by examining metadata, has been the focus of much recent work. Several methods try to identify duplicate entities, while focusing either on accuracy or on efficiency and speed  X  with still no perfect solution. We propose a combined layered approach for du-plicate detection with the main advantage of using Crowd-sourcing as a training and feedback mechanism. By using Active Learning techniques on human provided examples, we fine tune our algorithm toward better duplicate detec-tion accuracy. We keep the training cost low by gathering training data on demand for borderline cases or for inconclu-sive assessments. We apply our simple and powerful methods to an online publication search system: First, we perform a coarse duplicate detection relying on publication signa-tures in real time. Then, a second automatic step compares duplicate candidates and increases accuracy while adjust-ing based on both feedback from our online users and from Crowdsourcing platforms. Our approach shows an improve-ment of 14% over the untrained setting and is at only 4% difference to the human assessors in accuracy.
 H.3.2 [ Information Storage and Retrieval ]: Metadata; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Crowdsourcing, Duplicate Detection, Active Learning, Ma-chine Learning, Optimization
The Web is an immense repository of diverse information describing all possible entities. With theoretically infinite data sources, many entities are described in several places, leading to inherent duplicate data and/or metadata. In this paper we present a method to deal with the detection of duplicates in a collection of entities.

In duplicate detection, computers are particularly good at detecting duplicate candidates, but the difficult task is the actual binary decision if the two entity descriptions repre-sent the same real-world entity or not. This is exactly where humans excel and this task can be crowdsourced via an on-line marketplace, for a low cost. Coupled with automatic algorithms, Crowdsourcing can ideally leverage the best ca-pacities of both human and machine. We propose a way to use the crowd for machine learning, in an integrated system, where the two methods work together towards improving their performances on the task at hand.

Without restricting the generality of our method, we fo-cus on the special case of duplicate detection for scientific publications and we apply the findings to an online publica-tion search system,  X  X reeSearch X  1 . Our live system identifies metadata documents that refer to the same real-world publi-cation in order to group them into one merged representation at query time, to simplify the result list.

At the core of our duplicate detection method we still have a general purpose similarity scorer, working on attribute,value pairs without additional knowledge as to the semantic of the attribute, therefore keeping the solution usable by general duplicate detection tasks. The decision for a pair of enti-ties is based on several features learned by our method from human provided training data gathered using Amazon X  X  Me-chanical Turk 2 (MTurk) service in an active learning manner. It is naive to think that the crowd performs as good as, or better than experts, but due to the low cost, it has its ben-efits. To tackle the quality issue, we employ and compare several algorithms to compute a confidence in the workers.
The contributions of this paper include: (1) We introduce a novel way to use Crowdsourcing for identifying duplicates in a live system; (2) We describe how to use input from the crowd to continually improve an automatic algorithm; (3) We evaluate several methods to maximize the quality of crowd assessments; (4) We evaluate and discuss several aspects of using Crowdsourcing platforms. The current paper draws on the advancements done in Duplicate Detection, Active Learning, and Crowdsourcing.
Duplicate detection consists in finding similar records -entity descriptions -and deciding if two descriptions refer to the same real world entity. It is also known as entity linkage [10], merge-purge [9], deduplication [15], reference reconcil-iation [8], or resolution [2]. [6], a survey of entity matching, distinguishes three steps to the process: data preparation, http://www.freesearch.isearch-it-solutions.net http://www.mturk.com field matching, and duplicate detection. For data prepara-tion and field matching we use an approach presented in a previous work [13]. Field similarities are aggregated using weights depending on the field name and the final decision about duplication is done using a threshold. We resort to active crowd learning to find the weights and the threshold.
Active Learning[3] focuses on the costly acquisition of la-bels for the instances to be used for training in machine learning. The improvement in data quality via repeated la-beling in a setting involving multiple and noisy labelers is studied in [16]. In extreme class imbalance the labelers would be more useful for searching for instances of the missing class [1]. The ALIAS system [15] employs Active Learning also for deduplication. We learn how to deduplicate from the crowd, with lower costs, although it provides multiple noisy labels.
Crowdsourcing is a mean and facilitator for achieving Hu-man Computation[18, 19]. The quality of the crowd data has been recognized as an issue and addressed in several papers. The error-rate of workers can be evaluated using an EM algorithm [4], and biased workers, producing recov-erable errors, can be separated from workers that always give erroneous answers[11]. Also using an EM algorithm, in [14] the error-rates and the underlying actual hidden label are estimated in the absence of a golden standard. Selec-tive repetition of some micro tasks can improve data quality [16]. Active Learning can be combined with Crowdsourcing, in order to adaptively train machine learning algorithms, at a reduced cost. ZenCrowd[5] employs a probabilistic frame-work to link entities and identify unreliable workers. The authors of [17] adaptively learn an embedding of objects into an Euclidean space. In the absence of a golden stan-dard we actively learn from the crowd the best parameters for our automatic methods, simultaeously tackling the qual-ity issue.
Often, entities have two or more representations in real world databases. Duplicate records do not share a common key and/or they contain errors that make duplicate match-ing a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of stan-dard formats, or any combination of these factors.

Without restricting the generality of our methods, we will focus on the domain of digital libraries and give examples of scientific publications. Let us use the following notations: e is an entity, described as a set of attribute-value tuples: e { ( FieldName,FieldV alue ) } ; p i,j a pair of entities ( e that can be duplicates or not; P is the set containing all the pairs p i,j from which we want to identify the duplicates.
The entity matcher, called DuplicatesScorer , is intro-duced in [13], and we shortly review it here. The input to the DuplicatesScorer is a request consisting of segments, which may either be attribute value pairs or unqualified values. The requests intend to find an entity profile, that we interpret such that each segment of a request should match the sought entity. Given a request and a candidate entity, the generic matching module computes a matching score resulting from the aggregation of several features of two kinds: attribute level features and entity level features. At all levels, feature scores are aggregated by a weighted log-based tempered geometric mean. The label and value similarities are standard string similarity metrics 3 from the SimMetrics library 4 . The entity boosting factor represents how popular an entity is and is computed based on inter-articles Wikipedia links. The attribute boosting factor is a combination of attribute selectivity and attribute popular-ity, determined through optimization methods by learning from human provided examples.

The algorithm has the following parameters: DSParams = { ( FieldName,FieldWeight ) } . For a given pair of entities p i,j , it outputs an Automatic Decision Score ADS i,j Automatic Decision of the algorithm can have two values, AD i,j = 1 if ADS i,j  X  threshold indicating duplicates, and AD i,j = 0 if ADS i,j &lt; threshold , for non duplicates
Crowdsourcing is revolutionizing how labeled data for ma-chine learning is acquired, with marketplaces like MTurk be-ing able to provide training data rapidly and at a low cost. Through MTurk, one ore more remote Internet users called workers solve tasks provided by people who need work com-pleted called requesters . The units of work are called HITs , an abbreviation of Human Intelligence Tasks, and can be done in five minutes or less, for a monetary reward. Each HIT has an associated number of assignments , the maxi-mum number of unique workers that can solve it.

In order to help us decide on the uncertain duplicates that have scores around the threshold , and to improve our au-tomatic methods we create MTurk micro tasks. One HIT consists of 5 publication pairs for which the workers have to assign the duplicate or not duplicate labels. Differences be-tween the publications can be displayed highlighted in differ-ent colors. Detailed instructions and examples are provided in order to help the workers understand and better solve the task. Each HIT pays 5ct., and intially has 3 assignments.
We are employing an active learning technique. The data is sent to the MTurk in batches consisting of pairs for which the automatic algorithm is most uncertain. With each solved batch, the algorithm learns the parameters that yield results as close as possible to those provided by the crowd. The algorithm thus improves; although there will always be some pairs on which it is uncertain, this number will decrease, and the need for crowd input diminishes.
 The general steps taken by our method are described in Alg. 1. In a continuous iterative process we build a candi-date set, let it be assessed by the crowd, learn better pa-rameters for the automatic method, and update the status of the pairs for which the crowd X  X  decision is strong. For the publication pairs on which we don X  X  have a strong or full agreement within the workers, we get more votes by extend-ing the number of assignments of the HIT.
Let W i,j ( k ) be the vote that worker w k casted on the pub-lication pair p i,j . Its values can be: W i,j ( k ) = 1 if worker w voted p i,j as duplicates pair and W i,j ( k ) = 0 otherwise. c represents the confidence associated to worker w k , indicat-ing how good he is in solving our tasks, c k  X  [0 , 1]. W { w k | w k voted on p i,j } and P k = { p i,j | w k voted on p
Respectively Needleman-Wunch and a combination of Lev-ensthein and Monge-Elkan http://www.dcs.shef.ac.uk/~sam/stringmetrics.html Algorithm 1 Learning to Deduplicate from the Crowds Input: The set of all of pairs P , the sample size size Output: The pairs of publications that are identified as 1: Choose the initial DSParams ; P cand =  X  2: loop 3: Using DSParams compute scores for all p i,j  X  P 4: Identify publications with an uncertain score 5: Take a sample of size pairs and put them in the P cand 6: Prepare a batch of micro tasks with pairs from P cand 7: Retrieve the crowdsourced work and compute the 8: Put the high confidence pairs in P train 9: Keep the pairs remaining uncertain for the next itera-10: Based on P train optimize the choice of parameters and 11: Identify the pairs that the algorithm deems as dupli-12: return Best parameters DSParams , threshold and 13: end loop
Each worker w k that voted on the pair p i,j has a different weight in the final decision, based on his confidence value
The weight of the high-confidence workers can be boosted over that of the low-confidence workers using e c k instead of c in the computation of weight boost i,j ( k ) in Eq. 1.
From MTurk we get tuples of the form ( p i,j ,w k ,W i,j ( k )) and we aggregate the votes of single workers into the Crowd Soft Decision CSD i,j , to get a single assignment for p i,j close as possible to reality that gives an indication on how much the workers agree, and how strong their decision is
CD i,j is the aggregated Crowd Decision, computed for p i,j using all the workers in W i,j , and can have two values CD i,j = 1 if CSD i,j  X  0 . 5 designating p i,j as duplicates, and CD i,j = 0 if CSD i,j &lt; 0 . 5, indicating non duplicates.
A simple metric for evaluating the worker confidence can be the proportion of correctly classified pairs, when com-pared to the crowd X  X  decision.
To compute the workers confidence we use an EM algo-rithm as proposed in [4]. The algorithm takes as input the work done by the workers W i,j ( k ) for all pairs and outputs the worker confidences c k and final decisions. We initialize c , e.g. all workers are considered equally good, with c k = 1. We then repeat two steps until c k reaches convergence or for a certain number of iterations: (1) compute the soft de-cisions CSD ( p i,j ) for all the available pairs p i,j the worker confidences using Eq. 2, and (2) update all the worker confidences c k using Eq. 3.

MTurk requesters can evaluate how good the workers un-derstood and are able to solve the task via a qualification test . We devised a qualification test where the workers had to deduplicate 7 pairs of moderate difficulty, as prerequisite for solving tasks from a batch of 60 HITs. This proved not to be a very fruitful approach, as not many workers took our HITs. To complete this batch of tasks the workers needed more than a month, compared to 1-2 days for the setting where there was no prerequisite.
Using the reputation system to put a lower weight on the contribution of the bad workers, the Crowdsourced work gets as close as possible to a real ground truth, and we can use it to maximize the accuracy of the automatic method. We learn the parameters DSParams and threshold , that pro-vide results as close as possible to those of the crowd. We use a choice multi-objective Evolutionary Algorithm (includ-ing SPEA2 and NSGA2) as implemented in the OPT4J [12] library. We optimize either: the accuracy (overlap) of Dupli-catesScorer decision when compared to the crowd X  X  decision, or the correlation between the crowd X  X  soft decision and the DuplicatesScorer X  X  score.

When optimizing the Accuracy( Accuracy ), we will find both the DSParams and the threshold that maximize
If we optimize the correlation between the crowd X  X  soft de-cision and the Duplicate Scorer X  X  decision, we want to com-pare CSD i,j with ADS i,j , and find those DSParams that yield the best correlation between the two, and then we seek the threshold that gives the highest Accuracy as defined in Eq. 4. The correlation can be measured as the Mean Abso-lute Error( MAE ), the sum of the log of errors( sum-log-err ), or the Pearson Correlation( Pearson ) of the two series. We want to minimize the Mean Absolute Error and the sum of the log of errors, and to maximize the Pearson Correlation Coefficient(PCC) when comparing the CSD i,j and ADS i,j .
In our experiments the entities we deduplicate are scien-tific publications. The list of fields that a publication has in our system is: Title, By, In, Type, Publisher, Organization, Abstract . Not all publications have all the fields. If all would contain an Abstract , our task would be trivial.

The posted HITs consist of 5 pairs having ADS i,j  X  X  0 . 7 , 0 . 8 } obtained with DSParams= { (Abstract, 0.5), (Title, 1.0), (Or-ganization, 0.5), (By, 1.0), (Type, 0.5), (In, 1.0), (Pub-lisher, 0.5) }} and threshold = 0 . 75. We posted one batch of 60 HITs having a qualification test as prerequisite, and two batches containing 60 HITs and 119 HITs without.
 We retrieved a total number of 1132 assignments from MTurk, corresponding to 239 HITs solved by 78 unique workers, with an average time per HIT of 90 seconds. The average time for solving a HIT for a worker was of 145 sec-onds. The average number of HITs solved by a single worker was 72. For obtaining a ground truth against which we can compare the accuracy of our algorithm we labeled a number of 450 pairs, needing on average 2 minutes per HIT.
Manually computing the worker confidence against the 450 pairs that compose our ground truth, reveals an average w k of 0.85 with a standard deviation of 0.14. In comparison, the automatically computed worker confidence on the same data has an average of 0.81 with a standard deviation of 0.18 and on the whole data set 0.81 and 0.16 respectively. The confidence in the worker does not strongly correlate with the average time he invests in solving the HIT (PPC = 0.177) or with the number of solved HITS (PCC = -0.19).
The Crowd Decision can be computed using different strate-gies, depending on how we compute the worker confidence:
CSD MV represents the soft decision in the case of major-ity voting, where all c k = 1.

CSD iter represents the soft decision computed by using the worker confidences computed using the EM algorithm.
CSD boost iter represents the decision computed by using the boosted weight in computing the soft decisions using the same algorithm. Workers with a higher confidence will have a bigger importance in the decision.

CSD manual is obtained by using manually computed worker confidence on our own assessed ground truth.

CD heur represents a heuristic that disregards the worker confidence in the final crowd decision. The pair will be re-garded as a duplicate if all the initial 3 workers agreed, or if after requesting 2 more votes, at least 4 out of 5 workers agree on the duplicate status.

We present the results obtained by using different ways of obtaining the final decisions, combined with different opti-mization strategies, in terms of accuracy (A), precision (P) and recall (R) in Table 1. We compare the case where 3 workers were used for each pair, or 5 workers. Differences in accuracy of more than 0.01 are statistically significant by means of a one-tail paired t-test with p &lt; 0 . 05.
The best automatic decision strategy is CSD boost iter highest performance is obtained in the case of optimization for methods penalizing high differences between worker de-cisions and algorithm output, like Pearson or Sum-log-err . CD heur uses the best data in terms of agreement between workers and achieves very good results. Nevertheless the re-sults are very close to the simple CSD MV strategy using 3 instead of 5 workers when optimizing for Accuracy . Simi-larly, optimizing for Accuracy yields very good results con-sistent across the different decision strategies.

In our optimization setting, strategies that approximate the worker X  X  real confidence, like CSD MV , CSD iter and CSD boost iter , perform as good as CSD manual using the con-fidence computed based on our own assessed ground truth.
Compared with the original preliminary parameters choice, the CD heur strategy, when optimizing for Accuracy we find the best parameters to be DSParams = { (Abstract ,0.17), (Title, 0.98), (Organization ,0.01), (By, 0.3), (Type, 0.19), (In, 0.05), (Publisher, 0.26) } and threshold = 0 . 72. It is surprising to see that the In and By fields have a very low weight, although one would expect the opposite.
We integrate the proposed approach in the online search system for publications  X  X reeSearch X  [7].

In order to offer the user a clean view of the results to a search query the common practice is to group the dupli-cates together and show just one version. The other versions are available via a link to  X  X ll versions X . The detection and grouping of duplicates is intially based on signatures (Bibli-ographic Hash Keys 5 : hashes of aggregated normalized ba-sic metadata like author, title, year, venue). The duplicate candidates detected using signatures are then filtered us-ing an automatic algorithm that uses the methods described above to learn. The users of the system have the posibility to give feedback concerning the wrongly assigned status of publication pairs. Human input, through our own users, and through Crowdsourcing helps us better detect duplicates.
In Fig. 1 we describe the workflow for resolving a query, and how the duplicates detection comes into place. The query is forwarded to the Search Engine component, the re-sults pass through the Duplicates Detection and Grouping component to produce a list of duplicate candidates where the results are grouped based on signatures. The duplicate candidates, are checked against the user feedback, or used for the Crowdsourcing component. The Duplicates Learner learns which pairs of documents are duplicates, and improves the Duplicate Detection and Grouping component. The list of candidates is processed with the automatic algorithm and the signatures are updated. Documents belonging to a pair detected as duplicate, will have the same signatures. The du-plicates are passed through the Duplicate Merger, and the merged representation of the group is displayed.
We implemented different duplicate detection strategies in an online scientific publication search system. Fig. 2 shows the performance of different methods. sign detects dupli-cates based only on the publication signature. DS/m uses the default manual weights and threshold for the Dupli-catesScorer while DS/o uses the optimized, learned weights using the simplest, most cost-effective strategy, CSD with 3 workers optimized for Accuracy . sign + DS/m and sign + DS/o are very computationally efficient combined methods: first they group duplicate candidates by signature (for efficiency) and then base the duplicate detection de-cision on DS/m or DS/o respectively (for best accuracy). CD-MV is simply the crowd decision out of 3 workers using majority voting the performance of humans given this task.
Automatically learning features for a general purpose du-plicate detection algorithm using a very simple and cost-effective approach increases accuracy from 0.70 ( DS/m ) to 0.79 ( DS/o ). Looking at CD-MV we see that even humans perform only 4% better, with an accuracy of 0.83. http://www.gbv.de/wikis/cls/Bibliographic_Hash_ Key Figure 2: Accuracy(A), precision(P) and recall(R) of different duplicate detection strategies
While integrated in the overall system, sign + DS/o shows a perfect precision at the cost of recall. The improvement between sign + DS/m and sign + DS/o is of 5% in precision with no loss in recall or accuracy.
When using Crowdsourcing platforms, a solid task de-scription is of utmost importance. Workers do not have the same background and can have very different understand-ings of the the task description. Only after having several iterations over the task description, discussed with persons from different backgrounds we could gather better results.
Using qualification tests is only useful for some type of tasks. For us, for a relatively simple task, it drove most workers away. We imagine it is more useful for tasks where workers have to exhibit specific skills; the test should be different than the actual HIT, testing only these skills.
By comparing cases where all workers disagreed with our own judgment (this happened in 4% of the cases) we found out that most problems arise from workers being too quick and not paying enough attention to the task. Workers seem to take a quick look only at the beginning of metadata fields and not pay attention to the entire content. In some cases we could decide better only because our background and experience, e.g. two almost identical publications, one being a technical report and another one a PhD thesis.
In this paper we propose a novel way to deduplicate enti-ties using Active Learning based on micro tasks performed on a Crowdsourcing platform. By learning to approximate human decisions, we increase the duplicate detection accu-racy up to 14%. In comparison, human assessors perform only 4% better than our automatic system. We then show how to include such a duplicate detection module in a fully functional online publication search system where the focus is both on precision and on throughput. While providing a very efficient solution, we achieve perfect precision without affecting recall. As future work we will use a similar ap-proach for merging the duplicates we detect. We learn how humans merge two entities by merging values from similar fields and create a method for doing entity merging auto-matically improving the algorithms X  performance.

