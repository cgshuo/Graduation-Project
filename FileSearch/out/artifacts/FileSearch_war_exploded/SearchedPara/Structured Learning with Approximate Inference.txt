 Structured prediction models commonly involve complex inference problems for which finding ex-practice can be restricted to those for which inference is feasible, such as conditional random fields on trees [2] or associative Markov networks with binary labels [3]. More generally, however, effi-cient but approximate inference procedures have been devised that apply to a wide range of models, including loopy belief propagation [4, 5], tree-reweighted message passing [6], and linear program-ming relaxations [7, 3], all of which give efficient approximate predictions for graphical models of arbitrary structure.
 Since some form of inference is the dominant subroutine for all structured learning algorithms, it is natural to see good approximate inference techniques as solutions to the problem of tractable learning as well. A number of authors have taken this approach, using inference approximations as drop-in replacements during training, often with empirical success [3, 8]. And yet there has been little theoretical analysis of the relationship between approximate inference and reliable learning. We demonstrate with two counterexamples that the characteristics of approximate inference algo-rithms relevant for learning can be distinct from those, such as approximation guarantees, that make them appropriate for prediction. First, we show that approximations can reduce the expressivity of a model, making previously simple concepts impossible to implement and hence to learn, even though inference meets an approximation guarantee. Second, we show that standard learning algo-rithms can be led astray by inexact inference, failing to find valid model parameters. It is therefore crucial to choose compatible inference and learning procedures. With these considerations in mind, we prove that LP-relaxation-based approximate inference proce-dures are compatible with the structured perceptron [9] as well as empirical risk minimization with a margin criterion using the PAC-Bayes framework [10, 11]. computation of the optimal labeling In a prediction setting, the goal of approximate inference is to compute efficiently a prediction with the highest possible score. However, in learning a tight relationship between the scoring model and true utility cannot be assumed; after all, learning seeks to find such a relationship. Instead, we assume a fixed loss function L ( y | x ) that measures the true cost of predicting y given x , a distribution D over inputs x , and a parameterized scoring model S  X  ( y | x ) with associated optimal labeling function h  X  and inference algorithm A  X  . Exact inference implies A  X  = h  X  . Learning seeks the risk minimizer: Successful learning, then, requires two things: the existence of  X  for which risk is suitably low, and the ability to find such  X  efficiently. In this work we consider the impact of approximate inference on both criteria. We model our examples as pairwise Markov random fields (MRFs) defined over a graph G = ( V,E ) with probabilistic scoring model  X  are probabilistic, we also refer to Viterbi inference as maximum a posteriori (MAP) inference. The existence of suitable model parameters  X  is captured by the standard notion of separability. Definition 1. A distribution D (which can be empirical) is separable with respect to a model S ( y | x ) and loss L ( y | x ) if there exists  X  such that E x  X  D [ L ( h  X  ( x ) , x )] = 0 1 . However, approximate inference may not be able to match exactly the separating hypothesis h  X  . We need a notion of separability that takes into account the (approximate) inference algorithm. Definition 2. A distribution D is algorithmically separable with respect to parameterized inference algorithm A  X  and loss L ( y | x ) if there exists  X  such that E x  X  D [ L ( A  X  ( x ) , x )] = 0 . While separability characterizes data distributions with respect to models, algorithmic separability characterizes data distributions with respect to inference algorithms. Note that algorithmic separa-bility is more general than standard separability for any decidable model, since we can design an with provable approximation guarantees can make separable problems algorithmically inseparable. 3.1 LP-relaxed inference Consider the simple Markov random field pictured in Figure 1, a triangle in which each node has as otherwise. Then the joint probability of a configuration y = ( y 1 ,y 2 ,y 3 ) is given by and the MAP labeling is arg max y h P i,j I ( y i = y j )  X  ij i .
 Note that this example is associative; that is, neighboring nodes are en-couraged to take identical labels (  X  ij &gt; 0 ). We can therefore perform approximate inference using a linear programming (LP) relaxation and get a multiplicative approximation guarantee [3]. We begin by writing an inte-ger program for computing the MAP labeling; below,  X  i ( y i ) indicates node i taking label y i (which ranges over the two allowed labels for node i ) and  X  ij ( y i ,y j ) indicates nodes i and j taking labels y i and y j , respectively. Integer programming is NP-hard, so we use an LP-relaxation by replacing the integrality constraint with  X   X  0 . Letting i  X  j  X  = arg max ij  X  ij , it is easy to see that the correct MAP configuration assigns matching labels to nodes i  X  and j  X  and an arbitrary label to the third. The score for this allowed labels in equal proportion X   X  = 1 / 2  X  X s optimal.
 inference for this MRF has a relatively good approximation ratio of 3 / 2 . 3.2 Learning with LP-relaxed inference Suppose now that we wish to learn to predict labelings y from instances of the MRF in Figure 1 weight vector w = ( w 12 ,w 23 ,w 31 ) , letting  X  ij = w ij x ij .
 rule. Furthermore, there is margin: any weight vector in a neighborhood of (1 , 1 , 1) assigns the highest probability to the correct labeling.
 Using LP-relaxed inference, however, the problem is impossible to learn. In order to correctly label the instance x = (4 , 3 , 3) we must have, at a minimum,  X  12 &gt;  X  23 , X  31 (equivalently 4 w 12 &gt; 3 w 23 , 3 w 31 ) since the 0-loss labeling must have higher objective score than any other labeling. Reasoning similarly for the remaining instances, any separating weight vector must satisfy 4 w ij &gt; feature vector x = (4 , 3 , 3) . Then, As a result, LP-relaxed inference predicts  X  = 1 / 2 . The data cannot be correctly labeled using an LP-relaxation with any choice of weight vector, and the example is therefore algorithmically inseparable. We cannot expect to learn without algorithmic separability; no amount of training can hope to be successful when there simply do not exist acceptable model parameters. Nevertheless, we could draw upon the usual techniques for dealing with (geometric) inseparability in this case. Approximate inference introduces another complication, however. Learning techniques exploit as-sumptions about the underlying model to search parameter space; the perceptron, for example, as-lead to better predictions. While this is formally true with respect to an underlying linear model, inexact inference methods can disturb and even invert such assumptions. 4.1 Loopy inference Loopy belief propagation (LBP) is a common approximate inference procedure in which max-product message passing, known to be exact for trees, is applied to arbitrary, cyclic graphical models [5]. While LBP is, of course, inexact, its behavior can be even more problematic for learning. Be-cause LBP does not respond to model parameters in the usual way, its predictions can lead a learner away from appropriate parameters even for algorithmically separable problems.
 Figure 2: An MRF on which LBP is inexact. We now investigate the behavior of LBP on this example. In general, max-product LBP on pairwise y ranges over the possible labels for node j and N ( i ) is the neighbor set of node i . to 1 and passed in parallel, symmetry also implies that messages are completely determined by the the types of the relevant nodes. The updates are then as follows. Note that messages m ij (  X  1) remain fixed at 1 after any number of updates. Messages m AB (1) , m
BA (1) , and m BB (1) always take the form exp( p X  + q X  ) for appropriate values of p and q , and it is easy to show by iterating the updates that, for all three messages, p and q go to  X  while the ratio q/p converges to  X   X  1 . 089339 . The label 1 messages, therefore, approach 0 when  X  +  X  X  &lt; 0 and algorithm converges in either case. Figure 3: A two-instance training set. Within each instance, nodes of the same shading share a feature vector, as annotated. Below each instance is its correct labeling.
 the prediction of LBP as y LBP = sign(  X  +  X  X  ) . Intuitively, then, LBP gives a slight preference to the B -type nodes because of their shared edge. If  X  and  X  are both positive or both negative, or if the preference exerted by LBP is significant enough to flip the labels. For example, if  X  = 1 and  X  =  X  0 . 95 , the true MAP configuration is 1 but LBP converges to  X  1 . 4.2 Learning with LBP Suppose now that we wish to use the perceptron algorithm with LBP inference to learn the two-instance data set shown in Figure 3. For each instance the unshaded nodes are annotated with a feature vector x  X  = ( x  X  1 ,x  X  2 ) and the shaded nodes are annotated with a feature vector x  X  = ( x  X  1 ,x  X  2 ) . We wish to learn weights w = ( w 1 ,w 2 ) , modeling node potentials as before with  X  = w  X  x  X  and  X  = w  X  x  X  . Assume that edge potentials remain fixed using a suitably positive  X  . By the previous analysis, the data are algorithmically separated by w  X  = (1 ,  X  1) . On instance (a),  X  = 1 ,  X  =  X  0 . 95 , and LBP correctly predicts  X  1 . Instance (b) is symmetric. Note that although the predicted configurations are not the true MAP labelings, they correctly match the training labels. The weight vector (1 ,  X  1) is therefore an ideal choice in the context of learning. The problem is also separated in the usual sense by the weight vector (  X  1 , 1) .
 Since we can think of the MAP decision problem as comput-ing sign(  X  +  X  ) = sign ( w  X  ( x  X  + x  X  )) , we can apply the perceptron algorithm with update w  X  w  X   X  y ( x  X  + x  X  ) , where  X  y is the sign of the proposed labeling. The standard perceptron mistake bound guarantees that separable problems require only a finite number of iterations with exact infer-ence to find a separating weight vector. Here, however, LBP causes the perceptron to diverge even though the problem is not only separable but also algorithmically separable.
 Figure 4 shows the path of the weight vector as it progresses from the origin over the first 20 iterations of the algorithm.
 During each pass through the data the weight vector is up-dated twice: once after mislabeling instance (a) ( w  X  w  X  (1 , 0 . 95) ), and again after mislabeling continually moves in the opposite direction of w  X  = (1 ,  X  1) , and learning diverges. 4.3 Discussion To understand why perceptron learning fails with LBP, it is instructive to visualize the feasible both constraints are feasible, as depicted in Figure 5(a). For LBP, the preference given to nodes 2 and 3 is effectively a scaling of x  X  by  X   X  1 . 089339 , so a feasible weight vector must satisfy Figure 5: The feasible regions of weight space for exact inference and LBP. Each numbered gray halfspace indicates the region in which the corresponding instance is correctly labeled; their inter-section is the feasible region, colored black. w different feasible region of weight space, shown in Figure 5(b). It is clear from the figures why perceptron does not succeed; it assumes that pushing weights into the feasible region of Figure 5(a) will produce correct labelings, while under LBP the exact opposite is required.
 Algorithmic separability, then, is necessary for learning but may not be sufficient. This does not imply that no algorithm can learn using LBP; a grid search on weight space, for example, will be slow but successful. Instead, care must be taken to ensure that learning and inference are appropri-ately matched. In particular, it is generally invalid to assume that an arbitrary choice of approximate inference will lead to useful results when the learning method expects exact feedback. In contrast to the failure of LBP in Section 4, appropriate pairs of inference and learning algorithms do exist. We give two bounds using LP-relaxed inference for MRFs with log-linear potentials. First, under the assumption of algorithmic separability, we show that the structured perceptron of Collins [9] makes only a finite number of mistakes. Second, we show using the PAC-Bayesian framework [11] that choosing model parameters to minimize a margin-based empirical risk function (assuming  X  X oft X  algorithmic separability) gives rise to a bound on the true risk. In both cases, the proofs are directly adapted from known results using the following characterization of LP-relaxation. Claim 1. Let z = ( z 1 ,...,z k ) be the vector of 0/1 optimization variables for an integer program box constraints 0  X  z i  X  1 yields an LP with a feasible polytope having vertices Z 0  X  X  . Proof . Each z  X  Z is integral and thus a vertex of the polytope defined by box constraints alone. The remaining constraints appear in P and by definition do not exclude any element of Z . The addition of constraints cannot eliminate a vertex without rendering it infeasible. Thus, Z  X  X  0 . We can encode the MAP inference problem for MRFs as an integer program over indicators z with optimal vertex always exists, LP-relaxed inference given an input x computes We can think of this as exact inference over an expanded set of labelings Z 0 ( x ) , some of which may y are always translated into corresponding indicator values z . 5.1 Perceptron suppose that there exists a weight vector w  X  with unit norm and  X  &gt; 0 such that, for all i , w  X   X  z  X  X  0 ( x i ) . Then the structured perceptron makes at most R 2 / X  2 mistakes. Proof sketch . Let w k be the weight vector before the k th mistake; w 1 = 0 . Following the proof of Collins without modification, we can show that k w k +1 k X  k X  . We now bound k w k +1 k in the other by the update rule, The inequality follows from the fact that LP-relaxed inference maximizes w  X   X ( x k , z ) over all the two bounds, k 2  X  2  X k w k +1 k 2  X  kR 2 , hence k  X  R 2 / X  2 . 5.2 PAC-Bayes The perceptron bound applies when data are perfectly algorithmically separable, but we might also hope to use LP-relaxed inference in the presence of noisy or otherwise almost-separable data. The following theorem adapts an empirical risk minimization bound using the PAC-Bayes framework to show that LP-relaxed inference can also be used to learn successfully in these cases. The measure of empirical risk for a weight vector w over a sample S = ( x 1 ,..., x m ) is defined as follows. Intuitively,  X  R accounts for the maximum loss of any z that is closer in score than in 1-norm to the LP prediction. Such z are considered  X  X onfusable X  at test time. The PAC-Bayesian setting requires version of the bound can also be proved.
 Theorem 2 (adapted from Theorem 3 in [11]) . Suppose that loss function L ( y | x ) is bounded be-every potential value of LP( x ) . Let ` = dim( z ) be the number of indicator variables in the LP, and let R bound the 2-norm of a feature vector for a single clique. Let Q ( w ) be a symmetric Gaussian centered at w as defined in [11]. Then with probability at least 1  X   X  over the choice of a sample S of size m from distribution D over inputs x , the following holds for all w .
 The proof in [11] can be directly adapted; the only significant changes are the use of Z 0 in place of the set Y of possible labelings and reasoning as above using the definition of LP-relaxed inference. A number of authors have applied inference approximations to a wide range of learning problems, sometimes with theoretical analysis of approximation quality and often with good empirical results [8, 12, 3]. However, none to our knowledge has investigated the theoretical relationship between approximation and learning performance. Daume et al. [13] developed a method for using a linear model to make decisions during a search-based approximate inference process. They showed that perceptron updates give rise to a mistake bound under the assumption that parameters leading to cor-rect decisions exist. Such results are analogous to those presented in Section 5 in that performance bounds follow from an (implicit) assumption of algorithmic separability.
 Wainright [14] proved that when approximate inference is required at test time due to computational constraints, using an inconsistent (approximate) estimator for learning can be beneficial. His result suggests that optimal performance is obtained when the methods used for training and testing are appropriately aligned, even if those methods are not independently optimal. In contrast, we consider learning algorithms that use identical inference for both training and testing, minimizing a gen-eral measure of empirical risk rather than maximizing data likelihood, and argue for compatibility between the learning method and inference process. form of approximate inference. They show that this method can outperform exact inference learning when algorithmic separability holds precisely because approximation reduces expressivity; i.e., less complex models require fewer samples to train accurately. When the data are not algorithmically separable, exact inference provides better performance if a large enough sample is available. It is interesting to note that both of our counterexamples involve strong edge potentials. These are precisely the kinds of examples that are difficult to learn using independent classifiers. Effective use of approximate inference for learning depends on two considerations that are irrele-vant for prediction. First, the expressivity of approximate inference, and consequently the bias for learning, can vary significantly from that of exact inference. Second, learning algorithms can mis-interpret feedback received from approximate inference methods, leading to poor results or even divergence. However, when algorithmic separability holds, the use of LP-relaxed inference with standard learning frameworks yields provably good results.
 Future work includes the investigation of alternate inference methods that, while potentially less suitable for prediction alone, give better feedback for learning. Conversely, learning methods that are tailored specifically to particular inference algorithms might show improved performance over those that assume exact inference. Finally, the notion of algorithmic separability and the ways in which it might relate (through approximation) to traditional separability deserve further study.
  X  We show that inference approximations (like LPs or loopy BP) can quality of approximation
