 We present a graph-theoretic approach to discover storylines from search results. Storylines are windows that offer glimpses into in-teresting themes latent among the top search results for a query; they are different from, and complementary to, clusters obtained through traditional approaches. Our framework is axiomatically developed and combinatorial in nature, based on generalizations of the maximum induced matching problem on bipartite graphs. The core algorithmic task involved is to mine for signature structures in a robust graph representation of the search results. We present a very fast algorithm for this task based on local search. Experi-ments show that the collection of storylines extracted through our algorithm offers a concise organization of the wealth of information hidden beyond the first page of search results.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.5 [ Information Storage and Retrieval ]: On-line Information Services X  Web based services ;G.2[ Discrete Math-ematics ]: Graph Theory X  Graph algorithms Algorithms, Experimentation, Measurement, Human Factors Link analysis, Communities, Clustering, Storylines, Search results
As the richness of content on the web grows, so does the social and economic significance of the web. Web searches are increas-ingly becoming the default starting point for consumer product pur-chases, term papers, vacation plans, curiosity-driven exploration of topics, etc. The role of the search engine as an entry point to the millions of interesting slices of the web is therefore more sharply Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. accentuated. A question that naturally arises is how best to utilize a browser X  X  screen in summarizing the thousands of web pages that mention the handful of terms a user types in to a search engine.
The present generation of search engines, beginning with Al-taVista up to the currently most popular Google, have taken the viewpoint of ranking the search results in a linear order, and pre-senting the top ten or so results on the first page, with pointers to the next ten, and so on. This straightforward approach has served us remarkably well, and it is a fair guess that more than half of all user queries are adequately handled by the top page in the search results. There are two main reasons for this: first, a good search engine is often capable of promoting to the top spot the best page relevant to the query, and secondly, most queries tend to have many highly relevant pages on the web, so just about any of them would serve well as the top result.

An interesting phenomenon occurs when one studies the top 100 pages for a query. Search engines routinely optimize the result set primarily for the top 10 positions; the pages listed in positions 11 X  100 (for example) share many interesting characteristics. For ex-ample, these pages may be viewed as good reflections of the qual-ity of the  X  X eb presence X  of the topic, as discovered by a good but mechanical ranking algorithm. Secondly, these results are usually relevant to the query, often contain valuable pieces of information, but are not necessarily the best pages on the topic. Finally, the rela-tive merits of these pages are not always obvious; for example, for the query  X  X ree sap car X  (to find out how to remove tree sap from automobiles), we find that the page ranked 11 isn X  X  particularly su-perior to the one ranked 49. A possible reason for the latter two phenomena is that search engines like Google employ global rank-ing mechanisms (eg., PageRank), and the top 100 results are just the most important places on the web where the query terms are mentioned.

To summarize, documents 10 X 100 in a typical search result are good sources of valuable pieces of information, usually from reli-able websites; what, if any, are the viewpoints on the query topic latent in these pages? Are they mere restatements of what is con-tained in the top 10 pages, or are they untapped sources of added value to the user?
As an example, consider the query  X  X on Kleinberg. X  A Univer-sity of Wisconsin page describing a colloquium talk by Kleinberg is perhaps not the most exciting result for this query, but it is a mean-ingful snippet of information from a snapshot of the web at some point in time. However, when one notices that the Wisconsin page is one among many announcements of Kleinberg X  X  talks at various places, what emerges is the analogue of what the news media con-siders  X  X n interesting storyline. X  The collective weight of evidence offered by a handful of pages in the top 100 suggests that this is an angle from which to summarize the web presence of the topic  X  X on Kleinberg X .

The analogy with a newspaper storyline is compelling. The col-lection of search results for a particular query may be thought of as the collection of facts, chronicles, thoughts and ideas that abound following a major news event. For example, after the Superbowl newspapers identify several storylines: the main news story about the championship game, one about outstanding contributions by key players, stories of unlikely heroes, key revenges/comebacks, travel and tourism tips about the city where the game takes place, the latest collection of Superbowl television commercials, half-time shows, etc. Newspapers have mastered the art of presenting these stories, arranging them on the (sports) front page to pique the read-ers X  interest, and directing them to the inner pages that contain most of the content.

In this paper, we study the problem of how to robustly formu-late what constitutes a good storyline within search results; we also address the question of how to mine the top 100 (or so) results to uncover the various angles from which the results may be summa-rized.

The starting point of our formulation is the observation that, much as in newspaper storylines, each storyline lurking within search results has its unique vocabulary. In the example  X  X on Kleinberg X  mentioned above, it is not hard to notice that most announcements of Kleinberg X  X  talks share the words  X  X bstract, X   X  X istinguished, X   X  X em-inar, X   X  X loan, X   X  X nvestigator, X   X  X lmaden, X  etc. This type of vocabulary is indeed rather unique to University talk announcements, where a fairly standard template is used (containing words like  X  X bstract X ,  X  X eminar X , etc.), and a brief biographical sketch of the speaker is in-cluded (hence the words  X  X loan X  and  X  X nvestigator X ). These words are also quite uncommon among the other top 50 pages for this query. Thus this rather small set of words serves as a signature that unify a collection of pages thematically.

This example also highlights some important aspects of discov-ering and presenting collections of thematically unified pages from among search results.

The first fact is that the notion of a storyline is based on local structure among a handful of documents; therefore, we do not ex-pect traditional clustering and classification approaches to identify these small focused collections. We will say more about this in Sec-tion 2.1. In contrast, more focused and combinatorial approaches that explicitly scour the results for  X  X ignature structures X  (such as a small set of terms that uniquely characterize a collection of doc-uments) are likely better suited for the task of finding hidden sto-rylines. The latter fact is an interesting twist in the development of ideas relevant to information organization and retrieval. Klein-berg X  X  HITS algorithm and its successors [10, 5] may be thought of as applying classification concepts (specifically, latent semantic indexing) to link analysis on the web; these ideas have had tremen-dous influence on web search technologies. Our proposal to use signature structures as the one outlined above may be viewed as an application of ideas from link analysis X  X pecifically the work of Kumar et al. [11] X  X ithin the domain of text analysis.

The example above also raises an intriguing question about how to present a storyline discovered from the search results. Several search engines suggest various possible  X  X uery refinements, X  in terms of additional terms to be included to the query terms, and al-low the user to choose one of them. We feel that it is not a good idea to offer the query refinement viewpoint to the user when summariz-ing an interesting collection of documents. It is probably somewhat the most popular sporting event in the U.S. puzzling to a typical web user to be shown terms like the  X  X bstract X  and  X  X loan X  as possible ways to refine the query  X  X on Kleinberg. X  Rather, a simple list of (titles of) pages that are considered to be a group might serve as a better way to indicate to the user what the collection is about. To do this, however, it is very important to have robust algorithms to discover the collection, so that the titles and snippets of the web pages automatically convey to the user how these pages are related.

The idea of mining the top 50 or 100 results may also be viewed as a method to rerank the best documents for a given query. Cur-rently, several heuristics exist (homepage detection, hub/faq identi-fication, etc.) that are aimed at improving the quality of the top few results. Our work suggests to mine the characteristics and structure in the top 100 pages as a reranking mechanism that will produce a more complete summary of the slice of the web pertaining to the query topic.
 Technical contributions.

We first outline a formulation of the problem of reorganizing search results with the intent of highlighting the important story-lines. To do this, we develop a semi-axiomatic approach, where we list certain desiderata that we would like our algorithms to sat-isfy. The compilation of the desiderata is motivated by fairly nat-ural requirements, and the goal of establishing these is to cast the mining problem as a combinatorial optimization problem. Roughly speaking, given the term X  X ocument relation, we wish to find many respectively, of documents and terms, such that most terms in T occur in most documents in D i , and very few terms in T i the documents in D j ,for j = i . Each ( D , T ) pair in the collection will correspond to a storyline.

We show that natural formulations of this problem are NP-complete, via a reduction from the maximum induced bipartite matching prob-lem introduced by Stockmeyer and Vazirani [12]. Nevertheless, we show that algorithms based on local search and dynamic program-ming yield excellent solutions in practice. Specifically, we design two algorithms for this problem, one based on a generate-and-prune approach employing local search, and one based on dynamic pro-gramming ideas, growing a collection of partial storylines that are best with respect to some cost function. Both algorithms take ad-vantage of a pre-processing step, where we present a robust method to identify the collection of terms in a document that are relevant to a set of given query terms.

Finally, we present a number of experimental results highlight-ing the hidden storylines uncovered by our algorithm, and based on these, present some thoughts about how best to integrate the sto-rylines discovered with the rank-ordering produced by the search engine.
In this section we describe some of previous work in areas that are most relevant to this paper.
 Link analysis.

The first body of related work is in the area of search algorithms that try to exploit hyperlink information. Link analysis algorithms in the context of web search, starting with the HITS algorithm [10] and its subsequent enhancements [1, 5], and the PageRank algorithm [2], have been the hallmark of many commercially suc-cessful search engines. (For a detailed account of link analysis in web search algorithms, see [14].) The Google search engine ( google.com ) is based on the PageRank algorithm and ideas be-hind the Teoma search engine ( teoma.com ) are inspired by the HITS algorithm. Clustering search results.

The next is in the area of clustering, especially applied to clus-tering web search results. One of the early works on clustering web search results was done by Zamir and Etzioni [15] (see also [16]); their technique was to extract phrases from the search re-sult snippets and to identify phrases that are common to groups of documents. The concepts of result set clustering and post-retrieval document clustering have been studied in the information retrieval community (cf. [6]); traditionally, they have been used to cluster documents in the result set based on the degree of relevance, to filter out irrelevant documents, and to support context-based relevance feedback. Clustering result sets has also been studied in the infor-mation visualization community where the goal is to present the search results to the user in the best possible way. Latent semantic indexing LSI [7] and other spectral methods (e.g., [9]) are popu-lar techniques to cluster especially large collection of documents. Our work differs fundamentally from the body of work on cluster-ing, since our framework attempts to simultaneously find groups of documents and terms that can be mutually characterized by each other. We will say more about the differences in Section 2.1.
Commercial and experimental search engines have been inter-ested in the problem of clustering web search results. The now-defunct Manjara and its current improved incarnation called Eigen-Cluster ( www-math.mit.edu/cluster ) provides a clustering of search engine results; the implementation is based on a spec-tral clustering algorithm [9]. Vivisimo ( vivisimo.com )offersa document clustering product that is an overlay to a search engine and can organize search results on the fly. Teoma ( teoma.com ) organizes search results into communities and presents them to the user; the user has the option of refining his/her search using the keywords presented for each community. Google has a clustering agent called Google Sets ( labs.google.com/sets ). Wisenet ( wisenut.com ), AlltheWeb ( www.alltheweb.com ), and many other search engines offer clustering information on top of search results. For a comprehensive account of clustering search engine results, see the article by Calishain [4].
 Trawling.

As we mentioned earlier, our problem can be related to that of trawling [11]. Trawling is a process to automatically enumerate communities from a crawl of the web, where a community is de-fined to be a dense bipartite subgraph. One way to trawl the web is to look for bipartite cliques. An algorithm to accomplish this enu-meration (in especially massive graphs) was described in [11]. Our formulation is inspired by the notion of communities that was de-fined in trawling. However, we cannot use the trawling algorithm per se in our case since our notion of cliques is more general, and the degree characteristics of our underlying graph make it hard to apply the combinatorial algorithm of [11] for our instances. On the other hand, our graphs are relatively smaller and fit entirely in main memory, so our set of tools are more extensive (and include local search and dynamic programming).
 Re-ranking clusters.

The storylines uncovered by our algorithm leads to the following combinatorial re-ranking question. Suppose there is a linear order-ing of elements of a universe and the goal is to rank given subsets of the universe, where the ranking should satisfy some basic axioms. In our setting, the universe corresponds to the top 100 results, the subsets correspond to the collections of webpages found by our al-gorithm, and the question is how to re-rank the clusters, taking into account the ranking of the web pages themselves. This problem has been considered before; Cai [3] shows that under reasonable axioms, ranking functions do not exist for this problem.
In this section, we describe a mathematical formulation of the storyline extraction problem.

Let Q denote a query (a set of terms). Let D denote the set of documents returned by the search engine for the query Q .Fora document d ,let T ( d ) denote the multiset of terms in d ,andlet denote the union, over all d  X  D ,ofthesets T ( d ) . Similarly, for a term t ,let D ( t ) denote the set of all documents that contain the term t .Let G =( D , T , E ) denote the bipartite graph, where the LHS set consists of one vertex per document and the RHS set T consists of one vertex per term, and the edge relation E  X  D  X  T consists of pairs ( d , t ) where document d contains term t .

Informally, our notion of a storyline consists of a set D of doc-uments and a set T of terms that more or less pinpoint each other, that is, a document d contains most of the terms in T if and only if it belongs to D , and similarly, a term occurs in most of the doc-uments in D and rarely in the others if and only if it belongs to the set T .

As an example, consider the query  X  X ndira Gandhi X  2 ;theterms  X  X awaharlal, X   X  X ssassination, X   X  X terilization, X   X  X ahadur X  gether in almost all, and only in, pages that contain her biography. Thus if we knew that a document d from the result set contains all these terms, we can immediately conclude that it is one of the bio-graphical pages, as opposed to the numerous other top 50 (and top 10) pages about the various institutions named after her. In fact, if we simply looked at the documents in D that contain the term  X  X terilization, X  they all turn out to be biographical pages of Indira Gandhi; however, to robustly characterize a group of pages as the-matically unified, and to do it with a degree of confidence, it helps to find a collection of terms all of which pick essentially the same set of 5 X 10 documents from the top 50 results. This also under-scores our earlier observation that the set of documents that form a storyline tends to share a vocabulary that sets it apart from the rest of the documents. See Figure 2 for an example of signature of storylines in a graph.
 M
Let k , l be two positive integers. Let  X  ,  X  be two constants such that 0  X   X  &lt;  X   X  1. Given a bipartite graph G =( D , T sequence of pairs of sets (called storylines ) ( D 1 , T 1 ) , ( D 2 , T 2 ) ,..., ( D s , T s ) , where D i  X  D , T i  X  T . a former Prime Minister of India for the colorful details of these terms, the reader is invited to read Indira Gandhi X  X  biography (0) Large disjoint subsets: for each i , | D i | X  k , | T D  X  D j = / 0 and T i  X  T j = / 0 ; (1a) Every document in a storyline consists of most of the terms that define the storyline: for each i , for each d  X  D i  X  | T i | ; (1b) Every term in a storyline appears in most of the documents that make up the storyline: for each i , for each t  X  T i  X  | D i | ; (2a) No term is popular in documents in storylines other than the one it defines: for i = i , for each t  X  T i , | D ( t )  X  D (2b) No document contains too many terms that define the other storylines: for each i = i , for each d  X  D i , | T ( d )  X  T (3) Many storylines: s is as large as possible.

When k , l ,  X  ,  X  are fixed, we refer to the resulting problem as the ( k , l ,  X  ,  X  ) -STORYLINE problem.

THEOREM 1. For any integers k , l &gt; 0 and any  X  ,  X  such that 0  X   X  &lt;  X   X  1 ,the ( k , l ,  X  ,  X  ) -STORYLINE problem is NP-hard, and the corresponding decision problem of whether there are at least s pairs of sets is NP-complete.

P ROOF . Stockmeyer and Vazirani [12] showed that the max-imum induced bipartite matching (MIBM) is NP-hard. Equiva-lently, they showed that the following problem is NP-complete: given a bipartite graph H =( U , V , F ) (where F  X  U  X  V )andanin-teger r , the question is whether there are subsets U  X  U and V  X  V such that | U | = | V | X  r , and the induced subgraph on U  X  V (that is a perfect matching 4 .
 We reduce MIBM to the ( k , l ,  X  ,  X  ) -storyline problem as follows. Suppose we are given an instance H =( U , V , F ) of the maximum induced bipartite matching problem. Define the bipartite graph G =( D , T , E ) as follows: Each vertex u  X  U will define k vertices u ,..., u k in D ; each vertex v  X  V will define l vertices v in T .If ( u , v )  X  F , then we connect u 1 ,..., u k to v b =  X  l .If ( u , v )  X  F , then no edge exists between any of the u and any of the v j  X  X .

Notation. We will refer to the u j  X  X  and the v j  X  X  as copies ,re-spectively, of u and v ; conversely, we will call u and v the parents , respectively, of the u j  X  X  and the v j  X  X . Finally, we will refer to two children of the same parent as siblings.

We claim that H has an induced bipartite matching of size s if there are subsets U  X  U and V  X  V such that | U | = | V | X  s and the induced graph on ( U , V ) is a perfect matching, then each pair ( u , v )  X  U  X  V yields a storyline ( D u , T v ) ,where D
Conversely, suppose we are given s -many ( k , l ,  X  ,  X  ) -storylines ( D 1 , T 1 ) ,..., ( D s , T s ) in G . We will construct an induced perfect matching of size s in H .Let U = i D i and V = i T i . We will  X  X ro-that wasn X  X  extracted while processing ( D 1 , T 1 ) ,..., ( D
For each i , we claim that D i consists only of vertices whose par-ents are different from the parents of all the vertices in D Suppose not. Then there is a vertex u j  X  D i that is a sibling of some vertex u j  X  D i , i &lt; i . However, the neighbors of u so does u j , contradicting requirement (2b) in the definition of a ( k , l ,  X  ,  X  ) -storyline. Thus, the set ( U , V ) is said to be a perfect matching in H =( U , V , F ) if | U | = | V | and for each u  X  U , there is exactly one v  X  V such that ( u , v )  X  F { u  X  U | u j  X  D i for some j } consists of vertices that are not in { u  X  U | u j  X  D i for some j and some i &lt; i } .
 Let u denote one such vertex, and let u j  X  D i be the copy of u in D .Let v h  X  T i denote the vertex that has the most neighbors in D ; since the average number of D i -neighbors of vertices in T  X  k , it follows that the degree if v h in D i is at least  X  k .Let u D i be one of the neighbors of v h . Clearly, since ( u must be that ( u , v )  X  F . Thus it remains to show that v has not already been matched by one of the edges we picked in rounds 1 ,..., i  X  1. Suppose to the contrary that we had picked in round it follows that the degree of v h in D i is at least  X  k . By definition of the bipartite graph, we know that if v h and v h both have non-zero degree, then their neighborhood is identical, therefore v the definition of a ( k , l ,  X  ,  X  ) -storyline.
In this section, we will briefly discuss why our formulation above is substantially different from what may be considered traditional methods in clustering. The field of clustering is a mature math-ematical discipline, with a wide variety of well-known and well-understood methods that have been analyzed extensively from var-ious viewpoints; therefore, it is hard to precisely name a handful of methods as the traditional ones. Nevertheless, we will note that many of the methods that exist in the literature have some features in common. Namely, in a large class of methods, the points to be clustered are points in an ambient metric space, so there is a natural notion of distance between points. More generally, there are graph-based clustering methods where one is given a number of points together with pairwise distances (in the metric case, the pairwise distances obey the triangle inequality). The clustering problem in these settings is to pick several groups of vertices that satisfy some specified criteria (minimum/maximum constraints on the distances between intra-and inter-cluster points, cluster separations, etc.).
In another class of methods, especially ones based on eigenvec-tor methods, instead of pairwise distances, one starts with pair-wise similarities (often they are interchangeable, especially in inner product spaces, as is commonly the case in scenarios where eigen-vector methods are employed).

A more unifying viewpoint of distance-and similarity-based clus-tering methods is that they are based on an underlying binary func-tion between the given set of points. In our formulation outlined in Section 2, note that this is not the case. We do not attempt to cluster the documents based on term-induced binary distance or similarity measures; similarly we do not attempt to cluster the term space based on document-induced binary measures. Rather, we attempt to identify as many document set/term set pairs with specified oc-currence patterns among them. Thus we simultaneously divide the document-and term-space into groups with desirable characteris-tics. The characteristics we employ are substantially more holistic than can be captured via any binary measure. This is especially clear, considering the fact that the documents we cluster are the search results for specific query terms; therefore they are likely to contain X  X t the macro level X  X uch the same broad set of terms, and hence will be deemed  X  X imilar X  or  X  X lose X  under any fairly nat-ural binary measure. It is, in fact, the common existence of a fairly large set of  X  X ignature X  terms that sets a storyline apart from others. The reader may recall the example of the query  X  X on Kleinberg X  mentioned in the Introduction. Here, most of the documents re-called by a search engine have similar vocabulary at a global level; nevertheless, there are clear cues such as the set of terms {  X  X b-stract, X   X  X eminar, X   X  X loan, X   X  X efreshments X  } , which pinpoints web pages on Kleinberg X  X  talks, the set {  X  X cientist, X   X  X nvented, X   X  X na-lyzed, X   X  X ITS, X   X  X oogle X  } , which pinpoint articles in blogs and in the popular media about Kleinberg X  X  work, and so on. There is no similarity or distance that can be defined apriori that can capture instance-dependent clusters of this kind.

There is some work in the clustering literature that calls for sep-arate discussion. Specifically, the concept of co-clustering is espe-cially relevant in our context; applied to our setting, co-clustering treats the incidence matrix of the document X  X erm relation as spec-ifying a joint probability distribution (of two random variables D cluster a document (resp. term) belongs to, such that the mutual information between  X  D and  X  T is as close to the mutual information between D and T as possible. (See [8] for an excellent overview, and related literature, especially the information bottleneck method of Tishby et al. [13], a one-sided precursor to co-clustering.) While there are superficial similarites (specifically, simultaneous group-ing of documents and terms), there are several differences between co-clustering and our method. Our formulation is explicitly com-binatorial, and we do not require that all documents or terms be placed into clusters (esp. the pairs that have a high probability mass on them, which are important in maximizing mutual information). We do not seek to  X  X xplain X  the large scale characteristics of the document X  X erm relation; rather, our goal is to identify explicit (and often, quite small) signature structures that point to some underly-ing semantic structure in the relation.
 Finally, let us say a few words comparing our method to that of Zamir and Etzioni [15], who presented an efficient clustering al-gorithm for documents that works roughly as follows. First one (implicitly) creates a list of all (fairly short) trailing subsequences of sentences from the documents; next, one compiles, for each several of these sets are collapsed based on overlap (via a single-link clustering method). Thus, implicitly, their algorithm clusters documents without expressly using any binary measure of similar-ity or distance among the documents. Nevertheless, one can de-fine a graph on the set of documents based on how many com-mon term sentence-suffixes two documents have, and then define a binary measure of similarity based on the intersection of neigh-borhoods of two documents in this graph. The resulting clustering is roughly what one obtains by identifying dense neighborhoods in this graph of low diameter. Another difference between our method and theirs is that, since their algorithm was based on the suffix tree data structure, they did not consider arbitrary subsets of terms (be-yond suffixes of sentences). In our formulation (and certainly as the examples illustrate), the small set of terms that define a story-line often need not occur within the same sentence in the document; furthermore, algorithmically we face a more challenging problem (NP-hard), hence our recourse to local search and dynamic pro-gramming like methods.
In this section, we outline the pre-processing steps that we em-ploy in creating the document X  X erm bipartite relation to which we will apply the algorithms of Section 3.2 and 3.3. The main goal of the steps outlined here are to identify, given a collection D of documents and a (small) set Q of query terms, the terms from each d  X  D that are, in some sense, most relevant to the terms in Q . Given a document d ,wedefineagraph G d whose vertex set is T ( d ) , (excluding a standard list of common stopwords), and where an edge is present between terms t , t iff t and t occur together within a sentence. For the purpose of this step, a sentence is any textual unit with a natural semantics, e.g., in the context of HTML pages, anchortext, titles, etc., qualify as sentences. Once G constructed, we perform connectivity analysis on G d and discard the small connected components. This has the desirable effect of automatically eliminating all irrelevant noise present in web docu-ments, such as text from templates, sidebars, advertisement links, etc., leaving us with a very accurate semantic summary of the doc-ument. (We could enhance the robustness of this structure further, for example, by eliminating all edges between terms that co-occur only once.)
Once the significant connected components are identified in G we conduct a breadth-first traversal on these components, starting at the vertices that correspond to terms in Q , and continue the traver-sal until we have collected some pre-determined number of terms (say 50 X 100). If none of the query terms occurs in the document, we simply start the traversal from the vertex of maximum degree. Thus, the terms we collect are at a short semantic distance from the query terms within the document, that is, they co-occur with one of the query terms, or co-occur with terms that co-occur with the query terms, etc.

Note that with this focused term collection method, the resulting document X  X erm bipartite graph is kept rather sparse. This implies that with a reasonable bit of engineering, the scheme is fairly prac-tical, since the amount of data per document is roughly the same amount of data that search engines routinely serve as  X  X nippets X  along with each search result (highlighting where in the document the query terms appear). In fact, our storyline extraction algorithms can even be run as a client-side computation, upon request by the user.
For subsets D  X  D , T  X  T ,let D = D \ D and let T = T \ T .If D is a subset of documents and T is a subset of terms, let E ( D , T ) denote the set of edges in the subgraph induced by the vertex sets D and T .

We now describe a simple heuristic to iteratively find storylines in the processed graph. Each step of the iteration consists of first identifying a dense bipartite subgraph of a specific size, next apply-ing a local resize procedure that possibly alters the size of the sub-graph, and finally applying a local swap procedure to improve the quality of the storyline. At the end of the step, the documents and terms corresponding to the storyline are removed from the graph completely and the iteration is repeated. (1) We now outline the method based on local search to iden-tify dense bipartite subgraphs of a specified size in the graph G . Recall that a similar goal was formulated in the context of web trawling [11]. In trawling, the graph consists of hundreds of mil-lions of nodes and identifying a dense bipartite subgraph is quite formidable, even in a heuristic sense. The problem was addressed by first identifying complete bipartite subgraphs (cores) of dense bipartite subgraphs, whose existence in many cases was guaran-teed by a theorem in extremal graph theory, and then expanding the cores to dense bipartite subgraphs. In our case, however, the situation is different. Our graphs only have a few thousand nodes and edges. Given the medium size of our graph, looking only for cores in this graph is quite restrictive as not all dense bipartite sub-graphs will contain cores. Coupled with the fact that we can hold our entire graph in memory, we can aim for a heuristic algorithm that finds dense bipartite subgraphs directly.
We now describe a simple local search heuristic for the densest bipartite subgraph of size k  X  . Suppose ( D , T ) , | D | = k , | T | = is the current solution (how to get a starting solution will be ex-plained shortly). We apply the following procedure which consists of several local swaps: Repeat until there are no more changes to D and T :
If  X  d  X  D , d  X  D such that | E ( D  X  X  d }\{ d } , T ) | &gt; | E ( D , T ) | ,
If  X  t  X  T , t  X  T such that | E ( D , T  X  X  t }\{ t } ) | &gt; | E ( D , T ) | ,
First of all, it is easy to see that the procedure is guaranteed to converge and find a local maximum, since all the quantities are finite. Secondly, the size of the subgraph is preserved across each local swap.

To arrive at an initial solution ( D , T ) , we adopt one of the fol-lowing two strategies. The first is a greedy one: start with an empty graph and keep adding nodes till it is of size k  X  ; at each step, pick a node which will contribute the greatest to the density. The second is a random one: pick k documents as D and terms as T .

The output of this step is the densest subgraph that results from applying local swaps to greedy/random starting solutions. (2) We apply the following local resize step to allow the story-lines to grow or shrink beyond the original size of k  X  . Suppose the current storyline is ( D , T ) . We apply the following procedure: Repeat until there are no more changes to D and T or too many changes have occurred:
If  X  d  X  D such that | E ( { d } , T ) | X  ( 2 / 3 ) | T | ,then
If  X  d  X  D such that | E ( { d } , T ) | X  ( 1 / 3 ) | T | ,then
If  X  t  X  T such that | E ( D , { t } ) | X  ( 2 / 3 ) | D | ,then
If  X  t  X  T such that | E ( D , { t } ) | X  ( 1 / 3 ) | D | ,then (3) In this step, we use local swap once again, but this is to im-prove the quality of the storylines as prescribed by our formulation, rather than just optimize the density of the induced subgraph. Local swap can be used in conjunction with any or all of quality measures that are described below.

Suppose ( D , T ) is a storyline. Then, we define the following measures of quality:
Q 3 ( D , T )= min Note that Q 1 captures how dense (on average) the storyline is; Q 3 is an extremal version of Q 1 where we focus on the minimum induced degree of the documents in D . Thus, a storyline satisfying criteria (1a) and (1b) in our formulation would have high values for these quantities. Likewise, Q 2 and Q 4 capture how well (on the average/in the worst case) the storyline satisfies criteria (2a) and (2b) in the formulation. A good storyline satisfying our criteria would have low values for these quantities.

It is easy to see that the above algorithm is very simple and can be implemented in an efficient manner.
Our next algorithm is based on ideas underlying dynamic pro-gramming. Let s denote some integer parameter, say 100. We be-gin with s arbitrary storylines ( D 1 , T 1 ) ,..., ( D s has exactly one term (chosen either greedily by min-degree, or ran-domly) and D i consists of all documents that contain the unique term in T i . Then we visit each term t  X  T in turn, and extend each storyline ( D , T ) by adding t to T (if it is not already present in T ). Thisgivesusupto2 s distinct storylines (even though the document sets could be identical for many of the storylines). We evaluate each storyline ( D , T ) with respect to various measures, including | D | , | T | , | D || T | , Q 3 ( D , T ) , Q 4 ( D , T ) .Let c tions such that For each storyline ( D , T ) , define the total cost by The cost function c 1 is chosen so that it is lowest when | D | and | T | are roughly k and l , respectively, where k and l are from the problem formulation (in practice, k and l are approximately 5), and becomes close to 1 if either | D | or | T | is too small or too large, or if the product | D || T | is too small or too large. The cost function c is typically chosen to be some constant multiple of Q 3 ,and c chosen to be some constant multiple of Q 4 .

Once the costs are computed forall the (up to) 2 s storylines we have, we sort them based on the costs, and retain the s storylines of lowest cost (after eliminating some of the storylines whose under-lying document sets are duplicates of the document sets of many other storylines). This done, we proceed to the next term in ter we have processed all terms in T ,wehaveupto s storylines, and we output the ones with total cost below some pre-specified threshold.

Note that having three cost functions allows us to vary the degree to which each of the quantities | D | , | T | , Q 3 ( D , T ) ,and Q influences the total cost, and hence the quality, of the storylines discovered. For example, if we simply optimize on | D || T | , ignoring the influence of Q 3 and Q 4 , we will end up with the entire collection of documents and terms; if we focus entirely on Q 3 , any one edge is sufficient; if we focus on | D || T | and Q 3 , we will find large (but possibly unbalanced) and dense subgraphs (e.g., the most popular terms, or the most dense documents), etc; if we focus entirely on Q , together with | D | , | T | , | D || T | , but ignoring Q subgraphs that may not be dense but whose terms are rare outside the subgraph. By carefully balancing these parameters, we have the ability to produce storylines that are dense within and sparse without, satisfying our goals.

Notice that, similar to dynamic programming, this method al-lows us a compact implementation with an s  X | T | matrix, where in cell ( i , t ) , we store whether term t was included in the i -th best storyline, the cost of this storyline, and a pointer to the storyline that includes (besides the initial terms) terms up to the predecessor of t .

Once the best storylines are identified at the end of the pass through all the terms, we remove the corresponding terms, and re-peat the process until no more new storylines are extracted.
We now present some highlights of the storylines uncovered by our algorithm for various queries. Our experiment consisted of 205 queries (that were extracted from the Lycos ( lycos.com ) weekly top queries from 2003 and from a locally available query log). We retrieved the top 100 documents for each query using Vivisimo ( vivisimo.com ). On average, the graph correspond-ing to each query had 98 document nodes, 3304 term nodes, and 6747 document X  X erm edges.

We ran the local search and dynamic programming based algo-rithms for each of the query. On average, the algorithms took un-der nine seconds (the implementation of our local search was fairly naive X  X ne could use sophisticated data structures to considerably speed up the local search). The average number of storylines found was 10.7 for each query and an average storyline had around 6.4 documents and 9.9 terms. The average number of edges in the in-duced subgraph of the storylines is 41.7; these numbers indicate that the storylines extracted by our algorithms are highly dense sub-graphs. The largest storyline for each query had, on average, 10.7 documents, 28.5 terms, and 154.9 edges. This indicates that the largest storyline for each query is not as dense as an average story-line, suggesting that our algorithm is not biased towards the size of the storylines.

Since it is very difficult to evaluate the  X  X uality X  of storylines as perceived by a human user, or to compare their structure with that of results from (one of numerous) clustering methods, we will present some statistics, and some case studies of our algorithms.
We evaluate the four quality measures for the queries. The aver-age values and variances of thes measures are  X  ( Q 1 )= 0 . 536 ,  X  ( Q 0 . 132;  X  ( Q 2 )= 0 . 059 ,  X  ( Q 2 )= 0 . 0280;  X  ( Q 3 measures Q 3 and Q 4 are less closely concentrated around the mean than the other two X  X his is not surprising, since these are  X  X orst case X  measures (based on min/max rather than average). Based on visual examination of many of the results, we notice that a low value of Q 4 and a large gap between Q 3 and Q 4 always seems to indicate a storyline of high quality. This also yields a natural way to rank the storylines, and also to threshold them to ensure very high quality.
We present some case studies of the storylines uncovered by our algorithms for various queries. To avoid clutter, we will pro-vide only a subset of the terms, documents (and titles) for some of the prominent storylines. For each document, we also provide the ranks of the documents in the original results. (1) The first query is  X  X lu Epidemic. X  In Table 1, we see that the first storyline contains information about flu (identified by terms like  X  X accines X ,  X  X trains X ), the second contains seasonal news (iden-tified by terms like  X  X eaths X ,  X  X eported X ), the third is about bird flu Spanish flu epidemic from 1918 (identified by terms like  X  X panish X ,  X 1918 X ). (2) The second query is  X  X on Kleinberg. X  In Table 2, we see that the first storyline contains information about lectures on some as-pect of his research (identified by terms like  X  X etworked X ,  X  X orld X ), the second contains bibliographic information (identified by terms from titles of his publications), the third storyline is about Klein-berg X  X  IBM Almaden connections. (3) The third query is  X  X hailand Tourism. X  In Table 3, we see that the first storyline contains information about travel information on Thailand (keywords are  X  X kytrain X ,  X  X eservations X ) and the second storyline has an economic flavor to it (keywords like  X  X ndustries X ,  X  X otential X ). (4) The fourth query is  X  X rench schools religious head ban. X  In Table 4, the first storyline consists news articles on the broad topic (referring to various religious groups, eg., Sikhs), the second sto-
Flu Epidemic [7 17], 89; terms = vaccines strains infection viruses . . . 30. MSNBC -The genetic genesis of a killer flu http://www.msnbc.com/news/624982.asp?cp1=1 37. Flu Center http://www.bcm.tmc.edu/pa/flucenter.htm 70. Flu Shot did not stop the flu epidemic http://suewidemark.netfirms.com/flushots.htm [24 6], 86; terms = deaths spread reported united . . . 3. Sheboygan-Press: Sheboygan couldnt escape flu . . . http://www.wisinfo.com/sheboyganpress/news 4. Bird flu epidemic spreads to Pakistan as death . . . http://www.guardian.co.uk/international 5. CDC says flu epidemic appears to be waning http://www.duluthsuperior.com/mld [6 7], 26; terms = bird department studies avian . . . 9. Poultry vaccine might worsen flu epidemic http://www.rense.com/general49/poul.htm 53. Portrait of a probable killer: Viral double act . . . http://www.nature.com/nsu/030324 [5 7], 20; terms = spanish medical 1918 pandemic . . . 42. The Flu Epidemic of 1918 http://www.viahealth.org/archives 57. The American Experience  X  Influence 1918 http://www.pbs.org/wgbh/amex/influenza/
Jon Kleinberg [5 8], 36; terms = models world networked properties . . . 11. WebShop 2002 Abstract: Jon Kleinberg http://www.webuse.umd.edu/abstracts2002/ 22. Jon Kleinberg http://www.cs.rochester.edu/seminars/ 37. events: CIS Distinguished Lecture Series: Jon Kleinberg http://dp.seas.upenn.edu/news-20011113-http://robotics.stanford.edu/  X cs528 [5 28], 122; terms = approximation neighbor server flow . . . 1. Jon Kleinberg X  X  Homepage http://www.cs.cornell.edu/home/kleinber/ 3. DBLP: Jon M. Kleinberg http://www.informatik.uni-trier.de/  X ley/db 12. Jon Kleinberg http://www.cs.cornell.edu/annual_report/ 17. DiSC -Jon M. Kleinberg http://www.sigmod.org/sigmod/ [6 7], 26; terms = ibm center prabhakar almaden . . . 13. kleinberg http://www.cs.cornell.edu/faculty/ 35. Trawling emerging cyber-communities automatically http://www8.org/w8-papers/4a-search-mining 40. Yuntis: Collaborative Web Resource Categorization . . . http://www.ecsl.cs.sunysb.edu/yuntis/ 44. MIT EECS Events, 1995-96 http://www.eecs.mit.edu/AY95-96/events/
Thailand Tourism [4 5], 15; terms = skytrain reservations. . . 11. Amazing Thailand Tourism Travel Directory http://www.thailand-travelsearch.com/ 22. Amazing Thailand Tourism Travel Forums http://www.thailandtravelforums.com/ 26. Thailand Airlines, Hotels Resorts. . . http://www.thailandtravelsearch.com/thailand/ 44. Thailand Tourism: Vision 2002 http://www.info.tdri.or.th/library/quarterly/ [4 6], 15; terms = potential worldwide spread industries 16. Asian Market Research News:. . . http://www.asiamarketresearch.com/news 38. Thailand Outlook.com : The offical gateway. . . http://www.thailandoutlook.com/top_menu 43. Suwan Site-Learn More About Thailand http://www.geocities.com/Heartland 44. Thailand Tourism: Vision 2002 http://www.info.tdri.or.th/library passage of the bill in the parliament, and the third consists of view-points and analysis from more religious organizations (keywords like  X  X elief X ,  X  X aith X ). (5) The fifth query is  X  X olin Powell. X  In Table 5, we see that the first storyline offers viewpoints questioning the war, and the second storyline is on his biography ( X  X ronx X ,  X  X arents X ). (6) The sixth query is  X  X tkins diet. X  Table 6 shows two storylines X  the first one on books, information, etc., while the second storyline consists of more critical reviews of the diet.
We note that for each of the queries, the storylines we chose to present are not the only ones uncovered by the algorithm; rather, they were chosen to convey storylines with strikingly different view-points from the generic top 10 page for the queries. We make sev-eral observations about the results we obtain. (1) In many cases, storylines, even though they might be quite small, turn out to be very useful and distinctive. (2) If we examine a given storyline, the ranks of the pages present in the storyline span the entire spectrum 1 X 100 of the results. Most often, it is the case that for many of the pages in a storyline, the rank is much more than 10. In some storylines, even the best rank is quite poor. (3) By giving a chance to pages ranked well below the top 10, our algorithm addresses the  X  X yranny of the majority X  X  X n other words, it gives a chance for the minority viewpoints on the query to be reflected adequately in the first page of search results. (4) The algorithm can uncover interesting communities espe-cially when it is not obvious such communities exist. By becoming aware of the existence of several different discussion groups, the user searching for information about removing tree sap from cars can directly post her/his query and benefit directly. In this case, note also that all the ranks are beyond 10.
French schools religious head ban [4 7], 19; terms = stigmatize helsinki sikh law lead 4. CNN.com -Chirac: Ban headscarves in schools http://www.cnn.com/2003/WORLD/europe/12/17/ 21. CNN.com -France backs school head scarf ban http://www.cnn.com/2004/WORLD/europe/02/10/ 35. WorldWide Religious News http://www.wwrn.org/parse.php?idd=9611&amp;c=24 40. Rights groups fear French cult bill would curb. . . http://www.cesnur.org/testi/fr2K_july1.htm [4 7], 18; terms = overwhelmingly hanifa conspicuous. . . 0. KTVU.com -Education -Head Scarves Ban In French. . . http://www.ktvu.com/education/2836420 17. French parliament votes to ban headscarves in schools http://www.brunei-online.com/bb/thu 36. France-Head-Scarves, 1st Writethru http://www.cbc.ca/cp/world/040210 52. AP Wire  X  02/10/2004  X  Lawmakers OK French . . . http://www.sunherald.com/mld/sunherald [5 7], 17; terms = beliefs, society, council, faith. . . 19. Diocese of Stamford http://www.stamforddio.org/ 23. The Observer  X  Special reports  X  Schools X  bid . . . http://observer.guardian.co.uk/islam/story/ 39. FOXNews.com -Top Stories -. . . http://www.foxnews.com/story 41. Khilafah.com -Germany paves way for hijab ban http://www.khilafah.com/home/ 47. Islam Online -News Section http://www.islam-online.net/English/News Table 4: Sample storylines for  X  X rench schools religious head ban. X 
Colin Powell [6 15], 72; terms = civilian missiles reagan . . . 11. Kurt Nimmo: Colin Powell, Exploiting the Dead . . . http://www.counterpunch.org/nimmo09202003.html 19. washingtonpost.com http://www.washingtonpost.com/wp-srv/nation/ 20. AlterNet: The Unimportance of Being Colin Powell http://www.alternet.org 23. Colin Powell http://www.worldworks.org/politics/cpowell.htm [5 28], 122; terms = bronx earned parents 1937 . . . 9. Colin Powell http://www.infoplease.com/cgi-bin/id/A0880273 15. Amazon.com: Books: My American Journey http://www.amazon.com/exec/obidos/ASIN/ 17. Powell, Colin L. http://www.state.gov/r/pa/ei/biog/1349.htm 48. General Colin L. Powell -Biography http://teacher.scholastic.com/barrier/ 49. AskMen.com -Colin Powell http://www.askmen.com/men/business_politics/
Atkins Diet [4 7], 22; terms = weight diets protein purposes. . . 36. Dr Atkins Diet -Plan, Recipe, Books and more http://www.dr-atkins-diet.org/ 44. Amazon.com: Books: Dr. Atkins X  New Diet Revolution http://www.amazon.com/exec/obidos/ASIN 72. Amazon.com: Books: Dr. Atkins X  New Diet Revolution http://www.amazon.com/exec/obidos 97. Atkins Diet &amp; Low Carbohydrate Weight-Loss Support http://www.lowcarb.ca/ [4 7], 18; terms = tissue starches effective. . . 1. Atkins diet exposed: why it is complete bull http://www.supplecity.com/articles 8. Atkins Diet http://www.dieting-review.com/atkins.htm 15. Weight Loss and the Atkins Diet http://weightloss-and-diet-facts.com 86. Phil Kaplan Revisits The Atkins Diet http://www.philkaplan.com/thefitnesstruth
We have presented a combinatorial framework to data-mine web search results with the aim of uncovering storylines that may be buried in the highly-ranked pages. Our framework leads to a fam-ily of simple, natural, and efficient algorithms based on detecting dense bipartite subgraphs in the term X  X ocument relation. Experi-mental evidence suggests that there is much value to be attained by mining the search results beyond the first screenful, and we hope that these ideas will influence web search paradigms of the future.
An interesting future avenue is to work with enhanced formula-tions of the term X  X ocument relation, perhaps derived from natural language understanding, Another question is to find methods that can scale well to the task of mining the top 500 or 1000 search results X  X hose pages might offer interesting viewpoints on the query topic as well.

A larger issue to consider is the mining of an archive of web pages to extract interesting storylines; this will be particularly rel-evant, say, 50 years from now when queries of the form  X  X omputer science research during 1990 X 2020 X  would be conceivably quite interesting. A ranked list of 17,000 web sites is almost surely not the answer one would like to see. [1] K. Bharat and M. Henzinger. Improved algorithms for topic [2] S. Brin and L. Page. The anatomy of a large-scale [3] J-Y. Cai. On the impossibility of certain ranking functions. [4] T. Calishain. Clustering with search engines. [5] S. Chakrabarti, B. Dom, D. Gibson, R. Kumar, P. Raghavan, [6] D.R.Cutting,D.R.Karger,J.O.Pedersen,andJ.W.
 [7] S.C.Deerwester,S.T.Dumais,T.K.Landauer,G.W.
 [8] I. Dhillon, S. Mallela, and D. Modha. Information-theoretic [9] R. Kannan, S. Vempala, and A. Vetta. On clusterings: Good, [10] J. M. Kleinberg. Authoritative sources in a hyperlinked [11] R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins. [12] L. Stockmeyer and V. Vazirani. NP-Completeness of some [13] N. Tishby, F.C. Pereira, and W. Blalek. The information [14] P. Tsaparas. Link Analysis Ranking Algorithms . PhD thesis, [15] O. Zamir and O. Etzioni. A dynamic clustering interface to [16] O. Zamir, O. Etzioni, O. Madani, and R. M. Karp. Fast and
