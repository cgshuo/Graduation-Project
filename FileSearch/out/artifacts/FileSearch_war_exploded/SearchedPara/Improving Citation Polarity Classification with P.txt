 Citations have been categorized and studied for a half-century (Garfield, 1955) to better under-stand when and how citations are used, and to record and measure how information is ex-changed (e.g., networks of co-cited papers or au-thors (Small and Griffith, 1974)). Recently, the value of this information has been shown in practi-cal applications such as information retrieval (IR) (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We ex-pect that by identifying and labeling the function of citations we can improve the effectiveness of these applications.

There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation func-tion (Chubin and Moitra, 1975). Recent stud-ies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch  X  afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation clas-sification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; distinction likely owes to the prominence of sen-timent analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. By concentrating on citation polarity we are able to compare our classification to previous citation polarity work. This choice also allows us to access the wealth of existing data containing polarity an-notation and then frame the task as a domain adap-tation problem. Of course the risk in approaching the problem as domain adaptation is that the do-mains are so different that the representation of a positive instance of a movie or product review, for example, will not coincide with that of a posi-tive scientific citation. On the other hand, because there is a limited amount of annotated citation data available, by leveraging large amounts of anno-tated polarity data we could potentially even im-prove citation classification.

We treat citation polarity classification as a sen-timent analysis domain adaptation task and there-fore must be careful not to define features that are too domain specific. Previous work in citation po-larity classification focuses on finding new cita-tion features to improve classification, borrowing a few from text classification in general (e.g., n -grams), and perhaps others from sentiment analy-sis problems (e.g., the polarity lexicon from Wil-son et al. (2005)). We would like to do as little feature engineering as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon prod-uct reviews. Currently a popular approach for ac-complishing this is to use deep learning neural net-works (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks us-ing only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glo-rot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denois-ing autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the po-larity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art performance in Amazon product domain adapta-tion, we are hopeful it will also be effective when switching to a more distant domain like scientific citations. 3.1 Corpora We are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source dataset. There are two corpora available that contain citation function annotation, the DFKI Citation Corpus (Dong and Sch  X  afer, 2011) and the IMS Citation Corpus (Jochim and Sch  X  utze, 2012). Both corpora have only about 2000 instances; unfortunately, there are no larger corpora available with citation annotation and this task would benefit from more annotated data. Due to the infrequent use of neg-ative citations, a substantial annotation effort (an-notating over 5 times more data) would be nec-essary to reach 1000 negative citation instances, which is the number of negative instances in a sin-gle domain in the multi-domain corpus described below.
 classifying citation function (Dong and Sch  X  afer, 2011), but the dataset also includes polarity an-notation. The dataset has 1768 citation sentences with polarity annotation: 190 are labeled as pos-itive , 57 as negative , and the vast majority, 1521, are left neutral . The second citation corpus, the tations: 1836 are labeled positive and 172 are la-beled negative . Jochim and Sch  X  utze (2012) use annotation labels from Moravcsik and Murugesan (1975) where positive instances are labeled confir-mative , negative instances are labeled negational , and there is no neutral class. Because each of the citation corpora is of modest size we combine them to form one citation dataset, which we will refer to as CITD. The two citation corpora com-prising CITD both come from the ACL Anthol-ogy (Bird et al., 2008): the IMS corpus uses the ACL proceedings from 2004 and the DFKI corpus uses parts of the proceedings from 2007 and 2008. Since mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979 X 2003, 2005 X 2006, and 2009.

There are a number of non-citation corpora available that contain polarity annotation. For these experiments we use the Multi-Domain Senti-Blitzer et al. (2007). We use the version of the MDSD that includes positive and negative labels for product reviews taken from Amazon.com in the following domains: books, dvd, electronics, and kitchen. For each domain there are 1000 pos-itive reviews and 1000 negative reviews that com-prise the  X  X abeled X  data, and then roughly 4000 Corpus Instances Pos. Neg. Neut.
 DFKI 1768 190 57 1521 IMS 2008 1836 172  X  MDSD 27,677 13,882 13,795  X  were preprocessed so that for each review you find a list of unigrams and bigrams with their frequency within the review. Unigrams from a stop list of 55 stop words are removed, but stop words in bigrams remain.

Table 1 shows the distribution of polarity labels in the corpora we use for our experiments. We combine the DFKI and IMS corpora into the CITD corpus. We omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD. It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al., 2002).

The citation corpora presented above are both unbalanced and both have a highly skewed distri-bution. The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as  X  X nlabeled X  rather balanced. For this reason, in line with previous work us-ing MDSD, we balance the labeled portion of the CITD corpus. This is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of posi-tive sentences. The IMS corpus can have multiple labeled citations per sentence: there are 122 sen-tences containing the 172 negative citations from Table 1. The final CITD corpus comprises this balanced corpus of 358 labeled citation sentences plus another 22,093 unlabeled citation sentences. 3.2 Features In our experiments, we restrict our features to un-igrams and bigrams from the product review or citation context (i.e., the sentence containing the citation). This follows previous studies in do-main adaptation (Blitzer et al., 2007; Glorot et al., 2011). Chen et al. (2012) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features.

Previous work in citation classification has largely focused on identifying new features for improving classification accuracy. A significant amount of effort goes into engineering new fea-tures, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch  X  afer, 2011). However, there seems to be little consen-sus on which features help most for this task. For example, Abu-Jbara et al. (2013) and Jochim and Sch  X  utze (2012) find the list of polar words from Wilson et al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem remains that there seems to be no clear set of features for citation polarity classification.

The lack of consensus on the most useful cita-tion polarity features coupled with the recent suc-cess of deep learning neural networks (Collobert et al., 2011) further motivate our choice to limit our features to the n -grams available in the product re-view or citation context and not rely on external resources or tools for additional features. 3.3 Classification with mSDA For classification we use marginalized stacked de-noising autoencoders (mSDA) from Chen et al. cept of denoising  X  introducing noise to make the autoencoder more robust  X  from Vincent et al. (2008), but does the optimization in closed form, thereby avoiding iterating over the input vector to stochastically introduce noise. The result of this is faster run times and currently state-of-the-art performance on MDSD, which makes it a good choice for our domain adaptation task. The mSDA implementation comes with LIBSVM, which we replace with LIBLINEAR (Fan et al., 2008) for faster run times with no decrease in accuracy. LIB-LINEAR, with default settings, also serves as our baseline. 3.4 Outline of Experiments Our initial experiments simply extend those of Chen et al. (2012) (and others who have used MDSD) by adding another domain, citations. We train on each of the domains from the MDSD  X  Figure 1: Cross domain macro-F 1 results train-ing on Multi-Domain Sentiment Dataset and test-ing on citation dataset (CITD). The horizontal line indicates macro-F 1 for in-domain citation classifi-cation. books, dvd, electronics, and kitchen  X  and test on the citation data. We split the labeled data 80/20 following Blitzer et al. (2007) (cf. Chen et al. (2012) train on all  X  X abeled X  data and test on the  X  X nlabeled X  data). These experiments should help answer two questions: does a larger amount of training data, even if out of domain, improve ci-tation classification; and how well do the differ-ent product domains generalize to citations (i.e., which domains are most similar to citations)?
In contrast to previous work using MDSD, a lot of the work in domain adaptation also leverages a small amount of labeled target data. In our second set of experiments, we follow the domain adap-tation approaches described in (Daum  X  e III, 2007) and train on product review and citation data be-fore testing on citations. 4.1 Citation mSDA Our initial results show that using mSDA for do-main adaptation to citations actually outperforms in-domain classification. In Figure 1 we com-pare citation classification with mSDA to the SVM baseline. Each pair of vertical bars represents training on a domain from MDSD (e.g., books) and testing on CITD. The dark gray bar indicates the F 1 scores for the SVM baseline using the 30,000 features and the lighter gray bar shows the mSDA results. The black horizontal line indicates the F 1 score for in-domain citation classification, which sometimes represents the goal for domain adaptation. We can see that using a larger dataset, even if out of domain, does improve citation clas-sification. For books, dvd, and electronics, even the SVM baseline improves on in-domain classifi-cation. mSDA does better than the baseline for all domains except dvd. Using a larger training set, along with mSDA, which makes use of the un-labeled data, leads to the best results for citation classification.

In domain adaptation we would expect the do-mains most similar to the target to lead to the highest results. Like Dai et al. (2007), we mea-sure the Kullback-Leibler divergence between the source and target domains X  distributions. Accord-ing to this measure, citations are most similar to the books domain. Therefore, it is not surprising that training on books performs well on citations, and intuitively, among the domains in the Amazon dataset, a book review is most similar to a scien-tific citation. This makes the good mSDA results for electronics a bit more surprising. 4.2 Easy Domain Adaptation The results in Section 4.1 are for semi-supervised domain adaptation: the case where we have some large annotated corpus (Amazon product reviews) and a large unannotated corpus (citations). There have been a number of other successful attempts at fully supervised domain adaptation, where it is as-sumed that some small amount of data is annotated in the target domain (Chelba and Acero, 2004; Daum  X  e III, 2007; Jiang and Zhai, 2007). To see how mSDA compares to supervised domain adap-tation we take the various approaches presented by Daum  X  e III (2007). The results of this comparison can be seen in Table 2. Briefly,  X  X ll X  trains on source and target data;  X  X eight X  is the same as  X  X ll X  except that instances may be weighted dif-ferently based on their domain (weights are chosen on a development set);  X  X red X  trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions;  X  X inInt X  linearly interpolates predictions using the source-only and target-only models (the interpola-tion parameter is chosen on a development set);  X  X ugment X  uses a larger feature set with source-specific and target-specific copies of features; see (Daum  X  e III, 2007) for further details.

We are only interested in citations as the tar-get domain. Daum  X  e X  X  source-only baseline cor-responds to the  X  X aseline X  column for domains: books, dvd, electronics, and kitchen; while his target-only baseline can be seen for citations in the last row of the  X  X aseline X  column in Table 2.
The semi-supervised mSDA performs quite well with respect to the fully supervised ap-proaches, obtaining the best results for books and electronics, which are also the highest scores over-all. Weight and Pred have the highest F 1 scores for dvd and kitchen respectively. Daum  X  e III (2007) noted that the  X  X ugment X  algorithm performed best when the target-only results were better than the source-only results. When this was not the case in his experiments, i.e., for the treebank chunking task, both Weight and Pred were among the best approaches. In our experiments, training on source-only outperforms target-only, with the exception of the kitchen domain.

We have included the line for citations to see the results training only on the target data ( F 1 = 51 . 9 ) and to see the improvement when using all of the unlabeled data with mSDA ( F 1 = 54 . 9 ). 4.3 Discussion These results are very promising. Although they are not quite as high as other published results have shown that you can improve citation polarity classification by leveraging large amounts of an-notated data from other domains and using a sim-ple set of features. mSDA and fully supervised approaches can also be straightforwardly combined. We do not present those results here due to space constraints. The combination led to mixed results: adding mSDA to the supervised approaches tended to improve F 1 over those approaches but results never exceeded the top mSDA numbers in Table 2. Teufel et al. (2006b) introduced automatic citation function classification, with classes that could be grouped as positive, negative, and neutral. They relied in part on a manually compiled list of cue phrases that cannot easily be transferred to other classification schemes or other scientific domains. Athar (2011) followed this and was the first to specifically target polarity classification on scien-tific citations. He found that dependency tuples contributed the most significant improvement in results. Abu-Jbara et al. (2013) also looks at both citation function and citation polarity. A big con-tribution of this work is that they also train a CRF sequence tagger to find the citation context, which significantly improves results over using only the citing sentence. Their feature analysis indicates that lexicons for negation, speculation, and po-larity were most important for improving polarity classification. Robust citation classification has been hindered by the relative lack of annotated data. In this pa-per we successfully use a large, out-of-domain, annotated corpus to improve the citation polarity classification. Our approach uses a deep learning neural network for domain adaptation with labeled out-of-domain data and unlabeled in-domain data. This semi-supervised domain adaptation approach outperforms the in-domain citation polarity classi-fication and other fully supervised domain adapta-tion approaches.

Acknowledgments. We thank the DFG for funding this work (SPP 1335 Scalable Visual An-alytics ).
