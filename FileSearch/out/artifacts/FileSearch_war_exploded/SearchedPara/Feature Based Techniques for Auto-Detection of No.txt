 Worms are malicious code that infect a host machine and spread copies of itself through the network to infect other hosts. There are different kinds of worms such as: Email worms, Instant Messaging worms, Internet worms, IRC worms and File-sharing Networks worms. Email worm, as the name implies, spreads through infected email messages. The worm may be carried by attachment, or the email may contain the host is immediately infected. Email worms use the vulnerability of the email software at the host machine and sends infected emails to the addresses stored in the address book. In this way, new machines get infected. Examples of email worms are  X  X 32.mydoom.M@mm X ,  X  X 32.Zafi.d X ,  X  X 32.LoveGate.w X ,  X  X 32.Mytob.c X , and so on. Worms do a lot of harm to computers and people. They can clog the network traffic, cause damage to the system and make the system unstable or even unusable. 
There has been a significant amount of research going on to combat worms. The Once a new worm appears, researchers work hard to find a unique pattern in the code signature of the worm. Thus, a worm can be detected from its signature. But the intervention and it may take long time (from days to weeks) to discover the signature. Since worms can propagate very fast, there should be a much faster way to detect them before any damage is done. 
We are concerned with the problem of detecting new email worms without knowing their signatures. Thus, our work is directed towards automatic (i.e., without any human intervention) and efficient detection of novel worms. Our work is inspired by [1], which does not require signature detection; rather, it extracts different features infected. They employ two classifiers in series, the first one is Support Vector Machine (SVM) and the next one is Na X ve Bayes (NB). We will refer to this two-layer approach as  X  X VM + NB series X  approach. They report a very high accuracy, as well as very low false positive and false negative rate. But we address several issues accuracy. Second, they do not apply cross validation on the dataset. Third, our layer approach in terms of cross validation accuracy on a balanced dataset. Fourth, We deal with all these problems and provide efficient solutions. individual performances of NB and SVM with their SVM + NB series counterpart. We show that the series approach is less effective than either NB or SVM alone. So, rearrange the dataset so that it becomes more balanced. We divide the dataset into two portions: one containing only known worms (and some clean emails), the other containing only a novel worm. Then we apply a three-fold cross validation on the  X  X nown worm X  dataset, and we test the accuracy of each of the learned classifiers on the  X  X ovel worm X  dataset. We report the cross validation accuracy, and novel improve the efficiency of classification task. PCA is commonly used to extract patterns from high dimensional data, especially when the data is noisy. Besides, it is a simple and nonparametric method. Since the original dataset contains a total of 94 data to a lower dimension; revealing the underlying simple pattern hidden in the data. Fourth, we build decision tree, using the WEKA [2] implementation of C4.5 [3] better classification accuracy. We report the features as well as the classification rules obtained from the decision tree. automatic email worm detection, section 3 describes the feature reduction and experiments, section 6 discusses the results, and section 7 concludes with future guidelines for research. There are different approaches to automate the detection of worms. These approaches are mainly of two types: behavioral and content-based. Behavioral approaches analyze the behavior of messages like source-destination addresses, attachment types, message frequency etc. Content-based approaches look into the content of the message, and try to detect signature automatically. There are also combined methods that take advantage of both techniques. 
An example of behavioral detection is social network analysis [4, 5]. It detects worm infected emails by creating graphs of network, where users are represented as nodes, and communications between users are represented as edges. A social network is a group of nodes among which there exists edges. Emails that propagate beyond the group boundary are considered to be infected. But the drawback of this system is that worms can easily bypass social networks by intelligently choosing the recipient lists, by looking at recent emails in the user X  X  outbox. Statistical analysis of outgoing emails is another behavioral approach [6, 7]. Statistics collected from frequency of communication between clients and their mail and thus worms are detected. 
Example of content based approach is the EarlyBird System [8]. In this system, statistics on highly repetitive packet contents are gathered. These statistics are analyzed to detect possible infection of host or server machines. This method generates content signature of worm without any human intervention. Results reported by this system indicated very low false positive rate of detection. Other examples are the Autograph [9], and the Polygraph [10], developed at Carnegie Mellon University. 
There are other approaches to detect early spreading of worms, such as employing honeypot X  [11]. A honeypot is a closely monitored decoy computer that attracts attacks for early detection and in-depth adversary analysis. The honeypots are designed to not send out email in normal situations. If a honeypot begins to send out emails after running the attachment of an email, it is determined that this email is an email worm. 
Martin et al. [12] also report an experiment with email data, where they apply a statistical approach to find an optimum subset of a large set of features to facilitate the classification of outgoing emails, and eventually, detect novel email worms. 
Another approach by Sidiroglou et al. [13] employs behavior-based anomaly detection, which is different from the sign ature based or statistical approaches. Their approach is to open all suspicious attachments inside an instrumented virtual machine looking for dangerous actions, such as writing to the Windows registry, and flag suspicious messages. 
Although our approach is feature-based, it is different from the above feature-based detection approaches in that we apply PCA, and decision tree to reduce the dimension of data. Rather than choosing a subset of features, PCA finds a linear combination of other hand, we apply decision tree to identify the most important features, thereby removing redundant or noisy features. Both these approaches achieve higher accuracy. Firstly, we briefly describe the features that are used in email worm detection. These features are extracted from a repository of outgoing emails collected over a period of while per-window features are features of a collection of emails sent within a window of time. Secondly, we describe our feature reduction techniques, namely, PCA and J48. Finally, we briefly describe the two-layer approach and its limitations. 3.1 Feature Description depending on the presence or absence of this feature in a data point. There are a total of 94 features. Here we describe some of them. 3.1.1 Per Email Features i. HTML in body : Whether there is HTML in the email body. This feature is used exploited by worm writers. It is a binary feature. ii. Embedded image : Whether there is any embedded image. This is used because a buggy image processor of the email client is also vulnerable to attacks. It is a binary feature. iii. Hyperlinks : Whether there are hyperlinks in the email body. Clicking an infected link causes the host to be infected. It is also a binary feature. iv. Binary Attachment : Whether there are any binary attachments. Worms are mainly propagated by binary attachments. This is also a binary feature. v. Multipurpose Internet Mail Extensions ( MIME ) type of attachment : There are different MIME types, for example:  X  X pplication/msword X ,  X  X pplication/pdf X ,  X  X mage/gif X ,  X  X ext/plain X  etc. Each of these types is used as a binary feature (total 27). vi. UNIX  X  X agic number  X  of file attachments : Sometimes a different MIME type is assigned by the worm writers to evade detection. Magic numbers can accurately detect the MIME type. Each of these types is used as a binary feature (total 43). vii. Number of attachments : It is a continuous feature. viii. Number of words/characters in subject/body : These features are continuous. Most worms choose random text, whereas a user may have certain writing characteristics. Thus, these features are sometimes useful to detect infected emails. 3.1.2 Per Window Features i. Number of emails sent in window : An infected host is supposed to send emails at a faster rate. This is a continuous feature. distinguish between normal and infected host. This is a continuous feature too. iii. Average number of words/characters per subject, body; average word length : These features are also useful in distinguishing between normal and viral activity. iv. Variance in number of words/characters per subject, body; variance in word length: These are also useful properties of email worms. attachments, whereas most infected emails do contain them. 3.2 Feature Reduction and Selection classification tasks because i) it increases the running time of the classification algorithms, ii) it increases chance of overfitting, and iii) large number of instances are required for learning tasks. We apply PCA to obtain a reduced dimensional data and apply decision tree to select a subset of f eatures, in order to eliminate these problems. 3.2.1 Principal Component Analysis: Reducing Data Dimension PCA finds a reduced set of attributes by projecting the original attributes into a lower dimension. We observe that for some optimal dimension of projection, the reduced dimensional data observes a better accuracy in detecting novel worms. PCA not only reduces the dimension of data to eliminate all these problems, but also discovers hidden patterns in data, thereby increasing classification accuracy of the learned classifiers. As high dimensional data contains redundancies and noises, it is much instances. The learned hypothesis is likely to be too complex and susceptible to overfitting. PCA reduces the dimension, without losing much information, and thus allows the learning algorithms to find a simpler hypothesis that is consistent with the training examples, and thereby reduces the chance of overfitting. But it should be noted that PCA projects data into a lower dimension in the direction of maximum dispersion. Maximum dispersion of data does not necessarily imply maximum separation of between  X  class data and/or maximum concentration of within  X  class data. If this is the case, then PCA reduction may result in poor performance. That is why we apply PCA to reduce the 94-dimensional data into different lower dimensions, ranging from 5 to 90, and select the optimal dimension that achieves the highest classification accuracy. 3.2.2 Decision Tree: Feature Selection Using Information Gain Feature selection is different from dimensio n reduction because it selects a subset of There are different feature selection techniques available, such as greedy selection, deteriorates. But the problem with this selection approach is that it takes a lot of time and depending on the order of selection, results vary significantly. We apply the second, it applies information gain to select best features, and finally, we can extract a  X  X egative X  class. Thus, we are not only aware of the essential attributes but also get an more rules to obtain a generalized characteristic of different types of worms. 
Information gain is a very effective metric in selecting features. Information gain classifying the training data [14]. If we split the training data on this attribute values, then information gain gives the measurement of the expected reduction in entropy after the split. The more an attribute can reduce entropy in the training data, the better collection of examples S is given by (1): possible values (0, 1). Entropy of subset S is computed using the following equation: negative examples in S . Computation of information gain of a continuous attribute is a little tricky, because it has infinite number of possible values. One approach followed by Quinlan [3] is to find an optimal threshold, and split the data into two halves. The optimal threshold is found by searching a threshold value with highest information gain within the range of values of this attribute in the dataset. We use J48 for building decision tree, which is an implementation of C4.5. Decision tree algorithms choose the best attribute based on information gain criterion at each level of recursion. Thus, the final tree actually consists of the most important attributes that can distinguish between the positive and negative instances. The tree is further pruned to reduce chances of overfitting. Thus, we are able to identify the features that are necessary and the features that are redundant, and use only the necessary features. Surprisingly enough, in our experiments we find that only four/five features are necessary among the ninety-four features. The decision trees generated in our experiments have better classification accuracies than both original and PCA reduced data. 3.3 Classification Techniques We apply the NB [15] and SVM [16] classi fiers in our experiments. We also apply the SVM+NB series classifier and J48 Decision Tree classifier. NB, SVM and the series classifiers are applied on the original data and the PCA-reduced data, while the J48 is applied on the original data only, because the classifier itself selects a subset of features, discarding redundant ones. The SVM+NB series is implemented as per [1]. We do not recommend using the series classifier because of the following reasons. First, it is not practical. Because we must come up with a set of parameter values such that the false positive of SVM becomes zero. Given a set of continuous parameters, assumption in this approach is wrong. Because, even if we happen to find an optimal since this optimal point is obtained from the training data. Third, if NB performs poorly on a particular test set, the output would also be poor. Because, both NB and SVM must perform well to produce a good series result, if any one fails, the combined approach would also fail. In our experimental results, we have indicated the effect of all these problems. We have collected the worm dataset used in the experiment by Martin et al. [1]. They have accumulated several hundreds of clea n and worm emails over a period of two years. All these emails are outgoing emails. Several features are extracted from these emails as explained in section 3.1. There are six types of worms contained in the dataset: VBS.BubbleBoy, W32.Mydoom.M, W32.Sobig.F, W32.Netsky.D, W32.Mydoom.U, and W32.Bagle.F. 1000 worm emails. The worm emails are made up of 200 examples from each of the 1200 clean emails and 200 infected messages. As we have mentioned earlier, the dataset is imbalanced. So we apply both cross validation and novel worm detection in our experiments. In our distribution, each balanced set contains 1600 clean email messages, which are the combination of all the Also, each balanced set contains 1000 viral messages (from original training set), marked as  X  X nown worms X  and 200 viral messages (the sixth worm from the original randomly divide the set of 2600 (1600 clean + 1000 viral) messages into three equal sized subsets. We take two subsets as training set and the remaining set as the test set. This is done three times by rotating the testing and training sets. We take the average following tables. Besides testing the accuracy of the test set, we also test the detection accuracy of the learned classifier on the  X  X ovel worm X  set. This accuracy is also averaged over all runs and shown as novel detection accuracy . 
For SVM, we use libsvm [17] package, and apply C-Support Vector Classification (C-SVC) with the radial basis function using  X  X amma X  = 0.2 and  X  X  X =1. We use our own C++ implementation of NB. We use the WEKA [2] implementation of J48, with pruning applied. We extract rules from the decision trees generated using J48. results found from the original data. In section 6.2, we discuss the results found from the reduced dimensional data using PCA. In section 6.3, we discuss the results obtained using J48. 6.1 Results from the Original Dataset The results are shown in table 1. Table 1 reports the accuracy of the cross validation and novel detection for each dataset. The cross validation accuracy is shown under the column  X  Acc X  and the accuracy of detecting novel worms is shown under the column  X  novel detection acc X  . Each worm at the row heading is the novel worm for that dataset. In table 1, we report accuracy and novel detection accuracy for each of the six worm types. From the results reported in table 1, we see that SVM observes the best accuracy among all classifiers. The best accuracy observed by SVM is 99.77%, on the sobig.f dataset, while the worst accuracy observed by the same is 99.58%, on the mydoom.m dataset. 6.2 Results from the Reduced Dimensional Data (Reduced by PCA) The following chart (Fig. 1) shows the results of applying PCA on the original data. The X axis represents the dimension of the reduced dimensional data, which has been unreduced or original dimension. Fig. 1 shows the cross validation accuracy for different dimensions. The data from the chart should be read as follows: a point ( x , y ) SVM, averaged over all six datasets, where each dataset has been reduced to x dimension using PCA. 
Fig. 1 indicates that at lower dimensions , cross validation accuracy is lower, for each of the three classifiers. But SVM is found to have achieved its near maximum accuracy when data dimension is 30. NB and SERIES reaches within 2% of maximum accuracy at dimension 30 and onwards. All classifiers attain their maximum at the highest dimension 94, which is actually the unreduced data. So, from this observation, we may conclude that PCA is not effective on this dataset, in terms of cross validation accuracy. The reason behind this poorer performance on the reduced dimensional data is possibly the one that we have mentioned earlier in section 3.2. The reduction by PCA is not producing a lower dimensional data where dissimilar class instances are maximally dispersed and similar class instances are maximally concentrated. So, the classification accuracy is lower at lower dimensions. 
We now present the results, at dimension 25, similar to the results presented in previous section. Table 2 compares the novel detection accuracy and the cross validation accuracy of different classifiers. We choose this particular dimension aspects: cross validation accuracy, false positive and false negative rate and novel detection accuracy. We conclude that this dimension is the optimal dimension of projection by PCA. 
Results in Table 2 indicate that accuracy and novel detection accuracy of SVM are higher than NB, respectively. Also, as mentioned in previous section, we again observe that accuracy and novel detection accuracy of SVM+NB is worse than both NB and SVM. Thus, SVM is found to be the best among these three classifiers, in both unreduced and reduced dimensions. Worm Type NB SVM SVM+NB 6.3 Results Obtained from J48 Table 3 reports the accuracy, novel detectio n accuracy, #of selected features and tree average novel detection accuracy of J48 is the highest (70.9%) among all the classifiers both in the original and PCA-reduced dataset. Besides, the average very close to the best accuracy (SVM, 99.67%) in the original dataset. Surprisingly enough, on average only 4.5 features have been selected by the decision tree algorithm, which means almost 90 other features are redundant. It is interesting to see decision trees. Each rule is expressed as a disjunction of one or more conditions. We use the symbol  X   X   X  to denote conjunction and  X   X   X  to denote disjunction. We are able to detect the reason (explained later) behind the poor performance of all the classifiers in Bubbleboy dataset, where all of them have 0% novel detection accuracy. Worm Type Acc (%) Novel detection Worm rules: if any of the following rules is satisfied then it is a worm Rule I ( from Mydoom.m dataset ) :  X   X   X  Rule IV ( from Mydoom.u dataset ) :  X  Rule V ( from bagle.f dataset ) : [ (RatioAttach &gt; .6)  X  (VarAttachSize &lt;= 7799173)]  X  Rule VI ( from Bubbleboy dataset ) : [ (NumFromAddrInWindow&gt;1)  X  (AttachmentIsText0= 1)] By looking at the above rules, we can easily find some important features such as: 
VarWordsInBody, RatioAttach, MeanWordsInBody, NumAttachments, VarAttachSize , and so on. Using the above rules, we can also summarize general characteristics of worm. For example, it is noticeable that for most of the worms, RatioAttach &gt;= 0.7 , as well as would be effective against a new attack. 
The rule VI above is obtained from the  X  X ubbleboy X  dataset. But only one of the 200 test cases satisfies this rule, so the novel detection accuracy is only 0.5%. Other detected by the generalizations obtained on other five worm types. In this work, we explore three different data mining approaches to automatically detect email worms. The first approach is to apply either NB or SVM on the original dataset, without any feature reduction, and tr ain a classifier. The second approach is to reduce data dimension using PCA and apply NB or SVM on the reduced data and train a classifier. The third approach is to se lect best features using decision tree such as J48 and obtain classification rules. Resu lts obtained from our experiments indicate that J48 achieves the best performance. Looking at the rules extracted from the decision tree, we conclude that the feature space is actually very small, and the classifiers are quite simple. That is why the tree based selection performs better than more complex. In that case, the second approach would have been more effective. data. In summary, all the approaches are possible to apply, depending on the characteristic of the dataset. 
In future, we would like to continue our research in detecting worms by combining feature-based approach with content-based approach to make it more robust and efficient. Besides, rather than relying entirely on features, we are willing to focus on worms. Finally, we would also like to obtain a larger and richer collection of dataset. 
