 While the vast majority of clustering algorithms are partitional, man y real world datasets have inherently overlapping clusters. Sev-eral approaches to nding overlapping clusters have come from work on analysis of biological datasets. In this paper , we inter -pret an overlapping clustering model proposed by Segal et al. [23] as a generalization of Gaussian mixture models, and we extend it to an overlapping clustering model based on mixtures of any regu-lar exponential family distrib ution and the corresponding Bre gman divergence. We pro vide the necessary algorithm modications for this extension, and present results on synthetic data as well as sub-sets of 20-Ne wsgroups and EachMo vie datasets.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing; I.2.6 [ Articial Intelligence ]: Learning Algorithms Ov erlapping clustering, exponential model, Bre gman divergences, high-dimensional clustering, graphical model.
Most clustering methods partition the data into non-o verlapping regions, where each point belongs to only one cluster . In a variety of important applications, though, overlapping clustering , wherein some items are allo wed to be members of two or more disco v-ered clusters, is more appropriate. For example, in biology , genes often simultaneously participate in multiple processes; therefore, when clustering micro-array gene expression data, it is appropri-ate to assign genes to multiple, overlapping clusters [23, 4]. Simi-larly , when clustering documents into topic cate gories, documents may contain multiple rele vant topics and an overlapping cluster -ing might be more rele vant [22]. In the 20-Ne wsgr oups benchmark dataset, articles with multiple topics are cross posted to multiple newsgroups. Ideally , a clustering algorithm applied to this data would allo w articles to be assigned to multiple topic labels and Cop yright 2005 ACM 1 X 59593 X 135 X  X /05/0008 ... $ 5.00. would redisco ver the original cross-posted articles. In the Eac h-Mo vie dataset used to test recommender systems, man y mo vies be-long to more than one genre, such as  X Aliens X , which is listed in the action, horror and science ction genres. An overlapping clus-tering algorithm applied to this data should automatically disco ver such multi-genre mo vies.

In this paper , we generalize an approach to overlapping clus-tering introduced by Segal et al. [23], hereafter referred to as the SBK model. The original method was presented as a specialization of a Probabilistic Relational Model (PRM) [14] and was speci-cally designed for clustering gene expression data. We present an alternati ve vie w of their basic approach as a generalization of stan-dard mixture models. While the original model maximized lik e-lihood over constant variance Gaussians, we generalize it to work with any regular exponential family distrib ution, and correspond-ing Bre gman divergences, thereby making the model applicable for a wide variety of clustering distance functions [2]. This gen-eralization is critical to the effecti ve application of the approach to high-dimensional sparse data, such as typically those encountered in text mining and recommender systems, where Gaussian models and Euclidean distance are kno wn to perform poorly . Further , we propose a novel algorithm dynamicM that assigns instances to mul-tiple clusters for the general model. We also outline an alternating minimization algorithm that monotonically impro ves the objecti ve function for overlapping models for any regular exponential family distrib ution.

In order to demonstrate the generality and effecti veness of our approach, we present experiments in which we produced and eval-uated overlapping clusterings for subsets of the 20-Ne wsgr oups and Eac hMo vie data sets mentioned abo ve. An alternati ve  X stra w man X  algorithm for overlapping clustering is to produce a standard proba-bilistic  X soft X  clustering by mixture modeling and then mak e a hard assignment of each item to one or more clusters using a thresh-old on the cluster membership probability . The ability of thresh-olded soft clustering to produce good overlapping clusterings is an open question. Consequently , we experimentally compare our ap-proach to an appropriate thresholded soft clustering and sho w that the proposed overlapping clustering model produces groupings that are more similar to the original overlapping cate gories in the 20-Ne wsgr oups and Eac hMo vie data.

A brief word on notation: uppercase letters such as X signify a matrix, whose i th row vector is represented as X i , j vector is represented as X j , and whose entry in row i and column j is represented as X j i as well as X i j .
In this section, we give a brief introduction to the PRM-based SBK model. Probabilistic Relational Models (PRMs) [14] extend the basic concepts of Bayesian netw orks into a frame work for rep-resenting and reasoning with probabilistic relationships between entities in a relational structure. The SBK model is an instantia-tion of a PRM for capturing the relationships between genes, pro-cesses, and measured expression values on DN A microarrays. The structure of the instantiated model succinctly captures the underly-ing biological understanding of the mechanism generating the ob-serv ed microarray values  X  namely , that genes participate in pro-cesses, experimental conditions cause the invocation of processes at varying levels, and the observ ed expression value in any particu-lar microarray spot is due to the combined contrib utions of several dif ferent processes. The SBK model places no constraints on the number of processes in which any gene might participate, and thus gene membership in multiple processes, i.e., overlapping cluster -ing, naturally follo ws.

The SBK model works with three matrices: the observ ed real expression matrix X (genes experiments) , a hidden binary mem-bership matrix M (genes processes) containing the membership of each gene in each process, and a hidden real acti vity matrix A (pr ocesses conditions) containing the acti vity of each process for each experimental condition. The SBK modeling assumes that the expression value X j i corresponding to gene i in experiment j has a Gaussian distrib ution with constant variance. The mean of the distrib ution is equal to the sum of the acti vity levels A the processes h in which gene i participates so that p ( X that M and A are independent so that p ( M ; A ) = p ( M ) that X j i 's are conditionally independent given M i and A are assumed to be component-wise independent as well. Assum-ing that elements of A are uniformly distrib uted, considering the log-lik elihood of the joint distrib ution, we have To nd the value of the hidden variables M and A , the SBK model uses an EM approach [12]. The E step involv es nding the best estimates of the binary genes-process memberships M . The M step involv es computing the prior probability of gene membership in each process p ( M ) and the process-condition acti vations A .
The core parameter estimation problem is much easier to under -stand if we recast it as a matrix decomposition problem, initially ignoring the priors. With the kno wledge that there are k rele vant processes in the observ ations, we want to nd a decomposition of the observ ed expression matrix X 2 n d into a binary mem-bership matrix M 2f 0 ; 1 g n k and a real valued acti vation matrix A 2 k d such that jj X MA jj 2 is minimized. Hence, the problem is one of matrix factorization, and the dif culty arises from the fact that M is a binary matrix.
In this section, we rst outline a simplistic way of getting over-laps from soft-clustering based on mixture models. Then, we pro-pose our model for overlapping clustering, hereafter referred to as MOC, as a generalization of the SBK model.
Given a set of n data points f X i g n i = 1 in d , represented by a n d matrix X , tting a mixture model to X is equi valent to as-suming that each data point X i is dra wn independently from a prob-ability density p ( X i j Q ) =  X  k h = 1 a h p h ( X i j k is the number of mixture components, p h is the probability den-sity function of the h th mixture component with parameters q a h are the component mixing coef cients such that a h 0 and  X  sumed to be generated from only one underlying mixture compo-nent. Let Z be a n k boolean matrix such that Z i j is 1 if the j component density was selected to generate X i , and 0 otherwise. Let z i be a hidden random variable corresponding to the inde x of the 1 in each row Z i : every z i is therefore a multinomial random vari-able, since it can tak e one of k discrete values. Since the Z matrix is unkno wn, the optimum parameters Q of the mixture model can be obtained using the well-kno wn iterati ve Expectation Maximization (EM) algorithm [12]. The probability value p ( z i = h j X con vergence of the EM algorithm gives the probability of the point X being generated from the h th mixture component. Using these probabilities, mixture models are often used to generate a parti-tional clustering of the data, where the points estimated to be most probably generated from the h th mixture model component are con-sidered to constitute the h th partition.

In order to use the mixture model to get overlapping clustering, where a point can deterministically belong to multiple clusters, one can choose a threshold value l such that X i belongs to the h enable X i to belong to multiple clusters. Ho we ver, there are two problems with this method. One is that the choice of the parameter l , which is dif cult to learn given only X . Secondly , this is not a natural generati ve model for overlapping clustering. In the mixture model, the underlying model assumption is that a point is generated from only one mixture component, and p ( z i = h j X i ; Q the probability of X i being generated from the h th mixture compo-nent. Ho we ver, an overlapping clustering model should generate X by simultaneously acti vating multiple mixture components. We describe one such model in the next section.
The overlapping clustering model that we present here is a gener -alization of the SBK model described in Section 2. The SBK model minimizes the squared loss between X and MA , and their proposed algorithms is not applicable for estimating the optimal M and A corresponding to other loss functions. In MOC, we generalize the SBK model to work with a broad class of probability distrib utions, instead of just Gaussians, and propose an alternate minimization algorithm for the general model.

The most important dif ference between MOC and the mixture model is that we remo ve the multinomial constraint on the matrix Z , so that it can now be an arbitrary boolean matrix. To distin-guish from the constrained matrix Z , we denote this unconstrained boolean matrix as the membership matrix M . Ev ery point X has a corresponding k -dimensional boolean membership vector M the h th component M h i of this membership vector is a Bernoulli ran-dom variable indicating whether X i belongs to the h th cluster . So, a membership vector M i with multiple 1's directly encodes the fact that the point X i belongs to multiple clusters.

Let us now consider the probability of generating the observ ed data points in MOC. A is the acti vity matrix of this model, where A represents the acti vity of cluster h while generating the j of the data. The probability of generating all the data points is where Q = f M ; A g are the parameters of p , and X j i 's are condition-ally independent given M i and A j . In MOC, we assume p to be the density function of any regular exponential family distrib ution, and also assume that the expectation parameter corresponding to X is of the form M i A , so that E [ X i ] = M i A . In other words, using vector notation, we assume that each X i is generated from an ex-ponential family density whose mean M i A is determined by taking the sum of the acti vity levels of the components that contrib ute to the generation of X i , i.e., M h i is 1 for the acti ve components.
Using the abo ve assumptions and the bijection between regular exponential distrib utions and regular Bre gman divergences [2], the conditional density can be represented as: where d f is the Bre gman divergence corresponding to the chosen exponential density p . For example, if p is the Poisson density , d is the I-di vergence; if p is the Gaussian density , d f is the squared Euclidean distance [2].

Similar to the SBK model, the overlapping clustering model tries to optimize the follo wing joint distrib ution of X , M and A : p (
X ; M ; A ) = p ( M ; A ) p ( X j M ; A ) = p ( M ) p ( A Making similar model assumptions as in Section 2, we assume that M and A are independent of each other apriori and A is dis-trib uted uniformly over a suf ciently lar ge compact set, implying that p ( M ; A ) = p ( M ) p ( A )  X  p ( M ) . Then, maximizing the log-lik elihood of the joint distrib ution gives max where a ih = p ( M h i ) is the (Bernoulli) prior probability of the i -th point having a membership M ih to the h -th cluster .
In this section, we propose and analyze algorithms for estimating the overlapping clustering model given an observ ation matrix X . In particular , from a given observ ation matrix X , we want to estimate the prior matrix a , the membership matrix M and the acti vity matrix A so as to maximize p ( M ; A ; X ) , the joint distrib ution of The key idea behind the estimation is an alternating minimization technique that alternates between updating a , M and A .
The prior matrix a can be directly calculated from the current estimate of M . If p h denotes the prior probability of any point be-longing to cluster h , then, for a particular point i , we have a p h ( 1 p h ) variable, and the Bernoulli distrib ution is a member of the expo-nential family , the maximum lik elihood estimate is just the sample mean of the suf cient statistic [2]. Since the suf cient statistic for Bernoulli is just the indicator of the event, the maximum lik elihood estimate of the prior p h of cluster h is just p h = 1 n  X  one can compute the prior matrix a using these update equations.
In the main alternating minimization technique, for a given X the update for M has to minimize Since M is a binary matrix, this is inte ger optimization problem and there is no kno wn polynomial time algorithm to exactly solv e the problem. The explicit enumeration method involv es evaluating all 2 possibilities for every data point, which can be prohibiti ve for even moderate values of k . So, we investig ate simple techniques of updating M so that the loss function is minimized.

There can be two ways of coming up with an algorithm for up-dating M . The rst one is to consider a real relaxation of the problem and allo w M to tak e real values in [ 0 ; 1 ] . For particular choices of the Bre gman divergence, specic algorithms can be de-vised to solv e the real relax ed version of the problem. For example, when the Bre gman divergence is the squared loss, the correspond-ing problem is just the bounded least squares (BLS) problem given by min rithms [6]. No w, from the real bounded matrix M , one can get the cluster membership by rounding M ih values either by proper thresh-olding [23] or randomized rounding. If k 0 clusters get turned  X on X  for a particular data point, the SBK model performs an explicit 2 search over the  X on X  clusters in order get impro ved results. Another alternati ve could be to keep M in its real relax ed version till the overall alternating minimization method has con verged, and round it at the very end. The update equation of the priors p h to be appropriately changed in this case.

Although the real relaxation approach seems simple enough for the squared loss case, it is not necessarily so for all Bre gman diver-gences. In the general case, one may have to solv e an optimization problem (not necessarily con vex) with inequality constraints, be-fore applying the heuristics outlined abo ve. In order to avoid that, we outline a second approach that directly tries to solv e the inte ger optimization problem without doing real relaxation.

We begin by making two observ ations regarding the problem of estimating M : (1) In a realistic setting, a data point is more lik ely to be in very few clusters rather than most of them; and (2) For each data point i , estimating M i is a variant of the subset sum prob-lem that uses a Bre gman divergence to measure loss. Taking the rst observ ation a step further , for a domain if it is well understood (or desirable) that each data point can belong to at most k ters, for some k 0 possibly signicantly smaller than k , then it may be computationally feasible to perform an explicit search over all the possibilities: k 1 + k 2 + + k k equality holds if k 0 k = 2. Note that for k 0 = 1, the overlapping clustering model essentially reduces to the regular mixture model. Ho we ver, in general, such a brute-force search may only be feasible for very small value of k 0 . Further , it is perhaps not easy to decide on such a k 0 apriori for a given problem. So, we focus on design-ing an efcient way of searching through the rele vant possibilities using the second observ ation.

The subset sum problem is one of the hard knapsack problems [9] that tries to solv e the follo wing: Given a set of k natural numbers a ;:::; a k and a tar get number x , nd a subset S of the numbers such that  X  a a set of real numbers, and tries to nd a subset such that the sum over the subset is the closest possible to x . In our case, we measure closeness using a Bre gman divergence and we have multiple tar get numbers to which we want the sum to be close. In particular , then the problem is to nd M i such that M i = argmin Thus, there are m tar get numbers X i 1 ;:::; X im , and for each tar get number X i j the subset is to be chosen from A 1 j ;:::; A loss is the sum of the indi vidual losses, and the problem is to nd a single M i that minimizes the total loss.
Using the inherent bias of natural overlapping problems to put each point in low number of clusters, and the similarities of our formulation to the subset sum problem, we propose the algorithm dynamicM (Algorithm 1). The algorithm is moti vated by the Apri-ori class of algorithms in data mining and Shaple y value compu-tation in co-operati ve game theory [17]. It is important to note that no theoretical claim is being made regarding the optimality of dynamicM . The belief is that such an efcient algorithm will work well in practice, as the empirical evidence in Section 5 suggests. Algorithm 1 dynamicM
The algorithm dynamicM starts with 1 cluster turned  X on X  and greedily looks for the next best cluster to turn  X on X  so as to min-imize the loss function. If such a cluster is found, then it has 2 clusters turned  X on X . Then, it repeats the process with the 2 clus-ters turned  X on X . In general, if h clusters are turned  X on X , dynamicM considers turning each one of the remaining ( k h ) clusters  X on X , one at a time, and computes loss corresponding to the membership vector with ( h + 1 ) clusters turned  X on X . If, at any stage, turning  X on X  each one of the remaining ( k h ) clusters increases the loss function, the search process is terminated. Otherwise, it picks the best ( h + 1 ) th cluster to turn  X on X , and repeats the search for the next best on the remaining ( k h 1 ) clusters.

Such a procedure will of course depend on the order in which clusters are considered to be turned  X on X . In particular , the choice of the rst cluster to be turned  X on X  will partly determine which other clusters will get turned  X on X . The permutation dependenc y of the problem is some what similar in avor to that of pay-of f com-putation in a co-operati ve game. If h players are already in co-operation, the value-add of the ( h + 1 ) th partner will depend on the permutation follo wing which the rst h were chosen. In order to design a fair pay-of f strate gy, one computes the average value-add of a player , better kno wn as Shaple y value, over all permutations of forming co-operations [17].

Then, in theory , dynamicM should consider each all possible per -mutations, keep turning clusters  X on X  follo wing each permutation to gure out the lowest loss achie ved along that particular permu-tation, and nally compute the best membership vector among all permutations. Clearly , such an approach would be infeasible in practice. Instead, dynamicM starts with k threads, one correspond-ing to each one of the k clusters turned  X on X . Then, in each thread, it performs the search outlined abo ve for adding the next  X on X  cluster , till no such clusters are found, or all of them have been turned  X on X . The search is similar in avor to the Apriori algorithms, or, dy-namic programming algorithms in general, where an optimal sub-structure property is assumed to hold so that the search for the best membership vector with ( h + 1 ) clusters turned  X on X  starts from that with h clusters turned  X on X . Effecti vely , dynamicM searches over k permutations, each starting with a dif ferent cluster turned  X on X . The other entries of the permutation are obtained greedily on the y. Since dynamicM runs k threads to achie ve partial permuta-tion independence, the best membership vector over all the threads is selected at the end. The algorithm has a worst case running time of O ( k 3 ) and is capable of running with any distance function.
We now focus on updating the acti vity matrix A . Since there are no restrictions on A as such, the update step is simpler than that for M . Note that the only constraint that such an update needs to satisfy is that MA stays in the domain of f . We give exact updates for particular choices of Bre gman divergences: the squared loss and the I-di vergence, since we use only these in section 5.
In case of the square loss, since the domain of f is , the prob-lem min A k X MA k 2 is just the standard least squares problem that can be exactly solv ed by A = M  X  X , where M  X  is the pseudo-in verse of M , and is equal to ( M T M ) 1 M T in case M T M is invertible. In case of I-di vergence or un-normalized relati ve entrop y, the problem min has been studied as a non-ne gative matrix factorization technique [19]. The optimal update for A for given X ; M multiplicati ve and is given by In order to pre vent a division by 0, it mak es sense to use max and max (  X  i M h i ; e ) as the denominators for some small constant e &gt; 0. With the abo ve updates, the respecti ve loss functions are pro vably non-increasing. In the case of a general Bre gman diver-gence, the update steps need not necessarily be as simple and will be investig ated as a future work.
This section describes the details of our experiments that demon-strate the superior performance of MOC on real-w orld data sets, compared to the thresholded mixture model.
We run experiments on three types of datasets: synthetic data, mo vie recommendation data, and text documents. For the high-dimensional mo vie and text data, we create subsets from the origi-nal datasets, which have the characteristics of having a small num-ber of points compared to the dimensionality of the space. The pur -pose of performing experiments on these subsets is to scale down the sizes of the datasets for computational reasons but at the same time not scale down the dif culty of the tasks, since clustering a small number of points in a high-dimensional space is a compara-tively dif cult task.

Synthetic data : In [23], apart from demonstrating their approach on gene microarray data and evaluating on standard biology databases, Segal et al. also sho wed results on microarray-lik e synthetic data with a clear ground truth since the biology databases are generally belie ved to be lacking in coverage. The synthetic data was gen-erated by sampling points from the overlapping clustering model and subsequently adding noise. We used a similar technique to cre-ate three synthetic datasets of dif ferent sizes: (1) small-synthetic : a dataset with n = 75, d = 30 and k = 10; (2) medium-synthetic : a dataset with n = 200, d = 50 and k = 30; and (3) lar ge-synthetic : a dataset with n = 1000, d = 150 and k = 30. For the synthetic datasets we used squared Euclidean distance as the cluster distor -tion measure in the overlapping clustering algorithm, since Gaus-sian densities were used to generate the noise-free datasets.
Mo vie Recommendation data : The EachMo vie 1 dataset has 5-point user ratings for the 74,424 mo vies in the collection. The cor -responding mo vie genre information is extracted from the Internet Mo vie Database (IMDB) 2 collection. If each genre is considered as a separate cate gory or cluster , then this dataset also has naturally overlapping clusters since man y mo vies are annotated in IMDB as belonging to multiple genres, e.g., Aliens belongs to 3 genre cat-egories: action, horror and science ction. We created 2 subsets from the EachMo vie dataset: (1) mo vie-taa : 300 mo vies from the 3 genres  X  thriller , action and adv enture; and (2) mo vie-afc : 300 mo vies from the 3 genres  X  animation, family , and comedy . We clustered the mo vies based on the user recommendations to redis-cover genres, based on the belief that similarity in recommendation proles of mo vies gives an indication about whether the y are in related genres. For this domain we use I-di vergence with Laplace smoothing as the cluster distortion measure.

Text data : Experiments were also run on 3 text datasets deri ved from the 20-Ne wsgr oups collection 3 , which has 20,000 documents from 20 Usenet newsgroups. We processed the original newsgroup articles to reco ver the multiple newsgroup labels on each message posting. From the full dataset, a subset was created having 100 postings in each of the 20 newsgroups, from which the follo wing three data subsets were created with varying levels of overlap in the topics: (1) news-similar -3 ; (2) news-r elated-3 ; and (3) news-dif fer ent-3 . Details of these datasets are outlined in [3]. The vector -space model of each data subset was created using standard text pre-processing methods [13], and each data subset has 300 points in high-dimensional space ( &gt; 1000 words). In this case, I-di vergence was again used as the Bre gman divergence for overlapping cluster -ing, with suitable Laplace smoothing.

We used an experimental methodology similar to the one used to demonstrate the effecti veness of the SBK model [23]. For each dataset, we initialized the overlapping clustering by running k-means clustering, where the additi ve inverse of the corresponding Bre g-man divergence was used as the similarity measure and the number of clusters was set by the number of underlying cate gories in the dataset. The resulting clustering was used to initialize our overlap-ping clustering algorithm.

To evaluate the clustering results, precision, recall, and F-measure were calculated over pairs of points. For each pair of points that share at least one cluster in the overlapping clustering results, these measures try to estimate whether the prediction of this pair as be-ing in the same cluster was correct with respect to the underlying true cate gories in the data. Precision is calculated as the fraction of pairs correctly put in the same cluster , recall is the fraction of actual pairs that were identied, and F-measure is the harmonic mean of precision and recall. http://research.compaq.com/SRC/eachmo vie http://www .imdb .com http://www .ai.mit.edu/people/jrennie/20Ne wsgroups
Table 1 presents the results of MOC versus the standard mixture model for the datasets described in Section 5.1. Each reported re-sult is an average over ten trials. For the synthetic data sets, we compared our approach to thresholded Gaussian mixture models; for the text and mo vie data sets, the baselines were thresholded multinomial mixture models. Table 1 sho ws that for all domains, even though the thresholded mixture model has slightly better pre-cision in most cases, it has signicantly worse recall: therefore MOC consistently outperforms the thresholded mixture model in terms of overall F-measure, by a lar ge mar gin in most cases. Ta-ble 1 also sho ws that the performance of MOC impro ves empiri-cally as the ratio of the data set size to the number of processes increases.

Table 2 compares the performance of using the dynamicM algo-rithm versus the bounded least squares (BLS) algorithm follo wed by local search, in the M estimation step in MOC. BLS/search gets better results on precision, which is expected since BLS is the opti-mal solution for the real relaxation of the M estimation problem for the Gaussian model. Ho we ver dynamicM outperforms BLS/search on the overall F-measure. Moreo ver, BLS is only applicable for squared Euclidean distance, whereas dynamicM can be applied for M estimation with any distance function.

Detailed inspection of the results revealed that MOC gets over-lapping clusterings that are closer to the ground truths for the text and the mo vie data. For example, for mo vie-afc , the average num-ber of clusters a mo vie is assigned to is 1.19, whereas MOC cluster -ing has an average of 1.13 clusters per mo vie. The thresholded mix-ture model got posterior probability values very close to 0 or 1, as is very common in mixture model estimation for high-dimensional data: as a result there was almost no cluster overlap for various choices of the threshold value, and points were assigned to 1.00 clusters on an average in the thresholded mixture models. MOC was also able to reco ver the correct underlying multiple genres in man y cases, e.g., the mo vie  X Toy Story X  in the mo vie-afc dataset belongs to all the three genres of animation, family and comedy in this dataset, and MOC correctly put it in all 3 clusters.
The main purpose of the experiments in this section is to illus-trate that the overlapping clustering model can be generalized to work for exponential models beyond Gaussians. We have not pro-vided results on the biological datasets in this section due lack of space. Ho we ver, note that if we run our algorithm on the biologi-cal data using BLS/search and a Gaussian model, then we will get exactly the same results as the SBK model [23].
Possibility theory , developed in the fuzzy logic community , al-lows an object to  X belong X  to multiple sets in the sense of having high membership values to more than one set [5]. In particular , unlik e probabilities, the sum of membership values may be more than one [22]. One of the earlier works on overlapping cluster -ing techniques with the possibility of not clustering all points was presented in [20]. Most recent work in overlapping clustering has been primarily dri ven by the needs of microarray analysis. Several methods for obtaining overlapping gene clusters, including gene sha ving [16] and mean square residue bi-clustering [8] have been proposed. Before the PRM based SBK model was proposed, one of the most notable efforts was the the plaid model [18], wherein the gene-e xpression matrix was modeled as a superposition of several layers of plaids (subsets of genes and conditions).

Bre gman divergences were concei ved and have been extensi vely studied in the con vex optimization community [7]. Ov er the past few years, the y have been successfully applied to a variety of ma-chine learning issues, for example to unify seemingly disparate concepts of boosting and logistic regression [11]. More recently , the y have been studied in the conte xt of clustering [2].
Our formulation has some similarities to generalized linear mod-els (GLMs) [21, 10]. Ho we ver, there are a few very important dif-ferences. In GLMs [21], a multidimensional regression problem of the form d f ( Y ; f ( BZ )) is solv ed where Z is the (kno wn) input variable, Y is the (kno wn) response and f is the so-called canon-ical link function deri ved from f . The problem is to nd B and can be solv ed using iterati vely re-weighted least squares (IRLS) in the general case. Extension to the case where both B and Z are unkno wn and one alternates between updating B and Z has been studied by Collins et al. [10] while extending PCA to the exponen-tial families. Although several extensions [15] of the basic GLM model to matrix factorization have been studied, except for the well kno wn instance of non-ne gative matrix factorization (NMF) using I-di vergence [19], all formulations use the canonical link function and hence are dif ferent our formulation. Moreo ver, our model con-straints M to be a binary matrix, which is never a standard con-straint in GLMs.
In contrast to traditional partitional clustering, overlapping clus-tering allo ws items to belong to multiple clusters. In several im-portant applications in bioinformatics, text management, and other areas, overlapping clustering pro vides a more natural way to dis-cover interesting and useful classes in data. This paper has intro-duced a broad generati ve model for overlapping clustering, MOC, based on generalizing the SBK model presented in [23]. It has also pro vided a generic alternating minimization algorithm for ef-ciently and effecti vely tting this model to empirical data. Finally , we have presented experimental results on both articial data and real newsgroup and mo vie data, which is more general and effec-tive than an alternati ve  X nai ve X  method based on thresholding the results of a traditional mixture model.

A few issues regarding practical applicability of MOC needs fur -ther investig ation. It maybe often desirable to use dif ferent expo-nential family models for dif ferent subsets of features. MOC allo ws such modeling in theory , as long as the total divergence is a con vex combination of the indi vidual ones. Further , MOC can potentially benet from semi-supervision [3] as well as be extended to a co-clustering frame work [1].
 Ackno wledgements: The research was supported in part by NSF grants IIS 0325116, IIS 0307792, and an IBM PhD fello wship.
