 1. Introduction
Information is often classified into categories for efficient browsing, retrieval, and dissemination. In practice, since each category often corresponds to a specific domain, lots of documents in the real world may be entered at any time, but only a small portion of the documents may be classified into some cate-gories.Therefore,documentfiltering(DF)anddocumentclassification(DC)shouldbeintegratedtogether to autonomously classify suitable documents into suitable categories .

PreviousrelatedstudiesoftenintegratedDFandDCina X  X  X eamless X  X  X ay.Eachcategorywasassociated with a threshold to autonomously make a yes X  X o decision for each document. Conceptually, a document was X  X  X ejected X  X  byacategoryifits degreeofacceptance(DOA) withrespect tothecategory(e.g.similarity with the category or probability of belonging to the category) was lower than the category X  X  threshold; otherwiseitwas X  X  X ccepted X  X .Withthehelpofthethresholds,DFwasactuallyachievedinthecourseofDC. Each document could be classified into zero, one, or several categories.

Obviously, the performances of such classifiers heavily depend on the setting of the thresholds. A threshold that is too high (low) may mislead the classifier to reject (accept) too many documents, deteri-orating the performances of both DF and DC (i.e. classifying unsuitable documents into unsuitable cat-egories). Previous studies developed two types of thresholds: absolute thresholds and relative thresholds . AbsolutethresholdsweremanuallypredefinedwithoutconsideringtheDOAvaluesofmultipledocuments.
Forexample,aBayesianclassifiercouldacceptadocumentusingaprobabilitythresholdof0.5(Chai,Ng, &amp;Chieu,2002),whileaboostingclassifier couldaccept adocument ifthetotalweightofthosehypotheses thatacceptedthedocumentwashigherthanthetotalweightofthosehypothesesthatrejectedthedocument (Kim, Hahn, &amp; Zhang, 2000; Schapire, Singer, &amp; Singhal, 1998).

Since absolute thresholds are often quite difficult (and even implausible) to predefine, many studies preferred tuning relative thresholds. Relative thresholds were often tuned by performing some statistical analysisontheDOAvaluesofasetofdocumentsforthresholdtuning(e.g.analyzingtherankings,Yang, 2001) and document-category similarities of the threshold tuning documents (Callan, 1998; Chai et al., 2002;Lewis,Schapire,Callan,&amp;Papka,1996;Schapireetal.,1998;Yang,2001;Yang&amp;Lin,1999;Zhang &amp; Callan, 2001). The thresholds were thus relative to the DOA values of the threshold tuning documents.
They were often tuned in the hope to optimize the system X  X  performance with respect to a target measure (e.g.the F -measure, which integratesprecision P andrecall R by theformula F
As illustrated in Fig. 1, if the classifier may successfully distinguish all non-relevant documents from rel-evantdocuments(i.e.perfectlymapthemtotwoseparatepartsoftheDOAspectrum),thethresholdsmay be identified properly, maximizing the performance of the system with respect to the target measure.
Unfortunately, as noted by many previous studies (e.g. Arampatzis, Beney, Koster, &amp; van der Weide, 2000; Zhang &amp;Callan, 2001), such perfect mapping (DOA estimation)often cannot occur. In practice,we canonlyexpectthatnon-relevant(relevant)documents tend tobemappedtotheleftpart(rightpart)ofthe spectrum. Table 1 summarizes two main phenomena that cause the imperfect mapping: (1) there is no classifier that may achieve 100% correct classification for all documents (Yang, 2001), and (2) even such a classifier exists, not all documents may be classified without any controversy. Therefore, a document that one believes to be similar to (different from) a category could not always get a higher (lower) DOA value with respect to the category. These phenomena imply that unreliable DOA estimations are inevitable. An unreliable DOA estimation is actually a noise, which may mislead the thresholding process, and in turn mislead the system to classify unsuitable documents into unsuitable categories.
 Therefore,samplingreliableDOAestimationsisanecessityforthresholding,andhenceforbothDFand
DCaswell.Wethusexploretheissueinthispaper.Actually,theconceptofsamplinghasbeenemployedin previous DF and/or DC studies. However, the main focus was placed on improving the classifier (rather than the thresholding process) by selecting those documents that could not be classified properly (rather than those classified properly). For example, in the hope to make the classifier able to distinguish non-relevant documents from relevant documents, boosting (Schapire et al., 1998), adaptive resampling (Iyengar,Apte,&amp;Zhang,2000),andqueryzoning(Singhal,Mitra,&amp;Buckley,1997)tendedtoselectthose documents that had been (or were likely to be) misclassified. These methods are not suitable for thres-holding, since a document that cannot be classified properly often cannot have reliable DOA estimations either, introducing lots of noises to the thresholding process.

Sampling of reliable DOA estimations is a significant challenge. It should be both (1) parameter-free , sinceunreliableestimationsmaybescatteredintheDOAspectrum,andhencenosimpleparametermaybe defined to separate reliable estimations from unreliable estimations, and (2) adaptive , since whether an estimation is reliable heavily depends on the current status of the classifier, including the contents of individual categories, the classification methodology, and its setting (e.g. the feature set).
Wethuspresent atechniqueAS4T( a daptive s ampling for t hresholding),whichadaptstotheclassifier X  X  performance in DOA estimation, and accordingly samples reliable estimations without requiring any parameters. In the next section, AS4T is presented. Section 3 presents an experiment that was designed to investigatethecontributionsofAS4Tusingreal-worlddataunderdifferentcircumstances(e.g.differenttest datasettings,trainingdatasettings,andclassifiersettings).Theresultsshowthat,byhelpingtoderivemore properthresholds,AS4Tmaysignificantlypromotevariousclassifiers X  X erformancesonDFandDCunder the different circumstances. It may also make the classifiers X  performances more stable. The contributions are of practical significance to the classification of suitable documents into suitable categories. 2. Adaptive sampling for thresholding Fig. 2 illustrates the role of AS4T in the traditional flow of classifier building and threshold tuning. Trainingdata(documentsannotatedwiththeircorrespondingcategories)isoftenseparatedintotwosets:a classifier building set and a threshold tuning set (or a validation set; Yang, 2001). The former is used to build the classifier, while the latter is used to tune a relative threshold for each category. In threshold tuning, the classifier is invoked to estimate the DOA value of each document in the threshold tuning set.
The estimations then serve as the basis for thresholding. For each category c , before the thresholding process for c is triggered, AS4T identifies those estimations that are more likely to be reliable for c . The estimationsarethenpassedtothethresholdingprocesstofindathresholdfor c .Theclassifierbuiltandthe thresholds tuned then act together to autonomously perform integrated DF and DC for incoming docu-ments.
 Table 2 outlines the algorithm of AS4T in sampling reliable DOA estimations for each category c (ref.
InputI).Asnotedabove,beforeAS4Tisactivated,theclassifierhasbeeninvokedtoestimatetheDOAof each document (in threshold tuning set) with respect to each category. The results are stored in a set D ofthree-tuples h d ; C d ; E d i ,where d isadocument, C d
DOA estimations of d with respect to all categories (ref. Input II). Among the DOA estimations, AS4T returns a set T of reliable estimations for relevant and non-relevant documents of c (ref. Return).
ThebasicideaofAS4Tistosampletheestimationsforthosedocumentsthat(1)shouldbeaccepted(i.e. by c .Theseestimations couldbemorereliablesince theclassifier iscompetenttorecognizethecontentsof the documents. The idea is implemented as follows. For each document d , AS4T finds a winning set W , which contains the most possible j C d j categories to which d may be classified (by consulting each DOA estimation e d ; x in E d ,ref.Step2.1).If d isrelevantto c and c isreallyin W ,theestimationfor d issampled (ref.Step2.2.1).Similarly,if d isnon-relevantto c and c isreallynotin W ,theestimationfor d issampled as well (ref. Step 2.3.1). These estimations could be more reliable.

Samplingofreliableestimationsmayalsobeviewedasremovalofunreliableestimations.AS4Tactually removestwokindsofunreliableestimationsfor c :(1)theestimationsforthoserelevantdocumentsofwhich relevantdocumentsofwhichtheclassifierallows c to X  X  X ob X  X  X thercategories(i.e. c isin W ).Theexistenceof the former tends to make the threshold too low (since the thresholding process tends to allow c to accept suchdocuments,butsuchestimationstendtobelower),whiletheexistenceofthelattertendstomakethe threshold too high (since the thresholding process tends to prevent c from accepting such documents, but such estimations tend to be higher). By removing such estimations, a better threshold may be expected.
It should be noted that, AS4T might finally find that all estimations for relevant documents of c are unreliable.Thiscasemayhappenwhentheclassifierisincompetenttoclassify all therelevantdocumentsof c .Althoughthiscaseshouldberare,whenithappens,recallcannotbecalculatedinthresholding(sinceno relevant documents are sampled for thresholding, while the denominator of recall is simply the number of relevant documents). Since recall is often an essential component of the target measure (e.g. F the thresholding process operates, this case may make the thresholding process unable to find a threshold for c .Therefore,AS4TalwaysaddsarelevantdocumentwhoseDOAestimationisMax_DOA(ref.Step3), where Max_DOA is the maximum possible DOA value the classifier may assign to a document (e.g. for a classifier using cosine similarity, MAX_DOA is 1.0).

Also note that AS4T actually performs sampling by adapting to the classifier X  X  performance, which heavily depends on the settings of the classifier, including the contents of individual categories, the clas-sification methodology, and its setting (e.g. the feature set). For each kind of setting, AS4T identifies reliableDOAestimationstotunethethresholds.Thesethresholdsmaybeincorporatedintotheclassifierto achieve higher performances in both DF and DC. Moreover, AS4T determines the reliability of a DOA estimation by observing the interrelationships among the categories (i.e. Steps 2.1 X 2.3 mentioned above), withoutrequiringtosetortuneanyparameter(e.g.areliabilitythreshold)foreachindividualcategory.As notedabove,unreliableestimationsmaybescatteredintheDOAspectrum,andhenceitisdifficulttofinda simple parameter that may separate reliable estimations from unreliable estimations. 3. Experiment
An experiment was designed to investigate the contributions of AS4T. To conduct objective and thor-ough investigation, AS4T was evaluated under different circumstances, including (1) different kinds of test data, (2) different settings of training data, (3) different underlying classification methodologies, and (4) different settings for the classification methodologies. Table 3 summarizes the different circumstances, which are to be elaborated in later subsections. The experiment showed that, by helping to derive more proper thresholds, AS4T could significantly improve the classifiers X  performances and make their perfor-mances more stable.
 3.1. The experimental data
The experimental data came from Reuter-21578, which was a public collection for related researches ( http://www.daviddlewis.com/resources/testcollections/reuters21578 ). There were 135 categories (topics) in the collection. We employed the ModLewis split, which skipped unused documents and separated the documents into two parts based on their time of being written: (1) the test set, which consisted of the documentsafterApril8,1987(inclusive),and(2)the training set,whichconsistedofthedocumentsbefore
April 7, 1987 (inclusive). The test set was further split into two subsets: (1) the in-space subset, which consistedof3022testdocumentsthatbelongtosomeofthecategories(i.e.fallintothecategoryspace),and (2) the out-space subset, which consisted of 3166 documents that belong to none of the categories. They helped to investigate the systems X  performances in DC and DF, respectively. A thresholding strategy that could be of real use should help the system to both (1) properly classify in-space documents, and (2) properly filter out out-space documents.

Assuggested by previous studies (e.g. Yang, 2001), the training set was randomly splitinto two subsets aswell:the classifier building subsetandthe threshold tuning (orvalidation)subset.Theformerwasusedto buildtheclassifiers(i.e.theNaiveBayesandtheRocchioclassifiers,whicharetobedescribedlater),while thelatterwasusedtotuneathresholdforeachcategory.Therefore,toguaranteethateachcategoryhadat least one document for classifier building and one document for threshold tuning, we removed those categories that had fewer than two training documents, and hence 95 categories remained. Among the 95 categories, 12 categories had no test documents. From both theoretical and practical stand-points, these categories deserve investigation (Lewis, 1997), although they were excluded by several pre-vious studies (e.g. Chai et al., 2002; Yang, 2001). After removing those documents to which no categories were assigned (i.e. not belonging to any of the 95 categories), the training set contained 7780 documents.
Moreover,sincepreviousstudiesdidnotsuggestthewayofsettingthedocumentsforclassifierbuildingand threshold tuning, we tested two settings to conduct more thorough investigation: 50 X 50% and 80 X 20%, in which twofold and fivefold cross validation were conducted, respectively. 3.2. The evaluation criteria
The classification of in-space test documents and the filtering of out-space test documents require dif-ferent evaluation criteria. For the former, we employed precision ( P ) and recall ( R ). Both P and R were common evaluation criteria in previous studies (Lewis, 1995; Yang, 2001). P was estimated by [total number of correct classifications/total number of classifications made], while R was estimated by [total number of correct classifications/total number of correct classifications that should be made]. To integrate
P and R into a single measure, the well-known F -measure was employed: F where b isaparametergoverningtherelativeimportanceof P and R .Asinmanystudies,wesetto1(i.e.the
F 1 measure), placing the same emphasis on P and R .

Note that P , R , and F 1 were  X  X  X icro-averaged X  X  rather than  X  X  X acro-averaged X  X . Macro-averaged F the average of the F b values for individual categories, where the F basedonprecisionandrecallfor c (Yang,2001).Itwasnotemployedintheexperiment,sinceweincluded those categories that had no test documents (for the reasons noted above, ref. Section 3.1), making pre-cisionandrecallvaluesforthesecategoriesincomputable(sincethedenominatorsforcomputingthevalues couldbezero).Therefore,toobservetheperformancesforindividualcategories,weanalyzed X  X  X umulative X  X  micro-averaged data, which is to be elaborated Section 3.4.1 and illustrated in Figs. 4 and 6.
On the other hand, to evaluate the filtering of out-space test documents, we employed two criteria: filtering ratio (FR) and average number of misclassifications for misclassified out-space documents (AM). FRwas estimated by [number of out-spacedocuments filteredout/number ofout-space documents],while
AMwasestimatedby[totalnumberofmisclassifications/numberofout-spacedocumentsmisclassifiedinto the category space]. A thresholding strategy should guide the system to reject more out-space documents (i.e. higher FR) and avoid misclassifying out-space documents into many categories (i.e. lower AM). As P and R ,FRandAMcomplementedeachotherbyfocusingondifferentaspects.Forexample,supposethere are M out-spacedocuments,andsystem A misclassifies1out-spacedocumentinto2categories,andsystem
B misclassifies 2 out-space documents into 2 categories. Although both systems make 2 misclassifications, system A is better in FR (  X  M 1 = M vs.  X  M 2 = M ), while system B is better in A M(2/1 vs. 2/2). FR and A Mmay thus support more in-depth comparison of system performances.

Thein-spacecriteria(i.e. P , R ,and F 1 )andtheout-spacecriteria(i.e.FRandAM)werethusemployedin the experiment. They could collectively help to precisely identify the thresholding strategy that is really better in real-world environments in which both in-space and out-space documents could be entered (actually, the number of out-space documents is often much larger). 3.3. The systems evaluated
AS4T was tested under two kinds of classification methodologies: probability-based methodology and vector-basedmethodology.Fortheformer,weemployedaNaiveBayesclassifier(NB),whileforthelatter, weemployedaRocchioclassifier(RO).Theyallassociatedeachcategory c withaclassifier.Uponreceiving a document d , the classifier estimated the similarity between d and c (i.e. DOA of d with respect to c )in order to make a binary decision for d : accepting d or rejecting d .

NB was frequently employed and evaluated with respect to various techniques, including DF (e.g. Kim etal.,2000),non-hierarchicalDC(e.g.Larkey&amp;Croft,1996;Yang&amp;Lin,1999)andhierarchicalDC(e.g.
Dhillon,Mallela,&amp;Kumar,2002;Koller&amp;Sahami,1997;McCallum,Rosenfeld,Mitchell,&amp;Ng,1998).It wasshowntobecompetitive(andevenbetter)whencomparedwithvariousstate-of-the-artDCtechniques (e.g.neuralnetworksandsupportvectormachine;Yang&amp;Lin,1999;Dhillonetal.,2002).Morespecially, NB pre-estimated the conditional probability P  X  w j c  X  for every feature w and category c (with standard
Laplace smoothing to avoid the probabilities of zero). The  X  X  X imilarity X  X  between a document d and a category c was P  X  c  X  where TF  X  w ; d  X  was the times a feature w occurs in d , and notc was a dummy category covering all doc-uments irrelevant to c .
 ROisagoodinstanceofvector-basedclassification.ItwascommonlyemployedinDC(e.g.Wu,Phang,
Liu, &amp;Li, 2002), DF (e.g. Schapireetal., 1998; Singhal etal., 1997), andretrieval (e.g.Iwayama, 2000)as well.ROconstructedavectorforeachcategory,andthesimilaritybetweenadocument d andacategory c was estimated using the cosine similarity between the vector of d and the vector of c . More specially, the vectorforacategory c wasconstructedbyconsideringbothrelevantdocumentsandnon-relevantdocuments of c : g 1 (i.e.thedocumentsin c ),while N wasthesetofvectorsfornon-relevantdocuments(i.e.thedocumentsnotin setting was promising.

Both NB and RO required a fixed (predefined) feature set, which was built using the documents for (chi-square)weightingtechnique.Thetechniquehasbeenshowntobemorepromisingthanothers(Yang&amp;
Pedersen, 1997). As noted above, there is no perfect way to determine the size of the feature set. Setting a properfeaturesetsizewasoftenanexperimentalissue(e.g.McCallumetal.,1998;Yang&amp;Pedersen,1997).
Therefore,toconductmorethoroughinvestigation,wetriedmanyfeaturesetsizes,andreportedtheresults under four different feature set sizes: 2000, 4000, 8000, and 12,000.

TomakeDFandDCdecisions,bothNBandROalsorequiredathresholdingstrategytosetathreshold for each category. As in many previous studies (e.g. Callan, 1998; Chai et al., 2002; Lewis et al., 1996; Schapireetal.,1998;Yang,2001;Yang&amp;Lin,1999;Zhang&amp;Callan,2001),NBandROwereallowedto tune a relative threshold for each category by analyzing document-category similarities. The threshold tuning documents were used to tune each relative threshold. As suggested by many studies (e.g. Yang, 2001), the thresholds were tuned in the hope to optimize the system X  X  performance with respect to F and RO with such a thresholding strategy were named NB+Thresholding and RO+Thresholding, respectively. Moreover, we also designed a version of NB that employed a fixed threshold of 0.5. This versionofNBwasnamedNB+FixedThreshold.Itwastestedinseveralpreviousstudiesaswell(e.g.Chai et al., 2002). NB+FixedThreshold and NB+Thresholding served as the baselines to measure the contri-bution of AS4T to NB (i.e. NB+AS4T), while RO+Thresholding served as the baseline to measure the contributions of AS4T to RO (i.e. RO+AS4T). 3.4. Results and analysis
Experimental results are discussed from three aspects: (1) results on in-space test documents (for investigating the contributions of AS4T on DC), (2) results on out-space documents (for investigating the contributions of AS4T on DF), and (3) standard deviations of the performances (for investigating the stability of the systems enhanced with AS4T). 3.4.1. Results on in-space test documents
Fig. 3 shows the effects of introducing AS4T to NB when processing in-space test documents under the 50 X 50% training data split. The results showed that NB+Thresholding was poor in precision, while
NB+FixedThresholdandNB+AS4Thadsimilarperformances,whichtendedtodeclinewhenthefeature set size was larger than 8000. A detailed analysis showed that, since NB tended to produce extremely high ( 1.0) or low ( 0.0) document-category similarity values (Bennett, 2000), setting a fixed threshold of 0.5 (as NB+FixedThreshold did) could be a good way. On the other hand, when NB was allowed to tune a threshold (as NB+Thresholding did), it would become sensitive to the erroneous similarity estimations, which were often far away from their correct values (since NB tended to produce extremely high or low estimations).

Fig. 4 illustrates the contributions of AS4T to NB+Thresholding. We sorted the categories in descending order according to their sizes (i.e. number of relevant training documents), and compared the  X  X  X umulative X  X  X erformancesofNB+AS4TandNB+Thresholdingwhenthesizeofthefeaturesetwas2000 (inthissetting,NB+Thresholdingachieveditshighest F 1 ).Theperformanceswerecumulativeinthesense that, for each data point  X  i ; p  X  in the performance curves, p (i.e. P , R ,or F thelargest i categoriesonly(when i  X  95,theperformancereadingswerethesameasthosereportedinFig. 3). The cumulative data facilitated the analysis of the performances for individual categories (recall that macro-averaged criteria, which aim to evaluate the performances for individual categories, could not be employed in the experiment, ref. Section 3.2). The results indicated that AS4T contributed significant improvementsonsomecategories(ref.thosecategoriesonwhichthereweresignificantperformancedrops).
AnalysesshowedthatthesecategoriescontainedahigherpercentageofdocumentswhoseDOAestimations were unreliable (inappropriate). Sampling reliable estimations is thus essential for thresholding. AS4T significantly improved thresholding for RO as well. Fig. 5 shows the performances of
RO+ThresholdingandRO+AS4Tonin-spacetestdocumentsunderthe50 X 50%trainingdatasplit.Fig.6 showstheircumulativeperformanceswhenthefeaturesetsizewas4000(inthissetting,RO+Thresholding achieveditshighest F 1 ).Again,AS4Tcontributedsignificantimprovementsonsomecategories.Inaddition to the cases similar to those mentioned above for NB+Thresholding, we also found several cases where
RO+Thresholding assigned higher DOA estimations to some documents, which were non-relevant to the categories (e.g. the category  X  X  X alm-oil X  X ), although they mentioned some key concepts (e.g.  X  X  X alm X  X  and  X  X  X alm oil X  X ) of the categories. The DOA estimations were thus not reliable, and could lead to a threshold that was too high. AS4T removed the estimations to avoid misleading the thresholding process.
Table 4 summarizes DC performances (in F 1 ) of all systems with the best settings of their feature sets found in the above experiments. The performances under different splits of training data (i.e. 50 X 50% and 80 X 20%)arelistedseparately.Sinceweperformedtwofoldandfivefoldcrossvalidationfor50 X 50%and80 X  20% splits respectively, Table 4 lists the average performances of the systems. The results showed that contributions of AS4T under the 80 X 20% split were quite similar to those under the 50 X 50% split. More-over, AS4T also made NB and RO able to achieve similar performances (ref. the performances of
NB+AS4T and RO+AS4T). These results confirmed the applicability of AS4T to different classification methodologies under different training environments. 3.4.2. Results on out-space test documents
Fig. 7 illustrates the contributions of AS4T to NB on filtering out out-space test documents under the 50 X 50% training data split. As noted above, for out-space data, we were concerned with the filtering ratio (FR)andtheaveragenumberofmisclassificationsformisclassifiedout-spacedocuments(AM).Theresults showed that, the performances of NB+Thresholding were poorer than NB+AS4T and NB+Fixed-
Threshold. To compare the performances of NB+AS4T and NB+FixedThreshold, we were interested in the case where the feature set size was 8000. This was because, when the feature set size was 8000,
NB+FixedThreshold achieved its better performances on both DC and DF (ref. Figs. 3 and 7, respec-tively),andadditionalexperimentsshowedthatwhenthefeaturesetbecamelarger,itsperformanceonDC declined. Under such a better setting for NB+FixedThreshold, NB+AS4T significantly outperformed
NB+FixedThreshold in both FR (0.34 vs. 0.28, or 21% improvement) and A M(1.13 vs. 1.23, or 8% reduction), without sacrificing DC performances (recall that NB+AS4T and NB+FixedThreshold had quite similar performances on DC, recall Fig. 3).
 AS4T could significantly improve DF performances of RO as well. Fig. 8 shows the performances of
RO+ThresholdingandRO+AS4Tonout-spacetestdocumentsunderthe50 X 50%trainingdatasplit.We wereinterestedinthecasewherethefeaturesetsizewas12,000,sinceunderthissetting,RO+Thresholding achievedits betterperformances onboth DCandDF (ref.Figs. 5 and8, respectively).The resultsshowed that, under this setting, AS4T significantly outperformed RO+Thresholding in both FR (0.61 vs. 0.26, or 135%improvement)andAM(1.21vs.1.87,or35%reduction),andatthesametimesignificantlyimproved DC performances of RO+Thresholding (recall Fig. 5).

Itisinterestingtonotethat,whencomparedwithin-spacedtestdata,out-spacetestdatawaslessrelated to the data for classifier building and threshold tuning. Therefore, the improvements on out-space data further justified the contributions of AS4T: the thresholds derived using AS4T could be more proper. The contributionsarealsoofpracticalsignificance,sinceinpractice,therearemuchmoreout-spacedocuments than in-space documents. A system for integrated DF and DC requires more proper thresholds.
ItisalsointerestingtocomparethecontributionsofAS4TtoNBandROonbothDCandDF.Asnoted above, NB+AS4T and RO+AS4T achieved similar DC performances (recall Table 4). However, the re-sults on out-space documents showed that RO+AS4T obviously had better DF performances than NB+AS4T (0.61 vs. 0.34 in FR), making RO+AS4T more competent in conducting integrated DC and
DF. 3.4.3. Standard deviations of the performances
InadditiontothecontributionsonDFandDC,wewerealsoconcernedwiththe stability ofthesystems enhanced with AS4T (i.e. NB+AS4T and RO+AS4T). Together with the above-mentioned investigation onDC andDF, the contributions ofAS4T may be furtherjustified ifthe system enhanced with AS4T has more stable performances under different settings.

Table 5 summarizes all systems X  performance deviations (in F 50%split).ItisinterestingtocomparetheperformancedeviationsofthesystemsenhancedwithAS4T(i.e. NB+AS4T and RO+AS4T) and their corresponding best baselines (i.e. NB+FixedThreshold and
RO+Thresholding). The results showed that, RO+AS4T was more stable than RO+Thresholding in all cases, and NB+AS4T was more stable than NB+FixedThreshold when NB+FixedThreshold was equipped with suitable feature sets (recall that NB+FixedThreshold preferred a feature set of size 4000 X  12,000).Moreover,Table 6summarizesall systems X  performance deviations(in F ofthefeatureset.Again,theresultsshowedthatthesystemsenhancedwithAS4Thadsmallerperformance deviations than their corresponding best baselines.

AS4T could thus make the classifiers X  performances more stable in facing different settings of training data and feature sets. Together with the contributions on DF and DC, AS4T was shown to be helpful in deriving more proper thresholds for the various classifiers to achieve both better and more stable perfor-mances. 4. Conclusion and extension
Thresholdingisafundamentalbasistoachieve high-performanceintegratedDFandDC.Byderivinga properthresholdforeachcategory,suitabledocumentsmaybeclassifiedtosuitablecategories,facilitating boththeindexingandthedissemination ofinformation. Unfortunately,thresholdingisoften basedonthe classifier X  X DOAestimations,whichcannotalwaysbereliable,duetotwocommonphenomena:noclassifier may avoid all errors, and not all documents may be classified without any controversy. Unreliable esti-mations are actually noises that may mislead the thresholding process to derive improper thresholds (too high or too low), which in turn force the system to classify many unsuitable documents into unsuitable categories.

We thus present a technique AS4T to sample reliable DOA estimations for proper thresholding. From the viewpoint of operation modes, AS4T is both adaptive and parameter-free in the sense that it helps the thresholding process to derive proper thresholds by adapting to the classifier X  X  status, without needing to define any parameters. From the viewpoint of operation performances, AS4T may significantly promote various classifiers X  performances in DF and DC, and make the classifiers X  performances more stable. The contributions are of practical significance for real-world integrated DF and DC.

AlthoughthecontributionsofAS4Thavebeenjustifiedintwodifferentclassificationmethodologies(i.e. the probability-based NB and the vector-based RO), it is interesting to investigate the contributions of
AS4T to other classification methodologies. We expect that AS4T is general and applicable to various classification methodologies, since its development is motivated by the common phenomena (i.e. no clas-sifier may avoid all errors, and not all documents may be classified without any controversy) faced by various classifiers. The investigation aims to justify the expectation.
 References
