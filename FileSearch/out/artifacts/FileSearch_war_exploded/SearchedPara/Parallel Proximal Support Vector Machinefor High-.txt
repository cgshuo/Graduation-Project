 Proximal support vector machine (PSVM) is a simple but effective classifier, especially for solving large-scale data clas-sification problems. An inherent deficiency of PSVM lies on its inefficiency for dealing with high-dimensional data. In this paper, we propose a parallel version of PSVM (PPSVM). Based on random dimensionality partitioning, PPSVM can obtain partitioned local model parameters in parallel, with combined parameters to form the final global solution. In fact, PPSVM enjoys two properties: 1) It can calculate model parameters in parallel and is therefore a fast learning method with theoretically proved convergence; and 2) It can avoid the inversion of large matrix, which makes it suitable for high-dimensional data. In the paper, we also propose a random PPSVM with randomly partitioned data in each iteration to improve the performance of PSVM. Experimen-tal results on real-world data demonstrate that the proposed methods can obtain similar or even better prediction accu-racy than PSVM with much better runtime efficiency. I.5.4 [ Pattern Recognition ]: Applications X  computer vi-sion, text processing Algorithms Corresp onding author.
 Proximal support vector machine, parallel, high dimension-ality
Learning and classifying large scale high dimensional data is a challenging task for machine learning community, where research commonly refers to incremental or parallel meth-ods for solutions [4, 7]. In general, incremental methods [7] partition the original data into data blocks, and sequentially handle each blocks by using a single processor based learn-ing model. Alternatively, a parallel learning model [4] takes the advantage of multi-processors computing infrastructure to speedup the learning process. If the data blocks are parti-tioned reasonably, parallel methods can obtain much better efficiency than a single processor based model.

Similar to incremental methods, parallel model learning can also be achieved by reconstructing the traditional batch methods. In this study, we focus on the proximal support vector machine (PSVM) [2]. PSVM is a fast method for dealing with large-scale data. Because PSVM has to cal-culate the matrix inversion with its size determined by the dimensionality, it is commonly regarded not appropriate for solving high-dimensional classification problems.
In this paper, we propose a parallel PSVM (PPSVM) method which intends to enable PSVM for high-dimensional data classification. PPSVM directly partitions the feature space into several blocks. Each block contains all training instances but only a subset of features. As a result, the orig-inal high-dimensional PSVM problem is transformed into several low-dimensional ones which can be processed in par-allel. PPSVM is an iterative method and its convergence is validated in the paper. Furthermore, we propose a random version of PPSVM with the feature space randomly parti-tioned, in each iteration, to improve the prediction accuracy. Experimental comparisons demonstrate that PPSVM and its random version are efficient for obtaining better predic-tion accuracies and faster convergence speed than PSVM. PSVM is a simple yet effective SVM classifier [2]. Assume X = [ x 1 ; x 2 ; :::; x n ] T 2 R n m is the instance matrix consist-ing of n instance column vectors x i (1 i n ). The symbol D is diagonal matrix for assigning the label to each instance, the symbol 1 is a column vector in n -dimensional space. The objective function describes an unconstrained optimization problem [2, 1], where # 2 R m 1 is a parameter vector, is a scalar, is a positive tuning parameter, and y is an error variable. Setting the gradient with respect to [ # T ; ] T and solving the equations # and [2, 1], we have, where ! equals [ # T ; ] T , I is an identity matrix, the symbol Z = [ X; 1 ] can be viewed as an extended sample matrix, A equals Z T Z , and L = D 1 is the column vector L assigning a class label +1 or 1 for each instance. The parameters # and form the optimal separating hyperplane [6, 2, 1]. The hyperplanes x T # = 1 determinate the width of margin, and the positive and negative samples are pushed apart by optimizing the objective function (1).

Note that, PSVM needs to calculate the inversion of ma-trix  X  A = I + A as defined in Eq. (2), which makes PSVM ineffective for large matrix  X  A whose size is determined by the feature dimension.
Proximal SVM is essentially equivalent to solve the fol-lowing problem, The final solution is given in Eq. (2), without considering whether the dimensionality m is much larger than the num-ber of instances n or not. Section 2 indicates that PSVM is not efficient for handling high-dimensional data. To solve this problem, we observe that the terms Z! and L in Eq. (3) can be decomposed as follows, such that where the size of  X  X abel X  vector L i is the same as L . Then, if we can have all solutions in equations it is straightforward to integrate parameters ! i (1 i t ) of Eq. (7) as the final model ! ,
Algorithm 1 lists detailed procedures of PPSVM. In each iteration, a parallel node ( i.e. , a processor) calculates the partial model parameter in Step 6 and the master node col-lects partial models to form global model parameters in Step Algori thm 1 Pseudocode of PPSVM algorithm 8. The lab el vector is then updated in Step 9 for next iter-ation.
 The convergence of PPSVM: Assume m &gt; n , Eq. (3) is underdetermined equation and Eq. (2) is its fitting solu-tion. By contrast, Eq. (6) depicts overdetermined equation set, and Eq. (7) is the corresponding solutions. Since Eq. (3) is underdetermined, it can fit the label vector L much better than Eq. (7) for overdetermined equation (6). In each iteration, it is inevitable for Eq. (7) to produce fitting error. Actually, Eq. (7) describes the least square solutions, which is the best fitting when minimizing the sum of square of differences between model and original data.
 Theorem 1. PPSVM algorithm is convergent.

Proof. Assume that, in the first iteration, the fitting difference is e (1), we have where ! 1 is the model parameter about L collected after the first iteration. Then, W = ! 1 , therefore, the difference for model W is also equal to e (1).
 In the second iteration, the label L is updated by L ZW = L Z! 1 , like Eq. (8), the difference e (2) is where ! 2 is the model parameter about L getting after the second iteration. Since new L is the residual error of old L and ZW , we have e (2) e (1). Eq. (9) also implies that the fitting difference between L and ZW for current model parameter W is also e (2). Therefore, for the succedent iter-ation, the fitting difference of W for L satisfies the following inequation, where 1 &lt; j k . The above inequity indicates that the proposed algorithm is convergent.
 The time co mplexity of PPSVM: In each iteration, Al-gorithm 1 needs to solve t subproblems, the corresponding complexity is O ( n 2 d + d 3 + d 2 + dn ) where d is the dimension-ality of Z i and d = m t . To solve overdetermined problems, we ha ve d &lt; n , and the complexity is about O ( dn 2 + d Assume that, for each iteration, the communication cost is in each iteration. In each iteration, the complexity is O ( dn 2 + d 3 + t ) which is equal to O ( n 2 m t + m 3 t 3 complexity is approximately equal to O ( k ( n 2 m t + m 3 For PS VM, the time complexity is about O ( n 2 m + m 3 ) which is much more expensive than that of PPSVM for similar k and t and big dimension number m . Moreover, if the dimen-sionality m is too large, PSVM can not calculate the matrix inversion. In contrast, because the dimensionality d is much smaller than m , PPSVM can directly deal with it. The random version of PPSVM: We can easily convert PPSVM to a random version. We only need to adjust the Step 5: randomly partition the data, and Step 8: sort and add !  X  X  to produce W , which result in better performances than the non-random version.
We use PSVM as the benchmark method, and mainly re-port the learning efficiency and prediction accuracy with re-spect to different benchmark data sets. All experiments are validated using Matlab 6.5 software, on PCs with 2.80 GHz CPU and 2 GB memory. Our test bed consists of two real-world data sets: 20 News Group (20NG) [3] and ORL face data set. Both of them have high dimensionality. 20NG data sets have a hierarchical structure. This data set includes 20 classes, each instance has 74000 dimensional-ity. Because PSVM is ineffective for high dimensional data (and often runs out of memory in our experiments), we use some small subsets from 20NG, shown in Table 1[5], to make valid comparisons with the proposed methods.
 Figure 1: Samples for original ORL images(1st-5th columns) and the respective texture images(6th-10th columns). We use the\photographic plate like" as texture for sparse description.

Cambridge ORL face data set includes 400 images from 40 persons and each person has 10 pictures. Each image has a fixed size 112 92, with the corresponding pixel vector (10 304 dimensions) directly used as the features. Fig. 1 shows the 10 pictures of one person in the first five columns. To compare the performance of the three methods (PSVM, PPSVM, and Random PPSVM), we report their average performance over ten time repetitions for each data set. In each repetition, for both data sets, 80% data are randomly chosen for model training, and the rest data are used for testing. For ORL data, we first extract the corresponding texture. Then, the original data become sparse, as shown in the last five columns in Fig. 1.

For parallel algorithms, we must consider the communi-cation cost between the master processor and each paral-lel node. In our test environment, the Local Area Net-work(LAN) speed is 100MBps. Transferring 1MB (include the fitting difference L i and parameter ! i of each subprob-lem, etc), the theoretical communication time is about 0.01 second and the actual time collected from our test en-vironment is about = 0 : 3 second in each iteration. In our lab, 40 PCs can be used. So, the largest node number t is 40.

Fig. 2 reports the runtime of three algorithms with re-spect to the number of parallel nodes. Because PSVM is a batch method, its runtime is constant. For the proposed two algorithms, the runtime includes two parts: 1) parallel calculation time, and 2) communication cost. As a result, the two parts can be balanced, as shown in Fig. 2. In this trial, the data set is all the data about talk topic from 20NG, including 4000 instances. To compare the efficiency of the three algorithms, we choose 4000 feature dimensions with non-zero column vector values. Fig. 2 indicates that, the proposed methods are much faster than PSVM (In the following experiments, we use 40 nodes). Figure 2: Runtime on Talk data from 20NG with n = m = 4000 . In Fig. 3, we report the results of three algorithms on Multi10 1 ; 2 ; 3 dat a sets, where Fig. 3(a) shows the fitting difference, Fig. 3(b), (c) and (d) show the prediction ac-curacy on Multi10 1 ; 2 ; 3, respecti vely. From Fig. 3, we observe that, as the iteration increases, 1) the fitting differ-ence approaches to that of PSVM; and 2) the proposed two algorithms achieve better prediction accuracy than PSVM.
From Fig. 3, we also observe that, the random version has lower fitting difference and better prediction accuracy than PPSVM. This is mainly attributed to the random feature partitioning in each iteration, where the random combina-tion of the fitting lines makes random PPSVM more effective for nonlinear classifications.
 The dimensionality of the data used above is at most 4000. So all three algorithms can be directly used to train the prediction model. When the dimensionality increases, it is inefficient or even infeasible for PSVM to train prediction model.
 ORL image dataset has 10304 (112 92) dimensions and Fig. 4 shows the performance of the proposed algorithms on all 400 face data, the corresponding results are similar to those in Fig. 3. More specifically, Fig. 4 (a) indicates that the fitting differences from PPSVM and its random version (c) Pred iction accuracy on
Multi10 2 Figure 3: Results on Multi10 1 , Multi 10 2 and Multi1 0 3 . are ap proximately equal to 0, showing better convergence effectiveness than Fig. 3(a). Fig. 4 (b) shows similar results to that in Fig. 3 (b), (c) and (d): random PPSVM achieves better performance than PPSVM.
In Fig. 5, we report the results on the talk topic from 20NG. This topic includes 4 categories, each containing 1000 instances. We randomly choose 10000 dimensions for com-parisons. Fig. 5 shows similar results as those in Figs. 3 and 4.
In this paper, we proposed a Parallel PSVM (PPSVM) and its random version for high-dimensional data classifica-tion. The proposed methods have the following properties: 1) they are parallel in nature and are fast for handling high Figure 5: Results of Talk data set from 20NG, in-cluding 4000 instances in 10000 dimensional spaces. dimensional data; 2) they can solve large matrix inversion in an effective way; 3) they are convergent; and 4) they can obtain better prediction accuracy than the basic PSVM, es-pecially using the random version of PPSVM.
 This work was supported in part by the Australian Research Council Discovery Project Grant (No. DP1093762), the Na-tional 973 Program (No. 2010CB327906), the National Sci-ence Foundation of China (NSFC) Grant (No. 61170223), the China Postdoctoral Science Foundation (CPSF) Grant (No. 2011M501189), the State Key Laboratory Program Grant (No. RCS2009K003), and the National High Tech-nology Research and Development Program of China (No. 2009AA01A346). [1] G. Fung and O. Mangasarian. Incremental support [2] G. Fung and O. L. Mangasarian. Proximal support [3] K. Lang. Newsweeder: Learning to filter netnews. In [4] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [5] N. Slonim, N. Friedman, and N. Tishby. Unsupervised [6] V. N. Vapnik. The Nature of Statistical Learning [7] Z. Zhu, X. Zhu, Y.-F. Guo, and X. Xue. Transfer
