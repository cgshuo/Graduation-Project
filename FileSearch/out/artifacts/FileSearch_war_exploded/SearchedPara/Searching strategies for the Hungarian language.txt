 1. Introduction
The majority of European languages belong to the Indo X  X uropean family and thus they share various syn-tactic features as well as words in their basic lexicon, as least from a phonological point of view. The Hungar-ian, Finnish and Basque languages however have fewer characteristics in common with these languages. The
English lexicon for example has only a few words with Hungarian origins (e.g., saber, paprika, goulash), while the Hungarian lexicon contains many more words borrowed from the English language (e.g., modern, inter-view, sport, jury, pedigree, computer, internet).

During the first CLEF ( www.clef-campaign.org ) evaluation campaigns ( Peters et al., 2006 ), the emphasis was placed on the Roman (e.g., French, Italian, and Spanish) and Germanic (e.g, German, Dutch, and Swed-ish) family of languages ( Sproat, 1992 ). From an IR point of view these languages are closer to the English while Hungarian represents a special case, especially given its more complex morphology and agglutinative aspects. Moreover, only a few IR experiments have been conducted with the Hungarian language. In fact, not until 2005 did the CLEF evaluation forum include this language in one of its tracks, when a real and rea-sonably large test collection respecting the required international standards was developed ( Harman, 2005;
Buckley &amp; Voorhees, 2005; Gordon &amp; Pathak, 1999 ). The main objective of our paper is therefore to carry out studies on the Hungarian language. This paper is divided as follows. Section 2 presents the context and related works, while Section 3 depicts the main characteristics of the test collection. Section 4 briefly describes the IR models used during our experiments. Section 5 evaluates three stemming approaches together with a comparison of the retrieval effectiveness of word-based schemes, and those where words are automatic decom-pounded. The main findings of this paper are summarized in Section 6 . 2. Context and related work
In order to define pertinent matches between search keywords and documents, very frequently occurring terms in any given language are usually removed. These words tend not to have clear and important meanings (e.g., the, in, but, some). For the Hungarian language and following the guidelines suggested by Fox (1990) , we first created a list of the top 200 most frequently occurring words found in the corpus, from which certain words were removed (e.g., police, minister, president, Magyar). To this list we manually added articles (e.g., etc.), possessive pronouns (e.g., my =  X  X  X nye  X  m X  X ,  X  X  X nye  X  mek X  X , ... ), prepositions be =  X  X  X enni X  X , are =  X  X  X annak X  X , has =  X  X  X eki van X  X , ... ). The final stopword list we suggest contained 761
Hungarian terms, a greater number than those usually proposed for the English language (e.g., Fox, 1990 suggested 421 terms, while the SMART system included 571). Our list or determinants may occur in numerous forms, reflecting the fact that Hungarian grammar comprises several grammatical cases.

On the other hand it must be recognized these lists were established on the basis of certain arbitrary deci-sions ( Savoy, 1999 ), even though commercial information systems tend to adopt a very conservative approach with only a few stopwords. The DIALOG system for example uses only nine items (namely  X  X  X n X  X ,  X  X  X nd X  X ,  X  X  X y X  X ,  X  X  X or X  X ,  X  X  X rom X  X ,  X  X  X f X  X ,  X  X  X he X  X ,  X  X  X o X  X , and  X  X  X ith X  X ) ( Harter, 1986 ). Another example is the WIN system, which ignores the single word ( X  X  X he X  X ) when indexing documents, but a larger stopword list may be used when analyzing the request ( Moulinier, 2004 ).

Once high-frequency words were removed, an indexing procedure generally applied a stemming algorithm in order to conflate word variants into the same stem or root. In developing such a procedure, we may define a light stemming approach whereby the stemmer removes only inflectional suffixes related to number (singular vs. plural), gender (masculine, feminine) or representing grammatical cases (e.g. in Latin  X  X  X osae X  X  and  X  X  X osa-rum X  X  are related to the nominative form  X  X  X osa X  X ). We could also remove derivational suffixes, usually those used to form new words belonging to another part of speech (e.g., power, powerful, powerlessly).
In the rest of this section, we report on the main morphological difficulties characteristic of the Hungarian language (Section 2.1 ) and describe how we could generally derive a stemmer for those languages having more complex morphologies (Section 2.2 ). Finally, we will explain how compound words have a significant impact on retrieval effectiveness. 2.1. Main aspects of Hungarian morphology
The Hungarian language shares certain similarities with the Finnish language. Although both languages do not strictly belong to the same family, they can be viewed as cousins. Comparable to the Latin or the German languages, Hungarian is characterized by many grammatical cases (23 in total, although some are limited to a set of nouns or appear only in fixed and predefined forms). Each Hungarian case has its own unambiguous plural, as in  X  X (I see) the houses/the fires X  X ). Grammatical cases are often denoted through adding a suffix to nouns, and also to names. The Hungarian name for the city of Paris is  X  X  X a  X  rizs X  X , and thus we may encounter from inside) Paris), with these forms corresponding to the English preposition  X  X  X n X  X , respectively  X  X  X rom X  X .
Three other grammatical cases correspond to the English preposition  X  X  X ver X  X , and three other forms are related to the meaning of  X  X  X ear X  X . From these examples, we usually find that English prepositions do not have a direct translation, but rather their meaning appears in a grammatical case and therefore in the corresponding suffix. The attachment of suffixes is not limited to geographic names. For example, with the proper Hungarian or  X  X  X rd } osne  X   X  X  (the Erdos X  wife).

The Hungarian suffixes may also be used in conjunction with possessive pronouns (my, their) as in  X  X  X a  X  za-mat  X  X  ( X  X (I see) my house X  X ), with the suffix  X -(a)m X  used to indicate the English pronoun  X  X  X y X  X . Thus, a suffix could represent four types of information; namely case, possessive pronoun, number (singular/plural), and the
Combining these suffixes may produce forms such as  X  X  X a  X  zaimat  X  X  ( X  X (I see) my houses X  X ) where the plural form of the houses X  X ).

Finally, the morphological rules are not too strict and the inclusion of vowels is sometimes allowed in order agglutinative aspects may be found in other languages such as Turkish, where the noun  X  X  X v X  X  (house) may take on the form  X  X  X vler X  X  (the houses),  X  X  X vlerim X  X  (my houses) and  X  X  X vlerimde X  X  (in my houses).

From an IR point of view, certain Hungarian linguistic aspects are easier to process. For example, a gender distinction (feminine/masculine/neutral) is not attached to a noun (as in English with she/he/it =  X  X  } o X  X  or with vira  X  gok  X  X  (the beautiful flowers). The only exception is the plural form used with a copulative verb (e.g., the flowers are beautiful =  X  X  X  vira  X  gok sze  X  pek  X  X ). 2.2. Stemming strategies
In the IR domain we usually assume that stemming is an effective means of enhancing retrieval efficiency by conflating several different word variants into a common form. Most stemming approaches achieve this through applying morphological rules for the language involved (e.g., see Lovins, 1968; Porter, 1980 for the English language). In such cases suffix removal is also controlled through the adjunct of quantitative restrictions (e.g.,  X -ing X  would be removed if the resulting stem had more than three letters as in  X  X  X unning X  X , but not in  X  X  X ing X  X ) or qualitative restrictions (e.g.,  X -ize X  would be removed if the resulting stem did not end with  X  X  X  as in  X  X  X eize X  X ). Moreover, certain ad hoc spelling correction rules are applied to improve conflation accuracy (e.g.,  X  X  X unning X  X  gives  X  X  X un X  X  and not  X  X  X unn X  X ), due to certain irregular grammar rules, usually applied to facilitate easier pronunciation.

Such simple stemming procedures (algorithmic stemming) ignore word meanings and tend to make errors, usually due to over-stemming (e.g.,  X  X  X eneral X  X  becomes  X  X  X ener X  X , and  X  X  X rganization X  X  is reduced to  X  X  X rgan X  X ) or to under-stemming (e.g., with Porter X  X  stemmer, the words  X  X  X reate X  X  and  X  X  X reation X  X  do not conflate to the same root). For this reason the use of an on-line dictionary has been suggested as a means of obtaining better con-flation ( Krovetz, 1993 ).

Compared to other languages having more complex morphologies ( Sproat, 1992 ), English is considered quite simple and the use of a dictionary to correct stemming procedures could be more helpful for those other languages such as French ( Savoy, 1993 ). When a language has an even more complex morphology, deeper ian ( Hala  X  scy, 2006 )), where lexical stemmers are clearly more elaborate and not always freely available (e.g.,
Xelda system at Xerox). They are more labor intensive and their implementation is complex. Moreover their use depends on a large lexicon and a complete grammar for the language involved. These application also requires more processing time and could thus be problematic, especially when document collections are very large and dynamic (e.g., within a commercial search engine on the Web). Additionally, lexical stemmers must be capable of handling unknown words such as geographical names, products, proper names or acronyms (out-of-vocabulary problems). Lexical stemmers thus cannot be viewed as error-free approaches. Finally, it must be recognized that when inspecting language usage and real corpora, the observed morphological vari-ations are less extreme than those that might be imagined when inspecting the grammar. Kettunen and Airo (2006) indicate for example that in theory Finnish nouns have around 2000 different forms, yet in actual col-lections the occurrence of most of these forms is rare. As a matter of fact in Finnish, 84 X 88% of the occur-rences of inflected nouns are generated by only six out of a possible 14 cases.

While stemming schemes are normally designed to work with general texts, some may also be especially designed for a specific domain (e.g., in medicine) or a given document collection, such as that developed by
Xu and Croft (1998) , which used a corpus-based approach. This more closely reflects language usage (includ-ing word frequencies and other co-occurrence statistics), instead of a set of morphological rules in which the frequency of each rule (and therefore its underlying importance) is not precisely known.

In analyzing the IR stemming performance, Harman (1991) demonstrated that no statistically significant improvements could be obtained from applying any of three different stemming strategies, namely those of
Lovins (1968), and Porter (1980) as well as a basic stemming technique conflating singular and plural English word forms (and based on three rules). A query-by-query analysis revealed that stemming did indeed affect performance, even though the number of queries showing improvements was nearly equal to the number of queries resulting in decreased performance. Other studies ( Hull, 1996 ), usually limited to one language (Eng-lish), show that modest improvements can result from applying a stemmer. When compared with approaches that ignored stemming, differences were not always statistically significant.

It was also surprising to note that during the last CLEF evaluation campaigns ( Peters et al., 2006 ), partic-ipants suggested a limited number of stemmers and only attempted to compare a few of them. For example, when evaluating the two statistical stemmers used for five languages, Di Nunzio, Ferro, Melucci, and Orio (2004) showed that relative retrieval performances would vary for each of these languages. This means that any given stemming approach may work well for one language yet not for another. When compared to statis-tical stemmers, Porter X  X  stemmers seem to work slightly better. For German, Braschler and Ripplinger (2004) showed that for short queries stemming may enhance mean average precision by 23%, compared to 11% for longer queries. Finally, Tomlinson (2004) evaluated the differences between Porter X  X  stemmer and the lexical stemmer (in which stemming is based on a dictionary of the corresponding language and a more complex mor-phological analysis). Moreover, Tomlinson (2004) found that for the Finnish and German languages, the lex-ical stemmer tended to produce better results statistically, while for the Dutch, Russian, Spanish, French and
English languages performance differences were small and insignificant. For the Swedish language, the algorith-mic stemmer produced mean average precision that was statistically better than a lexical stemming approach. 2.3. Compound words
Compound word construction (e.g., handgun, viewfinder) is another morphological characteristic that may have an impact on retrieval effectiveness. Most European languages involve some form of compound con-struction, indicated by a hyphen in some cases (e.g, in Hungarian  X  X  X o  X  ze  X  p-Euro  X  pa X  X  (Central Europe) or in
French  X  X  X orte-clefs X  X  (key ring)) or by the suffix attached to the genitive case (e.g., in German with the  X  X -s X  X  suffix in  X  X  X ebensversicherungsgesellschaftsangestellter X  X  =  X  X  X eben X  X  (life) +  X  X -s X  X  +  X  X  X ersicherung X  X  (insur-ance) +  X  X -s X  X  +  X  X  X esellschaft X  X  (company) +  X  X -s X  X  +  X  X  X ngestellter X  X  (employee)).
 In general however no  X  X  X lue X  X  is used to build compound forms from two or more words, such as in the English (viewpoint) or the German ( X  X  X ankangestelltenlohn X  X  =  X  X  X ank X  X  +  X  X  X ngestellter X  X  +  X  X  X ohn X  X  (salary)).
Such word composition is not limited to the Germanic family, however, for similar compound constructions (chief of the family).

In Hungarian, typical compound word formations would be  X  X  X agyarorsza  X  g X  X  =  X  X  X agyar X  X  (hungar-may of course introduce errors by decompounding terms into semantically unrelated forms or into forms having other meanings. The word  X  X  X reakfast X  X  fox example may be split into the existing words  X  X  X reak X  X  and  X  X  X ast X  X , thus introducing other unrelated meanings (fracture, gap, escape, luck/speedy, dissipated, firmly, etc.).
However, the real underlying difficulty is not the presence of such compound forms but the fact that such forms may vary between the request and the relevant documents. Recently, Braschler and Ripplinger (2004) showed that decompounding German words could significantly improve retrieval performance. To automat-ically break up compound words into their different components, Chen (2003) suggested using a word list, and then obtaining their frequencies directly from the trained corpus. Savoy (2003) proposed looking at impossible or improbable letter sequences as a means of defining breaking point(s). 3. Test collection
The corpus used in our experiments is composed of articles extracted from the newspaper Magyar H X   X  rlap, published in 2002. This corpus was made available for the CLEF evaluation campaigns in 2005 and 2006, and contains 49,530 documents or around 105 MB of data, encoded in UTF-8 format. On average, each article contains about 142 indexing terms (or 108 distinct indexing terms) with a standard deviation of 140 (mini-mum: 2, maximum 4984). A typical document in this collection begins with a short title (  X  followed by the first paragraph under the  X  LEAD  X  tag, and finally the body (  X  example covering a news about hurricanes in Cuba. Except for the two terms and names  X  X  X exiko  X  and  X  X  X uca-ta  X  n X  X , the rest of the words in the document differ radically from our lexicon. As such it is almost impossible to get a general idea of Hungarian document contents.

This test collection contains 98 topic descriptions (see examples listed in Table 2a for English 2b for Hungarian). Each description is subdivided into four different fields, namely a unique identifier (  X  a brief title (  X  TITLE  X  ), a full statement of the user X  X  information need (  X  tion that helps in assessing the topic (  X  NARR  X  ). The available topics cover various subjects (e.g.,  X  X  X onsumer
Boycotts X  X ,  X  X  X ootball Refereeing Disputes X  X , or  X  X  X ottery Winnings X  X ), and include both regional ( X  X  X wiss Ref-erendums X  X ,  X  X  X rial of Paul Touvier X  X ) and international coverage ( X  X  X heft of The Scream X  X ). In order to work within more realistic conditions, we will build our queries using only the title section of the topic description (or T). Additional information about the elaboration of this document collection or topics can be found in Braschler and Peters (2004), and Peters et al. (2006) .

In this Hungarian collection, both documents are provided without any additional or specific editorial con-trol or verification. Some documents may therefore be only partially available (some parts could have been removed) and spelling errors may occur in documents or in topic descriptions, without being explicitly introduced. This could happen for example when examining the performance of an IR system being used within more difficult contexts.
 The relevance judgments were made by human assessors during the CLEF 2005 evaluation campaign for
Topics #251 X 300, and in year 2006 for Topics #301 X 325 and Topics #351 X 375. Two topics (#307  X  X  X ilms Set in Scotland X  X , and #370  X  X  X he Harry Potter Phenomenon X  X ) were removed because no relevant information on them was found in the corpus. From an inspection of these relevance assessments, the average number of rel-evant articles per topic was 22.93 (median: 16; standard deviation: 21.96). Topic #272 ( X  X  X zech President X  X 
Background X  X ) had only one pertinent document while Topic #311 ( X  X  X nemployment in Europe X  X ) had the greatest number of relevant articles (134).

During the indexing process in our automatic runs, we retained only the following logical sections from the original documents:  X  TITLE  X  ,  X  LEAD  X  ,  X  TEXT  X  ,and  X  tain phrases such as  X  X  X elevant document report ...  X  X  or  X  X  X eressu  X  nk olyan cikkeket, amelyek ...  X  X . Finally, exceptions, such as  X  X  X e  X  sume  X   X  X  or  X  X  X liche  X   X  X ) were replaced by their corresponding non-accentuated letter.
Removing accents from Hungarian words may however generate additional semantic ambiguity (e.g., between (blood)). In our evaluations, we investigated the effective impact of removing accents, a practice applied successfully by several of the best-performing approaches in several CLEF evaluation campaigns involving various languages ( Peters, Gonzalo, Braschler, &amp; Kluck, 2004; Peters et al., 2006 ).
 4. IR models
In order to obtain a broader view of the relative merit of the various retrieval models and stemming approaches, we used two vector-space schemes and three probabilistic models. First we adopted the classical tf idf model. In this case the weight attached to each indexing term was the product of its term occurrence frequency (or tf ij for indexing term t j in document d i similarities between documents and requests, we computed the inner product after normalizing (cosine) the indexing weights.

During the first TREC evaluation campaigns better weighting schemes were suggested, especially schemes assigning more importance to the first occurrence of a term, compared to any successive and repeated occur-rences. Therefore, the tf component was computed as the ln( tf ) + 1. Moreover, we might assume that a term X  X  presence in a shorter document would provide stronger evidence than in a longer document, leading to more complex IR models; for example the IR model denoted by  X  X  X nu X  X  ( Buckley, Singhal, Mitra, &amp; Salton, 1996 ). In addition to these two vector-space schemes, we also considered probabilistic models such as that of
Okapi ( Robertson, Walker, &amp; Beaulieu, 2000 ). As a second probabilistic approach we implemented the Geo-metric-Laplace (GL2) model, taken from the Divergence from Randomness (DFR) framework ( Amati &amp; van
Rijsbergen, 2002 ) whereby the two information measures formulated below are combined: in which Prob 1 ij is the pure chance probability of finding tf other hand, Prob 2 ij is the probability of encountering a new occurrence of term t occurrences of this term had already been found.
 Within this framework, the GL2 model was based on the following formulae: where tc j is the number of occurrences of term t j in the collection, n the number of documents in the corpus, l the length of document d i , mean dl (=150) the average document length, and c a constant (fixed at 1.75).
Finally, we also considered an approach based on a language model (LM) ( Hiemstra, 2000 ), known as a non-parametric probabilistic model (the Okapi and GL2 are viewed as parametric models). Probability esti-mates would thus not be based on any known distribution (as in Eq. (2) ) but rather estimated directly, based on occurrence frequencies in document d i or corpus C . Within this language model paradigm, various imple-mentations and smoothing methods might also be considered, and in this study we adopted a model proposed by Hiemstra (2000) as described in Eq. (4) , which combines an estimate based on document ( P [ t corpus ( P [ t j j C ]).
 where k j is a smoothing factor (fixed at 0.35 for all indexing terms t indexed with the term t j ,and lc the size of the corpus C . 5. Evaluation 5.1. Evaluation methodology
To evaluate our various IR schemes, we adopted the mean average precision (MAP) computed by the trec _ eval software to measure retrieval performance (based on a maximum of 1000 retrieved records). This performance measure has been used by all evaluation campaigns for more than 15 years in order to objectively compare various IR strategies, particularly regarding their ability to retrieve relevant items (ad hoc tasks) ( Braschler &amp; Peters, 2004; Buckley &amp; Voorhees, 2005 ).

Using the mean as a measure of the system X  X  performance signifies that we attached an equal importance to all queries. Comparisons between two IR strategies will therefore not be based on a single query with respect to those available in the underlying test-collection or when specifically created in order to demonstrate that a given IR approach must be rejected. We also believe that it is important to conduct experiments involving the largest possible number of observations. To achieve this goal, we combined the topic descriptions from the
CLEF 2005 and 2006 evaluation campaigns in order to base our findings on a relatively large number (98 observations).

To statistically determine whether or not a given search strategy would be better than another, we applied the bootstrap methodology ( Savoy, 1997 ), ( Abdou &amp; Savoy, 2006 ). In our statistical tests, the null hypothesis
H stated that both retrieval schemes produce similar MAP performance. Such a null hypothesis would be accepted if two retrieval schemes returned statistically similar MAP, otherwise it must be rejected. Thus, in the experiments presented in this paper, statistically significant differences were detected by a two-sided non-parametric bootstrap test (significance level a = 5%).

In order to consider the best practices available, we implemented some of the most effective IR models based on the latest NTCIR ( Noriko, 2005 ) or CLEF evaluation campaigns ( Peters et al., 2006 ). We are con-vinced when comparing IR models and strategies, it is not really appropriate to base our findings on IR mod-els known for having relatively poor retrieval effectiveness. When working with really effective IR models in terms of relatively high MAP, it could be more difficult to identify statistically significant performance improvements.

Finally, it is also well known that the basis for comparisons between two (or more) IR strategies must be similar, using the same document collection and the same topics, as was mentioned by Buckley and Voorhees (2005) .  X  X  X he primary consequence of the noise is the fact that evaluation scores computed from a test collection are relative scores only. The only valid use for such scores is to compare them to scores computed for other runs using the exact same collection X  X  ( Buckley &amp; Voorhees, 2005, p. 73 ).

Thus, it is clearly impossible to compare the performance obtained using an English test collection with that achieved based on another document collection written in the Hungarian language or directly performances obtained from the CLEF 2005 topics with those of CLEF 2006. Even if this were possible, we could not directly compare our performance measures with those available in the CLEF proceedings, due to the fact that the official evaluations were based on longer topic descriptions (TD) and also due to the clearly different con-texts (participants had to meet strict deadlines and did not have access to the relevance judgments needed in determining the best parameter settings for their IR systems). 5.2. IR models and stemming evaluation
Using the words as indexing units, Table 3 depicts the MAP achieved by our five different IR models under three different stemming strategies (none, light, and stemmer 2). Based on the article example shown in Table 1 , we could conclude that an indexing strategy based on words is quite reasonable. In fact, unlike the
Chinese language, the words are conventionally delimited and also relatively short, unlike some German words (e.g.,  X  X  X riedensnobelpreis X  X  =  X  X  X rieden X  X  (peace) +  X  X  X obel X  X  +  X  X  X reis X  X  (prize)).

In Table 3 , the best performance under a given condition is depicted in bold. The first column indicates the tested IR model, the second (under the label  X  X  X one X  X ) the retrieval performance when ignoring the stemming procedure. The third column (labeled  X  X  X ight X  X ) lists the results of a light stemming approach adapted to remove only the number (plural form using two rules in the stemmer and depicted in Fig. 1 ), the possessive markers (e.g.,  X  X  X y X  X  with 17 rules) and the various grammatical cases (using 21 rules, examples given in Fig. 2 ).
Finally the last column (labeled  X  X  X temmer 2 X  X ) lists the MAP obtained by a more aggressive stemmer, tion/ordered). We introduced 17 additional rules in order to achieve this goal, and some examples are given in Fig. 3 .

Using the best performance as a baseline (shown in bold in Table 3 ), we wanted to compare its retrieval effectiveness with other search models under the same condition (or same column). Statistically significant dif-ferences are indicated by an asterisk ( X  X  *  X  X ) after the corresponding MAP value. Table 3 thus shows that the
Okapi model always provided the best retrieval performance, usually significantly better than the other search approaches. The only exception was when comparing the Okapi (0.1832) and Lnu (0.1793) models without stemming (labeled  X  X  X one X  X ). A query-by-query analysis revealed that the Okapi model produced better aver-age precision for 43 queries (over a total of 98 queries), while for 40 others Lnu performed better; the same performance was achieved for the 98 43 40 = 15 queries. Compared to the classical tf idf IR model, the improvements resulting from the Okapi model varied from 37.5% (using the light stemmer) to 34.4% (with Stemmer 2).

A comparison of stemming strategies needs to be done column by column. As a first experiment, we used baseline IR performances obtained when ignoring the stemming procedure (column labeled  X  X  X one X  X ). After applying the light stemming (column  X  X  X ight X  X ) or our more aggressive stemmer ( X  X  X temmer 2 X  X ), the perfor-mance obtained after applying stemming was always statistically better than that achieved when ignoring stemming. As depicted in the last line of Table 3 , the mean improvement over the baseline was around 50.7% for the light stemmer, and 60.7% for the more aggressive stemmer.

Following Harman X  X  study ( Harman, 1991 ), we may assume that different stemmers do indeed produce dif-ferent results, but performance differences are not statistically significant. To verify this assumption, we used the performance results of the light stemmer (column  X  X  X ight X  X ) as a baseline. The statistical test indicated that when compared with this baseline, the performance is always statistically significant (we underlined the cor-responding MAP values in Table 3 ). Using the more aggressive stemmer, we obtained significantly better per-formance than the light stemming approach. Unlike the English, the Hungarian morphology was more complex and thus a more aggressive word normalization procedure provided significantly better MAP.
The effect of applying a stemmer could be illustrated by inspecting some of the queries. Overall, the more aggressive stemming strategy was not able to find any relevant item for four requests over 98. When ignoring the stemming procedure however, the search system could not find any pertinent information for 11 requests.
The greatest improvement after adopting  X  X  X temmer 2 X  X  was obtained with Topic #279 ( X  X  X wiss referendums X  X  model resulted in an average precision (AP) of 0.0257, by retrieving six relevant documents (in ranks 11, 37, 87, 203, 227, and 579). In this case, the underlying query was composed of two words  X  X  X vajci X  X  and  X  X  X ep-szavazasok X  X  (the accents have been removed by the indexing system). Using Stemmer 2, the search terms were  X  X  X vajc X  X  and  X  X  X epszavaz X  X , and this query obtained an AP of 0.8944, retrieving eight relevant documents in the first eight positions (the last one appears in position 182). The performance difference between these two que-ries was not related with the Swiss word ( X  X  X va  X  jci X  X ) appearing in all relevant documents, but with the word referendum ( X  X  X e  X  pszavaza  X  sok X  X ). In relevant documents, we encountered the forms  X  X  X e  X  pszavaza  X  ssal X  X , ming stage.

On the other hand, when comparing the two stemmers, a query-by-query analysis revealed that the largest improvement was obtained with Topic #255 ( X  X  X nternet Junkies X  X  or  X  X  X nternetfu  X  gg } ok X  X ) having six relevant doc-uments. With the light stemmer, this query only retrieved three documents, with all of them being relevant (AP: 0.5). Using the aggressive stemmer, the Okapi model obtained an average precision of 0.9762, retrieving only seven documents. All these articles were pertinent, except for the item ranked sixth. With the light stem-ming, the query consisted of a single search term ( X  X  X nternetfugg X  X ) while the forms appearing in the relevant
For this request, the stem was the verb form  X  X  X u  X  gg X  X  (to depend on), and the topic used a form indicating that go } se  X  g X  X ) with various grammatical case endings ( X -gel X  or  X -ben X ). The light stemmer removed these endings, and 5.3. And the diacritics?
In the previous experiments, all diacritics were removed (both in the documents and in the queries). As shown in certain examples in Section 2 , diacritics may be useful in clearly identifying a word X  X  meaning, and without them increased polysemy could occur, such as evidenced by the word forms (e.g.,  X  X  X er X  X  (hurt) and  X  X  X e  X  r X  X  (blood)). Removing diacritics when stemming may also be helpful however, as evidenced by the (e)t X  but also by modifying the accent. As another example, the noun  X  X  X eve  X  l X  X  (letter) is written as  X  X  X evelek X  X  in the plural form, and the accent disappears.

In order to verify the impact on retrieval effectiveness caused by keeping or removing diacritics, we repeated the two experiments shown in Table 3 in their corresponding runs, after having preserving the diacritics.
Table 4 illustrates performance differences between runs with no stemming (under the label  X  X  X one &amp; dia-critics X  X  and  X  X  X one X  X ) and those with our light stemmer (under  X  X  X ight &amp; diacritics X  X  and  X  X  X ight X  X ). The  X  X  X ight &amp; diacritics X  X  column displays the results when diacritics were preserved in the documents, the queries and stemming rules.

A comparison of both indexing strategies is shown in the  X  X  X one X  X  and  X  X  X one &amp; diacritics X  X  columns, reveal-ing that the performance differences are rather small and always is favor of diacritic removal. Taking the per-formance levels in the  X  X  X one X  X  column being used as a baseline, our statistical tests showed no statistically significant differences could be detected, e.g. both IR strategies resulted in the same performance levels.
The last column in Table 4 demonstrates that removing the diacritic marks when applying a stemming pro-cedure lead to more effective retrieval, and this improvement was more significant (5.9% in average) over a similar approach with diacritics. It is only with the Okapi model (0.2842 vs. 0.2572) however that performance differences were statistically significant. Finally, it is interesting to note that the performances listed in the  X  X  X one &amp; diacritics X  X  column are those achieved when only the inverted file contained correctly spelled words. 5.4. Automatic decompounding
As a second indexing strategy, we decided to automatically decompounding Hungarian compound words (e.g.,  X  X  X unkanap X  X  =  X  X  X unka X  X  (work) +  X  X  X ap X  X  (day)). It is known that such linguistic constructions are used frequently in German, but they are also present in the Hungarian language. We had previously saw Topic #255 containing the compound term  X  X  X nternetfu  X  gg } ok X  X  ( X  X  X nternet Junkies X  X ). After applying our decompound-ing scheme, the query consisted of one compound construction ( X  X  X nternetfugg X  X ) and two single terms ( X  X  X ntern X  X  and  X  X  X ugg X  X ). From examining relevant items, we can see that some of them used the compound con-words separately ( X  X  X nternetezik X  X  and  X  X  X u  X  gg } ose  X  ggel X  X ).
 Our automatic decompounding approach ( Savoy, 2004 ) increased the mean query size, from 2.21 to 3.22. As shown in Table 5 , the IR performance increased but the previous findings were the same. First, the best MAP was obtained by the Okapi model and the performance differences with other approaches were usually statistically significant (as indicated by an  X  X  *  X  X ). Using the retrieval effectiveness obtained by IR models ignor-ing the stemming procedure as baseline (column  X  X  X one X  X ), the two stemmers performed significantly better and, as shown in the last line, the mean improvement that resulted was similar. Finally, using the light stem-mer as a baseline, the more aggressive stemmer resulted in significantly better MAP for all IR models.
When comparing word-only indexing scheme ( Table 3 ) with an indexing scheme using compounds and their composite parts ( Table 5 ), the largest difference was achieved by Topic #271 ( X  X  X ay Marriages  X  X  or  X  X  X el-
Okapi model using the word-only approach achieved an AP of 0.1111 (the result list was limited to a single document that was also pertinent). After applying our decompounding algorithm, the AP was 0.6184 (804 articles were retrieved, the first four were pertinent and other pertinent items were found in ranks 7, 8, 96, and 280). The reason of course for this performance difference is revealed upon inspecting both queries. In the first, the query is limited to one search term (the compound term  X  X  X eleghazas X  X ), while in the second case there are three stems ( X  X  X eleg X  X ,  X  X  X azas X  X  and  X  X  X eleghazas X  X ) allowing a better matches to extract the relevant items. 5.5. Using different topic formulations Previously we only considered the shortest topic formulation (see examples given in Table 2 ). During the
CLEF campaigns, the official evaluation was based on the query composed of the title and descriptive parts (TD) of the topic. Finally, we also consider the longest query formulation using all topic fields (TDN).
Table 6 shows the evaluation obtained with these three topic formulations, using the best stemmer (namely  X  X  X temmer 2 X  X ) and after decompounding the Hungarian terms. In the second row of this table, we indicated the mean query size of these three topic formulations. When considering longer topic formulations (TD or
TDN), the GL2 probabilistic model performed better than the Okapi, and the performance difference with the Okapi model was even statistically significant when using the longest topic formulation (TDN) (as indi-cated by an  X  X  *  X  X ).

While using the title-only query formulation as a baseline and comparing the two longer topic formula-tions, performance differences were statistically significant for the classical tf idf model (values underlined in Table 6 ). Compared to the title-only queries, the mean improvement was rather small, +6.1% when using
TD queries or +6.4% for the longest topic formulation. 5.6. Using different indexing units
In order to represent documents and queries, we used a word-based indexing approach and the words resulting from decompounding. As a language-independent approach, we might consider 4-gram indexing strategy ( McNamee &amp; Mayfield, 2004 ). The evaluation of these three indexing strategies was done using title-only topic formulation as shown in Table 7 . The Okapi probabilistic model produces the best IR perfor-mance, and is usually significantly better (denoted by an  X  X  model are however usually not significant). When using the decompounding strategy as a baseline, the perfor-mance differences were only statistically significant with word-only indexing strategies (values underlined).
Comparing the 4-gram strategy with an indexing scheme using compounds and their composite parts, the largest difference was achieved by Topic #306 ( X  X  X TA Activities in France X  X  or  X  X  X TA-teve  X  kenyse  X  gek Franci-aorsza  X  gban X  X ), consisting of six relevant items. The Okapi model combined with a 4-gram indexing scheme achieved an AP of 0.0101 (relevant items ranked in positions 68, 81, 306, 646, and 932) while the decompound-ing indexing approach resulted in an AP of 0.5807 (relevant items ranked in positions 1, 2, 7, 8, and 9). The underlying query was composed of five search terms, namely  X  X  X ta X  X ,  X  X  X evekenyseg X  X  (activity),  X  X  X ranciaorszag X  X  (France),  X  X  X ranci X  X  (French) and  X  X  X rszag X  X  (country). In this case, the 4-gram generated multiple matches for the compound construction  X  X  X ranciaorsza  X  gban X  X  and the word  X  X  X eve  X  kenyse  X  gek X  X . These matches retrieved many non-relevant documents that did not have the right actor (ETA in this case) but others such as  X  X  X rance Te  X  le  X  com X  X  or  X  X  X acques Chirac X  X .

By contrast, Topic #315 ( X  X  X oping in Sports X  X  or  X  X  X oppingola  X  s a sportban X  X ) consisting of 73 relevant items obtained an AP of 0.6713, using the Okapi model combined with 4-gram indexing scheme, and only 0.289 with the same search model combined with the decompounding scheme (the query was  X  X  X oppingol X  X ,  X  X  X por X  X ). The n -gram indexing scheme had the advantage of allowing multiple matches (for the doping concept in this case) which clearly boosted the number of relevant articles for this request. 6. Conclusion
In this paper we described the most important linguistic features of the Hungarian language, from an IR perspective. Not only does this language use a relatively large set of unambiguous suffixes, but its morphology is also complex, due to the use of possessive pronouns being sometimes added to the suffix construction. Using a test collection extracted from the CLEF-2005 and 2006 suite containing 98 requests, we evaluated three probabilistic and two vector-space models. When using the title-only queries, the Okapi model resulted in the most effective retrieval, under a variety of conditions.

This paper also presents a light stemming strategy used to remove only inflectional suffixes, as well as a more aggressive algorithmic stemmer used to remove some derivational suffixes. Compared to IR models ignoring the stemming procedure, the mean improvement is around +53% for the light stemmer, and +67% for the more aggressive stemmer. When considering the English language ( Harman, 1991 ) in which both stemmers tend to produce statistically similar performance, a comparison of these two stemmers shows that a more aggressive approach produces significantly better results. These performance differences become evident upon analyzing some of the queries.

Also evaluated in this paper is the application of an automatic decompounding algorithm in order to sep-arate compound construction (e.g., viewpoint) into their composite parts. Such an approach produces signif-icantly better MAP (around + 10%) than an approach based on a word-only indexing scheme. Finally, including more search terms into the topic formulation (T vs. TD) improves retrieval effectiveness by 6%, an enhancement that is not always statistically significant.

For the Hungarian language, additional work and experiments are needed to obtain a more complete view of the stemming problem. One solution may be to apply more complex morphological analysis based on a lexical stemmer or on a lemmatizer ( Hala  X  scy, 2006 ). A second may be to consider the language usage more closely through adding, modifying or removing rules applied in an algorithmic stemming approach, so that they take the frequency of various grammatical rules into closer consideration.
 Acknowledgment This research was supported in part by the Swiss NSF under Grant # 200021-113273.
 Appendix. Term weighting formulae
When assigning an indexing weight w ij to reflect the importance of the term t model is based on the following weighting formula: where nt i indicates the number of indexing terms included in d slope = 0.1 and pivot = 75 in our experiments), and mean dl indicates the average document length. The Oka-pi model is based on the following weighting formula: where b , k 1 , are constants fixed at b = 0.75, k 1 = 1.2 in our experiments.
 References
