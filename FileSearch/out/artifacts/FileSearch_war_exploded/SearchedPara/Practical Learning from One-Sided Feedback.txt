 In many data mining applications, online labeling feedback is only available for examples which were predicted to be-long to the positive class. Such applications include spam filtering in the case where users never check emails marked  X  X pam X , document retrieval where users cannot give rele-vance feedback on unretrieved documents, and online adver-tising where user behavior cannot be observed for unshown advertisements. One-sided feedback can cripple the perfor-mance of classical mistake-driven online learners such as Per-ceptron. Previous work under the Apple Tasting framework showed how to transform standard online learners into suc-cessful learners from one sided feedback. However, we find in practice that this transformation may request more la-bels than necessary to achieve strong performance. In this paper, we employ two active learning methods which reduce the number of labels requested in practice. One method is the use of Label Efficient active learning. The other method, somewhat surprisingly, is the use of margin-based learners without modification, which we show combines implicit ac-tive learning and a greedy strategy to managing the explo-ration exploitation tradeoff. Experimental results show that these methods can be significantly more effective in practice than those using the Apple Tasting transformation, even on minority class problems.
 I.5 [ Pattern Recognition ]: General; H.2.8 [ Information Systems ]: Data Mining Algorithms, Experimentation online learning, streaming data, active learning, apple tast-ing, data mining Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
The problem of learning from one-sided feedback was in-troduced by Helmbold, Littlestone, and Long [9], who de-scribed this as the the Apple Tasting problem: the problem of learning to identify sweet apples from visual cues. Of course, an apple taster gets no feedback from those apples it rejects, only from those that it actually chooses to taste. This is a variant of the standard online learning framework. In one-sided feedback, the learner only receives feedback when it predicts a positive label for the given example. That is, the only way a learner can see the true label of an example is to predict that it is a member of the positive class.
Data mining streams with one-sided feedback is an im-portant problem with a range of practical applications. For example, an email filter may send messages to either a spam folder for bad email or a ham folder for good email. If a lazy user never checks the spam folder, as is often the case, then the learner will only receive feedback on messages that were predicted to be ham. There are a wide variety of other ap-plications with one-sided feedback. In a document retrieval setting, users cannot give relevance feedback for documents not shown [17]. In an online advertisement setting, user re-action can only be learned for advertisements actually dis-played [1]. Oil prospectors only learn if there is, indeed, oil in a predicted location after they finish drilling, and learn nothing about locations not drilled.

The problem of learning from one-sided feedback defeats several classical online learning algorithms, such as Percep-tron [15] and Winnow [13]. These mistake-driven algorithms suffer in this scenario, especially in the presence of noise, as the online updates tend to sacrifice recall for precision and may recognize very few positives. Helmbold, Littlestone and Long showed how to convert any standard online learner, in-cluding these mistake-driven methods, into an apple tasting algorithm by randomly sampling from those examples pre-dicted to be in the negative class [9] with resultant mistake bounds. However, this method samples uniformly from the predicted negatives, and thus does not necessarily request labels for the most informative examples.

We propose that practical data mining on one-sided feed-back streams is best done with active learning methods. We show that Label Efficient active learners perform well from one-sided feedback, requesting fewer labels than the Ap-ple Tasting methods. We also show, somewhat surprisingly, that margin-based learners such as Online Support Vector Machines (SVMs) and Perceptron with Margins both learn effectively from one-sided feedback without modification .In the one-sided feedback scenario, it turns out that margin-based learners implicitly use an active learning strategy and a greedy search solution to the exploration/exploitation trade-off. Our experiments show that both types of active meth-ods can achieve high levels of performance with many fewer labels than the Apple Tasting solution, and that the margin-based methods are often the most effective.

The remainder of this paper proceeds as follows. Section 2 gives preliminary background on one-sided feedback and reviews the Apple Tasting transformation with an eye to-wards possible improvements for practical use. In Section 3, we discuss the application of Label Efficient active learners to one-sided feedback problems. In Section 4, we show that in many cases margin-based methods can learn effectively from one-sided feedback without transformation due to im-plicit uncertainty sampling and a greedy approach to the ex-ploration/exploitation tradeoff. Section 5 covers difficulties posed for learning from one-sided feedback in minority-class distributions. Experimental results are in Section 6, and the final section contains our conclusions.
We are concerned with the problem of online learning from one-sided feedback, first described as the Apple Tast-ing problem [9]. We assume a distribution D on a space of examples X = R d , and each example x i has an associated label y i . There is a learner L with a hypothesis function h (  X  ): R d  X  X  X  1 , 1 } predicting the label of a given example. The learner is allowed to update its hypothesis when it is shownanexampleandlabelpair( x i ,y i ). There is an oracle T that returns a (possibly noisy) label y i for a given x
Learning proceeds in a (potentially unbounded) number of rounds, { t 1 , ..., t max } .Given D , L , T , for each round t
In this paper, we assume that the cost of requesting a label for a truly negative example is equivalent to the cost of misclassifying a negative example, while the cost of re-questing a label for a truly positive example is zero. This is equivalent to saying that the only way to request a label for a given example is to predict that it is a positive.
To illustrate the issu es surrounding the one-sided feed-back problem, we first show that noisy one-sided feedback can break classical mistake-driven online learners such as Perceptron [15] and Winnow [13]. These learners update their hypotheses only on mistakes. However, under the one-sided feedback scenario, they are never told about mistakes made when they predict a negative label, so no updates can occur. The only mistakes they will update on are those for which they predicted a positive label for a negative example.
Updating on one-sided errors creates a ratcheting effect shown in Figure 1. Once the hyperplane has been shifted towards the positive side, it can never be shifted back. If the noise rate p&gt; 0, then the hypothesis will converge to one which predicts every a negative label for every example. (Proof omitted.) Even if there is no noise, if the data are not linearly separable in the feature space then the hypothesis will converge to one which never mistakes a negative for a positive. (Proof omitted.) This can cause recall levels for the positive class to suffer greatly, as we show in our experiments. Even if the data are separable, a poor initial hypothesis may never be improved. Thus, purely mistake driven learners are unsuitable for one-sided feedback.
Helmbold et al. proposed a solution to the one-sided feedback problem and analyzed it theoretically using the mistake bound model from learning theory [9]. They showed that if a learner can be forced to make a maximum of either M p mistakes on positive examples or M n mistakes on nega-tives from full feedback from a given (noiseless) distribution, then it can be transformed into a learner making at most M that distribution. These mistake conditions can be met for Perceptron or Winnow by setting an initial bias.

Their solution (the  X  X pple Tasting method X , hereafter), relies on occasional random sampling from those examples which are predicted to have negative labels. When an ex-ample is sampled, a label request is made to the oracle by flipping the predicted label from -1 to 1. A label re-quest is made on step i when h ( x i )=  X  1 with probabil-ity p = found so far among the examples for which labels have been specifically requested. [9] Intuitively, this method samples the learner X  X  error rate to determine how much exploration is needed. As 1+ m n i grows, more labels are requested be-cause the observed error rate is high. When this estimate of the error rate decreases, fewer labels are requested.
Given:  X &gt; 0 ,dataset X =( x 1 ,y 1 ) ,..., ( x n ,y n ) Initialize: w := 0 , K =0 For each x i  X  X do : Figure 2: Pseudo-code for Label Efficient active learner.
While this Apple Tasting transformation offers a robust solution to the problem of learning from one-sided feedback, we note that there are areas of possible improvement for practical use, using active learning and examining different exploration/exploitation strategies.

The Apple Tasting method samples from the negative pre-dictions in a uniform manner, without taking into account the certainty of the prediction. Although uniform sampling enables theoretical guarantees of correctness for purely sep-arable data [9], it is not always the most efficient way to learn a good hypothesis in practice. Active learning meth-ods attempt to choose informative examples to learn from. Uncertainty sampling is one such method, in which examples are chosen based on how uncertain the current hypothesis is about their label [12]. Other active learning methods in-clude Query by Committee, in which disagreement among possible learners is cause for sampling [7]; choosing examples based on how much they would reduce the current version space [3]; and estimating how much the example would re-duce training error if its label were known [16]. We propose that active learning can improve on the Apple Tasting bounds in practice on one-sided feedback problems. The methods we explore in this paper are based on uncer-tainty sampling, which is computationally efficient. Label Efficient learners use uncertainty sampling methods to ad-just the probability that a label will be requested for a pre-dicted negative. We also show that margin-based learners implicitly use a fixed form of uncertainty sampling to request labels. Exploring other active learning methods in one-sided feedback problems remains for future work.

The Apple Tasting solution strikes a particular balance between exploration and exploitation , to use terminology from reinforcement learning [21], by requesting more labels when the estimated error rate is high. Exploration of the data space allows the learner to acquire new knowledge and better estimate the optimal hypothesis  X  however, this ex-ploration may incur cost associated with label requests. Ex-ploiting previous knowledge carries no exploration cost, but may incur misclassification cost if the hypothesis is faulty. Determining the optimal balance between exploration and exploitation apriori for an arbitrary task is an open prob-lem, and different approaches may be better for different situations. The Label Efficient learner uses a strategy simi-lar to that of Apple Tasting, by attempting to explore more when observed error rate is high. Margin-based learners im-plicitly use a greedy exploration strategy that we describe in Section 4, that can request many fewer labels in practice but offers no theoretical guarantees.

The methods we explore in this paper can be organized as follows. Classical Perceptron, with no active learning and no exploration, fails on one-sided learning. Apple tasting adds exploration to solve this problem, but without active learn-ing may request more labels than necessary. Label Efficient methods request fewer labels and maintain theoretical guar-antees. Margin-based methods use a greedy exploration to further reduce the number of needed labels in many cases, but at the sacrifice of theoretical guarantees. The following sections examine these last two learners in more detail.
An online active learner must decide whether or not to request a label for a given example at that particular time step, without knowledge of the future or the ability to recon-sider at a future point. When labels are costly, this creates resource allocation issues. The Label Efficient problem is to learn well with few label requests. Although this prob-lem was posed in the standard online learning setting [8], it has natural application with one-sided feedback where sam-pling from the negative predictions carries cost, and sam-pling from positive predictions is essentially free. To our knowledge, this is the first use of label efficient learners on one-sided feedback problems.

Cesa-Bianchi et al. proposed a label efficient active learner based on the perceptron algorithm (see Figure 2 for pseudo-code, simplified for the case of normalized example vectors) and give bounds on the expected number of mistakes and the expected number of label requests for linearly separable data [2]. The method adapts to the number of known mistakes seen so far, and samples more frequently when higher error rates are observed. Furthermo re, unlike other label efficient learners that have been proposed, this method does not re-quire the user to specify a maximum number of examples to label, and instead manages the exploration/exploitation problem adaptively, given an initial setting of parameter  X  . Finally, this active method takes uncertainty into account and is more likely to sample points that lie close to the clas-sification hyperplane.

This method can be applied in the case of one-sided feed-back. Here, requesting a label for a given example forces the learner to predict a positive label for a given example. Label requests are made on all positive examples. In terms of the pseudo-code, Z i = 1 whenever y i =1. Themethod Given: C ,dataset X =( x 1 ,y 1 ) ,..., ( x n ,y n ) : Initialize: w := 0 , b := 0 , seenData := {} For each x i  X  X do : was originally analyzed in terms of the classical Perceptron algorithm; we apply it to other linear classifiers as well.
One claim of this paper is that margin-based learners can be effective learners from one-sided feedback. In this sec-tion, we review the essentials of two margin-based learn-ers, Online SVMs and Perceptron with Margins. We then demonstrate how margin updates enable learning from one-sided feedback, revealing implicit uncertainty sampling and a greedy exploration/exploitation tradeoff strategy. This section concludes with an examination of conditions that can cause this greedy strategy to fail.
Here, we review two margin-based learners: Online SVMs and Perceptron with Margins. Both of these methods are linear classifiers that update their linear hypothesis not only on mistakes, but also on correctly classified examples that lie close to the classification hyperplane, enabling learning from one-sided feedback.

SVMs offer a statisticly robust method to classification. (See the introductory text by Sch  X  olkopf and Smola [19] for a complete discussion.) Briefly, an SVM computes a clas-sification hyperplane which maximizes the margin between two classes by minimizing the following objective function: with the constraints:
Here, w is the weight vector of the hypothesis, with bias b : minimizing the || w || 2 creates the maximum possible margin. Each misclassified example x i carries cost  X  i . The parameter C sets the balance between the goals of minimizing misclas-sification cost and maximizing the margin.

Any standard SVM operating in a batch learning mode may be converted to an Online SVM using the simple wrap-per pseudo-code shown in Figure 3. We implemented the Online SVM used in our experiments using Platt X  X  SMO, an iterative solver [14].

Notice that the Online SVM stores every example, but only requires updates when it has made a mistaken predic-Figure 5: Pseudo code for Perceptron with Margins. tion, or when a correct example lies within the margins  X  that is, when | f ( x i ) | &lt; 1. Online SVMs do not need to update on any correctly classified example outside the mar-gins due to the Karush-Kuhn-Tucker conditions, which show that such examples cannot become support vectors until a mistake is observed [19]. By the same conditions, any exam-ple lying within the margins causes an update and will move the classification hyperplane. This is a key point which en-ables Online SVMs to learn from one-sided feedback, as we discuss later in this section.

While classical Perceptron seeks only to minimize training error [15], the Perceptron with Margins attempts to create a margin m similar to that of SVMs. Unlike SVMs, however, Perceptron with Margins is not guaranteed to find the maxi-mum margin. However, for linearly separable data there are lower bounds on the size of the margin [11].

Like Online SVM, the Perceptron with Margins updates its hypothesis both on mistaken predictions and on correctly predicted examples that lie within the margin of the classifi-cation hyperplane (see Figure 5 for pseudo-code [11]). Note that classical Perceptron is equivalent to Perceptron with Margins using parameter m = 0, as classical Perceptron only updates on mistakes. This is the critical distinction that allows Perceptron with Margins to learn from one-sided feedback, while classical Perceptron fails.
At first, it may seem counter-intuitive that any learner can learn effectively from one-sided feedback without modi-fication. We now show the intuition driving the finding that margin-based learners can indeed learn in this scenario with no modification.

Recall that classical learners such as Perceptron are sub-ject to ratcheting because they can only recognize one kind of mistake in the one-sided feedback scenario. Margin-based learners are resistant to ratcheting as they can update their classification in both directions. As before, misclassified neg-atives still cause updates moving the hyperplane more to-wards the positive, correctly classified negatives have no ef-fect and misclassified positives have no effect as no feedback is given. Furthermore, correctly classified positives that lie outside the margins also cause no update to occur.
The key difference is: margin based learners update their hypothesis on correctly classified examples that lie within the margin. (See Figure 4.) The hyperplane may be pulled towards the positive by misclassified negatives and pushed towards the negative by positive examples classified within the margins. These hypothesis updates are not irreversible, and the hyperplane can converge to a good hypothesis. 15, a misclassified negative, pulls the hyperplane towards the positive. Figure 6: Implicit Uncertainty Sampling for Percep-tron with Margins. The margin-based learner with hypothesis h and margins m + and m  X  learning from one-sided feedback reduces to an active learner with hypothesis h and margins m + and h using uncer-tainty sampling in the region between h and h .
Here, we demonstrate the claim that margin-based meth-ods that are applied to one-sided feedback problems implic-itly use active learning to sample from negative predictions.
Reduction 1. A margin-based learner L with margin m learning from one-sided feedback reduces to an active margin-based learner L with margin m 2 . The sampling rule for this active learner is to perform uncertainty sampling on any ex-ample x i for which the prediction f ( x i )  X   X  m 4 .
Assume the learner has classification hyperplane h defined by its weight vector w , with margin planes m + on the posi-tive side and m  X  on the negative side, and that the distance between each margin and the hyperplane is m 2 (see Figure 6). Now consider the hyperplane h , which lies halfway between h and m + .Wecanview h as a classification hyperplane for learner L , with margins m + on the positive side and h on the negative side, each at a distance of m 4 from h .Because h is translated distance m 4 from h , L will score each x f ( x i )= f ( x i )+ m 4 .

Active learner L requests the label y i for any example x found to lie between h and h  X  that is, for any example such that f ( x i )  X  X  X  m 4 .Thislabel y i is always available, because x i lies to the positive side of h and one-sided feedback will be provided to L . L requests labels for its predicted nega-tives that lie close to its classification hyperplane: a simple form of uncertainty sampling [18]. Furthermore, L requests labels for all examples that lie to the positive side of h ,and such labels are also always available to L under the one-sided feedback scenario. Thus, L performs uncertainty sampling on any x i such that f ( x i )  X  X  X  m 4 . Figure 7: Exploration and Exploitation. If the ini-tial hypothesis is h ,thenexamples1and2cause margin updates pushing h e out towards m  X  , but not beyond it unless an example is found to lie between h and h e .

As a margin-based learner, L updates on mistakes or ex-amples found within the margins of h , computing a new hypothesis h . L then adopts a new hypothesis from L as follows. If L is a Perceptron with Margins, the new hypoth-esis h for L will be h = h  X  m 4 .If L is an Online SVM, the new hypothesis will simply be h = h because the SVM will optimize over all labeled examples that have been seen to that point, regardless of its starting hypothesis. Thus, L reduces to L .
One of the primary problems in online active learning is resource allocation [8], often referred to as the explo-ration/exploitation tradeoff [21]. It is difficult to determine apriori the best balance between sampling (which may incur labeling cost) and prediction without sampling (which may incur misclassification cost) for arbitrary distributions [21]. The Apple Tasting [9] and Label Efficient [2] methods both attempt to strike a balance by estimating the error rate. However, determining the error rate incurs sampling cost, and the upper bounds computed for these methods may not be tight in practice. For example, the current hypothesis may be good but many labels may need to be requested be-fore the error rate confirms this. Margin-based methods use a greedy approach to balancing this tradeoff that can require many fewer labels in practice. However, performance for this greedy method cannot be guaranteed due to the possibility of malicious or pathological distributions.

The greedy strategy is the following. When h is a hy-perplane consistent with all seen labeled data, the learner L only requests labels for examples that it predicts to be positive. This is a conservative strategy emphasizing ex-ploitation, and incurring zero labeling cost. Note that at this time, examples on the negative side of m  X  are strongly believed to be negatives, and those between h and m  X  are suspected to be negative.

When a new example x i is found to lie within the mar-gins, between h and m + , the learner is willing to explore, and the hyperplane is shifted through margin-updates, to create a new hyperplane h e , with margins m e + and m e  X  (see Figure 7). Assuming a moderate learning rate (that is,  X   X  m 2 for Perceptron with Margins or non-extreme values of C for Online SVMs), h e will lie somewhere between h and m  X  , causing the learner to sample from the suspected neg-atives (from the perspective of h ), but never from examples strongly believed to be negatives. Each new positive exam-ple between h and the new m e + will push h e closer to m However, each such update will shrink the gap between h and m e + ,until h = m e + .Atthispoint, h e can be located no further toward negative than m  X  .

Note that h is still consistent with all the seen, labeled data at this point (although h is no longer maximum mar-gin). The only thing that will cause an update now is mis-classifying a negative, or finding a positive between h and h . Either of these cases would show that the original h is no longer consistent with the seen data, and L must recompute anew h and start again with the conservative strategy.
Thus, unlike the Apple Tasting or Label Efficient strate-gies, margin-based learners do not sample from all predicted negatives with positive probability  X  only from those that are close to h , and only when there have been sufficiently many positives found between h and m + to encourage fur-ther exploration. One way to view this method is as a greedy strategy for finding support vectors with few label queries  X  very few negatives are sampled unnecessarily.
Before moving on in this discussion, some caveats are in order as this greedy exploration strategy can be stalled or defeated by certain pathological distributions. Linearly separable distributions that include large gaps may cause margin-based methods to cease making progress. This will occur whenever the probability of of an example landing in the space between h and h e is zero. For many interesting distributions, this only occurs in the margin between the two classes. However, it is possible to have such a gap within a single class (see Figure 8), which will have the same effect when the side of the gap is greater than m 2 .Gappydistri-butions may be dealt with by increasing the margin size. Note that this provides a second intuition for the failure of classical Perceptron: when m = 0, every distribution is a gappy distribution.

In some cases, a distribution may not be linearly separable in the feature space, but we may still wish to find a hypoth-esisthatminimizeslossaswiththesoftmarginSVM.In many of these cases, the margin-based methods will be suc-cessful, as the greedy exploration will continue in the limit so long as the expected loss per example from examples lying between h and m + is less than some set threshold. However there is the possibility of striped distributions (see Figure 8) that can cause the greedy exploration to fail. A stripe is a region of at least width m 2 where the expected cost of applying the surrounding area X  X  class label to examples from that region is greater than the cost of applying the opposing label. As with gaps, stripes may be dealt with by increasing the margin size in some cases, or by adjusting misclassifi-cation costs, or by finding a transformation of the feature space that renders the data linearly separable (removing the stripes).

Another possible failure of margin-based learners can be caused by malicious orderings of the examples. In a noisy domain, it is possible for an adversary to select a sequence of incorrectly labeled examples that will cause ratcheting by one-sided learners. Such malicious orderings may be possible in certain one-sided feedback applications such as optimal placement of banner advertisements.

Finally, when the learning rates are set too high, the learner may overshoot the positive class after it has misclas-sified a negative (or a series of negatives). If the resulting hyperplane is placed beyond the positive class, classifying everything as a negative, then the learner will be ratcheted and no further learning will take place.
Learning from one-sided feedback is particularly challeng-ing in when the positive class is a minority in the distribu-tion. This is a challenging problem for active learning in general. It has been shown that for non-homogeneous dis-tributions, such as minority-class distributions, a linear clas-sifier such as Perceptron using active learning can require as many labels to achieve a given error rate as the same method without active learning [6]. That is, active learning may give no benefit in these situations. This is unfortunate, as mi-nority class problems are common in practical data mining.
Thus, active learning solutions to the one-sided feedback problem necessarily suffer in the case of minority class dis-tributions. Furthermore, Apple Tasting and Label Efficient methods that attempt to sample error rate may have diffi-culty because the measured e rror may be low even when the entire minority class is misclassified. Margin-based meth-ods may have similar difficulty, where a long sequence of observed negatives may cause ratcheting in the hyperplane. These issues may be ameliorated by assigning different mis-classification costs to the two classes.

In our experiments with minority class one-sided feedback, we test two standard methods of cost weighting: example weighting, and threshold biasing.

One method of assigning misclassification costs is to weight examples from the minority class more heavily. In batch set-tings, this may be done by oversampling from the minority class, but this may lead to inefficiency with SVMs as the computational cost of this method increases with the size of the data set. One method of example cost weighting with SVMs is to use different values of the C parameter for each class [19], allowing errors of each class to be weighted differently in the loss function. We can modify the Percep-tron update rule to consider costs as well, using the update w := w + y i  X  x i c y i on each mistake or margin update. Here, c y i is the cost of making a mistake on an example with class label y i .

A second method of assigning misclassification costs with linear classifiers is to add an additional biasing constant to the classification threshold, effectively translating the hyper-plane. This allows the learner to trade errors of one class for another. As has been thematic in this paper, in one-sided learning biasing the hyperplane towards the negative also has the effect of creating additional uncertainty sampling, and forces the learner to request labels from predicted neg-atives lying near the unbiased hyperplane. This gives addi-tional robustness to the problem of one-sided learning from minority-class distributions.
In this section, we report experiments in one-sided feed-back scenarios from two domains: spam filtering with rela-tively even class distributions, and document classification with minority class distributions. These results give strong support for the use of active learning for one-sided feedback.
Spam filtering is a natural application of online learn-ing from one-sided feedback. In a typical email system, a learning-based filter is applied to distinguish between wanted emails, and unwanted emails known as spam . The messages predicted to be wanted by the user (sometimes referred to as ham emails) are sent to the user X  X  inbox, while the pre-dicted spam are sent to a spam box. The online learner relies on user feedback to update its hypothesis. However, in many situations a user may never check the emails that are sent to the spam box  X  no feedback is given on any message predicted to be spam (see Figure 9). This is a dif-ferent scenario than is typically considered in the evaluation of spam filtering methods, which normally assume that feed-back will be given on every message regardless of its classi-fication [5]. However, the high accuracy that many machine learning methods achieve in this full-feedback scenario may criticized as being overly optimistic. While the full-feedback scenario represents the ideal user from the perspective of the learner, the one-sided feedback scenario represents the ideal learning scenario from the perspective of the user.
We construct an online learning task with one-sided feed-back as follows. On each round, a learner is shown a mes-sage and asked to predict its label from { X  1 , 1 } for spam and ham respectively. When a positive label is predicted, the true label is revealed to the learner and it may update its hypothesis. If the learner wishes to sample the label for a message it predicts to be spam, it directs that message into the inbox by predicting a positive label. Thus, label requests have the same cost as false positives.

We map spam messages to feature vectors using the method-ology from [20], which has produced state of the art re-sults with spam data. We use a 4-mer feature space, which consists of the set of all (possibly overlapping) contiguous character strings of length 4. The first 3000 characters of each email (including header, body, and any attachments) are mapped to sparse binary vectors in this feature space. These vectors are then normalized with the Euclidean norm to correct for differences in message length. This feature space is used for all test on spam data.
 We tested three methods, Online SVMs, Perceptron with Margins, and classical Perceptron (which is equivalent to Perceptron with Margins where m = 0.) For Online SVMs, we used the Relaxed Online SVM described in [20], which has been shown to give nearly identical performance to the simple Online SVM on spam data with greatly reduced cost for large data sets. We set parameters C = 100 and buffer size 1000, updating on all margin errors. For Perceptron with Margins, we used m = 2 as a default parameter, and learning rate 1. For classical Perceptron, we used the same learning rate and m =0.

We tested these methods on one-sided feedback in three ways: in their unmodified version, with the addition of La-bel Efficient sampling, and with the addition of Apple Tast-ing sampling. The results reported for the Label Efficient methods were for optimal values of the parameter  X &gt; 0 found in coarse grained trials on tuning data. Both Apple Testing and Label Efficient methods are randomized algo-rithms; we report average results over 10 trials with each. We tested other sampling strategies, including threshold bi-asing, -greedy, and the Softmax variant, but do not report these results as they were not competitive on these data sets. Finally, we report results on these data sets using full feedback for comparison.

We use the two largest publicly available labeled data sets of spam and ham email, which are the trec05p-1 data set of 92,189 messages with a 43% ham rate [5] and the trec06p data set of 37,822 messages with a 34% ham rate [4]. Both of these benchmark corpora have a canonical or-dering for online learning which we use for repeatability. In preliminary tests and where parameter tuning was needed, we used a separate corpus, the smaller publicly available spamassassin corpus of 6032 examples.

Evaluating the performance of spam filtering methods is typically done by measuring the area under the ROC curve [5], which accounts for potentially uneven misclassification costs by assuming the ability to freely vary the classifica-tion threshold. In the one-sided feedback scenario, threshold modification after the fact is problematic, as the predicted class has implications on what feedback was available dur-ing learning. Thus, we evaluate performance using precision, recall, and the F-measure from the classifier X  X  actual thresh-Table 1: Results for Email Spam filtering. We re-port F1 score, Recall, Precision, number of False Negatives (lost ham) and number of False Positives (spam in inbox) for with one-sided feedback. We report results with full feedback for comparison. old. However, as a sanity check, we did calculate the ROC curve areas for each of the results, and these results agreed with the trends reported in this section.

Precision ( P ), recall ( R ), and the F  X  measure are defined in terms of true positives (ham in the inbox), false positives (spam in the inbox), true negatives (spam in the spam box) and false negatives (ham in the spam box). When a learner makes a label requests, it is counted as a false positive when the example is a negative. Label requests for positive exam-ples are counted as true positives. The measures are com-puted as follows:
The F  X  measure gives a single number summary of clas-sifier performance, where the parameter  X  determines how much weight to assign to precision and recall. We report the F1 measure results as a conservative view; we computed scores for F2 through F4 and found they emphasized our reported trends.

The results for experiments on both data sets are reported in Table 1. In general, they show a clear win for the margin-based methods, which had the highest F1 scores and lowest false positive rate, which is equivalent to making the fewest label requests. A spam filter using the unmodified Online SVM or Perceptron with Margins would place roughly half as much spam in the user X  X  inbox as the Apple Tasting meth-ods, while making roughly the same amount (or fewer) of misclassifications on ham. These results were not far from those achieved with full feedback.

As expected, classical Perceptron algorithm was defeated by the one-sided feedback scenario. Of the three methods of fixing classical Perceptron, the addition of margins (turning classical Perceptron to Perceptron with Margins) was more effective than either the Label Efficient or Apple Tasting strategies with classical Perceptron on all one-sided feed-back tests. This supports the notion that the margin-based greedy exploration strategy is effective with class distribu-tions.

Finally, the Label Efficient method out-performed the Ap-ple Tasting method on all trials. Recall that the Label Effi-cient method also contains implicit use of uncertainty sam-pling, while Apple Tasting relies on uniform sampling of the negative predictions. This supports the claim that active learning is a good strategy for one-sided learning.
In the previous experiments, the margin-based methods performed nearly as well from one-sided feedback as from full-feedback. This was true for the case in which the class distribution was roughly even. We now turn to the case in which the positive class is a minority class. As discussed in Section 5, the minority class case can pose problems for learning from one-sided feedback. We examine two methods of cost weighting to address this issue: example weighting and threshold biasing. Our initial tests showed that cost weighting is necessary with all of the methods on minority class problems.

For motivation, we consider the scenario in which a user wishes to be given documents of a specified class from a document stream. The user is only able to provide feedback on those documents that she actually sees, and she wishes to see as few irrelevant documents as possible while missing very few relevant ones. This is a special case of learning from relevance feedback , and has wide application in fields such as information retrieval, and data mining of text streams such as news articles or communications monitoring. In such scenarios, there are often many fewer relevant documents than irrelevant ones, creating a minority class distribution.
We explore this scenario using the data from the Reuters-21578 data set for text classification, a standard benchmark data set [10]. We used the Mod-Apte split, and used the 9603 documents in the training data set in their given order to create an online learning task. The smaller test data set was used for initial tests and parameter tuning. We created binary classification tasks for each of the ten most common document classes to create a set of one-against-rest classification tasks. We used word stemming, removed stop words, and used normalized binary word-based feature Table 2: Threshold Biasing on Minority-Class Doc-ument Classification. Results show best F1 score for each method over all parameter values.
 vectors. We removed any documents that did not contain at least one word in the text body. We also seeded each data set by moving the first occurrence of a positive example to the beginning of the online learning task. The data sets all contained minority positive class distributions, ranging from 30% positive down to 1.9% positive.
 As before, we tested Online SVMs and Perceptron with Margins in their unmodified forms, with Apple Tasting, and with the Label Efficient method. We used two methods of cost weighting: example weighting and threshold bias-ing. For each weighting method, we tested a spectrum of weighting parameter values and report the best F 1scorefor each learning and weighting method. The threshold biasing values we tested were { 0, 0.008, 0.016, 0.032, 0.064, 0.128, 0.25, .75, 1 } . For SVM example weighting, we fixed C pos at 100 (consistent with initial on the separate tuning data), and adjusted C neg from 0.001 to 128 in rough power of two increments to create the relative cost weighting. For Percep-tron with Margins, we used the weighted Perceptron update rule described earlier, and held c neg at 1 while testing c in powers of two from 1 to 128. The  X  parameter for the Label Efficient method was set to 0.0001, set by initial trials on the separate tuning data.

The results reported in Tables 2 and 3 do not show a single clear winner across all trials. However, they do re-veal some important general trends. First, minority-class problems clearly present difficulties for all approaches to one-sided feedback. The strongest performance was on the Table 3: Example Weighting on Minority-Class Doc-ument Classification. Results show best F1 score for each method over all parameter values.
 largest minority classes, earn and acq , while the weakest performances were on several of the smallest classes. Sec-ond, the margin-based and Label Efficient methods both out-perform the Apple Tasting methods on a wide major-ity of trials, with both Online SVMs and Perceptron with Margins. Third, the margin-based methods out-performed both Apple Tasting and Label Efficient methods on a ma-jority of tests. However, on some tests, the margin-based methods were not able to learn effectively, suffering ratchet-ing as on the ship and trade results with threshold biasing for Perceptron with Margins. These few failures are typified by the extreme minority class distribution. Fourth, we note that threshold biasing was the more effective cost weighting method with Online SVMs, but example weighting was more effective with Perceptron with Margins. These results show that different data tasks may require different approaches for best results under one-sided learning from skewed data sets, but that margin-based methods would be a strong first choice.
The goal of this paper was to show that active learning improves performance from one-sided feedback. We have shown that the Label Efficient active learner can be ap-plied to one-sided learning with good results. We have also showed that margin-based learners are active learners in the one-sided feedback scenario, and can exceed the performance of both the Apple Tasting and Label Efficient methods under many circumstances. One interesting ancillary contribution is the notion that margin-based learners may be used in con-junction with Apple Tasting or Label Efficient methods for additional robustness to pathological cases.
The Label Efficient method is well designed for active learning in an online setting, and adapts naturally to the problem of one-sided feedback. While margin-based meth-ods are not generally considered to be active learners, we have shown that they do perform implicit active learning using uncertainty sampling in the one-sided feedback sce-nario. The margin-based methods gave best performance on spam data, approaching that of the full-feedback scenario. This performance was possible because the data contains relatively even class distribution and is quite linearly sep-arable. Performance on minority class problems was more mixed, with both margin-based methods and Label Efficient methods performing well. The greedy exploration strategy used by margin-based methods only failed on extreme mi-nority class distributions, and even then only with certain cost weighting schemes.

There are numerous practical data mining applications that suffer from the problem of one-sided feedback. We have empirically demonstrated the utility of these methods in two important tasks. In particular, the result that near state-of-the-art spam filtering is possible from one-sided feedback has important implications for large scale email systems. Future applications involving one-sided feedback may range from geo-statistical data mining to discover ore and oil deposits to personalized online news agents that learn effectively from one-sided feedback about the relevance of shown articles.
While we have concerned ourselves with linearly separa-ble problems in this paper, these approaches can easily be adapted to the non-linearly separable case through feature transformation using kernel methods. Lastly, future work in one-sided feedback includes the interesting idea of one-sided feedback and regression. Is it possible to learn not just a classification function, but a full regression function effectively from feedback only on examples predicted to be above a given score threshold? This may have considerable impact in a variety of fields, including online advertising. [1] N. Abe and T. Kamba. A web marketing system with [2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. [3] D. Cohn, L. Atlas, and R. Ladner. Improving [4] G. V. Cormack. TREC 2006 spam track overview. In [5] G. V. Cormack and T. R. Lynam. TREC 2005 spam [6] S. Dasgupta. Analysis of a greedy active learning [7] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. [8] D. Helmbold and S. Panizza. Some label efficient [9] D. P. Helmbold, N. Littlestone, and P. M. Long. [10] S. Hettich and S. D. Bay. The UCI KDD archive. [11] W. Krauth and M. M  X  ezard. Learning algorithms with [12] D. D. Lewis and W. A. Gale. A sequential algorithm [13] N. Littlestone. Learning quickly when irrelevant [14] J. Platt. Sequenital minimal optimization: A fast [15] F. Rosenblatt. The perceptron: A probabilistic model [16] N. Roy and A. McCallum. Toward optimal active [17] G. Salton and C. Buckley. Improving retrieval [18] G. Schohn and D. Cohn. Less is more: Active learning [19] B. Sch  X  olkopf and A. Smola. Learning with Kernels: [20] D. Sculley and G. Wachman. Relaxed online support [21] R. S. Sutton and A. G. Barto. Reinforcement
