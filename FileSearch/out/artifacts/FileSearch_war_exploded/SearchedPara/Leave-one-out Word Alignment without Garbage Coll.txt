 Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been pro-posed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This pa-per contributes to this effort through refining the widely used expectation-maximization (EM) algo-rithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000).
The EM algorithm for WA has a great influ-ence in SMT. Many well-known toolkits includ-ing GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyM-GIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013).

However, the EM algorithm for WA is well-known for introducing  X  X arbage collector ef-fects. X  Rare words have a tendency to collect garbage, that is they have a tendency to be erro-neously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac  X a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s , from the GALE Chinese-English Word Alignment and Tagging Training annotated word alignment. The Chinese word  X  X E ZHANG, X  denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-alignment is shown in Figure 1(b). It can be seen that w r is erroneously aligned to multiple English words.

To find the cause of this, we checked the align-ments in each iteration i of s , denoted a i s . We found that in a 1 s , w r together with the other source-side words were aligned with uniform probability to all the target-side words since the alignment models provided no prior information. However, in a 2 s , w r became erroneously aligned, because the alignment distribution 3 of w r was only learned from a 1 s , thus consisted of non-zero values only for generating the target-side words in s . Therefore, the alignment probabilities from the rare word w r to the unaligned words in s were ex-traordinarily high, since almost all of the proba-bility mass was distributed among them. In other words, the story behind these garbage collector ef-fects is that erroneous alignments are able to pro-vide support for themselves; the probability distri-bution learned only from s is re-applied to s . In this way, these  X  X arbage collector effects X  are a form of over-fitting.

Motivated by this observation, we propose a leave-one-out EM algorithm for WA in this pa-per. Recently this technique has been applied to avoid over-fitting in kernel density estima-tion (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leave-one-out likelihood estimation is performed. Fig-ure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word  X  X E ZHANG X  is identical to the human annotation. The most related work to this paper is train-ing phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to im-prove translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a founda-tion of most MT systems, our method have a wider application.

Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l 0 prior to the estimation of prob-ability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach.

Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., Figure 1: Examples of supervised word alignment. (a) gold alignment; (b) standard EM (GIZA++); (c) Leave-one-out alignment (proposed). 2011), also attempt to address the issue of over-fitting, however EM algorithms related to the pro-posed method have been shown to be more effi-cient (Wang et al., 2014). This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation. The main notation used in this section is shown in Table 1. 3.1 Standard EM for IBM Models 1, 2 and To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data. In order to maximize the likelihood of the alignment model  X  given the data S , the fol-lowing two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney, 2003),
Expectation Step (E step) : calculating the con-ditional probability of alignments for each sen-tence pair, P ( a | s ,  X  ) = where  X  ali ( i | i  X  , I ) is the alignment probability and  X  lex ( f | e ) is the translation probability. Note that f a foreign sentence ( f 1 , . . . , f J ) e an English sentence ( e 1 , . . . , e I ) s a sentence pair ( f , e ) a an alignment ( a 1 , . . . , a J ) where f j is
B i a list of the indexes of the foreign words
B i,k the index of the k -th foreign word
B i is the average of all elements in B i  X  i the largest index of an English word  X  i the fertility of e i
E i the word class of e i  X   X  an probabilistic model  X  a leave-one-out probabilistic model for x ( s , a ) the number of times that an event x
N x ( s ) the marginal number of times that an Table 1: Main Notation. Note that N x ( s ) = P IBM models 1, 2 and HMM model, this summa-tion is performed by dynamic programming; for IBM model 4, it is performed approximately us-ing the best alignment and its neighbors. (1) is a general form for IBM model 1, model 2 and the HMM model.

Maximization step (M step) : re-estimating the probability models, where N i  X  ,I ( s ) is the marginal number of times e i  X  is aligned to some foreign word if the length of e is I , or 0 otherwise; N i | i  X  ,I ( s ) is the marginal number of times the next alignment position after i  X  is i in a if the length of e is I , or 0 otherwise; n e ( s ) is the count of e in e ; N f | e ( s , a ) is the marginal number of times e is aligned to f . 3.2 Leave-one-out EM for IBM Models 1, 2 Leave-one-out EM for WA differs from standard EM in the way the alignment and translation prob-abilities are calculated. Each sentence pair will have its own alignment and translation probability models calculated by excluding the sentence pair itself. More formally, leave-one-out EM for WA are formulated as follows,
Leave-one-out E step : employing leave-one-out models for each s to calculate the conditional probability of alignments P ( a | s ,  X   X  s ) = one-out alignment probability and translation probability, respectively.

Leave-one-out M step : re-estimating leave-one-out probability models, 3.3 Standard EM for IBM Model 4 The framework of the standard EM for IBM Model 4 is similar with the one for IBM Models 1, 2 and HMM Model, but the calculation of align-ment probability is more complicated.

E step : calculating the conditional probabil-ity through the reverted alignment (Och and Ney, 2003), where B 0 means the set of foreign words aligned with the empty word; P ( B 0 | B 1 , . . . , B I ) is as-sumed to be a binomial distribution for the size of B 0 (Brown et al., 1993b) or an modified distri-bution to relieve deficiency (Och and Ney, 2003).
The distribution P ( B i | B i  X  1 , e i ) is decomposed as  X  where  X  fer is a fertility model;  X  hea is a probabil-ity model for the head (first) aligned foreign word;  X  oth is a probability model for the other aligned foreign words.  X  hea is assumed to be conditioned on the word class E  X  (Och and Ney, 2003) and the implementation of GIZA++ and CICADA.

M step : re-estimating the probability models, where  X  i is a difference of the indexes of two for-eign words. 3.4 Leave-one-out EM for IBM Model 4 The leave-one-out treatment were applied to the three component probability models  X  fer ,  X  hea and  X  oth of IBM model 4.

Leave-one-out E step : calculating the condi-tional probability through leave-one-out probabil-ity models P ( a | s ,  X   X  s ) = P ( B 0 | B 1 , . . . , B I )  X 
Leave-one-out M step : re-estimating the leave-one-out probability models, 3.5 Handling Singletons Singletons are the words that occur only once in corpora. Singletons cause problems when apply-ing leave-one-out to lexicalized models such as the When calculating (6) and (14) for singletons, the denominators become zero, thus the probabilities are undefined.

For singletons, there is no prior information to guide their alignment, so we back off to uniform distributions. In that case, the alignments are pri-marily determined by the rest of the sentence.
In addition, singletons can be in the target side abilities become zero. This is handled by setting a minimum probability value of 1 . 0  X  10  X  12 , which was decided by pilot experiments. 3.6 Implementation Details To alleviate memory requirements and increase speed, our implementation did not build or store the local alignment models explicitly for each sen-tence pair. The following formula was used to effi-ciently calculate (5), (6) and (14 X 16) to build tem-porary probability models, where x is a alignment event. Our implemen-tation maintained global counts of all alignment events cal counts N x ( s ) from each sentence pair s . clulated as, The global counts to be maintained are P are memory cost is, where |E| is the size of English vocabulary, |F| is the size of foreign language vocabulary, I s is the length of the English sentence of s , and J s is the length of the foreign sentence of s .

The calculation of the leave-one-out translation model is performed for each English word and for-eign word in s . Therefore, the time cost is,
In addition, because the local counts N ( f and n e ternal memory such as a hard disk will not slow down the running speed much. This will reduce the memory cost to This cost is independent to the number of sentence
The speed of the proposed method can be boosted through parallelism. These calculations on each sentence pair can be performed indepen-dently. We found empirically that when our im-plementation of the proposed method is run on a 16-core computer, it finishes the task earlier than The proposed WA method was tested on two language pairs: Chinese-English and Japanese-English (Table 2). Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks. GIZA++ and our own implementation of standard EM were used as baselines. 4.1 Experimental Settings The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT cor-pus. They are from the same domain, both con-tain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free contains a set of 1,235 sentence pairs that are man-ually word aligned.

The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word were segmented into words using the Kyoto Text 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.

GIZA++ was run with the default Moses set-tings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We imple-mented the proposed leave-one-out EM and stan-dard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations.

The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watan-implemented aligner AGRIPPA, to support our in-house decoders OCTAVIAN and AUGUSTUS.

In all experiments, WA was performed indepen-dently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F 1 (Och and Ney, 2003). The proposed method gave rise to higher quality align-ments in all our experiments. The improvement in F 1 , precision and recall based on IBM Model 4 is in the range 8.3% to 9.1% compared with the GIZA++ baseline, and in the range 5.0% to 17.2% compared with our own baseline.

The most meaningful result comes from the comparison of the models trained using standard EM log-likelihood training, and the proposed EM leave-one-out log-likelihood training. These mod-els are identical except for way in which the model likelihood is calculated. In all our experiments the proposed method gave rise to higher quality align-ments. The standard EM implementation achieved tences.
 1 P R F 1 P R alignment performance approximately compara-ble to GIZA++, whereas the proposed method ex-ceeded the performance of both implementations. 4.3 End-to-end Translation Quality BLEU scores achieved by the phrase-based and from different alignment results, are shown in Table 4. Each experiment was conducted three times to mitigate the variance in the results due to MERT. The results show that the proposed align-ment method achieved the highest BLEU score in all experiments. The improvement over the base-line is in range 0.03 to 1.03 for phrase-based sys-tems, and ranged from 0.43 to 1.30 for hierarchical systems.

Hierarchical systems benifit more from the pro-posed method than phrase-based systems. We think this is because that hierarchical systems are more sensitive to word alignment quality than phrase-based systems. Phrase-based systems only Figure 2: Curve of word alignment accuracy ( F 1 ) under training corpora of different sizes. 1 P R F 1 P R take contiguous parallel phrase pairs as translation rules, while hierarchical systems also use patterns made by subtracting (inner) short parallel phrases from (outer) longer parallel phrases. Both the outer and inner phrases typically need to be noise-free in order to produce high quality rules. This puts a high demand on the alignment quality. 4.4 Effect of Training Corpus Size Training corpora of different sizes were employed to perform unsupervised WA experiments and MT experiments (see Tables 5 and 6).

The training corpora were randomly sampled from the Chinese-English manual WA corpora and the parallel training corpus. The manual WA cor-pus has a priority for being sampled so that the gold WA annotation is available for MT experi-MT; (b) Hierarchical MT. ments.

The settings of the unsupervised WA experi-ments and the MT experiments are the same with the previous experiments. In the WA experiments, GIZA++, our implemented standard EM and the proposed leave-one-out EM are applied to training corpora with the same parameter settings as the previous. In the MT experiments, the WA results of different methods and the gold WA (if available) are employed to extract translation rules; the rest settings including language models, development and test corpus, and parameters are the same as the previous.

On word alignment accuracy, the proposed method achieved improvements of F 1 from 0.041 to 0.090 under the different training corpora (Table 5. The maximum improvement compared with GIZA++ is 0.069 when the training corpus has 4,000 sentence pairs. The maximum improvement compared with our own implement is 0.090 when the training corpus has 64,000 sentence pairs.
Figure 2 shows that the extent of improvements slightly changes under different training corpora, but they are all quite stable and obvious.

On translation quality, the proposed method achieved improvements of BLEU under the dif-ferent training corpora. The improvements ranged from 0.19 to 1.72 for phrase-based MT and ranged from 0.25 to 3.02 (see Table 5). The improve-ments are larger under smaller training corpora (see Figure 3).

In addition, the BLEUs achieved by the pro-posed method is close to the ones achieved by gold WA annotations. The proposed method slightly outperforms the gold WA annotations when us-ing the full manual WA corpus of 18,057 sentence pairs. 4.5 Comparison to l 0 -Normalization and The proposed leave-one-word word align-ment method was empirically compared to l and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) 12 . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem. This enables the probability distributions on rare words to be estimated more effectively. In this way, these two GIZA++ variants are related to the proposed method. l 0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES. For the settings of l -normalized GIZA++ that are not in common with GIZA++ were the default settings. As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1  X  4 and HMM model were turned on.

The experimental results are presented in Ta-ble 7. The experiments were run on the Chinese-English language pair. The word alignment qual-ity was evaluated separately for all words and for various levels of rare words. The leave-one-out method outperformed related methods in terms of precision, recall and F 1 when evaluated on all words.

Rare words were categorized based on the num-ber of occurences in the source-language text of the training data. The evaluations were carried out on the subset of alignment links that had a rare word on the source side. Table 7 presents the results for thresholds 1, 2, 5 and 10. The proposed method achieved much higher preci-sion on rare words than the other methods, but performed poorly on recall. The Kneser-Ney Smoothed GIZA++ had higher recall. The ex-planation might be that the leave-one-out method punishes rare words more than the Kneser-Ney smoothing method, by totally removing the de-rived expected counts of current sentence pair from the alignment models. This leads to rare words being passively aligned. In other words, the leave-one-out method would align rare words un-less the confidence is high. Therefore, we plan to seek a method to integrate Kneser-Ney smoothing into the proposed leave-one-out method in the fu-ture work.
 The BLEU scores achieved by phrase-based SMT and hierarchical SMT for different align-ment methods are presented in Table 7. The proposed method outperforms the other methods. The Kneser-Ney Smoothed GIZA++ performed the second best. We tried to further analyze the relation between word alignment and BLEU, but found the analysis was obscured by the many processing stages. These stages include paral-lel phrase extraction (or translation rule extraction from hierarchical SMT), log-linear model, MERT tuning and practical decoding where a lot of prun-ing happened. This paper proposes a leave-one-out EM algo-rithm for WA to overcome the over-fitting prob-lem that occurs when using standard EM for WA. The experimental results on Chinese-English and Japanese-English corpora show that both the WA accuracy and the end-to-end translation are im-proved.

In addition, we have a interesting finding about the effect of manual WA annotations on train-ing MT systems. In a Chinese-English parallel training corpus of 18,057 sentence pairs, the man-ual WA annotation outperformed the unsupervised WA results produced by standard EM algorithms. However, the unsupervised WA results produced by proposed leave-one-out EM algorithm outper-formed the manual WA annotation.

Our future work will focus on increasing the gains in end-to-end translation quality through the proposed leave-one-out aligner. It is a interest-ing question why GIZA++ achieved competitive BLEU scores though its alignment accuracy mea-sured by F 1 was substantially lower. The answer to this question which may reveal essence of good word alignment for MT and eventually help to im-prove MT. In addition, we plan to improve the pro-posed method by integrating Kneser-Ney smooth-ing.
 We appreciated the valuable comments from the reviewers.
