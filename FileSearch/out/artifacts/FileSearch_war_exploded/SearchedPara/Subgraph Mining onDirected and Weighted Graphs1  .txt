 Today X  X  complex data can often be described by graphs. The nodes in a graph are objects while the edges illustrate connections between the objects. Exam-ples include biological networks or socia l networks. A common property of such graphs is the existence of densely connected subgraphs. These clusters or com-munities are separated by less dense regions in the graph. We can gain a benefit of finding such interesting subgraphs. Based on a biological network, the devel-opment of useful drugs deduced from functional modules in protein interaction networks, is one example. In online commercial systems one can use interesting subgraphs for target delivery of custome rs. Customers in the same interesting subgraph show similar behavior.

Beside the connections between the obj ects in many cases also the directions and the weights are given. Let us consider a graph that represents the network traffic e.g. of the Internet. Edges that connect the routers of an ISP usually have higher weights/traffic amount than edges to nodes reflecting an end-user PC. These weights are important to identify the core and hence the dense subgraph of the total graph. Second, in general end-users generate more downlink traffic than uplink traffic; thus, the ingoing and outgoing edges are not identical and should be treated separately. Another example is an author graph where the edge weights can be interpreted as the number of co-written papers and the direction as the first author vs. co-author relationship. Overall, the identification of interesting subgraphs based on directed and weighted graphs is an important research field.
 Related Work. Several graph mining algorithms have been presented to the community. The identification of optimal dense subgraphs based on some ob-jective function is usually a hard problem [1], so that approximations or simple models are used. Some simple models for the detection of dense subgraphs in-clude the identification of cliques or more meaningful of quasi-cliques [2, 3]. These algorithms usually generate a huge output, even if we retain only the maximal quasi-cliques, and the subgraphs overlap to a very high extend, result-ing in marginal differences between subgraphs. Furthermore only undirected and unweighted graphs are used.

Another area is graph partitioning. Algorithms from this area try to divide the graph in flat parts; within these parts the nodes are strongly connected while between different subgraphs only loose connectivity exists. Models based on the maximum flow principle [4] or the k-partite graph partitioning [5] follow this paradigm. Another approach using physical principles is presented in [6]. Further techniques are the spectral cluster ing [7] or relational clustering [8]. One problem is that usually the number of interesting subgraphs or the size of the groups must be given. Furthermore, each node belongs to or is considered as a cluster even it is not well suited for this. Thus, the SCAN model [9] additionally identifies hubs or outliers; noise nodes are not included in the clusters. Directed and weighted graphs are not considered.

Moreover, several hierarchical/recurs ive methods exist that split up the graph step by step in smaller subgraphs, e.g. based on the cut-principle [10 X 12]. Par-adigms based on edge betweenness are also used [13] and were extended to allow overlapping subgraphs [14, 15]. However, expensive recalculations are often performed. The modularity [16 X 18] is another well know measure, which is used for the recursive identification of subg raphs. All methods g enerate a complete hierarchy of interesting subgraphs. Each cut through this hierarchy represents a more or less meaningful partitioning of the graph in interesting subgraphs. However, many cuts are possible and the user is cluttered with a huge amount of subgraphs. Additionally, the construction of a complete hierarchy results in a high runtime.
 Our Contributions. Our model is able to determine the interesting subgraphs on directed and weighted graphs. The dir ection of edges is a particular aspect, which leads to different definitions of dense subgraphs. Furthermore we present a definition of our interesting subgraphs based on the principle of density calcu-lation. The number of clusters and their sizes are automatically determined and the clusters are not obfuscated by noisy nodes, i.e. nodes that do not belong to any interesting subgraph. In this section we present our model for the mining of interesting subgraphs. Section 2.1 starts with some preliminaries. In Section 2.2 the density calculation on graphs is described, followed by our dense subgraph definitions in Section 2.3. In Section 2.4 algorithmic aspects are di scussed and Section 2.5 concludes with some complexity results. 2.1 Preliminaries A directed and weighted graph G is a tuple ( V, E, w )withnodes V ,edges E  X  V  X  V , and a weighting function w : E  X  R + .Apathin G is a list of nodes nodes M ,ifthereexistsapath &lt;p 1 ,...,p n &gt; with p 1 = u , p n = v and p i  X  M . Usually M = V , i.e. one can use all nodes in the graph for the path.
For density calculation, each object influences any other object with a cer-tain value. The sum of influences determines the density of an object/a point in the data space. The influence value is usually based on two criteria. First, one calculates the distance between the objects, e.g. for two vectors o , p the Euclidean distance d 2 ( o, p ). Second, the distances are wei ghted according to a weighting function W : R  X  R . For this purpose often kernel functions are used as the Uniform, the Gaussian or the Epanechnikov kernel. The Epanechnikov kernel is known to be efficient and effective [19]. The overall influence of an object o on to scale the distances. The smaller the distance between two objects the higher is their influence on each other. The overall density of an object p is then calculated as the sum of influences inf luence ( o, p ) for each object o in the database. 2.2 Density Computation on Graphs The challenge in our task is to consider the underlying graph structure to define a meaningful density calculation. Thus, in a first step we have to use graph based distances. Nodes that are  X  X tronger X  connected, i.e. are more similar with respect to the selected distance function, should have a higher influence on each other. For ease of presentation we assume that smaller edge weights correspond to higher connectivity of the nodes. If th e reverse is true, e.g. the number of co-writtenpapersshouldbehighforastr ong connection between two nodes in an author graph, we can simply transform the weights to 1 /w ( u, v ). In our approach we use the shortest path distance between two nodes s, d as a representative for a graph based distance function.
 Definition 1. Shortest path distance and influence Given a weighted graph G =( V, E, w ) , the shortest path distance between node s and d is defined as The influence based on the Epanechnikov kernel is defined by By this definition the influence decreases quadratically with the distance and the influence of a node s is not restricted to its direct neighbors but we affect also nodes that are reachable over longer paths. Thus, the connectivity of the nodes is more precisely reflected. In Figure 1 the node d 3 is influenced by s even though it is not a direct neighbor. Furthermore, we do not simply count the number of near located nodes but we weight them according to their distance. Or even stronger, with the non-linear weighting of W a single very close node results in a higher influence than two not as close nodes. Due to the compact support of the Epanechnikov kernel W we only have to calculate the shortest path distances up to h , i.e. we do not need to analyze all nodes; in contrast to functions with a non-compact support as the Gaussian kernel. Thereby we increase the efficiency of our method. In contrast to classical density computation our influence function need not to be symmetric, i.e. inf luence ( s, d ) = inf luence ( d, s ) is possible. This non-symmetry is particularly appropriate for directed graphs.

Another important aspect in graphs is the possible influence on nodes that are at a first glance not influenced. In Figure 1 for example the edge between the nodes s and d 2 has a too high weight; thus, s does not influence d 2 based on this edge. However, we can use a  X  X et our X  including other nodes to get an positive influence. Therefore, we distinguish two different node sets: First, the overall set of nodes on which the node s has influence on, the so called influence region. Second, the directly influenced nodes that correspond to closely located neighboring nodes.
 Definition 2. Influence region and direct influence Given a graph G and a node s , the influence region region ( s ) of s is defined by The set direct ( s ) contains all nodes that are directly influenced by s and is de-fined by This distinction is based on the intuition that one only interacts with good friends that are directly known to oneself, e.g. to spread a rumor. However, a rumor spreaded by a person can also reach persons not only in its neighborhood, but along a path of good friends. In Figure 1 we get region ( s )= { s, d 1 ,d 2 ,d 3 } and direct ( s )= { d 1 } and obviously direct ( s )  X  region ( s ) holds. By our definition, the influence region is connected. Even stronger, s can reach all other nodes in its influence region.

Now we are able to calculate the density of a node d . We have to determine all nodes that have an influence on d ; in contrast to the nodes which d itself influences, according to the non-symmetry. We call this set of objects the reverse influence region and define revRegion ( d )= { s  X  V | d  X  region ( s ) } . The density of a node d is the sum of all influences from the objects in this set. Definition 3. Density of a node The density of a node d is defined by In Figure 1 the reverse influence region of node d 2 is { s, d 1 ,d 2 } . The actual density can be calculated based on the individual influence values. The higher the density the more interesting is the node in our graph. 2.3 Density Based Clusters on Graphs Our idea is to identify interesting subgraphs, i.e. subgraph clusters, by dense areas that are separated by sparse areas. For vector spaces similar approaches show good performance even in the presence of noise [20, 21]. We will call a node dense if its density exceeds a threshold  X  . All nodes fulfilling this minimal density criterion are the core nodes, as these nodes are the candidates that build the cores of different clusters.
 Definition 4. Core nodes Given a graph G and a minimal density  X  , the set of core nodes is defined by Starting with a core node, we add further core nodes, which are in a certain range of the node, to our cluster to let it grow. In this step we have to take care of the underlying graph structure. Two aspects are important and have to be considered.
 Aspect 1: We have to grow the clusters along our direct influence definition. A node only interacts with the nodes in its direct influence region; thus, if these nodes are core nodes they should belong to the same cluster. In Figure 2 a directed graph is given where the core nodes are marked with solid lines and the non-core nodes with dashed lines. F or ease of presentation we do not show the densities or edge weights. If we select v 1 , the nodes { v 3 ,v 4 } = direct ( v 1 )  X  coreN odes should correspond to the same dense region and hence to the same interesting subgraph. Because a direct ed graph is given, also the predecessors { s  X  coreN odes | v 1  X  direct ( s ) } = { v 2 } have to be included. This is equivalent to v 1  X  direct ( v 2 )  X  coreN odes . The same procedure is now applied for v 2 , v 3 and v 4 , so that we add the nodes v 5 and v 6 to the cluster.
 Definition 5. Core of a cluster A non-empty subset C  X  coreN odes is a core of a cluster iff and C is minimal among these sets. By this definition, a cluster grows until for each node the required property is fulfilled. In Figure 2 the two minimal sets that fulfill this property are highlighted. These minimal sets are the dense areas that build our clusters cores. In Figure 3 we show examples for the identification of two different cores to point out the advantages of our definition. In Figure 3(a) we get two cores even though the two core nodes v 1 and v 2 are connected. The connectio nisonlyveryloose;thus, both nodes do not influence each other. In Figure 3(b) we get two cores even if v 1 and v 2 are reachable over a path. This path, however, has to use the non-core node v 3 . This non-core node indicates the existence of two different clusters. Aspect 2: The second aspect we have to cons ider is the non-symmetry of our direct influence region, i.e. we can have d  X  direct ( s )  X  s  X  direct ( d ). Why are directed graphs a particular challenge? For this, we first consider the Definition 5 for undirected graphs. Consequently with our model we get d  X  direct ( s )  X  s  X  direct ( d ) and Definition 5 simplifies to  X  v  X  C :  X  s  X  coreN odes : s  X  direct ( v )  X  s  X  C for a minimal non-empty set C .

A useful property for interpreting the cores is the core path property. A path (within a cluster core) that uses only direct influence edges and that connects s with d is called a core path for s and d . Formally we define the existence of such a path with respect to a cluster core C and two nodes by: coreP ath C ( s, d )= TRUE  X  X  X  v 1 ,...,v n  X  C : In Figure 4(a) the node set C = { v 1 ,...,v 7 } is a valid cluster core. The nodes v 4 and v 7 for example are connected via a core path. The path &lt;v 4 ,v 1 ,v 5 ,v 7 &gt; uses only nodes within C and each successor is in the direct influence region of its predecessor. If each pair of nodes within the core C is connected via a core path, C fulfills the core path property. In Figure 4(a) this property holds. Definition 6. Core path property The core C fulfills the core path property iff
One can prove that a cluster core in an undirected graph always fulfills the core path property. Furth ermore, a cluster core C is maximal with respect to the core path property, i.e. there exists no C  X  C that fulfills also the core path property. In Figure 4(a) we cannot add further nodes to C without violating the core path property. Thus, in an undirected graph the cluster cores correspond to the maximal sets that fulfill the core path property.

For a directed graph the Definition 5 and the maximal sets with respect to the core path property do not necessarily lead to the same results. In Figure 4(b) we get the cluster core C = { v 1 ,...,v 7 } . However, as one can see the node v 4 is not connected to v 7 via a core path; the nodes v 3 and v 5 are not connected at all. In directed graphs the core path property is a more restrictive criterion for a cluster. The Definition 5 is fulfilled by the nodes { v 1 ,...,v 7 } in Figure 4(b) while are highlighted in Figure 4(b). Thus, for directed graphs we can define another stronger definition for a core of a cluster: Definition 7. Strong core of a cluster A non-empty subset C  X  coreN odes is a strong core of a cluster iff C fulfills the core path property and C is maximal.
 Obviously each strong core SC isasubsetofacore C , i.e. SC  X  C .Thus,we have the restrictive strong core property , which yields small cluster cores, and the weaker core property, which yields larg er clusters. We want to analyze a further version in between these extremes. In cont rast to the strong core property, where each pair of nodes is reachable in both directions, we make a relaxation that only requires a core path in one direction.
 Definition 8. Semi-strong core of a cluster A non-empty subset C  X  coreN odes is a semi-strong core of a cluster iff and C is maximal.
 In Figure 4(b) for example the node set C = { v 1 ,v 2 ,v 3 ,v 4 } forms a semi-strong core. The node v 3 can reach all nodes in C via a core path, v 2 the nodes { v 1 ,v 4 } and v 1 the nodes { v 2 ,v 4 } . For each pair we get at least one core path in a single direction. Precluster and postcluster. The cores are the most important nodes that form the clusters. Additionally, two other sets of nodes can be defined. Given a cluster core C , the densities of all nodes w ithin the core exceed a certain threshold. Thus, an interesting node set contains all nodes that account for the density of the core, i.e. removing one of these nodes the densities of the core nodes change. We call this set a precluster. On the other hand we can also define the postcluster of C . This set contains all objects that are influenced by the core.
 Definition 9. Precluster and postcluster Given a cluster core C , the precluster Pre ( C ) and postcluster Post ( C ) contain the nodes 2.4 Graph-Theoretic View and Algorithmic Aspects In the following we want to point out a graph-theoretic view of our model that helps us to implement an efficient algorithm. We first transform our graph. We remove all non-core nodes and those edges that do not contribute to a direct influence. Only along the remaining nodes and edges a core could grow. Overall, this so called residual graph is defined by V = coreN odes and E = { ( s, d )  X  E | d  X  direct ( s )  X { s, d } X  coreN odes } . In Figure 5 we show an original graph; non-core nodes are highlighted with dashed lines. The two edges labeled with  X  X oise edge X  should indicate, that v 3 and the other node are not directly influ-enced by v 2 even if they are connected. If we remove these two edges as well as the nodes n 1 and n 2 , the residual graph on the right is obtained. The weak components [22] of the residual graph are our cluster cores following Definition 5. The strong components [22] build our strong cluster cores following Definition 7. In Figure 5 (right) we highlighted these node sets. The semi-strong cores obey a more complex structure. D erived from the strong components we can construct a quotient graph. Each node in the quotient graph corresponds to a strong component (cf. Fig. 6). T he nodes are connected with a directed edge if at least one edge in the original graph exists between these node sets. This quotient graph is a DAG (directed acyclic graph). Each maximal path from a root node to a leaf node is then a semi-strong component. In Figure 6 we get the the two semi-strong cores. Thus, the strong, semi-strong and weak components in the residual graph correspond to the definitions of the cluster cores. Due to space limitations we omit the proof.

A comparison of the three definitions yields an interesting conclusion. While the weak and strong compon ents are disjoint sets, the semi-strong node sets can overlap. The cores of our clusters following the semi-strong definition are allowed to share some objects. This overlap or disjointness is already visible in our example in Figure 5 and 6 respectively.

Based on the graph-theoretic properties, our algorithm can efficiently deter-mine the interesting subgraphs. We call our algorithm GDens , due to the density calculation in graphs. First, for our density calculation we have to determine the shortest path distances. We use an adaption of Dijkstra X  X  algorithm. By this we can use an early stopping of the density calculation if the distances exceed the maximal distance h . We have to apply Dijkstra X  X  algorithm for each node to determine the overall densities.

After the density calculation step we have to determine the cores of the clus-ters. For our weak core definition we have to identify the weak components in the residual graph; this can be done by a depth-first search procedure. The strong-components and hence the quotient g raphs within each weak component are identified by Tarjan X  X  algorithm. Finally, we generate the maximal paths within the quotient graphs to identify the semi-strong cores.

Summarized, our GDens utilizes efficien t graph algorithm methods for iden-tifying the interesting subgraphs. Our beforehand defined core definitions can be reduced to well known properties in the residual graph and hence emphasize the use of these cluster cores. In total, we get the possibility to flexibly identify and interpret different interesting subgraphs based on the density calculation technique in directed and weighted graphs. 2.5 Complexity Analysis We briefly analyze the complexity of our algorithm with respect to a graph G =( V, E, w ). Let us assume that in average the influence is larger than zero for a fraction of x percent of all nodes. Based on the complexity of Dijkstra X  X  algorithm and as we have to use Dijkstra X  X  algorithm for each node to determine the densities, the overall complexit y for the density computation step is Obviously in the worst case the influence is larger than zero for all nodes, i.e. x = 1, and we have a dense graph, i.e. O ( | E | )= O ( | V | 2 ). In this case we can infer the worst case complexity of If we assume the positive influence of a node is restricted to a constant number given, we get a complexity of In the second step, we have to determine the cores. The depth-first procedure for identifying the weak-components in the residual graph has a complexity of O ( | V | + | E | ). Tarjan X  X  algorithm for identifying the strong-components has a complexity of O ( V max + E max ), if V max is the maximal number of nodes for all weak components and E max is the maximal number of edges. For the semi-strong core definition we additionally have to generate the maximal paths within the quotient graph. Assuming that we identify k quotient graphs each with n strong components we get an additional complexity of O ( k  X  e n/e ) (proof skipped). In realistic scenarios we have k, n | V | and hence the term is almost negligible. In the next section we analyze the runtime and quality of our GDens algorithm. Setup. We use two variants of our algorithm. GDens (core) uses the identified cluster cores as the interesting subgraphs. GDens (all) includes the precluster and postcluster to build the interesting subgraphs. For comparison we use the algorithms of [6], called Voltage ,and[13],called Edge Betweenness . All input parameters are optimized. All implementations are in Java. For runtime and quality comparison we generate synthetic data following the methods in [13, 18] and adding uniformly distributed edge weights. In average, edge weights within clusters are two times smaller than edge w eights between clusters. As not stated otherwise we hide 20 clusters with 10 . 000 nodes and 30 . 000 edges. Clustering quality is measured with the F1 score [23, 24]. The F1 value is the harmonic mean of precision and recall, i.e. an ide ntified subgraph has to detect most of the nodes (recall) but also only the nodes (precision) of a hidden cluster. Keep in mind that for our algorithm two F1 scores can be calculated based on GDens (core) or GDens (all).
 Cluster core definitions. In Figure 7 we analyze the effects of our different cluster core definitions. On the left the F1 value is illustrated. The solid bars correspond to the quality for GDens (all). The white bars inside correspond to the quality if only the core is considered, i.e. GDens (core) is used. The quality of GDens (core) decreases slightly with a more restrictive core definition. The cores following the strong cluster co re definition are very small and hence we cannot detect all nodes of the clusters. Ho wever, if we include the pre/postcluster the quality for all cluster core definitions is high. In the middle the runtime is analyzed. The weak and semi core definitions run in nearly equal time, while the semi-strong core needs more calculations. The determination of the paths within the quotient graph is a more complex task. On the right, the number of subgraphs in the mining result is printed. As expect ed the weak core definition yields the fewest clusters, while the other two definitions split the weak component in several smaller ones. In the following experiments we focus on the weak core definition as the quality is high and the runtime is low.
 Noise. In Figure 8 we increase the number of noise edges in the graph, i.e. we add edges that do not belong to the communities. The higher the value the more difficult is the identification of interesting subgraphs. The quality of GDens (core) is not affected by adding noise; th e dense areas are still identified. The high quality of GDens (all) decreases slightly because the pre/postcluster include more and more nodes that do not belong to the cluster. However, even for very high percentages of noise GDens shows in both variants high quality. The quality of the Voltage algorithm remains unchanged with very low quality results. Edge betw. reaches a very high quality for zero p ercentage of noise but then it rapidly decreases. Both algorithms cannot identify the true hidden clusters.
 Number of nodes. In Figure 9 (left) we plot the runtime of the algorithms with respect to the number of nodes. Our GDens algorithm is far more efficient than the other approaches. The Edge betw. method did not even finish within 12 hours for a dataset with 12500 nodes. Additionally in Figure 9 (right) we analyze the quality. While the quality of GDens in both variants stays on a high level or even increases a bit, the quality of Voltage decreases. In large graphs this algorithm cannot identify good patterns. Edge betw. has always low quality. Hidden clusters. In the next experiment we analyze the effects if the num-ber of hidden clusters in the graph is altered. Due to the high runtime of Edge betw. the number of nodes is set to 2000. As depicted in Figure 10 (left), with increasing number of clusters the qualit y of GDens is not or only less affected. With increasing number of clusters and fixed number of nodes, the interesting subgraphs get smaller and hence their identification is harder. The Edge betw. algorithm shows a similar decrease but on a much lower quality level. Interest-ingly the quality of Voltage increases. The algorithm is better in the detection of small clusters. However, the quality of GDens (all) is never reached by this algorithm. Considering the runtime of the algorithms in Figure 10 (right), we see that the quality increase of Voltage is paid with a high and also increasing runtime. The runtime of GDens is orders o f magnitudes lower. Furthermore, with more and hence smaller subgraphs the runtime even decreases.

Summarized, our GDens outperforms the competing algorithms in quality as well as in runtime. It is able to detect the interesting subgraphs also in noisy settings and automatically identifies the number of clusters.
 Parameter variation. In our next experiment in F igure 11 we vary the param-eter  X  . On the right y-axis we indicate the number of identified clusters. First, the number increases because the hidden clusters are split-up in several smaller ones due to a higher  X  .Afterwards,thenumberd ecreases to zero because no more core objects are identified. Correspondingly, the quality of the clustering (left y-axis) based on the core objects d ecreases. Consider ing GDens (all) the quality drops but at a later step. The objects that are included in the hidden clusters but not identified by the cores are now contained in the pre/postclusters. DBLP data. To analyze real world data, we use the DBLP data set where nodes represent authors and edges/edge-weig hts the number of co-written papers. We generate a graph based on all publications from 2004-2008 and we extracted the largest connected component with 228k nodes and 1458k edges. In Figure 12 we present some results with respect to a varying  X  value. The F1 value cannot be determined for this data set because the true clustering structure is not known. Instead we plot the number of identifie d cluster cores with more than 5 nodes, i.e. these clusters correspond to large collaboration groups. Additionally, the runtime of the algorithm is presented. Both measures decrease continuously, i.e. we identify less collaboration groups but increase the efficiency. We want to point out the high efficiency of our GDens algorithm also on this large DBLP data set. Unweighted graphs. In the next experiment we want to focus on the advantage of our algorithm to handle weighted graphs. In Figure 13 we show the difference in clustering quality if instead of the weighted graph an unweighted one is used, i.e. we ignore the weights by setting these to a constant value. As one can see in all cases the quality of the unweighted clustering is smaller. Especially if we only consider the core of the cluster (middle column) the quality decreases. This experiment supports the need for interesting subgraph mining algorithms that incorporate the weights of edges as our model does. We introduce a novel technique to identify interesting subgraphs using the method of influence functions for calculating the densities of nodes. Our model can handle directed and weighted graphs and we show in experiments that using this infor-mation increases the quality of the clustering result. We present three types of dense subgraphs that account for the direction of edges in the graph. Our GDens algorithm identifies the number of clusters automatically and it is robust with re-spect to noise. In experiments we demon strate the high quality and low runtime of our GDens algorithm compared to other subgraph mining methods.
 This work has been supported by the UMIC Research Centre, RWTH Aachen University, Germany.

