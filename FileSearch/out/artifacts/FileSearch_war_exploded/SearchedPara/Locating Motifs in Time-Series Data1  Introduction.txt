 Data clustering is one of the primary data mining tasks [4]. In [6], Keogh et al. made a surprising claim that clustering of time-series subsequences is mean-ingless. Their claim is based on the fact that a data point at a certain time in a time-series appears in m adjoining sliding windows where m is the window size. The mean of all such time-series subsequences will be an approximately constant vector, which makes any time-series subsequence clustering approaches meaningless. Finding motifs is proposed as an effective solution for this problem [2, 7, 8]. For finding motifs, a time-series subsequence of length m (or a time-series subsequence in a sliding window of size m ) is treated as a data point in an m -dimensional space. Two time-series subsequences of length m are similar, if the two corresponding m -dimensional data points are similar. The similar-ity is controlled by their distances, and the similar time-series subsequences are grouped as m -dimensional data points in a ball of radius r . A motif in a time-series dataset is then a dense ball with most data points after removing trivial matches in an m -dimensional space. Here, removing trivial matches is a process of removing those meaningless time-series subsequences that should not con-tribute to the density, and therefore is a solution to the meaningless time-series subsequence clustering problem [6]. For example, let a time-series subsequence of length m from a position l denoted as t [ l ] and assume that it is mapped into an are similar (within distance r ). Therefore, s i +1 should be removed, because the two corresponding time-series subsequences t [ i +1] and t [ i ] are almost the same, and counting such trivial matches makes clustering of time-series subsequences meaningless. Fig. 1 shows an example of a motif, which appears 3 times in a time-series after trivial matches have been removed.
 motifs (dense balls) or report the density of balls that is less than it should be, because they only consider existing data points as potential centers to find their neighbors in radius of r [2, 7, 8]. In real time-series data, it is highly possible that there exist most dense balls where no existing data points can be their centers. In this paper, we formalize this problem as a problem of finding top-k motif balls in a dataset S ,inan m -dimensional space, R m , where a motif ball is a ball, with a user given radius r , that contains most data points and is not contained in other balls. We do not limit the number of potential centers of balls to |S| . In other words, the potential centers are countless, because they can be anywhere in the region covered by the dataset S . That makes it possible for us to find better and/or more motif balls and is challenging. Contributions: The main contributions include the following.  X  We formalized the problem of finding motifs as finding continuous top-k motif balls by allowing non-existing data points in a dataset S as centers of balls of radius r .  X  We proposed effective heuristic approaches to find continuous top-k motif balls. Our approach first identifies a set of candidate sets where each candidate set is enclosed by a ball whose maximal radius is 1 1
The set of candidate sets identified are minimized in order to reduce redundant computation. Second, we approach two simple but effective heuristics to find motif balls of radius r within each candidate set.  X  We conducted extensive experimental studies and our results showed that our approaches can significantly improve the quality of motifs found in time-series datasets.
 Organization: Section 2 gives the problem definition. The algorithms for find-ing k -ball are given in Section 3. Experimental results are given in Section 4. Finally, Section 5 concludes the paper.
 We say t [ i ] t [ j ]if i&lt;j . Consider a subsequence t [ i ]asan m -dimensional data point s i . In other words, there exists a one-to-one mapping  X  from t [ i ]to s i such as  X  ( t [ i ]) = s i and  X  S = { s 1 ,s 2 ,  X  X  X  s n } X  R m where s i =  X  ( t [ i ]) and n = | T | X  m +1. { s between two data points, and d () is a distance function (for example, Euclidean distance). Note: as a unique feature of this problem, a ball B is defined with a center c which does not necessarily belong to S but R m . In the following, we call a ball whose center is in S a discrete ball , and a ball whose center is not in S a continuous ball .A discrete ball is contained by some continuous ball when their radii are the same. Below, otherwise stated, a ball is a continuous ball. radius and the set of points on the boundary of the ball, respectively. The density of a ball is the total number of data points in B , denoted as | B | . In addition, given a set of data points D ,weuse ball ( D ) to denote the smallest enclosing ball that contains all data points in D .A ball function can be implemented using a move-to-front strategy as the miniball function given in [9, 3]. Let ball r ( D )bea boolean function which returns true if the radius of ball ( D )islessthanorequal to r ( rd ( ball ( D ))  X  r ) and otherwise false . We simply say a set of data points, s and there does not exist s k outside the r -ball such as t [ i ] t [ k ] t [ j ]. the highest density. B i is the top-1 motif ball after the corresponding r -balls for B ,  X  X  X B i  X  1 are removed from S . parison with finding discrete k -ball . Figure 2 shows an example where S = { s d ( s 4 ,s 1 ), are all equal to r + , for a very small &gt; 0. There does not exist one data point. In other words, all the discrete motif balls contain only a single data point. But, in fact, the four data points belong to a continuous motif ball whose center is o 1  X  the center of the four data points in S . Note o 1 /  X  X  and the number of times of calling the distance function d (), the time complexity of discrete motif balls is O ( n 2 ), because it basically needs to find those data points within a distance r for every data point in a dataset. Several efficient methods were proposed in [2, 7]. The time complexity of finding continuous motif balls is O (2 n ), in terms of the number of times of calling the function ball (), because it needs to check if any subset of the dataset can be a motif ball. It is important to note that the cost for continuous motif balls to check whether a set of data points is a r -ball is the complexity of the ball function which is O (( m + 1)!( m +1) n ) in an m -dimensional space [3]. When the dimensions are less than 30, the ball function [3] is efficient. When the dimensions are large, the ball function becomes noticeably slow. Hence, effective heuristics are on demand to find better and more motif balls with reasonable overhead, which is the focus of this paper. Our k -ball algorithm is given in Algorithm 1, which takes three parameters: a data set S , a radius r ( &gt; 0) and a positive number k&gt; 0. First, it generates a set of candidates (line 2). Second, it calls allBall () to find potential motif balls which will be stored in a tree structure (lines 3-5). Third, it calls top-k -ball () to report k -ball s (line 6). Below, we discuss i) generating candidate sets (the function gen ), ii) finding motif balls, and iii) identifying k -ball s. Algorithm 1. k -ball ( S ,r,k ) 3.1 Candidate Set Generation We take a divide-and-conquer approach to divide the entire set of S into subsets, C ,  X  X  X  ,C |S| , and identify motif balls in each C i such as the set of motif balls is the union of motif balls found in every C i . In doing so, for a given dataset S , the complexity of the continuous motif balls changes from 2 n to n  X  2 l , with the hope that l n , where n = |S| and l is the maximum size of a subset C i . The similar approach was also taken in [2, 7]. Here, we call C i a candidate set is enclosed in a smallest enclosing ball, ball ( C i ), whose center is s i  X  X  , and its radius, rd ( ball ( C i )), is less than or equal to 2 r .
 Lemma 1. Any motif ball, if it exists in S , then it exists in a candidate set C i . a ball of radius 2 r , can be large, and the number of motif balls of radius r in a radius-2 r ball can be considerably large. In this paper, we further divide C i into a set of subsets, C i j , such as the distance of every two data points in C i j is less than or equal to 2 r . We call such a C i j as a min candidate set in a sense that it may miss a motif ball if we further remove a data point from C i j . There are many such min candidate sets in C i , we call a min candidate set, C i j (  X  C i ), as a maxmin candidate set if there does not exist C i k (  X  C i ) such as C i j  X  C i k . Identifying a maxmin candidate set can significantly reduce computational cost, because all motif balls found in C i j can be identified in C i k .
 Lemma 2. Any motif ball B ,ifitexistsin S , exists at least in a maxmin can-didate set C i j .
 candidate sets. The set of motif balls to be found is the union of all motif balls found in all maxmin candidate sets.
 Lemma 3. In an m -dimensional space, the maximum radius of the smallest en-closing ball for a maxmin candidate set is 1 1 +  X  , the maximum radius is C l k is a maxmin candidate set in a candidate set C l . It is possible that the same maxmin candidate set may appear in different candidate sets, such as C i l = C j k for i = j ,ora C i l in a candidate set C i is a subset of C j k , because we use a divide-and-conquer approach which first generates a set of candidate sets and then generates a set of maxmin candidate sets from each candidate set. The issue here is how to reduce such redundant computation. A naive solution is to generate a set of all maxmin candidate sets first and then remove redundant and obtain maxmin candidate sets. However, it requires a large memory space and long CPU time. We propose a simple but effective pruning strategy to ensure that a C i l is only computed once if there C i l = C j k ,and C i l will not be computed s as its center. Therefore, any maxmin candidate set C i l of C i must contain the data point s i . Remark 1. ( Filtering maxmin candidate sets )Given C i l , and let s k be the first data point in C i l following the order . The strategy is: C i l does not need to be computed if s k = s i where s i is the center of C i . It does not miss any motif balls because there must exist a maxmin candidate set C k j in C k such as C j k = C i l and the first data point in C k j is s k .
 is s i following . Suppose we filter maxmin candidate sets using Remark 1, we further claim that there is no such C i l  X  C j k , where C i l is a maxmin candidate set in C i whose center is s i ,and C j k is a maxmin candidate set in C j whose center is s j . We consider two cases, i) s i s j and ii) s j s i . There is no case element which indicates that s i cannot be in C j k , because of the filtering. There is no case ii), because if C i l  X  C j k , then the distance between s j and any data because, if so, there must exists C i l  X  C i l so C i l is removed, and C i l is removed because its first data point is s j but not s i . It implies that C i l does not exist. Lemma 4. Based on filtering maxmin candidate sets (Remark 1), the union of remaining maximin candidate sets { C i 1 ,C i 2 ,  X  X  X } from all candidate sets C i are sufficient to answer continuous k -ball and the entire maxmin candidate sets left are minimal such as there does not exist C i j  X  C k l A-priori property. The problem becomes to find all min candidate subsets that are not included in any other candidate subsets using existing maximal frequent itemsets mining approaches. Alternatively, the maxmin candidate sets can be identified as finding cliques in an undirected graph, G ( V, E ), for every candidate set C i . Here, V = C i (for a candidate set) and E  X  V  X  V such that ( v i ,v j )  X  E if d ( v i ,v j )  X  2 r . A clique in a graph is a maximal complete subgraph and a maxmin candidate is such a clique.
 can be processed efficiently for the following reasons. First, in a high dimensional space, due to the curse of dimensionality [5], the data is sparse. Second the radius r for measuring similarity is considerably small. Therefore, the number of data points in a candidate set (within radius of 2 r ) is limited. Note: the complexity of clique is linear with the number of nodes in a graph and is exponential with k which is the maximal complete subgraph. The number of nodes is small in our problem. The clique can be efficiently identified because possible maximum k is small accordingly. We adapt the clique algorithm [1] which uses branch-and-bound technique to cut off branches that cannot lead to a clique, and shows efficiency for identifying maxmin candidate sets in our experimental studies. 3.2 Finding All Balls Given a dataset S , we can obtain a set of maxmin candidate sets, C = {C native of the implementation of allBall () in Algorithm 1, to find motif balls in a maxmin candidate set C l . The all motif balls found in S are the union of all motif balls found in C l .  X  approx-Ball-1 : Given a maxmin candidate set C l ,let P be the data points on the boundary of the smallest enclosing ball of C l . It assumes that the estimated center of the smallest enclosing ball of C l is the mean vector of the data points in C l , and removes a data point in P from C l that is the furthest to the mean vector until a r -ball is identified. The algorithm is given in Algorithm 2.  X  approx-Ball-mean : Given a maxmin candidate set C l , it also assumes that the estimated center of the smallest enclosing ball of C l is the mean vector of the data points in C l . In each iteration, it computes the mean vector of data points and treats it as the center followed by removing those data points that are not within the distance r of the center and adding those within the distance r . The algorithm is given in Algorithm 3.
 Algorithm 2. approx-Ball-1 ( C l ,r ) Algorithm 3. approx-Ball-mean ( C l ,r ) 3.3 CB-tree In this section, we show how we maintain a set of motif balls for the final process of identifying k -ball s. A CB-tree is a tree that maintains all motif balls that are not contained by any others. In a CB-tree , a node represents a data point, a path from the root to a leaf represents a motif ball. The data structure for a
CB-tree consists of a header table and a tree structure. A node in the tree contains pointers to its children and a pointer to its parent. For each data point, there is an entry in the header table that contains a list of pointers pointing to the occurrences of the data points in different paths in the tree. The center and radius of a ball are kept in the leaf node of the corresponding path. Let T be a
CB-tree ,and B be a motif ball (a dataset). Several primitive operations are defined on T : search( T , B ) to search whether the superset of B (including itself) exists in T as a path, insert( T , B ) to insert B into T , subsets( T , B )toidentify a set of motif balls (paths) in T that are contained in B , delete( T , B ) to delete B from T , and condense( T ) to condense T .Atree T is not condensed if there are some edges that can be removed to represent the same balls. For example, either two paths s 1 .s 2 .s 3 and s 1 .s 3 .s 5 where both paths only have s 1 as their as their common prefix. The latter is condensed. The insertion of a motif ball into T in Algorithm 1 (line 5) is implemented as follows. First, it checks if there is a superset of B in T already by calling search( T , B ). If there is such a path representing a superset of B , then there is no need to do insertion. Otherwise, it inserts B into T and at the same time removes all motif balls (paths) that are contained in subsets( T , B ). Finally, it calls condense( T ) to make the tree condensed. We maintain a list of CB-tree s where each CB-tree maintains a set of motif balls for a candidate set C i with a header and a tree. 3.4 Identifying Top k -ball s Identifying top k -ball s can be implemented as operations on the list of CB-tree s. The longest path found in the list of CB-tree s is the top-1 motif ball. We remove edges from the leaf node of the longest paths if they are not shared by others, and keep its center. Suppose that we have already identified l motif balls where l&lt;k . We can easily identify the next motif ball if it has longest path after removing the nodes that are already in the former l motif balls. Trivial matches will be removed in this step. We implemented our continuous k -ball algorithm (Algorithm 1) with two heuristic algorithms for allBll() , namely, approx-Ball-1 (Algorithm 2) and approx-Ball-mean (Algorithm 3). Below we denote the two continuous k -ball algorithms as Ball-1 and Ball-m , respectively. Also, based on [2, 7], we imple-mented a discrete k -ball algorithm, denoted Ball-d , which first identifies a set of motif balls for S where a motif ball B i = { s j | d ( s i ,s j )  X  r } is at the cen-ter s i  X  X  , second, maintains the motif balls found in a CB-tree , and third identifies k -ball using the same routine of top-k -ball () in Algorithm 1. All algorithms use Euclidean distance function and were implemented using C++ in-cluding some optimizing techniques in [2, 7]. All experiments were performed on a Pentium IV 2.53GHz PC with 512MB memory, running Microsoft Windows 2000. We report our findings for the three algorithms Ball-1 , Ball-m and Ball-d . tick-by-tick real dataset (year 2001 and year 2002). We report our finding using two representative stocks, A and B . The lengths of A and B are 12,802, as shown in Fig. 3. Based on a time-series, T ,ofsize | T | , we generate an m -dimensional dataset S of size n = | T | X  m + 1, where m is the sliding window size. Due to space limit, we only report the results for m = 128. We normalized time-series subsequences in every sliding window into the range [-1,1]. We show our testing results using a radius r in the range of 0.8 and 1.15, because the similarity between every two adjoining time-series subsequences in the two time-series, A and B , forms a normal distribution where the peak value is 1.4 and majority values are in the range of 0.7 and 2.3. And r =0 . 8 is the smallest radius that the density of motif balls is greater than 1. the CPU time, we measure the quality of a motif ball using a centric value. The centric value takes two factors into consideration: the distance between the mean of all data points in a motif ball and the center of the motif ball, and the density of a motif ball. Here, all data points in a motif ball mean all remaining data points after removing the trivial matches and the density of a motif ball is the density of the remaining data points. Suppose B = { s 1 ,s 2 ,  X  X  X  ,s n } is a motif ball, with nm -dimensional data points. Let the mean of B be mean ( B )= 1 n n i =1 s i .The centric of B is given below. The smaller the centric value is, the better quality of 4.1 The Total Number of Motif Balls and The CPU Time Fig. 4 (a) and (b) show the total number of motif balls found for time-series A and B . Both Ball-1 and Ball-m significantly outperform Ball-d . The curves of Ball-1 and Ball-m decrease sometimes when r&gt; 1 . 0 in Fig. 4 (a) and (b), because when the radius r becomes larger, a motif ball can contain more data points. The CPU time for Ball-1 , Ball-m and Ball-d are shown in Fig. 4 (c) and (d). The CPU time includes the time for generating candidate sets, finding motif balls and inserting motif balls into CB-tree s. The CPU time does not include the time to report top-k balls from CB-tree s, because it penalizes the algorithms that find more and better motif balls. The CPU time for Ball-1 is higher than that for Ball-m , because Ball-1 needs to call ball function repeatedly, in particular when the radius of the smallest enclosing ball for a candidate set is large. The time of computing cliques takes such little time that could be ignored, because in a high-dimensional dataset, there are not many data points in a candidate set C i when the radius r is small, due to the curse of dimensionality [5]. 4.2 Top-k Motif Balls Fig. 5 (a) and (b) show the density of the top-1 motif ball for A and B separately, while varying the radius r , where m = 128. In all cases, Ball-1 and Ball-m outperform Ball-d , because both can report motif balls with more data points. Fig. 5 (c) and (d) show the centric values. Also, Ball-1 and Ball-m outperform Ball-d . Fig. 5 (c) and (d) suggest that the data points in the motif balls found by Ball-1 and Ball-m are close to the center of the motif balls, whereas the data points in the motif balls found by Ball-1 can be rather various. The centric value of Ball-m in Fig. 5 (c) when r =0 . 8 is close to one of Ball-d is because there is only one remaining point in the top-1 motif ball after removing trivial matches as we can refer to Fig. 5 (a) and the mean of all data points is itself. and (b) show the density, and Fig. 6 (c) and (d) show the centric values for the top-15 motif balls for r =1 . 1. The top-k motif balls found by Ball-d is inferior to both Ball-1 and Ball-m , in terms of both the density and the centric values. In this paper, we formalized the problem of locating motifs as finding con-tinuous top-k motif balls by allowing non-existing data points in a dataset S as centers of balls of radius r , and proposed effective heuristic approaches to find continuous top-k motif balls. First, our approach identifies a minimal set of maxmin candidate sets where each maxmin candidate set is enclosed by a ball whose max radius is 1 1 compromise efficiency, we propose two heuristics to find motifs within each maxmin candidate set. Our experimental results showed that our approaches can significantly improve the quality of motifs found in time-series datasets. Acknowledgment. The work described in this paper was supported by grants from the Research Grants Council of the Hong Kong SAR (CUHK4229/01E) and ARC Discovery Grant (DP0346004).

