 Due to the continuous and rampant increase in the size of domain specific data sources, there is a real and sustained need for fast processing in time-sensitive applications, such as medical record information extraction at the point of care, genetic feature extraction for personalized treatment, as well as off-line knowledge discovery such as creating evi-dence based medicine. Since parallel multi-string matching is at the core of most data mining tasks in these applica-tions, faster on-line matching in static and streaming data is needed to improve the overall efficiency of such knowl-edge discovery. To solve this data mining need not effi-ciently handled by traditional information extraction and re-trieval techniques, we propose a Block Suffix Shifting-based approach, which is an improvement over the state of the art multi-string matching algorithms such as Aho-Corasick, Commentz-Walter, and Wu-Manber. The strength of our approach is its ability to exploit the different block struc-tures of domain specific data for off-line and online parallel matching. Experiments on several real world datasets show how our approach translates into significant performance im-provements.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  search process Algorithms multiple-pattern matching, block suffix shift, pre-processing, feature extraction Partial work done during Siemens internship.
 Copyright 2008 ACM 978-1-59593-991-3/08/10 ... $ 5.00.
In recent years, the ever increasing amount of web data is exploited by more and more data mining applications, such as information extraction from electronic data [15], discov-ering interesting usage patterns in text collections, [12] find-ing advertising keywords on web pages [23], detecting spam web pages [20] and mining the shopping preferences of web surfers. Most of these data mining applications employ auto-matic or semi-automatic machine learning techniques that make use of features as the atomic units of information. Because for online web data-mining applications, speed is of essence, effective algorithms for feature extraction are of outmost importance. In particular, information extraction (parallel concept extraction from blogs, news, instance mes-sages etc), online and off-line search (e.g. retrieval), and traffic scanning (e.g. network scanning [16], online query log processing [10]) are application classes in which mas-sive concept sets are sought in large unstructured and semi-structured online datasets.

Data pre-processing, including feature extraction for sta-tistical information extraction and classification tasks, is a considerable bottleneck. Existing advanced multi-pattern matching algorithms are crucial in time-sensitive process-ing of high volume data. The explosion in the size and number of information sources observed in recent years em-phasizes the need for identifying large concepts sets in vast amounts of data. In most cases, fast parallel multi-pattern matching algorithms are needed for rapid and timely pro-cessing of data. In the news domain, in order to process large corpora, resource-driven information extraction relies on matching large free text databases against massive con-cept sets drawn from resources such as gazetteers, otologies, semantic networks etc. This processing step is often time consuming, yet inherently paralellizable.

One of the most frequent applications using DNA databases involves approximate and exact searching for specific se-quences. Part of the task is identifying all occurrences of multiple sequence simultaneously [18]. Performing this task efficiently is often a requirement, especially considering the rate at which genomic databases (corpora) are growing. In processing large text corpora, resource-driven extraction re-lies on matching large free text databases against sizable concept sets drawn from resources such as gazetteers, otolo-gies, semantic networks etc.

Words, n-grams and patterns (expressions) of interest are among the most frequently used features/concepts. To ex-tract these kinds of features/concepts, the above applica-tions need a fundamental but often neglected step: efficient Figure 1: The role of the BSS algorithm in the web data extraction and analysis field multi-string simultaneous matching which is also referred to as parallel multi-concept extraction , an often time consum-ing, yet inherently parallelizable process.

The multi-pattern search approach is extremely useful in situations when the phrases to be extracted are known while the text on which the search is performed is unknown ahead of time, very large, and continuously growing. Under these constraints, indexing is not a viable option: non-linear algo-rithms are not scalable and extraction cannot be performed online. Current state of the art multi-pattern extraction al-gorithms such as Aho-Corasick (AC) [6], Commentz-Walter (CW) [7], and Wu-Manber (WM) [22] provide solutions with theoretical complexity linear in the size of patterns and text to be matched against, but with non-negligible constants. In many online data sources and web datasets, additional domain-specific constraints have the potential to maintain the low theoretical complexity while significantly reducing the extraction/matching time. For example, a pervasive constraint takes advantage of the fact that across multiple domains, databases ( texts ) are naturally organized in terms of semantic units or blocks (genetic blocks, words, fields etc).
In this paper, we present a new online exact parallel multi-pattern matching algorithm to search for multiple patterns simultaneously, the Block Suffix Shifting ( BSS ) algorithm. The BSS algorithm implements shifting functionality on seg-ments of text while maintaining an exact match solution. It is faster than previous character-level algorithms and is scal-able to a very large number of patterns. Once the pattern set is given, the BSS algorithm achieves efficient pattern match-ing performance for the constantly changing and growing online data, especially for free-text environments. More-over, we present experiments in several popular online data sets and show improvements on real data and real applica-tions. Besides efficiency, the method is also very flexible, allowing efficient solutions in different settings. The pattern data structure needs to be constructed only once and can be reused for multiple texts, i.e. multiple query log databases. The BSS algorithm has the following advantages:
In Section 2, we discuss the related work. Section 3 elabo-rates the detailed state machine construction and the match-ing process of the BSS algorithm. In Section 3.4.1, we show the improvements of the BSS algorithm compared with the AC algorithm on an example. Section 4 analyzes the algo-rithm complexity. We analyze the experimental results in Section 5 and conclude in Section 6.
Aho-Corasick (AC) [6], Commentz-Walter (CW) [7] [24], and Wu-Manber (WM) [22] are representative multiple-string matching algorithms. They are widely used in several no-table application areas including text processing, speech recog-nition, information retrieval, network security, and compu-tational biology. The AC algorithm serves as the basis for the UNIX tool fgrep [5] while the Wu-Manber(WM) algo-rithmisusedin glimpse [19]. However, these algorithms seldom apply a shift operation in practice for numerous pat-terns and large text.

The Aho-Corasick (AC) algorithm [6] is a linear-time algo-rithm based on an automata approach: combines automata with Knuth-Morris-Pratt algorithm. It combines automata with an extension of the Knuth-Morris-Pratt (KMP) algo-rithm [13] by a method that includes an automaton based approach using suffix tree-like links [11] [9] The AC algo-rithm constructs a state machine using the pattern set, suc-cessively reading individual characters in the text and con-comitantly traversing the state machine through predefined goto and failure functions, and occasionally emitting out-puts.
 The AC algorithm scans one character at a time from text T and compares it with the current state in the pattern-built state machine. The reason for the impossibility of a shift is that the AC algorithm works at the character level: it treats both patterns and text as character sequences without con-sidering their natural structure. The underlying alphabet is considered to be some small set of characters, such as ASCII or a DNA alphabet. Each edge of the AC state machine is labeled with a character. This provides a natural way to apply the AC algorithm and the small alphabet size can take advantage of the simple byte operations implemented at the machine level. As part of the matching process, the AC algorithm checks each character successively, that all the characters in the text are treated equally and the assump-tion is that pattern instances can start from any character in the text. If we encounter a mismatch of a character c i the text T , AC still needs to check the next character c since it can start a new pattern instance, regardless of the location of c i +1 .
 The Commentz-Walter (CW) algorithm [7] combines the Boyer-Moore (BM) [8] method with the Aho-Corasick algo-rithm. Although it claim that it enables AC to shift, the Commentz-Walter algorithm is only faster than the AC al-gorithm on small pattern sets and long minimum pattern lengths.

The Wu-Manber(WM) algorithm [25] presents a different approach that also based on the idea of the Boyer-Moore algorithm. The Wu-Manber(WM) algorithm only uses the bad-character shift of the Boyer-Moore algorithm and con-siders the characters in the text to be separated in blocks of size B instead of one by one. In order to preserve the size of the alphabet, B cannot be large. In practice, they use either B=2 or B=3. As we increase the number of pat-terns, for each block there are multiple patterns that match it, the algorithm still needs to evaluate them one by one and thus the advantage of shifting can not be shown.. The performance of the Wu-Manber(WM) algorithm is heavily constricted by the length of the shortest pattern.
Although the Wu-Manber(WM) algorithm claims a capa-bility to shift due to its derivation from the Boyer-Moore al-gorithm, with increasing pattern size the possibility that the current block in the text matches some patterns increases dramatically and the shift does not happen (shift value=0). For the Wu-Manber(WM) algorithm, the shift value was zero 5% of the time for 100 patterns, 27% for 1000 patterns, and 53% for 5000 patterns [22]; Moreover, when a block of size B in the text matches with the suffixes of some pat-terns, the Wu-Manber (WM) algorithm has to check these patterns one by one and the advantage of shifting can not be shown.
Formally, the multi-pattern-matching problem can be pre-sented as follows:
The goal is to locate and identify all occurrences of all the patterns of P in T . The matched substrings in T may also overlap with one another.
Although some literatures [22] [17] claim that the perfor-mance of the Wu-Manber (WM) algorithm is better than the AC algorithm in practice, no literature gives concrete experimental results to show the difference. Moreover, the new SNORT [21] release note officially shows that the WM algorithm is slower than the AC algorithm and the former will be deprecated. Our Block Suffix Shifting (BSS) algo-rithm is derived from the AC algorithm because the AC algorithm is very robust and works well for large text and many patterns. The most important improvement is that the BSS algorithm shifts over the text T on many segments to heavily reduce the matching time without missing any result.

We propose this idea based on an interesting observation: both the text and the patterns can be viewed as a sequence Figure 2: (a): The general block/seperator property; (b): The example T in the basic block/seperator property; (c): The example T in the composite block/seperator property; of blocks ( b 1 , b 2 , ...) and the separators between the blocks ( s 1 , s 2 , ...) (see Figure 2 a). We call it the  X  X lock/separator X  property. The blocks can have variable lengths and the sep-arators can be situated in different positions. This prop-erty holds for many applications. According to the different data structures, we further define the property in two lev-els: the basic  X  X lock/separator X  property and the composite  X  X lock/separator X  property.

The basic  X  X lock/separator X  property is applicable to any free texts, e.g., the online documents in the digital libraries, the web pages, and the blogs etc. Because word is the short-est meaningful unit in the free-text, we define each block in the basic  X  X lock/separator X  property as a word (see Figure 2b). All blocks share the equal chance to match a pattern.
In addition, some other web data has structured or semi-structured format, e.g., the online query log records, the XML files, and the DNA database etc. Figure 2c shows an randomly downloaded query log record. Considering such structure information can facilitate the data matching per-formance. Once the patterns are given, we can quickly de-cide which segments in T we should check and which we should not. If the patterns are the query keywords, we only have to check  X  X 2 X  in Figure 2c and shift over all the others. We say that these web data follows the composite  X  X lock/separator X  property and such a segment is the com-posite  X  X lock X  because it can consist several words .
The separators can be space characters, punctuation as well as user-specified text, and even null for the equal-length blocks, which are not relevant to the task at hand. In or-der to identify blocks , the document tokenization may need to be performed. There are two main tokenization cate-gories: orthography-oriented tokenization and dictionary-based tokenization[14]. Dictionary-based tokenization tech-nique uses a dictionary to look for possible word strings/ combinations. It is normally used for tokenizing ideograph-ical languages such as Chinese, Japanese, etc. Through to-kenization we can detect the boundary of each word using orthographical clues, such as space,punctuation marks, etc. In this paper, we focus on searching patterns from the web data in English and use orthography-oriented tokenization techniques. We define the separator as space characters and we preprocessed the text to remove the punctuation as well as the redundant spaces.
The data structure imposes an important constraint: if the text follows a  X  X lock/separator X  property, any pattern instance may only start with the beginning of a block .In another sentence, it is impossible to start a new pattern in the middle of a block . This constraint is applicable to the text in both the basic block and the composite block.
For the example in Section 2 ,if c i +1 is not the first charac-ter of a word w j , there is no need to compare c i +1 with the pattern data structure. Moreover, the constraint provides the opportunity to shift over all the entire suffix of w j c i +1 to the last character of w j ) and start a new match-ing iteration from the beginning of the next word w This novel view of the data can speed up the entire pattern matching process for a more efficient performance. The BSS algorithm enables to skip over many segments in the text T without missing any matches. For the text following the basic  X  X lock/separator X  property, the skipped segments are the suffixes of all the mismatched words . For the text follow-ing the composite  X  X lock/separator X  property, the skipped segments are the suffixes of all the mismatched composite blocks . Most often, the probability to mismatch is heav-ily larger than the probability to match, and therefore the amount of such segments in T is large. Besides being fast, the method is also very flexible, allowing efficient solution for many text variation as long as both the text and pattern share the block/separator property.

Similar to the AC algorithm, the behavior of the BSS al-gorithm is dictated by three functions: a goto function g ,a failure function f , and an output function output .Atbegin-ning, the BSS algorithm determine the states and the goto function. In order to take advantage of the simple byte op-erations , the BSS algorithm determines the states and con-structs the goto functions at the character level using the exactly same method as that of the AC algorithm. Because of the limited space, please see the detailed goto function and the output function in [6];
Example 1. p 1 =  X  X ent acura car X  ; p 2 =  X  X cura car repair X  ; p =  X  X ar deal X  ; p 4 =  X  X arseat safety X  ;
Figure 3 shows the state machine generated by the AC algorithm and the BSS algorithm for Example 1 , which each pattern is a randomly selected query keyword. The state machine is a rooted directed tree with fifty states totally. Each state is represented by a number from 0 to 49 . State 0 is designated as the root state s 0 . We only display the failure functions that do not go to the root state.
Defining and specifying failure functions is a critical part of the BSS algorithm. Before elaborating on the algorith-mic details, we show reasons for the low performance of the AC algorithm, which does not work well for the web data domain. Let us consider another example shown as follows. Example 2. p1= X  X el X , T= X  X eel hurt X  .
 According to the AC algorithm (Figure 4a), we have an occurrence of p1 in the T= X  X  eel eye X  .However,itisnot a real occurrence.  X  X el X  is an independent word in p1 .If we find a match in T , the matched  X  X el X  should also be an independent word, instead of a substring of a word. For these cases, the BSS algorithm makes the first modification:
Modification 1. For each mismatch (e.g., h = e ), if there is no path to continue in the state machine, the fail-ure function of the current state points to the root state ( f ( s 0 ,h )= s 0 ) . BSS then aborts the matching process for the current word (  X  X eel X  )in T , shifts the reminder word suf-fix (  X  X el X  ), and jumps to the beginning of the next block/word (  X  X urt X  ). Let us consider the third example shown as follows. Example 3. p1= X  X el X , T= X  X ar eeel X  .
 AC will generate another  X  X ubstring matching X  result for Example 3 (Figure 4a): T= X  X ar e eel  X  . For the first word  X  X ar X  in T , there is a mismatch ( a = e )and f ( s 1 )= s We stop the matching process for the word  X  X ar X  , skip the remaining word suffix (  X  X  X  ) and jump to the beginning of the next word. For the second word  X  X eel X  , the failure functions of AC in Figure 4a ( f ( s 2 )= s 1 ) breaks the block boundary constraint: if a pattern matches a section of the text, every matched character pair from both the pattern and the text sections should have the same corresponding character index number (CIN) . However, in Figure 4a ,the CINs connected by the failure function are 1 and 2(1 = 2) . Figure 4c and Figure 4d show such failure functions between two patterns  X  X ain X  and  X  X id X  with both AC and BSS.

In order to avoid such mismatches, the BSS algorithm makes the second modification based on the AC algorithm (see Figure 4b and Figure 4d ):
Modification 2. The BSS algorithm deletes such failure functions and reset them to the root state s 0 directly. Let us consider the following example.
 Example 4. p1= X  X b X , p2= X  X  ab X , T= X  X  at X  .
 For those failure functions that connect two characters with the same CINs in the AC algorithm, should we keep all of them? The answer is no because we notice that some of them generate useless backward jumps and waste time on worthless matching operations without possibility to get any result. The BSS algorithm can avoid such wastes and jump to the next word earlier.

According to the Aho-Corasick algorithm, Example 4 has two failure functions between p1 and p2 (see Figure5 ). The matching process of the T over the trie is as following: goto ( s 0 ,e )= s 2 , goto ( s 2 ,  X  )= s 4 , goto ( s 4 cause t = b, f ( s 5 )= s 1 ;Because t = b, f ( s 1 )= s 0 t = e and t = a, f ( s 0 )= s 0 . We stop at the root state s Neither p1 nor p2 is found.

Although the AC algorithm does not generate any wrong result, there are redundant backward jumping and matching actions by doing the r = b judgement twice: when we fail at s 5 with the input  X  X  X  over the goto function  X  X  X  ,itis unnecessary to jump to s 1 because the only goto function of s 1 is also  X  X  X . Using the Aho-Corasick algorithm, we do two additional useless operations: f ( s 5 )= s 1 and t = b , which compounds for a lot of wasted time when the T grows. Suppose s 1 has another child besides b (see Figure 5c), the BSS algorithm should keep the failure function f ( s 5 )= s Figure 3: The state machine of Example 1 generated by the AC algorithm (left) and the BSS algorithm (right) Figure 4: Sample state machines generated by the AC algorithm (a,c) and the BSS algorithm (b,d) Figure 5: Examples of the redundant failure func-tions of the AC algorithm (a, c) and the modification of the BSS algorithm (b, d) (see Figure 5d) because the new child does not equal to b ,if we fail at s 5 , we may can continue at the new child. To deal with such problems, our BSS algorithm does the following change:
Modification 3.  X  s  X  ,s  X  ,f ( s  X  )= s  X  ,if s  X  has different goto function as that of s  X  , and both of them have the same CINs, BSS keeps this failure function (see Figure5d).
Based on above examples, can we make the following statement? For any failure function in the AC algorithm f ( s  X  )= s  X  ,ifboth s  X  and s  X  have the same CINs and the same goto functions, should we reset this failure function of s  X  to the root state f ( s  X  )= s 0 , in order to reduce the re-dundant jumping and match? The answer is no because we should consider the possible match through the propagated failure functions.

The propagated failure function is defined such that there are a set of non-zero states S = s 1 ,s 2 , ..., s i ,i  X  isfy the condition: f ( s i )= s i  X  1 ,f ( s i  X  1 )= s i  X  2 s .

In Figure 3 (left),the state machine generated by the AC algorithm for the Example 1, there are five sets of the propa-gated failure functions: ( s 1 , s 33 , s 23 , s 14 ), ( s ( s in the Example 1 from  X  X cura car repair X  to  X  X cura car X  , we should delete the states from s 24 to s 30 correspondingly. However, we still keep the propagation set ( s 33 , s 23 because although s 14 and s 23 have the same goto function (null) and the same CIN number, f ( s 23 )= s 33 enables us to reach s 33 and s 33 has different goto functions: goto ( s  X  X  X )= s 39 ,and goto ( s 33 , X   X )= s 34 . Suppose T= X  X ent acura car deal X  , we can arrive at s 38 to get an output along the propagated failure functions. In order to find all the possible matches, the BSS algorithm do the following modification comparing with the AC algorithm:
Modification 4.  X  s  X  and s  X  ,f ( s  X  )= s  X  , both of them have the same CINs. If s  X  has the same goto function as that of s  X  ,but f ( s  X  ) = s 0 , the BSS algorithm keeps this failure function f ( s  X  )= s  X  .
For all the failure functions of the AC algorithm, we clas-sify them into two groups: go-to-the-root-state failure func-tions, and do-not-go-to-the-root-state failure functions. The BSS algorithm modifies a subset of the second group and leaves the first group intact.

For each state s  X  ( s  X  = s 0 ), according to the AC algo-rithm, f ( s  X  )= s  X  .If s  X  = s 0 , we allow this failure function or else we will have to decide whether allow it or reset it.
For a better understanding, we summarize the failure func-tion settings of the BSS algorithm using the following con-crete modification and adaptation of the AC algorithm. We only keep the failure functions that satisfy the condition: 1. s  X  and s  X  have the same character index numbers 2. s  X  has different goto function that s  X  does not have,
For all the others, the BSS algorithm resets to the root state s 0 . If we arrive at s 0 through a failure function, it follows that a mismatch occurred in the text, allowing the BSS algorithm to shift. The pseudocode of the BSS trie construction is shown in Algorithm 1.

Algorithm 1 : BSS Trie Construction Pseudo Code begin end
Table 1 elaborates the steps that the BSS algorithm re-sets the do-not-go-to-the-root-state failure functions for the Example 1. The detailed reset reasons are also displayed.
After the BSS algorithm constructs the three functions based on the pattern set, the matching stage is ready to begin. All the shifts occur in this stage.

For the text T , the BSS algorithm traverses it character by character to take advantage of the simple byte operation of the computer for the efficient performance. For each input character, the BSS algorithm checks the goto function of the current state: if fail , it follows the failure function to the new state. If the new state is the root state s 0 ,the BSS algorithm skips the reminder suffix of the current block , shifts to the beginning of the next block , and restarts the matching process from s 0 .The go-to-the-root-state failure function means that no possible pattern in P can match with the current block in T . In another sentence, when the failure function goes to s 0 , the shift value is greater than zero. Otherwise, the shift value is zero. This value reflects the length of the segment to be shifted.

Figure 6a displays the match phase of the AC algorithm for T =  X  X ovie made about super intelligent children X  and the patterns p 1 =  X  X otor X  , p 2 =  X  X tock X  , p 3 =  X  X hildrensplace.com X  ; According to the AC algorithm, we have to scan all the 43 characters without any shift. Figure 6b displays the match phase of the BSS algorithm for the same T and P .Be-cause this T follows the basic  X  X lock/separator X  property, Table 1: The failure function changes of the BSS algorithm on the Example 1 the BSS algorithm shifts on the block suffix of each mis-matched blocks. With these shifts, we only have to check 16 over 43 characters. Figure 6c displays the match phase of the BSS algorithm for another example T with patterns p =  X  X andidacy exam X  , p 2 =  X  X tock X  ,and p 3 =  X  X amada.com X  . The T contains three randomly selected AOL query logs that follows the composite  X  X lock/separator X  property.
Using Example 1, suppose T= X  X heck out these promo-tions at selected thrifty car rental locations the acura mdx is all-new for 2007 and always check the carseat safety X . The AC algorithm has to scan all the 137 characters with-out any shift. However, the BSS algorithm only checks 53 characters and shifts over all the underlined suffixes:  X  X heck out these promotions at selected thrifty car rental locations the acura mdx is all-new for 2007 and always check the carseat safety X  .
In this section, we analyze the time complexity of our al-gorithm. As the BSS algorithm encounters mismatches, the advantage of the algorithm becomes apparent. In a practical setting of a multi-pattern matching problem, generally the probability for a mismatch is considerably larger than the probability for a match, especially when the text is large.
The best case scenario for the BSS algorithm occurs when mismatches are encountered on the first character in each block and the blocks are of equal length t .Thus,thetime complexity is O(m+ (n/t) + z) ,ascomparedwith 0(m + n +z) of AC, where z is the number of pattern occurrences in T . Although the complexity is still linear, since most of the time n m and n z , the matching time is decided by n  X  which is the target of the BSS algorithm. More-over, because of the equal-length blocks, it becomes trivial to skip entire blocks without identifying separators, yielding a considerable speedup. Figure 6: (a): The match phase for the example free text based on the AC algorithm; (b): The match phase for the example text in the  X  X asic X  Block/Seperator property based on the BSS algo-rithm; (c): The match phase for the example text in the  X  X omposite X  Block/Seperator property based on the BSS algorithm;
The AC algorithm can be viewed as the worst case sce-nario of the BSS algorithm since the former has to check and match every character in the text. For the BSS algorithm, this scenario appears in the following two cases. However, in free-text contexts, due to word diversity, these boundary cases are very infrequent.
We implemented experiments comparing the BSS algo-rithm with two famous multi-pattern matching algorithms (the AC and the WM algorithm) on the real web data. In addition to compare the matching performance in terms of the speed, we also investigate the impact of algorithm on the following parameters: the size of the pattern set P  X  m ,the size of the text T  X  n , the diversity of the pattern length, the average mismatch location in the blocks, the diversity of the block length in T , the size of the shortest pattern, the alphabet size  X  , the modification steps of the BSS algorithm, and the level of the  X  X lock/separator X  property;
We are interested in quantifying the average text scanning speed (MB/second) on the test data and the average number of the shifted characters per block. For each experimental setting, we randomly select T and P from the web data, perform twelve separate runs and show the averages. All the experiments were conducted on a computer with Intel Core Due 1.83GHz CPU, 1 Gigebytes of RAM. The operation system is Windows XP. The classic AC and WM algorithms adopt the code presented in SNORT[21] and are implemented in JAVA. We also implement the BSS algo-rithm in JAVA, by modifying the available version of the AC algorithm.
In order to investigate performance of each algorithm as well as its advantages and disadvantages, our experiments were performed mainly on the the online web data: the query logs and the electronic documents in the digital li-braries. The query logs originate from low to high-traffic web sites such as well known search engines, retail store web portals, news web sites, social networks, personal web sites etc. This data covers rich information such as the query itself, date, time, source ip, type of redirection etc, which is used extensively for statistics gathering, clustering, as well as query log search. For all of these tasks, fast, user-driven access to specific information from query logs is necessary, especially when considering the fact that this type of data is constantly changing and constantly growing. To illustrate the immensity of these data sets, consider the daily query volume of US users observed [2] based on March 2006 in several high-traffic search engines Google: 91 million, Ya-hoo: 60 million, MSN: 28 million, AOL: 16 million, Ask: 13 million. Given such large data sets, there is a growing need for swift and efficient processing while at the same time allowing fast search and extraction.

The second data source are the scientific chemistry papers collected from the digital library (Royal Chemistry Society [4]). Another important data set for our experiment is the KDD Cup datasets [3] 1997. All the free-text data follows the basic Block/Seperator property while all the query log data follows the composite Block/Seperator property. All the test data has three different alphabet sizes: the English free text (  X  =96) and the DNA data (  X  =4).
English alphabet : In our experiment, the English free texts come from two useful web data: The size of the alpha-bet  X  is 96 (all visible characters). The total AOL query log size is 650 MB . The randomly selected 100 , 000 patterns fall under two categories: the query keywords and the clicked URLs; The total size of the electronic chemical documents is 500 MB and we randomly select 50 , 000 chemical names to construct the pattern set.
 DNA alphabet(  X  =4) : All test data comes from the DNA data set [1] in gcg format. The alphabet = { A, C, G, T The total size of the text is 250MB and the the total number of patterns we used is 50 , 000. Figure 7(a) shows the results of the performance of the BSS algorithm vs. the AC algorithm on the AOL query log, against six different sizes of the pattern sets: m = { 1000, 5000, 10000, 20000, 30000, 50000 } . For each set, we ran-domly select the patterns with diverse lengths. We apply the algorithms on the same text T (50 MB ) for each pattern set. On the query log data, the BSS algorithm maintains a high performanceoverACforevery m : the improvement rates of the BSS algorithm are 165.1%, 222.9%, 209.6%, 209.04%, 257.38%, and 208.75% respectively. The  X  X SS+4 X  refers to the implemented BSS algorithm with all four modification steps based on the AC algorithm (see Section 3.4).
We notice that the BSS algorithm has a performance-decreasing trend along the growth of the number of patterns. It is not surprising because as the increasing of m , the state machine is enlarged with more states, which provide more composite  X  X lock/separator X  property follow the basic  X  X lock/separator X  property online scientific documents that follow the basic  X  X lock/separator X  online scientific documents that follows the basic  X  X lock/separator X  chances for T to traverse. The ratio of the matched seg-ments in T will be increased and less block suffixes the BSS algorithm can shift. When m reaches a very large value that all the blocks in T match with P , a worst case scenario hap-pens for the BSS algorithm and its performance curve will meet with the curve of AC in the figure.
We performed similar experiments on the AOL query logs in order to investigate the effect of n on the run time. The six value of n are: 10 M ,30 M ,50 M ,100 M ,150 M , and 200 M . Figure 7(b) shows the run time of the BSS algorithm versus the run time of AC with the same pattern size m = 10000. The results confirm: (i) the performance improvement of the BSS algorithm is steady as the increasing of n ; (ii) the performance of the BSS algorithm is linear to n .
In order to investigate the distribution of the pattern length and its effect on the match performance, we fur-ther divide the randomly selected 100 , 000 patterns from the AOL query log into four groups according to the pattern length in term of words: one-or-two-word patterns (50145), three-word patterns (21100), four-word patterns (12982), and five-and-above-word patterns (15772). It shows that in the query log field, a considerable number of query concepts are short (one or two words).

To study the effect of pattern length on matching perfor-mance, we randomly select 250 patterns from each group and apply both the BSS algorithm and the AC algorithm on the same text (200 MB ). The results are shown in Fig-ure 7(c) . For all the four groups covering both short and long pattern lengths, the performance gain for the BSS al-gorithm over AC is 202.1%, 178.9%, 184.3%, 193.7% and 186.1s% respectively.

Comparing with the query logs, the patterns in the chem-istry field share a similar property: many patterns are short in terms of the number of characters. Figure 10 shows the detailed statistics on the pattern length of the 50 , 000 pat-terns from the online documents in the chemistry domain.
In the three diagram of Figure 7, we compare the text scanning speed of the BSS algorithm on the texts in dif-ferent  X  X lock/Seperator X  property levels. Each pair of the comparison are implemented with the same text size n and the same pattern set size m . The experimental results show that the BSS algorithm works extremely well on the text in the composite  X  X lock/Seperator X  property. It is not sur-prising because as long as we know the structure of the text in advance, we can easily know which blocks we should scan and which blocks we can shift over. With a minor change on one single line in the matching phase source code, we can efficiently speed up the performance.
We believe that the larger the average block size is, on average, the more characters in T the BSS algorithm can shift and the more significant the performance improvement the BSS algorithm can achieve. In order to test our conjec-ture, we investigate the distribution of the block length first (see Figure 8(a)) based a randomly selected 350 MB online scientific chemistry papers.

We divided all the 34 . 54 millions of blocks in the T into four categories according to their block sizes: between 1 between 4  X  7, between 8  X  10, and above 10. The block size is defined as the number of the characters in a block. We randomly select blocks from each category to construct the test data. Within the same testing environment, we eval-uate the improvement of the BSS algorithm by calculating how many characters in T are shifted by the BSS algorithm. Figure 8(b) shows both the average block length of each category and the average number of the shifted characters the BSS algorithm achieves. The fifth column represents the results for the DNA data with equal block length (10).
Our experimental result confirms our conjecture that as the increasing of the block length, the BSS algorithm makes more performance improvement comparing AC: for the four categories, the BSS algorithm shifts 7.6%, 29.3%, 45.3%, and 47.1% respectively over the whole text. We notice that the BSS algorithm has a performance-increasing trend, along the growth of the average block size in T (see Figure 8(c)). It is not surprising because as the increasing of the block size, with the same text size m , we have less blocks. With the same mismatch possibility, there are much larger segments in T that the BSS algorithm shifts.
We pay attention to this parameter because it is heavily related to the performance of the Wu-Manber algorithm: each shift value can not be larger than the length of the shortest pattern. However, we believe that the performance of the BSS algorithm is not restricted by this parameter. In this section, we still use the four data sets prepared in the Section 6.6, which the length of the shortest pattern are 3, 4, 8, and 11 respectively. In order to confirm our conjecture, we manually reduce the length of the shortest pattern in each data set to 3 by adding a 3-character pattern  X  X NA X  to each pattern set. The experimental results show that with such a small pattern, the BSS algorithm still keep the same average number of the shifted characters for each block as displayed in Figure 8(b) and the same performance improvement over AC in Figure 8(c).
We believe that the earlier the mismatches happen, the more shifts the BSS algorithm can do and the more improve-ments the BSS algorithm can achieve. Figure 9(a) shows the performance difference between the BSS algorithm and the AC algorithm over three different mismatch locations: the end of each block, the middle of each block, and the beginning of each block. For the first location, the BSS al-gorithm should scan every characters in the text without the capability to shift. This is the worst scenario and the performance of the BSS algorithm is equal to that of the AC algorithm. Because of the diversity of the free text, we treat our previous experiments as the second case  X  mismatches happen in the middle of blocks on average.

In order to obtain the experimental results for the third mismatch location, we manually generate the testing data by adding a special character at the beginning of each block in T . When the BSS algorithm reads such character, it fails and shifts to the beginning of the next block. The same thing happens until the end of T . Figure 9(a) confirms our conjecture.
In this section we test the BSS algorithm on the text and the pattern set that are composed of the equal-length blocks. We get the DNA data from the online data set [1]. The size of the alphabet (A, C, T, G) is four. Each block is with the fixed length 10. Because of the fixed block length, we can calculate the shift value for each state in the state machine and store the value in advance. Such operation can make the BSS algorithm more efficient by avoiding the time to judge where is the beginning of the next block in the match stage. With the same text size ( n =10MB), we test the BSS algorithm and the AC algorithm in six different values of m: 1,000, 5,000, 10,000, 20,000, 30,000, 50,000 . The text scanning speeds of AC are 10.21, 8.18, 7.5, 6.72, 5.35, 4.33 (MB/second) while the text scanning speeds of the BSS algorithm are 18.25, 15.04, 13.83, 12.48, 11.64, 8.98 (MB/second) respectively (see Figure 9(b)). In order to see the effect of the modification steps of the BSS algorithm, we test two versions: the BSS algorithm with the first two modification steps (Section 3.4.2 and 3.4.3, see the curve  X  X SS+2 X  in Figure 9(c)), and the BSS algo-rithm with all four modification steps (Section 3.4.4 and 3.4.5, see the curve  X  X SS+4 X ). We also compare these two versions with the AC algorithm and the WM algorithm us-ing the same AOL query data set (n=50MB). The results show that the first two modification steps increase AC al-gorithm by 150% on average, and the last two modification steps increase the  X  X SS+2 X  by 5%.
In this paper we presented the Block Suffix Shifting ( BSS ), a new online exact multi-pattern matching algorithm to search for multiple patterns simultaneously. The BSS al-gorithm is faster than previous character-level algorithms and is scalable to a very large number of patterns. The most important improvement of the BSS algorithm is the significant shifting functionality on segments of text, which exploits the natural structure of text itself. In addition, the BSS algorithm avoids the typical  X  X ubstring X  false positive errors. Experimental results show significant improvements in text scanning speed and reduced redundancy failure func-tions which lead useless backward jumping. [1] ftp://genome-ftp.stanford.edu/pub/yeast/data [2] http://searchenginewatch.com. [3] http://www.kdnuggets.com/datasets/kddcup.html. [4] http://www.rsc.org/. [5] http://www.unet.univie.ac.at/aix/cmds/aixcmds2/ [6] C. M. Aho AV. Efficient string matching: an aid to [7] C.-W. B. A string matching algorithm fast on the [8] M. J. Boyer RS. A fast string searching algorithm. In [9] C.-H. Chang and S.-C. Lui. Iepad: information [10] J. Chen and T. Cook. Mining contiguous sequential [11] H. Chim and X. Deng. A new suffix tree similarity [12] A. Don, E. Zheleva, M. Gregory, S. Tarkan, L. Auvil, [13] V. P. Donald Knuth; James H. Morris, Jr. Fast [14] K. Fredriksson. On-line approximate string matching [15] V. Gedov, C. Stolz, R. Neuneier, M. Skubacz, and [16] S. Kals, E. Kirda, C. Kr  X  ugel, and N. Jovanovic. [17] S. Kim and Y. Kim. A fast multiple string-pattern [18] Y. Liu, L. V. Lita, S. Niculescu, P. Mitra, and C. L. [19] U. Manber. Agrep, an approximate grep. In [20] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. [21] M. Roesh. Snort: Lightweight intrusion detection for [22] U. M. Sun Wu. A fast algorithm for multi-pattern [23] W. tau Yih, J. Goodman, and V. R. Carvalho.
 [24] B. W. Watson and R. E. Watson. A new family of [25] S. Wu and U. Manber. Fast text searching with errors.
