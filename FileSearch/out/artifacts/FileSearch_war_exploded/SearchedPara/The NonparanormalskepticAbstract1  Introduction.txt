 Han Liu hanliu@cs.jhu.edu Fang Han fhan@jhsph.edu Ming Yuan myuan@isye.gatech.edu John Lafferty lafferty@uchicago.edu Larry Wasserman larry@stat.cmu.edu Undirected graphical models provide a powerful frame-work for exploring the interrelationships among a large number of random variables, and have found routine use in analyzing complex and high dimensional data. An undirected graphical model for the joint distribu-tion P of a random vector X = ( X 1 ,...,X d ) is as-sociated with a graph G = ( V,E ), where each vertex i corresponds to a component variable X i . The pair ( i,j ) is not an element of the edge set E if and only if X i is independent of X j given ( X k : k 6 = i,j ). In the graph estimation problem, we have n observations of the random vector X , and wish to estimate the edge set E .
 The simplest method for estimating the graph when the dimension d is small is to assume that X has a mul-tivariate Gaussian distribution, and then test the spar-sity pattern of the inverse covariance or precision ma-trix  X  =  X   X  1 , based on the sample covariance b  X  n . A drawback is that the dimensionality d must be strictly smaller than n . In the high dimensional setting where d &gt; n , a number of methods have recently been pro-posed and studied. Meinshausen &amp; B  X uhlmann (2006) propose a method based on parallel lasso regressions of each X i on ( X j : j 6 = i ). Yuan &amp; Lin (2007) and Banerjee et al. (2008) study the estimator con-structed using the log-likelihood of  X  under a joint Gaussian model, penalized by an ` 1 penalty on  X  to encourage sparsity. The estimator b  X  can be efficiently computed using the glasso algorithm (Friedman et al., 2008). Other estimators have been studied based on the use of the Dantzig selector (the gDantzig selector of Yuan (2010)), or based on estimating a sparse pre-cision matrix under constraints on k  X  b  X  n  X  I k  X  (the CLIME estimator of Cai et al. (2011)). Strong theo-retical properties of these estimators have been estab-lished, including rates of convergence and consistency of graph selection.
 Despite the popularity of the Gaussian graphical model and the theoretical properties that the high dimensional estimators enjoy, the Normality assump-tion is restrictive, and conclusions inferred under this assumption could be misleading. Liu et al. (2009) propose the nonparanormal to relax the Gaussian as-sumption. A random vector X belongs to a non-paranormal family if there exists a set of univari-ate monotonic functions { f j } d j =1 such that f ( X ) := ( f 1 ( X 1 ) ,...,f d ( X d )) T is Gaussian. The nonparanor-mal is a type of Gaussian copula model (Klaassen &amp; Wellner, 1997). Liu et al. (2009) provide a learning algorithm for this model that has the same computa-tional cost as the glasso. The method is based on a Winsorized estimate of the marginal transformations f , followed by an estimate of the precision matrix using the transformed data. A convergence rate of O ( cision matrix in the Frobenius and spectral norms. However, it is not clear whether or not this rate of convergence is optimal.
 In this paper we show that the rate of conver-gence obtained by Liu et al. (2009) is, in fact, not optimal, and we present an alternative procedure that is rate optimal. The main idea is to exploit nonparametric rank-based statistics including Spear-man X  X  rho and Kendall X  X  tau to directly estimate the unknown correlation matrix, without explicitly calculating the marginal transformations. We call this approach the nonparanormal skeptic (since the S pearman/K endall e stimates p reempt t ransformations to i nfer c orrelation). The estimated correlation matrix is then plugged into existing parametric procedures (the graphical lasso, CLIME, or the graphical Dantzig selector) to obtain the final estimate of the inverse cor-relation matrix and graph.
 By leveraging existing analysis of different parametric methods (Ravikumar et al., 2009; Cai et al., 2011), we prove that although the nonparanormal is a strictly larger family of distributions than the Gaussian, the nonparanormal skeptic achieves the optimal para-metric rate O ( p n  X  1 log d ) for precision matrix esti-mation. The extra modeling flexibility thus comes at almost no cost of statistical efficiency. Moreover, by avoiding the estimation of the transformation func-tions, this new approach has fewer tuning parameters than the nonparanormal estimator proposed by Liu et al. (2009). Numerical studies are provided to sup-port our theory. In this section we briefly describe the nonparanormal family and the Normal-score based graph estimator proposed by Liu et al. (2009).
 Let A = [ A jk ]  X  R d  X  d and v = ( v 1 ,...,v d ) T  X  R . For 1  X  q &lt;  X  , we define k v k q = q  X   X  , we define the matrix ` q -operator norm  X  , the matrix norm can be more explicitly repre-the leading singular value and is often called the spec-tral norm. We also define k A k max = max j,k | A jk and k A k 2 F = P j,k | A jk | 2 . We denote v \ j = ( v note by A \ i, \ j the submatrix of A obtained by remov-ing the i th row and j th column, and A i, \ j the i th row of A with its j th entry removed. The notation  X  min ( A ) and  X  max ( A ) is used for the smallest and largest sin-gular values of A . 2.1. The Nonparanormal Let f = ( f 1 ,...,f d ) be a set of monotonic univari-ate functions and let  X  0  X  R d  X  d be a positive-definite correlation matrix, with diag  X  0 = 1 . We say a d -dimensional random variable X = ( X 1 ,...,X d ) T has a nonparanormal distribution X  X  NPN d ( f,  X  0 ) if f ( X ) := ( f 1 ( X 1 ) ,...,f d ( X d )) T  X  N (0 ,  X  0 ) . For continuous distributions, Liu et al. (2009) show that the nonparanormal family is equivalent to the Gaussian copula family (Klaassen &amp; Wellner, 1997). Clearly the nonparanormal family is much richer than the Normal family. However, the conditional indepen-dence graph is still encoded by the sparsity pattern of  X  0 = ( X  0 )  X  1 ; that is,  X  0 (Liu et al., 2009). 2.2. The Normal-score based Nonparanormal Let x 1 ,...,x n  X  R d be n data points and let I (  X  ) be the indicator function. We define b F j tive distribution function of X j . Liu et al. (2009) study estimates of the nonparanormal transformation func-is a Winsorization (or truncation) operator defined as T n ( x ) =  X  n (1  X   X  n )  X  I ( x &gt; 1  X   X  n ) with  X  n = 1 / (4 n 1 / 4 formed data, where The nonparanormal estimate of the inverse correla-tion matrix b  X  ns can be obtained by plugging b S ns into the glasso. Under certain conditions, Liu et al. (2009) show that However, it is not clear whether or not the rate in (2.2) is optimal. In the following, we show that it is not optimal and can be greatly improved using different estimators. In this section we propose a different approach for es-timating  X  0 that achieves a much faster rate of con-vergence, without explicitly estimating the transfor-mation functions. 3.1. Main Idea The main idea behind our alternative procedure is to exploit Spearman X  X  rho and Kendall X  X  tau statistics to directly estimate the unknown correlation matrix, without explicitly calculating the marginal transfor-mation functions f j .
 Let r i j be the rank of x i j among x 1 j ,...,x n j and  X  r 1 n
X b  X  Both can be viewed as a form of nonparametric cor-relation between the empirical realizations of two ran-dom variables X j and X k . Note that these statistics are invariant under monotone transformations. For Gaussian random variables there is a one-to-one map-ping between these two statistics; details can be found in Kruskal (1958). Let e X j and e X k be two indepen-dent copies of X j and X k . We denote by F j and F k the CDFs of X j and X k . The population versions of Spearman X  X  rho and Kendall X  X  tau are given by  X  jk := Corr sign( X j  X  e X j ) , sign( X k  X  e X k ) . (3.2) Both  X  jk and  X  jk are association measures based on the notion of concordance. We call two pairs of real and disconcordant if ( s  X  t )( u  X  v ) &lt; 0. For Gaussian copula distributions, the following im-portant lemma connects Spearman X  X  rho and Kendall X  X  tau to the underlying Pearson correlation coefficient  X  Lemma 3.1 (Kruskal (1958)) . Assuming X  X  NPN ( f,  X  0 ) , we have Motivated by this lemma, we define the following es-correlation matrix  X  0 : in later sections, the final graph estimators based on Spearman X  X  rho and Kendall X  X  tau have similar theo-retical performance. In the following sections we omit the superscript  X  and  X  and simply denote the esti-mated correlation matrix as b S . 3.2. The Nonparanormal skeptic with The estimated correlation matrices b S  X  and b S  X  can be directly plugged into different parametric Gaussian graph estimators to obtain the final precision matrix and graph estimates. 3.2.1. The Nonparanormal skeptic with the The main idea of the graphical Dantzig selector is to take advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix. The detailed algorithm is given below, where  X  is a tuning parameter.
  X  Estimation: For j = 1 ,...,d , calculate  X  Symmetrization: In the first step, for j th dimension, we regress X j on X \ j using the Dantzig selector. The obtained regres-sion coefficients b  X  j can then be exploited to estimate the elements  X  0 jj and  X  0 \ j,j in the inverse correlation matrix  X  0 . Within each iteration, the Dantzig selector selector in (3.6) can be formulated as a linear program. 3.2.2. The Nonparanormal skeptic with The estimated correlation coefficient matrix b S can also be plugged into the CLIME estimator (Cai et al., 2011), which is defined by b  X  = arg min where  X  is the tuning parameter. Cai et al. (2011) show that this convex optimization can be decomposed into d vector minimization problems, each of which can be cast as a linear program. Thus, CLIME has the potential to scale to very large problems. 3.2.3. The Nonparanormal skeptic with the We can also plug in the estimated correlation coeffi-cient matrix b S into the graphical lasso: b  X  = arg min One thing to note is that b S may not be positive semidefinite. While the formulation (3.11) is convex, certain algorithms (like the blockwise-coordinate de-scent algorithm or Friedman et al. (2008)) may fail. However, other algorithms such as projected Newton X  X  method or first-order projection do not have such pos-itive semidefiniteness assumptions. 3.3. Computational Complexity Compared to the corresponding parametric methods like the graphical lasso, graphical Dantzig selector, or CLIME, the only extra cost of the nonparanormal skeptic is the computation of b S , which requires the calculation of d ( d  X  1) / 2 pairwise Spearman X  X  rho or Kendal X  X  tau statistics. A naive implementation of Kendall X  X  tau matrix requires O ( d 2 n 2 ) computation. However, efficient algorithms based on sorting and balanced binary trees have been developed to calcu-late this with computational complexity O ( d 2 n log n ) (Christensen, 2005).
 If we assume that each data point is unique (no  X  X ies X  in computing ranks), then Spearman X  X  rho statistic can be written as ranks are obtained, the statistic b S  X  can be computed with cost O ( d 2 n log n ). We now present our main result, which shows that b S  X  and b S  X  estimate the true correlation matrix  X  0 at the optimal parametric rate in high dimensions. Such a result allows us to leverage existing analyses of dif-ferent parametric methods (e.g., the graphical lasso, graphical Dantzig selector, and CLIME) to analyze the nonparanormal skeptic estimator. 4.1. Concentration Properties of the We first prove the concentration properties of the esti-mators b S  X  and b S  X  . Let  X  0 jk be the Pearson correlation the k X k max norm, we show that both b S  X  and b S  X  are close to  X  0 at the optimal parametric rate. Our re-sults are based on different versions of the Hoeffding inequalities for U-statistics.
 Theorem 4.1. For any 0 &lt;  X  &lt; 1 , whenever n  X  max we have Therefore, let  X  = 1  X  d 2 , for n  X  Proof. The proof can be found in Theorem 4.1 of the long version of this paper; see Liu et al. (2012). The next theorem illustrates the concentration prop-erty of b S  X  .
 Theorem 4.2. For any n &gt; 1 , with probability at least 1  X  1 /d , we have Proof. The proof can be found in Theorem 4.2 of the long version of this paper; see Liu et al. (2012). This leads to the following  X  X eta-theorem, X  show-ing that even though the nonparanormal skeptic is a semiparametric estimator, it achieves the optimal parametric rate in high dimensions.
 Theorem 4.3. Suppose we plug the estimated corre-lation matrix b S  X  or b S  X  into the parametric graphical lasso (or the graphical Dantzig selector, or CLIME). Under the same conditions on  X  0 that ensure the con-sistency of these parametric methods, the nonpara-normal skeptic achieves the same parametric rate of convergence for both precision matrix estimation and graph recovery.
 Proof. The proof is based on the observation that the sample correlation matrix b S is a sufficient statistic for all three methods X  X he graphical lasso, graphical Dantzig selector, and CLIME. The conclusions of the analysis of Ravikumar et al. (2009); Cai et al. (2011) hold as long as there exists some constant c such that This condition is guaranteed from (4.1) and (4.2) of Theorems 4.1 and 4.2.
 Corollary 4.1. Over all the parameter spaces of  X  0 such that the graphical lasso, graphical Dantzig, or CLIME are minimax optimal under Gaussian models, the corresponding nonparanormal skeptic estimator is also minimax optimal for the same parameter space of  X  0 under the nonparanormal model.
 Remark 4.1. Even though in this section we only present the results on the graphical Dantzig selector, graphical lasso, and CLIME, similar arguments should hold for almost almost all methods that use the corre-lation matrix  X  0 as a sufficient statistic. In this section we investigate the empirical perfor-mance of different graph estimation methods on both synthetic and real datasets. In particular we consider the following methods:  X  Normal  X  the Gaussian graphical model.  X  npn-spearman  X  the nonparanormal skeptic using Spearman X  X  rho.  X  npn-tau  X  the nonparanormal skeptic using the Kendall X  X  tau.
 More thorough numerical comparisons can be found in the longer technical report (Liu et al., 2012). 5.1. Numerical Simulations We adopt the same data generating procedure as in Liu et al. (2009). To generate a d -dimensional sparse graph G = ( V,E ), let V = { 1 ,...,d } cor-respond to variables X = ( X 1 ,...,X d ). We asso-ciate each index j  X  { 1 ,...,d } with a bivariate data point ( Y (1) j ,Y (2) j )  X  [0 , 1] 2 where Y ( k ) 1 ,...,Y Uniform[0 , 1] for k = 1 , 2. Each pair of vertices ( i,j ) is included in the edge set E with probability where y i := ( y (1) i ,y (2) i ) is the empirical observation of ( Y tance. Here, s = 0 . 125 is a parameter that controls the sparsity level of the generated graph. We restrict the maximum degree of the graph to four and build the inverse correlation matrix  X  0 according to  X  0 jk = 1 if j = k ,  X  0 jk = 0 . 245 if ( j,k )  X  E , and  X  0 jk = 0 other-wise; the value 0 . 245 guarantees positive definiteness of  X  0 . Let  X  0 =  X  0  X  1 . To obtain the correlation ma-trix, we simply rescale  X  0 so that all diagonal elements are one. We then sample n data points x 1 ,...,x n from the nonparanormal distribution NPN d ( f 0 ,  X  0 ) where for simplicity we use the same univariate transforma-tions on each dimension, i.e., f 0 1 = ... = f 0 d = f To sample data from the nonparanormal distribution, we also need g 0 := ( f 0 )  X  1 . We use the power trans-formation g 0 ( t ) = sign( t ) | t | 3 and the Gaussian CDF identifiability conditions.
 To generate synthetic data, we set d = 100, resulting in 100 2 + 100 = 5 , 050 parameters to be estimated. The sample sizes are varied between n = 100, 200 and 500. Three conditions are considered, correspond-ing to using the power transformation, the Gaussian CDF transformation, and linear transformation (or no transformation).
 The nonparanormal skeptic estimators npn-spearman and npn-tau are two-step procedures. In the first step we obtain an estimate b S of the correlation matrix; in the second step we plug b S into a parametric graph es-timation procedure. In this numerical study, we con-sider the graphical lasso, parallel lassos (Meinshausen-B  X uhlmann), and the Dantzig selector. Further details can be found in Liu et al. (2012).
 We adopt false positive and false negative rates to eval-uate the graph estimation performance. Let b G  X  = ( V, b E  X  ) be an estimated graph using the regularization parameter  X  in the graphical lasso procedure (3.11). The number of false positives when using the regular-ization parameter  X  is The number of false negatives at  X  is defined as We further define the false negative rate (FNR) and false positive rate (FPR) as FNR(  X  ) := Let  X  be the set of all regularization parameters used to create the full path. The oracle regularization pa-rameter  X   X  is defined as The oracle score is defined to be FNR(  X   X  ) + FPR(  X   X  ). Let FPR := FPR(  X   X  ) and FNR := FNR(  X   X  ). Table 5.1 provides numerical comparisons of the three meth-ods on datasets with different transformations using two graph estimation algorithms (glasso and parallel lasso methods), where we repeat the experiments 100 times and report the average FPR and FNR values with the corresponding standard errors in the paren-theses. We also conducted experiments using the graphical Dantzig selector. Since it achieves perfor-mance similar to the parallel lasso procedure, we do not show its quantitative results.
 To illustrate the overall performance of the methods over the full regularization paths, the averaged ROC curves for n = 200 ,d = 100 over 100 trials are shown in Figure 1, using FPR(  X  ) , 1  X  FNR(  X  ) . From the  X  X o transform X  plot, we see that when the data are truly Gaussian, there is almost no difference between nor-mal , npn-spearman , and npn-kendall . From the power transformation and CDF transformation plots in Fig-ures 1, we see that the performance of the nonpara-nomal skeptic estimators ( npn-spearman and npn-tau ) are comparable. In this case, both methods signifi-cantly outperform the corresponding parametric meth-ods (the graphical lasso, parallel lassos, or graphical Dantzig selector). 5.2. Equities Data In this section we apply the nonparanormal skep-tic on the stock price data from Yahoo! Finance ( finance.yahoo.com ). We collected the daily clos-ing prices for 452 stocks that were consistently in the S&amp;P 500 index between January 1, 2003 and Jan-uary 1, 2008. This gives altogether 1,257 data points, each data point corresponding to the vector of closing prices on a trading day. With S t,j denoting the clos-ing price of stock j on day t , we consider the variables X tj = log ( S t,j /S t  X  1 ,j ) and build graphs over the in-dices j . We simply treat the instances X t as indepen-dent replicates, even though they form a time series. We Winsorize every stock so that its data points are within six times the mean absolute deviation from the sample average.
 The 452 stocks are categorized into 10 Global Industry Classification Standard (GICS) sectors, including Consumer Discretionary (70 stocks), Consumer Staples (35 stocks), Energy (37 stocks), Financials (74 stocks), Health Care (46 stocks), Industrials (59 stocks), Information Technology (64 stocks) Telecommunications Services (6 stocks), Materials (29 stocks), and Utilities (32 stocks). It is expected that stocks from the same GICS sector should tend to be clustered together. Figure 2 illustrates the estimated npn-spearman graph, with the nodes colored according to the GICS sector of the corresponding stock. The tuning parameter is automatically selected using the StARS stability based approach (Liu et al., 2010). We see that stocks from the same GICS sector tend to be grouped together. We proposed the nonparanormal skeptic , which uses Spearman and Kendall statistics to estimate correla-tion matrices. The method is computationally effi-cient, and can be viewed as an alternative to esti-mation of the transformations in the nonparanormal model. We showed that the method achieves the op-timal parametric rate of convergence for both graph and parameter estimation.
 The research of Han Liu, John Lafferty, and Larry Wasserman was supported by NSF grant IIS-1116730 and AFOSR contract FA9550-09-1-0373.
 Banerjee, O., Ghaoui, L. E., and d X  X spremont, A.
Model selection through sparse maximum likelihood estimation. Journal of Machine Learning Research , 9:485 X 516, March 2008.
 Cai, Tony, Liu, Weidong, and Luo, Xi. A constrained l1 minimization approach to sparse precision matrix estimation. Journal of the American Statistical As-sociation , 106:594 X 607, 2011.
 Christensen, David. Fast algorithms for the calculation of Kendall X  X   X  . Computational Statistics , 20(1):51 X  62, 2005.
 Friedman, Jerome H., Hastie, Trevor, and Tibshi-rani, Robert. Sparse inverse covariance estimation with the graphical lasso. Biostatistics , 9(3):432 X 441, 2008.
 Klaassen, Chris A. J. and Wellner, Jon A. Efficient estimation in the bivariate normal copula model:
Normal margins are least-favorable. Bernoulli , 3(1): 55 X 77, 1997.
 Kruskal, William H. Ordinal Measures of Association. Journal of the American Statistical Association , 53 No. 284.:814 X 861, 1958.
 Liu, Han, Lafferty, John, and Wasserman, Larry. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. Journal of Machine Learning Research , 10:2295 X 2328, 2009.
 Liu, Han, Roeder, Kathryn, and Wasserman,
Larry. Stability approach to regularization selection (StARS) for high dimensional graphical models. In
Proceedings of the Twenty-Third Annual Conference on Neural Information Processing Systems (NIPS) , 2010.
 Liu, Han, Han, Fang, Yuan, Ming, Lafferty, John, and
Wasserman, Larry. High dimensional semiparamet-ric Gaussian copula graphical models. Arxiv preprint arXiv:1202.2169 , 2012.
 Meinshausen, N. and B  X uhlmann, P. High dimensional graphs and variable selection with the lasso. Annals of Statistics , 34(3), 2006.
 Ravikumar, Pradeep, Wainwright, Martin, Raskutti,
Garvesh, and Yu, Bin. Model selection in Gaussian graphical models: High-dimensional consistency of ` 1 -regularized MLE. In Advances in Neural In-formation Processing Systems 22 , Cambridge, MA, 2009. MIT Press.
 Yuan, Ming. High dimensional inverse covariance ma-trix estimation via linear programming. Journal of Machine Learning Research , 11:2261 X 2286, 2010. Yuan, Ming and Lin, Yi. Model selection and estima-tion in the Gaussian graphical model. Biometrika ,
