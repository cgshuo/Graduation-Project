 1. Introduction XML has emerged as a lingua franca of the WWW and is rapidly replacing other formats as the preferred form for information ranging from protocol exchange messages to full doc-uments and databases. With this rapid growth, and the conversion of information resources to XML, comes an increasing need for effective search and retrieval of XML documents and their constituent elements. The XML retrieval problem (as formulated for the Initiative for the Evaluation of XML Retrieval or INEX) (Fuhr et al. 2002) is to retrieve not only complete documents, but also the component parts of those documents that may contain relevant information. Thus, an effective retrieval system for XML retrieval must deal with retrieval and ranking of both full documents and components derived from the document structure. In this research, and in the Cheshire II system used for the research, we define a document component ,o r simply component ,a sa continuous segment of an XML document representing some part of an XML document tree structure, and comprised of one or more XML document elements (i.e., spans of data consisting of a begin tag, and ending with the corresponding end tag).
 In the research reported here, we examine the application of data fusion methods to the XML retrieval problem. The basic notion of  X  X ata fusion X  or  X  X eta-search X  approaches to IR is quite simple and intuitively appealing. Early observations by researchers exam-ining different algorithms and query combination methods (Croft 2000, Shaw and Fox 1994, Belkin et al. 1995) indicated that no single retrieval algorithm could be shown to some combination of different search strategies should be more effective than any single strategy. 602 LARSON
In principle, we would expect that the more evidence the system has about the relationship between a query and a document (including the sort of structural information about the documents found in XML documents), the more accurate it should be in predicting the probability that the document will satisfy the user X  X  need. Other researchers have also shown that additional information about the location and proximity of Boolean search terms can also be used to provide a ranking score for a set of documents (Hearst 1996). In addition, the inference net model for IR has shown that Boolean search elements can be used as additional evidence of the probability of relevance in the context of a larger network of probabilistic evidence (Turtle and Croft 1990).

The concept of data fusion tested in early TREC evaluations, where a number of partici-pating groups found that fusion of multiple retrieval algorithms provided an improvement ove ra single search algorithm (Shaw and Fox 1994, Belkin et al. 1995). With ongoing improvements of the algorithms used in the TREC main (i.e., ad hoc retrieval) task, later analyses (Lee 1997, Beitzel et al. 2003) found that the greatest effectiveness improvements appeared to occur between relatively ineffective individual methods. These researchers also observed that the fusion of ineffective techniques, while often approaching the effectiveness of the best single IR algorithms, seldom exceeded them for individual queries and never e xceeded their average performance (Beitzel et al. 2003). However, these observations were based on the retrieval of full documents where the query results from multiple algorithms were combined into a single result set.
 were based on retrieval from different representations, or component parts, of documents (Katzer et al. 1982, Das-Gupta and Katzer 1983) showed that those different representations provided similar overall retrieval performance, but retrieved different sets of relevant and non-relevant documents.

In the following experiments we combine not only separate algorithms, but also combina-tions of separate indexes derived from different components of XML documents, where the index statistics are derived from the individual components within the document and collec-tion rather than from the entire document, with result lists that merge components ranging from full documents to individual bibliographic entries from a document X  X  references. In this analysis we are examining a pair of hypotheses: H :F or XML retrieval, there is no difference between effective individual search algorithms and fusion of multiple algorithms (the null hypothesis).
 H : Fusion of the results of searches of different components of XML documents is more effective than searches of single components.

The research reported here extends our work on fusion approaches conducted for the 2002 and 2003 INEX Evaluations (Larson 2003, 2004) using the Cheshire II XML retrieval system. The basic approach used in INEX and in the current evaluation was to combine the results of searching different document components and different probabilistic retrieval algorithms within single search operation. The use of different algorithms, and combinations of algorithms for structured document retrieval has been examined (in a non-XML context) by others (Fuller et al. 1994, Wilkinson 1994, Navarro and Baeza-Yates 1995, Kaszkiel and A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 603 Zobel 1994). Most earlier approaches used vector space algorithms and documents with simpler structure (such as the simple SGML structures used for TREC documents). The INEX database used in this research consists of all Computer Science publications of the IEEE for the years 1995 X 2002 in the complex XML format used in the publication process. Thus the collection is a medium-scale digital library of Computer Science information about 500 MB in size.
 for combining different retrieval algorithms and XML components (including full arti-cles) using data fusion methods with the INEX XML test collection. The following section scribe the experimental methodology, the test collection used, and the characteristics of the queries. Finally, results of the evaluation, suggestions for future work and conclusions are presented. 2. The retrieval algorithms and fusion operators In his analysis of fusion approaches to improving retrieval performance (Lee 1997) found that the best results were obtained by combining algorithms where similar sets of relevant documents were returned but that retrieved different sets of non-relevant documents. With criteria. The first algorithm is based on logistic regression and the second is the well-known Okapi BM-25 algorithm.

We conducted an analysis of the overlap between the result lists retrieved by these algo-rithms. We found that on average, over half of the result lists retrieved by each algorithm in these overlap tests were both non-relevant and unique to that algorithm, fulfilling Lee X  X  criteria for effective algorithm combination: similar sets of relevant documents and different sets of non-relevant. We will return to parts of this overlap analysis in the later evaluation and discussion section.

In the remainder of this section we describe the Logistic Regression and Okapi BM-25 algorithms that were used for the evaluation and we also discuss the methods used to combine the results of the different algorithms. The algorithms and combination methods are implemented as part of the Cheshire II XML/SGML search engine (Larson 2003, 2002) which also supports a number of other algorithms for distributed search and operators for merging result lists from ranked or Boolean sub-queries. 2.1. Logistic regression algorithm The logistic regression (LR) algorithm used in this study was originally developed by Cooper et al. (1992) and shown to provide good full-text retrieval performance in the TREC ad hoc task and in TREC interactive tasks (Larson 2001) and for distributed IR (Larson 2002). As originally formulated, the LR model of probabilistic IR attempts to estimate the probability of relevance for each document based on a set of statistics about a document collection and a set of queries in combination with a set of weighting coefficients for those 604 LARSON regression analysis of a sample of a collection (or similar test collection) for some set of queries where relevance and non-relevance has been determined. More formally, given a documents or components are presented to the user ranked in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P ( R | Q , D ) uses the  X  X og odds X  of relevance given a set of S statistics, s i , derived from the query and database, such that: where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of the sample collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:
Based on the structure of XML documents as a tree of XML elements, we define a  X  X ocument component X  as an XML subtree that may include zero or more subordinate XML Document Type Definition (DTD) for the INEX test collection used in this study ( &lt; Thus, a component might be defined using any of these tagged elements. However, not therefore we defined the retrievable components selectively, including document sections and paragraphs from the article body, and bibliography entries from the back matter (see T able 2).

Naturally, a full XML document may also be considered a  X  X ocument component X . As discussed below, the indexing and retrieval methods used in this research take into account process and for extraction of the parts of a document to be returned in response to a query. Because we are dealing with not only full documents, but also document components (such as sections and paragraphs or similar structures) derived from the documents, we will use C to represent document components in place of D . Therefore, the full equation describing A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 605 the LR algorithm used in these experiments is: where Q is a query containing terms T , |
Q | is the total number of terms in Q , |
Q tf j is the frequency of the j th term in a specific document component, qt f j is the frequency of the j th term in Q, n cl is the document component length measured in bytes.
 N is the number of components of a given type in the collection.

This equation, used in estimating the probability of relevance in this research, is essentially the same as that used in Cooper et al. (1994). The coefficients were estimated using relevance judgements and statistics from the TREC/TIPSTER test collection. In this evaluation we used the same coeffients for each of the main document components used. This means, that we, in effect, are treating all components smaller than a full document as if they were e xactly that, small documents. 2.2. Okapi BM-25 algorithm description of the algorithm in Robertson and Walker (1997), and in TREC notebook pro-ceedings (Robertson et al. 1998). As with the LR algorithm, we have adapted the Okapi BM-25 algorithm to deal with document components: Where (in addition to the variables already defined): K is k 1 ((1  X  b ) + b  X  cl / a v cl ), k , b and k 3 are parameters (1.5, 0.45 and 500, respectively, were used), 606 LARSON a v cl is the average component length measured in bytes, w (1) is the Robertson-Sparck Jones weight, r is the number of relevant components of a given type that contain a given term, R is the total number of relevant components of a given type for the query.

Our current implementation uses only the a priori v ersion (i.e., without relevance infor-mation) of the Robertson-Sparck Jones weights, and therefore the w (1) v alue is effectively just an IDF weighting. The results of searches using our implementation of Okapi BM-25 and the LR algorithm seemed sufficiently different to offer the kind of conditions where data fusion has been shown to be be most effective (Lee 1997), and our overlap analysis of results for each algorithm (described in the evaluation and discussion section) has confirmed this difference and the fit to the conditions for effective fusion of results. 2.3. Boolean operators The system used supports searches combining probabilistic and (strict) Boolean elements, as well as operators to support various merging operations for both types of intermediate result sets. Although strict Boolean operators and probabilistic searches are implemented two parallel logical search engines. Each logical search engine produces a set of retrieved probabilistically ranked set or an unranked Boolean result set. When both are used within in a single query, combined probabilistic and Boolean search results are evaluated using the assumption that the Boolean retrieved set has an estimated P ( R | Q bool , C ) = 1 . 0 for each document component in the set, and 0 for the rest of the collection. The final estimate for the probability of relevance used for ranking the results of a search combining strict Boolean and probabilistic strategies is simply: where P ( R | Q prob , C )i s the probability of relevance estimate from the probabilistic part of the search, and P ( R | Q bool , C )i s the Boolean. In practice the combination of strict Boolean  X  X ND X  and the probablistic approaches has the effect of restricting the results to those items that match the Boolean part, with ranking based on the probabilistic part. Boolean  X  X OT X  provides a similar restriction of the probabilistic set by removing those document components that match the Boolean specification. When Boolean  X  X R X  is used the probabilistic and Boolean results are merged (however, items that only occur in the Boolean result, and not both, are reweighted as in the  X  X uzzy X  and merger operations described below. A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 607
A special case of Boolean operators in Cheshire II is that of proximity and phrase matching operations. In proximity and phrase matching the matching terms must also satisfy prox-imity constraints (both term order and adjacency in the case of phrases). Thus, proximity operations also result in Boolean intermediate result sets. 2.4. Result combination operators The Cheshire II system used in this evaluation provides a number of operators to combine the intermediate results of a search from different components or indexes. With these op-erators we have available an entire spectrum of combination methods ranging from strict Boolean operations to fuzzy Boolean and normalized score combinations for probabilistic and Boolean results. These operators are the means available for performing fusion op-erations between the results for different retrieval algorithms and the search results from different components of a document. We will only describe four of these operators here, because they were the only types used in the evaluation reported in this paper.
The MERGE MEAN operator combines the two result lists (like a Boolean OR) but takes the mean of the weights from items in both lists or half the weight of items in only a single list. Similarly, the MERGE NORM operator combines the two results but it performs the min-max normalization of the weights suggested by Lee (1997) before it takes the mean of the weights from items in both lists and half of the weight of items in only a single list. The MERGE NSUM operator performs min-max normalization of the weights, but sums the normalized weights. The MERGE CMBZ operator is based on the  X  X ombMNZ X  fusion algorithm developed by Shaw and Fox (1994) and used by Lee (1997). In our version we take the normalized scores, but then further enhance scores for components appearing in both lists (doubling them) and penalize normalized scores appearing low in a single result list, while using the unmodified normalized score for higher ranking items in a single list. 3. Experimental methods In this section we discuss the data and methods used in conducting the evaluation. We begin by describing the test collection and the indexing methods applied to it. We then describe the query processing and combinations of operators used in the experiments, and the evaluation methods and metrics. 3.1. Indexing the INEX collection The INEX test collection (version 1.4) is composed of an XML document collection, sets of search topics and document component relevance assessments. The INEX XML document collection contains the full content of the IEEE Computer Society X  X  journal publications starting in 1995, which represents 12107 article-level documents (about 525 MB in size). The specific contents and coverage are described in the introductory paper of this special issue.
During the INEX evaluation process, each participating organisation submitted a number of candidate topics in XML format (figures 1 and 2 show typical INEX topics), and some 608 LARSON of these were selected (along with occasional modifications) as the queries for each year. After all groups had returned their retrieval results for the queries, the results were pooled and relevance assessments of selected document components (and other components within the same document) were performed (in most cases) by the participants who submitted the topics. For this research we used the 2003 INEX topics and assessments for the evaluation. INEX topics include two query types, CO or  X  X ontent Only X , like that shown in figure 1 and CAS or  X  X ontent and Structure X , as shown in figure 2, in which an extended form of XPath is used to describe the document elements and retrieval criteria. The CAS queries were used for two retrieval tasks,  X  X CAS X  and  X  X CAS X , where the specified structural elements were treated as suggestions or requirements, respectively. The INEX 2003 topics include 30 CAS topics and 30 CO topics. In this evaluation we used both the INEX CO and CAS topics, although our primary focus is on the CO topics. For both types of query some of the conventions of internet search engines have been adapted to indicate special processing for search terms included in the title element. These include  X  +  X  preceding a word or phrase that should be present in the result component,  X   X   X  X  o indicate results should not contain a term, and double-quotes around terms to indicate that the e xact phrase is desired in the results. However, these criteria need not (necessarily) be treated as Boolean constraints on the results, and relevance assessments do not strictly enforce them in all cases. A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 609
As noted above, the system used in this research permits document components to be defined, indexed and retrieved as if they were individual documents, with separate indexes and ranking statistics used during retrieval. In addition to flexible indexing, it includes fa-cilities for document component retrieval and display, including the ability to request any individual XPATH specification from any document selected during searching. The system also permits a variety of term extraction methods to be specified for indexed elements, in-cluding proximity information and different data types (dates, integers, etc.). Only keyword e xtraction with proximity was used in the tests reported here. In addition several types of normalization can applied to the indexing data extracted from the text nodes of the XML document components. In this study we used only an slightly enhanced version of the Porter 610 LARSON stemmer for the extracted indexes (the enhancements correct some of the incorrect stems for specific words).

Each index generated by the system can have its own specialized stopword list, so that, for e xample, XML elements containing corporate names can have a different set of stopwords from document titles or personal names.

Most of the indexes used for the evaluation used keyword with proximity extraction and stemming of the keyword tokens. Exceptions to this general rule were date elements (which were extracted using date extraction of the year only) and the names of authors which were e xtracted without stemming or stoplists to retain the full name.

T able 1 lists the document-level (/article) indexes created for INEX and the XPaths of the document elements from which the contents of those indexes were extracted. As noted above, the system permits document component subtrees to be treated as separate documents A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 611 with their own separate indexes. Tables 2 and 3 describe the XML components defined for the evaluation and the component-level indexes that were created for them.

T able 2 shows the components and the path used to define them. The COMP SECTION ments, permitting each individual section of a article to be retrieved separately. Similarly, each of the COMP BIB, COMP P ARAS, and COMP FIG components, respectively, treat that can be retrieved separately from the entire document.
 612 LARSON
T able 3 describes the XML component indexes created for the components described elements). These indexes make the contents of a number of individual document components av ailable for searching. For example, sections (COMP SECTION) of the INEX documents Bibliographic references in the articles (COMP BIB) are made accessible by the author names, titles, and publication date of the individual bibliographic entry, with proximity searching supported for bibliography titles. Individual paragraphs (COMP P ARAS) are searchable by any of the terms in the paragraph, also with proximity searching. Individual figures (COMP FIG) are indexed by their captions, and vitae (COMP VITAE) are indexed by keywords within the text, with proximity support. 3.2. Query characteristics and tests Analysis of the INEX 2002 relevance assessments showed that relatively large document components were more likely be judged relevant. This also makes sense, given that the CO queries, in most cases specified a discussion of a topic as the desired result, and elements our INEX 2003 submission, and the subsequent tests reported here, we restricted results to document components including: complete articles, the article body only, individual sections (including nested subsections), and paragraphs (including the different tag types shown for the COMP P ARAS component in Table 2). The query construction method described below wa s applied to each of these component types, and the results combined using the min-max normalization of the weights in the result lists, as suggested by Lee (1997).
The INEX topics, as discussed above and shown in figures 1 and 2 may include suggested constraints on the terms used. That is, they specify phrases, desired terms and deprecated the query were extracted. In all of the tests reported here only the terms from  X  X itle X  and  X  X eywords X  elements of the topic were used in constructing the queries.

The simplest form of query is one where no phrases, suggested terms, or deprecated terms were specified in the topic. In this case the query (for a given index and document component) probabilistic algorithm to use in searching and the index to be searched. This type of query alone was used in the  X  X ROB B ASE X  and  X  X KAPI B ASE X  tests shown in Tables 5 and 6 which use, respectively, the LR and Okapi algorithms.

If the terms extracted from a topic include a phrase specification, the phrase was extracted and a Boolean phrase search of the given index was added to the simple query above as a sub-query, combined with the probabilistic part of the query using the MERGE MEAN, MERGE NORM or MERGE CMBZ, operators described above.

Deprecated terms were handled by constructing a Boolean subquery using a Boolean NOT (restricting the results to components that did not contain the deprecated terms). Desired terms were extracted and a separate ranked search performed with only the desired terms, but with increased query term frequency for them. The results of these subqueries were then A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 613 combined with the simple (base) search using the MERGE NORM or MERGE CMBZ operator. Because topics often contained all of the above types of term specification, the resulting generated queries were often quite complex and this was compounded by the use of multiple indexes and components for most queries.

The tests shown in Tables 5 and 6 that include  X  FULL X  in the name include all of the above e xpansions of the topic terms in the queries. Thus,  X  X ROB FULL X  and  X  X KAPI FULL X  use the LR and Okapi algorithms, respectively, and include the full expansion.
Fo r fusion operations between different indexes for a particular document component, the MERGE NORM operator was used to combine the sub-query results. In Tables 5 and 6  X  X USION FULL X  combines full queries of only the topic, sec w ords, and para w ords indexes for both LR and Okapi,  X  X USION T FULL X  combines both the topic, alltitles, sec w ords, sec title, and para w ords,  X  X USION TA FULL X  adds the abstract index to this. As in the preceding,  X  X USION T P ABS FULL X  and  X  X USION T P ABS FULL X  use the same indexes, but perform an additional LR search of the abstract and extract and merge the abstract in the final results used in evaluation. The  X  X USION T CMBZ X  run used the same indexes as  X  X USION T FULL X , but instead of using the MERGE NORM operator, it used the MERGE CMBZ operator which enhances scores for components appearing in both intermediate result lists, and penalizes the lower ranked scores in a single list. 3.3. Evaluation metrics The INEX evaluation metrics described here are discussed in greater detail in G  X  overt et al. (2003) and Kazai et al. (2004). The INEX evaluations involved relevance assessments of the submitted results of each participating group on two (separate though related) dimensions. The dimensions were Exhaustivity , describing the extent to which the document component discusses the topic of request, and Specificity , describing the extent to which the document component focuses on the topic of request. For exhaustivity assessments a 4-point scale wa s used in the INEX evaluation: 0: Not exhaustive, the document component does not discuss the topic of request at all. 1: Marginally exhaustive, the document component discusses only few aspects of the topic 2: Fairly exhaustive, the document component discusses many aspects of the topic of 3: Highly exhaustive, the document component discusses most or all aspects of the topic
To assess specificity, another 4-point scale was used: 0: Not specific, the topic of request is not a theme of the document component. 1: Marginally specific, the topic of request is a minor theme of the document component 2: Fairly specific, the topic of request is a major theme of the document component. 3: Highly specific, the topic of request is the only theme of the document component. 614 LARSON
Fo r the calculation of the Recall and Precision analogs used for INEX, two different quantizations were used that map the assessed values of these two dimensions for document components into a single value representing relevance. These quantization functions on e xhaustiveness ( e ) and specificity ( s ), f quant ( e , s ): ES  X  [0 , 1] are:  X 
A  X  X trict X  quantization which indicates whether a given retrieval approach is capable of retrieving highly exhaustive and highly specific document components. This is defined as:  X 
A  X  X eneralized X  quantization that scores document elements according to their degree of relevance, defined as:
Based on the quantized relevance values, procedures that calculate recall/precision curves for standard document retrieval can be applied directly to the results of the quantization functions. The primary measure used in comparing the test results is the Mean Average Precision (MAP). It is worth noting that for those readers unfamiliar with INEX and the ev aluation tools that the calculation uses more data points in calculating MAP, and different interpolation than in TREC. The MAP values for INEX CO retrieval are considerably lower than those seen in TREC (the highest MAP of an official content-only run, over all a more complete discussion of these metrics and their derivation. 4. Evaluation and discussion As the above discussion of metrics indicates, the relevance judgements used in the INEX test collection are not predicated on binary relevance judgements at the article level, but instead on XML component retrieval with scales of both specificity and exhaustivity. Con-sequently the fusion approaches that we have been been exploring must consider both the optimal combinations of search elements and algorithms that should be used in the retrieval process. For these experiments we have not re-estimated the logistic regression parameters or examined the possibility of differential weightings that could be applied to the search elements to best estimate the probability of relevance for a given query and document ele-ment or combination of elements. However, in the conclusions we will show some recent A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 615 preliminary results from re-estimating the logistic regression parameters for different XML components.

In this section we will first examine the overlap analysis that explores the distribution of relevant components in the results obtained with different retrieval algorithms. We then e xamine how retrieval of individual components compares to combination of components in the results. We will then present and discuss how some of the fusion methods tested perform relative to the base methods for both the content-only (CO) and the strict content implications of the analyses and results for XML retrieval. 4.1. Overlap analysis In our introduction to the retrieval algorithms used in this study we pointed out how (Lee 1997) found that the most effective fusion algorithms were those that combined base algo-rithms that retrieved similar sets of relevant documents, but different sets of non-relevant documents. In this section we examine the overlap of relevant and non-relevant results for the LR and Okapi BM-25 algorithms.

We conducted an analysis of the results and relevance for base versions of each algorithm (PROB B ASE and OKAPI B ASE, as described above) using the INEX 2003  X  X ontent Only X  queries and relevance judgements and the top-ranked 1500 items for each algorithm. In Table 4 the raw counts of components retrieved retrieved only by PROB B ASE or OKAPI B ASE along with the counts for components retrieved by both algorithms. Note that the  X  X oth X  numbers are not the results of fusion, but of the analysis and comparison of the components returned by each separate algorithm. As the table shows, on average, only 47.75% of the combined results were retrieved by both algorithms, while the remaining 52.25% of each result set was unique to one algorithm or the other. Table 4 also shows the percentages of the retrieved components judged to be relevant (for all non-zero combinations of specificity and exhaustivity). On average, the majority of the relevant items (67.43%) were retrieved by both algorithms, and the PROB B ASE-only and OKAPI B ASE-only items were 14.13% and 18.44% respectively.

In addition to the data shown in Table 4 we also examined the retrieved items that matched the relevant items under the  X  X trict X  metric for relevance. The results were not radically different from the generalized relevance criteria discussed above. The common results had the majority (68.68%) of the highly relevant items (using the  X  X trict X  metric) while the PROB B ASE-only and OKAPI B ASE-only items had 15.19% and 16.14% respectively. Interestingly, these similar average percentages mask the fact that each algorithm performed quite differently on different queries, with each algorithm out-performing the other in some cases, and for a few queries each algorithm uniquely retrieved more relevant documents than appeared in the common set. This is shown in Table 4 where, for example, PROB B ASE X  X  logistic regression algorithm is clearly is superior for topic 96, while the OKAPI B ASE BM-25 algorithm is superior for topic 111. Needless to say, we are investigating what causes this differential for particular queries, but we have no firm conclusions to report yet.
Many additional analyses of the overlap between the results from the base algorithms and the fusion results were conducted as well. These showed, as expected, that fusion is not 616 LARSON A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 617 a guarantee that all of the relevant items of individual algorithms will become part of the fused results. Of course, it not possible to know at retrieval time which of the components that are retrieved by different algorithms are relevant and which are not. Therefore both relevant and non-relevant common items in the fusion process are given enhanced weights fusion, on average, improves the rankings for relevant components.

On average, over half of the result lists retrieved by each algorithm in these overlap tests were both non-relevant and unique to that algorithm, fulfilling Lee X  X  criteria for effective algorithm combination: similar sets of relevant documents and different sets of non-relevant. This analysis tends to confirm Lee X  X  observations. Because the majority of items retrieved common relevant items are promoted, however common non-relevant items will also tend to be promoted. 4.2. Component retrieval analysis We conducted a number of tests where the retrieval results using different individual indexes and individual components were compared to each other, and to the base and fusion methods. The results are shown graphically in figure 3, where each document component is labeled according to it X  X  XML tag, with the exceptions of  X  X aras X  and  X  X ections X  which represent all of types of paragraphs, and sections respectively. All of these tests made use of the LR algorithm.

As figure 3 shows, none of the individual components come at all close to the the per-formance of the baseline LR method PROB B ASE, which merges the  X  X aras X ,  X  X ections X ,  X  X rticle X , and  X  X dy X  into a single search result list. We used t-tests on paired samples at the individual query level to test for significant differences between each of the individual component results and the merged baseline results. In spite of the small sample used in this analysis (only the 30 INEX 2003 Content-Only queries) there were statistically significant differences between each of the component results and the combined baseline results, with t v alues ranging from a low of  X  3.299 to  X  5.320 with significance at the 0.003 level or better.

This provides fairly strong support for our H 1 hypothesis that fusion of the results of searches of different components of XML documents is more effective than searches of single components. However, this observation must be accompanied by a caveat. Because the INEX evaluation method requires that judges rate not only the specific component retrieved, bu t also the parents and children of that component in the XML document structure, there is necessarily overlap of components judged relevant. Attempts are underway to provide a solution to this issue for the INEX 2004. The overlap problem in XML retrieval and proposed solutions are discussed in Kazai et al. (2004). 4.3. Content-only evaluation results of the INEX evaluation metrics, respectively. In these tables P shows the percentage 618 LARSON difference for the test from the  X  X ROB B ASE X  baseline and  X  O shows the difference from  X  X KAPI B ASE X . We also used t-tests on paired samples at the individual query level to test for significant differences between each of the base methods and the other methods. Because of the small sample used in this analysis (only the 30 INEX 2003 Content-Only queries) the best methods were only able to show significance at 0.07 or better under the strict metric (these are shown with asterisks next to the values in Table 5). With the gen-eralized metric the only worst performing fusion method showed a statistically significant difference from the base methods (and that only at the 0.011 significance level when com-pared to the LR base, and the 0.038 level when compared to the Okapi base). Thus, for this limited sample size we are unable to strongly reject our H 0 null hypothesis, that there is no difference between effective individual search algorithms and fusion of multiple al-gorithms. However the results obtained here are very encouraging for the effectiveness of fusion methods for XML retrieval.
 A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 619
Figures 4 and 5 show the Recall/Precision curves for strict quantization of the base algo-rithms (PROB B ASE and OKAPI B ASE) in combination with the full expanded queries (figure 4) or the best performing fusion query (FUSION T FULL). Figures 6 and 7 show the same tests with the generalized quantization metric.

As Tables 5 and 6 indicate, the use of query expansion, as discussed in Section 3.2, appears to offer some benefit over unexpanded queries for both quantizations, PROB FULL shows improvement over PROB B ASE and OKAPI FULL shows improvement over OKAPI B ASE. What is somewhat more interesting is that under strict quantization the LR approach in PROB FULL performs better than either okapi test, but for generalized quantization both Okapi tests perform better than either LR test (and indeed better than some of the fusion 620 LARSON approaches. This implies that the Okapi algorithm is better at identifying a wider range of degrees of perceived relevance, while the LR algorithm is better at identifying the highly relevant items.
 When the two algorithms are combined (with only topic and word searches in FU-SION FULL) the results for both the strict and generalized measures are better than any of the single algorithms. This is different from the kind of results reported in Beitzel et al. (2003), and seems to confirm the improvements from data fusion reported by Lee (1997). When the searches include a separate ranking of title searches merged with the topic searches the performance is further improved and performs the best for both quantizations of all of the query forms examined here. It is worth noting that if the FUSION T CMBZ run under strict quantization had been submitted during the official INEX 2003 evaluation, it would have been the second highest ranking run, and the FUSION T FULL run under generalized quantization would have been the fifth highest ranked run.
 A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 621
However, it appears that element indexes cannot be arbitrarily combined in attempting to improve performance, as the reported results show, adding the abstract index results in re-duced performance relative to topic and titles alone. Dozens of other combinations of merger operators and indexes were tested, and only the best performing ones are reported here. It is likely to be the case that different XML collections will require different combinations of indexes and operators to achieve similar results. 4.4. SCAS evaluation results We also applied the fusion approaches tested above to the  X  X ontent and structure X  (SCAS) task of INEX. The Mean Average Precision results for some of these SCAS tests are shown in Table 7. The table shows that the LR-based queries (indicated by  X  X CAS.P X  in the names) seem to be generally less effective than the Okapi-based queries (including  X  X CAS.O X  in the run names). Of course, the SCAS queries are in general more complex than the CO queries, and make use of many additional merging operations driven by the individual Xpath queries. The main operator needed in SCAS queries was the  X  X ESTRICT X  622 LARSON A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 623 operator that requires components matching queries in one part of an XPath to be contained constructed for the INEX topic shown in figure 2 is shown in figure 8. In that query the fusion and restriction operators described above are operators used to combine the results of the component searches (using the indexes shown in Tables 1 and 3) for both Okapi BM-25 (specified by  X  X  +  X ) and LR (specified by  X  X  X ). This query is automatically generated from the  X  &lt; title &gt;  X  element of the INEX topic. Because SCAS queries impose structural constraints on the results, they tend to be more limited in scope than the CO queries, that is, there is less, or no, flexibility in the choice of the elements of the document to return.
The results shown in Table 7 use single digits in the name to indicate particular combina-tions of fusion and restriction operators and single letters to indicate the ranking algorithm used. Thus, a test containing  X  X 2 X  in the name used the same fusion and restriction oper-ation as a test containing  X  X 2 X , but the former used Okapi BM-25 and the latter used the 624 LARSON A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 625 LR algorithm. The Fusion tests (indicated by names beginning with  X  X CAS.FUS X ) each combine results from different combinations of fusion and restriction operators, those with numbers only in the last part of the name are Okapi only tests, and the others combine LR and Okapi results. The best performing SCAS test under the generalized evaluation metric wa sa n Okapi run (SCAS.O.8) that used the  X  X ERGE NORM X  operator when a  X  X ND X  wa s used in an  X  X bout X  clause in a query, and  X  X ERGE SUM X  was used for  X  X R X . For Xpath expression with separate  X  X bout X  clauses in nodes on different levels in the document tree, the  X  X ESTRICT FROM X  operators were used. Terms with  X  +  X ,  X   X   X , and quotes were handled the same way as in the CO runs, with added search elements for exact phrase matching, additional query term weighting for  X  +  X  and use of Boolean  X  X OT X  for  X   X   X . Figures 9 and 10 show the generalized recall-precision metrics for the SCAS runs above. Figure 9 shows the LR and Okapi results and figure 10 shows the different fusion results. 626 LARSON individual query level to test for significant differences between the least effective single algorithm and the others, for both the strict and generalized metrics. Under the  X  X trict X  metric, this compared the LR test  X  X CAS.P.2 X  as the baseline to the other tests. Two of the Okapi-based fusion results ( X  X CAS.FUS.258 X  and  X  X CAS.FUS.78 X ) showed a significant difference from the baseline (with t = 2 . 318 and 2.340 respectively) at the 0.029 or better level. In comparing  X  X CAS.P.2 X  under the generalized metric, the only statistically signif-icant difference (at the 0.044 level) was in the  X  X CAS.O.8 X  test. Interestingly, under the strict metric, the  X  X CAS.o.8 X  test was not as different from the baseline as the fusion results in spite of a overall higher score.

Essentially these results tend confirm the observations of our overlap analysis, that the different algorithms do perform differently on a query by query basis, and when the re-sults of different algorithms are fused some of that distinction is lost. That is, the fusion results including the same algorithm as the baseline tend to be more similar to the base-line than those do not. However, on average fusion appears to improve the overall aver-age results by deriving at least some of the relevant items from the results of different algorithms. 5. Conclusions and further research At this early stage of the INEX test collection, with a small number of queries and relevance judgements, statistical significance is difficult to obtain, so these observations and conclu-sions will need to be re-tested and confirmed as the query collection expands. Revisiting our hypotheses for this research: H :F or XML retrieval, there is no difference between effective individual search algorithms and fusion of multiple algorithms (the null hypothesis).
 H : Fusion of the results of searches of different components of XML documents is more effective than searches of single components.
 Although we are unable to strongly reject H 0 , the results seem to suggest that it may be cautiously rejected, subject to further testing. For H 1 the evidence is much stronger, and in f act required by the INEX task.

However, there is much room for further study, in particular this study did not include language models of XML, which have proved to be highly effective in the INEX evalua-tions. Future work will extend this study to include language model based XML retrieval algorithms and test it in combination with the logistic regression and Okapi algorithms tested here.
 algorithm based on the INEX 2003 relevance assessments. In fact, separate formulae were derived for each of the major components of the INEX XML document structure, providing a different formula for each index/component of the collection. These formulae were used in the official ad hoc runs submitted for the INEX 2004 evaluation. For testing purposes we resubmitted the INEX 2003 CO queries used in the  X  X ROB B ASE X  tests in this paper, and A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 627 were able to obtain a mean average precision of 0.1158 under the strict metric and 0.1116 for the generalized metric, thus exceeding the best fusion results reported here. However, these tentative results cannot be properly compared to the tests in this paper, because the data used for training the LR model was obtained using the relevance data associated with the same topics. The true test will be the results in INEX 2004.

In closing we offer a few general observations, questions and directions for futher research derived from these analyses: 1. All fusion operators combine the scores of individual result lists, and all tend to promote 2. The differences in performance on a query-by-query basis between the LR and Okapi 3. In TREC the use of  X  X lind feedback X  has proven to be quite effective in boosting per-
In this paper we have examined the fusion of different algorithms and document com-ponents in content-oriented and content and structure XML retrieval. The results indicate that several of the fusion approaches that we tested do perform better than the individual algorithms, and also that some Boolean structural constraints are beneficial (or necessary) for XML retrieval.

XML retrieval is becoming an increasingly important area in IR research, and it provides an ew context in which the algorithms and techniques developed over the past half century can be re-examined and evaluated.
 Acknowledgments This work was supported in part by the National Science Foundation and Joint Information Systems Committee(U.K) under NSF International Digital Libraries Program aw ard #IIS-9975164.
 References 628 LARSON A FUSION APPROACH TO XML STRUCTURED DOCUMENT RETRIEVAL 629
