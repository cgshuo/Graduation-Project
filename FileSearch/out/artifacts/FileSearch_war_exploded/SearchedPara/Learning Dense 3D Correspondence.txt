 Establishing 3D correspondence between surfaces such as human faces is a crucial element of class-specific representations of objects in computer vision and graphics. On faces, for example, cor-responding points may be the tips of the noses in 3D scans of different individuals. Dense corre-spondence is a mapping or  X  X arp X  from all points of a surface onto another surface (in some cases, including the present work, extending from the surface to the embedding space). Once this mapping is established, it is straightforward, for instance, to compute morphs between objects. More im-portantly, if correspondence mappings between a class of objects and a reference object have been established, we can represent each object by its mapping, leading to a linear representation that is able to describe also new objects of similar shape and texture (for further details, see [1]). The practical relevance of surface correspondence has been increasing over the last years. In com-puter graphics, applications involve morphing, shape modeling and animation. In computer vision, an increasing number of algorithms for face and object recognition based on 2D images or 3D scans, as well as shape retrieval in databases and 3D surface reconstruction from images, rely on shape rep-resentations that are built upon dense surface correspondence.
 on two objects, we treat correspondence as a machine learning problem and propose a data-driven approach that learns the relevant criteria from a dataset of given object correspondences. In stereo vision and optical flow [2, 3], a correspondence is correct if and only if it maps a point in one scene to a point in another scene which stems from the same physical point. In contrast, corre-spondence between different objects is not a well-defined problem. When two faces are compared, only some anatomically unique features such as the corners of the eyes are clearly corresponding, while it may be difficult to define how smooth regions, such as the cheeks and the forehead, are supposed to be mapped onto each other. On a more fundamental level, however, even the problem of matching the eyes is difficult to cast in a formal way, and in fact this matching involves many of the basic problems of computer vision and feature detection. In a given application, the desired correspondence can be dependent on anatomical facts, measures of shape similarity, or the overall layout of features on the surface. However, it may also depend on the properties of human percep-tion, on functional or semantic issues, on the context within a given object class or even on social convention. Due to the problematic and challenging nature of the correspondence problem, our cor-respondence learning algorithm may be a more appropriate approach than existing techniques, as it is often easier to provide a set of examples of the desired correspondences than a formal criterion for correct correspondence.
 In a nutshell, the main idea of our approach is as follows. Given two objects O seeking a correspondence mapping  X  such that certain properties of x (relative to O in  X  ( x ) (relative to O 2 )  X  they are invariant. These properties depend on the object class and as explained above, we cannot hope to characterize them comprehensively a priori. However, if we are given examples of correct and incorrect correspondences, we can attempt to learn properties which are invariant for correct correspondences, while for incorrect correspondences, they are not . We shall do this by providing a dictionary of potential properties (such as geometric features, or texture properties) and approximating a  X  X rue X  property characterizing correspondence as an expansion in that dictionary. We will call this property warp-invariant feature and show that its computation can be cast as a problem of oriented PCA.
 The remainder of the paper is structured as follows: in Section 2 we review some related work, whereas in Section 3 we set up our general framework for computing correspondence fields. Fol-lowing this, we explain in Section 4 how to learn the characteristic properties for correspondence and continue to explain two new feature functions in Section 5. We give implementation details and experimental results in Section 6 and conclude in Section 7. The problem of establishing dense correspondence has been addressed in the domain of 2D images, on surfaces embedded in 3D space, and on volumetric data. In the image domain, correspondence from optical flow [2, 3] has been used to describe the transformations of faces with pose changes and facial expressions [4], and to describe the differences in the shapes of individual faces [5]. An algorithm for computing correspondence on parameterized 3D surfaces has been introduced for creating a class-specific representation of human faces [1] and bodies [6]. [7] propose a method that is designed to align three dimensional medical images using a mutual information criterion. Another interesting approach is [8]: they formulate the problem in a probabilistic setup and then apply standard graphical model inference algorithms to compute the correspondence. Their mesh based method uses a smoothness functional and features based on spin images. See the review [9] for an overview of a wide range of additional correspondence algorithms.
 Algorithms that are applied to 3D faces typically rely on surface parameterizations, such as cylin-drical coordinates, and then compute optical flow on the texture map as well as the depth image [1]. This algorithm yields plausible results, to which we will compare our method. However, the approach cannot be applied unless a parameterization is possible and the distortions are low on all elements of the object class. Even for faces this is a problem, for example around the ears, which makes a more general real 3D approach preferable. One such algorithm is presented in [10]: here, the surfaces are embedded into the surrounding space and a 3D volume deformation is computed. The use of the signed distance function as a guiding feature ensures correct surface to surface map-pings. We build on this approach that is more closely presented in Section 3.
 A common local geometric feature is surface curvature. Though implicit surface representations allow the extraction of such features [11], these differential geometric properties are inherently in-stable with respect to noise. [12] propose a related 3D geometric feature based on integrals and thus more stable to compute. We present a slightly modified version thereof which allows for a much easier computation of this feature from a signed distance function represented as a kernel expansion in comparison to a complete space voxelisation step required in [12]. In order to formalize our understanding of correspondence, let us assume that all the objects O of class O are embedded in X X  R 3 . Given a reference object O a correspondence can then be expressed as determining the deformation function  X  : X X  X  which maps each point x  X  X  on O We further assume that we can construct a dictionary of so-called feature functions f R , i = 1 , .., n capturing certain characteristic properties of the objects. [10] propose to use the signed distance function, which maps to each point x  X  X  the distance to the objects surface  X  with positive sign outside the shape and negative sign inside. They also use the first derivative of the signed distance function, which can be interpreted as the surface normal. In section Section 5 we will propose two additional features which are characteristic for 3D shapes, namely a curvature related feature and surface texture.
 We assume that the warp-invariant feature can be represented or at least approximated by an expan-sion in this dictionary. Let  X  : X X  R n be a weighting function describing the relative importance of the different elements of the dictionary at a given location in X . We then express the warp-invariant feature as f object specific; for the target object there is a slight modification in that the space-variant weighting  X  ( x ) needs to refer to the coordinates of the reference object if we want to avoid comparing apples and oranges. We thus use f t since we will only require f t To determine a mapping  X  which will establish correct correspondences between x and  X  ( x ) , we minimize the functional The first term expresses a prior belief in a smooth deformation. This is important in regions where the objects are not sufficiently characteristic to specify a good correspondence. As we will use a Support Vector framework to represent  X  , smoothness can readily be expressed as the RKHS norm k  X  k H of the non-parametric part of the deformation function  X  (see Section 6). The second term measures the local similarity of the warp-invariant feature function extracted on the reference object f r and on the target object f t and integrates it over the volume of interest.
 This formulation is a modification of [10] where two feature functions were chosen a priori (the signed distance and its derivative) and used instead of f morph, these functions should be reasonably invariant. In contrast, the present approach starts from the notion of invariance and estimates a location-dependent linear combination of feature functions with a maximal degree of invariance for correct correspondences (cf. next section). We consider location-dependent linear combinations since one cannot expect that all the feature functions that define correspondence are equally important for all points of an object. For example color may be more characteristic around the lips or the eyes than on the forehead.
 This comes at the cost, however, of increasing the number of free parameters, leading to potential difficulties when performing model selection. As discussed above, it is unclear how to characterize and evaluate correspondence in a principled way. The authors of [10] propose a strategy based on a two-way morph: they first compute a deformation from the reference object to the target, and af-terwards vice versa. A necessary condition for a correct morph is then that the concatenation of the quality criterion even when no ground truth is available, all model selection approaches based on such a criterion need to minimize (1) many times and the computation of a gradient with respect to the parameters is usually not possible. As the minimization is typically non-convex and rather expensive, the number of free parameters that can be optimized is small. For locally varying param-eters as proposed here such an approach is not practical. We thus propose to learn the parameters from examples using an invariance criterion proposed in the next section. We assume that a database of D objects that are already in correspondence is available. This could for example be achieved by manually picking many corresponding point pairs and training a regres-sion to map all the points onto each other, or by (semi-)automatic methods optimized for the given object class (e.g., [1]). We can then determine the optimal approximation of the warp-invariant feature function (as defined in the introduction) that characterizes correspondence using the basic features in our dictionary. The warp-invariant feature function should be such that it varies little or not at all for corresponding points, but its value should not be preserved (and have large variance) for random non-matching points. To approximate it, we propose to maximize the ratio of these variances over all weighting functions  X  . Thus for each point x  X  X  , we maximize Here, f r database object respectively.  X  random point sampled from it. We take the expectations over all objects in our database, as well as non corresponding points randomly sampled from the objects.
 Because of the linear dependence of f with the empirical covariances where we have drawn N random sample points from each object in the database.
 This problem is known as oriented PCA [13], and the maximizing vector  X  ( x ) can be determined by solving the generalized eigenvalue problem C eigenvector corresponding to the maximal eigenvalue  X  ( x ) , we obtain the optimal weight vector  X  ( x ) =  X   X  ( x ) v ( x ) using the scale factor  X   X  ( x ) = v ( x ) T C  X  ( x ) v ( x )  X  1 / 2 . Note that by using this scale factor  X   X  ( x ) , the contribution of the feature function f (1) will vary locally compared to the regularizer: as  X  ( x ) is somewhat arbitrary during the optimiza-tion of (1) the average local contribution will then approximately equal E  X  it will have a big influence in (1). If not, the smoothness term k  X  k implying that the local correspondence is mostly determined through more global contributions. Note, moreover that while we have described the above for the leading eigenvector only, nothing prevents us from computing several eigenvectors and stacking up the resulting warp-invariant feature functions f 1 then is plugged into the optimization problem (1) using the two norm to measure deviations instead of the squared distance. In our dictionary of basic feature functions we included the signed distance function and its deriva-tive. We added a curvature related feature, the  X  X igned balls X , and surface texture intensity. 5.1 Signed Balls Imagine a point x on a flat piece of a surface. Take a ball B and compute the average of the signed distance function s : X X  R over the ball X  X  volume: If the surface around x is flat on the scale of the ball, we obtain zero. At points where the surface is bent outwards this value is positive, at concave points it is negative. The normalization to the value Figure 1: The two figures on the left show the color-coded values of the  X  X igned balls X  feature at different radii R . Depending on R , the feature is sensitive to small-scale structures or large-scale structures only. Convex parts of the surface are assigned positive values (blue), concave parts negative (red). The three figures on the right show how the surface feature function that was trained with texture intensity extends off the surface (for clarity visualized in false colors) and becomes smoother. In the figure, the function is mapped on surfaces that are offset by 0, 5 and 15 mm. of the signed distance function at the center of the ball allows us to compute this feature function also for off-surface points, where the interpretation with respect to the other iso-surfaces does not change. Due to the integration, this feature is stable with respect to surface noise, while mean curvature in differential geometry may be affected significantly. Moreover, the integration involves a scale of the feature.
 We propose to represent the implicit surface function as in [10] where a compactly supported kernel expansion is trained to approximate the signed distance. In this case the integral and the kernel summation can be interchanged, so we only need to evaluate terms of the form R this basic integral only depends on the distance between the kernel center x It is compactly supported if the kernel k is. Therefore, we propose to pre-compute these values numerically for different distances and store them in a small lookup table. For the final expansion results with about ten to twenty distance values.
 For the case where the surface looks locally like a sphere it is easy to show that in the limit of small balls the value of the  X  X igned balls X  feature function is related to the differential geometric mean curvature H by I 5.2 Surface properties  X  Texture The volume deformation approach presented in Section 3 requires the use of feature functions de-fined on the whole domain X . In order to include information f |  X   X  of the object whose interior volume is  X  , e.g. the texture intensity, we propose to extended the surface feature f | closer to the surface. At larger distances from the surface, f should be smoother and tend towards the mean feature value. This is a desirable property during the optimization of (1) as it helps to avoid local minima. Finally, the feature function f and its gradient should be efficient to evaluate. We propose to use a multi-scale compactly supported kernel regression to determine f : at each scale, from coarse to fine, we select approximately equally spaced points on the surface at a distance related to the kernel width of that scale. Then we compute the feature value at these points averaged over a sphere of radius of the corresponding kernel support. With standard quadratic SVR regression we fit the remainder of what was achieved on larger scales to the training values. Due to the sub-sampling the kernel regressions do not contain too many kernel centers and the compact support of the kernel ensures sparse kernel matrices. Thus, efficient regression and evaluation is guaranteed. Because all kernel centers lie on the surface and reach to different extents into the volume X depending on the kernel size of their scale, we can model small-scale variations on the surface and close to it, whereas the regression function varies only on a larger scale further away from the surface. Implementation. In order to optimize (1) we followed the approach of [10]: we represent the deformation  X  as a multi-scale compactly supported kernel expansion, i.e., the j -th component, Figure 2: Locations that are marked yellow show an above threshold, relative contribution (see text) of a given feature in the warp-invariant feature function. C is the surface intensity feature, B the signed balls feature ( R = 6 mm ), N the surface normals in different directions. Note that points where color has a large contribution (yellow points in C) are clustered around regions with characteristic color information, such as the eyes or the mouth. function k : X X X X  R . The regularizer then is k  X  k 2 We approximate the integral in (1) by sampling N according to the measure  X  ( x ) and minimize the resulting non-linear optimization problem in the coefficients  X  j As a test object class we used 3D heads with known correspondence [1]. 100 heads were used for the training object database and 10 to test our correspondence algorithm. As a reference head we used the mean head of the database. The faces are all in correspondence, so we can just linearly average the vertex positions and the texture images. However, the correspondence of the objects in the database is only defined on the surface. In order to extend it to the off-surface points x generated these locations by first sampling points from the surface and then displacing them along their surface normals. This implied that we were able to identify the corresponding points also on other heads.
 For each kernel center x one run through the database we computed for each head the values of all proposed basic feature functions for all locations, corresponding to kernel centers on the reference head, as well as for 100 randomly sampled points z . The points z should be typical for possible target locations  X  ( x during the optimization of (1). Thus, we sampled points up to distances to the surface proportional to the kernel widths used for the deformation  X  . We then estimated the empirical covariance matrices n  X  n where n is the number of used basic features. The parameters C reg  X  one for each scale  X  were determined by optimizing computed deformation fields from the reference head to some of the training database heads. We minimized the mismatch to the correspondence given in the database. Feature functions. In Figure 1, our new feature functions are visualized on an example head. Each feature extracts specific plausible information, and the surface color can be extended off the surface. Learned weights. In Figure 2, we have marked those points on the surface where a given feature has a high relative contribution in the warp-invariant feature function. As a measure of contribution we took the component of the weight vector  X  ( x multiplied it with the standard deviation of this feature over all heads and all positions. Note that the weight vector is not invariant to rescaling the basic feature functions, unlike the proposed measure. Finally, we normalized the contributions of all features at a given point x the relative contribution. In the table below the relative contribution of each feature is listed. Here and below, S is signed distance, N surface normals, C the proposed surface feature function trained with the intensity values on the faces, and B is the  X  X igned balls X  feature with radii given by the percentage numbers scaled to the diameter of the head.
 The signed distance function is the best preserved feature (e.g. all surface points take the value zero up to small approximation errors). The resulting large weight of this feature is plausible as a surface-to-surface mapping is a necessary condition for a morph. However, combined with Figure 2 Figure 3: The average head of the database  X  the reference  X  is deformed to match four of the target heads of the test set. Correct correspondence deforms the shape of the reference head to the target face with the texture of the mean face well aligned to the shape details. we can see that the method assigns plausible non-zero values also to other features where these can be assumed to be most characteristic for a good correspondence.
 Correspondence. We applied our correspondence algorithm to compute the correspondence to the test set of 10 heads. Some example deformations are shown in Figure 3 for a dictionary consisting of S, N (hor, vert, depth) C, B (radii 3% and 8%). Numerical evaluation of the morphs is difficult. We compare our method with the results of the correspondence algorithm of [1] on points that are uniformly drawn form the surface (first column) and for 24 selected marker points (second column). These markers were placed at locations around the eyes or the mouth where correspondence can be assumed to be better defined than for example on the forehead. Still, the error made by humans when picking these positions has turned out to be around 1 X 2mm. The table below shows mean results in mm for different settings. If all weights are equal independent of location or feature (a), the result is not acceptable. A care-ful weighting of each feature separately, but independent of location (b)  X  as could potentially be achieved by [10]  X  improves the quality of the correspondence. To obtain these weights we av-eraged the covariance matrices C Section 4, but independent of x . However, a locally adapted weighting (c) outperforms the above methods and using more than one eigenvector (d-f) further enhances the correspondence. Note that although the results are not identical to [1], our algorithm X  X  accuracy is consistent with the human labeling on the scale of the latter X  X  presumed accuracy (1-2mm). For uniformly sampled points, the differences are slightly larger (4mm), but we need to bear in mind that that algorithm X  X  results cannot be considered ground truth. Experiment (g) which is identical to (c) but with the color and signed balls feature omitted demonstrates the usefulness of these additional basic feature functions. Computation times ranged between 5min and one hour and depended significantly on the number of scales used (here 4 ), the number of kernel centers generated, and the number of basic features included in the dictionary. For large radii R the signed balls feature becomes quite expensive to compute, since many summands of the signed distance function expansion have to be accumulated. Our method to select the important features for each point in advance, i.e. before the optimization is Figure 4: A morph between a human head and the head of the character Gollum (available from www.turbosquid.com ). As Gollum X  X  head falls out of our object class (human heads), we assisted the training procedure with 28 manually placed markers. started, would allow for a potentially high speed-up: At locations where a certain feature has a very low weight, we could just omit it in the evaluation of the cost function (1). We have proposed a new approach to the challenging problem of defining criteria that characterize criterion from examples of correct correspondences. The approach thus applies machine learning to computer graphics at the early level of feature construction. The learning technique has been implemented efficiently in a correspondence algorithm for textured surfaces.
 In the future, we plan to test our method with other object classes. Even though we have concentrated in our experiments on 3D surface data, the method may be applicable also in other fields such as to align CT or MR scans in medical imaging. It would also be intriguing to explore the question whether our paradigm of learning the features characterizing correspondences might reflect some of the cognitive processes that are involved when humans learn about similarities within object classes.
