 Cold start is one of the most challenging problems in recommender systems. In this paper we tackle the cold-start problem by propos-ing a context-aware semi-supervised co-training method named CSEL. Specifically, we use a factorization model to capture fine-grained user-item context. Then, in order to build a model that is able to boost the recommendation performance by leveraging the context, we propose a semi-supervised ensemble learning algo-rithm. The algorithm constructs different (weak) prediction mod-els using examples with different contexts and then employs the co-training strategy to allow each (weak) prediction model to learn from the other prediction models. The method has several distin-guished advantages over the standard recommendation methods for addressing the cold-start problem. First, it defines a fine-grained context that is more accurate for modeling the user-item preference. Second, the method can naturally support supervised learning and semi-supervised learning, which provides a flexible way to incor-porate the unlabeled data.

The proposed algorithms are evaluated on two real-world datasets. The experimental results show that with our method the recommendation accuracy is significantly improved compared to the standard algorithms and the cold-start problem is largely allevi-ated.
 H.4.m [ Information Systems Applications ]: Miscellaneous Cold-start; Recommendation; Semi-supervised Learning
Recommendation plays an important role in many fields and has attracted a lot of research interest. For example, Netflix has re-leased an interesting fact that about 75% of its subscribers watch are from recommendations. In a recommender system such as Net-flix and Amazon, users can browse items and choose those items they are interested in, while the system would also recommend to Figure 1: Average popularity and RMSE on items with differ-e nt popularity. The dataset is from MovieLens ( D  X  1 , Cf. Section 6.1 them the items that the system thought best match their preferences. Afterward, the user may provide feedback (such as rating, usually represented as a score between, for example, 1 and 5) on how the user thinks about an item after she/he has experienced the item. One important task for the recommendation engine is to understand users X  personalized preferences from their historic rating behaviors.
Another important, and actually more challenging task is how to improve the recommendation accuracy for the new (or rarely rated) items and the new (or inactive) users. Comparing to the pop-ular items, for the newly released ones and the old items that are rarely rated by users, it is difficult for the standard recommenda-tion approaches such as collaborative filtering approach to provide high-quality recommendations. Figure 1 shows some preliminary results in our experiments. The recommendation error (by mean-square error, i.e., RMSE) increases quickly with the decrease of popularity of the item. The average error of the most unpopular items ( Bin 10 ) almost doubles that of the popular items ( Bin Table 2 for details). The problem also exists for the newly entered users or the inactive users who have not contributed enough ratings. Technically, this problem is referred to as cold start . It is prevalent in almost all recommender systems, and most existing approaches suffer from it [22].

Despite that much research has been conducted in this field, the cold-start problem is far from solved. Schein [22] proposed a method by combining content and collaborative data under a single probabilistic framework. However, their method is based on Bayes classifier, which cannot accurately model the user X  X  fine-grained preference. Lin et al. [17] addressed the cold-start problem for App recommendation. They use the social information from Twitter to help App recommendation. The method is effective for dealing with the problem as it introduces external social information for building the recommendation model. However, if we deepen the analysis, we can easily find that for building a practical and accu-rate recommendation model, the available (useful) information is usually from different sources. A single model would be ineffec-tive in this sense. Thus the question is, how to build models using the information of different sources and, more importantly, how the different models can help each other? Besides, there are also some other challenges in the cold-start problem. For example, since the available labeled data from different sources is often limited, it is important to develop a semi-supervised learning method so as to leverage the unlabeled data.
 Solution and Contributions. In this paper, we aim to conduct a systematical investigation to answer the above questions. In or-der to capture users X  preferences, a fine-grained context-aware ap-proach is proposed which incorporates additional sources of infor-mation about the users and items rather than users X  rating informa-tion only. Furthermore, we propose a semi-supervised co-training method to build the recommendation model. The model not only is able to make use of unlabeled data to help learn the recommenda-tion model, but also has the capacity to build different sub models based on different views (contexts). The built sub models are then combined by an ensemble method. This significantly alleviates the cold-start problem.
 We evaluate the proposed model on a publicly available dataset, MovieLens. The results clearly demonstrate that our proposed method significantly outperforms alternative methods in solving the cold-start problem. The overall RSME is reduced by 3-4%, and for the cold-start users and items the prediction accuracy is improved up to 10% ( p &lt; 0 . 05 with t -test). Beyond accurate rec-ommendation performance, our method is also insensitive to pa-rameter tuning as confirmed in the sensitivity analysis. Finally, we use several case studies as the anecdotal evidence to further demon-strate the effectiveness of our method. To summarize, contributions of this work include: Organization. Section 2 formulates the problem; Section 3 presents a fine-grained user-item context model; Section 4 describes the proposed semi-supervised co-training algorithm (CSEL); Section 5 discusses related work; finally, Section 6 presents the experimental results and Section 7 concludes the work.
Here we present required definitions and formulate the cold-start problem in recommender systems.
 Let I be the set of items and U be the set of users in the system. We use r ui to denote a rating that user u  X  U gives to the item i  X  I , and use L = { r ui }  X  I  X  U to denote the set of all ratings. Further let U = { x ui } denote all the unknown ratings, i.e., U = I X U X  L . We use the notation | . | to denote the cardinality of a set, for example | L | indicates the number of ratings in the set L . In addition, each user/item may have some attributes. For example, a user may have gender, age and other attributes and an item may be associated with multiple genres. In this work gender, age and occupation are referred to as the context of users , whereas genres are referred to as the context of items . The goal of the cold-start recommendation is to leverage all the available information to learn a recommendation function f so that we can predict the rating of user u for item i , i.e., Here in the definition, if the rating r ui is already available, i.e., r ui  X  L , the learning objective is to minimize the difference of the to predict the most possible rating for x ui  X  U with high accuracy.
In the following, for an item i  X  X  let d i be i  X  X  popularity, which is obtained by d i = m i m , where m i is the number of users who have rated i and m is the number of users in the recommender system. For a user u  X  U , let d u be u  X  X  popularity, which is obtained by d u = n u n , where n u is the number of items rated by u and n is the total number of items.
To deal with a general recommendation problem, we can con-sider the standard factorization algorithm [15] as a baseline model. It is a model-based collaborative filtering (CF) algorithm proposed by the winning team in the Netflix competition 1 , which is designed for improving the prediction accuracy of the Netflix movie recom-mender system. This algorithm can be regarded as a state-of-the-art algorithm. The model is defined as
Here, the observed rating is factorized into four components: global average  X  , item bias b i , user bias b u , and user-item inter-action q T i p u that captures the user u  X  X  personalize preference on item i . By minimizing the difference of the real ratings and the factorized ratings in the labeled training data, we can estimate the is referred to as FactCF in the rest of the paper.

However, the standard CF approach cannot deal with the cold-start problem. As shown in Figure 1, the recommendation error increases quickly when directly applying the standard approach to unpopular items.

In this paper, we propose a novel semi-supervised co-training ap-proach, i.e., CSEL , to address the cold-start problem. Specifically, at the high-level, the approach consists of two stages: h ttp://www.netflixprize.com/leaderboard
Often in a recommender system, many users supply very few ratings, making it difficult to reach general conclusions on their taste. One way to alleviate this problem is to incorporate additional sources of information, for example, recommender systems can use the attributes of the users and items to build a context of the user preference. Following this thread, we propose a context-aware fac-torization model by leveraging  X  X ore general X  sources such as age and gender of a user, or the genres and tags of an item that are in-dependent of ratings. Saying that they are  X  X ore general X  than the ratings is in a sense that a rating is given by a specific user to a spe-cific item, whereas these sources can be learned from all the users or items that share the same category. In this section, based on the FactCF algorithm given in Eq. 1 we propose a context-aware model that incorporates the context information into the model.
Here we start enhancing the model by letting item biases share components for items linked by the genres. For example, items in a certain genre may be rated somewhat higher than the average. We therefore add shared bias parameters to different items with a common information. The expanded model is as follows  X  r ui =  X  + b u + b i + q T i p u + 1 | g enres ( i ) | then the total bias associated with an item i sums both its own specific bias modifier b i , together with the mean bias as-view these extensions as a gradual accumulation of the biases. For example, when modeling the bias of i , the start point is on the top of this start point. A similar method has been also used for music recommendations [7].

The second type of  X  X eneral X  information we incorporate into the model is the context (attributes) of users such as age and gender. For example, we can enhance Eq. 2 as follows:  X  r where b a , b o and b s are the biases associated with the user X  X  age, occupation and gender, respectively. Moreover, we propose mixing the user and item X  X  contexts as below to obtain a further optimiza-tion:  X  r where b ug , b ia , b io and b is are the parts mixing the contexts. Specifically, b ug is user u  X  X  bias in genre g , i.e., b preference in g . We can use a stochastic gradient descent (SGD) algorithm to learn the parameters. Compared to b g that is indepen-dent of specific users, here b ug is only updated when the current rating (in L ) used for training is given by u . Likewise, b b is are item i  X  X  preferences by the users in the categories of a, o and s , respectively, and they X  X l be updated in the learning process when the rating is associated with i and the users are in the corresponding categories.
 Model Learning. By combining all the context information, we can define an objective function to learn the context model by min-imizing the prediction errors over all the examples in L . where  X  r ui can be any model defined in Eqs. 2-4; || X || indicate the 2-norm of the model parameters and  X  is the regularization rate. The objective function can be solved by the stochastic gradient descent (SGD). SGD processes the training examples one-by-one, and up-date the model parameters corresponding to each example. More specifically, for training example r ui , SGD lowers the squared pre-diction error e 2 ui = ( r ui  X   X  r ui ) 2 by updating each individual pa-rameter  X  by where  X  is the learning rate. Thus, the parameters are updated by moving in the opposite direction of the gradient, yielding:
For each type of learned parameter we set a distinct learning rate and regularization rate. This grants us the flexibility to tune learn-ing rates such that, e.g., parameters that appear more often in a model are learned more slowly (and thus more accurately). Simi-larly, the various regularization coefficients allow assuming differ-ent scales for different types of parameters.

The model given by Eq. 4 turns out to provide a better perfor-mance than the other ones in terms of prediction accuracy therefore it is adopted in this work for the further optimization with semi-supervised co-training.
Based on the learned contexts, we propose a semi-supervised co-training (CSEL) framework to deal with the cold-start problem in recommender systems. CSEL aims to build a semi-supervised learning process by assembling two models generated with the above context-aware model, in order to provide more accurate pre-dictions. Specifically, CSEL consists of three major steps.
Algorithm 1 gives the algorithm framework. In the following more details will be given about the methods of constructing multi-ple regressors, constructing the teaching sets, and assembling. Note that in principle, the algorithm can be easily extended to accommo-date multiple regressors (more than two). In this work, for the sake of simplicity we focus on two regressors.
Generally speaking, in order to generate different learners for ensemble, one way is to train the models with different examples by manipulating the training set, while the other way is to build up different views by manipulating the attributes [6], which are both adopted in this paper to generate multiple regressors for co-training. Constructing Regressors by Manipulating the Training Set. A straightforward way of manipulating the training set is Bagging. In each run, Bagging presents the learning algorithm with a training set that consists of a sample of k training examples drawn randomly with replacement from the original training set. Such a training set is called a bootstrap replicate of the original training set.
In this work two subsets are generated with the Bagging method from the original training set, and two regressors are trained on the two different subsets, respectively, for the purpose of working collaboratively in the co-training process. The basic learner for generating the regressors could be the standard factorization model (Eq. 4), i.e., h 1 ( u, i ) = h 2 ( u, i ) =  X  + b u + b i + q T i p u
Some related work tries to use diverse regressors to reduce the negative influence of the newly labeled noisy data [30]. Our ex-periments also showed that a good diversity between the regressors indeed helps much in improving the performance of co-training. Thus it is an important condition for generating good combinations of regressors. Intuitively the diversity between the two regressors generated in this way are coming from the different examples they are trained on, and can be evaluated by their difference on pre-dictions. Moreover, another important condition to make a good ensemble is, the two regressors should be sufficient and redundant . Then being translated to this case, it means each training set should be sufficient for learning, respectively and the predictions made by the two individual regressors should be as accurate as possible.
However, the above two conditions, diversity and sufficiency, are contradictive to each other. That is, let the size of the two subsets generated with the Bagging method both be k , then with the orig-inal training set size fixed, when k is big, the overlap between the two subsets will be big as well, which will result in a small diversity between regressors. In the mean time the accuracy performance of individual regressors will be good with a big set of training exam-ples. And vice versa when k is small. Thus in the experiments we need to find an appropriate value of k for the trade-off between these two criteria. This method is referred to as M s hereafter. Constructing Regressors by Manipulating the Attributes. An-other way to generate multiple regressors is to divide attributes into multiple views , such as h ( u, i ) =  X  + b u + b i + q T i p u + 1 | g enres ( i ) | The two regressors are both a part of the model given in Eq. 4. Unlike M s , here the regressors are both trained on the entire train-ing set. To guarantee enough diversity between the constructed re-gressors, the contexts are separated, specifically, user-related con-texts and item-related contexts are given to the different regressors. Further, in order to keep a good performance in terms of prediction accuracy (with respect to the sufficient condition) for the individual regressors, the common part,  X  + b u + b i + q T i p u , is kept for both regressors. This method is referred to as M v hereafter. Constructing Regressors by a Hybrid Method. As described above, M s is to train a same model on different subsets whereas M v is to construct different models and let them be trained on a single set. The hybrid method is to construct different regressors as well as training them on the different subsets of the attributes. Obviously, it will bring in more diversity between the regressors. The hybrid method is referred to as M sv .
Now with the multiple regressors at hand, the task becomes how to co-train the different regressors. Specifically, we need to con-struct a  X  teaching set  X  T j for each regressor j and use it to teach its peer (the other regressors). This is a key step for launching the semi-supervised learning (SSL). One challenge here is how to de-termine the criteria for selecting unlabeled examples from U as to build the teaching set. The criteria we used in this work is the regressor X  X  confidence. Every candidate example has a specific confidence value to reflect the probability of its prediction to be ac-curate. As given in the algorithm, before SSL processes a smaller set U  X   X  U is drawn randomly from U because U could be huge. Confidence for the FactCF Model. In many cases the user can benefit from observing the confidence scores [11], e.g. when the system reports a low confidence in a recommended item, the user may tend to further research the item before making a decision. However, not every regression algorithm can generate a confidence by itself, like the factorization regressor this work is base d on. Thus it is needed to design a confidence for it.

In [30] the authors proposed the predictive confidence estima-tion for the kNN regressors. The idea is that the most confidently labeled example of a regressor should decrease most the error of the regressor on the labeled example set, if it is utilized. How-ever, it requires the example to be merged with the labeled data and re-train the model, and this process needs to be repeated for all candidate examples. Similarly, in [10] another confidence measure was proposed, again, for the kNN regressor. The idea is to calculate the conflicts level between the neighbors of the current item for the current user. However, the above two methods are both based on the k nearest neighbors of the current user and cannot be directly applied to our model. In the following we define a new confidence measure that suits our model.

Confidence in the recommendation can be defined as the sys-tem X  X  trust in its recommendations or predictions [11]. As we have noted above, collaborative filtering recommenders tend to improve their accuracy as the amount of data over items/users grows. In fact, how well the factorization regressor works directly depends on the number of labeled examples (ratings) that is associated with each factor in the model. And the factors in the model pile up together to have an aggregate impact on the final results. Based on this idea, we propose a simple but effective definition for the confidence on a prediction made by regressor j for example x ui : where N is the normalization term; d ( j ) u and d ( j ) i item i  X  X  popularities associated with regressor j .

The idea is, the more chances (times) a factor gets to be updated during the training process, the more confident it is with the pre-dictions being made with it. The rating number that is associated with the user-related factors b u and p u in the model is expressed by d , while the rating number that is associated with the item-related factors b i and q i is expressed by d i . Here the assumption is that the effects of the parameters are accumulative.
 Confidence for the Context-aware Models. With the similar idea the confidence for the context-aware models is defined as below: where c stands for the general information including genre, age, gender, occupation, etc.; d c represents the fraction of items that fall in a certain category c . For example, d g 1 is the fraction of items that fall in the genre  X  X ictions X , d g 2 is the fraction of items that fall in the genre  X  X ctions X , etc., with G = { g 1 , ..., g are 18 genres in the MovieLens dataset). Likewise, d a , d are associated with the categories of A = { a 1 , ..., a 7 range of age is divided into 7 intervals), S = { male, female } and O = { o 1 , ..., o 20 } (20 kinds of occupations), respectively, can be obtained in the same way.

Specifically, for the model with the item-view, i.e., h 1 the confidence for x ui is C 1 ( x ui ) = d (1) u  X  d (1)  X  d ber of genres i belongs to. On the other hand, for the model with the user-view, i.e., h 2 in Eq. 8, the confidence is C d u  X  d (2) i  X  d a  X  d s  X  d o . This way the confidence depends on the view applied by the model.

With this definition, the two regressors (given by Eq. 6) being trained on different sample sets (M s ) will naturally have different d of u being divided into the first sub-set, while its d (2) rating numbers of u being divided into the second sub-set, and d likewise. Therefore, it will result in two different confidence values for these two models.
 Note that in this work the confidence value of a prediction, C ( x ui ) , is for selecting examples within a single model j , in order to steer the process of semi-supervised learning. Hence they are not comparable between different models. For the same reason the normalization part N is a constant within a model and thus can be omitted.
 Constructing and Co-training with the Teaching Set. For each regressor j , when constructing the  X  X eaching set X  T j with C to avoid focusing on the examples that are associated with the most popular users and items that have relatively high d ( j ) rather than directly selecting the examples with the highest con-fidence, we obtain a probability for each candidate example based on C j ( x ui ) and select with the Roulette algorithm [1]. Specifically, the probability given to an example x ui is calculated by:
What X  X  noteworthy is, in the experiments we found that simply selecting examples with P r ( x ui , j ) sometimes cannot guarantee the performance. Moreover, we observed that a big diversity be-tween the outputs of the two regressors is important for the opti-mization. The experiments show that for the selected examples, if the difference between the predictions made for them is not big enough the teaching effect can be trivial. This is consistent with the conclusion in [20] where the authors emphasize the diversity between the two learners are important. To guarantee a progress in SSL we propose to apply an extra condition in the teaching set se-lection. That is, only those examples with a big difference between the predictions made by the two regressors are selected. Specifi-cally, when an example x ui is drawn from U  X  with P r ( x predictions made by the two regressors are compared to a threshold  X  . If the difference between them is bigger than  X  then x into T j , otherwise it is discarded.

Next, the examples in the teaching set are labeled with the pre-dictions made by the current regressor, likewise for its peer regres-sor. Note again that for the sake of diversity the teaching sets for the two regressors should be exclusive.

At last the regressors are updated (re-trained) with these new la-beled data. In this way the unlabeled data are incorporated into the learning process. According to the results given in the experiments, by incorporating the unlabeled data the CSEL strategy provides a significant boost on the system performance. This process of con-structing the teaching sets and co-training will be repeated for t iterations in the algorithm.
The final step of the algorithm is to assemble the results of the regressors after the optimization by SSL. A straightforward method is to take the average, i.e., where l is the number of regressors being assembled. In our case, to assemble results from two regressors it is h ( u, i ) = h ( u , i )] . However, this method does not consider the confidence of the different regressors. To deal with this problem, we assemble the results by a weighted vote of the individual regressors. Here the confidence can be used as the weights of assembling, i.e.,
Another option is to weight the regressors by  X  j that corre-sponds to regressor j  X  X  accuracy in the training set: h ( u, i ) = P j =1 ...l  X  j h j ( u, i ) . It is trained with the loss function ear regression. However, this method requires the model to be trained on all the labeled examples again and in the end it did not yield a better performance in our experiments. Some other options can also be adopted such as selecting the regressor h with the minimum errors in the training set, i.e., h ( u, i )  X  arg min j P r ple in L that is associated with u k and i k . This method also turned out to fail at providing the optimal results in our experiments.
Note that these ensemble methods can either be applied right af-ter the training of individual models without semi-supervised learn-ing, or after the semi-supervised learning. The former way makes use of the labeled data only, while the latter one makes use of both labeled and unlabeled data.
 Moreover, the ensemble results can be used in different ways. That is, in each iteration the ensemble of the two newest regressors X  predictions is obtained and used to label the examples in the teach-ing sets, rather than using their own predictions as described be-fore. The reason is, firstly, according to our experiments the ensem-ble predictions always performs better than the individual results, which promises a bigger improvement for the algorithm; Secondly, since selecting the examples with high confidences only means the current regressor is more confident about them than the other ex-amples, it does not say anything about its peer, which means if the original prediction is actually better noises will be brought into the peer regressors.
Cold-start is a problem common to most recommender systems due to the fact that users typically rate only a small proportion of the available items [21], and it is more extreme to those users or items newly added to the system since they may have no ratings at all [22]. A common solution for these problems is to fill the missing ratings with default values [5], such as the middle value of the rating range, and the average user or item rating. A more reliable approach is to use content information to fill out the miss-ing ratings [4, 18]. For instance, the missing ratings can be pro-vided by autonomous agents called filterbots [9], that act as or-dinary users of the system and rate items based on some specific characteristics of their content. The content similarity can also be used  X  X nstead of X  or  X  X n addition to X  rating correlation similarity to find the nearest-neighbors employed in the predictions [16, 23]. From a broader viewpoint, our problem is also related to the recom-mendation problem, which has been intensively studied in various situations such as collaborative recommendation [13] and cross-domain recommendation [26]. However, the cold-start problem is still largely unsolved.

Dimensionality reduction methods [2, 8, 14, 25] address the problems of limited coverage and sparsity by projecting users and items into a reduced latent space that captures their most salient fea-tures. There are also some more recent work that proposed some new methods to tackle the cold-start problem in different applica-tions such as [17, 19, 24, 29]. Work [19] tries to address the cold-start problem as a ranking task by proposing a pairwise preference regression model and thus minimizing the distance between the real rank of the items and the estimated one for each user. Work [29] follows the idea of progressively querying user responses through an initial interview process. [24] is based on an interview process, too, by proposing an algorithm that learns to conduct the interview process guided by a decision tree with multiple questions at each split. [17] aims to recommend apps to the Twitter users by apply-ing latent Dirichlet allocation to generate latent groups based on the users X  followers, in order to overcome the difficulty of cold-start app recommendation. Unlike the above work that addresses the cold-start problem in some specific applications with specific extra sources, our work possesses more generosity. The framework for the CSEL algorithm proposed in this work can basically ac-commodate any kind of regressors without any extra information required.

The semi-supervised learning algorithm adopted in this paper falls in the category of disagreement-based SSL [31]. This line of research started from Blum and Mitchell X  X  work [3]. Disagreement-based semi-supervised learning is an interesting paradigm, where multiple learners are trained for the task and the disagreements among the learners are exploited during the semi-supervised learn-ing process. However, little work has been done making use of the semi-supervised techniques in the literature of recommender systems although it is a natural way to solve these problems. Fol-lowing this line of research we propose a semi-supervised learn-ing algorithm specifically designed for the factorization regressors that outperforms the standard algorithm in both the overall system performance and providing high quality recommendations for the users and items that suffer from the cold-start problem. Datasets. We evaluate our methods on a public available dataset, MovieLens, consisting of 100 , 000 ratings (1-5) from 943 users on 1 , 682 movies. In the experiments, the algorithms are performed at two stages of the system. The first stage is the early stage, i.e., when the system has run for 3 month. 50 , 000 ratings are collected given by 489 users to 1 , 466 items. The second stage is the late stage with the entire dataset collected during 7 months. The datasets at these two stages are referred to as D 1 and D 2 , respectively.
The proposed methods are also evaluated on a different version of MovieLens dataset consisting of 1 , 000 , 000 ratings provided by 6 , 040 users for 3 , 900 movies. The experiments are performed on two stages of it as well. The first stage is when the system has run for 3 months, by this time 200 , 000 ratings are collected for 3 , 266 items given by 1429 users. The second stage is again the full dataset collected in 34 months. They are referred to as D 1 and D  X  2 for this larger version MovieLens, respectively. The reason we did not do the evaluations on the Netflix dataset is, it only provides the ratings without any attributes or context information about users or items, which makes it impossible to perform the context-aware algorithm and the CSEL method by manipulating the attributes (M v ) proposed in this work.

Besides the historical ratings MovieLens datasets also provides some context information, including 19/18 genres associated with items (movies) and age, occupation and gender associated with users. Gender is denoted by  X  X  X  for male and  X  X  X  for female, age is given from 7 ranges and there are 20 types of occupations.
In the evaluation, the available ratings were split into trai n, vali-dation and test sets such that 10% ratings of each user were placed in the test set, another 10% were used in the validation set, and the rest were placed in the training set. The validation dataset is used for early termination and for setting meta-parameters, then we fixed the meta-parameters and re-built our final model using both the train and validation sets. The results on the test set are reported. Comparison Algorithms. We compare the following algorithms.
Besides the state-of-the-art FactCF algorithm we also use an-other standard algorithm, k-NN, to serve as a baseline. Two kinds of k-NN algorithms are implemented and compared to our meth-ods. With user-based k-NN, to provide a prediction for x ui nearest neighbors of u who have also rated i are selected accord-ing to their similarities to u , and the prediction is made by taking the weighted average of their ratings. With item-based k-NN, the k nearest neighbors of i that have also been rated u are selected to form the prediction. Note that as another standard recommen-dation approach, the content-based algorithm has a different task from ours and thus not considered in this work. It aims to provide top-N recommendations with the best content similarities to the user, rather than providing predictions.

When training the models we mainly tuned the parameters manu-ally and sometimes resorted to an automatic parameter tuner (APT) to find the best constants (learning rates, regularization, and log ba-sis). Specifically, we were using APT2, which is described in [27]. All the algorithms are performed with 10-fold cross-validations. Evaluation Measures. The quality of the results is measured by the root mean squared error of the predictions: where r ui is the real rating and  X  r ui is the rating estimated by a recommendation model. The overall performance on the four datasets are given in Table 1. The CSEL methods outperform both k-NN and FactCF algorithms in terms of the overall RMSE. Specifically, compared to FactCF the prediction accuracy averaged on all the test examples is improved by 3.4% on D 1 , 3.6% on D 2 , 3.2% on D  X  1 and 3% on D  X  tively, with CSEL v performing the best among the methods. An Table 1: The overall RMSE Performance on the Four Datasets. unexpected result is that the hybrid method CSEL s v does not out-perform CSEL v . It is because the double splitting, i.e., on both the training set and views, leads to a bigger diversity (see Table 4) as well as worse accuracy for the regressors. As expected, FactCF out-performs k-NN on the overall performance by exploiting the latent factors. We tried different values of k from 10 to 100, k = 20 gives the best results. CSEL presents a much bigger advantage over the k-NN algorithms, i.e., 12% over UB and 16% over IB. The optimal solution is obtained by the CSEL v for all datasets.
 Cold-start Performance. Except for the overall RMSE perfor-mance, we also present the recommendation performance accord-ing to the popularity of items. Specifically, we estimate the pop-ularity of each item based on the number of ratings and partition all the items into 10 bins according to their popularity with equal number, the average RMSE scores are obtained for each bin. The results on these 10 bins over dataset D 2 are depicted in Table 2 corresponding to Bin (1) i through Bin (10) i . Likewise, all the users are partitioned into 10 bins, the average RMSE scores are reported for each bin, corresponding to Bin (1) u through Bin (10) ble. The average rating number in each bin is given, too. With the users and items being grouped by their popularity we can observe how well the cold-start problem is solved. For the presented results, the differences between the algorithms are statistically significant ( p &lt; 0 . 05 with t -test). Due to space limitation, we do not list re-sults on all the datasets, as according to the experimental results the behaviors of the algorithms on the four datasets ( D 1 , D D 2 ) turned out to be similar with each other.

Table 2 demonstrates how well the cold-start problem can be ad-dressed by the proposed CSEL algorithm. The performance of the k-NN algorithms shows again that the cold-start problem is com-mon to all kinds of recommendation algorithms, i.e., the items and users with less ratings receive less accurate predictions. The reason is that the effectiveness of the k-NN collaborative filtering recommendation algorithms depends on the availability of sets of users/items with similar preferences [12]. Compared to the baseline algorithms, CSEL successfully tackles the cold-start problem by largely improving the performance of the unpopular bins. Again, CSEL v provides the best results, compared to FactCF, RMSE drops up to 8.3% for the most unpopular user bins and 10.1% for the most unpopular item bins. Compared to UB k-NN, it drops up to 13.5% for the most unpopular user bins and 22.3% for the most unpopular item bins, respectively.
We further evaluate how the different components (context-aware factorization, ensemble methods, etc.) contribute in the proposed algorithm framework. In the following the performance of the proposed context-aware model and ensemble methods are given, and the results of CSEL are further analyzed. Table 2: RMSE Performance of Different Algorithms on D 2 . B in  X  the set of most active users; Bin (10) u  X  the set of most inactive users. Bin (1) i  X  the set of most popular items; Bin (10) Table 3: RMSE Performance of the evolving model. RMSE r educes while adding model components. # Models D 1 D 2 D  X  1 D  X  2 1)  X  + b u + b i + q T i p u 0.931 0.93 0.926 0.859 2 ) 1) + b i o + b ia + b is ( h v 1 ) 0.922 0.925 0.920 0.853 Context-aware Model. T able 3 depicts the results of the evolving model by adding on the biases of b i  X  and b u  X  separately. This way the effect of each part can be observed. Obviously, b ug contributes more than b i  X  for the final improvement in RMSE. The reason can be traced back again to the rating numbers falling into into each category. Specifically, the average rating number of items is 60 and the average rating number of users is 106. The b i  X  and b lated to the specific i and u now, which means these ratings are then divided into these categories, i.e., S, O, A and G . Roughly speak-ing, the ratings for each item are divided by 2 for S , 20 for O and 7 for A , respectively, whereas the ratings for each user are shared be-tween 18 genres but not exclusively, which leaves relatively richer information for b ug than b io , b ia and b is .

With the context-aware model, the overall RMSE is improved by 1.7% on D 1 , 1.3% on D 2 , 1.5% on D  X  1 and 1% on D  X  2 compared to FactCF. The performance on the cold-start problem is depicted in Table 2. There is a certain level improvement on all the bins comparing to the baseline algorithms.
 Ensemble Results. In Table 1 we give the ensemble results ob-tained by two approaches M s and M v , respectively. Let h h s 2 be the regressors generated by M s , h v 1 and h v 2 be the ones generated by M v . Their performance serves as a start point for the SSL process. The corresponding ensemble results are depicted as Ensemble s and Ensemble v , respectively. The ensemble meth-ods present a much better results in overall RMSE than the FactCF algorithm. Given the individual regressors h v 1 and h v 2 mance in Table 3, by simply assembling them together before any optimization with SSL, the accuracy has been largely improved.
Table 2 shows the performance on bins. Comparing to Context, with a slightly better performance on the overall RMSE, their per-formance on the unpopular bins are better. The biggest improve-ment is observed in the last bin, where the RMSE is reduced by 4.6% for users and 6% for items. The empirical results demon-strate that the ensemble methods can be used to address the cold-start problem. For a more theoretical analysis for the ensemble methods, please refer to [6].
 Semi-supervised Co-training Algorithm (CSEL). In order to generate the bootstrap duplicates with the M s method, we set a parameter 0  X   X   X  1 as the proportion to take from the origi-nal training set. In our evaluation, by trying different values  X  is set to 0.8, i.e., 0 . 8  X | T | examples are randomly selected for each duplicate, which provides the best performance in most cases.
As mentioned in Section 4.2, selecting the examples with a big difference between h 1 and h 2 to form the teaching set is essential to the success of the SSL process. In order to do so, a threshold  X  is set to select the examples with the top 10% biggest differ-ence. So, according to the distribution of the difference depicted in Figure 2 we have  X  = 0.675 for M s , 0.85 for M v and 1 for M spectively. As expected, M sv generates the most diverse regressors whereas M s generates the least. During the iterations the difference changes with the updated regressors. That is, if there is not enough unlabeled examples meet the condition the threshold will automat-ically shrink by  X  = 0 . 8  X   X  . The performance on solving the cold-start problem on D  X  2 is depicted by the comparison of the left and right figures in Figure 3, which presents the results on user bins of the FactCF method and the CSEL v method, respectively. The Figure 2: Difference Distribution between the Multiple Regr es-sors Generated for the CSEL Methods (on D 2 ). We estimate the difference between the predictions made by the multiple regressors generated with different methods, i.e., M s , M M sv . The threshold of difference is set according to the distri-butions for selecting the examples to form the teaching set. Figure 3: Comparison between the FactCF algorithm (left) and C SEL v (right) on the cold-start problem for D  X  2 . The average RMSE scores on user bins with different popularity are given. RMSE on the cold-start bins drops dramatically. The performance on those bins are much more closer to the performance on the pop-ular bins now. In other words, the distribution of RMSE becomes much more balanced than FactCF. Meanwhile, the performance on the popular bins has also been improved, which means the overall RMSE performance is generally improved. The improvement on the unpopular bins means the cold-start problem is well addressed in the sense that the predictions for the users and items that possess limited ratings become more accurate, which makes it possible to provide accurate recommendations for them.
 Efficiency Performance. Roughly speaking, the cost of the al-gorithm is t times of the standard factorization algorithm since the individual models are re-trained for t times. In the experiments we found that teaching set size N has a big influence on convergence. We tried many possible values of N from 1 , 000 up to 20 , 000 with respect to the optimization effect and time efficiency, and found that 20% of the training set size is a good choice, e.g., N = 8 , 000 for D 1 and N = 16 , 000 for D 2 , which leads to a fast convergence and optimal final results. Besides, we found the quality of the teaching set, i.e., difference and accuracy of the predictions made for the se-lected examples, has a even bigger impact on the performance and convergence of CSEL. With the methods described in Section 4.2 we managed to guarantee the best examples that satisfy the condi-tions are selected. The convergence point is set to when the RMSE in the validation set does drop any more for 2 iterations in a row. The entire training process is performed off-line.
 Table 4: Diversity of the Individual Regressors h 1 and h and Post-CSEL Methods.
 Table 5: Predictions made for the users and items with differe nt popularity.
 Diversity Analysis. T he diversity of the regressors pre-and post-performing the CSEL Methods is depicted in Table 4 for all the datasets under estimation. Here diversity is defined as the aver-age difference between the predictions made by the regressors. In the experiments we observed that the diversity drops rapidly when the accuracy goes up. At the point of convergence the regressors become very similar to each other and the ensemble of the results does not bring any more improvements at this point. As mentioned before diversity is an important condition to achieve a good perfor-mance. Apparently, simply distinguishing the teaching sets of the two regressors cannot help enough with maintaining the diversity and some strategies can be designed in this respect.

Moreover, when generating the multiple regressors with M v M s there is a setback on their performance, i.e., h 1 and h perform as good as the full model (Eq. 4) being trained on the full training set. For example, as shown in Table 3, h v 1 and h a setback in accuracy compared to the full model. Although by simply assembling the two regressors together the accuracy can be brought back (see Table 1), some improvement could still be done in this respect to obtain an even better initial point at the beginning of the SSL process.
To have a more intuitive sense about how the algorithms work in different cases, we pick two users and two items from the dataset and look at their predictions. Let u 1 be an active user and u an inactive user, i 1 be a popular item and i 2 be an unpopular item. Then four examples x 11 , x 12 , x 21 , x 22 are drawn from the dataset with the given users and items, where x kj is given by u k This way x 11 is rated by an active user to a popular item whereas x 22 is rated by an inactive user to an unpopular item, etc., and the performance of the algorithms on them can be observed. In this case study we pick the four examples that have the real ratings, r
The results are given in Table 5, showing that providing accurate predictions also helps with personalization. That is, to recommend items that the end-user likes, but that are not generally popular, which can be regarded as the users X  niche tastes. By providing more accurate predictions for these items, the examples such as x 1 2 or x 22 can be identified and recommended to the users who like them. Thus the users X  personalized tastes can be retrieved.
This paper resorts to the semi-supervised learning methods to solve the cold-start problem in recommender systems. Firstly, to compensate the lack of ratings, we propose to combine the contexts of users and items into the model. Secondly, a semi-supervised co-training framework is proposed to incorporate the unlabeled exam-ples. In order to perform the co-training process and let the models teach each other, a method of constructing the teaching sets is in-troduced. The empirical results show that our strategy improves the overall system performance and makes a significant progress in solving the cold-start problem.

As future work, there are many things worth to try. For example, our framework provides a possibility for accommodating any kinds of regressors. One interesting problem is how to choose the right regressors for a specific recommendation task. Meanwhile, this semi-supervised strategy can be easily expanded to include more than two regressors. For example, three regressors can be con-structed and a semi-supervised tri-training can then be performed on them. Finally, it is also intriguing to further consider rich social context [28] in the recommendation task.
 Acknowledgements. This work is partially supported by Na-tional Natural Science Foundation of China (61103078, 61170094), National Basic Research Program of China (2012CB316006, 2014CB340500), NSFC (61222212), and Fundamental Research Funds for the Central Universities in China and Shanghai Leading Academic Discipline Project (B114). We also thank Prof. Zhi-Hua Zhou for his valuable suggestions. [1] T. Back. Evolutionary Algorithms in Theory and Practice: [2] R. Bell, Y. Koren, and C. Volinsky. Modeling relationships at [3] A. Blum and T. Mitchell. Combining labeled and unlabeled [4] M. Degemmis, P. Lops, and G. Semeraro. A [5] M. Deshpande and G. Karypis. Item-based top-n [6] T. G. Dietterich. Ensemble methods in machine learning. In [7] G. Dror, N. Koenigstein, and Y. Koren. Yahoo! music [8] K. Goldberg, T. Roeder, D.Gupta, and C. Perkins.
 [9] N. Good, J. Schafer, and J. etc. Combining collaborative [10] G. Guo. Improving the performance of recommender [11] J. Herlocker, J. Konstan, and J. Riedl. Explaining [12] P. B. Kantor, F. Ricci, L. Rokach, and B. Shapira. [13] I. Konstas, V. Stathopoulos, and J. M. Jose. On social [14] Y. Koren. Factorization meets the neighborhood: a [15] Y. Koren. The bellkor solution to the netflix grand prize. [16] J. Li and O. Zaiane. Combining usage, content, and structure [17] J. Lin, K. Sugiyama, M.-Y. Kan, and T.-S. Chua. Addressing [18] P. Melville, R. Mooney, and R. Nagarajan. Content-boosted [19] S.-T. Park and W. Chu. Pairwise preference regression for [20] L. Rokach. Ensemble-based classifiers. Artificial Intelligence [21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Using [22] A. Schein, A. Popescul, L. Ungar, and D. Pennock. Methods [23] I. Soboroff and C. Nicholas. Combining content and [24] M. Sun, F. Li, J. Lee, K. Zhou, G. Lebanon, and H. Zha. [25] G. Takacs, I. Pilaszy, B. Nemeth, and D. Tikk. Scalable [26] J. Tang, S. Wu, J. Sun, and H. Su. Cross-domain [27] A. Toscher and M. Jahrer. The bigchaos solution to the [28] Z. Yang, K. Cai, J. Tang, L. Zhang, Z. Su, and J. Li. Social [29] K. Zhou, S.-H. Yang, and H. Zha. Functional matrix [30] Z.-H. Zhou and M. Li. Semi-supervised regression with [31] Z.-H. Zhou and M. Li. Semi-supervised learning by
