 In this supplementary material, we provide details about our method and its two variants: ( A ) variational The approximate posterior distributions of the dimensionality reduction part can be found as where the tilde notation denotes the posterior expectations as usual, i.e., g f (  X  )=E q (  X  ) [ f (  X  )]. The kernel-specific components have the following approximate posterior distribution: where the mean and covariance parameters are a  X  ected by the kernel weights, the composite components, and other kernel-specific components in addition to the projection matrix and the corresponding kernel matrix. The approximate posterior distributions of the multiple kernel learning part can be found as where the mean and covariance parameters of the kernel weights are calculated using the kernel-specific and composite components.
 The composite components have the following approximate posterior distribution: where it can be seen that the inference transfers information between the two domains. Note that the composite H z variables of domain Z are used when updating the random variables of the domain X . The approximate posterior distribution of the predicted outputs is a product of truncated normals: components. Fortunately, the truncated normal distribution has a closed-form formula for its expectation. (KBMF2K), for the case with a single kernel function for each domain. Figure 7 shows the graphical model of KBMF2K with latent variables and their corresponding priors. The distributional assumptions of the simplified model are dependence on  X  for clarity. We can write the factorized variational approximation as and define each factor in the ensemble just like its full conditional: We can bound the marginal likelihood using Jensen X  X  inequality: and optimize this bound by maximizing with respect to each factor separately until convergence. The approximate posterior distribution of a specific factor  X  can be found as The approximate posterior distributions of the ensemble can be found as We modify our proposed model for binary-valued outputs to also handle real-valued outputs. Figure 8 illustrates the graphical model of the modified kernelized Bayesian matrix factorization with twin multiple kernel learning (KBMF2MKL) with latent variables and their corresponding priors. The distributional assumptions of the modified KBMF2MKL model are As short-hand notations, all hyper-parameters in the model are denoted by  X  = {  X   X  ,  X  ,  X  , , g , h , y } , all prior variables by  X  = {  X  x ,  X  z ,  X  x ,  X  z } , and the remaining random variables by  X  = the factorized variational approximation as and define each factor in the ensemble just like its full conditional: We can bound the marginal likelihood using Jensen X  X  inequality: and optimize this bound by maximizing with respect to each factor separately until convergence. The approximate posterior distribution of a specific factor  X  can be found as The approximate posterior distributions of the ensemble can be found as
