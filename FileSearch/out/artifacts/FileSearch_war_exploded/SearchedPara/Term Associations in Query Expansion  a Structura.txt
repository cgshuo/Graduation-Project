 Many successful query expansion techniques ignore informa-tion about the term dependencies that exist within natural language. However, researchers have recently demonstrated that consistent and significant improvements in retrieval ef-fectiveness can be achieved by explicitly modelling term de-pendencies within the query expansion process. This has created an increased interest in dependency -based models.
State-of-the-art dependency-based approaches primarily model term associations known within structural linguistics as syntagmatic associations, which are formed when terms co-occur together more often than by chance. However, structural linguistics proposes that the meaning of a word is also dependent on its paradigmatic associations, which are formed between words that can substitute for each other without effecting the acceptability of a sentence. Given the reliance on word meanings when a user formulates their query, our approach takes the novel step of modelling both syntagmatic and paradigmatic associations within the query expansion process based on the (pseudo) relevant documents returned in web search. The results demonstrate that this approach can provide significant improvements in web re-trieval effectiveness when compared to a strong benchmark retrieval system.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Terms: Algorithms, Experimentation Keywords: Query Expansion, Term Dependencies  X  School of Information Systems, Queensland University of Technology, Brisbane, Qld, Australia, 4001  X  Australian e-Health Research Centre, CSIRO, Brisbane, Qld, Australia, 4001  X 
Department of Computer Science, Qld University of Tech-nology, Brisbane, Qld, Australia, 4001
Dependency-based models of information retrieval have demonstrated superior retrieval effectiveness over models that ignore term dependencies, like tf.idf and language modelling approaches [8, 9, 6, 2]. These approaches often use the in-tuition that terms that co-occur in context with the query terms within a document are likely to assist retrieval effec-tiveness. These dependencies are similar to those defined as syntagmatic associations used within structural linguistics.
Before Chomsky X  X  theories of generative grammar, linguis-tics was dominated by the structuralist theories of Ferdinand de Saussure (1916). Saussure proposed that the meaning of a word was created from its syntagmatic and paradig-matic associations. Syntagmatic associations are formed between words that co-occur above chance within natural language [7]. Typical examples include, sun -hot and coffee -taste . The association between two words is considered paradigmatic if they can substitute for one another in a sen-tence without effecting the acceptability of the sentence [7]. Typical examples include article -paper and dog -cat . These definitions indicate that syntagmatic and paradigmatic as-sociations can be modelled solely from occurrence patterns of words observed in natural language.

The motivation for modelling both syntagmatic and paradig-matic information within the information seeking process stems from the reliance on word meanings when a user for-mulates their query. Consider the example query ( best cof-fee machine ). The user X  X  information need may rely on associations to words like  X  lowest, price, tasting, espresso, maker  X . These associations can be argued to have syn-tagmatic: ( best -price ; tasting -coffee ; espresso -machine ); and paradigmatic: ( best -lowest ; coffee -espresso ; machine -maker ) associations with the original query terms.
Given state-of-the-art dependency-based approaches pri-marily model syntagmatic information and the reliance of the information seeking process on word meanings, it was hy-pothesised that: modelling both syntagmatic and paradig-matic associations in the information retrieval process would provide significant improvements in retrieval effectiveness. Preliminary evaluations testing this hypothesis incorporated an efficient, computational model of word meaning, known as the Tensor Encoding (TE) model [10], within the query expansion process. This approach, known as Tensor Query Expansion (TQE), demonstrated significant improvements in ad hoc retrieval effectiveness over the unigram relevance model, on small newswire collections [11].
However, our research aims to assess this approach on the TREC 2012 Web track (i.e., on a large web collection) compared to a much stronger benchmark system (based on the Google retrieval service). Dependency-based query expansion techniques, such as Latent Concept Expansion (LCE), are growing in popular-ity and have demonstrated superior effectiveness over those that ignore term dependencies [9, 5]. Most are based on the likelihood estimates of terms, or the co-occurrence in-formation of a possible expansion term with a query term, and hence within a (pseudo) relevance feedback setting can be argued to model syntagmatic associations [12]. However, paradigmatic information is modelled by looking at the vo-cabulary terms that co-occur often with a query term and the potential expansion term (i.e., not the co-occurrence be-tween the query term and potential expansion term itself).
The TE model allows the TQE approach to model paradig-matic associations between a sequence of terms Q = ( q 1 ,...,q and a vocabulary term w , using a novel estimation tech-nique: where f ij is the unordered co-occurrence frequency of terms i and j seen within a sliding context window moved across the set of (pseudo) relevant documents, V k is the vocabulary created from the set of k (pseudo) relevant documents), and Z par normalizes the distribution. The context window size is often set to 1, as this has been shown to effectively model paradigmatic associations [10].

In a (pseudo) relevance feedback setting, the Dirichlet smoothed likelihoods estimates of query terms within the (pseudo) relevant documents have been shown to efficiently and effectively model syntagmatic information [12], and hence was chosen as the syntagmatic measure in this work. This means that the TQE approach becomes a unigram relevance model when relying solely on the syntagmatic measure.
Past research, using primarily paradigmatic information sourced from WordNet 1 to expand query representations was unable to achieve consistent improvements in retrieval ef-fectiveness [13]. Corpus-based, query expansion techniques, that implicitly model syntagmatic and paradigmatic associa-tions have been presented in the past and have demonstrated significant improvements in retrieval effectiveness on small newswire collections [5, 1]. The features of the TQE ap-proach that separates it from previous corpus-based, query expansion techniques is (i) its ability to explicitly model and combine measures of syntagmatic and paradigmatic infor-mation within a single, formal framework, and (ii) its supe-rior efficiency.

Given the TQE approach has only been evaluated on small newswire data sets and against models based within the unigram language modelling framework, an evaluation on web-scale retrieval tasks compared to strong benchmark sys-tems is required before wider conclusions can be drawn. The TREC WebTrack provides such an opportunity. LCE, which is based on the MRF document ranking model is of-ten considered to be a strong benchmark model [5]. How-ever, the TREC forum allows systems to be compared (i.e.,
A hand-crafted ontology of English words grouped into cog-nitive synonyms, http://wordnet.princeton.edu/ not just models). Therefore, a stronger benchmark is possi-ble. Based on industry reputations, the Google web service is chosen to underpin our benchmark model. This choice is supported by our benchmark submission achieving an ERR=0.29 compared to an ERR=0.313 for the best TREC 2012 WebTrack submission [4]. The average of all TREC 2012 Web Track submissions was ERR=0.187. The benchmark model is created in the following way. The ClueWeb09-Category B documents are indexed using the  X  X ndexing without spam X  approach [14]. Each query is then issued to the Google retrieval service 2 and the top 60 retrieved documents are filtered using the spam filtered ClueWeb09 Category B index 3 . This filtered list is then padded, to create a ranked list of 10,000 documents, us-ing the ranked documents returned by a unigram language model on the spam filtered index. These rankings form our benchmark system ( GBline ) and this process is depicted in Figure 1.
To augment query representations within the TQE ap-proach, an estimate of the conditional probability P ( w | Q ), i.e. the probability of selecting a vocabulary term w as an expansion term given the query Q , is provided by: where w is any term in the TE vocabulary (formed from the set of k pseudo relevant documents returned by the benchmark model -GBline), Q is the sequence of original query terms, s par ( Q,w ) is the paradigmatic measure shown in Equation (1), s syn ( Q,w ) is the syntagmatic measure (i.e., the Dirichlet smoothed likelihood estimates),  X   X  [0 , 1] mixes the paradigmatic s par () and syntagmatic s syn () measures, and Z normalises the resulting distribution.
 From these estimates an augmented query representation Q 0 is created, as shown in Figure 2. In our study, this updated query representation is passed to a unigram lan-guage model to perform the final search on the spam filtered ClueWeb09 Category B index. A unigram language model was used as Google often treats long queries in a reductive approach, resulting in no documents matching the search. The system depicted in Figure 2 is referred to as GTQE in the remainder of this paper. The ClueWeb09 Category B dataset and the TREC Web Track 51-200 topics were used to evaluate the approaches presented in Section 3; collection statistics are reported in Table 1. The documents were stopped using the standard INQUIRY stop-word list and stemmed using a Krovetz stem-mer, as implemented by Indri toolkit 4 . Queries were formed from the title components of the TREC topics. http://www.google.com
We limited the number of documents retrieved with Google to 60 because of Google X  X  policies regarding the retrieval service at the time.
Available at http://sourceforge.net/projects/lemur Table 1: Collection statistics for the TREC ClueWeb09 Category B collection. | q | represents the average length of the queries, the value in brackets is the standard deviation of the query lengths, and | D | is the average document length.

Documents were indexed using the  X  X ndexing without spam X  method; the Waterloo spam list with threshold of 0.45 was used to estimate spam-likelihood of documents [14]. Index-ing and retrieval approaches were implemented using the Indri toolkit. The parameters used within the unigram lan-guage model were based on the Indri defaults.
Tuning of the GTQE system parameters was achieved by training on ERR@20 using the TREC Web topics from 2010 and 2011 (i.e., 51-150). The test runs were performed on the 2012 TREC Web track topics (151-200). The test parameter values used by the GTQE system were Number of feedback documents equal to 19, number of expansion terms equal to 14, and TE model mixing parameter (  X  in Equation (2)) equal to 0.1. The ERR@20 of the TQE system during train-ing (i.e., on topics 51-150) varied between 0.1201 and 0.1302 for (i) 5 to 25 expansion terms, and (ii) 4 to 30 feedback documents.
In this section we compare the results of the two retrieval systems (GBline and GTQE) on the task of ad hoc web retrieval. MAP, P@20, ERR@20 and nDCG@20 for the top ranked 10,000 documents for both system are reported in Table 2.
 GBline 0 . 305 0 . 117 0.290 0 . 167
GTQE 0.396 b 0.158 b 0.249 0.192 Table 2: Retrieval performance on the TREC 2012 Web Track ad hoc retrieval task. Superscript b indi-cates statistically significant differences (calculated using a paired t-test p &lt; 0 . 05 ) over the benchmark (GBline). The best result for each evaluation mea-sure appears in boldface. Brackets indicate the per-centage change from GBline to GTQE.

The results demonstrate that expanding query representa-tions using TQE can provide significant improvements over GBline on binary metrics (i.e. MAP and P@20). Binary metrics are those which use relevance judgements of 0 (non relevant) and 1 (relevant) for each document. However, no significant difference in retrieval effectiveness was noted on the graded metrics (ERR@20 and nDCG@20).

Graded metrics are those that base their effectiveness score on documents that are assigned a relevance judgement in a range, e.g., between 0 and 4. In addition, measures that use graded judgements, such as ERR, bias the scores for systems that return relevant documents toward the very top of the ranked list (i.e., in positions 1, 2 and 3). This causes a heavy discounting to occur for relevant documents ranked lower in the list, as seen from the expression used to calculate ERR at rank k [3]. Given Google rankings are likely augmented with click through data and editorial choice, the GBline sys-tem (Figure 1) is able to ensure highly relevant documents are ranked in the top few positions. However, as the GTQE system (Figure 2) performs its final ranking using a unigram language model, which does not use such information, it is not surprising that the GTQE system is unable to achieve significant improvements over GBline on the graded metrics (i.e., ERR@20 and nDCG@20).

Note that the GTQE system achieved significant improve-ments over GBline on the P@20 metric (Table 2), indicating that many more relevant documents were returned in the top 20 by GTQE than GBline. It may be therefore reason-able to deduce that significant improvements on ERR@20 and nDCG@20 may be achievable if a final re-ranking step, that took into account these graded relevance judgements, was added to the GTQE system (Figure 2).

To provide a comparison with an alternate query expan-sion approach, it is worth noting that our implementation of TQE becomes a unigram relevance model when  X  = 0 (i.e., uses the syntagmatic measure to produce estimates). The ef-fectiveness of TQE when  X  = 0 was reported as ERR=0.241, 3% less than TQE.
Robustness analysis includes considering the ranges of rel-ative increase/decrease in effectiveness and the number of queries that were improved/degraded, with respect to some baseline. Figure 3 illustrates the relative increase/decrease of P@20 scores for GBline and GTQE over the average of all TREC 2012 Web track submissions (MeanWT12) 5 . Figure 3: Robustness comparison of the GBline and GTQE systems when compared with MeanWT12.

Figure 3 shows that the GTQE system provides more con-sistent improvements over MeanWT12 than the GBline sys-tem -as indicated by the distribution of GTQE being posi-tioned more to the right than GBline.

This graph also highlights the adverse impact GTQE has on seven queries -as seen at the extreme left of the graph. Initial investigations indicate that this effect may be due to the very short nature of the effected queries, which have an average length of 1.8 (c.f., 2.7 for the test set), which may impact the effectiveness of modelling the word associations. More detailed investigations into this effect are left for future work.
Dependency-based models of information retrieval primar-ily use information about word associations known as syn-tagmatic associations. Within structural linguistics, word meanings are induced from syntagmatic and paradigmatic
P@20 was used as Section 4.3 suggests that a comparison on ERR@20 or nDCG@20 is unlikely to be meaningful. associations. Given the reliance on word meanings in the information seeking process it was hypothesised that mod-elling both syntagmatic and paradigmatic information within a dependency-based approach would provide significant im-provements in retrieval effectiveness.

The TQE approach provides a formal framework in which to achieve this. When the TQE approach is used to expand query representations on the TREC 2012 WebTrack signif-icant improvements in retrieval effectiveness are achieved when compared to a strong benchmark system created from the Google retrieval service. [1] J. Bai, J.-Y. Nie, G. Cao, and H. Bouchard. Using [2] C. Carpineto and G. Romano. A Survey of Automatic [3] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [4] C. Clarke, N. Craswell, and E. Voorhees. Overview of [5] Y. Hou, Z. Zhao, D. Song, and W. Li. Mining Pure [6] Y. Lv and C. Zhai. Positional Relevance Model for [7] J. Lyons. Introduction to Theoretical Linguistics . [8] D. Metzler and W. B. Croft. A Markov Random Field [9] D. Metzler and W. B. Croft. Latent Concept [10] M. Symonds, P. Bruza, L. Sitbon, and I. Turner. [11] M. Symonds, P. Bruza, L. Sitbon, and I. Turner. [12] M. Symonds, P. Bruza, G. Zuccon, L. Sitbon, and [13] E. M. Voorhees. Query Expansion Using [14] G. Zuccon, A. Nguyen, T. Leelanupab, and
