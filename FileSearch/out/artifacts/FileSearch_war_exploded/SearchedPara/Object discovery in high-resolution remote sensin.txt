 Dihua Guo  X  Hui Xiong  X  Vijayalakshmi Atluri  X  Nabil R. Adam Abstract Given its importance, the problem of object discovery in high-resolution remote-sensing (HRRS) imagery has received a lot of attention in the literature. Despite the vast amount of expert endeavor spent on this problem, more efforts have been expected to discover and utilize hidden semantics of images for object detection. To that end, in this paper, we address this problem from two semantic perspectives. First, we propose a semantic-aware two-stage image segmentation approach, which preserves the semantics of real-world objects during the segmentation process. Second, to better capture semantic features for object disco-very, we exploit a hyperclique pattern discovery method to find complex objects that consist of several co-existing individual objects that usually form a unique semantic concept. We consider the identified groups of co-existing objects as new feature sets and feed them into the learning model for better performance of image retrieval. Experiments with real-world datasets show that, with reliable segmentation and new semantic features as starting points, we can improve the performance of object discovery in terms of various external criteria. 1 Introduction With the advance of remote sensing technology and the increase of the public interest, the remote-sensing imagery has been drawing the attention of people beyond the traditional scien-tific user community. Large collections of high-resolution remote-sensing (HRRS) images are becoming available to the public, from satellite images to aerial photos. More than 10 different proposed Earth observing satellite systems are orbiting in space, such as IKONOS, QuickBird, and AISA, whose spatial resolutions are from 0.8 to 5 meters in the panchro-matic bands. Different agencies, such as U.S. Geographic Survey (USGS) and New Jersey Meadowlands Commission (NJMC), provide aerial photos with increasing resolutions from as early as 1980s. The global and repetitive nature of these HRRS systems opens up unlimited opportunities for new products and new information derivable from imagery.

To date, the remote sensing image databases are only for browsing and fetching images according to metadata rather than retrieving images according to users X  interest. Users are expected to have extensive domain knowledge. Therefore, the major user communities of remote-sensing images are military and the Earth scientists To make the HRRS image retrieval more user friendly, we have to allow the users to access image databases through semantic-based queries in addition to metadata-based queries. However, it remains a challenging task to identify semantic-objects in HRRS images. While HRRS images share some common features with traditional images, they possess some special characteristics that make object discovery more complex. This motivates our research work.
 Motivating examples. Users are interested in different types of objects on the Earth as well as groups of objects with various spatial relationships. For example, let us consider Emergency Response Officers who are trying to find shelters to accommodate a large number of people. However, shelters are not distinguishable in Remote Sensing (RS) images. Instead, the officers could search for baseball fields, because most probably, a baseball field is connected to a school and the school could be used as a temporary shelter in emergency. In addition, qualified shelters should not be far away from water sources. Therefore, a query might be  X  select all the baseball fields in Newark within 1 mile from any water body  X . Another interesting application domain would be urban planning. With HRRS image retrieval, we may have the task to find out  X  the disinvestment area in the industrial area of Hudson county  X . This task indicates that we need to identify the industrial areas with a lot of empty lots. While traditional Content Based Image Retrieval (CBIR) techniques discover objects such as buildings and water bodies, these two examples demonstrate that one needs to discover semantic objects such as schools and urban areas from RS or HRRS images.

Based on the above observation, we categorize the target objects that can be recognized in RS or HRRS images into three levels of concepts: (1) Basic Terrain Types; (2) Individual Objects; and (3) Composite Objects. The first concept level is to distinguish the basic terrain type of the area covered by the images. There are several basic ground layouts: bare land, mountain, water, residential area, forest, etc. The second type of objects includes individual objects that are recognizable in images, such as individual buildings, road segments, road intersections, cars, etc. Objects in the third concept level are composite objects that consist of several individual objects that form a new semantics concept. If we organize and represent all the information, we can retrieve objects from HRRS images, as shown in Fig. 1 .
While some promising work has been developed by researchers from different aspects [ 1  X  6 ], our research focus is on identifying semantic-objects is composed of individual objects that form a new semantic concept. To fulfill our goal, we need to investigate the following two issues that specifically relate to HRRS images:  X  How can we segment HRRS images appropriately?  X  Can we identify semantic-objects by selecting semantic features?
Along this line, this paper addresses the problem of automatically annotating images using a relevance-based statistical model on HRRS images. Specifically, to answer the questions listed above, we propose a two-stage object-based semantic image segmentation approach that extends the hierarchical HRRS image segmentation algorithm [ 7 ], as well as exploits the hyperclique pattern discovery method [ 8 ] to create new semantic features and feeds them into the relevance-based statistical learning model. The extended two-stage semantic seg-mentation approach avoids a general pitfall for image segmentation: using a single algorithm to solve the universal problem. The proposed approach takes the image semantics into consi-deration and segments images accordingly. In addition, hyperclique patterns have the ability to capture a strong connection between the overall similarity of a set of objects and can be naturally extended to identify co-existing objects in HRRS images. Traditionally, by using a training set of annotated images, the relevance-model [ 9 ] can learn the joint distribution of the blobs and words. Here, the blobs are image segments acquired directly from the image segmentation procedure. Our approach extends the meaning of blobs by identifying the co-existing objects/segments as new blobs. The proposed approach has been tested using the USGIS high-resolution orthology aerial images. Our experimental results show that, with reliable segmentation and new semantic features as starting points, the performance of learning model can be improved according to several external criteria.
 Overview. The remainder of this paper is organized as follows: in Sect. 2 ,wedescribethe problems and challenges for object discovery in HRRS imagery. Section 3 reviews previous related research work. In Sect. 4 , we introduce a two-stage image segmentation method and propose a semantic feature selection method. Section 5 provides experimental results. Finally, we draw conclusions and suggest future work in Sect. 6 . 2 Domain challenges In this section, we identify some domain challenges for object discovery in HRRS images. Foremost, we should investigate HRRS data. We notice that, in addition to the challenges faced by the scenery digital image understanding systems, HRRS images have some spe-cial characteristics, which pose additional challenges to image object discovery. These are discussed below.  X  HRRS imagery is big. Here the word  X  X ig X  has a two-fold meaning. First, every image  X  HRRS images contain tremendous details compared to low-resolution RS images. This  X  HRRS images often contain heavy salt-and-pepper noises. Indeed, existing object dis- X  Spatial relationships among objects are critical to the semantics of HRRS images. The-Based on the above characteristics of HRRS images, we summarize the domain challenges for object discovery in HRRS images as follows.  X  First, it is nontrivial to do feature selection for image retrieval in HRRS images. In [ 6 ],  X  Also, HRRS images usually lack salient regions and carry a lot of noise [ 7 ]. This problem  X  Finally, another challenge faced by the HRRS image annotation is the importance of 3 Preliminary and related work In this section, we describe some basic concepts related to image retrieval and present the related work. 3.1 Three query levels Accessing a desired image from a repository might involve a search for images that depict specific types of objects or scenes, evoke a particular mood, or simply contain a specific texture or pattern. Generally speaking, the user query could be as the following:  X  The presence of a particular combination of color, texture or shape features (e.g. red cross)  X  The presence or arrangement of a specific type of objects (e.g. a vase on a table)  X  The depiction of a particular type of events (e.g. a baseball game)  X  The presence of a named individual, location, or event (e.g. the Empire State Building)  X  A subjective emotion expressed by an image (e.g. leisure) According to [ 10 ], a query can be categorized into three levels with increasing complexity.
The first level comprises of retrieval by primitive features such as color, texture and shape or the spatial location of image elements. Examples of such queries might include  X  X ind pictures with round yellow objects on top X  and  X  X ind images with two objects next to each other X . This level of retrieval uses features that are both objective and directly derivable from the images themselves, without the need to refer to any external knowledge bases. The first level has severe limitations, because it is hard to ask the common users to specify the low level features as a specialist.

The second level comprises of retrieval by derived features, involving some degree of semantic inference about the image. To answer queries at this level, some outside knowledge is required.

The third level of queries refers to image retrieval by abstract attributes. This involves a significant amount of high-level reasoning about the meaning and purpose of the objects or scenes depicted. We can divide these queries into two sub-directories. One is retrieval of named events or types of activity, such as  X  X ind the image of a Jazz Performance X . Another is retrieval of images with emotional or religious significance, such as  X  X ind an image depicting anxiety X . To achieve this level of searching, complex reasoning and subjective judgment may be required.
 Most of the pioneer content-based image retrieval systems, such as Query by Image Content(QBIC), VisualSEEk and Photobook, do a very good job in retrieving and ana-second level, not to mention satisfying the third level queries. To bridge the gap between the low-concept-level features and the semantic meanings of the images is the main purpose of this research. 3.2 High-resolution remote sensing image retrieval Three major research fields are related to HRRS image retrieval:  X  In photogrammetry community, most of the research has been focusing on individual  X  Remote-sensing image retrieval approach focuses on terrain type recognition [ 16  X  18 ].  X  There is other research that concentrates on problems of semantic-based image retrieval. 3.3 A multi-level image retrieval model Most of the image retrieval approaches use multilevel abstraction mechanisms to support content-based image retrieval. As depicted in Fig. 2 , the top three levels, which correspond to the three user query levels, are the three key functionalities as well: feature extraction, object recognition, and domain-specific spatial reasoning and semantic modeling. This figure provides the essentials of content-based image retrieval.
 statistical relations between visual features and keywords. These methods can discover some hidden semantics of images. However, these methods annotate scenery images according to the presence of individual objects in each image. Spatial relations among objects are not taken into consideration.

The co-occurance model [ 5 ] investigates the co-occurrence of words with image regions created using a regular grid. Each divided part inherits all words from its original image. Mori et al. [ 5 ] makes clusters from all divided images using vector quantization, accumulates the frequencies of words of all partial images in each cluster, and then calculates the likelihood for every word. It tends to require large number of training samples to estimate the correct probability and also tends to map frequent words to every blob. Also, Zhou et al. [ 20 ] proposed an elegant MIMLSVM algorithm for image classification. In this work, each image is also represented by a number of blobs which are clustered into representative patterns for images.
Under keyblock model [ 21 ], images are partitioned into equal-sized blocks and then each image is represented as a code matrix in which the elements are the indices of the keyblocks in a codebook. Based on this image representation, information retrieval and database analysis techniques developed in the text domain are transformed to image retrieval. The codebook, using keyblocks as vocabulary, is generated by applying generalized Lloyd algorithm (GLA) and pair wise nearest neighbor algorithm (PNNA) to the segmented grids in a training set. For each image in the database as well as the query, decompose it into block. Then, for each of the blocks, find the closest entry in the codebook and store the index correspondingly. Each image is then a matrix of indices, which can be treated as one-dimensional codes of the keyblocks in the codebook. After converting image features into properties that are similar to text document, they build the n -block model after the n -gram model used for statistical language modeling. This seems to be an application independent approach. However, this approach has an underlying assumption, which assume that objects in the image are usually salient enough so that they occupy more than one blocks. Otherwise, the segmented blocks will not be significant enough to be identified as keyblock. Moreover, all the above approaches do not take the context of image regions into consideration. Also, this approach did not use any textual annotation for images. For each keyblock, the semantic meaning of block is not considered either.

Duygulu et al. [ 1 ], Feng et al. [ 2 ], and Wang et al. [ 6 ], all tried to map between segmented regions or grids and keywords supplied with the images using a method based around statis-tical algorithm. The variations are the details of the models. For example, Multiple Bernoulli relevance model [ 2 ] make sure that the annotating keywords are independent and not affected by other words. Also, in [ 6 ], researchers assign weight for different features based on their histogram distribution. 3.4 Representing spatial relationships As presented in Sect. 1 , our goal is to discover composite objects. For example, parks, airports, and baseball fields are all composite objects. In the motivating examples, both shelter and disinvestment area are composite objects. As one can notice, the spatial relationships among objects play a critical role in identifying composite objects and interpreting the semantics of HRRS images.

Those spatial relationships are critical and cannot be ignored in HRRS images. Hence, in HRRS images, users pay more attention on composite objects than on individual objects. This suggests that we have to examine the spatial relationships among objects when we try to identify objects in HRRS images.

General approaches for modeling spatial semantics are based on identifying spatial rela-tionships among objects once they are recognized and marked by the lower layer using boun-ding boxes or volumes. Several techniques have been proposed to formally represent spatial knowledge at this layer. These include: semantic networks, mathematical logic, constraints, inclusion hierarchies, and frames [ 22 ]. Semantic networks is a graph-based approach to represent spatial concepts and relationships. Constraints-based methodology uses constraints, which are the relationships between two or more objects need to satisfy in knowledge base. Mathematical logic projects the spatial relationships in the form of 2D strings to form a partial ordering of object projections in 2D. Another category of spatial representation is inclusion hierarchies which group together semantically related objects. Frames are used to represent knowledge relevant to particular objects, situations, or concepts. A frame usually consists of a name and a list of attribute-value pairs. However, none of the above approaches can be integrated with CBIR systems. Most of the CBIR systems need quantified features as input. How to integrate these representations with other quantitative input is a research challenge.

In HRRS image retrieval, the observed domain knowledge regarding spatial relationships is that the topological relations [ 23 ], such as disjoint, meet, overlap, contains, covers, inside, etc. , can be simplified as exist and not-exist in the definition of some composite objects. For example, with the repetitive appearance of single house roof, we can roughly conclude that the image capture the ground truth of the residential area. If we utilize this co-existence pattern, next time when we observe a large number of similar unlabeled rectangular shaped objects in an image, it will have a high probability that the image covers a residential area. Another example is that the co-existence of parking lot and large building roofs implies that it could be a shopping mall. It is not important to know whether the parking lot is left or right to the roof. This observation encouraged us to explore new approach to represent the spatial relationships among individual objects. 4 Object discovery with semantic feature selection In this section, we introduce a method for O bject dis C overy with semanti C feat U re s E lection (OCCUE). Figure 3 shows an overview of the OCCUE method. A detailed discussion of each step of OCCUE is given in the following subsections. 4.1 Two-stage object-based semantic image segmentation As the first step of the OCCUE method, image segmentation divides an image into separated regions. In a large-scale HRRS image database, the images naturally belong to different semantic clusters. For example, most of HRRS images can be categorized into four main semantic clusters at the land cover level including grass, water, residence and agriculture [ 24 ]. These land-cover level semantic clusters can also be divided into semantic subclusters at an object level. For these subclusters, the distinguishing primitive features are different. Moreover, the objects in each land-cover cluster are very different. For example, the objects in urban areas are usually road segments, single house roofs, or small vegetated areas. In contrast, woods and grass are dominant in suburban areas. Likewise, different composite objects also appear in different land-cover clusters. For instance, a park is always a large contiguous vegetated area. This different scale distinguishes parks from gardens.
Given the special characteristics of HRRS imagery, in this paper, we propose a two-stage object-based approach to increase segmentation reliability as well as take the image semantics into consideration. In the first stage, we segment the HRSS images reso-lution according to their land cover. In the second stage, we segment labeled images under different land cover using different parameters. The outcomes of the second step are indi-vidual objects or components of an individual object. Our segmentation approach satisfies the uniqueness of RS images by considering and preserving the differences of images with different semantic context. Another major advantage of using proposed approach is that this segmentation approach can reflect the hierarchies that exist in the structure of the real-world objects which we are detecting. By abstracting houses, buildings, roads and other objects, people can identify residential areas and the aggregation of several residential areas yields a town.

Figure 4 illustrates the image process procedure which each image has to go through. To segment image at the land cover level, the segmentation method consists of three steps [ 7 ]: 1. Hierarchical splitting. Algorithm 1 shows the process of hierarchical splitting, which 2. Optimizing. This step adjusts the splitting result, if the results of the reduced resolution 3. Merging. Algorithm 2 shows the merging step, in which the adjacent blocks with similar
After the land-cover level segmentation, images are segmented into small regions using eCognition along with different input parameters according to land-cover type [ 25 ]. For example, if the land-cover of an image is categorized as woods, shape features should have much less weight compare to images of residence land-cover. Each segment is represented by the traditional features, e.g. colors, textures and shapes, as well as the geometric features. eCognition utilizes a bottom-up-region-merging technique starting with one-pixel. In sub-sequent steps, smaller image segments are merged into bigger ones [ 25 ]. We believe that this is one of the easy-to-use and reliable segmentation tools for HRRS images, given the Algorithm 1 Hierarchical Splitting characteristics of the HRRS images: (1) with salt and pepper noises; (2) affected by the atmosphere and the reflective conditions.

The following extracted features represent major visual properties of each image segment.  X  Layer values are features concerning the pixel channel values of an image segment, mainly  X  Shape features include area (measured by pixel), length/width ratio which is the eigenva- X  Texture features evaluate the texture of an image segment based on the gray level Algorithm 2 Merging  X  Position features refer to the positions of segments within an image. 4.2 Fuzzy classification After we segment the images into relatively homogeneous regions, the next step is to group similar image segments into a reasonable number of classes, referred as blob tokens in [ 6 ]. Segments in each class are similar even though they are not spatially connected. In the lite-rature [ 6 ], unsupervised classification algorithms is employed using the primitive features or weighted features. Using the weighted features would successfully reduce the dimensionality compared with using all primitive features as clustering algorithm input. However, we used supervised classification method that is efficient in grouping image segments into semantic meaningful blobs.

Specifically, fuzzy logic based supervised classification is applied to generate blobs. Star-ting with an empty class hierarchy, we manually insert sample classes and using the features description as definition of a certain class. While nearest neighbor and membership functions are used to translate feature values of arbitrary range into a value between 0 (no membership) and 1 (full membership), logical operators summarize these return values under an overall class evaluation value between 0 and 1. The advantages of fuzzy classification are [ 25 ]  X  Translating feature values into fuzzy values standardizes features and allows to combine  X  It enables the formulation of complex feature descriptions by means of logical operations
Finally, fuzzy classification also helps to merge the neighboring segments that belong to the same class and get a new semantic meaningful image blob which truly represents the feature and not just a part of it. 4.3 Hyperclique patterns As we argued in the previous section, the next challenge is to find the co-existence objects.
In this paper, hyperclique patterns [ 8 ] are what we used for capturing co-existence of spatial objects. The concept of hyperclique patterns is based on frequent itemsets.
Association rule mining can take two approaches in mining images [ 26 , 27 ]. The first one involves mining image visual features only while the second one involves mining both visual and textual data. In this research, we take the first approach.

In this subsection, we first briefly review the concepts on frequent itemsets, then describe the concept of hyperclique patterns.

Let I ={ i 1 , i 2 ,..., i m } be a set of items. Each transaction T in database D is a subset of I . We call X  X  I an itemset. The support of Xsupp ( X ) is the fraction of transactions containing X .If supp ( X ) is no less than a user-specified minimum support, X is called a frequent itemset. The confidence of association rule X 1  X  X 2 is defined as con f ( X 1  X  X ) = supp ( X 1  X  X 2 )/ supp ( X 1 ) . It estimates the likelihood that the presence of a subset X 1  X  X implies the presence of the other items X 2 = X  X  X 1 .

If the minimum support threshold is low, we may extract too many spurious patterns minimum support threshold is high, we may miss many interesting patterns occurring at low levels of support, such as (caviar, vodka). To measure the overall affinity among items within an itemset, the h -confidence was proposed in [ 8 ]. Formally, the h -confidence of an itemset P ={ i 1 , i 2 ,..., i m } is defined as hcon f ( P ) = min k { con f ( i k  X  P  X  i k ) } .Givenaset of items I and a minimum h -confidence threshold h c , an itemset P  X  I is a hyperclique pattern if and only if hcon f ( P )  X  h c . A hyperclique pattern P can be interpreted as that the presence of any item i  X  P in a transaction implies the presence of all other items P  X  X  i } in the same transaction with probability at least h c . This suggests that h -confidence is useful for capturing patterns containing items which are strongly related with each other. A hyperclique pattern is a maximal hyperclique pattern if no superset of this pattern is a hyperclique pattern. 4.4 Converting spatial relationship into feature representation Approaches for modeling spatial relationships can be grouped into three categories: graph-based approaches, rule based approaches, and mathematical logic using 2D strings as the projections of the spatial relationships. However, none of this can be used as input for statistical Cross Relevance Model (CRM). In addition, we concentrate on the presence of the objects in the image rather than the complex geometric or topological spatial relationships. For example, consider a golf course, we are interested in the appearance of the well textured grassland, sand, non-rectangle water-body in a relatively small region. Whether the sand is left or right to the water-body is not important. In OCCUE, we apply hyperclique pattern discovery algorithm [ 8 ] to detect co-existing objects.
 Example 2.1 After segmentation, images are represented by the blob ID as shown in #7 shade type II , #24 grass type IV) usually appears together. We have supp ( b 3 ) = 82 and con f ( b 7  X  b 3 , b 24 ), con f ( b 24  X  b 3 , b 7 )) = 60%. According to the definition of hyper-we treat the set of these three blobs as a new semantic feature. We treated these newly dis-covered hyperclique pattern as new blobs in additional to the existing blobs. Meanwhile, the original blobs #3, #7, and #24 are deleted from the original table. Table 1 will be converted to Table 2 . The new blobs are represented using 3 digits number in order to distinguish from the original blobs. We convert the spatial relationship into a measurable representation, so that we can apply statistical model in the next step. 4.5 A model of image annotation Suppose we are given an un-annotated image in image collection I  X  C .Wehavetheobject representation of that image I ={ o 1 ... o m } , and want to automatically select a set of words { w 1 ...w n } that reflect the content of the image.

The general approach is widely accepted by statistical modeling approach. Assume that for each image I there exists some underlying probability distribution P (  X | I ) for all words. We refer to this distribution as the relevance model of I [ 28 , 29 ]. The relevance model can be thought of as an urn that contains all possible objects that could appear in image I as well as all words that could appear in the annotation of I . We assume that the observed image representation { o 1 ... o m } is the result of m random samples from P (  X | I ) . Equation ( 1 ) explains that the probability of any given word w appears in image I can be represented by the probability of word w appears given the blobs that image I contains.
In order to annotate an image with the top relevance words, we need to know the probability of observing any given word w when sampling from P (  X | I ) .Therefore,weneedtoestimatethe probability P (w | I ) for every word w in the vocabulary. Given that P (  X | I ) itself is unknown, the probability of drawing the word w can be approximated by training set T of annotated images, as shown in Eq. ( 2 ), in which J represents every image in training set T . Assuming that observing w and blobs are mutually independent for any given image, and identically distributed according to the underlying distribution P (  X | J ) . This assumption gua-rantees we can rewrite equation ( 2 ) as follows: We assume the prior probability P ( J ) follows uniform over all images in training set T . We follow [ 9 ] and use smoothed maximum likelihood estimates for the probabilities in Eq. ( 3 ). The estimations of the probabilities of blob and word given image J are obtained by: Here, Num (w, J ) and Num ( o , J ) represents the actual number of times the word w or blob o occurs in the annotation of image J . Num (w, T ) and Num ( o , T ) is the total number of times w or o occurs in all annotation in the training set T . | J | denotes for the aggregate count of all words and blobs appearing in image J, and | T | denotes the total size of the training set. The smoothing parameter  X  J and  X  J determine the interpolation degree between the maximum likelihood estimates and the background probabilities. Due to the different occurrence patterns between words (Zipfian distribution) and blobs (uniform distribution) in images, we separate the two smoothing parameter as  X  J and  X  J .
 Finally, Eqs. ( 1 ) X ( 5 ) provide the mechanism for approximating the probability distribution P (w | I ) for an underlying image I. We annotate images by first estimating the probability distribution P (w | I ) and then select the highest ranking n words for the image. Note that the value of n is generally user-specified. 5 Experimental evaluation In this section, we present experiments to evaluate the performance of object discovery with semantic feature selection using real-world HRRS image data sets. Specifically, we show: (1) an example set of identified semantic spatial features, (2) the comparison results of our object-based semantic image segmentation with the segmentation function provided by in commercial tool ENVI, and (3) a performance comparison between our OCCUE model and a state-of-the-art Cross-media Relevance Model (CRM) model [ 9 ]. 5.1 The experimental setup Experimental data sets. Since our focus in this paper is on HRRS images rather than regular scenery images, we will not adopt the popular image dataset Corel, which is considered as a benchmark for evaluating the performance of image retrieval algorithms. Instead, we use the high resolution orthoimagery of the major metropolitan areas. This data set is distributed by United States Geological Survey (USGS X  http://www.usgs.gov/ ). The imagery is avai-lable as Universal Transverse Mercator (UTM) projection and referenced to North American Datum of 1983. For example, the New Jersey orthoimagery is available as New Jersey State Plane NAD83. The file format is Georeferenced Tagged Image File Format(GeoTIFF). Since we are not interested in the GIS information, we treat the GeoTIFF images as 3-bands color images. We choose metropolitan areas, such as New Jersey, and Washington DC, since we believe that most composite objects users may be interested in are located in urban area. When we build our image data set, we intentionally include different landscapes.
Data preprocessing. We downloaded the images of 1-foot resolution in the New York metro area, Boston and Springfield MA and Washington DC. Each raw image is about 80 MB, which is then be processed using the Remote Sensing Exploitation Platform (ENVI X  http:// www.ittvis.com/envi/ ). Images with blurred scene or with no major interesting objects, such as square miles of woods, are discarded. For images that contain objects we are interested in, we grid the image into small pieces (2,048  X  2,048 pixels). Finally, we have 800 images in our experimental data set and there are 32 features: 10 color features, 10 shape features and 12 texture features.

Keywords. The keywords used to annotate the semantics of the HRRS images are also different from the traditional scenery images. First of all, they are not attainable directly from the data seta as those of Corel images. Rather, it is manually assigned by domain experts. These keywords can be divided into three groups: keywords regard land cover, individual objects, and composite objects. Table 3 lists some sample keywords.

Validation. In our experiments, we divided the data set into 10 subsets with equal number of images. We performed ten-cross validation. For each experiment, 8 randomly selected sub-dataset are used as training set, a validation set of 80 images and a test set of 80 images. The validation set is used to select the model parameters. Every images in the data set is segmented into comparatively uniform regions. The number of segments in each image, and the size of each segment (measured by the number of pixels) are empirically selected using the training and validating sets.

Blobs. Image Blobs are basic units to form semantic features. A fuzzy classification algorithm is first applied to generate image blobs. In our experiment, we generated 30 image blobs. Table 4 shows some examples of image blobs. Also, Fig. 5 shows a small sample image and its blob representation. The more objects an image contains, the more complex it blob representation is. On average, a test image contains approximately 12 X 15 different blobs.

Evaluation metrics. To evaluate the annotation performance, we apply some external metrics including Precision, Recall, and F -measure. Specifically, we judge the relevance of the retrieved images by looking at the manual annotations of the images. A Recall measure is defined as the number of the correctly retrieved images divided by the number of relevant images in the test data set. The Precision measure is defined as the number of correctly retrieved images divided by the number of retrieved images. In order to make a balance between the recall and precision measures, we also compute the F -measure which is defined as 5.2 Semantic features All images with identified image blobs are used to identify the co-occurrence of image blobs. Specifically, we exploited a hyperclique pattern discovery method to find complex objects that consist of co-existing image blobs, which usually form a unique high-level semantic concept and are treated as spatial semantic features. For instance, Table 5 shows some example semantic features and composite-objects they represent. In addition, as the extension of our previous work, we also identify the semantic features as a concept hierarchy for better knowledge representation. As shown in Fig. 6 , in both images we identify hyperclique pattern [3,4,24] which is assigned as industrial building according to Table 5 . If we explore one more step, we find that it also contains composite object lawn [8,18]. We define a new composite object: office building, which provides better environment to employees, to distinguish it from industrial building whose main purpose is large storage and better transportation. 5.3 Results of parameter selection The hyperclique pattern discovery algorithm has two parameters: support and h -confidence. We examine the impact of these two parameters on the performance of object annotation. The minimum support and the h -confidence thresholds would affect object discovery. For example, the set of blobs (1, 2, 5, 9, 10) can be identified as co-existing objects with minimum support 0.05 and h-confidence 0.4, while it could not be identified when we change the mini-mum support to 0.15. Figure 7 shows the F -measure values with the change of minimum support and h -confidence thresholds. As can be seen, the F -measure values vary at dif-ferent support and h -confidence thresholds. However, we can observe a general trend is that the F -measure values increase with the increase of H -confidence. Also, the maximum F -measure value is achieved when the support threshold is relatively high. This is reaso-nable, since a relatively high support threshold can guarantee statistical significance and provide a better coverage of objects. For this reason, in our experiments, we set relatively high support and h -confidence thresholds. 5.4 OCCUE versus CRM We compared the annotation performance of the two models, the CRM model and the OCCUE model. We annotate each test image with one word from the land-cover level, three words from the composite object level.

Ta b l e 6 shows the comparison results. In the table, we can observe that, for both land-cover level and composite-object level, the performance of OCCUE is much better than that of CRM in terms of Precision, Recall, and F -measure. For instance, for the composite-object level, the F -measure value is improved from 0.2274 (CRM) to 0.4119 (OCCUE). This improvement is quite significant. 5.5 The effect of the semantic segmentation By exploiting different segmentation algorithms on HRRS images, we would have different image blobs, which can make a huge impact on the performance of the semantic feature selection. Therefore, in this subsection, we evaluate the performance of our segmentation results with those from the commercial system ENVI. We chose ENVI since it is widely used in processing remote sensing images. With ENVI, each raw image was segmented into smaller regions (blobs) which were fed into our OCCUE model. By using the tools available in ENVI for image segmentation, we can identify fewer semantic-features than we can get from our proposed segmentation approach. Figure 8 shows the final comparison results. As can be seen, our two-stage object-based semantic image segmentation algorithm can lead to much better performance of object discovery than the traditional segmentation method provided in ENVI. 5.6 The robustness of our semantic feature selection Here, we evaluate whether our semantic feature method is robust in terms of dealing with different atmosphere conditions. Ideally, our approach should get similar annotation for the images which are taken under different air conditions. Aerial images of different areas usually were taken on different days, we manually selected a group of test images, which cover the New York area and were taken under the same air condition. Then, we compare the result on this selected group with other random selected test groups. The average F -measure values, given h -confidence as 0.6, are shown in Fig. 9 . As can be seen, the F -measure values from the New York images are consistent with those from the random groups. This result implies that the semantic features extracted by our method are robust enough to tolerant different air conditions. 6 Conclusions and future work In this paper, we studied the problem of object discovery in High-resolution remote-sensing (HRRS) imagery. Specifically, we first proposed a semantic-aware two-stage image segmen-tation approach, which preserves the semantics of real-world objects during the segmentation process. Then, we exploited a hyperclique pattern discovery technique to capture groups of co-existing individual objects, which usually form high-level semantic concepts. We treated these groups of co-existing objects as new semantic features and fed them into the learning model. As demonstrated by our experimental results, with more reliable segmentation and new semantic feature sets, the learning performance can be significantly improved.
There are several potential directions for future research on this topic. For example, we plan to adapt Spatial Auto-Regression (SAR) model [ 30 ] for object discovery in HRRS images. The SAR model has the ability in measuring spatial dependency, and thus is expected to have a better prediction accuracy for spatial data.
 References Author Biographies
