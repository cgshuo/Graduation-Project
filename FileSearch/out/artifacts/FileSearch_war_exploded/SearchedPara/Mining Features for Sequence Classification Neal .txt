 Classification algorithms are difficult to apply to sequential examples because there is a vast number of potentially useful features for describing each example. Past work on feature selection has focused on searching the space of all subsets of features, which is intractable for large feature sets. We adapt sequence mining techniques to aEi as a preprocessor to select features for standard classification algorithms such as Naive Bayes and Winnow. Our experiments on three different datasets show that the features produced by our algorithm improve classification accuracy by lo-50%, Some classification algorithms work well when there are thousands of features for describing each example (e.g, [Littlestone, 19881). In some domains, however, the number of potentially useful features is exponential in the size of the examples. Data mining algorithms (e.g., [Zaki, 19981) have been used to search through billions of rules, or patterns, and select the most interesting ones. In this paper, we adapt data mining techniques to act as a preprocessor to construct a set of features to use for classification. 
In past work, the rules produced by data mining algorithms have been used to construct classifiers primarily by ordering the rules into decision lists (e.g. [Segal and Etzioni, 1994, Liu et al., 19981) or by merging them into more general rules that occur in the training data (e. f, [Lee et @., 1998]~ In this paper, we convert t e patterns discovered y the mmmg algorithm into a set of boolean features to feed into standard classification algorithms. The classification algorithms, in turn, assign weights to the features which allows evidence from different rules to be combined in order to classify a new example. 
While there has been a lot of work on feature selection, it has mainly concentrated on non-sequential domains. In contrast, we focus on sequence data in which each example is represented as a sequence of  X  X vents X , where each event might be described by a set of predicates. Examples of sequence data include text, DNA sequences, web usage data, and execution traces. 
In this paper we combine two powerful mining paradigms: sequence mining, which can efficiently search for patterns that are correlated with the target 
Copyright ACM 1999 l-581 13-143-7/99/08...%.00 classes, and classification, which learns to weigh evi-dence from different features to classify new examples. 
We present FEATUREMINE, a scalable disk-based fea-ture mining algorithm. We also specify criteria for se-lecting good features, and present pruning rules that allow for more efficient feature minin FEATUREMINE integrates pruning constraints in the af gorithm itself, in-stead of post-processing, enabling it to efficiently search through large pattern spaces. 
We now formulate and present an al minin some mte set of possible values. Let Z be the set of all possible feature-value pairs. A sequence is an ordered list of subsets of Z. For example, if Z = {A, B, C.. .}, then an example sequence would be Al? -+ A + BC. A sequence a! is denoted as ((~1 + CYZ + . . . -+ a,) where each sequence element cri is a subset of 1. The length of sequence (CQ + a2 + . . . + a,) is n and its width is the maximum size of any CY~ for 1 5 i 5 n. We say that cy is a subsequence of /3, denoted as (Y 4 /I, if there for all CX~. For example, Al3 -+ C is a subsequence of 
AB + A + BC. Let C be a set of class labels. An example is a pair (a,~) where (Y = (~1 + ~2 + . . . + cr, is a sequence and c E C is a label. Each example has a unique identifier eid, and each (pi has a time-stamp at which it occurred. An example (cu,c) is said to contain sequence p if p + 01. Our input database 2) consists of a set of examples. 
This means that the data we look at has multiple sequences, each of which is composed of sets of items. The frequency of sequence p in 2), denoted the fraction of examples in V that contain . et be a sequence and c be a class label. The confidence of the rule p =+-c, denoted con probability that c is the labe of an example in 2) given that it contains sequence p. That is, conflp,c,rD) = fr(P, ?cffr(P, v), w h ere 2), is the subset of examples m D wit class la el c. A sequence is said to be frequent if its frequency is more than a user-specified nain-freq threshold. A rule is said to be strong if its confidence is more than a user-specified min-confthreshold. Our goal is to mine for frequent and strong patterns. Figure 1 shows a database of examples. There are 7 examples, 4 belonging to class cl, and 3 belonging to class ~2. 
In general there can be more than two classes. We are loo X ; X , X  cl. The rule C + c2 has confidence 3/4 = 0.75, while the rule C +-cl has confidence l/4 = 0.25. A sequence classifier is a function from sequences to 
C. A classifier can be evaluated using standard metrics gure 1: A) Original Database, B) New Database w jolean Features such as accuracy and coverage. can be used as features for classification. Recall that the input to most standard classifiers is an example represented as vector of feature-value pairs. We represent a example sequence cy as a vector of feature-value pairs by treatin each sequence /?i as a boolean feature that is true 1 ,&amp; 3 cr. For example, suppose 2 the features are fi = A + D, f2 = A -+ BC, and f3 = CD. The sequence AB + BD + BC woul~, X ,~ represented as (fl, true), (fi, true that features can  X  X kip X  steps: holds in AB + BD -+ BC. 
We now specify our selection criteria for selecting features to use for classification. Our objective is to find sequences such that representing examples with these sequences will yield a highly accurate sequence classifier. However, we do not want to search over the space of all subsets of features [Caruana and 
Freitag, 1994]), but instead want to evaluate each feature in isolation or by pair-wise comparison to other candidate features. Certainly, the criteria for selecting features might depend on the domain and the classifier being used. We believe, however, that the following domain-and-classifier-independent heuristics are useful for selecting sequences to serve as features: 1) Features should be frequent. 2) Features should be distinctive of at least one class. 3) Feature sets should not contain redundant features. 
The intuition behind the first heuristic is simply that rare features can, by definition, only rarely be useful for classifying examples. In our problem formulation, this heuristic translates into a requirement that all features have some minimum frequency in the training set. Note that since we use a different min-freq for each class, patterns that are rare in the entire database can still be frequent for a specific class. We only ignore those patterns which are rare for any class. The intuition for the second heuristic is that features that are equally likely in all classes do not help determine which class an example belongs to. Of course, a conjunction of multiple non-distinctive features can be distinctive. In this case? our algorithm prefers to use the distinctive conjunctron as a feature rather than the non-distinctive conjuncts. We encode this heuristic by requiring that each selected feature be significantly correlated with at least one class that it is frequent in. 
The motivation for our third heuristic is that if two features are closely correlated with each other, then either of them is as useful for classification as both are together. We show below that we can reduce the number of features and the time needed to mine for features by pruning redundant rules. In addition to wanting to prune features which provide the same information, we also want to prune a feature if there is another feature available that provides strictly more information. Let M(f,V be the set of examples in 2, that contain feature f. IJ e say that feature fl subsumes feature f2 with respect to predicting class c in data set 2) iff M(fz,D,) C M(fl,Dz),) and M(fl,D-,) G 
M(f2,2&gt;,,). Intuitively, if fi subsumes f2 for class c then fi is superior to f2 for predicting c because fl covers every example of c in the trainin data that fi covers and fi covers only a subset o f the non-c examples that f2 covers. The third heuristic leads to two pruning rules, in our feature mining algorithm described below. The first pruning rule is that we do not extend (i.e, specialize) any feature with 100% accuracy. 
Let fi be a feature contained by examples of only one class. Specializations of fl may pass the frequency and confidence tests in the definition of feature mining, but will be subsumed by fi. The following lemma captures this pruning rule: 
Lemma 1: If fi 4 fj and conf(fi,c,V) = 1.0 then fi subsumes fj with respect to class c. 
Our next pruning rule concerns correlations between individual items. Recall that the examples in 2) are represented as a sequence of sets. We say that A u B in examples 2, if B occurs in every set in every sequence in 
V in which A occurs. The following lemma states that if A + B then any feature containing a set with both 
A and B will be subsumed by one of its generalizations, and thus we can prune it: Lemma 2: Let cy = (~1 -+ (~2 -+ . . . -+ a, where 
A,BEaiforsomel&lt;i&lt;n. IfA+B,thenawillbe subsumed by CY~ + . ..ai-1 + (CX~ -B) + CX~+I... + (-in. 
Feature mining: We can now define the feature min-ing task. The inputs to the FEATUREMINE algorithm are a set of examples 2) and parameters min-freq, max,, and maxi. The output is a non-redundant set of the frequent and distinctive features of width max, and length maxi. Formally: Given examples V and parameters min-freq, maxW, and maxi return feature set F such that for every feature fi and every class cj E C, if Zength(fi) 5 maxi and width(fi) 5 max, and fr(/3,Vcj) &gt; min-freq(cj) and conf (p,cj,V) is signifi-cantly greater (via chi-squared test) than 
F contains fi or contains a feature that with respect to class cj in data set 2). 
We now present the FEATUREMINE algorithm which levera es existing data mining techniques to efficieFn$ mine eatures from a set of training examples. f 
TUREMINE is based on the recently proposed SPADE algorithm [Zaki, 19981 for fast discovery of sequential patterns. SPADE is a scalable and disk-based algo-rithm that can handle millions of example sequences and thousands of items. Consequently FEATUREM-INE shares these properties as well. To construct FEATUREMINE, we adapted the SPADE algorithm to search databases of labeled examples. FEATUREMINE mines the patterns predictive of all the classes in the database, simultaneously. As opposed to previous ap-proaches that first mine millions of patterns and then apply pruning as a post-processing step, FEATUREMINE integrates pruning techniques in the mining algorithm itself. This enables it to search a large space, where previous methods would fail. 
FEATUREMINE uses the observation that the subse-quence relation 5 defines a partial order on sequences. 
If (Y + p, we say that a is more general than ,0, or /I is more specific than o. The relation 5 is a monotone specialization relation with respect to the frequency fr(a, X  X ), i.e., if ,0 is a frequent sequence, then all sub-sequences a 5 0 are also frequent. The algorithm sys-tematically searches the sequence lattice spanned by the subsequence relation, from general to specific sequences, in a depth-first manner. Figure 2: Sequence Lattice and Frequency Computation 
Frequency Computation: FEATUREMINE uses a vertical database layout, where we associate with each item X in the sequence lattice its idlist, denoted C(X), which is a list of all example IDS (eid) and event time (time) pairs containing the item. Given the sequence idlists, we can determine the support of any &amp;se uence by simply intersecting the idlists of any two of its length subsequences. A check on the cardinality of the resulting idlist tells us whether the new sequence is frequent or not. Figure 2 shows that the idlist for A --+ B is obtained by intersectin the lists of A and B, i.e., L(A + B) = C(A) II LfB). Similarly, 
L(AB + B) = C(A + B maintain the class index 1 each example. Using this table we are able to determine the frequency of a sequence in all the classes at the same time. For example, A occurs in eids {1,2,3,4,5,6}. 
However eids { 1,2,3,4} have label cl and { 5,6} have label cs. Thus the frequency of A is 4 for cl, and 2 for ca. The class frequencies for each pattern are shown in the frequency table. To use only a limited amount of main-memory FEA-
TUREMINE breaks up the sequence search space into small, independent, manageable chunks which can be processed in memory. This is accomplished via suffix-based partition. We say that two Ic length sequences are in the same equivalence class or partition if they share a common k -1 length suffix. The partitions, such as {[A], [B], [Cl}, based on length 1 suffixes are called par-ent partitions. Each parent partition is independent in the sense that it has complete information for gener-ating all frequent sequences that share the same suffix. 
For example, if a class [X] has the elements Y + X, and .Z + X. The possible frequent sequences at the next step are Y + 2 -+ X, 2 + Y + X, and (YZ) + X. 
No other item Q can lead to a frequent sequence with the suffix X, unless (QX) or Q + X is also in [Xl. 
P = { parent partitions, Pi} each parent partition in a depth-first manner, as shown in the pseudo-code of Fi ure procedure is a partition, ong with the idlist for each of its elements. Frequent sequences are generated by intersecting the idlists of all distinct pairs of sequences in each partition and checking the cardinality of the resulting idlist against min-sup ci). found to be frequent for some c ass ci at the current level form partitions for the next level. This process is repeated until we find all frequent sequences. 
Integrated Constraints: FEATUREMINE integrates all pruning constraints into the mining algorithm itself, instead of applying pruning as a post-processing step. As we shall show, this allows FEATUREMINE to search very large spaces efficiently, which would have been infeasible otherwise. The Rule-Prune procedure eliminates features based on our two pruning rules, and also based on length and width constraints. While the first pruning rule has to be tested each time we extend a sequence with a new item, there exists a very efficient one-time method for applyin idea is to first compute the requency of all 2 length sequences. Then if P(B(A) = fr(AB) then A w B, and we can remove AB rom t e suffix partition [B . appear toget er in any set of any sequence. 
We now describe experiments to test whether the fea-tures produced by our system improve the performance of the Winnow [Littlestone, 19881 and Naive Bayes [Duda and Hart, 19731 classification algorithms. We ran experiments on three datasets. In each case, we ex-perimented with various settings for min-freq, max,, and maxi to generate reasonable results. We report the values used, below. 
Random parity problems: We first describe a non-sequential problem on which standard classification algorithms perform very poorly. The problem consists of N parity problems of size M with L distracting, or irrelevant, features. For every 0 5 i 5 N and 0 5 j 5 M, there is a boolean feature Fi,j-Additionally, for 0 2 lc 5 L, there is an irrelevant, boolean feature 
Ik. To generate an instance, we randomly assign each boolean feature true or false with 50150 probability. 
An example instance for N = 3,M = 2, and L = 2 is ( Fr,r=true, Fi,s=false, Fz,r=true, Fz,s=true, Fs,r=false, Fs,2=false, Ir=true,ls= false ). There are N x M + L features, and 2NxM+L distinct instances. We also choose N weights wr, . . . . WN which are used to assign each instance one of two class labels (ON or 
OFF) as follows. An instance is credited with weight Wi iff the ith set of M features has an even parity. That is, the  X  X core X  of an instance is the sum of the wei 
Wi for which the number of true features in fi,r, . . . i,M is even. If an instance X  X  score is greater than half the sum of all the weights, Cy=, wi, then the instance is assigned class label ON, otherwise it is assigned OFF. 
Note that if M &gt; 1, then no feature by itself is at all indicative of the class label ON or OFF, which is why parity problems are so hard for most classifiers. The job of FEATUREMINE is essentially to figure out which features should be grouped together. Example features produced by FEATUREMINE are (fi,r=true, fr,z=true), and (fd,i=true, fd,s=false). We used a min-freq of .02 to .05, maxi = 1 and maxW = M. 
Forest fire plans: The FEATUREMINE algorithm was originally motivated by the task of plan monitoring in stochastic domains. As an example domain, we constructed a simple forest-fire domain based loosely on the Phoenix fire simulator [Hart and Cohen, 19921. 
We use a grid representation of the terrain. Each grid cell can contain vegetation, water, or a base. At the beginning of each simulation, the fire is started at a random location. In each iteration of the simulation, the fire spreads stochastically. The probability of a cell igniting at time t is calculated based on the cell X  X  ve etation, the wind direction, and how many of the ccl s neighbors are burning at time t -1. Additionally, 7, bulldozers are used to contain the fire before they reach the bases. For each example terrain, we hand-designed a plan for bulldozers to dig a fire line to stop the fire. The bulldozer X  X  speed varies from simulation to simulation. 
An example simulation looks like: 
We form a database of instances from a set of simulations as follows. Because the idea is to predict success or failure before the plan is finished, the instance itself is a list of all events that happen by some time 
Ic, which we vary in our experiments. We label each instance with SUCCESS if none of the locations with bases have been burned in the final state, or FAILURE otherwise. Thus, the job of the classifier is to predict if the bulldozers will prevent the bases from burnin 
B iven a partial execution trace of the plan. Examp e B7 eatures produced by FEATUREMINE in this domain are (MoveTo BDl X2) + (time6), and (Ignite X2) + (time8 
MoveTo Y3) The first sequence holds if bulldozer BDl moves to the second column before time 6. The second holds if a fire i correlations used by our second pruning rule described in section 2.2 arise in these data sets. For example, 
Y8 -+ Ignite arises in one of our test plans in which a bulldozer never moves in the eighth column. 
For fire data, there are 38 boolean features to describe each event. Thus there are ((38 x 2)maZw )maZl possible composite features for describing eat h sequence of events. In the experiments reported here, we used a min-freq = .2, max, = 3, and maxi = 3. 
Context-sensitive spelling correction: We also tested our algorithm on the task of correcting spelling errors that result in valid words, such as substitut-ing there for their ([Golding and Roth, 19961). For each test, we chose two commonly confused words and searched for sentences in the l-million-word Brown cor-pus [Kucera and Francis, 19671 containing either word. 
We removed the target word and then represented each word by the word itself, the part-of-speech tag in the 
Brown corpus, and the position relative to the target word. For example, the sentence  X  X nd then there is politics X  is translated into (word=and tag=cc pas=-2) + (word=then tag=rb pas=-1) + (word=is tag=bez pos=+l) -+ (word=politics tag=nn pos=+2). Example features clude (pos=+3) + word=the), indicating that the word the occurs at least 3 words after the target word. and (pas=-4) + (tag=nn a noun occurs within h word. These features were significantly \ in the training set. In the experiments reported here, we used a min-freq = .05, max, = 3, and maxi = 2. 3.1 Results 
For each test in the parity and fire domains, we mined features from 1,000 examples, pruned features that did not pass a chi-squared significance test (for correlation to a class the feature was frequent in) in 2,000 examples, and trained the classifier on 5,000 examples. Thus, the entire training process required 7,000 examples. 
We then tested the resulting classifier on 1,000 fresh examples. The results in Tables 1 and 2 are averaged re-tested the classifier on fresh examples in each trial). 
For the snelline: correction. we trained on 80 nercent of the eximple; in the Brown corpus and tested on the remaining 20 percent. During training, we mined features from 500 sentences and trained the classifier on Table 1 shows that the features produced by FEA-compared using the feature set produced by FEA-
TUREMINE with using only the primitive features them-selves, i.e. features of length 1. Both Winnow and 
Naive Bayes performed much better with the features produced by FEATUREMINE. In the parity experiments, the mined features dramatically improved the perfor-mance of the classifiers and in the other experiments the mined features improved the accuracy of the classi-fiers by a significant amount, often more than 20%. 
Table 2 shows the number of features evaluated and the number returned, for several problems. For the largest parity problem, FEATUREMINE evaluated more than 7 million features and selected only about 200. 
There were in fact 100 million possible features (there are 50 booleans features, giving rise to 100 feature-value pairs; we searched to depth M = 4.) but most were rejected implicitly by the pruning rules. 
Table 3 shows the impact of the A w B pruning rule on mining time. The results are from one data set from each domain, with slightly higher values for maxi and maxv than in the above experiments. The pruning rule did not improve mining time in all cases, but made a tremendous difference in the fire world problems, where the same event descriptors often appear together. 
Without A w B pruning, the fire world problems are essentially unsolvable because FEATUREMINE finds over 20 million frequent sequences. 
A great deal of work has been done on feature-subset selection, motivated by the observation that classifiers can perform worse with feature set T than with some 
F X  c F (e.g., [Caruana and Freitag, 19941). The algorithms explore the exponentially large space of all subsets of a given feature set. In contrast, we explore exponentially large sets of potential features, but evaluate each feature independently. The feature-subset approach seems infeasible for the problems we consider, which contain hundreds of thousands to millions of potential features. [Golding and Roth, 19961 applied a Winnow-based algorithm to context-sensitive spelling correction. They use sets of 10,000 to 40,000 features and either use all of these features or prune some based on the classification accuracy of the individual features. They obtain higher accuracy than we did. Their approach, however, involves an ensemble of Winnows, combined by majority weighting, and they took more care in choosing good parameters for this specific task. Our goal, here, is to demonstrate that the features produced by FEATUREMINE improve classification performance. 
Data mining algorithms have often been applied to the task of classification. [Liu et al., 19981 build decision lists out of patterns found by association mining. [Ali et al., 19971 and [Bayardo, 19971 both combine association rules to form classifiers. Our use of sequence mining is a generalization on association mining. Our pruning rules resemble ones used by [Segal and Etzioni, 19941, which also employs data mining techniques to construct decision lists. Previous work on using data mining for classification has focused on combining highly accurate rules together. By contrast, our algorithm can weigh evidence from many features which each have low accuracy in order to classify new examples. [Liu and Setiono, 19981 describes recent work on scal-ing up feature-subset selection. They apply a proba-bilistic Las Vegas Algorithm to data sets with 16 to 22 features. One of the problems is a parity problem, much like the one described above, which contains 20 features (N=2,M=5,L=lO). Their algorithms, thus, search the space of all 220 subsets of the available features. For comparison? we have applied our algorithms to parity problems with 50 features, which results in 100 feature-value pairs. Our algorithm then searches over the set of all conjunctions of up to max, feature-value pairs. 
FEATUREMINE can handle millions of examples and thousands of items, which makes it extremely scalable. 
Our work is close in spirit to [Kudenko and Hirsh, 19981, which also constructs a set of sequential, boolean features for use by classification algorithms. They employ a heuristic search algorithm, called FGEN, which incrementally generalizes features to cover more and more of the training examples, based on its classification performance on a hold-out set of training data, whereas we perform an exhaustive search (to some depth) and accept all features which meet our selection criteria. Additionally, we use a different feature langua e and have tested our approaches on different classi a ers than they have. 
