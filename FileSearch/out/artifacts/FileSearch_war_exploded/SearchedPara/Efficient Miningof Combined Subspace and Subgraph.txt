 In recent years, real world networks have become bigger and also more numerous. Their growing availability motivated researchers and practitioners to analyze and use them for several purposes. One aim is the cluster analysis of graph data, which can be done in various ways [1] including the task of mining densely connected subgraphs hidden in one large graph. This task is useful for, e.g., social network analysis. Besides partitioning approaches [10, 9] some methods assume that the given graph naturally divides into (possibly overlapping) subgraphs of certain patterns, e.g. cliques or  X  -quasi-cliques [20, 8].

Restricting the considerations to the n odes X  relations only, however, does not realize the full potential for knowledge extraction. Usually for all objects a va-riety of additional information is available in form of attribute data (cf. Fig. 1). This information allows for finding homogeneous node sets. In order to gain more informative patterns it is preferable to consider relationships together with shared characteristics. As shown, e.g., in [6, 18], clustering methods using both information sources can o utperform methods using just a single one. Recently introduced techniques aim at combining traditional clustering (using attributes) and dense subgraph mining (using relationships). They group objects based on a high connectivity as well as on a high si milarity concerning their attribute values. This responds to the requirements of many applications: To reduce en-ergy consumption in sensor networks, the long distance reports of connected sensors with similar measurements can be accumulated and transfered by just one representative. In systems biology , functional modules can be determined, which are groups of highly interacting genes with similar expression levels. In marketing.

While the domain X  X  data usually represents a multitude of different recorded in two of their three measurements. In such scenarios, applying full-space cluster-ing leads to questionable clustering results since irrelevant dimensions strongly obfuscate the clusters. Subspace clustering methods solve this problem by find-ing clusters in their locally relevant subspace projections of the attribute data [7]. Consequentially, recent approache s [12, 2 X 4] combine the paradigms of dense subgraph mining and subspace clustering .

These methods enable us to detect more meaningful clusters in the data, like, e.g., the sensor group 3 , 4 , 6 , 7 in Fig. 1. However, combining the paradigms of dense subgraph mining and subspace clustering poses several efficiency chal-lenges. First, analyzing subspace projections is inherently hard since the num-ber of subspaces grows exponentially in the number of attributes. Second, as shown in [2], to obtain high quality clusterings, an unbiased synthesis of both paradigms has to be conducted. Thus, the clustering process has to realize a complex optimization to fairly trade off the cluster properties  X  X ize X ,  X  X ensity X , and  X  X imensionality X . Last, often an overlap between clusters is reasonable since objects can belong to multiple clusters when regarding different attribute sub-sets. Musicians of an orchestra, e.g., may share similar musical interests but probably will practice sports with different persons. However, if the clustering model allows clusters to overlap, it is indispensable to avoid redundancy in-duced by highly overlapping clusters. As known from usual subspace clustering, redundancy elimination is highly complex [11, 13].

As we have seen so far, for a proper combination of subspace clustering and dense subgraph mining, a model has to handle numerous aspects. Although mostly not accommodating all requirements, previous approaches already have high runtime and space consumptions. Thus, an execution on large datasets (if possible at all) is not efficient. In our work we deal with all the aforementioned aspects, but lay special focus on the efficiency challenges .

We start by taking the idea of GA Mer [2] to the next level. While GA Mer restricts the underlying clustering model to just greedily select good clusters for aim for a globally optimizing clustering model. Since even the previous models are rarely efficiently computable, it is not surprising that such a model, aiming at a global optimization, has a high complexity. We therefore analyze our model X  X  complexity to identify the most critica l parts, which inhibit an efficient execu-tion. We substitute these critical parts through highly efficient heuristics that, however, influence the clustering quality only marginally. Thorough experiments demonstrate that our algorithm not only is far superior to all other approaches in terms of runtime but also shows better quality in nearly all experiments. Our main contributions are: (a ) We develop a novel clustering model for a result having globally maximal quality, allowing clusters to overlap in general, and avoiding redundancy (b) We propose the efficient algorithm EDCAR exploiting the GRASP principle and approximating the optimal result. Recently, clustering methods have been introduced analyzing graph data in com-bination with attribute data . [6] transforms the network into a distance and com-bines it with the original feature distance. Afterwards any distance-based clus-tering method can be applied. The clusters are difficult to interpret since they do not have to obey a certain graph structure. In [19] the attribute information is transformed into a graph and densely connected subgraphs are mined by com-bining this novel graph with the original one. The work of [18] uses a combined objective function extending the modularity idea. All three approaches [6, 19, 18] perform full-space clustering on the attributes. [21] enriches the graph by fur-ther nodes corresponding to (categorical) attribute values and connects them to nodes showing this value. The clustered objects are only pairwise similar and no specific relevant dimensions can be defined. Furthermore, the previous methods determine disjoint clusters.

Only a few approaches deal with subspace clustering and dense subgraph min-ing . CoPaM X  X  [12] combination of both paradigms, however, is not sound since it solely maximizes the number of nodes; the density of subgraphs and the subspace dimensionality are incidental. Furthermore, CoPaM does not eliminate redun-dancy, which fast leads to an overwhelming result size. The GA Mer approach [2] simultaneously considers the density, the size, and the dimensionality of clusters by trading off these characteristics. Furthermore, GA Mer uses a redundancy model to confine the result to a manageable size. A disadvantage, however, is the simple determination of the final clustering: GA Mer does not globally ex-amine the result but simply successively adds (in a greedy manner) clusters to the result. Thereby, the resulting clus tering does not necessarily correspond to the most interesting one. In [3, 4] a clus ter definition has been introduced for finding arbitrarily shaped subspace clusters in graphs with feature vectors. The work uses the same redundancy model as proposed in [2].

The major drawback of all methods is their high runtime and large space requirement, which prevents an application on larger datasets. EDCAR ( E fficient D etermination of C lusters regarding A ttributes and R ela-tionships) realizes a novel clustering model. The model is based on the cluster definition introduced and already verified for its effectiveness in GA Mer [2]. The input of our model is a vertex-labeled graph G =( V,E,l ) with vertices V , edges E  X  V  X  V and a labeling function l : V  X  R d where Dim = { 1 ,...,d } is the set of dimensions. We assume an undirected graph without self-loops. We set of vertices O  X  V . 3.1 Clustering Model Our method combines objectives from subspace clustering and dense subgraph mining. Thus, the desired clusters are sets of objects O  X  V that are meaningful subspace clusters in the attribute space and also form dense subgraphs within the input graph. For identifying subspace clusters, we adapt the cell-based model of DOC [15]. According to this definition the values of all objects in a subspace cluster vary at most by a threshold w in the relevant dimensions. For identifying dense subgraphs, we use the definition of quasi-cliques [8]. The density of a O | ( v,o )  X  E }| is the vertex degree restricted to the set O .
 Definition 1. (Twofold cluster [2]) Atwofoldcluster C =( O,S ) is a set of vertices O  X  V and a set of dimensions S  X  Dim with the following properties  X  ( l ( O ) ,S ) is a subspace cluster with dimensionality | S | X  s min  X  O is a quasi-clique with density  X  ( O )  X   X  min  X  the induced subgraph of O is connected and | O | X  n min The resulting clusters are meaningful in the attribute space as well as in the graph. For example in Fig. 2 (choosing w =0 . 5, n min =3,  X  min =0 . 4and s min =2)thevertexset C 1 = a twofold cluster with the relevant dimensions 1 and 4 (marked in blue).
Based on the above definition, the number of node sets fulfilling this definition is potentially very large and probably many clusters will overlap (e.g. C 1 and C 2 in Fig. 2). Furthermore, some subsets of the clusters are twofold clusters as general since one node can belong to several meaningful groups, clusters that are too similar to each other often contain nearly the same information.
Since these redundant clusters are not beneficial but obstructing, they should be excluded from the result. To identify a redundant cluster C w.r.t. another cluster C , several properties have to apply. First, the structural information of the corresponding clusters has to be similar, i.e. they have to share a large portion of their vertices and their dimensions. Second, the cluster C should be less interesting than the cluster C ; otherwise one would prefer C . Formally, the redundancy of C w.r.t. C is based on the following relation: Definition 2. (Redundancy relation) Given the redundancy parameters r obj  X  [0 , 1] and r dim  X  [0 , 1] , the binary redundancy relation  X  red is defined by: Using the parameters r obj and r dim the user can determine to which extent two clusters may overlap without being defined as redundant. For example, with r obj = r dim =0 . 5 the cluster C 2 would not be redundant w.r.t. C 1 . Although do not overlap. With the values r obj =0 . 5and r dim =0, C 2 would be redundant w.r.t. C 1 .
 Cluster selection based on global optimization. Our goal in selecting the final clustering is a solution, that (a) does not contain clusters that are redun-dant to each other, i.e. it has to be redundancy-free, and (b) is most interesting. While the GA Mer method greedily selects clusters according to their quality, we perform a more sophisticated selection. Instead of deciding locally which cluster to select next for the result, we perform a global optimization to get the most interesting clustering. We, thus, do not prefer the selection of single interesting wards, but we select the overall most interesting clustering . Correspondingly, we require of our Result that the sum of its clusters X  qualities is maximal compared to all other possible clusterings. Formally, the maximum quality clustering is defined as follows: Definition 3. (Maximum quality clustering) Given the set of all twofold clus-ters Clusters , the maximum quality clustering Result  X  Clusters fulfills  X  (redundancy-freeness)  X  X  C i ,C j  X  Result : C i  X  red C j  X  (maximum quality sum)  X  X  Res  X  Clusters : Res fulfills the redundancy-Fig. 3 shows an example for the final clusterings of GA Mer and EDCAR: Nine clusters, their quality values, and the redundancy relation are illustrated. The quality sums of the overall clusterings are depicted on the right. GA Mer se-lects the cluster C 2 since it is not redundant w.r.t. any other cluster. A greedy selection according to the quality val ues is performed. In EDCAR, cluster C 2 is not selected for the final clustering. While C 2 has a high quality itself, its admittance would prohibit the clusters C 3 , C 4 ,and C 6 . However, by including these clusters and excluding C 2 , our final clustering has a higher quality (39 . 7 vs. 29 . 6). As the example illustrates, EDCAR optimizes the interestingness of the overall clustering, which can yield better results but is computationally more challenging. 3.2 Complexity Analysis The complexity of our clustering model is given by the following two theorems (proofs on the web). First, the overall complexity of our model, i.e. of generating the twofold clusters and selecting the maximum quality clustering, is #P-hard. Theorem 1. Given a vertex-labeled graph G =( V,E,l ) , determining the maxi-mum quality clustering according to Def. 3 is #P-hard w.r.t. | V | .
 Second, even if the set of twofold clusters Clusters is given, selecting the maxi-mum quality clustering Result  X  Clusters (cf. Def. 3) is NP-complete w.r.t. the input size.
 Theorem 2. Given a set of twofold clusters Clusters , selecting the maximum quality clustering according to Def. 3 is NP-complete w.r.t. | Clusters | . Conclusions. From Theorem 1 we can infer that the input size | Clusters | can be exponential in | V | . Overall we can identify two parts that, especially in combination, prevent an efficient determination of the optimal solution: the tremendous amount of clusters used as candidates for the optimal clustering and the complexity of the selection process for a final subset of these clusters. As shown, efficiently determining a maxim um quality clustering is not possible. Thus, we develop the heuristic algorit hm EDCAR to ensure an efficient execu-tion. We have to tackle two major challenges: First, we have to reduce the number of result candidates (Section 4.1). We cannot use the whole set Clusters of ex-ponentially many candidates as the input for the selection procedure. Second, we have to resolve the NP-hardness of the s election process itself (Section 4.2). 4.1 Reduce the Number of Result Candidates This first phase generates the cluster candidates among which the subsequent process chooses the final clustering. The goal is to efficiently determine a set of twofold clusters that is of manageable size and of high quality .

To analyze sets of vertices whether the y are twofold clust ers, we enumerate them using the set enumeration tree [17]. An exemplary tree for a graph with four vertices is shown in Fig. 4. Each node of the tree represents a set of vertices O  X  V .Eachnode O is associated with a candidate set cand O extends its parent node O through one of the vertices in cand O .Thus,the subtree of a node O represents all potential clusters X with O  X  X  X  O  X  cand O . By pruning a vertex v from the candidate set of a node O , the search space can the highlighted subsets in Fig. 4 would disqualify themselves as clusters without further analysis. EDCAR employs all pruning methods of [2].

We want to avoid analyzing each node along each (non-pruned) path in the set enumeration tree since this could lead to an exponential number of twofold clusters which are used as candidates for the final clustering. To reduce the number of candidates we implement two d ifferent strategies. In the first step, we avoid analyzing all paths of the tree by systematically determining single paths along which interesting clusters can be expected. By selecting a polynomial number of paths we will also only get a polynomial number of candidates. This
Algorithm 1 path selection is illustrated in Fig. 5 by the solid lines. Even though this method reduces the number of clusters considerably, this set is still unnecessarily large and will be reduced in a second step. Sin ce along each selected path the object sets successively grow by one vertex (cf. Fig. 4), the clusters along this path are most likely redundant to each other. Using all these clusters as the input for the selection step is needless since most clusters will be discarded anyway. Based on the definition of our redundancy relation we know that clusters with high quality are preferred. Thus, instead of using all clusters, we select along each path just the cluster with the highest quality (cf. dots in Fig. 5). Overall, we realize by our two strategies that only few candidates are used as the input for the cluster selection step. In the remaining of this section, we present more details on our path selection technique.
 GRASP for efficient path selection. To systematically (and efficiently) achieve a selection of interesting paths, we adapt the GRASP principle (Greedy Randomized Adaptive Search Procedure) [14, 16], Naively, one could randomly determine a path. This, however, does not assure to generate high quality clus-ters. Alternatively, one could decide at node O which successor v  X  cand O (po-tentially) leads to a good cluster and one descends in the subtree with the high-est potential. We use a function g ( v | O ) to estimate the potential of each node v  X  cand the next subsection. This approach corresponds to a greedy construction of the path. The huge advantage of this greedy method is that the graph structure and the cluster definition can be incorporated into the estimation function g to rate the potential of the path. Thus, it corresponds to an informed search and high quality clusters can be expected. Disadvantageously is the risk of reaching only local maxima and generating always very similar paths. These problems do not hold for the randomized approach, which is able to generate a diversity of paths in an uninformed fashion.

To exploit the advantages of both methods (informed and randomized search), we use the GRASP principle, which acts as a metaheuristic to combine them. Several studies show that this principle often leads to optimal or nearly optimal solutions [14]. According to the GRASP principle, we first construct a restricted candidate list (RCL) corresponding to a set of potentially meaningful vertices for expanding the path. Afterwards, we randomly select one vertex v  X  RCL O to descend into a subtree. Formally, with g m =min
By choosing  X  =1 we can simulate the greedy approach whereas  X  =0 corre-sponds to a completely randomized select ion. The determination of a single path is shown in Algorithm 1, line 5 to 10: In line 6 we determine the candidate set cand O for the current vertex set O , which is then reduced using the pruning techniques from [2]. Next we determine the RCL as described above and add a randomly selected node to O . This procedure is repeated until cand O =  X  , i.e. no more nodes can be added to the path.

As depicted in Fig. 5 we want to descend in several paths. This is done by line 2 and we use a randomly determined  X  to trade off the two GRASP principles for each novel path, leading to more stab le results [16]. Overall, we efficiently generate different paths containing high quality clusters based on the estimated potential.
 Potential of Paths. At last, we have to determine the potential of a sub-tree. Since our goal is to maximize the sum of qualities, we want to use the quality as our estimation function g ( v | O ). If efficiency was not required, one notes the subspace of the corresponding vertex set. However, efficiency is cru-cial in our case. While the size and the subspace can be efficiently determined, the exact density  X  ( O  X  X  v } ) is computationally expensive. Keep in mind that g ( v | O ) has to be evaluated for each v  X  cand O . Therefore, we approximate the density  X  ( O  X  X  v } ) of the potential cluster O  X  X  v } by the lower bound  X  ( O,v ) can be efficiently computed for different vertices v because it is mostly independent of v . We only have to compute the term deg O ( v ). Our overall esti-mation function is g ( v | O )=  X  ( O,v ) a  X  ( | O | +1) b  X | S ( O  X  X  v } ) | c 4.2 GRASP for Efficient Clustering Selection So far, we reduced the number of candidates used as the input for the cluster selection step. Now we approximate the maximum quality clustering (Def. 3)) itself since its determination is NP-hard. We again use the GRASP principle. Therefore, we relax Def. 3 by only demanding the resulting clustering Res to be redundancy-free and maximal: (  X  X  C,C  X  Res : C  X  red C )  X  (  X  C  X  Cands \ Res :  X  C  X  Res : C  X  red C  X  C  X  red C ). Starting with an empty result Res , we now successively add further clusters C  X  Cands basedonthe GRASP principle. Since our goal is to find clusterings with a high quality sum, adding C to Res , by the quality of clusters: h ( C | Res )= Q ( C ). This value is already given at this time. Thus, no additional computation has to be done. The
Algorithm 2 selectClustering(...) pseudo code for selecting the subset is given in Algorithm 2. Since our model requires redundancy-freeness, we are able to remove in line 9 all clusters that induce such a redundancy. These clusters can no longer be added to Res .
To further increase the overall quality, we conduct in line 10-14 a local search on the set of valid clusterings. The idea is to replace a cluster C  X  Res by a set of not yet selected clusters New C to get the potentially better clustering Res \{ C } X  New C .Theset New C is built by collecting clusters from Cands \ Res , in decreasing order w.r.t. their quality values, as long as the redundancy-freeness property of the overall result Res \{ C } X  New C is not violated. Thus, an effi-cient greedy approach can be used to generate New C . Formally, for each clus-ter X  X  Cands \ Res not selected for New C it holds: X/  X  New C  X  X  X  C  X  New C :( X  X  red C )  X  X  X  C  X  Res \{ C } :( X  X  red C  X  C  X  red X ). The overall neighborhood of a clustering Res is the whole set of such generated alternatives Neighborhood ( Res )= { Res \{ C } X  New C | X  C  X  Res } . If no better clustering in the neighborhood exists, we have reached a local maximum and the cluster selection in this iteration is finished. 4.3 Overall Processing Scheme The two phases of our method, generating a small number of candidates and selecting the resulting clustering based on these candidates, lead to an overall efficient execution. Since we select just the one cluster with the highest qual-ity along each path, however, lower qu ality clusters do not get the chance to be selected for the result. Neverthele ss, also clusters with lower qualities can contribute to the overall result, as C 9 in Fig. 3. To give these low-quality but valuable clusters the chance to be considered as result candidates, we repeat both phases recurrently (cf. Algorithm 3).
 with highest quality along each path. In subsequent iterations Res is used to avoid considering redundant candidates. We thus block redundant parts of a path and select the most interesting cluster among the remaining non-redundant ones as additional candidate. Overall, we generate in each iteration only candidates clusters can be added to the final clustering. Thus, we perform the clustering selection phase on the enriched set Cands  X  Res to get the novel preliminary result Res  X   X  Cands  X  Res . Overall, our processing interweaves the generation and the selection of clusters by cyclically invoking both phases. The method automatically terminates if no further non-redundant candidates can be found. We compare EDCAR with GA Mer [2] and CoPaM [12]; both consider subspaces and dense subgraphs. As a further competitor we use the extension Cocain  X  of Cocain [20] as described in [2]. Efficiency is measured by the approaches X  runtime. All experiments were conducted on Opteron 2.3GHz CPUs using Java6 64 bit. Methods that did not finish within two days were aborted. Clustering quality is calculated via the F1 value [5]. We use several public real world datasets and synthetic data, by default with 80 clusters, each with 15 nodes, a density of 0.6 and 5-10 relevant dimensions out of 20 dimensions. 6% of the clusters X  nodes overlap. We provide all datasets with descriptions, executables, and parameter settings on our website .
 Database size. First, we vary the number of ver tices in the graph by increasing the number of clusters and keeping the number of objects per cluster fixed. As depicted in Fig. 6(a) (top), EDCAR is several orders of magnitude faster than all competing approaches (note the logarithmic scale on both axes). EDCAR is the only method applicable on large data sets. Especially CoPaM is no longer executable for these settings since its limited redundancy model leads to an im-practicably large amount of clusters. GA Mer scales worse than EDCAR, too. It has to analyze the complete set of all twofold clusters, leading to a high runtime. Although EDCAR uses an even more complex clustering model, the runtime is lower since we systematically generate and select only the most interesting clusters.

Besides EDCAR X  X  high efficiency, we obs erve in Fig. 6(a) (bottom) its high effectiveness. Despite the used approximations, the quality of EDCAR is similar or even higher than that of GA Mer . The remaining approaches CoPaM and Cocain  X  achieve only low qualities, as also shown in [2]. This experiment has shown that EDCAR is also applicable on large datasets. Though, for the follow-ing experiments we chose medium-sized datasets to enable a comparison with the other algorithms.
 Cluster size. In this experiment, we keep the number of clusters fixed but increase the number of vertices per clust er. This setting is more challenging since larger clusters correspond to longer paths in the set enumeration tree. In Fig. 6(b) (top) we observe only a slow increase in runtime for EDCAR. Since the number of hidden clusters is fix, the number of required iterations in EDCAR is almost constant (about 15). All competing approaches show heavily increasing runtimes and are not applicable at an early stage. Fig. 6(b) (bottom) shows nearly perfect quality for EDCAR. The qualities of the other methods decrease to different extents. The advantage of our novel clustering model becomes apparent. Graph density. In Fig. 6(c) we increase the gra ph X  X  density by adding edges between the clustered nodes. Again, EDCAR is orders of magnitudes faster con-firming the usefulness of our solution. Data dimensionality. In Fig. 7 we increase the data X  X  dimensionality. Though the runtimes of the competing methods do not increase significantly, they are not applicable for larger datasets due to the extreme memory usage. CoPaM is not applicable at all. The other methods have to manage a tremendous amount of clusters whereas our algorithm generates incrementally a small set of clusters. Overall, all experiments indicate that EDCAR achieves far better runtimes than all competitors. EDCAR is the only method applicable to large datasets. At the same time the results of our approximation achieve high effectiveness. Real world data. We use gene data 2 and their interactions (3548 nodes; 8334 edges; 115d), an extract of the Arxiv database 3 (27769; 352284; 300d), patent information 4 (492007;528333; 5d), and a co-author graph extracted out of the DBLP database 5 (133097; 631384; 2695d). Since for real world data no hidden clusters are given, we analyze in Fig. 8 d ifferent properties of the clustering results (runtime, avg. numb er of vertices, density, dim ensionality of the detected clusters). For all datasets EDCAR is orders of magnitude faster than the other methods. GA Mer is only applicable on three of the datasets. CoPaM can only be executed on the gene data and achieves extremely high runtimes. Cocain  X  finished on none of the datasets within 2 days. The clusters identified by EDCAR and GA Mer have nearly similar properties. Thus, our approximations do not impair the clustering quality.

An exemplary cluster from EDCAR X  X  clu stering result on the DBLP co-author graph is shown in Fig. 9. Here, each node represents an author, each edge cor-responds to a co-authorship, and the 2695 attributes of a node indicate the conferences which an author has attended. Fig. 9 illustrates a cluster consisting of 12 authors who jointly published papers at the conferences IEEE ICME, ACM Multimedia, and TREC (i.e., the cluster is located in a 3d subspace). Please note all authors collaborated together. EDCAR is the only method that can handle this data set; all competing methods fail due to their high runtime and space complexity. We introduced the method EDCAR for efficiently detecting clusters showing high density in graphs as well as feature similarity in subspace projections. Our model combines subspace clustering wi th dense subgraph mining and performs an overall optimization of the result to get the most interesting, redundancy-free clustering. Based on the proven complexity of our model, we developed the algo-rithm EDCAR to efficiently calculate an approximate solution. By interweaving the process of cluster generation and cluster selection, which both make use of the GRASP principle, EDCAR determines high quality clusters and ensure low runtimes. Thorough experiments demonstrate that EDCAR has high effective-ness and at the same time constantly outperforms all competing approaches in terms of efficiency.
 Acknowledgments. This work has been supported by the UMIC Research Centre, RWTH Aachen University, Germany.
 Depression has gradually become a common mental illness in the modern era. Ac-but less than 25% of those people receive adequate treatment (Saraceno, B. 2002). 
Depression is a type of mental disease without apparent symptoms, especially dur-their drastic mood swings are caused by depression. With the goal to combat this potential depression cases given the written materials of the subjects. 
Depression detection has been studied for a while but most of the existing depres-manually identified (usually by another human). Second, even if the potential candi-test takers may not take the test seriously. Q1. What kind of classifier should be designed? A binary classification tool that Q2. Given the answer from Q1, how to obtain the labeled training data? Q3. What kinds of features are useful for this task? Q4. What kind of evaluation procedure is co nsidered as a sound mechanism to as-pression detection framework. We investigate how to design a learning-based system that is capable of determining pression. Furthermore, we investigate how the temporal information can be exploited in such task. 
Our training data is collected from the most popular bulletin board system (BBS) anonymous property of the cyber world, users are more willing to express their true feelings on web platforms than they do in the real world. This makes such public-sharing platform a good source of data for depression detection, and consequently enables our study on depression detection through text mining. 
In PTT, there are more than twenty thousand boards focusing on wide range of top-thousands of new posts every day. The main reason we use data from PTT for depres-a lows users to write down what are on their minds. 2.1 Methodology Overview To solve Q1 as described previously, one conventional solution for depression detec-non-depression content to c etc.). If, say, a bunch of t e very possible all non-techn sion articles X . Similar draw b writings are used as negat i negative samples, then we there are significantly mor e a multi-class classification classes to be distinguished f
To further investigate t h rios of our system. First, a when somebody intents to suffering depression. For s u to some extent, already car r with negative emotion are w non-expert to distinguish b e carry some negative thoug h automatic system for such tive depression detection e depressed emotion from th o
The second usage scena r amount of writings (most l i candidates for early treatm e tion system can act as an a didates for further treatme n articles from many other ty p
To handle these two sce shown in Figure 2). In th e input manuscript into  X  X e g classified as negative-emo t  X  X epression X  or  X  X adness X  c dered as a depression can d framework satisfies both s c first scenario and an addi t used to filter non-negative deals with a simpler task t h are closely related concept s 2.2 Feature Generatio n In the first stage, we exploi IDF values of words as the tion 1 and Equation 2, n i,j i articles. In equation 2, th e people whose articles cont a In the second stage, we try temporal information to ha and sadness. The intuition i people not only in the wri t produced. Below we will d e
We first select two boar d members consist of mostly nary sadness emotion (mor e section 3.1). Figure 3 sho w clues. 
In Figure 3(a), we obse r ever, on the Prozac board, time (from 18:00 to 6:00 n nificant. Similarly in Figur e much more frequently du r trend. Both observations m better recognize depression
The posting time of a post on BBS, blogs, or micro-blogs is usually fine-grained, Table 1. Then we can apply TF-IDF in equation 1 and 2 to generate the value of tem-timeslot-term" pair. 
Each of these temporal categories has its own meaning. The first three categories divide a day into two sections,  X  X aytime X  vs.  X  X ight X  or  X  X orking hours X  vs.  X  X on-sion. Finally, we define the category  X  X orkday-Weekend X  to reflect the observation leads to different mental conditions or behaviors between workday and weekend. 
There are 12 different categories in Table 1. Thus, one single term feature can be converted to 12 dif ferent temporal features. Assuming there is a message posted on the right-most column in Table 1. 3.1 How the Training Data Can Be Obtained? We conduct experiment on PTT data, and use 5-fold cross validation to obtain the es with negative emotions from those with non-negative emotions. We choose posts the most popular board on PTT but also due to the variety of posts with different types of write-ups. and express their emotions. The word Prozac is the name of medicine for treating which contain posts created by people with depression to express their thoughts and basic statistics of the training data. 3.2 Stage 1  X  Negative vs. Non-negative Classifier Most of the posts in PTT are in Traditional Chinese. Therefore we focus on Chinese posts in this experiment, though the proposed technique is language universal. We in Chinese as they are more likely to be s top-words without apparent meaning. Be-end, 5622 unigram terms are chosen. 
Here we collect all pos ts of a user to extract the unigram features. Note that the validation accuracy reaches 96.17%, which means negative-posts and non-negative-posts can be easily separated based on content. 3.3 Stage 2  X  Depression vs. Sadness Classifier shown in Table 4. The 5-fold cross validation accuracy is 81.86%, which signifies that depression and sadness posts are indeed separable, but doing so is much harder than separating negative vs. non-negative write-ups. 
To verify the quality of training data and learning process, we also examine the weight of each feature. Table 5 shows th e top 40 depression features, and Table 6 shows the top 40 sad features. All features have been translated into English. 
From the top 50 depression features, we can see that some of them are related, such and crowd . The term knife might indicate the most common tool they used or im-keywords for sad posts are more general. 
We then conduct experiments to observe the performance of using only temporal exploited (see Table 8), the performance does show consistent improvement. In most features, the accuracy can be boosted to 84.51%, which is significantly higher than using only term features. To further confirm the effe c demo and manual evaluat i People can simply copy/p a There is no need to fill in a n
Figure 4 shows the scr e timestamp are submitted, t h of the subject possessing n e 4.1 Manual Evaluatio n We extract posts from an o our system can identify s o writings. Diary board is a p ings and thoughts. People h less, we believe that amon g small fraction of them su ff system can identify some o 2071 users. Figure 5 illustr a
We feed the posts from our two-stage classifier. W diagnose (based on the w r identified are truly potenti a knowledge to diagnose de p read a brief document abo u Diagnostic and Statistical M ican Psychiatric Associati choose from four options : depression, and no depress i
Table 9 shows the diagn o as  X  X ajor depression X  if a t depression X  if at least one o top suspects our system id e The agreement is even higher (21/30) for non-expert evaluators (see Table 10). The subjects who are suffering depression but are unaware of it from their writings. 5.1 Automated Depression Detection Neuman and Kedma (2010) propose a system called Pedesis to automatically detect users with depression on the web, specifically on blogs. Their main idea is that de-terms are not the only indicators. By the help of a search engine, they extract sen-words to identify users with depression. This knowledge-driven method is fundamen-tally different from our learning-based method. There are researches applying machine learning methods for depression detection. We can classify them into four categories based on the sources used for detection: text, speech, facial expression and electroencephalogram (EEG). to create, while we use PTT which contains a gradually increasing resource whose Aamodt et al. (2010) develop a decision support system for depression diagnosis us-ing case-based reasoning. However, to compare a new case with past cases, the sys-limited. 
Speech . Based on the theory that people with depression have slow and monoton-tection and compare performances of different feature combinations. Their experi-ments show MFCC and short time energy out perform other features. Sanchez et al. the speaker is depressed. cial actions to perform depression detection. They found both manual FACS coding and active appearance modeling (AAM) are correlated with depression. Maddage et 
EEG . Hosseinifard et al. (2011) use EEG signal as the features for depression clas-detection system like this can hardly be used as screening tool for large amount of candidates. 
The related work shows that different sources of information can serve as the clue for depression detection. Our work is novel because we have not yet found any work that proposes similar two-stage online screening framework with empirical demon-stration on the effectiveness of the designed temporal features. 5.2 Temporal Information in Depression Eastwood and Stiasny (1978) conduct an analysis of hospital admission of neurotic considered. They find that depression admission for women reaches highest peak in November, and for men, in April. Both studies have confirmed a correlation between depression candidates, which strengthen our proposal to introduce the temporal in-formation as features. The main contributions of this paper are listed below: 1. We propose two practical real-world usage scenarios for depression detection, 3. We identified an important resource, namely BBS, which allows us to automati-4. We developed an online real-time depression detection engine which does not The framework, data, and features we have proposed can easily be applied to design which we believe are also useful clues for depression detection. Acknowledgements. This work was supported by National Science Council, National Taiwan University and Intel Corporation under Grants NSC101-2911-I-002-001, NSC101-2628-E-002-028-MY2 and NTU102R7501.

