 In social media, age prediction aims t o determine the age of one online user by lever-aging his/her published content or his/her social information. For example, Figure 1 shows a n online user in a social media website. When the age is not available , we could exactly infer her age to be 23 from her published message  X  X  X  X  23 X  . Age prediction has been an essential pre -processing step in many social applications. Generally, age classification and age regression are two foundational tasks in age prediction. Different from the age classification conce rned with classify ing the users into several age groups [1] , age regression focuses on predicting the user  X  X  age with a discrete variable indicat-ing an exact age number [ 2,3 ] .

Conventional approaches to age regression focus on supervised learning where suf-ficient labeled data is essential for training the model. However, to exact ly annotat e the gold en label of online user  X  X  age is extremely difficult [ 4 ] . A better way to obtain the labeled data is to ask for the online users to obtain their real ages . However, such way of collecting data is rather time -consuming and expensive. 
In this paper, we propose an active learning approach to address above challenge by better exploiting the unlabeled data to reduce the scale of annotation data. In active lea -rning, the simplest and most commonly used query framework is uncertainty sampling [ 5 ] where a n uncertainty measurement is designed to pick most unconfidently classified instances in each iteration. In a classification problem, it is easy to measure the uncer-tainty by leveraging the posterior probabilities provided by the classifiers. For instance, in a 2 -class classification problem, the posterior probability of one class around 0.5 is thought to be a very unconfidently classified score. However, in a regression problem, there is no such estimated probability which can be used directly because the possible predictions in regression are infinite [ 6 ] .

To tackle the above difficulty , we propose a novel method to estimate the labeling un confidence of an unlabeled instance. Specifically, our method generates multiple ran-dom feature subspaces to train a committee of regressors and then leverage the com-mittee of regressors to estimate th e labeling un confidence of each instance. The moti-vation of using feature subspaces to generate multiple subspace regressors is due to the fact that the feature space (either using textual features or social features) in age regres-sion is extremely high an d we believe that a feature subspace of a certain scale is suffi-cient to train a good regressor. F or example, in our data collected form the social web-site called Sina Weibo, the dimension of the textual features (i.e., word unigram fea-tures), is normally larger than 200,000 while the dimension of the social features is larger than 600,000. In principle, our method is a specific implement at ion of the famous Q uery By Committee (QBC) method which has been successfully applied in active learning [ 7 ] . For clari ty, we refer to our method as subspace -based QBC. To the best of our knowledge, this is the first attempt to employ multiple feature subspaces to generate the committee in QBC for active learning in a regression task.

The remainder of this paper is organiz ed as follows. Section 2 overviews related work on age regression and active learning for regression. Section 3 introduces some background on data collection and the basic model for age regression. Section 4 pre -sents the active learning algorithm for age regression. Section 5 evaluates the pro -posed approach. Finally, Section 6 gives the conclusion and future work. This section gives an overview of related work on age prediction and active learning for regression respectively . 2.1 Age Prediction Over the last decade, the overwhelming majority of studies model age prediction as a classification problem . For instance, Peersman [1] apply a text categorization approach to age classification with textual features only. Some other studies, such as Macki nnon and Warren [ 8 ] , and Rosenthal and McKeown [ 9 ] , explore social features to enhance the performance of age classification. 
Compared to age classification, related work on age regression is much less. Nguyen [ 2 ] explore textual features, such as word un igrams, POS unigrams and bigrams, to-gether with gender features in age regression via a linear regression model. Their em-pirical studies find that word unigrams can achieve reasonable performance and that POS patterns are strong indicators of the old age. Another contribution of their work is their joint model for performing age regression with three different genres of data. More recently, Nguyen [ 3 ] further explore age prediction of Twitter users with a linear re-gression model. They find that an automatic system can achieve better performance than human being.

To the best of our knowledge, no previous studies have been conducted their research on active learning for age prediction. 2.2 Active Learning for Age Regression Active learning has been extensively explored in both natural language processing (NLP) and machine learning (ML) communities. For a quick and overall understanding the research issue of active learning, please refer to two comprehensive sur v eys, i . e., NLP and ML communities respectively.

While most previous studies focus on the scenario of classification problems [ 12 ] , o nly a few studies address the active learning issue on regression problems. Burbidge [ 1 3 ] employ QBC in active learning for regression by measuring disagreement as the variance among the committee members X  output predictions . The committee members in their approach are generated by using several subsets of the training data.
To the best of our knowledge, no previous studies focus on active learning for re-gression with random feature subspaces . In this section, we give some background on data col lection and the basic regression model for age regression.
 3.1 Data Collection Our data is collected from Sina Micro -blog ( http://weibo.com/ ), a famous Micro -blogging platform in China. From the website, we crawl each user  X  s homepage which contains user information (e.g. , name , age, gender, verified type ), and their posted messages. The data collection process start s from some randomly selected user s , and iteratively gets the data of their follower s and following s . We remove those unsuitable users who are verified as organization s because the age attributes of these users make no sense. Besides , although the posted messages are the basic and majo r factor to predict user ages [ 2 ] , some users post very few messages . To guarantee the reliability of the data, we remove those somehow non -active users who post less than 50 messages. In total, we collect the homepages of about 12000 users, together with their posted messages.

Figure 2 shows the user distribution in different ages. From this figure, we can see that the data distribution of user ages are rather imbalanced. Most users are young whose ages are in the range of 19 -28. 3.2 Basic Model for Age Regres sion In this study, we model age prediction as a regression model and apply support vector machines (SVM) to estimate the regression function [1 4 ] .

G iven the training data  X   X  yR  X  , o ur goal is to find a function  X   X  fx that maps the input fx is a linear function, taking the form: function  X   X  fx becomes solv ing the following convex optimization problem: between the flatness of  X   X  fx and the amount up to which deviations larger than  X  are outputs of real numbers into integers. In active learning, both labeled data L and unlabeled data U are available and the goal is to improve the performance by exploiting unlabeled data, finally reducing annotation cost. In other words, we hope to get better performance quickly when we only manually labeled a limited number of instances from unlabeled da ta U . 4.1 Textual and Social Features model. In the literature, various features, such as word unigrams, and social behaviors, have been successfully adopted on age prediction [ 9 ] . In this study, we categorize these features into two main groups, textual and social features. The former contains the fea-tures, generated from the user -generated messages, e.g., word unigrams, while th e latter contains the features, generated from the user soci al behavior s, e.g., follower list and following list. Table 1 shows all the features in the two categories.

Among textual features, BOW features are most popular in age prediction and proven very effective due to the fact that word features reflect concerning topics , which can distinguish users of different ages . POS patterns are also popular textual features to capt ure the writing styles of the users.

Among social features, the 4 statistical features, i.e., those with # of, capture the social behaviors of a user. The Time features capture the user habits on posting mes-sages. For example, users of 20 -24 ages might be more likely to post their messages very late at night. Followings and followers reflect the interests of users which p rovide an effective window to infer users X  ages. 4.2 Active Learning with QBC Generally, active learning can be either stream -based or pool -based [1 5 ] . T he main dif-ference between the two is that the former scans through the data sequentially and se-lects infor mative samples individually, whereas the latter evaluates and ranks the entire collection before selecting most informative samples at batch. As a large collection of samples can easily gathered once in age regression, pool  X  based active learning is adopted in this study.

In the study, we utilize query by committee (QBC) method as our basic active learn-ing framework. Originally, query by committee (QBC) is a group of active learning approaches which employ a committee of learners to select an unlabeled examp le at standard pool -based active learning alg o rithm with QBC method. In this algorithm, the way of learn ing a committee of member classifiers and the confidence measuri ng strat-egy are two crucial components which will be discussed in the next subsection in detail . 4.3 QBC with Random Feature Subspaces To generate a committee of learners, we adopt the Random Subspace Generation (RSG) approach to generate multiple learners trained with several feature subspaces [ 1 6 ] . As-x w w w  X  , described by m features. RS G first randomly selects r ( rm  X  ) features and obtains an r -dimensional random subspace of the original m -dimensional feature space. In this way, a modified training set dimensional samples regression leaner can be trained in random subspaces S x using the modified training set . In our implementation, we set N to be / mr and thus N disjoint feature subspaces are utilized to generate N subspace regression learners.

Active learning aims to select the most un certain (unconfident) sample rather than the most certain sample. Thus, we select an unlabeled example at which their regression predictions are maximally dis agreed. Formally, given the regression results from the committee of lea r ners Where ' y is the estimated result of the committee, calculated as follow s : The more the unconfidence score is, the more un confidently the sample is predicted.
Figure 4 shows the algor ithm of our QBC -based approach to selecting un confident samples. Note that we only give the algorithm description on the textual features. A similar description is obvious for the social features and joint features . In this section, we have systematically evaluated our approach to active learning for age regression . 5.1 Experimental Settings Data Setting The data collection has been introduced in Section 3.1. We extract a balanced data set from the collected data by selecting 200 samples in each age and the age is limited in the range of 19 to 28, totally 10 age categories. We use 80% of the data in each age category as the training data and the remaining 20% data as test data. In active learning, we randomly select 10 users in each age ca tegory from the training data as the initial labeled data and the remaining training data as unlabeled data.
 Regression Algorithm &amp; Features W e use the libSVM ( http://www.csie.ntu.edu.tw/~cjlin/libsvm/ ) tool to implement our SVM regression algorithm with the linear kernel and t he features as described in Table 1.
 Evaluation Metric We employ the coefficient of determination 2 R to measure the regression perfor-mance. Coefficient of determination 2 R is used in the context of statistical models with the main purpose to predict the future outcomes on the basis of other related infor-line fits the data well [1 7 ] . 5.2 Experimental Results In this subsection, we present the experiment al result s when we leverage different kinds of features r espectively including textual features, social features and joint features (combine social and textual feat ures ) to perform active learning algorithm on age re-gression. W hat  X  s more, we adopt several comparable experiments during the process of investigatin g the effect of active learning on age regression.

Before reporting the results of active learning, we first investigate the performances of different kinds of features for age regression in a supervised learning setting. Table 2 shows the age regression r esults of fully supervised learning (i.e., all training data is used as labeled data to train the regressor) when different kinds of features are utilized . From this table, we can see that BOW, f ollowing l ist and follower list occupy a large amount of features and perform apparently better than other kinds of textual and social while adding other types of features in social features is more helpful than using f ol-lowing l ist features only. Finally, we can see that t he performance becomes best at 0.5 20
F or thorough comparison, some active learning approaches are implemented includ-ing:  X  Random: which randomly selects the samples from the unla beled data for manual  X  S ubsample -based QBC : which divides labeled samples into several groups and  X  S ubspace -based QBC (Our approach) : which concretes the implementation in 
In our implementation, we run these approaches 5 times and report the average re-features, tex tual features or joint features are utilized individually .

Figure 5 compares our approach with other active learning approaches by varying the number of the selected samples for manually annotation and a ll approaches are per-formed with social features, textual features or joint features respectively. From this figure, we can see that Subsample -based QBC is effective when social features are used. However, when textual and joint features are used, Subsampl e -based QBC per-forms even worse than the random selection approach. Our approach Subspace -based QBC apparently outperforms other two approaches no matter what kind of features is used. Significance test with t -test shows that our approach significantly outperforms the other two approache s ( p -value&lt;0.01) when less than 600 unlabeled samples are selected no matter of features are used.

The number of the feature subspace is an important parameter in our approach. Fig-ure 6 shows the performance of QBC based on subspace with different size of the fea-ture spaces when we utilize textual features, social features or jo i nt features individu-ally. From Fig ure 6, we can see that our approach Subspace -based QBC consistently outperforms the random selection approach when varying the number of feature sub-spaces. For a nice performance, a choice of the number between 6 and 16 is recom-mended to be the size of fea ture subspace. In this paper, we propose an active learning approach to age regression for better ex-ploiting the unlabel ed data to improve the performance. O ur approach leverages three kinds of featur e s, namely textual fe a tures, social features and joint features (combining textual and social fea tures). Moreover, we propose a QBC -style approach to active learning for age regression. In our approach, we solve the unconfidence estimation problem in our regression model by using a committee of feature -subspace regressors. Evaluation shows that our ap proach, namely subspace -based QBC, e f fectively im-proves the performance in active learning.

In our future work, we would like to improve the performance on age regression by exploring more features. Moreover, we would like to apply our approach to active l earning on regression in some other NLP tasks. T his research work has been partially supported by two NSFC grants, No.61375073 and No.61273320, one the State Key Program of National Natural Science of China No.61331011.

