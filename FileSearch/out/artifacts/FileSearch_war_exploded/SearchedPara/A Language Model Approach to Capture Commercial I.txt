 A fundamental task of sponsored search is how to find the best match between web search queries and textual adver-tisements. To address this problem, we explicitly character-ize the criteria for an advertisement to be a  X  X ood match X  to a query from two aspects (it should be relevant with the query from information perspective, and it should be able to capture and satisfy the commercial intent in the query). Correspondingly, we introduce in this paper a mixture lan-guage model of two parts: a commercial model which char-acterizes language bias of commercial intent leveraging on users X  clicks on advertisements, and an informational model which is a traditional language model with consideration of the entropy of each word to capture informational relevance. We then introduce a regularized expectation-maximization (EM) algorithm model for parameters estimation, and inte-grate query commercial intent into the scoring function to boost overall click efficiency.

Empirical evaluation shows that our model achieves bet-ter performance as compared to a well tuned classical lan-guage model and deliberated TFIDF-pLSI model (6% and 5% precision improvement at our operating point in produc-tion environment of 30% recall, and 5.3% and 6.3% AUC improvement), and performs superior to the KL Divergence language model for tail queries (0.5% nDCG improvement). Live traffic test shows over 2% CTR lift and 2.5% RPS lift as well.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Measurement, Performance, Experimentation Online Advertising, Sponsored Search, Language Model, Com-mercial Intent, Relevance
Sponsored search has become a $10 B + business [15] in recent years. In sponsored search, most often, the advertise-ments are selected by  X  X xact match X  between queries and bid phrases. This kind of intuitive methods, though are effective in general situations, do have some obvious drawbacks, such as low coverage for tail queries and not robust in precision. On the other hands, language model, a prominent IR mod-el, though has been successfully applied to many problems and has gained superior performance, faces two challenges when applying to sponsored search: 1) how to characterize and capture commercial intent . In sponsored search, besides informational relevance, an equally, may be more, importan-t objective is to identify and serve users X  commercial intent which may bring in further commercial activities, and 2) how to estimate model parameters reliably .Advertisementsare relatively short, usually just tens of words (in our work we do not take landing pages into consideration because they are usually very noisy and are not available all the time), not sufficient for model estimation.

Firstly we regard an advertisement as a  X  X ood match X  to a user query if it satisfies two criteria: 1) it is relevant with the query from information perspective, and 2) it captures and satisfies the commercial intent (if there is any) in the query. Correspondingly, from the demand perspective, we explicitly model users X  intents in raising a query from two aspects, informational intent and commercial intent. For example when a user has an interest on iPad2. He may raise a query like  X  X Pad2 with Wi-Fi 3G X  to learn some infor-mational knowledge about features such as Wi-Fi and 3G, as well as to learn some commercial characteristics like the price and how to buy, and then leveraging on these informa-tion he may make a decision about whether to take further commercial activities.

Accordingly, from the supply perspective, a  X  X lever X  adver-tiser is expected to deliberately organize his advertisements to satisfy both of the two kinds of intents concurrently to maximize the attraction of the advertisements. In this sense, we design our language model as a mixture model of an in-formational model and a commercial model, in which the informational model is a traditional language model with consideration of the entropy of each word for information-al relevance, while the commercial model characterizes lan-guage bias of commercial intent leveraging on users X  clicks on advertisements. Further more, we explicitly introduce query commercial propensity into the scoring function to boost commercial queries.

Secondly, we leverage on both local and global informa-tion to try to estimate model parameters more reliably. We first utilize global information such as query set and click logs to compute the global word importance in expressing informational intent and commercial intent respectively, and then incorporate them with local information such as term frequency to estimate the informational model and commer-cial model. We then introduce a regularized EM algorithm to refine the parameter estimations together with the model selection probability estimation, which integrates the global and local information better. Empirical evaluation and live traffic performance confirms the effectiveness of our model.
As illustrated in formula (1), in classical language model, we generally use a model  X  which is a distribution over word-s to characterize a people X  X  language habit. For documents retrieval tasks, we usually regard a document D as a sample from  X  , and then use this sample to estimate a model  X  D to approximate  X  (the V in formula (1) is the vocabulary of the language of the document D ). Maximum likelihood es-timation (MLE) is widely used in the estimation, according to which P ( w | D ) is equal to the relative term frequency of w in D as in formula (2). We then assume that the query is generated from some language model as well, and in this sense the likelihood that the query Q is generated from  X  can be used to score D .
For sponsored search, we assume that a person has differ-ent language biases in expressing commercial intent and in-formational intent. The generation procedure for each query word then includes two steps: the user first implicitly selects a language model corresponding to his instant intent (com-mercial or informational), and then select a word within the selected language model. With these assumptions, we define our language model  X  (or  X  D in estimation) as a Bernoul-li mixture model of two multi nomial models, a commercial language model  X  C and an informational language model  X  I
In the more precise form in formula (4),  X  i is the model selection probability with constrain that i  X  X  I,C }  X  i =1.
The model parameters estimation and model selection probability estimation appear in the following sub sections.
We use the commercial model  X  C to characterize an adver-tiser X  X  language bias when he expresses commercial intent. Given a document D ,intuitively  X  C can be estimated by re-garding the commercial portion of D as a sample, which can be formulated as formula (5), in which C denotes  X  the person is under commercial intent  X  or can be equally defined as  X  the commercial model  X  C is selected  X , so P ( w | C, D ) defines the words distribution of the commercial portion of D .
With this definition, challenge comes soon that it is hard to tell the commercial portion of a document from other portions, and thus we can X  X  use MLE to estimate the model directly. Instead, we transform the formula using Bayesian method as formula (6), in which the task is transformed to compute P ( w,C | D ).

Our lines of thinking to estimate P ( w,C | D ) originate from two perspectives, global context perspective and local con-text perspective. We heuristically compute two estimations denoted as P g ( w,C | D )and P l ( w,C | D )fromthetwoper-spectives respectively, and then linearly combine them as the final estimation of P ( w,C | D ).
We estimate P g ( w,C | D ) as formula (7), in which P ( w can be estimated using MLE as in formula (2), and P ( C | refers to  X  given D as a prior sample, and suppose that a per-son raises a word w , the probability that w derives from the commercial intent of the person  X . As illustrated in formu-la (8), we make an independency assumption here that the posterior probability of a person X  X  commercial intent given a word he has raised is a global characteristic of the word (an example of global characteristic is IDF) determined by the global context such as global query set or click feedback set, while has no relationship with the local context in which the word exists (here the local context refers to the documen-t D ). In this way, the problem is transformed to compute P ( C | w ).

We refer to Sandeep X  X  work in estimating advertisability of tail queries [11] to estimate P ( C | w ) utilizing click feed-back. We make two assumptions: 1) P ( C | w ) is uniform for (independent of) users and advertisers, and 2) each instance of click or not click on advertisements served for a query is a referendum of the commercial intent of each word in the query independently. The computation method is as formula (9), in which c ( q ) indicates the number of times users click some advertisements after raising the query q , while n ( q ) indicates the number of times users raise q without clicking any advertisements.

Another thinking line for this problem is to introduce top-ics as mediator as follows.

In formula (10), P ( w | t i )and P ( t i | D ) denotes words dis-tribution on a topic and topics distribution on a document respectively. Both of them can be solved by pLSI [8][7] or L-DA [2] etc., and P ( C | t i ,w ) refers to  X  suppose a person issues aword w to express a topic t i , the probability that w derives from the commercial intent of the person  X . As in formula (11), we make an independency assumption here that the posterior probability of a person X  X  commercial intent given the word he has raised is totally decided by the local con-text in which the word occurs, that is the topic the person is to express using the word, but has no relationship with the global context. In this way, the problem is transformed to compute P ( C | t i ), which boils down to compute P ( t and P ( t i ).

In our implementation, we first train 1000 topics utilizing pLSI (we sample 100 million queries no matter commercial or not from an industrial search engine X  X  query logs in one month, and use the corpus of the top 5 search result pages for each query to train the topics), and then leverage on fold-in algorithm [3] to compute P ( t i | C ) on a set of click-through data of query-advertisement pairs in three months and compute P ( t i ) on the corpus of all advertisements of the three months. Here we regard ( P ( t i | C )) as the topics distri-bution on the corpus of clicked advertisements, in which an advertisement clicked i times will be counted 1 + log i times in occurrence to weight its importance, while regard P ( t as the topics distribution on corpus of all advertisements no matter shown or not.
Besides commercial intent, we want to capture the ordi-nary  X  X elevance X  as well. Generally this can be achieved by a traditional language model such as [12]. In our work we refine the traditional language model to explicitly character-ize  X  X elevance X  from the informational perspective. We use the informational model  X  I to characterize an advertiser X  X  language bias when he expresses informational intent. Sim-ilar with  X  C ,intuitively  X  I can be estimated by regarding the informational portion of D as a sample, which can be formulated as formula (12), in which I denotes  X  the person is under informational intent  X  or can be equally defined as  X  the informational model  X  I is selected  X , and thus P ( w defines the words distribution of the informational portion of D .
We make an assumption that the relative frequency of a word in the informational portion of a document can be approximated by the proportion of the quantity of infor-mation from information theory perspective, and thus we define P ( w | I,D ) as the ratio of information content of word w in D towards the sum of information content of all words in D . In formula (13), ic ( w ) denotes the information con-tent of w , which equals to the term frequency of w in D (that is tf ( w, D )) multiplies the self information of w (that is  X  log P ( w )). We first estimate P ( w )usingMLEonacor-pus of 100 million user queries sampled from a n industrial search engine X  X  query logs, and then use Bayesian smoothing to smooth it with the one estimated on a larger archive of web pages of the search engine.

In this section we present a regularized EM algorithm to estimate the model selection probability and to refine the model parameters estimated in the above two subsections in the mean time.

In classical EM algorithm [5][10], we generally denote the log likelihood of a document D to be generated from a mix-ture model as formula (14), in which  X  j denotes the model selection probability. In order to maximize L ( D ), in E step, EM algorithm estimates the expectation of the complete likelihood denoted as Q (  X  ;  X  o ) in formula (15), in which  X  denotes all parameters while  X  o denotes values of  X  es-timated in the last iteration, and z ( w,j ) here is a hidden variable which denotes the probability that w is generated from model  X  j and boils down to the form in formula (16), and  X  (1  X  j  X  j ) is the Lagrange Multiplier corresponding to the constrain that j  X  j =1. Thenin M step, EM algo-rithm finds the values of  X  which maximize Q (  X  ;  X  o ), that is to find  X  ( o +1) =argmax
Q (  X  ;  X  o )=
The key idea of our regularized EM algorithm is as fol-lows: the model selection probability and the refined model parameters estimations should maximize the likelihood that the document is generated from the mixture model as much as possible, with regularization that the refined estimations should be similar with the original estimations (computed as in subsection 2.1 and 2.2) as much as possible. In this way, we expect to integrate the local information and global information better.

Q R (  X  ;  X  o )=  X Q (  X  ;  X  o )+(1  X   X  )
As illustrated in formula (17), in the regularized EM al-gorithm, we denote the original model parameters estima-tions got from subsections 2.1 and 2.2 as  X  g C and  X  g I tively, and denote the refined parameters to be computed by regularized EM algorithm as  X  C and  X  I corresponding-ly. We then define the regularized expectation of complete likelihood Q R (  X  ;  X  o ) for EM algorithm as a combination of two portions: 1) the expectation of complete data like-lihood Q (  X  ;  X  o ) as in classical EM algorithm, and 2) the KL-Divergence of original estimations towards new estima-tions j D KL (  X  g j  X  j ). In formula (17),  X  ( j  X  j  X  1) and  X  j ( w P ( w |  X  j )  X  1) are Lagrange Multipliers with respect to limitations of j  X  j =1and w P ( w |  X  j )=1.

There are closed form solutions for the regularized EM problem as follows. The estimation of  X  j remains the same as in original EM, while P ( w |  X  j ) reflects the influence from both the document and the original estimations. z ( w,j )is same as in formula (16).
Generally in classical language model we utilize the query likelihood function P ( Q |  X  D ) to score the document, which denotes the likelihood that the query Q is generated from the language model  X  D estimated by document D .
In sponsored search we have a special and critically im-portant demand. We want to serve more advertisements to queries with high commercial intent than queries with low commercial intent. The benefits are double folded: 1) it im-proves user experience by satisfying users who need to reach advertisements to facilitate their commercial activities with-out bothering users who do not want to see advertisements at present, and 2) it increases the number of advertisements candidates going to the second (ranking) stage for valuable queries, which helps to increase the probability to draw user-s X  clicks. In this sense, we integrate query commercial intent into query likelihood function.

P ( Q, C |  X  D )= P ( Q |  X  D ) P ( C | Q,  X  D )= P ( Q |
In formula (20), C denotes  X  user is under commercial in-tent  X , and P ( Q, C |  X  D ) denotes  X  the likelihood that a user raises a query Q from the language model estimated from D and the user is to express his commercial intent  X . Be-cause the commercial intent is only expressed by Q ,itis independent with  X  D . The method to compute P ( C | Q )is same with Sandeep X  X  work of query X  X  advertisability [11] as follows, in which S  X  Q and | S | X  k and k is a truncating parameter to avoid biasing long queries.

In practice, we use the Bayesian transformed function to score advertisements as formula (22).

P ( D,C | Q )= P ( Q, C
Here we interpret P ( Q | D )as P ( Q |  X  D ), which further e-quals to  X  w  X  Q P ( w |  X  D ) tf ( w,Q ) with the query words inde-pendency assumption. For words which exist in query Q without existing in D we assign to them a non-zero prob-ability in the model  X  D as P ( w |  X  D )=  X P ( w ), in which P ( w ) is the global prior probability, and  X  is a documen-t specific parameter which equals to  X  | D | +  X  [14] (  X  is the parameter of Dirichlet smoothing that is widely used in tra-ditional language models). With further assuming P ( Q )=
And thus we get the final scoring function as formula (24).
In this section we first evaluate the effectiveness of our model on editorial data set by comparing with a classical baseline language model, the TFIDF and the TFIDF-pLSI vector space model, as well as the deliberated KLD language model, and then show live traffic performance. Our evalua-tion dataset consists of 87690 query-ad pairs (14000 queries) manually labeled by professional editors.
In this section we compare the performance of mixture language model with that of baseline language model. We evaluate the effectiveness of biasing query commercial intent in scoring function as well. The details are as follows.
Baseline LM : we use Hema Raghavan X  X  refinement [14] of the classical Multinomial language model for sponsored search as baseline. In Hema X  X  evaluation, this baseline per-forms considerately better than the query-rewriting method and TFIDF. In implementation we use P ( D | Q )= P ( Q | D as scoring function and use MLE and Dirichlet smoothing to estimate P ( w | D ) (both unigrams and phrases from a large phrases dictionary are used). We set the smoothing factor  X  to be 0.5 which achieves the best performance in our dataset. P ( D ) derives from the host trust score corresponding to the URL of the advertisements.

Mixture LM : the methods and dataset used to imple-ment our mixture language model are the same as described in section 2 and section 3. The combination coefficient of the P ( w,C | D ) in computing  X  C is 0.6. In the regularized EM algorithm, we re-scaled the Q R (  X  ;  X  o ) to comparable granu-larity with j D KL (  X  g j  X  j )andset  X  to be 0.3. These two parameters are chosen empirically by evaluations. Figure 1: comparison with Baseline LM in PR.

Mixture LM without QC : the only difference with  X  X ix-ture LM X  is that we do not use the commercial queries biased scoring function but use the one same as Baseline LM. Our Mixture LM performs better than Baseline LM. As in Figure 1, the precision improvement when recall is less than 0.4 is in range 5%-9%. At our operating point in product environment (of 0.3 recall) the precision improvement is 6%. On tail portion when recall is above 0.8 there is about 1% precision lift as well. In terms of ROC, from Table 1 we can see that the improvement in AUC is 5 . 3%.

Both the mixture model estimation methods and com-mercial query biased scoring function (QC for abbreviation) contribute to the improvement. However, they contribute in different ways. Mixture model estimation focuses on adver-tisements side and contributes through differentiating terms X  roles in expressing people X  X  intents, while QC focuses on query side and contributes through relatively boosting s-cores for high commercial intent queries which deserve more advertisements.
In this section we compare the performance of mixture language model with that of TFIDF vector space model and TFIDF-pLSI combined vector space model. Before imple-menting language model, we have deployed well tuned T-FIDF and TFIDF-pLSI combined model in production whose performance are very promising. The details of candidates are as follows.

TFIDF : we elaborately refined the classical TFIDF mod-el [16] for evaluation. We introduced term importance t leveraging on click feedback and tuned different weights z for different zones (title, description and bid phrases) by sim-ulated annealing. The advertisements side feature weight for term w is then t (1 + log( that of query side is similar. The matching score is cosine similarity. Both unigrams and phrases are used.

TFIDF-pLSI : in order to integrate semantic benefits in-to matching, we combined TFIDF with pLSI topic model linearly. We trained 1000 topics and utilize fold-in algo-rithm [8][3] to compute P ( t i | Q )and P ( t i | D ),andthenuse cosine similarity for scoring. We elaborately adjusted the combination coefficient to reach the optimal performance.
Mixture LM performs superior than TFIDF and TFIDF-Figure 2: comparison with TFIDF and pLSI in PR.
 pLSI combination. As illustrated in Figure 2, for recall in range of 0.1 and 0.8 the precision lift is between 4% and 12%. Particularly for recall in the area we are the most interested in (near 0.3) for production deployment, the precision im-provement is about 5%. In terms of ROC, from Table 1 we can see that the improvements in AUC are 6.1% and 5.0%.
The improvement on tail portion is some more significan-t than that on head portion. The reasons are two-folded. First, TFIDF does have good discrimination ability on head portion. Our investigation has confirmed that it performs better than Baseline LM when recall is low. The integra-tion of term importance derived from click feedback further sharpens it X  X  performance in this area. Second, language model is more robust. Its scores distribution is more con-centrated than TFIDF. Even Baseline LM performs better TFIDF when recall is in middle and tail area. Mixture LM strengthens this advantage further.

Besides, we find that semantic match does help to improve syntactic match. Combined with pLSI, TFIDF gains about 1% precision lift in our interested recall range.
In evaluation, we differentiated hot and tail queries ac-cording to whether they have at least three  X  X elevant ad-vertisements X  in the logs of an industrial search engine in one month. An advertisement is regarded as a  X  X elevant ad-vertisement X  for a query if the absolute click number of the query-advertisement pair exceeds a threshold l 1 and the ex-pected click number predicted by click modeling exceeds a threshold l 2 . With these limitations, there are about 9000 queries regarded as  X  X ot queries X  and the left 5000 queries are  X  X ail queries X . We use w P ( w |  X  Q )log P ( w |  X  D ing. For hot queries, we integrated  X  X elevant advertisements X  into query models estimation. The combination coefficient  X  of background prior P ( w | C ) is 0.05 in EM estimation.
We first retrieve the top n advertisements for each query from the two models and then evaluate precision and recall on different n , which comes out that the overall precision of KLD LM is slightly better than Mixture LM, about 0.5% overall. However, as illustrated in Table 2, the performance on nDCG is opposite, in which Mixture LM is about 0.5% better than KLD LM. These results show that these two models are comparable for hot queries generally. KLD LM is somewhat better at telling the good from the bad, while Mixture LM is somewhat better at distinguishing the best from the fair. For tail queries, Mixture LM is 2%-4% better than KLD LM. Considering the overlap of advertisements in feedback collection and editorial dataset which may have boosted the performance of KLD LM and the volume of tail queries, we think Mixture LM is superior for our advertise-ments retrieval tasks.
Besides the offline evaluations, in order to compare the online performance of different models, we split the live search traffic into different  X  X uckets X , a Baseline LM buck-et, a TFIDF-pLSI bucket and a Mixture LM bucket. Users in each bucket are served by advertisements retrieved by the corresponding model in addition to other matching ap-proaches already existing in the system such as query rewrit-ing, advertisements expansion and so forth. As such, the CTR (click through rate) and RPS (revenue per search) of the Mixture LM bucket is 3.0% and 3.6% higher than that of TFIDF-pLSI bucket; and the improvements are 2.0% and 2.5% respectively comparing with the Baseline LM.
The fundamental work on advertisments selection of spon-sored search mainly focus on query expansion [13] or bid ter-m generation [15]. Besides IR technologies, some machine learning approaches are exploited as well, such as Dustin X  X  relevance model [6] and Broder X  X   X  X earn when to advertise X  model [4]. In recent years, some work pay attention to the commercial nature of online advertising, such as Pandy X  X  work in estimating advertisability of tail queries [11] and Ashkan X  X  work on queries X  commercial intent [1].

Early work of language model mainly remain in the family of query-likelihood scoring [12]. As a generalization, the KL-divergence language model [18] which supports feedback integration through estimating a query language model is regarded as a  X  X tate-of-the art X  approach [17]. Flourishing works in query language model estimation include Zhai X  X  mixture model feedback method [18] and Croft X  X  relevance model [9] etc. Very recently, Raghavan [14] introduced the classical language model into searching advertising.
In this paper we focus on characterizing and capturing the relevance and commercial intent in sponsored search. We de-velop a mixture language model of a commercial model and an informational model for this purpose. The evaluation on both editorial data and live traffic confirm the effectiveness of our model. For future work, we are still investigating the generalization of our work to content match advertising as well as some personalization areas such as sentimental analysis and news recommendation. This work is sponsored by Yahoo! Global R&amp;D Center, Beijing. [1] A. Ashkan and C. L. Clarke. Term-based commercial [2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [3] T. Brants and F. Chen. Topic-based document [4] A. Broder, M. Ciaramita, and M. Fontoura. To swing [5] A. P. Dempster and N. M. Laird. Maximum likelihood [6] D. Hillard and S. Schroedl. Improving ad relevance in [7] T. Hofmann. Probabilistic latent semantic analysis. In [8] T. Hofmann. Probabilistic latent semantic indexing. In [9] V. Lavrenko and W. B. Crof. Relevance-based [10] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [11] S. Pandey and K. Punera. Estimating advertisability [12] J. M. Ponte and W. B. Croft. A language modeling [13] F. Radlinski, A. Broder, and P. C. etc. Optimizing [14] H. Raghavan and R. Iyer. Probabilistic first pass [15] S. Ravi, A. Broder, and E. Gabrilovich. Automatic [16] G. Salton and C. Buckley. Term weighting approaches [17] C. Zhai. Statistical language models for information [18] J. Zhai and J. Lafferty. Model-based feedback in the
