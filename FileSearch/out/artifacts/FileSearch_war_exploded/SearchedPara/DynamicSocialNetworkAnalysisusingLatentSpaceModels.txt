 This pap er explores two asp ects of social net work mo deling. First, we generalize a successful static mo del of relationships into a dynamic mo del that accoun ts for friendships drifting over time. Second, we sho w how to mak e it tractable to learn suc h mo dels from data, even as the num ber of entities n gets large. The generalized mo del asso ciates eac h entity with a point in p -dimensional Euclidean laten t space. The points can move as time progresses but large moves in laten t space are improbable. Observ ed links between entities are more likely if the entities are close in laten t space. We sho w how to mak e suc h a mo del tractable (sub-quadratic in the num-ber of entities) by the use of appropriate kernel functions for similarit y in laten t space; the use of low dimensional KD-trees; a new ecien t dynamic adaptation of multidi-mensional scaling for a rst pass of appro ximate pro jection of entities into laten t space; and an ecien t conjugate gra-dien t update rule for non-linear local optimization in whic h amortized time per entity during an update is O (log n ). We use both syn thetic and real-w orld data on up to 11,000 enti-ties whic h indicate near-linear scaling in computation time and impro ved performance over four alternativ e approac hes. We also illustrate the system operating on twelve years of NIPS co-authorship data. Social net work analysis is becoming increasingly imp ortan t in man y elds besides sociology , including intelligence anal-ysis [1], mark eting [2] and recommender systems [3]. Here we consider learning in systems in whic h relationships drift over time. In 2002, Raftery et al[4] introduced a mo del similar to Multidimensional Scaling in whic h entities are as-sociated with locations in p -dimensional space, and links are more likely if the entities are close in laten t space. In this pap er we form ulate an extension of this static mo del that allo ws link prediction and visualization in a dynamic set-ting. Unlik e most of the existing mo dels our work tak es the sequen tial asp ect of the data into accoun t.
 A friendship graph is one in whic h the nodes are entities and two entities are link ed if and only if they have been observ ed to collab orate in some way. We try to embed an evolving friendship graph in a p dimensional laten t space. We exp ect that with time entities will come closer together forming new groups, or moving away from one another. These kind of changes in interaction patterns are very common in social net works, where people move in and out of neigh borho ods forming new friend circles. This mo del will also help us pre-dict whether two entities will form a connection at timestep t , given the kind of relation they had over the past timesteps. At rst sigh t a much simpler algorithm migh t seem prefer-able: predict x and y are link ed at time t if and only if they were link ed at time t 1. But in man y cases even without having been link ed at all, two entities can be very close to one another because of common friends or friends of friends. In this pap er we supp ose that eac h observ ed link is asso ci-ated with a discrete timestep, so eac h timestep pro duces its own graph of observ ed links, and information is preserv ed between timesteps by two assumptions. First we assume en-tities can move in laten t space between timesteps, but large moves are improbable. Second, we mak e a standard Mark ov assumption that laten t locations at time t + 1 are condi-tionally indep enden t of all previous locations given laten t locations at time t and that the observ ed graph at time t is conditionally indep enden t of all other positions and graphs, given the locations at time t (see Figure 1). This is the same assumption as in HMMs and Kalman Filters.
 Let G t be the graph of observ ed pairwise links at time t . Assuming n entities, and a p -dimensional laten t space, let X t be an n p matrix in whic h the i th row, called x i , corre-sponds to the laten t position of entity i at time t . Our con-ditional indep endence structure is sho wn in Figure 1. For most of this pap er we treat the problem as a trac king prob-lem in whic h we estimate X t at eac h timestep as a function of the curren t observ ed graph G t and the previously esti-mated positions X t 1 . We want if we put a uniform prior on X t . In Section 2 we design mo dels of P ( G t j X t ) and P ( X t j X t 1 ) that meet our mo del-ing needs and whic h have learning times that are tractable as n gets large. In Sections 3 and 4 we introduce a two-stage pro cedure for locally optimizing equation (1). The rst stage generalizes linear multidimensional scaling algorithms to the dynamic case while carefully main taining the abil-ity to computationally exploit sparsit y in the graph. This gives an appro ximate estimate of X t . The second stage re-nes this estimate using an augmen ted conjugate gradien t approac h in whic h gradien t updates can use KD-trees over laten t space to allo w O ( n log n ) computation per step. Figure 1: Mo del through time Let d ij = j x i x j j be the Euclidean distance between entities i and j in laten t space at time t . We will not use a t subscript on these variables except where it is needed for clarit y. We denote link age at time t by i j , and absence of a link by i 6 j . p ( i j ) denotes the probabilit y of observing the link. We use p ( i j ) and p ij interc hangeably . Follo wing [4] the probabilit y of a link between i and j , de-noted as p L ij is where is a constan t whose signi cance is explained shortly . P ( G t j X t ) is then simply The likeliho od score function intuitiv ely measures how well the mo del explains pairs of entities who are actually con-nected in the training graph as well as those that are not. So far this mo del is similar to [4]. To extend this mo del to the dynamic case, we now mak e two imp ortan t alterations. First, we allo w entities to vary their sociabilit y. Some enti-ties participate in man y links while others are in few. We give eac h entity a radius , whic h will be used as a sphere of interaction within laten t space. We denote entity i 's radius as r i .
 We introduce the term r ij to replace in Equation (2). r ij is the maxim um of the radii of i and j . Intuitiv ely, an entity with higher degree will have a larger radius. Thus we de ne the radius of entity i with degree i as c ( i + 1) so that r is c ( max ( i ; j ) + 1), and c will be estimated from the data. In practice, we estimate the constan t c by a simple line-searc h on the score function. The constan t 1 ensures a nonzero radius.
 The second alteration is to weigh the link probabilities by a kernel function. We alter the simple logistic link prob-abilit y p L ij , suc h that two entities have high probabilit y of link age only if their laten t coordinates are within radius r of one another. Bey ond this range there is a constan t noise probabilit y of link age. For later optimization we will need the kernelized function to be con tinuous and di eren tiable at d ij = r ij . Thus we pick the biquadratic kernel. Using this function we rede ne our link probabilit y as This is equiv alen t to having, We plot this function in Figure 2B.
 Thus the full expression of the rst part of the mo del log-likeliho od is given by, = X = X + #( i j;d ij &gt; r ij ) log + #( i 6 j;d ij &gt; r ij ) log(1 ) where #(expression) denotes the num ber of pairs satisfying the expression. The second part of the score penalizes large displacemen ts from the previous time step. We use the most obvious Gaus-sian mo del: eac h coordinate of eac h laten t position is inde-penden tly sub jected to a Gaussian perturbation with mean 0 and variance 2 . Thus log P ( X t j X t 1 ) = Here we are trying to optimize the log-lik eliho od of the graphs G 1: t , conditioned on the laten t positions X 1: t t T , T being the total num ber of timesteps. This is a forw ard inference, since we constrain the positions on eac h timestep to be similar to the last time-step only . However we also presen t a global optimization of all time-steps together in section 5.4. We generalize classical multidimensional scaling (MDS) [5] to get an initial estimate of the positions in the laten t space. We begin by recapping what MDS does. It tak es as input an n n matrix of non-negativ e distances D where D i;j denotes the target distance between entity i and entity j . It pro duces an n p matrix X where the i th row is the position of entity i in p -dimensional laten t space. Let the coordinates of n points in a p dimensional Euclidean space be given by x i ; ( i = 1 : n ) where x i = ( x i 1 ;::::;x X denote the unkno wn coordinate matrix. The Euclidean distance between points i and j is given by Also let ~ D denote XX T , suc h that If X is cen tered, then we can write From equations 6 and 7 it follo ws that, Substituting a ij for 1 2 d 2 ij we have a new matrix A , and Thus we have , where H is the idemp oten t cen tering matrix, suc h that However we do not kno w the true distance matrix D or the resulting similarit y matrix ~ D . Therefore classical MDS works with the dissimilarit y matrix D obtained from the data [6]. ~ D is the similarit y matrix obtained from D using Let be the matrix of the eigen vectors of ~ D , and be a diagonal matrix with the corresp onding eigen values. Denote the matrix of the rst p positiv e eigen values by p and the corresp onding columns of by p . From this follo ws the expression of classical MDS, i.e. X = p 1 2 p . MDS nds where jj F denotes the Frob enius norm [6].
 Tw o questions remain. Firstly , what should be our target distance matrix D ? Secondly , how should this be extended to accoun t for time? The rst answ er follo ws from [4] and de nes D ij as length of the shortest path, nhops ij from i to j in graph G . We restrict this length to a maxim um of three hops in order to avoid the full n 2 computation of all-shortest paths. Thus D is a dense mostly constan t matrix, suc h that, In spite of having this linear appro ximation step as an ini-tialization to the nonlinear optimization describ ed in Section 4, we want to accoun t for the temp oral asp ect, so that we begin with a educated guess. Therefore we do not want the positions of entities to change drastically from one time step to another. Hence we try to minimize j X t X t 1 j F along with the main objectiv e of MDS. Let ~ D t denote the ~ D matrix deriv ed from the graph at time t . We form ulate the above problem as minimization of j ~ D t X t X T t j F + j X t X where is a parameter whic h con trols the imp ortance of the two parts of the objectiv e function. The above does not have a closed form solution. However, by constraining the objectiv e function further, we can obtain a closed form so-lution for a closely related problem. The idea is to work with the distances and not the positions themselv es. Since we are learning the positions from distances, we change our constrain t (during this linear stage of learning) to encour-age the pairwise distance between all pairs of entities to change little between eac h time step, instead of encouraging the individual coordinates to change little. Hence the new function we try to minimize is given by Minimizing Equation 10 is equiv alen t to minimizing the trace of ( ~ = Constan t w.r.t X t + (1 + )( X t X T t ~ D t + X t 1 X The trace of Equation (11) is minimized at an ane com bi-nation of the curren t information from the graph, and the coordinates at the last timestep. Namely , the new solution satis es, We plot the trace of the two constrain t functions in Fig-ure 2A.The steep er surface belongs to our second constrain t. It can be seen that this new function has two minima, namely X t = + X t 1 whereas the rst one is much more at, and has an unique minima at X t 1 . An eigendecomp osition of the righ t hand side of the solution, as in Equation 12, mini-mizes the objectiv e function. We shall explain the role of , by varying it between two extreme values. When is zero, X
X T t equals ~ D t , and we ignore all information except the curren t graph. When !1 , X t X T t equals X t 1 X T t 1 and we are entirely concerned with keeping entities stationary in the laten t space. So works like a forgetting factor. We now have a metho d whic h nds laten t coordinates for time t that are consisten t with G t and have similar pair-wise distances as X t 1 . But although all pairwise distances may be similar, the coordinates may be very di eren t. In-deed, even if is very large and we only care about pre-serving distances, the resulting X t may be any re ection, rotation or translation of the original X t 1 . We solv e this by applying the Procruste an transform to the solution X t Equation 12. This transform nds the linear area-preserving transformation of X t that brings it closest to the previous con guration X t 1 . The solution is unique if X T t X t 1 nonsingular [7], and for zero cen tered X t and X t 1 , is given by X t = X t UV T , where X T t X t 1 = USV T , using con ven-tional notation for Singular Value Decomp osition(SVD). Before moving onto stage two's nonlinear optimization we must address the scalabilit y of stage one. The naiv e imple-men tation (SVD of the matrix from equation 12) has a cost of O ( n 3 ), for n nodes, since both ~ D t , and X t X T n n matrices. We use the power metho d [8], to nd the p eigen vectors and values. The naiv e implemen tation of this is O ( n 2 ) for n nodes, since both ~ D t and X t X T t are dense n n matrices. However with some care its possible to exploit the dense mostly constan t structure of the distance matrix D obtained from the graph. The power metho d involves the multiplication of a matrix with a vector iterativ ely, till it con verges to the rst eigen value-v ector pair. Cho osing the next starting vector to be perp endicular to the rst eigen-vector, we nd the second eigen value-v ector pair too. The beaut y of this metho d is that, in case the underlying ma-trix is sparse, the matrix-v ector multiplication involves only n 2 f computation per iteration, where f is the fraction of nonzero entries in the matrix. We brie y sketch the equa-tions to sho w how we avoid O ( n 2 ) computation despite the fact that our matrices are dense. The heart of power metho d is computing v 0 = ~ Dv , where ~ D is de ned in Equation(9). Thus eac h iteration involves computing n entries of v 0 . We rst represen t D 0 = D 2 t as a linear com bination between a dense fully constan t matrix D 0 d and a sparse matrix D All the entries of D 0 d have the constan t value c 0 = c shall denote eac h entry of D 0 d ,and D 0 s by dd ij , and ds resp ectiv ely. For simplifying the notation we drop the sux t for the time being. We also denote an elemen t-wise square by D 2 in this section.
X The complexit y of calculating the above, is as follo ws: the rst part requires O ( n 2 f ) time in one iteration of power metho d, since D 0 s is sparse; The row, column, and overall means i.e. ds j , ds i , and ds need to be computed once, with a cost O ( n 2 f ) overall . Once they are computed, the rest of the terms of Equation(13) tak e O ( n ) time per itera-tion of the power metho d. Also, we don't create the X t X matrix. Since we use the power metho d, in one iteration the computation comes down to, The above has a time complexit y of O ( pn ), where p is the num ber of dimensions of the laten t space. Thus the net cost of power-metho d is O ( n 2 f + n + pn ) per iteration. Stage one nds reasonably consisten t locations for entities whic h t our intuition, but it is not tied in any way to the probabilistic mo del from Section 2. Stage two uses this rea-sonable initial guess as a starting point and then applies nonlinear optimization directly to the mo del in Equation 1. We use conjugate gradien t (CG) whic h was the most e ec-tive of sev eral alternativ es attempted. The most imp ortan t practical question is how to mak e these gradien t compu-tations tractable, esp ecially when the mo del likeliho od in-volves a double sum over all entities.
 We must compute the partial deriv ativ es of logP ( G t j X logP ( X t j X t 1 ) with resp ect to all values x i;k ;t and k 2 1 ::p . First consider the P ( G t j X t ) term: @ log P ( G t j X t ) However K , the biquadratic kernel introduced in Equation 3, evaluates to zero and has a zero deriv ativ e when d ij Plugging this information in (14), we have, Equation (14) now becomes This simpli cation is very imp ortan t because we can now use a spatial data structure suc h as a KD-tree in the low dimensional laten t space to retriev e all pairs of entities that lie within eac h other's radius in time O ( rn + n log n ) where r is the average num ber of in-radius neigh bors of an entity [9; 10]. The computation of the gradien t involves only those pairs. A sligh tly more sophisticated tric k lets us compute log P ( G t j X t ), in O ( rn + n log n ) time.
 From equation(4), we have In the early stages of Conjugate Gradien t, there is a danger of a plateau in our score function in whic h our rst deriv ativ e is insensitiv e to two entities that are connected, but are not within eac h other's radius. To aid the early steps of CG, we add an additional term to the score function, whic h penalizes all pairs of connected entities according to the square of their separation in laten t space , i.e. P i j d 2 ij .
 Weigh ting this by a constan t pConst , our nal CG gradien t is Here we give the gures from the di eren t timesteps through our algorithm, on a sim ulated dataset consisting of 10 enti-ties. The true mo del represen ts the actual spatial positions of the entities, from whic h the links were generated using our probabilistic mo del. Figures 3(A) and (C) sho w the true mo del at the rst and second timestep. Figure 3(B) gives the result of the entire algorithm i.e. our MDS with = 0, i.e. classical MDS on the data in (A) follo wed by conjugate gradien t. Through gures 3(D), (E), and (F) we motiv ate the time-v arian t MDS, Pro crustean transform and nally the conjugate gradien t step in our algorithm. Figure (D) sho ws the result of time-v arian t MDS with = 10, on the data in (C), and the coordinates learned in (B). Note that though entity ody is no more connected to amy in timestep 2, it is not placed far apart from the latter, since they were connected in the former timestep. This sho ws how the initial MDS step tak es into consideration the coor-dinates from the last timestep as well. Now look at (E). It is obtained by applying Pro crustean transform on the co-ordinates from (D), so that it aligns as closely as possible to the coordinates in (B). Careful observ ation rev eals that this step in this certain case just has rotated the coordi-nates in step (D) to have the same orien tation as in (B). This demonstrates the necessit y of using the Pro crustean transform. As MDS deals directly with distances the re-sulting con guration can be rotated without a ecting the mutual distances between the entities. The nal gure is (F), whic h gives the result of applying conjugate gradien t on step (E), and also the re-estimated radii from the cur-ren t time-step. It is eviden t from the gures that for these small num ber of entities conjugate gradien t after MDS does not yield much impro vemen t. However for larger num ber of entities MDS helps only as a very educated guess to ini-tialize conjugate gradien t, whic h subsequen tly mak es very signi can t impro vemen ts. We rep ort exp erimen ts on syn thetic data generated by a mo del describ ed below and the NIPS co-authorship data [11], and some large subsets of citeseer. We investigate three things: abilit y of the algorithm to reconstruct the laten t space based only on link observ ations, anecdotal evaluation of what happ ens to the NIPS data, and scalabilit y. We generate syn thetic data for six consecutiv e timesteps. At eac h timestep the next set of two-dimensional laten t coordi-nates are generated with the former positions as mean, and a gaussian noise of standard deviation = 0 : 01. Eac h entity is assigned a random radius. At eac h step , eac h entity is link ed with a relativ ely higher probabilit y to the ones falling within its radius, or con taining it within their radii. There is a noise probabilit y of 0 : 1, by whic h any two entities i and j outside the maxim um pairwise radii r ij are connected. We generate graphs of sizes 20 to 1280, doubling the size every time. Accuracy is measured by dra wing a test set from the same mo del, and determining the ROC curv e for predicting whether a pair of entities will be link ed in the test set. We exp erimen t with six approac hes:
A. The True mo del that was used to generate the data B. The DSNL mo del learned using the above algorithms.
C. A random mo del, guessing link probabilities randomly
D. The Simple Counting mo del (Con trol Exp erimen t). This
E. Time-v arying MDS: The mo del that results from run-
F. MDS with no time: The mo del that results from ignor-Figure 4 sho ws the ROC curv es for the third timestep on a test set of size 160. Table 1 sho ws the AUC scores of our approac h and the ve alternativ es for 3 di eren t sizes of the dataset over the rst, third, and last time steps. As we can see, from the gures in all the cases, the true mo del has the highest AUC score, follo wed by the mo del learned by DSNL. The ROC curv e of the simple coun ting mo del goes up very steeply , since it righ tly guesses some of the links in the test graph from the training graph. How-ever it also predicts the noise as links, and ends up being beaten by the mo del we learn. The results sho w that it is not sucien t to only perform Stage One. When the num-ber of links is small, MDS without time does poorly com-pared to our temp oral version. However as the num ber of links gro ws quadratically with the num ber of entities, regu-lar MDS does almost as well as the temp oral version: this is not a surprise because the generalization bene t from the previous timestep becomes unnecessary with sucien t data on the curren t timestep. In this section we sho w how we selected the value of . We vary from 0.0 to 90.0, in steps of 10.0 and plot the AUC scores of the mo dels resulting from time-v arying MDS in Figure 5.W e rep eat this exp erimen t for mo del sizes 40, 80, 160, and 320. All the exp erimen ts sho w that AUC score of the resulting mo del from the MDS step gro ws better as increases from 0 to around 10 or 20. However after that the AUC scores start decreasing. This sho ws the two extreme simple predictiv e mo dels whic h could be used. One is the time invarian t classical MDS, when = 0. The other ex-treme is when is in nite, i.e. we only use the coordinates learned from the former timestep for predicting the beha vior of the curren t timestep, ignoring information from curren t time step.W e choose = 10 from the empirical results pre-sen ted in Figure 5 .We choose a bigger value of = 30 for Figure 4: ROC curv es of the six di eren t mo dels describ ed earlier for test set of size 160 at timestep 3, in sim ulated data. the massiv e datasets. In this section we try to see how e ectiv e our initialization step is. We examine the AUC scores of two di eren t mo dels. One starts with the MDS step , and the other starts with the random initialization. We also give the total time tak en for both the variations to con verge.
 First we look at the starting log likeliho od of the two proba-bilistic mo dels with di eren t initializations. We will refer to the metho d with MDS initialization as metho d 1, and the one with random initialization as metho d 2. From the ta-bles 2 and 3 we see that metho d 1 starts with a much higher log-lik eliho od. Thus MDS is a much more educated guess than the random initialization. It ends with a better log-likeliho od also. This di erence between the log-lik eliho ods of the nal mo dels increases with the complexit y of the mo del, i.e. the num ber of entities. However the AUC scores on the test set generated from the true generativ e mo del is not very di eren t for the mo dels learned by metho ds 1 and 2. Table 1: AUC score on graphs of size n for six di eren t mo d-els (A) True (B) Mo del learned using DSNL, (C) Random Mo del, (D) Simple Coun ting Mo del, (E) Multidimensional Scaling with time, and (F) MDS without time.
 Another very striking observ ation is the running time(in sec-onds) of these two metho ds per time-step. Metho d 1 tak es almost half as much time as metho d 2. So we conclude that conjugate gradien t follo wed by MDS con verges almost twice as fast as the metho d with random initialization , and it con verges to a solution with higher log-lik eliho od. The transition mo del presen ted in section 2.2 was a forw ard metho d of nding positions, given the past coordinates, and the curren t coordinates. Now we also compare the perfor-mance of a mo del with a forw ard bac kward metho d with the forw ard one. In this new metho d, we try to maximize the posterior probabilities of data of all time-steps. The P ( G t j X t ) part remains the same, though the second part incurs an extra term, whic h penalizes any shift of the curren t coordinates w.r.t. the next time-step as well. The additional term is P n i =1 j X i;t X i;t +1 j 2 = 2 2 . We rst nd the positions for eac h timestep using our time varian t exten-sion of classical MDS. After this batc h initialization of all timesteps, we try to learn the coordinates at time t so that the resulting mo del explains the curren t graph, and is con-strained to be very close to the coordinates of the mo dels of the former and later timesteps.
 We compare these two metho ds by presen ting their resp ec-tive training, and test log-lik eliho ods, along with the AUC scores in table 4. We see that the log-lik eliho od of the Table 2: Log-lik eliho od, and time tak en per iteration of Con-jugate gradien t with MDS initialization
Time log-lik eliho od log-lik eliho od AUC Total mo del from the forw ard-bac kward metho d gets better than the forw ard-only metho d as we increase the num ber of en-tities. However the AUC score on unseen data is almost comparable for these two metho ds. In this section we presen t a subset of the NIPS dataset, ob-tained by choosing a well-connected author, and including all authors and links within a few hops. We dropp ed authors who app eared only once and we merged the timesteps into three groups: 1987-1990 (Figure 6A), 1991-1994(Figure 6B), and 1995-1998(Figure 6C). In eac h picture we sho w the links for that timestep and highligh t a few well connected people with their radii. These radii are learned from the mo del. Remem ber that the distance between two people is related to the radii. Tw o people with very small radii are consid-ered far apart in the mo del even if they are physically close. To give some intuition of the movemen t of the rest of the points, we divided the area in the rst timestep in 4 parts and used di eren t colors and shap es for the points in eac h. This coloring is preserv ed throughout all the timesteps. In this pap er we limit ourselv es to anecdotal examination of the laten t positions. For example, with Burges C and V apnik V we see that they had very small radii in the rst four years, and were further apart from one another, since there was no co-publication. However in the second timestep they move closer, though there are no direct links. This is because of the fact that they both had co-published with neigh bors of one another. On the third time step they mak e a connection, and are assigned almost iden tical coordinates, since they have a very overlapping set of neigh bors. We end the discussion with entities Hinton G , Ghahr amani and Jordan M . In the rst timestep they did not coauthor with one another, and were placed outside one-another's radii. In the second timestep Ghahr amani Z , and Hinton G coauthor with Jordan M . However since Hinton G had a large radius and more links than the former, it is harder for Table 3: Log-lik eliho od, and time per iteration of Conjugate gradien t with random initialization
Time log-lik eliho od log-lik eliho od AUC Total time step after after per him to meet all the constrain ts, and he doesn't move very close to Jordan M . In the next timestep however Ghahr amani has a link with both of the others, and they move substan-tially closer to one another. It is possible and often useful to have more than two laten t dimensions. For visualization we can only utilize up to three dimensions. But it migh t be necessary to have the entities in a higher dimensional laten t space for impro ved link pre-diction. For the NIPS corpus we exp erimen ted on learning mo dels with 1 to 3 dimensional laten t space. Based on the results presen ted here, we decided to use 2 dimensions. We split the NIPS dataset randomly in di eren t sized test, and training sets. We trained 3 di eren t mo dels with 1,2 and 3 laten t space dimensions and computed the AUC scores for predicting the links in the test data.
 We presen t the AUC scores of the datasets for the 4 : 1, and 3 : 2 ratio cuts of training and test set in table 5. As the ratio goes up the AUC scores go down in general. This is exp ected as we are increasing the test set size w.r.t the training set size, thus constraining the training phase to less evidence of connections. We see that in both cases the 2-dimensional mo del performs at least as good as the 3 dimensional mo del in terms of prediction. This is why we chose 2 dimensions for visualizing the NIPS dataset. We successfully applied our algorithms to net works of sizes up to 11,000. These graphs were extracted from the cite-seer dataset. We will only men tion the performance on two datasets. The rst one con tained 9364 entities from citeseer, publication years spanning over 6 years : 1998 to 2003. We split up eac h year's links in a 9:1 train:test set partition. The computation took around three and half hours. The AUC scores for the six timesteps ranged from 0.78 to 0.86. The second dataset with 11,218 entities was more densely con-tak en for score calculation vs num ber of entities. nected. It had four years of information (1998-2001). This took around four and half hours, and the AUC scores under similar setting as before ranged from 0.80 to 0.92. Figure 6D sho ws the performance against the num ber of en-tities. When KD-trees are used and the graphs are sparse scaling is clearly sub-quadratic and nearly linear in the num-ber of entities, meeting our exp ectation of O ( n log n ) perfor-mance. Similar exp erimen ts for gradien t computation also sho w that its complexit y is O ( n log n ). This pap er has describ ed a metho d for mo deling relation-ships that change over time. We believ e it is useful both for understanding relationships in a large mass of historical data and also as a tool for predicting future interactions, and we plan to explore both directions further. This pa-per presen ted both a forw ard pass and a forw ard-bac kward algorithm on the Mark ov chain over timesteps. In the sec-ond technique we optimize the global likeliho od instead of treating the mo del as a trac king mo del. We also plan to ex-tend this in order to nd the posterior distributions of the coordinates follo wing the approac h used in the static case by [4].
 Table 4: Training and Test log-lik eliho od, and AUC scores for forw ard-bac kward vs forw ard metho d Time training test AUC training test AUC step LogLik e LogLik e LogLik e LogLik e 1 -622.30 -688.91 .87 -694.978 -790.62 .87 2 -633.46 -705.31 .87 -691.775 -763.30 .88 1 -2257.9 -2497.05 .85 -2547.31 -3047.48 .84 2 -2322.26 -2459.13 .85 -2710.72 -2881.05 .84 1 -9384.79 -9850.77 .84 -11592.2 -12623.2 .82 2 -9473.83 -9985.53 .83 -11849.7 -12795.7 .83 1 -45189.6 -46462.7 .81 -64153.1 -68754 .79 2 -44126.3 -46756.5 .81 -62573.3 -67271.8 .79 Table 5: AUC scores on the NIPS dataset over 3 timesteps for di eren t num ber of dimensions We are very grateful to Anna Golden berg for her valuable insigh ts. We also thank Paul Komarek for some helpful dis-cussions. [1] J. Schro eder, J. J. Xu, and H. Chen. Crimelink explorer: [2] J. J. Carrasco, D. C. Fain, K. J. Lang, and L. Zhukov. [3] J. Palau, M. Mon taner, and B. Lopez. Collab oration [4] A. E. Raftery , M. S. Handco ck, and P. D. Ho . Laten t [5] R. L. Breiger, S. A. Boorman, and P. Arabie. An al-[6] I. Borg and P. Gro enen. Modern Multidimensional Scal-[7] R. Sibson. Studies in the robustness of multidimen-[8] David S. Watkins. Fundamentals of Matrix Computa-[9] F. Preparata and M. Shamos. Computational Geome-[10] A. G. Gra y and A. W. Mo ore. N-b ody problems in sta-[11] R. Salakh utdino v, S. T. Roweis, and Z. Ghahramani.
