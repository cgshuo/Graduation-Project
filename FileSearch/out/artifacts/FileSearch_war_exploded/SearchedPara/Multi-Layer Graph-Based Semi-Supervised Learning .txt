 Semi-supervised learning is to exploit the vast amount of unlabeled data in the world. This paper proposes a scalable graph-based technique leveraging the distributed comput-ing power of the MapReduce programming model. For a higher quality of learning, the paper also presents a multi-layer learning structure to unify both visual and textual in-formation of image data during the learning process. Ex-perimental results show the effectiveness of the proposed methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithms, Experimentation Image Retrieval, MapReduce, Semi-Supervised Learning
Semi-supervised learning (SSL) is to automatically exploit lots of unlabeled data in the presence of a small amount of labeled data. Among the SSL methods, graph-based meth-ods are quite popular where both labeled and unlabeled data are modeled as vertices followed by adding a weighted edge between any two vertices. This paper focuses on image data, where most previous works concentrated on the visual infor-mation of image contents only and then weighted each edge with the visual similarity between two image data (vertices). Previous work in [5] indicated that it is desirable to consider both visual and textual information for the learning quality improvement. As a result, (ad-hoc) early-and late-fusion methods, which fuse visual and textual information before or after learning, were proposed accordingly. On the other hand, learning on a large-scale dataset poses new challenges for the graph construction and the learning process, while most existing methods scale poorly with the data size [3]. In summary, this paper has three main contributions: (1) we propose a multi-layer learning structure to fuse image graphs seamlessly, (2) we combine distributed computation and a the textual graph. Take the visual graph as an example. As shown in the top layer, an inferring process is first activated to propagate the label information (e.g., a value) from each labeled vertex to its adjacent vertices. More precisely, the map procedure weights the (label) value on each labeled ver-tex while the reduce procedure collects the weighted value(s) for the unlabeled vertices. After the inferring processes, the fusion process is activated. The fusion process fuses every two vertices in the different graphs but referring to the same image, in a weighted fashion with a user-specified parame-ter  X   X  (0 , 1) ( cf. Eq. (2), e.g.,  X  = 0 . 5 for averaging) so that the two graphs can communicate with each other. Fi-nally, both visual and textual graphs are updated by assign-ing each fused value back to the two corresponding vertices. The overall learning process is iterated until convergence.
This section details our multi-layer SSL algorithm, ex-tended from [7], targeting at large-scale datasets. As space is limited, we shall concentrate on the operations in the vi-sual graph, and shall not mention those in the textual graph. For simplicity, we assume each vertex in a visual graph con-tains a single label (i.e., value). However, our work can be extended to multiple labeling problems.

In the sequel, let G be the sparse visual graph with vertex set X = { x 1 , ..., x n } , where n is the vertex number; let W  X  R n  X  n be the similarity matrix of G , where W ij denotes the edge weight between vertices x i and x j ; let D  X  R n  X  n be a diagonal matrix with D ii equaling the inverse square root of the sum of the i -row of W ; let L = { 1 , ..., c } be the label set of G ; let Y  X  R n  X  c be a matrix with Y ij = 1 if x i is initially labeled as j , and Y ij = 0 otherwise; let F  X  R n  X  c be a classification matrix, where each vertex x i will be marked as a label y i = argmax j  X  c F ij ; let  X   X  (0 , 1) be a user-specified parameter. Without loss of generality, assuming that initially the first l (typically, l n ) vertices { x 1 , ..., x l } are individually labeled as { y 1 , ..., y l } X  X  , and the other ( n  X  l ) vertices { x ( l +1) , ..., x n } are unlabeled. Our objective is to predict the labels of the unlabeled vertices.
Initially, we assign Y to F . Then we create a normalized weight matrix S , which equals the product of D times W times D . Next, we iteratively perform the following two operations (for inferring labels and fusing two graphs). where the meaning of a matrix with the superscript X ( t, vis ) X  is twofold: obtained in the t -th iteration, and used for learn-ing on the visual graph (i.e., F (0 ,vis ) = F ). Similarly, F (( t +1) ,txt ) is the classification matrix for the textual graph obtained in the ( t + 1)-th iteration. Note that  X  is the fused parameter ( cf. Section 2.1). Note that fusion (Eq. (2)) in the t -th iteration can only be done after inferring labels (Eq. (1)) in both of the visual graph and textual graphs in the t -th iteration. The iterative process is repeated until a certain condition is met. Finally, following the definition, given a classification matrix, we assign the labels of the unlabeled vertices accordingly (i.e., y i = argmax j  X  c F ij , 1  X  i  X  n ).
Note that all of the (sparse) matrix operations above are implemented based on the MapReduce programming model. It is worth noting that the rationale behind matrix multi-plication is to distribute the multiplication task into n par-titions and work on several servers.

