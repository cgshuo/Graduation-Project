
Many 0/1 datasets have a very large number of vari-ables; however, they are sparse and the dependency struc-ture of the variables is simpler than the number of vari-ables would suggest. Defining the effective dimensionality of such a dataset is a nontrivial problem. We consider the problem of defining a robust measure of dimension for 0/1 datasets, and show that the basic idea of fractal dimension can be adapted for binary data. However, as such the frac-tal dimension is difficult to interpret. Hence we introduce the concept of normalized fractal dimension. For a dataset D , its normalized fractal dimension counts the number of independent columns needed to achieve the unnormalized fractal dimension of D . The normalized fractal dimension measures the degree of dependency structure of the data. We study the properties of the normalized fractal dimension and discuss its computation. We give empirical results on the normalized fractal dimension, comparing it against PCA.
Many binary datasets occurring in data mining applica-tions are, on one hand, complex as they have a very large number of columns. On the other hand, some of those datasets could be potentially simple, as they are very sparse or have lots of structure. In this paper we consider the prob-lem of defining a notion of effective dimension for a binary dataset. We study ways of defining a concept of dimension that would somehow capture the complexity or simplicity of the dataset. Such a notion of effective dimension can be used as a general score descr ibing the complexity or sim-plicity of the dataset; some potential applications of the in-trinsic dimensionality of a dataset include model selection problems in data analysis; it can also be used in speeding up certain computations (see, e.g., [9]).

For continuous data there are many ways of defining the dimension of a dataset. One approach is to use decompo-sition methods such as SVD, PCA, or NMF (nonnegative matrix factorization) [14, 19] and to count how many com-ponents are needed to express, say, 90% of the variance in the data. This number of components can be viewed as the number of effective dimensions in the data.

In the aforementioned methods it is assumed that the dataset is embedded into a higher-dimensional space by some (smooth) mapping. The other main approach is to use a different concept, that of fractal dimensions [3, 9, 15, 23]. Very roughly, the concept of fractal dimension is based on the idea of counting the number of observations in a ball of radius r and looking what the rate of growth of the number is as a function of r . If the number grows as r k , then the dimensionality of the data can be considered to be k .Note that this approach does not provide any mapping that can be used for the dimension reduction. Such mapping does not even make sense because the dimension can be non-integral.
Applying these approaches to binary data is not straight-forward. Many of the component methods, such as PCA and SVD are strongly based on the assumption that the data are real-valued. NMF looks for a matrix decomposi-tion with nonnegative entries and hence is somewhat better suited for binary data. However, the factor matrices may have continuous values, which makes them difficult to in-terpret. The component techniques aimed at discrete data (such as multinomial PCA [6] or latent Dirichlet allocation (LDA) [4]) are possible alternatives, but interpreting the re-sults is hard.

In this paper we explore the notion of effective dimen-sion for binary datasets by using the basic ideas from frac-tal dimensions. Essentially, we consider the distribution of the pairwise distances between random points in the dataset. Denoting by Z this random variable, we study the ratio of log P ( Z&lt;r ) and log r , for different values of the r ,andfit a straight line to this; the slope of the line is known as the correlation dimension of the dataset.

Interpreting the correlation dimension of discrete data turns out to be a difficult task. To assist interpetation, we normalize the correlation dimension by considering what would be the number of variables in a certain random dataset with independent columns having the same corre-lation dimension. This normalized correlation dimension is our main concept.

We study the behavior of the correlation dimension and the normalized correlation dimension, both theoretically and empirically. We give approximations for correlation dimension, in the case of independent variables, showing that it decreases when the d ata becomes more sparse. We also give theoretical evidence indicating that positive cor-relations between the variables lead to smaller correlation dimensions.

Our empirical results for gen erated data show that the normalized correlation dimension of a dataset with K inde-pendent variables is very close to K , irrespectively of the sparsity of the attributes. We demonstrate that adding posi-tive correlation decreases the dimension. For real datasets, we show that different dataset s have quite different normal-ized correlation dimensions, and that the ratio of the number of variables to the normalized correlation dimension varies a lot. This indicates that the amount of structure in the datasets is highly variable. We also compare the normal-ized correlation dimension against the number of PCA com-ponents needed to explain 90% of the variance in the data, showing interesting differences among the datasets. The rest of this paper is organized as follows. In Section 2 we define the correlation dimension for binary datasets and we analyze the correlation dimension in Sec-tion 3. The correlation dimension produces too small val-ues and hence in Section 4 we provide means for scaling the dimension. In Section 5 we represent our tests with real world datasets. In Section 6 we review the related literature and Section 7 is a short conclusion. The proofs are omitted due to the space limitations.
There are several possible definitions of the fractal di-mension of a subset of the Euclidean space; see, e.g., [3, 23] for a survey; the R  X  enyi dimensions [23] form a fairly general family. The standard definitions of the fractal dimension are not directly applicable in the discrete case, but they can be modified to fit in.

The basic idea in the fractal dimensions is to study the distance between two random data points.

We focus on the correlation dimension. Consider a 0/1 dataset D with K variables. Denote by Z D the random vari-able whose value is the L 1 distance between two randomly chosen points from D ; thus 0  X  Z D  X  K . Informally, the correlation dimension is the slope of the line fitted in the log-log plot of ( r, P ( Z D &lt;r )) .

More formally, we first define the function f : N  X  R to be f ( r )= P ( Z D &lt;r ) . We extend this function to real numbers by linear interpolation.

Let 0  X  r 1 &lt;r 2  X  K . Then the different radii r and the function f for a given dataset D determine the point set I ( D, r 1 ,r 2 ,N )= { (log r, log f ( r )) | We usually omit the parameter N for the sake of brevity.
For example, assume that P ( Z D  X  r )  X  r d for some d , that is, the number of pairs of points within distance d grows as r d .Then I ( D, r 1 ,r 2 ) is a straight line and the correlation dimension is equal to d .
 Definition 1. The correlation dimension cd R ( D ; r 1 ,r a binary dataset D and radii r 1 and r 2 is the slope of the least-squares linear approximation I ( Z, r 1 ,r 2 ) .
Assume that we are given  X  1 and  X  2 such that 0  X   X  1 &lt;  X  where the radii r i are set to be max f  X  1 (  X  i ) , 1 .The reason for truncating r i is to avoid some misbehavior oc-curring with extremely sparse datasets.

That is, I ( D, r 1 ,r 2 ) is the set of points containing the logarithm of the radius r and the logarithm of the frac-tion of pairs of points from D that have L 1 distance less than or equal to r . The correlation dimension is the slope of the line that fits these points best. The difference be-tween cd R ( D ; r 1 ,r 2 ) and cd A ( D ;  X  1 , X  2 ) is that cd defined by using the absolute bounds r 1 and r 2 for the ra-dius r , whereas cd A uses the parameters  X  1 and  X  2 to spec-cd
A ( D ;1 / 4 , 3 / 4) is the correlation dimension obtained by first computing the values r 1 and r 2 such that one quarter of the pairs of points have distance below r 1 , and one quarter of the pairs have distance above r 2 . The dimension is then obtained by computing N +1 points (log r, log f ( r )) with r least-squares sense.

How can we compute the correlation dimension of a bi-nary dataset D ? The probability P ( Z D &lt;r ) can be com-puted where I ( | x  X  y | &lt;r ) is the indicator function having value 1 if | x  X  y | &lt;r ,andvalue 0 otherwise. Computing the val-ues P ( Z D &lt;r ) for all integers r can thus be done trivially in time O ( N 2 K ) ,where N is the number of points in D and K is the number of variables. A sparse matrix repre-sentation yields to a running time of O ( NM ) ,where M is the total number of 1 X  X  in the data: If point i has m i 1 X  X , then i m i = M , and computing the all pairwise distances takes time
If the number of points in a dataset is so large that quadratic computation time in the number of points is too slow, we can take a random subset D s from D and estimate the probability P ( Z&lt;r ) by or by
In this section we analyze the properties of the corre-lation dimension cd A ( D ;  X  1 , X  2 ) for binary datasets. We show the following results under some simplifying assump-tions. First, we prove that if the original data has in-dependent columns, then the correlation dimension grows as the probabilities of the individual variables get closer to 0 . 5 . Second, we show that in the independent case cd
A ( D ;  X , 1 of attributes (columns) in the dataset. Third, we prove that if the variables are not independent, then the correlation di-mension is smaller than for a dataset with the same margins but independent variables.

For the analysis we need to make some simplifying as-sumptions. One complication i s caused by the fact that the definition of cd R ( D ; r 1 ,r 2 ) involves the slope of a set of points. However, note that I ( D, r 1 ,r 2 , 1) contains only two points, and hence we have
Similarly, in the case of cd A ( D ;  X  1 , X  2 , 1) we have r and r 2 such that  X  i = f ( r i ) , and hence Throughout this section we assume that the parameter N in I ( D, r 1 ,r 2 ,N ) is equal to 1 .
 Proposition 2. Assume that the dataset D has K indepen-dent variables, and that th e probability of the variable i be-ing 1 is p i for each i , and let q i =2 p i (1  X  p i ) . Assuming that K is large enough, we have where C (  X  ) is a constant depending only on  X  . In par-ticular, if all probabilities p i are equal to p , then for q = 2 p (1  X  p ) we have
The proposition indicates that the correlation dimension is maximized for variables as close to 0 . 5 as possible. Corollary 3. Assume the dataset D has independent columns. The correlation dimension cd A ( D ;  X , 1  X   X  ) is maximized if the variables have frequency 0 . 5 .
Proposition 2 also tells that for a dataset with indepen-dent identically distributed columns, the dimension grows as a square root of the number of columns. If  X  =1 / 4 ,then the constant C (  X  ) is about 0 . 815 .

The correlation dimension has an interesting connection to the average distance in randomly picked point pairs. Proposition 4. Assume that the dataset D has K indepen-dent variables, and that the probability of variable i being 1is p i .Let q i = i 2 p i (1  X  p i ) .Let  X  = i q i be the average distance of two randomly picked points.

Assume that we are given two constants c 1 and c 2 such that 0  X  c 1 &lt;c 2  X  1 . Then we can approximate the correlation dimension as where C ( c 1 ,c 2 ) depends only of c 1 and c 2 .
Note that Proposition 4 gives an approximation for the quantity cd R , while Proposition 2 is about cd A ; this, how-ever, is a superficial difference. More important is the fact that in Proposition 4 we look at the case where the bounds r 1 and r 2 are on the same side of the mean, whereas the bounds corresponding to  X  and 1  X   X  from Proposition 2 are on the two sides of the mean. This implies that Propo-sition 4 gives a stronger bound: the dimension grows as a function of the mean  X  , not as a function of  X / X  . Example 5. Let D be a dataset with K dimensions, and consider the set D obtained by copying each variable in D to N new variables. Then and hence
Given a dataset D with K columns, we denote by ind ( D ) a random binary dataset having K independent variables such that the probability of i th variable being 1 is equal to the probability of i th column of a random trans-action sampled from D being 1 . Alternatively, ind ( D ) can be considered as a dataset obtained by permuting each col-umn of D independently. We conjecture that the correlation dimension of D is always smaller than the correlation di-mension of ind ( D ) , given that the original variables are all positively correlated.
 Conjecture 6. Assume the marginal probability of all orig-inal variables are less than 0 . 5 , and that all pairs of original variables are positively correlated. Then i.e., the correlation dimension of the original data is not larger than the correlation dimension of the data with each column permuted randomly.

Support for this conjecture is provided by the fact that the variance Var [ Z D ] of the variable Z D canbeshowntobeno more than the variance Var Z ind( D ) ; this does not, how-ever, suffice for the proof. The intuition behind the above conjecture is similar to what one observes in other types of definitions of dimension: if we randomly permute each col-umn of a dataset, we expect to see the rank of the matrix to grow, and also explain an increase the number of PCA components needed to explain, say, 90% of the variance. In the experimental section we show the empirical evidence for Conjecture 6.
The definition of correlation dimension (Definition 1) is based on the definition of correlation dimension for con-tinuous data. We have argued that the definition has some simple intuitive properties: for a dataset with independent variables the dimension is smaller if the variables are sparse, and the dimension seems to shrink if we add structure to the data by making variables positively correlated.

However, the scale of the correlation dimension is not very intuitive: the dimension of a dataset with K indepen-dent variables is not K , although this would be the most nat-ural value. The correlation dimension gives much smaller values and hence we need some kind of normalization.
We showed Section 3 that under some conditions inde-pendent variables maximize the correlation dimension. In-formally, we define the normalized correlation dimension of a dataset D to be the number of variables that a dataset with independent variables must have in order to have the same correlation dimension as D does.

More formally, let ind ( H, p ) be a dataset with H inde-pendent variables, each of which is equal to 1 with proba-bility p . From Proposition 1 we have an explicit formula for cd
A (ind ( H, p );  X , 1 If the dataset would have the same marginal frequency, say s , for each variable, the normalized correlation dimension of a dataset D could be defined to be the number H such that are as close to each other as possible.

The problem with this way of normalizing the dimension is that it takes as the point of comparison a dataset where all the variables have the same marginal frequency. This is very far from being true in real data. Thus we modify the definition slightly.

We first find a value s such that cd A (ind ( K, s );  X , 1  X   X  )=cd A (ind ( D );  X , 1  X   X  ) , i.e., a summary of the marginal frequencies of the columns of D : s is the frequency that variables of an independent dataset should have in order that it has the same correlation dimension as D has when the columns of D have been ran-domized. We define the normalized correlation dimension , denoted by ncd A ( D ;  X , 1  X   X  ) ,tobeaninteger H such that is minimized. Proposition 2 implies the following state-ment.
 Proposition 7. Given a dataset D with K columns, the di-mension ncd A ( D ;  X , 1  X   X  ) can be approximated by ncd A ( D ;  X , 1  X   X  )  X 
For examples, see the beginning of the next section.
In this section we describe our experimental results. We first describe some results on synthetic data, and then dis-cuss real datasets and compare the normalized correlation dimension against PCA.

Unless otherwise mentioned, the dimension used in our experiments was cd A ( D ;1 / 4 , 3 / 4 , 50) .
In this section we provide empirical evidence to support the analysis in Sections 3 and 4. In the first experiment we
Figure 1. Normalized correlation dimension for data having K independent dimensions for K  X  X  50 , 100 , 150 , 200 } . generated 100 datasets with K independent columns and random margins p i . For each dataset, the margins p i were randomly picked by first picking p max uniformly at random from [0 , 1] . Then, the probability p i was picked uniformly from [0 ,p max ] ; this method results in datasets with differ-ent densities. The box plot in Figure 1 shows that the nor-malized dimension is very close to K , the number of vari-ables in the data. This shows that for independent data the normalized correlation dimension is equal to the number of variables, and that the sparsity of the data does not influence the results.

In the second experiment we tested Proposition 2 with synthetic data. We generated 100 datasets having indepen-dent columns and random margins, generated as described above. Figure 2 shows the correlation dimension as a func-tion of  X / X  ,where  X  =E[ Z D ] and  X  2 =Var[ Z D ] .The figure shows the behavior predicted by Proposition 2: the normalized fractal dimension is a linear function of  X / X  , and the slope is very close to C (1 / 4) = 0 . 815 .
Figure 2. Correlation dimension as a func-tion of  X / X  for data with independent columns (see Proposition 2). The y -axis is cd A ( D ;1 / 4 , 3 / 4) and the x -axis is  X / X  ,where  X  =E[ Z D ] and  X  2 =Var[ Z D ] . The slope of the line is about C (1 / 4) = 0 . 815 .

The theoretical section analyzes only the simplest form of the correlation dimension, that is, the case where N =1 . We tested how the dimension behaves for different N .In order to do that, we used generated datasets from the previ-ous experiments and plotted cd A ( D ;1 / 4 , 3 / 4 , 50) against cd
A ( D ;1 / 4 , 3 / 4 , 1) . We see from Figure 3 that the corre-lation dimension has little dependency of N .
Figure 3. Correlation dimension cd A ( D ;1 / 4 , 3 / 4 , 50) as a function of cd A ( D ;1 / 4 , 3 / 4 , 1) for data having K in-dependent dimensions for K  X  X  50 , 100 , 200 } .

In the fourth experiment we verified the quality of the approximation of Proposition 4. We used the same data in the previous experiment. Figure 4 shows the correlation di-mension against  X  =E[ Z D ] , the average distance of two random points. From the figure we see that Proposition 4 is partly supported: the correlation dimension behaves as a linear function of  X  . However, the slope becomes more gentle as the number of columns increases.
Figure 4. Correlation dimension as a function of  X  for data with independent columns (see
Proposition 4). The y -axis is cd A ( D ;1 / 4 , 3 / 4) and the x -axis is  X  =E[ Z D ] , the average dis-tance between two random points.

Our fifth experiment tested how positive correlation af-fects the correlation dimensi on. Conjecture 6 predicts that positive correlation should decrease the correlation dimen-sion. We tested this conjecture by creating random datasets D such that column i depends on column i  X  1 .Let X i be variable number i in the generated dataset. We generated data by a Markov process between the variables:
P ( X and where X =[ X 1 ,...,X k ] is the random element of D .
The reversal probabilities t i were randomly picked as follows: For each dataset we picked uniformly a random number t max from the interval [0 , 1] .Wepicked t i uniformly from the interval [0 ,t max ] . Note that if the reversal proba-bilities were 0 . 5 , then the dataset would have independent columns. Denoting Z = Z D ,wehave
A rough measure of the amount of correlation in the data is t = 2 t i (1  X  t i ) . Figure 5 shows the correlation di-mension as a function of the quantity t . We see that the datasets with strong correlations tend to have small dimen-sions, as the theory predicts.
Figure 5. Correlation dimension as a func-tion of t , a rough measure of correlation in a dataset. The y -axis is cd A ( D ;1 / 4 , 3 / 4) and the x -axis is the quantity t = 2 t i (1  X  t i ) ,where t i is the reversal probability between columns i and i  X  1 .

Next, we go back to the first experiment to see whether the normalized correlation dimension depends on the spar-sity of data. Note that sparse datasets have small  X  = E[ Z D ] . Figure 6 shows the normalized correlation dimen-sion as a function of  X  for the datasets used in Figure 1. We see that the normalized dimension does not depend of sparsity, as expected.

We tested Proposition 7 by plotting the normalized di-mension as a function of K cd A ( D ) 2 / cd A (ind ( D )) used the generated datasets from the previous experiment and from our fifth experiment, as well. Figure 7 reveals that the approximation is good for the used datasets.
Figure 6. Normalized correlation dimension as a function of  X  , the average distance be-tween two random points. The x -axis is  X  =
E[ Z D ] and the y -axis is ncd A ( D ;1 / 4 , 3 / 4) .
Figure 7. Normalized correlation dimension as a function of K cd A ( D ) 2 / cd A (ind ( D )) 2 .
The left figure contains datasets with inde-pendent columns and in the right figure adja-cent columns of the datasets depend on each other.
In this section we investigate how our dimensions behave with 9 real-world datasets: Accidents , Courses , Kosarak , Paleo , POS , Retail , WebView-1 , WebView-2 and 20 News-groups . The basic information about the datasets is summa-rized in Table 1.

The datasets are as follows. 20 Newsgroups 1 is a collec-tion of approximately 20 000 newsgroup documents across 20 different newsgroups [18]. Data in Accidents 2 were ob-tained from the Belgian  X  X nalysis Form for Traffic Acci-dents X  forms that is filled out by a police officer for each traffic accident that occurs w ith injured or deadly wounded casualties on a public road in Belgium. In total, 340 183 traffic accident records are included in the dataset [12]. Table 1. The basic statistics of the datasets.

The column K corresponds to the the num-ber of columns and the column N to the num-ber of rows. The last column is the density of 1 X  X  in percentages.
 WebView-1 497 59 602 149 639 0 . 51 WebView-2 3 340 77 512 358 278 0 . 14 The datasets POS 3 , WebView-1 4 and WebView-2 5 were con-tributed by Blue Martini Software as the KDD Cup 2000 data [16]. POS contains several years worth of point-of-sale data from a large electronics retailer. WebView-1 and WebView-2 contain several months worth of click-stream data from two e-commerce web sites. Kosarak 6 consists of (anonymized) click-stream data of a Hungarian on-line news portal. Retail 7 is a retail market basket data supplied by an anonymous Belgian retail supermarket store [5]. The dataset Paleo 8 contains information of species fossils found in specific paleontological sites in Europe [10]. Courses is a student X  X ourse dataset of courses completed by the Com-puter Science students of the University of Helsinki.
We began our experiments by computing the correlation dimension cd A ( D ;1 / 4 , 3 / 4) for each dataset. In order to do that, we needed to estimate the probabilities P ( Z D &lt;r ) . Since some of the datasets had a very large amount of rows (see Table 1), we estimate the probabilities P ( Z D &lt;r ) by where I ( | x  X  y | &lt;r ) is 1 if | x  X  y | &lt;r ,and 0 otherwise. The set D s was a random subset of D containing 10 000 points. Since Paleo and Courses have small number of rows,nosamplingisusedand D s was set to D for these datasets. The evaluation times are discussed in the end of the section.
We also computed cd A (ind ( D );1 / 4 , 3 / 4) , the corre-lation dimension for the datasets with the same column margins but independent columns. Our goal was to use these numbers to provide empirical evidence for the the-oretical sections. To calcu late the dimensions we need to estimate the probabilities P Z ind( D ) &lt;r . The estimation was done by generating 10 000 points from the distribution The dimensions cd A ( D ) and cd A (ind ( D )) are given in Table 2. We see that the dimensions are very small. The rea-son is that the datasets are quite sparse. We also observe that cd
A (ind ( D )) is always larger than cd A ( D ) , which sug-gests that there is at least some structure in the datasets.
In addition, we used cd A (ind ( D )) to verify Proposi-tion 2. This was done by computing  X / X  ,where  X  = Note that Proposition 2 suggests that  X  C (1 / 4)  X  0 . 8 .Table2 shows us that this is indeed the case.

Table 2. Correlation dimensions of the datasets. In the second column, D =ind( D ) .

The third column is the fraction  X / X  ,where  X  =E[ Z D ] and  X  2 =Var[ Z D ] . The fourth col-umn is an estimate of the coefficient C (1 / 4) obtained by dividing cd A ( D ) with  X / X  .
 WebView-1 1 . 27 1 . 93 2 . 26 0 . 86 WebView-2 1 . 01 2 . 58 3 . 05 0 . 85
We continued our experiments by calculating the nor-malized correlation dimension ncd A ( D ;1 / 4 , 3 / 4) . For this we computed the probability s such that cd A (ind ( K, s );  X , 1  X   X  )=cd A (ind ( D );  X , 1  X   X  ) using binary search. Also, the normalized dimension itself was computed by using binary search. The normalized di-mensions are given in Table 3.
 Recall that the normalized correlation dimension of data D indicates how many variables a dataset D with indepen-dent columns should have so that the distributional behavior of the pairwise distances between points would be about the
Table 3. Normalized correlation dimensions of the datasets.
 Accidents 469 220 0 . 47 222 . 91 WebView-1 497 190 0 . 38 214 . 33
WebView-2 3 340 359 0 . 11 512 . 97 same in D and D . Thus we note, for example, that for the Paleo data the dimensionality is about 15, a fraction of 11% of the number of columns in the original data.
 The last column in Table 3 is the estimate predicted by Proposition 7. Unlike with the synthetic datasets (see Sec-tion 5.1), the estimate is poor in some cases. A probable reason is that the examined datasets are extremely sparse, and hence the techniques used to obtain Proposition 7 are no longer accurate. This is supported by the observation that Accident has the best estimate and the largest density. We also tested the accuracy of Proposition 7 with 20 Newsgroups dataset 9 . In Figure 8 we plotted the normalized correlation dimension as a function of the estimate. We see that the approximation overestimates the dimension but the accuracy is better than in Table 3.
Figure 8. Normalized correlation dimension as a function of K cd A ( D ) 2 / cd A (ind ( D )) 2 . Each point represents one newsgroup in 20 Newsgroups dataset.

We will compare the normalized correlation dimensions against PCA in the next subsection.

Next we studied the running times of the computation of the correlation dimension. Computing the distance of two binary vectors can be done in O ( M ) time, where M is the number of 1 X  X  in the two vectors. Hence, estimating the probabilities u sing Equation 1 can be done in O ( | D s | where L is the number of 1 X  X  in D . We need also to fit the slope to get the actual dimension, but the time needed for this operation is negligible compared to the time needed for estimating the probabilities. Note that in our setup, the size of D s was fixed to 10 000 (except for Paleo and Courses ). Hence, the running time is proportional to the number of 1 X  X  in a dataset. The running times are given in Table 4.
Table 4. The running times of the correlation dimension in seconds for various datasets.

Time/# of 1 X  X : time in milliseconds divided by the number of 1 X  X  in the data.

There are different approaches for measuring the struc-ture of a dataset. In this section we study how the normal-ized dimension compares with PCA.

We performed PCA to our datasets and computed the percentage of the variance explained by the M first PCA variables, where M =ncd A ( D ) . Additionally, we cal-culated how many PCA components are needed to explain 90% of the variance. The results are given in Table 5. We observe that ncd A ( D ) PCA components explain rela-tively large portion of the variance for Accidents , POS ,and WebView-1 , but explains less for Paleo and WebView-2 .
The most interesting behavior is observed in the Paleo dataset. We see that whereas PCA dimension says that Pa-leo should have relatively high dimension, the normalized dimension suggests a very small value. We know that Paleo has a very strong structure (by looking at the data) so this suggests that the PCA approach overestimates the intrinsic dimension for Paleo . This behavior can perhaps be partly explained also by considering the margins of the datasets. The margins of Paleo are relatively homogeneous whereas the margins of the rest datasets are skewed.

Table 5. Normalized correlation dimensions versus PCA for various datasets. The sec-ond column is the percentage of variance ex-plained by ncd A ( D ) variables and the third column is the number of variables needed to explain 90% of the variance.
 WebView-1 190 87 . 89 208
WebView-2 359 59 . 73 1 394
There has been a significant amount of work in defin-ing the concept of dimensionality in datasets. Even though most of the methods can be adapted to the case of binary data, they are not specifically tailored for it. For instance, many methods assume real-valued numbers and they com-pute vectors/components that have negative or continuous values that are difficult to interpret. Such methods in-clude, PCA, SVD, and non-negative matrix factorization (NMF) [14, 19]. Other methods such as multinomial PCA (mPCA) [6], and latent Diric hlet allocation (LDA) [4] as-sume specific probabilistic models of generating the data and the task is to discover latent components in the data rather than reasoning about th e intrinsic dimensionality of the data. Methods for exact and approximate decomposi-tions of binary matrices in Boolean semiring have also been proposed [11, 21, 22], but similarly to mPCA and LDA, they focus on finding components instead of the intrinsic dimensionality.

The concept of fractal dimension has found many ap-plications in the database and data mining communities, such as, making nearest neighbor computations more effi-cient [24], speeding up feature selection methods [29], out-lier detection [27], and performing clustering tasks based on the local dimensionality of the data points [13].
Many different notions of complexity of binary datasets have been proposed and used in various contexts, for in-stance VC-dimension [2], discrepancy [7], Kolmogorov complexity [20] and entropy -based concepts [8, 25]. In some of the above cases, such as Kolmogorov complexity and entropy methods, there is no direct interpretation of the measures as a notion of dimensionality of the data as they are measures of compressibility. VC-dimension measures the dimensionality of discrete data, but it is rather conserva-tive as a binary dataset having VC-dimension d means that there are d columns such that the projection of the dataset on those coordinates results all possible bit vectors of length d . Hence, VC-dimension does not make any difference be-tween datasets { 0 , 1 } d and { x  X  X  0 , 1 } K : K i =1 x although there is a great difference when d&lt;&lt;K .Further-more, computing the VC-dimension of a given dataset is a difficult problem [26].

Also the work on random projections and dimensional-ity reductions, such as in [1], is related but that line of re-search has different goals than ours. Finally, methods such as multidimensional scaling (MDS) [17] and Isomap [28] focus on embedding the data (not necessarily binary) in low-dimensional spaces with small distortion, mainly for visualization purposes.
We have given a definition of the effective dimension of a binary dataset. The definition is based on ideas from frac-tal dimensions: We studied how the distribution of the dis-tances between two random data points from the dataset be-haves, and fit a slope to the log-log set of points. We defined the notion of normalized correlation dimension. It measures the number of dimensions of the appropriate density that a dataset with independent variables should have to have the same correlation dimension as the original dataset.
We studied the behavior of correlation dimension and normalized correlation dimension, both theoretically and empirically. Under certain simplifying assumptions, we were able to prove approximations for correlation dimen-sion, and we verified these results using synthetic data.
Our empirical results for real data show that different datasets have clearly very different normalized correlation dimensions. In general, the normalized correlation dimen-sion correlates with the number of PCA components that are needed to explain 90% of the variance in the data, but there are also intriguing differences.

Traditionally, dimension means the degrees of freedom in the dataset. One can consider a dataset embedded into a high-dimensional space by some (smooth) embedding map. Traditional methods such as PCA try to negate this embed-ding. Fractal dimensions, however, are based on different notion, the behavior of the volume of data as a function of neighborhoods. This means that the methods in this paper do not provide a mapping to a lower-dimensional space, and hence traditional applications, such as feature reduction, are not (directly) possible. However, our study shows that frac-tal dimensions have promising properties and we believe that these dimensions are important as such.

A fundamental difference between the normalized cor-relation dimension and PCA is the following. For a dataset with independent columns PCA has no effect and selects the columns that have the highest variance until some selected percentage of the variance is explained. Thus, the number of PCA components needed depends on the margins of the columns. On the other hand, the normalized correlation di-mension is always equal to the number of variables for data with independent columns.

Obviously, several open problems remain. It would be interesting to have more general results about the theoretical behavior of the normalized correlation dimension. In the empirical side the study of the correlation dimensions of the data and its subsets seems to be a promising direction.
