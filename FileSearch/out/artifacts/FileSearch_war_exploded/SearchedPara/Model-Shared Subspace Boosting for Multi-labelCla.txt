 Typical approaches to the multi-label classification prob-lem require learning an independent classifier for every la-bel from all the examples and features. This can become a computational bottleneck for sizeable datasets with a large label space. In this paper, we propose an efficient and ef-fective multi-label learning algorithm called model-shared subspace boosting (MSSBoost) as an attempt to reduce the information redundancy in the learning process. This algo-rithm automatically finds, shares and combines a number of base models across multiple labels, where each model is learned from random feature subspace and bootstrap data samples. The decision functions for each label are jointly es-timated and thus a small number of shared subspace models can support the entire label space. Our experimental results on both synthetic data and real multimedia collections have demonstrated that the proposed algorithm can achieve bet-ter classification performance than the non-ensemble base-line classifiers with a significant speedup in the learning and prediction processes. It can also use a smaller number of base models to achieve the same classification performance as its non-model-shared counterpart.
 H.1.0 [ Information System ]: Models and Principles Algorithms, Performance, Theory Multi-label classification, random subspace methods, boost-ing, model sharing
Multi-label classification is a concept in learning systems that refers to the simultaneous categorization of a given in-Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. put example into a set of multiple labels. The need for multi-label classification in large-scale systems is increasing in diverse applications such as music categorization, image and video annotation, text categorization and medical ap-plications. Detecting multiple semantic labels from images based on their low-level visual features [2, 15, 14] and auto-matically classifying text documents into a number of topics based on their textual context [25, 18] are some of the typi-cal multi-label applications. In reality, multimedia and text data collections can contain hundred thousa nds to m illions of data items, and these items can be associated with thou-sands of different labels. Most existing algorithms do not scale well to such a high computational demand, because they need to learn an independent classifier for every possi-ble label using all the data samples and the entire feature space. Thus it is a great challenge and opportunity for us to design more advanced multi-label learning algorithms that can learn and predict multiple labels in an accurate and efficient way.

To speed up multi-label classification without performance degradation, one approach is to exploit the information re-dundancy in the learning space. To this end, researchers have proposed several ensemble learning algorithms based on random feature selection and data bootstrapping. The random subspace method (RSM) [12] takes advantage of both feature space bootstrapping and model aggregation, and combines multiple base models learned on a randomly selected subset of the feature space. RSM considerably re-duces the size of the feature space in each base model, and the model computation is more efficient than a classifier di-rectly built on the entire feature space. Combining bagging (bootstrap aggregation) and RSM, Breiman has developed a more general algorithm called random forest [4]. Random forest aims to aggregate an e nsemble of unpruned classi-fication/regression trees using both bootstrapped training examples and random feature selection in the tree induc-tion process. Random forest can be learned more efficiently than the baseline method, and it has empirically demon-strated superiority compared to a single tree classifier. For the multi-label scenario, above algorithms need to learn an independent classifier for every label or assume that the un-derlying base models can produce multi-label predictions.
However, they ignore an important fact that different la-bels are not independent of each other, or orthogonal to one another. On the contrary, multiple labels are usually inter-related and connected by their semantic interpretations, and hence exhibit certain co-occurrence patterns in the data col-lections. For example, in an image collection, the label  X  X ar X  Figure 1: Redundancy in the decision space demon-strated using a 2-D synthetic dataset with 3 labels. The dashed circles indicate the true decision bound-ary for each label. almost always co-occurs with the label  X  X oad X , while the concept  X  X ffice X  is not likely to appear with  X  X oad X . Inter-related multi-labels exhibit the redundancy of information in the label space. This observation inspires us to further improve the classification efficiency by exploiting the label space redundancy. To achieve this goal in an ensemble learn-ing algorithm, we can allow the underlying base models to be shared across multiple labels and thus the decisions of base models can be re-used in several places at the same time. To illustrate, Figure 1 shows a 2-D synthetic dataset with 3 labels, where its characteristics are explained in Sec-tion 6.1. Let us assume that each base model is learned from a depth-1 decision tree that can only produce an axis-parallel decision boundary. In this case, if these models are not allowed to be shared, we need at least 12 base models to construct accurate classifiers for all three labels, because each label requires at least 4 models for capturing its bound-aries. However, as we can observe, the decision boundaries of all three labels are closely related to each other. As it results, only 5 base models are needed to construct classi-fiers of a similar quality, if these models can be shared across multiple decision functions.

In this paper, we propose a boosting-type learning algo-rithm called model-shared subspace boosting (MSSBoost). It merges ensemble learning, random subspace sampling, and models sharing techniques to automatically find, share and combine a number of random subspace models across multiple labels. MSSBoost is able to reduce the informa-tion redundancy in the label space by jointly optimizing the loss functions over all possible labels. Meanwhile, since its base models are built on a small number of bootstrap data samples and a randomly selected feature subspace, the pro-posed algorithm can scale to sizeable datasets with a large number of labels. Our experiments show that MSSBoost can outperform the non-ensemble baseline classifiers with a significant speedup on the learning and prediction process. It can also use a smaller number of base models to achieve the same classification performance as its non-model-shared counterpart. As a by-product, the proposed algorithm can automatically discover the underlying context between mul-tiple labels, and this offers us more insights to analyze the data collections.

Our following discussions are organized as follows. Sec-tion 2 presents the basic notations and terminologies used in this paper. Section 3 illustrates the random space bag-ging methods in detail. Section 4 describes the proposed MSSBoost algorithm and analyzes its learning properties as well as its computational complexity. Section 5 reviews the related efforts and compares them with our approaches. Section 6 presents the experimental results on multiple data collections and Section 7 concludes our discussions.
In this section, we present the formal notations and termi-nologies that we use in our study. Let X be the domain of all possible features, and the number of features for X is M .Let Y be a finite set of labels of size L , L = |Y| . Without loss of generality, we assume the labels are { 1 , ..., L } . In the multi-label classification setting, each data point x  X  X  can be as-signed multiple labels from Y . For instance, for an image an-notation problem where  X  X uilding X ,  X  X ky X  and  X  X ater X  are the possible labels, we can associate an image with the labels of both  X  X uilding X  and  X  X ky X . Let y l  X  X  X  1 , +1 } indicate if the data example x belongs to the l th label, l  X  [1 ,L ]. There-fore, we can represent a labeled example as ( x , y ), where y = { y l } l =1 ..L . Given a set of training data { ( x the goal of the multi-label learning algorithms is to produce a decision function of the form F : X X Y X  R .Herewe use a simpler representation by assuming each label l has its own decision function F l : X X  R , and the corresponding prediction y l is determined by sign ( F l ( x )).
Ensemble learning approaches such bagging [3] and boost-ing [8] aim to improve the classification performance by com-bining the outputs from a family of base models H , a.k.a. weak learners. Each base model h  X  X  is a binary classifier that produces a real-valued predictions 1 h : X X  R ,andwe assume | h (  X  ) | X  1. The base models can be generated from different types of algorithms such as decision tree and sup-port vector machines. Some classification algorithms also attempt to optimize a loss function C ( y, f ), i.e. AdaBoost uses the exponential loss C ( y, f )=exp( yf (  X  )), and Logit-Boost uses the logistic loss C ( y, f )=log(1+exp( yf (  X  Given the base models available, the learning algorithms can find a linear combination of base models to construct a composite decision function, F ( x )= t  X  t h t ( x ), where  X  is the combination weight.
Bagging, boosting and the random subspace method are among the most popular ensemble learning approaches. Bag-ging approach was proposed by Breiman [3], and it incorpo-rates the approaches of bootstrapping and classifier aggrega-tion. Multiple sets of training examples can be generated by random sampling the original training set with replacement. The decision functions based on multiple samples can be aggregated by majority voting or averaging. Previous the-oretical and empirical results have shown that bagging can reduce the variance of the prediction outputs and thus sig-nificantly improve the performance of unstable classifiers [3]. However, a simple bagging a pproach can under-perform in the imbalanced set scenarios, because as multi-label classi-fication problem often faces imbalance distribution in the dataset, the bootstrap examples may only contain few or even none of the minority labeled data. As shown by recent Algorithm 1 The round-robin ra ndom subspa ce bagging (RSBag) algorithm for multi-label classification. Input: training data { x i , y i } i =1 ...N , x i  X  R M ,numberof total base models T ,numberoflabels L , data sampling ratio r (  X  1), feature sampling ratio r f (  X  1). 1. For t = 1 to T, 2. Output the classifier y l = sign [ F l ( x )]. studies [22, 7], artificially making the class prior equal by down sampling is usually more effective and efficient than simple bagging. This technique is called  X  X symmetric bag-ging X  or  X  X alanced bagging X .

The random subspace method (RSM) [12] is another ex-ample of ensemble learning approach that takes advantages of bootstrapping and aggregating classifiers. However, in-stead of bootstrapping the training samples, RSM modifies the data examples from the other dimension by randomly se-lecting a subset of the features to learn the base models. By reducing the size of the feature space, RSM can usually be computed more efficiently than the baseline. The learning performance of RSM can benefi t from operating on a small number of features, especially when the number of training examples are insufficient. In this case, more stable classifiers can be built with a relatively larger amount of training data with respect to the feature space.

The connections between bagging and RSM has inspired the development of a more general algorithm called random forest [4], which aggregate a n ensemble of unpruned clas-sification/regression trees using both bootstrapped training examples and random feature selection in the tree induction process. Random forest has empirically demonstrated to be able to outperform a single tree classifier such as CART and C4.5. It also yields a generalization error comparable with AdaBoost. However, ensemble learning approaches were not limited to tree classifiers. The extended idea of bagging was applied in a video retrieval task [16], and the random forest idea was used in an image retrieval task [22]. In general, we term the algorithmic combination of bagging and random subspace selection as  X  X andom subspace bagging X  (RSBag) classifiers (a.k.a., Asymme tric Bagging and Random Sub-space classifiers in previous work [22]), and define it as,
Definition 1. A random subspace bagging classifier is a composite classifier combining a collection of base models { h ( x ,  X  t ) } T t =1 where {  X  t } are independent identically dis-tributed random vectors. Each classifier generates a predic-tion output based on training data and  X  t . 2
To apply the random subspace bagging algorithms for the multi-label classification problem, we present a round-robin random subspace bagging approach in Algorithm 1. This algorithm first selects a label to work with in a round robin fashion. Then we learn a base model based on Nr d bal-anced bootstrap samples from the positive and the negative data, together with Mr f random samples from the feature indices, where r d is called the data sampling ratio and r called the feature sampling ratio . Both sampling ratios are determined by the input parameters and typically they are less than 1. At the end we aggregate all the base models for the same label into a composite classifier. This algorithm is similar to a balanced version of random forests [22, 7] with the random vectors  X  t as the indices of X t + ,X t  X  and F The only difference is that it can use any binary classifier to construct the base models without being limited to the decision trees. To shed lights on why the random subspace bagging works, we present an upper bound for its generaliza-tion error. The importance of the interaction between two ingredients, the model strength s and the model correlation  X   X  , for minimizing the upper bound on the classification error is shown in the following theorem:
Theorem 1. For each label l in the random subspace bag-ging algorithms described in Algorithm 1, an upper bound for the generalization error is given by, where s l is the strength of base models for the label l , i.e., and  X  is the correlation between any two base models, i.e., Proof: Immediately follow the proof of Theorem 2.3 in [4] by viewing multi-label classification as a set of binary clas-sification problems on each label l .

Therefore, classification accuracy can be improved by min-imizing the model correlation  X   X  while maintaining the model strength s . Randomly sampling the training examples and feature subspace is useful to diversify the base models, and meanwhile reduce the model size. As long as the quality of each bootstrap models is not significantly deteriorated, ran-dom subspace bagging is able to generate a classifier with smaller model size, while produce a comparable performance with the single classifier learned from the entire data space. This claim is confirmed by the experiments in Section 6.
The base models learned from bootstrap samples and fea-ture subspace provide a foundation for developing scalable ensemble learning algorithms for multi-label classification. However, because the base models are learned independently on each label, they may contain redundant decision informa-tion that can be shared across a number of labels. By ex-ploiting this kind of information redundancy, we can further reduce the size of ensemble classifiers and keep the classi-fication performance intact. To automatically discover the base models that can be shared among labels, we switch the Figure 2: Illustration of sampling, boosting and model sharing of 6 base models for 3 labels. model aggregation strategies from bagging to a boosting-type algorithm [8], as a boosting-type approach can search the function space for the most useful models in a gradient way. By combining the idea of random subspace sampling, boosting and model sharing, we propose a new algorithm called model-shared subspace boosting (MSSBoost), as il-lustrated in Figure 2. First, we detail the entire process of the proposed model, followed by an analysis on the learn-ing properties and discussions on the computational require-ments so as to emphasize its scalability.
In order to discover the best shared subspace models over the entire label space, MSSBoost iteratively finds the most useful subspace base models, shares them across labels, and combine them into composite classifiers. Algorithm 2 shows the detailed algorithm of MSSBoost. We begin by initializ-ing a pool of K base models h k (  X  ), where h k (  X  ) is learned on the label l ( k ) using random subspace and data bootstrap-ping methods (i.e., the step 1(b)-(e) in Algorithm 1). In this paper, we simply set K = L and l ( k )= k ,whichmeans only one base model are generated for each label l 3 .In each iteration t ,wesearchtheentiremodelpoolforthebest fitted model h t and its corresponding combination weights  X  t = {  X  t l } L l =1 . This can be achieved by minimizing a joint logistic loss function summed over all the labels. One impor-tant issue here is to select a proper loss function. Extreme loss functions such as the exponential loss functions are not robust against outliers, because they tend to assign much higher weights to the noisy data. Following the idea of Log-itBoost [9], we adopt a logistic loss C ( y, f )=log(1+ e and thus the optimization step can be rewritten as, To find the optimal h t and  X  t , we can iterate over all can-didate models in the pool H p and compute the correspond-ing  X  t l using adaptive Newton steps, of which the details are shown in Section 4.2. After h t is identified from the pool, we update the decision function F l ( x )foreachlabelusingthe selected model, and then replace this model by a new model  X  h t learnedonthesamelabelas h t . Finally, the composite classifiers F l are used to provide the prediction results. Algorithm 2 The model-shared subspace boosting (MSS-Boost) algorithm for multi-label classification.
 Input: training data { x i , y i } i =1 ..N , x i  X  R M ,numberof total base models T ,numberoflabels L , data sampling ratio r d , feature sampling ratio r f . 1. Set F l ( x )=0foreachlabel l . Initialize a pool of 2. For t = 1 to T, 3. Output the classifier y l = sign [ F l ( x )].
Remark: Typical boosting algorithms learn base models using a re-weighted distribution in each iteration. Instead of doing this, we decide to select the most useful model from a candidate model pool. We choose this method mainly because of its efficiency. In the traditional boosting strategy, a total number of L  X  T base models is built in the training process. This is far more expensive than to generate the L + T base models in the current implementation.

It is interesting to show that the random subspace bagging method (RSBag) is a special case of MSSBoost, if we select h as the candidate model h k learned from the label l ( k )in a round-robin fashion, and set the corresponding  X  t l ( k with other  X  t as 0. Moreover, a non-model-shared counter-part of MSSBoost, or called non-shared subspace boosting (NSBoost), can be obtained by modifying the step 3(a) in Algorithm 2 as follows: for each candidate model h k ,we only allow the corresponding  X  t l ( k ) tohaveanon-zerovalue while setting the other  X  t to 0. So in both of these two algo-rithms, the base models from one label are no longer shared to the F l of the other labels. We will compare both RSBag and NSBoost with the proposed MSSBoost algorithm in our experiments.
Given a base model h t  X  X  p , we need to find a set of combination weights  X  t that helps to minimize the joint loss function. To this end, we apply an efficient adaptive Newton algorithm. Adaptive Newton algorithm can be summarized as the following weighted least-squares regression step: where H =( h t ( x 1 ) , ..., h t ( x N )) T , p il =1 / (1 + e w p , y il =( y il +1) / 2, Z l =( z l 1 , ..., z lN ).
Proposition 1. Eqn(2) is a one-step adaptive Newton update for the joint logistic loss function.
 Proof: Let us consider the update F l ( x )+  X  t joint loss function with respect to  X  t , We can compute the first derivative and second derivative at  X  t l =0 ,  X  l ,  X  X  2 (  X  t ) Therefore the Newton update can be written as,
For each mo del h t in the candidate model pool, we can compute its corresponding combination weights  X  t .Then Eqn(1) is applied to find the best pair of {  X  t ,h t } that can best minimize the joint loss function. Theoretically, rather than only running one-step update, we can compute mul-tiple consecutive steps of Newton updates in the step 2(a) of Algorithm 2 in order to get a better estimate for  X  t l iteration. However, since the boosting update also aims to optimize the same criterion in the outer loop, increasing the number of consecutive Newton updates inside will bring lit-tle advantage on the final prediction results. Therefore we only run one-step Newton update to compute each  X  t l .This strategy is also adopted in [9].
In this section, we analyze two learning properties for the proposed algorithm. First, we derive an upper bound for the cumulative training error over all the labels:
Theorem 2. Let F il = F l ( x i ) . The following upper bound holds for the cumulative training error, where I ( x )=1 if x is true and otherwise I ( x )=0 . Proof: Based on the inequalities log(1+ e  X  x ) &gt; 0andwhen x  X  0, log(1 + e  X  x )  X  1, we can show that where the right hand side is exactly i l C ( y il ,F il )based on its definition.
 Theorem 2 suggests that it is reasonable for the MSS-Boost algorithm to directly optimize the joint logistic loss function, which serves as an upper bound for the training er-ror over all the labels. Therefore minimizing this joint loss function can guarantee to produce a low training error in the learning process. Next, we show the proposed algorithm also satisfy the boosting property [17], i.e., if the hypothesis space of base models satisfy the  X  X eak learnability X  (i.e., the weighted margin of base models is at least  X &gt; 0ineachit-eration), the loss function will go towards 0 with sufficiently large training samples, and thus the generalization error will go to 0 with an increasing margin [9]. The satisfaction of the boosting property can be proved as follows,
Theorem 3. Suppose the base models h k in the candidate pool satisfy the weak learnability for the label l ( k ) , i.e., for the i th data example. Then for any value S 0 ,wecan show that after at most T = 2 N 4 L 4  X  2 S 2 joint loss C (  X  ) no more than S 0 L .
 Proof Sketch: Let C t iteration t , i.e., C t l = i C ( y il ,F il ), and the joint loss is C be one C t l  X  S 0 . Without loss of generality, let us assume C 1  X  S 0 . In the following we will show the improvement in the joint loss function is at least  X  model h 1 (learned on the label 1) is selected from the model pool. For the sake of simplicity, we ignore the iteration index t in the rest of the discussion unless stated otherwise.
Because the joint loss function is minimized in each itera-tion, C 1 must be less than the initial joint loss NL and thus log(1 + e  X  y i 1 F i 1 )  X  NL,  X  i . By combining this fact with the following inequality, where  X  0 is a constant, we can have where  X  0 =(1  X  2  X  NL ) /N L .Since N and L is large in general, we can simplify  X  0 =1 /N L .

Using the inequalities in (3) and (4), we can derive an lower bound for the negative gradient of C 1 w.r.t.  X  1 ,
Next, we bound the second derivative of the change in the loss function under the assumption that | h i 1 | X  1,
If we combine the two bounds together and take a step size  X  1 =  X S 0 N 2 L 2 , the reduction of the loss function C least  X  function of the other labels will at least be unaffected, the joint loss function C will also at least be reduced by  X  In this case, if a different candidate model h k is selected in MSSBoost, its joint loss reduction must be more than using h 1 based on our model selection criterion. Now since the initial joint loss is NL and thus within this number of still have a joint loss large than S 0 L . This obviously gives a contradiction and thus the b oosting property is proved.
The computational complexity of MSSBoost mainly comes from (a) the generation of base models from bootstrapped data/feature spaces, and (b) minimization of the joint loss function using adaptive Newton steps. Since the second step only involves the operations of matrix computations and a scalar inverse, its effect on the computational complexity of the overall system is not influential as the calculations are much faster than a generation of base models in first step.
As noted in Section 4.1, our current implementation needs to generate L + T base models in total, where each model is learned with Nr d examples and Mr f features. Thus the to-model learning time with n examples and m features. Sim-ilarly, the total prediction time is T  X  t p ( Nr d ,Mr f t ( n, m ) is the base model prediction time. The form of t and t p is determined by the choice of the underlying base models. For example, the learning time of decision trees is t ( n, m )= O ( mn log n ) (Chapter 9 in [11]) and the predic-tion time is t p ( n, m )= O ( mn )ifweassumethebasemodel size is proportional to the number of data and features. Most current support vector machine(SVM) implementa-tions such as [13] have a learning time of t l ( n, m )= O ( mn and a prediction time of t p ( n, m )= O ( mn )ifthenumber of support vectors is proportional to the number of train-ing data. Compared with a baseline classifier that learns amodelfrom N examples and M features for each label, MSSBoost can achieve a speedup of T + L L r d r f in learning and T L r d r f in prediction with decision trees. Similarly, with SVMs, MSSBoost can achieve a speedup of T + L L r 2 d r f r d r f in learning and prediction respectively. This can re-sult in a significant speedup in general. For instance, if we set r d =0 . 1, r f =0 . 1and T =3 L , the learning and predic-tion time of MSSBoost with SVMs can be reduced around 250 times and 33 times over the baseline.
Ensemble learning approaches such as boosting and bag-ging [8, 3, 18] have been demonstrated to be effective in im-proving the classification accuracy of weak learned models. By combining the idea of ensemble learning with random subspace methods, random forest [4] shows the possibility of developing efficient and effective classifiers using a set of tree models. The similar idea has also been applied to the task of image retrieval with SVMs as the base mod-els [22]. Along another line, Caruana et al. [6] proposed an ensemble selection approach for choosing the most useful models from a large model library via a forward stepwise selection method. However, since most ensemble learning approaches are developed on binary classification, their ex-tensions to multi-label classification either need to maintain independent binary classifiers for each label, or require the underlying models to be able to generate multi-label out-puts, which prevent them from capturing the common repre-sentations across labels in the ensemble aggregation level. In this sense, the AdaBoost.OC algorithm [19] is a more closely related to our work, which me rges AdaBoost with the Error-Correcting Output Codes (ECOC) to handle the multi-class learning problem with a pool of binary classifiers. It only needs to keep track of one set of data distributions for mul-tiple classes at the same time. However, because multi-class learning problems assume each data point can only associate one single class, their algorithm is not directly applicable to the multi-label problem.

In the machine learning community, the idea of sharing the common information among multiple labels have been investigated by the methods called  X  X ulti-task learning X , which has also been known as  X  X ransfer learning X  and  X  X earn-ing to learn X . These methods handle the multi-label classi-fication problem by treating each label as a single task and generalizing the correlations among multiple tasks. From another viewpoint, they can also be thought as learning a common feature representation across all the tasks. Many methods have been proposed for multi-task learning, such as neural networks [5], regularization learning methods [1], hierarchical Bayesian models [26] and so on. For exam-ple, Zhang et al. [26] proposed a latent variable model for multi-task learning which assumes that tasks are sparse lin-ear combination of basic classifiers. Ghamrawi et al. [10] proposed a multilabel conditional random field classifica-tion model that directly parameterize label co-occurrences in multi-label classification. The listed approaches are usually more computationally expensive than the baseline single-task learning algorithms because they often use the single-task learners in an iterative process and require a complex inference effort to estimate the task parameters. This clearly contrasts with the MSSBoost algorithm which can consider-ably improve the learning efficiency in multi-label scenarios. In addition, MSSBoost can be build on any base models without being limited to a specific type.

The problem of exploring and leveraging the connections across labels have also been seen in many other research areas. One such example is the domain of image annota-tion and object recognition which aims to detect a number of scenes and objects in the given images. For instance, Snoek et al. [21] proposed an semantic value chain architec-ture including a multi-label learning layer called context link . Yan et al. [24] studied various multi-label relational learn-ing approaches via a unified probabilistic graphical model representation. However, these methods need to construct an additional computational intensive layer on top of the base models, and they do not provide any mechanisms to reduce the redundancy among labe ls other than utilizing the multi-label relations. Torralba et al. [23] developed a joint boosting algorithm that finds the common image features shared across multiple labels. But this method are specif-ically built above the level of image features rather than the general base models in our methods. In addition, its learning process is computationally demanding, as it needs to search either 2 L  X  1 sharing patterns or L ( L +1) / 2inits simplified version.
We demonstrate the effectiveness and efficiency of the pro-posed MSSBoost algorithm on several multi-label data col-lections, including a synthetic dataset and two real-world multimedia collections. The statistics of the data collections are summarized in Table 1.
 Figure 4: The error curves vs. number of base mod-els used for the synthetic dataset.

To illustrate MSSBoost can automatically discover a set of common base models shared across multiple labels, we gen-erate a synthetic dataset with 3 labels in a two-dimensional feature space. This dataset contains 1000 data points uni-formly sampled inside a rectangle area. The samples of each label follow a different Gaussian distribution. In total, we have 200, 200 and 400 samples for the first, second and third label respectively. Some of these samples are associated with multiple labels and some of them do not have any labels. We randomly select 75% of the samples to construct a training set and left the rest to be a testing set. Figure 1 shows the data distribution and their labels in the testing set. The training set has a similar distribution with the testing set. The base models are learned using decision stumps, i.e., de-cision trees of depth 1, on a randomly selected 1-dimensional feature space and a set of bootstrap training samples with a sample ratio 10%, or equivalently 75 samples.

Figure 3(a) shows the decision boundaries of the first eight base models generated by MSSBoost. Many of these base models can be re-used in the decision functions of several labels. The base model 1 can serve as the right boundary for all three labels, and the base model 2 can serve as the left boundary. With a mere 8 base models but allowing them shared among labels, MSSBoost constructs fairly ac-curate decision boundaries for all three labels as shown in Figure 3(b). Figure 3(c) shows the decision boundary of NS-Boost where the model sharing ability is disabled. We find that the decision boundaries are noticeably worsen even if they use the same number of base models.
 To provide a deeper insight for the learning algorithms, Figure 4 plot the learning curves for three learning algo-rithms, i.e., MSSBoost, NSBoost and RSBag. The number of base models grows from 1 to 20. We can observe that MSSBoost converges to its optimal performance with only 5 base models, while NSBoost requires a number of at least 12 based models to achieve the similar performance and RSBag is not able to capture the most useful models within 20 iter-ations. This additional decision power of MSSBoost clearly stems from the introduction of model-sharing ability into the learning process.
The following experiments evaluate the proposed algo-rithms based on a image annotation task on two real-world multimedia collections. The purpose of this task is to cate-gorize a given image or video keyframe into a set of manually defined semantic labels based on low-level visual features.
TREC The TRECVID video collections have become the
Consumer The second dataset is a consumer image col-10 5  X  1 . 3  X  10 6  X  1 . 3  X  10 6  X  1 . 2  X  10 6 10 5  X  8 . 0  X  10 5  X  8 . 0  X  10 5  X  8 . 0  X  10 5
In both datasets, three types of low-level features are gen-erated for each image [15]: 166-dimensional color correlo-gram, 81-dimensional color moments, and 96-dimensional co-occurrence texture vector. We concatenate these three features into a 343-dimensional vector and use it as the image representation to learn the visual models. For the task of image annotation, the classification accuracy is not a preferred performance measure since the number of labeled samples is usually much smaller than non-labeled data. In-stead, NIST defines average precision as a measure of effec-tiveness. For each label, let R be the number of true labeled images in the collection of N images. At any given index j , let R j be the number of correctly labeled images in the top j most confident images predicted by the learning algorithm. Let I j =1ifthe j th image is relevant and 0 otherwise. The average precision (AP) is then defined as 1 R N j =1 R j Mean average precision (Mean-AP) is the mean of average precisions for all the labels.

We randomly select 75% of the data examples to construct a training set, and use the remaining 25% for testing. The training time are measured on a Windows machine with a 2.16GHz CPU and 2G main memory. The data sampling ra-tio r d and the feature sampling ratio r f for base models are determined using a two-fold cross validation on the training set. First, we learn a baseline classifier with all the data and features. Then we search a list of pre-defined ratios for the minimal ones that can achieve at least 80% of the baseline performance on average. Finally, we have r d =0 . 2 ,r f =0 . 1 for the TREC collection and r d =0 . 1 ,r f =0 . 1 for the Con-sumer collection. We learn the base models using SVMs with a RBF kernel K ( x i ,x j )= e  X   X  x i  X  x j 2 because the im-age data are typically not linearly separable.  X  is set to 0 . 05 based on two-fold cross validation. Although SVMs are not considered as  X  X eak classifiers X  in general, learning SVMs on a small size of data samples and feature subspace can be quite unstable and thus unable to significantly reduce the training error. In this case, plugging SVMs into a boosting learning algorithm can be beneficial. In order to improve the robustness for estimating  X  t , we apply a  X  2 test with a confidence interval of 2.5% to filter out the irrelevant base models for each label before it s learning process. We run all the ensemble learning approaches up to 100 iterations. To increase robustness of performance retrieval reporting for each approach, we repeat the experiments for 10 times and report their average performance. Table 2 summarizes the overall performance (in terms of Mean-AP), training time (in terms of seconds), number of base models and total model size on both multimedia col-lections for six learning algorithms, include a baseline algo-rithm which learns an independent model for each label from all the data examples and feature space (Baseline), a bag-ging algorithm which combines 10 bagging models learned via the baseline classifier for each label (Bagging), a ran-dom subspace algorithm that learns an independent model for each label with the same data/feature sampling ratios as the default setting (Subspace), a round-robin random sub-space bagging algorithm (RSBag), a non-model-shared sub-space boosting algorithm(NSBoost) and the model-shared subspace boosting algorithm(MSSBoost). The confidence interval on the classification performance are reported on a confidence level of 95%. The total model size is com-puted by multiplying the number of support vectors with the number of features. First, we can observe that RSBag can gain considerable improvement over the random sub-space learning algorithm, which suggests that aggregating multiple subspace SVM classifiers can be helpful. NSBoost brings an further performance gain with the same number of base models and a slightly more computational intensive training process by selecting the most useful base models in a gradient way. Finally, MSSBoost outperforms all five other algorithms in both collections by means of allowing base models to be shared across labels. Its improvement is statistically significant over four out of all five methods, i.e., Baseline, Subspace, RSBag and NSBoost. It is worth point-ing out that MSSBoost has a considerably shorter learning process and a much smaller model size than the baseline classifiers. For instance, it achieves a 60-fold speedup for the training process and a 40-fold reduction on the final model size on the TREC collection. This observation confirms the effectiveness and efficiency of MSSBoost.
 Figure 6: Comparison of the prediction time be-tween Baseline and MSSBoost on all the testing examples. For MSSBoost, we identify the minimal number of base models needed to achieve 100% and 90% of baseline performance, and report the predic-tion time. Left: TREC, Right: Consumer.
 In Figure 5 we compare the learning curves of RSBag, NS-Boost and MSSBoost for both multimedia collections when the number of base models is growing from 1 to 100. All three algorithms have better Mean-APs with more base mod-els available. From the learning curves, we find that MSS-Boost can bring even more noticeable improvement when the number of base models is small. For example, when us-ing 35 base models in the TREC collection, the Mean-AP of MSBoost is about 7% higher (or relatively 20% higher) than that of NSBoost. From another viewpoint, MSSBoost can use a lower number of base models to reach the same level of classification performance as NSBoost and RSBag. For instance, MSSBoost only needs a minimum of 36 base mod-els to produce a Mean-AP of 0.38 in TREC, while NSBoost and RSBag will require 54 models and 60 models to reach the same level. Figure 6 compares the prediction time of MSSBoost with the baseline algorithm. As we can observe, if MSSBoost is expected to have the same performance as baseline, it can reduce the prediction overhead by 20 times for the TREC collection and 30 times for the Consumer col-lection. If the expectation of MSSBoost is lowered to 90% of the baseline performance, the prediction overhead can be reduced by 35 times for the TREC collection and 60 times for the Consumer collection.

Figure 7 shows the label-by-label performance comparison between RSBag, NSBoost and MSSBoost in the TREC col-lection. We split the figure into two regions with a dashed Figure 7: Average precision of RSBag, NSBoost and MSSBoost for each label in the TREC collection. vertical line. The right region contains all the labels that do not aggregate any base models from other labels, and the left region contains everything else. Inside each region, we sort the labels in a descending order of their frequencies. It can be found that the benefits of MSSBoost mainly comes from the labels that leverage base models from other related labels. Among them, the improvement of MSSBoost is quite consistent on the frequent labels, where it provides higher average precision than the other two methods on 8 out of the 9 most frequent labels. Overall, MSSBoost outperforms NSBoost on 30 (out of 39) labels and it outperforms RS-Bag on 34 (out of 39) labels, which are both statistically significant with a p -value &lt; 0 . 01 using sign test.
Finally, Figure 8 illustrates the combination weight  X  t l the base models h t for the iteration t from 1 to 30. The se-lected base models in the earliest stage are mostly built from some general labels such as  X  X ars X ,  X  X tudio X ,  X  X eader X  and so on. A close examination on the other dimension of the matrix indicates that most labels make use of the predic-tions from other related labels. For instance, the label  X  X r-ban X  can benefit from the models of its sub-concepts such as  X  X ar X ,  X  X oad X ,  X  X irplane X  and so on. On the other hand, the label  X  X olice X  can benefit from its super-concept  X  X ilitary X . These examples shows the advantages of model-sharing from two different perspectives.
In this paper, we proposed a boosting-type learning algo-rithm called model-shared subspace boosting (MSSBoost). It can automatically find, share and combine a number of Figure 8: Matrix representation for the combination weight learned by MSSBoost. Each column indi-cates a selected base model with its associated label shown above. Each row indicates a label. Each grid corresponds to the weight of each base model, where blue means positive and grey means negative. random subspace models across multiple labels. This al-gorithm is able to reduce the information redundancy in the label space by jointly optimizing the loss functions over all the labels. Meanwhile, they enjoy the advantage of be-ing built on small base models, which are learned on a small number of bootstrap data samples and a randomly selected feature subspace. Our experimental results on a synthetic dataset and two real-world multimedia collections have demonstrated that the proposed MSSBoost algorithm can outperform the non-ensemble baseline classifiers with a significant speedup on the learning and prediction process. It can also use a smaller number of base models to achieve the same classification performance as its non-model-shared counterpart. As a by-product, the proposed algorithm can automatically discover the underlying context between mul-tiple labels, which offers us additional insights to analyze the data collections. Our future work includes extending the proposed approach to other multi-label classification tasks such as text classification and protein function prediction.
