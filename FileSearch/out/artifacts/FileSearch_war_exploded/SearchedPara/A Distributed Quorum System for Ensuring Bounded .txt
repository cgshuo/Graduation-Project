 Good scalability, high availability and low latency are required by modern distributed machines to improve availability and performance by duplicating and parallelizing services among multiple replicas. To provide predictably low latency and throughput for reads and writes, systems often sacrifice consistency guarantees for data items [1]. However, weak-consistency systems make no general guarantees on the staleness of data items returned except that the system will eventually return the most recent version in the absence of new writes [2]. 
Distributed quorum systems [3] [4] can be used to ensure strict consistency across multiple replicas of a data item by overlapping the read and write replica sets: given N replicas and read and write quorum sizes R and W , R + W &gt; N is guaranteed. However, if R replicas have the same data items, the quorum system has duplicated overheads when responding to requests for the same data item. By employing partial quorums where R + W  X  N , the duplicated overheads are reduced in quorum replication. However, the staleness of the data item is potentially unbounded and sometimes enterprise customers cannot accept such unbounded staleness, even though these quorum systems have a high probability of returning the freshest value [1]. 
We have devised a key-value store with a new algorithm for partial quorum replication that guarantees bounded staleness. In our partial quorum replication, we use t -visibility [1], which means a data item must be available for reads within t seconds after it is written. With this constraint, read data items retuned to clients are the most recently changed t seconds before or written within t seconds. 
To guarantee t -visibility, we propose a new protocol to share the replication statuses among replicas. With this mechanism, each replica is aware of the existence of a data mechanism in our key-value store. 
We make the two major contributions in this paper:  X 
Protocol to guarantee t -visibility: We developed a new protocol to share replication statuses using wall clock information among the replicas of quorum systems. The shared replication status is used to guarantee the t -visibility of the returned values.  X 
Cadidas: We implemented a new key-value store, Cadidas, which guarantees t -visibility by using our new protocol. Cadidas clients can use partial quorums to improve their performance by specifying t -visibility in their SLAs. 
This paper is structured as follows: Section 2 describes quorum systems in theory and in practice. Section 3 describes the Cadidas implementation. Section 4 reports empirical performance results for Cadidas. Section 5 surveys the related work. In this section, we provide background regarding quorum systems both in the theoretical academic literature and in practice. We begin by introducing prior work on traditional quorum systems. We next discuss practical quorum systems, focusing on the most widely deployed protocol for storage systems employing quorum replication. Finally, we survey consistency to ensure bounded staleness in quorum systems. 2.1 Quorum Systems in Theory A quorum system is one of the popular strategies to replicate data in distributed systems [3] [4]. With quorum replication, a data item is shared among multiple replicas. When a new version of the data item is written, copies of the data item are sent to a set selected by comparing the versions of the fetched copies. For each operation, the read and write quorums are chosen from all of the replicas in the quorum system. Usually, the sizes of the read and write quorums are fixed, R and W respectively, that are less than the total number of the replicas N . overlapping replica in the read and write quorums always exists, consistency is ensured quorum, this data item is available by read ing the copies in its read quorum, because included in the read quorum. When multiple overlapping replicas exist in the two quorums, there are redundant operations for reads and writes. overlapping replicas in the read and write quorums may not exist, there are fewer redundant operations because the number of overlapped replicas in the two quorums is decreased. Though the performance of these partial quorum systems is better than for strict quorum systems, they cannot ensure consistency because there may be no replicas in quorums are randomly selected and the gaps between the versions of data items in the replicas probabilistically small after the data item is written multiple times. However, the staleness of data items in replicas is potentially unbounded and the clients may read too obsolete data items. 2.2 Quorum Systems in Practice In practice, many of the key-value store systems use quorums as a replication mechanism for eventual consistency, including Dynamo [11], Cassandra [16], and Voldemort [15]. These systems use one quorum system for each key-value pair and the Their clients select a subset of the replicas to access the value of the key. 
Though practical quorum systems are based on the well-studied theoretical quorum (configurations of R + W &gt; N ensures consistency). 
Second, theoretical quorum systems assume that all of the writes for a data item can be ordered only by the replicas holding that data, unlike some of the implemented quorum systems [12] [15]. In practical system, the clients must assist in setting the order of the writes. In Dynamo [12] and Voldemort [14], replicas generate vector clocks to order the writes for a value of a key and the clients send a value with a new vector clock generated on a replica for each write. The vector clocks are partially ordered and some of them may be in conflict. All of the values of the conflicting versions are kept in the replicas and the clients select or generate a value from them when they read a value of the key. 
In spite of these differences, the way partial quorums are used is the same: The sizes for R and W are set low so there may be no overlapping nodes exist in the read and write quorums ( R + W  X  N ). In practical quorum systems, the sizes of R and W are configurable improved while reducing the redundant processing in the replicas. However, the staleness of the values returned in partial quorums is potentially unbounded, which means their clients may read obsolete valu es. These inconsistencies are not acceptable for many applications, such as banking applications. 2.3 Bounded Staleness Staleness is measured by two axes in the literature: versions [1] [11] [8] and wall clock item is guaranteed. Using a wall clock, reading a version t seconds after it is written is guaranteed. These metrics are called k -staleness and t -visibility respectively in [1]. In the work described here, we focus only on t -visibility because financial systems have many time-based constraints (For example, Japanese FX systems must guarantee slippage within 10 seconds). 
The t -visibility idea is defined in [1] 1 .In this definition, a committed version means a value written in W replicas. Definition. A quorum system guarantees t -visibility consistency if any read quorum started at least t units of time after the latest version committed returns at least one value that is at least as recent as the last committed version. 
Fig. 1 shows two writes and two reads in a partial quorum system that consists of r ( k because two replicas ( W =2) must write them. been already committed at c 5 that is more than 2s before c 9 . 
Though constraints for staleness have been studied in database replication [13] [14], they assumes writes are totally ordered. There are few studies for practical quorum systems [1] and no implementation to support t -visibility in our survey. We have designed Cadidas (Continuously Available DIstributed DAta Store), a new exchange key and versions of stored values and recognize each others replication statuses. With these shared statuses, the replicas can check the freshness of their stored values and respond to their read requests from clients while ensuring t -visibility. If the replica can ensures t -visibility, then a client only needs to access a few replicas in the partial read quorum. 3.1 Architecture Cadidas consists of a number of machines that can store large number of key-value pairs. The key-value pairs are partitioned among the shards by using hashcodes for the keys, where N servers (replicas) are assigned to each shard. A quorum system is constructed for the reading and writing of a key-value pair with R and W replicas, and vector clocks are used to order all of the writes for each key-value pair. The value of N , operation, a client identifies the N replicas for a key from its hashcode and selects the R or W replicas from the N replicas to construct the read or write quorums. 
All of the clients and the replicas are implemented in Java. Key-value pairs are stored in memory, and they are recovered when a replica restarts due to a failure. 3.2 Synchronizing the Replication Statuses To bound the staleness of a replica, each replica maintains a real time vector [9] [10] for each key-value pair. The vector has an entry for each replica in the quorum system. r at real time t . For example, if a replica has a real time vector [10:00:02, 10:00:00, 10:00:02, of r 1 at 10:00:00, and of r 2 at 10:00:01. 
Replicas periodically exchange their latest v ector clocks to confirm the latest values exchanged. With these exchanged vector clocks and their times, each replica can maintain a real time vector for each key-value pair. 
In a distributed environment, there may be no global clock. Therefore, each replica uses its local clock to record when the vector clocks are exchanged. All of the communication is pull-based and the time is recorded when the communication starts. Fig. 2 shows an example of the message exchanges between two replicas, r 0 and r 1 to Similarly, from the second and third communication exchanges started at c 4 and c 7 , r 0 communication exchange, to reduce the communication overheads, r 1 sends an empty message because r 1 knows 2 vc 2 and vc 2 X  were received by r 1 . 
While sharing the latest vector clocks among the replicas, each replica maintains a real time vector for each key-value pair. Fig. 3 shows an example of the change history of the real time vectors for the same read and write requests shown in Fig. 1. The local vector for a key-value pair is expressed in the form [ c i , c j , -], that means this pair was the latest at c i and c j in r 0 and r 1 , respectively, but was not stored at r 2 . entry becomes c 7 since r 0 knew vc 2 was fresh at r 1 based on the exchange from c 7 . 
Since each real time vector indicates when the value of each replica was fresh in each of the other replicas, t -visibility can be guaranteed by using the real time vectors. time c 4 . Because there are two seconds between c 4 and c 6 and two replicas ( R =2) have the value, r 0 can guarantee the 2s-visibility for the returned value at c 6 . 
Though there is overhead in sharing the vector clocks to maintain the real time implementation techniques. First, the requests and responses of versions for multiple key-value pairs are combined when a replica is requesting one replica X  X  versions. Second, if there are no updates for some key-value pairs since the last exchange, the responses don X  X  need to include those vector clocks. Third, each replica only needs to possible, such as sending only the differences in vector clocks, avoiding sending vector clocks for write-only values. Because the numbers of replicas in practice are limited to three or four, the communication overhead s is small after these optimization are applied. 3.3 Implementation Replica. The replicas store lists of values and their vector clocks for each key-value pair. When the pair of a new value and a new vector clock is received to write the value returns an acknowledgement to the sender. These lists store only the latest values. If a written vector clock is later than the others in the list of vector clocks, then the older vector clocks and its corresponding values are removed from the lists. 
When a key arrives for reading a key-value pa ir, the replica returns to the sender the key X  X  lists of values and vector clocks. When time t is received with the key to specify t -visibility, each replica checks the real time vector of the key-value pair, and returns an error if t -visibility is not guaranteed. Client. Fig. 4 shows the client algorithm to read and write key-value pairs. Mostly, it behaves the same as the clients of Voldemort except for reading the staleness values. 
Each client identifies all of the replicas of a quorum for a key-value pair and sends requests to a subset of the replicas to read and write the values of the keys. To write a clock on the replica. With this new vector clock, the client sends the new value to all of the replicas and waits for W acknowledgements. 
To read a value, a client selects a replica fr om the identified replicas and tries to read replica cannot guarantee t -visibility, the client reads values of R replicas among the N replicas (strict quorum). If they return multiple values, then the client repairs them and application of Cadidas when the application initializes the client. The repaired value is sent only to a strict quorum and is not sent to a partial quorum. This is because the returned values from a partial quorum must be shared among at least R replicas (an error is returned if the values are not shared in R replicas). On the other hand, the returned values from a strict quorum may not be shared among R replicas, and the client needs to ensure the repaired value is available for reads in the future. In this section, we evaluate how much t -visibility improves the performance. First, we compare the throughputs of the original Cadidas and an incomplete Cadidas that disables the version exchange function and evaluate the overhead to support bounded staleness. Second, we assess the performance by changing the t of t-visibility. For both of the evaluations, we used the YCSB (Yahoo! Cloud Serving Benchmark). 4.1 Configurations We configured one shard in the system to focus on the maximum throughput the system can provide. We set N to three because some practical systems use three replicas for each quorum. The values of R and W were set to two. 
To evaluate the overhead, we implemented a generic quorum system by disabling the function for version exchanges in Cadidas. By setting R to one and then two, we created partial and strict quorums. 
We then used YCSB to evaluate the performance and memory usage of the key-value stores. YCSB is intended to benchmark data storage systems for cloud systems by providing configurable workloads that simulate Web applications. There are six predefined core workloads with YC SB: update-heavy, read-mostly, read-only, read-latest, short-ranges and read-modify-write. We used update-heavy, read-mostly and read-only with the respective read-write ratios of 50:50, 95:5, and 100:0. In these workloads, the clients ask to access the values of the keys with a zipfian distribution. The number of the records was configured as 100,000. All of the servers in the key-value store share all of the data to evaluate the replication performance. That is, there is no partitioning of the key-value stores. 
The machine configuration for this evalua tion consists of four machines (each a 64-bit 2-core POWER6 4.0-GHz x2 with 12 GB of RAM, Red Hat Enterprise Linux Server release 6.4) for the four replicas and one machine (a 64-bit 8-core Xeon E5-2680 2.7-GHz x2 with 32 GB of RAM, Red Hat Enterprise Linux Server release 6.3) for the client. All of the machines are conn ected via 1-Gbps Ethernet cables. 4.2 Results First, we evaluated the overhead to guarantee bounded staleness. We configured Cadidas to allow any staleness of the values, so the staleness of the returned values was ignored. Because the Cadidas always uses partial quorums to read values, the overhead of the version-exchange mechanism described in 3.2 can be evaluated by comparing that mechanism with the partial quorum. We used several intervals between the versions exchanged in the replicas and measured throughputs with YCSB. We used sixty threads of the test driver and measured five times the average of five throughputs of 60-seconds runs. 
Fig. 5 shows the throughputs of the strict quorum, partial quorums, and Cadidas with different frequencies for the version exchanges. To evaluate only overheads of the version Cadidas with zero intervals showed worse performance than the strict quorum, the throughputs of Cadidas with the non-zero intervals were better than the strict quorum and mostly similar to the partial quorum. In addition, the frequency of version exchanges did results showed the overhead for version exchange in Cadidas was small. Second, we evaluated the performance improvement by allowing reads of stale data. The frequency of version exchanges was set to 50 ms. Fig. 6 shows the throughput of t-visibilities. When the read ratio in workloads increased, Cadidas performed better for all t-visibility settings. Also, with 50-ms-visibility and larger visibility, there was little performance improvement. 
Fig. 7 shows the throughputs of Cadidas with 1-s-visibility, a strict quorum, and a partial quorum with different numbers of clients. The performance of Cadidas was degraded when more than fifty clients were running. This is because the Cadidas replicas became overloaded and the Cadidas c lients used their strict quorums to read the values. When 128 clients were running, 84.7% values in update-heavy, 12.8% values in read-mostly, and 0.2% values in read-only were read from strict quorums. 
Fig. 8 shows the guaranteed staleness with the replicas of Cadidas in the evaluation of Fig. 7. When the ratio of writes increased, the guaranteed staleness became worse as request and the version exchanges were stalled among the replicas. Recall that the CAP Theorem states that it is not possible to have all three of consistency, availability, and partition tolerance, so eventual consistency to provide availability in the face of partitions is a popular consistency model in practical systems [1]. In this paper, we discussed an eventually consistent key-value store that uses quorum replication and their optimized the performance with reducing the consistency of the replications with deterministic bounds on the staleness. 
There are studies of distributed systems that support consistency with bounded staleness for reads. Beehive [6] provides shared memory systems to a parallel programming model with a guarantee of delta consistency to ensure the written data becomes visible to all readers within delta time. Using a similar concept, timed serial consistency and timed causal consistency were proposed based on sequential and causal consistency [7]. FRACS [8] provides optimized read performance with replicas that buffer the updates to be replicated within a configured time window and responds to read requests with stale data that has been already replicated. In addition to staleness, TACT [9] [10] uses the numbers of numerical errors and order errors as criteria for the data to be returned by exchanging replication statuses and maintaining real time vectors within the replicas. AQuA [11] provides optimization for each client to select a replica that responds to a read request with speci fied bound on the staleness while predicting response time. These systems do not manage the numbers of replicas to write and read each value. In contract, quorum systems, including Cadidas, can improve write latencies by avoiding delays for the slow replicas. Quorum systems can also respond partial quorum does not satisfy the required freshness. 
In several databases [13] [14], the staleness is bounded in a the primary-backup architecture with lazy replication. However, when the primary of the servers crashes, the clients must wait several seconds until a replica becomes the new primary. 
As discussed in Section 2.2, Dynamo [12] and Voldemort [15] use quorum systems to read and write data. Cassandra [16], behaves in the same way except for the customized read-repair function. Though they su pport strict and partial quorums, bounded staleness is not guaranteed. Cadidas provides a mechanism to support bounded staleness for read values. This paper described a highly available and high-performance key-value store, Cadidas, which supports a new kind of quorum replication. With our replication, replicas maintain fresh values by periodically exchanging their latest versions with other replicas and by responding to read requests from clients only when they can ensure the freshness requested by the clients. We evaluated the overhead of the version exchanges and the performance of Cadidas with the YCSB benchmarks. We confirmed our replication outperforms the existing replication approach that supports strict consistency, while Cadidas has only small overhead for the version exchanges. 
