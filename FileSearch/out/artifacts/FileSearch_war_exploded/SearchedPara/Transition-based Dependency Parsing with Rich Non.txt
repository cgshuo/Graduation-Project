 Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010) utilize a deter-ministic shift-reduce process for making structural predictions. Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local fea-tures, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; Mc-Donald et al., 2005; McDonald and Nivre, 2007).
Recent research has addressed two potential dis-advantages of systems like MaltParser. In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon greedy one-best search, and positive results were re-ported. In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone.

In this short paper, we study a third aspect in a statistical system: feature definition. Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing ac-curacy. Various recent attempts have been made to include non-local features into graph-based de-pendency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Transition-based parsing, by contrast, can easily accommodate arbitrarily complex representations involving non-local features. Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transition-based dependency parsers incorporate some non-local features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a). We explore considerably richer feature representations and show that they improve parsing accuracy significantly.

In standard experiments using the Penn Treebank, our parser gets an unlabeled attachment score of 92.9%, which is the best result achieved with a transition-based parser and comparable to the state of the art. For the Chinese Treebank, our parser gets a score of 86.0%, the best reported result so far. In a typical transition-based parsing process, the in-put words are put into a queue and partially built structures are organized by a stack. A set of shift-reduce actions are defined, which consume words from the queue and build the output parse. Recent research have focused on action sets that build pro-jective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, which the actions are:  X  Shift , which removes the front of the queue  X  Reduce , which pops the top item off the stack;  X  LeftArc , which pops the top item off the  X  RightArc , which removes the front of the Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized percep-tron (Collins, 2002) for global learning and beam-search for decoding. Unlike both earlier global-learning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels. Hence our work is in line with Titov and Henderson (2007) in using labeled transi-tions with global learning. Moreover, we will see that label information can actually improve link ac-curacy. At each step during a parsing process, the parser configuration can be represented by a tuple h S, N, A i , where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. Denoting the top of stack from single words S 0 wp; S 0 w; S 0 p; N 0 wp; N 0 w; N 0 p;
N 1 wp; N 1 w; N 1 p; N 2 wp; N 2 w; N 2 p; from word pairs S 0 wp N 0 wp; S 0 wp N 0 w; S 0 w N 0 wp; S 0 wp N 0 p; S 0 p N 0 wp; S 0 w N 0 w; S 0 p N 0 p from three words with S N leftmost and rightmost modifiers of S S of N shown in Table 1. These features are mostly taken from Zhang and Clark (2008) and Huang and Sagae (2010), and our parser reproduces the same accura-cies as reported by both papers. In this table, w and p represents the word and POS -tag, respectively. For example, S that takes the word and POS -tag of N bines it with the word of S
In this short paper, we extend the baseline feature templates with the following: Distance between S Direction and distance between a pair of head and modifier have been used in the standard feature templates for maximum spanning tree parsing (Mc-Donald et al., 2005). Distance information has also been used in the easy-first parser of (Goldberg and Elhadad, 2010). For a transition-based parser, direction information is indirectly included in the LeftArc and RightArc actions. We add the dis-tance between S bining it with the word and POS -tag of S as shown in Table 2.

It is worth noticing that the use of distance in-formation in our transition-based model is different from that in a typical graph-based parser such as MSTParser. The distance between S correspond to the distance between a pair of head and modifier when an LeftArc action is taken, for example, but not when a Shift action is taken. Valency of S The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al. (2009) and Sagae and Tsujii (2007). We include similar infor-mation in our model. In particular, we calculate the number of left and right modifiers separately, call-ing them left valency and right valency , respectively. Left and right valencies are represented by v in Table 2, respectively. They are combined with the word and POS -tag of S templates.

Again, the use of valency information in our transition-based parser is different from the afore-mentioned graph-based models. In our case, valency information is put into the context of the shift-reduce process, and used together with each action to give a score to the local decision. Unigram information for S The head, left/rightmost modifiers of S leftmost modifier of N arc-eager transition-based parsers we are aware of through the combination of their POS -tag with infor-mation from S the feature templates  X  X rom three words X  in Table 1. We further use their word and POS -tag information as  X  X nigram X  features in Table 2. Moreover, we include the dependency label information in the unigram features, represented by l in the table. Uni-gram label information has been used in MaltParser (Nivre et al., 2006a; Nivre, 2006).
 Third-order features of S Higher-order context features have been used by graph-based dependency parsers to improve accura-cies (Carreras, 2007; Koo and Collins, 2010). We include information of third order dependency arcs in our new feature templates, when available. In Table 2, S of S rightmost modifier of S modifier of N include unigram word, POS -tag and dependency labels of S POS -tag combinations with S 0 and N 0 .
 Set of dependency labels with S As a more global feature, we include the set of unique dependency labels from the modifiers of S and N and POS -tag of S In Table 2, s the left and right of the head, respectively. Our experiments were performed using the Penn Treebank ( PTB ) and Chinese Treebank ( CTB ) data. We follow the standard approach to split PTB 3, using sections 2  X  21 for training, section 22 for develop-ment and 23 for final testing. Bracketed sentences from PTB were transformed into dependency for-and Sagae (2010), we assign POS -tags to the training data using ten-way jackknifing. We used our imple-mentation of the Collins (2002) tagger (with 97.3% accuracy on a standard Penn Treebank test) to per-form POS -tagging. For all experiments, we set the beam size to 64 for the parser, and report unlabeled and labeled attachment scores (UAS, LAS) and un-labeled exact match (UEM) for evaluation. Z&amp;C08 transition 91.4% 41.8%  X 
H&amp;S10 91.4%  X   X  this paper baseline 91.4% 42.5% 90.1% this paper extended 92.9% 48.0% 91.8% MSTParser 91.5% 42.5%  X  K08 standard 92.0%  X   X  K&amp;C10 model 1 93.0%  X   X  K&amp;C10 model 2 92.9%  X   X  4.1 Development Experiments Table 3 shows the effect of new features on the de-velopment test data for English. We start with the baseline features in Table 1, and incrementally add the distance, valency, unigram, third-order and label set feature templates in Table 2. Each group of new feature templates improved the accuracies over the previous system, and the final accuracy with all new features was 93 . 14% in unlabeled attachment score. 4.2 Final Test Results Table 4 shows the final test results of our parser for English. We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row  X  X &amp;C08 transition X ), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row  X  X &amp;S10 X ), and graph-based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row  X  X 08 baeline X ), and the two models of Koo and Collins (2010). Our extended parser sig-nificantly outperformed the baseline parser, achiev-Z&amp;C08 transition 84.3% 32.8%  X 
H&amp;S10 85.2% 33.7%  X  this paper extended 86.0% 36.9% 84.4% ing the highest attachment score reported for a transition-based parser, comparable to those of the best graph-based parsers.

Our experiments were performed on a Linux plat-form with a 2GHz CPU. The speed of our baseline parser was 50 sentences per second. With all new features added, the speed dropped to 29 sentences per second.

As an alternative to Penn2Malt, bracketed sen-tences can also be transformed into Stanford depen-dencies (De Marneffe et al., 2006). Our parser gave 93.5% UAS, 91.9% LAS and 52.1% UEM when trained and evaluated on Stanford basic dependen-cies, which are projective dependency trees. Cer et al. (2010) report results on Stanford collapsed de-pendencies, which allow a word to have multiple heads and therefore cannot be produced by a reg-ular dependency parser. Their results are relevant although not directly comparable with ours. 4.3 Chinese Test Results Table 5 shows the results of our final parser, the pure transition-based parser of Zhang and Clark (2008), and the parser of Huang and Sagae (2010) on Chi-nese. We take the standard split of CTB and use gold segmentation and POS -tags for the input. Our scores for this test set are the best reported so far and sig-nificantly better than the previous systems. We have shown that enriching the feature repre-sentation significantly improves the accuracy of our transition-based dependency parser. The effect of the new features appears to outweigh the effect of combining transition-based and graph-based mod-els, reported by Zhang and Clark (2008), as well as the effect of using dynamic programming, as in-Huang and Sagae (2010). This shows that feature definition is a crucial aspect of transition-based pars-ing. In fact, some of the new feature templates in this paper, such as distance and valency, are among those which are in the graph-based submodel of Zhang and Clark (2008), but not the transition-based sub-model. Therefore our new features to some extent achieved the same effect as their model combina-tion. The new features are also hard to use in dy-namic programming because they add considerable complexity to the parse items.

Enriched feature representations have been stud-ied as an important factor for improving the accu-racies of graph-based dependency parsing also. Re-cent research including the use of loopy belief net-work (Smith and Eisner, 2008), integer linear pro-gramming (Martins et al., 2009) and an improved dynamic programming algorithm (Koo and Collins, 2010) can be seen as methods to incorporate non-local features into a graph-based model.

An open source release of our parser, together with trained models for English and Chinese, are We thank the anonymous reviewers for their useful comments. Yue Zhang is supported by the Euro-pean Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement no. 247762.
