 In this paper, the proposed LIPED (LIfe Profile based Event Detection) employs the concept of life profiles to predict the activeness of event for effective event detection. A group of events with similar activeness patterns shares a life profile, modeled by a hidden Markov model. Considering the burst-and-diverse property of events, LIPED identifies the activeness status of event. As a result, LIPED balances the clustering precision and recall to achieve better F1 scores than other well known approaches evaluated on the official TDT1 corpus. H.3.3 [Information Search and Retrieval]: Clustering, H.2.8 [Database Applications]: Data mining, G.3 [Probability and Statistics]: Time series analysis.
 Algorithms, Experimentation, Performance Event Detection, Life Profiles, Hidden Markov Models, Clustering. The publishing activities on the Internet nowadays are prevailing that when an event occurs, autonomous reporters may publish related documents along the event's life span. Those documents are sequenced according to their chronological order to form a document stream. The task of event detection is to automatically identify events and track the related documents from the document stream in order to facilitate people in comprehending successively occurring events. Recently, DARPA-s ponsored Topic Detection and Tracking (TDT) project [4] aimed at promoting event detection research. The project describes an event as something that occurs at some place and time associated with specific actions. In many ways, event detection is similar to the traditional document clustering which partitions a set of documents into several coherent parts. However, in a stream environment, documents are continuously and chronologically generated and the detection process only knows already published documents. The incremental clustering algorithm [4] is employed to cluster documents sequentially. In this method, documents are processed one by one in their chronological order, and are considered as a member of a cluster if the text similarity between the document and the cluster is above a pre-defined threshold. When there is no cluster similar to the document, a new cluster is created and the document is treated as the first document of the newly detected event. One major problem of the incremental clustering algorithm is the threshold setting. Ideally, the threshold should be adaptable to the status of the cluster, which reflects the development of the event. This is because, when an event becomes popular, information sources publish documents about various aspects of it, which makes the topics of the event drifted, and thus decreases the text similarity. This " burst-and-diverse " phenomenon of news events corresponds to the dynamic behavior of stream environments [7]. Accordingly, the performance of incremental clustering algorithm can be enhanced if the activeness status of detected events is known. When a cluster is active, relaxing the similarity threshold of clustering will allow the inclusion of more related documents without severely damaging the clustering precision. After the cluster becomes inactive, raising the threshold can reject irrelevant documents. In this paper, we propose the concept of life profiles which utilizes hidden Markov models (HMMs) [9] to model the dynamic activeness of events. During the event detection process, each cluster is associated with appropriate life profiles and together with the activeness status to determine a dynamic clustering similarity threshold. The experiments show the proposed LIPED achieves at least 5% improvement in F1 score to other known approaches on the official TDT1 corpus. Recently, modeling the status of items in a stream environment has received much attention due to its wide usages. Aggarwal [1] developed velocity density estimation to diagnose changes in evolving data streams. The method can be applied to GIS systems to monitor the trend of traffic in a certain area. A location is busy if data has moved close to it. On the other hand, a location is inactive if data moves away from it gradually. The core of Aggarwal's method is grounded in kernel density estimation [1]. For an estimated spatial location, velocity density estimation calculates the difference between the forward and reverse time slice density estimates within a time window. The forward density estimate increases if data in the time window moves forward to the given location. On the other hand, if data falls back from the location, the forward density estimate decreases. The definition of reverse density estimate is similar to the forward estimate except that it reverses the time window. By subtracting these two density estimates, the velocity density estimation can determine whether recent data moves forward to or falls back from the given location, after which the trend of the data can be precisely quantified. Aizen, et al. [2] utilized hidden Markov models (HMMs) [9] to track the dynamic popularity variation of products on an E-Commerce website. In this method, each state in the hidden Markov model represents a degree of popularity and the weights of links among states describe the probability of popularity variations. Given the transaction log of an E-Commerce web site, the popularity of a product can be identified by calculating the most likely state transitions of the product. Even though Aizen's method can be used to identify product popularity, it violates an HMM constraint; that is, the sum of the weights of the outgoing arcs from a state must be equal to 1. Since Aizen's method decides the weight of arcs with a heuristic, this constraint can not always be satisfied. Another problem with Aizen's method is that it omits the dimension of time; therefore, two sequential purchases in one hour are no different to two sequential purchases in one week. However, in reality, the former can be read as active, while the later is inactive. Finally, Aizen's method uses a single model to represent all sorts of activeness trends. However, there are various styles of activeness variations. Some burst in the beginning and gradually fade out, while others might have an inactive start but develop incrementally. Aizen's method would be more practical if it provides different models for different trends. Our method is similar to Aizen's method in some degree. However, instead of using a single model to describe various trends, we propose a framework to use life profiles for different trends. Moreover, the method we propose treats time as an important factor; therefore, it is more reliable than Aizen's method. Finally, rather than use a heuristic to determine the arc weights, we propose a method to train the models that the learned results fully obey the HMMs constraints. A life profile stands for a pattern of activeness developments of events. There are life profiles that describe various styles of dynamic activeness of events. Therefore, to identify the status of an event, we have to know the event's life profile first. Formally, outcomes, i.e. the number of related documents per day, of an event x up to time t , and let LP ={ lp 1 , lp 2 , ..., lp profiles. Then, the most likely life profile lp MAP of the event is: 
Based on Bayes theorem [8], the above equation can be expanded into the following form: lp with the maximum product of the likelihood P ( O x,t priors, it is reasonable to assume that the prior probabilities of every life profile are the same [10]. Then Equation 2 can be converted into the following form: The most likely life profile of the event is the one that has the maximum likelihood. To calculate the likelihood value, we have probabilistic state transition diagram. In this model, the status of an event is a set of states S ={ state 1 , state 2 , ..., state Additionally, the model comprises two kinds of probabilities: an probability for every pair of states. For every time slot, whose length is predefined, an event transits from its current state to the next state following the transition probability. The state transition probabilities are defined by a N N  X  matrix, A , in which an state j , and can be interpreted in terms of conditional probabilities, a vector  X  , where an element  X  i indicates the possibility that a life profile starts off in state i . Note that, to follow the summation-one axiom of probability [8], the sum of the initial state probabilities and the weights of the outgoing arcs from each state must be equal to 1; that is,  X  N i=1  X  i =1 and  X  N j=1 a i,j =1, i  X  . Furthermore,  X  and a i,j  X  0, j i ,  X  . Actually, any state transition diagram is a Markov model since it possesses two Markov properties: limited horizon and stationary [12]. By modeling a life profile as a state transition diagram, the likelihood of Equation 3 can be calculated when the state transition sequence of the event is known. And the likelihood is simply the multiplication of the weights of the traversed arcs and the initial state probability. However, in practice, the only thing we know about the event is its outcome sequence, i.e. O Therefore , to calculate the likelihood, we extend our life profile modeling from a Markov model to a hidden Markov model. Basically, one can regard HMMs and Markov models as the same thing, except that during a state transition HMMs emit an outcome value. Accordingly, HMMs have an additional set of probability: the probability of outcome values. With the outcome values and their probabilities, HMMs make the likelihood calculable. In this paper, we adopt the state-emission HMMs, where the outcome produced by a state transition depends solely on the ending state. We use the symbol B to represent the outcome symbol probabilities, and an element b i,k stands for the probability of emitting k after entering state i . Formally, a HMM-based life profile lp can be specified by a five-tuple ( S , K ,  X  , A , B ) where S domain of observed outcomes.  X  , A , and B are the probabilities of the initial states, the state transitions and the outcome symbols, respectively. In the following subsections, we illustrate how these five components can be acquired from a training data set. The observed outcome K of an event is the number of related documents per day. The profile component S is associated with a positive integer N , the number of states of a life profile. The larger However, a large N also increases the complexity of profile training. Besides, the performance improvement may be negligible once the number of states is sufficiently large [2]. After determining the value of N , we construct a set S ={ state ..., state N } for each life profile. The outcome symbol probabilities B are determined by the given training examples. A training example is a sequence of observed outcomes of an event, O o , ..., event. An element o x,t is the observed outcome at time slot t . From the training examples, we obtain the following statistics about the observed outcome: max and min . Then a state i is associated with a parameter  X  i using the following formula:  X  (4) where  X  X  N min max ) (  X  =  X  . Afterward we treat the emitting value of each state as a ra ndom variable with a sample sp ace K and the value b i,k is the corresponding probability. Each state may be thought of as a Poisson random variable. Then, the probability b is calculated by the following formula: Note that the formula is no different to the probability mass function of Poisson distribution, except that we replace the Poisson probability parameter  X  with  X  i . One important benefit of using probability distributions for observed outcomes is that the HMM constraint is preserved; for every state, the probability mass of all possible outcomes must be 1. When the domain of an observed outcome is indefinite or infinite, such as the natural numbers of [0,  X  ), the constraint is very difficult to preserve without using mathematically sound methods, such as the above proposed method. Another benefit of the proposed method is that it may associate each state with real world semantics, according to the applications and outcome distributions. States with high  X  i can be regarded as active states because events located in the high  X  i states probably will generate many related documents immediately. Once states are associated with real world semantics, we can effectively explain the trends of learned life profiles by investigating the initial state and state transition probabilities. The probabilities A and  X  are learned from training examples. Let the set of observed outcome sequences { O 1 , O 2 , ..., O training examples of a life profile lp . The acquired A and  X  together with S , K , and B are expected to maximize the following probability: In other words, the desired A and  X  , together with other model components, have to perfectly interpret the training examples. Ideally, if the training examples contain the information about state transitions, the values of A and  X  can be calculated by using the maximum likelihood estimation. For example, if there are ten transitions in the training examples departing from state i and six of them move to state j , then the value of a maximum likelihood estimation is 6/10. However, in practice, the only thing we know about a training example is the observed outcome sequence; the corresponding state transitions are not explicitly represented. Thus, to obtain the values of A and  X  with such implicit training examples, we adopt the Forward-Backward algorithm [5], one kind of EM algorithm [6], which is frequently applied to estimate model parameters from incomplete data. In this case, the estimated parameters are A and  X  . The training examples are incomplete because the information about the state transitions is unobservable. Formally, let O x =&lt; o x,1 where o x,i  X  K , be the observed outcome sequence of a training event and &lt; s x,1 , s x,2 , ..., the event visits, i.e. at time t , the event traverses from state s s , and emits an observed outcome o x,t . Given sufficient training data for a life profile, the Forward-Backward algorithm first executes the following two steps until Equation 6 is converged. z E step: estimate the unobservable state transitions of each z M step: refine the life profiles components A and  X  with the To calculate the unobservable state transitions, as well as the values of A and  X  in the above two steps, the Forward-Backward algorithm utilizes two variables: the forward variable  X  and the backward variable  X  , which are defined as follows:  X  ( i )= P ( o x,1 , o x,2 , ..., o x,t , s x,t = state i and  X  ( i )= P ( o x,t+1 ,..., Briefly, the forward variable is the total probability of all possible transitions that a training example produces the observed given life profile. Conversely, the backward variable represents the total probability of seeing the rest of the observed outcomes of O , given that the example is in state i at time t . Both values of the forward and backward variables can be efficiently obtained with the dynamic programming algorithm [5]. With these two variables, under a life profile lp can be calculated as follows: P ( O x , s x,t = i | lp ) = P ( o x,1 , ..., o x,t , s x,t = i , o x,t+1 , ..., = P ( o x,1 , ..., o x,t , s x,t = i | lp )  X  P ( o x,t+1 =  X  x,t ( i )  X  x,t ( i ) (9) Then, the likelihood between a life profile lp and a training example O x is simply the summation of all possible states using Equation 9: where 1  X  t  X  | O x |. With Equations 9 and 10, the probability that the training example O x and a life profile lp can be calculated as follows: P ( i,j ) = P ( s x,t =i , s x,t+1 =j | O x , lp ) = P ( s x,t =i , s x,t+1 =j , O x | lp ) / P ( O x | lp ) By summing up all possible ending states, we obtain the probability that state i is visited at time t under a training example O and a life profile lp : distributions of unobservable state transitions of all training examples under a life profile lp . The M step then utilizes those estimated distributions to refine the values of A and  X  : ) (13) and  X  ) (14) Note that, Equations 13 and 14 have no differences in the maximum likelihood estimation, except that the state transitions are now expressed in terms of probabilities rather than frequencies. The above subsections illustrate how a life profile can be obtained from training examples. To sum up, we first calculate the statistics of max and min from the training examples. Next, we construct a state diagram with N states for a life profile and treat each state as a random variable in order to obtain the outcome symbol probabilities. Finally, the equations 7 to 14 run iteratively to acquire the values of initial state and state transition probabilities. With each iteration of E and M steps, the likelihood between a life profile and its training examples rises. Even though this process may not find the best values of A and  X  , the re-estimation guarantees that the profile components will converge to a local maximum [12]. In this section, the proposed LIPED employs the concept of life profiles to predict the activeness of event for effective event detection. One unique feature of LIPED is that the similarity threshold, used to decide whether a document is similar enough to be a member of a cluster, is adaptive to the status of the event. In the following sub-sections, we first show the data models used in LIPED; and then present the procedures of event detection, status maintenance, and threshold strategies. Documents and clusters are two basic units in event detection. In LIPED, documents are represented as vectors in the conventional vector space model (VSM) [13]. A vector in VSM is a set of tuples of terms and their weights. Generally, the higher weights, the more significant the terms are. We use the TF-IDF scheme [13] for term weight. Experiments show that the TF-IDF scheme indeed provides a high indexing performance [13]. For clusters, we adopt the time window method [15] to represent the context of a detected event. In this way, a cluster is described by a set of recently detected documents within a time wi ndow. The similarity between an incoming document d and a cluster c is the maximum cosine value between d and c 's constituent documents. In addition, each cluster c in LIPED has four variables, c.threshold , c.output , c.  X  , and c.  X  . Briefly, c.threshold  X  [0,1] is the similarity threshold between a document and cluster c ; and c.output  X  observed outcome of cluster c in the current time slot. In this observed outcome of a cluster as the number of detected documents. Variables c.  X  and c.  X  keep the various probabilities of probabilities, together c.output , are used for status maintenance purposes of cluster c . We detail the usages of these variables later. Basically, LIPED is a time window based incremental clustering algorithm [15] with an adaptable similarity threshold updated for employing the method described in Section 3. These profiles are used to predict the status of clusters (i.e. detected events). During the detection process, each incoming document d is compared to the previous m documents and is added into a cluster if the similarity is greater than the cluster's threshold c.threshold . Even though the proposed method is a soft clustering that a document can belong to more than one cluster as long as the similarities are above their thresholds, it can be easily changed to a hard clustering that a document can only belong to a single cluster, which is usually the one that has the maximum similarity. If there is no cluster that satisfies the similarity constraint, a cluster of newly detected event is created with d as the first document. After assigning a document to a cluster, we increase the cluster's c.output by one to correct the observed outcome. When all documents in a time slot are processed, each cluster executes the function StatusMaintenance illustrated as in Figure 1 to adjust its status according to the observed outcome during the current time slot. The procedure StatusMaintenance is a modification of Viterbi algorithm [14], which calculates the best state transition sequence of an outcome sequence against a HMM. But in LIPED only the current state of the best transition sequence is required for the similarity threshold adjustment. Therefore, we make use of the optimal substructure [14] of Viterbi algorithm to record the probabilities related to the current state into two matrices, c.  X  and Equation 7, stores the probability that cluster c belongs to life However, instead of considering all possible state transitions that lead to state i , c.  X  [ l,i ] only regards the best case. of profile components designates the index of life profile that the component belongs to. Afterward, every time a cluster executes the function StatusMaintenance , the matrices are updated to reflect the cluster's observed outcome (the lines 6 and 7 of Figure 1: StatusMaintenance  X  inputs: LP ={ lp 1 , lp 2 , ..., lp 2: Create two temporal matrices  X  * and  X  * of L  X  3:  X  * = c.  X  ,  X  * = c.  X  ; 4: For l = 1 to L ; 5: For j = 1 to N ; 6: c.  X  [ l,j ] =  X  N i=1  X  * [ l,i ] *a l i,j *b 7: c.  X  [ l,j ] = max N i=1  X  * [ l,i ] *a l i,j *b 8: End for; 9: End for; 1). c.  X  [ l,i ] is updated by summing up the probabilities of all possible leaving states in the last time slot, multiplied with the state transition probabilities and the outcome symbol probability regarding the observed outcome, while c.  X  [ l,i ] only considers the highest probability. With these two matrices, the likelihood between a cluster c and a life profile l , likelihood =  X 
N i=1 c.  X  [ l,i ]; the most likely life profile of the cluster, max_lp is the one with the maximal likelihood, i.e. max_lp =argmax L l=1 likelihood l c ; and the current state against a life profile l , current_state l c , is argmax N i=1 c.  X  [ l,i ]. Once the probabilities related to the current state are updated, the threshold of a cluster is adjusted. We define an adaptive threshold range by parameters thd top and thd down , where 0  X  thd and give each state of the life profiles a value within the range as follows: where thd ( i ) returns the similarity threshold of state i . The formula evenly divides the range into N parts and associates active states with a lower threshold according to the burst-and-diverse property of news events. Our threshold strategy determines the threshold of a cluster by considering the most potential next state of the most likely profile. The evaluation is based on the official TDT1 corpus from Linguistic Data consortium [4]. The TDT contest rules are followed to evaluate the compared methods. The performance is evaluated using the following six TDT official metrics [4], measure ( F1 ), and cost ( c ). Among these metrics, F1-measure is considered as an objective measure for its balance between precision and recall. The micro-average method is applied for the global performance measurement. To acquire life profiles for LIPED, we convert the 25 labeled events in TDT1 into sequences according to the number of constituent documents per day. Then, the hierarchical agglomerative clustering algorithm is applied to cluster the resulting sequences into a class hierarchy. In this way, a node in the hierarchy can be regarded as a life profile representing the constituent sequences. Moreover, LIPED runs on each of the top four depths (root is considered as the first depth) of the profile hierarchy to investigate the influence of the depth of hierarchy in the evaluation. For each depth, the two-fold cross validation [10] is applied to induce credible results. That is, for each node of a certain depth, half of the constituent sequences are randomly selected for life profile training as described in Section 3, and the events of remaining sequences are used as test data for performance evaluation. The performance of the depth 0 can be considered as Aizen's method [2] b ecause it builds a single life profile from the whole training data. Three well-known inter-cluster similarity functions are considered for HAC, which are single-link complete-link and average-link. The similarity between two sequences is obtained by using the dynamic time warping (DTW) algorithm [11] which is frequently used to measure the distance between time series with different lengths. The parameters used in LIPED are defined as follows: N =20, thd top =0.26, thd down =0, and m =2000 (the size of the time window). The performance evaluations of LIPED and three well-known methods, including the time window method [15], Aizen's method [2], and the time-based threshold method [3] are presented in Table 1. Since LIPED incorporates the concept of life profiles into the time window method, the time window method is used as the baseline for comparisons. 
Table 1. The comparison results on the official TDT1 corpus. Figure 2, the F1 comparisons on the official TDT1 corpus.
 For the baseline method, we present the published result with the optimal similarity threshold 0.23 [15] denoted "Time window [15]" in Table 1, and the results of our implementation denoted as "B0.20",  X  X 0.23", and "B0.26" (where "B0.26" performs best in our implementation). In implementing the baseline method, we follow the algorithm and the method parameters, e.g. the time window size, mentioned in [15]. However, our IDF values are obtained from TDT1 corpus, while in [15] the IDF values are obtained from an unpublished corpus; hence the result of B0.23 is different from the published measurement. The profile hierarchies of the complete-link and the average-link approaches are almost the same except for the depth 3; thus, both achieves similar performance results. From Table 1, we find that the life profiles of the complete/average-link perform better than those of the single-link in terms of the F1 score. This is because the profile hierarchy produced by the single-link strategy is generally not cohesive that worsens the performance. In summary, LIPED performs better in F1 score than the baseline method for all experiment setting, even only using a single life profile (i.e. Aizen's method), which can be observed from Figure 2. Besides, the improvement over the baseline method is about 5% in term of the F1 score. The detection performance, especially the recall, increases when using more specific profiles (e.g. depth 1 and 2 in this experiment) until the life profile presents an over-fitting phenomenon (e.g. depth 3). Even though slight decrease in precision (from 0.839 to 0.776; about 7% in average) indicates that relaxing threshold for active events may misclassify the noisy document in a wrong event; the reward from the recall is significant. Almost 16% improvement (from 0.565 to 0.656) in r ecall is accomplished when utilizing the learned life profiles. Moreover, to achieve a similar recall of LIPED, the threshold of the baseline method has to be relaxed to 0.2, which results in a low precision of 0.541 while LIPED has a better score of 0.776 in 43% improvement. Another interesting observation of Figure 2 is the falling F1 of the depth 3 of LIPED. The reason of the falling is that the life profiles in the depth 3 usually comprise a single event. The insufficient training data results in over-fitting phenomena of the learned profiles that decrease the detection performance. This observation, together with the results of Aizen's method, emphasizes the quality of life profiles. In contrast to the over specific profiles, too general life profiles, such as that of Aizen's method, ignore the unique feature of constituent events that diminish the detection performance as well. In this paper, we propose a generic framework to model the activeness of events using the HMM-based life profiles. Also, the learned profiles are used in the proposed LIPED for efficient event detection. In comparison with other methods on the official TDT1 corpus, LIPED achieves the best F1 score and makes a 5% improvement in F1 score to the baseline approach. The performance of LIPED depends on the quality of life profiles; too general or too specific life profiles would result in inferior detection results. As life profiles are learned from events with similar activeness developments, training data collection and life profiles preparation become important issues. Furthermore, as the outcome sequence of a training event can be viewed as a time series, life profiles preparation is closely related to time series analysis that state of the art time series clustering methods may be borrowed to enhance LIPED system. [1] Aggarwal, C. C. A Framework for Diagnosing Changes in [2] Aizen, J., Huttenlocher, D., Kleinberg, J., and Novak, A. [3] Allan, J., Papka, R., and Lavrenko, V. On-Line New Event [4] Allan, J., Carbonell, J., Doddington, G., Yamron, J., and [6] Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum [7] Kleinberg, J. Bursty and Hierarchical Structure in Streams. [8] Ghahramani, S. Fundamentals of Probability. Prentice Hall, [9] Markov, A. A. An example of statistical investigation in the [10] Mitchell, T. M. Machine Learning. McGraw-Hall, 1997. [11] Myers, C., Rabiner, L. R., and Rosenberg, A. E. Performance [12] Rabiner, L. R. A Tutorial on Hidden Markov Models and [13] Salton, G. Automatic Text Processing: The Transformation, [14] Viterbi, A. J. Error bounds for convolutional codes and an [15] Yang, Y., Pierce, T., and Carbonell, J. A Study on 
