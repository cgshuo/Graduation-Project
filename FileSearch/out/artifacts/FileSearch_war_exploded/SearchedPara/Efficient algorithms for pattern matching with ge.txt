 Kimmo Fredriksson  X  Szymon Grabowski Abstract We develop efficient dynamic programming algorithms for pattern matching p Moreover, we require that if p i matches t j , then p i +1 should match one of the text symbols t invariant matching, i.e., the matching condition becomes t j [ p i + s , for some constant s determined by the algorithms. We give algorithms that have efficient average and worst case running times. The algorithms have important applications in music information retrieval and computational biology. We give experimental results showing that the algorithms work well in practice.
 Keywords String matching Sparse dynamic programming Bounded length gaps Character classes Transposition invariance 1 Introduction Background Many notions of approximateness have been proposed in string matching literature, usually motivated by some real problems. One of seemingly underexplored problem with applications in music information retrieval (MIR) and molecular biology (MB) is pattern matching with gaps (Crochemore et al. 2002 ). In this problem, gaps (text substrings) of length up to a are allowed between each pair of matching pattern characters. Moreover, in MIR applications the character matching can be relaxed with d -matching, i.e., the pattern character matches if its numerical value differs at most by d to the corre-sponding text character. In MB applications the singleton characters can be replaced by classes of characters, i.e., text character t matches a pattern character p if t [ p , where p is some subset of the alphabet.
 i.e., invulnerability to shifting the whole pattern (over an integer alphabet) by any fixed value. It is motivated by the fact that humans recognize a melody by the intervals between successive notes rather than the pitches themselves.
 Previous work Let us start the review from the problem without transposition invariance. The first algorithm in this setting (Crochemore et al. 2002 ) is based on dynamic program-ming, and runs in O ( nm ) time, where n and m are the lengths of the text and pattern, respectively. This basic dynamic programming solution can also be generalized to handle more general gaps while keeping the O ( nm ) time bound (Pinzo  X  n and Wang 2005 ). The basic algorithm was later reformulated (Cantone et al. 2005 a) to allow to find all pattern occur-rences, instead of only the positions where the occurrence ends. This needs more time, however. The algorithm in Cantone et al. ( 2005 b) improves the average case of the one in Cantone et al. ( 2005 a) to O ( n ), but they assume constant a . Bit-parallelism can be used to improve the dynamic programming-based algorithm to run in O  X d n = w e m  X  n d  X  and O  X d n = w ed ad = r e X  n  X  time in worst and average case, respectively, where w is the number of bits in a machine word, and r is the size of the alphabet (Fredriksson and Grabowski 2006 ).
For the a -matching with classes of characters there exists an efficient bit-parallel non-deterministic automaton solution (Navarro and Raffinot 2003 ). This also allows gaps of different lengths between each pair of successive pattern characters. This algorithm can be trivially generalized to handle ( d , a )-matching (Cantone et al. 2005 b), but the time com-plexity becomes O  X  n d a m = w e X  in the worst case. For small a the algorithm can be made to run in O ( n ) time on average. The worst case time can be improved to O  X  n d m log  X  a  X  = w e X  (Fredriksson and Grabowski 2006 ), but this assumes equal length gaps.

Sparse dynamic programming can be used to solve the problem in O  X  n  X jMj min f log  X  d  X  2  X  ; log log  X  m  X g X  time, where M X f X  i ; j  X jj p i t j j d g (and thus jMj nm ) (Ma  X  kinen 2003 ). This can be extended for the harder problem variant where transposition invariance and character insertions, substitutions or mismatches are allowed together with ( d , a )-matching (Ma  X  kinen et al. 2005 ). In this case the jMj factor becomes nm .
Our results We develop several algorithms, for both major problem variants. Our techniques are based on sparse dynamic programming and bit-parallelism. Our first algorithm for ( d , a )-matching without transposition invariance is essentially a reformulation of the algorithm in Ma  X  kinen et al. ( 2005 ). The worst case running time of the algorithm is O  X  n  X jMj X  : Our variant has the benefit that it generalizes in straight-forward way to handle general and even negative gaps, important in some MB applications (Mehldau and Myers 1993 ; Myers 1996 ). We then give several variants of this algorithm to improve its average case running time to close to linear, while increasing the worst case time only up to O  X  n  X jMj X  log  X  n  X  X  a  X  X  : This algorithm assumes fixed integer alphabet. We also present two simple and practical algorithms that run in O ( n ) time on average for a = O ( r / d ), but have O  X  n  X  min  X  nm ; jMj a  X  X  worst case time, for any unbounded real alphabets. One of these algorithms is then modified to work in sublinear time in average. Finally, we extend our recent non-deterministic finite automaton-based algorithm (Fredriksson and Grabowski 2006 ) in order to improve its average case time complexity to sublinear for realistic parameter combinations, without compromising its worst case time complexity.

These are the first algorithms that achieve good average and worst case complexities simultaneously, and they are shown to perform well in practice too.
We also present two algorithms handling the problem with transposition invariance, both based on bit-parallelism and having attractive average case time complexities and performing reasonably well in the worst case. 2 Preliminaries where p i , t j [ R for some integer alphabet R of size r . The number of distinct symbols in the pattern is denoted by r p . We sometimes call the set of distinct symbols in the pattern the pattern alphabet.

In d -approximate string matching the symbols a , b [ R match, denoted by a = d b , iff we sometimes write A  X  a d B :
In all our analyses we assume uniformly random distribution of characters in T and P , and constant a and d / r , unless otherwise stated. Moreover, we often write d / r to be terse, but the reader should understand that we mean (2 d + 1)/ r , which is the upper bound for the probability that two randomly picked characters match.

The dynamic programming solution to ( d , a )-matching is based on the following recurrence (Crochemore et al. 2002 ; Cantone et al. 2005 a): between this position and the position j is at most a .If p i = d t j , then we try to extend the match by extending the gap, i.e., we set D i , j = D i , j -1 if the gap does not become too large. an occurrence ending at position j whenever D m -1, j = j . This is simple to implement, and the algorithm runs in O ( nm ) time using O ( nm ) space.

We first present efficient algorithms to the above problem, and then show how these can be generalized to handle arbitrary gaps, tackling with both upper and lower bounded gap lengths, and even negative gap lengths, and using general classes of characters instead of d -matching. 3 Row-wise sparse dynamic programming The algorithm we now present can be seen as a row-wise variant of the sparse dynamic programming algorithm of the algorithm in Ma  X  kinen et al. ( 2005 , Sect. 5.4). We show how to improve its average case running time. Our variant can also be easily extended to handle more general gaps (see Sect. 7 ). 3.1 Efficient worst case From the recurrence of D it is clear that the interesting computation happens when t j = d p i , and otherwise the algorithm just copies previous entries of the matrix or fills some of the cells with a constant.

Let M X f X  i ; j  X j p i  X  d t j g be the set of indexes of the d -matching character pairs in P always defined if P occurs at t h ... j for some h \ j . The new recurrence is and -1 otherwise. Computing the d values is easy once M is computed. As we have an integer alphabet, we can use table look-ups to compute M efficiently. Instead of computing obtained in O ( d n ) worst case time, and the average case complexity is O ( n ( dr P / r + 1)). Note that jMj is O ( nm ) in the worst case, but the total length of all the lists is at most increasing order.

Consider a row-wise computation of d . The values of the first row d 0, j correspond one to d traverses L and M linearly, and hence runs in O  X  n  X jMj X  worst case time. We now consider improving the average case time of this algorithm. 3.2 Efficient average case The basic sparse algorithm still does some redundant computation. To compute the values d positions with matching pattern prefixes decreases exponentially on average when the prefix length i increases. Yet, the list length | L [ p i ]| will stay approximately the same. The the number of matching pattern prefixes on that row, rather than on the number of d -matches for the current character on that row.

The modifications are simple: (1) the values d i , j =-1 are not maintained explicitly, they are just not stored since they do not affect the computation; (2) the list L [ p i ] is not traversed sequentially, position by position, but binary search is used to find the next value that may satisfy the condition that L [ p i ][ h ] -d i -1, k B a + 1 for some h , k . Consider now the average search time of this algorithm. The average length of each list we just copy the values in L [ p 0 ] to be the first row of d . For the subsequent rows we execute one binary search over L [ p i ] per each stored value in row i of the matrix. Hence in general, q ), where q = 1 -(1 -d / r ) a +1 \ 1 is the probability that a pattern symbol d -matches in a text window of length a symbols. Summing up the resulting geometric series over all rows we obtain O  X  n d the worst case search time is also increased to O  X  n  X jMj log  X jMj = m  X  X  : We note that this can be improved to O  X  n  X jMj log log  X  X  nm  X  = jMj X  X  by using efficient priority queues (Johnson 1982 ) instead of binary search. 3.3 Faster preprocessing The O ( d n ) (worst case) preprocessing time can dominate the average case search time in some cases. Note however, that the preprocessing time can never exceed O  X  n  X jMj X  : We now present two methods to improve the preprocessing time. The first one reduces the worst case preprocessing cost to O  X  achieves O ( n ) preprocessing time, but the worst case search time is slightly increased. 3.3.1 O  X  The basic idea is to partition the alphabet into r = of size each alphabet symbol s , its respective [ s -d , s + d ] interval wholly covers H  X  I , and also can partially cover at most two I h intervals. Two kinds of lists are computed in the preprocessing, L B (for  X  X  X oundary X  X  cases) and L C (for  X  X  X ore X  X ). For each character t j from text T , at most 2  X  correspond to the pattern alphabet symbols from the partially covered intervals I h . For example, if R  X f 0 ; ... ; 29 g ; symbols 1, 2, 18, and 19 (if not, the respective lists are not built at all). Figure 1 illustrates. Similarly, each character t j also causes to append j to O  X  correspond to the I h intervals wholly covered by [ t j -d , t j + d ].

More formally (and still assuming for simplicity that d is a square number) text position j is appended to the lists L B [ p i ] for Likewise, j is appended to the lists L C  X  p i = Clearly, the preprocessing is done in O  X  time.

The search is again based on a binary search routine, but in this variant we binary search two lists: L B [ p i ] and L C  X  p i = at some L C list. This increases both the average and worst case search cost only by a constant factor.

We can generalize this idea and have a preprocessing/search trade-off. Namely, we may factor k in the search. For k = log( d ) the preprocessing cost becomes O ( n log( d )), and both the average and worst case search times are multiplied by log( d ) as well. 3.3.2 O(n) time preprocessing We partition the alphabet into d r = d e disjoint intervals of width d . With each interval a list corresponds to the characters i d ... min{( i + 1) d -1, r -1}. During the scan over the text in the preprocessing phase, we append each index j to up to three lists: L [ k ] for such k that k d B t j B ( k + 1) d -1, L [ k -1] (if k -1 C 0), and L [ k + 1] (if k  X  1 d r = d e 1). Note that no character from the range [ t j -d ... t j + d ] can appear out of the union of the three corresponding intervals. Such preprocessing clearly needs O ( n ) space and time in the worst case.
 Now the search algorithm runs the binary search over the list L [ k ] for such k that k d B p i is there can be other text positions stored on L [ k ] too, as the only thing we can deduce is analogously. After at most a + 1 read indexes from L [ k ] we either have found a d -match prolonging the matching prefix, or we have fallen off the ( a + 1)-sized window. As a result, the worst case time complexity is O  X  n  X jMj X  log  X  n  X  X  a  X  X  : The average time in this variant becomes O ( n + n ad / r log( n )). Algorithm 1 shows the complete pseudo code. 3.4 Improved algorithm for large a In this section we present a variant of the row-wise SDP algorithm, particularly suited to problem instances with large a .

In the preprocessing, we again compute lists L [ p i ] = { j | t j = d p i }. But now we also store ( a + 1) -1, there are two pointers, showing the leftmost and the rightmost item with the value from the interval [ j , j + a + 1]. These pointers are kept in two 2-dimensional arrays, named L and R : More formally, the elements of L and R are defined in the following way: assuming the minimum and the maximum is seeked over a non-empty slice of a list L [ p i ]. If this is not the case, the respective pointers are set to null. In total, the extra preprocessing cost is O ( d n + r P n / a ) in time, and O ( r P n / a ) space, in the worst case.
The search is basically prefix prolongation. A specific trait of the algorithm is that during the search we are not interested in finding all matching prefixes: what is enough are (at most) two prefixes per an ( a + 1)-sized chunk of each row (except for the last row, where we perform an extra scan, to be described later). The end positions of those prefixes are maintained in two auxiliary arrays, C L and C R ; of size b n /( a + 1) c each. They are initialized with the exact copy of the rows L X  p 0 and R X  p 0 ; respectively.

Now we assume the matrix row i we are in is at least 1. W.l.o.g. we also assume that we are in the column at least a + 1. For each k [ 1 ... n /( a + 1) -1 we read L X  p i ; k and R X  p i 1 ; k 1 ; and if both are non-null and L X  p i ; k R X  p i 1 ; k 1 is at most a + 1, then we have found this difference cannot be greater than a + 1, so testing for a positive difference of non-null values is all we need). Affirmative answer again corresponds to finding a relevant prefix (and requires updating C L  X  k ), but a negative one means that we have to look for a prefix pro-longation somewhere further in the current chunk. In such case, we perform a binary search R X  p i ; k ; to find the smallest value being greater than L X  p i 1 ; k : The interval has as most a + 1 items, so the binary search cost is O (log( a )). If this results in a failure (which happens only if the considered interval is empty), it means that we do not have a prefix ended in the current chunk, and C L  X  k should be updated with a null value.

Analogously we proceed at the right boundary of each chunk. The invariant for the procedure
As mentioned, the last row requires an extra scan, to find all the d -matches between the C  X  k  X C R  X  k or C R  X  k  X C L  X  k  X  1 ; so we must be careful not to count duplicates more than once. This stage needs O ( n ) time, i.e., is always dominated by the preprocessing time.
The overall search complexity can be bounded by O ( n + nm log( a )/ a ), but a closer look may have up to a + 1 items over which we binary search, but in total there are only jMj matches in the matrix, which can be much less than nm . This means that on average there are O  X  a jMj =  X  nm  X  X  items in a chunk, and equal number of matches in chunks leads also to the worst overall case, which is trivially implied from the convexity of the log function.
On the theoretical side, we note that the achieved worst-case complexity dominates over existing algorithms on the pointer machine (where, e.g., bit-parallelism is forbidden), in the case jMj X  O  X  nm  X  : 4 Column-wise sparse dynamic programming In this section we present a column-wise variant. This algorithm runs in O ( n + n ad / r ) and O  X  n  X  min  X jMj a ; nm  X  X  average and worst case time, respectively.

The algorithm processes the dynamic programming matrix column-wise. Let us define last prefix occurrence D as (Eq. 1 ). The pattern matching task is then to report every j such that D m 1 ; j  X  j : As seen, this is easy to compute in O ( nm ) time. In order to do better, we maintain a list of window prefix occurrences W j that contains for the current column j all the rows i such that j D i ; j a where i 2W j :
Assume now that we have computed D and W up to column j -1, and want to compute D and W for the current column j . The invariant is that i 2W j 1 iff j D position j and row i + 1. In such case we update D i  X  1 ; j to be j , and put the row number i + 1 into the list W j : This is repeated for all values in W j 1 : After this we check if also p 0 d -matches the current text character t j , and in such case set D 0 ; j  X  j and insert j into W j : Finally, we must put all the values i 2W j 1 to W j if the row i was not already there, and still it holds that j D i ; j a : This completes the processing for the column j .
Algorithm 2 gives the code. Note that the additional space we need is just O ( m ), since only the values for the previous column are needed for D and W : In the pseudo code this is implemented by using W and W 0 to store the prefix occurrences for the current and previous column, respectively.
The average case running time of the algorithm depends on how many values there are on average in the list W : Similar analysis as in Sect. 3 can be applied to show that this is algorithm runs in O ( n + n ad / r ) average time. In the worst case the total length of the lists for all columns is O  X  min  X jMj a ; nm  X  X  ; and therefore the worst case running time is O  X  n  X  min  X jMj a ; nm  X  X  ; since every column must be visited. The preprocessing phase only needs to initialize D ; which takes O ( m ) time.
 Finally, note that this algorithm can be seen as a simplification of the algorithm in Ma  X  kinen et al. ( 2005 , Sect. 5.4). We avoid the computation of M in the preprocessing phase and traversing it in the search phase. The price we pay is a deterioration in the worst case time complexity, but we achieve simpler algorithm that is efficient on average. This also makes the algorithm alphabet independent. 5 Simple algorithm In this section we will develop a simple algorithm that in practice performs very well on small ( d , a ). The algorithm inherits the main idea from Algorithm 1, and actually can be seen as its brute-force variant. The algorithm has two traits that distinguish it from Algorithm 1: (i) the preprocessing phase is interweaved with the searching (lazy evalua-tion); (ii) binary search of the next qualifying match position is replaced with a linear scan in an a + 1 wide text window. These two properties make the algorithm surprisingly simple and efficient on average, but impose an O ( a ) multiplicative factor in the worst case time bound.
 The algorithm begins by computing a list L of d -matches for p 0 : This takes O ( n ) time (and solves the ( d , a )-matching problem for patterns of length 1). The matching prefixes are then iteratively extended, subsequently computing lists:
List L i can be easily computed by linearly scanning list L i -1 , and checking if any of the in the worst case the total length of all the lists is runs in O  X  n  X  a jMj X  worst case time.

With one simple optimization the worst case can be improved to O  X  min f a jMj ; nm g X  (improving also the average time a bit). When computing the current list L i , Simple algorithm may inspect some text characters several times, because the subsequent text positions stored in L i -1 can be close to each other, in particular, they can be closer than a + 1 positions. In this case the a + 1 wide text windows will overlap, and same text positions are inspected more than once. Adding a simple safeguard to detect this, each value in the list L i can be computed in O ( a ) worst case time, and in O (1) best case time. In particular, if jMj X  O  X  nm  X  ; then the overlap between the subsequent text windows is O ( a ), and each value of L i is computed in O (1) time. This results in O ( nm ) worst case time. The average case is improved as well. Algorithm 3 shows the pseudo code, including this improvement.

Consider now the average case. List L 0 is computed in O ( n ) time. The length of this list summing up, the total average time is O ( n ). 5.1 Sublinear average case In this section we show how the average case time of Simple can be improved. The basic rather only those that have hope to be extended to a complete match of the whole pattern. In other words, some of the d -matches can be skipped. This can be achieved using Boyer X  Moore X  X orspool (BMH) (Horspool 1980 ) strategy. We therefore build the list L 0 using the BMH approach ( filtering ), and then continue with plain Simple to compute the lists L 1 ... m -1 . This can be seen as a verification phase.
 before, using Simple. We first need the following definition: that p 0 is aligned with t j . We then execute the following algorithm: 1. If | p 0 -t j | B d , then put j into the list L 0 . 3. Shift the pattern with j / j -( S [ t j -s ] + s ) to align t j -s with p S  X  t j s . 4. If j C 0, then go to 1. 5. Pass the computed list L 0 to Simple, and compute lists L 1 ... m -1 .
 occurrence overlaps this window, then some pattern character must d -match one of the characters in this window. We therefore compute the smallest shift to align some exist, then the pattern occurrence cannot overlap this window, and the whole pattern is shifted past the window, i.e., the shift is m + a + 1 characters.
The text scanning is performed backwards, as we want to put the starting positions (instead of ending positions) of the possible occurrences into the list L 0 . The only reason for this is to be compatible with Simple algorithm. Algorithm 4 gives the pseudo code.

As opposed to exact BMH matching, in this variant any shift requires O ( a ) prior character accesses. The average pattern shift can be lower-bounded by O (min( m ,1/ q )), where q is the probability of a d -matching symbol in ( a + 1)-window, that is, q =1 -(1 -small a is O ( n a 2 d / r ), which also dominates the verification phase. 6 Non-deterministic finite automata In this section we present an algorithm based on non-deterministic finite automaton. Preliminary version of this algorithm appeared in Fredriksson and Grabowski ( 2006 ). We first review that algorithm and then improve its average case running time. The problem of the algorithm in Navarro and Raffinot ( 2003 ) is that it needs m + ( m -1) a bits to rep-resent the search state. Our goal is to reduce this to O ( m log( a )), and hence the worst case time to O  X  n d X  m log  X  a  X  X  = w e X  : At a very high level, the algorithm can be seen as a novel combination of Shift-And and Shift-Add algorithms (Baeza-Yates and Gonnet 1992 ). The  X  X utomaton X  has two kinds of states: Shift-And states and Shift-Add states. The Shift-And states keep track of the pattern characters, while the Shift-Add states keep track of the gap length between the characters. The result is a systolic array rather than automaton; a high level description of a building block for character p i is shown in Fig. 2 . The final array is obtained by concatenating one building block for each pattern character. We call the building blocks counters .
To efficiently implement the systolic array in sequential computer, we need to represent each counter with as few bits as possible while still being able to update all the counters bit-parallelly.

We reserve  X   X d log 2  X  a  X  1  X e X  1 bits for each counter, and hence we can store b w /  X  c counters, i.e., to represent the value 0. (This representation has been used before, e.g., in Crochemore et al. ( 2005 ).) This means that the highest bit (  X  th bit) of the counter becomes 1 when the counter has reached a value a + 1, i.e., the gap cannot be extended anymore. Hence the lines 3 X 4 of the algorithm in Fig. 2 can be computed bit-parallelly as where msk selects the lowest bit of each counter. That is, we negate and select the highest counters. If a counter value is less than a + 1, then the highest bit position is not activated, and hence the counter gets incremented by one. If the bit was activated, we effectively add 0 to the counter.

To detect the d -matching characters we need to preprocess a table B , so that B [ c ] has i  X  th bit set to 1, iff | p i -c | B d . We can then use the plain Shift-And step: where we have reserved  X  bits per character in D as well. Only the lowest bit of each field has any significance, the rest are only for aligning D and C appropriately. The reason is that a state in D may be activated also if the corresponding gap counter has not exceeded a + 1. In other words, if the highest bit of a counter in C is not activated (the gap condition is not violated), then the corresponding bit in D should be activated:
The only remaining difficulty to solve is how to reinitialize (bit-parallelly) some subset of the counters to zero, i.e., how to implement the lines 1 X 2 of the algorithm in Fig. 2 . The bit vector D 0 has value 1 in every field position that survived the Shift-And step, i.e., in every field position that needs to be initialized in C . Then ( a + 1) to all the cleared fields.
 This completes the algorithm. Algorithm 5 gives the pseudo code. Algorithm 5 runs in O ( n ) worst case time, if m  X d log 2  X  a  X  1  X e X  1  X  w : Otherwise, several machine words are needed to represent the search state, and the time grows accordingly. However, by using the well-known folklore idea, it is possible to obtain O ( n ) average time for long patterns preprocessing takes O  X  m  X  X  r  X  dr P  X d m log  X  a  X  = w e X  time, which is O  X  m  X  X  r  X  d min f m ; r g X d m log  X  a  X  = w e X  in the worst case. 6.1 Sublinear average case We note that the idea (Navarro and Raffinot 2003 ) of combining the forward matching automaton with BNDM (Navarro and Raffinot 2000 ) works with our algorithm as well. We briefly sketch the idea.

Denote the pattern in reverse as P r . The set of its suffixes is f P r i ... m 1 j 0 i \ m g (note matching text substring is m . Assume that we are scanning the text window t are already reported. The algorithm matches the characters of the current window (backwards) as long as any of the suffixes ( d , a )-match, or we reach the beginning of the window. The algorithm remembers the longest suffix found from the window. The window is then shifted so that its starting position will become aligned with the last symbol of that suffix. This is the position of the next possible pattern occurrence. If the This process is repeated until the whole text is scanned. However, if we reached the end of the window, then it is possible that there is an occurrence starting at the text position j , which must be verified.

To implement the above algorithm efficiently, we use Algorithm 5 for both the backward matching and verification. Consider first the backward matching phase. We build the automaton using P r . For each window all the states are initialized to be active, in other words, the states corresponding to each of the suffixes of P r is ini-tialized to be active. This correctly models that we want to recognize every suffix of P r ending at t j + m -1 . We also must remove the self-loop from the automaton, since the automaton is used for recognition, not for searching, i.e., the main Shift-And step becomes To detect if some state is still active, we just check if D is not zero.

To verify the occurrences, we again use Algorithm 5 to scan the text from position j onwards using the original pattern P . Again the self-loop is removed from the initial state (which is the only state initialized to be active), hence the vector D must become zero after m + ( m -1) a steps, which is the maximum length of a pattern occurrence. This signals the end of the verification procedure.

The analysis is similar as that of plain BNDM. The differences are that we must always scan at least a characters in each window, and that the probability of a character match is totically for large m . For m larger than w /log( a ) we could also use only pattern prefixes of average time.

The worst case time of this algorithm becomes quadratic, as in the worst case the length of the shift is always O (1), i.e., each text character is scanned O ( m ) times. However, there are some  X  X  X tandard tricks X  X  that can be applied to combine the backward and forward (verification) scans so that either scans no text character twice (Crochemore et al. 1994 ; Navarro and Raffinot 2003 ). These work with our method as well. A somewhat simplified solution which achieves O (1) accesses to any text character in the worst case is as follows. Assume that for the current window the backward scan touched more than m /2 text this case we switch to forward matching. The window starts from the text position j , which is the next possible starting position of an occurrence. We search with the forward algo-text characters. The same is easily achieved for the forward scanning by saving the last scanned text position and the corresponding state vectors C and D , and resuming the search in the case of overlapped windows. This preserves the good worst case time of Algorithm 5. A more sophisticated solution is described in Navarro and Raffinot ( 2003 ), but the final result is the same. 7 Handling character classes and general gaps We now consider the case where the gap limit can be of different length for each pattern character, and where the d -matching is replaced with character classes, i.e., each pattern character is replaced with a set of characters. 7.1 Character classes In the case of character classes p i , R , and t j matches p i if t j [ p i . For Algorithms 2 and 3 we can preprocess a table C [0 ... m -1][0 ... r -1], where C [ i ][ c ]:= c [ p i . This requires O ( r m ) space and O ( r The search algorithm can then use C to check if t j [ p i in O (1) time. For large alphabets we time, respectively.

Algorithm 1 is a bit more complicated, since we need to have M preprocessed. First compute lists L 0 [ c ]={ i | c [ p i }. This can be done in one linear scan over the pattern. Then can consider d as the average size of the character classes. The search algorithm can now be used as is, the only modification being that where we used L [ p i ] previously, we now use L [ i ] instead (and the new definition of L ). 7.2 Negative and range-restricted gaps maximum ( a i B b i ) gap length for the pattern position i . This problem variant has important applications, e.g., in protein searching (see Mehldau and Myers 1993 ; Myers 1996 ; Navarro and Raffinot 2003 ). General gaps were considered in Navarro and Raffinot ( 2003 ) and Pinzo  X  n and Wang ( 2005 ). This extension is easy or even trivial to handle in all trickier, but still adaptable. Yet a stronger model (Mehldau and Myers 1993 ; Myers 1996 ) allows gaps of negative lengths, i.e., the gap may have a form g ( a i , b i ) where a i \ 0 (it is lapping in the text.

Consider first the situation where for each g ( a i , b i ): (i) a i C 0; or (ii) b i B 0. In either case we have a i B b i . Handling the case (i) is just what our algorithms already do. The case (ii) is just the dual of the case (i), and conceptually it can be handled in any of our dynamic programming algorithms by just scanning the current row from right to left, and using g ( -b i -2, -a i -2) instead of g ( a i , b i ).

The general case where we also allow a i \ 0 \ b i is slightly trickier. Basically, the only modification for Algorithm 1 is that we change all the conditions of the form 0 B g B a , where g is the formed gap length for the current position, to form a i B g B b i . Note that this does not require any backtracking, even if a i \ 0.

Algorithm 3 can be adapted as follows. For computing the list L i , the basic algo-both the situations b i B 0 and a i \ 0 \ b i . The scanning time for row i becomes now ( b i -a i + 1) d / r \ 1. The optimization to detect and avoid overlapping text windows clearly works in this setting as well, and hence the worst case time remains O  X  n  X  min f X  b a  X  1  X jMj ; nm g X  ; where for simplicity we have considered that the gaps are of the same size for all rows. 8 Transposition invariance In this section we consider transposition invariance. In this case pattern P ( d , a )-matches the i
B a + 1 and s [ { -r + 1 ... r -1}. That is the condition is the same as before, but we now allow that the symbols can be  X  X  X ransposed X  X  by some constant value. Now we also assume that the (integer) alphabet R is not arbitrary, but its symbols form a continuous occurrence in text can be in different key. 8.1 Transposition invariant Simple matches the text substring, but this time we must also maintain the set of possible trans-positions for each such text position. First notice that for any symbols p and t the transposition s = t -p makes the symbols match exactly. Taking the d condition into pair of symbols there are exactly 2 d + 1 allowed transpositions.

In the following we make the assumption that 2 d + 1 B w , where w is the number of represent the set of possible transpositions as a pair  X  s ; T X  ; where s = t -p (the base ) and T is the set of the 2 d + 1 possible offsets to the value s . More precisely, T is a bitvector of 2 d + 1 bits. If the k th bit of T is set, then the transposition s + k -d is valid.
Assume now that we have transpositions  X  s 1 ; T 1  X  and  X  s 2 ; T 2  X  ; and we want to compute the transposition  X  s ; T X  that agrees with both, i.e., If s 1 = s 2 then the intersection is an empty set, and we just set T to zero. Otherwise, if | s 1 -s 2 | B 2 d the intersection can be non-empty. To compute the intersection we must first bring T 1 and T 2 into the same base. This is easily achieved by shifting the bitvectors. Assume that s 1 \ s 2 . Then Symmetrically, if s 1 [ s 2 we obtain
Let us now consider extending a (possible) prefix match. Let the current pattern position p ,1 2 d +1 ) (we use exponentiation to denote bit-repetition). This location is a prefix match, if in the previous row there are matching prefixes within a -window, and their corresponding  X  s ; T 1  X  ; ... ;  X  s k ; T k  X  ; k a  X  1 : Then the set of transpositions extending the prefix match to position ( i , j )is where the union [ is simply computed as bit-wise or of the bitvectors T ; as they are all brought to the same base by the intersection operation. Hence, assuming that 2 d + 1 B w , this computation takes O ( a ) time. If the resulting set is non-empty, we put the position j rithm 6 gives the complete pseudo code.
The worst case time of this algorithm is O  X  nm a d d = w e X  : As in plain Simple, computing are longer on average. Clearly | L 0 |= n , since pattern prefix of length 1 matches every text position. Hence computing L 1 costs O ( a n ) time, and the resulting list is of length | L 1 |= In general, assuming that ad / r \ 1, the i th list is of length This is exponentially decreasing with the above assumption. Thus the average time becomes O ( a n ). 8.2 Transposition invariant DP We now present a basic dynamic programming solution that has better worst case complexity than the Simple algorithm. The algorithm (conceptually) maintains a matrix D matrix T as a + 1 sized window of the previous row: The (almost) na X   X  ve implementation of the above recurrence would result in algorithm with improve the average case, and finally reduce the O ( d ) factor using bit-parallelism.
Trivial algorithm implementing our recurrence for D i , j , k would need to scan a + 1 vectors from the previous row. This can be avoided by using counters maintaining the total number of  X  X  X oted X  X  transpositions for each ( a + 1)-window: Thus we can rewrite our main recurrence as The counters can be easily updated in O (1) time per value by incremental computation: This gives us O ( nm d ) worst case time. Note that only O ( m ar ) space is needed for D since only the past O ( a ) columns are needed at any time. This could be reduced to O ( m ad )by using the technique we used in Sect. 8.1 . Similarly C takes only O ( m r ) space, since only one column of counters is needed to be active at any time. Finally, T is not needed explicitly at all, we used it only as a tool for the presentation. Algorithm 7 gives the pseudo code, omitting initialization of the arrays, which are assumed to be all zero before the main loop. It also implements a cut-off trick discussed next. 8.2.1 Cut-off D i +1 ... m -1, j +1, k = 0. This is because there is no way the recurrence can introduce any other j = 0 ... a , then the match at the position j + 1 cannot be extended to p 0 ... p i +1 . This can be utilized by keeping track of the highest row number top of the current column j such that D sake we maintain an array Cco so that Cco i , j gives the total number of  X  X  X oted X  X  transpo-sitions for the last ( a + 1)-window: This is again trivial to incrementally maintain in O (1) time per computed D value. Hence after the row top for the column j is processed, the new value of top is computed as
Now consider the average time of this algorithm. Computing a single cell D i , j costs O ( d ) time. Maintaining top costs only O ( n ) time in total, since it can be incremented only by one per text symbol, and the number of decrements cannot be larger than the number of increments. The average time of this algorithm also depends on the average value of top , is obviously 1 (and top is at least 1). For rows i [ 0 the probability that T i ; j intersects with D Hence the expected length of a prefix match is at most 8.2.2 Bit-parallel algorithm O (1) in MIR applications. To see this, note that the counter values cannot exceed a + 1, so we can pack O ( w /log( a )) counters into a single computer word. All the inner loops (involving 2 d + 1 iterations) can then be computed parallelly, updating O ( w /log( a )) counters in O (1) time. The only non-trivial detail is the computation of minima of two sets of counters (parallelization of the line 19 of Algorithm 7), but the solution exists (Paul and Simon 1980 ), and is reasonably well-known. Note that for realistic assumptions (for MIR achieves O ( n ) time on average. However, in practice d is often so small that the benefit of this parallelization is negligible. 9 Experimental results We have run experiments to evaluate the performance of our algorithms. The experiments were run on Pentium4 2 GHz with 512 MB of RAM, running GNU/Linux 2.4.18 operating system. We have implemented all the algorithms in C, and compiled with icc 7 : 0 : We first experimented with ( d , a )-matching, which is an important application in MIR. For the text we used a concatenation of 7,543 music pieces, obtained by extracting the pitch values from MIDI files. The total length is 1,828,089 bytes. The pitch values are in the range [0 ... 127]. This data is far from random; the six most frequent pitch values occur 915,082 times, i.e., they cover about 50% of the whole text, and the total number of different pitch values is just 55. A set of 100 patterns were randomly extracted from the text. Each pattern was then searched for separately, and we report the average user times. Figure 3 shows the timings for different pattern lengths. The timings are for the following algorithms: DP: Plain Dynamic Programming algorithm (Crochemore et al. 2002 ); DP Cut-off:  X  X  X ut-off X  X  version of DP (as in Cantone et al. ( 2005b ); SDP RW: Basic Row-Wise Sparse Dynamic Programming; SDP RW fast: Binary search version of SDP; SDP RW fast PP: linear preprocessing time variant of SDP RW fast (Algorithm 1); SDP CW: Column-Wise Sparse Dynamic Programming (Algorithm 2); Simple: Simple algorithm (Algorithm 3); BMH + Simple: BMH followed by Simple algorithm (Algorithm 4); BP Cut-off: Bit-Parallel Dynamic Programming (Fredriksson and Grabowski 2006 ); NFA a : Non-deterministic finite automaton, forward matching variant (Navarro and Raffinot 2003 ), using O ( a ) bits per symbol;
NFA log( a ): Non-deterministic finite automaton, forward matching variant (Algo-rithm 5), using O (log( a )) bits per symbol.

We also implemented the SDP RW variant with O  X  but this was not competitive in practice, so we omit the plots.
SDP is clearly better than DP, but both show the dependence on m . The  X  X  X ut-off X  X  variants remove this dependence. The linear time preprocessing variant of the SDP  X  X  X ut-off X  X  is always slower than the plain version. This is due to the small effective alphabet size variant quickly becomes faster as m (and hence the pattern alphabet) increases. The col-umn-wise SDP algorithm and especially Simple algorithm are very efficient, beating everything else if d and a are reasonably small. For very small d and a and moderate m the BMH variant of Simple is even faster. For large ( d , a ) the differences between the algo-rithms become smaller. The reason is that a large fraction of the text begins to match the pattern. However, this means that these large parameter values are less interesting for this application. The bit-parallel algorithm (Navarro and Raffinot 2003 ) is competitive but suffers from requiring more bits than fit into a single machine word, yet Algorithm 5 is even slower, besides having more efficient packing. This is attributed to the additional (constant time per text character) overhead due to the more complex packing. 9.1 Transposition invariance We also experimented with the transposition invariant algorithms. The following algo-rithms were tested: BF-Simple: Plain Simple executed O ( r ) times;
Simple: Transposition invariant Simple (Algorithm 6); DP: (Transposition invariant) Dynamic Programming algorithm; DP Cut-off:  X  X  X ut-off X  X  version of DP (Algorithm 7).

The results are shown in Fig. 4 . In this case Simple is again clear winner, despite of the theoretical superiority of DP Cut-off. For large a DP (Cut-off) would eventually beat Simple, but in practical applications such large parameters are not interesting. 9.2 PROSITE patterns We also ran preliminary experiments on searching PROSITE patterns from a 5 MB file of concatenated proteins. The PROSITE patterns include character classes and general bounded gaps. Searching 1,323 patterns took about 0.038 s per pattern with Simple, and about 0.035 s with NFA. Searching only the short enough patterns that can fit into a single computer word (and hence using specialized implementation), the NFA times drops to about 0.025 s. However, we did not implement the backward search version, which is reported to be substantially faster in most cases (Navarro and Raffinot 2003 ). Finally, note that the time for Simple would be unaffected even if the gaps were negative, since only the magnitude of the gap length affect the running time. 10 Conclusions We have presented new efficient algorithms for string matching with bounded gaps and character classes. Some of those algorithms are designed to work under transposition invariance. Besides having theoretically good worst and average case complexities, the algorithms are shown to work well in practice. Finally, despite that the algorithms were designed for MIR, they have important applications in MB as well. In particular, we can handle even negative gaps efficiently.
 References
