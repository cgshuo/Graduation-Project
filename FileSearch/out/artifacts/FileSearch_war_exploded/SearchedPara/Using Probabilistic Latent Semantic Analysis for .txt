 Search engines, such as Google, Yahoo! and MSN, have been the major tools to help users find useful information on the Internet. However current search technologies essential to personalize web search as well as better the service. 
Many methods are proposed to study user X  X  interests and build user profiles based on user X  X  search history. These methods focus on analyzing content of the queries and web pages, but in some case there are no suitable descriptors such as keywords, topics, genres, etc. that can be used to accurately describe interests. According to [1] , web users usually exhibit different types of behaviors depending on their information needs. Thus, web users with similar web behaviors tend to acquire similar information when they submit a same query to search engine. [2] conducted experiments to verify better retrieval accuracy. The collaborative filtering technique used in [2] is based on nearest neighbor regression or so-called memory-based techniques. These memory-based methods are simple and intuitive on a conceptual level while avoiding the number of severe shortcomings as Hofmann point out in [3] : (i) The accuracy obtained by memory-based methods maybe suboptimal. (ii) Since no explicit scale well in terms of their resource requirements (memory and computing time). response should be in a very short time. (iv) Actual user profiles have to be kept for prediction, potentially raising privacy issues. 
Users X  previous web behaviors and other personal information can be used to clickthrough data to personalize Web search. Clickthrough data is a kind of search log that could be collected by search engine implicitly without any participation of users. relevance feedback or registering interest and demographic information. Through analysis of the clickthrough data, we could consider a single user X  X  behavior characteristic and take similar users X  interests into account, so as to identify the user X  X  search intention and thus improve the search results. 
To address the shortcomings of the memory-based methods mentioned above, we use a model-based technique called Probabilistic Latent Semantic Analysis (PLSA) relationship between users, queries and web pages on the usage data. The advantages of our method are as follows: 
The algorithm could compress the data into a compact model to automatically identify user search intention. The preference predictions could be comput ed in constant time so as to reduce online response time. 
The huge user profile data does not need to be kept. 
The experimental results also show that our proposed algorithm could achieve higher prediction accuracies on real data set collected by MSN search engine. present our model and show how to perform personalized web search based on the model. Our experiments and interpretation of the result is given in Section 4. Finally, we conclude this paper in Section 5. 2.1 Personalized Web Search [5] first proposed personalized PageRank and suggested to modify the global PageRank algorithm, which computes a universal notion of importance of a Web page. [6] used personalized PageRank scores to enable  X  X opic sensitive X  web searches. Because no experiments based on a user X  X  context, this approach actually cannot satisfy different information needs by different users. and web pages. [7] used ontology to model a user X  X  interests, which are studied from user X  X  browsed web pages. To distinguish long-term and short-term interests, [8] and other descriptors are not able to describe information needs accurately. behaviors and information needs and constructs user profiles through a memory-based collaborative filtering approach. Nevertheless it could not avoid the shortcoming listed in Sec.1. 2.2 Probabilistic Latent Semantic Analysis Latent semantic analysis (LSA) [11] stems from linear algebra and performs a information retrieval [12] . The key idea is to map high-dimensional count vectors to a lower dimensional representation in a so-called latent semantic space . Although LSA theoretical foundation remains to a large extent unsatisfactory and incomplete. Hofmann presented a statistical view on LSA which leads a new model, Probabilistic Latent Semantics Analysis (PLSA) [4] [13] , and provided a probabilistic solid statistical foundation than the standard LSA. The basic of PLSA is a latent class hidden factors underlying the co-occurrences among two sets of objects. That means given. PLSA uses Expectation-Maximization (EM) algorithm [14] to estimate the probability values which measure the relationship between the hidden factors and the two sets of objects. 
Because of its flexibility, PLSA has been used successfully in a variety of and co-citation analysis [18] [19] . Furthermore, web usage mining can also be based on PLSA. [1] presented a framework to use PLSA for discovery and analysis personalized web search. In our paper we present the approach and give an experimental evaluation. 3.1 Prediction Problem Description As we described in Sec.1, clickthrough data is collected by search engines without search engine returns search results corresponding to the query. Based on the search need. Search engines could record the behaviors as the clickthrough data. The users, queries and web pages are collected as a co-occurrence triple in the web log. There has submitted several days ago, the search engine can easily to calculate which page is most frequent and rank it to the top one for the user. We experiment the real data from MSN search engine. If selected pages are not new pages, in other words they occurred in any search tasks in the past 20 days, more than 70% precision are reached of the top ones. The other is the queries that the user never submitted. Then the problem is: Given the clickthrough data, which page should be recommended to The output is the page which is predicted the most possible needed page by the user. More generally, the top k pages will be interested as the recommendation problem. 3.2 Model Specification been called aspect model. This model is a latent semantic class model for co- X  with each observation. These unobserved classes stand by the hidden factors underlying the co-occurrence among the observed sets of objects. Therefore this model well capture unseen factor that lead to the fact that web users exhibit different types of behavior depending on their information needs. At the same time it well characterizes the hidden semantic relationship among users, queries as well as users, queries and web pages. Therefore, in our web search submits a query q to a search engine, and selects a page p from the results. In the The relationships are associated with the latent variables } , , { 2 1 k z z z Z z "  X  . The mixture model depends on a conditional independence assumption, namely each set of observed objects are independent conditioned on the state of the associated latent assumption, users, queries and web pages are independent when given search intentions. It means that a user u and a query q determine a latent search intention z , and latent variables in turn  X  X enerated X  web page p . Fig.1 depicts the model as a Bayesian network. Therefore, the joint our model is defined as follows: equivalent symmetric specification: 3.3 Model Fitting with the EM Algorithm PLSA uses the Expectation-Maximization (EM) [14] algorithm to estimate the probability value which measure the relationships between the hidden factors and the alternates two steps:  X  variable, based on the current estimates of the parameters;  X 
A maximization (M) step, where parameters are re-estimated to maximize the expectation of the complete data likelihood. training data, the log likelihood L of the data is: In the E-step, we compute: In the M-step, the formulae are: data until a local maximum optimal solution is reached. 3.4 Prediction in Practice Theoretically in our model, prediction is provided to users according to: memory and offline time cost with large data set. We make assumption that each user U into h groups, Q into i groups and P into j groups. The algorithm also give the we use PLSA to calculate the probability values which measure the relationships between C , D , E and Z , so in practice we predict the probability for a given 4.1 Dataset In our experiments, we use web log data collected by MSN search engine in December 2003. We select those users who use MSN search engine more 25 days in evaluate different scenarios, we design two data sets as following: query are selected as training set and the left as testing data, so we get 290,000 data records in training set and 78,000 data records in testing set. This data set is referred as the first data set in our experiment. data in the first 20 days as training data, while the testing data is from those of later 5 days. If some queries or web pages are in the testing data but not in the training data, without content-based techniques they are impossible to be predicted. So we remove and a testing set with 6,000 data records. We refer to this data set as the second data set in our experiment. 4.2 Evaluation Metric From the view of the user, the effectiveness of our method is evaluated by the list. For each rank 0 &gt; r , we calculate the number of triples that exactly rank the r th method when predicting the top r web pages. 4.3 Baseline Method We have implemented a baseline method to calibrate the achieved results. The method is based on cluster technique. We use the relationship between the users, times of each page is occurred with every user group and query group in the training set and give the prediction. Let 4.4 Experimental Results As we know, the number of cluster is difficult to decide. In our experiment, we tried several times to tune the parameters in order to get higher performance of clustering. optimization after 30-60 iterations. precision has been increased to 22.23% ov er the cluster-based method. The result shows that our model has strong ability of prediction. Without any direct information baseline method on the second data set. 4.5 Computational Complexity In the web search scenario, because of the amount of data is always large, the computational complexity is one of the crucial factors for a successful algorithm. One has to distinguish between the offline and online computational complexity. The actual predictions for specific us ers have to be made. In contrast, the latter deals with those computations that can only be performe d in real-time during the interaction with a specific user. Therefore the online comput ational complexity is more important here. PLSA algorithm has an online computational complexity of |) (| Z O . The detail complexity analysis of PLSA could be found in [20] . In this paper, we present an approach to perform better personalized web search based on PLSA. We consider the latent semantic relationship between users, queries and web pages by a three-way aspect model and use the proposed algorithm to deal with the sparsity problem. Meanwhile, the mode l could character the users X  search intention. An effective algorithm is proposed to learn the model and compute the preference prediction. The results on the real clickthrough data show that our proposed algorithm could achieve higher prediction accuracies than the baseline work. In the future, we consider integrating the content and the link information into the algorithm and doing the better prediction. 
