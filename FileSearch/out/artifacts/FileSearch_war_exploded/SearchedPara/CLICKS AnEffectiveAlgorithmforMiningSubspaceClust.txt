 f We presen t a novel algorithm called Clicks , that  X nds clus-ters in categorical datasets based on a searc h for k-partite maximal cliques. Unlik e previous metho ds, Clicks mines subspace clusters. It uses a sele ctive vertic al metho d to guaran tee complete searc h. Clicks outp erforms previous approac hes by over an order of magnitude and scales bet-ter than any of the existing metho d for high-dimensional datasets. These results are demonstrated in a comprehen-sive performance study on real and syn thetic datasets. Categories and Sub ject Descriptors: H.2.8 [Database Managemen t]: Database Applications -Data Mining General Terms: Algorithms.
 Keyw ords: Clustering, Categorical Data, K-partite Graph, Maximal Cliques, Data Mining
Clustering is one of the cen tral data mining problems; it aims to  X nd \naturally" occurring groups of points in a given dataset. Clustering of numeric (or real-v alued) data has been widely studied, but categorical (or discrete-v alued, sym bolic) data has receiv ed relativ ely less atten tion. There are sev eral challenges in clustering categorical attributes: i) No Natur al Order: The lack of an inheren t natural or-der on the individual domains, renders a large num ber of traditional similarit y measures ine X ectiv e. ii) High Dimen-sionality: Practical examples suggest that categorical data can have man y attributes, requiring metho ds that scale well with dimensionalit y. iii) Subsp ace Clusters: Man y categori-cal datasets, esp ecially sparse ones, do not exhibit clusters over the full set of attributes, thus requiring subspace clus-tering metho ds.

In this pap er, we presen t Clicks (an anagram of the bold letters in S ubspace CL uster I ng of C ategorical data via max-imal K -partite cliques), a novel algorithm for mining cat-egorical (subspace) clusters. Our main con tributions are: 1) We presen t a novel formalization of categorical clusters. This work was supp orted in part by NSF CAREER Aw ard IIS-0092978, NSF gran ts EIA-0103708 &amp; EMT-0432098, and DOE Career award DE-F G02-02ER25538.
 We summarize the dataset as a k -partite graph, and mine maximal k -partite cliques, whic h after post-pro cessing corre-spond to the clusters. The k -partite maximal clique mining metho d is interesting in its own righ t. 2) Clicks uses a sele ctive vertic al expansion approac h to guaran tee complete searc h; no valid cluster is missed. It also merges overlapping cliques to rep ort more meaningful clusters. 3) Clicks ad-dresses the main shortcomings of existing metho ds. Unlik e man y previous categorical clustering algorithms, Clicks can mine subspace clusters. Furthermore, it imp oses no domain constrain ts and is scalable to high dimensions. 4) Clicks outp erforms existing approac hes by over an order of magni-tude, esp ecially for high-dimensional datasets. These results are demonstrated in a comprehensiv e performance study on real and syn thetic datasets. Our de X nition of categorical clusters is based on ideas  X rst prop osed in [4]. Let A 1 ; : : : ; A n denote a set of cate-goric al attributes and D 1 ; : : : ; D n a set of domains , where D i = f v i 1 ; : : : ; v i m g is the domain for attribute A D i \ D j = ; for i 6 = j . A dataset is a subset of the cartesian pro duct of the attribute domains, given as D  X  D 1  X  : : :  X  D The num ber n of attributes is also referred to as the dimen-sionality of the dataset. An elemen t r = ( r:A 1 ; : : : ; r:A D is called a record , where r:A i 2 D i refers to the value for attribute A i in r . Eac h record also has a unique record id (rid), given as r :id .

Let S j  X  D i j a subset of values for attribute A i j . A k -subsp ace is de X ned as the cross-pro duct S = S 1  X  : : :  X  S some subset of k attributes A i 1 ; : : : ; A i k . Eac h S a projection of S on attribute A i j . If k = n , then the n -subspace is also called a full-sp ace . Giv en any two subspaces X = X 1  X  : : :  X  X m and Y = Y 1  X  : : :  X  Y n , we say that X is containe d within Y , denoted as X  X  Y , i X  m  X  n and 8 i 2 [1 ; m ] there exists a unique j 2 [1 ; n ], suc h that X i  X  Y j . Giv en any collection S of subspaces, M 2 S is a maximal subsp ace i X  there does not exist M 0 2 S , suc h that M  X  M 0 .

Let S = S 1  X  : : :  X  S k be a k -subspace, with k  X  n . A record r = ( r:A 1 ; : : : ; r:A n ) 2 D belongs to S , denoted r 2 S , i X  r:A i j 2 S j for all j 2 [1 ; k ]. The supp ort of S in dataset D is de X ned as the num ber of records in the dataset that belong to it; it is given as  X  ( S ) = jf r 2 D : r 2 S gj . S is called a frequent subsp ace if  X  ( S )  X   X  min , where  X  some user-de X ned minim um supp ort threshold. Under at-tribute indep endence, the expected supp ort of S in D is given pected dataset distributions by mo difying the de X nition for E [  X  ( S )]. Let  X  2 R + . De X ne a density indic ator function  X  ( S ) as follo ws:  X   X  ( S ) = 1 i X   X  ( S )  X   X   X  E [  X  ( S )], otherwise  X  ( S ) = 0. S is called a dense subsp ace i X   X   X  ( S ) = 1, that is, if its exp ected supp ort exceeds its actual supp ort by a user-de X ned factor  X  (also called density threshold ).
Tw o sets of pro jections S i and S j , are called strongly con-nected i X  8 v a 2 S i and 8 v b 2 S j , the 2-subspace f v is dense. S = S 1  X  : : :  X  S k is called a strongly conne cted sub-space i X  S i is strongly connected to S j for all 1  X  i &lt; j  X  k .
De X nition: (Categorical Cluster) Let D be a categor-ical dataset and  X  2 R + . The k -subsp ace C = ( C 1  X  : : :  X  C is a (subspace) cluster over attributes A i 1 ; : : : ; A maximal, dense, and strongly conne cted subsp ace in D . The projection C i is also called the cluster pro jection of C on attribute A i j . If k &lt; n , then C is called a subsp ace cluster or a k -cluster, otherwise C is called a full-sp ace cluster .
Our cluster de X nition requires the k -subspace C be dense, i.e, it should enclose more points than exp ected under at-tribute indep endence. Also only maximal clusters are mined to reduce the amoun t of redundan t information. Ideally one would like to disco ver only maximal, dense subspaces as clus-ters. However notice that densit y is not downwar d close d (i.e., for a dense subspace Y there may exist a subspace X  X  Y , suc h that X is not dense), whic h mak es it di X -cult to prune the searc h space. However, we believ e that dense subspaces are more informativ e than say purely fre-quen t ones. To mak e the searc h for dense spaces tractable we use the notion of strong connectedness, whic h is down-ward closed. This enables new candidate subspaces to be extended from existing (strongly connected) ones, leading to more e X cien t searc h.

Example 1. Consider the sample dataset D given in Ta-ble 1 with a total of three categoric al attributes A 1 ; A and six displaye d records. Her e D 1 = f a 1 ; a 2 ; a 3 f b attributes and their values are indep endent the expected sup-port for any pair of values, i.e, the 2-subsp ace f v i g  X  f v from di X er ent attributes A i and A j is E [  X  ( f v i g  X  f v 1 = 3  X  1 = 3  X  6 = 0 : 67 . Thus any pair of values that occurs at least 2 times (i.e,  X  ( f v i g  X  f v j g ) = 2 ) will be dense (sinc e 2 = 0 : 67 = 2 : 98 &gt;  X  ). Thus for  X  = 2 : 5 ther e is only one full-sp ace cluster in this dataset ( f a 2 g  X  f b 3 g  X  f c is an additional subsp ace cluster: ( f b 1 g  X  f c 1 g ) .
On the other hand, if we use  X  = 1 : 5 , then any pair of values that occurs onc e will be consider ed dense. Thus for  X  = 1 : 5 , ther e are 3 full-sp ace clusters in this dataset: ( f a 1 ; a 2 g  X  f b 1 g  X  f c 1 g ) , ( f a 2 ; a 3 g  X  f b f b 3 g  X  f c 2 ; c 3 g ) . Ther e are two additional subsp ace clusters: ( f a 2 g  X  f b 1 ; b 3 g ) , and ( f a 2 g  X  f c 1 ; c 2 teresting property of our appr oach is that the clusters found for higher  X  will always be containe d in a lower  X  , which can allow the user to produc e a cluster hier archy.
Full-space clustering has been an activ e researc h topic for a long time; more recen tly, since CLIQUE [1] introduced the problem, man y subspace clustering techniques have been also been prop osed. We focus here only on the relev ant researc h in categorical clustering.

K-mo des [9] is an extension to the k-means numeric clus-tering algorithm. COOLCA T [3] is based on the idea of entrop y reduction within the generated clusters. LIMBO [2] is a recen t information theoretic clustering based on the information-b ottlenec k framew ork. STIRR [5] uses a non-linear dynamical systems approac h to categorical clustering. It enco des the dataset into a weigh ted (with attribute-v alues as vertices) graph and iterativ ely propagates these weigh ts until con vergence to \basins". The main weakness of STIRR is that the separation of attribute values by their weigh ts is non-in tuitiv e and the post-pro cessing required to extract the actual clusters from the basin weigh ts is non-trivial. ROCK [7] uses an agglomerativ e hierarc hical clustering approac h, using the num ber of \links" (i.e., shared similar records) be-tween two records as the similarit y; it has O ( jD j 3 ) complex-ity, making it unsuitable for large problems. CA CTUS [4]  X rst builds a summary information from the dataset and it then computes cluster pro jections onto the individual at-tributes and then extends them to  X nd cluster candidates over multiple attributes. The extension step, though de-scrib ed in the pap er, was not implemen ted by the authors, but our augmen ted implemen tation sho wed a sev ere perfor-mance impact over the cactus baseline version (see Section 5.1). Note also that with the exception of CA CTUS, whic h mines only a limited class of subspace clusters, none of the previous metho ds can mine subspace clusters.

Other previous work has focused on binary or transac-tional data [13, 8]. Also relev ant is the problem of bi-clustering [11], whic h aims at  X nding subspace clusters on both attributes and records. However, these metho ds can-not typically handle the kinds of clusters we prop ose here by de X nition two values of the same attribute nev er co-o ccur in any transaction, ans as suc h they will not be part of the same cluster.
Clicks mo dels a categorical dataset as a k -partite graph where the vertex set (attribute values) is partitioned into k disjoin t sets (one per attribute) and edges exist only between vertices in di X eren t partitions, indicating dense relation-ships. The adjacency matrix of the k -partite graph serv es as a compressed represen tation of the data that can  X t into main memory for even very large datasets. Clicks maps the categorical clustering problem to the problem of enumerat-ing maximal k -partite cliques in the k -partite graph.
De X nition ( k -Partite Graph and Clique) Let D be a categoric al dataset over attributes A 1 ; : : : ; A n and V = S i =1 D i . The undir ected graph  X  D = ( V; E ) wher e ( v E ( )  X   X  ( f v i g  X  f v j g ) = 1 is called the k -partite graph of D . A subset C  X  V is a k-partite clique in  X  D i X  every pair of vertic es v i 2 C \ D i and v j 2 C \ D j (with i 6 = j ) are conne cted by an edge in  X  D . If ther e is no C 0  X  C such that C 0 is a k-partite clique in  X  D , C is called a maximal k-partite clique . A clique C is dense if  X   X  ( C ) = 1 in D . Lemma 2. Given a categoric al dataset D and a k -subsp ace C = C 1  X  : : :  X  C k with C j  X  D i j over attributes A i C is a k -cluster in D if and only if C is a maximal, dense k-partite clique in  X  D .

Example 3. Consider the example in Table 1. Let  X  = 2 : 5 , then any pair of values that occurs at least 2 times is dense in D , and thus ther e is an edge betwe en such vertic es in  X  D . The corresponding k -partite graph of D is shown in Figur e 1, using bold edges. It clearly has two clusters, one full-sp ace and one sub-sp ace. If  X  = 1 : 5 , then some other (thin) edges will be adde d to the graph. Mining this new graph will produc e the larger set of clusters mentione d in Example 1. It should be clear that clusters mine d at  X  = 2 : 5 are containe d in those at  X  = 1 : 5 , sinc e a lower  X  only adds edges to  X  D . Giv en a dataset D and a user-sp eci X ed densit y threshold  X  2 R + , we are interested in mining all full-space and sub-space clusters (i.e., all maximal, dense, and strongly con-nected subspaces) in D . Since densit y is not downward closed, we use the strongly-connected prop erty to mine L , the set of all maximal k -partite cliques in  X  D . We then follo w-up with a validation step, that veri X es whether  X  1 for all cliques C 2 L . This two-step approac h is very e X cien t, but it is not complete, since it is possible that some maximal clique C 2 L is not dense, whereas its sub-set C 0  X  C migh t be dense. To guaran tee completeness Clicks uses as another step the sele ctive vertic al expansion technique to enumerate subspaces of a non-dense maximal clique. Our exp erimen ts sho w that most of the  X nal clusters can be found using only the  X rst two steps, but if complete-ness is desired, all clusters will be guaran teed to be found for an additional cost. It should be noted that even with selectiv e vertical expansion Clicks is faster than previous categorical clustering metho ds. Note that Clicks can mine maximal k -partite cliques for any 1  X  k  X  n ; if k = n , the disco vered cliques are clusters over the full set of dimen-sions, and if k &lt; n then the disco vered cliques are subspace clusters. We also note that Clicks is  X  X xible enough to mine only (maximal) frequen t clusters if so desired (a minor change in the pre-pro cessing step accomplishes this).
The basic Clicks approac h consists of the three principal stages, sho wn in Figure 2, as follo ws: 1) Pre-pr ocessing : We create the k -partite graph from the input database D . We also rank the attributes for e X ciency reasons. 2) Clique De-tection : We enumerate all the maximal k -partite cliques in the graph  X  D . 3) Post-pr ocessing : We verify the supp ort of the candidate cliques within the original dataset to form the  X nal clusters. If completeness is desired we perform selectiv e sub-clique expansion of non-dense maximal cliques to  X nd the true maximal, dense cliques. Moreo ver, the  X nal clusters are optionally merged if they have signi X can t overlap.
In one scan of the dataset, Clicks collects the supp ort of all single and pairs of attribute values. From the pairs it computes the dense pairs f v a g  X  f v b g , and add an edge ( v a ; v b ) to  X  D , creating the full k -partite graph  X  D
Giv en  X  D and V = S n i =1 D i = f v 1 ; : : : ; v m g , the neighb ors of an attribute value v j are de X ned as N ( v j ) = f v k ( v ; v k ) 2 E g . Note also that, by de X nition, if v j ; v then v k 62 N ( v j ), since values of the same attribute nev er co-occur. However, for the clique enumeration step, we have to consider all values of an attribute to be implicitly connected.
The conne ctivity of vertex v j 2 D i is de X ned as: Intuitiv ely, connectivit y corresp onds to the neigh bors ( N ( v plus the remaining values of the attribute in question ( D However, if a given value does not co-o ccur with values of other attributes it cannot be part of a k -partite clique; its connectivit y should be zero. The connectivit y of a clique C is given as follo ws:  X  ( C ) = T v common to all vertices in C . Clicks ranks the set of all at-tribute values by decreasing connectivit y for e X cien t clique enumeration. Giv en a seed clique, Clicks adds a new vertex to the clique based on the next highest rank ed value. This can signi X can tly speed-up the searc h for maximal cliques.
The clique detection phase is based on a bac ktrac king searc h, that at eac h step, adds only those vertices to a clique that are in the connectivit y set of the clique. If more than one suc h vertex exists, attribute value ranking is used to break the tie. Clicks uses a recursiv e algorithm that at eac h stage tries to expand the curren t clique to ensure max-imalit y. It is similar in spirit to the Bron-Kerb osch (BK) algorithm [10], but whereas BK enumerates regular cliques, Clicks is designed for k -partite cliques. The pseudo-co de for the clique detection phase is sho wn in Figure 3. Initially DetectMaxCliques is called with the empt y clique C and the full, rank ed attribute value set R as a list of possible vertices to be used for an extension. In general, R represen ts the set of vertices that can be used to extend the curren t clique C . Up on return, the clique collection L con tains all maximal k -partite cliques in the dataset.
Note that foreach statemen ts pro cess attribute value rank-ings (based on connectivit y) in descending order. The pred-icate  X ( C ) evaluates to true i X  i) we want to mine subspace clusters, or ii) we want to mine full space clusters and C con tains at least one attribute value for every attribute of the dataset. Otherwise  X ( C ) is false . The set R D con tains all elemen ts of R that have their delete d  X  X g set. Similarly , R P is the subset of R that con tains all elemen ts that have their processe d  X  X g set.
 DetectMaxCliques starts by chec king if the curren t clique C is maximal (lines 1-2). If R = ; then there are no more el-emen ts to extend C , thus C is poten tially maximal. If in ad-dition  X  ( C ) = ; then C is a maximal clique, since an empt y connectivit y set means there are no additional vertices con-nected to all vertices in C . The only test that remains to be done is whether full/sub-space cliques are desired. For subspace clusters  X ( C ) is alw ays true, whereas for full-space clusters  X ( C ) is true only if C con tains at least one value from eac h attribute. Thus, C is added to the set of maximal cliques L i X   X ( C ) is true (line 2), and the searc h is con tinued at the previous level (line 3).

If R 6 = ; then C can poten tially be extended with vertices in R . The outer loop (line 5) attempts to add a value v to C in an e X ort to create a yet larger clique C 0 (line 6). Note also that at any given point in time R con tains only those attribute values that are connected to C . Hence, adding v 2 R to C will yield another clique C 0 . We mark v as deleted ( R D = R D [ f v g ), indicating that it was already considered in the clique construction (line 7).

To main tain the condition that all attribute values in R are connected to C , a R 0 matc hing C 0 needs to be con-structed before the recursiv e call. The inner foreach loop (line 8) scans all attribute values that were possible exten-sions to C and selects only those (line 9) that are in the connectivit y set of v that was added to C in line 6. For the  X rst vertex in R , we main tain a list of nodes already considered in R P (line 10).
 Finally , the algorithm recurses on the newly created clique C 0 with its matc hing attribute value ranking R 0 . If only full-dimensional clusters are to be detected we can prune part of the searc h space at this point; we can stop the recursion if the new clique C 0 cannot be extended to cover at least one value from all attributes, i.e, we recurse only if  X ( R 0 is true (lines 11-12).

Both R D and R P are also used for pruning. Consider two possible extensions v 1 and v 2 of a clique C . If an extension by v 1 was attempted before, the set of possible extensions to v 2 ( R 0 ) does not need to con tain v 1 . If a clique con tain-ing both v 1 and v 2 exists, it was disco vered when C was extended by v 1 , because in that case v 1 and v 2 form a dense 2-subspace and, hence, v 2 was part of the R 0 accompan ying v . The set R D prunes these cases by recording every value that has already been used to extend C . Similarly , if v was already part of the R 0 accompan ying v 1 , it need not be considered as an extension to C . This latter case is guarded against by the processe d attribute values R P .
Example 4. Consider the k-partite graph encoding  X  D in Figur e 1. An attribute value ranking of V is as fol-lows: a 2 (7) , b 1 (6) , c 1 (6) , b 3 (6) , c 3 (5) , a b (0) , wher e the conne ctivity cardinalities j  X  ( v ) j are given in parentheses. Figur e 4 shows a run of DetectMaxCliques on this example. Vertic es depicte d without circles denote search paths that wer e prune d due to R P , wher eas bold squar es indi-cate that a maximal clique was found. By following the edges up to the root we can construct the corresponding cliques. The R 0 sets can be read from the  X gur e by computing the union of all childr en of a node. For example, R 0 for clique f a 2 ; b 1 g (in the leftmost path) is f c 1 ; b 3 ; a 1 g . This example shows all  X ve full and subsp ace maximal cliques. For exam-ple f a 2 ; b 1 ; c 1 ; a 1 g is a full space clique, wher eas f a is a subsp ace clique.
Once the set of all the maximal k -partite (or n -partite) cliques L have been mined, the post-pro cessing phase in-volves a single scan of the dataset to coun t, for eac h candi-date clique C 2 L , the num ber of transactions in the dataset that supp ort it. If  X   X  ( C ) = 1, i.e, the supp ort of C is at least  X  times its exp ected supp ort, then C is a valid clique, and Clicks outputs it as a cluster. However, there are two chal-lenges that remain: 1) a maximal clique may fail the densit y test, whereas one of its sub-cliques may be dense. To guar-antee completeness, Clicks allo ws an optional sele ctive ver-tical expansion approac h to explore the sub-cliques induced by a non-dense maximal clique; we give more details of this step in the next section. 2) There may be man y overlap-ping cliques in L . In this case, it is desirable to merge those cliques that have signi X can t overlap into large cliques; we give details of this step below.

Note that overlapping cliques are mainly a result of the strict notion of strong connectedness for a cluster. For in-stance, consider a clique C = C 1  X  : : :  X  C k , and consider a vertex v m suc h that v m is dense w.r.t. all subspaces except for one, say C j = f v 1 ; : : : ; v l g . Assume that v w.r.t. all vertices in C j except for v a . In this case v not belong to the maximal clique C , but it may belong to another maximal clique C 0 that has a high degree of over-lapping subspaces with C . If we detect suc h a case, it would be preferable to merge suc h cliques into a single cluster.
The enhanced post-pro cessing step in Clicks implemen ts a novel metho d for merging a set of disco vered maximal cliques based on their common cover age , i.e., the num ber of records that are common to that set of cliques. Let L be the set of maximal cliques mined from  X  D . For ev-ery clique C i 2 L let i denote its unique clique id . We de X ne the term cset to denote any set of clique ids. Let C denote the database of csets obtained by replacing eac h record r 2 D with its cset, the set of clique ids that the record belongs to, given as cset ( r ) = f i : r 2 C i g can then mine the cset database C to obtain all the maxi-mal frequen t csets, denoted as F C , that are above a mini-mum frequency threshold  X  C , i.e., those csets that are co-supp orted by a minim um num ber of records. Note that F C can be e X cien tly mined using any maximal itemset mining metho d (suc h as GenMax[6]). For example, consider Ta-ble 1. Let C 1 = f a 2 g  X  f b 3 g and C 2 = f b 3 g  X  f c only maximal cliques in  X  D . Then the cset database C is given as: C = n fg ; f 1 g ; f 1 ; 2 g ; fg ; f 1 ; 2 g ; f 2 g ; : : : C with minim um supp ort  X  C = 2 yields the maximal cset f 1 ; 2 g suggesting that cliques C 1 and C 2 should be merged into one clique: f a 2 g  X  f b 3 g  X  f c 3 g .

Every cset X 2 F C is a poten tial set of cliques that can be merged. However F C may itself have overlapping max-imal csets, and of various sizes. Clearly we need a rank-ing of csets so that merging can be done in a systematic manner. A good ranking measure is the coverage, i.e., the num ber of records in D , that belong to the clique obtained after merging all cliques ids in a given cset. Unfortunately , computing the coverage for eac h X 2 F C can be very ex-pensiv e, since it would require multiple scans of the original database D . Instead, we introduce an appro ximate measure of coverage, called cover age weight , that does not need to access the original database D ; it uses the clique supp ort already computed from D in the validation step, and cset supp ort computed while mining F C . Intuitiv ely, the cover-age weigh t is an appro ximation of the inclusion/exclusion computation for the supp orting records. More formally , given any X 2 F C , where X = f 1 ;  X   X   X  ; m g is a set of clique ids (corresp onding to cliques  X  C 1 ; : : : ; C m  X  quen tly occur together, its cover age weight is de X ned as ! ( X ) =  X  P m i =1  X  D ( C i )  X   X  ( m  X  1)  X   X  C ( X ), where  X  denotes X 's supp ort within C . For merging decisions, all csets in F C are sorted in decreasing order of their coverage weigh t.

Figure 5 sho ws the pseudo-co de for the post-pro cessing phase, including the merging steps. After validating the set of mined maximal cliques L , by coun ting their supp ort and computing their densit y (line 1), we call selectiv e vertical ex-pansion if needed (line 2). This is follo wed by transforming the dataset D into the cset database C , whic h is mined at minim um supp ort  X  C to obtain all maximal frequen t csets F , using GenMax [6] (line 3). This set is sorted in decreas-ing order of coverage weigh t to obtain a total order (denoted by the relationship &gt; ! ) on all maximal csets (line 4).
We then pro cess eac h set X 2 F C in order of &gt; ! (line 6); X is added to F P (line 7) the set of pro cessed csets, that give the clique ids of cliques to be merged in the end. Since no clique can be merged twice, all clique ids that occur in X have to be remo ved from the not-y et-pro cessed csets Y &gt; ! X (lines 8-9). Finally , we create the  X nal set of merged clusters L by iterating through eac h cset Z 2 F P (line 12), and and merging the cliques accordingly (line 13). Before merging, we mak e a cop y of L in L 0 (line 10), so that we can merge cliques iden ti X ed by Z by looking at L 0 (line 13), whereas L con tains only the  X nal set of cliques. If a merged clique M (line 14) is poten tially dense we add it to the  X nal set of clusters (line 15).
All maximal cliques L are rep orted by the clique detec-tion phase, but some migh t be pruned out in post-pro cessing if they are non-dense; let L P  X  L denote all suc h pruned cliques. Sub-cliques of a pruned clique, however, migh t have required densit y. In suc h cases, to guaran tee completeness Clicks uses the sele ctive vertic al expansion approac h to con-sider all sub-cliques for eac h C 2 L P , and rep orts all the remaining maximal, dense cliques.
 For any vertex v in the k -partite graph  X  D of a dataset D , de X ne its ridset to be the set of all record ids where v occurs, given as  X  ( v ) = f r :id : r = ( r:A 1 ; : : : ; r:A D ; and r:A i = v g . The supp orting ridset for any clique C = C 1  X  : : :  X  C k , can then be de X ned as  X  ( C ) = T i 2 [1 ;k ] where  X  ( C i ) = S v D in Table 1, we have  X  ( a 2 ) = f 2 ; 3 ; 4 ; 5 g . For C = f a f b 1 g  X  f c 1 ; c 2 g , we have  X  ( C ) = (  X  ( a 1 ) [  X  ( a Note that the cardinalit y of the ridset gives the supp ort for the corresp onding clique. For example,  X  D ( f a 2 g ) = j  X  ( a 2 ) j = 4 and  X  D ( f a 1 ; a 2 g  X  f b 1 g  X  f c
The selectiv e vertical expansion step uses ridsets to ex-plore all sub-cliques for eac h C 2 L R . Starting from the ridsets for the single values in C , we build larger sub-cliques using a series of union and intersection operations on the cor-resp onding ridsets. The searc h stops when we have found all maximal dense sub-cliques con tained within C . For eac h suc h sub-clique, if it is not con tained within an already found maximal dense clique in LnL P , we output it as a true max-imal, dense clique.
This section presen ts a comparativ e study of Clicks ver-sus CA CTUS [4] and other metho ds like ROCK [7] and STIRR [5]. All testing was done on a hyper-threaded In-tel Xeon 2.8GHz with 6GB of RAM, running Lin ux. The code for CA CTUS was obtained from its authors.

All syn thetic datasets used in our exp erimen ts were cre-ated using the generation metho d prop osed in [5]. The gen-erator creates a user speci X ed num ber of records that are uniformly distributed over the entire data space. It allo ws for speci X cation of the num ber of attributes and the domain size on eac h attribute. The generator then injects a user speci X ed num ber of additional records in designated cluster regions, thus increasing the supp ort of these regions above their exp ected supp ort.

In the performance studies below we use three attributes with domain size of 100 (unless speci X ed otherwise), and we embed two clusters, located on the attribute values [0 ; 9] and [10 ; 19] for every attribute. Eac h cluster was created by adding an additional 5% of the original num ber of records in this subspace region. In all performance tests,  X  = 3 (dis-tinguishing num ber) and  X  = 3 were chosen for CA CTUS as suggested by Gan ti et al. Clicks was also con X gured to use  X  = 3. We will sho w that compared to previous metho ds, Clicks is orders of magnitude faster and/or deliv ers more intuitiv e and better clustering results. We  X rst compare Clicks with ROCK [7]. Even though ROCK assumes that inter-p oint similarities are given, Fig-ure 6 sho ws that Clicks still outp erforms ROCK by orders of magnitude. Since ROCK is too slow, we only compare Clicks with STIRR and CA CTUS below.

We next compare with CA CTUS. As noted earlier, the available CA CTUS implemen tation stops at the cluster pro-jection step, but does not extend these to pro duce the  X nal clusters. Note that the rep orted performance in [4] focuses only on I/O cost, and does not accoun t for CPU cost of extension and validation, since they were mainly interested in sho wing the e X ectiv eness of the small data summaries, even for very large datasets. To study the impact of these additional steps, we augmen ted CA CTUS with the cluster extension and validation steps.

Figure 7 sho ws the running time of CA CTUS with and without the additional steps, on datasets with up to 500,000 tuples. We see that CA CTUS with extensions is about 3 times slower than the base-line version, and the gap is in-creasing. This impact is largely due to the excessiv e num ber of pro jections that CA CTUS generates. In the remaining performance studies only the base-line CA CTUS version is used, since the version with extensions is too slow to be run on larger datasets.

Three tests on syn thetic datasets were performed to com-pare the performance of Clicks and CA CTUS w.r.t. num ber of records, num ber of attributes, and domain size, as sho wn in Figure 9: 1) Dataset Size: Syn thetic datasets with 10 attributes, and 100 values per attribute were used, while the total num ber of records was varied from one to  X ve million. Both metho ds scale linearly over the num ber of records in the dataset, but Clicks outp erforms CA CTUS (base-line) by an average of 20%. If we tak e CA CTUS with extensions into accoun t Clicks is at least 3-4 times faster. 2) Di-mensionalit y: Clicks is esp ecially scalable w.r.t. dimen-sions. On a dataset with 1 million records and 100 attribute values per dimension, Clicks outp erforms CA CTUS (base-line) by a factor 2 -3 (and thus CA CTUS (extension) by at least a factor of 6-9), when varying the num ber of attributes from 10 to 50, and the gap is increasing. 3) Domain size: Datasets with one million records and four attributes were used to measure the performance w.r.t domain size. The num ber of attribute values per attribute were varied from 50 to 500. Both metho ds perform equally well for less than 400 attribute values per domain. At this point, the run-time of CA CTUS dramatically increases, most likely due to memory shortage. For large domains, Clicks is thus over an order of magnitude faster than CA CTUS.

The STIRR [5] algorithm, as implemen ted by Gan ti et al. was also benc hmark ed. STIRR outputs the non-principal basins, i.e, weigh ted vertices, that iden tify the cluster pro-jection on eac h attribute. As in the case of CA CTUS, no clusters are actually output. However, it seems clear that the  X nal cluster extraction step in STIRR would cost at least as much as the extension step in CA CTUS.
To evaluate the qualit y of the clusters, three basic sce-narios were tested on syn thetic datasets, with  X  = 3, and with post-pro cessing turned o X , in order to verify the actual rep orted cliques before merging. The datasets, as sho wn in 8, con tained 105 ; 000 records in scenarios one and two. In scenario 3, 110 ; 000 records were used, re X  X cting addi-tional 5000 records in the third cluster. In all scenarios, attributes have 100 values, but clusters are embedded only on the ranges dra wn. For example, scenario 1 has two at-tributes with two well separated clusters, one on attribute values [0  X  9] and another on [10  X  19] on A 1 and A 2 ; there are no clusters on attribute values [20  X  99] even though there are points in the dataset, chosen uniformly at random, in that range.
 Scenario 1: used a dataset with clear separation of the two clusters on ranges [0  X  9] and [10  X  19]. Clicks de-tected both the clusters on the appropriate attribute values. The CA CTUS implemen tation rep orted 240 cluster pro jec-tions per attribute. These represen ted all subsets of size 3 ter pro jection but do not satisfy the maximalit y condition. Our CA CTUS extension connected all subsets of f 0 ; : : : ; 9 g on the  X rst attribute with the corresp onding subsets on the second attribute. Similarly , all subsets of f 10 ; : : : ; 19 g were connected on both attributes, yielding 115 ; 200 clusters (re- X  X cting the lack of maximalit y of the pro jections). The STIRR algorithm rep orted weigh ts of about 0 : 15 for the attribute values [0 ; 19] on both attributes, while the weigh ts of the attribute values in [20 ; 99] were computed to be about 0 : 08. According to the interpretation in [5] this corresp onds to a single cluster on [0 ; 19]  X  [0 ; 19], con X rming the lack of separation found in [4].
 Scenario 2: used a dataset with a sligh t overlap between two clusters on one of the two attributes. Clicks detected three initial cliques, two of whic h represen ted the original clusters and an additional clique on [7 ; 9]  X  [0 ; 19]. Note that the third clique is correct according to our cluster de X -nition. However, the merge step in the post-pro cessing step could optionally merge this third clique with one of the two primary cliques. CA CTUS, on the other hand, rep orted 480 cluster pro jections, whic h were subsets of the three clusters that Clicks rep orted. STIRR rep orted di X eren t weigh ts for attribute values (i) outside the clusters, (ii) inside one cluster, and (iii) inside both clusters. A non-trivial post-pro cessing step external to STIRR could perhaps separate the attribute values based on these weigh ts. Scenario 3: used a dataset with two clearly separated clusters and a third cluster that fully overlaps with the  X rst cluster on attribute A 1 , and with the second cluster on at-tributes A 2 and A 3 . Clicks rep orted two initial cliques on tively. These cliques were also the  X nal clusters generated by Clicks . This beha vior is correct w.r.t. the cluster de X ni-tion, as [10 ; 19]  X  [10 ; 19]  X  [10 ; 19] is not maximal. CA CTUS rep orted non-maximal subsets that yield about 312 million possible com binations. Veri X cation of their correctness was not possible on our curren t mac hine due to the complexit y of the extension operation. As in scenario 1, STIRR rep orted weigh ts of about 0 : 15 where a single cluster is presen t, 0 : 21 where clusters overlap, and 0 : 08 on all other attribute val-ues. Again, it is not obvious how to extract actual clusters from these weigh ts.
 These results con X rm that Clicks is sup erior to both CA CTUS and STIRR in detecting even the simplest of clus-ters!
We also assessed the e X ectiv eness of the post-pro cessing and selectiv e vertical expansion steps. Due to space limita-tions, we only give the conclusions. In the post-pro cessing step, we found that the merging time was not a X ected by the chosen  X  C value, since only a small fraction of the to-tal execution time is spent validating and merging clusters. We also found that selectiv e expansion can be between 2-5 times slower than the baseline metho d, mainly due to the building of ridsets. The added overhead of selectiv e min-ing was at most linear (in the num ber of records) w.r.t. the base-line, making it a computationally viable option even for large datasets. Most imp ortan tly, even with vertical min-ing enabled Clicks is faster than other metho ds on man y datasets.
We applied Clicks on sev eral real datasets, but here we presen t results only on Mushro om; see [12] for more details.
The Mushro om dataset (from UCI Mac hine Learning Rep os-itory { http://www.ics.uci.edu/mlearn) con tains 8124 records and 22 categorical attributes. Eac h record describ es one Mushro om specimen in terms of 22 physical prop erties (e.g., color, odor, and shap e) and con tains a lab el designating the specimen as either poisonous (3916 records) or edible (4208 records).
 Table 2: Mushro om: Confusion Matrix (Full Space) Table 3: Mushro om: Confusion Matrix (Subspace)
Clicks was con X gured to run with a low  X  value of 0 : 4 as the dataset is very sparse. Not surprisingly , man y of the candidate clusters were overlapping. By assigning eac h record to the  X rst cluster that con tains it, the confusion matrices sho wn in Tables 2 and 3 were generated for full dimensional and subspace clustering, resp ectiv ely. The two rows represen t the two original classes, poisonous(P) and edible (E), while the columns represen t the clusters that Clicks generated. Eac h cell records the percen tage of points that belong to that cell (e.g., in Table 2, 21.3% of all points belong to cluster C 2 and have the lab el 'P'). Note, that the class attribute was not used in the clustering.

Full-dimensional clustering initially yielded 256 candidate clusters whic h were then reduced to 213 clusters using a  X  value of 0 : 5% for the post-pro cessing step. Out of the 213 total clusters, 14 of them accoun t for 87 : 1% of all points; these clusters with highest supp ort values are sho wn explic-itly ( C 1 to C 14 ) in Table 2, while the other smaller clus-ters are group ed under the column Others . Note that these smaller clusters accoun t for only 4% of all points, and thus one can safely discard these clusters to yield 14 useful full-dimensional clusters whic h sho w perfect purit y w.r.t. the class lab el. Ab out 9% of the records could not be group ed (column None ) in any cluster.

For the subspace case Clicks pro duced 596 initial clusters (including full-space clusters). This num ber was reduced to 553 by merging with a  X  C setting of 5%. As in the full di-mensional case, a large num ber of clusters overlapp ed. By assigning eac h record to the  X rst cluster that con tains it, Table 3 was obtained. All points can be covered by only 19 clusters, of whic h C 1  X  C 14 are full-dimensional (same as before), and there are  X ve new subspace clusters C 15  X  C The subspace clusters clearly impro ved the result w.r.t. the unclustered records, since all records are now covered by some cluster (the None column has 0% of points, as op-posed to 8 : 9% in the full dimensional case). Cluster C 17 and C 18 sho w minor impurities (less than 1%) w.r.t. the class lab el. By using all 553 clusters a perfectly pure clus-tering is obtained. However, this level of gran ularit y will be inappropriate for most applications.
 Ackno wledgmen t: We would like to thank Venk atesh Gan ti for pro viding the source code for CA CTUS and STIRR, and Eui-Hong \Sam" Han for the ROCK implemen tation.
