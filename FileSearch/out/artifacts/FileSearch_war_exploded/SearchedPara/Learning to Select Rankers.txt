 Combining evidence from multiple retrieval models has been widely studied in the context of of distributed search, metasearch and rank fusion. Much of the prior work has focused on combining retrieval scores (or the rankings) assigned by different retrieval models or ranking algorithms. In this work, we focus on the problem of choosing between retrieval models using performance estimation. We propose modeling the differences in retrieval performance di-rectly by using rank-time features  X  features that are available to the ranking algorithms  X  and the retrieval scores assigned by the rank-ing algorithms. Our experimental results show that when choosing between two rankers, our approach yields significant improvements over the best individual ranker.
 Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation, Theory Keywords: Combining Searches, Learning to Rank, Metasearch
Combining evidence from multiple sources has been studied in various contexts [2, 1, 4, 6]. The basic premise for combining evidence from multiple retrieval models is that there is no single model that performs the best on all queries. Several rank fusion [7] and rank aggregation [4] approaches have been proposed to re-rank documents based on retrieval scores (or rankings) obtained from individual rankers. However, most of these approaches either learn a fixed (query independent) set of weights that are used to com-bine document scores or utilize a voting scheme for combining the rankings.

Instead of learning to combine document scores in a query de-pendent manner, we consider the problem of selecting a ranker for a given query. We propose a simple framework that directly predicts the differences in effectiveness between the results of different re-trieval models. In particular, we consider the web-search scenario, where a large number of features are often combined using sophis-ticated learning to rank algorithms (rankers). For the sake of sim-plicity, we assume that we have access to two different rankers that operate on the same set of features. We formally define the ranker selection problem as follows:
Problem Definition. Given two rankers, R a and R b , we choose one ranker to be the baseline ranker (say R b )  X  either arbitrarily, or based on the prior knowledge about the average performance of the
In terms of MAP, RankBoost is the best individual ranker, fol-lowed by FRank and Regression. Table 1 shows the potential for the use of query dependent ranker selection for named page finding. For example, RankBoost outperforms Regression on 90 queries, but performs worse on nearly half as many. Furthermore, we see that an oracle selection method can provide nearly 30% improve-ment over Regression, and nearly 15% improvement over FRank and 12% improvement over RankBoost.
 Table 1: Potential for Improvement using Ranker Selection. R b  X  R a MAP( R a ) Worse Better Oracle Selection Regression 0.5476 90 42 0.7096 FRank 0.6429 62 45 0.7316 We conduct two sets of selection experiments one with Rank-Boost as the baseline ranker, and the other with RankBoost as the alternate ranker. Even though the rankers train on differences in performance, the distribution of the positive and negative differ-ences change for each setting, thereby leading to different behavior in terms of the achieved improvements.
 Table 2: Ranker Selection effectiveness on a set of 225 name page R b R a MAP( R b ) MAP( R a ) MAP( R s ) Regression RankBoost 0.5476 0.6596 0.6623  X  FRank RankBoost 0.6429 0.6596 0.6591  X  RankBoost Regression 0.6596 0.5476 0 . 6722  X  RankBoost FRank 0.6596 0.6429 0 . 6607  X 
Results of the two selection experiments are tabulated in Ta-ble 2. When using RankBoost as the alternate ranker, selection yields improvements over both Regression and FRank. This is in part because RankBoost performs better than both these algo-rithms for most queries. However, selection does not provide sub-stantial improvements over RankBoost, the best individual ranker. On the other hand, when using RankBoost as the baseline ranker and Regression as the alternate ranker, we obtain substantial im-provements using selection. Interestingly, even though FRank has a higher MAP compared to Regression, using FRank as the alternate ranker yields smaller improvements. This suggests that effective-ness of the selection also depends on the type of ranking algorithm used, in addition to the performance of the ranker itself. The distribution of gains achieved for ranker selection between RankBoost and Regression is shown in Figures 2 (a) and (b). In both cases, we see that for a large fraction of the queries, choos-ing the alternate ranker results in gains, and very few cases result in losses. When using RankBoost as the baseline ranker, selec-tion uses Regression for a small number of queries (28), and pro-vides gains for subset (14), but the choice results in fewer losses (4). However, when using RankBoost as the alternate ranker, se-lection uses RankBoost for a large number of queries (198), out of which 83 queries result in gains and 29 result in losses. This sug-gests that while ranker selection yields substantial gains, it can also benefit from limiting losses due to poor selection. For example, thresholding on the predicted differences can reduce the number of queries for which the alternate ranker is queried.

