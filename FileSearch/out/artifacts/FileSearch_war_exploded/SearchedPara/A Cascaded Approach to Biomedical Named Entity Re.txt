
We propose a cascaded approach for extracting biomed-ical named entities from text documents using a unified model. Previous works often ignore the high computational cost incurred by a single-phase approach. We alleviate this problem by dividing the named entity extraction task into a segmentation task and a classification task, reducing the computational cost by an order of magnitude. A unified model, which we term  X  X aximum-entropy margin-based X  (MEMB), is used in both tasks. The MEMB model con-siders the error between a correct and an incorrect output during training and helps improve the performance of ex-tracting sparse entity types that occur in biomedical liter-ature. We report experimental evaluations on the GENIA corpus available from the BioNLP/NLPBA (2004) shared task, which demonstrate the state-of-the-art performance achieved by the proposed approach.
The vast amount of information now available in elec-tronic form, and sometimes only in electronic form, has led to immense interest in automatic information extraction. This is no different in the biomedical domain.

There are many databases that contain many new re-search results, e.g. the PubMed MEDLINE database. The amount of literature in MEDLINE grows by nearly 400,000 citations each year. For mining information from a biomed-ical database, a good pre-processing step is to extract the biomedical named entities of interest (such as proteins and DNAs). This step requires the identification of names in sci-entific text that is not as structured as traditional databases. The sheer volume and growth rate of biomedical literature, however, makes manual extraction of these entities a very difficult and time-consuming task. Methods automating these extraction processes are therefore very valuable and often indispensable.

While the performance of extracting named entities in a general-purpose domain such as newswire data has greatly improved (nearly 95% in accuracy) [14], the performance in the biomedical domain still leaves much room for im-provement (nearly 72 . 55% in the Coling 2004 (JNLPBA) shared task [18]). Indeed, as many applications rely on the accuracy of entity extraction -named entities are often the keywords used in text classification -this lackluster perfor-mance hampers further data mining work.

In general, named entities constitute an important sub-space of the whole feature space in many data mining tasks. In this paper, we formulate the entity extraction problem as a supervised learning task, and propose a cascaded frame-work for extracting biomedical named entities from un-structured text.

Our cascaded framework divides the extraction task into a segmentation task and a classification task. Both of these tasks use what we term a  X  X aximum-entropy margin-based X  (MEMB) model. This model considers the error made between a correct and an incorrect output during train-ing. In the segmentation task, we reduce the training cost by grouping biomedical named entities of interest into one super-class. Note that this also helps the segmentation of named entity types which are sparse in the training data. The same MEMB model is used in the classification task as well.
 We start with an introductory example to illustrate our MEMB model. In a typical named entity extraction prob-lem, we would train our system to extract named entities
Table 1. Number of different biomedical enti-ties in the corpus for the Coling 2004 shared task (JNLPBA). from a sentence, and the training examples consist of cor-rect examples only, i.e. a set of sentences with named en-tities marked. During training, the system implicitly gener-ates a set of negative examples. For example, if we are to extract RNA names from a sentence, compare the following sentences: where the symbols  X  [  X  and  X  ]  X  mark both the open and close boundaries of an entity. The first sentence is a correct example, while the second and the third sentences are not. If no hints were given that the second sentence is  X  X early X  correct, our system would need to make an extra effort to figure this out. It might even think that the second and third sentences are both  X  X rong to the same extent X . To alleviate this problem, we propose a model that associates an additive weight with each training instance ( x , y ) , where x is the input and y is the correct output, with an arbitrary output y .

Another practical difficulty is the uneven distribution of different biomedical named entities in text databases. For example, in the Coling 2004 shared task (JNLPBA), the training corpus was constructed by a controlled search on MEDLINE with the query terms  X  X uman X ,  X  X lood cells X , and  X  X ranscription factors X . From this search, 2,000 ab-stracts were selected and five types of biomedical named entities were marked up: protein, DNA, RNA, cell line, and cell type. The distribution of the different types of biomedi-cal entities is shown in Table 1. The final extraction perfor-mance is often lower for sparse entity types. While the most straightforward and easiest way to improve the extraction performance is to find and add more  X  X eatures X  to each en-tity type, this also comes with a greater computational cost. For example, in general, a direct application of a conditional random field (CRF) requires O ( N a N 2 c ) time for each itera-tion in training, where N c is the number of entity types and N a is the number of atomic features (see Section 3.1 for its definition), which is usually in the range of 0 . 1  X  5 millions.
Figure 1. A schematic representation of our cascaded framework for extracting biomedi-cal named entities (NEs). Both the SEG and CLASS modules use the MEMB model.

The main contributions of this paper are as follows: by using the proposed cascaded framework, we can reduce the training time by an order of magnitude -from O ( N a N 2 to
O ( N entropy margin-based X , is used to boost the extraction per-formance in this framework. We demonstrate this improve-ment in a biomedical named entity extraction task using the GENIA corpus from the JNLPBA shared task. In fact, our approach has reached state-of-the-art performance and out-performs all other systems. Details of experimental results and comparisons are also given.

The remaining sections of this paper are organized as fol-lows: Section 2 presents the formulation of the named entity extraction problem and our proposed approach. Experimen-tal results and analysis are presented in Section 3. Section 4 discusses related work. We conclude with Section 5.
We propose a cascaded framework for extracting biomedical entities in scientific text. The framework con-sists of two modules: a segmentation module (SEG) and a classification module (CLASS). Each module uses the same general model that considers the probability and the margin for the data being considered, which we term the  X  X aximum-entropy margin-based X  (MEMB) model. In the SEG module, we segment the text and identify entity can-didates without identifying their types. The SEG module is performed by grouping all entity types of interest into one super-type . In the CLASS module, each entity can-didate is classified into one of the entity types. One ad-vantage of using this cascaded approach is that segment-ing the text and identifying entity candidates are relatively simpler problems, as different types of biomedical entities share common features in the context of scientific text. An-other advantage is that we can select features that are suit-able for each module. While combining the two modules into a single phase is possible, the large number of features involved may sometimes degrade the performance. This phenomenon has been observed in many tasks. In partic-ular, in the context of biomedical entities, [11] reported that a conditional random field (CRF) using only orthographic features gave better performance than using a complete set of features that included semantic features. The third ad-vantage is that with the reduced number of features due to both the selected features for each module and the reduced number of output labels, the computational cost is substan-tially lower and thus the training time for our approach is significantly shorter than when combining the two sets of features.

In the following, we first describe our notations and present the general MEMB model in our framework. Then we present the details of its application to extracting biomedical entities in scientific text, which includes the training algorithm and the decision algorithm with analysis.
In the SEG module, each sentence in text is represented by a sequence x . The named entities in x can be segmented with the help of a segmentation sequence y . We assume there is only one correct segmentation y for each sentence x . The SEG module is required to give the correct output sequence y upon an input sequence x .

There is a feature vector F s =( F 1 ,F 2 ,...,F m ) , where m is the number of features, that associates each sentence x and each segmentation sequence y , i.e. F s = F s ( x , This is commonly known as joint feature representation or combined feature representation of inputs and outputs in the support vector machine (SVM) literature [17]. We denote the inner product of two vectors a and b as a  X  b .
In segmenting named entities in text, x is a sequence of words that forms a sentence, while y is represented as a sequence of labels. Each label is an element in a label set  X  . We use the IOB2 notation to identify a segment in a sentence, i.e.  X = { I,O,B } , where B represents a word that is the start of a named entity, I represents a word that is inside a named entity (but not the start), and O represents a word that is not part of a named entity. For example, an RNA named entity is in the following sentence:
We combine the words with the labels B and I together and identify  X  MTIIa mRNA  X  as a segment of a named en-tity. Note that in this IOB2 notation, a label O can be fol-lowed only by O or B but not I .

In the CLASS module, each input sequence x is an entity candidate obtained from the SEG module. For example, x =  X  EBV genome  X . The output y is reduced to a single label y that represents the type of the named entity x . The feature vector becomes F c = F c ( x ,y ) accordingly.
Our general model is a maximum entropy model with a margin incorporated. For ease of exposition, we restrict our discussion to the SEG module. This analysis can be sim-ilarly extended to the CLASS module with each sentence x and each segmentation sequence y being replaced by a named entity candidate and an entity type respectively. The general model can be described by where  X  =(  X  1 , X  2 ,..., X  m ) is the training model param-eter, Z ( x i ;  X  )= y  X  X  exp[  X   X  F ( x i , y )+ E ( y i the normalization factor, and Err ( y i , y )  X  0 is an error measure function of y a possible output y with equality when y = y i .Atra-ditional conditional random field (CRF) [5] model can be obtained by setting E ( y i , y )  X  0 . 2.2.1 Training Stage During training, suppose Y is the set of all possible outputs, with a set of training examples T = { ( x i , y i ) } ; our model gives a pseudo probability to each possible output y  X  X  for an input x i .
 where Z train ( x i ;  X  )= y  X  X  exp[  X   X  F ( x i , y )+ Err ( y i , y )] is the normalization factor. 2.2.2 Decision Stage The decision model is an adapted training model that re-flects the unavailability of the true output y i ; in other words, we do not know the true output y i and thus no information between y i and a possible output y is known.
 where  X  is the model parameter that we get from the train-ing stage, Z decision ( x i ;  X  )= y  X  X  exp[  X   X  F ( x i , the normalization factor. For convenience, we write Pr training ( y | x i ;  X  ) as P
 X  ( y | x i ) and Pr decision ( y | x i ;  X  ) as P D  X  ( y similarly for the normalization factors Z train ( x i Z the decision stage, we extract the named entities in the in-put sentence x i by finding the output  X y with the highest probability P D  X  ( y | x i ) with parameter  X  . In case there is a tie, we assume there is an ordering in for choosing  X y , such as the number or the maximum length of named entities. The same operation can be performed in the log space. Equation (5) can be written as Now we are ready to investigate the design and significance of the error measure function Err ( y i , y ) in the training model. Suppose in training, for a training instance ( x i the model gives the highest probability correctly to y i , i.e.  X y = y i , where Err ( y i , y i )=0 by definition. We can conclude  X   X   X  F ( x i , y i )  X   X   X  F ( x i , y )  X  Err ( y i , y ) (12) This is analogous to training the parameter  X  with margin Err ( y i , y ) in margin-based training (although  X  may not necessarily give the maximum margin in our case). When the training model gives the highest probability to the output y for the input x i , it also gives at least a margin defined by Err ( y i , y ) between all possible y  X  X  and y i for the input x i .

This analysis can be applied to the CLASS module in a similar manner.
Training the parameter  X  is intractable in general. Con-sider a sentence x of length p with IOB2 notation where the number of possible labels for a word is 3 . The total num-ber of possible outputs y for sentence x is 3 p , where p usually in the range of 10 to 20 . Note that if one sentence is long, the total training time will be dominated by that sentence because of the exponential increase in the number of possible y for that sentence. Enumerating all possible y is computationally expensive and generally impractical. Thus we follow the assumptions that are commonly made in a sequence labeling model and use a modified version of the forward-backward algorithm for the MEMB model. We assume first-order independence on the output y for the set of features F ( x , y ) and write F ( x , y ) as the sum of features at all positions of sentence x .
 where | x | is the length of sentence x , and f = ( f 1 ,f 2 ,...,f m ) . Each f k is a feature function defined at position i of sentence x . More precisely, it is a binary func-tion of the whole sentence x , the previous label y i  X  1 current (i.e. the i -th) label, and the position i . For example, it can be defined as f ( x ,y i  X  1 ,y i ,i )= We further assume that the error measure function Err ( y , y ) is decomposable. This measures the error be-tween two segmentation sequences y and y by directly counting the number of mismatched labels.
 where I ( x )=1 if x is true, and 0 otherwise.

We formulate the parameter estimation problem as an optimization problem. Assuming each sentence x i and its segmentation sequence y i is independently drawn from an underlying unknown distribution, we estimate the parame-ter  X  by maximizing the conditional likelihood over a set of training data T = { ( x i , y i ) } .
 where  X  is the parameter for the marginal distribution P ( x i ) . (See [15] for a detailed explanation and the ad-vantages of using a different parameter  X  to parameterize the marginal distribution of x i ). The probability is typically represented as a log-likelihood L (  X  ) for optimization [15]. The convexity of the form is helpful in parameter estima-tion.
 The problem is now transformed to searching for the pa-rameter  X  that can give the maximum log-likelihood over the set of training data. argmax Note that P  X  ( x i ) is dropped in the maximization because it does not involve  X  . The above formulation is similar to that in a linear-chain CRF except that the normalization factor Z  X  ( x i ) contains an additional term Err ( y i , y ) in it, i.e. if Err ( y i , y )  X  0 , it is reduced to a linear-chain CRF. We need to modify the CRF training procedure to account for this additional term. To perform maximization, we need to get the gradient of L (  X  ) . = = where Y is a random variable that takes a value in the output space Y , and E P T under the distribution P T  X  ( Y | x i ) . The major difficulty in our optimization process is the computation of the gradi-ent  X  X  (  X  ) that leads to the problem of finding the expec-tation E P T pectation, we make use of the first-order independence as-sumption in Equation (13) and the decomposability of the error function Err ( y , y ) in Equation (15). A variant of the forward-backward algorithm can be employed.

We first define two sets of variables that will be useful in computing the expectation: forward variables  X  p ( q ) to represent the joint probability that the word at position is labeled as q , and the words from position 1 to p appear; backward variables  X  p ( q ) to represent the conditional prob-ability that given the word at position p is labeled as q words from position ( p +1) to the end appear. Let y p be the correct label at position p . Each variable can be recursively calculated by the following equations: the two sets of variables, we can efficiently com-pute the normalization factor Z T  X  ( x ) and the expectation E Note that with the help of the forward and backward vari-ables, the number of operations is changed from being ex-ponential in the length of a sentence x (i.e. |  X  | | x | linear in | x | . The objective value L (  X  ) and the gradient  X  X  (  X  ) can be calculated efficiently with the normaliza-tion factor Z T  X  ( x ) and the expectation E P T provided. With these two pieces of information, we can use the L-BFGS algorithm [6], which is a limited-memory quasi-Newton method for conducting large-scale nonlinear optimization, to estimate the parameter  X  . The L-BFGS algorithm has a fast convergence rate and has been shown to achieve good performance in estimating parameters for maximum entropy models [7].

In practice, we need to add regularization terms to the log-likelihood to avoid overfitting. Assume a Gaussian prior with mean 0 and covariance  X  2 I ,
L (  X  )=  X  X  (  X  )= The role of regularization terms is particularly important in our model. If we consider Equation (11), it is easier to sat-isfy these constraints with large values of  X  i . That means a parameter  X  with a larger norm is preferred without regu-larization, which usually gives an overfitting model. In our experiments, a smaller  X  (i.e. a heavier penalty for a larger norm) tends to give better performance.
We apply the parameter  X  that is determined in the train-ing stage to the decision model. We get the best output se-quence y for an input sequence x by maximizing the con-task to enumerate all possible y . Wemakeuseofthefirst-order independence assumption and get the output in a dy-namic programming fashion. The decision procedure is a Viterbi decoding algorithm. Note that because the expo-nential function is a monotonic increasing function, there is no need to take the exponential function when searching for the maximum probability. Instead, comparing the indexes is enough (i.e. e x &gt;e y iff x&gt;y ). This can save decoding Algorithm 1 Grid-search for  X  and  X  . 1: INPUT: search range for  X  : [  X  a , X  b ] and  X  : [  X  2: while true do 6: MaxP =  X  X  X  7: for p =1 to 2 do 8: for q =1 to 2 do 9: Train the model on the training set with param-10: Test the model on the validation set and get per-11: if P&gt;MaxP then 12: MaxP = P 14: end if 15: end for 16: end for 17: terminate if no improvement 18:  X  a = min(  X  a , X  ) , X  b = max(  X  b , X  ) 19:  X  a = min(  X  a , X  ) , X  b = max(  X  b , X  ) 20: end while time and allow post-processing to be incorporated into the decision stage if needed.
In order to allow the flexibility to reflect the importance of the error measure function, we use a weighted error mea-sure function as follows: where  X  is the weight for the error measure function. From our empirical results, a weighted error measure function usually gives better results compared to a non-weighted one (i.e.  X  =1).

In order to give appropriate values for the regulariza-tion parameter  X  and the error weight  X  , we propose a grid search algorithm to find the appropriate parameter values. The idea is to split the training data into a training set and a validation set and divide the two-dimensional search space for  X  and  X  into large grids. In each grid, we use the center of the grid to train the model on the training set and test it on the validation set. We choose the grid that gives the best per-formance and continue to divide it into smaller grids. This process is repeated until there is no improvement on the per-formance or the performance is satisfied. The algorithm is presented in Algorithm 1
To evaluate the effectiveness of our proposed framework, we performed experiments in the biomedical domain which is considered to be more difficult than a general-purpose domain as mentioned in Section 1. We used the GENIA corpus provided in the JNLPBA shared task 1 to perform our experiments.

The GENIA corpus consists of 2,000 MEDLINE ab-stracts of the GENIA version 3 corpus with named entities in IOB2 format. The line indicating the MEDLINE abstract ID boundary information is not used in our experiments. There are 18,546 sentences and 492,551 words in the train-ing set and 3,856 sentences and 101,039 words in the eval-uation set. Each word is tagged with  X  X -X X ,  X  X -X X , or  X  X  X  to indicate the word is at the  X  X eginning X  (B) or  X  X nside X  (I) of a named entity of type X, or  X  X utside X  (O) of a named entity. The named entity types are: DNA, RNA, cell line, cell type, and protein. The distribution of different types of biomedical entities in both the training and evaluation data are listed in Table 1. Our task is to extract all the named en-tities in the evaluation set. For example, given the sentence: we need to extract  X  peripheral lymphocytes  X  X s  X  X ell type X  and  X  GR mRNA  X  X s X  X NA X .
To help our discussion, we define  X  X tomic features X  as those features that are based on the input sentence x only. By the first-order independence assumption, the final set of features is a combination of those atomic features with at most two class labels y i  X  1 ,y i  X   X  where y i  X  1 is the previ-ous label for the i -th word, and y i is the label for the current word. 3.1.1 SEG Module Features The atomic features used in the SEG module include word features, orthographic features, part-of-speech (POS), and two lexicons. The word features include unigram, bigram, and trigram (e.g. the previous word, the next word, and the previous two words), while the orthographic features [11] include capital letter, dash, punctuation, and word length. Word class ( WC ) features similar to [1] are also added, which replace a capital letter with  X  X  X , a lower case let-ter with  X  X  X , a digit with  X 0 X , and all other characters with  X   X . Similar brief word class ( BWC ) features are added by collapsing all consecutive identical characters in word class features into one character. For example, for the word NF-kappa , WC = AA aaaaa , and BWC = A a . These
Table 2. Word features used in the experi-ment: w 0 is the current word, w  X  1 is the pre-vious word, etc.

Table 3. Comparisons (in % ) with other sys-tems on the overall performance. All other systems except (Okanohara et al., 2006) em-ploy different deep knowledge resources. are listed in Tables 2 and 4. The POS features are added by the GENIA tagger 2 . All these features except the pre-fix/suffix features are applied to the neighborhood window [ i  X  1 ,i +1] for every word. Two lexicons for cell lines and genes are drawn from two online public databases: Cell Line Database 3 and BBID 4 . The prefix/suffix and the lexi-con features are applied to position i only. All the above fea-tures are combined with the previous label y i  X  1  X   X  , and the previous and current labels ( y i  X  1 ,y i )  X   X   X   X  to form the final set of features F s ( x , y ) . Because only three labels (i.e. B, I, O ) are needed, the total number of features is
O (3 2 N features.

We remove features that occur fewer than five times in the SEG module to reduce the computational cost. The total number of features is about 2 . 3 million. 3.1.2 CLASS Module Features In the CLASS module, the model only needs to determine the correct entity type for a given named entity candidate. The output sequence consists of one element only, i.e. y  X  Brief Word Class BWC
Table 4. Features used in the JNLPBA exper-iment. The features for Capital Letter, Digit,
Dash, and Punctuation are represented as reg-ular expressions. { protein, cell type, cell line, DNA, RNA } .
 The atomic features mentioned in Section 3.1.1 are com-bined with each possible label y to form the final set of features F c ( x ,y ) . These features are applied to each named entity candidate x from the SEG module. The total number of features is O ( N c N a ) where N c is the number of entity types. The total number of possible features in the CLASS module is less than that in the SEG module. We can afford to remove fewer features. Features that occur fewer than three times are removed, with a resulting total of about one million features. The experimental results are evaluated by the measure defined as where P is the precision and R is the recall. This exact-match scoring method doubly penalizes incorrect bound-aries for an output as false negatives and false positives. To allow comparisons with the results in the JNLPBA shared task, we use the same evaluation script from the shared task, which reports on the precision, recall, and the F 1 -measure on the evaluation data.
Table 5. Overall results (in % ) of our cas-caded approach. The first two rows show the performance of the individual module.

The SEG module (Segment.) achieves F 1 = 77 . 56% . The CLASS module (Class.) per-formance ( 92 . 77% ) is based on the fully cor-rect segmented testing data. The following rows show the actual extraction performance (segmentation followed by classification) for each type of entity, achieving an overall F 1 = 72 . 94% .

In the SEG module, our approach achieves an F 1 of 77 . 56% . The recall is of particular importance in the SEG module because it directly limits the number of correct an-swers that our approach can give. When we compare the SEG module recall ( 80 . 45% ) with the results reported in the JNLPBA shared task in Table 3, it is clear that subse-quent good classification results will yield a good overall F . We also compare the segmentation results with a CRF that uses the same set of features in Table 6. Generally, the MEMB model shows a greater relative loss reduction on F 1 for sparse entity types such as RNA. Because the fraction of different entity types differs in the training data and the evaluation data, some discrepancies are also found in DNA and cell type.

To test the performance of the CLASS module, we pre-pared a set of testing data with all the entities correctly seg-mented and used it to evaluate the module. The overall ac-curacy is 92 . 77% , which is higher than the reported clas-sification results ( 90 . 54% ) by a simple maximum entropy model in [4]. The detailed results are depicted in Table 8. With the performance in the two modules now available, we can get a rough estimate of the final extraction performance as shown in Table 5. Note that if the CLASS module accu-racy is 100% , the final extraction performance is identical to the segmentation performance, i.e. F 1 =77 . 56% .
The results of our experiments are summarized in Ta-bles 5, 9, and 10. The results show that our proposed ap-proach outperforms all the systems in the JNLPBA shared
Table 6. Comparisons (in % ) with CRF model on the segmentation performance. Generally, the MEMB model shows a higher relative loss reduction on F 1 for sparse named entities.

Small discrepancies are found when there is a large difference between the fraction of the named entities in the training data and that in the evaluation data , for instance, DNA and cell type. task. Their approaches include the Support Vector Machine (SVM), the Hidden Markov Model (HMM), the Maximum Entropy Markov Model (MEMM) and the Conditional Ran-dom Field (CRF), which use deep knowledge resources with extra costs in pre-processing and post-processing. For example, the best system, (Zhou and Su, 2004), used name alias resolution, cascaded entity name resolution, abbrevia-tion resolution and an open dictionary (around 700,000 en-tries). (Settles, 2004) used 17 lexicons that include Greek letters, amino acids, and so forth. (Finkel et al., 2004) used gazetteers and web-querying. Although our system outper-forms the best system, (Zhou and Su, 2004), we believe it can be further improved, since our cascaded approach does not prohibit the use of these deep knowledge resources.
We also compare our results with a recent work in [9] that uses a semi-CRF based on a feature forest without us-ing deep knowledge resources such as gazetteers or post-processing. Their system performance is only slightly lower than that of (Zhou and Su, 2004) and our system, and also outperforms the other systems. Another recent work by [4] uses a similar cascaded approach as ours by using two dif-ferent models in the two phases. They use additional infor-mation from the GENIA 3.02p version corpus to train their CRF. A simple maximum entropy model is used in classifi-cation. Post-processing is needed to correct the final results. Their reported classification performance is 90 . 54% , which is lower than our approach. Since they did not report the segmentation performance, we can only compare the final results in Table 3.

One interesting point to note is that our actual extraction performance is higher than the estimated performance as shown in Table 5. We believe the reason for this is that a large number of same features are used in both the SEG and CLASS modules. Therefore the entity candidates from
Table8.Results(in % ) of the CLASS module on fully correct segmentation data the SEG module are those that are  X  X ensitive X  to the given features, and the same is also likely to happen in the CLASS module, giving a better final performance than estimated.
In the JNLPBA shared task, eight named entity recogni-tion systems were used in extracting five types of biomedi-cal named entities [2, 3]. Their approaches include the Sup-port Vector Machine (SVM), the Hidden Markov Model (HMM), the Maximum Entropy Markov Model (MEMM) and the Conditional Random Field (CRF). Great empha-sis is placed on feature representations in these eight sys-tems. Besides a simple set of features, including word and orthographic, many of them use a very rich feature set that includes semantic and morphological features, part-of-speech, dictionaries, and so forth. The best system [18] uses  X  X eep knowledge X  such as name alias resolution, cas-caded entity name resolution, abbreviation resolution, and in-domain POS to extract biomedical named entities. Our work is different in that we keep the computational cost at a reasonable level and improve the extraction performance by dividing the named entity extraction problem into two tasks using the same general MEMB model.

The problem of extracting named entities from text can be formulated as a sequence labeling problem as shown in our approach using IOB2 notation. Since the CRF model [5] was introduced to solve sequence labeling prob-lems, it has shown very good performance in many tasks such as noun-phrase chunking and named entity recogni-tion [13, 8, 16, 12]. Our approach uses a MEMB model
Table 9. Correct LEFT boundary with correct entity type information (in % ).

Table 10. Correct RIGHT boundary with cor-rect entity type information (in % ). that can be viewed as a generalization of the CRF model. CRFs can be extended to semi-CRFs [10], where an ele-ment in a sequence (e.g. a word in a sentence) is gener-alized to a segment in a sequence (e.g. a named entity in a sentence). Semi-CRF has also been used in extracting biomedical named entities [9]. However, the computational cost for a semi-CRF is very heavy. In order to use it in practice, usually a reasonable upper bound for the length of named entities needs to be specified to reduce the computa-tional cost.

In employing a CRF in a named entity extraction prob-lem, one approach is to consider each entity type sepa-rately [11], such as using different labels for each entity type: B-protein, B-DNA and so forth. This increases the number of features due to the increased number of la-bels. The training cost will also be substantially higher.
A similar two-phase approach is used in [4]. Their CRF is trained on a different dataset that contains all other named entities such as lipid , multi cell , and other organic com-pound . They also use a rule-based post-processing to cor-rect the final results. Our approach uses the same MEMB model in the two phases without the extra computational cost needed in their system. A comparison of the final re-sults is given in Table 3.
We propose a cascaded framework for extracting biomedical named entities from unstructured text using a unified maximum-entropy margin-based (MEMB) model. The cascaded framework reduces the computational cost by an order of magnitude by dividing the extraction task into a segmentation task and a classification task. The MEMB model used in the two tasks improves extraction perfor-mance by considering the error made between a correct and an incorrect output during training. It also boosts the over-all accuracy, especially for sparse entity types. We demon-strate the effectiveness of our approach on the GENIA cor-pus from the BioNLP/NLPBA (2004) shared task, and make comparisons with the participating systems and some recent works. Our approach has reached state-of-the-art perfor-mance levels and achieved the best F 1 -measure.

