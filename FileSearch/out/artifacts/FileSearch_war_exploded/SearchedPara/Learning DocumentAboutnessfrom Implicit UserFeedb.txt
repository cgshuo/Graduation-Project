 Capturing the  X  X boutness X  of documents has been a key re-search focus throughout the history of automated textual information processing. In this work, we represent about-ness using words and phrases that best reflect the central topics of a document. We present a machine learning ap-proach that learns to score and rank words and phrases in a document according to their relevance to the document. We use implicit user feedback available in search engine click logs to characterize the user-perceived notion of term rel-evance. Using a small set of manually generated training data, we show that the surrogate training data from click logs correlates well with this data, thus eliminating the need to create data for training manually which is both expensive and fundamentally difficult to obtain for such a task. Fur-ther, we use a diverse set of features in our learning model that capitalize heavily on the structural and visual proper-ties of web documents. In our extensive experimentation, we pay particular attention to tail web pages and show that our approach trained on mainly head web pages generalizes and performs well on all kinds of documents. In several eval-uation methods using manually generated summaries and term relevance judgments, our system shows 25% improve-ment over other aboutness solutions.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Abstracting methods ; H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Relevance feedback Algorithms, Experimentation Term relevance, Click data, Ranking, Document structure
Document aboutness aims at creating a succinct represen-tation of a document X  X  subject matter [7, 18]. The document subject matter can be captured using keywords, named en-tities, concepts, terms (words and phrases), snippets or sen-tences. A representation using entities, concepts or terms can be used in several applications such as document cat-egorization, routing and topic detection. Contextual ad matching systems [24] that match documents to ads make use of systems that extract salient keywords from docu-ments. In this case, keywords are used to represent about-ness. Exploratory search applications offer related sugges-tions to user search queries [4, 5]. These suggestions are generated using the salient concepts in web pages. Auto-matic text summarization systems represent the document subject matter using salient snippets and sentences from the document. Such systems are used for several tasks includ-ing web search result summarization. Web search ranking functions can also hugely benefit from a system that reli-ably detects salient phrases or keywords from documents. Since a document can relate to a variety of topics to varying degrees and many of these topics may not necessarily have conventional names, the task of capturing aboutness using any given representation has proved to be difficult even for editors trained in information science.

In recent years, there has been a paradigm shift from the standard page view of the web to one that provides access to information of finer granularity such as entities, concepts and the relationships between them. A major building block for such a view of the web is a system that generates a succinct document representation that captures document aboutness. One way to succinctly represent the document aboutness for this purpose is to have a ranked list of concepts and entities together with their scores for a given document. A number of statistical and linguistic methods have been devised for selecting appropriate terms (e.g., unigrams, bi-grams, noun phrases, named entities). The scores or weights of the terms provide a measure of how well each term reflects what the document is about. The popular tf-idf formula uses a combination of each term X  X  frequency within a document along with the inverse of its frequency across the entire cor-pus of documents [26]. An alternative approach is to exploit positional information, such as a term X  X  occurrence in a title or early on within a document or paragraph [22, 23]. When a document collection is relatively homogeneous, as in a cor-pus of domain-specific scientific articles, knowledge of cue terms and writing style can often be employed to pinpoint probable key content terms and sentences [14].
In this paper, we present a machine learning approach for the construction of weighted and ranked term lists for web pages. Unlike other well-defined applications such as text categorization [27] or focused named entity recognition [33], developing training data and devising an evaluation methodology for our problem present daunting challenges of their own. As an editorial task, choosing and ranking terms that best reflect the contents of a document is a painstak-ing, time-consuming job, even when the subject matter is familiar to the editor (and it often is not). To overcome this challenge, we make use of the implicit user feedback when a user clicks on a search result document in response to the query issued by her. Aggregated user feedback over large collection of click logs captures in essence the relevance no-tion perceived by the user. A click serves as evidence that the document is about the query while an absence of a click (or skip) provides negative evidence. Thus a carefully con-structed aggregated click metric can act as a surrogate to a human generated term relevance judgment. We not only make the process faster by using click data for training but we also learn a much more valuable term relevance function  X  one which the user uses to associate relevant terms to doc-uments; as opposed to one that is manually fabricated using guidelines and rules for judgments. We have investigated into other sources such as anchor text and user generated tags for web pages with a similar motivation. In our work, we focus mainly on the term scoring/ranking component of the problem and use a dictionary to handle the term selec-tion problem.

The author of a webpage uses HTML to structure the webpage and present the page in such a way that important segments of the document are highlighted by use of addi-tional markup, change in bolding, font size or font color. Further, certain sections of the web page are more impor-tant than certain others i.e. page title is more important than a paragraph appearing in the latter half of the doc-ument. This relative importance of HTML segments of a web page also depends on the type of the web page  X  if it is a shopping page or an information rich wikipedia page. Thus there are several structural and visual clues available in a web page that can be exploited to learn which terms are better than the others. In our approach, we use the HTML markup and other visual information as features for our learning problem. In more traditional methods such as tf-idf , term importance increases proportionally to the num-ber of times the term appears in the document but is offset by the frequency of the term in the corpus. However, a term in a given document could be less frequent but very important if it appears in a visually attractive area of the webpage. Similarly, a term can be very frequent in a doc-ument but not important if it appears mostly in the boiler plate of the document. Further, what parts of the docu-ment become visually attractive depends on what kind of document it is. Thus, in general a web page is designed in a manner such that its structural layout and visual proper-ties correlate with the importance of terms appearing in the content. In our approach, we exploit this fact.

The paper is organized as follows. We first describe some related work and then precisely define the problem we solve. We then lay out our approach to tackling the term ranking problem, including the construction of our training sets, se-lection of features, and the use of gradient boosted trees to learn a term weighting formula. We then present several ex-periments to evaluate our results, comparing our term rank-ing solution with several competition aboutness solutions.
One thread of research on aboutness [6, 7, 18] has ap-proached the problem from the theoretical standpoint, ex-ploring the nature and meaning of concepts and proposing theoretical frameworks [8, 17]. A second thread has sought to develop pragmatic tools for representing the aboutness of documents. The tf-idf weighting scheme developed for the vector space retrieval has proved remarkably effective for a variety of applications. Lin and Hovy [22] champion the  X  X osition method X  for identifying topical phrases by the lo-cations of key sentences in texts of known genres and show how an  X  X ptimal position policy X  can be learned. They uti-lize word overlap between the sentences selected by their system and document abstracts to evaluate the effectiveness of their approach.

Research efforts in the semantic web domain have several components that have a similar flavor to document about-ness systems. Knowledge representation in the semantic web is largely based on ontologies. Ontology construction, ontol-ogy ranking [2, 21], ontology understanding [30] and reuse [3] remains challenging, mainly due to the skill, time, ef-fort, and domain specific knowledge required. Ranking of concepts and relations is often done to achieve all or some of the objectives in ontology research in the semantic web domain. Another similar thread of research aims at creat-ing a web of finer granularity objects such as concepts and entities, thus moving away from the page view of the web. In [12], the authors propose an entity ranking solution as a building block for doing a holistic entity search. Named entity extraction and ranking [25] is a popular research task and supports several applications. Zhang et al [33] exploit machine learning to identify important named entities in news articles, a task similar to our own, albeit much more narrowly scoped. Extraction and ranking of keywords from documents is also useful for contextual advertising to match ads to documents. The works presented in [29, 31] extract keywords from documents so that relevant ads can be dis-played together with the documents. Extracting relevant information from documents in the form of keywords or sen-tences is also done for document summarization tasks. Zha [32] proposes the principle of mutual reinforcement between salient sentences and salient terms to simultaneously extract key phrases and sentences.

The use of click data has been studied in the context of improving machine learned ranking for documents returned for web search queries [1, 20]. Some approaches for web page summarization [10, 28], directly use click data and other in-formation from click logs as features. It is important to note that click data is sparse and is not available for the large tail of web documents which do not appear enough number of times as search engine results. Thus using click data directly as a feature offers performance limitations on tail documents. In [19] the authors use click data obtained from their contextual shortcuts application for doing key-word ranking. The ranking of the keywords in this work is mainly based on the overall popularity/importance of en-tities since most of their features are built offline from the search engine query logs and none depend directly on the document. Further, our analysis of their system shows us that they focus mostly on entities and lesser on concepts. Due to the poor recall of their system, we do not compare our system to theirs. However, we do compare the perfor-mance of our system with that presented in [29] where the authors use a supervised learning method to learn to extract keywords from documents. They use click log features which again makes the comparison difficult in our experiments on tail web pages for which click log data is not present. Our approach differs from most of these approaches in the way we use click logs to create training data but do not use it as a feature. This enables us to use our approach on web pages  X  popular and unpopular alike. Much research has been done on analyzing the structure of web pages for various appli-cations [9]. In [11], the authors propose a method to do web page segmentation to determine visually and semanti-cally cohesive pieces. We exploit page structure to determine the visual and structural features in our work, however, we do so using a much simpler approach compared to these techniques. Even though our simple page structure analysis helps in improving our performance significantly over the existing solutions, we do plan to experiment with a more sophisticated page structure analysis solution in the future.
We represent the aboutness of a document using a ranked and scored list of terms present in the document. The scor-ing and thus the ranking is in order of relevance of the terms to the central topics of the document. We simplify the aboutness problem to a term relevance ranking problem which is defined as follows: Let T = { t 1 ,t 2 ,t 3 ... } be the set of terms present in a document or webpage d . The term ranking problem is to determine the rank order of the terms in T as per their relevance to d . In other words, we wish to determine the set R where where x y represents x is more relevant than y and x should be ranked higher than y . R represents the aboutness of the document d .

We simplify the term selection component of the problem by using a large dictionary of terms used to select terms in the document to be used for ranking. Our dictionary consists of about 27 million phrasal concepts and entities de-rived from web corpora and query logs and intended to cover the most  X  X nteresting X  phrases on the web. The dictionary also contains inflectional and lexical variants of the terms. In addition, it is also possible to store corpus level features that are computed offline during dictionary construction. Any technique or a combination of techniques that uses n-grams, a named entity recognizer, a noun phrase chunker, etc can be used for term selection. The choice of approach is governed by the computational feasibility of the approach, availability of resources such as named entity lists, etc and the accuracy of these approaches. Although the dictionary based approach offers limitations in terms of coverage of new entities or concepts, there is significant advantage in ef-ficiency and availability of corpus level features in using this approach.
We use the following terminology in this paper. Words and phrases are collectively referred to as terms . Sections of web pages that are ancillary to the apparent topical con-tent of the page, such as sections devoted to advertising or links/menus into other sections of a web site, are referred to as boilerplate . Query-click logs refer to data logged by a search engine which captures user activity such as queries and subsequent document clicks. Such data may also be referred to as click data or clickthrough data . Term rele-vance is used inter-changeably with term importance for a given document to indicate how well that term reflects what the document is about. We refer to a webpage by several synonyms. Any reference to a document or article can be assumed to be a page from the world wide web.
Our approach for term relevance scoring and ranking as a document aboutness solution is guided by the following principles. 1. The relevance of a term in a document can be guaged 2. Clearly defining term relevance and manually construct-3. We can learn to rank and score terms in a document
A web page is clicked on by a search engine user when (1) that page is returned as a result to his query by the search engine and (2) the user X  X  mental model of relevance deter-mines that the web page is relevant to his query and satisfies his information need. Both these things are evidence that leads us to believe that the terms present in the user X  X  query are important for the clicked document. However, there are two issues: (1) search engine results are not perfect and (2) users do not always click on results that are relevant. To circumvent this problem, we look at aggregated queries leading to clicks on that webpage by different users a sig-nificant number of times. Such an aggregated form of data contains a much more reliable signal about terms present in the queries perceived by users as important for a web page. Further, there is other user generated information such as anchor text on links pointing to a webpage, user-generated tags for the webpage from tagging/bookmarking websites such as delicious, etc that contain a similar implicit feed-back.

Suppose we have a set of sources H each containing im-plicit feedback that could help us learn term relevance. With every information source h  X  H , we associate a surrogate rel-evance function f h ( t,d ) that approximates the relevance of a term to a given document. This surrogate target function is defined using the information available in the source h . We associated a parameter  X  h with each of the sources h which indicates the importance of the source in its contribution to the overall relevance of term t . We train separate models m h learnt for each of the surrogate targets and combine the different model outputs to obtain the overall relevance as follows: where  X  h is the importance parameter for the source h . Lin-early combining the model outputs may not always be ideal but it imparts simplicity to our approach.
 Our surrogate target for learning: In our preliminary experiments, we observed that click logs tend to out weigh the other sources by significant proportions both in terms of the amount of data that they carry and also the reliabil-ity of the signal. User tags especially are very sparse and often unreliable due to that reason. The individual model trained on the user tags surrogate in particular performed very poorly for the ranking task. In our future work, we plan to investigate more into the other sources but for this paper we decided to use only click data for generating the surro-gate target for our learning problem. We first obtain all the terms in each of the documents in the training sample. Our term selection method uses the longest sequence matching to determine the terms as defined in our phrasal dictionary. It should be noted that queries may contain terms that are not present in the web page text. We discard those terms since they are really not part of the document.

Some details on the terminology used in this section: When-ever a document is displayed as a search result for a query, it counts as a view of the document. Whenever a document in the search result is clicked by a user, it counts as a click . When a user skips the document at n th position to click on the ( n + 1) th position, it counts as a skip for the docu-ment. For a given document d , we obtain queries that give rise to at least 32 views for d . For each of those queries, we have the click , view and skip information. We identify the terms in the queries using our term selection method and collect all terms in the click logs for a given document. A simple surrogate function for term relevance can be the click-through-rate (CTR) for queries propogated to the terms for a document. The surrogate target in this case is computed as CTR ( t,d ) = clicks/views . In order to measure the sound-ness of our surrogate target, we obtain manually generated judgments that ask trained editors to rate the relevance of a term in a document on a 4-point scale given by Excellent , Good , Fair and Bad . We obtain such judgments for about 200 documents with 50 terms from each document thus giv-ing rise to 10 , 000 judgments. We correlate this data with the values obtained from the surrogate target. The CTR sur-rogate does not correlate well with the editorial data. One reason for this is the well known position bias in the click behavior of users [13]. To overcome this problem, we devise a surrogate target which we call as click attractivity (CA) defined as CA ( t,d ) = clicks/ ( clicks + skips ). This surro-gate correlates well with the manually generated judgments as shown the figure 2.
 Figure 2: The click attractivity surrogate correlates well
Figure 1 depicts the traditional setup for generating train-ing data for term relevance ranking using manual judgments as generated by trained judges. A reasonably sized doc-ument collection for training of about 10 , 000 documents, each having roughly 50 terms, will need about 0 . 5 million judgments per judge. The judgment time required for 3 judges would be somewhere close to 150 days of manual ef-fort. Further, these judges need to be trained for the task using carefully constructed judgment guidelines. Our expe-rience with manual judgments suggests that for tasks such as this, the inter-judge disagreement can be quite high and training data thus generated need not be as pure as ex-pected. Our approach follows the process depicted in fig-ure 3 for generating training data. It differs significantly from the traditional method and allows us to generate huge amounts of data inexpensively and efficiently.
When designing the feature set of our term relevance prob-lem, we ask the question: Given a document d and a term t in d , what characteristics of t and d will help us decide if t is relevant to d ? To answer this question, we identify the following characteristics of t and d , each of which maps to a certain set of features.
Table 1 enumerates all the features that we use in each of the above listed characteristics. For lack of space, we do not describe all the enlisted features since most of them are self-explanatory. Note that the features with frequency (marked as frequency*) are not absolute frequencies but normalized frequencies. Eg., frequency* of a term in the title indicates a normalized frequency i.e. the ratio of the frequency to the total number of terms in the title.

We describe the intuition behind some of our non-trivial features below.

Visual Title of the document: We define the visual title of a webpage to be the most prominent heading of a webpage. For many webpages the visual title does not co-incide with the  X  X itle X  as found in the HTML of the page. The visual title usually is the text marked with the lowest numbered, earliest occurring heading tag or the highest font sized text in the document. Our corresponding feature cap-tures the relative number of occurrences of the term in the visual title of the document. Eg., The visual title of the wikipedia page on J. K. Rowling has title as J. K. Rowl-ing -Wikipedia, the free encyclopedia while the visual title appears in the &lt; h1 &gt; tag as J. K. Rowling . This helps us dampen down the term free encyclopedia . Our observation is that visual title is more often than not more specific than the HTML title and hence more reliable.

Relative offset of a subphrase of the term: We try to do some level of anaphora resolution or co-reference reso-lution by looking at the relative offset of shorter subphrases with respect to the longer phrase. Eg., In a wikipedia page on J. K. Rowling , after a first couple of sentences, the page starts referring to J.K. Rowling as Rowling or Harry Potter and the deathly hallows by just Deathly hallows . We want to handle these cases in a way that the occurrence of Rowling is treated equivalently to J.K. Rowling but we wish to do so only if Rowling occurs after J.K. Rowling in the document. The relative offset feature works together with the offset fea-ture of the term. Thus the features for Rowling will indicate that it appears at an offset x and a longer phrase subsuming it is present at an offset y and in this case y is less than x .
We set up our term relevance problem in a regression framework considering every training instance as a term-document pair for which we have an absolute judgment avail-able from the automatically generated training data. We use Gradient Boosted Trees [15] as our method for regression. We use regression for (1) ranking the terms as per their rel-evance and (2) determine weights for the terms as per their relevance.
Each term-document &lt; t,d &gt; pair is represented by a feature vector in a feature space as explained in Section 4.2 . For a term-document pair &lt; t,d &gt; , a feature vec-tor X = [ X T ,X D ,X T D ,X T T ] is generated and the features are derived using the different feature categories mentioned in section 4.2. Feature Class Example Features
Structural Characteristics of term t in a document d
Visual characteristics of term t in document d
General characteristics of the docu-ment d
We use Gradient Boosted Trees (GBT) based on the prin-ciple of functional gradient descent for our regression task. Suppose our training data is represented as { ( x i and we seek to find a function h such that g i  X  h ( x i = 1 ,...,N . If least squares is used as the loss function, the discrepancy between g i and h ( x i ) is given by ( g i  X  h ( x The gradient descent is applied in functional space to mini-mize this discrepancy. The crucial idea of functional gradi-ent descent is to find a function that interpolates the discrep-ancy and obtains an approximation of the negative gradient of it to form the next iterate of h i . In GBT, the approx-imation is done by fitting a regression tree to the sample values.

Please refer to [15, 16] for a complete description of Gra-dient descent and Gradient boosting. We use the following algorithm to do GBT regression.

At test time, given a document d our objective is to rank all the terms present in d . Suppose we have a trained GBT model f that is trained using the automatically generated training data. We represent every instance of term-document pair &lt; t,d &gt; of the test document d in the feature space which was used for training the GBT. Applying the GBT model f to such an instance gives us a score f ( t,d ). We obtain such as score for every term in a document. These scores can be directly used for ranking the terms in a document or they can be converted into weights to form the weighted term-vector.
We have designed our experiments to achieve the follow-ing goals: (1) Compare the performance of our system with the various competing approaches with respect to quality of term ranking and quality of the top 5 terms generated by different aboutness approaches. (2) Compare the perfor-mance of various approaches on tail web pages. (3) Analyze the gain obtained from using click data for training with re-spect to that obtained due to the use of a richer feature set. (4) Analyze the query prediction ability of our system on tail web pages which do not have any click data.
For our training data, we work with a collection of over 1.3 million popular web pages. These web pages are chosen as follows: we collect over 2 million most popular but non-adult and safe queries for a search engine. Web pages that were returned as top 5 results of these queries and have at least 32 views are chosen. For our training data, we want to get a document collection that is uniformly representa-tive of different document classes with respect to document length, URL depth and categories. To achieve this, we use a stratified sample of over 100,000 documents from the differ-ent document categories, URL depth buckets and document length buckets to obtain web pages for our training data.
For each web page in this collection, we obtain click data accumulated over a period of 6 months. This data is then tokenized and terms are extracted. Our term extraction method uses the longest sequence matching to determine terms as defined in our phrasal dictionary. We also have a collection of inflectional variants and acronyms that we use during the term extraction process. We also get the offset and subphrase information during term extraction. The web pages are parsed using the HTMLTree Parser 1 that parses an HTML file into the DOM (Document Object Model). Once built, the nodes of the tree (elements and text from the HTML file) can be traversed by a user-defined visitor function. We construct the structural and visual features from the parsed HTML web page. The term features de-pendent on corpus statistics such as document frequency, topicality, etc are obtained from the phrasal dictionary.
We use two test data collections for our experiments and evaluations namely random-all test collection: This is a random sample of web pages from all of the web. This contains popular and unpopular web pages, and random-tail test collection: This is a random sample of web pages from the  X  X ail X  of the web. These documents essentially get selected quite rarely as search results for user queries.
We compare our system (denoted as MLA for machine learnt aboutness) with a number of competing approaches listed below.
In our evaluations, we want to compare the different ap-proaches for term ranking using the following evaluation metrics. 1. Rank order precision: This determines the good-2. Term overlap: Here we ask the question: How good 3. Recall: Here we ask the question: Given the ground Evaluation Goal: The evaluation goal of this method is to compare the different aboutness solutions against the ideal form of ground truth i.e. one that is created by trained ed-itors/annotators.
 Ground truth data: The ground truth data for this eval-uation method is created by editors trained in the task of determining relevance of a term to a document. We took a set of 200 documents from the random-all test collection. For each of these documents the editors were shown about 50 terms from the respective documents. Given a term doc-ument pair, they were asked the question:  X  X s this term relevant to the document? X  They were required to answer this question on a 4-point scale EGFB (Excellent, Good, Fair, Bad). Thus, we generated roughly 10,000 judgments for our comparisons. We had 5 judges to do this task and the inter-annotator agreement between them was around 70%. In order to define relevance of a term to a document, the ed-itors were given guidelines which asked them to simulate the task of writing a summary for the document. An excellent term would be one which will be used in the first sentence of the summary while a term that X  X  fair will come much later in the summary, if at all.
 Evaluation results: Figure 4 shows relative performance of the various system using the NDCG metric. Our approach (MLA) outperforms all other approaches quite significantly in the term ranking quality of the top 5 terms. For the MoC system, the authors show that the query log and IR features are the most important features for their system. However, in our test set around 18% of the documents do not contain any query logs (from the tail documents) hence that fea-ture is empty. MLA1 contains all the features in MLA but uses hand constructed weights for the features. Thus, in a way, there is no information gain available from the query logs in this system. Figure 4 shows that MLA1 does better than MoC. Some of this gain is due to the richer HTML and visual features that MLA1 uses but some of it is also due to the fact that MoC performs worse than normal due to the lack of query data in our test collection. MLA2 uses the features from MLA that are not structural and visual in nature. It, however, uses click data to learn the model that it uses for ranking. MLA2 performs better than MLA1 at early ranks indicating that the query data training helps in ranking the terms at earlier ranks correctly. However, as the rank increases MLA1 shows a slightly better perfor-mance compared to MLA2 indicating that the learning using query data with limited features does not give us much gain at higher ranks. Thus, if we were to answer the question  X  X oes the gain in MLA comes from the click data training or the sophisticated feature set? X  this above comparison in performance of MLA1 and MLA2 is useful. It shows that the gain from the click data training helps perform well in the early ranks even with the basic feature set. However, the richer feature set helps us improve the performance in the higher ranks since without the sophisticated features, it is hard to discriminate between terms appearing at higher ranks. When we combine the gain from the query log learn-ing and the richer feature set (which is our approach MLA), we get a significantly better performance for the top 5 or 10 terms.
 Discussion: When the NDCG values for higher ranks such as 10 are considered, MLA still performs marginally better than the other approaches. However, at higher ranks such as 20, the NDCG values almost start converging and in some cases, MLA performs poorly compared to simple approaches such as TF-IDF. This is expected since most documents for Figure 4: Comparing the different aboutness solu-tions using the NDCG metric. On the x-axis is the rank of the term and on the y-axis is the NDCG value. short and we cannot except 20 or so terms to be relevant and thus be covered by an aboutness solution. Aboutness systems that don X  X  do well in the early ranks will tend to have slightly better NDCG values in higher ranks. Evaluation Goal: This evaluation method compares the different aboutness solutions using a slightly indirect form of judgment data i.e. by asking the editors to create a free text summary for a given web page. We use a test set from the tail test collection which ensures that we measure how well our system, which is trained on click data for popular pages, generalizes for pages which have little or no click data. Ground truth data: If a person were to summarize a document in 3 to 4 sentences, he will make use of the most relevant terms in the web page. We have access to human generated summaries that are generated for tasks such as search results summaries and other web browsing applica-tions. These summaries are hand constructed by several editors for most popular and some less popular pages de-scribing what a particular page is about. Guidelines given to editors for this task specify that the summaries need to be generated by keeping the general aboutness of the web page in mind. Further, the editors are also instructed to make use of terms from the web page although use of exact sen-tences from the web page is not required. Eg., One of the editor-generated summaries for the homepage of Walmart is  X  X hop Walmart Online for Low Prices on Top Brands in Computers, TVs, Toys, GPS, Video Games, DVDs, Music, Apparel, Housewares ...  X . We use two test sets for this evaluation method. A collection of 1000 web pages from the random-all test collection and a collection of 1000 web pages from the random-tail test collection. For a given web page, summaries from different editors are combined and terms in those summaries are extracted using our term extraction method. These terms are de-duped and their frequency of occurrence is obtained. This data is then used as ground truth for our evaluation purposes.
 Evaluation results: Table 2 summarizes the performance of the various approaches on the random-all test collection while Table 3 records the results on the random-tail test col-lection. Our approach outperforms all the other systems in both cases. However, the numbers are slightly better for all systems on the random test set as opposed to the tail test set. This is because the tail set contains more shorter doc-uments with little or no text (such as flash pages, etc). The aboutness results for such documents will be quite sparse. Note that the performance of MoC on the tail document set is much lower compared to its performance on the random test set. This is because for the tail document set, the query log feature (which is the most powerful feature) is empty. The performance of MoC on tail documents falls even lower than TF-IDF and Prisma. This brings forth the point that we want to drive home that for general tasks such as docu-ment aboutness, click logs cannot be used as a direct feature in a machine learning model.
 Method Jaccard Multiset Avg Recall TF-IDF 0.3 0.54 0.41 0.51 Prisma 0.32 0.55 0.43 0.34 MoC 0.34 0.57 0.45 0.52 MLA1 0.44 0.58 0.62 0.58 MLA2 0.42 0.52 0.69 0.5 MLA 0.51 0.62 0.75 0.65 Table 2: Results comparing the various approaches on Method Jaccard Multiset Avg Recall TF-IDF 0.32 0.57 0.42 0.48 Prisma 0.34 0.52 0.49 0.42 MoC 0.28 0.49 0.34 0.2 MLA1 0.4 0.51 0.58 0.52 MLA2 0.37 0.48 0.6 0.48 MLA 0.52 0.58 0.72 0.62 Table 3: Results comparing the various approaches on Discussion : Our approach performs significantly better on documents where the frequently occurring words are not very important. Consider the IMDB web page on Loni An-derson . The top 10 terms pulled up by TF-IDF, Prisma and our approach are as shown in Table 4. We see that the top ranked terms by our system are much better than those of the other systems. This IMDB page contains Loni An-derson X  X  profile and a long listing of works by her. Each of those listings contains words such as TV, USA, etc which are treated as important by TF-IDF and Prisma. While our approach assigns more weight to the more central and hence more important terms in the page.
 Table 4: Top ranked terms for the IMDB page on Loni Anderson Experiment goal: In this experiment, we demonstrate the query predictability of our system for those web pages that do not have any click data (queries) associated with them. From a dataset of about 1 million query url pairs with rel-evance judgments, we grab about 400,000 urls that do not contain any click data. For these urls, we show that the queries in the judgment data set that are relevant to the corresponding document, are ranked higher by our system compared to those that are not relevant.
 Ground truth data: For this experiment, we use the rele-vance judgments for query url pairs as ground truth. Editors judge the relevance of a query to a document on a 5-point scale (Perfect, Excellent, Good, Fair, Bad). For the urls that do not have any click data associated with them, we get the top 5 ranked terms from our system. We look at the overlap of these terms with the relevance judgment queries for the corresponding documents.
 Experiment results: Figure 5 shows the distribution of the top 5 terms ranked by our system for the urls that do not contain click data over the relevance grade of the queries for those documents. For each relevance grade (plotted on the x-axis) of the queries, we determine what percentage of queries are covered by the top 5 terms determined by our system. The y-axis shows this percentage of coverage. The plot shows that the coverage of terms in the aboutness output correlates well with the relevance judgment of the queries i.e. Perfect grade queries are covered in the top 5 aboutness terms over 75% of the times while the bad grade queries are covered less than 20% of the times.
 Figure 5: Distribution of top 5 terms by our system for Discussion: Query click data has been found to be useful for improving search result ranking and several click data features have been successfully used for improving the search ranking function [20]. However, such click data is available for roughly 20% of the entire of web. Thus for 80% of the documents, the click data features are empty. In such a sce-nario, the ability to predict queries for web pages that don X  X  have any click data can very useful. Our preliminary exper-iment shows that our aboutness solution can predict terms (and hence queries) for documents. The top ranked terms can thus be used for populating some click data features. As future work, we plan to explore this further to show its impact on the search ranking function.
We have described a representation for document about-ness which finds use in several text processing applications. Our method for solving document aboutness involves a ma-chine learning approach to rank terms as per their relevance to a document. Our system outperforms several competing aboutness solutions. We also show an interesting application of our system in query prediction on tail web pages. One direction of future work is to investigate this further and use it as a feature to improve the search ranking function. Other areas of future work include training over more ho-mogeneous subsets of web pages ( e.g. , news, encyclopedias), investigate the use of other user generated sources for surro-gate target, and the use of a more sophisticated approach for determining the structural features and also a more robust boilerplate detection for generating the features. [1] E. Agichtein, E. Brill, and S. T. Dumais. Improving [2] H. Alani and C. Brewster. Ontology ranking based on [3] H. Alani, C. Brewster, and N. Shadbolt. Ranking [4] P. G. Anick. Using terminological feedback for web [5] P. G. Anick and S. Tipirneni. Interactive document [6] P. D. Bruza and T. W. C. Huibers. Investigating [7] P. D. Bruza and T. W. C. Huibers. A study of [8] P. D. Bruza, D. W. Song, and K. F. Wong. Aboutness [9] D. Cai, S. Yu, J. rong Wen, and W. ying Ma.
 [10] D. Chakrabarti, R. Kumar, and K. Punera.
 [11] D. Chakrabarti, R. Kumar, and K. Punera. A [12] T. Cheng, X. Yan, and K. C. Chang. Entityrank: [13] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [14] D. Eichmann. Extraction of document structure for [15] J. Friedman. Stochastic gradient boosting. Technical [16] J. H. Friedman. Greedy function approximation: A [17] B. Hjorland. Towards a theory of aboutness, subject, [18] W. J. HUTCHINS. On the problem of  X  X boutness X  in [19] U. Irmak, V. von Brzeski, and R. Kraft. Contextual [20] T. Joachims. Optimizing search engines using [21] M. Jones and H. Alani. Content-based ontology [22] Y. Lin and E. Hovy. Identifying topics by position. In [23] Y. Lin and E. Hovy. The automated acquisition of [24] F. Radlinski, A. Z. Broder, P. Ciccolo, E. Gabrilovich, [25] H. Rode, P. Serdyukov, and D. Hiemstra. Combining [26] G. Salton and M. J. McGill. Introduction to Modern [27] F. Sebastiani and C. N. D. Ricerche. Machine learning [28] J. tao Sun, Q. Yang, and Y. Lu. Web-page [29] W. Tau-Wih, J. Goodman, and V. R. Carvalho.
 [30] G. Wu, J. Li, T. Li, and K. Wang. Understanding an [31] X. Wu and A. Bolivar. Keyword extraction for [32] H. Zha. Generic summarization and keyphrase [33] L. Zhang, Y. Pan, and T. Zhang. Focused named
