 Haiping Lu hplu@ieee.org Konstantinos N. Plataniotis kostas@comm.toronto.edu Anastasios N. Venetsanopoulos tasvenet@ryerson.ca Ryerson University, Toronto, ON, M5B 2K3, Canada Various machine learning problems take multi-dimensional data as input, which are formally called tensors. The elements of a tensor are to be addressed by several indices and the number of indices used in the description defines the order of the tensor object, with each index defining one  X  X ode X  (Lathauwer et al., 2000). Many real-world data are naturally tensor ob-jects. For example, matrix data such as gray-level images are second-order tensors, gray-scale video se-quences and 3-D objects are third-order tensors. In ad-dition, streaming data and mining data are frequently organized as third-order tensors. For instance, data in environmental sensor monitoring are often organized in three modes of time, location and type, and data in web graph mining are commonly organized in three modes of source, destination and text. Other appli-cations involving tensorial data include data center monitoring, social network analysis, network forensics and face recognition (Faloutsos et al., 2007). In these practical applications, tensor objects are often spec-ified in a high-dimensional tensor space, leading to the so-called curse of dimensionality. Nonetheless, the class of tensor objects in most applications are highly constrained to a subspace, a manifold of intrinsically low dimension (Shakhnarovich &amp; Moghaddam, 2004), and feature extraction or dimensionality reduction is frequently employed to transform a high-dimensional data set into a low-dimensional space of equivalent representation while retaining most of the underlying structure (Law &amp; Jain, 2006).
 The PCA is a classical linear method for unsupervised dimensionality reduction that transforms a data set consisting of a large number of interrelated variables to a new set of uncorrelated variables, while retain-ing as much as possible the variations present in the original data set (Jolliffe, 2002). PCA on tensor ob-jects requires their reshaping (vectorization) into vec-tors in a very high-dimensional space, which not only results in high computational and memory demands but also breaks the natural structure and correlation in the original data (Ye, 2005; Ye et al., 2004; Lu et al., 2008a). It is believed by many researchers that po-tentially more compact or useful representations can be obtained from the original form and PCA exten-sions operating directly on the tensor objects rather than their vectorized versions are emerging recently (Ye et al., 2004; Lu et al., 2008a; Xu et al., 2005). In (Shashua &amp; Levin, 2001), the tensor rank-one de-composition (TROD) is used to represent a class of im-ages based on variance maximization and (greedy) suc-cessive residue calculation. A two-dimensional PCA (2DPCA) is proposed in (Yang et al., 2004) that con-structs an image covariance matrix using image ma-trices as inputs. However, linear transformation is applied only to the right side of image matrices so the image data is projected in one mode only, result-ing in poor dimensionality reduction. A more general algorithm named generalized low rank approximation of matrices (GLRAM) was introduced in (Ye, 2005), which applies two linear transforms to both the left and right sides of input image matrices and results in a better dimensionality reduction than 2DPCA. GLRAM is developed from the perspective of approx-imation while the generalized PCA (GPCA) is pro-posed in (Ye et al., 2004) from the view of variation maximization, as an extension of PCA. Later, the con-current subspaces analysis (CSA) is formulated in (Xu et al., 2005) for optimal reconstruction of general ten-sor objects, which can be considered as a generaliza-tion of GLRAM, and the multilinear PCA (MPCA) introduced in (Lu et al., 2008a) targets at variation maximization for general tensor objects in the exten-sion of PCA to the multilinear case, which can be con-sidered as a further generalization of GPCA.
 However, none of the existing multilinear extensions of PCA mentioned above takes an important property of PCA into account, i.e., PCA derives uncorrelated features, which contain minimum redundancy and en-sure independence among features. Instead, most of them produce orthogonal bases in each mode. Al-though uncorrelated features imply orthogonal projec-tion bases in PCA, this is not necessarily the case for its multilinear extension. With this motivation, this paper investigates multilinear extension of PCA that can produce uncorrelated features. We propose a novel uncorrelated multilinear PCA (UMPCA) for unsuper-vised tensor object dimensionality reduction (feature extraction). UMPCA is based on the tensor-to-vector projection (TVP) (Lu et al., 2008b) and it follows the classical PCA derivation of successive variance maxi-mization (Jolliffe, 2002). Thus, a number of elemen-tary multilinear projections (EMPs) are solved to max-imize the captured variance with the zero-correlation constraint. The solution is iterative in nature, as many other multilinear algorithms (Xu et al., 2005; Ye et al., 2004; Shashua &amp; Levin, 2001).
 The rest of this paper is organized as follows. Section 2 reviews basic multilinear notations and operations, as well as the concept of tensor-to-vector projection. In Sec. 3, the problem of UMPCA is formulated and the solution is derived as a sequential iterative process. Next, Sec. 4 evaluates the effectiveness of UMPCA in the popular face recognition task through comparison with PCA, MPCA and TROD. Finally, the conclusions are drawn in Sec. 5. This section introduces the multilinear notations, op-erations and projections needed in the presentation of UMPCA, and for further pursuing of multilinear alge-bra, (Lathauwer et al., 2000) is a good reference. The important notations used in this paper are listed in Table 1 for handy reference. 2.1. Notations and basic multilinear operations Due to the multilinear nature of tensor objects, new notations have been introduced in the literature for mathematical analysis. Following the notations in (Lathauwer et al., 2000), we denote vectors by low-ercase boldface letters, e.g., x ; matrices by uppercase boldface letters, e.g., U ; and tensors by calligraphic letters, e.g., A . Their elements are denoted with in-dices in parentheses. Indices are denoted by lowercase letters and span the range from 1 to the uppercase letter of the index, e.g., n = 1 , 2 ,...,N .
 by N indices i n , n = 1 ,...,N , and each i n addresses the n -mode of A . The n -mode product of a tensor A tensor with entries: The scalar product of two tensors A , B  X  R I 1  X  I 2  X  ...  X  I is defined as: &lt; A , B &gt; = X A rank-one tensor A equals to the outer product of N vectors: A = u (1)  X  u (2)  X  ...  X  u ( N ) , which means that A ( i 1 ,i 2 ,...,i N ) = u (1) ( i 1 )  X  u (2) ( i 2 )  X  ...  X  u values of indices. 2.2. Tensor-to-vector projection In order to extract uncorrelated features from tenso-rial data directly, we employ the TVP introduced in (Lu et al., 2008b), which is a more general form of the projection in (Shashua &amp; Levin, 2001) and con-sists of multiple EMPs. An EMP is a multilinear pro-projection vector in each mode, i.e., k u ( n ) k = 1 for n = 1 ,...,N , where k  X  k is the Euclidean norm for scalar y through the N unit projection vectors as where U = u (1)  X  u (2)  X  ...  X  u ( N ) . An EMP can be viewed as a constrained linear projection since &lt; X , U &gt; = &lt; vec ( X ) ,vec ( U ) &gt; = [ vec ( U )] where vec (  X  ) denotes the vectorized representation. The TVP of a tensor object X to a vector y  X  R 1 ,...,P , which can be written concisely as { u ( n ) T p 1 ,...,N } P p =1 : where the p th component of y is obtained from the p th TROD (Shashua &amp; Levin, 2001) in fact seeks a TVP to maximize the captured variance, however, it takes a heuristic greedy approach. In the next section, we propose a systematic, more principled formulation by taking consideration of the correlation among features. In addition, the TVP for dimensionality reduction here is related mathematically to the parallel factor analysis (PARAFAC) originated from psychometrics (Harsh-man, 1970), also known as the canonical decomposi-tion (CANDECOMP) (Carroll &amp; Chang, 1970), which is popular in factor analysis of multi-way data, i.e., tensors. However, they are developed from different perspectives. The PARAFAC in the factorization lit-erature aims to decompose a higher-order tensor, often formed by arranging lower-order tensors, into a num-ber of rank-one tensorial factors explaining the for-mation of the data. In contrast, the objective of the TVP for dimensionality reduction here is to learn a low-dimensional (subspace) representation of a class of tensor objects from a number of samples so that the underlying (class) structure is well captured. This section proposes the UMPCA for unsupervised dimensionality reduction of tensor objects by first for-mulating the UMPCA objective function and then adopting the successive variance maximization ap-proach and alternating projection method to solve the problem. In the presentation, for the convenience of discussion, the training samples are assumed to be zero-mean 1 so that the constraint of uncorrelated features is the same as orthogonal features (Koren &amp; Carmel, 2004). 3.1. Problem formulation Following the standard derivation of PCA given in (Jolliffe, 2002), we consider the variance of the princi-pal components (PCs) one by one. In the TVP setting, the p th PCs are { y m p ,m = 1 ,...,M } , where M is the number of training samples and y m p is the projection of the m th sample X m by the p th EMP { u ( n ) T p ,n = cordingly, the variance is measure by their total scatter S where  X  y p = 1 M P m y m p . In addition, let g p denote the p th coordinate vector, with its m th component g ( m ) = y m p . A formal definition of the unsupervised multilinear feature extraction problem to be solved in UMPCA is then given in the following: A set of M tensor object samples {X 1 , X 2 , ..., X M } are available for training. Each tensor object X mension of the tensor and N denotes the Kronecker product. The objective of the UMPCA is to find a TVP, which consists of P EMPs { u ( n ) p  X  R I n  X  1 ,n = 1 ,...,N } P p =1 , mapping from the original tensor space R y such that the variance of the projected samples, mea-sured by S y T subject to the constraint that the P coordinate vec-tors { g p  X  R M ,p = 1 ,...,P } are uncorrelated. In other words, the UMPCA objective is to determine a set of P EMPs { u ( n ) T p ,n = 1 ,...,N } P p =1 that maxi-mize the variance while producing features with zero-correlation. Thus, the objective function for the p EMP is { u ( n ) T p ,n = 1 ,...,N } = arg max where  X  pq is the Kronecker delta (defined as 1 for p = q and as 0 otherwise). 3.2. The UMPCA algorithm To solve the UMPCA problem (6), we follow the suc-cessive variance maximization approach in the deriva-tion of PCA in (Jolliffe, 2002). The P EMPs { u p ,n = 1 ,...,N } P p =1 are determined one by one in P steps, with the p th step obtaining the p th EMP: Step 1 : Determine the first EMP { u ( n ) T 1 ,n = Step 2 : Determine the second EMP { u ( n ) T 2 ,n = Step p ( p = 3 ,...,P ) : Determine the p th EMP In order to solve for the p th EMP { u ( n ) T p ,n = 1 ,...,N } , we need to determine N sets of parameters correspond-ing to N projection vectors, u (1) p , u (2) p ,... u ( N ) each mode. Unfortunately, simultaneous determina-tion of these N sets of parameters in all modes is a complicated non-linear problem without an existing optimal solution, except when N = 1, which is the classical PCA where only one projection vector is to be solved. Therefore, we follow the approach in the alternating least square (ALS) algorithm (Harshman, 1970) to solve this multilinear problem. For each EMP to be determined, the parameters of the projection vec-tor u ( n  X  ) p for each mode n  X  are estimated one mode by one mode separately, conditioned on { u ( n ) p ,n 6 = n the parameter values of the projection vectors in the other modes.
 To solve for u ( n  X  ) p in the n  X  -mode, assuming that { u p ,n 6 = n  X  } is given, the tensor samples are pro-jected in these ( N  X  1) modes { n 6 = n  X  } first to obtain the vectors becomes to determine u ( n  X  ) p that projects the vector samples {  X  y ( n  X  ) m p ,m = 1 ,...,M } onto a line so that the variance is maximized, subject to the zero-correlation constraint, which is a PCA problem with the input samples {  X  y ( n  X  ) m p ,m = 1 ,...,M } . The corresponding to-tal scatter matrix  X  S ( n  X  ) T to solve for the P EMPs. For p = 1, the u ( n  X  ) 1 that maximizes the total scatter u ( n  X  ) T 1  X  S ( n  X  ) projected space is obtained as the unit eigenvector of  X  S show how to determine the p th ( p &gt; 1) EMP given the first ( p  X  1) EMPs. Given the first ( p  X  1) EMPs, the p th EMP aims to maximize the total scatter S y T subject to the constraint that features projected by the p th EMP are uncorrelated with those projected by the first ( p  X  1) EMPs. Let  X  Y ( n  X  ) p  X  R I a matrix with  X  y ( n  X  ) m p as its m th column, i.e.,  X  h  X  y correlated with { g q ,q = 1 ,...,p  X  1 } can be written as Thus, u ( n  X  ) p ( p &gt; 1) can be determined by solving the following constrained optimization problem: The solution is given by the following theorem: Theorem 1. The solution to the problem (10) is the (unit-length) eigenvector corresponding to the largest eigenvalue of the following eigenvalue problem: where and I I Proof. First, Lagrange multipliers can be used to transform the problem (10) to the following to include all the constraints: where  X  and {  X  q ,q = 1 ,...,p  X  1 } are Lagrange multi-pliers.
 The optimization is performed by setting the partial derivative of F ( u ( n  X  ) p ) with respect to u ( n  X  ) Multiplying (16) by u ( n  X  ) T p results in which indicates that  X  is exactly the criterion to be maximized, with the constraint on the norm of the projection vector incorporated.
 Next, a set of ( p  X  1) equations are obtained by multi-2 g Let and use (13) and (14), then the ( p  X  1) equations of (18) can be represented in a single matrix equation as following: Thus, Since from (14) and (19), the equation (16) can be written as Using the definition in (12), an eigenvalue problem is to be maximized, the maximization is achieved by set-ting u ( n )  X  p to be the (unit) eigenvector corresponding to the largest eigenvalue of (11).
 a unified solution for UMPCA: for p = 1 ,...,P , u ( n  X  ) is obtained as the unit eigenvector of  X  ( n  X  ) p  X  S ( n ciated with the largest eigenvalue. Algorithm 1 sum-marizes the UMPCA developed here.
 Algorithm 1 Uncorrelated Multilinear Principal Component Analysis (UMPCA) Input: A set of tensor samples {X m  X  ality P , and the maximum number of iterations K . for p = 1 to P do end for 3.3. Initialization, projection order and As an iterative algorithm, the UMPCA may be af-fected by the initialization method, the projection or-der and the termination conditions. Due to the space constraint, these issues, as well as the convergence and computational issues, are not studied here. Instead, we adopt simple implementation strategies for them. First, we use the uniform initialization for UMPCA, where all n -mode projection vectors are initialized to have unit length and the same value along the I n di-mensions in n -mode, which is equivalent to the all ones vector 1 with proper normalization. Second, as shown in Algorithm 1, the projection order, which is the mode ordering in computing the projection vectors, is from 1-mode to N -mode, as in other multilinear algorithms (Ye, 2005; Xu et al., 2005; Lu et al., 2008a). Third, the iteration is terminated by setting K , the maximum number of iterations. The proposed UMPCA can potentially benefit various applications involving tensorial data, as mentioned in Sec. 1. Since face recognition has practical impor-tance in security-related applications such as biomet-ric authentication and surveillance, it has been used widely for evaluation of unsupervised learning algo-rithms (Shashua &amp; Levin, 2001; Yang et al., 2004; Xu et al., 2005; Ye, 2005). Therefore, in this section, we focus on evaluating the effectiveness of UMPCA on this popular classification task through performance comparison with existing unsupervised dimensionality reduction algorithms. 4.1. The FERET database The Facial Recognition Technology (FERET) database (Phillips et al., 2000) is widely used for testing face recognition performance, with 14,126 images from 1,199 subjects covering a wide range of variations in viewpoint, illumination, facial ex-pression, races and ages. A subset of this database is selected in our experimental evaluation and it consists of those subjects with each subject having at least eight images with at most 15 degrees of pose variation, resulting in 721 face images from 70 subjects. Since our focus here is on the recognition of faces rather than their detection, all face images are manually cropped, aligned (with manually annotated coordinate information of eyes) and normalized to 80  X  80 pixels, with 256 gray levels per pixel. Figure 1 shows some sample face images from two subjects in this FERET subset.
 4.2. Face recognition performance comparison In the evaluation, we compare the performance of the UMPCA against three PCA-based unsupervised learning algorithms: the PCA (eigenface) algorithm (Turk &amp; Pentland, 1991), the MPCA algorithm (Lu et al., 2008a) 2 and the TROD algorithm (Shashua &amp; Levin, 2001). The number of iterations in TROD and UMPCA is set to ten, with the same (uniform) initial-ization used. For MPCA, we obtain the full projection and select the most descriptive P features for recogni-tion. The features obtained by these four algorithms are arranged in descending variation captured (mea-sured by respective total scatter). For classification of extracted features, we use the nearest neighbor classi-fier (NNC) with Euclidean distance measure.
 Gray-level face images are naturally second-order ten-sors (matrices), i.e., N = 2. Therefore, they are input directly as 80  X  80 tensors to the multilin-ear algorithms (MPCA, TROD, UMPCA), while for PCA, they are vectorized to 6400  X  1 vectors as in-put. For each subject in a face recognition experiment, L (= 1 , 2 , 3 , 4 , 5 , 6 , 7) samples are randomly selected for unsupervised training and the rest are used for testing. We report the results averaged over ten such random splits (repetitions).
 Figures 2 and 3 show the detailed results 3 for L = 1 and L = 7, respectively. L = 1 is an extreme small sample size scenario where only one sample per class is available for training, the so-called one training sample (OTS) case important in practice (Wang et al., 2006), and L = 7 is the maximum number of training samples we can use in our experiments. Figures 2(a) and 3(a) plot the correct recognition rates against P , the di-mensionality of the subspace for P = 1 ,..., 10, and Figs 2(b) and 3(b) plot those for P = 15 ,..., 80. From the figures, UMPCA outperforms the other three methods in both cases and across all dimensionality, indicating that the uncorrelated features extracted directly from the tensorial face data are more effective in classifi-cation. The figures also show that for UMPCA, the recognition rate saturates around P = 30, which can be explained by observing the variance captured by in-dividual features as shown in Figs. 2(c) and 3(c) (in log scale). These figures show that the variance captured by UMPCA is considerably lower than those captured by the other methods, which is due to its constraints of zero-correlation and TVP. Despite capturing lower variance, UMPCA is superior in the recognition task performed. Nonetheless, when the variance captured is too low, those corresponding features are no longer descriptive enough to contribute in classification, lead-ing to the saturation.
 In addition, we also plot the average correlation of in-dividual features with all the other features in Figs. 2(d) and 3(d). As supported by theoretical deriva-tion, features extracted by PCA and UMPCA are un-correlated. In contrast, features extracted by MPCA and TROD are correlated, with TROD features have higher correlation on average.
 The recognition results for P = 1 , 5 , 10 , 20 , 50 , 80 are listed in Table 2 for L = 2 , 3 , 4 , 5 , 6, where the best recognition results among the four methods are shown in bold. More detailed results are omitted here to save space. From the table, UMPCA achieves supe-rior recognition results in all cases except for P = 80 and L = 2, where the difference with the best results by MPCA is small (0 . 3%). In particular, for smaller P (1 , 5 , 10 , 20), UMPCA outperforms the other algo-rithms significantly, demonstrating its superior capa-bility in classifying faces in low-dimensional spaces. This paper proposes a novel uncorrelated multilinear PCA algorithm, where uncorrelated features are ex-tracted directly from tensorial representation through a tensor-to-vector projection. The algorithm succes-sively maximizes variance captured by each elemen-tary projection while enforcing the zero-correlation constraint. The solution employs the alternating pro-jection method and is iterative. Experiments on face recognition demonstrate that compared with other unsupervised learning algorithms including the PCA, MPCA and TROD, the UMPCA achieves the best re-sults and it is particularly effective in low-dimensional spaces. Thus, face recognition through unsupervised learning benefits from the proposed UMPCA and in future research, it is worthwhile to investigate whether UMPCA can contribute in other unsupervised learning tasks, such as clustering.
 The authors would like to thank the anonymous re-viewers for their insightful comments. This work is partially supported by the Ontario Centres of Excel-lence through the Communications and Information Technology Ontario Partnership Program and the Bell University Labs -at the University of Toronto. Carroll, J. D., &amp; Chang, J. J. (1970). Analysis of indi-vidual differences in multidimensional scaling via an n-way generalization of  X  X ckart-young X  decomposi-tion. Psychometrika , 35 , 283 X 319.
 Faloutsos, C., Kolda, T. G., &amp; Sun, J. (2007). Min-ing large time-evolving data using matrix and tensor tools. Int. Conf. on Data Mining 2007 Tutorial . Harshman, R. A. (1970). Foundations of the parafac procedure: Models and conditions for an  X  X xplana-tory X  multi-modal factor analysis. UCLA Working Papers in Phonetics , 16 , 1 X 84.
 Jolliffe, I. T. (2002). Principal component analysis, second edition . Springer Serires in Statistics. Koren, Y., &amp; Carmel, L. (2004). Robust linear di-mensionality reduction. IEEE Trans. Vis. Comput. Graphics , 10 , 459 X 470.
 Lathauwer, L. D., Moor, B. D., &amp; Vandewalle,
J. (2000). On the best rank-1 and rank-( R 1 ,R 2 ,...,R N ) approximation of higher-order ten-sors. SIAM Journal of Matrix Analysis and Appli-cations , 21 , 1324 X 1342.
 Law, M. H. C., &amp; Jain, A. K. (2006). Incremental non-linear dimensionality reduction by manifold learn-ing. IEEE Trans. Pattern Anal. Mach. Intell. , 28 , 377 X 391.
 Lu, H., Plataniotis, K. N., &amp; Venetsanopoulos, A. N. (2008a). MPCA: Multilinear principal component analysis of tensor objects. IEEE Trans. Neural Netw. , 19 , 18 X 39.
 Lu, H., Plataniotis, K. N., &amp; Venetsanopoulos, A. N. (2008b). Uncorrelated multilinear discriminant analysis with regularization and aggregation for ten-sor object recognition. IEEE Trans. Neural Netw. accepted pending minor revision.
 Phillips, P. J., Moon, H., Rizvi, S. A., &amp; Rauss, P. (2000). The FERET evaluation method for face recognition algorithms. IEEE Trans. Pattern Anal. Mach. Intell. , 22 , 1090 X 1104.
 Shakhnarovich, G., &amp; Moghaddam, B. (2004). Face recognition in subspaces. Handbook of Face Recog-nition (pp. 141 X 168). Springer-Verlag.
 Shashua, A., &amp; Levin, A. (2001). Linear image cod-ing for regression and classification using the tensor-rank principle. Proc. IEEE Conf. on Computer Vi-sion and Pattern Recognition (pp. 42 X 49).
 Turk, M., &amp; Pentland, A. (1991). Eigenfaces for recog-nition. Journal of Cognitive Neurosicence , 3 , 71 X 86. Wang, J., Plataniotis, K. N., Lu, J., &amp; Venetsanopou-los, A. N. (2006). On solving the face recognition problem with one training sample per subject. Pat-tern Recognition , 39 , 1746 X 1762.
 Xu, D., Yan, S., Zhang, L., Zhang, H.-J., Liu, Z., &amp;
Shum;, H.-Y. (2005). Concurrent subspaces analy-sis. Proc. IEEE Computer Society Conf. on Com-puter Vision and Pattern Recognition (pp. 203 X 208). Yang, J., Zhang, D., Frangi, A. F., &amp; Yang, J. (2004). Two-dimensional PCA: a new approach to appearance-based face representation and recogni-tion. IEEE Trans. Pattern Anal. Mach. Intell. , 26 , 131 X 137.
 Ye, J. (2005). Generalized low rank approximations of matrices. Machine Learning , 61 , 167 X 191.
 Ye, J., Janardan, R., &amp; Li, Q. (2004). GPCA: An effi-cient dimension reduction scheme for image com-pression and retrieval. The 10 th ACM SIGKDD
Int. Conf. on Knowledge Discovery and Data Min-
