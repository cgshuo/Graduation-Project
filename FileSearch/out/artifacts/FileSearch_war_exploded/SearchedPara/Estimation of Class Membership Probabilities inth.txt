 When a classifier predicts a class for an evaluation sample in a document clas-sification, estimating the probability with which the sample belongs to the pre-dicted class (class membership probability) is useful in many applications. As an example for human decision making, w e describe the need of class member-ship probabilities in  X  X n automatic occupation coding system X  in social surveys. The occupation coding is a task for various statistical analyses in sociology, in which researchers assign occupational codes to occupation data collected as re-sponses to open-ended questions in social surveys. To help the human annotators (coders), we have developed an automa tic occupation coding system with ma-chine learning [13], as well as a system called the NANACO system [14], which displays outputs from the automatic system as candidates of occupational codes. The NANACO system is currently being applied in important social surveys in Japan such as the JGSS 1 , in which the coders have asked us to supply a measure of confidence for the first-ranked candidate of their confidece decisions. In fact, class membership probabilities are widely noticed in different applications [5,3].
Although the class membership probabilities can be estimated easily using classification scores provided from a classifier (hereafter referred to as scores ) 2 , estimates should be calibrated because t hey are often quite different from true values. Representative proposed methods include Platt X  X  method [10] and various methods by Zadrozny et al. [15,16,17]. Platt [10] used Support Vector Machines (SVMs) and directly estimated class membership probabilities using a sigmoid function to transform scores into the probabilities. Zadrozny et al. proposed a  X  X inning X  method for Naive Bayes [15], and Isotonic regression for SVMs and Naive Bayes [17]. In the notable binning method ( Zadrozny X  X  binning method ), the authors used the first-ranked scores of training samples, rearranged training samples according to their scores and made bins with equal samples per bin. In the Isotonic regression method, Zadrozny et al. proposed a method for a multiclass classifier by dividing a multiclass classifier into binary classifiers. In the document classification, a multiclass classification is often applied. When a multiple classifier outputs a score for each class, a predicted class is determined not by the absolute value of the score but by the relative position among the scores. Therefore, the class membership probabilities are likely to depend not just on the first-ranked score but also on other scores. We propose a new method for estimating class membership probabilities of the predicted class, using scores not only for the predicted class but also for other classes. In the proposed method, we first make an accuracy table by counting the number of correctly classified training samples in ea ch range or cell (hereafter referred to as cell ) of scores. We then apply smoothing methods such as a moving average method to the accuracy table to yield reliable probabilities (accuracies). In order to determine the class membership probability of an unknown sample, we first calculate the scores of the sample, then find the cell that corresponds to the scores, and output the values associat ed in the cell in the accuracy table. 2.1 Platt X  X  Method Platt [10] proposed a method using SVM s and directly estimating class mem-bership probabilities using a sigmoid function P ( f )=1 / { 1+ exp ( Af + B ) } with the score f because f is substituted into a monotonically increasing sig-moid function. The parameters A and B were estimated with the maximum likelihood method beforehand. To avoid overfitting the train set, Platt used an out-of-sample model. Using five types of datasets from Reuters [4] and other sources, a good experimental result was obtained for probability values. Ben-nett [2], however, using the Reuters dataset showed that the sigmoid method could not fit the Naive Bayes scores. Bennett proposed other sigmoid families in approximating the posterior distribution given the Naive Bayes log odds ap-proximation. Zadrozny et al. [17] also showed that Platt X  X  method could not be applied for some datasets and proposed a different approach. 2.2 Zadrozny X  X  Binning Method Zadrozny et al. [15] proposed the discrete non-parametric binning method, which indirectly estimates class membership probabilities by referring to the  X  X ins X  made for the Naive Bayes classifier bef orehand. The method is described as follows. First, samples are rearranged in order of the values of their scores, and intervals are created to ensure that the number of samples falling into each area (bin) equals a fixed value. For each bin Zadrozny et al. computes lower and upper boundary scores. Next, the accuracy of samples in each bin is calculated. Finally, an evaluation of the new sample is done using the score to find a matching bin, and the accuracy of the bin is then assigned to the sample. Using KDD X 98 datasets, a good experimental result was obtained in terms of some evaluation criteria such as Mean Squared Error (MSE) or average log-loss (bin=10). The method has a problem, however, in answering how the best number of bins can be determined. 2.3 Isotonic Regression and Expansion for a Multiclass Classifier Based on a monotonic relationship betw een classifier scores and accuracies, Zadrozny et al. [17] next proposed a method via the PAV (Pair-Adjacent Vi-olators) algorithm, which has been widely researched for problems of Isotonic regression. As a result of experiments for SVMs and Naive Bayes, PAV per-formed slightly better than the sigmoid method, while it always worked better than the binning method. For a multiclass classifier, they applied PAV as fol-lows. First they transformed a multiclass classifier into binary classifiers by a one-against-all code matrix, all-pairs, and a one-against-all normalization. Next they calibrated the predic tions from each binary classifier and combined them to obtain target estimates. The performance of PAV for a multiclass classifier using 20 Newsgroups dataset 3 for Naive Bayes was much better in terms of MSE, although the error rate was not good. 2.4 Comparison of the Methods Niculescu-Mizil et al. [8] compared 10 classifiers for calibration using Platt X  X  method with Zadrozny X  X  method via Isotonic regression using 8 datasets from UCI and other sources, and showed that Platt X  X  method was most effective when the data was small, while Isotonic regression was more powerful when there was sufficient data to prevent overfitting 4 . Jones et al. [5] compared Isotonic regres-sion with the sigmoid method, and showed that the sigmoid method outper-formed Isotonic regression by root-mean squared error and log-loss. The reason for the outperformance was that Isotonic regression tended to overfit. We propose generating an accuracy tabl e using multiple scores and by applying a smoothing method to the accuracy table as follows.
 STEP 1 Create cells for an accuracy table.
 STEP 2 Smooth accuracies.
 STEP 3 Estimate class membership probability for an evaluation sample.
Before describing the details of STEP 1-3, we explain the reason for using multiple scores. We assume that a multiple classifier outputs a score for each class. A predicted class is determined n ot by the absolute value of the score, but by the relative position among the scores because the predicted class for an evaluation sample is a class with the largest value in multiple scores. For example, even if the first-ranked score is low, as l ong as the second-ranked score is very low (the difference between the two classe s is large), the classification output will likely be reliable. In contrast, when the score for the first-ranked class is high, if the score of the second-ranked class is equally high (a negligible difference in score between the two classes), then the classification output is unreliable. Therefore, the class membership probabilities are likely to depend not just on the first-ranked score, but also on other scores. For effective calibration, it may be better to use not only the first-ranked score, but also other-ranked score. STEP 1. To generate an accuracy table, we need a pair of scores and clas-sification status (incorrect /correct) for each sample. To obtain these pairs, we divide the whole of the training dataset into two datasets: a) a training dataset to make  X  X n accuracy table X  and b) a test dataset for the table. We employed cross-validation. For example, in a 5-fold cross-validation, we divide the whole training data into five groups, and use four-fifths of the data to build a classifier with the remaining one-fifth of the data used to output scores from the classifier; we repeat the process four more times (a total of five times) by changing the data used for training and outputting the score to make an accuracy table.
We create cells for an accuracy table as f ollows. First, the score is used as an axis, divided into even intervals. For example, the size of an interval may be 0.1 on SVMs. In the case of using multiple scores, this step takes place for each score. When we use the first-ranked scores and the second-ranked scores, we split a rectangle up into several intervals. S econd, we decide, on the basis of the score, to which cell each training sample belongs. Finally, we check the classification status (correct/incorrect) of the training samples in each cell and calculate the accuracy of that cell, that is, its ratio o f correctly classified samples. In this method, an accuracy table can be made fo r any number of scores (dimensions) used because the training samples do n ot need be sorted according to their scores for create cells. However, this p roposed method has a similar problem as Zadrozny X  X  binning method in that we can discover the best size of cell intervals only by experiments. Furthermore, beca use the number of samples for a cell may be different, the reliabilities of accuracies in cells are probably variant. To solve the problem, we use coverage for each cell as weight.
 STEP 2. The original accuracy table generat ed above does not yield reliable probabilities (accuracies), when there are no or very few samples for some cells. Therefore, we propose smoothing on the o riginal accuracy table. There are sim-ple smoothing methods such as Laplace X  X  law (Laplace) and Lidstone X  X  law (Lid-stone) [6]. In this paper, we denote, for an observed cell c ( f )inwhich f is the classification score, the number of training data samples that appear in the cell by N ( c ( f )) and denote the number of correctly classified samples within all the samples in the cell by N p ( c ( f )). The smoothed accuracy P Lap ( f )isformulated added pseudo-counts. The value of  X  for Lidstone was determined for each accu-racy table, using cross-validation within the training data.

In both Laplace and Lidstone, accuracies are smoothed using solely the sam-ples in the cell in question. However, further examination of the entire accuracy table shows that nearby cells fairly often have similar accuracies. Therefore, us-ing the accuracies of cells near the tar get cell should be effective. We apply some smoothing methods such as a moving average method (MA) and a median method (Median) [1], which use values near the smoothing value target. The MA and Median are computed acco rding to the following formula: where Nb ( c ( f )) is the set of cells that are adjacent to cell c ( f ) whose accuracy can be defined (i.e., there is at least one sample), and n gives | Nb ( c ( f )) | +1. Furthermore, we propose an extended MA, the moving average with coverage method (MA cov), in which cells with many samples are more weighted in ac-curacy computation becaus e the accuracy of those cells are more reliable. where C ( c ( f )) is the number of the samples in the cell c ( f ) divided by the number of all the samples. In this paper, we simply use the cells directly neighboring the target cell as surrounding cells. For example, in the case of using the first-ranked score and the second-ranked score, we use nine cells; the up cell, down cell, left cell, right cell, the diagonal cells and the target cell.
 STEP 3. We first calculate the classification scores of the sample, then find the range or cell that corresponds to the scores, and output the values associated in the range or cell in the accuracy table. 4.1 Experimental Settings Classifier. We used SVMs, and also used the Naive Bayes classifier for exper-iments to show the generality of the proposed method. The reason why we se-lected SVMs is that SVMs are widely applied to many applications in document classification [4]. Since SVMs are a binary classifier, we used the one-versus-rest method [7] to extend SVMs to a multiple classifier 5 . Following Takahashi et al. [13], we set the SVMs kernel function to be a linear kernel.
 DataSet. We used two different datasets: th e JGSS dataset, which is Japanese survey data, and UseNet news articles (20 Newsgroups), which were also used in Zadrozny et al. X  X  experiments [17] 6 . First, we used the JGSS dataset (23,838 samples) taken from respondents who had occupations [13]. Each instance of the respondents X  occupation data consists of four answers:  X  X ob task X  (open-ended), and  X  X ndustry X  (open-ended), b oth of which consisted of much shorter texts than usual documents, and have approximately five words in each, and  X  X mployment status X  (close-ended), and  X  X ob title X  (close-ended). We used these features for learning. The number of categories was nearly 200 and by past occupation coding, each instance was encoded into a single integer value called an occupational code. We used JGSS-2000, JGSS-2001, JGSS-2002 (20,066 samples in total) for training, and JGSS-2003 (3,772 samples) for testing. The reason why we did not use cross-validation is that we would like to imitate the actual coding process; we can use the data of the past surveys, but not of future surveys. To generate an accuracy table, we used a 5-fold cross-validation within the training data; we split 20,066 samples into five subsets with equal size, used four of them for temporary training and the rest for outputting the pairs of the scores and the status, and repeated five times with different combinations of four subsets. We used all the pairs to make an accura cy table. The second dataset, (the 20 Newsgroups dataset), consists of 18,828 articles after duplicate articles are removed. The number of categories is 20, corresponding to different UseNet discussion groups [9]. We employed a 5-fold cross-validation. Cell Intervals. We experimentally determined the best cell intervals. For these experiments, we created some accuracy ta bles with different cell intervals: 0.05, 0.1, 0.2, 0.3, and 0.5 etc. For example, Table 1 shows the relationships of cell intervals and the number of cells in the case of the first-ranked scores used.
 Evaluation Metrics. We used the log-likelihood of test data to evaluate each method in Experiment 1. Larger values of log-likelihood are considered to be bet-ter. For simplicity, we use the negative log-likelihood 7 . As an evaluation method in Experiment 2, we used a reliability diagram, a ROC (receiver operating char-acteristic) curve, reliability for each coverage, accuracy for each threshold, and the ability to detect misclassified samples. 4.2 Experiment 1: Comparison of the Methods The Proposed Method for Creating Cells. Before Experiment 1, we con-ducted simple experiments to confirm the effectiveness of the proposed method for making cells with equal cell inter vals by comparing the proposed method with the method with equal samples for each cell. We used the values without smoothing and used only the first-ranked scores. Table 2 shows the results in the best cases by changing the number of cells from 7 to 60. The tendencies in other cases with the different number of cells were much the same as in Ta-ble 2. Thus, we confirmed the effectiveness of the proposed method for creating cells 8 .
 Evaluations by Log-Likelihood. Tables 3 and 4 show the negative log-likelihood of the JGSS dataset and the 20 Newsgroups dataset by the pro-posed methods as well as other methods for different numbers of used scores and different intervals on SVMs, respectively 9 . The Lidstone column shows the re-sult when the predicted optimal value of  X  is used. The dash (-) indicates that we cannot compute log-likelihood for those cases because the argument of the log function in some cells was 0. We discuss the results in Tables 3 and 4. First, for the SVMs, the best case for each dataset was the method using both the first-ranked score and the second-ranked score, in which we applied the moving average with coverage method (cell inter vals=0.1). Second, for each method we obtained better log-likelihood scores when we used multiple scores than when we used a single score. In particular, using both the first-ranked score and the second-ranked score was the best for both datasets. The reason is that in multi-class classification, the probability of the first-ranked class depends not only upon the first-ranked scores, but also upon the second-ranked scores as mentioned in Section 3. Third, in the case of using mul tiple scores with smaller cell intervals (e.g. 0.1), smoothing methods such as MA or MA cov, which use the accuracies of cells near the target cell, were more effective than the other methods.
To show the generality of the above conclusions, we conducted experiments of the same kind as in the above-mentioned experiments, except for Lidstone, using the Naive Bayes classifier for the 20 Newsgroups dataset. In Table 5, the dash (-) indicates the same meaning as in Tables 3 and 4. We obtained the same results as shown in Tables 3 and 4. First, the best of all cases was the method using both the first-ranked score and the second-ranked score, in which we smoothed by the Moving Average method with larger number of cells (e.g. 30). Second, for each method we obtained a better log-likelihood when we used multiple scores than when we used a single score. Third, in the case of using multiple scores with a larger number of cells (e.g. 30), smoothing methods such as MA or MA cov were effective.
Finally, we obtained results of the sigmoid method using multiple scores as shown in the right column in Tables 3 and 4. For expansion of the sigmoid function, we used the formula: P Log ( f 1 ,  X  X  X  ,f r )=1 / (1 + exp( r i =1 A i f i + B )), where f i represents the i th-ranked classification score. The parameters A i (  X  i ) and B are estimated with the maximum likelihood method. In the sigmoid method, we also showed that the values of log-likelihood were better when we used multiple scores than when we used a single score. The sigmoid method showed an average performance in the methods in the two tables in each dataset. 4.3 Experiment 2: Evaluation of the Proposed Method Reliability Diagram and ROC curve. We used the reliability diagram and the ROC curve to evaluate the proposed method. To plot a reliability diagram, we used an average of the estimates of the samples in each interval (e.g. [0 , 0 . 1]) as a predicted value ( x ) and the average of actual values corresponding to the estimates as a true value ( y ). Figure 1 shows three reliability diagrams in the JGSS dataset. In the proposed method, all points were near the diagonal line. In the reliability diagram, the farthe st a point is from the diagonal line, the worse the performance. As a whole, the proposed method was better than both a method without smoothing and the sigmoid method. This tendency was the same as in the 20 Newsgroups dataset. Figure 1 also shows three ROC curves in the 20 Newsgroups dataset. On a ROC curve, the nearer a ROC curve is to upper left line, the better a method is. The proposed method was the best of the three methods, and this tendency was the same as in the JGSS dataset.
We also investigated the predicted va lues by the proposed method and the actual values by increasing the coverage every 10% from 10% to 100%. Although there is a limitation such that both the predicted value and the actual value are not the values of the sample itself, the p redicted values are nearly the same as the actual values in descending order and ascending order in both datasets. Accuracy for each Threshold. If we could select samples accurately enough either to process or not to process using our own threshold, the work done by humans would be lighter. We investigat ed accuracies of the proposed method by increasing the thresho ld of estimates every 0 . 1from0to0 . 9. The proposed method always outperformed both a method without smoothing and sigmoid methods in both datasets. For example, when the threshold was set to 0 . 9, accuracies were approximately 96% in the JGSS dataset and 96% in the 20 Newsgroups dataset. As for coverage, the proposed method scored then second and first in the JGSS dataset and the 20 Newsgroups dataset, respectively. Ability to Detect Misclassified Samples. We ordered all the test instances by ascending order of the estimated class membership probability and counted the number of error samples in the set of samples with low probability. We compared our method with the raw score method [12], in which the distance from the separation hyperplane is directly used instead of the probability. We evaluated these methods by the ratio of the detected errors. Figure 2 shows the number of error samples detected by the proposed method and those by the raw score method in both datasets. The proposed method always surpassed the raw score method in each dataset. In the 20 Newsgroups dataset especially, the proposed method performed better when coverage was lower, which is desirable for us, since, in practice, we would like to find many errors by manually checking only a small amount of data. The reason for the difference of the two methods is not clear in the JGSS dataset. A possible explanation would be that the JGSS dataset has many very short samples, among which there are only a few active features. Those short samples do not have enough information for precise probability estimation. In this paper, to estimate class membership probabilities, we proposed using multiple scores outputted by classifiers, and generating an accuracy table with smoothing methods such as the moving average method or the moving aver-age with coverage method. Through the experiments on two different datasets with both SVMs and Naive Bayes classifiers, we empirically showed that the use of multiple classification scores was effective in the estimation of class mem-bership probabilities, and that proposed smoothing methods for the accuracy table worked quite well. Further researc h will be necessary to discover effective cell intervals. The use of information criteria such as AIC (Akaike Information Criteria) [11] is the next research area we intend to pursue.
 Acknowledgements. This research was partially supported by MEXT Grant-in-Aid for Scientific Research (c)6530341.

