 News sources on the Web generate constant streams of in-formation, describing many aspects of the events that shape our world. In particular, geography plays a key role in the news, and enabling geographic retrieval of news articles in-volves recognizing the textual references to geographic lo-cations (called toponyms ) present in the articles, which can be difficult due to ambiguity in natural language. Toponym recognition in news is often accomplished with algorithms designed and tested around small corpora of news articles, but these static collections do not reflect the streaming na-ture of online news, as evidenced by poor performance in tests. In contrast, a method for toponym recognition is presented that is tuned for streaming news by leveraging a wide variety of recognition components, both rule-based and statistical. An evaluation of this method shows that it outperforms two prominent toponym recognition systems when tested on large datasets of streaming news, indicating its suitability for this domain.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Design, Performance Toponym recognition, geotagging, streaming news News plays a large role in today X  X  information society. Thousands of newspapers all over the world publish a con-stant stream of many thousands of news articles every day  X  T his work was supported in part by the National Science Foundation under Grants IIS-10-18475, IIS-09-48548, IIS-08-12377, CCF-08-30618, and IIS-07-13501.
 to serve our need for news. The rise of the Internet has allowed their access from anywhere in the world through newspapers X  online presence, and has fueled intense compe-tition as evidenced by a swift and sometimes tempestuous news cycle. Blogs, tweets, and other social media have also expanded the realm of news to include citizen journalism. Our goal is to collect, analyze, and comprehend this stream-ing, ever-changing mass of information, to make it easily retrievable for humans. Specialized techniques are required to achieve this goal.

Importantly, news often has a strong geographic com-ponent. Newspapers often characterize their readership in terms of where their readers live, and include news articles describing events that are relevant to geographic locations of interest to their readers. Thus, in our database of news articles, we attempt to understand the geographic content present in each article, to enable retrieval queries with a geographic component. This process of understanding is known as geotagging of text, and amounts to identifying locations in natural language text, and assigning lat/long values to them. In this way, geotagging can be considered as enabling the spatial indexing of unstructured or semistruc-tured text. This spatial indexing provides a way to exe-cute both feature-based queries ( X  X here is X happening? X ) and location-based queries ( X  X hat is happening at location Y ? X ) [4]. Systems using geotagging have recently flourished and have been implemented in a wide variety of domains, such as web pages [3, 16, 21], blogs [22], encyclopedia ar-ticles [8, 27], Twitter messages [26], spreadsheets [2, 11], the hidden Web [12], and of most relevance for us, news articles [5, 7, 13, 14, 23, 25, 28]. The methods in this pa-per were developed for the NewsStand system [28], which uses a geotagger to associate clusters of news articles with the geographic locations mentioned in them, thereby en-abling users to explore the news visually on NewsStand X  X  interactive map interface. In addition, commercial prod-ucts for geotagging text are available, including MetaCarta X  X  Geotagger 1 , Thomson Reuters X  X  OpenCalais 2 , and Yahoo! X  X  Placemaker 3 , the latter two of which we investigate here.
The process of geotagging consists of finding all textual references to geographic locations, known as toponyms , and then choosing the correct location interpretation for each toponym (i.e., assigning lat/long values). These two steps, known respectively as toponym recognition (which we inves-tigate in this paper) and toponym resolution (also an im-portant problem, but not discussed in this work), are dif-http:// metacarta.com http://opencalais.com http://developer.yahoo.com/geo/placemaker ficult because of several kinds of ambiguity present in lo-c ation names. In particular, many names of places are also names of other type of entities, called geo/non-geo ambiguity (e.g.,  X  X tanley Jordan X ,  X  X ristol Palin X , and  X  X aris Hilton X  are persons, while  X  X ristol X ,  X  X aris X , and  X  X ordan X  are also toponyms), and many different places have the same name, called geo/geo ambiguity (e.g., over 40, 50, and 60 places around the world named  X  X ordan X ,  X  X ristol X , and  X  X aris X , respectively). These examples and others proved problem-atic when testing OpenCalais and Placemaker in our ex-perimental evaluation of toponym recognition (described in Section 4), and served as a motivation for our research.
Toponym recognition can be considered as a subset of a more general problem studied in natural language pro-cessing, called named-entity recognition (NER). Whereas to-ponym recognition involves finding entities in text that cor-respond to geographic location names, named-entity recog-nition involves finding locations, as well as entities of other types (e.g., names of people and organizations). In our ex-ample sentence X  X ordan visited London last Friday X , the out-put from a toponym recognizer would include the location  X  X ondon X , while correct output from a named-entity recog-nizer would also include  X  X ordan X  as a person, and possibly  X  X riday X  as a day of week. Sometimes evidence is stronger for a particular entity interpretation versus another inter-pretation. For example, in the pattern  X  X visited Y  X , the  X  X isited X  verb lends credence to X being a person and Y be-ing a location, since locations are visited by people. Machine learning-based NER systems will often discover patterns like these from corpora of entity-annotated documents, and use them to build a language model by which entities and entity types can be predicted, given the linguistic context. Given toponym recognition X  X  status as a subproblem of NER, tools developed for the more general problem of NER can be used for toponym recognition. In this case, the gen-eral strategy is to take an input document, execute an off-the-shelf NER system on the document (e.g., LingPipe 4 , Stanford NER 5 , ANNIE 6 ), and take the location entities. Once location entities are found, location interpretations are assigned from a gazetteer, and in the toponym reso-lution step, one of the interpretations is chosen for each to-ponym. However, this simple strategy is problematic. Be-cause NER is a more general problem, systems developed for NER tend to be tuned for this more general problem, rather than specifically for locations, so they may be less accurate in detecting locations. Also, when evaluating NER systems on our domain of news articles (described in Section 4), we found that they were biased toward precision at the expense of recall. This may be due to the small size and homogene-ity of corpora used in training NER systems, which do not capture the fast moving and ever changing nature of the news cycle. While this bias is not unacceptable for NER, it is problematic when used in a geotagging system, since toponym recognition upper-bounds recall for the entire geo-tagging process (i.e., toponym recognition and resolution). As a result, the low recall of these NER systems severely lim-its the entire geotagging process X  X  recall, and thus we saw the need for more comprehensive techniques.

Bearing these considerations in mind, the toponym recog-nition process we designed for processing streaming news has a considerably more flexible architecture. Our multi-http:// alias-i.com/lingpipe http://nlp.stanford.edu/ner http://gate.ac.uk/ie faceted toponym recognition process uses standalone NER software as only one of many recognition methods, of po-tentially varying quality. We include rule-based recognition in the form of entity dictionary tables, cue word matching (e.g.,  X  X County X ), and toponym refactoring. In addition, we leverage statistical NLP tools in the form of NER soft-ware with postprocessing filters, and part-of-speech (POS) tagging with additional recognition rules. At the end of the entire procedure, we attempt to reconcile entity types, and establish groups of entities to be resolved concurrently, by grouping textually similar entities together. Essentially, we designed this multifaceted toponym recognition procedure in keeping with our goals to be flexible enough to capture variations that occur in streaming news, as well as to be as all-inclusive as possible when recognizing toponyms, in or-der to maximize the procedure X  X  recall (i.e., to miss as few toponyms as possible). Our toponym resolution procedure, described by Lieberman et al. [14], serves to restore preci-sion to the process by dropping supposed toponyms with no supporting evidence for any of their possible interpreta-tions, as evidenced by the higher overall precision reported by Lieberman et al. [14].

In summary, our key contributions include: Our recognition procedure can be broken into two stages. First, we generate an initial set of possible entities using many sources of evidence (Section 2). Second, we execute a variety of postprocessing filters that attempt to resolve en-tity types using additional forms of evidence (Section 3). We also incorporated our toponym recognition method into the NewsStand system and evaluated it by comparing it against two state-of-the-art competing systems (Section 4).
Our first step is to look for a curated, small set of well-known locations and other entities appearing in the docu-ment X  X  text, which serves as a convenient baseline for to-ponym recognition. This set of entities is gathered from several tables in our gazetteer, which is based on GeoN-ames 7 , and is updated and kept current on a daily basis. In particular, we collect lists of continents, countries, and top-level administrative divisions (e.g., states, provinces), and search for them among the document X  X  tokens. In addition, we search for common abbreviations for all of the above (e.g.,  X  X alifornia X  can be abbreviated as  X  X alif. X  or  X  X A X ). We also search for demonyms , which are words used to refer http:// geonames.org to people from a particular place (e.g.,  X  X erman X ,  X  X ary-l ander X ). Demonyms, while not locations proper, have some aspect of location that can be useful in recognizing and re-solving toponyms, in that the location they represent can contribute to an overall sense of locality for the document. We iterate over the document X  X  tokens, looking for groups of tokens that match an entry in an entity table, and if we find such a match, we create an entity of the corresponding type. For location entities, we also associate each entity with the proper location interpretation from the gazetteer.
Next, we recognize additional entities of many and var-ied types by using an entity dictionary, containing names of entities that commonly appear in the news. We use this dictionary to recognize both toponyms and non-toponyms, because knowing that a particular entity strongly refers to a non-toponym is useful in resolving geo/non-geo ambiguities. For example, knowing that  X  X pple X  is a famous brand name allows us to discount the possibility that  X  X pple X  refers to a small city in Ohio, in the absence of strong evidence. In addition to particular instances of entities, the entity dictio-nary also contains many cue word patterns which serve as keywords to identify entities of various types. For example, the phrase  X  X ounty of X  strongly indicates that one or more following tokens corresponds to a location. We search for entities and cue words among the document X  X  tokens, and collect matches as entities. For cue words in the dictionary, we search for adjacent capitalized tokens as the correspond-ing entity. Our entity dictionary was constructed by observ-ing the output from our toponym recognition and resolution processes and checking for recognition errors, to discover which geo/non-geo ambiguities proved most problematic in our domain. The entity dictionary is by no means com-plete, but it serves as a useful starting point for a toponym recognition process in the news domain. In addition, as we discover new sources of ambiguity, the dictionary is updated with new classes of entities, so it is always evolving.
Table 1 contains a set of entity types and examples of en-tities and entity cue words present in the entity dictionary. All examples shown in the table are also names of various locations around the world, indicating the high degree of geo/non-geo ambiguity present in toponyms. In addition, we added many different forms of spatial cues to account for minor variations in how the cue words are used. For ex-ample, both  X  X Lake X  and  X  X ake X  X  are common variants of the  X  X ake X  cue. Universities are another special case be-cause of the many ways in which they are specified in text, especially with multi-campus university systems. For ex-ample,  X  X niversity of Maryland at College Park X  might be written  X  X niversity of Maryland, College Park X ,  X  X niversity of Maryland in College Park X ,  X  X niversity of Maryland X  College Park X , or other similar ways. Each of these variants are encoded into the entity dictionary X  X  matching rules.
Next, we use a POS tagger to find proper noun phrases, which are useful in recognizing locations because locations tend to consist of proper nouns. We search for sequences of proper noun tokens, and consider them as locations. In ad-dition, because our tokenizer considers possessive forms (i.e.,  X  X  X  X ) and hyphens as distinct tokens, we include these ele-ments in location names if they connect sequences of proper nouns as well. These are useful for capturing locations such as  X  X rince George X  X  County X , in which  X  X  X  X  separates the proper noun sequences  X  X rince George X  and  X  X ounty X . In addition, we also consider simple prepositional modifiers as proper noun separators, which will capture phrases such as  X  X niversity of Texas at Arlington X . For each proper noun phrase we find, we add an entity of type X  X roper noun phrase X  to the entity pool for this document, since we cannot deter-mine a more specific type using POS tags alone. We use TreeTagger 8 , a decision tree-based POS tagger, trained on the Penn TreeBank corpus.

Obviously, not all proper noun phrases are locations, so this technique will be underprecise for toponym recognition in that it will capture many noun phrases that are not loca-tions, such as names of people, organizations, and other en-tities. However, despite its lack of precision, finding proper noun phrases is consistent with our goal of high recall X  X hat is, not missing any locations present in the document. At this stage of processing, we are not overly concerned with precision in location recognition, since that will be restored in the toponym resolution step, where erroneous location interpretations will be filtered.
As a final toponym recognition method, we leverage tools developed to address the problem of named-entity recogni-tion (NER). NER seeks to discover typed entities present in an input text, which usually includes at a minimum entities such as people, organizations, and importantly, locations. As noted earlier, NER methods have their limitations when used for toponym recognition, due to NER being a more general problem. However, in keeping with our philosophy of multifaceted toponym recognition, we include NER in our toponym recognition procedure. As an NER package, we use the Stanford NLP Group X  X  NER and IE package, which is built around a conditional random field (CRF) classifier. We used the language model included with the Stanford NER distribution, a three-class classifier to find persons, organi-zations, and locations, which was obtained by training on a mixture of CoNLL, MUC-6 and MUC-7, and ACE corpora.
We feed the article text to the NER system, and save the person, organization, and location entities into our en-tity pool. To avoid frequently noisy output entities, we only keep the entities that have a minimum score of 0.95. One observation is that this NER method captured similar enti-ties as found by collecting proper noun phrases (described in Section 2.3), a result which is not overly surprising as named entities tend to consist of proper nouns. However, using the NER system offers the benefit of determining entity types, in addition to simply finding entities. Knowing entity types helps to avoid geo/non-geo errors, as non-location entities can generally be disregarded.
 Rather than simply using the output entities from the NER system directly, we perform a number of postprocess-ing steps that serve to avoid some common pitfalls with which the Stanford NER system has trouble. These post-processing steps are executed sequentially and act as entity filters. For example, we found that some output entities were fragmented, in that the boundaries were chosen in-correctly, erroneously including or excluding nearby tokens, and we created filters to address this and other problems. Each filter is described below. Note that scores and score thresholds mentioned in each filter X  X  description correspond to scores assigned by the Stanford NER package. http:// www.ims.uni-stuttgart.de/projekte/corplex/ TreeTagger indicating the high level of geo/non-geo ambiguity in location names.
The following sections contain examples of entities pre-s ented within their textual context. For ease of presentation, we visually distinguish these entities using brackets. For ex-ample, in the text  X  X n [ LOC College Park], the mayor. . .  X ,  X  X 
LOC College Park] X  X efers to the entity under consideration, while the surrounding text serves as context. Capturing the distinction between entity and context will be important for several filters described below.
Oftentimes, the NER system will find an entity in the proper context, but select the entity boundaries incorrectly. For example, it may select  X  X quatorial [ LOC Guinea] X  rather than the correct X  X  LOC Equatorial Guinea] X . In this example, the selected entity was correct, but the boundaries were not correct. Furthermore, the specific context in which an entity was found can effect how the NER system selects boundaries for the entity. In other words, the NER system may extract e  X  X  LOC Equatorial Guinea] X  in one part of the document, and e 2  X  X quatorial [ LOC Guinea] X  in another, simply due to the linguistic context in which e 1 and e 2 were found. This filter attempts to correct these fragmentation errors by ex-panding entity boundaries using other entities found in the text. In particular, we try to expand entities that are sub-strings of other entities. In our example, we can expand e ( X  X uinea X ) to  X  X quatorial Guinea X  because the e 2  X  X  preced-ing token,  X  X quatorial X , matches the initial portion of e Note that we do not expand across sentence boundaries.
In general, to accomplish this entity boundary expansion, we search for entities that are substrings of other entities. We say that an entity e 1 dominates another entity e 2 if e is a substring of e 1 . First, we group entities together based on domination, so that entities which are substrings of each other are grouped together. Algorithm 1 provides a pseu-docode listing for this procedure, named GroupEntities . The output for GroupEntities is a set of entity buckets B , with each bucket b containing a set of entities, one of which dominates all entities in b and is designated b  X  X  bucket head , and is denoted head ( b ). After initializing the set of output buckets B (line 2), we sort the entities in decreas-ing order of length (3). We iterate over each entity E i and bucket B k  X  X ote that initially, since | B | = 0, the inner loop is not entered when i = 1 (4 X 16). First, we check whether head ( B k ) dominates E i , and if so, we add E i to B k Algorithm 1 G roup entities according to dominance. 1: p rocedure GroupEntities ( E ) 2: Initialize set of entity buckets B  X  X } 3: Sort entities E by decreasing length 4: for i  X  1 . . . | E | do 5: for k  X  1 . . . | B | do 8: break to next i 12: break to next i 13: end if 14: end for 16: end for 17: return B 18: end procedure Otherwise, if E i d ominates head ( B k ), we add E i to B and set head ( B k )  X  E i (11), since the dominance prop-erty is transitive and hence E i will also dominate all entities in B k . If we find an appropriate bucket b for E i , we con-tinue with E i +1 ; otherwise, we create a new bucket b with head ( B k ) = E i , and add b to B (15). Eventually, all entities in E will have been placed into appropriate bucket.
Now that entities have been grouped into buckets based on dominance, we can attempt to expand entities within buckets. We implemented two strategies for entity expan-sion, which we term strict and loose expansion. Put simply, strict expansion means that we only expand entities in a bucket b if they contain enough nearby tokens so that they can be expanded to match head ( b ). On the other hand, loose expansion attempts to expand each entity in b using other entities in b . In particular, we compare each entity e  X  b to each longer entity e  X   X  b in order of decreasing length, and we expand e to e  X  if the proper nearby tokens exist that make it equivalent to e  X  .

The advantage of strict expansion is that it ensures greater accuracy for expanded entities, since if expansion succeeds, it is unlikely that the expanded entity is erroneous, due to the larger number of tokens required for a successful expan-sion. However, strict expansion X  X  major drawback is that the head entity of each entity bucket may be unique in the doc-ument, affording no opportunity to correct fragmentation e rrors present in entities in the bucket. That is, simply be-cause an entity is long does not make it very relevant for the document as a whole. For example, consider a document where the NER system collected entities e 1  X  X  LOC College Park] X  (correct), e 2  X  X ollege [ LOC Park] X  (incorrect), and e  X  X 
ORG College Park X  X  Fire Department] X  (correct). All these entities would be placed in the same entity bucket, with e 3 as the bucket head. Under strict expansion, each of e 1 and e 2 would be compared with e 3 only. Neither would be expanded, which is fine for e 1 , but e 2 would remain unex-panded and erroneous, since it could not be expanded to match e 3 . However, under loose expansion, in addition to a comparison with e 3 , e 2 would be compared to e 1 and hence would be correctly expanded due to the appropriate preced-ing token  X  X ollege X . To capture more of these cases, we use loose expansion in our entity expansion filter.
One problem with NER systems is that entity types may be chosen incorrectly X  X ven for multiple instances of the same entity in the same document X  X ue to differences in the way that entities are referred to. For example, an article may initially mention the person  X  X aul Washington X , and simply  X  X ashington X  later, though both refer to the same person. While the first can easily be recognized as a person due to the presence of both a given name and surname, the second entity may be incorrectly typed as location because it only consists of a surname that is also a common loca-tion name. Articles can also refer to people by their given name alone, especially when mentioning childrens X  names or the names of celebrities, since referring to a person by their given name reflects a higher level of familiarity or empathy. At times, organization names may also be typed incorrectly, as in  X  X ia Motor Cars X  which is frequently referred to as simply  X  X ia X . The former is more easily recognized as an organization than the latter, which may be mistaken for a person X  X  name or even a location.

This filter attempts to correct these typing errors for frag-ments of larger entities found elsewhere in the document. The filter proceeds by selecting source entities from which entity types will be propagated. The selected entities in-clude person entities consisting of at least two tokens (given name and surname), and organization entities consisting of at least three tokens. Furthermore, only entities with scores of above 0.90 are selected, ensuring high quality among the source entities. After selecting the source entities, the first and last tokens are taken from each entity and associated with the source from which they were taken. Finally, entity types are propagated to low scoring entities by searching for entities with scores below 0.60 and containing one of the to-kens extracted above. If such an entity e contains one of the tokens t , e  X  X  type is set to the type of the entity from which t was taken. This procedure captures given names and sur-names of person entities, as well as the primary portion of organization names. Because only the first and last tokens of each source entity are matched, the filter allows for partial matching of entities, which is useful given the NER system X  X  penchant for entity fragmentation.
After finding entities using a combination of the methods described above, we proceed with a sequence of filters that act as postprocessing to remove potential errors. Filters are applied in the order listed and are described in detail below. Table 2: Entity names modified by the name refactoring fil-ter. Cue words are expanded and shifted within the entity to generate new query names for each entity. Arrows indicate the match and action performed for each pattern.

O ftentimes, location names can be referred to in multi-ple ways. For example, locations of a particular type such as  X  X ounty X  often have the word  X  X ounty X  in their names. However, the position of  X  X ounty X  in the location name can vary by locale. For example, in the US,  X  X ounty X  often ap-pears as a suffix, as in  X  X rince George X  X  County X . However, counties of Ireland often feature  X  X ounty X  as a prefix, as in  X  X ounty Kildare X . In addition, abbreviations of  X  X ounty X  such as  X  X o. X  are not uncommon in news articles. Further-more, a specific type of spot location frequently mentioned in local newspapers are local public and private schools, and these may be written in any number of ways (e.g.,  X  X alter Johnson HS X ,  X  X alter Johnson High X ). This filter X  X  purpose is to account for these entity name variations, and refac-tor entity names to generate extra query names that will be matched properly in our gazetteer. The filter contains a list of regular expressions to match against entity names, and if a match is made, suitable substitutions are performed.
Table 2 contains some of the entity name patterns that are refactored by this filter. The patterns fall into four main classes: prefix abbreviations, suffix abbreviations, suf-fix shifting, and school expansion. In prefix and suffix ex-pansion, common abbreviations used in location names are expanded. For example,  X  X t. Meade X  would be expanded to  X  X ort Meade X . For suffix shifting, location suffixes such as  X  X ounty X  are shifted before and after the main location name, so a location such as  X  X ounty Kildare X  would be ex-panded to  X  X ildare County X  and  X  X ounty of Kildare X . Fi-nally, school expansion searches for partial names of schools, which are indicated by a school name and a school type key-word, such as  X  X rimary X ,  X  X iddle X ,  X  X S X , or  X  X igh X . Note that the filter may erroneously match and expand query names for entities that are not locations. For example,  X  X o. X  is also a common abbreviation of X  X ompany X  X nd as such fre-quently appears in business names. Thus,  X  X ord Motor Co. X  will be incorrectly expanded to  X  X ord Motor County X . How-ever, this erroneous expansion will not be overly problematic as it is in keeping with our goal of high recall in toponym recognition. That is, having erroneous query names such as  X  X ord Motor County X  will not cause problems because they will be corrected by the toponym resolution procedure, ei-ther by not being present in the gazetteer, or by having little evidence for such interpretations.
T o distinguish between toponyms and other types of en-tities, we note that many entities tend to be active , in that they perform actions (e.g., people, organizations), while lo-cations tend to be passive , in that they do not. For example, it would make sense for a person to X  X ay X  X omething, while in general it would not for a location to  X  X ay X  something. More generally, the grammatical subject of an active voice verb can be thought of as performing the action described by the verb. We can leverage the part of speech tags assigned by the POS tagger to find entities that perform actions, which in turn disqualifies them from being toponyms.

To find active entities, the filter searches for entities fol-lowed by an active voice verb, or by an adverb and an active voice verb. In this way, the method effectively performs a shallow parsing of the sentence. For each such entity of type  X  X OC X  (location), the type is reset to  X  X NPP X  (proper noun phrase). In other words, the entity is no longer considered as a location. Note that this method does not provide evidence for a particular entity type X  X .g., determining whether such an entity is a person or organization. However, since we are primarily concerned with distinguishing between toponyms and non-toponyms, this lack of evidence can be overlooked.
One caveat with this method is that it does not properly account for metonymy associated with toponyms. Metonyms are a frequent occurrence in articles about, for example, lo-cal or international politics, where a government may be referred to by the city of its primary geographic presence. For example,  X  X ashington X , literally meaning  X  X ashington, DC X , is often used metonymically to refer to the US gov-ernment, an organization which can be considered an ac-tive entity. As a result, in a sentence such as  X  X ashington stated on Monday. . .  X ,  X  X ashington X  would be disqualified as a toponym. However, we note that repeated instances of  X  X ashington X  X ould likely provide a means of correcting this error, as metonymic references are relatively uncommon in text [10]. As a result, additional instances of  X  X ashington X  would not likely be metonymic, and could be used to correct the earlier error through a voting scheme. Alternatively, we could incorporate a metonymy recognition method into this filter, such as that proposed by Leveling and Hartrumpf [10].
Sometimes, the correct interpretation of toponym evidence itself is in question. For example, consider a sentence begin-ning:  X  X n Russia, U.S. officials. . .  X  In this sentence, both  X  X ussia X  and  X  X .S. X  refer to countries. However, consider that the form  X  X ussia, U.S. X  might be mistaken for a par-ticularly common form of evidence termed object/container evidence, which can be briefly described as a pair of to-ponyms, one of which contains the other in a geographic sense. Considering this evidence interpretation, we might erroneously think that the phrase  X  X ussia, U.S. X  might refer to any of several populated places named Russia in the US in New York, New Jersey, or Ohio.

To help clear up this evidential ambiguity, we can use ev-idence by taking note of another grammatical concept, that of the noun adjunct . Noun adjuncts are nouns that function as adjectives by modifying other nearby nouns. In our exam-ple sentence,  X  X .S. X  is a proper noun adjunct that modifies the plain noun  X  X fficials X . Because of its primary connection with  X  X fficials X  through the noun adjunct relationship, us-ing it in object/container evidence would not be warranted. By detecting noun adjuncts, we prevent toponyms acting as noun adjuncts from participating in other filters used in toponym resolution. We detect them by finding entities fol-lowed by a plain noun.
Having grouped entities into equivalence classes in the pre-vious step, we can now leverage these entity groups to im-prove the overall quality of entity and toponym recognition. Note that in a group of entities as determined above, some entities will have more specific types than others, due to the heterogeneous nature of our toponym recognition methods. For example, entities found using the POS tagger (i.e., se-lecting proper nouns, described in Section 2.3) will have an unknown type, while entities found using the NER system (described in Section 2.4) will have more specific types. We can propagate entity types within each group to make the types within a group consistent, in a similar fashion as was done for the NER system X  X  postprocessing. Having consis-tent entity types is useful because though the entities in a group have the same referent, the context in which each en-tity reference appears differs. To propagate entity types, we examine entity types within each group. If a group g con-tains untyped entities as well as entities all of a single type t , we set the untyped entities to type t . However, if there are more than one type of entities in g , the types are not prop-agated. Compared with a simple type voting scheme (e.g., setting the types of all entities in a group to the most fre-quent entity type), this scheme ensures a high quality of type propagation, since conflicts disqualify type propagation.
We incorporated our own toponym recognition methods into the NewsStand system [28], and compared with those of two prominent competitors: Thomson Reuters X  X  OpenCalais and Yahoo! X  X  Placemaker. Although both OpenCalais and Placemaker are closed-source commercial products, and do not make public how they work, they provide public Web APIs which allow for automated geotagging of documents, with relatively liberal rate limits. As a result, they have been used extensively in state-of-the-art geotagging and entity recognition research (e.g., [1, 17, 22, 29, 31]). Placemaker provides a toponym recognition service, while OpenCalais performs recognition of toponyms, and recognition of other types of entities as well. In addition, both OpenCalais and Placemaker are full geotagging systems X  X hat is, they per-form toponym resolution as well. While toponym resolution is an important problem in its own right, in this work, we are only concerned with toponym recognition. As a result, even though OpenCalais and Placemaker assign lat/long val-ues to each toponym reported as output, we disregard these lat/long values in our evaluation. In other words, we use OpenCalais and Placemaker in their toponym recognition capacity only, and do not include toponym resolution in their performance scores. Also note that at the time of writ-ing, neither OpenCalais nor Placemaker offered a means of tuning the precision/recall balance, so we could not explore this aspect of the systems. From our experimental results described in Section 4.4, it appears that these systems are tuned for precision, but we could not verify this over a range of precision/recall values due to lack of tuning capability.
We continue with a description of existing geotagging cor-pora used in related work (Section 4.1). Next, we measure toponym statistics in a large collection of news gathered by NewsStand, as measured by our own toponym recognition method as well as OpenCalais and Placemaker (Section 4.2). Then, we describe a new corpus of hand-annotated news ar-Table 3: Corpora used in geotagging-related research, show-i ng sources, and document and toponym counts. Note that document and toponym counts refer to annotated counts, not total counts.
 ticles created from NewsStand X  X  constantly streaming news d ata (Section 4.3). We conclude with accuracy measure-ments for all three methods in two corpora of annotated news articles, as well as in streaming news (Section 4.4).
To get a sense of the corpora used in geotagging research, we present Table 3, which contains a listing of researchers and the corpora they used in their geotagging-related re-search. For each corpus, we give the source and total number of annotated documents and toponyms. In some cases, the exact numbers of documents and toponyms were difficult to determine due to lack of detail. Also note that the sources listed in the table were often used by multiple researchers, and here we present only an example usage of each source. The table reveals the relatively small sizes of annotated cor-pora used in geotagging research, with the number of anno-tated documents and annotated toponyms having averages of about 436 and 3348 and maxima near 1000 and 7000, respectively. These numbers stand in stark contrast to the huge volume of news retrieved by NewsStand in just a single day, which is roughly 40k documents and 250k toponyms. Furthermore, most corpora include articles from only one or two news sources, usually newswire, which amounts to a heavily biased sample, given the variety and number of news sources and writing styles all over the world.

However, one commonality that is apparent from the val-ues in Table 3 is that the average number of toponyms in each article is remarkably consistent, with each article hav-ing 7 X 8 toponyms with few exceptions. This range is es-pecially prevalent for corpora consisting of news articles, which is our domain of interest. One exception includes the Wikipedia corpus of Overell and R  X  uger [19], with an average of 1.4 toponyms per article. However, Overell and R  X  uger only considered toponyms in each article that also corre-spond to links to other Wikipedia pages; since generally only the first instance of an entity mentioned in an article is linked, this explains the seeming lack of toponyms. Another anomalous measure is the 15.1 toponyms per article reported by Roberts et al. [24], which are likely due to the consider-ation of locations nested within other entities as toponyms (e.g.,  X  X  LOC New York] Police Department X ). Another, un-fortunate commonality among the corpora used in geotag-ging research is that most are unavailable due to copyright restrictions, thereby making direct algorithmic comparisons on the same data generally not possible. In addition, a bet-ter measure for how frequently toponyms occur in text would Table 4: Counts of articles, distinct sources, and geotagged toponyms for several days X  worth of news, sampled at differ-ent time periods.
 be the ratio of toponyms to words, which would better ac-c ount for variations in news article length. However, this data was not often presented by authors. Nonetheless, 7 X 8 toponyms serves as a useful rule of thumb for the number of toponyms expected in articles of reasonable length.
Now that we have characterized typical toponym counts in news articles, we wish to determine whether NewsStand X  X  geotagger has performance that approaches our expectations in terms of toponym recall. To measure performance, we sampled seven days X  worth of news from various days in November 2010, and executed NewsStand X  X  geotagger on the news articles collected on each day. The days were chosen randomly, except we ensured that we had at least one of each day of the week, to account for the typically lower volume of news published on weekends. We collected articles from news feeds that published at least five arti-cles on each sampling day, to ensure a measure of consis-tency among the collected data. Furthermore, we limited the sampling to articles with at least a word count of 300, which ensures a reasonable minimum length for the news articles and served to filter out erroneously-processed doc-uments (e.g., articles that had been improperly extracted from their HTML source). Sampling in this fashion resulted in filtering out about half of each day X  X  articles.
For each set of sampled articles, we tabulated the total number of toponyms recognized by NewsStand X  X  toponym recognition process. Table 4 reflects these counts.  X  X ources X  indicates the number of sampled news sources from which sampled articles were taken. For each day, we include the to-tal number of toponyms reported by our recognition method that have at least one interpretation in our gazetteer. The last column contains the toponym-document fraction X  X he number of toponyms with gazetteer interpretations over the number of sampled articles containing those toponyms.
We make several interesting observations from these statis-tics. First, and most importantly, we see that the majority of sampled days have toponym fractions between 7.2 and 7.5, which fall precisely in our expected range of 7 X 8 to-ponyms. The outliers of 9.3 and 10.5 are not totally unex-pected given that they were measured on weekends which imply a different pattern of news publication. Overall, the measured toponym fractions are strong indications that our toponym recognition method identifies an appropriate num-ber of toponyms. Next, in examining the number of articles and sources, the numbers show that our sampling resulted in a large number of articles from a variety of sources on each day, which demonstrates the extreme variety in our ar-ticle samples. This stands in contrast to the small size and homogeneity of corpora used in previous geotagging-related research (described in Section 4.1), and the large number Table 5: Corpora used in evaluating recognition accuracy. and variety of articles lends weight to the credence of our m easured toponym fractions.

This evaluation method can be easily applied to very large collections of articles, making it ideal for continual testing of performance on streaming and ever-changing collections of news. Of course, it says nothing of how many of the toponyms are correct, which is addressed in Section 4.4.
We used two corpora in our evaluation. For the first cor-pus, we used LGL , introduced by Lieberman et al. [14], which consists of 621 articles from 114 local newspapers, with a total of 4765 annotated toponyms. The goal in creat-ing LGL was to create a collection of news from smaller news sources, rather than the major news sources typically used in creating article corpora, since the former significantly out-number the latter on the Web. As a result, LGL is useful for testing the accuracy of our toponym recognition method for a variety of smaller news sources. However, it does not cap-ture the larger, major news stories that are often described and published in multiple news sources. Note that these ma-jor news stories naturally form clusters in NewsStand, and it is not unusual to have clusters of 100, 200, or even 1000 articles for especially major and ongoing news stories.
To capture these stories, we created another corpus con-sisting of sizable clusters of news articles found by News-Stand, and termed Clust . To create Clust, we selected clus-ters that had sizes of 5 X 100 articles, and contained articles from at least four unique news sources. The clusters were sampled between January and April 2010. This sampling strategy ensures reasonable cluster sizes which allows for enough variation among articles in the cluster. Further-more, having multiple news sources ensures that different news sources are used, rather than many copies of the same article everywhere, which might result from erroneous pre-processing. In total, we sampled 1080 clusters containing a total of 13327 news articles, from 1607 distinct news sources. For each cluster, we randomly selected one article for manual annotation, resulting in 1080 annotated articles containing 11962 toponyms, with a median of 8 toponyms per article. Because multiple news sources and by extension their audi-ences are represented in each cluster, we expect the stories in Clust to have more journalistic impact, as well as a wider geographic significance, than the stories in LGL.
Table 5 summarizes and compares statistics for the LGL and Clust corpora. Clust has roughly twice as many anno-tated articles, and toponyms, as LGL. However, the most striking difference between LGL and Clust is the compo-sition of toponym types in each corpus. Since LGL was created as a corpus of articles from smaller newspapers, and Clust as a corpus of larger news stories, we expect the to-ponyms in LGL to correspond to smaller places, and those in Clust to correspond to larger places. The type statistics in Table 5 reflect these expectations. Nearly half of anno-tated toponyms in LGL correspond to cities, and of those toponyms, two-thirds are cities under 100k population. On the other hand, Clust, consisting of larger news stories, has only 33% of toponyms corresponding to cities, and of those toponyms, nearly two-thirds are cities over 100k population. In addition, the fractions of country and state toponyms in Clust are larger than those in LGL, while the fraction of county toponyms in LGL is larger than those in Clust. These measurements reflect our motivations for using LGL and creating Clust, and show that these corpora, used to-gether, allow for an effective evaluation on both smaller and larger news stories from a variety of news sources.
Having established the credibility of our two evaluation corpora, we are ready to examine our toponym recognition method X  X  accuracy and compare its performance to that of OpenCalais and Placemaker. For each of NewsStand, Open-Calais, and Placemaker, we consider two versions of each method: the original algorithm, referred to as, e.g.,  X  X S X , and the original algorithm with a postprocessing filter that removes output toponyms if they have no interpretations in our gazetteer, denoted with a subscript G , e.g.,  X  X S G  X . By doing so, we can determine the effect of using a gazetteer on toponym recognition, as well as characterize to some ex-tent the gazetteers used by OpenCalais and Placemaker. To measure performance, we use the well-known measures pre-cision and recall , which for a set of ground truth toponyms G and a set of system-generated toponyms S , are defined as
I n addition, we consider two different criteria for deter-mining whether a ground truth toponym g matches a system-generated toponym s . The first, termed exact matching, states that g and s are equivalent if the starting and end-ing offsets of each are equal. The second, termed overlap matching, relaxes this criterion by allowing g and s to sim-ply overlap in their offset ranges for them to match. Both are useful in characterizing the performance of toponym recogni-tion. Exact matching could be considered the gold standard for measuring performance. However, overlap matching is sometimes necessary to avoid improper penalization due to gazetteer differences and other factors. For example, con-sider a ground truth toponym  X  X  LOC New York state] X  and system-generated  X  X  LOC New York] state X , which is correct, but is not an exact match and is an overlap match. Over-lap matching serves a similar purpose as methods such as BLEU [20], in that partial matches are not overly penalized. We measured all algorithms X  performance over both the LGL and Clust corpora. Table 6 contains results for the LGL corpus. NewsStand, OpenCalais, and Placemaker are referred to as  X  X S X ,  X  X C X , and  X  X M X , respectively. In ad-dition, for | G  X  S | , P , and R , exact and overlap matching are reported as two numbers in the table in  X  X /O X  form ( | S | is unaffected by the matching method used). Compar-ing NewsStand against OpenCalais and Placemaker reveals that both NewsStand variants greatly outperform the com-Table 6: Toponym recognition performance in the LGL cor-p us ( | G | = 4765). In all cases, the NewsStand variants have highest toponym recall. Table 7: Toponym recognition performance in the Clust cor-p us ( | G | = 11564). As with LGL, NewsStand had highest recall. petition in terms of toponym recall, having at least 0.10 an d in some cases 0.20 or higher recall over OpenCalais and Placemaker, when measured using both exact and overlap matching. NewsStand X  X  high recall comes at the expense of toponym precision; however, remember that in NewsStand, toponym recognition is only considered one stage of an inte-grated geotagging process, and toponym precision is restored by later stages of processing. The gazetteer postprocessing done for NewsStand G demonstrates this effect, dramatically improving precision with little corresponding decrease in re-call. In addition, as mentioned earlier, our geotagging pro-cedure is based on that of Lieberman et al. [14], who report a precision over 0.80 and correspondingly high recall for LGL, thus showing that precision is indeed restored.

Examining OpenCalais X  X  and Placemaker X  X  performance, we can see that these methods are much more biased toward toponym precision at the expense of recall, which is taken to the extreme in the case of OpenCalais (i.e., at least 50% less than NewsStand). Note that NewsStand and Placemaker are comparable in terms of F 1 scores (harmonic mean of precision and recall), while OpenCalais X  X  is lower, illustrat-ing the potential precision/recall tradeoff. Also, performing gazetteer postprocessing for OpenCalais G has little effect, while for Placemaker G , a significant boost in precision is noted using exact matching, along with a significant decrease in recall when using overlap matching. These results seem-ingly indicate that Placemaker X  X  toponym matching rules differ from our own. Examining differences between exact and overlap matching, we see that NewsStand and Place-maker are significantly affected by allowing overlap matches, while OpenCalais and all the gazetteer-filtered algorithms (i.e., NewsStand G , OpenCalais G , Placemaker G ) are mostly unaffected. For NewsStand, this is likely due to dropping many non-toponyms that were selected by NewsStand X  X  fil-ters (e.g., proper noun phrases not present in the gazetteer). Comparing | S | of NewsStand and NewsStand G , a very large number of toponyms were dropped by the gazetteer filtering, which accounts for the hefty precision increase. For Place-maker, gazetteer and matching differences can account for the performance difference. Figure 1: Toponym recognition performance on the Clust c orpus measured over time.
 Table 7 contains performance results for the Clust corpus. The NewsStand algorithms again outperform the competi-tion in terms of recall, by an even larger margin than was seen for LGL, while OpenCalais and Placemaker are tuned for toponym precision. In addition, examining differences between LGL and Clust, we see that the performances scores for Clust are generally higher across all algorithms than the corresponding scores in LGL, with the only exception being Placemaker X  X  recall. This difference indicates that in some sense, Clust X  X  toponyms are easier to recognize than those of LGL, likely due to the greater presence of large, easily recognized toponyms such as country names.
We have shown that NewsStand X  X  multifaceted toponym recognition procedure has a high recall for articles from both small, local news sources (LGL) as well as larger, better-known sources (Clust). However, measuring performance over an entire static corpus does not well reflect day-to-day toponym recognition performance on a constant stream of news data. To better characterize day-to-day performance, we split the Clust corpus into weekly samples of articles, and measured precision and recall for NewsStand G using overlap matching over each sample. Effectively, this test determines whether the NewsStand method would perform well if executed within that time range.

Figure 1 shows the performance of our toponym recog-nition procedure on the Clust corpus, measured over time. Performance in terms of both precision and recall is rela-tively consistent over all time periods tested, with a mean of 0.739 precision and 0.868 recall. In addition, the standard deviations of precision and recall are 0.029 and 0.018, which serve as further evidence of the method X  X  performance sta-bility. These results indicate that the NewsStand toponym recognition process is well suited for streaming news.
We have introduced a multifaceted toponym recognition method that is especially suited for the streaming news do-main, which poses special challenges. In particular, stream-ing news is constantly in motion and ever-changing, which advises against the sole use of methods based on static cor-pora of news. Our recognition method involves many sources of evidence, and in our evaluation, was shown to outper-form the competition in terms of toponym recall, the crucial measure of success. In future work, we plan to perform a more in-depth investigation of the individual components of toponym recognition used in our procedure, to determine t heir overall utility, as well as their performance for spe-cific classes of toponyms. We also plan to investigate our heuristics X  use within machine learning techniques such as coreference analysis [18] to determine their suitability in this domain. As more news sources move online, algorithms like ours which are tailored for streaming news will be vital to handle the resulting data deluge. [1] R. Abascal-Mena and E. L  X opez-Ornelas. Geo informa-[2] M. D. Adelfio, M. D. Lieberman, H. Samet, and K. A. [3] E. Amitay, N. Har X  X l, R. Sivan, and A. Soffer. Web-a-[4] W. G. Aref and H. Samet. Efficient processing of win-[5] D. Buscaldi and B. Magnini. Grounding toponyms in [6] D. Buscaldi and P. Rosso. A conceptual density-based [7] E. Garbin and I. Mani. Disambiguating toponyms in [8] W. Kienreich, M. Granitzer, and M. Lux. Geospatial [9] J. L. Leidner. An evaluation dataset for the toponym [10] J. Leveling and S. Hartrumpf. On metonymy recogni-[11] M. D. Lieberman and J. Lin. You are where you edit: [12] M. D. Lieberman, H. Samet, J. Sankaranarayanan, and [13] M. D. Lieberman, H. Samet, and J. Sankaranarayanan. [14] M. D. Lieberman, H. Samet, and J. Sankaranarayanan. [15] D. Manov, A. Kiryakov, B. Popov, K. Bontcheva, [16] B. Martins, H. Manguinhas, and J. Borbinha. Extract-[17] B. Martins, I. Anast  X acio, and P. Calado. A machine [18] A. McCallum and B. Wellner. Conditional models of [19] S. E. Overell and S. R  X  uger. Using co-occurrence models [20] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. [21] R. S. Purves, P. Clough, C. B. Jones, A. Arampatzis, [22] T. Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang. An ef-[23] G. Quercini, H. Samet, J. Sankaranarayanan, and M. D. [24] K. Roberts, C. A. Bejan, and S. Harabagiu. Toponym [25] H. Samet, B. E. Teitler, M. D. Adelfio, and M. D. [26] J. Sankaranarayanan, H. Samet, B. Teitler, M. D. [27] J. Str  X  otgen, M. Gertz, and P. Popov. Extraction and ex-[28] B. E. Teitler, M. D. Lieberman, D. Panozzo, J. Sankara-[29] R. Tobin, C. Grover, K. Byrne, J. Reid, and J. Walsh. [30] R. Volz, J. Kleb, and W. Mueller. Towards ontology-[31] A. Weichselbraun. A utility centered approach for eval-
