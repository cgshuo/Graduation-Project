 Katherine A. Heller heller@gatsby.ucl.ac.uk Sinead Williamson saw56@cam.ac.uk Zoubin Ghahramani zoubin@eng.cam.ac.uk Engineering Department, University of Cambridge, Cambridge, UK The idea of partial membership is quite intuitive and practically useful. Consider, for example, an individ-ual with a mixed ethnic background, say, partly Asian and partly European. It seems sensible to represent that individual as partly belonging to two different classes or sets. Such a partial membership represen-tation may be relevant to predicting that individual X  X  phenotype, or their food preferences. We clearly need models that can coherently represent partial member-ship.
 Note that partial membership is conceptually very dif-ferent from uncertain membership. Being certain that a person is partly Asian and partly European, is very different than being uncertain about a person X  X  ethnic background. More information about the person, such as DNA tests, could resolve uncertainty, but cannot make the person change his ethnic membership. Partial membership is also the cornerstone of fuzzy set theory. While in traditional set theory, items ei-ther belong to a set or they don X  X , fuzzy set theory equips sets with a membership function  X  k ( x ) where 0  X   X  k ( x )  X  1 denotes the degree to which x partially belongs to set k .
 In this paper we describe a fully probabilistic approach to data modelling with partial membership. Our ap-proach makes use of a simple way of representing par-tial membership using continuous latent variables. We define a model which can cluster data but which fun-damentally assumes that data points can have par-tial membership in the clusters. Each cluster is repre-sented by an exponential family distribution with con-jugate priors (reviewed in section 3). Our model can be seen as a continuous latent variable relaxation of clustering with finite mixture models, and reduces to mixture modelling under certain settings of the hyper-parameters. Unlike Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and Mixed Membership models (Ero-sheva et al., 2004), which also capture partial mem-bership in the form of attribute-specific mixtures, our model does not assume a factorization over attributes and provides a general way of combining exponential family distributions with partial membership. The complete specification of our model is provided in sec-tion 4. Learning and inference are carried out using Markov chain Monte Carlo (MCMC) methods. We show in particular that because all the parameters in our model are continuous, it is possible to employ a full hybrid Monte Carlo (HMC) algorithm, which uses gra-dients of the log probability, for inference (section 5). Our Bayesian Partial Membership (BPM) model bears interesting relationships to several well-known mod-els in machine learning and statistics, including LDA (Blei et al., 2003), mixed membership models (Ero-sheva et al., 2004), exponential family PCA (Collins et al., 2002), and Discrete Components Analysis (Bun-tine &amp; Jakulin, 2006). We discuss these relations in section 6, where we also relate our model to fuzzy k-means. In section 7, we present both synthetic and real-world experimental results using image data and voting patterns of US senators. We conclude with fu-ture work in section 8. We can derive our method for modeling partial mem-berships from a standard finite mixture model. In a finite mixture model the probability of a data point, x n given  X , which contains the parameters for each of the K mixture components (clusters) is: where p k is the probability distribution of mixture component k , and  X  k is the mixing proportion (frac-tion of data points belonging to) for component k 1 . Equation 1 can be rewritten using indicator variables  X  n = [  X  n 1  X  n 2 ... X  nK ] as follows: where  X  nk  X  { 0 , 1 } and P k  X  nk = 1. Here we can notice that if  X  nk = 1 this means that data point n belongs to cluster k (also p (  X  nk = 1) =  X  k ). Therefore the  X  nk denote memberships of data points to clusters. In order to obtain a model for partial memberships we can relax the constraint  X  nk  X  X  0 , 1 } to now allow  X  nk to take any continuous value in the range [0 , 1]. How-ever, in order to compute the probability of the data under this continuous relaxation of a finite mixture model, we need to modify equation 2 as follows: The modifications include integrating over all values of  X  n instead of summing, and since the product over clusters K from equation 2 no longer normalizes we put in a normalizing constant c , which is a function of  X  n and  X . Equation 3 now gives us a model for partial membership.
 We illustrate the difference between our partial mem-bership model and a standard mixture model in figure 1. Here we can see contours of the Gaussian distri-butions which can generate data in the mixture model (left) and the partial membership model (right), where both models are using the same two Gaussian clusters. As an example, if one of these clusters represents the ethnicity  X  X hite British X  and the other cluster repre-sents the ethnicity  X  X akistani X , then the figure illus-trates that the partial membership model will be able to capture someone of mixed ethnicity, whose features may lie in between those of either ethnic group (for ex-ample skin color or nose size), better than the mixture model. In the previous section we derived a partial member-ship model, given by equation 3. However we have not yet discussed the form of the distribution for each cluster, p k ( x n |  X  k ), and we will now focus on the case when these distributions are in the exponential family. An exponential family distribution can be written in the form: p k ( x n |  X  k ) = exp { s ( x n ) &gt;  X  k + h ( x n ) + g (  X  where s ( x n ) is a vector depending on the data known as the sufficient statistics ,  X  k is a vector of natu-ral parameters , h ( x n ) is a function of the data, and g (  X  k ) is a function of the parameters which ensures that the probability normalizes to one when integrat-ing or summing over x n . We will use the short-hand x n  X  Expon(  X  k ) to denote that x n is drawn from an exponential family distribution with natural parame-ters  X  k .
 If we plug the exponential family distribution (equa-tion 4) into our partial membership model (equation 3) it follows that: where x n comes from the same exponential family dis-tribution as the original clusters p k , but with new nat-ural parameters which are a convex combination of the natural parameters of the original clusters,  X  k , weighted by  X  nk , the partial membership values for data point x n . Computation of the normalizing con-stant c is therefore always tractable when p k is in the exponential family.
 A probability distribution p (  X  k ) is said to be conju-gate to the exponential family distribution p ( x n |  X  k p (  X  k | x n ) has the same functional form as p (  X  k ). In particular, the conjugate prior to the above exponen-tial family distribution can be written in the form: where  X  and  X  are hyperparameters of the prior. We will use the short-hand,  X   X  Conj(  X  , X  ) . We now have the tools to define our Bayesian partial membership model. Consider a model with K clusters, and a data set D = { x n : n = 1 ...N } . Let  X  be a K -dimensional vector of positive hyperparameters. We start by draw-ing mixture weights from a Dirichlet distribution: Here  X   X  Dir(  X  ) is shorthand for p (  X  |  X  ) = malization constant which can be expressed in terms of the Gamma function 2 . For each data point, n , we draw a partial membership vector  X  n which represents how much that data point belongs to each of the K clusters: The parameter a is a positive scaling constant drawn, for example, from an exponential distribution p ( a ) = be  X  ba , where b&gt; 0 is a constant. We assume that each cluster k is characterized by an exponential family distribution with natural parameters  X  k and that Given all these latent variables, each data point is drawn from In order to get an intuition for what the functions of the parameters we have just defined are, we return to the ethnicity example. Here, each cluster k is an eth-nicity (for example,  X  X hite British X  and  X  X akistani X ) and the parameters  X  k define a distribution over fea-tures for each of the k ethnic groups (for example, how likely it is that someone from that ethnic group likes pizza or marmite or bindi bhaji). The parame-ter  X  gives the ethnic composition of the population (for example, 75%  X  X hite British X  and 25%  X  X ak-istani X ), while a controls how similar to the popu-lation an individual is expected to be (Are 100% of the people themselves 75%  X  X hite British X  and 25%  X  X akistani X ? Or are 75% of the people 100%  X  X hite British X  and the rest are 100%  X  X akistani X ? Or some-where in between?). For each person n ,  X  n gives their individual ethnic composition, and finally x n gives their individual feature values (e.g. how much they like marmite). The graphical model representing this generative process is drawn in Figure 2.
 Since the Bayesian Partial Membership Model is a gen-erative model, we tried generating data from it us-ing full-covariance Gaussian clusters. Figure 3 shows the results of generating 3000 data points from our model with K = 3 clusters as the value of parameter a changes. We can see that as the value of a increases data points tend to have partial membership in more clusters. In fact we can prove the following lemmas: Lemma 1 In the limit that a  X  0 the exponential family BPM is a standard mixture model with K com-ponents and mixing proportions  X  .
 Lemma 2 In the limit that a  X   X  the exponential family BPM model has a single component with natural parameters P k  X  k  X  k .
 Proofs of these lemmas follow simply from taking the limits of equation 8 as a goes to 0 and  X  respectively. We can represent the observed data set D as an N  X  D matrix X with rows corresponding to x n , where D is the number of input features. 3 Let  X  be a K  X  D matrix with rows  X  k and  X  be an N  X  K matrix with rows  X  n . Learning in the BPM consists of inferring all unknown variables,  X  = {  X  ,  X  ,  X  ,a } given X . We treat the top level variables in the graphical model in Figure 2,  X  = {  X  ,  X  , X ,b } as fixed hyperparameters, although these could also be learned from data. Our goal is to infer p (  X  | X ,  X  ), for which we decide to em-ploy Markov chain Monte Carlo (MCMC).
 Our key observation for MCMC is that even though BPMs contain discrete mixture models as a special case, all of the unknown variables  X  of the BPM are continuous. Moreover, it is possible to take deriva-tives of the log of the joint probability of all variables with respect to  X  . This makes it possible to do infer-ence using a full Hybrid Monte Carlo (HMC) algorithm on all parameters. Hybrid (or Hamiltonian) Monte Carlo is an MCMC procedure which overcomes the random walk behaviour of more traditional Metropo-lis or Gibbs sampling algorithms by making use of the derivatives of the log probability (Neal, 1993; MacKay, 2003). In high dimensions, this derivative information can lead to a dramatically faster mixing of the Markov chain, analogous to how optimization using derivatives is often much faster than using greedy random search. We start by writing the probability of all parameters and variables 4 in our model: We assume that the hyperparameter  X  = 1, and omit it from our derivation. Since the forms of all distri-butions on the right side of equation (11) are given in section 4, we can simply plug these in and see that: log  X ( P k  X  k )  X  P k log  X (  X  k ) + P k (  X  k  X  1) log  X  + log b  X  ba + N log  X  ( P k a X  k )  X  N P k log  X ( a X  k ) The Hybrid Monte Carlo algorithm simulates dynam-ics of a system with continuous state  X  on an en-ergy function E (  X  ) =  X  log p ( X ,  X  |  X  ). The deriva-tives of the energy function  X  E (  X  )  X   X  ) provide forces on the state variables which encourage the system to find high probability regions, while maintaining detailed balance to ensure that the correct equilibrium distri-bution over states is achieved (Neal, 1993). Since  X  has constraints, e.g. a&gt; 0 and P k  X  k = 1, we use a tranformation of variables so that the new state vari-ables are unconstrained, and we perform dynamics in this unconstrained space. Specifically, we use a = e  X  ,  X  valid in this new space, the chain rule needs to be ap-plied to the derivatives of E , and the prior needs to be transformed through the Jacobian of the change of variables. For example, p ( a ) da = p (  X  ) d X  implies p (  X  ) = p ( a )( da/d X  ) = ap ( a ). We also extended the HMC procedure to handle missing inputs in a princi-pled manner, by analytically integrating them out, as this was required for some of our applications. More details and general pseudocode for HMC can be found in (MacKay, 2003). The BPM model has interesting relations to several models that have been proposed in machine learning, statistics and pattern recognition. We describe these relationships here. Latent Dirichlet Allocation: Using the notation introduced above, the BPM model and LDA (Blei et al., 2003) both incorporate a K -dimensional Dirich-let distributed  X  variable. In LDA,  X  n are the mix-ing proportions of the topic mixture for each docu-ment n . Each word in document n can then be seen as having been generated by topic k , with probability  X  nk , where the word distribution for topic k is given by a multinomial distribution with some parameters,  X  . The BPM also combines  X  nk with some exponen-tial family parameters  X  k , but here the way in which they are combined does not result in a mixture model from which another variable (e.g. a word) is assumed to be generated. In contrast, the data points are in-dexed by n directly, and therefore exist at the doc-ument level of LDA. Each data point is assumed to have come from an exponential family distribution pa-rameterized by a weighted sum of natural parameters  X  , where the weights are given by  X  n for data point n . In LDA, data is organized at two levels (e.g. docu-ments and words). More generally, mixed membership (MM) models (Erosheva et al., 2004), or admixture models, assume that each data attribute (e.g. words) of the data point (e.g. document) is drawn indepen-dently from a mixture distribution given the member-ship vector for the data point, x nd  X  P k  X  nk P ( x |  X  LDA and mixed membership models do not average natural parameters of exponential family distributions like the BPM. LDA or MM models could not generate the continuous densities in figure 3 from full-covariance Gaussians. The analagous generative process for MM models is given in figure 4. Since data attributes are drawn independently, the original clusters (not explic-ity shown) are one dimensional and have means at 0, 10 and 20 for both attribute dimensions. We can no-tice from the plot that this model always generates a mixture of 9 Gaussians, which is a very different be-havior than the BPM, and clearly not as suitable for the general modeling of partial memberships. LDA only makes sense when the objects (e.g. documents) being modelled constitute bags of exchangeable sub-objects (e.g. words). Our model makes no such as-sumption. Moreover, in LDA and MM models there is a discrete latent variable for every sub-object corre-sponding to which mixture component that sub-object was drawn from. This large number of discrete latent variables makes MCMC sampling in LDA potentially much more expensive than in BPM models.
 Exponential Family PCA: Our model bears an interesting relationship to Exponential Family PCA (Collins et al., 2002). EPCA was originally formu-lated as the solution to an optimization problem based on Bregman divergences, while our model is a fully probabilistic model in which all parameters can be in-tegrated out via MCMC. However, it is possible to think of EPCA as the likelihood function of a proba-bilistic model, which coupled with a prior on the pa-rameters, would make it possible to do Bayesian in-ference in EPCA and would render it closer to our model. However, our model was entirely motivated by the idea of partial membership in clusters, which is enforced by forming convex combinations of the nat-ural parameters of exponential family models, while EPCA is based on linear combinations of the param-eters. Therefore: EPCA does not naturally reduce to clustering, none of the variables can be interpreted as partial memberships, and the coefficients define a plane rather than a convex region in parameter space. The recent work of Buntine and Jakulin (Buntine &amp; Jakulin, 2006) focusing on the analysis of discrete data is also closely related to the BPM model. The frame-work of (Buntine &amp; Jakulin, 2006) section III B ex-presses a model for discrete data in terms of linear mixtures of dual exponential family parameters where MAP inference is performed. Section V B also pro-vides insights on differences between using dual and natural parameters.
 Fuzzy Clustering: The notion that probabilistic models are unable to handle partial membership has been used to argue that probability is a subtheory of or different in character from fuzzy logic (Zadeh, 1965; Kosko, 1992). In this paper we described a probabilis-tic model for partial membership which may be of use in the many application domains where fuzzy cluster-ing has been used.
 Fuzzy K-means clustering (Bezdek, 1981) itera-tively minimizes the following objective: J = X rameter,  X  nk represents the degree of membership of data point n in cluster k ( P k  X  nk = 1), and d 2 ( x n , c is a measure of squared distance between data point x n and cluster center c k . By varying  X  it is possi-ble to attain different amounts of partial membership, where the limiting case  X  = 1 is K-means with no partial membership. Although the  X  parameters rep-resent partial membership, none of the variables have probabilistic interpretations.
 IOMM: Lastly, this work is related to the Infi-nite Overlapping Mixture Model (IOMM) (Heller &amp; Ghahramani, 2007) in which overlapping clustering is performed, also by taking products of exponential fam-ily distributions, much like products of experts (Hin-ton, 1999). However in the IOMM the memberships of data points to clusters are restricted to be binary, which means that it can not model partial member-ship. We generated a synthetic binary data set from the BPM, and used this to test BPM learning. The syn-thetic data set had 50 data points which each have 32 dimensions and can hold partial memberships in 3 clusters. We ran our Hybrid Monte Carlo sampler for 4000 iterations, burning in the first half. In or-der to compare our learned partial membership assign-ments for data points ( X  L ) to the true ones ( X  T ) for this synthetic data set, we compute (  X  U =  X  L  X  &gt; L ) and ( U of cluster membership shared between each pair of data points, and is invariant to permutations of clus-ter labels. Both of these matrices can be seen in figure 5. One can see that the structure of these two ma-trices is quite similar, and that the BPM is learning the synthetic data reasonably. For a more quantita-tive measure table 5c gives statistics on the number of pairs of data points whose learned shared membership differs from the true shared membership by more than a given threshold (the range of this statistic is [0,1]). We also used the BPM to model two  X  X eal-world X  data sets. The first is senate roll call data from the 107th US congress (2001-2002) (Jakulin, 2004), and the second is a data set of images of sunsets and towers. The senate roll call data is a matrix of 99 senators (one senator died in 2002 and neither he nor his replacement is included) by 633 votes. It also includes the outcome of each vote, which is treated as an additional data point (like a senator who always voted the actual out-come). The matrix contained binary features for yea and nay votes, and we used the BPM to cluster this data set using K = 2 clusters. There are missing val-ues in this dataset but this can easily be dealt with in the HMC log probability calculations by explicitly rep-resenting both 0 and 1 binary values and leaving out missing values. The results are given in figure 6. The line in figure 6 represents the amount of membership of each senator in one of the clusters (we used the  X  X emo-crat X  cluster, where senators on the far left have partial memberships very close to 0, and those on the far right have partial memberships extremely close to 1). Since there are two clusters, and the amount of member-ship always sums to 1 across clusters, the figure looks the same regardless of whether we are looking at the  X  X emocrat X  or  X  X epublican X  cluster. We can see that most Republicans and Democrats are tightly clustered at the ends of the line (and have partial memberships very close to 0 and 1), but that there is a fraction of senators (around 20%) which lies somewhere rea-sonably in between the extreme partial memberships of 0 or 1. Interesting properties of this figure include the location of Senator Jeffords who left the Republi-can party in 2001 to become an independent who cau-cused with the Democrats. Also Senator Chafee who is known as a moderate Republican and who often voted with the Democrats (for example, he was the only Re-publican to vote against authorizing the use of force in Iraq), and Senator Miller a conservative Democrat who supported George Bush over John Kerry in the 2004 US Presidential elections. Lastly, it is interesting to note the location of the Outcome data point, which is very much in the middle. This makes sense since the 107th congress was split 50-50 (with Republican Dick Cheney breaking ties), until Senator Jeffords became an Independent at which point the Democrats had a one seat majority.
 We also tried running both fuzzy k-means clustering and Dirichlet Process Mixture models (DPMs) on this data set. While fuzzy k-means found roughly simi-lar rankings of the senators in terms of membership to the  X  X emocrat X  cluster, the exact ranking and, in par-ticular, the amount of partial membership (  X  n ) each senator had in the cluster was very sensitive to the fuzzy exponent parameter, which is typically set by hand. Figure 7a plots the amount of membership for the Outcome data point in black, as well as the most extreme Republican, Senator Ensign, in red, and the most extreme Democrat, Senator Schumer, in blue, as a function of the fuzzy exponent parameter. We can see in this plot that as the assignment of the Outcome data point begins to reach a value even reasonably close to 0.5, the most extreme Republican already has 20% membership in the  X  X emocrat X  cluster. This re-duction in range does not make sense semantically, and presents a trade-off between finding reasonable values for  X  n in the middle of the range, versus at the ex-tremes. This kind of sensitivity to parameters does not exist in our BPM model, which models both ex-treme and middle range values well.
 We tried using a DPM to model this data set where we ran the DPM for 1000 iterations of Gibbs sampling, sampling both assignments and concentration parame-ter. The DPM confidently finds 4 clusters: one cluster consists solely of Democrats, one consists solely of Re-publicans, the third cluster has 9 of the most moderate Democrats and Republicans plus the  X  X ote outcome X  variable, and the last cluster has just one member, Hollings (D-SC). Figure 7b is a 100x100 matrix show-ing the overlap of cluster assignments for pairs of sen-ators, averaged over 500 samples (there are no changes in relative assignments, the DPM is completely confi-dent). The interpretation of the data provided by the DPM is very different from the BPM model X  X . The DPM does not use uncertainty in cluster membership to model Senators with intermediate views. Rather, it creates an entirely new cluster to model these Sena-tors. This makes sense for the data as viewed by the DPM: there is ample data in the roll calls that these Senators are moderate  X  it is not the case that there is uncertainty about whether they fall in line with hard-core Democrats or Republicans. This highlights the fact that the responsibilities in a mixture model (such as the DPM) cannot and should not be interpreted as partial membership, they are representations of un-certainty in full membership. The BPM model, how-ever, explicitly models the partial membership, and can, for example, represent the fact that a Senator might be best characterized as moderate (and quan-tify how moderate they are). In order to quantify this comparison we calculated the negative log predictive probability (in bits) across senators for the BPM and the DPM (Table 1). We look at a number of different measures: the mean, median, minimum and maximum number of bits required to encode a senator X  X  votes. We also look at the number of bits needed to encode the  X  X utcome X  in particular. On all of these measures except for maximum, the BPM performs better than the DPM, showing that the BPM is a superior model for this data set.
 Lastly, we used the BPM to model images of sunsets and towers. The dataset consisted of 329 images of sunsets or towers, each of which was represented by 240 binary simple texture and color features. Partial assignments to K = 2 clusters were learned, and figure 8 provides the result. The top row of the figure is the three images with the most membership in the  X  X un-set X  cluster, the bottom row contains the three images with the most membership in the  X  X ower X  cluster, and the middle row shows the 3 images which have closest to 50/50 membership in each cluster (  X  nk  X  0 . 5). In this dataset, as well as all the datasets described in this section, our HMC sampler was very fast, giving reasonable results within tens of seconds.
 In summary, we have described a fully probabilistic approach to data modelling with partial membership using continuous latent variables, which can be seen as a relaxation of clustering with finite mixture models. We employed a full Hybrid Monte Carlo algorithm for inference, and our experience with HMC has been very positive. Despite the general reputation of MCMC methods for being slow, our model using HMC seems to discover sensible partial membership structure after surprisingly few samples.
 In the future we would like to develop a nonparamet-ric version of this model. The most obvious way to try to generalize this model would be with a Hierarchi-cal Dirichlet Process (Teh et al., 2006). However, this would involve averaging over infinitely many poten-tial clusters, which is both computationally infeasible, and also undesirable from the point of view that each data point should have non-zero partial membership in only a few (certainly finite) number of clusters. A more promising alternative is to use an Indian Buffet Process (Griffiths &amp; Ghahramani, 2005), where each 1 in a row in an IBP sample matrix would represent a cluster in which the data point corresponding to that row has non-zero partial membership, and then draw the continuous values for those partial memberships conditioned on that IBP matrix.
 References
