 Aggregating search results from a variety of heterogeneous sources or verticals such as news, image and video into a sin-gle interface is a popular paradigm in web search. Although various approaches exist for selecting relevant verticals or optimising the aggregated search result page, evaluating the quality of an aggregated page is an open question. This paper proposes a general framework for evaluating the qual-ity of aggregated search pages. We evaluate our approach by collecting annotated user preferences over a set of ag-gregated search pages for 56 topics and 12 verticals. We empirically demonstrate the fidelity of metrics instantiated from our proposed framework by showing that they strongly agree with the annotated user preferences of pairs of sim-ulated aggregated pages. Furthermore, we show that our metrics agree with the majority user preference more often than the current diversity-based information retrieval met-rics. Finally, we demonstrate the flexibility of our framework by showing that personalised historical preference data can improve the performance of our proposed metrics.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Measurement, Experimentation aggregated search, evaluation, performance metric, diversity
With the emergence of various vertical search engines ded-icated to certain media types and genres, such as news, im-age, video, it is becoming popular to present results from a set of specific verticals dispersed throughout the standard  X  X eneral web X  results, for example by adding image results to the ten blue links for the query  X  X ictures of flowers X . This new search paradigm is often known as aggregated search [4]. The three main challenges that arise in realising such sys-tems are vertical selection ( VS ), item selection ( IS ), and result presentation ( RP ). Vertical selection deals with de-ciding which verticals are implicitly intended by a query. Item selection deals with selecting a subset of items from each vertical to present on the aggregated page. Result pre-sentation deals with organising and embedding the various types of items on the result page. The most common presen-tation strategy for aggregated search is to merge the results into one ranked list of so-called blocks , and is now the  X  X e facto X  standard in many search engines.

Although various approaches exist for selecting relevant verticals [4, 5] and for optimising aggregated search pages [2, 17], evaluating the quality of aggregated search pages is still a challenge. Consider the query X  X oga poses X  which sug-gests that a visual element in the result page would be useful to many users. Furthermore consider that 75% of users who issue this query would prefer  X  X mage X  results, 60% would prefer  X  X ideo X  results, and 10% would prefer  X  X ews X  results, to  X  X eneral web X  results. Figure 1 shows three possible ag-gregated search pages 1 (A, B, and C) for the sample query. It is clearly difficult to objectively ascertain the aggregated search page that represents a more effective returned set, as there are a variety of compounding factors that could af-fect a user preference. A user may prefer a page because of his/her preference towards a specific vertical (vertical pref-erence). In such a case, a user may prefer page A because it contains more images. A user who prefers a result set with more items that are topically relevant might prefer page C, whereas a user who prefers more relevant items towards the top of the page (presentation preference) might prefer page B. Furthermore, a user who desires a more diverse returned set (vertical diversity) may prefer page C. Any combination of those factors can influence the perceived quality and user preference of the pages.

In this paper, we propose a general framework for instan-tiating metrics that can evaluate the quality of aggregated search pages in terms of both reward and effort . Specifically, we develop an approach that uses both topical-relevance and vertical-orientation information to derive the utility of any
R and N represent a Relevant or Non-relevant result re-spectively. given aggregated search page. Our approach is flexible and takes into account any combination of items retrieved, any combination of verticals selected, and the positions of those results on the presented page.

This paper makes several contributions: (i) we propose a framework for the evaluation of aggregated search pages that capture both effort and reward in a formal way; (ii) we outline a novel approach for simulating aggregated search pages and collect a large set of user preferences over page pairs; (iii) we demonstrate the effectiveness of the metrics derived from our framework by comparing them with several existing IR metrics; and (iv) we show that our metrics can be personalised for each user and, therefore, can be further improved using training data.

Related work is reviewed in Section 2. In Section 3, we formally outline the problem of aggregated search evalua-tion and list the assumptions made in this work. In Section 4, we propose a general framework from which we derive a number of metrics. A method for collecting the page pref-erences of users is outlined in Section 5. Subsequently in Section 6, these are used to evaluate the performance of our metrics against baseline ones. We also show how the perfor-mance of our metrics can be improved using training data. Conclusions and future work are discussed in Section 7.
Various works evaluating one component of an aggregated search system in isolation exist. Vertical selection in aggre-gated search has been studied in [4, 5, 15, 26]. Much of this research aims to measure the quality of the set of se-lected verticals, compared with an annotated set obtained by collecting manual labels from assessors [4, 5, 26] or de-rived from user interaction data [15]. The annotation can be binary [4, 5] or graded [26]. The quality of a particular ver-tical selection approach is mostly evaluated with standard measures of precision and recall using the binary annotated set. Our work also evaluates this key component by util-ising graded vertical-orientation information derived from a multi-assessor preferred vertical annotation set [26], as this allows for a more refined evaluation scheme.

Recent attempts to evaluate the utility of the whole ag-gregated search page [3, 17] consider the three key com-ponents of aggregated search ( VS , IS , RP ) together. Our work takes a similar holistic approach and proposes a general evaluation framework for measuring aggregated search page quality. For example, [17] evaluate the utility of a page based on a user engagement metric (CTR). This evaluation frame-work requires large-scale user interaction data, which may not always be available. In addition, it is not feasible to col-lect user interaction data for all possible page combinations. Others [6] evaluate the utility of the page by asking annota-tors to make assessments based on a number of criteria (e.g. relevance, diversity). Although this work is a comprehen-sive way to evaluate aggregated pages, it remains costly to gather assessments for all possible aggregated pages.
The most similar work [3] to ours collects preferences on block pairs from users and measures the page quality by cal-culating the distance between the page in question and the ideal (reference) page; the shorter the distance, the better the page. One advantage is that any possible combination of vertical blocks that form an aggregated page can be tested, from a block-oriented point of view (without regard to item selection). However, when the results retrieved for a vertical (block) change, the assessments previously gathered may not be reusable, as the preference will undoubtedly change ac-cordingly. As our work follows the Cranfield paradigm, once the assessments (both item topical-relevance and vertical-orientation) are gathered, it can be applied to evaluate any possible aggregated search page (any combination of vertical selection, item selection and result presentation). Therefore, our work leads to a more robust, inexpensive, and reusable approach for evaluating aggregated search pages.

Topical diversity is an important topic. Various diversity-aware IR metrics have been proposed [8, 10, 18], captur-ing the importance of each subtopic, the degree to which an item represents the subtopic, and the topical-relevance of the item. Diversity-based metrics can promote returned sets that are both topically relevant and diverse. A simplistic way of adapting these metrics to aggregated search is to treat subtopics as verticals and subtopic importance as vertical-orientation. In this way, all existing diversity-based IR met-rics can be adapted to evaluate aggregated search. Although in principle suitable to evaluate aggregated search, diversity-based metrics are not appropriate for use with block-based pages where user behaviour is different; for instance user browsing behavior within a block containing images may be different to that within a block containing  X  X eneral web X  re-sults. Furthermore, the various types of items (text, image, etc.) that need to be accounted for in an aggregated search scenario are not explicitly modelled in diversity-based met-rics. For example, the effort in reading a piece of text is g reater than the effort in viewing a picture. Our framework is better adapted to the task of aggregated search, and mod-els all key components simultaneously.

Others [20] have proposed an aggregated search metric that captures both vertical diversity and topical diversity. It can be noted that the framework developed in this paper can also be extended to incorporate topical diversity, but due to space limitations, we will leave this as future work.
We introduce some formal notation and outline some of the main assumptions used throughout this work.
An aggregated search page P is composed of a set of blocks { B 1 , B 2 , ...B n } , where each block B i consists of a set of items { I i 1 , I i 2 , ...I im } . An item can be a  X  X eneral web X  page or a vertical result. Only snippets of each item appear on the ag-gregated search page. We make several assumptions 2 about the page P : (i) results are presented into blocks from top to bottom, and within each block, items are shown either from left to right (Image, Video) or from top to bottom (News, Recipe); (ii) each block B i consists of items originating from only one vertical; (iii) only one block of each type is placed on a page (with the exception of  X  X eneral web X  blocks); and (iv) a block consists of one  X  X eneral web X  item or k vertical items. This is different to previous work [3, 17] where ver-tical block could be embedded into only three positions of  X  X eneral web X  results (top of the page, middle of the page, bottom of the page). We relax this assumption and allow a vertical block to be slotted between any two  X  X eneral web X  blocks on the page.
Our objective is to develop metrics that measure the qual-ity of any possible aggregated search page. The metrics must work regardless of the selected verticals, the items retrieved from each vertical, and where the vertical results are posi-tioned on the page. To achieve this, we assume that the following two types of relevance assessments are available:
The two relevance assessments are assumed to be made in-dependently. The concept of vertical-orientation [26] reflects the perceived usefulness of a vertical from the user perspec-tive prior to viewing vertical results and without regard to the quality of the vertical results. The vertical-orientation assessment is obtained by comparing each vertical in turn to
T hese assumptions are made in accordance with existing aggregated search systems.
In this work, we assume that topical-relevance assessments are binary. the reference  X  X eneral web X  X ertical, by asking users whether items from this vertical are likely to improve the quality of a standard web page. Consequently, the vertical orienta-tion of the Web ( orient ( W | W, q )) is deemed to be 0 . 5, as we can imagine that a user would randomly select a page when presented with two similar  X  X eneral web X  pages. The topical-relevance assessment of each item contributes to the measurement of relevance for each retrieved result. This type of assessment can be made using similar pooling tech-niques [13] to those used in TREC.

With these two assessment types, we assume that a user obtain the highest reward by reading the most topically rel-evant item, originating from the most highly oriented ver-tical, first. With this assumption, only the vertical (or verticals) with a higher orientation than the  X  X eneral web X  ( orient ( V | W, q ) &gt; 0 . 5) should be presented on the aggre-gated search page; all other verticals should be suppressed.
We make some assumptions about how users interact with an aggregated search page P :
Given that our metrics are based on average user and that there is usually only a limited number of items per block, this simple within block user browsing model is appropriate.
We aim to develop metrics that evaluate an aggregated search page similarly to how a user might. Given two pages P 1 and P 2 , we wish to measure their effectiveness in satisfy-ing a user information need using a utility function Util ( P ). If a user prefers P 1 over P 2 for a given query, the utility measure should lead to Util ( P 1 ) &gt; Util ( P 2 ).
Following [11], the utility of a page is determined by re-ward and effort . A page with a high utility should satisfy the average user information need with relatively little effort. We define the utility metric Util ( P ) of the page P based on all blocks { B 1 , B 2 , ...B n } on the page. When a user reads page P , a block B i on the page P has a certain probabil-ity Exam ( B i ) of being examined. This probability might depend on the position of the block presented, the snippet type of the items (image, text) within the block, or the sat-isfaction level after reading previous blocks B 1 to B i  X  1 probability Exam ( B i ) can be estimated depending on the type of browsing model assumed.

After the user decides to read block B i , he/she will be rewarded with some gain G ( B i ) coming from reading all the items I i 1 to I im within that block. Here we assume that the topical-relevance of the item snippet is a good indication of the relevance of the item itself. Therefore, by reading all the items within the block B i , the user will also have spent some effort E ( B i ) in reading this block. Therefore, based on our assumptions, we define the utility of the page Util ( P ) as the expected gain of reading a page divided by the expected e ffort spent: where | P | is the number of blocks on page P . To ensure suitable normalisation over a set of queries, we define a nor-malized utility score nUtil ( P ), similar to nDCG [12]. We normalise the score of the utility of page P by that of the ideal page P ideal :
Until now, we have defined a general evaluation frame-work for any aggregated search page that considers both reward and effort simultaneously. Consequently, for two pages P 1 and P 2 , we can say P 1 is better than the other when nUtil ( P 1 ) &gt; nUtil ( P 2 ). In the following sections, we instantiate the gain G ( B i ), the effort E ( B i ), and the exami-nation probability Exam ( B i ) of the blocks. We then outline how to normalise the Util ( P ) metrics by constructing an ideal page. Finally, we incorporate a simple personalisation parameter that captures the degree to which a user prefers vertical diversity on an aggregate search page.
Given a block B i containing a set of items ( I i 1 , I i 2 I im ) originating from vertical V j , we would expect that if the vertical is highly oriented given the query, the user will achieve a higher gain. We denote this block orientation as Orient , which is related to the task of vertical selection . Fur-thermore, we would expect that the more topically relevant items a block contains, the higher the gain for the user. We denote the topical-relevance of the block as Topic . Before combining these two factors, we define the gain relating to the vertical-orientation of the block B i : where orient ( V j | W, q ) is a value between 0 and 1. The func-tion g () is used so that the relative gain of the vertical can be altered using a tuning parameter  X  . The orient ( V j | W, q ) value is defined as the fraction of users that would prefer the vertical V j to be added to the  X  X eneral web X  results W . As the  X  X eneral web X  is the pivot to which verticals are added, if orient ( V j | W, q ) &gt; 0 . 5, then adding the vertical should be rewarded. If orient ( V j | W, q ) &lt; 0 . 5, the gain of the block should be less than the  X  X eneral web X  results (i.e. 0.5). Therefore, we use a pivot at the 0 . 5 value through which g () must pass. The following function satisfies these criteria: A graph of the function g ( x,  X  ) is shown in Figure 2. This function controls how much the gain increases as the vertical-orientation level increases. When  X  is small (1 &lt;  X  &lt; 10), we obtain a more steep curve; highly oriented verticals are more rewarded, and conversely, low orientated verticals are more penalised. When  X  equals to 10, the reward is exactly the same as the vertical orientation orient ( V j | W, q ).
Now we define the gain relating to topical-relevance of the Figure 2: Function g ( ) for Controlling Reward on Orientation with Various Parameter  X  . items within B i : | B i | is the number of items within block B i and qrel ( I ik is the binary relevance assessment of the item I ik . In short, we use the sum of the binary relevance judgments of the items as the topical-relevance gain of all the items within the block B i .

Now that we have defined the gain of a block in terms of both vertical-orientation and topical-relevance, we combine these in a suitable manner. Specifically, we combine the gain based on the above two criteria: where  X  is the tuning parameter as described above. We combine these two factors in an independent manner as both vertical-orientation and topical-relevance are related to the quality of the block. Either a low oriented block (low Orient ( Bi )) or a topically irrelevant item (low T opic ( Bi )) would result in an unsatisfied user.
We now consider the effort E ( B i ) spent in examining a block B i . Based on the assumed block-based user browsing behavior, the effort of examining a block is defined as the accumulative effort of reading all the items within it: where | B i | is the number of items within block B i , E ( I the effort spent in reading the item I ik .

Several factors may affect the effort spent in examining an item E ( I ik ): the media type of the snippet (text, image) or the size of the snippet (text length). We assume that there are only three categories of item snippet ( X  X mage X ,  X  X ext X  and  X  X ideo X ). Furthermore, we assume that  X  X mage X ,  X  X ext X  and  X  X ideo X  have a standard size. Based on [23], the time taken to assess the relevance of an image is estimated 2.34 seconds, while the time taken to assess a text snippet is 7.02 seconds. We extrapolate that a video takes twice as much time to assess as a text 4 ( 14 seconds). Therefore, the relative effort taken to examine each snippet type is shown in Table 1 and is used as the unit of effort. These settings are not optimal and have been chosen heuristically after a review of the literature. Identifying more optimal settings is outside the scope of this work.
We concentrate on defining the user browsing model for examining a block Exam ( B i ) on a page. Several models ex-ist [8, 9, 16] that aim to predict the probability with which a user will examine an item. Position models [16] use only the position of the item in a result set. The cascade model [8] uses the relevance of the items previously examined, the intuition being that a sufficiently satisfied user will not con-tinue to examine extra items. Motivated by the fact that users tend to be attracted by vertical results and the vi-sual attention on them will increase the examination prob-ability of other nearby web results, the attention model [9] aims to capture the visual attractiveness of the page. We do not propose a new user browsing model for aggregated search. Rather, we adopt these different models and incor-porate them into our framework, namely the position exam-ination models DCG [12] and RBP [16], the cascade model ERR [8], and the attention model ATT [9].

To adapt ERR to block examination, we assume that the satisfaction of viewing previous blocks is defined as the av-erage gain of viewing each item within the block. For ATT ,  X  dist is the distance between the item under consideration and the closest vertical that has the attention bias (image and video). As we do not have access to query logs to ac-curately estimate the attention bias parameter  X  , instead of assuming that  X  is a position-specific parameter, we assume that  X  is a global variable that is constant for all positions. In addition, there will be attention-bias only when results from image or video verticals are presented on the page. The standard  X  is obtained by exploring the optimal setting in a development set.
A summary of the non-normalised utility metrics that can be instantiated in our framework are listed in Table 2. We have a suite of metrics that reward pages that contain highly oriented verticals, contain topically-relevant items, promote topically-relevant blocks earlier on the page, for less effort. The utility metrics must be normalised by the ideal aggre-gated page. To obtain the latter, we require a brute-force approach that calculates the metric score for all pages, and then selects the page with the maximal score as the ideal page ( arg max ( U til ( P ))  X  P ). This approach is not viable, given the number of possible combinations of various com-ponents of aggregated search. Therefore, we use a greedy algorithm to select a subset of aggregated pages from all the pages that exist, and only select the optimal page from this set. The idea is to use a simple metric for each com-
W e assume that users need to open and view the video item to assess its topical-relevance. ponent, and only select the pages that perform optimally for all those components. This is described in Section 5.2, where the simulation of aggregated page pairs is discussed.
Previous research [26] has shown that different users have different preferences with regard to the type of vertical. A vertical with low orientation to a query for the average user may still be beneficial to users that prefer a very diverse information space. Therefore, we define a personalised ver-tical diversity preference factor to capture this scenario. We achieve this by linearly combining the normalised utility of the page with the vertical recall. This introduces a person-alised preference parameter  X  i :
I Util ( P ,  X  i ) = (1  X   X  i )  X  nUtil ( P ) +  X  i  X  vRecall ( P ) (8) where  X  i is a parameter between 0 and 1 for user i , and con-trols the trade-off between vertical diversity and the quality of the aggregated search page. vRecall ( P ) represents the fraction of all verticals that are presented on page P . The larger  X  i is, the more the user prefers a page with items originating from different verticals (high vertical diversity).
To validate the fidelity of our metrics (how they agree with actual user preferences of aggregated search pages), we collected a set of pairwise preference assessments over ag-gregated page pairs. We first present the data and material used for this purpose. We then simulate a set of aggregated search pages that vary in different levels of quality for each topic. Afterwards, we select a set of page pairs (two sim-ulated pages) for each topic. Finally, we collect preference assessments for the page pairs for all topics. We outline some statistics and analysis of the assessments gathered.
We use an aggregated search test collection [25] created by reusing the existing web collection ClueWeb09. This test collection consists of a number of verticals (listed in Table 3), each populated by items of that vertical type, a set of topics (320) expressing information needs relating to one or more verticals, and assessments indicating the topical-relevance of the items and the perceived user-oriented usefulness of their associated verticals to each of the topics. The verti-cals are created either by classifying items in the web col-lections into different genres (e.g. Blog, News) or by adding items from other multimedia collection (e.g. Image, Video). The topics and topical-relevance assessments of items that vary in genres are obtained by reusing assessments developed in TREC evaluation tasks (TREC Web Track and Million-Query Track). The vertical-orientation information of each topic [26] is obtained by only providing the vertical names (with a description of their characteristics) and asking a set of assessors to make pairwise preference assessments, com-paring each vertical in turn to the reference  X  X eneral web X  vertical ( X  X s adding results from this vertical likely to im-prove the quality of the ten blue links? X ).

We select a subset of topics from which to collect assess-ments. We ensure that this subset of topics still conforms to the real-world distribution of aggregated search covering a wide range of needs with different highly oriented verticals. Therefore, we selected 56 topics detailed in Table 4.
F or each topic, we simulate a set of aggregated search pages. As indicated in Section 3, we assume that a page consists of ten  X  X eneral web X  blocks (one  X  X eneral web X  page is a block) and up to three vertical blocks dispersed through-out those ten blocks (where each vertical block consists of a fixed number of three items). Recall that there are three key components of an aggregated search system that can be varied: (i) Vertical Selection ( VS ); (ii) Item Selection ( IS ); and (iii) Result Presentation ( RP ). We generate pages by simulating an aggregated search system in which the three components vary in quality.

The assessments for vertical-orientation were created by gathering annotations across several users. For the pro-cess of varying VS , for a given vertical V i and query q , we consider the vertical to have a high vertical orientation if orient ( V i | W, q ) is greater than 0 . 75 5 . We simulate four dif-ferent vertical selection strategies, namely Perfect , ReDDE , CORI , Bad . Perfect selects all the highly oriented verticals, while Bad randomly selects the maximum number (three) of lowly oriented verticals. ReDDE and CORI rank the ver-ticals according to the ReDDE [21] and CORI [7] resource selection approaches, and select the top K ranked verticals.
W e select the threshold as 0 . 75 as 75% assessors majority preference is a suitable percentage whereby the assessments are neither too noisy (50%) or stringent (100%). Further-more, it creates a vertical intent distribution across the top-ics that realistically conforms to the real-world [4].
For IS we simulate three potentially different levels of relevance. These are Perfect , BM25 , and TF . Perfect selects all items in the vertical that are topically relevant. BM25 and TF select the top three ranked items from the rankings provided by the BM25 and a simple TF (term frequency) weighting respectively, with the PageRank score as a prior for both BM25 and TF .

For RP , we simulate three different result presentation approaches: Perfect , Random and Bad . Perfect places the vertical blocks on the page so that gain could potentially be maximised, i.e. all the relevant items are placed before non-relevant items. However, if these items are part of a vertical, we position the highest orientated vertical first. Random randomly disperses the vertical blocks on the page while maintaining the position of the  X  X eneral web X  blocks. Bad reverses the perfectly presented page.

By varying the quality of each of the three key compo-nents, we can vary the quality of the result pages created by an aggregated search system in a more controlled way. For each topic, we can create 36 (4  X  3  X  3) pages 6 . In ad-dition, the snippet of each item is automatically generated by the Lemur Toolkit and the presentation style conforms with typical search page presentation (presenting the verti-cal name in front of vertical results). Using this approach we can create a near ideal aggregated page for a query by using Perfect VS , Perfect IS , and Perfect RP . This is a greedy approach to the problem and is used as our method of normalisation for nUtil .
We now describe the selection of page pairs so that they can be presented to a user for judgment. One way to achieve this is to randomly sample two aggregated search pages, and collect a sufficient set of user preference judgments. How-ever, following [3], we attempt a broad categorisation of the aggregated search pages into  X  X ins X  according to page qual-ity, i.e. H (High), M (Middle) and L (Low). We can then provide a more in depth analysis of the performance of the metrics over different regions of the page space.

Although we do not know the quality of all the pages, we can roughly estimate the page quality using the quality of
C ertain combinations of VS, IS, and RP do not create unique simulated pages. the components that created the page. We estimate this by a ssuming that the three components contribute equal im-portance to the quality of the page. We then evaluate each component respectively using a suitable metric. The qual-ity score of the page is determined by linearly combining the metric score for each component. This is a coarse ap-proach of determining the quality of the page. We use the F-measure (VS), Mean Precision (IS), and Kendall-tau cor-relation (RP). We then rank all the pages according to the three linearly combined metrics and evenly categorise the pages in the ranking into  X  X  X ,  X  X  X  and  X  X  X  bin respectively.
We now have a method of comprehensively analysing how various metrics perform over the whole page space by se-lecting pages from these pre-assigned bins. Specifically, we have six bin pairs, H-H, H-M, H-L, M-M, M-L, L-L, which uniformly represent all the entire page space for the queries (albeit in coarse intervals). For each pair of bins, we ran-domly select 8 page pairs from it. Consequently, we select in total 48 (6  X  8) page pairs for each topic.
Our preference assessment data is collected over the Ama-zon Mechanical Turk crowd-sourcing platform, where each worker was compensated $0.01 for each assessment made. A page pair was presented with the topic (title and descrip-tion) shown in the upper position of the assessment page. This was followed by a pair of aggregated pages shown side-by-side. The assessor was provided with three options when making the assessments:  X  X eft page is better X ,  X  X ight page is better X  and  X  X oth are bad X . The latter option captures the scenario where a user is confused due to the poor page quality 7 . For each page pair, we collect four assessments (from four different assessors). The total number of assess-ments made during this preference collection process was 10752 (56  X  48  X  4). Following [19], a quality control was ensured by including 500  X  X rap X  X ITs. Each  X  X rap X  X IT con-sists of a triplet ( q, i, j ) where either page i or j was taken from a query other than q . We interpreted an assessor pre-ferring the set of extraneous results as evidence of malicious or careless assessments and assessors who failed more than two trap HITs were discarded.
Of the 203 assessors who contributed HITs, 39 had their assessments removed from the assessment pool due to failing more than 2 trap HITs. For the remaining 164/203, partic-ipation followed a power law distribution where about 12% (20/164) of the assessors completed about 60% (6522/10752) of our HITs. We also found out that assessors rarely select the  X  X oth are bad X  options provided as only 7% (684/10752) of the assessments are of this option.

We want to answer the following question: RQ1 Do users agree with each other when assessing aggregated search pairs? Therefore, we measured annotator agreement of preferences of aggregated page pairs using Fleiss X  Kappa [24] (denoted by K F ), which corrects for agreement due to chance. Fleiss X  Kappa is convenient because it ignores the identity of the assessor-pair, and is designed to measure agreement over in-stances labeled by different (even disjoint) sets of assessors. The results are shown in Table 5.
T he option X  X oth are good X  is not included because this in-formation can be potentially obtained by investigating inter-assessor agreement for definite preferences.
 Table 5: Statistics of User Preference Assessment Agreement over Various Quality Bins.
We observe that assessor agreement on presentation-pairs w as K F = 0 . 241, which is considered fair agreement [24]. This result is similar to previous research [3, 26], which re-affirm that evaluating aggregated search is not an easy task, and that various users have their own assumptions about what a good page is. Of all 10752 aggregated page-pairs, 8051 (74.8%) had a majority preference of at least 3/4 and only 2231 (20.7%) had a perfect 4/4 majority preference. It is perhaps not surprising that assessor agreement is not high as agreement on page-pairs requires that assessors make similar assumptions about the cost of different types of er-rors. Furthermore, the low inter-assessor agreement may be explained by the fact that users make different assump-tions regarding the importance of each aggregated search component ( VS , IS , RP ). Alternatively, it may be that as-sessors have a hard time distinguishing between good pre-sentations. Following previous research [3], given this low level of inter-assessor agreement, rather than focusing on the metrics agreement with each individual preference, we focus on their agreement with the majority preference (3/4 or greater, and 4/4) in the evaluation. We investigate the fidelity 8 [22] of the proposed metrics. We leave an investigation on the reliability of the metric (dis-criminative power [18]) for future work. We aim to answer the following questions: 1. RQ2 With standard parameter settings, are the stan-2. RQ3 Can we learn personalised parameters from his-To demonstrate the fidelity of our four metrics ( AS DCG , AS RBP , AS ERR and AS AT T ), we compare them with exist-ing IR metrics. We utilise both user-oriented IR metrics cap-turing topical-relevance ( nDCG [12], P @10), and diversity-aware metrics (  X  -nDCG [10], D -nDCG [18], D #-nDCG [18], IA -nDCG [1]) which we adapt to incorporate vertical diversity. We select the latter as they are the most prevalent user-oriented IR metrics. Their adaptation is as follows: (i) we replace subtopic importance with orient ( V | W, q ); (ii) we substitute the user model for ranks to the one that applies to blocks; and finally (iii) we normalise according to the ideal aggregated search page.
T he extent to which an evaluation metric measures what it is intended to measure.
To measure the performance of the metrics, we calcu-l ate the percentage of agreement (percentage of those pairs for which the metric agrees with the majority preference of users). The larger the percentage of agreement, the more accurately the metric can predict the user preference of any aggregated search page pairs, and the higher the metric fi-delity. A two-tailed t-test (significant at the p &lt; 0 . 01 level, denoted by N or H ) is used to show which metric correlates more significantly with the user preferences 9 .
To answer RQ2 , we carry out a set of experiments where we employ the prevalent standard parameter settings for the metrics used in IR experiments. We utilise the standard log discount function for all DCG related metrics ( AS DCG nDCG ,  X  -nDCG , D -nDCG and D #-nDCG ). We set the  X  parameter in  X  -nDCG to 0 . 5 and  X  to 0 . 5 for D #-nDCG . For our proposed metrics, we set  X  = 10 (a linearly increas-ing vertical-orientation function) and  X  i = 0 . 0 (no person-alised vertical diversity preference) as the standard param-eters. For the user persistence parameter in AS RBP , we set  X  = 0 . 8 as this value best correlates with the user brows-ing behavior from a real-world query-log data [16]. These standard settings instantiate a simple metric (e.g. AS DCG similar to existing topical diversity-aware metrics that in-corporate subtopic importance probability ( D -nDCG ). The standard  X  of AS AT T is obtained by exploring the optimal setting in a development set that contained 500 preference page pairs that contain visually attractive results (results coming from Image and Video).

Our evaluation, the fidelity of the metrics, thus focuses on the agreement (of each metric) with the user preferences over the set of aggregated search results. As we have already categorised page pairs into various quality  X  X ins X  (H-H, H-M, H-L, M-M, M-L, L-L), we report the experimental results over different bin pairs, in order to understand each metric performance over the whole evaluation space. Our exper-iments have two parts: (i) when fixing the assumed user browsing model (e.g. DCG), we compare the performance of our proposed metrics with existing IR metrics; (ii) under the proposed framework, we compare user models to inves-tigate which ones make more accurate prediction of the user preferences on aggregated page pairs.
We present results for a majority preference of 3/4 or greater, or 4/4, in Table 6. The significance is calculated in comparison with one of the proposed metrics, AS DCG . Our metrics have higher agreement with user preferences for the H-M, H-L and M-L bins compared to the less discrimina-tive bins (H-H, M-M, or L-L). In addition, for page pairs with higher majority user agreement (4/4 instead of 3/4), our metrics tend to make more accurate prediction of the user preferences. After closer examination, we observe that the metrics agreement with the majority user preference is higher on pairs where there is greater consensus between assessors. This is similar to reported in [3].

We also observe that overall the proposed aggregated search metrics ( AS DCG ) work better than existing IR metrics ( nDCG
W e also used the sign test [3]. For all page pairs with majority of preference, our proposed metrics performed sig-nificantly better than random. Since we are interested in comparing metrics, we do not report the sign test outcomes. and P @10). They have a significantly better performance across almost the entire metric space. This is not surprising given that the proposed metrics incorporate aspects unique to aggregated search (vertical-orientation), which can af-fect user preferences. Indeed, when the page quality is ex-pected to be high, traditional IR metrics that do not con-sider vertical-orientation perform worse than the proposed metrics. But it is worth noting that nDCG performs signif-icantly better than other metrics on L-L page pairs. This might be because as the returned verticals are of low orien-tation, and for these types of page pairs, simply measuring topical relevance of items might correlate more with the user browsing behavior than considering the additional vertical orientation; when assessing two low-quality pages, the user is trying to find more topically relevant items, without regard to the orientation of the vertical.

For the diversity-aware metric,  X  -nDCG performs signif-icantly worse than the proposed metrics. This is because  X  -nDCG implicitly penalises the within vertical redundancy of items. This evaluation strategy is not appropriate when presenting results from the same vertical in a block. A close examination shows that this degraded performance is due to the over-penalisation for items within each vertical. Al-though recent research [14] has suggested that  X  may be tuned on a per query basis to either promote or discount ex-tra items from the same sub-topic (vertical), we leave this for future work. In addition, instead of fully utilising the graded orient ( V | W, q ) information,  X  -nDCG treats relevant verti-cals in a binary sense, another reason that may cause the degraded performance.

The other existing diversity-aware metric D -nDCG per-forms comparably well. This is not surprising as when em-ployed with standard parameter setting, D -nDCG is most similar to the proposed aggregated search metrics ( AS DCG The major difference is that AS DCG captures the effort of examining result snippets of different types. D #-nDCG performs significantly worse than D -nDCG over the entire simulated page space used for evaluation in the context of aggregated search. This proves that simply promoting verti-cal diversity without considering vertical-orientation can de-grade the evaluation performance. In addition, as we will see later, because of the various users vertical diversity prefer-ence, personalised vertical diversity can be a better strategy for the evaluation of aggregated search. Finally, IA -nDCG also performs considerably worse than AS DCG . A close ex-amination suggests that this is due to the over-rewarding of the vertical results in a page.

When we assume a uniform effort distribution of the re-sulting snippets, which can be of various types, the metric performances decrease from 67 . 3% to 65 . 6%. However, this decrease is not statistically significant. This might be due to the small number of topics promoting image or video verti-cal results. Estimation of the efforts associated with reading snippet of various types on a large-scale dataset is needed.
For the proposed metrics with various user models ( AS DCG AS RBP , AS ERR and AS AT T ), their agreements with the users majority preference (3/4 or greater) are shown in Ta-ble 7 10 . We observe that the metric agreements are com-
T he results of metric agreement with 4/4 users majority preference is similar and is, therefore, not included due to space limitations. paratively similar; although, overall, the metrics based on p osition-based user models ( AS DCG and AS RBP ) perform consistently better than the adapted cascade model metric AS ERR or the attention-based model AS AT T .

We further see that comparatively AS ERR performs bet-ter on H-M and H-L bins and worse on others. The de-graded performance might be due to the fact that only bi-nary topical-relevance assessments (of items) are available and the metric largely rewards the top relevant results. This also partly explains why AS ERR performs particularly well between high quality pages (highly oriented and relevant re-sults are presented at the top of the page) and low quality pages. It is most likely that instead of considering the entire page, most assessors looked only at the early results of the page when assessing.

However, surprisingly, by incorporating attention bias (of visually attractive vertical results) into the position-based model, the performance of the metric AS AT T degrades, com-pared with AS DCG . This might be due to the inaccurate es-timation of the attention bias  X  from our small-scale exper-iments. After closer examination, it may be that assessors have a considerable preference bias on pages that contain visually attractive results (image, video) [3]. Therefore, the preference assessment between pages containing image and video verticals may be noisier, which could result in a natu-ral bias for those types. Further experiments are needed to explain and understand this bias and its effect.

In comparing AS DCG and AS RBP , although it is observed that AS RBP performs slightly better for page pairs con-sisting of pages with high quality agreements, the result is not significant. As the only difference between AS DCG and AS RBP is the position-based discounting factor (the user browsing model), the slight improvement is caused by the different user model. This user browsing modelling factor is examined in more detail later.
Although the results of our proposed metrics are promis-ing when compared with existing IR metrics, the results should be treated with caution as the agreement is not sub-stantial (the best performance is 67 . 7% from our proposed metric AS RBP ). After a close examination of the user pref-erences, compared with the metric prediction, the reasons for this include: (i) the vertical-orientation annotations [26] may not fully agree with the real user preference of verti-cals (they are noisy estimations); and (ii) although three Table 7: Proposed Metric Agreements with 3/4 User Majority Preferences: Comparison of User Ex-amination Models. key components of aggregated search are captured, we have o nly used simple default values for some of the parameters. This motivates further experiments that aim to learn per-sonalisation parameters from historical data.
We can improve the performance of our metrics by learn-ing suitable parameter settings using training data, thus ad-dressing the research question RQ3 . We only use AS RBP as an example. We recall that AS RBP has three parameters:  X  that controls the degree to which vertical orientation is re-warded;  X  that controls the user browsing behavior in terms of user persistence; and  X  i that controls the degree to which a user prefers a diverse aggregated page.

Training is done in two stages. First, we learn suitable values for  X  and  X  independently of  X  i . We categorised the user preference data into five sets and use five-fold cross val-idation for training and testing. We set  X  i = 0 . 0 (users do not prefer vertical-based diverse results unless the vertical provides better results) and iterate through different settings of values:  X  (from 1 to 100) and  X  (from 0 . 5 to 1 . 0). The optimal combination is obtained with  X  = 7 . 0 and  X  = 0 . 85 indicating that users generally favour results that contain highly-oriented verticals, and that users do not have a per-sistent browsing behaviour (they care more about the results returned in a high position in the page). The corresponding results are shown in Table 8. The performance of the metric is improved over the standard parameter settings from 67 . 7% to 72 . 6%. This improvement is due to the better estimation of two parameters  X  and  X  concerned with two main aspects of aggregated search, vertical selection and result presenta-tion. By learning from historical data, AS RBP (and other metrics) can better capture these two aspects. Second, we fix the optimal settings for  X  and  X  and learn personalised Table 8: Learned A S RBP Metric Agreements with User X  X  Majority Preferences for All Page Pairs.
 user preference parameters for diversity (  X  i ). Although not optimal, this is sufficient to analyse the  X  X ersonalisable X  pa-rameter independently of others.

As we need sufficient data for learning the parameter, we only test this over the top twenty X  X ead X  X ssessors who made most of the assessments. Like with previous setting, for each assessor, we separate assessor data into five sets and use five-fold cross validation to train and test. For the overall performance, we average the performance for all those as-sessors. The results are also shown in Table 8. The optimal setting for  X  i varies from 0 . 15 to 0 . 4 among different asses-sors whereas the average optimal setting for those assessors is 0 . 23. Similar to [26], this demonstrates that each user has his/her own understanding and preference over the diversity of the results. We can report that by using this personalised parameter, the prediction of the metric agreement with the majority of user preference is improved significantly, from 72 . 6% to 75 . 9%. This effectively illustrates that aggregated search can be improved if we have a better understanding of each user preference over the diversity of results. This is particularly useful for systems that can gather personalised interaction data for their users.
We introduced a general evaluation framework that cap-tures several traits unique to aggregated search. We instan-tiated a suite of metrics for evaluating aggregated search pages from this framework. We presented a methodology to collect user preferences over aggregated pages, which al-lowed us to measure various aspects of our proposed metrics. We did this by simulating aggregated search pages of differ-ent quality for a range of topics. The approach allowed us to analyse different parts of the aggregated page pair space. Furthermore, we showed that the proposed metrics correlate well with the majority user preferences and that traditional IR metrics are not well suited to the task. In addition, while some diversity-based metrics can be adapted to measure the preference between page pair, they are not ideal. By in-stantiating several non-tuned versions of metrics from our framework, we showed that these metrics are at least com-parable to diversity-based IR metrics. We also showed that our metrics have the ability to tune their behaviour for pages for which personalised preference data is available.
Future work will involve extending and setting several pa-rameters of the metrics so that they more closely correlate with user preferences for the sets of page pairs. In particu-lar, when query-log data is available, we can further extend the framework by proposing new user browsing models for aggregated search and investigate their values. We will also devise new approaches to utilise the available implicit user feedback data to better estimate the parameters. Another interesting challenge will be to compare the effectiveness and weakness of existing diversity-aware metrics for evaluating aggregated search, and study the ability of our framework to generalise them.
 Acknowledgments This work was supported partially by both the EU LiMoSINe project (288024) and by the Irish Research Council X  X  INSPIRE (FP7 COFUND) programme. Any opinions, findings, and recommendations expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors.
