 1.Introduction
Text categorization is the problem of automatically assigning predefined categories to free text docu-ments. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years ( Yang &amp; Liu, 1999 ).

A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the fea-ture space ( Joachims, 1998 ). The native feature space consists of the unique terms (words or phrases) that occur in documents, which can be tens or hundreds of thousands of terms for even a moderate-sized text collection. This is prohibitively high for many machine learning algorithms. If we reduce the set of features considered by the algorithm, we can serve two purposes. We can considerably decrease the running time of the learning algorithm, and we can increase the accuracy of the resulting model. In this line, a number of square test (CHI) most effective in aggressive term removal without losing categorization accuracy in their experiments ( Yang &amp; Pedersen, 1997 ). They also discovered that IG and CHI scores of a term are strongly correlated.

Another major characteristic of text categorization problems is the high level of feature redundancy ( Joachims, 2001 ). While there are generally many different features relevant to a classification task, often several such cues occur in one document, and these cues are partly redundant. Naive Bayes, which is a pop-ular learning algorithm, is commonly justified using assumptions of conditional independence or linked dependence ( Cooper, 1991 ). However, theses assumptions are generally accepted to be false for text. To remove these violations, more complex dependence models such as Bayesian network classifiers have been developed ( Sahami, 1998 ), but they require complex models by trading efficiency.

Most previous works of feature selection emphasized only the reduction of high dimensionality of the feature selection method is IG. IG works well with texts and has often been used. IG looks at each feature features are not redundant with each other, IG is very appropriate. But in cases where many features are highly redundant with each other, we must utilize other means, for example, more complex dependence models.

In this paper, for the high dimensionality of the feature space and the high level of feature redundancy, we propose a new feature selection method which selects each feature according to a combined criterion of information gain and novelty of information. The latter measures the degree of dissimilarity between the feature being considered and the previously selected features. MMR provides precisely such functionality ( Carbonell &amp; Goldstein, 1998 ). So we propose MMR-based feature selection method which strives to re-duce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.

In machine learning field, some greedy methods that add or subtract a single feature at a time have been posed a method for incrementally constructing random field ( Pietra et al., 1997 ). Their method builds increasingly complex fields to approximate the empirical distribution of a set of training examples by allow-ing potential functions, or features, which are supported by increasingly large sub-graphs. Each feature is assigned a weight, and the weights are trained to minimize the Kullback X  X eibler divergence between the field and the empirical distribution of the training data. Features are incrementally added to the field using a top X  X own greedy algorithm, with the intent of capturing the salient properties of the empirical samples while allowing generalization to new configurations. However the method is not simple, and this is prob-lematic both computationally and statistically in large-scale problems.
Koller and Sahami proposed another greedy feature selection method which provides a mechanism for eliminating features whose predictive information with respect to the class is subsumed by the other fea-tures ( Koller &amp; Sahami, 1996 ). This method is also based on the Kullback X  X eibler divergence to minimize the amount of predictive information lost during feature elimination.

In order to compare the performances of our method and greedy feature selection methods, we imple-mented Koller and Sahami  X  s method, and empirically tested it in Section 4 .

We also compared the performance of conventional machine learning algorithms using our feature selec-tion method with that of support vector machine (SVM) in Section 4 . SVM is a new learning method intro-duced by Vapnik et al. ( Vapnik, 1998 ). Previous works show that SVM consistently achieves good performance on text categorization tasks, outperforming existing methods substantially and significantly of feature redundancy, SVM is known that it does not help much with sophisticated feature selection ( Joachims, 2001 ).

The remainder of this paper is organized as follows. In Section 2 , we describe the information gain and divergence-based feature selection. Section 3 presents in-depth experiments, discussions and the results. Sec-tion 4 concludes the research. 2.Informationgainanddivergence-basedfeatureselection
In this section, we describe the maximal marginal relevance (MMR) and the MMR-based feature selection. 2.1. Maximal marginal relevance
Most modern IR search engines produce a ranked list of retrieved documents ordered by declining rel-rior criterion. A first approximation to relevant novelty is to measure the relevance and the novelty independently and provide a linear combination as the metric.

The linear combination is called  X  marginal relevance  X   X  X .e. a document has high marginal relevance if it is both relevant to the query and contains minimal similarity to the previously selected documents. In docu-ment retrieval and summarization, marginal relevance should be maximized, hence the method is labeled as MMR ( Carbonell &amp; Goldstein, 1998 ).

R = IR( C , Q , h ), i.e., the ranked list of documents retrieved by an IR system, given C and Q and a rele-vance threshold h , below which it will not retrieve documents ( h can be a degree of match or number of documents); S is the subset of documents in R which is already selected; R n S is the set difference, i.e. the set of as yet unselected documents in R ; Sim 1 is the similarity metric used in document retrieval and a relevance ranking between documents (passages) and a query; and Sim ferent metric.

Given the above definition, MMR computes incrementally the standard relevance-ranked list when the parameter k = 1, and computes a maximal diversity ranking among the documents in R when k = 0. For intermediate values of k in the interval [0,1], a linear combination of both criteria should be optimized.
 2.2. MMR-based feature selection
We propose a MMR-based feature selection (MMR_FS) which selects each feature according to a com-bined criterion of information gain and novelty of information. We define MMR_FS as follows: gain scores, and IGpair is the information gain scores of co-occurrence of the word(feature) pairs. IG and IGpair are defined as follows: where p ( w i ) is the probability that word w i occur, w occur, p ( w i , j ) is the probability that w i and w j co-occur, and w but w i or w j can occur (i.e. p  X  w i ; j  X  X  1 p  X  w i ; j
Given the above definition, MMR computes incrementally the information gain scores when the param-eter k = 1, and computes a maximal diversity among the features in R when k = 0. For intermediate values of k in the interval [0,1], a linear combination of both criteria should be optimized. 3.Experiments
In order to compare the performance of MMR_FS method with conventional IG and greedy feature methods with four different learning algorithms: Naive Bayes, TFIDF/Rocchio. Probabilistic Indexing (PrTFIDF ( Joachims, 1997 )) and Maximum Entropy using Rainbow ( McCallum &amp; Kachites, 1996 ). Rain-bow is an executable program that does document classification. It provides Naive Bayes, TFIDF/Rocchio, Probabilistic Indexing (PrTFIDF), K -nearest neighbor, Maximum Entropy and SVM.

We also compared the performance of conventional machine learning algorithms using our feature selec-tion method and SVM using all features.

MMR-based feature selection (MMR_FS) and greedy feature selection method (Koller &amp; Sahami  X  s method) require quadratic time with respect to the number of features. To reduce this complexity, for each dataset, we first selected 1000 features using IG, and then we applied MMR_FS and Greedy method to the selected 1000 features.
 For all datasets, we did not remove stopwords. We performed 10-fold cross validation for all datasets.
MMR_FS method needs to tune for k . It appears that a tuning method based on held-out data is needed here. We tested our method using 11 k values (i.e. 0,0.1,0.2, ... ,1) and selected the best k values. 3.1. Datasets 3.1.1. Reuters-21578
The Reuters-21578 corpus contains 21578 articles taken from the Reuters newswire. Each article is typ-number of categories is 114.

Following ( Koller &amp; Sahami, 1996 ), we construct two subsets from Reuter corpus. The first, Reuters1, which are likely to have many similar words used in different contexts across topics. We also construct Reu-ters3 which consists of articles on the most frequent topic  X  earn  X  ,  X  acq  X  , and  X  money-fx  X  . 3.1.2. WebKB
This dataset contains WWW-pages collected from computer science departments of various universities in January 1997 by the World Wide Knowledge Base ( WebKB ) project of the CMU text learning group.  X  department  X  and  X  staff  X  . The remaining part of the corpus contains 4199 documents in four categories. Since the web pages are in HTML format, they contain much non-textual information. We filtered out
MIME headers and HTML tags. 3.2. Experimental results
Fig. 1 displays the performance curves for four different machine learning algorithms on Reuters1 after term selection using MMR_FS (number of features is 25). When the parameter k = 0.5, all machine learning algorithms have best performance and significant improvements compared to the conventional information gain (i.e. k = 1) with a statistical significance at the 99% level. Moreover, all machine learning algorithms have improvements compared to SVM using all features.

Figs. 2 X 4 display the performance curves for Naive Bayes, TFIDF, and PrTFIDF on Reuters1 using three feature selection methods (MMR_FS with k = 0.5, Greedy, and conventional IG), all features with no feature selection (5595 terms), and SVM using all features, respectively. As seen from these figures, MMR_FS is more effective than Greedy and IG, and moreover, produces improvements of conventional machine learning algorithms over SVM which is known to give the best classification accuracy. MMR_FS can reduce the feature vocabulary from 5595 terms to 25 X 100 with best performance in accuracy. For exam-ple, the vocabulary is reduced from 5595 terms to 100 (98.2% reduction), and the accuracy is improved from 97.50% to 99.31% in Naive Bayes (the micro-averaged F1 is improved from 97.59 to 99.31).
Fig. 5 shows the same performance curves on Reuters2 after term selection using MMR_FS (number of features is 400). In this dataset, most machine learning algorithms also have best performance and significant improvements compared to conventional IG (with a statistical significance at the 90% level) and SVM using all features, when the parameter k = 0.5.

Figs. 6 X 8 show the performance curves for Naive Bayes. TFIDF, and PrTFIDF on Reuters2 using the three feature selection methods, all features (5478 terms) and SVM using all features, respectively. In these figures, MMR_FS is also more effective than greedy method and IG, and moreover, produces improve-ments over SVM. MMR_FS can reduce the vocabulary from 5478 terms to 200 X 400 with best performance in accuracy for the corpora even with many similar overlapping terms such as Reuters2.

Fig. 9 shows the performance of three machine learning algorithms on Reuters3 using MMR-based fea-ture selection method. In this dataset, MMR_FS has significant improvements compared to IG. Using
MMR_FS, for example, the accuracy is improved from 86.5% to 91.0% and the micro-averaged F1 is im-proved from 86.9 to 91.2 in Naive Bayes (number of feature is 50).

Fig. 10 shows the performance of three machine learning algorithms on WebKB also using the same three feature selection methods and all features (41763 terms). In this dataset, again MMR_FS has the best performance and significant improvements compared to Greedy and IG. Using MMR_FS, for example, the vocabulary is reduced from 41763 terms to 200 (99.5% reduction), and the accuracy is improved from 85.78% to 90.64% in Naive Bayes. Using Greedy and IG, however, the accuracy is improved from 85.78% to about 87% in Naive Bayes. PrTFIDF is most sensitive to feature selection method. Using
MMR_FS the best accuracy is 82.69%. Using Greedy and IG, however, the best accuracy is only 72 X  74%. In this dataset, however, MMR_FS does not produce improvements of conventional machine learn-ing algorithms over SVM.

The observation in Reuters and WebKB are highly consistent. MMF_FS method is consistently more effective than Greedy and IG method on two datasets, and sometimes produces improvements of simple conventional classifiers even over state-of-the-art SVM. 4.Conclusion
In this paper, we proposed an information gain and divergence-based feature selection method which strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.

We carried out extensive experiments to verify the proposed method. The experiments were performed using three different machine learning algorithms on both Reuters and WebKB dataset. Based on the exper-iment results, we can verify that our MMR-based feature selection is more effective than Koller and Sah-ami  X  s method, which is one kind of greedy methods, and conventional information gain which is commonly used in feature selection for text categorization. Besides, MMR-based feature selection method sometimes produces improvements of simple conventional machine learning algorithms over SVM which are known to give the best classification accuracy.

A disadvantage in using MMR-based feature selection is that the computational cost of computing the pairwise information gain (i.e. IGpair) is quadratic time with respect to the number of features. To reduce this computational cost, we can use the MMR-based feature selection method on the reduced feature set resulting from IG as our experiments in Section 4 . Another drawback of our method is the need to tune for k . It appears that a systematic tuning method based on held-out data is needed in the future. References
