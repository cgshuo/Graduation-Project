 Gaussian mixture speaker model (GMM) statistically represents the underlying sounds or vocal tract configurations that ch aracterize a person X  X  voice, and it has been proven very effective for speaker recognition [1,2]. Usually, Gaussian mixture density functions use diagonal covariance matrices. The advantage is that the model is simple and easy for computation [3,4,5]. However this also reduces the likelihood of the data. In order to compensate the losing likelihood, many approaches have presented in recent years. Ljolje has demonstrated that explicitly modeling the correlation between feature elements can improve the performance of recognition [6]. The drawback of this method is that the orthonormal transformation is outside of the statistical framework and is not optimized together with GMM parameters. Kuo-Hwei [3] approach, the transformation matrix is regarded as a set of statistical parameters. Chih-chien [7] proposed a classification sc heme that incorporates Karhunen-Loeve transform (KLT) [8] and GMM for text-independent speaker identification. Transformation based method is also applied in speaker adaptation algorithm [9]. 
However, the mixture number of GMM must be given beforehand in all these approaches. Charles A. B [10] advanced a cluster approach of parameter estimation for GMM with EM algorithm, in which the mixture number was regarded as an estimated parameter as the same as the other parameters such as the mean vectors and the covariance matrices. Thus the parameters estimated by the cluster approach are more accurately to depict the distribution of feature than the ordinary EM algorithm. 
This paper presents a new approach to estimate the GMM parameters. We integrate embedded transformation. Fig. 1 illustrates the conceptual block diagram. The approach is referred as transformed cluster GMM (TC-GMM), and we refer to GMM with embedded transformation as TE-GMM and GMM with cluster EM algorithm as CE-GMM. In the new approach, the mixture number, together with other parameters of GMM i.e. the weights, the mean vectors and the diagonal covariance matrices would be estimated by the improved cluster EM algorithm. In the experiment, we investigate the performance of the TC-GMM and the other two methods. 2.1 Transformation Embedded GMM with Diagonal Covariance Matrices In the research of linear transformation with diagonal covariance matrices, two methods are usually used. In the early approach, the transformation matrix and diagonal-covariance Gaussian mixture parameters are modeled separately. In this paper, we apply another approach, that is, the transformation matrix and the diagonal-covariance Gaussian mixture are combined into a uniform statistical model [3]. 
In order to estimate the parameters of Gaussian mixtures, it is necessary to determine the number of mixtures. In the section, let us assume that this model has K vector to be modeled using a Gaussian mixture distribution. Then the parameters are Then the Gaussian mixture density function is given by Where each covariance matrix k R can be explicitly decomposed into eigenvalue matrix k  X  and eigenvector matrix  X  , that is T k k R  X   X  X  = .The  X  is tied across all Gaussian components while each k  X  is a component-specific matrix. The parameters of the statistical model are denoted as } , , 1 , , , { K k k k k L =  X   X  =  X   X   X  . 
The ML estimation of the set  X  is obtained by maximizing the likelihood function, Because  X  couldn X  X  be solved directly, the EM algorithm is used. Starting from an initial model  X  , the new model  X  Where ) , (  X  n y k p is the posteriori probability and ) , (  X 
In addition, two constraints can help to obtain the re-estimated formulas, that is 1
The re-estimated formulas of weights k  X  i.e. Since the derivation of the formulas for  X   X  ) diagonal elements, i.e., 
Then the column vector m  X  solved from the following nonlinear equations: Where: covariance matrix is applied in the computation of Gaussian mixture density function. 2.2 Improved Cluster EM Algorithm with Diagonal Covariance Matrices speech feature distribution using tran sformation embedded GMM, the number K must be determined without any priori information. The number K would not be fitted with the actual feature distribution in most cases. In order to search the better parameters, the cluster EM algorithm [10] is applied. In the algorithm, the number of mixture K is also regard as a GMM parameter. The complete set of parameters are given by the ) )
 X  . The log of the probability of the entire training sequence is then given by 
In the same way, the maximum likelihood (ML) estimate is used to estimate the parameter K and } , , 1 , , , { K k k k k L =  X   X  =  X   X   X  . It is given by 
However, the ML estimate of K is not well defined because the likelihood may always be made better by choosing a large number of mixtures. The addition of a penalty term in the log likelihood of account for the over-fitting of high order models is generally adopted to estimate the model order. In this paper, the minimum description length (MDL) estimator [11] is used and the expression of minimization is Where N is the length of the feature vectors, M is the dimension of a vector and L is 
The new estimates of weights k  X  the  X 
The four re-estimated formulas show how to update the parameter  X  , they do not show how to change the model order K . That is, how to decrement the number of mixtures from K to K -1 is the remaining question. In the paper, we reduce the number problems must to be solved. One problem is determining which two mixtures should to be merged and the rule of determination. After the two mixtures are fixed, how to compute the values of the parameters of the new mixture is another problem. The two problems would be discussed in blendi ng each other. Suppose two mixtures, l and m , may be effectively merge in a single mixture, then ) matrix of the new mixture. Using (15) and (16), a distance function is defined as 
With the function (13), it is now possible to search over the set of all pairs, ( l , m ), to find the pair which minimizes d ( l , m ), i.e. initial condition for EM optimization with K -1 mixtures. Unfortunately, if the order of model is 0 K , cluster EM algorithm must do 0 K ordinary EM processes, which take a long time. In this paper, the two step estimation algorithm is advanced. In the accurately re-search the parameters which minimize the value of MDL with the order 
The final cluster EM algorithm is given in the following steps. 1. Initialize the order of model with a large number 0 K . 2. Initialize } , , 1 , , , { 0 K k k k k L =  X   X  =  X   X   X  using k-means cluster algorithm. 3. Apply the EM algorithm to compute the parameters of the new estimate  X  Specifically, using (4) and (5) to compute weight k  X  (7) and (8) to re-estimate  X  4. Set  X   X  5. Record the final parameter ) ( K  X  , and compute the value ) , ( ) ( K K MDL  X  . number of mixtures with n step length, set n K K  X   X  , and go back to step 3. 7. Choose the value * ) ( n K and parameters ) ( * ) ( n K  X  which minimize the value of MDL. 8. Re-initialize the order of model with 1 * ) ( +  X  n K n , then repeat step 2 to 3 until the last number of order is 1 * ) (  X  + n K n in which the step length is 1. 9. Finally find the value * K and parameters ) ( * K  X  which minimize the value of 3.1 Database and Feature environment of the ordinary laboratory with 8 kHz sampling rate and finally 8-bit A-law coding quantization. It consist 30 speakers (16 males and 14 females). The whole database is recorded in Chinese Mandarin and every speaker has his (her) own dialect more or less. Each utterance contains about 3 minutes speech. There are 3 sessions for each target speaker with 3 different contents. Each speaker pronounces 10 sequences of 4 connected digits about 30 seconds in session 1 which we called digital session. In session 2, the speaker pronounces The wind and the sun of Aesop's fable in Chinese edition. This session is called fixed content session and about 1 minute. The 3rd environment or tell what he has done during the day or something else. Anyway, speakers were kindly suggested not to say th e same thing from speaker to speaker and suggested to speak randomly and colloquially. This session is limited in 1 minute. 
In the experiments, the speech data of session 2 is chose to train the models models. The speech data were processed into frames of 256 samples, with a frame advance of 128 samples. Each frame was represented by a 24 component feature vector consisting of 12 MFCCs plus their first order derivatives. 3.2 Results First, we examine the process of the two step estimation algorithm described in speakers is examined. Fig. 2 shows the firs t estimation process of the two speakers in MDL values of the two speakers occurs at K =25 and K =30 in the rough. Fig. 3 shows the The second set of experiments compares the performance of the proposed TC-GMM with TE-GMM and CE-GMM. TC-GMM and CE-GMM have the same underlying structure and they both can achieve a best set of parameters including the mixture number * K , but model the covariance matrices in different way. The performance is compared in terms of th e average error rates and computational complexity. The results are shown in Table 1. Let the initial number of mixtures K , Then the computational numbers in various mixture numbers of TC-GMM and smaller integer function. The listed computational time is for processing one input utterance. The results show that TC-GMM is about 50% time saved than CE-GMM. TC-GMM has a better performance than CE-GMM and the error rates with TC-GMM decrease 2.1% compared with CE-GMM on average. The improvement of performance is because of applying embedded transformation matrices. 
TC-GMM and TE-GMM have the same way in modeling the covariance matrices, but the different structures are applied. Table 2 shows the performance of in terms of the error rates for various testing data sets . These error rates decrease as the mixture number increases when the number of mixture is below 32, while the error rates have a little change up and down when the number is greater than 32. The performance becomes saturation when the mixture number is around 32. The error rates directly reach the point of saturation by using the proposed model in two testing data sets. On rates are smaller in the digital session than in the free speech sess ion since the context in this session is random and completely independent with the training data set. Testing data sets In this paper, TC-GMM is developed to integrate the improved cluster algorithm into transformation. The transformation matrix, the mixture number and other traditional model parameters are simultaneously estimated according to a maximum likelihood new method outperforms the traditional GMM with cluster EM algorithm. Moreover, compared with the transformation embedde d GMM, the experiments show that TC-GMM can directly achieve the best point of saturation with the right mixture number in which the error rates and computational complexity are balanced. 
