 This paper presents two methods, named Item X  and User X  centric, to evaluate the quality of novel recommendations. The former method focuses on analysing the item X  X ased rec-ommendation network. The aim is to detect whether the network topology has any pathology that hinders novel rec-ommendations. The latter, user X  X entric evaluation, aims at measuring users X  perceived quality of novel recommenda-tions.

The results of the experiments, done in the music recom-mendation context, show that last.fm social recommender, based on collaborative filtering, is prone to popularity bias. This has direct consequences on the topology of the item X  based recommendation network. Pure audio content X  X ased methods (CB) are not affected by popularity. However, a user X  X entric experiment done with 288 subjects shows that even though a social X  X ased approach recommends less novel items than our CB, users X  perceived quality is better than those recommended by a pure CB method.
 H3.3 [ Information Search and Retrieval ]: Information filtering, Selection process; G.2.2 [ Graph Theory ]: Graph algorithms Algorithms, Measurement, Human Factors recommender systems, evaluation, novelty, long tail, popu-larity, complex network analysis  X  X f you like The Beatles you might like... X  X  . Now, ask several people and you will get lots of different X  X  X . Each person, according to her ties with the band X  X  music, would be able to propose interesting, surprising or expected X  X  X . Nonetheless, asking the same question to different recom-mender systems one is likely to get similar results. Indeed, two out of five tested music recommenders contain John Lennon, Paul McCartney and George Harrison in their top X  10 ( last.fm and the.echotron.com ). Yahoo! Music recom-mends John Lennon and Paul McCartney (1st and 4th po-sition, respectively), whereas Mystrands.com only contains John Lennon (at top X 10). Furthermore, Amazon X  X  top X 30 recommendations for the Beatles X  White Album is strictly made of other Beatles X  albums (all of a sudden, at the fourth page of the navigation there is the first non X  X eatles album; Exile on Main St. , by The Rolling Stones) 1 .

One can agree or disagree with all these lists of Beatles X  similar artists. However, it is clear that there are a very few, if none at all, serendipitous recommendations (the rest of the similar artists were, in no particular order: The Who, The Rolling Stones, The Beach Boys, The Animals, and so on). Thus, novel recommendations are sometimes necessary in order to improve the user X  X  experience and discovery in the recommendation workflow. The main goal of this pa-per is to evaluate the quality of novel recommendations in recommender systems, in terms of providing unknown, but relevant, items to users.

This paper is structured as follows: section 2 presents the background and related work to providing novel recom-mendations. Sections 3 and 4 present two new approaches, named respectively, Item X  and User X  X entric evaluation, to evaluate different recommendation algorithms in terms of novelty. Then, section 5 presents the experiments performed in the context of the music domain, comparing three algo-rithms (collaborative filtering, content based, and a hybrid approach). Finally, the paper discusses our main findings in section 6, and concludes in section 7.
There is no clear recipe for providing good and useful rec-ommendations to users. Still, there are at least three key elements that should be taken into account. These are: nov-elty, familiarity, and relevance [8]. According to Wordnet dictionary 2 , novel ( adj. ) has two senses:  X  X ew X  X riginal and of a kind not seen before X ; and  X  X efreshing X  X leasantly new or different X . Likewise, familiar ( adj. ) is defined as  X  X ell known or easily recognised X . Ideally, a user should be fa-miliar with some of the recommended items, in order to im-prove confidence and trust in the system. Also, some items
All websites were accessed during May, 2008. http://wordnet.princeton.edu should be unknown to the user (discovering hidden items in the catalog). A system should also give an explanation of why those X  X nknown X  X tems were recommended, providing a higher confidence and transparency on these recommenda-tions. The difficult job for a recommender is, then, to find the proper level of familiarity, novelty and relevance for each user.
It has been largely acknowledged that serendipity and novelty are relevant aspects in the recommendation work-flow [13]. Indeed, there is some existing work that explic-itly addresses these aspects. For instance, five measures to capture redundancy are presented in [20]. Using these measures the system can infer whether an item X  X hat is considered relevant X  X ontains any novel information to the user. In [19], the authors define novelty in terms of the user knowledge, and her degree of interest in a given item. Weng et. al propose, in [18], a way to improve the quality and novelty of the recommendations by means of a prede-fined taxonomy of topics, and hot topic detection using as-sociation rules. Other proposals include disregarding items if they are too similar to something the user has already seen [5], or simple metrics to measure novelty and serendip-ity based on the average popularity of the recommended items [21]. In fact, recommenders that appropriately dis-count popularity may increase total sales [9]. Generally speaking, the most popular items in the collection are the ones with higher probability that a given user will recog-nise, or be broadly familiar with. Likewise, one can as-sume that items with fewer interaction X  X ating, purchas-ing, previewing X  X ithin the community of users are more likely to be unknown [21]. In this sense, the Long Tail distribution X  X n terms of popularity X  X f the catalog [2] as-sists us in deciding how novel or familiar an item could be.
Even though these approaches focus on providing novel and serendipitous recommendations, there is not any frame-work that consistently evaluates the provided recommenda-tions. Thus, there is a need in designing evaluation metrics to deal with the effectiveness of novel recommendations, not only measuring prediction accuracy, but taking into account other aspects such as usefulness and quality [10, 1]. Nov-elty metrics should look at how well a recommender system made a user aware of previously unknown items, as well as to what extent users accept the new recommendations [10]. We present two complementary methods to analyse and evaluate novel recommendations. On the one hand, an item X  X entric evaluation method analyses the item X  X ased recommendation network. The aim is to detect whether the intrinsic topology of the network has any pathology that hin-ders novel recommendations. On the other hand, a user X  centric evaluation aims at measuring the perceived quality of the recommendations.
In this section, we propose several metrics to analyse an item X  X ased recommendation graph; being nodes the items, and the edges denoting the (weighted) similarity among the items. The metrics are derived from Complex Network and Social Network analysis, and are presented in section 3.1. Afterwards, novelty analysis based on item popularity is pre-sented in section 3.2.
The average shortest path (or mean geodesic length) measures the distance between two vertices i and j . They are connected if one can go from i to j following the edges in the graph. The path from i to j may not be unique. The minimum path distance (or geodesic path) is the shortest path distance from i to j , d ij . The average shortest path in a network of size N is:
In a random graph, the average path approximates to h d r i  X  logN log h k i , where h k i denotes the mean degree of all the nodes. The longest path in the network is called its diam-eter ( D ). In a recommender system, mean geodesic length and diameter inform us about the global navigation through the recommendation network.

The strong giant component ( SGC ) of a network is the set of vertices that are connected via one or more geodesics, and are disconnected from all other vertices. Typically, net-works possess one large component that contains a majority of the vertices. It is measured as the % of nodes that includes the giant component. In a recommender system, SGC in-forms us about the catalog coverage, that is the total per-centage of available items the recommender recommends to users [10].
The degree distribution is the number of vertices linked to a vertex, usually denoted k . The degree distribution p is the number of vertices with degree k : where v is a vertex, and deg( v ) is its degree. More fre-quently, the cumulative degree distribution (the fraction of vertices having degree k or larger), is plotted:
A cumulative plot avoids fluctuations at the tail of the distribution and facilitates the evaluation of the power co-efficient  X  , in case the network follows a power law. In a directed graph, that is when a recommender algorithm only computes the top X  X  most similar items, P ( k in ) and P ( k the cumulative incoming (outcoming) degree distribution, are more informative. Cumulative degree distribution de-tects whether a recommendation network has some nodes that act as hubs. That is, that they have a large amount of attached links. This can clearly affect the recommendations and navigability of the network.

Another metric used is the degree correlation . It is equal to the average nearest X  X eighbour degree, k nn , as a function of k : The curve is divided in three parts: head, mid and tail ( X model, F ( x ) , has  X  = 0 . 73 and  X  = 1 . 02 . where p ( k  X  | k ) is the fraction of edges that are attached to a vertex of degree k whose other ends are attached to vertex of degree k  X  . Thus k nn ( k ) is the mean degree of the vertices we find by following a link emanating from a vertex of degree k .

A closely related concept is the degree X  X egree corre-lation coefficient , also named assortative mixing , which is the Pearson r correlation coefficient for degrees of vertices at either end of a link. In the case of a monotonically in-creasing (decreasing) k nn means that high X  X egree vertices are connected to other high X  X egree (low X  X egree) vertices, resulting in a positive (negative) value of r [14]. In rec-ommender systems, it measures to which extent nodes are connected preferentially to other nodes with similar charac-teristics.
The clustering coefficient, C , estimates the probability that two neighbouring vertices of a given vertex are neigh-bours themselves. C is defined as the average over the local measure , C i [17]: where E i is the set of existing edges that are direct neigh-bours of i , and k i the degree of i . C i denotes, then, the portion of actual edges of i from the potential number of total edges. For random graphs, the clustering coefficient is defined as C r  X  h k i /N . Typically, real networks have a higher clustering coefficient than C r .
The previous section presented properties to analyse the topology of an item X  X ased recommendation network. Now, we need to add the novelty component. The main idea is to correlate the above presented metrics with the Long Tail curve of the catalog. E.g. are the hubs in the network the most popular items? Are the most popular items connected with other popular items, and viceversa?
The Long Tail of a catalog is measured in terms of fre-quency distribution (e.g. purchases, downloads, etc.), ranked by popularity. Figure 1 (left) depicts the Long Tail for 260,525 music artists 3 . The horizontal axis contains the list of artists ranked by total playcounts. E.g. The Bea-tles, at position 1, have more than 50 million playcounts. We combine this information together with the recommen-dation network, to detect those items that could be both novel and relevant for a given user profile.
The Long Tail model, F ( x ), simulates any heavy X  X ailed distribution [11]. It models the cumulative distribution (in %) of the Long Tail data. F ( x ) equals to the share of total volume covered by objects up to rank x : where  X  is the factor that defines the S  X  X hape of the func-tion,  X  is the total volume, and N 50 is the number of objects that cover half of the total volume, that is F ( N 50 ) = 50.
Once the Long Tail is modelled using F ( x ), we can divide the curve in three parts: head, mid, and the tail part. The boundary between the head and the mid part of the curve is defined by:
Likewise, the boundary between the mid part and the end of the tail is:
The data was gathered from last.fm website during July, 2007. Last.fm provides plugins for virtually any desktop music player. Figure 1 (right) depicts the cumulative distribution of the Long Tail of 260,525 music artists. Interestingly enough, the top X 737 artists account for 50% of the total playcounts, F (737) = 50, and only the top X 30 artists hold around 10% of the plays. In this sense, the Gini coefficient measures the inequality of a given distribution, and it determines the im-balance degree. In our Long Tail example, 14% of the artists hold 86% of total playcounts, yielding a Gini coefficient of 0 . 72. This value denotes an imbalanced distribution, higher than the 80/20 Pareto rule. Figure 1 (right) shows, too, the head of the curve, X head  X  mid which consists of only 82 artists, whereas the mid part has 6573 ( X mid  X  tail = 6655). The rest of the artists are located in the tail part.
Once each item is located in the head, mid, or tail part, the next step is to combine the properties of the items X  similarity network with the Long Tail information. Two main analyses are performed. First, we measure item relationships in each part of the curve. That is, for each item that belongs to the head part, compute the percentage of similar items that are located in the head, mid and tail part (similarly, for the items in the mid and tail part). This measures whether the most popular items are connected to other popular items, and viceversa. Second, we measure the correlation between an item X  X  location in the Long Tail and its indegree. This allows us to detect whether the hubs in the network are the most popular items. Section 5.1 presents the results compar-ing two different music artists recommendation algorithms: a social recommender based on collaborative filtering, and content X  X ased audio filtering.

Item X  X entric evaluation measures the topology of the net-work, and combines this information with the Long Tail of the collection. Although, without any user intervention it is impossible to evaluate the quality and user satisfaction of the recommendations, which does not necessarily correlate with predicted accuracy [13]. The following section tries to overcome this limitation.
As of today, user X  X entric evaluation has been largely stud-ied. The most common approaches are based on the leave X  X  X  out method [6]. Given a dataset where a user has implicitly or explicitly interacted with (via ratings, purchases, down-loads, previews, etc.), split the dataset in two disjunct sets: training and test. The evaluation of the accuracy is based only on a user X  X  dataset, so the rest of the items of the catalog are ignored. The evaluation process includes several metrics such as: predictive accuracy (Mean Absolute Error, Root Mean Square Error), decision based (Mean Average Preci-sion, Recall, F X  X easure, and ROC), and rank based metrics (Spearman X  X   X  , Kendall X   X  , and half X  X ife utility) [10].
The main problem, though, is developing evaluation met-rics to deal with the effectiveness of the recommendations. That is, not only measuring prediction accuracy, but taking into account other aspects such as usefulness and quality [1].
When evaluating serendipity and novelty, it is clear that feedback from the users is needed [13]. That is to say, users must examine the recommended items and measure, Figure 2: User X  X entric plus feedback evaluation method. The test dataset is expanded to those items that the user has not yet seen in the system. The system gets feedback of the recommendations in or-der to determine their quality. to some extent, whether they accept the list of recommenda-tions. Figure 2 presents this evaluation method, we named user X  X entric plus feedback . The evaluation dataset is expanded to those items that the user has not yet seen (i.e. rated, purchased, previewed, etc.). The recommendation algorithm presents relevant items from outside the user X  X  dataset, and asks for feedback. Feedback gathering can be done in two ways: implicitly or explicitly. Measuring im-plicit feedback includes, for instance, the time spent in an item X  X  webpage, purchasing or previewing an item, etc. Ex-plicit feedback is based on two related questions: whether the user already knew the item, and whether she likes it or not. Obviously, it requires an extra effort from the users, but at the same time it provides unequivocal information about the intended dimensions (which in the case of implicit measures could be ambiguous or inaccurate). Section 5.2 presents the results comparing three different music recom-mendation algorithms: collaborative filtering (CF), content X  based audio filtering (CB), and a hybrid approach (HY).
In order to put into practice the proposed methods, we performed two experiments in the music recommendation field. It is worth noting that music is somewhat different from other entertainment domains, such as movies, or books. Tracking users X  preferences are mostly done implicitly, via their listening habits. Moreover, a user can consume any song several times, even repeatedly and continuously. Re-garding the evaluation process, music recommendation al-lows us instant feedback with a, say, 30 seconds excerpt.
The following section presents an item X  X entric evaluation comparing two different artists X  networks, one based on col-laborative filtering, and the other one based on content X  based audio similarity. After that, section 5.2 presents the results for the recommended tracks, using the user X  X entric evaluation with explicit feedback.
The main goal here is to compare novel recommendations according to two different algorithms: a social recommender based on collaborative filtering (CF), and content X  X ased au-dio filtering (CB). CF artist similarity was gathered from Table 1: Artist recommendation network properties for last.fm collaborative filtering (CF), and content X  based audio filtering (CB). N is the number of nodes, and h k i the mean degree, h d d i is the avg. shortest directed path, and h d r i the equivalent for a random network of size N , and D is the diameter of the net-work.  X  in is the power X  X aw exponent of the cumu-lative indegree distribution, and r is the indegree X  indegree Pearson correlation coefficient (assortative mixing). C is the clustering coefficient, and C r the equivalent for a random network. last.fm , using Audioscrobbler webservices 4 , and selecting the top X 20 similar artists. Last.fm has a strong social compo-nent, and their recommendations are based on the classic item X  X ased algorithm 5 [16]. To compute artist similarity, we use content X  X ased audio analysis from a music collection ( T ) of 1.3 Million tracks of 30 sec. samples. Audio analy-sis considers not only timbral features (e.g. Mel frequency cepstral coefficients), but some musical descriptors related to rhythm and tonality (e.g. key and mode) [7]. Then, to compute artist similarity we used the most representative tracks, T a , of an artist a , with a maximum of 100 tracks per artist. For each track, t i  X  T a , we obtain the most similar tracks (excluding those from artist a ): and get the artists X  names, A sim ( t The list of (top X 20) similar artists of a is composed by all A sim ( t i ) , ranked by frequency and weighted by the audio similarity distance:
Network properties of the two datasets are shown in Table 1. Both networks present the small X  X orld phenomena [17]. They have a small average directed shortest path, h d d i , close to its equivalent random network, h d r i . Also the clustering coefficients, C , are significantly higher than C r . This is an important property, because allows users surfing to any part of a music collection with a small number of mouse clicks, using only local information from the network [12].
The main differences between the two networks are the assortative mixing (Pearson r coefficient), and the power X  law  X  exponent. CF presents a high assortative mixing ( r = 0 . 92). That means that the most connected artists are prone http://www.audioscrobbler.net/data/webservices/
Although, it is clear that last.fm is using, as well, informa-tion gathered from social tagging.
 Figure 3: Item X  X entric evaluation. Assortative mix-ing (indegree X  X ndegree correlation coefficient) for last.fm collaborative filtering (CF), and content X  based (CB). CF presents assortative mixing ( r CF = 0 . 92 ), whereas CB does not ( r CB = 0 . 04 ). to be similar to other top connected artists. CB does not present any indegree correlation coefficient, thus artists are connected independently of their inherent properties. Figure 3 depicts this phenomena.

Regarding the power X  X aw  X  exponent, CF has  X  = 2 . 31, similar to those detected in many scale X  X ree networks, in-cluding the world wide web linking structure [4]. These networks are known to show a right X  X kewed power law dis-tribution, P ( k )  X  k  X   X  with 2 &lt;  X  &lt; 3, relying on a small subset of hubs that control the network [3].
Item X  X entric evaluation shows that the topology of the two networks is rather different. Now, we need to combine network analysis with the artists X  Long Tail location. Table 2 presents artist similarity divided into the three sections of the curve (head, mid, and tail). Given an artist, a i , it shows (in %) the Long Tail location of its similar artists (results are averaged over all artists). In the CF network, given a very popular artist from the head part, the probability of reaching (in one click) a similar artist in the tail is zero. Actually, half of the similar artists are located in the head part (82 artists), and the rest in the mid area. Artists in the mid part are tightly related to each other, and only 1/5 of the similar artists are in the tail part. Finally, given an artist in the tail, its similar artists remain in the same area. On the other hand, CB promotes much more the mid and tail parts in all the cases.

Another experiment analyses whether the hubs in the net-work (artists with higher indegree) are also the most pop-ular artists. Figure 4 presents the results, and CF con-firms the hypothesis; the artists with higher indegree are the ones with more playcounts, with a Pearson correlation value of r CF = 0 . 38. In CB, hubs are more spread out through all the curve. Moreover, a Markovian stochastic process is used to simulate someone surfing the recommen-dation network. Indeed, each row in Table 2 can be seen as a Markov chain transition matrix, M , being the head, r CF = 0 . 38 , r CB = 0 . 10 .
 Table 2: Item X  X entric evaluation. The table shows the similarities among artists, and their location in the Long Tail. Each row represents, also, the Markov chain transition matrix for CF and CB. mid and tail parts the different states. The values of M denote the transition probabilities, p i,j , between two states i , and j (e.g. p CF head,mid = 0 . 5468). The Markovian tran-sition matrix, M k , denotes the probability of going from any state to another state in k steps (clicks). The initial distribution vector, P (0) , sets the probabilities of being at a determined state at the beginning of the process. Then, P ( k ) = P (0)  X  M k , denotes the probability distribution after k clicks, starting in the state defined by P (0) . Using P and defining P (0) = (1 H , 0 M , 0 T ), we can get the probability of reaching the tail, starting in the head part. Table 3 shows the number of clicks needed to reach the tail from the head, with a probability p head,tail  X  0 . 4. In CF, one needs five clicks to reach the tail, whereas in CB only two clicks are needed.

Yet, we need to evaluate the quality of the relationships among artists, as well as the effects of the hubs when pro-viding novel recommendations to the users. The following section is devoted to giving some insights to these questions.
A user X  X entric evaluation, with explicit feedback, was per-formed in order to analyse three music recommendation al-Table 3: Item X  X entric evaluation. Long Tail navi-gation in terms of a Markovian stochastic process. Second column depicts the number of clicks ( k ) to reach the tail from the head part, with a probability p head,tail  X  0 . 4 . Third column shows the probability distribution after k clicks. gorithms (CF, CB and hybrid) when providing novel songs 6 CF song similarity data come, again, from last.fm . For the CB method, audio similarity is based on equation 9. Hybrid method (HY) is based on combining related artists from All-music.com musicologists, and CB audio similarity at track level. Given a seed track, the most similar tracks are com-puted this way: first, select the related artists (according to the experts) from the artist seed track. Then, rank all the tracks from the related artists, according to the audio similarity.
The experiment was based on providing personalised song recommendations to users, using some seed tracks from their top X 20 most played artists, based on their last.fm profiles. Recommended songs (evenly distributed from CF, CB and HY) had no metadata displayed (neither artist name nor song title), but only a preview of 30 seconds. Provided feed-back included whether the user knew the song ( no , recall only the artist , recall artist name and song title ), and the quality of the recommendation X  X hether she likes the song or not X  X n a rating scale from 1 ( I don X  X  like it ) to 5 ( I like it very much ). After running the experiment during March 2008, 5,573 tracks were rated by 288 users (average of 19 tracks rated per user).
The experiment is available at: http://foafing-the-music.iua.upf.edu/survey Method Case % Avg.Rating (Stdev) Table 4: User X  X entric evaluation. Novelty analy-sis for last.fm collaborative filtering (CF), Hybrid (HY), and audio content X  X ased (CB) algorithms.
 Recall A&amp;S means that a user recognises both A rtist name and S ong title.
Table 4 presents the overall results for the three algo-rithms. It shows, for each algorithm, the percentage of songs that users identify in the recommendations (i.e. they are fa-miliar with), as well as the novel, unknown ones, and the quality of the recommendations (average rating and stan-dard deviation).

To compare the three algorithms we performed a one X  way ANOVA within subjects. For familiar recommenda-tions (including both artist and song known , and recall only artist ) there is no statistically significant difference among the ratings of the three algorithms. The main differences are found in the ratings of unknown songs (3 . 03 CF vs. 2 . 77 vs. 2 . 57 CB , F (2 , 287) = 17 . 27, with p  X  0 . 01), and in the percentage of unknown songs (71.69% CF vs. 78.34% HY vs. 80.97% CB , F (2 , 287) = 32 . 69, with p  X  0 . 01). In the former case, Tukey X  X  test for pairwise comparisons confirms that CF scores higher than HY and CB, at 95% family-wise confidence level. However, according to the latter case, CF generates more familiar songs than CB and HY (also vali-dated by the corresponding Tukey X  X  test). Thus, CB and HY provide more novel recommendations, although their quality is not as good as CF (see Figure 5).
The results from the item X  X entric analysis show that, us-ing last.fm CF algorithm, the popularity effect that arose from the community has consequences in the recommenda-tion network. This reveals a somewhat poor discovery ratio when just browsing through the network of similar music artists. It is not easy to reach relevant Long Tail artists, starting from the head or mid parts. Moreover, similar artists all located in the tail area do not always guarantee novelty. A user that knows quite well an artist in the Long Tail is likely to know most of the similar artists too (e.g. the solo project of the band X  X  singer, collaborations with other musicians, and so on). Thus, these might not be considered good novel recommendations to that user, but familiar ones.
The key Long Tail area in CB are the artists located in the mid part. These artists allow users to navigate inside the Long Tail acting as entry points, as well as main destinations when leaving the Long Tail. Users that listen to mainly very unknown (Long Tail) music are likely to discover artists Figure 5: User X  X entric evaluation. Box X  X nd X  whisker plot showing the ratings for unknown songs. that are in the mid part, and that are easily reachable from the artist in the tail. One should pay attention, too, to the quality data in the Long Tail. Assuming that there exists some extremely poor quality items, CB is not able to clearly discriminate it. In some sense, the popularity effect drastically filters these low quality items. Although, as shown in [15] popularity is only partly determined by quality.

Regarding user X  X entric evaluation, in general, user per-ceived quality for novel, unknown, recommendations is on the negative side (avg. rating around 3 / 5 or less, see Ta-ble 4). This probably emphasises the need for adding more context when recommending unknown music. Users might want to understand why a song was recommended. Rec-ommender systems should give as many reasons as possible, even including links to external sources (reviews, blog en-tries, etc.). Besides, the limitation in the experiment of using only 30 sec. samples did not help to assess the quality of the song. Yet, there are lots of industrial music recom-mender systems that can only preview songs due to licensing constraints. So, our experimental constraint is not that far from the reality.

An interesting result obtained is that, to provide famil-iar items, any of the three proposed algorithms works fine. This has some implications when designing a recommender system. For instance, to provide a Radio X  X  X  X a X  X arte experi-ence, the system does not need to rely exclusively on millions of users, but it can do that quite well with a state X  X f-the X  X rt audio similarity algorithm.

The context X  X ree and popularity agnostic CB algorithm sometimes points in the wrong direction (it is not that easy to discriminate between a, say, classical guitar and a harp-sichord, based solely on the audio content), and gives poor or non X  X ense recommendations. This leaves room for im-provement the audio similarity algorithm. In this sense, the proposed Hybrid approach drastically reduces the space of possible similar tracks, to those artists related with the orig-inal artist. This avoids, most of the time, the mistakes per-formed by the pure CB. CF tends to be more conservative, providing less novel recommendations, but of higher quality.
We can envision different solutions to cope with novelty in recommender systems. The first one is to use CF, pro-moting unknown artists by means of exploiting the Long Tail information of the catalog, and the topology of the rec-ommendation network. Another option is switching among algorithms when needed. E.g. to avoid the cold X  X tart prob-lem and, at the same time, to promote novelty, the best option is to use CB or HY. After a while, the system can move to a stable CF or HY approaches. Moreover, the sys-tem should be able to change the approach according to the user X  X  needs. Sometimes, a user is open to discovering new artists and songs, while sometimes she just wants to listen to her favourites. Detecting these modes and acting accordingly would increase user X  X  satisfaction with the sys-tem. However, this leads us to the topic of user profiling, which is a different kind of problem than the one dealt in this paper. In this paper we have presented two methods, named Item X  and User X  X entric evaluation. The former focuses on analysing the topology of the recommendation network, and then combining the results with the popularity of the items. The latter method, user X  X entric aims at measuring the per-ceived quality of the recommendations using explicit feed-back.

The results of the experiments show that CF is prone to popularity bias, affecting both the topology of the network, and the novelty recommendation ratio, whilst pure CB is not affected by popularity. However, a user X  X entric exper-iment performed with 288 subjects and 5,573 tracks shows that, even though CF recommends less novel items than CB, users X  perceived quality is higher than that for those recom-mended by pure CB.

Future work includes expanding the analysis of the recom-mendation network, taking into account its dynamics. This could be used, for instance, to detect  X  X ype X  items. Re-garding user X  X ased recommendation algorithms, a similar network analysis can be applied, now the nodes being users. An interesting feature would be to detect trendsetters, and its effect when providing novel recommendations. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] C. Anderson. The long tail. Why the Future of [3] A. L. Barab  X asi and R. Albert. Emergence of scaling in [4] A.-L. Barab  X asi, R. Albert, H. Jeong, and G. Bianconi. [5] D. Billsus and M. J. Pazzani. User modeling for [6] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [7] P. Cano, M. Koppenberger, and N. Wack. An [8] ` O. Celma and P. Lamere. Music recommendation [9] D. M. Fleder and K. Hosanagar. Blockbuster Culture X  X  [10] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [11] K. Kilkki. A practical model for analyzing long tails. [12] J. M. Kleinberg. Navigation in a small world. Nature , [13] S. M. Mcnee, J. Riedl, and J. A. Konstan. Being [14] M. E. J. Newman. Assortative mixing in networks. [15] M. J. Salganik, P. S. Dodds, and D. J. Watts. [16] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [17] D. J. Watts and S. H. Strogatz. Collective dynamics of [18] L.-T. Weng, Y. Xu, Y. Li, and R. Nayak. Improving [19] Y. Yang and J. Z. Li. Interest-based recommendation [20] Y. Zhang, J. Callan, and T. Minka. Novelty and [21] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
