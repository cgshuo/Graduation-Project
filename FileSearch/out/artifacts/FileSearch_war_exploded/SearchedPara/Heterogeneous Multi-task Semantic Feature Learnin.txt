 Multi-task Learning (MTL) aims to learn multiple related tasks si-multaneously instead of separately to improve generalization per-formance of each task. Most existing MTL methods assumed that the multiple tasks to be learned have the same feature representa-tion. However, this assumption may not hold for many real-world applications. In this paper, we study the problem of MTL with heterogeneous features for each task. To address this problem, we first construct an integrated graph of a set of bipartite graphs to build a connection among different tasks. We then propose a multi-task nonnegative matrix factorization (MTNMF) method to learn a common semantic feature space underlying different heteroge-neous feature spaces of each task. Finally, based on the common semantic features and original heterogeneous features, we model the heterogenous MTL problem as a multi-task multi-view learn-ing (MTMVL) problem. In this way, a number of existing MTMVL methods can be applied to solve the problem effectively. Extensive experiments on three real-world problems demonstrate the effec-tiveness of our proposed method.
In many real-world scenarios, one needs to collectively solve a number of related tasks, where little side information (e.g., labels) is available for each task. To solve this kind of problems, multi-task learning (MTL) has been proposed [5].

Most existing MTL methods assumed that all the tasks have the same feature representation as shown in Figure 1(a). Though this assumption holds for some applications, it may not hold for many other applications. For example, given an emergent event in social media, suppose that one task is to predict whether a social post in text is related to the event, and another task is to predict whether a social image is related to the event. On one hand, these two tasks are related as they are to make predictions on the same event. On the other hand, for each of these two tasks, label information is lim-ited as the event is emergent. Therefore, MTL methods are desir-able to solve these two tasks. However, most existing MTL meth-ods are not applicable here because the feature representations of these two tasks are totally different (i.e., image pixels v.s. textual words) as shown in Figure 1(b). In this paper, we proposed a new method to solve the problem of MTL with heterogeneous feature spaces.

Our motivation is that though the instances of the different and related tasks are represented in different feature spaces, they may share a same semantic feature space. As in the example on social media event prediction described above, though an event-related image and an event-related post are represented differently, they should share the same semantic meanings because both of them describe the event. Once such a common semantic feature space is discovered, it can be used to share knowledge among multiple heterogeneous tasks, and thus improve their learning performance.
Specifically, in this paper, we assume multiple tasks share the same output space (i.e., the class labels of different tasks are the same or at least overlapping). Firstly, for each task, we build a bipartite graph to model the relationship between the labeled in-stances and the class labels. Secondly, based on the bipartite graph-s of each task, we integrate them (i.e. the corresponding multiple tasks) through the layer of the class labels. After that we further build a correlation matrix between the class labels and the input features for each task based on the integrated graph. Finally, we propose a Multi-Task Nonnegative Matrix Factorization (MTNM-F) method on the constructed correlation matrices as well as the original instance-feature matrices, which consists of both labeled and unlabeled data of each task, to learn a common semantic fea-ture space underlying the multiple tasks. Once the semantic feature space is learned, together with the original heterogeneous features for each task, we can apply Multi-Task Multi-View Learning (MT-MVL) methods to solve the target heterogenous MTL problem.
In summary, our proposed MTNMF method has several advan-tages. 1) It solves the problem of heterogeneous multi-task learning without requiring any correspondences between tasks. 2) It ful-ly makes use of both labeled and unlabeled instances to learn the common semantic feature space for multiple tasks. 3) The learned semantic features and the original features can be concatenated to form a MTMVL problem, where a number of existing MTMVL methods can be applied.
In the past decade, MTL has attracted a lot of attention. Pre-vious work on MTL was focused on learning multiple tasks with homogeneous features. Most methods aim to learn common fea-tures among different tasks [5, 2, 15], or common predictive struc-ture underlying different tasks [1, 6], or common prior of model parameters among different tasks [11, 21, 22].

Recently, several heterogeneous MTL methods have been pro-posed. Zhang and Yeung [24] proposed the Multi-task Discrimi-nant Analysis (MTDA) algorithm, which aims to learn transforms for instances of heterogeneous features such that the transformed instances of the same class from different tasks are closer to each other. However, MTDA is a supervised learning method, which fails to exploit unlabeled instances to learn to the transformation. He et al. [14] also proposed the MUSH algorithm for heterogenous MTL. However, in MUSH, some correspondence among inputs of different tasks is assumed to be given in advance.

In transfer learning, there have been some methods proposed for cross-domain/task learning with heterogenous features [25, 10, 20, 8]. However, different from transfer learning, the objective of MTL is not to transfer knowledge from a domain/task to another domain/task, but learn a prediction model for each task simultane-ously by exploiting relatedness among the tasks.

Multi-Task Multi-View Learning (MTMVL) is a special setting of MTL, where each task has multiple views rather than a single view. State-of-the-art approaches to MTMVL include IteM 2 which is a transductive learning method, regMVMT [23], CSL-MTMV [16] and MAMUDA [17], which are inductive learning methods.
In this paper, we denote by X ( i;j ) the element in the i -th row and j -th column of a matrix X ,  X  X  X  X  the Frobenius norm, and I identity matrix. In addition, we denote by [ N : M ] ( N &lt; M ) a set of integers in the range between N and M inclusively.

Suppose we are given T related classification tasks. For each task t  X  [1 : T ] , there are n t labeled and m t unlabeled instances. The dimension of an instance of the t -th task is d t . A nonnegative matrix X t  X  R n t  X  d t  X  0 is used to denote the labeled instances of task t , each row of which represents an instance. Accordingly, P  X  R m t  X  d t  X  0 is used to denote the corresponding unlabeled in-stances of task t . For simplicity, we suppose that the classification tasks are binary, and different tasks have the same set of class label-s. Let Y t  X  [  X  1 ; 1] n t  X  1 be the label vector of the labeled instances of task t . Recall that, for each task t , we have a set of labeled instances X t and their corresponding labels Y t . Based on X t , we first build a bipartite graph to capture the relationship between the labeled instances and the class labels. To be specific, we use a matrix U t  X  C to represent the bipartite graph for task t , where C is the number of classes 1 , U t ( i;j ) =1 if the i -th instance belongs to the j -th class, otherwise, U t ( i;j ) =0 . As the class labels of multiple tasks are assumed to be the same or at least overlapping, the multiple bipartite graphs can be integrated into a unified graph through the layer of the class labels. An example of the integrated graph of a pair of tasks (i.e., a pair of bipartite graphs) is shown in Figure 2.
Note that in general, C can be larger than 2. For simplicity, here we assume the multiple classification tasks be binary.

Based on the matrix U t , which is used to represent the rela-tionship between instances and classes, we can further generate a correlation matrix G t between input features and class label-s for task t by setting G t = X t  X  U t  X  R d t  X  C , where G feature of the k -th labeled instance of task t . It can be shown that G t ( i;j ) is large if there are many instances whose the values of the i -th feature are large and class labels are the j -th class. This implies that G t ( i;j ) is large if the i -th feature and the j -th class have strong correlation. For each task, to extract latent semantic features, one can apply NMF to decompose G t to latent factor matrices, where W t  X  R d t  X  k , H t  X  R k  X  C , and k is the dimension of the latent semantic space. Each column in W t can be referred to as a base vector of the latent space, which is represented by a linear combination of the original task-specific features. The i -th column of H t can be referred to as the latent semantic representation for i -th class. The constraint W t  X  W t = I is to ensure the solution to be unique and reduce redundancy [9].

For MTL, because the multiple tasks to be learned are assumed to be related, it is more desirable to learn the semantic features for each task jointly by exploiting their relatedness. In addition, as from the integrated graph, the layer of label classes is shared by all the tasks, intuitively, we can collectively learn W task by enforcing the class representations { H t } X  X  to be the same. Therefore, we propose to decompose { G t }  X  X  jointly as follows:
Note that the optimization problem (2) can facilitate knowledge transfer among multiple tasks through sharing H in semantic fea-ture learning for different tasks. However, (2) only makes use of labeled data because G t is constructed based on labeled instance only. However, in MTL, because labeled data for each task are scarce, the semantic features learned from (2) may not be robust and reliable. Therefore, how to exploit unlabeled data in semantic feature learning for different tasks is crucial. Intuitively, besides performing collective NMF on { G t } X  X  to learn W t jointly, one can also apply NMF on the original data, which include both labeled and unlabeled instances, to learn W t for each task. To be specific, solving the following NMF problem: resentation of X  X  t under the new bases { W t } X  X . By combining (2) and (3), our proposed Multi-Task NMF method for semantic fea-ture learning can be written as follows: where t &gt; 0 is a tradeoff parameter to balance the importance be-tween the labeled and unlabeled data. For simplicity, t is set to 1 in this paper, which means that we assume the two terms in (4) be equally important. Note that the optimization problem (4) is not convex for V t , W t and H jointly. To solve (4), we use an alter-native optimization approach to alteratively optimize one variable while fixing the other variables. The update rules of the alterative optimization approach are summarized as follows: matrices W t , V t , and H , respectively. By applying the update rules in (5), (6) and (7), the solution converges to a local optima. Existing approaches to proving the convergence of the NMF algorithm can be adapted to prove the convergence of our proposed algorithm.
For each task t , one can use the learned matrix W t to map the original data to the common semantic space underlying all the T tasks via X ( c ) t = X t W t . Combining with the original features, in total, there are T +1 views for a heterogeneous MTL problem with T tasks. MTMVL techniques can be applied. In this paper, we adopt the regMVMT algorithm [23] 3 .
In Table 1, N p and N n are the numbers of positive and negative instances, respectively. On the second and third datasets, different tasks have some overlapping features. Besides comparing with het-erogeneous MTL methods on the three datasets, we also compare MTNMF with homogeneous multi-task learning baselines on the second and third datasets.
 Table 1: Statistics of Data Sets with Heterogeneous Features 20Newsgr oups &amp; ImageNet Classification: The documents are from the 20 Newsgroups dataset 4 , while the images are from the ImageNet dataset 5 .

Email Spam Detection [3]: each task has a set of specific fea-tures which only include the words appear in the corresponding person X  X  emails. When conducting comparison experiments with
Due to the limited space, the detailed proof is omitted.
In general, any MTMVL method can be adopted. http://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.image-net.org/download-features homogeneous MTL methods, we use another feature representation for emails that is based on a unified vocabulary for all the tasks.
Sentiment Classification: we use the multi-domain sentiment classification dataset [4]. The features are similarly constructed as for Email Spam dataset.
For each configuration, we perform 10 random trials and report the average classification accuracy.
TSVM: Transductive SVM (TSVM) [18] is a semi-supervised learning method, we use the SVM-light 6 implementation for the TSVM classifier.

NMF: we first apply NMF on both the labeled and unlabeled data to learn semantic features for each task separately as shown in Eq.(3), and then train a TSVM classifier for each task with the learned semantic features separately.

MTDA: Multi-task Discriminant Analysis (MTDA) [24] is a multi-task learning algorithm that can deal with heterogeneous features across different tasks.
Four multi-task learning algorithms aim at problems with ho-mogeneous features are tested, they are GMTL [19], rMTFL [12], DirtyMTL [15] and RMTL [7].
Comparison results of MTNMF with the first group of baseline methods on the three datasets are shown in Tables 2, 3, 4 respec-tively. As shown on the tables, MTNMF performs better than NMF though both of these two methods are based on nonnegative matrix factorization. This is because the semantic features for different tasks extracted by MTNMF are not only based on the factorization on the original data matrix, but also based on the factorization on the integrated bipartite graphs which capture the correlation among different tasks. Moreover, in general, multi-task learning methods, MTNMF and MTDA, outperform the methods that learn differen-t tasks individually, such as TSVM and NMF. This is because for each task, labeled information is too sparse to learn a precise pre-diction model. Last but not least, the superiority of MTNMF over MTDA suggests that the semantic features learned by MTNMF is more effective for solving multi-task learning problems with het-erogeneous features.
 Table 3: Experimental Results for 20Newsgroups&amp;Imagenet
In the second series of experiments, we compare performance between different methods under varying numbers of labeled and unlabeled training data. Note that the sizes of labeled and unlabeled data are set to be the same for this series of experiments. The results are shown in Figure 3, where both average results and standard deviation of 10 random runs are reported. As can be seen from the http://svmlight.joachims.or g/ figure, MTNMF performs best under different number of training data for these 3 problems, which shows the advantage of learning a shared latent semantic space from multiple tasks. (a) 20Ne wsgroups &amp; ImageNet Figur e 3: Experimental Results for 3 Problems (Heterogeneous Features)
To conduct comparison experiments with homogeneous MTL methods, i.e., the second group of baseline methods, we use the Email Spam Detection and Sentiment Classification dataset with a unified feature representation for different tasks as described in Section 5.1. As the baseline methods are supervised learning ap-proaches, in this series of experiments, we only use labeled train-ing data for all the comparison methods including MTNMF. The comparison results in terms of classification accuracy are shown in Figure 4 under varying sizes of training instances. As can be seen from the figures, by extracting semantic features for each task col-lectively, MTNMF can also boost the performance of homogeneous MTL.
Figur e 4: Experimental Results (Homogeneous Features) In this paper, we propose the Multi-Task Nonnegative Matrix Factorization (MTNMF) method to solve multi-task learning prob-lems with heterogeneous feature spaces. In MTNMF, a set of inte-grated bipartite graphs are built based on the labeled data to model the relationship between original features and class labels among multiple tasks. A collective NMF method is then proposed to ex-tract common semantic features from the integrated bipartite graphs as well as the unlabeled data for different tasks. Experiments on 3 real-world problems demonstrate the effectiveness of the proposed method.
This work is supported by the National Natural Science Foun-dation of China (No. 61473273, 61473274, 61175052, 61203297), National High-tech R&amp;D Program of China (863 Program) (No. 2014AA015105, 2013AA01A606).
