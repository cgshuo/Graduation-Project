 Web search queries without hyperlink clicks are often re-ferred to as abandoned queries . Understanding the reasons for abandonment is crucial for search engines in evaluat-ing their performance. Abandonment can be categorized as good or bad depending on whether user information need-s are satisfied by result page content. Previous research has sought to understand abandonment rationales via us-er surveys, or has developed models to predict those ra-tionales using behavioral patterns. However, these models ignore important contextual factors such as the relationship between the abandoned query and prior abandonment in-stances. We propose more advanced methods for model-ing and predicting abandonment rationales using contextu-al information from user search sessions by analyzing search engine logs, and discover dependencies between abandoned queries and user behaviors. We leverage these dependen-cy signals to build a sequential classifier using a structured learning framework designed to handle such signals. Our ex-perimental results show that our approach is 22% more ac-curate than the state-of-the-art abandonment-rationale clas-sifier. Going beyond prediction, we leverage the prediction results to significantly improve relevance using instances of predicted good and bad abandonment.
 Web search abandonment; Structured learning; User behav-ior analysis
It is well-known that search engines leverage user feed-back to improve result relevance [17]. Beyond explicitly asking human assessors to annotate the relevance of docu-ments with respect to queries, an alternative, more scalable approach is to use implicit feedback from searchers to learn document preferences. User clicks are among the most effec-tive signals to learn ranking functions [17], with the assump-tion that clicked documents are often more relevant than non-clicked ones. However, when presented with a search engine result page (SERP), users may elect not to click a result hyperlink. We refer to this scenario as search aban-donment . User studies have analyzed the rationales for a-bandonment and the associated search behaviors [19, 22, 23, 11, 28]. These studies revealed two primary abandonment rationales: (1) bad abandonment indicating user frustration and dissatisfaction, and (2) good abandonment suggesting satisfaction without needing to click. Good abandonmen-t is not uncommon in modern search engines, where direct answers such as weather and stock quotes are returned for queries with explicit intent. In addition, text from result snippets can also satisfy users X  information needs directly [15, 21].

Accurately categorizing abandoned queries into good and bad abandonment can be critical for search engine relevance improvement, search success estimation [13, 14], as well as helping other decision-making tasks such as online experi-mentation (e.g., A/B testing). In this paper, we extend pre-vious work in the area of abandonment classification/prediction [11], where the task is to predict the rationale for an observed abandonment instance. In particular, we explore the use of contextual information in search sessions to improve predic-tion accuracy. From user study data collected through an in-situ survey, we observed several key dependencies between abandoned queries within search sessions. Consequently, we elect to approach this problem using a structured learning framework, which can capture interactions between queries, as well as leveraging both query and session-level features for predicting abandonment rationales.

Specifically, this paper makes the following contributions: 1. We study user behavior associated with abandonment in a large-scale data set of logged search behavior. We ex-amine both query-level and session-level behaviors, and the relationship between the abandoned queries and other activ-ities in the full search session. We discover several key char-acteristics that distinguish bad abandonment from good a-bandonment, including query/session length and inter-query time. More importantly, we also discover that adjacent a-bandoned queries tend to share the same rationale. 2. Inspired by these characteristics, we model the a-bandonment prediction problem using a structured learn-ing framework. We propose a set of joint-label/observation features to help fully capture dependencies between aban-donment instances within the same search session. The new framework more accurately predicts abandonment rationales than state-of-the-art classifiers [11]. 3. We further leverage the prediction framework to im-prove search engine relevance. We propose a variant of click-through rate that consider abandonment rationale, and pro-pose a new way to extract pair-wise click preference data for training a new ranker. Experiments show that these using enhancements as features can improve ranking performance.
The remainder of this paper is organized as follows. Sec-tion 2 presents related work on Web search abandonment; Section 3 introduces the user survey data used in this study; Section 4 details the user behavior analysis from search en-gine logs; Section 5 presents our structured learning frame-work for abandonment prediction; Section 6 shows the em-pirical study, and Section 7 concludes with future work.
Traditionally, search-result clicks have been treated as positive signals from which search engines can learn to es-timate query-document relevance [17]. In the absence of clicks, it is difficult to explain whether SERP content was relevant and satisfied the searcher. In such cases, researchers often resort to editorial judging to gather feedback regarding abandonment rationales. Li et al. [19] explored the concept of good abandonment to distinguish between good and bad abandonment instances. In their definition, good abandon-ment describes a situation where a query was successfully addressed by the SERP without requiring clicks or query re-formulations. They analyzed such scenarios on both desktop and mobile devices in three geographic locales, by randomly sampling a small number of queries from search engine logs, and applied their own judgment criteria to categorize each query into one of the two cases. They showed that good abandonment is more common and more likely on mobile devices. The authors also discovered that the type of aban-donment varies significantly by locale and modality. Thuma et al. [24] showed that those who were accustomed to find-ing answers quickly tended to abandon their searches faster if they were dissatisfied. Chuklin and Serdyukov examined various aspects of abandonment, including its relationship with query extensions [9], and correlations between aban-donment and editorial judgments [8]. Most relevant to our work, Chuklin and Serdyukov also developed models to pre-dict good abandonment using topical, linguistic, and historic features [10]. However, they did not use session interactions or in-situ judgments from abandoning searchers, as we do here. This meant that they could only predict  X  X otentially good X  abandonment.

Rather than gathering third-party assessments of aban-donment rationales, Stamou et al. [22] employed a small number of participants to complete an external survey for each query they performed on a single day. In their study, 13% of queries had no clicks, categorized as either unin-tentional such as no search results retrieved and search got interrupted, or intentional such as spelling check or exam-ining the retrieved snippets to understand the query. In a follow-up study, the same authors recruited six participants, who each installed a browser plug-in to record search activ-ity and elicited abandonment rationales via a survey [23]. They found that 27% of queries were abandoned, and ratio-nales were split evenly between good and bad. In another small-scale study, Koumpouri et al. [18] studied SERP in-teractions and found that total dwell time/scrolling time, and copying caption text all correlated with satisfaction.
Diriye et al. [11] conducted a much larger user study of 928 searchers and analyzed their search behavior over a one-month period. They deployed a Web browser plug-in to each participant that displayed a survey whenever a user aban-doned a query. The survey asked participants whether their information need was met by the abandoned SERP. Their definition of abandonment was similar to Li et al. [19], with some important differences such as using a timeout of 30 minutes as one of the abandonment criteria, instead of a 24-hour period from Li and colleagues. Diriye et al. found that 41% of abandonments were bad, 32% abandonments were good, with the remaining 27% associated with alter-nate reasons such as choosing a better query before con-sidering the returned SERP. They developed a classifier to predict abandonment rationale by generating features from sessions, queries and documents and classify the data into four categories: SAT , DSAT , Unintentional ,and Other .
As an alternative to running a user study, abandonmen-t rationales can be estimated from logged behavioral da-ta. Castillo et al. [5] sought to improve the accuracy of click-based metrics by identifying truly bad abandonments. The authors developed a method to automatically assess each searcher in terms of their tenacity, i.e., how likely they were to abandon a query without trying hard enough be-fore they really failed. They then picked a set of tenacious users and measured the relative abandonment ratio between queries with direct answers and queries without. For the task of identifying bad abandonment, their method demon-strated over 80% precision for weather and reference queries. Chilton et al. [7] used searchers X  repeat search behavior to understand SERP relevance sans interaction behavior such as clicks. The authors focused on direct answers such as flight status, stock, and weather. They showed that certain types of good answers can compensate for missing clicks and still satisfy users X  information needs. Following this work, Bernstein et al. [3] combined log mining with paid crowd-sourcing to aggregate direct answers for tail queries.
Huang et al. [15] proposed to leverage mouse cursor move-ments as an implicit signal to improve search relevance in the absence of user clicks. They reported a gaze-tracking study in which they demonstrated that cursor position is closely related to eye gaze on SERPs. To improve search quality, the authors gathered relevance labels for 1200 query-URL pairs and estimated the correlation between the labels and cursor features. When a query has no clicks, hover features show a fair correlation with human labels. The authors al-so leveraged cursor features such as movement speed and trail length to distinguish good and bad abandonment, and highlighted some interesting trends (e.g., when answers were present, users moved their mouse cursor more slowly).
Sakai et al. [21] labeled good abandonment as direc-t information access. They also defined immediate infor-mation access as situations where searchers can locate, for example, the result snippet quickly from the SERP. The authors proposed a new way of evaluating direct and im-mediate information access using methods similar to multi-document summarization or question-answering evaluation-s. They considered the rank position of the snippet matches during evaluation and showed that their measure can cor-rectly reward systems that quickly return relevant snippets. Our research extends previous work in a number of ways. First, we characterize key aspects of abandonment behavior within search sessions, especially the close relationship be-tween adjacent abandonment instances. Second, we apply a structured learning framework, which is more sophisticated than the standard learning methods applied to this prob-lem thus far. This allows us to capture key dependencies between abandonment instances hitherto ignored in aban-donment modeling. Importantly, our analysis is performed using in-situ judgments from abandoning searchers, allow-ing us to more accurately model the abandonment process. Finally, we apply our predictions in a retrieval setting and show that we can significantly improve search relevance by directly modeling abandonment using our framework.
Previous work has thoroughly studied the problem of mod-eling search satisfaction at the session or task level [13, 14, 1]. Our work is related to this line of research because we use the entire session to provide context for every abandon-ment instance. However, our work also differs since we do not predict satisfaction at the session level; rather we do so for every abandoned query.
This section introduces the data set used in this study. We first describe the data collection process and then present some basic statistics of the data.
The data used were graciously provided by Diriye and col-leagues, the authors of a prior investigation on search aban-donment [11]. In that study the authors developed a Web browser plug-in that displayed a survey in a popup window to the searcher asking for an explanation whenever SERP a-bandonment was detected on the Bing search engine. Diriye et al. deployed the plug-in within Microsoft Corporation, where it was installed by employees who agreed to provide data on the URLs they visited and provide labels explaining the rationale for the abandonment when it occurred. The plug-in captured participant responses to questions in the popup as well as Web interaction data (including clicks and cursor movements using the methodology described in [4]) and SERP contents (including the top 10 results and pres-ence/absence of direct answers). In the remainder of this section, we focus on two aspects of the data important to our current study: (1) how abandonment was defined (i.e., when the popup survey should be shown to searchers), and (2) what information was collected by the popup. We refer the reader to [11] for specific details, but provide an overview here for completeness.

Abandonment in the data that we received was defined as a situation when the SERP was displayed and the fol-lowing two conditions were met: (1) no hyperlink clicks on results, advertisements, or direct answers, and (2) a trigger event occurs. For (2), in addition to defining what counts as abandonment, Diriye et al. also defined the point in time that the determination of no clicks (condition 1) should be made. The authors defined a number of abandonment trig-gers comprising one the following actions: manual re-query, browser tab closure, manual URL entry, click related search or spelling suggestion, change search scope, or timeout. Af-ter these two criteria were met, a popup survey was shown asking that respondents indicate their abandonment ratio-nale. The survey captured four reasons: (1) SAT :Satisfied with the content of the SERP; (2) DSAT : Dissatisfied with the results presented; (3) Interrupted or Unimportant :The user was interrupted in their search task (leading to a time-out or other action), or the abandonment was unintentional (e.g., closed browser accidentally), or (4) Other :Anyrea-son beyond what was captured in 1-3, with the rationale recorded directly in free text. For SAT, the popup also cap-tured whether participant satisfaction was related to direct answers on the SERP, the captions of the search results, or other SERP content.

To prevent the popup from appearing too often, Diriye et al.: (1) employed a trigger control mechanism that sup-pressed the popup for 50% of all SERP abandonments on a per user basis, and (2) imposed a popup limit of 10 times per day per user. This has implications for the current study since although we record all instances of abandonment, we do not have the abandonment rationales for all of them.
We now briefly describe the data set. The authors in [11] gathered a total of 7,419 labeled abandonment instances 1 from 928 participants. They also recorded participants X  click and browse activities, totaling of 739,505 URLs and 39,606 queries. After filtering irrelevant queries (e.g.,  X  X est X ), the resultant data set contained the following distribution of a-bandonment instances: 3,104 SATs, 2,524 DSATs, 501 In-terrupted or Unimportant, and 1077 Other. On average, a user contributed 7.4 labels, while the most productive user labeled 174 queries. We sorted the queries by their frequen-cies and noted that (1) top-ranked good abandonment were mostly stock or weather related (e.g.,  X  X ok X ,  X  X oston weath-er X ), and (2) bad abandoned queries were more diverse, but in general longer than good abandoned queries (e.g.,  X  X er-centage nfl games predicted winner percentage X ). In the next section, we analyze user behavior surrounding abandonment at the query and session level.
In this section, we examine the search abandonment be-havior and investigate the behavioral patterns associated with abandonment in satisfaction or dissatisfaction. The main focus of this section is to understand the differences between good and bad abandonment in a qualitative way. Since abandonment is directly related to user satisfaction, we will be using good abandonment and SAT , bad aban-donment and DSAT , interchangeably.
As we discussed in Section 3, the labels of our data are gathered at the query level, i.e. if a query does not have any hyperlink result click on its SERP (i.e., the query is aban-doned), a popup may be shown and the searcher is asked to provide the abandonment rationale. We first investigate the differences between good and bad abandonment on the query level, beginning with some query characteristics.
The box plots in Figure 1(a) show the differences in query lengths (in terms) of good (SAT) abandoned queries and bad
The authors in [11] reported using 1,799 labels collected over a 30-day period. However, they continued to collect data beyond the 30 days used in their original study. Our data set is substantially larger since it included abandon-ment instances collected during this additional time. Figure 1: Box plots of (a) query lengths (i.e. num-ber of terms) and (b) time to next query of bad abandoned queries and good abandoned queries. (DSAT) abandoned queries. Although both types of aban-doned queries do not have hyperlink clicks, from Figure 1(a), we see that good abandoned queries are on average short-er than bad abandoned queries with statistical significance (mean query length: SAT=2.42, DSAT=3.01; t (2996.0) =  X  9.65; p -value &lt; 2 . 2 e  X  16), which is consistent with pri-or work [1]. Since it is well-known that query length is an effective indicator of query difficulty, Figure 1(a) suggests that DSAT abandonments are associated with more difficult queries than those abandoned with SAT.

We find that query length is not the only feature that dif-fers significantly between good and bad abandonment. Fig-ure 1(b) shows the difference of time to next query of these two types of abandonment. The difference there is even more pronounced. For bad abandoned queries, the median time to next query is 0.3 minutes; and for good abandoned queries, the median is 10.07 minutes ( t (2866.4) = 19.03 with p -value &lt; 2 . 2 e  X  16). A short time interval (e.g., under 5 minutes) between two consecutive queries may imply dissatisfaction the first query and the subsequent reformulation of a simi-lar search intent. Figure 1(b) illustrates that bad abandoned queries are more likely to be followed by reformulation than good abandoned queries, and examining if an abandoned query is followed by a reformulation is a very effective way in identifying user satisfaction (and this was also shown in the analysis of features in the classifier developed by [11]).
In addition to considering abandonment at the individu-al query level, we also seek to understand how abandoned queries correlate with other closely related queries and the a-bandonment behavior within search sessions. We use search sessions identified using a 30-minute inactivity timeout [11].
From our recorded search log, we collect all sessions that contain at least one labeled abandonment query, 2,483 such sessions in total. Among these sessions, 446 sessions are single-query sessions. 74 . 9% are abandonment queries la-beled as SAT, and only 25 . 1% of them are labeled as DSAT. Furthermore, for sessions with multiple queries, we find DSAT abandonment queries are more likely before the last query in a session (i.e., 84 . 8% of DSAT abandonments hap-pen before the last queries of sessions versus 15 . 2% DSAT abandoned queries which are the last queries of sessions). Comparing with DSAT abandoned queries, SAT abandoned queries are less likely to occur before the last queries of ses-sions (60 . 3%) and more likely to be the last queries of ses-Figure 2: The transition probability between aban-donment labels within the same session. sions (39 . 7%). Both facts regarding abandoned queries in single-query sessions and multiple-query sessions are consis-tent with Figure 1(b), i.e. DSAT abandoned queries are more likely to be followed by reformulations and thus are more likely to co-exist with other queries and be followed by other queries in the same sessions.

The findings of the analysis presented thus far consistent-ly show that bad abandoned queries are more likely to be followed by reformulations, while good abandoned queries are more likely to be either the single query of a session or the last query of a session. Now we further examine the cas-es when there multiple labeled abandoned queries appear in the same search sessions. Specifically, we want to under-stand the likelihood of observing a session with both DSAT and SAT abandonment and understand the transitions be-tween the two rationales in the same session.

The transition between labeled abandonment query pair is defined as follows: if two labeled abandoned queries q and q 2 are in the same session and there is no other la-beled abandoned query between them (non-labeled query is permitted), then the transition between this query pair is T
L 1 L 2 ,where L 1 and L 2 are the SAT or DSAT labels of q and q 2 . From our data, we observe 268 cases of transition from DSAT to DSAT, 177 transitions from SAT to SAT, 57 transitions from DSAT to SAT, and 39 transitions from SAT to DSAT. The transition probabilities are illustrated in Figure 2. This means that good abandoned queries tend to co-exist in a session and same is true for bad abandoned queries. Conversely, it is much less likely for both SAT and DSAT abandoned queries to be observed in the same session.
We notice that usually when good abandoned queries co-occur in sessions, they are mostly related to simple facts. For example, a query  X 9 litres in quarts X  (labeled as SAT) is followed by another query  X 9 litres in gallons X , which is followed by a third query  X  X ow many quarts in a gallon? X  (labeled as SAT), all in the same session. In contrast, D-SAT abandoned queries of the same sessions are more likely to be reformulations of a single search intent. For example, in one session we observe  X  X exas mom death brain X  (no la-bel),  X  X exas mom brain injury X  (no label),  X  X exas mom head injury X  (labeled as DSAT) and  X  X exas mom football death X  (labeled as DSAT), and the queries labeled as bad abandon-ment clearly have the same search intent.

Finally, we examine the difficulty of sessions that are aban-doned due to satisfaction or dissatisfaction. For this purpose we use the value of the last abandonment label as a session label, and the number of queries in a session to estimate ses-sion difficulty, i.e., the amount of effort that a user expends. Figure 3 shows the difference of sessions abandoned with SAT or DSAT in terms of the numbers of queries in these sessions. We observe that users tend to spend less effort in sessions that are abandoned with SAT than those aban-doned with DSAT (means are SAT=2.26 and DSAT=2.93; t (314.3) =  X  2.37; p -value = 0 . 018). Even if we exclude single-query sessions (because we have already shown that Figure 3: Box plots of query numbers in sessions abandoned with DSAT and SAT. A session is aban-doned with DSAT if its last query is a DSAT aban-doned query, and a session is abandoned with SAT if its last query is a SAT abandoned query. single-query sessions are more likely to have SAT abandoned queries), SAT abandoned sessions still trend shorter than D-SAT abandoned ones (mean number of queries SAT=3.84; DSAT=4.76; t (172.0) =  X  1.84; p -value = 0 . 067).
The results presented in this section provide some valuable insights about the abandonment process, in particular the observed dependencies between an abandonment instance and the session within which it occurs. Structured learn-ing has been shown to be particularly useful at handling sequences where there are such dependencies [25, 2]. For this reason, we developed a structured learning framework for modeling abandonment. The findings of the analysis presented in this section helped inform the need for that framework and the design of some of the features it uses. Before proceeding, it is worth noting that in this analysis, we reported results using sessions, while we also performed similar analysis based on tasks [20]. The results from tasks were quite similar to sessions. Due to space, we will focus our analysis on sessions in this paper.
We can model our abandonment prediction task as struc-tured learning, where each label of the abandoned query can be chosen from a finite set, while there exists strong depen-dencies among the labels within each session. We leverage the linear structural Support Vector Machines (SVM) algo-rithm, shown to outperform other structured output predic-tion algorithms such as conditional random fields (CRF), for sequential labeling [25]. In particular, we extend the structural SVM which is isomorphic to a linear chain Hid-den Markov Model (HMM), by incorporating not only the label transition probability but also the features that can depend on any arbitrary pairs of labels. We first review the structural SVM algorithm and then discuss our extension.
In label sequenc e tagging, e ach input x =( x 1 , ..., x a sequence of feature vectors x i  X  R n , associated with a sequence of labels y =( y 1 , ..., y m ), where y i  X  X  1 , ..., k } is chosen from a finite set of predefined categories, e.g., Y = { N, NP, V... } for natural language parsing. The goal of linear structured SVM is to learn a discriminant function parameterized by w , where the features are the combined representation of both inputs and outputs  X ( x , y ),
To derive a maximum margin formulation, we first define the margin of a training example x i as so that the objective becomes to select w with || w ||  X  1 that maximizes min i  X  i . By allowing errors in the training set, a set of slack variables  X  is used along with the dual to optimize a soft-margin problem: where  X  i is the margin defined in eq. (2), and C&gt; 0isthe parameter that balances the trade-off between two terms.
The combined feature representation  X ( x , y )forsequence tagging contains both the emission features and the his-togram of label transition. The emission feature is defined as, for each x i and y i where the feature vector x i is placed in the j  X  X  position for y = j . The combined feature then becomes where [ P ] is the indicator function for the predicate P biggest challenge in optimizing this maximum margin frame-work is the large number of linear constraints, which is n |Y|  X  n for n training examples with the label space |Y| Consequently, standard quadratic programming solvers are unable to handle such a large search space. Efficient learning algorithms have been developed to address this issue, such as cutting-plane methods [25] and dual coordinate descent (DCD) methods [26]. So far, DCD is the fastest implemen-tation so we decided to use this for our problem, which has more advanced features and thus even larger feature space.
A limitation of the above framework is the simplified fea-ture representation for label dependencies. As can be seen from eq. (5), the dependency feature is only available in the format of label transition histograms, e.g., [ y i = j ][ This may not be an issue when the label space |Y| = k is large and the length of the training sequence | x | = m is suf-ficient to capture the strong dependencies between different labels. Nevertheless, in our scenario of abandonment ratio-nale prediction, the label space is quite small: either 0 or 1, and the length of training sequence is often not as adequate as in other applications such as NLP tagging.
 Therefore, we propose to extend the previous structured SVM framework by considering more label dependency fea-tures, which include the feature vector x along with two ad-jacent labels y i and y i  X  1 . These so-called joint-label/observation features have a general form of where  X  s (  X  ) maps a certain sequence between i  X  1and i x into a value, and S denotes the set of all such functions.
We use a specific example to illustrate eq. (6) in the a-bandonment scenario: let us denote  X  s ( x ,i  X  1 ,i ) the total query dwell time between i  X  1and i , while [ y i =0]and [ y  X  1 = 0] means whether both queries are bad abandon-ments. In this setting, we implicitly assume that if a us-er abandoned a query with dissatisfaction, there is a high chance that the user would abandon another one in a short period of time, which is also a bad abandonment. Another example of such a function could be the query similarities of all queries between i  X  1and i .

We want to clarify the notation in particular the subscript-s i  X  1and i here. Unlike the traditional sequence labeling tasks where y i and y i  X  1 arealwaysassumedtobeadjacen-t, in our scenario, there could exist several queries between y and y i  X  1 . Recall that labels are assigned to abandoned queries only. For those queries with clicks in a user search session, no labels are assigned to them. An example is illus-trated in Figure 4, which shows five queries in a user session with two abandoned queries x j and x l ,wherethetwoadja-cent training labels y j and y l are separated by three queries. The joint-label/observation function of these two labels are therefore defined between the three feature vectors x j , x and x l .

Table 1 lists all these features. Since many feature values are not nominal, e.g., dwell time of queries in seconds, it is often suggested to discretize them into several nominal bins, each of which takes value from either 0 or 1. Thus, for each of these features, we choose to separate them into 4 to 5 bins according to the distribution of their histograms.
With the enhanced features, we can re-write the combined feature representation as
Therefore, the inner product between two sequences can be calculated as +
At training stage, we need to solve the inference prob-lem to predict the optimal structure  X y for an input x with ground-truth label y :  X y  X  arg max where  X ( y ,  X y ) defines the loss of the two sequences, e.g., zero-one loss for classification or inverse loss for natural lan-guage parsing [25], and  X ( x , y ) is the enhanced feature rep-resentation in eq. (7). Given the large number of linear Figure 4: Illustration of our extended structured model. In traditional models, each instance x cor-responds to a label y , while in our model, only a-bandoned instances have labels. Traditional models often have two types of features, f ( x i ,y i ) the emis-sion features and f ( y i ,y j ) the label transition fea-tures. Our model extends it by also considering joint-label/observation features f ( x ,y i ,y j ) . constraints, we employee a Viterbi decoding method to effi-ciently find the optimal sequence. To solve the actual train-ing problem as shown in eq. (3), we use a dual coordinate descent (DCD) algorithm which considers the square hinge loss function. For details, readers are referred to [26, 6].
We now discuss the feature set used for prediction. In the previous section, we presented the structural features used for SVM in Table 1, which correspond to those f ( x ,y i ,y features in Figure 4. We have also shown the transition features f ( y i ,y j ) as the histogram count in eq. (7). This leaves only the set of emission features, or f ( x ,y i ), to be de-fined. In our study, we divide those features into three main categories, (i) query, (ii) SERP, and (iii) session features.
Query Features : We extract features for each query and its interaction with the immediate preceding and succeed-ing queries in the session. These features include the query length in terms of characters and tokens, whether the query is the first/last/only query in the session, whether the query is a URL-type query, and whether the current query is a reformulation of the previous query, or the next query is a reformulation of the current one. To determine this, we calculate the Levenshtein distance between two queries af-ter removing stop words. The similarity of two queries are calculated by dividing the Levenshtein distance by the max-imum query length. We use a threshold of 0.5 to determine whether a reformulation exists.

SERP Features : We also include the SERP features for the current query and for the immediately preceding and succeeding queries. Note that the current target query has no clicks, however its neighborhood queries may contain clicks, as shown in Figure 4. The SERP features reflec-t these situations by considering the previous/next query X  X  click count and click position. We also consider the presence of answers on the SERP by adding two answer features: the total number of answers shown and whether the SERP con-tains one or more good answers in the top region. We man-ually define a white list of good answers which have high probability of satisfying a user X  X  intent without the need for clicks. These good answers include weather, dictionary def-initions, and currency conversion.

Session Features : Following the definition used in [11], a search session starts with a query, and terminates with a 30-minute inactivity timeout. Session features contain the overall summary of a search session, including its entry point several nominal bins for ease of learning and inference. (e.g., search homepage or browser search bar), browser type (e.g., Firefox, Chrome), the total session length in terms of queries or total query dwell time , the total number of abandoned queries, and so on. Table 2 lists all features.
Note that in the work of Diriye et al. [11], the authors included many historical features such as overall query fre-quency, which relies on the access of a long-period of search logs and thus not immediately available from the current user session. In practice, calculating and maintaining these features are quite difficult and time-consuming, and thus not desirable as features that are easy to extract. Besides, considering potential applications of this research, e.g., on-line abandonment prediction for unseen (new) queries, or as a metric for online A/B experimentation, we have exclud-ed these features from our framework. Later we show that even without these features, our framework still significantly outperforms the previous model [11].
This section discusses experimental results on both aban-donment prediction and its application to improve the search quality of ranking functions.
From the total 7,419 instances, we selected 5,628 instances that belong to either SAT (good abandonment) or DSAT (bad abandonment) for our prediction task, excluding the other classes where the rationale was unrelated. This left us with 3,104 SAT and 2,524 DSAT cases, which we consider a fairly even distribution (meaning that did not to artificially rebalance the dataset). For learning, we treat SAT as pos-itive class (+1) and DSAT as negative class (-1). For each instance, we extract the 28 emission and session features shown in Table 2, as well as 12 structured features (some of which are shown in Table 1). This leads to 28 features for the baseline binary classifier which we shall discuss shortly, and 216 features for our structured learning framework.
To evaluate the performance of predictions, we measure both the precision and recall for both classes. We report the F 1 score that considers both metrics which is defined as F the prediction, defined as Accuracy =1  X  misclassified all We compare with the boosted decision tree classifier (B-DT) [12] which has shown superior prediction performance for binary classification in many tasks. In [11], the authors reported that BDT performed best of all classifiers they tried. So we consider BDT a strong baseline for this study. BDT has three parameters to tune: the number of leaf node L , the shrinkage parameter  X  and the number of iterations M .Inourexperiments,wetriedboth L =2and L = 10.
 The other two parameters are selected via cross-validation. In particular, we set the maximum value of M = 1000 and perform early termination if we observe that the accuracy of the validation set no longer improves.

To evaluate whether the proposed joint-label/observation features in Table 1 work, we also compare with a structured SVM framework that has only the emission features and the histogram of state transitions, i.e.,  X ( x , y ) in eq. (5). We name this baseline SSVM-Basic and our method with the entire feature space as SSVM-Advance .
In the learning procedure, we randomly split the 5,628 in-stances into training and test data with a 1:1 ratio, grouped by user sessions. We repeat the experiments 10 times for each algorithm and report the average performance. For SSVM, we further split the training data into validation and training and used two-fold cross validation to determine the balancing parameter C . It emerged that the optimal val-ue of C for both models are fairly small  X  C =0 . 15 for SSVM-Basic and C =0 . 2 for SSVM-Advance.
 Table 3 lists summarizes the prediction performance. Both SSVM framework significantly outperformed the baseline B-DT method. In particular, our best SSVM-Advance mod-el improved accuracy by over 22% relative to the strong baseline BDT method. Even without the advanced joint-label/observation features, we see that SSVM-Basic still out-performed BDT significantly with the addition of the label transition features. It confirmed that the structural depen-dencies between abandoned labels within sessions are truly helpful for prediction. Most noticeably, we see that the per-formance of the DSAT class benefits more from the struc-tured learning framework. In the baseline BDT method, the classifier did much worse on DSAT than SAT. With the introduction of structured features, both SSVM mod-els had better predictive power for DSAT than SAT class. Paired t -test results indicated that both SSVM X  X  perfor-mance improvements were statistically significant with p -value &lt; 0 . 0001. Note that improving the accuracy on DSAT queries has a profound impact on search engine relevance. A recent study 2 shows that the majority of user search tasks http://www.experian.com/hitwise/press-release-experianhitwise-reports-google-share-of-searche.html preceding and succeeding queries. are satisfied (over 80%). This class imbalance also affects previous studies [1, 14], which report high accuracy in SAT prediction but poor performance in DSAT prediction. We believe that our research is a big step forward in addressing this important issue.

Note that our baseline BDT performed slightly worse than the reported number of Section 6.7 in [11] in terms of F 0 . 5 score (0.7520 vs. 0.7847). One reason could be that their data set was much smaller (1,799 instances) than ours (5,628 instances) and may have had a different distribution. How-ever, our SSVM-Basic still outperformed their number no-ticeably in F 0 . 5 score (0.8237 vs. 0.7847), so did the best SSVM-Advance model ( F 0 . 5 = 0.9045).

Recall that at the end of Section 4 we mentioned that a task-based study was also conducted. In our experiments, we also trained a BDT using the task definition adopted by Liao et al. [20]. The accuracy of the task-segmented baseline was 0.7185, slightly worse than session baseline. We there-fore believe that a more sophisticated segmentation method does not necessarily improve the prediction performance, and continue our experiments with only session-based seg-mentation.

In comparing between two SSVM models, we still observe a substantial performance improvement. Since it is not quite straightforward to aggregate the feature weights for individ-ual features in the structured learning framework, we choose two methods to compare the structured features. In the first method, we add individual features in Table 1 to the SSVM-Basic model and compare the model performance. In the second method, we train the SSVM-Advance model with all structured features except for the one to evaluate. Table 4 lists the model accuracy of individual features. We observe that in both models, the SameQ feature is most important, followed by the ReformQ feature. At the same time, NQ and DT perform quite similarly, where both fea-tures measure the gap between two abandonments. Overall, the Min-Sim feature seems to be least effective, only adding marginal improvement to both models.

Since the structured features need at least two labels in each training sequence to take effect, we break down the predictive performance by the length of training sequences. Table 5 lists the accuracy results, where for example 2(867) means a total of 867 training sequences with two abandon-ment labels. Note that the actual user search sessions can be arbitrarily long including many clicked queries. We observe that when the length of the training sequences increases, both SSVM-Basic and SSVM-Advance improve their per-formance gradually. Meanwhile, BDT exhibited similar per-formance regardless of session length. This result again con-SSVM-Advance 0.8774 0.8639 0.8885 Table 3: Binary Prediction performance of accura-cy and F 1 score for SAT and SAT classes. Both structured SVM models substantially outperformed a strong baseline. Our models improve a lot on the DSAT class, see text for details.
 Table 4: The performance of individual structural features in terms of Accuracy. The second colum-n means the model plus that feature, higher means better. The third column means the model with-out that feature, so lower means the feature is more important. Top three features are highlighted. firms the importance of structured features in abandonment prediction given the high label dependency between queries.
To better understand feature performance among differ-ent label transition scenarios, in Figure 5, we plot the fea-ture weights of several structural features. Recall that each of these features is divided into four features during fea-ture construction according to the label transitions [ y i (0 , 1)][ y i  X  1 =(0 , 1)]. In (a), the label transition weights of indicate that it is indeed that transition to the same class is much more likely than to the opposite class, which confirms our discovery of label dependency in the user behavior anal-ysis presented earlier. Similarly, (b), (c) and (d) lists feature weights of the three most important structural features. It can be seen, for example, from the SameQ feature, similar queries are more likely to share the same abandonment label. Interestingly, from the DT feature, we see that all weight-s are positive except for the self-transition between DT  X  Table 5: Accuracy result broken down by the length of training sequences. Numbers in parentheses are the total number of sessions with that length. Figure 5: Feature weights of four transition features. Transition to the same class often has higher weight. This indicates that when the query dwell time increases, it is more likely that users are satisfied by the current SERP and thus causes the current query to be good abandonmen-t. It is also less likely (  X  0 . 13) that a bad abandonment will occur again after the user has expended more effort in examining the new SERP of the next query.
We further seek to use the prediction results to improve search quality. Specifically, we contribute a new Clickthrough-Rate (CTR) feature from good abandoned queries. We also propose to use bad abandonment as a signal to extract pref-erence data to enhance the training data used by search engine ranking algorithms.

New CTR Feature : by definition, CTR is calculated by, for each query-document pair, the total number of clicks divided by the total number of impressions, i.e., how many times the document was shown for that particular query. In the case of abandonment, users perform no click on any doc-ument, therefore the impression number increases but click number remains the same. However, for good abandoned queries, this calculation is inaccurate since the user X  X  infor-mation need is satisfied. We therefore propose to subtract the estimated number of good abandoned query impressions from all impression to get a more accurate CTR, i.e., New-CTR( Q, U )=
New Click Preference Data : search engines often learn the ranking functions from user preference data. It has been shown that pair-wise learning frameworks (e.g., RankSVM [16], LambdaMART [27]) consistently outperform point-wise learning methods (e.g., ordinal regression). Pair-wise train-ing data is often constructed by comparing the relevance of two documents returned for the same query. The learning algorithms then take these preferences and try to learn an optimal ranking function by minimizing a cost function, e.g., pair-wise loss [16, 27].

In the case of bad abandonment, a user was dissatisfied with all results. If the user kept reformulating the query and finally clicked one of the results, we can infer an implicit preference from this scenario. To be concrete, given a no-click query Q a which we labeled as bad abandonment, we examine the subsequent queries in the same session. If a query Q b has a click U ( c ) b and we think Q b is a reformulation of
Q a (using the same criteria as described in Section 5.3, we construct a set of pair-wise preference data from the session query Q a . Each pair in S indicates a preference relationship of
We study the impact of the new features from a log anal-ysis perspective using the logs of the Microsoft Bing search engine gathered from December 2012 and April 2013 from the U.S. search market. To form a baseline training file, we first randomly sampled 12,000 queries from the logs during that time. We then extracted 10 to 20 documents for each query and asked human assessors to annotate their query relevance on the following five-point scale: Perfect (5), Ex-cellent (4), Good (3), Fair (2), and Bad (1). Each query-document pair was judged by multiple assessors so its final label was determined by a majority vote. We further ex-tracted 400 ranking features for each pair, including some frequently used features such as BM25 and TF.IDF. Giv-en this feature set and the relevance judgments, we use the LambdaMART [27] algorithm to train a ranker.

To generate the new CTR feature, we ran our predictor on the logs of the same time period. Among the 12,000 queries, 3,765 queries had one or more good abandonment labels. On the other hand, we also discovered that 265,872 sessions had one or more bad abandonment followed by a reformulation. To prevent generating too much training data, we limit the number of training pairs in each session to be three, i.e., top-3 returned documents for the bad abandonment and the first clicked document by the reformulation. We further require a pair to appear at least five times in the logs to be considered in training. We generated 35,742 new training instances.
For training and testing, we randomly split the original 12,000 queries into five parts for five-fold cross validation. We use 4/5 for training and 1/5 for testing. The number of leaf nodes is set to be 10 for LambdaMART. The learning rate is set to be 0.1. The maximum training iterations is limited to be 100. We repeat the experiment five times and report on the average performance.

Table 6 lists the performance in terms of average NDCG at the third rank position (NDCG@3) scores and standard deviation. We see that the new CTR feature indeed captures the document relevance more accurately by improving the NDCG score for 0.4% over the old CTR feature and 1.3% over the baseline. On the other hand, we also tested the performance of adding different portion of the new training data to the original training data. The results clearly in-dicate that new training data is helpful. In particular, the performance improvement beco mes statistically significant after adding 80% or more new training data. The best per-formance is achieved by combining the new CTR feature and Baseline + New (CTR + Data) (100%) 0.6274  X  0.065 Table 6: Ranking performance of the new CTR fea-ture and new training instances from the outcome of abandonment prediction. Bolded results are sta-tistical significant. the new training data, which achieves 0.6274 NDCG score  X  an improvement of over 2% in absolute percentage.
Web search abandonment occurs frequently but is not ful-ly understood. Through analysis of logs from a large-scale user study, we discovered several characteristics of abandon-ment, namely: (1) bad abandoned queries are longer than good abandoned queries, (2) query dwell time of bad aban-donment is less than good abandonment, (3) session length given good abandonment is shorter than bad abandonment, and most importantly (4) bad abandonment is more likely to lead to another bad abandonment in the same session (same applies to good abandonment). Based on these char-acteristics, we proposed a structured learning framework to model abandonment using emission features and structured features. Experimental results demonstrated a significan-t improvement of our framework over the state-of-the-art boosted decision tree method presented in earlier work [11].
We leveraged our model to improve search relevance by devising more reliable CTRs and improving the quality of training data. We found strong relevance gains from apply-ing our model to enhance result ranking.

Future work will extend our research to predict abandon-ment rationales for individual users by considering factors such as user expertise and tenacity. We will also improve our prediction performance, and pursue the application of our model for tasks such as metric development and real-time searcher assistance.
