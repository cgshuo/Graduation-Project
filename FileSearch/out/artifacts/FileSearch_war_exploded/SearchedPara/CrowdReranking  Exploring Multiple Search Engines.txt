 Most existing approaches to visual search reranking pre-dominantly focus on mining information within the initial search results. However, the initial ranked list cannot pro-vide enough cues for reranking by itself due to the typically unsatisfying visual search performance. This paper presents a new method for visual search reranking called CrowdR-eranking , which is characterized by mining relevant visual patterns from image search results of multiple search en-gines which are available on the Internet. Observing that different search engines might have different data sources for indexing and methods for ranking, it is reasonable to assume that there exist different search results yet certain common visual patterns relevant to a given query among those results. We first construct a set of visual words based on the local image patches collected from multiple image search engines. We then explicitly detect two kinds of visual patterns, i.e., salient and concurrent patterns, among the vi-sual words. Theoretically, we formalize reranking as an op-timization problem on the basis of the mined visual patterns and propose a close-form solution. Empirically, we conduct extensive experiments on several real-world search engines and one benchmark dataset, and show that the proposed CrowdReranking is superior to the state-of-the-art works. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Performance, Experimentation.
 Visual search, search reranking, data mining.  X 
The explosive growth and widespread accessibility of com-munity contributed media contents on the Internet have led to surge of research activity in visual search [11]. Due to the great success of text search, most popular image and video search engines, such as Google [5], Yahoo! [23], and Live [16], build upon text search techniques by using the text information associated with media contents. This kind of visual search approach has proven unsatisfying as it en-tirely ignores the visual contents as a ranking signal.
To address this issue, search reranking has received in-creasing attention in recent years. It is defined as reorder-ing visual documents based on multimodal cues to improve search performance. The documents might be images or video shots. The research on visual search reranking has proceeded along two dimensions from the perspective of the external knowledge used: self-reranking which only uses ini-tial search results, and query-example-based reranking which leverages user-provided query examples. In this paper, we propose a new method, called CrowdReranking , by exploring multiple image and video search engines or sites which are available on the Internet.

The first dimension predominantly focuses on detecting the recurrent patterns solely in the initial search results, followed by using such patterns to perform reranking [6] [7] [9] [12] [20]. However, it is well-known that existing visual search engines do not have satisfying performance, mainly because of the noisy and even missing surrounding text. Therefore, the initial ranked list usually cannot pro-vide enough cues to detect recurrent patterns for reranking. For example, it can be observed in Figure 1 that there are few relevant results in the top search results of TRECVID 2007 [15], Engine I, and Engine II 1 . It is difficult to achieve satisfying reranking if we solely mine information within the initial search results.

To address this issue, the second dimension leverages a few query examples to train the reranking models [10] [13] [17] [24]. The search performance can be improved due to the external knowledge derived from these examples. The model-based methods in this dimension assume the avail-ability of a large collection of training samples. However, it is typical that training examples are too expensive to ob-tain as users are reluctant to provide enough query examples while searching.

On one hand, existing approaches have not been widely applied due to the very limited information they can mine Figure 1: The exemplary top seven search results selected from TRECVID 2007 [21] and four search engines. The query is  X  X treet market scene X  which is in the query list of TRECVID 2007 video search task. The green rectangles indicate relevant results. Note that the search results of TRECVID 2007 are obtained by our submission based on automatic text search [15]. [Best viewed in color] for guiding the reranking process. On the other, there exists rich crowdsourcing knowledge available online that can be used for reranking. For example, there are a number of search engines (e.g., Google [5], Yahoo! [23], and Live [16]) and social media sites (e.g., Flickr [4]) supporting different kinds of visual search abilities. The following observations inspire the idea of leveraging search results from multiple visual search engines for reranking.
Motivated by the above observations, we propose a new method for visual search reranking, called CrowdReranking. Rather than only uses a single search engine, CrowdRerank-ing is characterized by mining relevant visual patterns from the search results of multiple search engines. Given a query, finding the representative visual patterns, as well as their relative strengths and relations in multiple sets of images is the basis of CrowdReranking proposed in this study. As lo-cal features have proven effective for visual recognition in a large-scale image set [3] [9], we first construct a set of repre-sentative visual words based on the local image patches from multiple image search engines. We then explicitly detect two kinds of visual patterns relevant to the given query, i.e., salient and concurrent patterns, among the visual words. The salient pattern indicates the importance of each visual word, while the concurrent pattern expresses the interde-pendent relations among the visual words. The concurrent pattern, usually called context, is known to be informative for vision applications [22]. Intuitively, if a visual word is with high importance for a given query, then other words co-occurring with it would be prioritized. Therefore, we adopt a graph propagation method like PageRank [1] [9], by treating visual words as pages and their concurrence as hyperlinks. The stationary probabilities over the PageRank graph are represented as the salient pattern, while the concurrent pat-tern is estimated based on the propagation of the weights of graph edges. We then formalize reranking as an optimiza-tion problem which maximally preserves the initial ranked list and simultaneously matches the reranked list against the learned visual patterns as much as possible.

The remainder of this paper is organized as follows. Sec-tion 2 introduces the CrowdReranking approach. Section 3 shows experiments, followed by conclusions in Section 4.
The objective of CrowdReranking is to mine certain visual patterns which are relevant to a given query from the search results of multiple search engines, then these patterns are used to obtain an optimal reranked document list which has the best match against the mined patterns. The flowchart of CrowdReranking is illustrated in Figure 2. Given a tex-tual query, an initial ranked list of visual documents (images or video shots) is obtained by text search technique based on image surrounding text or video transcripts. Meanwhile, this query is fed to multiple image and video search engines or sites (e.g., Google, Yahoo!, Live, and Flickr image and video search engines) to obtain different lists of search re-sults. First, we detect a set of representative visual words by clustering the local features of image patches which are col-lected from the search results of multiple engines. We then construct a graph in which the visual words are nodes and the edges between the nodes are weighted by their concur-rent relations. Through a propagation process which takes the initial ranks and the reliability of search engines into ac-count, we can explicitly detect the relevant visual patterns, including salient and concurrent patterns. The reranking is then formalized as an optimization problem on the basis of the mined visual patterns, as well as the Bag-of-Words (BoW) representation of the initial ranked list. A close-form solution can be achieved to this optimization problem.
Suppose we have a document set X with N documents to be reranked, where X = { x 1 , x 2 , . . . , x N } . Let  X  r note the initial ranking score (i.e., relevance) and reranking score for document x i . In the reranking problem, the ini-tial ranking scores are to be preserved as they indicate the relevance information from text perspective. On the other hand, the reranked list should be consistent with the learned knowledge (i.e., visual patterns) from multiple search en-gines. Therefore, we can formulate the reranking problem by minimizing the following energy function: where  X r = [  X  r 1 ,  X  r 2 , . . . ,  X  r N ] T and r = [ r corresponds to the ranking distance , while Cons ( r , K ) cor-responds to the consistence between the reranked list r and the learned knowledge K . K indicates the learned knowledge from multiple search engines which also corresponds to the mined visual patterns in this paper. The parameter  X  tunes the contribution of knowledge K to the reranked list. When  X  = 0, the reranked list r will be the same as the initial ranked list.
As we look for common visual patterns across different ranked lists of images, the pattern representations should be invariant to a variety of degradations (scale, orientation, global or local appearance, and so on). We adopt scale-invariant feature transform (SIFT) descriptor with a Differ-ence of Gaussian (DoG) interest point detector in this work, as it has proven to be effective for large-scale visual recogni-tion [9] [14]. The interest point is referred to as local salient patch in this paper, each associated with a 128-dimensional feature vector. We further adopt K-Means to cluster the similar patches into  X  X isual words, X  and use Bag-of-Words (BoW) to represent each image [19]. For a given query, the visual patterns K will be mined from the visual words col-lected from the search results of multiple search engines.
Specifically, we investigate two kinds of visual patterns in this work: salient and concurrent patterns. The salient pattern indicates the importance of each visual word, while concurrent pattern expresses the interdependent relations among the visual words. The premise of using concurrence as hyperlinks is that if a visual word is viewed important, then other co-occurring or similar visual words also might be of interest. For example, for a query  X  X each, X  visual words extracted at the  X  X ea X  patch is ranked high, then the co-occurring  X  X and X  and  X  X ky X  patches should be also pri-oritized. Therefore, we adopt PageRank-like propagation framework and construct a graph with the visual words as nodes and co-occurrence between the visual words as hyper-links. Suppose we have L visual words, the visual pattern K is expressed as the combination of salient pattern q and concurrent pattern C : where q = [ q 1 , q 2 , . . . , q L ] T is a L -dimensional vector with each element indicating the salience or importance of a vi-sual word, and C = [ c mn ] ( L  X  L ) is a L  X  L matrix with each element indicating the hyperlink between two words.
Let W ( j ) denote the set of words that contain patches connecting to the patches in word j , and P ( i, j ) denote the set of patches in word i connecting to the patches in word j , as shown in Figure 3. The salience of word j after the k -th iteration, q j ( k ), is given in a way similar to PageRank [1]: q j ( k ) =  X q j (0) + (1  X   X  ) where | X | denotes the size of a set,  X  (0 &lt;  X  &lt; 1) is the weight balancing the initial and the propagated salience scores. q (0) = of the ` -th patch from word j in the initial ranked list.
Accordingly, the concurrent pattern is given by the aver-age weight between word i and j over the graph:
This section discusses the ranking distance Dist ( r ,  X r ) and consistence Cons ( r , K ) based on the mined knowledge K ( q , C ). Recently, some researchers have proposed various ranking distances, mainly including pointwise and pairwise distances as follows.  X  Pointwise ranking distance [6]:  X  Pairwise ranking distance [20]:
For the consistency, most existing reranking methods solely use the visual consistency within the initial search results, assuming that visual documents similar in appearance are with similar ranks [6] [20]. As we have mentioned, only min-ing within initial ranked list is not reasonable when there ex-ist much more irrelevant documents than relevant ones. In this work, we leverage the mined visual pattern K to define a more suitable consistence.

Let f n = [ f n 1 , f n 2 , . . . , f nL ] T denote the BoW representa-tion for image x n [19], the consistence is defined by Let s = [ s 1 , s 2 , . . . , s N ] T denote the vector with entries s P similarity between the visual representation of image x n the mined visual patterns. Based on the two types of ranking distances, we integrate the above two ranking distances in equation (5) and (6), as well as the consistence in (7), to equation (1), and have the following two objective reranking functions.  X  Reranking function using pointwise ranking distance: We called this optimization problem as pointwise mining-based reranking . We can obtain the solution of Equation (8) as follows (proven in Appendix): Obviously, equation (9) consists of two parts, i.e.,  X r and s , which corresponds to the initial ranked list and the learned knowledge, respectively. Therefore, the pointwise rerank-ing can be also viewed as the linear fusion between the ini-tial ranked list and the ranked list learned from the online sources.  X  Reranking function using pairwise ranking distance: Algorithm 1 The CrowdReranking algorithm.
 1: Collect data from multiple search engines by the same query. 2: Extract SIFT features for each image and construct the visual 3: Calculate salient and concurrent patterns by equation (3) and 4: Obtain the reranked list according to equation (9) or (11). We called this optimization problem as pairwise mining-based reranking . The solution of equation (10) with a con-straint r N = 0 can be derived as (proven in Appendix): where c = 2 ( Ue ) T ,  X  c and  X  s are obtained by replacing the last element of c and s with zero, respectively.  X  = D  X  U , where U = [ u mn ] ( N  X  N ) denotes an anti-symmetric matrix n )-element d nn = the last row of  X  with [0 , . . . , 0 , 1] (1  X  N ) .
In equation (11), there are also two parts, i.e.,  X   X   X  1  X  tial ranked list, while  X   X   X  1  X  s can be viewed as the learned knowledge biased by the initial ranked list. Therefore, the reranked list can be also viewed as the combination of the initial ranked list and the learned knowledge.
 In summary, we get the flowchart of CrowdReranking in Algorithm 1.
We conducted experiments on two image datasets. One is real-world image data collected from three popular im-age search engines (i.e., Engine I, II, and III) and a photo sharing site (i.e., Engine IV, also called  X  X eb set X  in short). We selected 29 representative top queries from the query his-tory of one popular search engine. These queries consist of a variety of types, including objects, people, event, entertain-ments, location, and time 2 . For each query, we collected about top 1,000 images returned by each search engine. Af-ter some parts of unaccessible searched images are filtered, the dataset contains 74,000 images in total, and the ranks of these images are kept as the initial ranked list.
The other dataset is the benchmark TRECVID 2007 test set [21] (called X  X V07 set X  X n short), which consists of 18,142 video shots. The search and reranking are performed on the basis of the keyframe and transcript for each shot. There are 24 text queries with their ground truth of relevance. The description of each query can be found in [21]. The text search engines.
 search results by Okapi BM25 [18] were used as the initial ranked list in the following experiments.

We can find that most queries in the real dataset are rep-resented as a general word or a simple phrase, while those in TV07 usually describe an event or a scene with much more words. The TV07 has much lower text search perfor-mance as the queries are more challenging. Therefore, the two datasets can be viewed as representative and comple-mentary datasets in both research and real applications.
For the Web set from the four sites, the relevance of each returned image to the corresponding query was manually labeled by three subjects on a 1-4 scales: (1)  X  X rrelevant, X  (2)  X  X air, X  (3)  X  X elevant, X  and (4)  X  X xcellent. X  The ground truth relevance of each image is the median scale of the three evaluations. We adopt NDCG as performance metric since it is widely used to deal with multiple relevance levels [8]. Given a query q , the NDCG score at the depth d in the ranked documents is defined by where r j is the rating of the j -th document, Z d is a normal-ization constant and is chosen so that a perfect ranking X  X  NDCG @ d value is 1. For TV07 set, we also used the NDCG to evaluate the performance based on the available relevance (i.e., two scales of  X  X ositive X  or  X  X egative X ).

In our experiments, for the Web set, we reranked the search result of each search engine by using the other three engines. For TV07 set, we used the mined visual patterns from all four search engines. We used top 100 images from each engine for visual pattern mining and top 500 images in the initial search results for reranking, since it is typical that there are very few relevant images after the top 500 search results. The number of visual words is empirically set to 2,000 [19].

To demonstrate the effectiveness of the proposed Crow-dReranking methods which include pointwise mining-based reranking and pairwise mining-based reranking , we compared with the following three state-of-the-art reranking methods. are the average of three search engines.
 For all these approaches, we select parameters according to their globally best performance setting in our experiments.
The experimental results are shown in Figure 4 and 5, from which we can see that the proposed two reranking ap-proaches outperform the others. Moreover, it can be ob-served that: search without reranking.
Furthermore, the performance improvements are consis-tent and stable X  X ost queries are improved compared to the initial ranked lists and have better performance than the other methods, as shown in Figure 6. The performance of some queries has significant improvement even their ini-tial search results are extremely poor, such as the query of  X (198) a door being opened X  and  X (217) a road taken from a moving vehicle through the front windshield. X  However, we can see that the performance of some queries degrades with the random walk and Bayesian reranking. This indicates that only mining within initial ranked list could not be al-ways enough to obtain satisfying reranking results. On the other hand, NPRF-based reranking, which also mines the external online sources, is not stable in terms of reranking.
Figure 5 shows the top 10 images of different engines and reranking approaches. It is difficult to discover relevant vi-sual patterns solely from the initial search results for the given query in TV07 set as there are only two samples some-what relevant. However, based on the search results from multiple search engines, we can mine salient and concurrent visual patterns about the query. As a result, the relevant documents can be ranked higher in the reranked list.
We also investigated the performance of CrowdReranking with different tradeoff parameter  X  in equation (1). Fig-ure 7 shows the performance of the pointwise and pairwise mining-based reranking methods with different  X  in terms of NDCG @10. From the figures, we can see that the per-formance curve is like a  X  X  X  shape as  X  increases.
In this paper, we have proposed a novel visual reranking method by mining relevant visual patterns from the search results which are available from existing search engines on the Internet. To the best of our knowledge, the proposed CrowdReranking represents the first attempt towards lever-aging crowdsourcing knowledge for visual reranking.
There are several open problems for further studies. First, the number of search engines or online resources is still lim-ited in this work. It would be a promising topic to discover more search engines and sites and investigate how many en-gines and sites are enough for reranking. Second, most of current search engines are multilingual systems. We can use the results from multilingual systems for reranking. Third, the well-organized online knowledge like Wikipedia and Me-diapedia can be also leveraged for visual reranking. [1] S. Brin and L. Page. The anatomy of a large-scale [2] L. B. Cremeant and R. M. Murra. Stability analysis of [3] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. [4] Flickr. http://www.flickr.com/. [5] Google image and video search. [6] W. Hsu, L. Kennedy, and S.-F. Chang. Video search [7] W. H. Hsu, L. S. Kennedy, and S.-F. Chang. Video [8] K. Jarvelin and J. Kekalainen. IR evaluation methods [9] Y. Jing and S. Baluja. PageRank for product image [10] L. Kennedy and S.-F. Chang. A reranking approach [11] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain. [12] Y. Liu, T. Mei, X.-S. Hua, J. Tang, X. Wu, and S. Li. [13] Y. Liu, T. Mei, X. Wu, and X.-S. Hua. Optimizing [14] D. Lowe. Object recognition with informative features [15] T. Mei, X.-S. Hua, W. Lai, L. Yang, and et al. [16] Microsoft Live image and video search. [17] A. Natsev, A. Haubold, J. Te  X  s i  X  c , L. Xie, and R.Yan. [18] S.-E. Robertson and K.-S. Jones. Simple, proven [19] J. Sivic and A. Zisserman. Video Google: A text [20] X. Tian, L. Yang, J. Wang, Y. Yang, X. Wu, and [21] TRECVID. [22] L. Wolf and S. Bileschi. A critical view of context. [23] Yahoo image and video search. [24] R. Yan, A. Hauptmann, and R. Jin. Multimedia
Rewrite equation (8) in the matrix way: Then, taking derivatives and equate it to zero, we can obtain Then, the solution is
Revisit equation (10) as follows: = min Rewrite it in the matrix way: where  X  = D  X  U , U = [ u mn ] ( N  X  N ) denotes an anti-symmetric matrix with u mn = 1  X  r matrix with its ( n -n )-element d nn = 2( Ue ) T . Taking derivatives and equate it to zero, we can obtain The solution of equation (17) is non-unique since the Lapla-cian matrix  X  is singular [2]. Referring to [20], we also simply add a constraint r N = 0 where N is the length of r . Thus we replace the last row of  X  with [0 , 0 , . . . , 0 , 1] obtain  X   X  , the last element of c and s with zero to obtain  X  c and  X  s , respectively. Then, the solution is
