 It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries, which are typically very short. In this paper we study several machine learning methods, including naive Bayes model, maximum entropy model, support vector ma-chine (SVM), and stochastic gradient boosting tree (SGBT), for navigational query identification in Web search. To boost the performance of these machine techniques, we exploit sev-eral feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features, in this paper, we study the prob-lem of identifying navigational queries with thousands of available features, extracted from major commercial search engine results, Web search user click data, query log, and the whole Web X  X  relational content. A multi-level feature extraction system is constructed.

Our results on real search data show that 1) Among all the features we tested, user click distribution features are the most important set of features for identifying navigational queries. 2) In order to achieve good performance, machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree, cou-pled with linear SVM feature selection is most effective. 3) With carefully coupled feature selection and classification approaches, navigational queries can be accurately identi-fied with 88.1% F1 score, which is 33% errorratereduction compared to the best uncoupled system, and 40% error rate reduction compared to a well tuned system without feature selection.
 H.4 [ Information Systems Applications ]: Miscellaneous  X  Dr. Peng contributes to this paper equally as Dr. Lu. Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. Experimentation Navigational Query Classification, Machine Learning
Nowadays, Web search has become the main method for information seeking. Users may have a variety of intents while performing a search. For example, some users may already have in mind the site they want to visit when they type a query; they may not know the URL of the site or may not want to type in the full URL and may rely on the search engine to bring up the right site. Yet others may have no idea of what sites to visit before seeing the results. The information they are seeking normally exists on more than one page.

Knowing the different intents associated with a query may dramatically improve search quality. For example, if a query is known to be navigational, we can improve search results by developing a special ranking function for navigational queries. The presentation of the search results or the user-perceived relevance can also be improved by only showing the top results and reserving the rest of space for other pur-poses since users only care about the top result of a nav-igational query. According to our statistics, about 18% of queries in Web search are navigational (see Section 6). Thus, correctly identifying navigational queries has a great poten-tial to improve search performance.

Navigational query identification is not trivial due to a lack of sufficient information in Web queries, which are nor-mally short. Recently, navigational query identification, or more broadly query classification, is drawing significant at-tention. Many machine learning approaches that have been used in general classification framework, including naive Bayes classifier, maximum entropy models, support vector ma-chines, and gradient boosting tree, can be directly applied here. However, each of these approaches has its own advan-tages that suit certain problems. Due to the characteristics of navigational query identification (more to be addressed in Section 2 ), it is not clear which one is the best for the task of navigational query identification. Our first contri-bution in this paper is to evaluate the effectiveness of these machine learning approaches in the context of navigational query identification. To our knowledge, this paper is the very first attempt in this regard.
Machine learning models often suffer from the curse of feature dimensionality. Feature selection plays a key role in many tasks, such as text categorization [18]. In this pa-per, our second contribution is to evaluate several feature selection methods and propose coupling feature selection with classification approaches to achieve the best perfor-mance: ranking features by using one algorithm before an-other method is used to train the classifier. This approach is especially useful when redundant low quality heterogeneous features are encountered.

Most previous studies in query identification are based on a small number of features that are obtained from limited resources [12]. In this paper, our third contribution is to explore thousands of available features, extracted from ma-jor commercial search engine results, user Web search click data, query log, and the whole Web X  X  relational content. To obtain most useful features, we present a three level system that integrates feature generation, feature integration, and feature selection in a pipe line.

The system, after coupling features selected by SVM with a linear kernel and stochastic gradient boosting tree as clas-sification training method, is able to achieve an average per-formance of 88.1% F1 score in a five fold cross-validation.
The rest of this paper is organized as follows. In the next section, we will define the problem in more detail and de-scribe the architecture of our system. We then present a multi-level feature extraction system in Section 3. We de-scribe four classification approaches in Section 4 and three feature selection methods in Section 5. We then conduct extensive experiments on real search data in Section 6. We present detailed discussions in Section 7. We discuss some related work in Section 8. Finally, we conclude the paper in Section 9.
We divide queries into two categories: navigational and informational. According to the canonical definition [3, 14], a query is navigational if a user already has a Web-site in mind and the goal is simply to reach that particular site. For example, if a user issues query  X  X mazon X , he/she mainly wants to visit  X  X mazon.com X . This definition, however, is rather subjective and not easy to formalize. In this paper, we extend the definition of navigational query to a more general case: a query is navigational if it has one and only one perfect site in the result set corresponding to this query. A site is considered as perfect if it contains complete infor-mation about the query and lacks nothing essential.
In our definition, navigational query must have a corre-sponding result page that conveys perfectness, uniqueness, and authority. Unlike Broder X  X  definition, our definition does not require the user to have a site in mind. This makes data labeling more objective and practical. For example, when a user issues a query  X  X ulton, NY X , it is not clear if the user knows the Web-site  X  X ww.fultoncountyny.org X . However, this Web-site has an unique authority and per-fect content for this query and therefore the query  X  X ulton, NY X  is labeled as a navigational query. All non-navigational queries are considered informational. For an informational query, typically there exist multiple excellent Web-sites cor-responding to the query that users are willing to explore.
To give another example, in our dataset, query  X  X ational earth science teachers association X  has only one perfect cor-responding URL  X  X ttp://www.nestanet.org/ X  and therefore is labeled as navigational query. Query  X  X anadian gold maple leaf X  has several excellent corresponding URL X  X , in-cluding  X  X ttp://www. goldfingercoin.com/ catalog gold/ cana-dian maple leaf.htm X ,  X  X ttp://coins.about.com/ library/weekly/ aa091802a.htm X  and  X  X ttp://www.onlygold.com/Coins/ Cana-dianMapleLeafsFullScreen.asp X . Therefore, query  X  X ana-dian gold maple leaf X  is labeled as non-navigational query.
Figure 1 illustrates the architecture of our navigational query identification system. A search engine takes in a query and returns a set of URLs. The query and returned URLs are sent into a multi-level feature extraction system that generates and selects useful features; details are presented in the next section. Selected features are then input into a machine learning tool to learn a classification model.
The multiple level feature system is one of the unique features of our system. Unlike prior work with a limited number of features or in a simulated environment [11, 12], our work is based on real search data, a major search en-gine X  X  user click information and a query log. In order to handle large amount of heteorgeneous features in an effi-cient way, we propose a multi-level feature system. The first level is the feature generation level that calculates statistics and induces features from three resources: a click engine, a Web-map and a query log. The second level is responsi-ble for integrating query-URL pair-wise features into query features by applying various functions. The third level is a feature selection module, which ranks features by using different methods. Below we present the details of the first two levels. The third level will be presented separately in Section 5 since those feature selection methods are standard.
Queries are usually too short and lack sufficient context to be classified. Therefore, we have to generate more fea-tures from other resources. We use three resources to gen-erate features: a click engine, a Web-map, and query logs. The click engine is a device to record and analyze user click behavior. It is able to generate hundreds of features auto-matically based on user click through distributions [16]. A Web-map can be considered as a relational database that stores hundreds of induced features on page content, an-chor text, hyperlink structure of webpages, including the inbound, outbound URLs, and etc. Query logs are able to provide bag-of-words features and various language model based features based on all the queries issued by users over a period of time.
 Input to feature generation module is a query-URL pair. For each query, the top 100 ULRs are recorded and 100 query-URLs are generated. Thus for each query-URL pair, we record a total of 197 features generated from the following four categories:
Since we record the top 100 results for each query and each query URL pair has 197 features, in total there are 19,700 features available for each query. Feature reduction becomes necessary due to curse of dimensionality [5]. Before applying feature selection, we conduct a feature integration procedure that merges redundant features.
We design a feature integration operator, named normal-ized ratio r k of rank k , as follows:
Thedesignofthisoperatorismotivatedbytheobser-vation that the values of query-URL features for naviga-tional query and informational query decrease at different rates. Taking the urlmr feature for example and consider-ing a navigational query  X  X almart X  and an informational query  X  X anadian gold maple leaf X , we plot the feature val-ues of top 100 URLs for both queries, as shown in Figure 2. As we can see, the feature value for the navigational query drops quickly to a stable point, while an information query is not stable. As we will see in the experiment section, this operator is most effective in feature reduction.

Besides this operator, we use other statistics for feature integration, including mean, median, maximum, minimum, entropy, standard deviation and value in top five positions of the result set query-URL pair features. In total, we now have 15 measurements instead of 100 for the top 100 URLs for each query. Therefore, for each query, the dimension of afeaturevectoris m =15  X  197 = 2955, which is much smaller than 197 , 000.
We apply the most popular generative (such as naive Bayes method), descriptive (such as Maximum Entropy method), and discriminative (such as support vector machine and stochastic gradient boosting tree) learning methods [19] to attack the problem.
A simple yet effective learning algorithm for classification Figure 2: urlmr query-URL feature for navigational query (upper) and a informational query (lower) is based on a simple application of Bayes X  rule In query classification, a query q is represented by a vector of K attributes q =( v 1 ,v 2 , ....v K ). Computing p ( q | case is not trivial, since the space of possible documents q =( v 1 ,v 2 , ....v K ) is vast. To simplify this computation, the naive Bayes model introduces an additional assumption that all of the attribute values, v j , are independent given the category label, c . That is, for i = j , v i and v conditionally independent given q . This assumption greatly simplifies the computation by reducing Eq. (2) to Based on Eq. (3), a maximum a posteriori (MAP) classifier can be constructed by seeking the optimal category which maximizes the posterior P ( c | d ): Eq. (5) is called the maximum likelihood naive Bayes classi-fier, obtained by assuming a uniform prior over categories.
To cope with features that remain unobserved during train-ing, the estimate of P ( v j | y ) is usually adjusted by Laplace smoothing where N y j is the frequency of attribute j in D y , N ing is add one smoothing, obtained by setting a j =1. We use add one smoothing in our experiments below.
Maximum entropy is a general technique for estimating probability distributions from data and has been success-fully applied in many natural language processing tasks. The over-riding principle in maximum entropy is that when nothing is known, the distribution should be as uniform as possible, that is, have maximal entropy [9]. Labeled train-ing data are used to derive a set of constraints for the model that characterize the class-specific expectations for the dis-tribution. Constraints are represented as expected values of features. The improved iterative scaling algorithm finds the maximum entropy distribution that is consistent with the given constraints. In query classification scenario, max-imum entropy estimates the conditional distribution of the class label given a query. A query is represented by a set of features. The labeled training data are used to estimate the expected value of these features on a class-by-class basis. Improved iterative scaling finds a classifier of an exponential form that is consistent with the constraints from the labeled data.

It can be shown that the maximum entropy distribution is always of the exponential form [4]: where each f i ( q ; y ) is a feature,  X  i is a parameter to be estimated and Z ( q ) is simply the normalizing factor to en-sure a proper probability: Z ( q )= Learning of the parameters can be done using generalized iterative scaling (GIS), improved iterative scaling (IIS), or quasi-Newton gradient-climber [13].
Support Vector Machine (SVM) is one of the most suc-cessful discriminative learning methods. It seeks a hyper-plane to separate a set of positively and negatively labeled training data. The hyperplane is defined by w T x + b =0, where the parameter w  X  R m is a vector orthogonal to the hyperplane and b  X  R is the bias. The decision function is the hyperplane classifier The hyperplane is designed such that y i ( w T x i + b )  X   X  ,  X  i =1 , ..., N ,where x i  X  R m is a training data point and y i  X  X  +1 ,  X  1 } denotes the class of the vector x i margin is defined by the distance between the two parallel hyperplanes w T x + b = 1 and w T x + b =  X  1, i.e. 2 / || The margin is related to the generalization of the classifier [17]. The SVM training problem is defined as follows: where the scalar  X  is called the regularization parameter, and is usually empirically selected to reduce the testing error rate.

The basic SVM formulation can be extended to the non-linear case by using nonlinear kernels. Interestingly, the complexity of an SVM classifier representation does not de-pend on the number of features, but rather on the number of support vectors (the training examples closest to the hyper-plane). This property makes SVMs suitable for high dimen-sional classification problems [10]. In our experimentation, we use a linear SVM and a SVM with radial basis kernel.
Like SVM, gradient boosting tree model also seeks a pa-rameterized classifier. It iteratively fits an additive model [8] such that certain loss function L ( y i ,f T ( x + i ) is minimized, where T t ( x ; X  t ) is a tree at iteration t , weighted by param-eter  X  t , with a finite number of parameters,  X  t and  X  is the learning rate. At iteration t ,tree T t ( x ;  X  ) is induced to fit the negative gradient by least squares. That is where G it is the gradient over current prediction function The optimal weights of trees  X  t are determined
If the L-2 loss function [ y i  X  f ( x i )] 2 / 2 is used, we have the gradient G ( x i )=  X  y i + f ( x i ). In this paper, the Bernoulli loss function is used and the gradient has the form
During each iteration of gradient boosting, the feature space is further partitioned. This kind of rectangular parti-tion does not require any data preprocessing and the result-ing classifier can be very robust. However, it may suffer from the dead zoom phenomenon, where prediction is not able to change with features, due to its discrete feature space par-tition. Friedman (2002) found that it helps performance by sampling uniformly without replacement from the dataset before estimating the next gradient step [6]. This method was called stochastic gradient boosting.
Many methods have been used in feature selection for text classification, including information gain, mutual infor-mation, document frequency thresholding, and Chi-square statistics. Yang and Pedersen [18] gives a good compari-son of these methods. Information gain is one of the most effective methods in the context of text categorization. In addition to information gain, we also use feature selection methods based on SVM X  X  feature coefficients and stochastic gradient boosting tree X  X  variable importance.
Information gain is frequently used as a measure of fea-ture goodness in text classification [18]. It measures the number of bits of information obtained for category predic-tion by knowing the presence or absence of a feature. Let y : i =1 ..m be the set of categories, information gain of a feature f is defined as where f indicates f is not present. We compute the infor-mation gain for each unique feature and select top ranked features.
Linear SVM (7) produces a hyperplane as well as a nor-mal vector w . The normal vector w serves as the slope of the hyperplane classifier and measures the relative impor-tance that each feature contribute to the classifier. An ex-treme case is that when there is only one feature correlated to sample labels, the optimal classifier hyperplane must be perpendicular to this feature axle.

The L-2 norm of w , in the objective, denotes the inverse margin. Also, it can be viewed as a Gaussian prior of random variable w . Sparse results may be achieved by assuming a laplace prior and using the L-1 norm [2].
 Unlike the previous information gain method, the linear SVM normal vector w is not determined by the whole body of training samples. Instead, it is determined by an opti-mally determined subset, support vectors, that are critical to be classified. Another difference is obvious: normal vec-tor w is solved jointly by all features instead of one by one independently.

Our results show that linear SVM is able to provide rea-sonably good results in feature ranking for our navigational query identification problem even when the corresponding classifier is weak.
Boosting methods construct weak classifiers using subsets of features and combines them by considering their predica-tion errors. It is a natural feature ranking procedure: each feature is ranked by its related classification errors.
Tree based boosting methods approximate relative influ-ence of a feature x j as where I 2 k is the empirical improvement by k -th splitting on x j at that point.

Unlike the information gain model that considers one fea-ture at a time or the SVM method that considers all the feature at one time, the boosting tree model considers a set of features at a time and combines them according to their empirical errors.
 Let R ( X ) be a feature ranking function based on data set X . Information gain feature ranking depends on the whole training set R Info ( X )= R Info ( X tr ). Linear SVM ranks fea-tures is based on a set of optimally determined dataset. That is, R SVM ( X )= R SVM ( X SV ), where X SV is the set of sup-port vectors. The stochastic gradient boosting tree (GSBT) uses multiple randomly sampled data to induce trees and ranks feature by their linear combination. Its ranking func-tion can be written as R SGBT ( X )= where X t is the training set randomly sampled at iteration t .
A total number of 2102 queries were uniformly sampled from a query log over a four month period. The queries were sent to four major search engines, including Yahoo, Google, MSN, and Ask. The top 5 URL X  X  returned by each search engine were recorded and sent to trained editors for labeling (the number 5 is just an arbitrary number we found good enough to measure the quality of retrieval). If there exists one and only one perfect URL among all returned URLs for a query, this query is labeled as navigational query. Otherwise, it is labeled as non-navigational query.
Out of 2102 queries, 384 queries are labeled as naviga-tional. Since they are uniformly sampled from a query log, we estimate there are about 18% queries are navigational.
The data set were divided into five folders for the purpose of cross-validation. All results presented in this section are average testing results in five fold cross validations.
Classification performance is evaluated using three met-rics: precision, recall and F1 score. In each test, Let n denotes the number of positive samples that correctly clas-sified (true positive); n  X  + denotes the number of negative samples that are classified as positive (false positive); n denotes the number of false positive samples that are classi-fied as negative (false negative); and n  X  X  X  denotes the num-ber of negative samples that are correctly classified (true negative). Recall is the ratio of the number of true positives to the total number of positives samples in the testing set, namely Precision is the ratio of the number of true positive samples to the number samples that are classified as positive, namely
F1 is a single score that combines precision and recall, defined as follows:
Table 1 shows the distributions of the top 50 features se-lected by different methods. All methods agree that click features are the most important. In particular, linear SVM and boosting tree select more click features than informa-tion gain. On the other hand, information gain select many features from anchor text and other metrics such as spam scores.
 Table 1: Distributions of the Selected Top 50 Fea-tures According to Feature Categories
Table 2 shows the distribution of the selected features ac-cording to feature integration operators. It shows which operators applied to result set query-URL pair wise features are most useful. We group the 15 operators into 5 types: vector, normalized ratios ( r k ,k =2 , 5 , 10 , 20), min/max, en-tropy/stand deviation, and median/mean. Vector group in-cludes all query-URL pair features in top 5 positions; nor-malized ratios are defined in (1). As we can see from the table, all feature integration operators are useful. Table 2: Distributions of the Selected Top 50 Fea-tures According to Integration Operators normalized ratios 8% 38% 22%
The number of selected features directly influence the clas-sification performance. Figure 3 shows relationship between the boosting tree classification performance and the number of selected features. As we can see, performance increases with cleaner selected features. However, if the number of selected feature is too small, performance will decrease. A number of 50 works the best in our work. We first apply four different classification methods: naive Bayes, maximum entropy methods, support vector machine and stochastic gradient boosting tree model over all available features. The results are reported in Table 3. As we can see, stochastic gradient boosting tree has the best performance with an F1 score of 0.78 .

We then apply those methods to machine selected fea-tures. We test 4 different feature sets with 50 number of fea-tures, selected by information gain, linear SVM and boosting tree. The combined set consists of 30 top features selected by linear SVM and 29 top features selected by boosting tree. Please note that the total number of features are still 50 since linear SVM and boosting tree selected 9 same features in their top 30 feature set. Figure 3: Classification performance F1 against number of features: 25, 50, 100, 200, 400, 800, and 2955 (all features) Table 3: Results of Various Classification Methods over All Features
Table 4 presents the results of the coupled feature selec-tion and classification methods. It is obvious that the perfor-mance of each method is improved by applying them to ma-chine selected clean features, except naive Bayes classifier. Surprisingly, the features selected by linear SVM are the best set of features. The results show that even if the under-lying problem is not linear separable, the linear coefficients of the large margin linear classifier still convey important feature information. When the stochastic gradient boost-ing tree is applied over this set of features, we get the best performance with 0.881 F1 score among all cross-methods evaluations. Without feature ablation, SGBT is only able to achieve 0 . 738 F1 score. That is, feature selection has an effect of error reduction rate 40% . Without introducing linear SVM in feature ablation, if SGBT works on the fea-ture set selected by its own variable importance ranking, it achieves 0.848 F1 score. That is to say, a cross methods coupling of feature selection and classification causes a 33% error reduction.
An interesting result from Table 1 is the features selected for navigational query identification. Those features are mostly induced from user click information. This is intu-itively understandable because if a query is navigational, the navigational URL is the most clicked one. On the other hand, it might be risky to completely rely on click infor-mation. The reasons might be 1) user click features may be easier to be spammed, and 2) clicks are often biased by various presentation situation such as quality of auto ab-straction, etc.

From Table 4, we observe that linear SVM and boosting tree have better feature selection power than information gain. The reason that information gain performs inferior to linear SVM and boosting tree is probably due to the fact that information gain considers each feature independently while linear SVM considers all features jointly and boosting tree composites feature rank by sum over all used features. The results show that URL, anchor text and other metrics are helpful only when they are considered jointly with click features.

The most important result is that the stochastic gradi-ent boosting tree coupled with linear SVM feature selection method achieves much better results than any other combi-nation. In this application, the data has very high dimension considering the small sample size. The boosting tree method needs to partition an ultra-high dimensional feature space for feature selection. However, the stochastic step does not have enough data to sample from [6]. Therefore, the boosted result might be biased by earlier sampling and trapped in a local optimum. Support vector machine, however, is able to find an optimally determined subset of training samples, namely support vectors, and ranks features based on those vectors. Therefore, the SVM feature selection step makes up the disadvantage of the stochastic boosting tree in its initial sampling and learning stages that may lead to a local optimum.

As expected, naive Bayes classifier hardly works for the navigational query identification problem. It is also the only classifier that performs worse with feature selection. Naive Bayes classifiers work well when the selected features are mostly orthogonal. However, in this problem, all features are highly correlated. On the other hand, classification methods such as boosting tree, maximum entropy model and SVM do not require orthogonal features.
Our work is closely related to query classification, a task of assigning a query to one or more categories. However, gen-eral query classification and navigational query identifica-tion are different in the problems themselves. Query classi-fication focuses on content classification, thus the classes are mainly topic based, such as shopping and products. While in navigational query identification, the two classes are in-tent based.

In the classification approaches regard, our work is re-lated to Gravano, et al. [7] where authors applied various classification methods, including linear and nonlinear SVM, decision tree and log-linear regression to classify query lo-cality based on result set features in 2003. Their work, however, lacked carefully designed feature engineering and therefore only achieved a F1 score of 0.52 with a linear SVM. Beitzel, et al.[1] realized the limitation of a single classifica-tion method in their query classification problem and pro-posed a semi-supervised learning method. Their idea is to compose the final classifier by combining classification re-sults of multiple classification methods. Shen, et al. [15] also trained a linear combination of two classifiers. Differ-ently, instead of combining two classifiers for prediction, we couple feature selection and classification.
 In the feature extraction aspect, our work is related to Kang and Kim 2003 [11] where authors extracted heteroge-nous features to classify user queries into three categories: topic relevance task, the homepage finding task and service finding task. They combined those features, for example URL feature and content feature, by several linear empiri-cal linear functions. Each function was applied to a different binary classification problem. Their idea was to empha-size features for different classification purposes. However, the important features were not selected automatically and therefore their work is not applicable in applications with thousands of features.
We have made three contributions in the paper. First, we evaluate the effectiveness of four machine learning ap-proaches in the context of navigational query identification. We find that boosting trees are the most effective one. Sec-ond, we evaluate three feature selection methods and pro-pose coupling feature selection with classification approaches. Third, we propose a multi-level feature extraction system to exploit more information for navigational query identifica-tion.

The underlying classification problem has been satisfacto-rily solved with 88 . 1% F1 score. In addition to the successful classification, we successfully identified key features for rec-ognizing navigational queries: the user click features. Other features, such as URL, anchor text, etc. are also important if coupled with user click features.

In future research, it is of interest to conduct cross meth-ods co-training for the query classification problem to utilize unlabeled data, as there is enough evidence that different training methods may benefit each other. [1] S. Beitzel, E. Jensen, D. Lewis, A. Chowdhury, [2] C. Bhattacharyya, L. R. Grate, M. I. Jordan, L. El [3] A. Broder. A Taxonomy of Web Search. In ACM [4] S. della Pietra, V. della Pietra, and J. Lafferty. [5] R.O.Duda,P.E.Hart,andD.G.Stork. Pattern [6] J. H. Friedman. Stochastic Gradient Boosting. [7] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein. [8] T. Hastie, R. Tibshirani, and J. Friedman. The [9] E. T. Jaynes. Papers on Probability, Statistics, and [10] T. Joachims. Text Categorization with Support Vector [11] I.-H. Kang and G. Kim. Query Type Classification for [12] U. Lee, Z. Liu, and J. Cho. Automatic Identification [13] R. Malouf. A Comparison of Algorithms for Maximum [14] D. E. Rose and D. Levinson. Understanding User [15] D. Shen, R. Pan, J.-T. Sun, J. J. Pan, K. Wu, J. Yin, [16] L. Sherman and J. Deighton. Banner advertising: [17] V. Vapnik. The Nature of Statistical Learning Theory . [18] Y. Yang and J. Pedersen. An Comparison Study on [19] S.C. Zhu. Statistical modeling and conceptualization
