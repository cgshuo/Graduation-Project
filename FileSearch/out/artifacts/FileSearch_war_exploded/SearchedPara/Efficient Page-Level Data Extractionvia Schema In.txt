 Chia-Hui Chang (  X  ) , Tian-Sheng Chen, Ming-Chuan Chen, and Jhung-Li Ding Search results from deep Web are information with high quality and are often useful for many information integration applications. Most researches pursue automated methods that use unannotated Web page as training examples. Therefore, many unsupervised web data extractions also assume a single da ta-rich section which contains search results such that intelligent mining techniques can be applied for record segmentation from a single page input. For example, [ 2 , 9 , 10 , 13 ], and all receive a single page as input and conduct analysis for set detection. We call these approaches as r ecord-level extraction systems.
 Since these unsupervised approaches do not require annotated pages as input, many systems also omit the wrapper generation procedure as required by supervised approaches, which receive annotated page s for training. The problem with these wrapper-free approaches is the concern to handle large volume of Web pages. Suppose there are N pages to be processed, wrappe r-free record-level extraction systems would have to be executed N times for each page as shown in Fig. 1 (a). On the other hand, if a wrapper is generated from a subset of training pages for schema and template induction, the rest can be wrapped quickly using the induced schema and template as shown in Fig. 1 (b). As extraction tasks based on known schema and template are usually more efficient than the complicate analysis process for schema and template induction, poten X  tial gain can be expected for unsupervised extraction systems with wrapper generation. approaches is that they do not make full use of the information provided by all training pages since the extraction system process pa ge independently. The concern is that the output schema may be different for different input pages even if they are generated by the same Web CGI program. Therefore, an add itional step to integrate all extracted data (schema matching) is required for thes e  X  X rapper-free X  unsupervised extraction systems. full-schema and provide a total solution for various kinds of extraction needs. The benefit of varieties from multiple pages also allow th e system to differentiate the roles of each element and decide whether they are templa te or data, set or tuple, option or fixed occurrence. That is to say, we have bett er chance to induce the complete schema and template from multiple pages.
 task that has not been fully explored. In this paper, we propose a page-level schema induction system with wrapper verification pr ocedure to speed up th e extraction of large scale web pages. The system, called UWID E (Unsupervised Wrapper Induction and Data Extraction), is composed of two subsystems. The first subsystem is for page-level schema induction, while the second subsystem is for data extraction and schema veri X  fication. The schema induction module conducts set region detection, record boundary segmentation, data alignment and schema generation. The verification module trans X  forms the induced schema into a finite state machine and assigns labels to each leaf node of a test page to achieve data extracti on. The experiments show UWIDE outperforms state-of-the-art page-level data extraction system TEX [ 11 ], ExAlg [ 1 ], and RoadRunner [ 4 ] in terms of schema correctness and extraction efficiency.
 Section 3 describes how to induce the full schema from unannotated input pages. Section 4 explains the verification procedure for testing phase. Effectiveness of schema induction and efficiency of data extractio n for test pages are presented in Sect. 5 . Finally, Sect. 6 concludes this paper and discusses future work. Researches on web information extraction ha ve been lasted for more than a decade. Several survey papers which classify these approaches from various aspects are avail X  able. For example, Laender et al. [ 7 ] propose taxonomy for characterizing Web data extraction tools based on the main features used (e.g. HTML-aware tools, NLP-based tools, Ontology-based tools, etc.) to ge nerate a wrapper in 2002. Chang et al. [ 3 ] classify various approaches from the task domain , the automation degree and the technique. Recently, Sleiman and Corchulo [ 12 ] also survey the existing proposals regarding region extractors.
 The analysis from various task domain shows that the input could be single or multiple pages while the output could be record-level or page-level extraction target. In fact, unsupervised record-level schema induction from singe page input has become the main stream of research in the past decade. On the contrary, only a few studies focus on page-level extraction from multiple pages. To the best of our knowledge, RoadRunner [ 4 ] in 2001, ExAlg [ 1 ] in 2003, FiVaTech [ 5 ] in 2010 and TEX [ 11 ] in 2013 are the only studies focusing on page-level extraction from multiple pages.
 Note that, supervised wrapper induction uses annotated training pages to generate wrappers and conduct data extraction for test pages. On the other hand, unsupervised approaches often proceed without wrapper gene ration since they can extract data directly from any training pages (annotation-free). Thus, no maintenance is required. Therefore, most wrapper ma intenance researches focus on supervised record-level wrappers generated from annotated training pages. Their wrapper verification only checks the validity of extracted data to remedy the situations when the wrappers work normally but the extracted data are invalid. For example, RAPTURE [ 6 ] and DATA X  PROG [ 8 ] are designed for two supervised wrapper induction systems WIEN and STALKER, respectively. For both systems, if the extraction process fails due to template change, the verification of the semantic of data is not necessary.
 Reviewing state-of-the-art page-level data extraction systems, TEX does not output schema; ExAlg and FivaTech, despite the generation of schema, does not implement the wrapper module; the only page-level data extraction system with wrapper generation based on induced schema and template is RoadRunner which works as a wrapper to align tag token sequence of an input page with the existing wrapper.
 In this paper, we argue that wr apper-free approaches, i.e. Fig. 1 (a), may not be efficient for large-scale data extraction. This is because a generated wrapper guided by the induced schema and template, i.e. Fig. 1 (b), is usually much faster than the complex analysis procedure for wrapper induction. The system, UWIDE, is composed of two pa rts: the first part is page-level schema induction from multiple pages, while the second part is the wrapper generation which is used to verify if a testing page complie s with the schema and extract data accordingly as shown in Fig. 2 .
 3.1 Pre-processing Our preprocessing step uses CyberNeko HTML Parser to transform the input pages into well-formed format and represent each page by a list of leaf nodes from the DOM (document object model) tree. We work on leaf nodes instead of tag tokens or word tokens since it can greatly reduce the data volume for multi-pages input. Each leaf node is composed of three attributes: path, text, and type. The benefit can be seen directly from the following statistics. For pages with an average of 23 KB, the numbers of word tokens and tag tokens are 3000 and 1130 respectively. In terms of leaf nodes, there are only 258 leaves per page, which is only 8.6 % word tokens. 3.2 Set Region Detection To locate data-rich sections , many approaches have been proposed. Most approaches operate on token sequences that are abstract ed from data records with various values. In this paper, we enumerate frequent leaf node patterns in every page for set region occurrences respectively, representing a frequent pattern with length 1. For patterns with length longer than one, we encode each l eaf node based on their path and content type to enable the discovery of repeat patterns. We categorize text content into five categories including Number , Date , Time , URL , and Email . If a text segment does not belong to any of these types, we denote it by String type. In Fig. 3 , all String type leaf nodes T T , T while Date type leaf nodes D 1 , D 2 , and D 3 with the same path are encoded with a different code  X  X  X .
 Given the encoded leaf node sequences, we choose the longest and the shortest sequences for pattern mining and remove smaller patterns that are dominated by larger patterns (i.e. the shorter pattern is a sub-string of a longer pattern and having the same occurrence frequency) to focus on maximal patterns. 3.3 Record Boundary Segmentation Enumerating repeat patterns only detect possible set region. The challenging part is deciding how repeat pattern is used for segmenting record boundary in each data region. Since the record boundary can be somewher e between two adjacent occurrences of a pattern or somewhere inside the pattern leaf nodes, we need some heuristics to determine the record boundary. The idea is based on the following observations of data records in DOM tree structures: (1) records are mutually exclusive, i.e. a leaf node cannot belong to two records at the same time; (2) a record is usually composed of adjacent leaf nodes, forming either a subtree or a forest; (3) all su btrees or forests of these records are rooted at the same level, sharing common ancestors.
 Given these observations, our approach for record boundary detection uses the youngest common ancestor (YCA) of all occurrences of a landmark to represent the set region and locates the largest subtree under YCA for each occurrence such that all record subtrees are not overlapped.
 FDA Discovery: For each leaf node (called a landmark, L ) in a frequent pattern p , we traverse up the path of every occurrence a 1 , a 2 , ..., a the first common ancestor of two adjacent leaf nodes (not the common ancestor of all occurrences) is met and keep the farthest dis tinct ancestors (FDAs) at the same level as record subtrees. For example, the three occurrences of landmark  X  Excerpt:  X  in Fig. 3 has three distinct nodes at level 6 in page P1. The algorithm stops at level 3 when CDA has only one node, &lt;dl&gt;, and outputs FDA set with {&lt;dd if we use code  X  X  X  or  X  X  X  in pattern  X  X BF C X  as landmark for FDA set discovery, the output FDA set will be {&lt;dt 0 &gt;,&lt;dt 3 &gt;,&lt;dt Clustering Non-FDA Sibling Nodes: Note that we might have remaining non-FDA sibling nodes under the YCA. Since they could be records with missing landmark, or auxiliary subtrees that are part of the reco rds, or ads inserted among data records, we need to determine if the sibling node is similar to the discovered FDA set with the same tag name. We used the edit distance of the leaf code string in the sibling subtree and the longest common subsequence (LCS) of the F DAs X  code strings to calculate the simi X  larity. If the similarity is greater than 0.7, the sibling node is considered a record with optional landmark and is added to the FDA set.
 their tag name and clustered to decide their subtree type. With this processing, we can assign these sibling nodes to its neighboring records based on their order in pages. For example, the non-FDA sibling nodes for landmark  X  Excerpt:  X  in P1 include {&lt;dt 0 &gt;,&lt;dd 2 &gt;,&lt;dt 3 &gt;,&lt;dt 5 &gt;}. Since &lt;dd the LCS of the FDAs X  code strings, i.e.  X  FC  X , and assigned a new cluster to &lt;dd the similarity is 0. Meanwhile, the three siblings with tag name &lt; dt &gt; all have the same code string  X  AB  X  and are clustered to the same group {&lt;dt Data Record Assembling: When all subtrees under the YCA is properly clustered, the next step is to assemble the nodes from various clusters to form data records. By exam X  ining the order of these clusters in multiple input pages, we can assemble these nodes before FDA node (with &lt;dd&gt; cluster) in both P1 and P2, while non-FDA cluster with node with the following FDA node and the &lt;dd&gt; sibling node to form data records, generating three data records {&lt;dt 0 &gt;,&lt;dd {&lt;dt 5 &gt;,&lt;dd 6 &gt;} for P1.
 Prioritizing FDA Sets: Since there could be more than one landmark or several repeat patterns discovered for a set region, we also need to decide how to handle interleaving FDA sets, i.e. the span of one FDA set over laps with the span of the second FDA set. To evaluate whether an output FDA set represents a set region well, we define compres X  sion ratio (i.e. the number of aligned columns divided by the average number of leaf nodes in a page) and set density (i.e. the number of non-null cells divided by the number larger density value divided by the compression ratio will be selected with higher priority. 3.4 Data Alignment After the segmentation of records, we will be able to conduct alignment based on segmented records from each page for the same data region. Similarly, data alignment has to be applied to non-set region for full page alignment as well.
 records. We start from the record with th e largest number of leaf nodes and define a match scoring function based on leaf nodes X  path, type and content. Let w denote the aligned result of k records, Cmode ( w i ) denote the content with the highest frequency in w and Maxcount ( w i ) denote the count of C mode ( w matching the node at position i of w with a new record r at position j , denoted by r defined as the sum of the matching score for path, type, and content between w (1) Path score: If path ( w i ) = path(r j ) then return 0.5, or 0 otherwise. (2) Type score: If type ( w i ) = type ( r j ) then return 0.25; if type ( w child relationship then return 0.125, or 0 otherwise. (3) Content score:
Match), else return 0.25 (Partial Match). b. Cmode ( w i )  X  content ( r j ): If Maxcount ( w i ) &lt;= 2 then return 0, else return  X 0.1 (Mismatch).
 Note that we give extra score (1 point) for full match when there are more than one record and a small penalty ( X 0.1) for mismatch. This is to guide nodes with the same content to be aligned together but still allow the alignment of various values of catego X  rical attributes. Therefore, a match score is ei ther a value between 0 to 1, or a big value 2. With this scoring, leaf nodes with same path, same type, or same content will have larger value, while parent-child type relation is also considered to tolerate different type matching.
 For each set region S i with the aligned column set C i also have a | | S i | |  X  | | C i | | array. As for non-set region, we obtain a denotes the set of input pages and C denotes the set of aligned columns. 3.5 Dynamic Encoding and Refinement While most of these techniques mentioned above have been applied in various scopes, the result is not always satisfactory. For ex ample, product descriptions in commercial Web pages usually present more diversity an d are hard to align since they may come from different vendors. As another example, several attribute value pairs can be incor X  rectly recognized as set regions. Meanwhile, repeat pattern mining for set region detec X  tion also relies on abstraction mechanism which needs to be tuned for input pages. In this paper, we propose a post-processing step to examine the alignment result and suggest dynamic encoding scheme for refinement.
 The refinement procedure is triggered by the performance of alignment result. We define column density for a column c , Cdens ( c ) as the percentage of non-null nodes in the column and set density S dens array, i.e. | | S i | |  X  | | C i | | .
 Detecting false positive set: When a set density is lower than a threshold (0.6), it often implies a false positive set being detected. Therefore, we dismiss all segmented records of the set region and include them with non-set region for full page alignment. Dynamic encoding for decorative tags: For deep Web pages, decorative tags are often used to emphasize query terms and result in lots of optional columns. Therefore, we count the number of low density columns between two template columns. If more than 30 % columns have density less than 0.6, we search for leaf nodes with one more deco X  rative tag than adjacent text leaf nodes in th is region. We then record these paths together with the data schema to guide the encoding for test pages.
 Merging low-density basic nodes: If low density ratio between two template columns is higher than 0.3 but no decorative tags are found after dynamic encoding, we attribute the cause to inconsistent format from various product descriptions without uniform template. In this case, we will combine columns in this region and record the common path of these columns for testing.
 we ignore the details and give an example schema tree as shown in Fig. 4 .
 In the extraction phase, the wrapper checks whether a testing page complies with the induced schema and template. The wrapper is a driver program which consists of two main modules: one is the transformation from the induced schema and template to finite state machine (FSM), while the other is th e label assignment for each leaf node of an input page. If there exists a label sequence which complies with the FSM, the data could be extracted accordingly. Otherwise, the page can be included to train a better schema. 4.1 Finite State Machine Construction Given the induced schema and template tree, we first build a state machine with basic transitions between adjacent leaf nodes. Two additional state  X  X  X  and  X  X  X  are added to denote the start and end state of the state machine (Fig. 5 (a)). For each composite data type (i.e. internal nodes), we record the start position and end position of the region and find the entrance an d exit states as described below.
 Finding Entrance Nodes: The major entrance node of a region comes from the start state X  X  previous state, while set regions have additional entrance state from their region end state. However, we also need to consider if such an entrance state is included in an option region or disjunction region. For example in Table 1 , the major entrance state for Set_1 include 0 (previous state of the start stat e) and 6 (end state). Since state 6 is inside option region Opt_2, we will also include the entrance state of Opt_2, i.e. state 4, in the entrance state for Set_1, whic h in turns will refer to Opt_1 and the entrance state for Opt_2, i.e. 2, is added to the entrance state.
 Finding Exit Nodes: Similarly, the major exit node of a region goes to the next state of its end state. For set regions, additional exit to the region start has to be included. If the added exit state is included in an option re gion or disjunction region, the exit state for the region has to be added. For example, the exit state of Set_1 in Table 1 is 7 (next state of the end state) and 1 (start state). Since state 1 is not inside any option region, the recursive process stops. For Opt_1, the exit state 5 is inside Opt_2; therefore, we also add the exit state of Opt_2, i.e. st ate 7, to the exit state of Opt_1.
 Given the entrance and exit state set for each composite region, we add transitions transitions from its entrance nodes to the start state and exit transitions from the end entrance states to its exit states. (3) If it is a disjunction region , we remove transitions between adjacent states in the region, and add entrance transitions from its entrance states to every state in the region and exit transitions from every state to its exit states. 4.2 Label Sequence Assignment problem, where each leaf node is a variable with a set of candidate labels as its domain nodes. The steps are briefed below.
 specified by training procedure) based on its tag path, type and content feature. Note that template states can only be a candidat e for leaf nodes with the same tag path and  X &lt;html&gt;&lt;body&gt;&lt;dl&gt;&lt;dt&gt; X  and a number content can only have B could have B 5 as its candidate. For such leaf node with a single candidate, we can then apply constraint imposed by FSM to reduce impossible candidate states of its adjacent leaf node.
 will choose a label sequence which best fit the input according to state transition prob X  ability and content similarity . In such cases, the verification is successful and we can output data for each attribute as shown in Fig. 5 . Otherwise, if no label sequence complies with the FSM, then the verification fails. The following experiments are designed to show the effectiveness of schema induction as well as the efficiency gain with wrapper generation for page-level (full schema) data extraction. 5.1 Schema Correctness Evaluation For schema correctness evaluation, we use 234 pages from 9 websites of ExAlg for the following experiments. The first 5 websites contain 82 list pages and the rest 4 websites contain 152 detail pages. We evaluate the effectiveness of schema induction by preci X  sion, recall, and F-measure in terms of the number of basic nodes (columns) for each website.
 where B a ns denotes the number of basic nodes in the golden dataset and B number of basic nodes extracted by our system dynamic encoding and refinement procedure. However, the precision is less than ExAlg because the scoring tends to split a categor ical data node into multiple columns. By combining these columns as a disjunctive no de via GUI, the precision can be further improved. Figure 6 also shows the result of running RoadRunner and TEX. Since Road X  Runner does not handle HTML tables, the performance is very limited. As for TEX, it outputs a lot of basic nodes for both list pages and detailed pages, showing the difficulties for full page alignment.
 5.2 Extraction Accuracy Evaluation Next, we compared our system with TEX us ing the data set of WEIR which contains 24,028 pages from 40 websites in four doma ins: soccer, stock, video games, and books. We evaluate the performance in terms of data cells: where AnsS denotes the number of cells to be extracted for a website in the golden answer provided by WEIR and ExtC denotes the number of leaf nodes (cells) correctly extracted by our system for the website.
 Figure 7 shows that UWIDE performs much better than TEX (81 %) with default threshold (94 %) or adjusted threshold (98 %). Adjusting threshold allows us to detect false positive set and trigger full page alignment with higher density. Meanwhile it also avoids unnecessary combination of low density basic nodes.
 5.3 Efficiency Gain with Active Training In this section, we collected 2,519 pages from 12 websites to show the efficiency gain with generated wrappers. We start with two pages (the largest and smallest) to train a model for each website and test on the rema ining pages using the generated wrapper. If some testing pages fail the extraction procedure, we randomly add a failure page for retraining. The process is repeated until al l pages pass the extraction/verification proce X  dure. In average, 12.5 pages (14.4 %) are requ ired to train a complete model for a website. framework over the 1-phase extraction using all training pages is shown in Fig. 8 where the websites are sorted by the number of input pages. In summary, the speedup is more significant for websites with more pages. However, the page size and the complexity of the schema also affect the speedup. For WEIR dataset, UWIDE needs 0.278 s to process a page when all pages are used for training, while TEX needs 0.408 s to process a page. In this paper, we argue that combining wrap per induction and data extraction in one step may not be efficient when they have to be repeated many times for large-scale web data extraction. When a few pages are able to support page-level schema induction, we don X  X  need to waste time to anal yze all pages, but use the generated schema &amp; wrapper for extracting data from the remaining pages. We confirm this conjecture by designing an unsupervised training module for page-level schema induction and a wrapper module for data extraction from testing pages. The wrapper induction module is an end-to-end solution to identify data records as well as data attributes for each record and page. The wrapper module assigns labels to the leaf node sequence of a test page to complete the extraction step according to the FSM generated from the page-level schema and template.
 induction and data extraction. In terms of efficiency, the training phase is quite fast since we only need to process leaf nodes. The efficiency gain of the proposed framework is 2.7 times faster than the framework without wrapper generation.

