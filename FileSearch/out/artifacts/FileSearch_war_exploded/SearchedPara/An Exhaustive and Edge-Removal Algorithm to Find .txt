 Web is a huge information resource and increases dramatically. Although the growth of web seems chaos, but in fact web shows a great deal of self-organization [3]. Web communities are very important structure in web. There are well-known, explicitly-development. Most of them manifest themselves as newsgroups, webrings, or as communities are set of pages which created by a group or people with common HITS [4][5], Companion[6][7], max flow/min cut[8] and trawling algorithm[1][2]. But there are many communities are implicit and their number overcomes that of explicit ones. Trawling algorithm mainly focused on implicit communities. 
In this paper, we have analyzed some forms of structure which are not considered by trawling. Then by borrowing edge-removal idea of Newman [9][10][11], we we then check the possible cores in it. At each process some edges are removed from same as trawling method. But our method is different from trawling in several ways. save a lot of disk space. Second, we extract cores based on edge removal which reduce the times of scanning dataset. Like Kumar [2], we also use term frequency to evaluate whether or not the potential cores can organize communities. 
The outline of the paper is as follow. In section 2, we introduce some basic related knowledge. In section 3, we describe the preparing procedure of dataset and link database. In section 4, we introduce new algorithm. In section 5, we describe topics of communities. In section 6, we arrange the dataset and experiment and some result examples. Conclusions and future works are shown in section 7. Web can be abstracted to a large directed gr aph G=(V, E). V is the set of nodes, E is the set of edges. A pair of nodes (u, v)  X  E means that there is a hyperlink between u and v. A bipartite graph is a graph whose vertex set can be partitioned into two sets, which we denote F and C. Every directed edge in the graph is directed from a vertex u in F to a vertex v in C, depicted in Fig.1(a). 
A bipartite graph is dense if many of the possible edges between F and C are present. The trawling algorithm is based on the hypothesis: the dense bipartite graphs that are signatures of web communities contain at least one core. A core is a complete bipartite graph with at least i vertices in F and at least j vertices in C. Thus, the core community by finding cores, and then use the cores to find the rest of the community. According to Rajagopalan [12], the data mining graph is bipartite with left hand side and right hand side, denoted as LHS and RHS respectively. The web pages are collected by a web crawler [13]. We only extract urls information we don X  X  repeatedly extract the links belong to same domain. For instance, while www.edu.cn/xxx and www.edu.cn/yyy arrive, we only reserve www.edu.cn/xxx. An edge file. 
We use 128bit MD5 hash fingerprint as id to record a url. Every edge will occupy web graph. One is the set of edges ordered by source id, denoted as DSL and other by destination id, denoted as DSR. Due to a set of edges stored contiguously in DSL(DSR), it is easy to get out-degree(in-degree). We employ BerkeleyDB to manage dataset and use its BTREE access mode with sorted key. This mode maintain key in sorted order automatically. 
Because of mirror web sites and duplicated pages, there are many pages are derived from same resources [14]. Many of them are highly inter-connected and tend their common out-links is exceed to 85 percent, to say the two pages are mirror. 
Many researches have showed that the distribution of in-degree obeys power-law [3]. This law was used in many community discovery algorithms to prune the dataset. The pages with very low in-links and high in-links are pruned. Too low in-links popular web sites, such as www.google.com, www.yahoo.com etc. 4.1 Defects of Trawling Algorithm The complete directed bipartite graph is a metaphor of community. We call complete bipartite graph as a core, denoted as Ci,j, where i and j are nodes in fans and centers. employs the criterion of a core. Consider the example of a C 4,4 . Let u be a node of in-degree exactly 4. Then u can belong to a C 4,4 , if and only if the 4 nodes that point to it have a neighborhood intersection of size at least 4. 
The trawling algorithm has three defects. First, some Ci,js will be missing in C 2,3 . Unfortunately, node p has in-degree 5, according to trawling criterion, it would be pruned when find C 3,3 and C 2,3 because p does not have in-degree 3. If node p is destroy the structure of other cores. For example, in Fig. 1(b), when we find C 3,3 all nodes related to C 3,3 are removed. Node p is also removed and the structure of C 2,3 is destroyed. Third, the enumeration by combin ing i and j is at high cost because every scan of dataset only fans with out-degree j and centers with in-degree i are taken into account. The next subsection is the new algorithm which is proposed to overcome these shortcomings. 4.2 Exhaustive Algorithm and Removal of Edges Derived from [4][5][6], they use node removal method to find communities in a graph and then related edges are removed. Our method avoids scanning whole dataset associated are deleted. 
Before we describe our algorithm, some definitions and notations should be given denote RHS of BG. We use C(x, BG) to denote the nodes in R(BG) pointed by x and L(BG) that point to the set of nodes in R(BG) which are also pointed by node x. Let BGw to denote the web graph, BGs to denote constructed bipartite graph and BGc denote the bipartite graph of Ci,j and contain node p. Definition BGs: For any given node p  X  L(BGw), BGs is a bipartite subgraph constructed from p, where L(BGs)={p, S(p, BGw)} and R(BGs)=C(p, BGw). where L(BGc)=LHS of Ci,j , R(BGc)=RHS of Ci,j and p  X  L(BGc). The Definition BGs explains that procedure of constructing BGs has two steps. Step 1 is to get the children of p in BGw to construct LHS of BGs. Step 2 is to get the siblings of node p, and then use p and its siblings construct RHS of BGs. BGc Proof: For any u  X  L(BGc), we have C(u, BGc)=R(BGc), likewise for any v  X  R(BGc) we have P(v, GBc)=L(BGc). Because p  X  (BGc), C(p, BGc)=R(BGc). BGs is R(BGc) L(BGc) According to Theorem 1 , two-step construction of subgraph is complete to include all possible Ci,js. Then next job is how to extract all possible BGc from BGs. In trawling, which have following form: Here x, y and z are sets. t is an integer(t&gt;0) to give a threshold value. 
We define two sets as H and L and p1=p. Let S(p1, BGs)={p2, p3, ..., pn}. For a certain t, we have: 
After n iteration, if Hn  X   X  , the intersection of all nodes in Hn should be great than or equal to t, that is to say, a C(| Ln |, | Hn |) is found. Then we can check if | Ln |  X  i and | Hn |  X  j, we output C( Ln, Hn). We choose C(p1, BGs) as initial value of H, it is because |C(p1, BGs)|=n. So that guarantees to find max size of neighborhood intersection. Theorem 2: The BGs is constructed bipartite graph from p, {BGc} is the set of BGc, nodes in L(BGs) with t={m, m-1,...,j}, and output C(L, H) with |Ln|  X  i, {BGc} can be found by Hn and Ln. |Hn|  X  t. Therefore, if a BGc exist and L(BGc) BGc can be found by Hn and Ln. Theorem 1 tell us that the constructed bipartite graph BGs from a node p is complete to ..., j to calculate Hn and Ln, where n=L(BGs) and m=R(BGs), we can find all possible cores. Following is the detail description of our algorithm with specified i and j. 4.3 The Implementation of Algorithm We need two pair of dataset to store BGw and BGs respectively. Each BG is represented by a pair of dataset. The BGw is stored in dataset DSL and DSR in hard disk. BGs is stored in dataset EOS and EOD in main memory. All operation is based DSL. Then we apply following algorithm until both DSL and DSR are empty. Thus, web graph empty. Step1: Get a node p from dataset DSL, if DSL is empty to the end of algorithm. Step2: If |C(p, BGw)|  X  i then construct BGs. Step3: Prune BGs with i and j. Step4: Extract BGc by U and V function. 
Step5: Delete edges from BGw. After completion of community X  X  extraction, next job is to decide topics of each community. Due to the communities generated from only linkage information, eventually the page content is used to found topics. In the researches mentioned before, this job is fulfilled by human effort. So a mechanized process for dealing with over a hundred thousand communities is necessary. Our intuition is very simple, rank the frequencies of terms. From the ranking list, we choose top N terms to make a term list. So the topic of each community will be correspondent to a term list. A stop list is needed. Because many words like  X  X , th e X  have not the meaning, they don X  X  help to find topic. Term in different field should be assigned different weight, for example, the terms appear in the title field will have higher weight than in other field. 6.1 Preparation Dataset During crawling procedure, crawler only d eals with potential fans and reserve linkage information. Crawling process continue until disk full. Because in this experiment we web graph that contain about 6.7 million pages and 9.4 million edges. Owing to one source page followed by at least 6 destination pages, the resulting nodes of graph will be great than 6.7 million. We delete mirror or near-duplicated pages. Then we create two dataset DSL and DSR and prune centers by DSR. After mirror deletion and in-degree pruning, the dataset contain 6.9 million edges. 6.2 Cores Extraction i,j=2, i,j=3 and i,j=4 respectively. Then we get 149K, 29K and 10K Cores. From the cores extracted, we can find many are not in cluded by trawling. Fig. 2(a) depicts the distribution of cores vs. fans. Fig. 2(b) depicts cores vs. centers. From the curve of in Fig. 2, the distribution of C i,j s versus fans and centers obey power-law. 6.3 Core Examples The cores extracted from SDL and SDR are numerous. To systematically arrange them into a reasonable structure is still an important job. This job is our next work. In this paper, we chose 3 cores by random. Each of them have topic on  X  X usic X ,  X  X griculture X  and  X  X reen building X . For each topic, top 3 urls of fans and centers are shown in Table 1. 
Agricul -ture
Green building Many communities are implicit and to discover them is a challenge job. In this paper, We proposed a new algorithm under exhaustive idea and removal of edges. A the subgraph. At each scan of dataset, we can find all possible cores and some edges are removed from web graph. In dataset collection phrase, we improve crawler by dealing with potential fans and save disk space considerably. We also use terms frequency and rank of frequencies to deduce possible topics of communities. The web pages are colleted with about 7 million pages. We have set up an experiment on i, j=2, i, j=3 and i, j=4 and find 149K, 29K and 10K cores respectively. 
The web communities are very important structures in web. Finding all possible implicit communities is still a huge project. The future work could be on these aspects. First, the page X  X  text content should be cons idered with linkage information. Second, the hierarchy. Forth, how to deal with the overlap is a worthwhile research. 
