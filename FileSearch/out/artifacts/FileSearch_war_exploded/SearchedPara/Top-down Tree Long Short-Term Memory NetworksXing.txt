 Neural language models have been gaining increas-ing attention as a competitive alternative to n-grams. The main idea is to represent each word using a real-valued feature vector capturing the contexts in which it occurs. The conditional probability of the next word is then modeled as a smooth function of the feature vectors of the preceding words and the next word. In essence, similar representations are learned for words found in similar contexts result-ing in similar predictions for the next word. Previ-ous approaches have mainly employed feed-forward (Bengio et al., 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov, 2012) in order to map the feature vec-tors of the context words to the distribution for the next word. Recently, RNNs with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhu-ber, 1997; Hochreiter, 1998) have emerged as a pop-ular architecture due to their strong ability to capture long-term dependencies. LSTMs have been success-fully applied to a variety of tasks ranging from ma-chine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image descrip-tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-tions, neural language models essentially predict se-quences of words. Many NLP tasks, however, ex-ploit syntactic information operating over tree struc-tures (e.g., dependency or constituent trees). In this paper we develop a novel neural network model which combines the advantages of the LSTM archi-tecture and syntactic structure. Our model estimates the probability of a sentence by estimating the gen-eration probability of its dependency tree. Instead of explicitly encoding tree structure as a set of fea-tures, we use four LSTM networks to model four types of dependency edges which altogether specify how the tree is built. At each time step, one LSTM is activated which predicts the next word conditioned on the sub-tree generated so far. To learn the repre-sentations of the conditioned sub-tree, we force the four LSTMs to share their hidden layers. Our model is also capable of generating trees just by sampling from a trained model and can be seamlessly inte-grated with text generation applications.
Our approach is related to but ultimately differ-ent from recursive neural networks (Pollack, 1990) a class of models which operate on structured in-puts. Given a (binary) parse tree, they recursively generate parent representations in a bottom-up fash-ion, by combining tokens to produce representations for phrases, and eventually the whole sentence. The learned representations can be then used in classi-fication tasks such as sentiment analysis (Socher et al., 2011b) and paraphrase detection (Socher et al., 2011a). Tai et al. (2015) learn distributed representa-tions over syntactic trees by generalizing the LSTM architecture to tree-structured network topologies. The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability.

Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All in-stances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estima-tion. Emami et al. (2003) and Sennrich (2015) esti-mate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003). Mirowski and Vlachos (2015) re-implement the model of Gubbins and Vlachos (2013) with RNNs. They view sentences as sequences of words over a tree. While they ignore the tree structures themselves, we model them explicitly.

Our model shares with other structured-based lan-guage models the ability to take dependency infor-mation into account. It differs in the following re-spects: (a) it does not artificially restrict the depth of the dependencies it considers and can thus be viewed as an infinite order dependency language model; (b) it not only estimates the probability of a string but is also capable of generating dependency trees; (c) finally, contrary to previous dependency-based language models which encode syntactic in-formation as features, our model takes tree structure into account more directly via representing different types of dependency edges explicitly using LSTMs. Therefore, there is no need to manually determine which dependency tree features should be used or how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-pletion challenge, a benchmark language modeling dataset. Our results outperform the best published results on this dataset. Since our model is a general tree estimator, we also use it to rerank the top K de-pendency trees from the (second order) MSTPasrser and obtain performance on par with recently pro-posed dependency parsers. We seek to estimate the probability of a sentence by estimating the generation probability of its depen-dency tree. Syntactic information in our model is represented in the form of dependency paths. In the following, we first describe our definition of depen-dency path and based on it explain how the proba-bility of a sentence is estimated. 2.1 Dependency Path Generally speaking, a dependency path is the path between ROOT and w consisting of the nodes on the path and the edges connecting them. To rep-resent dependency paths, we introduce four types of edges which essentially define the  X  X hape X  of a dependency tree. Let w 0 denote a node in a tree and w 1 , w 2 ,..., w n its left dependents. As shown in Figure 1, L EFT edge is the edge between w 0 and its first left dependent denoted as ( w 0 , w 1 ) . Let w (with 1 &lt; k  X  n ) denote a non-first left dependent of w 0 . The edge from w k  X  1 to w k is a N X -L EFT edge (N X stands for N EXT ), where w k  X  1 is the right adjacent sibling of w k . Note that the N X -L EFT edge ( w k  X  1 , w k ) replaces edge ( w 0 , w k ) (illustrated with a dashed line in Figure 1) in the original dependency tree. The modification allows information to flow from w 0 to w k through w 1 ,..., w k  X  1 rather than di-rectly from w 0 to w k . R IGHT and N X -R IGHT edges are defined analogously for right dependents.
Given these four types of edges, dependency paths (denoted as D ( w ) ) can be defined as follows bearing in mind that the first right dependent of ROOT is its only dependent and that w p denotes the parent of w . We use ( ... ) to denote a sequence, where () is an empty sequence and k is an operator for concatenating two sequences. (1) if w is ROOT , then D ( w ) = () (2) if w is a left dependent of w p (3) if w is a right dependent of w p A dependency tree can be represented by the set of its dependency paths which in turn can be used to
Dependency paths for the first two levels of the tree in Figure 2 are as follows (ig-noring for the moment the subscripts which we explain in the next section). D ( sold ) = (  X  ROOT , R IGHT  X  ) (see definitions (1) and (3a)), ( year ) = D ( sold ) k (  X  sold , L EFT  X  ) (see (2a)), ( manufacturer ) = D ( year ) k (  X  year , N X -L EFT  X  ) (see (2b)), D ( cars ) = D ( sold ) k (  X  sold , R IGHT (see (3a)), D ( in ) = D ( cars ) k (  X  cars , N X -R IGHT (according to (3b)). 2.2 Tree Probability The core problem in syntax-based language model-ing is to estimate the probability of sentence S given its corresponding tree T , P ( S | T ) . We view the prob-ability computation of a dependency tree as a gener-ation process. Specifically, we assume dependency trees are constructed top-down, in a breadth-first manner. Generation starts at the ROOT node. For each node at each level, first its left dependents are generated from closest to farthest and then the right dependents (again from closest to farthest). The same process is applied to the next node at the same level or a node at the next level. Figure 2 shows the breadth-first traversal of a dependency tree.
Under the assumption that each word w in a de-pendency tree is only conditioned on its dependency path , the probability of a sentence S given its depen-dency tree T is: where D ( w ) is the dependency path of w . Note that each word w is visited according to its breadth-first search order (BFS(T)) and the probability of ROOT is ignored since every tree has one. The role of ROOT in a dependency tree is the same as the begin of sentence token (BOS) in a sentence. When com-puting P ( S | T ) (or P ( S ) ), the probability of ROOT BOS) is ignored (we assume it always exists), but is used to predict other words. We explain in the next section how T REE LSTM estimates P ( w | D ( w )) . 2.3 Tree LSTMs A dependency path D ( w ) is subtree which we de-note as a sequence of  X  word , edge-type  X  tuples. Our innovation is to learn the representation of D ( w ) us-ing four LSTMs. The four LSTMs (G EN -L, G EN -R, G EN -N X -L and G EN -N X -R) are used to repre-sent the four types of edges (L EFT , R IGHT , N X L EFT and N X -R IGHT ) introduced earlier. G EN , N X , L and R are shorthands for G ENERATE , N EXT , L
EFT and R IGHT . At each time step, an LSTM is chosen according to an edge-type; then the LSTM takes a word as input and predicts/generates its de-pendent or sibling. This process can be also viewed as adding an edge and a node to a tree. Specifi-cally, LSTMs G EN -L and G EN -R are used to gen-erate the first left and right dependent of a node ( w 1 and w 4 in Figure 3). So, these two LSTMs are responsible for going deeper in a tree. While G
EN -N X -L and G EN -N X -R generate the remain-ing left/right dependents and therefore go wider in a tree. As shown in Figure 3, w 2 and w 3 are gener-ated by G EN -N X -L, whereas w 5 and w 6 are gener-ated by G EN -N X -R. Note that the model can handle any number of left or right dependents by applying G EN -N X -L or G EN -N X -R multiple times.

We assume time steps correspond to the steps taken by the breadth-first traversal of the depen-dency tree and the sentence has length n . At time step t (1  X  t  X  n ), let  X  w t 0 , z t  X  denote the last tuple in D ( w t ) . Subscripts t and t 0 denote the breadth-first search order of w t and w t 0 , respectively. z  X  { L EFT , R IGHT , N X -L EFT , N X -R IGHT } is the edge type (see the definitions in Section 2.1). Let W W | V | is the vocabulary size, s the word embedding size and d the hidden unit size. We use tied W e and tied W ho for the four LSTMs to reduce the number of pa-rameters in our model. The four LSTMs also share shared hidden states of all time steps and e ( w t ) the one-hot vector of w t . Then, H [ : , t ] represents D ( w at time step t , and the computation 2 is: where the initial hidden state H [ : , 0 ] is initialized to a vector of small values such as 0.01. According to Equation (2b), the model selects an LSTM based on edge type z t . We describe the details of LSTM z t in the next paragraph. The probability of w t given its dependency path D ( w t ) is estimated by a softmax function: We must point out that although we use four jointly trained LSTMs to encode the hidden states, the train-ing and inference complexity of our model is no dif-ferent from a regular LSTM, since at each time step only one LSTM is working.

We implement LSTM z in Equation (2b) using a deep LSTM (to simplify notation, from now on we write z instead of z t ). The inputs at time step t are x t and h t 0 (the hidden state of an earlier time step t 0 ) and the output is h t (the hidden state of cur-rent time step). Let L denote the layer number of LSTM z and  X  h l t the internal hidden state of the l -th layer of the LSTM z at time step t , where x t is  X  h 0 t h tiplicative gates and memory cells  X  c l t (at l -th layer) in order to address the vanishing gradient problem which makes it difficult for the standard RNN model to learn long-distance correlations in a sequence. Here,  X  c l t is a linear combination of the current input signal u t and an earlier memory cell  X  c l input information u t will flow into  X  c l t is controlled by input gate i t and how much of the earlier mem-ory cell  X  c l gate f t . This process is computed as follows: W W are weight matrices for f t .  X  is a sigmoid function and the element-wise product.

Output gate o t controls how much information of the cell  X  c l t can be seen by other modules: Application of the above process to all layers L , will yield  X  h L t , which is h t . Note that in implementation, all  X  c l t and  X  h l t (1  X  l  X  L ) at time step t are stored, although we only care about  X  h L t ( h t ). 2.4 Left Dependent Tree LSTMs T
REE LSTM computes P ( w | D ( w )) based on the de-pendency path D ( w ) , which ignores the interaction between left and right dependents on the same level. In many cases, T REE LSTM will use a verb to pre-dict its object directly without knowing its subject. For example, in Figure 2, T REE LSTM uses  X  ROOT , R
IGHT  X  and  X  sold , R IGHT  X  to predict cars . This in-formation is unfortunately not specific to cars (many things can be sold, e.g., chocolates , candy ). Consid-ering manufacturer , the left dependent of sold would help predict cars more accurately.

In order to jointly take left and right dependents into account, we employ yet another LSTM, which goes from the furthest left dependent to the closest left dependent (L D is a shorthand for left depen-dent). As shown in Figure 4, L D LSTM learns the representation of all left dependents of a node w 0 ; this representation is then used to predict the first right dependent of the same node. Non-first right de-pendents can also leverage the representation of left dependents, since this information is injected into the hidden state of the first right dependent and can percolate all the way. Note that in order to retain the generation capability of our model (Section 3.4), we only allow right dependents to leverage left depen-dents (they are generated before right dependents).
The computation of the L D T REE LSTM is al-most the same as in T REE LSTM except when z = G EN -R. In this case, let v t be the cor-responding left dependent sequence with length K ( v t = ( w 3 , w 2 , w 1 ) in Figure 4). Then, the hidden state ( q k ) of v t at each time step k is: where q K is the representation for all left depen-dents. Then, the computation of the current hid-den state becomes (see Equation (2) for the original computation): where q K serves as additional input for LSTM G EN -R . All other computational details are the same as in TreeLSTM (see Section 2.3). 2.5 Model Training On small scale datasets we employ Negative Log-likelihood (NLL) as our training objective for both T REE LSTM and L D T REE LSTM: where S is a sentence in the training set S , T is the dependency tree of S and P ( S | T ) is defined as in Equation (1).
On large scale datasets (e.g., with vocabulary size of 65K), computing the output layer activa-tions and the softmax function with NLL would become prohibitively expensive. Instead, we em-ploy Noise Contrastive Estimation (NCE; Gutmann and Hyv  X  arinen (2012), Mnih and Teh (2012)) which treats the normalization term  X  Z in  X  P ( w | D ( w t is to discriminate between samples from a data dis-tribution  X  P ( w | D ( w t )) and a known noise distribu-tion P n ( w ) via binary logistic regression. Assuming that noise words are k times more frequent than real words in the training set (Mnih and Teh, 2012), then the probability of a word w being from our model P large vocabulary models with the following training objective: where  X  w t , j is a word sampled from the noise distri-bution P n ( w ) . We use smoothed unigram frequen-cies (exponentiating by 0.75) as the noise distribu-tion P n ( w ) (Mikolov et al., 2013b). We initialize ln  X 
Z = 9 as suggested in Chen et al. (2015), but in-stead of keeping it fixed we also learn  X  Z during train-ing (Vaswani et al., 2013). We set k = 20. We assess the performance of our model on two tasks: the Microsoft Research (MSR) sentence com-pletion challenge (Zweig and Burges, 2012), and de-pendency parsing reranking. We also demonstrate the tree generation capability of our models. In the following, we first present details on model train-ing and then present our results. We implemented our models using the Torch library (Collobert et al., 2011) and our code is available at https:// github.com/XingxingZhang/td-treelstm . 3.1 Training Details We trained our model with back propagation through time (Rumelhart et al., 1988) on an Nvidia GPU Card with a mini-batch size of 64. The ob-jective (NLL or NCE) was minimized by stochastic gradient descent. Model parameters were uniformly initialized in [  X  0 . 1 , 0 . 1 ] . We used the NCE objec-tive on the MSR sentence completion task (due to the large size of this dataset) and the NLL objec-tive on dependency parsing reranking. We used an initial learning rate of 1.0 for all experiments and when there was no significant improvement in log-likelihood on the validation set, the learning rate was divided by 2 per epoch until convergence (Mikolov et al., 2010). To alleviate the exploding gradients problem, we rescaled the gradient g when the gradi-ent norm || g || &gt; 5 and set g = 5 g 2013; Sutskever et al., 2014). Dropout (Srivastava et al., 2014) was applied to the 2-layer T REE LSTM and L D T REE LSTM models. The word embedding size was set to s = d / 2 where d is the hidden unit size. 3.2 Microsoft Sentence Completion Challenge The task in the MSR Sentence Completion Chal-lenge (Zweig and Burges, 2012) is to select the correct missing word for 1,040 SAT-style test sen-tences when presented with five candidate comple-tions. The training set contains 522 novels from the Project Gutenberg which we preprocessed as fol-lows. After removing headers and footers from the files, we tokenized and parsed the dataset into de-pendency trees with the Stanford Core NLP toolkit (Manning et al., 2014). The resulting training set contained 49M words. We converted all words to lower case and replaced those occurring five times or less with UNK. The resulting vocabulary size was 65,346 words. We randomly sampled 4,000 sentences from the training set as our validation set.
The literature describes two main approaches to the sentence completion task based on word vectors and language models. In vector-based approaches, all words in the sentence and the five candidate words are represented by a vector; the candidate which has the highest average similarity with the sentence words is selected as the answer. For lan-guage model-based methods, the LM computes the probability of a test sentence with each of the five candidate words, and picks the candidate comple-tion which gives the highest probability. Our model belongs to this class of models.
Table 1 presents a summary of our results to-gether with previoulsy published results. The best performing word vector model is IV LBL (Mnih and Kavukcuoglu, 2013) with an accuracy of 55.5, while the best performing single language model is LBL (Mnih and Teh, 2012) with an accuracy of 54.7. Both approaches are based on the log-bilinear lan-guage model (Mnih and Hinton, 2007). A combi-nation of several recurrent neural networks and the skip-gram model holds the state of the art with an accuracy of 58.9 (Mikolov et al., 2013b). To fairly compare with existing models, we restrict the layer Parser MSTParser-2nd 92.20 88.78 91.63 88.44 T REE LSTM 92.51 89.07 91.79 88.53 T REE LSTM* 92.64 89.09 91.97 88.69 L D T REE LSTM 92.66 89.14 91.99 88.69 NN parser* 92.00 89.70 91.80 89.60 S-LSTM* 93.20 90.90 93.10 90.90 size of our models to 1. We observe that L D T REE L-STM consistently outperforms T REE LSTM, which indicates the importance of modeling the interac-tion between left and right dependents. In fact, L
D T REE LSTM ( d = 400) achieves a new state-of-the-art on this task, despite being a single model. We also implement LSTM and bidirectional LSTM language models. 3 An LSTM with d = 400 out-performs its smaller counterpart ( d = 300), however performance decreases with d = 450. The bidirec-tional LSTM is worse than the LSTM (see Mnih and Teh (2012) for a similar observation). The best performing LSTM is worse than a L D T REE L-STM ( d = 300). The input and output embeddings ( W e and W ho ) dominate the number of parame-ters in all neural models except for RNNME, de-pRNN+3gram and ldepRNN+4gram, which include a ME model that contains 1 billion sparse n-gram features (Mikolov, 2012; Mirowski and Vlachos, 2015). The number of parameters in T REE LSTM and L D T REE LSTM is not much larger compared to LSTM due to the tied W e and W ho matrices. 3.3 Dependency Parsing In this section we demonstrate that our model can be also used for parse reranking. This is not possi-ble for sequence-based language models since they cannot estimate the probability of a tree. We use our models to rerank the top K dependency trees produced by the second order MSTParser (McDon-perimental setup of Chen and Manning (2014) and Dyer et al. (2015). Specifically, we trained T REE L-STM and L D T REE LSTM on Penn Treebank sec-tions 2 X 21. We used section 22 for development and section 23 for testing. We adopted the Stanford ba-sic dependency representations (De Marneffe et al., 2006); part-of-speech tags were predicted with the Stanford Tagger (Toutanova et al., 2003). We trained T
REE LSTM and L D T REE LSTM as language mod-els (singletons were replaced with UNK) and did not use any POS tags, dependency labels or com-position features, whereas these features are used in Chen and Manning (2014) and Dyer et al. (2015). We tuned d , the number of layers, and K on the de-velopment set.

Table 2 reports unlabeled attachment scores (UAS) and labeled attachment scores (LAS) for the MSTParser, T REE LSTM ( d = 300, 1 layer, K = 2), and L D T REE LSTM ( d = 200, 2 layers, K = 4). We also include the performance of two neural network-based dependency parsers; Chen and Manning (2014) use a neural network classifier to predict the correct transition (NN parser); Dyer et al. (2015) also implement a transition-based depen-dency parser using LSTMs to represent the contents of the stack and buffer in a continuous space. As can be seen, both T REE LSTM and L D T REE LSTM out-perform the baseline MSTParser, with L D T REE L-STM performing best. We also initialized the word embedding matrix W e with pre-trained GLOVE vec-tors (Pennington et al., 2014). We obtained a slight improvement over T REE LSTM (T REE LSTM* in Table 2; d = 200, 2 layer, K = 4) but no im-provement over L D T REE LSTM. Finally, notice that L
D T REE LSTM is slightly better than the NN parser in terms of UAS but worse than the S-LSTM parser. In the future, we would like to extend our model so that it takes labeled dependency information into ac-count. 3.4 Tree Generation This section demonstrates how to use a trained L
D T REE LSTM to generate tree samples. The gen-eration starts at the ROOT node. At each time step t , for each node w t , we add a new edge and node to the tree. Unfortunately during generation, we do not know which type of edge to add. We therefore use four binary classifiers (A DD -L EFT , A DD -R IGHT , A
DD -N X -L EFT and A DD -N X -R IGHT ) to predict whether we should add a L EFT , R IGHT , N X -L EFT dicts true, we use the corresponding LSTM to gener-ate a new node by sampling from the predicted word distribution in Equation (3). The four classifiers take the previous hidden state H [ : , t 0 ] and the output em-bedding of the current node W ho  X  e ( w t ) as features. 6 Specifically, we use a trained L D T REE LSTM to go through the training corpus and generate hidden states and embeddings as input features; the corre-sponding class labels (true and false) are  X  X ead off X  the training dependency trees. We use two-layer rec-tifier networks (Glorot et al., 2011) as the four clas-sifiers with a hidden size of 300. We use the same L
D T REE LSTM model as in Section 3.3 to gener-ate dependency trees. The classifiers were trained using AdaGrad (Duchi et al., 2011) with a learning rate of 0.01. The accuracies of A DD -L EFT , A DD -R 94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-ure 5 shows examples of generated trees. In this paper we developed T REE LSTM (and L
D T REE LSTM), a neural network model architec-ture, which is designed to predict tree structures rather than linear sequences. Experimental results on the MSR sentence completion task show that L D T REE LSTM is superior to sequential LSTMs. Dependency parsing reranking experiments high-light our model X  X  potential for dependency pars-ing. Finally, the ability of our model to gener-ate dependency trees holds promise for text gen-eration applications such as sentence compression and simplification (Filippova et al., 2015). Although our experiments have focused exclusively on depen-dency trees, there is nothing inherent in our formu-lation that disallows its application to other types of tree structure such as constituent trees or even tax-onomies.
 We would like to thank Adam Lopez, Frank Keller, Iain Murray, Li Dong, Brian Roark, and the NAACL reviewers for their valuable feedback. Xingxing Zhang gratefully acknowledges the financial sup-port of the China Scholarship Council (CSC). Liang Lu is funded by the UK EPSRC Programme Grant EP/I031022/1, Natural Speech Technology (NST).
