 Reliable and real-time people counting is crucial in many applications. For example, estimating the crowd density in public places can help managers identify unsafe sit-uations and regulate traffic appropriately. As another example, the public museum can control the number of people entering according to the real-time people flow infor-mation. Therefore, it is necessary to develop an automatic system of estimating the number of people accurately and rapidly.

Computer-vision-based methods are widely used for people counting. Most previous works can only count moving people from a single camera [Chan et al. 2008; Kilambi et al. 2008; Rabaud and Belongie 2006]. However, these methods cannot estimate the number of still people. Furthermore, these counting systems are based on a single view, which means the count will be affected greatly when there is heavy occlusion and lack of visibility. Some multiview approaches were proposed to detect and track people in a dense crowd to avoid occlusion [Eshel and Moses 2008; Fleuret et al. 2008]. However, they also assumed that the people are moving.

In this article, we build a system for robust and fast counting the number of peo-ple (still or moving) under occlusion through multiple cameras. The basic idea of our people counting system is to detect the people in each frame from each single view, and then integrate the detecting information from multiple cameras. More specifically, our system tries to resolve the problem of people counting with occlusion by solving four subproblems, namely, feature extraction, fast detection, occlusion handling, and border control. Feature extraction is used to describe the people to handle the large variations in human appearance and poses, and thus ensures our system can detect still or moving people robustly. Fast detection is mainly to satisfy the demands of the real-time applications. Occlusion handling is to tackle the frequent occlusion and lack of visibility between people in crowded scenes, and thus ensures that our system can count people accurately. Border control is used to improve the speed and accuracy of moving people counting.

On the feature extraction level, we employ a robust people detector [Zeng and Ma 2010], which uses PCA (Principal Components Analysis) on multilevel edge and tex-ture to handle the large variations in people appearances and poses. The edge informa-tion is described by Histograms of Oriented Gradients (HOG) [Dalal and Triggs 2005], and the texture information is represented by a Local Binary Pattern (LBP) [Ojala et al. 2002]. To further improve the detection performance, PCA is used to reduce the dimension of the multilevel HOG-LBP feature set. It is worth mentioning that Wang et al. [2009] combine HOG and LBP to detect the human in still images, and have achieved convincing results. However, our PCA-based multilevel HOG-LBP descrip-tors (PMHL) will be shown more discriminative and more robust than state-of-the-art algorithms, such as the HOG-LBP method [Wang et al. 2009].

On the fast detection level, we use a two-stage cascade-of-rejectors structure [Zeng et al. 2010] to improve the speed of the PMHL method. The PCA-based multilevel HOG (PMH) feature is used at the first stage of the cascade, and the PCA-based mul-tilevel LBP (PML) is used at the second stage. In order to maintain an accuracy level similar to the PMHL algorithm, we improve the mi-SVM (Support Vector Machine for multiple instance learning) [Andrews et al. 2003] to train the PMH and PML features, respectively, and thus achieve high recall at each stage of the cascade. In this way, we can detect people quickly and reliably, while maintaining a similar accuracy to PMHL.
On the occlusion handling level, we use multiview visual fusion to handle the oc-clusion and lack of visibility between people in crowded scenes. Although there are some approaches that try to resolve partial occlusion in single camera systems [Lin and Davis 2010; Wang et al. 2009; Wu and Nevatia 2007], they cannot handle the heavy occlusion and lack of visibility in cluttered scenes. Using multiple views visual fusion is a logical step to recover information that might be missing in a particular view [Khan and Shah 2009]. Therefore we extend our fast people detection method from single-view input to multiple-views input by using a given homography [Hart-ley and Zisserman 2003]. We will show that the multiview visual fusion method can bring satisfactory improvement on the problem of heavy occlusion and lack of visibil-ity. Thus, we design an effective system for fast and reliable people (still or moving) counting by using multiple cameras.

To further improve the speed and accuracy of moving people counting, we com-bine our multiview fusion detection method with particle tracking [Arulampalam et al. 2002] to count the number of people moving in/out the camera view ( X  X order control X ). We first use our multiview fusion method to detect people in the first frame. Then, we detect and track each person moving in/out the border of camera view. In this way, the search space for people detection can be reduced greatly. Thus, the speed and accuracy of people counting can be improved significantly.

The rest of the article is organized as follows. Section 2 briefly summarizes the re-lated work. The PMHL detector is introduced in Section 3. Section 4 describes the de-tails of two-stage cascade-of-rejectors structure. The multiview visual fusion method is described in Section 5. Section 6 introduces our border control method. Experiments and evaluations are demonstrated in Section 7. Section 8 describes our deployed people counting system prototype. Section 9 compares well-known commercial people count-ing systems with our multiview people counting system. Finally, we give a conclusion in Section 10. There have been various computer-vision-based approaches to estimating the number of people in crowded environments. Most previous works focused on counting mov-ing people from a single camera [Chan et al. 2008; Kilambi et al. 2008; Rabaud and Belongie 2006]. Kilambi et al. [2008] introduced a group-based detection and track-ing strategy to estimate the number of people in urban environments. Chan et al. [2008] applied Gaussian process regression to count the number of people in a cam-pus. Rabaud and Belongie [2006] proposed a trajectory set clustering method to count moving objects in a scene. However, these methods assumed that the people are mov-ing, and thus cannot estimate the number of still people. Furthermore, these people counting systems are based on a single view, which cannot handle the problem of heavy occlusion and lack of visibility. Compared with these methods, our people counting sys-tem is based on our cascade PMHL detector and multiview fusion, which can detect still people with heavy occlusion robustly and quickly. Our system will use state-of-the-art techniques for the human detection, we now review them briefly.

In the past years, there are abundant algorithms on human detection from a sin-gle camera. Among them, the HOG descriptor proposed by Dalal and Triggs [2005] is one of the most successful human detectors in static images. The SVM-trained HOG detector has been proved to outperform other detectors such as Haar wavelets, PCA-SIFT, and Shape Contexts. Recently, a lot of approaches were proposed to improve the accuracy and efficiency of human detection. Felzenszwalb et al. [2009] introduced a deformable part template and a latent SVM training method to detect the deformable objects. Lin and Davis [2010] proposed a part-template tree model and its automatic learning algorithm for human detection. Xu et al. [2010] proposed a linear classifier on the weighted HOG features for human detection via L1-norm minimization to ob-tain weight and the sparse representation. Wang et al. [2009] combined the HOG and cell-structured Local Binary Pattern (LBP) to detect humans in still images. The HOG-LBP algorithm has achieved convincing detection results because the combination of edge information and texture information can capture the appearance of a human bet-ter. Our PMHL method was inspired by the HOG-LBP algorithm, and achieved more discriminative, more robust detection results (see Section 3 for detailed description).
Some approaches were proposed to improve the efficiency of human detection. Most approaches used a cascade-of-rejector structure to accelerate the detection speed be-cause this structure was first successfully applied to face detection [Viola and Jones 2004]. Zhu et al. [2006] combined the cascade-of-rejector structure with the variable-size blocks of HOG to improve the speed of the HOG detector. Chen and Chen [2008] employed intensity-based and gradient-based features to describe the pedestrians, and trained these features by using a boosted cascading structure with metastages to en-hance the detection accuracy and efficiency. Similar to the method of Zhu et al. [2006], we use a two-stage cascade-of-rejectors structure to improve the speed of PMHL al-gorithm. We also describe how to maintain a similar accuracy to PMHL by using the improved mi-SVM training method (see Section 4 for detailed description).

Some approaches were proposed to deal with the partial occlusion problem in single-camera systems. In Wang et al. [2009], the block-based mean shift algorithm is applied to segment the foreground region for each sliding window for handling the occlusion problem of human detection. Lin and Davis [2010] handle multiple occluded humans by a part-template tree model and iteratively optimizing through matching score re-evaluation and occlusion analysis. Although these approaches have made great suc-cess in handling partial occlusion, they cannot resolve the heavy occlusion and lack of visibility in cluttered scenes. Multiple cameras with overlapping fields of view are mostly used in detection and tracking for handling the heavy occlusion and lack of visibility that might be missing in a single view. Khan and Shah [2009] used multiple views and a planar homography constraint to detect the location of peoples X  feet, and thus resolve the occlusion between people in crowded scenes. Eshel and Moses [2008] detect people X  X  heads in a dense crowd to avoid occlusion by combining data from sev-eral views and extracting the height information for head segmentation. Both of these approaches are based on background subtraction to extract the foreground region, and thus can only detect moving people. Furthermore, when there are shadows of the people themselves, the detection accuracy is affected greatly. Compared to the other occlusion handling methods, our proposed approach does not rely on the background subtraction, and thus is suitable for both moving and still people. Furthermore, our ap-proach is more robust to the large variation of shadow, illumination, and background, and thus can resolve both partial and heavy occlusion better (see Section 5 for detailed description). Our people counting system is based on detecting the people in each frame from each single view. Thus we need a robust people detector to detect the objects in a still image. We use a novel combination of the multilevel HOG-LBP feature with PCA to describe the people. Figure 1 shows the process of our PCA-based multilevel HOG-LBP detector. HOG [Dalal and Triggs 2005] is an excellent descriptor for capturing the edge direction or the distribution of local intensity gradients of objects. And it has been applied suc-cessfully to detect upstanding humans. In order to improve the performance of HOG, we use the multilevel HOG feature to describe the people. Figure 2 illustrates the pro-cedure of the feature extraction. We first compute the gradient magnitude (Figure 2(b)) of the input image by using 1D [  X  1 0 1] masks. The size of the input image is 64  X  128 pixels with gamma normalization.

Second, we divide the gradient magnitude of the image into nonoverlapped blocks at three levels (Figure 2(c)). Each block at each level consists of four rectangle cells. The gradient magnitude of each pixel in the cell is voted into 9 bins according to the orientation of the pixel X  X  gradient. The nine orientation bins are evenly spaced over 0
Third, the histograms of the four cells in each block (Figure 2(d)) are concatenated into a feature vector of the block. The L2-Hys normalization [Dalal and Triggs 2005] of the feature vector is used to reduce the influence of the local variation in illumina-tion and foreground-background contrast. Then, the feature vectors of the blocks are concatenated into the feature vector of each level.

Finally, the three feature vectors corresponding to the three levels are concatenated into the final 6048D multilevel HOG feature vector (Figure 2(e)). As we mentioned in Section 2, only edge information is not discriminative enough for human detection. The combination of edge shape information and texture information will enhance the detection performance significantly. Thus, as in Wang et al. [2009] we use LBP [Ojala et al. 2002] as the texture descriptors of people.
 LBP is an excellent texture descriptor for its invariance to gray-scale and rotation. We extend the work of Ahonen et al. [2006], using multilevel block structured LBP to describe the people. We first divide the gray image of the input image into blocks at two levels, which is similar to the top and middle level of Figure 2(c). We don X  X  use the bottom level of the multilevel structure because the block size at this level is only 8  X  8, which is too low to be discriminative for the LBP pattern. Then, we calculate the histograms of the LBP patterns for each block. The LBP patterns we used are LBP 2 8 , 1 (Figure 3).

For each block at one level, pixels in the block with different uniform patterns [Ojala et al. 2002] are voted into different bins and all of the nonuniform patterns are voted into one bin. Each block is thus represented by a 59D feature vector that is normalized to an L2-Hys unit length. Note that we use a L2-Hys normalization scheme for the histograms of the blocks, which is better performance than the L1-sqrt normalization scheme used in Wang et al. [2009] (by 5% with a false positive of 10  X  4 ). Finally, the two LBP feature vectors corresponding to the two levels are concatenated into the final 2360D multilevel LBP feature vector. By combining the multilevel HOG and the multilevel LBP as the final 8408D feature set, we can robustly detect people with a large variation of poses, appearance, and background. To further improve the detection rate, we use PCA to discover the low-dimensional feature of multilevel HOG-LBP.

PCA, as a standard technique for dimensionality reduction, has been widely applied in the area of computer vision such as face recognition. We use PCA to reveal the hid-den structures of the multilevel HOG-LBP feature by computing a linear transforma-tion that maps the feature set from a high-dimensional space to a lower-dimensional space. This transformation can accurately represent the high-dimensional feature with a compact feature representation. More importantly, projecting the multilevel HOG-LBP feature onto the low-dimensional space can discard the redundant and noisy information. Thus the PCA-based multilevel HOG-LBP detector makes the detection more robust. Although our PMHL detector can improve the detection accuracy significantly, the dimensionality of the feature vector becomes extremely high. As a result, the speed of the detection is too slow to satisfy the demands of real-time applications because the PMHL method extracts the multilevel HOG features and the multilevel LBP features simultaneously. Furthermore, it is also time consuming to project the high-dimensional feature vector on the PCA subspace. In order to accelerate our human detection algorithm, we develop a coarse-to-fine method based on PMHL descriptors. The basic idea is to place coarse features at the first stage and put a more accurate features at the second stage. The features at the first stage can quickly filter out most nonpeople patterns, and the features at the sec-ond stage can select the positives in the remaining patterns.

To design the coarse-to-fine structure, we evaluate the performance of the PMH feature and PML feature, respectively. We discover that both PMH and PML can achieve real-time detection speed for a 320  X  240 image by using the integral his-togram [Porikli 2005] on a conventional desktop computer. Furthermore, PMH and PML can achieve 85% and 86% detection rate at 1 FPPI (false positives per image) respectively on the INRIA dataset [Dalal and Triggs 2005], which is close to that of PMHL (86% at 1 FPPI) by using linear SVM [Fan et al. 2008]. These results inspire us to speed up the PMHL algorithm. If we can enhance the recall of PMH and PML respectively, we can use the cascade-of-rejector approach for detecting humans fast, while maintaining a similar accuracy to PMHL.

Our fast human detection approach is based on the two-stage cascade-of-rejectors structure (Figure 4). For all sliding windows, we first classify them by using their PMH features at the first stage of the cascade. The majority of detection windows (those containing humans with low probability) are discarded based on the PMH features. The positive results from the first stage trigger the second-stage classifier which uses the PML features to decide the final results. The negative results at any stage are rejected immediately. In this way, we can rapidly reject as many negatives as possible at the first stage and select the positives at the second stage. In order to maintain an accuracy level similar to the PMHL algorithm, we must adjust the classifier at each stage to detect more than 93% of humans with a false positive per image of less than 3. However, it is hard to attain this goal if we only use the linear SVM to train the PMH and PML feature respectively.

To achieve high recall at each stage of the cascade, we treat the human detection as a Multiple Instance Learning (MIL) problem. MIL is a generalization of supervised classification in which training samples come in  X  X ags X . A bag is labeled positive if at least one sample in the bag is positive, and the bag is labeled negative if all samples in the bag are negative.

We combine MIL with the linear SVM to train the PMH and PML features respec-tively. Our algorithm was modified from mi-SVM proposed by Andrews et al. [2003]. Mi-SVM tries to look for a hyperplane such that there is at least one instance from each positive bag in the positive halfspace, while all instances from negative bags are in the negative halfspace. However, we try to look for a hyperplane such that most of the positive instances from each positive bag are in the positive halfspace, while most of the false positives from the positive bag are in the negative halfspace (Figure 5). ALGORITHM 1: The improved MI-SVM heuristics optimization algorithm
Our goal can be described more formally. Given a set of images each of which includes at least one human (positive bags), their associated labels are Y k =1( k  X  { 1 ,..., n } ). For all of the sliding windows (instances) x images, their labels are y i  X  X  1 ,  X  1 } .

We then employ the measure proposed in Doll  X  ar et al. [2009] to distinguish the positive windows and the false positive windows. Specifically, the overlapped area  X  between a detected window ( DW dt ) and a ground-truth positive window ( DW gt )must exceed 50%. The unmatched DW dt is treated as the false positive window.

Thus, the process of looking for the hyperplane can be described as the following optimization problem. We have where w and b are the parameters of the linear SVM, C &gt; 0 is the penalty parameter of the error term, and  X  i ( i  X  X  1 ,..., m } ) are the slack variables.

The preceding problem is a mixed integer programming problem, which is only a conceptual solution. In order to obtain the closed-form solution of formula (2), we derived a heuristics optimization algorithm. Our optimization algorithm is based on the fact that the hyperplane of the linear SVM will move to the direction of the negative samples in the binary case if we increase the number of the positive samples. Furthermore, we believe that learning is a process of iterative training. Algorithm 1 shows the process of our heuristics optimization. Although our cascade PMHL detector is effective to the partial occlusion in single-camera systems (Figure 6(b)), it cannot resolve the heavy occlusion and lack of visibil-ity. Thus, we use multiview visual fusion to recover information that might be missing in a particular view. As shown in Figure 6(b), 6(c), people who are occluded heavily or lack visibility from one view can be visible from another view at the same time, and thus can be detected by using our cascade PMHL detector. To count the number of people in multiple cameras with overlapping fields of view, we need to judge which objects detected from multiple views are the same objects. We first obtain the central points of the detected people. If two central points from two cameras respectively satisfy the homography constraint [Hartley and Zisserman 2003], we can conclude that the corresponding people are the same object (Figures 6(b) and 6(c)).
A homography is an invertible transformation that maps points in one image to the corresponding points in a second image. As illustrated in Figure 6(a), given a point p w =( x w , y w , z w ) that lines on a world coordinate system, and given the image point p the homography is a mapping relation between p 1 and p 2 . That is, we wish to find a homography matrix H such that p 1 = H  X  p 2 , namely
H is called the homography matrix induced by the world coordinate system and the image coordinate system. As illustrated in Figure 6(a), the homography constraint can be thought as a process of two-step projection: First, point p 1 from the image coordinate system of camera 1 is projected to a two-dimensional point p w 1 on the world coordinate system. Then, p w 1 is projected to a point p 2 on the image coordinate system of camera 2. Calculation of H can be found in Hartley and Zisserman [2003]. Through the homography constraint, we can judge which objects detected from multiple views are the same objects.

However, the detection error inevitably appears in the practical application. As a result, the central points of the same person detected from multiple views usually can-not satisfy the homography constraint. To avoid the influence of the detection error, we need to redefine the constraint relation between p 1 and p 2 . Specifically, let circle ( p , r ) denote all the points in the area of a circle. The center of the circle is p , and the radius of the circle is r . We can define the constraint as
If two points from two cameras respectively satisfy the constraint (4), we can conclude that the corresponding people are the same object. Thus we can effectively avoid the influence of the detection error, and can count the number of people through multiple cameras. Because our multiview fusion approach is based on the single-view detection method which are treated as the humans in our people counting system. As a result, the peo-ple counting result will be affected. To reduce the interference of the false positives, we can increase the number of cameras to ensure that each person in the region can be visible (or partially visible) for at least two cameras (see Section 6.3). Only those objects satisfying the homography constraint can be added into our people counting system. For two false positives produced by different cameras respectively, the proba-bility satisfying the constraint (4) is too slow to be considered. Thus, the false positives produced by each camera will not be added into our people counting system. Although our multiview fusion approach can effectively handle the occlusion and lack of visibility between people in crowded scenes, it is unnecessary to scan the whole image of each video frame for moving people counting. For most scenes we only need to count the number of people moving in/out the camera view ( X  X order control X ) if we first know the total number of people in the first frame. In this way, the search space for people detection can be reduced greatly. Thus, the speed and accuracy of people counting can be improved significantly.

As illustrated in Figure 7, the red rectangle denotes the outer boundary of the de-tection region, and the blue rectangle denotes the internal boundary of the detection region. We first count the people inside the blue rectangle for the first frame. For the rest of the frames, we only need to count the people between the red rectangle and the blue rectangle. For the detected people, we track them in each view separately. If peo-ple being tracked move outside of the red rectangle, the counting system will subtract the amount of corresponding people. If people being traced enter into the blue rect-angle, the system will stop tracking the corresponding people. Note that to facilitate the people counting for the rest of the frames, we do not count the people in the whole image of the first frame. Certainly, the counting result of the first frame is inaccurate if there are people between the red rectangle and the blue rectangle. However, our border control method can count the number of people accurately in the rest of the frames.
 We track the detected people by using a particle filter [Arulampalam et al. 2002]. The 2-dimensional normal distribution is used to generate the initial particle set. The number of particles of each object is twenty. The likelihood of each particle is calculated according to its cascade PMHL feature. The rest steps of tracking are described in Zeng and Ma [2010]. We now evaluate several aspects of our people counting system on different publicly available datasets. First, we evaluate the feature extraction method used in our sys-tem on the INRIA single-view datasets [Dalal and Triggs 2005]. Second, we show the computational cost of our fast detection method, and evaluate the performance of the improved MI-SVM training method on the INRIA dataset. Third, we compare the people counting performance between our fast detection approach and the previous single-view counting method on the PETS dataset [Ferryman and Shahrokni 2009]. Finally, we show the people counting performance of our multiview occlusion handling approach on two multiple-cameras datasets, including the EPFL multicamera dataset [Fleuret et al. 2008] and the PETS dataset. In this section, we first present the need for dimensionality reduction by using PCA. Then, we compare our PMHL detector with state-of-art algorithms on the INRIA single-view datasets. 7.1.1. Evaluation on Dimensionality Reduction. We evaluate the effect of PCA on the mul-tilevel HOG-LBP features. We use the INRIA person dataset [Dalal and Triggs 2005] as our test set. The INRIA person dataset contains 2416 96  X  160 positive samples and 1218 person-free images from a single view for training. We change the positive samples to the standard size, that is, 64  X  128 pixels. The 10000 negative examples are sampled randomly from the 1218 person-free images. This dataset also contains 1415 various size images for testing. Each of these images includes at least one human and is annotated. All of these images have large variations in illumination, clothing, and background.

To quantify the detection performance, we use Detection Error Trade off (DET) curves on log-log scales, which is the evaluation metrics used in Doll  X  ar et al. [2009]. The y-axis corresponds to the miss rate (1-recall), and x-axis corresponds to False Pos-itives Per Image (FPPI). FPPI is defined as Lower miss rate means better detection performance on the same FPPI.
 We first evaluate the performance of the multilevel HOG-LBP detectors on different PCA subspaces. We use a 8-fold cross-validation on the training dataset provided by the INRIA person dataset. Figure 8(a) shows that the performance of the PMHL detec-tor increases with the increase of the dimensionality of the PCA subspace. However, the performance of our detector drops when the number of dimensionality is increased beyond 180. Therefore, we use the PCA-based multilevel HOG-LBP with a dimension-ality of 180 as our final human detector.
 We then evaluate the performance of the multilevel HOG-LBP features without PCA to examine the effect of using PCA. Figure 8(b) shows that the multilevel HOG-LBP features without PCA reduce performance by 5% at 1 FPPI compared with the PMHL detector. Figure 8(c) illustrates the training dataset projected onto the first two dimensions for PCA, and shows that the class separation can be achieved better by projecting the multilevel HOG-LBP feature onto the low-dimensional space with PCA. 7.1.2. Evaluation of the PMHL Detector on INRIA Dataset. We use the linear SVM [Fan et al. 2008] (c = 1) to train and classify on the INRIA dataset. All 2416 positive samples and 6000 negative samples, sampled randomly from the 1218 person-free images, are used as the training set. As in Dalal and Triggs [2005] we retrain our model to improve the performance. Specifically, once the first model is obtained, we use it to search the false positives ( X  X ard examples X ) exhaustively from the 1218 negative training photos. The hard examples are added into the negative set to retrain and then obtain the final detector.

We reimplement the methods of HOG-LBP with occlusion handling [Wang et al. 2009] and HOG with linear SVM training Dalal and Triggs [2005] achieve similar results as these two algorithms. Our PMHL detector achieves 91% detection rate at 1 FPPI on the INRIA dataset (Figure 8(b)). Compared with Wang et al. [2009] and Dalal and Triggs [2005], we improve the performance by 5% and 13% respectively. Note that the HOG-LBP algorithm we compare in this article contains the process of occlusion handling, which is described in Wang et al. [2009]. For a 320  X  240 image, the detection time of the PMHL detector is about 3 seconds by using a dense scan (12,800 windows). In order to speed up the PMHL method while maintaining a similar detection accuracy, we evaluate our two-stage boosting approach on two public datasets. 7.2.1. Mi-SVM Training. We first evaluate the performance of our mi-SVM training method on the PMH and PML features. We choose 2416 positive samples and 6000 negative samples from the INRIA dataset as the initial training set. The set of positive bags consists of 300 full images (each containing at least one human). We thus obtain the PMH and PML detectors by using Algorithm 1. Figure 11 shows the performance comparison between the linear SVM and our mi-SVM training on the PMH and PML features, respectively. We achieve 95% detection rate at 3 FPPI for the PMH feature (Figure 9(a)) and 95.5% detection rate at 3 FPPI for the PML feature (Figure 9(b)). Both of these results show that our mi-SVM training approach can achieve more recall than the linear SVM. 7.2.2. Two-Stage Boosting. The high recalls of mi-SVM trained PMH and PML de-tectors enable us to use the two-stage cascade-of-rejectors structure to improve the speed of the PMHL algorithm. Table I presents a comparison of the detection time be-tween our approach and the PMHL algorithm in both sparse scan (800 windows) and dense scan (12,800 windows). Our cascade PMHL approach is 20 times faster than the PMHL algorithm, and results in a near real-time detection speed (2.5 GHz dual core CPU and 2GB RAM). Furthermore, our cascade PMHL approach maintains a similar accuracy to the PMHL algorithm on three datasets on the INRIA dataset (Figure 9(c)).
We also use our cascade-of-rejectors structure to speed up the multilevel LBP-HOG with PCA (PMLH) feature. But it cannot improve the detection speed greatly (see Table I). This is because that the computational speed of LBP is slower than that of HOG optimized with the integral histogram [Porikli 2005]. So the PMHL cascade-of-rejectors structure is a feasible way to real-time human detection.
 Some qualitative detection results by our detector on the two datasets are shown in Figure 10. Most previous people counting algorithms [Chan et al. 2008; Kilambi et al. 2008; Rabaud and Belongie 2006] can only detect moving people from a single camera. To compare with these approaches under the same conditions, we employ our two-stage boosting method to count the moving people from a single camera. If our single view method can outperform previous people counting algorithms, it is reasonable that our multiview method can perform much better than these methods. The compari-son is implemented on the PETS dataset. This dataset presents videos recorded at Whiteknights Campus, University of Reading, UK. The scenarios are filmed from mul-tiple cameras, and the multisensor sequences contain different crowd activities and up to approximately forty actors. This dataset also provides some single-view video sequences for people counting.

We choose sequences S1.L1.13-57 and S1.L1.13-59 (Figure 11) on the PETS dataset as our test sequences. Both of these two sequences contain more than twenty moving people. As illustrated in Figure 11, all of the people in these two sequences are moving. Thus we can use background subtraction to extract the foreground regions. By using background subtraction, the search space for people detection can be reduced greatly. Therefore, the speed and accuracy of people counting can be improved significantly. After extracting the foreground regions, our system then uses the cascade PMHL (see Sections 3 and 4) detector to detect the people in each of the foreground regions.
We compare our approach with the Privacy-Preserving Crowd Counting (PPCC) method [Chan et al. 2008]. The PPCC method is applied Gaussian process regres-sion to count the number of moving people, without using people detection or tracking. Figure 12 shows that our PMHL detector with background subtraction can track the ground truth better than the PPCC method on these two video sequences. We evaluate our multiview visual fusion with border control approach on two multiple-cameras datasets, including the PETS dataset [Ferryman and Shahrokni 2009] and EPFL dataset [Fleuret et al. 2008]. To test the advantage of detecting based on multiple cameras, we compare our multiview approach with our single-view method on these two datasets. We also compare our multiview method with a state-of-the-art multiview detection method on the PETS dataset. 7.4.1. Evaluation on the PETS Dataset. As described in Section 7.3, the scenarios on the PETS dataset are filmed from multiple cameras, and the multisensor sequences con-tain different crowd activities and up to approximately forty actors. The scenario we choose is the S2-L1, which is recorded by using views 1, 2, 5, 6, 7, 8. We choose se-quences recorded by views 5, 6, 7, and 8 as our test sets because these four cameras are placed evenly around the same scene, with the vertical viewing angle of each camera rotated at 90  X  relative to its neighbor, and thus enabling that people occluded heavily from one view can be visible from another view at the same time. So we can judge which objects detected from these four views at the same time are the same objects by using the homography constraint, and then accurately count the number of people in the field of view.

All the sequence frames from four views are manually annotated. If the overlap ratio between the annotated box and the detected box is larger than some threshold  X  , the detected box is treated as correct box. As in Ge and Collins [2010], we vary this threshold and compute the recall/precision metrics at each threshold.

We use our cascade PMHL detector trained on the INRIA dataset to detect the people from each view on the terrace sequences. Then, we use our multiview occlusion handling approach to fuse the detecting results from each single-view, and thus handle the heavy occlusion and lack of visibility in cluttered scenes. Figure 13(a) shows the performance comparison between our multiview occlusion handling approach and our single view detection approach. Compared with the detecting results from each single view, our multiview occlusion approach improves the recall performance greatly.
We then compare our approach with the multiview sampler method [Ge and Collins 2010], which used a sampling-based detection strategy to localize people in 3D images. Figure 13(b) shows that our proposed method is comparable to the multiview sam-pler method at all overlap threshold levels. It is worth mentioning that the multiview sampler method used background subtraction to extract the moving people, while our method does not. As far as we know, our multiview counting system is the first peo-ple counting system for counting both moving and still people from multiple cameras. Figure 14 shows some qualitative detection results from four views by using our fast cascade PMHL detector, and the people counting results by using our multiview fusion with border control approach on the PETS multicamera pedestrian dataset. 7.4.2. Evaluation on the EPFL Dataset. The EPFL multicamera pedestrian dataset pro-vides four multicamera sequences recorded at different scenes, including laboratory sequences, campus sequences, terrace sequences, and passageway sequences. All of the sequences are filmed from several synchronized video streams recorded at the same area under different angles. All cameras are mounted about two meters from the ground.

We choose the terrace sequences on this dataset as our test set. The terrace se-quences are filmed outside a building on a terrace. The terrace sequences are recorded by using four DV cameras with overlapping fields of view for around 3 1/2 minutes at 25 frames per second, resulting in over 5000 frames (360  X  288 pixels per frame). Com-pared with the other three sequences on this dataset, the terrace sequences contain more pedestrians and more heavy occlusion from one view.

In order to test the generalization capability of our fast cascade PMHL detector, we do not retrain this detector on the EPFL training dataset. Compared with the detect-ing results from each single view, our multiview occlusion handling approach improves the recall and precision significantly at all overlap threshold levels (Figure 15(a)).
We then apply our multiview occlusion handling approach to counting the number of people for 2000 frames of the sequence. We set the value of the r in formula (4) to 15 pixels through our empirical analysis. Figure 15(b) illustrates the counting per-formance comparison between our multiview occlusion handling with border control approach and the ground truth. Our multiview occlusion handling approach with bor-der control tracks the ground truth well for over 2000 frames. Moreover, our people counting system can run in nearly real time (10 frames/second with 2.5 GHz dual core CPU and 2GB RAM).

Our system reaches over 90% in accuracy for people counting, thus achieving con-vincing results on this dataset. Figure 16 shows some qualitative detection results from four single views by using our fast cascade PMHL detector, and the people count-ing results by using our multiview approach on the EPFL multicamera pedestrian dataset. To demonstrate the effectiveness of our approach, we have constructed a people count-ing system prototype for surveillance by using two existing surveillance cameras. As illustrated in Figure 17, the surveillance area is a square in our campus, and the size of surveillance area is about 50  X  50 meters. The two cameras are mounted about 10 meters high from the ground, viewing the square at about 50  X  below the horizon. Horizontally, one of the cameras is placed in the front of another. Our system attempts to obtain the real-time people flow information in the square by using the two surveil-lance cameras.

The video sequences are collected by the two cameras respectively at the same time (25 frames per second, 320  X  240 pixels per frame). Sample frames of the two video sequences are shown in Figure 17. We first use the cascade PMHL (see Sections 3 and 4) detector to detect the people in each of the foreground regions. Figure 18 shows that our cascade PMHL detector can obtain satisfactory recall on the two cameras. Then, we use our multiview occlusion handling approach to integrate the detecting results from each single view. Figure 18 shows that the performance of our multiview approach is much better than that of view 1 and view 2.

Thus, we can apply our multiview occlusion handling with border control approach to obtaining the real-time people flow information in the square. As illustrated in Figure 17, the red rectangle denotes the outer boundary of the detection region, and the blue rectangle denotes the internal boundary of the detection region. We first count the people inside the blue rectangle for the first frame by using our multiview occlu-sion handling approach. For the rest of the frames, we only need to count the people between the red rectangle and the blue rectangle (border control). For the detected people, we track them in each view separately. If people being tracked move outside of the red rectangle, the counting system will subtract the amount of corresponding peo-ple. If people being traced enter into the blue rectangle, the system will stop tracking the corresponding people.

Figure 19 shows the prototype of our multiview people counting system. Our system works stably, and tracks the ground truth satisfactorily for about 5 hours of video sequences (except the first frame). Figure 20 shows the people counting result on 5000 frames of the video sequences. The average difference between the counting estimate and the ground truth is 1.05. As the video sequences contain up to 35 moving people, the counting result on this real surveillance scene is acceptable. Furthermore, our people counting system can run in real time (10 frames/second with 2.5 GHz dual core CPU and 2GB RAM) on this scene. These results demonstrate that our system is robust and fast enough for counting the real-time people flow information in a real surveillance scene. Currently, there are a lot of available people counting systems for retail stores, public transportation, museums, etc. These systems use many different technologies which can be classified into three classes: infrared beams, thermal imaging, and computer vision. In this section, we list well-known people counting systems and give a compar-ison with our multiview people counting system (Table II). A typical infrared-beams-based system is ACOREL 1 which uses mixed infrared beams as the sensor. The sensors are usually mounted vertically inside any vehicle, or in retail outlets. The ACOREL system has been successfully employed in more than 15,000 locations world wide. Typical thermal imaging counting systems, such as SENSOURCE 2 and INFODEV 3 , use a thermal imaging sensor to detect the emitted heat from people. The sensors are usually mounted overhead, and the thermal systems are best suited for entrances where multiple people are moving in two directions (in and out) simultaneously. Typical single-view vision-based systems are Traffic Central 4 and TrueView People Counter 5 which have been embedded directly into IP cameras. People counting is done directly in the camera without tranferring data to a computer to process. The cameras are usually mounted straight above the point where people should be counted, and are also best suited for entrances where multiple people are moving in two directions simultaneously. As described in Section 9.3, typical vision systems use a single camera to count people moving in (or out) an entrance. As far as we know, there is no system which uses multiple cameras to count the number of people. Table II gives a comparison between our multiview people counting system and well-known people counting systems. We have presented a system that can estimate the number of people (still or moving) with heavy occlusion. The basic idea of our system is to use multiple cameras to in-tegrate the detecting information from each single camera through the homography constraint. The multiview visual fusion method can handle the problem of heavy oc-clusion and lack of visibility. Thus, we design an practical system for fast and reliable people (still or moving) counting by using multiple cameras.

Another novel aspect of our system is the proposed people detector based on the cascade PMHL feature, which can detect people from a single camera reliably and quickly. The PMHL detector combines the multilevel HOG and multilevel LBP as the feature set and uses PCA to discard the redundant and noisy information of the feature set. Compared with the state-of-the-art algorithms, our PMHL detector can handle the large variations in peoples X  appearances and poses more effectively. To accelerate the detection speed of the PMHL detector, we demonstrate a near real-time human detection framework that improves the speed to 10 times faster than the PMHL detector without loss of precision. This is achieved by the PMHL cascade-of-rejectors structure. The key of our approach is the process of the modified mi-SVM training, which can achieve high recall at each stage of the cascade.

By integrating the detecting information from each single camera, our people count-ing system achieves convincing results on two public datasets. To improve the speed and accuracy of moving people counting, we combine our multiview fusion detection method with border control to count the number of people moving in/out the camera view (border control). To demonstrate the effectiveness of our approach, we have con-structed a people counting system prototype for a real surveillance scene. Our system is robust and fast enough for counting the real-time people flow information over long time periods in a real surveillance scene. In the future, we intend to extend our work to other applications, such as event recognition and abnormality detection.
