 Learning with examples having multiple labels is an importa nt problem in machine learning and data mining. Such problems are encountered in a variety of ap plication domains. For example, in come from a discrete set, the problem is termed as multi-labe l classification. The aforementioned share a common source of input. An important characteristic s of these problems is that the labels an approach ignores the label correlations and leads to sub-optimal performance [20]. In this paper, we show how Canonical Correlation Analysis (C CA) [11] can be used to exploit label relatedness, learning multiple prediction problems simul taneously. CCA is a useful technique for modeling dependencies between two (or more) sets of variabl es. One important application of CCA a low-dimensional space directed by label information Y . This makes CCA an ideal candidate for extracting useful predictive features from data in the cont ext of multi-label prediction problems. which means that it cannot deal with missing data, and preclu des a Bayesian treatment which can standard CCA formulation.
 model can automatically select the number of correlation co mponents, and effectively capture the sparsity underlying the projections. Our framework is base d on the Indian Buffet Process [9], a ality reduction case, can incorporate additional unlabeled data one may have access to, making our CCA algorithm work in a semi-supervised setting. Thus, apar t from being a general, nonparamet-ric, fully Bayesian solution to the CCA problem, our framewo rk can be readily applied for learning related tasks.
 This paper is organized as follows. Section 2 introduces the CCA problem and its recently proposed work in section 6 drawing connections of the proposed method with previously proposed ones for this problem. . Canonical correlation analysis (CCA) is a useful technique for modeling the relationships among a set of variables. CCA computes a low-dimensional shared embedding of a set of variables such that the correlations among the variables is maximized in the emb edded space.
 u coefficient between the two variables in the embedded space i s given by Since the correlation is not affected by rescaling of the pro jections u constrained optimization problem. value problem: where  X  denotes the covariance matrix of size D  X  D (where D = D data samples X = [ x 2.1 Probabilistic CCA Bach and Jordan [3] gave a probabilistic interpretation of C CA by posing it as a latent variable model. To see this, let x and y be two random vectors of size D following latent variable model Equivalently, we can also write the above as where  X  = [  X  y .
 Bach and Jordan [3] showed that, given the maximum likelihoo d solution for the model parameters, CCA finds, thereby establishing the equivalence between the above probabilistic model and CCA. which suggested a maximum likelihood approach for paramete r estimation. However, it still as-sumes an apriori fixed number of canonical correlation components. In additi on, another important issue is the sparsity of the underlying projection matrix wh ich is usually ignored. crucial issue in the CCA model is choosing the number of canon ical correlation components which is set to a fixed value in classical CCA (and even in the probabi listic extensions of CCA). In the Bayesian formulation of CCA, one can use the Automatic Relev ance Determination (ARD) prior [5] on the projection matrix W that gives a way to select this number. However, it would be mo re appropriate to have a principled way to automatically figure out this number based on the data. We propose a nonparametric Bayesian model that selects the n umber of canonical correlation com-ponents automatically. More specifically, we use the Indian Buffet Process [9] (IBP) as a nonpara-metric prior on the projection matrix W . The IBP prior allows W to have an unbounded number of columns which gives a way to automatically determine the d imensionality K of the latent space associated with Z . 3.1 The Indian Buffet Process tivated by the need to model the latent feature structure of a given set of observations. The IBP has been a model of choice in variety of non-parametric Bayes ian approaches, such as for factorial and several others [9].
 features. Given an N  X  D matrix X of N observations having D features each, we can consider a decomposition of the form X = ZA + E where Z is an N  X  K binary feature-assignment matrix describing which features are present in each observation. Z vation n , and is otherwise 0. A is a K  X  D matrix of feature scores, and the matrix E consists of observation specific noise. A crucial issue in such models is the choosing the number K of latent it can have an unbounded number of columns and thus can be a sui table prior in problems dealing with such structures.
 The IBP derivation starts by defining a finite model for K many columns of a N  X  K binary matrix. Here m can be best understood by a culinary analogy of customers com ing to an Indian restaurant and se-each incoming customer n selects an existing dish k with a probability m how many previous customers chose that particular dish. The customer n then goes on further to additionally select P oisson (  X /N ) new dishes. This process generates a binary matrix Z with rows representing customer and columns representing dishes. Ma ny real world datasets have a sparseness Figure 1: The graphical model depicts the fully supervised c ase when all variables X and Y are observed. The semisupervised case can have X and/or Y consis ting of missing values as well. The graphical model structure remains the same property which means that each observation depends only on a subset of all the K latent features. This means that the binary matrix Z is expected to be reasonably sparse for many datasets. This discovering the number of latent features. 3.2 The Infinite CCA Model In our proposed framework, the matrix W consisting of canonical correlation vectors is modeled binary matrices, we represent the ( D 1 + D 2)  X  K matrix W as ( B  X  V ) , where B = [ B is a ( D  X  denotes their element-wise (Hadamard) product. We place an IBP prior on B that automatically determines K , and a Gaussian prior on V . Note that B and V have the same number of columns. Under this model, two random vectors x and y can be modeled as x = ( B y noise.
 In the full model, X = [ x each, and Y = [ y is the generative story for our basic model: where  X  is a diagonal matrix of size D  X  D where D = ( D having an inverse-Gamma prior..
 Since our model is probabilistic, it can also deal the proble m when X or Y have missing entries. of inputs and Y associated responses) when the labels for some of the inputs are unknown, making placing the IBP prior on the projection matrix W (via the binary matrix B ) also helps in capturing the sparsity in W (see results section for evidence). 3.3 Inference posterior distributions over them. We use Gibbs sampling wi th a few Metropolis-Hastings steps to do inference in this model. In what follows, D denotes the data [ X ; Y ] , B = [ B Sampling B: Sampling the binary IBP matrix B consists of sampling existing dishes, proposing new sampling existing dishes, an entry in B is set as 1 according to p ( B For sampling new dishes, we use an M-H step where we simultane ously propose  X  = given parameters  X  . We propose V new from its prior (Gaussian) but, for faster mixing, we propose Z new from its posterior.
 Sampling V: We sample the real-valued matrix V from its posterior p ( V N or ( V We define D  X  gamma prior and posterior also has the same form. Note that th e number of columns in V is the same as number of columns in the IBP matrix B .
 W T ( WW T +  X  )  X  1 D and  X  = I  X  W T ( WW T +  X  )  X  1 W , where W = B  X  V .
 Note that, in our sampling scheme, we considered the matrice s B IBP matrix B , and sampled them together using a single IBP draw. However, one could also sample them separately as two separate IBP matrices for B for sampling B could result in different number of nonzero columns in B could sample B introduce extra dummy columns ( | K sampler would be max { K real-valued matrices V Having set up the framework for infinite CCA, we now describe i ts applicability for the problem can individually learn a separate model for each task, doing this would ignore the label correla-final goal is prediction.
 More concretely, let X = [ x [ y 1 , . . . , y N ] an M  X  1 vector of responses for input x doing supervised dimensionality reduction for the inputs X . Note that the generalized eigenvalue problem posed in such a supervised setting of CCA consists of cross-covariance matrix  X  label covariance matrix  X  correlations and the label correlations. Such a subspace th erefore is expected to consist of much PCA that completely ignores the label information, or appro aches like Linear Discriminant Analysis (LDA) that do take into account label information but ignore label correlations. Multitask learning using the infinite CCA model can be done in two settings: supervised and semi-supervised depending on whether or not the inputs of test dat a are involved in learning the shared subspace Z . 4.1 Fully supervised setting dimensional, we need to either separately project it down on to the K dimensional subspace and do predictions in this subspace, or  X  X nflate X  each task paramet er back to D dimensions by applying the projection matrix W requires using the fact that P ( Z | X  X  can inflate each learned task parameter back to D dimensions by applying the projection matrix W We choose the second option for the experiments. We call this fully supervised setting as model-1. 4.2 A Semi-supervised setting In the semi-supervised setting, we combine training data an d test data (with unknown labels) as X then applied on the pair ( X , Y ) and the parts of Y consisting of Y to be imputed. With this model, we get the embeddings also for the test data and thus training and testing both take place in the K dimensional subspace, unlike model-1 in which training is d one in K dimensional subspace and prediction are made in the origina l D dimensional subspace. We call this semi-supervised setting as model-2. our results with the infinite CCA as a stand alone algorithm fo r CCA by using it on a synthetic our experiments on applying the infinite CCA model to the prob lem of multitask learning on two real world datasets. 5.1 Infinite CCA results on synthetic data In the first experiment, we demonstrate the effectiveness of our proposed infinite CCA model in discovering the correct number of canonical correlation co mponents, and in capturing the sparsity pattern underlying the projection matrix. For this, we gene rated two datasets of dimensions 25 and dataset had 4 correlation components with a 63% sparsity in t he true projection matrix. We then discovered by classical CCA, we found that it discovered 8 co mponents having significant correla-tions, whereas our model correctly discovered exactly 4 com ponents in the first place (we extract the MAP samples for W and Z output by our Gibbs sampler). Thus on this small dataset, sta ndard ting problem of classical CCA was also observed in [15] when c omparing Bayesian versus classical CCA). Furthermore, as expected, the projection matrix infe rred by the classical CCA had no exact sparsity was only about 25%. On the other hand, the projectio n matrix inferred by the infinite CCA demonstrating its effectiveness in also capturing the spar sity patterns. Model Yeast Scene Model-1 0.5842 0.3327 0.4402 0.5232 0.7533 0.3630 0.3732 0.6517 Model-2 0.6156 0.3463 0.4954 0.5386 0.7664 0.3742 0.3825 0.6686 Model-1 and Model-2 scores are averaged over 10 runs with dif ferent initializations. 5.2 Infinite CCA applied to multi-label prediction Scene) from the UCI repository. The Yeast dataset consists o f 1500 training and 917 test examples, each having 103 features. The number of labels (or tasks) per example is 14. The Scene dataset consists of 1211 training and 1196 test examples, each havin g 294 features. The number of labels per example for this dataset is 6. We compare the following mo dels for our experiments. The performance metrics used are overall accuracy, F1-Macr o, F1-Micro, and AUC (Area Under ROC Curve). For PCA and CCA, we chose K that gives the best performance, whereas this param-eter was learned automatically for both of our proposed mode ls. The results are shown in Table-1. As we can see, both the proposed models do better than the othe r baselines. Of the two proposed model, we see that model-2 does better in most cases suggesti ng that it is useful to incorporate sampling.
 (yeast has 14, scene has 6). We expect the performance improv ements to be even more significant when the number of (related) tasks is high. A number of approaches have been proposed in the recent past f or the problem of supervised dimen-ing dependence maximization (MDDM) [26]. None of these, how ever, deal with the case when the probabilistic PCA [25] that extends probabilistic PCA to th e setting when we also have access to labels. However, it assumes a fixed number of components and d oes not take into account sparsity of the projections. dimensional representation z  X  R K of inputs x  X  R D ( K  X  D ) for predicting multivariate outputs y  X  R M . An important notion in DRR is that of sufficient dimensional ity reduction (SDR) [10, 8] with X and Y being conditionally independent given Z .
 Among the DRR based approaches to dimensionality reduction for real-valued multilabel data, Co-variance Operator Inverse Regression (COIR) exploits the c ovariance structures of both the inputs and outputs [14]. Please see [14] for more details on the conn ection between COIR and CCA. Be-sides the DRR based approaches, the problem of extracting us eful features from data, particularly with the goal of making predictions, has also been considere d in other settings. The information bottleneck method aims to obtain a compressed representati on T of X that can account for Y . IB work [13], a joint learning framework is proposed which perf orms dimensionality reduction and multi-label classification simultaneously.
 approach [17], and CCA as a sparse solution to the generalize d eigenvalue problem [18] which is a sparse solution. Another recent solution is based on a dire ct greedy approach which bounds the correlation at each stage [22].
 The probabilistic approaches to CCA include the works of [15 ] and [1], both of which use an au-tomatic relevance determination (ARD) prior [5] to determi ne the number of relevant components, posed here is a more principled to determine the number of com ponents.
 finite generalization of both an unsupervised problem (spar se CCA), and (semi)supervised problem (dimensionality reduction using CCA with full or partial la bel information), with the latter being especially relevant for multitask learning in the presence of multiple labels.
 pending on what notion of task relatedness is assumed. Some o f the examples include tasks gener-that captures the task relatedness. We have presented a nonparametric Bayesian model for the Can onical Correlation Analysis problem to discover the dependencies between a set of variables. In p articular, our model does not assume a fixed number of correlation components and this number is de termined automatically based only information, can be used to automatically extract useful pr edictive features from the data. Acknowledgements We thank the anonymous reviewers for helpful comments. This work was partially supported by NSF grant IIS-0712764.
