 Chris Buckley Abstract Observations from a unique investigation of failure analysis of Information Retrieval research engines held in 2003 are presented. The Reliable Information Access Workshop invited seven leading IR research groups to supply both their systems and their experts to an effort to analyze why their systems fail on some topics and whether the failures are due to system flaws, approach flaws, or the topic itself. There were surprising results from this cross-system failure analysis. One is that despite systems retrieving very different doc-uments, the major cause of failure for any particular topic was almost always the same across all systems. Another is that relationships between aspects of a topic are not especially important for state-of-the-art systems; the systems are failing at a much more basic level where the top-retrieved documents are not reflecting some aspect at all. The investigatory framework and the lessons learned can serve as a model for needed future research in this area. Keywords Information retrieval evaluation Failure analysis Failure categorization Comparative system analysis 1 Introduction Experimental environments such as TREC show that retrieval results vary widely mation Retrieval (IR) systems and for any of the more advanced implementations using, for example, query expansion. Some retrieval approaches work well on one topic but poorly on a second, while other approaches may work poorly on the first topic, but succeed on the second. If we could determine in advance which approach would work well, then a dual approach could strongly improve performance. Unfortunately, no one knows how to choose good approaches on a per topic basis.

The goal of the Reliable Information Access (RIA) Workshop during the summer of 2003 was to understand the contributions of both system variability factors and topic variability factors to overall retrieval variability. The workshop brought together seven different top research IR systems and set them to common tasks. Comparative analysis of these different systems enables system variability factors to be isolated in a way that never before has been possible.

There were two main tracks to the RIA investigation of system and topic variability, one top down and one bottom up. This article describes the bottom up track: a massive comparative failure analysis. Each of six systems contributed one representative run. Then for each designated topic, a detailed manual analysis of each run with its retrieved doc-uments was done. The analysis goal was to discover why systems fail on each topic. Are failures due to system dependent problems such as query expansion weaknesses or system algorithm problems, or are the problems more inherent to the topic? For each topic, what would be needed to improve performance for each system? How can this be (theoretically) predicted by the system?
As may be gathered from the list of questions, the track was much more of an investi-gation than a controlled experiment. There were some general expectations and hypotheses before the workshop, but no attempt was made to rigorously prove or disprove them. Instead the effort was to have experts in the field observe what is actually happening in practice with IR research systems, and suggest what can be done about it. These observations have yielded hypotheses for individual topics which may be tested in later experiments. 2 Topic failure analysis investigation The RIA topic failure analysis investigation was an attempt to discover why current research IR systems fail and propose concrete areas of concentrated research to improve effectiveness performance. IR systems improved tremendously at the beginning of the 1990s as the TREC corpuses offered new large collections upon which to base statistical information retrieval. However, the improvement rate in the core IR retrieval algorithms has slowed down in the past few years.

One major part of the failure to improve is the perception that any individual group system-dependent failures from the topic-dependent failures. Some topics, or even aspects of topics, are hard for all systems. Other topics may be easier for some systems to do well on than other systems, but nobody currently understands why.

This investigation looked at IR system performance across a set of different research IR engines on a topic by topic basis. By comparing how each system succeeds or fails on any individual topic, strategies for successfully dealing with that topic can be devised. By considering enough individual topics, overall strategies for retrieval can be devised, along with identification of areas of research that need to be investigated before improvement can be expected.

Before the start of the workshop, the following observations had been made  X  Good research systems all have about the same average performance as measured by  X  Systems perform differently across topics.  X  On any given topic, performance varies across systems.  X  Systems retrieve quite different documents even if performance on a topic is about the
These facts suggested the following general hypotheses 1. Systems are failing on individual topics in different ways. 2. Systems are emphasizing different aspects of each topic. 3. Systems are looking at different relationships between aspects in a topic. 4. Systems will need to look at relationships between aspects to improve and these 2.1 Failure analysis standard runs The collection used for the standard runs were the 150 topics of TREC 6, TREC 7, and TREC 8, run on disks 4 and 5 of the TREC document distribution, not including the Congressional Record subcollection since that was used only for TREC 6. There were a total of 528,155 documents, averaging a bit over 3600 characters each.

Before the RIA workshop began, six of the seven participating groups each submitted a standard retrieval run that in some sense represented their group X  X  approach to IR. There were no restrictions on what was done in the run as long as it was completely automatic. The resulting set of runs were widely differing:  X  CMU: Lemur software implementing the KL-divergence based unigram language  X  UMASS: Lemur software implementing query-likelihood unigram language modeling  X  Waterloo: MultiText system with blind feedback using passage retrieval and hotspot  X  Sabir: SMART Version 14 vector space system with Lnu-ltu weighting. Used blind  X  Clairvoyance: CLARIT Java system based on full CLARIT indexing of sub- X  City: Okapi probabilistic system using the Okapi BM25 algorithm. There was no query
During week 1 of the workshop, it was discovered that the standard runs done before the workshop were not quite as standard as desired. Some groups had potentially indexed all fields of the documents while others had abided by the TREC restrictions of the appropriate years and not indexed the manually added fields (for example, the  X  X  X UB-JECT X  X  section of the LA TIMES). In addition, groups threw out different portions of some of the highly stylized TREC description topics ( X  X  A relevant document must identify ...  X  X ). To make results more comparable, it was decided to standardize a set of patterns in topics that all systems would discard, and to not include the manual field of the LA TIMES documents. All systems reran their standard runs during week 1, and the resulting runs were then unchanged for the rest of the workshop. 2.2 Topic failure analysis process allocated for the individual and group analysis. There were 28 people from 12 organiza-tions participating in the six week workshop, including 15 senior IR researchers, 12 PhD students in fields relating to IR, and one entering college freshman providing Web and systems expertise. On any one day, there would be from 13 to 25 participants present at the workshop; all participants were expected to do the failure analysis. Thus the total failure analysis effort during the workshop was a bit in excess of 1000 person hours.

This was the first time this sort of group comparative failure analysis had ever been done, so substantial effort during the first week was spent developing the process of failure analysis; what we wanted to do given our goals and the available tools. The details of the process changed as the participants gained experience throughout the workshop but the overall process remained the same after the first week. 1. The topic (or pair of topics) for the day was determined, with a leader being assigned 2. Each participant was assigned one of the six standard runs to examine, either 3. Each participant or team spent from 60 to 90 min investigating how their assigned 4. All participants assigned to a topic discussed the topic for 20 X 30 min, in separate 5. The topic leader summarized the results of the discussion in a short report (a template
Initially one topic per day was analyzed. People worked as teams on pairs of systems, with one person of the team being familiar with each system. The two systems were compared against each other as well as individually. People were rotated each day with the goal of each participant having a chance to work with somebody from each other system. This worked well though it could not be done fully; for example, there was nobody from City physically present. In this way, each participant learned the important details of the other systems from an expert, as well as learning how the various failure analysis tools could be used.

Given the available tools, it was just as easy to compare one system against all the other systems instead of just one other paired system, so that was done by week 2 of the workshop. By the end of week 2, people were assigned systems as individuals instead of teams, and two topics could be done in parallel. As new people arrived during the course of the workshop, they would be assigned to work as a team with an experienced participant for 2 X 3 days, and then were assigned to work individually.

There were several conflicting goals to consider when coming up with a template for failure analysis. The whole idea of failure analysis is to apply human intellect and expe-rience to the question of what is happening with a given system X  X  retrieval of documents. Having a detailed template that would be required to be filled out would defeat this purpose, especially given the very wide range of experience of both systems and tools among the workshop participants. For example, the Analyst Workbench (AWB) of Clairvoyance was an excellent tool for grouping documents and investigating what was happening with those groups. But it was difficult to learn well enough to be helpful, and was comparatively time consuming, and so it was only used by three or four participants. The template was designed to accommodate the reporting of results from various types of failure analysis investigations rather than prescribing the failure analysis was to be done. Figure 1 shows this template, along with explanatory text and comments (which were not part of the template or explicitly given to the participants) in italics.

The individual templates appeared to work well. They focused the less experienced participants on problems they could address while allowing the more experienced partic-ipants freedom to report what they found. A major weakness of the template was that people tended to keep track of the raw data of their observations as they went along, which was good, but did not make explicit their conclusions from their observations, which was not good. It might have been nice if people had added more to their conclusions after the group discussions of the topic, but in general they did not.
 There was no template for the report by the topic leader on the topic as a whole initially. It was felt that the topics varied so much that a template would be overly constraining. However, the variability of the topic reports in the first few weeks prompted the development of a second template (see Fig. 2 ) by the end of week 3. For this template, the explanatory text was included in the template.

Again, there was the conflict between demanding enough detail in the template to provide good information common across topics and demanding so much detail that the vidual template, we opted for mostly general questions; though having a top-level very general template item would have been helpful.

There was debate in the workshop on several occasions as to whether the topics should be categorized as to major failure causes as they were being analyzed. It was decided that for future investigations, coming up with a set of failure categories would be useful and needed, but during this workshop we did not know enough about the types of expected failures until the end. The danger of prematurely defining categories is that people may be tempted to force a topic into a pre-defined category that it may not really fit. One of the results of this workshop is a list of failure categories. 2.3 Failure analysis topic choice Given the large time requirements for failure analysis (from 11 to 40 person-hours per topic), it was obvious that not all 150 topics could be examined. We had hoped we could do 50 topics and we actually finished 45 topics. Ideally, we should randomly choose topics that fit our desired criteria. In practice, we did not know enough about what criteria we wanted. Our first attempt failed badly. We ranked topics by variability of the ranked document results; topic 368 was the most variable. While retrieved documents and scores varied widely, all systems did fairly well on it: MAP varied from .38 to .76. However, during the failure analysis for this topic participants ended up spending their time judging how well each system matched the TREC assessor X  X  line between marginally relevant documents and barely non-relevant documents; a task impossible to do well, and not intellectually interesting. It was then decided to focus on topics where the systems in general scored below the overall MAP average. The overall criteria used were  X  Average MAP of systems at or below the overall MAP average of about .21.  X  Large variance between system scores.  X  Variance in general not due to the City Okapi run (different vocabulary since it used
In addition, we analyzed several topics (about 4 each) that  X  Performed differently than others in some other RIA experiment.  X  Had the basic form of a TREC Question Answering question.

Note that the analyzed topics cannot be said to be random selection of all the topics. We did not analyze most of the easy topics (we analyzed only two out of the top 56 topics, ranked by average MAP), since they had high MAP for most systems. We also did not analyze many of the topics that all systems did extremely poorly on (only four out of the bottom 23 average MAP), since there was generally very little variability between system scores on those topics. Since each topic took at least 11 person-hours to analyze, we concentrated on those topics for which there was evidence of some system-dependent effect and some evidence of system failure, and analyzed over half of those topics. 2.4 Tools for topic failure analysis There were a wide variety of tools used in failure analysis. They include  X  WUI: The Waterloo User Interface was the major tool used for examining retrieved  X  Beadplot: Freely available from NIST. Beadplot represents each retrieved document of  X  AWB: The Analyst Workbench from Clairvoyance is a very nice package allowing  X  smart_std: The SMART system itself offers the ability to look at documents and  X  smart_retro: An adaptation of the SMART DFO approach performed on the  X  Web interface: The web interface to the failure analysis topics was an enormously  X  Individual systems: Many participants used their own system as a tool for looking at 3 Topic failure analysis categorization As discussed previously, there was not any attempt to construct explicit categories of topic failures as the topics were being analyzed. However, during the final week of the workshop, after topic failure analysis had finished, Chris Buckley constructed categories which in his opinion represented the different sorts of failures seen as important during the summer. 1. General success X  X resent systems worked well 2. General technical failure (stemming, tokenization) 3. All systems emphasize one aspect; missing another required term 4. All systems emphasize one aspect; missing another aspect 5. Some systems emphasize one aspect; some another; need both 6. All systems emphasize one irrelevant aspect; missing point of topic 7. Need outside expansion of  X  X  X eneral X  X  term ( Europe for example) 8. Need QA query analysis and relationships 9. Systems missed some difficult aspect that would need human help 10. Need proximity relationship between two aspects The categories above are roughly sorted in order of increasing Natural Language Understanding (NLU) being needed to improve performance once it is understood a topic belongs in that category. Topics were placed in the least restrictive category (towards the top of the list) that would give substantial improvement if the problem could be addressed.
The assignment of topics to these categories does not address the problem of how difficult it is to automatically discover what category the topic belongs to. Thus it may be extremely difficult and require full NLU and world knowledge to distinguish those topics those categories, possibly by using more information than is available in just the topic, it should be able to attack the missing aspect problem of category 4 in a straightforward fashion, while the missing aspect of category 9 topics will still be very difficult to attack.
It should be noted that these categories and assignments of topics to categories are the results of one person X  X  efforts. If 100 experts were asked to do the same exercise and given the same information (the filled in topic templates and the resulting discussions), there would undoubtedly be 100 different definitions of categories, and assignment of topics to categories. Nonetheless, some conclusions can be reached from this particular categorization.

The first conclusion is that the root cause of poor performance on any one topic is likely to be the same for all systems. Except for the six topics of categories 1 and 5, all systems failed for the same reasons (modulo the rare individual system blunders). Beadplot and other tools show that the systems were retrieving different documents from each other in general, but all systems were missing the same aspect in the top documents.

The other major conclusion to be reached from these category assignments is that if a system can realize the problem associated with a given topic, then for well over half the topics studied (at least categories 1 through 5), current technology should be able to improve results significantly. This suggests it may be more important for research to discover what current techniques should be applied to which topics, than to come up with new techniques. 4 Lessons learned about failure analysis This was the first large-scale failure analysis to be done on multiple automatic retrieval systems. As such, the lessons learned about doing failure analysis may be as valuable in the future as the lessons learned from doing the analysis. 4.1 Examining documents is central Of course, looking at documents (or parts of documents) is critical to failure analysis, but the extent to which our time was dominated by documents was still surprising. That may be due in part to the relative maturity of the available tools to the task at hand. We had good tools to examine documents, but had less experience with effective use of our other analysis tools.

The Waterloo User Interface (WUI) was essential to most analyses. Since it was built on top of a database system, it allowed us to easily access exactly the documents we wanted, in both snippet and full text form, with desired features highlighted.

Our other tools occasionally gave valuable information, but were less generally useful than we expected. The Beadplot tool for visually observing document ranks works well on smaller collections, but there are too many documents too similar to each other to really gain much insight on larger collections. The Analyst Workbench (AWB) is a very general, flexible tool, but suffered from its flexibility in the workshop: we needed to first know a particular problem of a topic/ranking to investigate. It might have been more useful as a confirmation tool to support/deny the hypotheses we reached. The Web interface was a great time saver, collecting all the known information about a topic in one place. 4.2 Templates are useful Having templates for both the individual system analysis and the group discussions were beneficial. Both were developed during the workshop to reflect what we were actually doing.
 We held a mid-term review of the failure analysis process. David Evans and Paul Kantor observed several sessions of the group discussions, both with and without tem-plates. Their general conclusion was that the template discussions more completely summarized performance on a topic.

The templates we used should be expanded in the future. In particular, a categorization of the conclusions should be added to allow group input into the categorization. Our conclusions on individual topics were too  X  X  X ree-form X  X  to be able to form general con-clusions across topics. However, be aware that categories will change, depending on system and state-of-the-art. At the beginning of the workshop, we were expecting a dif-ferent categorization than we ended up with X  X ne more focused on types of relationships between topic terms. In the end, term relationships were not the cause of many failures. 4.3 Comparative failure analysis was beneficial When doing an individual system failure analysis, it was quite helpful to be able to compare that system to other systems. Indeed, at the start of the workshop we were comparing only pairs of systems, but we quickly expanded that to compare a system against all other systems. There was a major benefit in being able to focus on documents on which other systems either ranked consistently higher or lower.

Even though systems in general failed for the same reasons as other systems, they often failed to a greater or lesser extent. A system might not emphasize a secondary aspect of a topic enough, but still emphasize it more than other systems. That was easily detectable when looking at documents and their comparative rankings across systems. 4.4 Verification of failure analysis hypotheses is desirable We did very little testing of our various hypotheses for individual system failures; we had planned to do much more. Unfortunately, the individual retrieval system modifications that would enable systematic testing of some of the hypotheses were not finished until near the end of the workshop. For other experiments in the workshop, we added the ability to import weights and/or terms from outside each system. We had hoped to be able to piggyback hypothesis testing on these changes (e.g., increase the weight on terms that failure analysis said were underweighted, or manually expand a topic with synonyms), but were unable to. We both ran out of time, and ran into the obstacle that the systems that had been modified for experiments throughout the workshop were all running different vari-ations than they had been running when the failure analysis runs were frozen. Ideally in the future, system modifications to enable failure analysis hypothesis testing should be done before the failure analysis starts. Not only would that enhance confidence in the results, but those modifications would be a valuable failure analysis tool in their own right. 5 Conclusion Most of the incoming hypotheses stated in Sect. 2 turned out not to be true. Restating the hypotheses: 1. Systems are failing on individual topics in different ways . Despite the fact that systems 2. Systems are emphasizing different aspects of each topic . Again, while this happened 3. Systems are looking at different relationships between aspects in a topic . We saw very 4. Systems will need to look at relationships between aspects to improve and these
One interesting retrieval approach suggested by these results is to try to do retrieval ranking via automatic failure analysis. The presence or absence of terms in the top doc-uments of a retrieved set can be automatically determined. If an aspect is not being represented in those top documents, then it can be given an increased weight or can be expanded by synonyms. The query can be reformulated and re-run until the retrieved top documents reflect the important aspects of the topic. As collection size continues to increase, general collection idf becomes a very blunt tool for weighting terms. This reformulation of the query allows for much more precise relative weighting of topic aspects.

Overall, the type of semantic relationship between aspects is not yet the primary cause of failure. There should be a lot of improvement possible without understanding rela-tionships, though in the long-term, relationships will be necessary. Finally, understanding the semantics of the topic well enough to just identify the important aspects would seem to be crucial for many topics.
 References
