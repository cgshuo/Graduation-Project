 The problem of finding a low-rank approximation of a target matrix through matrix factorization (MF) attracted considerable attention recently since it can be used for various purposes such as multi-class classification [1], and multi-task learning [7, 29].
 Singular value decomposition (SVD) is a classical method for MF, which gives the optimal low-combined with an explicit low-rank constraint, which is non-convex. In contrast, the trace-norm MF have been extensively studied [20, 6, 12, 25].
 Bayesian approaches to MF have also been actively explored. A maximum a posteriori (MAP) also been applied to MF [13, 18]. The VB-based MF method (VBMF) was shown to perform well in experiments, and its theoretical properties have been investigated [15]. However, the optimization problem of VBMF is non-convex. In practice, the VBMF solution is optimal solution by the ICM algorithm, but many restarts would be necessary to find a good local optimum.
 mal solution of VBMF can be computed analytically by solving a quartic equation. This is highly advantageous over the standard ICM algorithm since the global optimum can be found without any iterations and restarts. We next consider an empirical VB (EVB) scenario where the hyperparam-eters (prior variances) are also learned from data. Again, the optimization problem of EVBMF is non-convex, but we still show that the global optimal solution of EVBMF can be computed analyti-cally. The usefulness of our results is demonstrated through experiments.
 Recently, the global optimal solution of VBMF when the target matrix is square has been obtained shown to be highly useful in experiments. In this section, we formulate the MF problem and review a variational Bayesian MF algorithm. 2.1 Formulation holds. Thus this does not impose any restriction.
 A key assumption of MF is that U is a low-rank matrix. Let H (  X  L ) be the rank of U . Then the matrix U can be decomposed into the product of A  X  R M  X  H and B  X  R L  X  H as follows (see Figure 1): Assume that the observed matrix V is subject to the following additive-noise model: where E (  X  R L  X  M ) is a noise matrix. Each entry of E is assumed to independently follow the where  X  X  X  X  Fro denotes the Frobenius norm of a matrix. 2.2 Variational Bayesian Matrix Factorization  X  ( U ) =  X  A ( A )  X  B ( B ) , where  X  A ( A )  X  exp c assume that the product c a h c b h is non-increasing with respect to h .
 energy with respect to r ( A, B |V n ) : where  X  X  X  X  p denotes the expectation over p .
 and B in the case of MF) makes the calculation feasible: 1 posterior mean : By applying the variational method to the VB free energy, we see that the VB posterior can be expressed as follows:  X  .  X  a where I d denotes the d -dimensional identity matrix, and The iterated conditional modes (ICM) algorithm [4, 5] for VBMF (VB-ICM) iteratively updates  X  to obtain a local optimal solution. Finally, an estimator of U is computed as by the above formula in every iteration of the ICM algorithm. 2.3 Empirical Variational Bayesian Matrix Factorization In the VB framework, hyperparameters ( c 2 a data by minimizing the VB free energy, which is called the empirical VB (EVB) method [5]. By setting the derivatives of the VB free energy with respect to c 2 a optimality condition can be obtained (see also Eq.(4) in Section 3): The ICM algorithm for EVBMF (EVB-ICM) is to iteratively update c 2 a tion to  X  a algorithm. In this section, we derive an analytic-form expression of the VBMF global solution. The VB free energy can be explicitly expressed as follows.
 F where | X | denotes the determinant of a matrix. We solve the following problem: where S d ++ denotes the set of d  X  d symmetric positive-definite matrices. This is a non-convex and left singular vectors: 2 where the coefficients are defined by Let Then we can analytically express the VBMF solution b U VB as in the following theorem. Theorem 1 The global VB solution can be expressed as Sketch of proof: We first show that minimizing (4) amounts to a reweighed SVD and any minimizer the bounds, which completes the proof.
 since many iterations and restarts would be necessary to find a good solution by ICM. Corollary 2 The VB posteriors are given by where, for b  X  VB h being the solution given by Theorem 1, When the noise variance  X  2 is unknown, one may use the minimizer of the VB free energy with numerically based on Eq.(4) and Corollary 2. In this section, we solve the following problem to obtain the EVBMF global solution:
Given  X  2  X  R ++ , analytically. We can observe the invariance of the VB free energy (4) under the transform  X  Figure 2: Profiles of the VB free energy (4) when L = M = H = 1 , n = 1 , and  X  2 = 1 for observations V = 1 . 5 , 2 . 1 , and 2 . 7 . (a) When V = 1 . 5 &lt; 2 =  X  increasing and thus the global solution is given by c h  X  0 . (b) When V = 2 . 1 &gt; 2 =  X  When V = 2 . 7 &gt; 2 =  X  solution. c h := c a h c b h also as a hyperparameter.
 Let Then, we have the following lemma: Lemma 3 If  X  h  X   X  and c h =  X  c h . Otherwise, c h  X  0 is the only local minimum of the VB free energy. we find a local minimum c h  X  0 . Combining the stationary conditions (2) and (3), we derive a smaller solution corresponds to saddle points completes the proof.
 Figure 2 shows the profiles of the VB free energy (4) when L = M = H = 1 , n = 1 , and  X  2 = 1 or c h =  X  c h is the global solution.
 Let and corollary.
 by b c h =  X  c h if  X  h &gt;  X  Corollary 5 The global EVB solution can be expressed as Since the optimal hyperparameter value b c h can be expressed in a closed-form, the global EVB find a good solution. 5.1 Artificial Dataset We randomly created a true matrix V  X  = We set n = 1 , and an observation matrix V was created by adding independent Gaussian noise with variance  X  2 = 1 to each element. We used the full-rank model, i.e., H = L = 30 . The noise variance  X  2 was assumed to be unknown, and estimated from data (see Section 2.2 and Section 3). the initial values of the EVB-ICM algorithm as follows:  X  a orthonormal vectors,  X  a h and  X  b h were set to identity matrices multiplied by scalars  X  2 a respectively.  X  2 a plotted in the graph by the dashed line. The graph shows that the EVB-ICM algorithm reduces the speed was quite slow once in 10 runs, which was actually trapped in a local minimum. Next, we compare the computation time. Figure 3(b) shows the computation time of EVB-ICM over iterations and our analytic form-solution. The computation time of EVB-ICM grows almost linearly ing the single-parameter search for  X  2 . Thus, our method provides the reduction of computation time in 4 orders of magnitude, with better accuracy as a minimizer of the VB free energy. over 10 runs for VB with various hyperparameter values and EVB. A single hyperparameter value was commonly used (i.e., c 1 =  X  X  X  = c H ) in VB, while each hyperparameter c h was separately optimized in EVB. The result shows that EVB gives slightly lower generalization errors than VB with the best common hyperparameter. Thus, automatic hyperparameter selection of EVB works quite well.
 Figure 3(d) shows the hyperparameter values chosen in EVB sorted in the decreasing order. This 5.2 Benchmark Dataset MF can be used for canonical correlation analysis (CCA) [8] and reduced rank regression (RRR) [19] with appropriately pre-whitened data. Here, we solve these tasks by VBMF and evaluate the performance using the concrete slump test dataset [28] available from the UCI repository [2]. The experimental results are depicted in Figure 4, which is in the same format as Figure 3. The Overcoming the non-convexity of VB methods has been one of the important challenges in the Bayesian machine learning community, since it sometimes prevented us from applying the VB meth-ods to highly complex real-world problems. In this paper, we focused on the MF problem with no missing entry, and showed that this weakness could be overcome by computing the global optimal method, where hyperparameters are also optimized based on data samples. Since no hand-tuning parameter remains in EVBMF, our analytic-form solution is practically useful and computationally highly efficient. Numerical experiments showed that the proposed approach is promising. When c a h c b h  X  X  X  , the priors get (almost) flat and the quartic equation (5) is factorized as Theorem 1 states that its second largest solution gives the VB estimator for  X  h &gt; lim c a p
M X  2 /n . Thus we have Bayesian estimation when the model is non-identifiable (i.e., the mapping between parameters and Thus, it never appears in MAP estimation [15]. The probabilistic PCA can be seen as an example of MF, where A and B correspond to latent variables and principal axes, respectively [24]. The maximizer of the marginal likelihood.
 sian, the VB posterior is column-wise independent, and there exists no missing entry. They were necessary to solve the free energy minimization problem as a reweighted SVD. An important fu-Acknowledgments The authors appreciate comments by anonymous reviewers, which helped improve our earlier manuscript and suggested promising directions for future work. MS thanks the support from the FIRST program. RT was partially supported by MEXT Kakenhi 22700138.
