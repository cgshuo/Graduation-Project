 Data warehousing and on-line analytical processing(OLAP) are essential ele-ments of decision support, which has increasingly become a focus of the data-base industry [1]. Computation of a data cube is a very important problem in the area of data warehousing and OLAP. To fulfill the requirement of fast interactive multidimensional data analysis, database systems have to pre-compute aggrega-tion views on some subsets of dimensions and their corresponding hierarchies. For this task, many efficient cube computation algorithms have been proposed, such as ROLAP-based multi-dimensional aggregate computation [2], multi-way array aggregation [3], Top-k H-Cubing [4], and Star-Cubing [5]. Since computing the whole data cube not only requires a substantial amount of time, but also generates a huge number of cube cells to be stored. Many efficient computation methods have been put forward, such as partial materialization of a data cube [6], Condensed Cube [7], Dwarf [8], Quotient Cube [9], and Object Deputy Model [10]. However, there exist datasets in real applications like bioinformatics, sta-tistics, and text processing that are characterized by high dimensionality, e.g., over 100 dimensions, and moderate size, e.g., around 10 6 tuples that can not be well processed by these algorithms. Since a data cube grows exponentially with the number of dimensions, it is too costly in both computation time and storage space to materialize a full high dimensional data cube. For example, a data cube of 100 dimensions each with 10 distinct values may contain as many as 11 100 aggregate cells. Iceberg Cube [4] used a pruning method to avoid calculating the aggregations below a certain threshold, which is an effective way to derive nontrivial multidimensional aggregations, but the number of the cells in the com-puting result is still large. For example, an Iceberg cube with 6 million tuples and 60 dimensions will still produce about 2 60 cells when the threshold is set to be 5. Quotient Cube [9] compresses the data cube through sharing the tuples, but on the same condition the cube size is more than double of the Iceberg Cube [11]. So, it is not suitable for high dimensional datasets. The space complexity of Dwarf [8] was O ( T 1+1 / (log d C ) )[12],where d is the number of dimensions, C is the cardinality, and T is the number of tuples. In a high dimensional dataset above-mentioned where d is large, C is small, log d C could become quite small and the cube size still explodes. Frag-Cubing [11] proposed an algorithm based on partitions, but its space cost and time cost are still large.

To solve high dimensional cube computation, we propose a new efficient technique called compressed bitmap index cubing. The cube construction algo-rithm based on compressed bitmap index vertically partitions a high dimensional dataset into a set of disjoint low dimensional datasets called segments. For each segment, the local data cube is fully computed, and the bitmap index of each attribute value in the segment data cube is constructed. If the value of a bit in a bitmap is equal to 1, it indicates the attribute value is appeared in the corre-sponding tuple. Then we can re-construct the corresponding cuboid upon request using bit-AND operations. Since the experiments show that there are many con-tinuous 0-bit redundancy in the beginning and the end of the bitmap indices, we can compressed the bitmap indices using two pointers called start valid pointer and end valid pointer . Because of the fast speed of bit-AND operations and the 0-bit redundancy in the beginning and the end of the bitmap indices, the computation time of data cubes and the storage space spending on compressed bitmap indices are highly reduced.

The method proposed in this paper has excellent performance on computing data cubes with high dimension and low cardinality in both computation time and storage space. The smaller the distinct value is, the better the performance of the method is. In addition, experiments show that the computation time and the storage space of our method are competitive with the Frag-Cubing algorithm either with the dimensions varied fro m 10 to 80, or with the tuples ranged from 60 thousand to 160 thousand.

The remainder of the paper is organized as follows. Section 2 describes the motivation. We discuss the structure of the compressed bitmap index and two algorithms for cube constructing and querying in Section 3. Section 4 shows the results of experiments and analysis. We gives the conclusion in Section 5. To fulfill the requirement of fast interactive multidimensional data analysis, view materialization is very important, especially to the datasets in real applications like bioinformatics, statistics, and text processing that are characterized by high dimensionality, low distinct attribute values, and moderate size. Since a data cube grows exponentially with the number of dimensions, it is too costly in both computation time and storage space to materialize a full high dimensional data cube. We propose an efficient indexing technique, compressed bitmap index, to deal with the computation of such high dimensional data cubes. 2.1 Limitations of Other Cube Computation Algorithms Most of the traditional cube computation algorithms are focused on partial or fully materializing a data cube. When a cube is partly materialized, if a query can not be answered by the existing materialized views, the re-computation on the whole dataset is necessary. And the speed is sometimes the bottleneck. When a cube is to be fully materialized, the space cost and the time cost are sometimes hardly tolerant. Practices in real applications show that although data analysis tasks may involve a high dimensional space, most OLAP operations are performed only on a small number of dimensions at a time. Most analyses will drill down and pivot a small set of dimensions, and other dimensions are set with a certain value or all values involved, respectively [11]. In [11] the segment cube are fully materialized. The high dimensional dataset is vertically partitioned into a set of disjoint low dimensional datasets. For each partition, the local data cube is fully materialized. If the dimensions involved in a query are in the same partition, the results can be retrieved from the materialized segment cube directly. Otherwise the corresponding attributes in the separate materialized segment cubes are dynamically combined. This might be done efficiently and satisfy the response time of OLAP operations by a partition based method. 2.2 Limitations of Other Partition Based Algorithms Researchers have proposed a method based on partitions to compute high di-mensional data cubes before, e.g., Frag-Cubing algorithm [11]. But it also has its shortcomings. Formally, suppose a database has T tuples, C cardinalities, and D dimensions. In the algorithm Frag-Cubing each tuple ID is associated with D attributes and thus will appear D times in the inverted index. Since there are T tuple IDs in total, the entire inverted index will still need D  X  T integers [11]. For example, for a cube with 60-dimensional base cuboids of T tuples, the amount of space to store the fragment of size 3 is on the order of T ( 60 3 )(2 3  X  1) = 140 T . Suppose there are 10 6 tuples in the database and each tuple ID takes 4 bytes. The space needed to store the fragments of size 3 is roughly estimated as 140  X  10 6  X  4 = 560MB.

In the above expression, 140 indicates the number of the cuboids, and the 10 6  X  4 is the byte number of the index of each cuboid occupied. If we can reduce the space cost of each cuboid, the total space cost will be reduced. It is well known that bitmap index is suitable for the data with low cardinality, and the bit-AND operation runs faster than the intersecting operation. So we can use bitmap index to compute high dimensional data cubes. Let X  X  see the bitmap index using the same example above-mentioned. Each attribute value takes T 8 bytes to indicate all the tuples, the space one cuboid needed is between C  X  T 8 and Min ( C 3 , T )  X  T 8 bytes. So the total space is between 140  X  C  X  T 8 bytes 10 6 ,and C is 5, the space is between 87.5MB and 537.5MB. We can see that even on the worst case, the space is smaller than [9].And on the best case the space saving is very excellent. Let C is 10, the result will be between 175MB and 3325MB. With the changing of C from 5 to 10, the space is increasing. The space is very large on the worst case. Thereby if we make good use of bitmap index such as compressing it reasonably, we will get good effects. So we proposed an efficient indexing technique, compressed bitmap index, to compute high dimensional data cubes.

The above observations are very considerable to us. The idea of vertically partitioning the whole data cube into a set of disjoint low dimensional datasets is very good, but the storage of the inverted list and the intersecting operation take a very large amount of resource. If we can find a more effective technique, the performance would be more efficient. By investigating deeply into the bitmap index technique, we find that when we use bitmap index to indicate the ID-lists of a certain attribute value, the continuous 0 bits appeared many times in the beginning, in the end, and in the middle part of the bitmap index. The bitmap index itself is suitable for the low cardinality datasets, and the bit-AND operation is very fast, the rather that we can avoid the storage of the 0-bit redundancies in a bitmap index by recording the start and the end valid positions of the nonzero bits. Stemming from the above motivation, we propose a new method, called com-pressed bitmap index, and two algorithms: one for constructing a data cube, and the other one for processing queries. This new method will be able to han-dle OLAP on datasets with extremely high dimensionality and low cardinality. The general idea is to use a bitmap index compressed by two valid pointers on the divided partitions. The base dataset is projected onto each segment, and the data cube of each segment is fully materialized. With the pre-computed segment cubes, we can dynamically assemble the attributes and answer the queries on-line, which is done efficiently by bit-AND operations on the compressed bitmap indices. 3.1 Compressed Bitmap Index Many kinds of indices can be used in constructing data cubes, such as B-tree, Hash table. But considering the high dimensional datasets with low cardinality in each dimension, we use the bitmap index in order to storage the data effectively. And it converts the standard comparing, joining and aggregating operations to the bit arithmetic operations and reduces the runtime enormously. Thereby it can improve the performance of the system [13].

To illustrate the algorithm, a tiny dataset (see Table 1) is used as a running ex-ample. Let the cube measure be count (). Other measures will be discussed later. The following illustrates the construction and computation of the compressed bitmap index. Firstly, suppose that we divide the 5 dimensions in Table 1 into 2 independent segments, namely (A, B, C) and (D, E). In the real world appli-cations the attributes in a segment may be determined by the semantics of the data and the query patterns on the data.

Then, it should construct the corresponding bitmap indices (shown in Table 2). Each line in Table 2 records a value of an attribute and the bitmap index, which tells which tuples contain the value. For example, the value a 2 appears in tuple 4 and tuple 5, and then the bitmap index for a 2 contains two 1 bits in the 4th and 5th positions. We use the segment (A, B, C) as an example to illustrate the local materialization operation of the method. Use the bit-AND operation in Table 3 on the tid bitmap index of dimension A and dimension B in Table 2, and we can get the Cuboid AB . Similarly we can get the Cuboid ABC using the Cuboid AB and the tid bitmap index corresponding with c 1 in Table 2. Experiments show that the computation time complexity and the storage space complexity are increased linearly with the number of the dimensions when the number of the dimensions in each fragment is less than 4 and more than 2 [11]. Finally, the data cube of each segment is computed. Taking segment (A, B, C) as an example, there are 7 cuboids, namely A, B, C, AB, AC, BC and ABC ,tobe computed. We can compute the complete data cube by bit-ANDing the bitmap indices in table 2 in a bottom-up depths-first order in the cuboids lattice. For example, to compute the cell { a 1 ,b 2 ,  X  X  , we may bit-AND the bitmap indices of a 1 and b 2 to get a new bitmap index of After computing Cuboid AB , we can similarly compute Cuboid ABC by bit-ANDing all pair wise combinations between Table 3 and the row c 1 in Table 2. Note that the entry ( a 2 ,b 2 ) can be effectively discarded because it is all 0 bits. The same process can be applied to computing segment (D, E), which is completely independent from computing (A, B, C). So, it can be computed in parallel.
 In this method, we use different approaches to compute different aggregations. For the cube with only the tuple-counting measure, it is unnecessary to access the original dataset for aggregation since the number of the 1 bits in a corresponding index is equivalent to the number of tuples in a group. But for the solution to average () or su m (), it is necessary to keep an I D m easure array instead of the original dataset. For example, for computing average (), we just need to keep an array with three elements: ( tid, count, su m ). The measures of each aggregate cell can be computed by only accessing this I D m easure array.
Analysis and experiments on the real data distribution show that the bitmap index structure also has great redundancies. Cell a and Cell b (Cell: attribute value, its bitmap index and the corresponding aggregation value) have large amounts of continuous 0 bits in the beginning and the end (see Fig.1). The cases occur frequently in many cells under the combination of multi-dimensional datasets (see Table 3). So it is very necessary to reduce the redundancies.
The compressed bitmap index based algorit hm of cube construction registers the start position and the end position of non-zero using two pointers, called start valid pointer and end valid pointer , in order to compress the storage space of the bitmap index (shown in Fig.2). In this way we only need to store the two pointers and the bitmap index segment between them, but not the whole bitmap index, thereby the 0-bit redundancies in the beginning and the end are reduced, and the storage space is saved.
For example, there are two bitmap sequences I ndex 1 and I ndex 2 ,andthe valid pointer of I ndex 1 are beginP os 1 and endP os 1 , the valid pointer of I ndex 2 are beginP os 2 and endP os 2 . To bit-AND such two bitmap sequences only needs to bit-AND the bits between max( beginP os 1 , beginP os 2 )andmin ( endP os 1 , endP os 2 ), but not bit-AND all bits of the sequences. Especially, if the min( endP os 1 , endP os 2 ) is not bigger than max( beginP os 1 , beginP os 2 ), we need do nothing but set all the bits to 0. We only need ( endP os  X  beginP os +1)/8 bytes to store the result instead of ( the total nu m ber of the tuples /8) bytes, so the memory consuming is reduced greatly. 3.2 Cube Construction Algorithm Based on the above discussion, the algor ithm for constructing a data cube using compressed bitmap index can be summarized as follows.
 Algorithm 1. Compressed Bitmap index based Algorithm for cube Construction Input: a fact table D with n dimensions ( A 1 ,  X  X  X  ,A n ) ; Output: 1) a set of segment partitions { P 1 ,  X  X  X  ,P k } and the corresponding 2. for each tuple t in D do { 3. insert each &amp; tid, m easure ' into I D m easure array ; 4. construct bitmap index &amp; a i ,Bit m ap I ndex ' for each element a i of each 5. for each segment partition P i do { 6. compute the local cube S i using bit-AND operation; 7. compute the corresponding measures; 8. save S i and its measure on the disk. } CBAC algorithm firstly vertically partitions the dataset(line 1), secondly scans the original dataset(line 2) and extracts &amp; tid, m easure ' into the I D m easure array if the measure is not count ()(line 3), at the same time it constructs the bitmap index of each attribute(line 4), thirdly constructs the data cube of each segment(line 5) by the bit-AND operation(line 6), and then computes the ag-gregations of each segment(line 7), lastly stores the compressed bitmap index on the disk(line 8).
 To the line 3, if the aggregation is count (), it is not necessary to construct the I D m easure array because the number of 1 bits in the bitmap index equals the number of the tuples. And for other aggregations, e.g., average (), the aggrega-tions should be computed using the I D m easure array.

Because of the limitation of bitmap index itself [14], the algorithm X  X  perfor-mance will be affected by the datasets with high cardinalities. With the cardinal-ity increasing the number of 0 bits will increase, and the distribution of attribute values in the tuples will become randomization, so the effect of the compressed algorithm will be weaken. 3.3 Querying Algorithm M ' .Each a inquire ?. The first step is to gather all the instantiated a i  X  X  if there are any. We examine the partitions to check which a i  X  X  are in the same segments. Once that is done, we retrieve the bitmap indices associated with the instantiations at the highest possible aggregation level. For example, suppose a j and a k were in the same segment, we would then retrieve the bitmap indices from the ( a j ,a k )cuboid cells. The obtained bitmap indices are to be bit-ANDed to derive the instantiated base table. If all the bits in the bitmap indices are 0s, query processing stops and returns the empty result.

If there are no inquired dimensions, we simply fetch the corresponding mea-sures from the ID measure array and finish the query. If there is at least one inquired dimension, we continue as follows. For each inquired dimension, we re-trieve all its possible values and their associated bitmap indices. If two or more inquired dimensions are in the same segment, we retrieve all their pre-computed combinations and the bitmap indices. Once these bitmap indices are retrieved, they are to be bit-ANDed with the instantiated base table to form the local base cuboid of the inquired and instantiated dimensions.
 The above discussion leads our algorithm to processing all the possible queries. Note that function m erge index () is implemented by bit-ANDing the corre-sponding tid bitmap indices of the B D i  X  X  . Function co m pute cube () takes the merged instantiated indices and the inquired dimensions as input, derives the relevant base cuboid, and uses the most efficient cubing algorithm to compute the multi-dimensional cubes. The I D m easure array will be referenced after the cube is derived in this co m pute cube () function.

If the dimensions in a query are not in the same segment, we can bit-AND the bitmap indices of the different segments on line, and can get the right answer in time. The computation time complexity and storage space complexity are reduced to linear with the number of the dimensions as well as guaranteeing the response time requirement. So it was suitable for the computation of the above-mentioned high dimensional data cubes.
 Algorithm 2. Compressed Bitmap index based Algorithm for Querying(CBAQ). Input: 1) a set of segment partitions { P 1 ,  X  X  X  ,P k } and the corresponding Output: The computed measure(s) if the query contains only instantiated 1. for each P i { //instantiated dimensions 2. if P i  X  X  a 1 ,  X  X  X  ,a n } includes instantiation(s) 3. D i  X  P i  X  X  a 1 ,  X  X  X  ,a n } with instantiation(s); 4. B D 5. if P i  X  X  a 1 ,  X  X  X  ,a n } includes inquire(s) 6. Q i  X  P i  X  X  a 1 ,  X  X  X  ,a n } with inquire(s); 7. R Q 8. if there exists at least one not all 0 bits B D 9. B q  X  m erge index ( B D 1 ,  X  X  X  , B D 10. if there exists at least one not all 0 bits R Q 11. C q  X  co m pute cube ( B q , R Q 1 ,  X  X  X  , R Q In this section we will give the performance analysis and comparison of the pro-posed algorithm on datasets with different sizes, different dimension numbers and different cardinalities. All the experiments are conducted on an Intel Pentium-4, 2.4GHz system with 512MB RAM. The operating system runs Windows 2000 professional. And the dataset is KDD-CUP-99 [15] with 200000 tuples. The ex-periments in [2] are performed on datasets with 3 to 6 attributes, and obviously it is not to deal with high dimensional datasets. [4] contains only aggregates above certain thresholds, and the information it provides may not satisfy the requirements. Others are similar with them except [11]. So we compare the al-gorithms with the ones in [11]. To be in step with the Frag-Cubing algorithm and satisfy with the requirement of low cardinality on each dimension, we use 3 as the dimension number of each segment used in the Frag-Cubing algorithm in the following experiments. In the figures, we denote the cardinality as C, Frag-Cubing as FC, and CBAC as CB. 4.1 Computation Time and Storage Space Under Different Figure 3(a) describes the CBAC algorithm and the Frag-Cubing algorithm X  X  computation time changed with the number of dimensions varying from 10 to 80 and the cardinalities are 8 and 15 respectively. It shows that algorithm CBAC is notable on time saving by about 30%. The construction time increases linear with the increasing of the number of the dimensions and the distinct value of each dimension is not affected most to the time complexity. Figure 3(b) describes the CBAC algorithm and the Frag-Cubing algorithm X  X  storage space changed with the number of dimensions varying from 10 to 80 and the cardinalities are 8 and 15 respectively. In the two different cases, the compression ratio of CBAC is very notable, especially when the distinct value is 8, and the compression ratio achieved 80%. However, just as mentioned in Sect.3, it is affected by the limitation of bitmap index itself, when the distinct value increased from 8 to 15, the spatial compression ratio is decreased. 4.2 Computation Time and Storage Space Under Different Tuple Figure 4(a) and (b) respectively show the computation time and the storage space cost of the CBAC algorithm and the Frag-Cubing algorithm with the tuple number varying from 60 thousand to 160 thousand, where the cardinality is 15 and the number of dimensions is 40. From the figures we can see the savings of the computation time and the storage space of algorithm CBAC is very obviously, and it is linear with the increasing of the tuple numbers. Thus, our new algorithm is scalable.

In conclusion, the proposed algorithm CBAC is very suitable for computation of data cubes on the data of bioinformatics, statistics, and text processing that characterized by high dimensionality and low cardinality. And it has more no-table compression ratio and faster computing speed than Frag-Cubing. That is very significant for the computation of high dimensional data cubes. In this paper we propose an efficient indexing technique consisting of a bitmap in-dex, and two algorithms for cube computation and querying for high dimensional datasets. The compressed bitmap index structure has the following advantages: (a) a very fast bit-AND operation based on the compressed bitmap index; (b) greatly reduced operations of bit-ANDing and the memory consumption by the introduced start valid pointer and end valid pointer ; (c) effectively savings of the disk space of datacubes. The experimental results show that comparing with the Frag-Cubing algorithm computation time of the algorithm CBAC is saved by 30%, and the storage space is saved by more than 25%, and it is more applicable than the Frag-Cubing algorithm for the datasets with high dimensional and low cardinality.

When the number of the distinct value of each dimension is large, the per-formance of the CBAC will become worse. We intend to solve this problem to improve our compressing policy for the bitmap index. Not only in the begin-ning and the end, but also in the middle and other positions there are many continuous 0 bits. So we can compress the bitmap index further to improve the performance. At the same time, the incremental update is important as the com-putation and storage for an OLAP system, so another future work is to study the issues of on-line incremental update based on compressed bitmap index.
