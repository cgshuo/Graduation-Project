 Structure learning aims to discover the topology of a probabilistic network of variables such that this network represents accurately a given dataset while maintaining low complexity. Accuracy of representation is measured by the likelihood that the model explains the observed data, while complexity of a graphical model is measured by its number of parameters. Structure learning faces several challenges: the number of possible structures is super-exponential in the number of variables while the required sample size might be even exponential. Therefore, finding good regularization techniques is very important in order to avoid over-fitting and to achieve a better generalization performance. In this paper, we propose local constancy as a prior for learning Gaussian graphical models, which is natural for spatial datasets such as those encountered in computer vision [1, 2, 3]. For Gaussian graphical models, the number of parameters, the number of edges in the structure and the number of non-zero elements in the inverse covariance or precision matrix are equivalent measures of complexity. Therefore, several techniques focus on enforcing sparsity of the preci-sion matrix. An approximation method proposed in [4] relied on a sequence of sparse regressions. Maximum likelihood estimation with an ` 1 -norm penalty for encouraging sparseness is proposed in [5, 6, 7]. The difference among those methods is the optimization technique: a sequence of box-constrained quadratic programs in [5], solution of the dual problem by sparse regression in [6] or an approximation via standard determinant maximization with linear inequality constraints in [7]. It has been shown theoretically and experimentally, that only the covariance selection [5] as well as graphical lasso [6] converge to the maximum likelihood estimator.
 In datasets which are a collection of measurements for variables with some spatial arrangement, one can define a local neighborhood for each variable or manifold. Such variables correspond to one-dimensional neighborhood in which each point has two neighbors on each side of the closed contour. Similarly, one can define a four-pixel neighborhood for 2D images as well as six-pixel neighborhood for 3D images. However, there is little research on spatial regularization for structure learning. Some methods assume a one-dimensional spatial neighborhood (e.g. silhouettes) and that variables far apart are only weakly correlated [8], interaction between a priori known groups of variables as in [9], or block structures as in [10] in the context of Bayesian networks. Our contribution in this paper is two-fold. First, we propose local constancy , which encourages finding connectivities between two close or distant clusters of variables, instead of between isolated variables. It does not heavily constrain the set of possible structures, since it only imposes restric-tions of spatial closeness for each cluster independently, but not between clusters. We impose an ` -norm penalty for differences of spatially neighboring variables, which allows obtaining locally constant models that preserve sparseness, unlike ` 2 -norm penalties. Our model is strictly convex and therefore has a global minimum. Positive definiteness of the estimated precision matrix is also guaranteed, since this is a necessary condition for the definition of a multivariate normal distribution. Second, since optimization methods for structure learning on Gaussian graphical models [5, 6, 4, 7] are unable to handle local constancy constraints, we propose an efficient algorithm by maximizing with respect to one row and column of the precision matrix at a time. By taking directions involving either one variable or two spatially neighboring variables, the problem reduces to minimization of a piecewise quadratic function, which can be performed in closed form.
 complex synthetic model which includes locally and not locally constant interactions as well as independent variables. Our method outperforms the state-of-the-art structure learning techniques [5, 6, 4] for datasets with both small and large number of samples. We further show that our method has better generalization performance on real-world datasets. We demonstrate the ability of our method to discover useful structures from datasets with a diverse nature of probabilistic relationships and spatial neighborhoods: manually labeled silhouettes in a walking sequence, cardiac magnetic resonance images (MRI) and functional brain MRI.
 Section 2 introduces Gaussian graphical models as well as techniques for learning such structures from data. Section 3 presents our sparse and locally constant Gaussian graphical models. Section 4 describes our structure learning algorithm. Experimental results on synthetic and real-world datasets are shown and explained in Section 5. Main contributions and results are summarized in Section 6. In this paper, we use the notation in Table 1. For convenience, we define two new operators: the zero structure operator and the diagonal excluded product.
 A Gaussian graphical model [11] is a graph in which all random variables are continuous and jointly Gaussian. This model corresponds to the multivariate normal distribution for N variables x  X  R N with mean vector  X   X  R N and a covariance matrix  X   X  R N  X  N , or equivalently x  X  N (  X  ,  X  ) where  X   X  0 . Conditional independence in a Gaussian graphical model is simply reflected in the zero entries of the precision matrix  X  =  X   X  1 [11]. Let  X  = {  X  n x is preferred because it allows detecting cases in which two seemingly correlated variables, actually depend on a third confounding variable. The concept of robust estimation by performing covariance selection was first introduced in [12] where the number of parameters to be estimated is reduced by setting some elements of the precision matrix  X  to zero. Since finding the most sparse precision matrix which fits a dataset is a NP-hard problem [5], in order to overcome it, several ` 1 -regularization methods have been proposed for learning Gaussian graphical models from data.
 Covariance selection [5] starts with a dense sample covariance matrix b  X  and fits a sparse preci-sion matrix  X  by solving a maximum likelihood estimation problem with a ` 1 -norm penalty which encourages sparseness of the precision matrix or conditional independence among variables: for some  X  &gt; 0 . Covariance selection computes small perturbations on the sample covariance matrix such that it generates a sparse precision matrix, which results in a box-constrained quadratic programming. This method has moderate run time.
 The Meinshausen-B  X  uhlmann approximation [4] obtains the conditional dependencies by performing a sparse linear regression for each variable, by using lasso regression [13]. This method is very fast but does not yield good estimates for lightly regularized models, as noted in [6]. The constrained optimization version of eq.(1) is solved in [7] by applying a standard determinant maximization with linear inequality constraints, which requires iterative linearization of k  X  k 1 . This technique in general does not yield the maximum likelihood estimator, as noted in [14]. The graphical lasso method has run times comparable to [4] without sacrificing accuracy in the maximum likelihood estimator.
 Structure learning through ` 1 -regularization has been also proposed for different types of graphical models: Markov random fields (MRFs) by a clique selection heuristic and approximate inference [15]; Bayesian networks on binary variables by logistic regression [16]; Conditional random fields by pseudo-likelihood and block regularization in order to penalize all parameters of an edge simulta-neously [17]; and Ising models, i.e. MRFs on binary variables with pairwise interactions, by logistic regression [18] which is similar in spirit to [4].
 Cholesky factors of the precision matrix has been proposed in [8]. Instead of using the traditional lasso penalty, a nested lasso penalty is enforced. Entries at the right end of each row are promoted to zero faster than entries close to the diagonal. The main drawback of this technique is the assumption that the more far apart two variables are the more likely they are to be independent. Grouping of entries in the precision matrix into disjoint subsets has been proposed in [9]. Such subsets can model for instance dependencies between different groups of variables in the case of block structures. Al-though such a formulation allows for more general settings, its main disadvantage is the need for an a priori segmentation of the entries in the precision matrix. Related approaches have been proposed for Bayesian networks. In [10] it is assumed that variables belong to unknown classes and probabilities of having edges among different classes were enforced to account for structure regularity, thus producing block structures only. First, we describe our local constancy assumption and its use to model the spatial coherence of dependence/independence relationships. Local constancy is defined as follows: if variable x n dependent (or independent) of variable x n be dependent (or independent) of x n distant clusters of variables, instead of between isolated variables. Note that local constancy imposes restrictions of spatial closeness for each cluster independently, but not between clusters. for N variables, which correspond to spatially neighboring variables. Let b  X   X  R N  X  N be the dense where M  X  O ( N ) is the number of spatial neighborhood relationships. For instance, in a 2D image, M is the number of pixel pairs that are spatial neighbors on the manifold. More specifically, if pixel n 1 and pixel n 2 are spatial neighbors, we include a row m in D such that d mn 1 = 1 , d mn 2 =  X  1 and d mn 3 = 0 for n 3 /  X  X  n 1 , n 2 } . The following penalized maximum likelihood estimation is proposed: for some  X ,  X  &gt; 0 . The first two terms model the quality of the fit of the estimated multivariate normal distribution to the dataset. The third term  X  k  X  k 1 encourages sparseness while the fourth term  X  k D  X   X  k 1 encourages local constancy in the precision matrix by penalizing the differences of spatially neighboring variables.
 In conjunction with the ` 1 -norm penalty for sparseness, we introduce an ` 1 -norm penalty for local constancy. As discussed further in [19], ` 1 -norm penalties lead to locally constant models which preserve sparseness, where as ` 2 -norm penalties of differences fail to do so.
 The use of the diagonal excluded product for penalizing differences instead of the regular product of matrices, is crucial. The regular product of matrices would penalize the difference between the di-agonal and off-diagonal entries of the precision matrix, and potentially destroy positive definiteness of the solution for strongly regularized models.
 properties of the estimated precision matrix or the optimization algorithm, in the following Section 4, we discuss positive definiteness properties and develop an optimization algorithm for the specific case of the discrete derivative operator D . Positive definiteness of the precision matrix is a necessary condition for the definition of a multivari-ate normal distribution. Furthermore, strict convexity is a very desirable property in optimization, since it ensures the existence of a unique global minimum. Notice that the penalized maximum like-lihood estimation problem in eq.(2) is strictly convex due to the convexity properties of log det  X  on the space of symmetric positive definite matrices [20]. Maximization can be performed with respect to one row and column of the precision matrix  X  at a time. Without loss of generality, we use the last row and column in our derivation, since permutation of rows and columns is always possible. Also, note that rows in D can be freely permuted without affecting the objective function. Let: In term of the variables y , z and the constant matrix W , the penalized maximum likelihood estima-tion problem in eq.(2) can be reformulated as: where k Ay  X  b k 1 can be written in an extended form: Intuitively, the term k D 1 y k 1 penalizes differences across different rows of  X  which affect only val-ues in y , while the term columns of  X  which affect values of y as well as W .
 It can be shown that the precision matrix  X  is positive definite since its Schur complement z  X  and since v &gt; 0 and  X  &gt; 0 , this implies that the Schur complement in eq.(6) is positive. Maximization with respect to one variable at a time leads to a strictly convex, non-smooth, piecewise quadratic function. By replacing the optimal value for z given by eq.(6) into the objective function in eq.(4), we get: Since the objective function in eq.(7) is non-smooth, its derivative is not continuous and therefore methods such as gradient descent cannot be applied. Although coordinate descent methods [5, 6] are suitable when only sparseness is enforced, they are not when local constancy is encouraged. As shown in [21], when penalizing an ` 1 -norm of differences, a coordinate descent algorithm can get stuck at sharp corners of the non-smooth optimization function; the resulting coordinates are station-ary only under single-coordinate moves but not under diagonal moves involving two coordinates at a time.
 For a discrete derivative operator D used in the penalized maximum likelihood estimation problem the position corresponding to the two neighbor variables. Finally, assuming an initial value y 0 and minimizes: For simplicity of notation, we assume that r , s  X  R M use only non-zero entries of g and Ag on its definition in eq.(8). We sort and remove duplicate values in s , and propagate changes to r by adding the entries corresponding to the duplicate values in s . Note that these apparent modifications do not change the objective function, but they simplify its optimization. The resulting minimization problem in eq.(8) is convex, non-smooth and piecewise quadratic. Furthermore, since the objec-a closed form solution.
 The coordinate-direction descent algorithm is presented in detail in Table 2. A careful implemen-tation of the algorithm allows obtaining a time complexity of O ( KN 3 ) for K iterations and N algorithm converges quickly in usually K = 10 iterations. The polynomial dependency on the num-ber of variables of O ( N 3 ) is expected since we cannot produce an algorithm faster than computing the inverse of the sample covariance in the case of an infinite sample.
 Finally, in the spirit of [5], a method for reducing the size of the original problem is presented. Given a P -dimensional spatial neighborhood or manifold (e.g. P = 1 for silhouettes, P = 2 for a four-pixel neighborhood on 2D images, P = 3 for a six-pixel neighborhood on 3D images), the objective function in eq.(7) has the maximizer y = 0 for variables on which k u k  X   X   X   X  P X  . Since this condition does not depend on specific entries in the iterative estimation of the precision matrix, this property can be used to reduce the size of the problem in advance by removing such variables. Convergence to Ground Truth. We begin with a small synthetic example to test the ability of the method for recovering the ground truth structure from data, in a complex scenario in which our method has to deal with both locally and not locally constant interactions as well as independent variables. The ground truth Gaussian graphical model is shown in Figure 1 and it contains 9 variables arranged in an open contour manifold.
 Kullback-Leibler divergence, average precision (one minus the fraction of falsely included edges), average recall (one minus the fraction of falsely excluded edges) as well as the Frobenius norm be-tween the recovered model and the ground truth. For comparison purposes, we picked two of the state-of-the-art structure learning techniques: covariance selection [5] and graphical lasso [6], since it has been shown theoretically and experimentally that they both converge to the maximum likeli-hood estimator. We also test the Meinshausen-B  X  uhlmann approximation [4]. The fully connected as well as fully independent model are also included as baseline methods.
 Two different scenarios are tested: small datasets of four samples, and large datasets of 400 samples. Under each scenario, 50 datasets are randomly generated from the ground truth Gaussian graphical model. It can be concluded from Figure 2 that our method outperforms the state-of-the-art structure learning techniques both for small and large datasets. This is due to the fact that the ground truth Although this is a complex scenario which also contains not locally constant interactions as well as an independent variable, our method can recover a more plausible model when compared to other methods. Note that even though other methods may exhibit a higher recall for small datasets, our method consistently recovers a better probability distribution.
 A visual comparison of the ground truth versus the best recovered model by our method from small and large datasets is shown in Figure 1. The image shows the precision matrix in which red squares represent negative entries, while blue squares represent positive entries. There is very little differ-ence between the ground truth and the recovered model from large datasets. Although the model is not fully recovered from small datasets, our technique performs better than the Meinshausen-B  X  uhlmann approximation, covariance selection and graphical lasso in Figure 2.
 Real-World Datasets. In the following experiments, we demonstrate the ability of our method to discover useful structures from real-world datasets. Datasets with a diverse nature of probabilistic relationships are included in our experiments: from cardiac MRI [22], our method recovers global long range interactions between different parts; and from functional brain MRI [23], our method recovers functional interactions between different regions and discover differences in processing monetary rewards between cocaine addicted subjects versus healthy control subjects. Each dataset is also diverse in the type of spatial neighborhood: one-dimensional for silhouettes in a walking sequence, two-dimensional for cardiac MRI and three-dimensional for functional brain MRI. Generalization. Cross-validation was performed in order to measure the generalization perfor-mance of our method in estimating the underlying distribution. Each dataset was randomly split into five sets. On each round, four sets were used for training and the remaining set was used for measuring the log-likelihood. Table 3 shows that our method consistently outperforms techniques that encourage sparsity only. This is strong evidence that datasets that are measured over a spa-tial manifold are locally constant, as well as that our method is a good regularization technique that avoids over-fitting and allows for better generalization. Another interesting fact is that for the brain MRI dataset, which is high dimensional and contains a small number of samples, the model that assumes full independence performed better than the Meinshausen-B  X  uhlmann approximation, covariance selection and graphical lasso. Similar observations has been already made in [24, 25] where it was found that assuming independence often performs better than learning dependencies among variables. In this paper, we proposed local constancy for Gaussian graphical models, which encourages finding probabilistic connectivities between two close or distant clusters of variables, instead of between maximum likelihood estimation. Furthermore, we proposed an efficient optimization algorithm and proved that our method guarantees positive definiteness of the estimated precision matrix. We tested the ability of our method to recover the ground truth structure from data, in a complex scenario with locally and not locally constant interactions as well as independent variables. We also tested the generalization performance of our method in a wide range of complex real-world datasets with a diverse nature of probabilistic relationships as well as neighborhood type.
 There are several ways of extending this research. Methods for selecting regularization parameters for sparseness and local constancy need to be further investigated. Although the positive definite-ness properties of the precision matrix as well as the optimization algorithm still hold when including operators such as the Laplacian for encouraging smoothness, benefits of such a regularization ap-proach need to be analyzed. In practice, our technique converges in a small number of iterations, but a more precise analysis of the rate of convergence needs to be performed. Finally, model selection consistency when the number of samples grows to infinity needs to be proved.
 This work was supported in part by NIDA Grant 1 R01 DA020949-01 and NSF Grant CNS-0721701
