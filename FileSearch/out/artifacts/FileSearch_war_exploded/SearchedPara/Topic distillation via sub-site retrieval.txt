 1. Introduction
While the World Wide Web grows exponentially along with time, the amount of information that users can digest remains roughly constant. With such background, search engines came out as the widely recognized tools in helping users retrieve information from the Web. According to some previous surveys, among all the information needs of Web users, topic distillation is one of the most common and important forms. Here, topic distillation refers to the search scenario of distilling a small number of high-quality entry pages that are best representatives of a given broad topic. It was shown ( Broder, 2002 ) that about 39% Web search queries belong to topic distillation, while query log analysis has indicated an even higher proportion of 48%. Because of the popularity of topic distillation, the TREC conference has included it in the Web track since the year of 1999.

To the best of our knowledge, in most existing approaches for topic distillation, the single page was treated as the basic searching unit. Moreover, it has been a common practice in the literature of topic distillation to use some statistics of the query terms in the page content (such as TF-IDF ( Baeza-Yates &amp; Ribeiro-Neto, 1999 )) to compute a relevance score (using information retrieval models such as BM25 ( Robertson, 1997 )) and use hyperlinks to get an importance score (such as PageRank ( Page, Brin, Motwani, &amp; Winograd, 1998 )), then combine them to rank the retrieved web pages. However, this strategy has not yet been successful enough for topic distillation. According to the report of TREC 2003 ( Craswell &amp; Hawking, 2003 ), the best result produced by this strategy only has marginal retrieval accuracy: precision at 10 (P@10) of 12.8% and mean average prevision (MAP) of 15.43%. In other words, there is still a long way to go if one wants to get satisfactory search results for topic-distillation queries.

In order to improve the search performance for topic distillation, we investigated the ground truth provided by the TREC committee. Our observation is that the labeled positive answer for topic distillation is often the entry point of a collection of pages devoted to the query topic. That is, for topic distillation, whether a page is an appropriate answer to the query is not only determined by the page itself, but also by all the other pages for retrieval. Actually, its objective is not to find a single page but a group of pages, although only the entry point of the page group is labeled as positive answer. In other words, in order to improve the retrieval performance of topic distillation, we need to take the structural relationship between web pages (i.e., the website structure) into consideration.

As we know, the organizational structure of a website is usually represented by a tree (or sitemap), and the parent X  X hild relationship between any two pages can be derived from sitemap construction ( Qin, Liu, Zhang,
Chen, &amp; Ma, 2005 ). In this way, one can get all the descendent pages for a labeled entry page. According to our analysis, in many cases, some descendant pages are even more relevant than the labeled entry page in terms of TF-IDF ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ). The reason is that the labeled entry page may just be a navigation center with only a few words while concrete contents are placed in its descendant pages. Take
Table 1 for example, there are totally seven pages coming from the same site ( http://cio.doe.gov/ ) in the top 1000 retrieval results produced by the BM25 model ( Robertson, 1997 ) for the query  X  X  X ireless communica-tion X  X  of the topic distillation task in TREC 2003 Web track. If we base our ranking on the content relevance position because this page appears not as relevant to the query as its descendant pages  X  X  cio.doe.gov/wireless/ 3g/3g_index.htm  X  X ,  X  X  cio.doe.gov/wireless/background.htm  X  X , and  X  X  cio.doe.gov/wireless/wwg/wwg_index. htm  X  X .

To solve the aforementioned problem, we propose to change the basic searching units from single pages to sub-sites. Here, sub-site refers to the sub-tree of a website, which contains an entry page and all its descendent pages. For ease of reference, we denote such a sub-site rooted by page p as S ( p ). Consider the website shown in
Fig. 1 , where p j denotes a web page in this website. It is easy to understand that the pages in the solid circle two sub-sites S ( p 4 )={ p 4 , p 8 , p 9 , p 10 } and S ( p
With the concept of sub-site, we propose a novel search strategy, called sub-site retrieval, for topic distil-lation. With this strategy, an entry page can get the support from its descendent pages and thus may get a higher rank than before. More detail of this strategy will be introduced step by step in the following sections.
We also discuss how to provide a novel user interface for topic distillation under the concept of sub-site retrie-val. Our experiments on both TREC 2003 and 2004 showed that the proposed strategy can result in much better performance on topic distillation than previous approaches.

The rest of the paper is organized as follows: In Section 2 , we briefly introduce the task of topic distillation, and review some related works. In Section 3 , we first give an expansion model to describe the growth process of websites, and then propose a so-called PI algorithm for sub-site feature extraction based on the expansion model. We also discuss a possible user interface for sub-site retrieval. After that, we report our experimental results on the effectiveness and time complexity of sub-site retrieval in Section 4 . And at last, we give the con-cluding remarks in Section 5 . 2. Backgrounds 2.1. TREC Web track and topic distillation
There are totally three tasks in TREC Web track: topic distillation, homepage finding and named page find-ing. Our focus is on the task of topic distillation. However, to give readers a complete view, we will also briefly introduce the other two tasks.

Topic distillation is a general concept: given a broad topic, distill a small number of high-quality entry pages that possess the following properties 3 : (i) They should be principally devoted to the topic. (ii) They should not be part of larger sites which are also principally devoted to the same topic.
For example, for the topic of  X  X  X cience X  X , the following pages might be considered as desirable entry points in the .gov domain: http://www.nsf.gov/ (National Science Foundation); http://science.nasa.gov/ (NASA Science); http://www.science.gov/ (Government Science Portal); http://www.house.gov/science/welcome.htm (House Committee on Science).

However, the page http://www.house.gov/ will not be regard as a desirable entry point because it does not have the first property while the page http://www.nsf.gov/home/bio/ fails in not possessing the second property.

In general, entry pages as aforementioned often contain many access points to their child pages so that users can easily find further information about the topics contained in those pages. In other words, they are also good starting points for users to navigate the corresponding (sub) sites because they contain rich information of the organizational structures.

Homepage finding task is required to return the homepage of the query, and named page finding task aims to return the page whose name is exactly the query. Generally speaking, there is only one answer for homepage finding query and named page finding query.

As an example of differentiating these three tasks, we cite the TREC guideline as follows. Consider the query  X  X SGS X , which is the acronym for the US Geological Survey. Some interpretations of this query are:
Topic distillation : Find me homepages of sites describing aspects of the US Geological Survey. Possible answers include http://www.usgs.gov , http://water.usgs.gov , http://geography.usgs.gov , http://earth-quake.usgs.gov . (Another example:  X  X iteracy X , might mean find me .GOV sites about literacy, with answers including http://www.nifl.gov/ , http://nces.ed.gov/naal/ and http://nces.ed.gov/surveys/all/ .)
Homepage finding : Find me the URL of the USGS homepage ( http://www.usgs.gov ). I have forgotten or do not know that URL, or I prefer typing  X  X sgs X  to typing the full URL.

Named page finding : Find me the URL of a non-homepage, e.g., searching for http://www.usgs.gov/about-usgs.html with the query  X  X ntroduction to usgs X . The query is the name of the page in question (rather than, e.g., words describing its topic).

From the above analysis, we can conclude that the task of topic distillation is quite different from tradi-tional ad hoc retrieval. Therefore, treating each page as the basic searching unit might not work effectively for topic distillation, although it has been already working quite well for other search tasks. In this regard, we propose a novel concept: sub-site retrieval, in which the basic searching unit turns to be sub-site. Since ture information, we will give a brief literature review on how previous works utilize the structure information of the Web in the next sub-section. 2.2. Related works
Different from traditional information retrieval, Web search should consider both content information and structure information, such as hyperlink structure, sitemap structure, and page layout structure. There are inger, 1998; Bharat &amp; Mihaila, 2001; Brin &amp; Page, 1998; Chakrabarti, 2001; Chakrabarti, Joshi, &amp; Tawde, 2001; Kleinberg, 1999; Shakery &amp; Zhai, 2003 ). Roughly speaking, these methods can be divided into two cat-egories: one focuses on link analysis with the assistance of content information ( Bharat &amp; Henzinger, 1998;
Bharat &amp; Mihaila, 2001; Chakrabarti, 2001; Chakrabarti et al., 2001; Kleinberg, 1999 ); the other focuses on content information with the assistance of Web structure ( Brin &amp; Page, 1998; Shakery &amp; Zhai, 2003 ).
For the first category, HITS ( Kleinberg, 1999 ) is the representative. HITS first constructs a query specific sub-graph, and then computes the authority and hub scores on this sub-graph to rank the documents for topic distillation. Motivated by the success of HITS, many improved versions have also been proposed. For exam-ple, Bharat and Henzinger (1998) proposed to augment the HITS algorithm with content analysis, e.g., weighting edges and pruning nodes. Another example is the Hilltop algorithm ( Bharat &amp; Mihaila, 2001 ), which operates on a special index of  X  X  X xpert documents X  X . Results are ranked based on the match between the query and relevant descriptive text for hyperlinks on expert pages pointing to a given result page. Since expert documents are more reliable than general web pages, Hilltop can achieve good search accuracy. However, due to the scale limitation of expert documents, Hilltop is not so good at query coverage. Different from HITS and
Hilltop, in which the basic search unit is single page, Chakrabarti (2001), Chakrabarti et al. (2001) proposed a uniform fine-grained model: pages are segmented into regions according to their DOM trees, and then link analysis is performed on the level of regions.

For the second category, although attracted some attention in recent years, it has not yet been studied as comprehensively as the first category. The earliest work in this category can be dated back to Brin and Page (1998) , which propagates anchor text from one page to another through hyperlink to expand the feature set of web pages. Following that, Shakery and Zhai (2003) propagate the relevance score of a page to another page through hyperlink between them. They define the so-called hyper relevance score of each page as a function of three variables: the content similarity of a page to the query (self relevance), a weighted sum of the hyper rel-evance scores of all the pages that point to this page, and a weighted sum of the hyper relevance scores of all the pages this page points to. According to these definitions, their relevance propagation model can be written as x and x O are weighting functions for in-link and out-link pages. For implementation, they give three special cases of the relevance propagation model: the weighted in-link model, the weighted out-link model, and the uniform out-link model. Their experimental results show that relevance propagation generally performs better than using only content information. However, the amount of improvement is sensitive to the document col-lection and the tuning of parameters ( Shakery &amp; Zhai, 2003 ).

The basic idea of our proposed sub-site retrieval is similar to anchor text propagation ( Brin &amp; Page, 1998 ) and score propagation ( Shakery &amp; Zhai, 2003 ) to some extend, while their major difference is that these works made use of hyperlink structure but we consider the structure of sitemap. To have a clearer view of the rela-tionship between sub-site retrieval and other methods, we come out Table 2 . From this table, we can see that sub-site retrieval is unique in that (1) sub-site retrieval focuses on content analysis with the help of sitemap structure for topic distillation; (2) its basic search unit is sub-sites instead of single pages. 3. Sub-site retrieval
In this section, we will exhibit the details of sub-site retrieval. As mentioned in the introduction, we treat sub-sites as the basic searching units to improve the retrieval performance for topic distillation. And also as aforementioned, a very important step for sub-site retrieval is to extract effective feature representation for a sub-site. For this purpose, we will first propose a generative model to simulate the growing process of websites.
That is, how a sub-site is formed starting from its original idea. Then, we discuss how to recover that original idea based on an existing website in a reverse manner. Actually, this reverse engineering just corresponds to a feature extraction algorithm. After that, we can compute a relevance score for the sub-site using a certain rel-evance weighting function, just like what people have done for single page based retrieval. Along with it, we also show a possible user interface to better deliver the search results of sub-site retrieval. 3.1. Site expansion model
Although the Web is heterogeneous in both its structure and its content, we believe that there is still some-thing in common for most websites because their design purposes are similar: to deliver information to the users as clear as possible. Accordingly, we give a simple generative model to describe the general growing pro-cess of websites in the statistical sense. We call it the site expansion model, which is illustrated as follows.
When an author starts to build a website, he may only have some original idea about what this website should talk about. According to this idea, he may first build a homepage with some related materials. For example, in Fig. 2 (a), this primitive homepage is denoted by a circle. Obviously, the author will not be satisfied with only one primitive page in his website. He will enrich the website step-by-step. Suppose the original idea can be divided into four sub-ideas, distinguished by white, green, cyan and magenta respectively in Fig. 2 (a), and the author determines that three of them should be expatiated with new pages. Then, he will collect cor-responding materials and resources to realize three new pages to illustrate each sub-idea, as shown in Fig. 2 (b).
The region with horizontal bars in the child page denotes the new collected materials and the other region is inherited from its parent page. In the meanwhile, the original region for each sub-idea in the homepage is reduced: there may be only some snippets and/or a few hyperlinks left in the homepage for those sub-ideas because their concrete realizations can be found in the child pages. Then, if we extract the features of the sub-ideas from the homepage at this time, we cannot get enough information because most of the words describing them are scatted in the child pages even if they are indeed the main content of the site.
Note that the process described in the previous paragraph can be iterative. That is, for the next steps, some of the aforementioned sub-topics will also be enriched in similar manners and some grandchild pages are gen-erated. As shown in Fig. 2 (c), suppose the green circle consists of three sub-topics and two of them are expanded to new pages. In this divide X  X xpand process, a content-rich website will be build iteratively starting from only some original ideas. Through this process, the original idea of site is scattered in a hierarchically organized structure, and more and more materials and resources are brought into the site to support those original ideas.

To better illustrate this website expansion model, we give a simple example in Fig. 3 . Suppose we are con-structing a website about information retrieval (IR). We first create the root page www.ir.org to introduce some basic concept of IR. In this page, we briefly describe something about IR theory, multimedia IR, Web
IR, text mining, IR system, IR experimentation and so on. After that, we want to add more pages to support the homepage. For this purpose, we extract some content in the root page to formulate four sub-topics: IR theory, multimedia IR, Web IR, and text mining. Accordingly, we gather some materials and create four new child pages. Since we give detailed description of these sub-topics, we may simplify the corresponding content in the homepage to deduce duplication. After that, we can further split the sub-topic of multimedia
IR into some sub-sub-topics, such as image retrieval, audio retrieval and video retrieval. In such an iterative manner, we can eventually construct a content-rich website about IR. 3.2. Feature extraction for sub-site
In our proposed approach, sub-site feature extraction is just the reverse process of the website authoring process defined by the expansion model. This time, we already have a website, and our purpose is to recon-struct the original idea of each sub-site. As mentioned in the above sub-section, a page is only part of the real-reconstruct the original idea.

According to the expansion model, in each iterative step, a page will generate many child pages to expatiate its original idea and each child page may contain many other materials. Therefore, if we simply accumulate the and result in concept drift. To tackle this problem, we propose the Punished Integration (PI) algorithm for sub-site feature extraction. It integrates the features of all individual pages in the sub-site with the consider-ation of how much a page contributes to the whole sub-site.

Before illustrating the PI algorithm, we first give some notations. Suppose the website is organized as a tree i th level. In fact, the local index is the order of the page in its siblings. Take Fig. 4 for example, for page p the subscript s is  X  X 0,2,0 X  X , which means that its ancestor in the first level is page p second level is page p 0,2 . Use R ( p s ) to denote the set of the first-generation child pages of p of elements in R ( p s ) is denoted by i R ( p s ) i . Use l ( p of p s . Consider an example in Fig. 4 . The homepage of the website is p pages, which are denoted by p 0,0 , p 0,1 , p 0,2 respectively. p denoted by p 0,1,0 and p 0,1,1 . Furthermore, i R ( p 0 ) i =3, l ( p
Furthermore, we use h ( p s ) to denote the number of hierarchies in S ( p
And use F [ S ( p s )] to denote the feature of S ( p s ). Our PI algorithm is just to extract a suitable F [ S ( p better represent S ( p s ) than f ( p s ). As mentioned above, F [ S ( p the pages in S ( p s ) because of the concept drift. Thus, we had better punish the contribution of the features of child pages. In our algorithm, there are two kinds of punishments.
If a page is far from the entry point of the sub-site, it will contain more post-imported materials. Thus, it should be punished more sharply to weaken the concept drift. In particular a ( D l ) is designed as a monoton-ically decreasing function and its range is between 0 and 1, e.g., where k is an adjustable variant and 0 6 k 6 1. For the entry point, the punished factor always equals to one since D l =0. (b) The feature of a page will be further punished by the number of its sibling pages.

In the real website, some page may have many child pages. In this case, even with the punishment in (a), the concept drift is still very remarkable. Therefore, we must repress the magnitude of the concept drift derived from the amount of the sibling pages, in order to guarantee that the total contributions of child pages to the sub-site will not overwhelm their parent page.

By jointly considering (a) and (b), the feature of sub-site S ( p
From (4) , we can see that the feature of a sub-site consists of two parts. The first part is the feature of the entry point of the sub-site, while the second part is the sum of the features of other pages in the sub-site with appropriate punishments. Considering that Eq. (4) may be a little complex, we take Fig. 4 for example to illus-trate it as below.
 For sub-site S ( p 0,0,0 ), there is no child pages, and so
For sub-site S ( p 0,0 ), there are three first-generation child pages, and h ( p Similarly, for sub-site S ( p 0 ), we have
After feature extraction, we can use a certain relevant function to calculate the relevance score for each sub-site. For example, we can choose the BM25 function ( Robertson, 1997; Robertson &amp; Sparck Jones, 1976 ) because it is widely used in many kinds of previous work and always gives fairly good performances. Note the two main features used in BM25 are term frequency and document length. To apply sub-site retrieval with
BM25 ranking function, we should also extract these two features for each sub-site. 3.3. Search results visualization
After calculating the relevance scores for sub-sites to a given query, we can choose to only return the entry pages of these sub-sites in order to keep the same user interface with the traditional search engines. Also, we can choose to design a dedicated user interface for sub-site retrieval. For example, we can use the user interface as shown Fig. 5 for browsing search results.

As one can see, the new user interface has two columns. Each URL in the left column is the root page of a sub-site. These sub-sites are ranked according to their relevance score to the query. When user moves his mouse over an URL in the left column, the corresponding snippet of this URL becomes shadowed, and the right column shows the relevant pages with tree structure inside the sub-site. For example, in Fig. 5 , when the cursor is over the fourth sub-site with a URL of  X  X  X eic.usgs.gov X  X , the right column shows the inner struc-ture of this sub-site. Note that we do not show the complete structure of a sub-site in the right column.
Instead, we just visualize the most relevant part of it, i.e., the most relevant 10 of over 5000 pages in the sub-site  X  X  X eic.usgs.gov X  X . Moreover, we highlight each sub-tree inside the sub-site with different background. And these sub-trees are also ranked by their relevance. For example, the sub-tree  X  X  neic.usgs.gov/neis/qed/  X  X  is more relevant than  X  X  neic.usgs.gov/neis/epic/  X  X , and  X  X  X eic.usgs.gov/neis/epic/ X  X  is more relevant than  X  X  nei-c.usgs.gov/neis/qed/qed1.html  X  X . To smoothly pilot users from traditional search results visualization interface to our sub-site results interface, we make the new interface downwards compatible. If the user clicks the link  X  X  X urn off Sub-site view X  X  right aside the  X  X  X earch X  X  button, the right column is hidden, and the whole interface rolls back to the traditional one. 4. Experiments
In this section, we report the results of several experiments which were conducted to test the effectiveness and efficiency of sub-site retrieval for topic distillation. First, we will introduce the experimental settings and some implementation issues, then experimental results and discussions will be given. 4.1. Experimental settings
We chose the topic distillation task in TREC 2003 and TREC 2004 Web track as the benchmark. The cor-responding data set used is the  X  X .GOV X  X  collection, which was crawled in the year of 2002. It contains 1,247,753 documents. Within these documents, 1,053,111 are html files, which were used in our experiments. There are in total 50 topic distillation queries in TREC 2003 and 75 queries in TREC 2004.

To show the effectiveness of the sub-site retrieval, we used the same relevance ranking function BM25 ( Rob-ertson, 1997; Robertson &amp; Sparck Jones, 1976 ) to get retrieval lists based on both sub-site and single page based features. To construct the sitemap for each website, we adopted the method in ( Feng et al., 2005;
Qin et al., 2005 ). To be noted, for simplicity, we did not parse the web pages into title, body, anchor and so on. Instead we use the query term frequency in the free text of the whole html file as the feature for each web page. And although we use the sub-site as the basic searching unit, when doing quantitative evaluations, the performance of our proposed method, we used mean average precision (MAP) and precision at 10 (P@10) ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ) as the metrics in our experiments.

As the reference, the baseline method (single page based retrieval with BM25 ranking function) has MAP of 0.1237 and P@10 of 0.110 on TREC 2003, and MAP of 0.1324 and P@10 of 0.1693 on TREC 2004. 4.2. Average accuracy of sub-site retrieval
Fig. 6 shows the performance of our proposed sub-site retrieval method on topic distillation task of TREC 2003.

As we can see from Fig. 6 , both MAP and P@10 of sub-site retrieval are always better than the single page based baseline. The highest MAP of 0.1531 (the corresponding P@10 is 0.12) is achieved when k = 0.5. This equals to 20% improvement over the baseline. Furthermore, this result has been almost as good as the best result reported by TREC 2003 participants (with MAP of 0.1543), even if sub-site retrieval does not utilize any information about hyperlinks and web page layout.

Furthermore, as shown in Table 2 , the basic idea of our work, Brin and Page X  X  work (1998) , and Shakery and Zhai X  X  methods (2003) is the same. As a comparison algorithm, we also implemented the three relevance propagation models in our experimental platform according to the description of Shakery and Zhai (2003) .
We listed the best performance (MAP and P@10) of both the sub-retrieval model and relevance propagation models in Table 3 . We can see that our sub-site retrieval model outperforms all the three cases of relevance propagation models, which shows the effectiveness of the concept of sub-site retrieval.
 Fig. 7 shows the performance of sub-site retrieval on topic distillation task of TREC 2004. From Fig. 7 , the
MAP is almost always better than the baseline on TREC 2004. However, the P@10 overcomes the baseline only when k 6 0.7. Since k is the level decrease factor, this means that features propagated from children to parent should not be too large.

We list the best performance (MAP and P@10) for TREC 2004 of both the sub-retrieval model and rele-vance propagation model in Table 4 . Similar to Table 3 , our sub-site retrieval model get the best performance of MAP. Although the sub-site retrieval model do not overcome all the three cases of relevance propagation model for P@10, it is better than two of the three. Again, this approves the value of sub-site retrieval, not to mention the subjective benefit of introducing the new user interface. 4.3. Case study
Although the average accuracy of sub-site retrieval is better than single page based retrieval, sub-site retrie-val does not beat the baseline for every query. Here we show some good and bad examples from TREC 2003 topic distillation task.

Firstly, we consider a good example of sub-site retrieval model, the query  X  X  X rains railroads X  X . Table 5.1 lists the top 10 pages returned by BM25, and Table 5.2 lists the top 10 pages returned by sub-site retrieval model with k = 0.4. The bold rows are the relevant pages labeled by TREC committee. Note that for BM25, there is no relevant pages in top 10 results, which means that P@10 = 0 for this query. However, for sub-site retrieval model, there are two relevant pages, and P@10 = 0.2.

Nine of the top 10 pages (except the first page) in Table 5.1 have no child pages to support them, and so they cannot be ranked in top positions with our sub-site retrieval model. Although the first page ( http://nic-kles.senate.gov/ ) has many child pages, none of its child pages are relevant to the query, and so it cannot be ranked in top 10 positions with our sub-site retreival model either.

On the contrary, 8 of the 10 pages (except the 4th and the 10th pages) in Table 5.2 have child pages to sup-port them, and so they get high ranking scores with our sub-site retrieval model. To show that why the two labeled pages can be ranked in top 10 positions, we furthur list the BM25 ranking scores of their child pages in Tables 6.1 and 6.2 . As can be seen from Table 6.1 , both the two child pages of http://www.fta.dot.gov/ research/rsafe/rsafe.htm are very relevant to the query, and so it get the first position in sub-site retrieval model (as a compassion, it is ranked at the 35th position by the single page based BM25 function). The case for the page http://www.wutc.wa.gov/web1/wol/index.html is quite similar. Note that since some child pages of http://www.wutc.wa.gov/web1/wol/index.html are irrelevant (with rank position larger than 1000), thus it is eventually ranked after the page http://www.fta.dot.gov/research/rsafe/rsafe.htm .

Then we consider a bad example of sub-site retrieval model, the query  X  X  X ining gold silver coal X  X . Tables 7.1, 7.2 and 7.3 show the top 10 results of BM25, top 10 results of our sub-site retrieval model and ground truth provided by TREC committee separately. BM25 gets a relevance page in the 9th position, but our sub-site retrieval model gets no relevant pages in top 10 results. Note that 8 of the 10 pages in Tables 7.1 and 7.2 are the same, with only the different order. That is, sub-site retrieval model only adjusts a little for this query comparing with the baseline. The relevant page, http://www.itds.treas.gov/prec_metals.htm , has not been ranked in top 10, since it has not child pages to support it. From this example, we can see that if the relevant page has no child pages, sub-site retrieval model will perform badly on such queries. 4.4. Complexity analysis
In the previous subsections, we investigate the effectiveness of sub-site retrieval. However, for real-world applications, efficiency is another important factor in addition to effectiveness. In this regard, we evaluate the efficiency of sub-site retrieval in this section to see its potential of being used in search engines. Roughly speaking, typical architecture of a search engine has three components ( Baeza-Yates &amp; Ribeiro-nologies into search engine, we should consider these three components. Clearly, we could only embed sub-site retrieval into the second or third component. Note that the search engine indexes the Web offline, and con-ducts ranking with respect to the input query online. Since a real search engines should handle hundreds of ment sub-site retrieval online. So offline implementation is much more preferred if we want to apply them in real-world applications.

Search engines usually build offline inverted and forward indices to store the information of each term (including frequency, font size and so on) in web pages ( Baeza-Yates &amp; Ribeiro-Neto, 1999; Brin &amp; Page, 1998 ). In sub-site retrieval model, each web page corresponds to a sub-site, and so the inverted and forward indices should be revised to store term information in each sub-site. In fact, for a website, we can get the fea-tures of each sub-site in the website by propagating the features of each page to its parent page only once with bottom-up manner. Suppose the child pages of page p contain a particular word, and we need to propagate the occurrence frequency of this word to page p .If p already contains this particular word, we only need to modify its frequency; while if p does not contain the word, we need to add its ID to the forward index ( Brin &amp; Page, 1998 )ofpage p , and then update its term frequency. Comparatively, the relevance propagation models ( Sha-kery &amp; Zhai, 2003 ) could hardly be integrated into search engines, because scores do not exist in the offline indices but are dependent on the online relevance ranking algorithm used in the search engine.
Although sub-site retrieval has the potential to be integrated into search engines, whether it can finally sur-vive will depend on its efficiency. As we know, the update cycle of the indices in state-of-the-art search engines is very short, if sub-site retrieval model cannot fit the need of such frequent update, it still cannot be used. To gain more understanding of this issue, we further conduct an efficiency analysis.

To re-build the index for sub-site retrieval, we need to propagate all the unique words in each page to its parent, and the time complexity is qnt , where q is the average number of unique words per page, n is the total number of pages in the data corpus, and t is the time complexity of propagating an entity (the frequency of a term) from a page to its parent. According to ( Baeza-Yates &amp; Ribeiro-Neto, 1999; Brin &amp; Page, 1998 ), the average size of a web page is about 5 X 6K bytes. Therefore it is reasonable to assume that q 100. Further-more, as we know, typical search engines currently indexes 5 X 8 billion pages. So we let n =8  X  10 experiments (a PC with 1.5 GHz CPU and 2GB memory), it cost about 2.8 ms to propagate one word on a dataset with 10,000 pages. Then accordingly, we can estimate that it will take about 2.8  X  8  X  10 10,000 ms (or 62.2 h, or 2.6 days) to re-index the 8-billion pages. Obviously, this is an acceptable time com-plexity. Because the propagation only happens inside a website, the re-index process can be implemented par-allel in multiple websites, which can further reduce the time cost.

Based on the above discussions, we can come to the following conclusions: (1) Sub-site retrieval model can be integrated into the offline indexing component of real-world search (2) The time complexity of re-build index for sub-site retrieval is acceptable. (3) The re-index process for sub-site retrieval can be implemented parallel. 5. Conclusions
Topic distillation is one of the main information needs when users search the Web. It aims to find some entry points of websites highly relevant to the query. In previous approaches to topic distillation, the single page was treated as the basic searching unit. This strategy is inherited from general information retrieval, which has not fully utilized the structure information of the Web. According to the requirement of topic dis-tillation, the answer page to a topic distillation query should be the entry page which is highly relevant to the query. That is, not only the answer page should be relevant to the query, but also its child pages should be relevant to the query. In this paper, we focus on how to boost the performance of topic distillation with the help of sitemap structure. Our contribution includes: (1) We propose a novel concept for topic distillation, named sub-site retrieval, in which the basic searching (2) We provide the site expansion model to simulate the growth of a website, which is topic oriented. (3) With this website growth model, we propose an algorithm named Punished Integration for sub-site (4) To make sub-site retrieval practicable in real-world search engine, we further design a new user interface (5) We verified that the proposed sub-site retrieval approach leads to significant improvement of retrieval (6) We evaluated the time complexity of sub-site retrieval, which shows that sub-site retrieval can be inte-References
