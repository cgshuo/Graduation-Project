 Carnegie Mellon University manasp@cs.cmu.edu In recent years, individuals and corporate entities have gathered large quantities of personal data. Often, they may wish to contribute the data towards the computation of functions such as various the privacy of the individuals by releasing sensitive information such as their medical or financial records, addresses and telephone numbers, preferences of various kinds which the individuals may publicly available auxiliary information can still recover the information about individual, as was the case with the de-anonymization of the Netflix dataset [1].
 In this paper, we address the problem of learning a classifier from a multi-party collection of such private data. A set of parties P 1 ,P 2 ,...,P K each possess data D 1 ,D 2 ,...,D K . The aim is to learn a classifier from the union of all the data D 1  X  D 2 ...  X  D K . We specifically consider a logistic regression classifier, but as we shall see, the techniques are generally applicable to any classification algorithm. The conditions we impose are that (a) None of the parties are willing to share the data with one another or with any third party ( e.g. a curator). (b) The computed classifier cannot be reverse engineered to learn about any individual data instance possessed by any contributing party. The conventional approach to learning functions in this manner is through secure multi-party com-putation (SMC) [2]. Within SMC individual parties use a combination of cryptographic techniques and oblivious transfer to jointly compute a function of their private data [3, 4, 5]. The techniques typically provide guarantees that none of the parties learn anything about the individual data besides what may be inferred from the final result of the computation. Unfortunately, this does not satisfy condition (b) above. For instance, when the outcome of the computation is a classifier, it does not prevent an adversary from postulating the presence of data instances whose absence might change the decision boundary of the classifier, and verifying the hypothesis using auxiliary information if any. Moreover, for all but the simplest computational problems, SMC protocols tend to be highly expensive, requiring iterated encryption and decryption and repeated communication of encrypted partial results between participating parties.
 An alternative theoretical model for protecting the privacy of individual data instances is differential privacy [6]. Within this framework, a stochastic component is added to any computational mecha-nism, typically by the addition of noise. A mechanism evaluated over a database is said to satisfy differential privacy if the probability of the mechanism producing a particular output is almost the same regardless of the presence or absence of any individual data instance in the database. Dif-ferential privacy provides statistical guarantees that the output of the computation does not carry information about individual data instances. On the other hand, in multiparty scenarios where the data used to compute a function are distributed across several parties, it does not provide any mech-anism for preserving the privacy of the contributing parties from one another or alternately, from a curator who computes the function from the combined data.
 We provide an alternative solution: within our approach the individual parties locally compute an optimal classifier with their data. The individual classifiers are then averaged to obtain the final ag-gregate classifier. The aggregation is performed through a secure protocol that also adds a stochastic private, i.e. , no inference may be made about individual data instances from the classifier. This procedure satisfies both criteria (a) and (b) mentioned above. Furthermore, it is significantly less expensive than any SMC protocol to compute the classifier on the combined data.
 We also present theoretical guarantees on the classifier. We provide a fundamental result that the excess risk of an aggregate classifier obtained by averaging classifiers trained on individual subsets, compared to the optimal classifier computed on the combined data in the union of all subsets, is bounded by a quantity that depends on the size of the smallest subset. We prove that the addition of the noise does indeed result in a differentially private classifier. We also provide a bound on the true excess risk of the differentially private averaged classifier compared to the optimal classifier trained on the combined data. Finally, we present experimental evaluation of the proposed technique on a UCI Adult dataset which is a subset of the 1994 census database and empirically show that the differentially private classifier trained using the proposed method provides the performance close to the optimal classifier when the distribution of data across parties is reasonably equitable. In this paper, we consider the differential privacy model introduced by Dwork [6]. Given any two databases D and D 0 differing by one element, which we will refer to as adjacent databases , a randomized query function M is said to be differentially private if the probability that M produces a response S on D is close to the probability that M produces the same response S on D 0 . As the query output is almost the same in the presence or absence of an individual entry with high probability, nothing can be learned about any individual entry from the output.
 Definition A randomized function M with a well-defined probability density P satisfies -differential privacy if, for all adjacent databases D and D 0 and for any S  X  range ( M ) , In a classification setting, the training dataset may be thought of as the database and the algorithm learning the classification rule as the query mechanism. A classifier satisfying differential privacy implies that no additional details about the individual training data instances can be obtained with certainty from output of the learning algorithm, beyond the a priori background knowledge. Differ-ential privacy provides an ad omnia guarantee as opposed to most other models that provide ad hoc guarantees against a specific set of attacks and adversarial behaviors. By evaluating the differentially private classifier over a large number of test instances, an adversary cannot learn the exact form of the training data. 2.1 Related Work Dwork et al. [7] proposed the exponential mechanism for creating functions satisfying differential privacy by adding a perturbation term from the Laplace distribution scaled by the sensitivity of the function. Chaudhuri and Monteleoni [8] use the exponential mechanism [7] to create a differen-tially private logistic regression classifier by perturbing the estimated parameters with multivariate Laplacian noise scaled by the sensitivity of the classifier. They also propose another method to learn classifiers satisfying differential privacy by adding a linear perturbation term to the objective func-tion which is scaled by Laplacian noise. Nissim, et al. [9] show we can create a differentially private function by adding noise from Laplace distribution scaled by the smooth sensitivity of the function. While this mechanism results in a function with lower error, the smooth sensitivity of a function can be difficult to compute in general. They also propose the sample and aggregate framework for replacing the original function with a related function for which the smooth sensitivity can be easily computed. Smith [10] presents a method for differentially private unbiased MLE using this framework.
 All the previous methods are inherently designed for the case where a single curator has access to the entire data and is interested in releasing a differentially private function computed over the data. To the best of our knowledge and belief, ours is the first method designed for releasing a differentially private classifier computed over training data owned by different parties who do not wish to disclose the data to each other. Our technique was principally motivated by the sample and aggregate framework, where we considered the samples to be owned by individual parties. Similar to [10], we choose a simple average as the aggregation function and the parties together release the perturbed aggregate classifier which satisfies differential privacy. In the multi-party case, however, adding the perturbation to the classifier is no longer straightforward and it is necessary to provide a secure protocol to do this. The problem we address is as follows: a number of parties P 1 ,...,P K possess data sets D 1 ,...,D K regression classifier on the combined data such that no party is required to expose any of its data, and the no information about any single data instance can be obtained from the learned classifier. The protocol can be divided into the three following phases: 3.1 Training Local Classifiers on Individual Datasets weights  X  w j . This is obtained by minimizing the following objective function where  X  &gt; 0 is the regularization parameter. Note that no data or information has been shared yet. 3.2 Publishing a Differentially Private Aggregate Classifier The proposed solution, illustrated by Figure 1, proceeds as follows. The parties then collaborate to compute an aggregate classifier given by  X  w s = 1 K P j  X  w j +  X  , where  X  is a d -dimensional random variable sampled from a Laplace distribution scaled with the parameter 2 n min j n j . As we shall see later, composing an aggregate classifier in this manner incurs only a well-bounded excess risk over training a classifier directly on the union of all data while enabling the parties to maintain their privacy. We also show in Section 4.1 that the noise term  X  ensures that from the aggregate classifier. The definition of the noise term  X  above may appear unusual at this stage, but it has an intuitive explanation: A classifier constructed by aggregating locally trained classifiers is limited by the performance of the individual classifier that has the least number of data instances. This will be formalized in Section 4.2. We note that the parties P j cannot simply take their individually trained classifiers  X  w j , perturb them with a noise vector and publish the perturbed classifiers, because aggregating such classifiers will not give the correct  X   X  Lap 2 / ( n (1)  X  ) in general. Since individual parties cannot simply add noise to their classifiers to impose differential privacy, the actual averaging operation must be performed such that the individual parties do not expose their own classifiers or the number of data instances they possess. We therefore use a private multiparty protocol, interacting with an untrusted curator  X  X harlie X  to perform the averaging. The outcome of the protocol is such that each of the parties obtain additive shares of the final classifier  X  w , such that these shares must be added to obtain  X  w s . Privacy-Preserving Protocol We use asymmetric key additively homomorphic encryption [11]. A desirable property of such schemes is that we can perform operations on the ciphertext elements which map into known op-erations on the same plaintext elements. For an additively homomorphic encryption function  X  (  X  ) ,  X  ( a )  X  ( b ) =  X  ( a + b ) ,  X  ( a ) b =  X  ( ab ) . Note that the additively homomorphic scheme employed here is semantically secure, i.e., repeated encryption of the same plaintext will result in different ciphertexts. For the ensuing protocol, encryption keys are considered public and decryption keys are privately owned by the specified parties. Assuming the parties to be honest-but-curious, the steps of the protocol are as follows.

Stage 1. Finding the index of the smallest database obfuscated by permutation.
Stage 2. Obliviously obtaining encrypted noise vector from the smallest database.
Stage 3. Generating secret additive shares of  X  w s . The above protocol ensures the following (a) None of the K +1 participants, or users of the perturbed aggregate classifier can find out the size of any database, and therefore none of the parties knows who contributed  X  (b) Neither Charlie nor any of the parties P j can individually remove the noise  X  after the additive shares are published. This last property is important because if anyone knowingly could remove the noise term, then the resulting classifier no longer provides differential privacy. 3.3 Testing Phase A test participant Dave having a test data instance x 0  X  R d is interested in applying the trained classifier adds the published shares and divides by K to get the differentially private classifier  X  w s . He can then compute the sigmoid function t = 1 t  X  1 2 and with label 1 if t &gt; 1 2 . 4.1 Proof of Differential Privacy We show that the perturbed aggregate classifier satisfies differential privacy. We use the follow-ing bound on the sensitivity of the regularized regression classifier as proved in Corollary 2 in [8] restated in the appendix as Theorem 6.1.
 Theorem 4.1. The classifier  X  w s preserves -differential privacy. For any two adjacent datasets D and D 0 , Proof. Consider the case where one instance of the training dataset D is changed to result in an adjacent dataset D 0 . This would imply a change in one element in the training dataset of one party and thereby a change in the corresponding learned vector  X  w s j . Assuming that the change is in the classifier by  X  w 0 j . In Theorem 6.1, we bound the sensitivity of  X  w j as k  X  w j  X   X  w 0 j k 1  X  2 n an argument similar to [7], considering that we learn the same vector  X  w s using either the training datasets D and D 0 , we have by the definition of function sensitivity. Similarly, we can lower bound the the ratio by exp(  X  ) . 4.2 Analysis of Excess Error In the following discussion, we consider how much excess error is introduced when using a per-turbed aggregate classifier  X  w s satisfying differential privacy as opposed to the unperturbed classifier w  X  trained on the entire training data while ignoring the privacy constraints as well as the unper-turbed aggregate classifier  X  w .
 We first establish a bound on the ` 2 norm of the difference between the aggregate classifier  X  w and the classifier w  X  trained over the entire training data. To prove the bound we apply Lemma 1 from [8] restated as Lemma 6.2 in the appendix. Please refer to the appendix for the proof of the following theorem.
 Theorem 4.2. Given the aggregate classifier  X  w , the classifier w  X  trained over the entire training data and n (1) is the size of the smallest training dataset, The bound is inversely proportional to the number of instances in the smallest dataset. This indicates that when the datasets are of disparate sizes,  X  w will be a lot different from w  X  . The largest possible value for n (1) is n K in which case all parties having an equal amount of training data and  X  w will be closest to w  X  . In the one party case for K = 1 , the bound indicates that norm of the difference would be upper bounded by zero, which is a valid sanity check as the aggregate classifier  X  w is the same as w  X  .
 We use this result to establish a bound on the empirical risk of the perturbed aggregate classifier  X  w s =  X  w +  X  over the empirical risk of the unperturbed classifier w  X  in the following theorem. Please refer to the appendix for the proof.
 Theorem 4.3. If all data instances x i lie in a unit ball, with probability at least 1  X   X  , the empirical regularized excess risk of the perturbed aggregate classifier  X  w s over the classifier w  X  trained over entire training data is
J (  X  w s )  X  J ( w  X  ) + The bound suggests an error because of two factors: aggregation and perturbation. The bound increases for smaller values of implying a tighter definition of differential privacy, indicating a clear trade-off between privacy and utility. The bound is also inversely proportional to n 2 (1) implying an increase in excess risk when the parties have training datasets of disparate sizes.
 In the limiting case  X   X  , we are adding a perturbation term  X  sampled from a Laplacian distri-bution of infinitesimally small variance resulting in the perturbed classifier being almost as same as using the unperturbed aggregate classifier  X  w satisfying a very loose definition of differential privacy. With such a value of , our bound becomes Similar to the analysis of Theorem 4.2, the excess error in using an aggregate classifier is inversely proportional to the size of the smallest dataset n (1) and in the one party case K = 1 , the bound becomes zero as the aggregate classifier  X  w is the same as w  X  . Also, for a small value of in the one party case K = 1 and n (1) = n , our bound reduces to that in Lemma 3 of [8], While the previous theorem gives us a bound on the empirical excess risk over a given training dataset, it is important to consider a bound on the true excess risk of  X  w s over w  X  . Let us denote the  X  J ( w  X  ) = E [ J ( w  X  )] . In the following theorem, we apply the result from [14] which uses the bound on the empirical excess risk to form a bound on the true excess risk. Please refer to the appendix for the proof.
 Theorem 4.4. If all training data instances x i lie in a unit ball, with probability at least 1  X   X  , the true excess risk of the perturbed aggregate classifier  X  w s over the classifier w  X  trained over entire training data is We perform an empirical evaluation of the proposed differentially private classifier to obtain a char-UCI machine learning repository [15] consisting of personal information records extracted from
Figure 2: Classifier performance evaluated for w  X  , w  X  +  X  , and  X  w s for different data splits vs. the census database and the task is to predict whether a given person has an annual income over $50,000. The choice of the dataset is motivated as a realistic example for application of data pri-vacy techniques. The original Adult data set has six continuous and eight categorical features. We use pre-processing similar to [16], the continuous features are discretized into quintiles, and each quintile is represented by a binary feature. Each categorical feature is converted to as many binary features as its cardinality. The dataset contains 32,561 training and 16,281 test instances each with 123 features. 1 In Figure 2, we compare the test error of perturbed aggregate classifiers trained over data from five parties for different values of . We consider three situations: all parties with equal datasets containing 6512 instances (even split, n (1) = 20% of n ), parties with datasets containing 4884, 6512, 6512, 6512, 8141 instances ( n (1) = 15% of n ), and parties with datasets containing 3256, 6512, 6512, 6512, 9769 instances ( n (1) = 10% of n ). We also compare with the error of the classifier trained using combined training data and its perturbed version satisfying differential privacy. We chose the value of the regularization parameter  X  = 1 and the results displayed are averaged over 200 executions.
 The perturbed aggregate classifier which is trained using maximum n (1) = 6512 does consistently better than for lower values of n (1) which is same as our theory suggested. Also, the test error for all perturbed aggregate classifiers drops with , but comparatively faster for even split and converges to the test error of the classifier trained over the combined data. As expected, the differentially private classifier trained over the entire training data does much better than the perturbed aggregate classifiers with an error equal to the unperturbed classifier except for small values of . The lower error of this classifier is at the cost of the loss in privacy of the parties as they would need to share the data in order to train the classifier over combined data. We proposed a method for composing an aggregate classifier satisfying -differential privacy from classifiers locally trained by multiple untrusting parties. The upper bound on the excess risk of the perturbed aggregate classifer as compared to the optimal classifier trained over the complete data without privacy constraints is inversely proportional to the privacy parameter , suggesting an inherent tradeoff between privacy and utility. The bound is also inversely proportional to the size of the smallest training dataset, implying the best performance when the datasets are of equal sizes. Experimental results on the UCI Adult data also show the behavior suggested by the bound and we observe that the proposed method provides classification performance close to the optimal non-private classifier for appropriate values of . In future work, we seek to generalize the theoretical analysis of the perturbed aggregate classifier to the setting in which each party has data generated from a different distribution. [1] Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In IEEE Sympo-[2] Andrew Yao. Protocols for secure computations (extended abstract). In IEEE Symposium on [3] Jaideep Vaidya, Chris Clifton, Murat Kantarcioglu, and A. Scott Patterson. Privacy-preserving [4] Jaideep Vaidya, Murat Kantarcioglu, and Chris Clifton. Privacy-preserving naive bayes classi-[5] Jaideep Vaidya, Hwanjo Yu, and Xiaoqian Jiang. Privacy-preserving svm classification. [6] Cynthia Dwork. Differential privacy. In International Colloquium on Automata, Languages [7] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensi-[8] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Neural [9] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in [10] Adam Smith. Efficient, differentially private point estimators. arXiv:0809.4794v1 [cs.CR] , [11] Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In [12] Mikhail Atallah and Jiangtao Li. Secure outsourcing of sequence comparisons. International [13] Michael Ben-Or, Shari Goldwasser, and Avi Widgerson. Completeness theorems for non-[14] Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. Fast rates for regularized objec-[15] A. Frank and A. Asuncion. UCI machine learning repository, 2010. [16] John Platt. Fast training of support vector machines using sequential minimal optimization. In
