 Mining frequent itemsets in data streams is beneficial to many real-world applications but is also a challenging task since data streams are unbounded and have high arrival rates. Moreover, the distribution of data streams can change over time, which makes the task of maintaining frequent itemsets even harder. In this paper, we propose a false-negative oriented algorithm, called TWIM, that can find most of the frequent itemsets, detect distribution changes, and update the mining results accordingly. Experimental results show that our algorithm performs as good as other false-negative algorithms on data streams without distrib-ution change, and has the ability to detect changes over time-varying data streams in real-time with a high accuracy rate.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithms, Performance, Experimentation Data stream, Frequent itemset
Mining frequent itemsets in data stream applications is beneficial for a number of purposes such as knowledge dis-covery, trend learning, fraud detection, transaction predic-tion and estimation. However, the characteristics of stream data  X  unbounded, continuous, fast arriving, and time-chang-ing  X  make this a challenging task. Existing mining tech-niques that focus on relational data cannot handle streaming data well [4].

Mining frequent itemsets is a continuous process that runs throughout a data stream X  X  life-span. Since the total number of itemsets is exponential, it is impractical to keep statistics for each itemset due to bounded memory. Therefore, usually only the itemsets that are already known to be frequent are recorded and monitored, and statistics of other infrequent itemsets are discarded. However, data streams can change over time. Hence, an itemset that was once infrequent can become frequent if a stream changes its distribution. It is hard to detect such itemsets that change from infrequent to frequent, since they are not maintained due to limited mem-ory. Furthermore, even if we could detect these itemsets, we would not be able to obtain their statistics (supports), since mining a data stream is a one-pass procedure and history information is not retrievable. Distribution changes over data streams might have considerable impact on the mining results, but few of the previous works have addressed this issue.

In this paper, we develop a new algorithm, called TWIM, that can find most of the frequent itemsets in real time. It can also predict the distribution change and update the min-ing results accordingly. Our approach maintains two tum-bling windows over a data stream: a maintenance window and a prediction window. All current frequent itemsets are recorded and maintained in the maintenance window, and we use the prediction window to keep track of candidates that have the potential of becoming frequent if the distri-bution of stream values changes. Every time the windows tumble, we check if new frequent itemsets and candidates should be added, and if some existing ones need to be re-moved from the lists. Since we do not keep statistics for every itemset within the windows, memory usage is limited. ExperimentalresultsshowthatTWIMisaseffectiveaspre-vious approaches for non-time-varying data streams, but is superior to them since it can also capture the distribution change for time-varying streams in real-time. Let I = { i 1 ,i 2 , ..., i n } be a set of items .A transaction T accesses a subset of items I  X  X  . A data stream is an unbounded sequence of tuples that continuously arrive in real time. In this paper, we are interested in transactional data streams, where each tuple corresponds to a transaction.
Let T t = { T 1 ,T 2 , ..., T N t } be the set of transactions at time t . N t is the total number of transactions received up to time t . The data stream that contains T t is denoted by D Note that the number of items, n , is finite and usually not very large, while the number of transactions, N t , will grow monotonically as time progresses.

Definition 1. Given a transaction T j  X  X  t , and a subset of items A X  X  ,if T j accesses A (i.e., A X  I j ), then we say T j supports A .

Definition 2. Let sup ( A ) be the total number of trans-actions that support A .If S ( A )= sup ( A ) /N t &gt; X  ,where is a predefined threshold value, then A is a frequent itemset in
D T t under current distribution. S ( A ) is called the support of
A .
Mining frequent items and itemsets is a challenging task and has attracted attention in recent years. Jiang and Gru-enwald [5] provide a good review of research issues in fre-quent itemsets and association rule mining over data streams. One of the classical frequent itemset mining techniques for relational DBMSs is Apriori [1], which is based on the heuris-tic that if one itemset is frequent, then its supersets may also be frequent. However, Apriori-based approaches suffer from a long delay when discovering large sized frequent itemsets, and may miss some frequent itemsets that can be easily de-tected using TWIM. Most of the techniques proposed in lit-erature are false-positive oriented. False-positive techniques may consume more memory, and are not suitable for many applications where accurate results, even if not complete, are preferred.
We propose an algorithm called TWIM that uses two tum-bling windows to detect and maintain frequent itemsets for any data stream. The algorithm is false-negative oriented: all itemsets that it finds are guaranteed to be frequent under current distribution, but there may be some frequent item-sets that it will miss. However, TWIM usually achieves high recall according to our experimental results.

We define a time-based tumbling window W M for a given data stream, which we call the maintenance window since it is used to maintain existing frequent itemsets.

Since data streams are time-varying, a frequent itemset can become infrequent in the future, and vice versa. It is easy to deal with the first case. Since we keep counters for all frequent itemsets, we can check their supports periodically (every time W M tumbles), and remove the counters of those itemsets that are no longer frequent. However, in the latter case, since we do not keep any information about the cur-rently infrequent itemsets, it is hard to tell when the status changes. Furthermore, even if we can detect a new frequent itemset, we would not be able to estimate its support, as no history exists for it.

To deal with this problem, we define a second tumbling window called the prediction window ( W P ) on the data stream. It keeps history information for candidate itemsets that have the potential to become frequent. The size of W P is larger than W M , and it is predefined based on system resources, the threshold  X  , and the accuracy requirement of the support computation for candidates. Note that we do not actually maintain W P ; it is a virtual window that is only used to keep statistics. Hence, the size (time length) of W P can be as large as required.
 Figure 1 demonstrates t he relationship between W M and W
P .InFigure1, W M and W P are the windows before tumbling, while W M and W P are windows afterwards. When the end of W M is reached, it will tumble to the new position W
M .Everytime W M tumbles, W P will tumble at the same time. This is to ensure that the endpoints of W M and W P are always aligned, so that frequent itemsets and candidate itemsets can be updated at the same time.

Mining frequent itemsets requires keeping counters for all itemsets; however, the number of itemsets is exponential. Figure 1: Tumbling windows for a data stream Consequently, it is not feasible to keep a counter for all of them, and thus, we only keep counters for the following:
Any itemset A with  X   X  S ( A ) &lt; X  is considered a candi-date and included in C .Here  X  is the support threshold for considering an itemset as a candidate. Every time W P tum-bles, we evaluate all candidates in C . If the counter of one candidate itemset is below  X  , it is removed from C and its counter is released.  X  is user defined: smaller  X  may result in a higher recall, but consumes more memory since more candidates are generated; a high  X  value can reduce memory usage by sacrificing the number of resulting frequent item-sets.

Every time W M and W P tumble, the counters of all can-didates and the supports of all items will be updated. If one candidate itemset A  X  X  becomes frequent, then  X A  X  A t , A = A  X  X  might be a candidate. Similarly, if one infrequent item i becomes frequent at the time windows tum-ble, then  X A  X  X  T t , A = { i } X  X  can be a candidate.
One simple solution is to add all such supersets A into the candidate list C . However, this will result in a large increase of the candidate list X  X  size, since the total number of A each A or { i } can be |A T t | in the worst case. The larger the candidate list, the more memory required for storing coun-ters, and the longer it takes to update the list when W M and W P tumble. Many existing frequent itemset mining techniques for streams are derived from the popular Apriori algorithm [1]. Apriori increases the size of candidate super-sets by 1 at every run, until the largest itemset is detected. This strategy successfully reduces the number of candidates; however, in cases when the itemset size |I| is large, it may take extremely long time until one large frequent itemset is detected. Furthermore, since Apriori-like approaches only check the supersets of the existing frequent itemsets, the subsets of existing frequent itemsets are not considered. To solve these problems, we introduce the concept of smallest cover set defined as follows.

Definition 3. Givenanitemsetlist A = {A 1 , A 2 , ..., A m for  X A = {A 1 , A 2 , ... A r } ,where A 1 , A 2 , ... A r A 2  X  ...  X  X  r = A 1  X  X  2  X  ...  X  X  m and r&lt;m ,thenwesay A is a cover set of A , denoted as A C .
 Definition 4. Givenanitemsetlist A and all its cover set call A C s the smallest cover set of A , denoted as A SC .
When a candidate itemset or an infrequent item becomes frequent, the candidate list can be expanded from either di-rection, i.e., combining the new frequent itemset with all current frequent items in A T t or with the smallest cover set of
A T t . The decision as to which direction to follow depends on the application. If the sizes of the potential frequent itemsets are expected to be large, then the smallest cover set could be a better option. On the other hand, if small sized frequent itemsets are more likely, then Apriori-like ap-proaches can be applied. However, in many real-world sce-narios, it is hard to make such predictions, especially when the distribution of the data streams is changing over time. Hence, we apply a hybrid method in our approach. Our hybrid candidate prediction technique is as follows. At the time W M and W P tumble:
Property: For each itemset A with size k that moves from infrequent to frequent at tumbling point t ,let C A be the list of new candidates generated using our hybrid approach at Step 5. Let |C A | be the number of itemsets in C A ,and  X  be the total time required for all frequent itemsets in C to be detected. We can prove that |C A | + 2 | W M |  X   X  2 where p is the total number of frequent items in A T t .(The proof is omitted due to page limit.)
Notice that p , i.e. the number of frequent items, is de-termined by the nature of the stream and is not related to the chosen mining method. This property indicates that the time and memory usage of our hybrid candidate generation approach are correlated. They are bounded to a constant that is not related to the size of minimal cover set A SC Hence, this nice property guarantees that the overall mem-ory usage of the proposed hybrid approach is small, and its upper bound is only determined by the number of frequent items in the stream.
Our candidate prediction technique uses smallest cover set of
A T t to discover the most number of frequent itemsets in the shortest time. In this section, we present an approxi-mate algorithm that can find a good cover set for a given frequent itemset list A T t efficiently in terms of both time and memory.
The run time of this algorithm in the worst case is ( |A T n ) in the stream.
For any itemset that changes its status from frequent to infrequent, instead of discarding it immediately, we keep it in the candidate list C for a while, in case distribution drifts back quickly and it becomes frequent again. Every time W
M and W P tumble, C is updated: any itemset A X  X  with S ( A ) &lt; X  along with its counter is removed, and new qualified itemsets are added resulting in the creation of new counters for them.

For an itemset A that has been in C for a long time, if it becomes frequent at time t i , its support may not be greater than  X  immediately, because the historical transac-tions (i.e., the transactions that arrive in the stream be-fore t i ) dominate in calculating S ( A ). Therefore, in order to detect new frequent itemsets in time, historical transac-tions need to be eliminated when updating S ( A ) for every A X  X  .Since W M and W P are time-based tumbling win-dows, they tumble every | W M | time units. Hence, we can keep a checkpoint every | W M | time intervals in W P , denoted as chk 1 , chk 2 , ..., chk p ,where chk 1 is the oldest checkpoint, and p = | W P | / | W M | .Foreach A X  X  ,werecordthenum-ber of transactions arriving between chk i  X  1 and chk i that access A .When W M and W P tumbles, sup ( A )isupdated by expiring transactions before chk 1 .

Every time W M tumbles, we update support values for all the existing frequent itemsets. If the support of an itemset A drops below  X  , then we move it from the set of frequent itemsets A T t to the candidate list C , and the counter used to record its frequency will be reset to zero, i.e. sup ( This is to ensure that, if the distribution change is not rapid, A may stay in the candidate list for some time, as its history record plays a dominant role in its support. By resetting its counter, we eliminate the effect of historical transactions and only focus on the most recent ones.

New frequent itemsets will come from either the infre-quent items or the candidate list. Since we keep counters for all items i  X  X  , when an item becomes frequent, it is easy to detect and its support is accurate. However, for a newly selected frequent itemset A that comes from can-didate list C , its support will not be accurate, as most of its historical information is not available. If we keep calcu-lating its support as S ( A )= sup ( A ) /N t ,where N t is the number of all transactions received so far, this S ( A ) will not reflect A  X  X  true support. Hence, we need to keep an offset for A , denoted offset ( A ), that represents the number of transactions that were missed in counting the frequency of
A . A  X  X  support at any time t &gt;t should be modified to
S ( A )= sup ( A ) / ( N t  X  offset ( A )), where N t is the to-tal number of transactions received at time t , as the data stream monotonically grows.
We conduct a series of experiments to evaluate TWIM X  X  performance in comparison with three others: SW method [2], which is a sliding window based technique suitable for dynamic data streams, FDPM [7], which is also a false-negative algorithm, and Lossy Counting (LC) [6], which is a widely-adopted false positive algorithm. We use synthetic data streams in our experiments to gain easy control over the data distributions. We adopt parameters similar to those used in previous studies [3, 7]. The total number of different items in I is 1000, and the average size of transactions in T is 8. The number of transactions in each data stream is 100,000.
We evaluate the effectiveness of the four algorithms over four data streams with Zipf-like distributions. Since FDPM and LC cannot deal with time-varying streams, to fairly compare effectiveness, the test data streams do not have dis-tribution changes. The objective of these experiments is to test the performance of TWIM over streams with stable dis-tribution. The results indicate that, that TWIM performs at least as well as existing algorithms on streams without distribution change.
To evaluate the effectiveness of the four algorithms with different values of threshold  X  , we apply TWIM, SW, FDPM and LC to a data stream with Zipf 1.2, and vary  X  from 0 . to 2%. The results demonstrate that the effectiveness of TWIM is comparable with FDPM when  X  varies. TWIM X  X  recall is improved with higher  X  . Although SW always has a better recall than TWIM and FDPM, its precision never reaches 1. LC has a low precision even when  X  is high (2%).
To evaluate the effectiveness of these three algorithms over time-varying data streams, we created two data streams D s and D f using the same statistics as in Section 4.1, with Zipf = 1.5 and 50,000 transactions in each stream. Both of the streams start changing their distributions every 10,000 transactions. The change of D s is steady and slow, whereas D f has a faster and more noticeable change.

The results show that TWIM and SW adapt to time-varying data streams, while neither FDPM nor LC is sen-sitive to distribution changes. SW performs worse than TWIM in both tests. Mining results of TWIM over the stream with faster and more noticeable changes are better than the one that changes slower, while SW seems more suit-able to slower and mild changes. However, note that we may improve the mining results of TWIM for such slow-drifting data streams by reducing the sizes of W M and W P . We test TWIM on D s and D f and vary  X  from 0 . 4% to 1%. It is shown that the performance of TWIM can be improved by decreasing  X  . However, a low  X  value may result in higher memory consumption.

To evaluate the effect of tumbling window sizes, we vary the size of W M from 200 transactions to 1000 transactions, and W P from 1000 transactions to 4000 transactions, and test TWIM on these two streams. We notice that larger windows size may reduce TWIM X  X  recall, since sudden dis-tribution changes will be missed. On the other hand, large windows can ensure high accuracy of the estimated supports for candidate itemsets.
The major memory requirements for TWIM are the coun-ters used for all items, frequent itemsets, and candidates. We compare the maximal number of counters that we cre-ate for each of the previous experiments. According to the results, the memory consumed by SW is about four times of TWIM X  X  memory usage. TWIM uses slightly more memory than FDPM, and LC has the lowest memory requirement. We also notice that the memory consumption is inversely correlated to threshold  X  , and larger windows sizes result in more counters to be used.
In this paper, we propose a novel algorithm called TWIM for mining frequent itemsets. Our approach has the ability to detect changes in a data stream and update mining re-sults in real-time. We use two tumbling windows to maintain current frequent itemsets and predict distribution changes. A list of candidate itemsets that have the potential to be-come frequent if distribution changes is generated and up-dated during mining. Every time the two tumbling windows move, we apply a set of heuristics to update the candidate list and maintain frequent itemsets. Our approach produces only true frequent itemsets, and requires less memory. Ex-perimental results demonst rate that TWIM has promising performance on mining data streams with or without distri-bution changes.

We are currently investigating a number of issues, includ-ing proving the complexity for finding the k th frequent item-set in a data stream, developing more heuristics for main-taining candidate itemsets, designing a more sophisticated and more efficient counting system, and analyzing the rela-tionship among thresholds, window sizes, and memory space for different applications.
