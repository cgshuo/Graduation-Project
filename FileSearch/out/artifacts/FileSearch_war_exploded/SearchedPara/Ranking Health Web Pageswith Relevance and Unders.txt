 We propose a method that integrates relevance and under-standability to rank health web documents. We use a learn-ing to rank approach with standard retrieval features to de-termine topical relevance and additional features based on readability measures and medical lexical aspects to deter-mine understandability. Our experiments measured the ef-fectiveness of the learning to rank approach integrating un-derstandability on a consumer health benchmark. The find-ings suggest that this approach promotes documents that are at the same time topically relevant and understandable.
An increasing number of people rely on online health infor-mation to understand and manage their health; this informa-tion is commonly accessed through search engines [4]. The retrieval of incorrect or unclear health information poses po-tential risks as people may dismiss serious symptoms, use in-appropriate treatments or unfoundedly escalate their health concerns about common symptomatology [1, 10]. However, an extensive number of studies has shown that the average user experiences difficulty in understanding the content of a large portion of the results retrieved by current search engine technology, e.g., see [11].

In the context of consumer health information seeking, search engines should not only retrieve relevant information, but they should also promote information that is under-standable by the user and that is reliable and verified [10]. This paper tackles one aspect of this problem by investigat-ing the effectiveness of a learning to rank approach aimed at retrieving documents that are at the same time topically relevant and understandable by the user. Specifically, we employ a range of standard retrieval features to capture in-formation about relevance, and exploit a number of readabil-ity features comprising of readability measures and medical lexical aspects to determine document understandability.
Through experiments on a consumer health search col-lection, we show that our approach improves health search results, demonstrating that the combination of retrieval fea-tures and readability features within a learning to rank ap-proach best promotes search results that are relevant and understandable for the user.
Our work tackles the problem of retrieving health informa-tion in answer to queries issued by laypeople. This problem has been largely investigated in the context of the CLEF eHealth Evaluation Lab 1 , from which we take the data to evaluate the proposed approach. Specifically, the 2015 task provides a test collection to evaluate the effectiveness of search engines in answering self-diagnosing queries [6]. The evaluation framework explicitly accounts for both the topi-cal relevance of the search results and their understandabil-ity, interpreted as how easy it is for a layperson to under-stand the content of a specific search result. This is done using understandability-biased evaluation measures, where gains obtained from relevant information are weighted by how hard it is for a layperson to understand that informa-tion [15, 16]. In this paper we use the CLEF eHealth 2015 test collection along with the explicit understandability as-sessments distributed and the understandability-biased RBP measure (see [6, 15, 16]). In addition, we further expand the understandability-biased evaluation framework by modify-ing the Bpref measure in the same spirit of understandability-biased RBP (see Section 4.4).

Our approach exploits a number of readability measures as features for the learning to rank approach. The mea-sures we employ are based on surface-level characteristics of text, such as characters, syllables and word counts [3]. For example, the Dale-Chall readability formula is based on a corpus of words that can be understood by fourth-grade students; the Flesch-Kincaid measure instead computes a readability score based on a weighted combination of the number of words and the number of syllables in a sentence. The Gunning-Fog index combines the intuitions of these two approaches using sentence length and frequency of  X  X om-plex X  words. Previous work that has explored the under-http://sites.google.com/site/clefehealth/ standability of the health content retrieved by search en-gines extensively relied on these measures (along with other surface-level measures we also use to compute readability features) [12]. Readability measures specific to the health domain have been proposed, e.g. [13]. These however rely on the mapping of the text content of documents to health terminologies and the assessment of readability based on hi-erarchy and relationships encoded in it: a computationally intensive and error prone process. We defer the use of these techniques to compute readability features in the context of learning to rank to future work. We do however use clini-cal terminologies (Mesh, ICD) and dictionaries (Drugbank, CHV) to compute readability by testing whether a word is present in such resources.

We are not the first to explore the use of readability fea-tures to improve search engine results. Collins-Thompson et al. have shown the benefits of personalising search results to the reading levels of individual users [2]. Similarly, Tan et al. have modelled both the comprehensibility of texts and the users reading proficiency to improve content rank-ing [9]. The understandability issue is crucial in consumer health search, and the application of readability measures has not been well explored yet. Zuccon et al. have encoded readability measures as language modelling priors but have found no improvements in search results [17]. In this paper, we investigate the application of new approaches to effec-tively include understandability in consumer health search.
In this work we study the effectiveness of a learning to rank approach that exploits retrieval features and readabil-ity features. The hypothesis is that the combined use of these feature sets not only improves results in terms of top-ical relevance, but promotes search engine results that are more understandable by the general public. Here, readabil-ity measures (and other features) are used as a proxy for document understandability. To validate this hypothesis, we investigated a number of retrieval and readability features, which we then alternated and combined to verify the con-tribution of each feature type to the improvement of search engine results.
 The features used in our investigation are summarised in Table 1. As retrieval features, we used the matching scores provided by a number of common retrieval models, along with query independent features and document score modi-fiers. To compute these features, we indexed two fields: the document titles only; and the entire document bodies.
As readability features, we used a large number of existing readability measures, as well as lexical and morphological measures. Readability features fall into four main categories: Traditional formulas: these are the existing well known readability formulas for general text. A thorough descrip-tion of these formulas can be found in [3].
 Surface measures: these are basic syntactic and lexical features, based on document statistics. Examples of this type of feature are the number of characters, syllables, words, and sentences present in a document. This category also includes the word length distribution in documents, e.g., #( | Word | &gt; 6) is the number of words in a document with more than 6 characters.
 General vocabulary related measures: these are com-mon lexical features used to assess text difficulty, e.g. pro-Table 1: Features used in the learning to rank pro-cess; the number of features for each group is re-ported in parenthesis. ? : scores from both titles and document bodies are used (thus doubling the num-ber of features).  X  : raw feature values and values normalised by number of words in a documents are used.  X  : raw feature values and values normalised by number of sentences in a document are used. portion of numbers, stopwords, and common words in doc-uments.
 Medical vocabulary related measures: these are lexi-cal and morphological features specifically adapted to the scientific domain, such as acronyms, or greco-latin affixes; or adapted to the medical domain, such as the number of terms present in lexicons such as Drugbank, Mesh or the
ICD. We also compute the number of terms from the Con-sumer Health Vocabulary (CHV) contained in documents, as well as the sum of the terms X  difficulty scores from CHV and the document mean score.
 For features in the last three categories, we computed their raw values, as well as their average values per sentence and per word, i.e., we divided the value of the feature by the number of words and sentences in the document.
To investigate methods that provide users with search re-sults that are both topical and understandable in answer to health queries, we use the CLEF 2015 eHealth Evaluation Lab collection [6]. This collection contains approximately 1 million web pages and 66 queries from people seeking self-diagnosing information. A key aspect of this collection is that it contains explicit graded assessments of both the top-ical relevance of documents to queries and the understand-ability of documents, which indicates whether a document is hard to read (label 0), somewhat hard to read (1), some-what easy to read (2), and easy to read (3). Almost 70% of the documents are judged as easy or somewhat easy to read, and just 2% are both highly relevant and easy to read.
We used the learning to rank framework provided in Ter-rier 4.0 with Jforest and LambdaMART [5]. To extract readability features, we preprocessed the documents using boilerplate following the methods by Palotti et al. [7]. To learn and evaluate the learning to rank models, we used a leave-one-out approach where we train models using 60 top-ics, validate them using 5 topics to optimise the number of iterations and finally test with the learned model using a single topic; this process is repeated to test on all 66 topics. We set nDCG as the metric to optimize as it considers differ-ent grades of relevance, and we explored five functions f ( d ) for the relevance label assigned to each document d in the training and validation steps (listed in Table 2). ReadTRel is the direct product combination of topical relevance and understandability, which would assign label 0 (i.e., not rel-evant) to documents that have understandability of 0 (i.e., very hard to read). As this renders irrelevant those relevant documents with an understandability score of 0, we designed the function readP 1 TRel in which the relevance of very hard to understand documents has a not null contribution to the score of the documents.
We first tested retrieval and readability features 2 rately; we then evaluated the effectiveness of their combina-tion. The two left-most plots of Figure 1 report the values of P@10 and MAP obtained by the learning to rank ap-proaches. A baseline system based on BM25 with param-eters set to their default values in Terrier is also shown as a horizontal line. Note that P@10 was the primary mea-sure used in CLEF 2015. As would be expected, the results show that retrieval features contributed more than readabil-
I n these experiments, we included in the readability fea-tures also the raw retrieval score of the baseline (BM25), but not all other retrieval features.
 T able 2: The five variants used for document labels for the learning to rank approach. ity ones to increase the relevance of document ranking. How-ever, the best P@10 and MAP were obtained when both feature types were combined, suggesting that (1) learning to rank using both feature types improves the general re-sult ranking, and (2) readability features improve relevance-based ranking.
In this section, we study effectiveness according to the understandability-biased evaluation. Figure 1 reports the value of uRBPgr, a graded version of RBP where the gain of a document is a joint function of the relevance label and the understandability label [15, 16]. The persistency pa-rameter of RBP (  X  ) was set to 0.8 (as in previous work [15, 16]). The results show that retrieval features alone did not improve uRBPgr; neither did the readability features alone. Their combination however improved uRBPgr, but not con-sistently across different label functions.

Further analysis of the results revealed that rankings ob-tained by the learning to rank models trained with read-ability only features contained many unassessed documents (on average only  X  79% of top 10 documents were assessed); while most of the documents obtained with retrieval features only or combination were assessed ones (  X  94% of the top 10 documents were assessed). Thus, results for readability only features may be affected by the lower coverage of the assess-ments. To overcome this limitation, we adopted a version of Bpref modified in the spirit of the understandability-biased evaluation framework (uBpref). Bpref considers only doc-uments that have been explicitly assessed with respect to their relevance. uBpref also considers only assessed docu-ments (for relevance and for readability); the (binary) gain from the relevance status of an assessed document (0: irrel-evant, 1: relevant) is multiplied by the graded gain from the understandability assessment (with weights as in uRBPgr).
Figure 1 reports the results evaluated with uBpref. The results suggest that the effectiveness of learning to rank with readability only features was underestimated because of the many unassessed documents. Readability only features, in fact, led to increased uBpref over the baseline; often higher than the combination. While using all features did not lead to the highest uBpref, they provided consistent gains over the baseline across different label functions.
We performed a feature ablation study to analyse the im-pact of features on the effectiveness of systems. We experi-mented with the best two models from our previous experi-ment (Figure 1), using all features (combining retrieval and readability features) and using as document labels the prod-Table 3: Feature ablation study based on the two best methods in Figure 1. The best results are in bold. Diamonds, squares, triangles markers indicate statistical significance (paired t-test w.r.t. baseline) with p &lt; 0.05, p &lt; 0.1, p &lt; 0.15, respectively. Table 4: Effectiveness comparison of different sys-tem variations with respect to the rank obtained using understandability assessments. uct of topical relevance and readability scores ( readTRel and readP 1 TRel ). At each ablation step, we removed a fea-ture group from the readability features, and then learned and evaluated a new model. Retrieval results are shown in Table 3. When document labels were assigned using readTRel as a function, the removal of some feature groups improved results over the model that used all features. For example, removing surface features improved the results of P@10 from 0.3394 to 0.3606.
We analysed whether learning to rank did learn to prefer more understandable documents. We trained a model us-ing only readability features, not including the raw retrieval score of the baseline, and experimented with a different func-tion for document labels. We report Bpref for each system variation with respect to understandability assessments only, as we are only interested to know if the assessed documents were correctly ranked. Additionally, we compared the rank-ing generated by each system with the perfect order of docu-ments according to their understandability; for that, we used  X 
AP [14], as it is based on average precision and assigns more weight to differences in the top rankings 3 .

The learning to rank model learned how to better rank documents according to understandability when it was ex-clusively trained with functions based on understandability labels (see Table 4, second row). In this case, baseline effec-tiveness was improved by 5% for Bpref (where understand-ability assessments are used in place of relevance ones) and 7% for  X  AP . On the other hand, when the model was trained combining readability and retrieval features, document un-derstandability did not appear to be learnt.
We describe a method that integrates understandabil-ity in the ranking of search results for consumer health search. This method is based on a learning to rank ap-proach that combines features capturing topical relevance
Usually  X  AP is used to compare systems rankings; here we used it to compare documents rankings. and features measuring the readability of health documents. We found that the combination of retrieval features and readability features indeed did improve search engine re-sults, both for relevance and understandability retrieval measures. The provision of documents that are both rele-vant and understandable in answer to health related queries is an important requirement for next generation search en-gines [8]. Source code, all data analysed in this paper, and results (including other baseline models and evaluation measures) are available online at http://github.com/ielab/ sigir2016-ranking-relevance-understandability.

As future work, we want to perform a wider feature anal-ysis and evaluation. The results in Section 4.5, in fact, indi-cate that feature selection may improve system effectiveness. JP and AH were supported by Horizon 2020 program (H2020-ICT-2014-1) n o 644753 (KCONNECT). JP has also been sup-ported by the ESF project ELIAS.
