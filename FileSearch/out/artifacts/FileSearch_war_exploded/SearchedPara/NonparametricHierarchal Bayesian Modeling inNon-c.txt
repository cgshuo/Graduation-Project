 An important problem in the non-contractual marketing domain is discovering the customer lifetime and assessing the impact of customer's characteristic variables on the life-time. Unfortunately, the conventional hierarchical Bayes model cannot discern the impact of customer's character-istic variables for each customer. To overcome this problem, we present a new survival model using a non-parametric Bayes paradigm with MCMC. The assumption of a conven-tional model, logarithm of purchase rate and dropout rate with linear regression, is extended to include our assumption of the Dirichlet Process Mixture of regression. The exten-sion assumes that each customer belongs probabilistically to different mixtures of regression, thereby permitting us to estimate a different impact of customer characteristic vari-ables for each customer. Our model creates several customer groups to mirror the structure of the target data set.
The effectiveness of our proposal is con rmed by a com-parison involving a real e-commerce transaction dataset and an arti cial dataset; it generally achieves higher predictive performance. In addition, we show that preselecting the actual number of customer groups does not always lead to higher predictive performance.
 H.2.8 [ Database Management ]: Database Applications| Data Mining ; G.3 [ Mathematics of Computing ]: Prob-ability and Statistic| Survival Analysis, Probabilistic Algo-rithms 1 -1 Hikarinooka Yokosuka-Shi, Kanagawa 239-0847 Japan CRM, Model choice, Non-parametric Bayes, MCMC
The concepts of customer relationship management (CRM) have been recently gaining wide attention in business and academia [1][2]. This approach focuses on allocating re-sources to support business activities in order to gain a competitive advantage. CRM focuses on managing the rela-tionship between a company and its current and prospective customers. A good relationship with the customer leads to higher customer value.

Estimating customer lifetime and the impact of the cus-tomer's characteristic variables on pro table lifetime is an important goal in CRM marketing. Historically, survival analysis has usually been carried out by applying statistical models.

In the non-contractual marketing domain, we cannot ob-serve the customer's dropout, e.g. e-commerce site, brick-and-mortar shop and free web service, customers can halt their ow of transactions with no explicit noti cation of their dropout.

This problem was rst recognized by Schmittlein, Morri-son, and Colombo [3]. They proposed ParetoNBD model; it estimates pro table lifetime using Recency-Frequency(RF) data. RF data includes purchase frequency, day of the rst purchase, and day of the last purchase. This model is at-tracting the attention of researchers and practitioners be-cause of its increasing importance in new types of market-ing, such as CRM, and One-to-One Marketing. Their work is highly regarded and follow up research has been con-ducted [4][5][6][7][8]. Abe [8] proposed a hierarchical Bayes extension to the Pareto/NBD model to estimate the impact of the customer's characteristic variables on pro table life-time duration. The hierarchical Bayes model(HB model), whose lifetime parameter is a function of customer charac-teristics, can achieve this in one step.

HB model assumes a single functional relationship be-tween lifetime parameters and the customer characteristics; so a single set of coefficients (impact of the customer's char-acteristic variables) is estimated using data from all cus-tomers in the sample. These coefficients are effective in sup-porting marketing decisions for average, but they do not su pport customized marketing decisions for individual cus-tomer. This is because the HB model pools the impact over all customers. The most recent trend in CRM is for per-sonalized actions, e. g. promotions or recommendations, for each customer, so estimating the impact of the customer's characteristic variables one by one is an important goal. It permits the identi cation of the most effective customers in terms of increasing lifetime. For example, Abe [8] dis-covered the marketing knowledge that keeping a food cor-ner fully stocked is effective in decreasing the dropout rate for the store. Unfortunately, this knowledge cannot identify the customers who could be prevented from dropping out through promotion of the food corner.

A simple solution is a model that includes as many coeffi-cients as their customers; each customer has one coefficient. However, this raises the identi cation problem (degree of freedom problem) in estimation. This is because a set of customer parameters and a set of customers' characteristic variables are needed to estimate a set of customer coeffi-cients.

To overcome this problem, we propose a new model that can estimate the impact of the customer's characteristic vari-ables one by one using a non-parametric Bayes paradigm. Our model is based on an HB model that includes a multiple coefficient to which each customer belongs probabilistically.
The key feature of this model is the mathematical pre-sentation of a dynamic coefficient distribution; it is based on the Dirichlet Process Mixture (DPM). DPM is a non-parametric Bayes model that can estimate both coefficients (parameters) and the number of coefficients, in a natural Bayesian paradigm. Accordingly, this model can provide dynamic coefficients without prior setting of the number of coefficients or coefficient parameters. In other words, our model makes the following assumptions; Finite coefficients are sampled from a potentially in nite set of unobserved coefficients, and each customer can be assigned to each co-efficient probabilistically as in soft clustering.
Our model has 2 merits at the practical level, and these merits are the innovations of our research. At rst, our model offers greater accuracy with trustful prior distribu-tions using multiple coefficients. Secondly, our model can discern the impact of the customer's characteristic variables for each customer.

The effectiveness of our proposal is con rmed by a com-parison involving a real E-commerce transaction dataset and an arti cial dataset.

The next section introduces related works on survival anal-ysis and mixture distribution analysis. Section 3 describes the proposed model and compares it against the conven-tional model. Section 4 explains how our model uses the MCMC method for making the estimations. Section 5 presents experimental result conducted on three datasets of various types; the model's performance is compared to that of the conventional model. Section 6 presents empirical analyses conducted on real e-commerce dataset. Section 7 presents the discussions followed by the conclusions in Section 8.
Schmittlein et al [7] calibrate a Pareto/NBD model sep-arately for each segment speci ed by the SIC code. The proposed model, by including segmentation variables in a hierarchical manner, allows estimation of all segments simul-taneously, thereby increasing the degrees of freedom. The model can also incorporate non-nominal explanatory vari-ables.

Reinartz et al [9] and Abe [8] proposed new models for estimating lifetime and the impact of the customer's char-acteristic variables. Reinartz and Kumar proposed a 2-step model, paretoNBD for lifetime and regression to discover the impact of the customer's characteristic variables. Abe proposed a hierarchical Bayes model and MCMC estima-tion which sets up the customer's characteristic variables as a prior distribution. Both approaches can provide lifetime and impact, but the latter approach, whose dropout param-eter is a function of customer characteristics, can achieve these in one step, thus providing correct error assessments for statistical inferencing. Accordingly, our model is based on the hierarchical Bayes approach.

Conventional models require that the number of compo-nents be given in preliminary step. (e. g. Schmittlein [7] sets the segmentation number in a preliminary step, while Reinartz et al [9] and Abe [8] set the number of components to 1.)
Our model sets the number of components as unknown, and each customer is assigned to each component proba-bilistically. This means model complexity (the number of component) must be estimated from the given data. Esti-mation of K , the number of components, is a special kind of model choice problem, for which there is a number of possible solutions [13] : Approach 1 The number of components is decided after Approach 2 The number of components is estimated as a
We focus on the latter, because it exempli es more natu-rally the Bayesian paradigm and offers a much wider scope for inferencing, including model averaging in the non-parametric approach to mix estimation 1 .

Approach 1 above pertains strongly to the testing per-spective, the entropy distance approach being based on the KL divergence between a K component mixture and its pro-jection on the set of K 1 mixtures, in the same spirit as Dupuis and Robert [16]. Additionally, this solution can not provide correct error assessments for statistical inference in practical tasks.

Our approach, the non-parametric extension of the HB model, can identify the impact of the customer's charac-teristic variables. If the number of components is always estimated to be 1, HB and our model are equivalent.
I n addition, the unusual topology of the parameter space invalidates standard asymptotic approximations of testing procedures [15] This section describe the assumptions of the conventional HB model (Conventional hereafter) and the proposed model (Proposed). Conventional has 3 assumption as follows. Assumption 1 Poisson purchases. While active, each cus-Assumption 2 Exponential life time. Each customer re-Assumption 3 Individuals' purchase rate and dropout
Assumptions 1 and 2 are identical to the behavioral as-sumptions of Pareto/NBD model, and their validity has been con rmed by other researchers. Assumption 3 is the addi-tional assumption of HB model. Proposed replaces assump-tion 3 as follows.
 Assumption 3a Individuals' purchase rate and dropout
To determine the impact of the customer's characteristic variables one by one, the model needs to set groups that use different parameters for their lognormal distributions. Un-like Conventional, Proposed sets multiple customer groups, and each group has a different lognormal distribution. In Proposed, a customer belongs to all groups probabilistically (like soft clustering).
Following past research [7], Figure 1 depicts our notations of recency and frequency data ( x; t x ; T ). Lifetime starts at time 0 (when the rst transaction occurs and/or the mem-bership starts) and customer transactions are monitored un-til time T . x is the number of repeat transactions observed in time period (0 ; T ), with the last purchase ( x th repeat) occurring at t x . Hence, recency is de ned as T t x . is unobserved customer lifetime. Using these mathematical no-tations, the preceding model assumptions can be expressed as follows: log ( ) log ( ) M N V 0 = ; 0 = MVN denotes a multivariate normal distribution. , the Poisson distribution parameter, is the purchase frequency per period while the customer is active. , the parameter of exponential distribution, is the dropout rate.

A model that link purchase rates and dropout rates, and , to customer characteristic can offer insights into the frequency of transactions and increasing the lifetime. The approach of Conventional is to use the logarithm of and as a linear regression as follows. where index i is added to emphasize that the rate parameters are for customer i . d i is a G 1 column vector that contains G characteristics of customer i . is a G 2 parameter vector and e is a 2 1 error vector that is normally distributed with mean 0 and variance 0 . This formulation replaces 0 in the previous section with 0 d i . When d i contains only a single element of 1 (i.e., an intercept only), this model reduces to the previous no-covariate case.

Note that Conventional, which uses a single , can not determine the impact of the customer's characteristic vari-ables one by one, therefore, Proposed sets multiple for multiple customer groups. The model sets an in nite num-ber of groups, and sets a different to each group that the user belongs to. We de ne the Dirichlet process (DP) as a prior distribution for unknown . Famous implementa-tions of DP are the stick-breaking process and the Chinese restaurant process (CRP). Our model adopts the latter, be-cause it readily suits the MCMC procedure. CRP handles a potentially in nite group mixture in theory, but it makes a number of groups according to the given data structure [12]. h i = c; i 2 1 ; :::; N ; c 2 1 ; :::; C means customer i belongs to the c th group, and is assigned c . The equation is as follows.
P ( h i = k j ; ) is expressed as follows; n is the number of customers, and n ( k ) is number of customers for which h = k .
 P ( h i = k j i ; i ) /
We set a unique distribution for P ( ; j h i = new ).
The likelihood function for RF data ( x; t x ; T ) is given by the following simple expression. z is an indicator function de ned as 1 if a customer is active at time T and 0 otherwise. Another latent variable is a dropout time, y , when z = 0. See Abe 2009 for details.

L ( x; t x ; T j ; ; z; y ) =
Because we observe neither z nor y , we treat them as missing data and apply a data augmentation technique [17]. To simulate z in our MCMC estimation procedure, we can use the following expression as the probability of a customer being active at T .

P ( z = 1 j ; ; t x ; T ) = 1
We are now in a position to estimate parameters, timate the joint destiny, we sequentially generate each pa-rameter, given the remaining parameters, from its condi-tional distribution until convergence is achieved. The pro-cedure is described below.
 Step 1 Set initial value for i ; 8 i .
 Step 2 Sample z i according to Equation 9, for each i . Step 3 Sample y i using truncated exponential distribution Step 4 Sample i with independent MH algorithm using Step 5 Sample h i and K using Equation 6 and 7 according Step 6 Sample k ; 0 with multivariate normal regression Step 7 Iterate Step2-Step6 until convergence is achieved. Each step is explained below.

Steps 2 and 3 generate z and y which are needed by Equa-tion 8 in step 4.

In step 4, given z i , and y i , Equation 8 is used to generate i and i , which are transformed into i by taking their logarithms. An independent Metropolis-Hasting algorithm is used to generate i rst then i ; the proposed distribution is lognormal. Unlike Conventional, Proposed uses of prior distribution for each i . h i is used for customer i . Step 5 is the additional model choice step of Proposed. Equations 6 and 7 are used to generate h i and K with CRP. In the rst MCMC cycle, none of the h i of customers are decided( h i = 0; 8 i ). so equation 7 is not used. The number of clusters is optimized for each CRP step using the given data.

As for step 6, see Bayesian textbooks elsewhere for details on multivariate normal regression update [19] [20] [21]. The hyper parameters of this lognormal distribution, and 0 , F igure 3: Graphical Model of RF Data Generation are estimated in a Bayesian manner with a multivariate nor-mal prior and an inverse Wishart prior, respectively:
These distributions are standard in Baysian regression [19] [20] [21]. We set a non-informative prior distribution for the hyper-parameters.
 Figure 2 show our graphical model. RF i include x and t . Figure 3 show the detail of RF i generation.
Proposed was evaluated in terms of the qualitative effec-tiveness of non-parametric coefficient division and compared to conventional methods.
 Evaluations that use arti cial purchase data have 2 merits. First, the arti cial data can be generated to cover various sit-uations and parameters, such as number of components( ) and degree of noise. Datasets used in the experiments are shown in "About Data". Second, arti cial data can set un-observable parameters, i.e. i , i , h i , and activeness at the end of calibration. A real dataset was also tested, results are in the next section, but it can not provide wide parameter coverage.

The MCMC steps were repeated for 15,000 iterations, of which the last 5,000 were used to infer the posterior distribu-tion of the parameters. Convergence was monitored visually and checked with the Geweke test [18].
The results of Proposed are compared with Conventional and K-given model. K-given model has the number of com-ponents, K , xed in advance. It assumes Dirichlet distri-butions for the prior distribution with which customer is a ssociated with a component. The appendix provides de-tails.

Accordingly, Conventional and K-given model are equiva-lent if K = 1, and Proposed and K-given model are equiva-lent if the number of distributions is actually K (If K is es-timated to be 1, Proposed and Conventional are also equiv-alent).

Proposed was compared against the benchmark models in terms of t in the calibration period and prediction in the validation period. As disaggregate performance measures, correlation and mean squared errors (MSE) between pre-dicted and actual, , , and each coefficient for individual customers were used. Additionally, loglikelihood of being active at the end of calibration was compared as permitted by the arti cial data. The experiment used 4 types of arti cial purchase data, Each dataset had 100 customers and was designed to exhibit 2 types of impact from characteristic variables. Dataset 1 has less white noise for and , and a mixture of 2 types of customers. 50 customers have h i = 1 ; 1 = (1 ; 1), and the other 50 customers h i = 2 ; 2 = ( 1 ; 1). White noise 2 and 2 are 0.1. Dataset 2 has stronger white noise for and , and a mixture of 2 types of customers. takes the same value as dataset 1. White noise and and a mixture of 4 types of customers. 50 customers have h = 1 ; 1 = (1 ; 1), 50 customers have h i = 2 ; 2 = (1 ; 4). 50 customers have h i = 3 ; 3 = ( 1 ; 1), and 50 customers have h i = 4 ; 4 = ( 1 ; 4). white noise and are 1. Dataset 4 has stronger white noise for and , and a mixture of 8 types of customers; they has 2 independent dimensions in d i . 50 customers have h i = 1 ; 1 = (1 ; 1; 1 ; 0), 50 cus-tomers have h i = 2 ; 2 = (1 ; 4; 1 ; 0). 50 customers have h = 3 ; 3 = ( 1 ; 1; 1 ; 0), 50 customers have h i = 4 ; ( 1 ; 4; 1 ; 0), 50 customers have h i = 5 ; 5 = (1 ; 0; 1 ; 1), 50 customers have h i = 6 ; 6 = (1 ; 0; 1 ; 4), 50 customers have h i = 7 ; 7 = ( 1 ; 0; 1 ; 1), and 50 customers have h i = 8 ; 8 = ( 1 ; 0; 1 ; 4). white noise and are 1. Every dataset includes just a single type of d i , to simplify the division problem; and are independent.

Datasets 1 and 2 have the same number of components, but different white noise variance, 0 . We can evaluate the models in terms of their effectiveness and robustness for Dataset 3 and 4 has a more complex tasks, since dataset 3 includes 4 components, and dataset 4 include 8 components in 2 dimensions for d i . We can evaluate model effectiveness on these complex cases.

Arti cial transaction datasets 1-4 were produced by the following steps. ( t x ; T ) are natural numbers. Step 1 Set single d i with normal distributions 2 for each Step 2 Set and with 0 h i d i + e . e represents white noise. Step 3 Set T with unique distribution 3 for each customer. Step 4 Set with exponential distributions using parame-mea ns are 3, and covariance of 1 we set 1 &lt; T &lt; 365 for datasets 1, 2 and 4, 182 &lt; T &lt; 365 for dataset 3, because dataset 3 was designed to exhibit frequency dropout. Step 5 Set x and t x with active period min ( ; T ) for Pois-Table 1 shows data details.

Figure 4, 5, 6 and 7 plot data distributions between d and .

Arti cial data are unrealistic values, for example, trans-a ction x is repeated over one thousand times. These ex-periment are intended to evaluate unobserved parameters (coefficient, lifetime, , ).
Table 2 lists the results for datasets 1-4. Mark ( y ) means actual number of components in K-given model. Mark (*) means the highest score. In the results for datasets 1 and 2, HB, K-given (who has actual number of components), and Proposed achieved higher evaluation scores in every cri-teria, MSE, correlation and loglikelihood. Looking at the results in more detail, K-given and Proposed are effective in terms of evaluating (2 different coefficients). On the other hand, Conventional is effective with regard to evaluation. Comparing datasets 1 and 2, K-given model shows lower cri-teria scores, so we can con rm the robustness of Proposed in terms of 0 .

Figure 8, 9, 10 and 11 are histograms of the number of components in MCMC as estimated by Proposed. Mark (*) means actual number of components. Proposed esti-mated that 2 components were predominate in dataset 1, and that 40% of the MCMC cycle were covered by 3 compo-nents (1 additional component) in dataset 2. As white noise 0 strengthened, Proposed created an additional temporary coefficient for robustness.
 In the results for dataset 3, K-given model (K=2) and Proposed showed higher evaluation scores in every crite-ria, MSE, correlation and loglikelihood. The more compo-nents the dataset included, the lower was the criteria score achieved by Conventional. It is natural to consider that K-given model is a generalization of Conventional (HB model equals K-given model when K=1). Focusing on the ac-tual number of components (K=4) K-given model, achieved lower criteria scores than when the false number of compo-nent (K=2) was used. This result indicates that an actual data generative model does not always t the data if the model uses multi-stage estimation like a survival model. For example, it is difficult to estimate the and of one shot customers (x=0). Survival models may perceive them to have large or small , The actual number of components models often fail in coefficient estimation because of their and value estimations. On the other hand, K=2, not the actual number of components, allowed the model to achieve higher prediction performance. This results from making the same temporary group of customers with fuzzy and values. The same problem of components can be seen in the results for dataset 4.

The discussion section addresses this problem.
The real database contained e-commerce transactions cap-Fi gure 8: Distributions of the Number of Compo-nent in Dataset 1 Fi gure 9: Distributions of the Number of Compo-nent in Dataset 2 Fi gure 10: Distributions of the Number of Compo-nent in Dataset 3 tured over a 162 week period (26/09/09{02/11/12) at com-mercial website. It includes data gathered by random sam-pling 3,000 customers who purchased at least one item from the website during the rst 81 weeks. The rst 81 weeks of data were used for model calibration and the second 81 weeks of data were used for model validation.

Proposed was compared against Conventional and K-given (K=2,4,8) in both the calibration period and the validation period (prediction performance). We used as disaggregate performance measures, correlation and mean squared errors Fi gure 11: Distributions of the Number of Compo-nent in Dataset 4 (MSE) between predicted and observed numbers of trans-actions for individual customers. As an aggregate measure, we used root mean square (RMS) between predicted and ob-served weekly cumulative transactions. These measures are generally used [8] in the evaluation of non-contractual sur-vival model. Characteristics of customer d include 4 types of value, amount of discount price, e-mail membership, av-erage price of transaction and intercept. For evaluating the effect of d , every model was compared to themselves with no characteristic except intercept.
Table 3 shows data details.
Figure 12 show the distribution of correlation between log ( ) and log ( ). The average of correlations is 0.131. Ta-ble 4 show the results for the real dataset. Conventional and Proposed yielded higher evaluation scores for the disaggre-gate performance measures. For aggregate measures, Con-ventional, K-given (K=4), and Proposed showed high and roughly equivalent evaluation scores. Conventional and Pro-posed have almost the same prediction performance, how-ever, Proposed performs better at aggregate tracking. This can be seen from the time-series tracking of the cumulative number of transactions in Figure 13. The line at week 81 separates the validation from the calibration period. On the other hand, Conventional performs better at disaggregate tracking.

Compared to their dummy characteristic model, they of-fered superior prediction performance as con rmed by the RMS values. The result suggest the effectiveness of the cho-sen characteristics. In particular, K-given (K=4) and Pro-posed achieved greater performance than Conventional when they use the characteristics of customer d as a prior distri-bution.

Furthermore, Proposed can extract the impact of the cus-tomer's characteristic variables. They can be used for ex-tracting a list of customer for which discounts are effective for extending customer lifetime or E-mail is effective for in-creasing transaction number.
We discuss the difficulty of deciding K and emphasize the importance of our proposal; the proposed model determines the number of components from the target dataset. The results for arti cial dataset 3 show that the K-given model achieved a lower criteria score for the actual number of com-ponents (K=4) than for an erroneous number of compo-Fi gure 12: Distribution of Correlation Between log ( ) and log ( ) for EC Data Fi gure 13: Weekly Time-series Tracking Plot for EC Data nents(K=2). This raises the question of the effectiveness of multiple stage estimation.
 Table 5 lists the results for dataset 3 for just k estimation. For this, actual and were input to MCMC, steps 5 and 6. The K-given model and Proposed achieved higher criteria scores for the actual number of components (K=4) in this additional experiment. This is different from the previous result. Thus only the proposed model could achieve high criteria scores in both cases, single and multiple estimation.
These results indicate that the survival model approach has difficulty in achieving high prediction performance with-out using DPM in multiple stage estimation, even if the user (researcher) has actual knowledge of customer group num-ber.
In this paper, we introduced the goals of discovering cus-tomer lifetime and the impact of the customer's character-istic variables on lifetime duration for each customer. We developed a nonparametric mixture model to achieve both goals. We extended the assumption of the conventional model, logarithm of and with linear regression, by the additional assumption of DPM of regression. It assesses the structure of the target data set and determines the number of groups that yield high prediction accuracy automatically.
Experiments on arti cial datasets showed the effectiveness and robustness of our model, and the results for a real data set showed the superior prediction performance of our model Table 5: Results of Additional Experiment on Data 3 wit h chosen characteristics, over the conventional HB model and the parametric Bayes (K-given) model. Additionally, we showed that actual number of components given models do not always suit multiple stage estimation. [1] Rust R. T. and T. S. Chung, Marketing models of [2] Sun B., Technology innovation and implications for [3] Schmittlein D. C., D. G. Morrison, and R. Colombo , [4] Fader P. S., B. G. S. Hardie and K. L. Lee, Counting [5] Fader P. S., B. G. S. Hardie and K. L. Lee, RFM and [6] Reinartz W. J. and V. Kumar, On the pro tability of [7] Schmittlein D. C. and R. A. Peterson, Customer base [8] Abe, M., "Counting Your Customers" One by One: A [9] Reinartz W. J., and V. Kumar, "The Impact of [10] D. Blackwell and J. B. MacQueen, "Ferguson [11] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. [12] J. Pitman, Combinatorial Stochastic Processes, [13] Marin J. M., Mengersen K. and Robert C. P., [14] Mengersen K. and Robert C., Testing for mixtures: A [15] Lindsay B., Mixture Models: Theory, Geometry and [16] Dupuis J. and Robert C., Model choice in qualitative [17] Tanner, M. A., W. H. Wong, The calculation of [18] Geweke J., Evaluating the accuracy of sampling-based [19] Congdon P., Bayesian Statistical Modelling, London, [20] Gelman A., J. B. Carlin, H. S. Stern and D. B. Rubin, [21] Rossi P. E., G. Allenby and R. McCulloch, Bayesian
DPM is not the only way to determine the impact of the customer's characteristic variables one by one. We introduce the K-given model as one such different other model. In the K-given model we can set the number of components, K from out of model. Our model decides K from the given data structure,
Each K-given model set has a different corresponding to the group that the user belongs to. The model takes the Dirichlet distribution as a prior distribution for h i equation is as follows. is a vector including K of 1. P ( h i = k j ; ) is expressed as follows.
 method. MCMC step 5 in subsection 4.1 is changed as fol-lows.
 Step 5-a Sampling h i and K with using Equation 12 and
