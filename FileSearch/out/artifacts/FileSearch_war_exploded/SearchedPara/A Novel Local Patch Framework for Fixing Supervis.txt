 In the p ast decades, machine learn in g models, esp ecially sup e r-vised learnin g algorithms, hav e been widely used in various r eal world app lications. However, no matter how strong a learnin g model is, it will suffer from the p rediction errors when it is a p-p lied to real world p roblems. Due to the black bo x n ature of su-p ervised learnin g mod els, it is a challen gin g p roblem to fix the sup ervised learnin g mod els by further learnin g from the failure cases it generates. In this p ap er, we p rop ose a novel Lo cal Patch Framework (LPF) to locally fix sup ervised learn in g models by learnin g fro m its p redicted failure cases. Sin ce the learnin g mod-els are gen erally globally op timized during training p rocess, our p rop osed LPF assumes that most of the learning errors are led by local errors in the mod el. Thus we aim to break the black bo x es of learnin g mod els by identify ing and fixin g the local errors of var i-ous models automatically . The p rop osed LPF has two key steps, which are local error region subsp ace learnin g and local p atch model learn in g. Through this way , we aim to fix the errors of learnin g models locally and automatically with certain gen eraliz a-tion ability on unseen testing data. Exp eriments on both classific a-tion and rankin g p roblems show that the p rop osed LPF is effective and outp erforms the origin al algorithms and the incr emental learn-ing model.
 H.3.3 [ Information S torage and Retrieval ]: Information Search and Retrieval  X  Relevance feedback ; I.2.6 [ Artificial Inte l-ligence ]: Learning; I.5.1 [ Pattern Recognition ]: M odels  X  Stati s-tical Algorithms, Performance, Exp erimentation Sup ervised Learn in g M odel, M odel F ixing, Local Patch, M etric Learning Sup ervised learnin g has been widely app lied in almost every co r-ner of our daily lives. For examp le, sp am email classification [36] , face r eco gnition for security sy stems [31 ], text classification in smart Web browsin g tools [20 ] and sear ch results rank in g in commer cial sear ch en gin es [23 ] etc. hav e signif icantly chan ged our way s of life. Though sup ervised learnin g algor ithms and co r-resp onding app lications have attracted much attention from both academia and industry , many learnin g models are suffer in g from the p rediction errors when they are app lied to real world p roblems no matter how strong the theoretical found ation is. M ultip le re a-sons may lead to the low p erformance of learn in g models. For instance, the noisy and biased trainin g dataset, the dy namically chan gin g data distribution and the weak ness of learners etc. are p ossible causes for p rediction errors. Desp ite the reasons that lower down the learn in g mod el p rediction p ower, in this work, we p rop ose to reduce the model p rediction errors by fixin g the sup e r-vised learning models after they are trained.  X  Notice the fact that in various real world learnin g p roblems such as search en gin e results rank in g, sp eech or handwritin g r eco gn i-tion etc. , the feedback from users, which are user jud gments to show whether some model p rediction results are correct or not, could be av ailable . These user f eedbacks are good sign al for lear nin g models to find the limitation of them and then imp rove accordin gly . Thus we are motivated to learn to imp rove the learn-ing models from the f eedback data. However, it is a challen gin g p roblem for us to fix the learnin g models since they are gener ally black bo xes to end users and could not be exp lained why an error is taken in. A bunch of related works [6, 2 6, 28, 29 ] have been aware of this p roblem and hav e tried to solve it by boosting [8 ] or incremental learn in g [22 ]. However, in p ractice, they generally make the learnin g model too comp lex and inefficient to imp rove model p erformance. The major limitation in these p revious works is that the learning models are gen erally fixed in a global view of * This work wa s done when first author wa s an intern of M SR Asia P ermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee prov ided t hat copies are not made or dist ribut ed for profit or commercial advant age and that copies bear this not ice and the full cit ation on the first page. To copy requires prior specif ic permission and/or a fee.
 CIKM X 12, Oct ober 29  X  November 2, 2012, Maui, HI, USA.
 Copyright 2012 ACM 978 -1 -4503 -1156 -4/12/10...$15.00. the data sp ace, which will take in new p rediction errors in one region when fixing the old errors in another region. In this p ap er, we p rop ose to fix the sup ervised learnin g models by learnin g from the feedback d ata in a local view instead of glob ally model up dating. In detail, we p rop ose a novel Local Patch Framework (LPF), in which the wrongly p redicted cases will be clustered and lo cal p atch models will be learn ed to fix each failure case cluster locally . In LPF, to minimize the sid e effect to original success cases, we first aggregate the failure cases as close to each other as p ossible in a new f eature sp ace, usin g sp ace transfo r-mation technique, i.e. metric learn in g [35 ]. This can be r egarded as the error region identify ing step . Then to fix the local errors, we learn a lo cal p atch model for each group of the similar failure cases. After identify ing and fixin g the error s existin g in models, it is exp ected that the model will have fewer errors on the feedback dataset and acquir e gen eralizability to imp rove its p erformance on future unseen cases. The locality of the p atch models enables them both effective and efficient on the feedback dataset, while bringing as less imp act on original data as p ossible. As a summary of contributions of this p ap er, we p rop ose the p rob-lem of local model fixin g, which aims to break the black bo x es of learnin g models automatically and generalize the imp roved model. Also, we p rovide a novel feasible solution LPF for our p rop osed p roblem to learn from the failure cases in feedb ack d ataset to form a new mod el with better p erformance. Our p rop osed solution solves this p roblem in a local view, hav in g less negative imp act on the good cases p redicted by the origin mother model co mp ared to global view solutions. M u ch time is saved on maintain in g ori g-inal success cases and we can p ut all our efforts on fitting the feedback d ataset and future unseen cases. Through exp eriments on both classification and r ankin g p roblems, we can see that the LPF can fix the local model errors without model retrainin g or manual efforts. The local errors are p rop erly fixed and the imp act on origin al success cases is minimized. On future unseen cases, we find good generalizability in LPF, which has a large imp rov e-ment on the classification accuracy in classification p roblem and nDCG [18 ] in rankin g scen ario co mp ared to other global solutions. The rest of the p ap er is organized as follows. In Section 2, we review the related works and some r elated algorithms which will be utilized in our p rop osed solution. In Section 3, we formally define the p roblem to be addressed in this p ap er. In Section 4, we p rop ose the novel Local Patch Framework in detail. After that, in Section 5, we emp irically verify the effectiven ess of our p rop osed app roach in both classification and search rank in g p roblems. F i-nally , in Section 6, we draw the conclusions and list our future works along this research direction. A number of p revious studies have attempted to imp rove the p e r-formance of sup ervised learnin g models by model integr ation [1 , 8] and retrainin g. The related works could be generally catego-rized into two categories. The first group of related works p ays major attention to make full use of the trainin g data and avoid p ossible over-fitting [4]. Boot strap aggregating (Baggin g) [1] is one of the most rep resentatives in this algorithm category . It p ro-p oses to combine multip le sup ervised learnin g models trained by randomly resamp led data fro m the same source trainin g set. Ho w-ever, this kind of models [3] will not change as lon g as the train-ing p rocess is ended. Consequently , it is hard for these models to take advantage of the information fro m n ew trainin g data or feed-back data in real world ap p lications. The second category of rela t-ed works aim to learn from the exp licit or imp licit feedback d ata from users, which could be achiev ed by model retrainin g or in-cremental learning [22 ]. Incremental learn in g was p rop osed for a learner to successively take in inp uts and train itself to outp ut new hyp othesis toward the target concep t. Since its first introduction, various incremental algorithms [6 , 13, 16 , 17, 21, 29] have been p rop osed to imp rove sup ervised learning models from different p ersp ectives such as iterative learnin g [11 ] or feedback learnin g [22]. One of the state-of-the-art incremental learnin g algor ithms could be the Learn++ algorithm [5, 26 , 27 , 28 ]. Learn++ algor ithm is a framework in-sp ired by AdaBoost [8, 9 ]. It uses extra models to fit the new training data, which will act on the whole data sp ace. However, with the increasing numb er of models, it becomes more and more costly and ineffective to add n ew glob al models to fit a small feedback d ataset. Different from all these related efforts, we p ro-p ose to fix the model in a local view, affect in g only the failure p art of the data. Thus no efforts are wasted on maintaining success cases by using multip le new models. Besides the related works mentioned above, in our solution detail, the metric learnin g [35 ] is an imp ortant technology for deriving our p rop osed solution. Thus we give a br ief review to classical metric learnin g algorithm in this section. M etric learnin g was originally p rop osed to learn distance metrics for imp roving the p erformance of clusterin g p roblems [10 ]. M any algorithms [12, 15, 24, 30, 33 , 34 ] for metric learnin g h ave tried to solve the convex optimization p roblems by aggregatin g the instances with the same class lab el as close as p ossible in a low d imension al feature sp ace. They are gen erally categor ized as linear method [12, 35 ] and no n-linear method [30]. Linear method p rop oses to learn a transfo r-mation matrix for new distance metric so that  X  X imilar X  points end up close to each other in new feature sp ace. Nonlinear method takes advantage of the  X  X ernel function X  to produce nonlinear transforms of the inp ut sp ace so that it reaches the same go al as linear method does. In our work, we utilize linear method in me t-ric learn in g since it emp irically works well in LPF and it is more efficient comp ared to nonlinear method. With the emer gin g of n ew p roblems in real world ap p lications, the sup ervised learnin g mod els are attracting mu ch attention from both academia and industry . However, a co mmon limitation of many learnin g algor ithms is that the y are generally black bo x to thei r users, which means it is hard to exp lain and imp rove the learnin g models once they are trained even though we frequ ently observe the wron gly p redicted results. On the other hand, no ma t-ter how strong a learnin g model is, in most cases, we cannot gua r-antee one hundred p ercent p recision in real world ap p lications so that we have high p robability to observe wrongly p redicted cases in feedbacks from users. This motivates us to figure out a way to imp rove the p erformance of learnin g models if takin g users  X  feed-back into account. The p revious methods such as incremental learnin g [22 ] or mod el retrain in g may suffer from imbalance b e-tween size of training data and size of feedback data. Thus the new model may either be too co mp lex or has little imp rovement on feedback data. In this work, we exp lore to fix learnin g models in a local view. In other words, we p rop ose to learn local p atches from the user feedback data for locally fixin g the error of sup e r-vised learnin g mod els, which will not affect many of the well p redicted cases when fixing the wrongly p redicted cases. M athematically , let x stands for the feature vector of an arbitrary data samp le in a sup ervised machine learnin g p roblem. Sup p ose scrip t is used to distinguish differ ent trainin g data samp les and (a) Cases has user feedback; (b) wrongly and correctly predicted cases in original feature space; (c) failure cases in learned subspace; (d) patch applied to the failure cases . is the target label of the training samp le the sup ervised lear n in g algorithm gener ally learns a p rediction model f (.), which aims to map a f eature vector x to its class lab el accordin g to numerical p rediction, from the train in g set . In many learn in g p roblems such as article classification [20 ] and web search rank-ing [23], we may collect exp licit or imp licit user feedb ack when we app ly the p rediction model f (.) to real world data. Sup p ose is the dataset containin g user f eedbacks, which shows whether the p redicted labels are correct or not when we app ly ing f (.) to real world data, we p rop ose to learn an imp roved p rediction model g (.) for fixin g the wron gly p redict ed cases in f eedback data and gene r-alizing to other unseen cases. There are multip le straightforward ways for fixin g the sup ervised learnin g models. For examp le, we can retrain a n ew model usin g there are generally much more d ata samp les in than their coun-terp arts in . Consequently , the retrained model will p refer to fit well but has little consideration to , which means the ob-served failure cases in still cann ot be fixed. Another way could be the incr emental learnin g [22 ], which considers the feedback data as a stream and continuously adds extra models to adjust the p rediction to fit the feedback data. However, it is very likely that those extra models, which fix the original model in a global view, will hav e negative effects on the well p redicted cases in original model. This will either lead to lar ge ensemb le of models to ba l-ance or result in relatively low imp rovement to original model. All these limitations motivate us to exp lore whether we can f ix the learnin g models in a lo cal view instead of f ixin g them globally by model retraining or incremental learning. In our work, we aggregate the failure cases to a local region by met ric learnin g and then fix them by a p atch model. The p atch model p learns fro m the corr esp onding failure cases. It p rovides an additional p rediction to fix the original model. The local con-cep t in our app roach is stressed as a p atch region def ined by a Gaussian function K , which rep resents a normal distribution of p atch model effect. If the feature vector x is closer to the p atch region centroid, the p atch model p (.) will have more effect on it. As a summary , giv en a learn in g model f (.) train ed by as well as the feedback data , we aim to learn to modify f (.) to a new model g (.) in a local p atch learning framework, i.e. is the distance b etween x and the centroid of the p atch. N is the number of p atches. From the definition, we can see that if x is far away from the centroid of the p atch, K will only have little effect on the final p rediction. Thus, the p atch model can only affect the cases close to the corresp onding p atch, and this imp licitly reduces the number of cases the p atch model will affect. In this section, we p rop ose a novel Local Patch Framework to locally fix errors in sup ervised learn in g models by learnin g from feedback d ata . On one hand, we aim to fix errors in through the local p atch learnin g without taking in more new errors. On the other hand, we aim to gener alize the mod el p atches to imp rove the overall model p rediction p ower. As an overview of the p rop osed solution, the LPF consists of two key step s, which are (1) sub-sp ace metric learnin g to learn K ; and (2) local p atch modelin g to learn for 1,2, . A toy examp le, which could b e used to il lustrate the key idea of LPF is giv en in Figure 1. Sup p ose all data samp les which will have exp licit or imp licit user feedb ack are dep icted in Figur e 1 -(a). This dataset will include both wrongly p redicted cases and co r-rectly p redicted cases. After collect in g the user feedback, we use different size to highlight the three wrongly p redicted cases in Figure 1-(b). Notice the fact that if we want to fix the three error cases, we either retrain our global model or fix them case by case since they distribute far away from each other if we want to fix them simultaneously . On one hand, the model retraining cannot guar antee to fix the three failure cases without taking in more n ew errors. On the other hand, if there are more failure cases in real world ap p lications, it is almost imp ossible to fix them case by case. Thus we are motivated to group the failur e cases to fix them together. As shown in Figure 1-(c), we p rop ose to utilize the sub-sp ace metric learnin g to p roject all data samp les in to another feature sp ace such that the error cases are group ed together and other good cases are exp ected to distribute relatively far from them. Finally , we learn a p atch model to locally fix the error cases without affecting the correct ly p redicted cases, which is illustrated in Figure 1-(d). In the next two sub-sections, we introduce the detail of the two key step s in LPF. To locally fix an arbitrary sup ervised learnin g mod el by learnin g from the feedback data , we p rop ose to firstly learn the local p atch regions K , which are also known as local man ifolds, to solve as more failure cases as p ossible together in . However, as demonstrated in Figure 1-(b), the failure cases may gener ally distribute diffusely in the feature sp ace, the worst case is that it will need us fix them almost case by case if we do not want to take in new errors. This situation conflicts with our target, which is to fix the model errors without taking in more new errors. In this work, we p rop ose to utilize the subsp ace metric learnin g to project the correctly p redicted cases as well as wrongly p redicted cases to another feature sp ace, within which the failure cases are group ed into several segments and they are exp ected to distribute far away from the correctly p redicted cases in . As introduced in the Related Work section, there are a nu mber of metric learnin g algorithms availab le. In this work, since our go al is to p rop ose the novel LPF for local mod el f ixin g, we introduce our solution, which is emp irically verified to work well in the p rop ose LPF framework. We cannot gu arantee it is the optimal amon g all me t-ric learning algorithms for any datasets in LPF. Our metric learnin g algorithm is strongly insp ired by the M ax i-mally Collap sing M etric Learn in g (M CM L) algorithm [12 ], in which a linear transformation of the origin al f eature sp ace is learned. In p articular, for each feature vector , we define a con-d itional distribution over as where A is the transformation matrix. We use C to denote the index set of success cases, and C to denote the index set of failure cases in . We divid e the failur e cases into N disjoint clusters since we use N p atches and use to denote the Cluster id of fai l-ure case i . For any failur e case i , we d efine an ideal distribution as: For any feature vector in a sp ecific cluster, if we have , map p ed to a single p oint, and ar e inf initely far away from other categor ies X  cases in the transformed subsp ace. Since we want to aggr egate the failure cases, we can learn A to satisfy , by minimizing the KL divergence K : which is equivalent to maximizing By differentiating f with resp ect to A , we obtain the gradient rule for learning A as: where . The major differen ce between our metric learn in g algorithm with M CM L is that we only consider the failure cases in the objective function, while in M CM L the cases from all the categor ies are considered. This can mak e our comp utation much more efficient. As for the reason, according to our discussion in the beginnin g p art of this section, our p atches are mainly app lied to the group ed failure cases and it is not necessary to group the correctly p redict-ed cases into comp act clusters. Besides, as the numb er of failure cases is typ ically much fewer than the success cases in real world app lications, this reduction can greatly accelerate the training p rocess. To determine the Cluster id of each failure case, we p rop ose to use an iterative algor ithm, which works like K-means, for comp u-tation al p urp ose. Before the iteration, we initialize the comp ut a-tion by assignin g rando m Cluster id to all the f ailur e cases. And then we up date the Cluster id by choosing the nearest cluster and calcu late the centroid of the cluster by averagin g t he v ectors a s-signed to it iteratively . After each iteration, A is up dated accordin g to equation 6. The whole p rocedure is listed as Algorithm 1 below, where is a learning rate p arameter. Algorithm 1 Clustered M etric Learning 1: P rocedure ClusteredMetr icLearning ( N ) 2:  X  random group id ( C ) 3: Initialize A. 4: f or 1 to outer -iteration do 5: f or all group id k do 6: c omp ute mean vector for group k : 7:  X  8: e nd for 9: f or all C do 10: up date  X  argmin  X   X  11: end for 12: for 1 to inner -iteration do 13: comp ute gradient by equation 6. 14: up date A  X  A + 15: end for 16: end for 17: end procedure After running the algor ithm 1 on the f eedback d ata , we can get both an transformation matrix A and the centroids of all the failure case clusters, which are denoted by , for 1,2, . Next, for each failure case cluster, we comp ute its radius as the distance from its mean vector to the farthest failure case assigned to it, i.e.  X  ma  X   X  . Through this way , we define our local p atch regions for p atch model learnin g in the new p rojected vector sp ace as follows.
 Local Patch Region : For each cluster k, we define a corr esp ond-ing p atch with as its centroid, and as its variance. In all our exp eriments of this p ap er, we select -1 0 and -150 . Detailed emp irical analy sis will be given in Section 5.1 .2.2. After we group the failure cases into clusters in a p rojected n ew feature sp ace, another key step of LPF is to learn the local p atch models such that each failure case cluster cou ld be fixed by one local p atch model. In this p rop osed LPF solution, the p atch model can be any sup ervised learn in g models, as lon g as it takes in the same feature vectors and outp ut the p rediction results as its orig i-nal mother model to be fixed. As some instances, the linear r e-gression, lo gistic regression, neural n etwork etc. are all feasible solutions for LPF. In this p ap er, t he learnin g framework is sketched in the algorithm 2 listed below. Algorithm 2 Learn p atch models 1: P rocedure LearnPatchModel 2: for t = 1 to max -iterations do 3: for k = 1 to N do N = #Patches 4: learn the k -th p atch model for one iteration. 5: end for 6: end for 7: end proced ure We first define a maximum iteration and then up date the p atch models iteratively . Durin g each iteration, we enu merate all p atch models and up date the p atch model once. The up dating p rocedure is described as follows. We enu merate all the cases in and u p-date the p atch model accordin g to the p rediction fro m imp roved model g (.) and the corresp onding label l . Sup p ose we define a cost function Cost ( g (.) , l ), which dep ends on the sp ecific p atch model typ e (linear r egression, lo gistic regression, neur al n etwork etc.), to measure the p erformance of model g (.), we should try to minimize the cost function during the p rocess of learning the p atch models. where W is the p arameters of the p atch models. This minimiz a-tion can be ach ieved by stochastic gradient descent. From the cost function, we can straightforwardly derive the up date rule of the p arameter in the k -th p atch model: where is a p re-defined p ositive learnin g r ate. For , we have where is dep endent on the sp ecific p atch model. The training data of the p atch models p otentially consists of all the cases in . However, we can clearly find in this imp roved model that if the case x is far away from the p atch  X  X  centroid, K will be close to 0, which consequently almost making no contribution to the gradient up date in equation (8) and (9). Ther e-fore, for those cases distant from the p atches, we can safely ignore them in learnin g. This en ables the p atch model to main ly focus on rep airing the failure cases, rather than to maintain the huge nu m-ber of success cases. After we have identified the local p atch region and learn ed the corresp onding p atch models, for any feature vector x , we use the p atched model instead of the original p rediction f ( x ). In this way , our p rop osed LPF can be ap p lied to locally fix errors in sup ervised learnin g models by learnin g fro m feedb ack data . The p atched model can effectively fix the errors in without taking in mor e new errors and be generalized to imp rove the overall model p rediction p ower. In this section, we app ly the p rop osed LPF on both classical cla s-sification p roblem and search rankin g p roblem to verify its effe c-tiveness. In Section 5.1, we show how the LPF help s imp roving learnin g models on sever al p ublic datasets for classification by using two commonly used base mod els. In Section 5.2, we show how much LPF help s imp roving r ankin g model by using the rank-ing en gin e of a co mmonly used commercial search en gine as base model. Without loss of generality , in the exp eriments of this p ap er, we mainly verify the algor ithm on the binary classification p roblem s since multi-class classification could be consid ered as multip le binary classification p roblems. The d atasets we used in the exp e r-iments come from the commonly used UCI M achine Learnin g Dataset [7], which are p ublicly accessible for rep eating all the exp eriments. Accordin g to our emp irical study by using Lo gistic Regr ession and SVM , they already work well on many subsets of UCI, which means there is no many errors need to be fixed on some UCI subsets. Thus we select three UCI subsets which do not have the extremely high classification p recision by using the state of the art classifiers. They are Sp ambase, Waveform and Op tical Digit Reco gn ition. The Sp ambase dataset is a collection of 4, 601 emails with 57 features each, containin g sp am emails and non-sp am emails, which is used to learn a sp am filter. The Waveform dataset collects 3 classes of waves with 40 features each. It con-tains totally 5, 000 instances distributed equally in the 3 classes. The Optical Digit Recogn ition dataset is used to recognize digit numbers from op tical images. It contains 5, 620 instances with 64 features each, equally distributed in 10 classes. For the last two multiclass classification p roblems, we treat class 1 as the p ositive class and the other classes as the negative class to conv ert them into binary classification p roblems. For each dataset, we randomly shuffle it and sep arate the whole dataset into three p arts for simu-lating the trainin g data, f eedback data and testing data. In detail, 60% of the data is used for train in g. 20% of the d ata acts as the feedback dataset for local p atch learnin g, and the rest 20% of the data is used as the unseen d ata for testing p urp ose. As for the base models for classification in LPF, the Supp ort Vector M achine (SVM ) [2 , 19 ] and Logistic Regr ession (LR) [14] are used. It should be stressed that these base models are treated as black boxes durin g the exp eriments. The LPF does not require p rior knowledge to the base models. Differ ent base models ar e just used as an examp le to show that LPF can gen erally imp rove model p erformance regardless of the sp ecific base model. The exp eriments are condu cted in three step s. First, we train the base models from trainin g d ataset and test them on feedb ack d ata. Second, b ased on the failure cases in feedback set, we learn a subsp ace to aggr egate failure cases and learn lo cal p atches to fix the errors of the model. Third, we retrain a mod el from both train-ing and feedback d ata as the second baseline algorithm. We also imp lement an incremental learn in g algorithm [5] , which adds extra base models to glob ally fix the original model, as the third baseline algor ithm. Finally , we ap p ly local p atch es on the original model and treat the sum of or iginal outp ut and p atch outp ut as the final p redictions. The new model is tested on the test dataset and used to comp are with original model, retrained model and incr e-mental learning model.
 To evaluate the exp erimental results, the classification accuracy (CA) is used as the main metric, where the accuracy is defined as To address the local concep t in our LPF app roach, we define two concep ts reflecting the rate of covered cases, which are Over all Case Coverage (OC) and Failure Case Cov erage (FC). The d efin i-tions of these two concep ts are as follows. There are two factors that could be considered as p arameters, which are iteration number in learnin g local p atch models and number of p atches. In this p ap er, we set the iteration number as 150 and ap p ly 2 p atches on each model. The p atch model in our exp eriments is chosen as linear regression for simp licity . In this subsection, we show the exp erimental r esults in detail. In Section 5.1.2.1, we show the overall ev aluation of LPF in contrast to the baselines. In Section 5.1 .2.2, we address the local con cep t of LPF by rep orting case coverage in the exp eriment. Op tdigit 0.9066 0.9724 0.9306 0.9689 As stated in p roblem d efinition section, we aim to find a model to rep air the cases in feedback dataset which are wron gly p redicted by the origin al mod el. Table 1 lists the classification accuracy of original model and the p atched model on feedback dataset. From Table 1, we can clearly find out that after we add some local p atches on the original model, the p erformance on feedback d a-taset imp roves much. This supp orts our conclusion that the p atched model successfully rep airs a lar ge p art of the failur e cases in feedb ack dataset. Sin ce we observed similar results on both SVM model and LR model, our local model p atchin g framework is insensitive to the learnin g models so we can ap p ly this fram e-work on various sup ervised lea rn in g models in r eal world ap p lic a-tions. The gener alization ab ility is crucial for our LPF. Only if our p atched model outp erforms the original mod el on unseen dataset can we confirm its p otential value in real world ap p lications. In this exp eriment, we comp are the classification accuracy of orig i-nal mod el, r etrained model, incremental learn in g mod el (I L) and the local p atched model (LPF) on a test set unused in the training p rocess. The exp eriment results are shown in Table 2.
 The p atched model shows good gener alizability in this exp eriment. It imp roves the classification accuracy of original model on test dataset to a large extent. The p atched model also shows large imp rovement comp ared to the r etrained model and incr emental learning model.
 To verify the statistical signif ican ce of our exp eriments, we run a statistical T-test exp eriment to comp are the p erformance of lo cal p atched model versus retrained model and in cremental learnin g (IL) model r esp ectively . In this T-test exp eriment, we equally divide the whole dataset as 5 p arts. In each round of the exp er i-ment, we choose 1 of the 5 p arts as the feedback dataset, another 1 p art as the test dataset and the rest as the training dataset. Table 3 shows the T-test results in totally 20 rounds of the exp eriment. The values are all smaller than 0.001, which is consider ed as the signif icant threshold. Thus the T -test result strongly supp orts our conclusion that the local p atched model has a much better p erfo r-mance than the retrained model and incr emental learnin g model. In real world app lications, the local model p atching framework is exp ected to largely imp rove the p erformance of sup ervised lear n-ing models without model retraining or manual efforts. Intuitively , in our local p atch model, we want a high Failure Case Coverage (FC) an d a low Over all Case Cover age (OC) , so that the p atched model will affect as few data as p ossible and recover as many failure cases as p ossible. That  X  s the originality of the con-cep t  X  local  X  . Figur e 2 p lots the case coverage trend of the three datasets with the increase of the iteration number in M etric L ear n-ing (Algorithm 1). As we have observed similar trends with LR model, we only rep ort the results with SVM model in these fi g-ures. We can con clude from these f igures that with the increase of the iteration, overall case coverage tends to decrease in all of the three datasets, which satisfies our exp ectation to locally fix the Op tdigit 0.7551 0. 4735 0.6333 0.1690
Figure 3. Relationship between classification accuracy and learnin g mod el, i.e. aff ect only small p art of the data. Although overall case coverage reaches a low level at the end of the iter a-tion, the failure case cover age d ecreases much slower than over all coverage. This exp lains the good p erforman ce of the local p atch model: most of the failure cases in test dataset are covered by the local p atches and will p otentially be fixed by the p atch model. Table 4 lists the case coverage at the end of the iteration in the exp eriment, in which we can observe a low ov erall cov erage against a high failure case cover age. The decr ease of case cove r-age in Figure 2 is caused by the aggregation of failure cases in feedback set. The p atches tend to be smaller to cover the aggr e-gated f ailur e cases. Sin ce the d ata distribution is similar in feed-back set and test set, it is exp ected that case coverage in both of them is close to each other. Thus the overall case coverage of test dataset will decrease with that of feedback dataset . In our exp eriment, number of p atches and the number of iteration in p atch model learnin g are two imp ortant p arameters, which in-fluence the p erformance of the p atched model mu ch. In this p art, we analy ze the sensitivity of our p arameter through sev eral exp e r-iments. Sin ce we h ave observed similar results in both SVM mod-el and LR model, we only rep ort the sensitivity analy sis for SVM mod el due to the sp ace limit. In this local model p atching framework, one imp ortant p arameter is the number of p atches N . It affects the p erformance of the lo cal p atch framework. When N is too small, there are r elatively more cases to be covered by every p atch, and the variances of the p atches are eventually bigger, which may cover too many well p redicted cases in real ap p lication, thus harm the model p erfo r-mance and weak en the effect of locality in this local p atch algo-rithm. To the contrary , when N is too lar ge, the p atches have r el a-tively small var iance, but the co mp utational comp lexity increases, and the small number of cases in a p atch may result in overfitting and weaken the gener alizability of the LPF. This p arameter N should vary with the size of the p roblem and the size ratio b e-tween training dataset and the feedback dataset. In our exp eriment, the p atch number 2 are chosen after p arameter analy sis, we first run several rounds of the exp eriment with diffe r-ent p atch number 2, 4, 6, 8. The classification accuracy of those three datasets in this exp eriment with and without local p atches is p lotted in Figure 3 . The results of same dataset are p lotted with similar colors. Though the results vary by using different number of p atches, we can find from this figur e that models with local p atches are alway s stronger than original mod els no matter how many p atches we use . In all of the three datasets, when p atch number is 2, the lo cal p atched model r eaches its maximum classification accur acy . Thu s 2 is chosen as the p atch number in our exp eriments. The iteration here stands for the iteration in Patch M odel Learnin g (Algorithm 2). In the exp eriments, we gener ally treat the p atched model learned after 150 iterations as the final r esult. In this p art, we p lot the change of classification accur acy with the increase number of the iteration in Figure 4 to show the trends of model p erformance. From those figures, we can clear ly find out that the local p atch p rovides much higher classification accur acy than three baselin e mod els. The classification accuracy of local p atch model tends to be stable or fluctuate around a certain level after a certain numb er of iterations. Thus the local model p atching framework is not so sensitive with the it eration number in p atch model learning as long as it is large enough. Besides the classification models, we also test the LPF on the search rank in g mod el, which has lar ger data scale than exp er i-ments in Section 5.1. In Section 5.2.1, we introduce the exp er i-ments setup , which is a little different from its counterp art in cla s-sification p roblem. In Section 5.2.2, we give some exp erimental results on a real search rankin g data set of a commonly used commercial search engine. Unlike classification p roblems, the aim of rankin g is to p redict the order of a docu ment set, accord in g to the relevan ce b etween the documents with some user-sp ecified query text. The state-of-the-art app roach for this p roblem is assignin g a relev ance score b e-tween any &lt;query , document&gt; ( &lt;q, d&gt; for short) p air. When rank-ing, a query text is given, and the documents is sorted accordin g to their relevance score with this query . To tackle this p roblem with machine learnin g, we collect a lar ge number of &lt;q, d&gt; p airs, and manually label their r elev ance levels. Then we train a model using this data-co llection to automatically learn the score of &lt;q, d&gt; the ranking model works much like a regression model which takes a &lt;q, d&gt; p air as a case and outp ut its relevance scor e. Ho w-ever, the only imp ortant thing is the order of the scores of &lt;q, d&gt; s under a fixed query rather than its absolute score.
 Since the correctness of ranking results for a given query is d e-termined by the order of the whole documents, it is not obvious to determine which cases are failure cases. For examp le, for two posite we can X  X  simply determine which case is wrong Therefore we introduce a failure case selection p rocedure in the ranking p roblems. In our app roach, we imp lement a greedy algor ithm to select fai l-ure cases as the feedback data for p atch learnin g. The algorithm is Second, we enu merate every p air of docu ment under the same query and draw an ed ge between the corr esp onding p oints in the grap h if they are ranked in a wrong order. Third, we iteratively the failure case and remov e all the ed ges connected to it until there X  X  no edge left In this way we can e pect to collect a rel a-tively small number of failur e cases used in the local p atch framework to reduce comp utational comp lexity . After the failure case selection for p atch learnin g, all other step s are the same as the exp eriments configuration in classification p roblems. Our exp eriments on rankin g p roblem are condu cted on a realistic dataset extracted from a commonly used commer cial search en-different features each. Each &lt; q , d &gt; p air is assigned a 5 level grad e by editorial jud gments, where score 1 stands for totally irrelev ant and score 5 stands for strong r elev ant. An offlin e ve r-sion of the online GBRT rankin g mod el [32] comp osed of 1,492 regr ession trees, which is the winner algor ithm in the learnin g to rank challen ge hosted by Yahoo! Inc in M ay 2010, is selected as the baseline algorithm in our exp eriments. The model is trained The incremental learn in g on GBRT, which is imp lemented as addin g more mod els at the end of the original regression trees, is also treated as baseline algorithm in our exp eriments to comp are LPF with incremental learnin g. To evalu ate the exp erimental r e-sults, we utilize the classical nDCG [18] as the main metric, which is derived from DCG. The nDCG at a p articular rank p os i-tion k is defined as: where stands for the graded relev ance of the result at p osition i and IDCG indicates an ideal DCG . Therefore, nDCG measures the gain of do cuments based on their p ositions in the ranking r e-sult list. Table 5 rep orts the nDCG results for the testing dataset for rank-ing. The incremental learn in g on GBRT reach es this imp rovement to original mod el after addin g 100 extra models. In contrast, with only 10 p atches, LPF reaches a much high er nDCG result than original GBRT and incremental GBRT. The reason of p oor incremental learnin g results could be that 100 extra models are still too small comp ared to the size of original 1,492 regression trees. On the other hand, LPF successfully a g-gregates the failure cases and affects only 8.0% of the overall cases, most of which are failure cases. The concentration of LPF on fixin g the failure cases greatly imp roves its p erformance on rankin g p roblem. Figure 5 p lot the nDCG@1 results with the in-crease of iteration number, in which we can find a large imp rov e-ment in LPF comp ared to original GBRT and incremental learn-ing results. In this p ap er, we p rop osed a Local Patch Framework (LPF) to fix the limitation o f sup ervised learnin g models in a local view in-stead of imp roving them glob ally by model retrainin g or incr e-mental learnin g. Our major contribution could be summar ized from the following sever al p ersp ectives. Firstly , we p rop ose the local mod el fixin g p roblem, aimin g to break the black bo x es of learnin g models automatically and generalize the imp roved model. Second ly , we p rovide a nov el feasib le solution for our p rop osed p roblem, which is named as LPF, to learn from the failure cases in feedback dataset to form a n ew model with better p erformance. In our p rop osed solution, which aims to solve this p roblem in a lo cal view, it will hav e less negative imp act on the good cases p redicted by the origin model in contrast to the related solutions p rop osed in glob al v iew. Thus we can save much time on maintain in g original success cases and p ut all our efforts on fitting the new feedback dataset and future unseen cases. The exp erimental results on both classification models and r ankin g models show that the p rop osed local p atch model can well fix mod el errors to imp rove the overall model p erformance without model retraining or manual efforts. In our next step work, we will focus on ap p ly ing the LPF on more real world app lications and imp rove its p erformance on app lic a-tions with higher real-time requirements. Also, insp ired by the excitin g exp erimental results, we will build theoretical foundation for this framework and p rovide b etter systematic way for machine learning model fixing. [1] L. Breiman. Baggin g Pred ictors. Machine Learning , 24, 1996, [2] C. Cortes, V. Vap nik. Sup p ort-vector networks. Machine [3] A. C. Davison, D. V. Hinkley . Bootstrap Methods and their [4] T. Dietterich. Overfitting and Undercomp uting in M achine [5] Z. Erdem, R. Polikar, F. Gurgen, and N. Yumusak. Ensemble [6] Z. Erdem, R. Polikar, F. Gurgen, and N. Yumusak. Reducing [7] A. Frank, A. Asuncion (2010). UCI M achine Learning R e-[8] Y. Freund, R. E. Schap ire. A Decision-Theoretic Generaliz a-[9] Y. Freund, R. E. Schap ire. A Short Introduction to Boosting. [10] K. Fukunaga, T. E. Flick. An Op timal Global Nearest Neigh-[11] M. Fulk, S. Jain, D. N. Osherson. Op en Problems in Sy stems [12] A. Globerson, S. Roweis. M etric Learning by Collap sing [13] Y. Hai, W. He, L. Fan. An Incremental Learning Algorithm [14] J. M . Hilbe. Logistic Regression Models . Chap man &amp; [15] S. C. H. Hoi, W. Liu, Shih-Fu Chang. Semi-Sup ervised Di s-[16] G. Hulley , T. M arwala. Evolving Classifiers: M ethods for [17] S. Jain, S. Lange, and S. Zilles. Towards a Better Unde r-[18] K. Jarvelin, J. Kekalainen. Cumulated G ain -based Rvaluation [19] T. Joachims. Learning to Classify Text Using Support Vector [20] K. Lang. Newsweeder: Learning to Filter Netnews. Proceed-[21] S. Lange, G. Grieser. On the Strength of Incremental Learn-[22] S. Lange, T. Zeugmann. Incremental Learning from Positive [23] Tie-Yan Liu. Learning to Rank for Information Retriev-[24] B. M cFee, G. Lanckriet. M etric Learning to Rank. In ICM L [25] A. M ohan, Z. Chen, K.Q. Weinberger. Web-search Ranking [26] M . M uhlbaier, A. Top alis, R. Polikar. Learn++.M T: A New [27] R. Polikar, L. Udp a, S. S. Udp a, V. Honavar. Learn++: An [28] R. Polikar, L. Udp a, S. Udp a, and V. Honavar. Learn++: An [29] G. Ramos-Jime  X nez, Jose  X  del Camp o-A  X  vila, and R. M o-[30] I.W. Tsang, P.M . Cheung, J.T. Kwok. Kernel Relevant [31] M . A. Turk, A. P. Pentlan d. Face Recognition Using Eigen-[32] S. Ty ree, K. Q. Weinberger, K. Agrawal, and J. Pay kin. Pa r-[33] K. Q. Weinberger, J. Blitzer, L. K. Saul. Distance M etric [34] K. Q. Weinberger, G. Tesauro. M etric Learning for Kernel [35] E. P. Xing, A. Y. Ng, M . I. Jordan and Stuart Russell. Dis-[36] S. Youn and D. M cLeod. Efficient Sp am Email Filtering 
