 1. Introduction
Automatic text classi fi cation is one of the key tasks in various problems such as spam fi ltering in which the main aim is to get rid of unwanted emails, email foldering that aims to group the incoming messages into folders and sentiment classi fi cation where the main goal is to recognize whether a document expresses a positive or negative opinion. Because of this, text categorization has become an attractive research area for many researchers in the last two decades. One of the fundamental problems in text categoriza-tion is document representation. The conventional approach is the bag-of-words (BOW) ( Sebastiani, 2002 ). In this representation, a subset of the terms that exist in the training collection is selected after sorting them using a term selection measure such as , Gini index or information gain (IG) ( Chen et al., 2009; Liu et al., 2009; Yang et al., 2012 ). Then, the document vectors are con-structed using the frequencies and inverse document frequencies ( tf idf ) of the selected terms where the frequency of a term denotes the number of times it occurs in the document under concern. Alternatively, as a more simple method, binary represen-tation is used where the feature value of a term is one if it appears in the document and zero otherwise. Experiments have shown that the feature value of a term, also known as its weight, can be more effectively calculated as the product of two factors, the term frequency and the collection frequency factors where the latter is used to take into account the discriminative abilities of different terms ( Debole and Sebastiani, 2003 ).

In the BOW-based approach, the orders of words and their syntactic relations are not taken into account. As an extension to the BOW-based approach, the use of syntactic phrases and word sequences ( n -grams) that are also known as statistical phrases is studied ( Mladenic and Grobelnik, 1998; Lewis, 1992b ). With the use of syntactic phrases, grammatical relations are also taken into consideration. Alternatively, n -grams which are generally de as consecutive occurrences of pairs (bigrams) or triples of terms (trigrams) are employed to extract novel features ( Caropreso et al., 2001; Tan et al., 2002; Bekkerman and Allan, 2004;
Mladenic and Grobelnik, 1998 ). The main motivation for consider-ing phrases is that a sequence of adjacent terms may be more discriminative than the individual terms in some cases. For instance, when considered individually, the terms  X  bill  X  in the phrase  X  bill gates  X  may not be as informative as the phrase itself about the topic of the document ( Bekkerman and Allan, 2004 ).
Taking this into account, features representing phrases are de where a phrase is said to occur if the corresponding sequence of adjacent terms appears in the document under concern. As another alternative, the use of termsets (or, compound features, itemsets) de fi ned as the co-occurrences of terms having arbitrary order and position is also studied ( Figueiredo et al., 2011; Tesar et al., 2006 ). In this approach, irrespective of their positions and order, if all terms appear, the corresponding termset is said to occur. Syntactic and statistical phrases are subsets of the set of all termsets. Since the number of termsets increases exponentially with the size of the vocabulary, termsets generally include pairs of terms but not triples. Experiments conducted on various datasets have shown that, when termsets or phrase-based features are concatenated with the BOW-based representation, better scores are generally achieved com-pared to the cases that exclude BOW and use only the termsets or phrases-based features ( Lewis, 1992a; Boulis and Ostendorf, 2005 ).
As in the BOW-based approach, selection of a good subset of co-occurrence based features is important, and various criteria are utilized for this purpose. In his study on the use of syntactic phrases, Lewis (1992b) has argued that high dimensionality of the feature spaces, rare occurrence of distinct phrases and high redun-dancy due to synonymy are the major factors for achieving worse results compared to the BOW-based representation. Following his study, extensive work is carried out on selecting a good subset of co-occurring terms (  X zg X r and G X ng X r, 2010; F X rnkranz, 1998; Tan et al., 2002; Bekkerman and Allan, 2004 ). For instance, IG ( Tan et al., 2002 ) and mutual information (MI) ( Bekkerman and Allan, 2004 ) are used for selecting a subset of bigrams. Redundancy of features is a criterion that is considered for computing a discrimi-native set of features for text categorization ( Baker and McCallum, 1998 ). This criterion is also used for selecting a good subset of bigrams. For instance, Boulis and Ostendorf (2005) argued that bigrams may not help improving the BOW representation when they are correlated with the features in the BOW-based representa-tion, mainly due to the increased complexity especially when the training data is limited. They proposed a new measure to quantify the redundancy of a given bigram by considering the terms included in the bigram and reported improved accuracies on three different datasets. In a recent study, signi fi cant improvements compared to the BOW-based representation are achieved by apply-ing pruning on both words and lexical dependencies (  X zg X r and G X ng X r, 2010 ). In fact, a weakness stated by Lewis is avoided by eliminating the rare words and the term dependencies with low occurrences. Figueiredo et al. (2011) underlined the importance of employing the most informative terms in termset generation. As a discrimination criterion, the number of classes in which the termsets appear is considered. Signi fi cantly better scores are achieved on four benchmark datasets by employing termsets of pairs of terms which are not restricted to be adjacent. The use of thresholds on the number of documents each phrase or termset appears in the training set is also
The studies mentioned above mainly aim at developing more intelligent schemes for selecting the best subset of phrases or termsets to be used together with BOW. However, in the case of BOW-based representation, term weighting is shown to be as important as selection and, various other measures such as relevance frequency and probability based scheme are proposed to replace the idf factor ( Lan et al., 2009; Liu et al., 2009 ). Using these weighting schemes, it is also shown that signi fi cantly better performance scores can be achieved when compared to using binary or tf idf based representation in the case of BOW. On the other hand, the termsets-based features are generally de fi ned as binary where the feature value is computed as one if the corresponding termset appears ( Figueiredo et al., 2011 ) and phrases-based features are de either binary or real-valued. In the case of real-valued features, only the frequencies are generally co nsidered for their weighting.
In this study, a novel framework is proposed for selecting and weighting of termsets including non-adjacent pairs of terms. The idea is based on revising the de fi nition of termset-based features. Consider a termset of two different terms. In the conventional representation, a termset is said to occur if both terms exist in the document. The proposed approach is based on utilizing the joint occurrence statistics of the terms for termset selection and weighting. More speci selecting and weighting termsets i s performed by considering which term(s) occurred. The main motivation for this approach can be better explained by an example. Let us re-consider the  X  bill gates Ifeitherofthetermsismissing,theindividualtermsofthephraseare not as informative as the phrase itself as mentioned above. Hence, only the co-occurrence of these terms is deemed as valuable. However, there are other cases for which this phrase is not repre-sentative. For instance, consider the termset  X  tennis court argued that the occurrence of both terms supports the sports topic. But, different from the previous example, the occurrence of the term without the second term also supports the same topic. Hence, it maybeusefultoassignlargeweightstothetermsetinbothofthese cases. The occurrence of the second term but not the fi rst may also be as law. In other words, the term  X  court  X  may not be discriminative on its own since it appears in both sports and law related documents, but it becomes more informative when evaluated together with  X  It can be concluded that co-occurrence is not essential for a termset to represent valuable information. As a matter of fact, instead of focusing on only the co-occurrence of the terms, evaluation of all three possibilities in selecting and weight ing termsets is promising. In this study, the joint occurrences of the individual terms within the termsets including two terms are investigated for their selection and weighting. The conventionally used selection and weighting schemes are adapted to employ this information. Experiments con-ducted on three widely used benchmark datasets have shown that the proposed scheme is remarkably superior to the baseline.
The rest of this paper is organized as follows. In Section 2 ,a brief review about the related work is presented. In Section 3 , the proposed framework is described. The experiments conducted on three different datasets are presented in Section 4 . The conclusions drawn and the future work are provided in Section 5 . 2. Related work
In co-occurrence based document representation, there are three major steps. These steps are de fi ning the features, selecting the best subset of these features and weighting the selected features. In this section, a literature review about the work carried out on these tasks is presented. 2.1. De fi nition of co-occurrence based features
The co-occurrence based features can be categorized into three groups, namely syntactic phrases, statistical phrases and termsets. 2.1.1. Syntactic phrases
Syntactic phrases are sequences of words ordered according to grammatical relations. Noun phrases, verb phrases and adjective phrases are typical syntactic phrases. The use of syntactic phrases for text classi fi cation was fi rstly studied by Lewis (1992b) .He studied the use of BOW and syntactical phrases-based features separately and reported that syntactic phrases do not provide better scores compared to the BOW-based representation. Dumais et al. (1998) have observed that using syntactic phrases in addition to BOW generally degrades the performance achieved by using BOW alone. Scott and Matwin (1999) also noted that syntactic phrases do not provide a better representation compared to BOW. However, it is shown that voting over the outputs of the classi making use of BOW and phrase-based representation can provide better scores than the individual systems. This veri fi es that the phrases and BOW-based representations may complement each other. The fi ndings of Nastase et al. (2006) supported his idea.
In particular, they studied the use of syntactically related pairs of words together with BOW and have shown that their approach provides improved accuracies compared to the BOW-based repre-sentation. More recently,  X zg X r and G X ng X r (2010) have shown that augmenting BOW with 37 lexical dependencies based features leads to signi fi cant improvements when compared to the BOW-based representation.

Although the use of grammatical relations between words is common to all of these studies, the types of the relations and the pruning levels considered to eliminate less frequent features are different. It can be argued that selecting a good subset of syntactic phrases is crucial for achieving improved performance scores by augmenting the BOW-based representation. 2.1.2. Statistical phrases
Statistical phrases, also known as n -grams, have been more extensively studied for text categorization. In this approach, sequences of n adjacent terms are used to de fi ne co-occurrence based features. The sequences of pairs (i.e. bigrams ) and triples of words (i.e. trigrams ) are generally considered where higher lengths are not found to be useful. Mladenic and Grobelnik (1998) have shown that the BOW-based representation can be successfully enriched by employing n -grams, n r 3. Similarly, F X rnkranz (1998) reported that sequences longer than three are not useful. Although the number of bigrams employed by Tan et al. (2002) to augment the BOW-based representation is 2% of the number of unigrams, improved classi fi cation performances are obtained. Instead of augmenting the BOW-based representation, Caropreso et al. (2001) kept the number of features used fi xed where the bigrams are used to substitute some of the unigrams. However, they could not achieve promising results. Bekkerman and Allan (2004) studied the use of discriminative bigrams together with BOW. In their study, a bigram is a considered to be a candidate to be selected if its mutual information score is higher than the scores of the individual terms. They achieved improved scores compared to the BOW-based representation. Boulis and Ostendorf (2005) also studied the use of bigrams together with BOW on three datasets.
They considered the additional information that each bigram brings when compared to its unigrams for choosing a good set of bigrams and reported improvements compared to BOW.

The use of varying length statistical phrases (multi-words) is also addressed. Zhang et al. (2008) studied the construction of multi-word based n -grams that have varying lengths. The multi-words are computed by comparing different sentences to fi consecutive matching word sequences. However, the performance scores achieved were inferior to BOW. The similar problem is also addressed by Peng et al. (2013) where a context graph based approach is proposed to identify signi fi cant statistical phrases of arbitrary lengths. On the contrary, they reported signi fi improved precision and recall scores compared to BOW, bigram and trigram based representations on two different datasets.
The common problem that is generally addressed in the use of statistical phrases is the selection of a good subset. Otherwise, a large set of additional features would be considered together with a large set of words which may lead to the problem of curse of dimensionality. The main difference among the existing studies is the criteria considered for selection. It can be concluded that the selection criteria are decisive regarding the performance of the categorization system. A review of the selection schemes widely utilized is presented in Section 2.2 . 2.1.3. Termsets
In the termset-based approach, co-occurrences of different terms which are not necessarily adjacent is considered in de fi features. In this approach, the terms do not need to form a syntactically meaningful sequence since their order is not important.
In general, a subset of available terms is considered in de termsets since all possible combi nations of terms correspond to a huge set. For instance, Za X ane and Antonie (2002) employed pairs of frequent terms to de fi ne 2-termsets. By combining frequent terms and frequent 2-termsets, candidate 3-termsets are then generated.
Association rules are computed to construct the resultant text classi fi cation system. Their simulation studies have shown that the results obtained are generally worse compared to the BOW-based representation. The study is later extended to employ the frequencies of the termsets during generating classi fi cation rules ( Rak et al., 2005 ). Experimental results have shown that it is bene fi frequencies of termsets in text classi fi cation. Tesar et al. (2006) studied the use of both bigrams and 2-termsets. Based on their experiments, they argued that bigrams are more appropriate for text categorization. However, they reported that the use of termsets or bigrams do not provide any improvement to the BOW-based representation. Recently, Figueiredo et al. (2011) performed extensive experiments on the use of termsets for text categorization. In their study, individually discriminative terms are considered for de termsets. A subset of the termsets obtained is then selected by applying a threshold on the document frequencies. The fi nal set of 2-termsets to augment BOW is computed by selecting discriminative ones. A dominance score that is in versely proportional with the number of distinct classes the termset under concern appears is used for this purpose. They reported signi fi cantly better scores compared to BOW and bigrams-base d representations.
 The selection of termsets is even more crucial than n -grams.
The main reason is that a termset is assumed to exist regardless of the order of the terms. Statistical and syntactical phrases are made up of adjacent terms which increase the probability of obtaining discriminative pairs. However, termsets may include terms which appear in different parts of the documents. We believe that these should be the major reasons for its being less attractive compared to the statistical and syntactical phrases-based approaches. 2.2. Selecting and weighting co-occurrence based features
The review presented in Section 2.1 clearly indicates that the selection of the co-occurring terms is an important problem for the text categorization task and various strategies are developed for this purpose. On the other hand, although the terms having higher discriminative power are ensured to contribute more to categoriza-tion by employing collection frequency factors in BOW-based systems, this is generally underestimated when co-occurrence based features are utilized. More speci fi cally, either binary or term frequency based representation is generally employed when both terms and co-occurrence based features are used. In this section, we review in more detail the selection and weighting schemes that are available in the literature. Table 1 presents ten well-known/recent studies and the criteria used for selecting the co-occurring terms.
We can categorize the criteria into two groups. The fi rst group includes the supervised metrics MI, Kullback  X  Leibler (KL) diver-gence, IG, odds-ratio (OR) and  X  2 where the class labels of the documents are utilized. These are well known metrics for the general feature selection task ( Ogura et al., 2011 ). Dominance that is de fi ned as the conditional probability of a class given that the termset occurred also belongs to this group. The second group includes unsupervised measures which do not take into account the labels of the documents. These are support and term frequency ( tf ).
Support, which is also known as the document frequency, is de as the number of documents where a termset or phrase occurs.
Using a threshold on the term frequency corresponds to specifying the minimum number of times that a termset or phrase must occur in the training set. It can be seen in the table that support is the most popular. It should also be noted that, in majority of the studies, two or more measures are employed.

Table 2 presents the term weighting schemes utilized in the studies mentioned above. It can be seen that the most popular representations are term frequency and binary. When termsets are considered, the number of times each term of the termset occurs may be different. For instance, the fi rst may occur only once whereas the second occurs more than ten times. In such cases, a new de fi nition for the frequency of the termset is necessary. As a matter of fact, binary representation is generally used for termsets.
It should be noted that both symmetric and asymmetric collection frequency factors are developed for the BOW-based representation. Asymmetric factors consider the terms that mainly occur in the positive class as more important than those in the negative class where symmetric ones consider the terms that mainly occur in the negative class as valuable as those in the positive class. For instance, the relevance frequency ( RF )isan asymmetric scheme de fi ned as ( Lan et al., 2009 ) RF  X  t i  X  X  log 2 2  X  A max f 1 ; C g ;  X  1  X  where A and C denote the number of positive and negative docu-ments which contain the term t i respectively. The multi-class odds ratio (MOR) is a symmetric term weighting scheme de fi ned as ( Chen et al., 2009; Erenel et al., 2011 ) MOR  X  t i  X  X  log 2 2  X  max AD BC ; BC AD ;  X  2  X  where B and D denote the number of positive and negative documents which do not contain t i . Several other supervised term weighting schemes for BOW-based representation exist in the literature ( Erenel et al., 2011 ). The majority of these schemes such as  X  2 ratio and information gain were originally proposed for feature selection ( Debole and Sebastiani, 2003; Lan et al., 2009; Altay and Erenel, 2010 ). Erenel et al. (2011) studied the weighting behaviors of fi ve of these schemes by analyzing their contour lines. In that study, they also proposed a novel weighting approach that is based on the occurrence probabilities of terms in different classes and compared their scheme with the other weighting schemes.

It should be noted that, since the BOW-based features are concatenated with the co-occurrence based ones, the use of the best-fi tting weights for both co-occurrence and BOW-based fea-tures is necessary to obtain more discriminative composite feature vectors. On the other hand, the use of supervised weighting schemes taking into account the occurrences of the terms in different classes is not well studied in the case of co-occurrence based features. This study incorporates extension of our previous efforts on computing better term weights by using supervised techniques to weighting termsets. 3. Proposed framework
The proposed approach for selecting and weighting termsets of unordered word pairs, f t i ; t j g for binary text classi on the co-occurrence statistics of the individual terms in positive and negative classes. In other words, rather than focusing only on whether they both occur or not, the proposed framework also takes into consideration the cases where one of the terms appears but not the other. The main motivation is to employ the joint statistics of the terms for selecting and weighting termsets. Consequently, discriminative information that may exist in the occurrence of one term but not the other is quanti fi ed and utilized in document representation.

Consider the example illustrated in Fig. 1 where the positive class corresponds to  X  law  X  and includes two documents, d The negative class denoted by  X  law  X  contains three documents, d d and d 5 . Assume that there are two terms where t 1 denotes the term  X  tennis  X  and t 2 denotes  X  court  X  . It can be seen that the positive documents do not include t 1 . The BOW-based representa-tion is presented in the second row of the fi gure where the and second elements of the document vectors correspond to t and t 2 respectively. In this example, without any loss of generality, we assumed that the weights of t 1 and t 2 are a and b respectively in all documents. In text categorization, the inner product is the most-widely used similarity measure during classi fi cation. Using this measure, it can be seen that the similarity of d 1 and d d , and d 1 and d 5 is the same. In other words, BOW is not able to differentiate between some positive and negative documents. The last row presents the proposed representation where the third weight of this feature is c when it is nonzero. In this case, the similarity of d 1 and d 2 is greater than the similarity of d and the similarity of d 1 and d 5 . Consequently, the positive docu-ments are more similar to each other than to the negative ones.
In order to implement such a representation, the information elements employed in widely used selection schemes, A , B , C and D , are fi rstly modi fi ed to take into account the occurrence of only one of the terms as presented in Table 3 . It can be easily seen that the de fi nition of occurrence is modi fi ed. More speci fi cally, a termset is assigned a nonzero weight if either or both of the terms occur. For instance, ^ A is the number of positive documents where at least one of the terms of the termset under concern appears. On the other hand, a termset does not occur if none of the terms appears in the given document. In the following context, the terms employed for de fi ning a termset will be referred as members of the termset.
Consider the well-known selection scheme,  X  2 de fi ned as 2  X  N  X  AD BC  X  2  X  A  X  C  X  X  B  X  D  X  X  A  X  B  X  X  C  X  D  X 
Replacing the original information elements with their modi forms, the  X  2 values of the termsets denoted by ^  X  2 can be computed as ^  X  2  X  N  X   X  ^ A  X  ^ C  X  X  ^ B  X  ^ D  X  X  ^ A  X  ^ B  X  X  ^ C  X  ^ D  X 
It should be noted that the proposed information elements can be used with other selection schemes. In selecting a subset of termsets, as in almost all studies in the literature, the support values of the termsets are fi rstly computed. The termsets whose members do not co-occur in minimum of three different training documents are discarded. ^  X  2 is then used to sort the remaining termsets before selection.

After selecting an apriori speci fi ed number of termsets, their weights which are composed of two factors, namely the term frequency and the collection frequency factor are computed for all documents. The collection frequency factor of a termset depends on the member terms that appear in the document under concern.
In other words, the collection frequency factor of a given termset is document dependent. With the use of this weighting scheme, individual and pairwise occurrences are separately evaluated in constructing the document vectors. Four new information ele-ments de fi ned for this purpose are presented in Table 4 where N and N denote the total numbers of positive and negative training documents respectively, and f t i ; t j g denotes the complement of f t ; t g . These elements are used when only one of the members occurs as follows:
P : The number of positive documents which include t i but not t
Q : The number of negative documents which include t i but not t
R : The number of positive documents which do not include t but include t j .

S : The number of negative documents which do not include t but include t j .
  X  N  X  P  X  and  X  N Q  X  are considered in computing the termset weights. When both members occur, the information elements
A , B , C and D are used. Consequently, the termset weights are de fi ned by considering the appearing member term(s) and the corresponding information elements.

Consider the relevance frequency ( RF ) given in Eq. (1) . The weight of the termset f t i ; t j g based on RF can be formulated as follows: c
RF  X f t ; t j g X  X 
Similarly, multi-class odds ratio (MOR) can be de fi ned for termsets as follows: d
MOR  X f t i ; t j g X  X  occur It can be easily seen that individual or joint occurrences of the member terms of a termset are weighted separately. Consider the help of proposed weighting, the occurrence of  X  tennis  X  court  X  may produce a large weight while the occurrence of  X  court  X  but not  X  tennis  X  is assigned a small weight.
The term frequency factor is computed for each termset as the sum of the member frequencies. Let tf i and tf j denote the term frequencies of the members in the document under concern. Then, the term frequency factor is computed as  X  tf i  X  tf j  X  . The overall weight is fi nally obtained as the product of the two factors. For weight of the termset f t i ; t j g is computed as w  X f t i ; t j g X  X  X  tf i  X  tf j  X  c RF  X f t i ; t j g X  X  7  X  Similarly, other collection frequency factors such as d MOR  X f t can be employed simply by replacing c RF  X f t i ; t j g X  .
The document vectors are constructed by concatenating BOW and termset-based representations. The product of term frequency and collection frequency factor is also utilized in BOW-based representation. For instance, using RF as the collection frequency factor, the weight of the term t i is computed as w  X  t i  X  X  tf i RF  X  t i  X  X  8  X  In the simulation experiments presented in the following section, the collection frequency factor is set to be the same in both BOW and termset-based representations. 4. Experiments
In all simulations, the F 1 score is used as the performance measure that is de fi ned as the harmonic mean of precision ( P ) and recall ( R )as F  X  2 P R P  X  R ; P  X  TP TP  X  FP ; R  X  TP TP  X  FN  X  9  X  where TP , FP and FN denote true positives, false positives and false negatives respectively. Macro and micro F 1 scores are generally used to compute the average performances on a given dataset. Macro F 1 is the average of the F 1 scores computed for each category ( Sebastiani, 2002 ). On the other hand, micro F computed using the total TP , FP and FN values when all categories are considered. Both macro and micro F 1 scores are used in performance evaluation of the systems implemented. 4.1. Datasets
Three widely used datasets are employed for evaluating the proposed framework. These are the ModApte split of top ten classes of Reuters-21578, 20 Newsgroups and OHSUMED. Reuters-21578 ModApte Top10 has a skewed category distribution. There are a total of 9980 news stories. It is a subset of Reuters, including the largest ten categories in the corpus ( Debole and Sebastiani, 2004 ). 20 Newsgroups is a larger corpus of 20,000 newsgroup documents that are more uniformly distributed among twenty different categories. It is freely available at  X  people.csail.mit.edu/ jrennie/20Newsgroups/  X  . OHSUMED corpus which includes 20,000 medical abstracts adopted by Joachims is considered ( Joachims, 1998 ). There are totally 23 categories, each corresponding to a different cardiovascular disease. Half of the corpus is used for training. For all datasets, the positive class is de fi category under concern and the negative class is de fi ned as the union of all documents in the other categories. 4.2. Experimental setup
The documents are fi rstly preprocessed for stopwords removal using SMART stoplist ( Buckley, 1985 ) and then stemming is applied using the Porter stemming algorithm ( Porter, 1980 ). The document lengths are normalized afterwards using cosine normalization approach. Due to the high dimensionality of document vectors, it is experimentally veri fi ed by various researchers that support vector machines (SVM) provide superior performance compared to various others such as naive Bayes ( Lan et al., 2009 ). In our simulations, SVM light toolbox with linear kernel ( Joachims, 1998, 1999 )and k NN are utilized. Default cost-factor value ( C  X  1 = avg  X  x inverse of the average of the inner product values of the training data is employed for SVM. On several datasets, it is observed that the F 1 scores generally plateau after 5000 features when SVM is used ( Lan et al., 2009 ). As a matter of fact, the top 5000 features ranked by  X  2 are used in the BOW-based representation for SVM.
It is well-known that k NN achieves its best scores on smaller number of features compared to SVM ( Lan et al., 2009 ). Moreover, the best-fi tting number of features and the value of k are dataset dependent. The macro F 1 scores of the BOW-based approach are computed for 100, 200, 400, 500, 1000 and 2000 terms and k
A f 5 ; 10 ; 15 ; 20 ; 25 ; 30 g and the best parameter values are deter-mined. The number of terms are computed as 200, 100 and 100 respectively for Reuters-21578, 20 Newsgroups and OHSUMED. The best values of k are computed as 30, 5 and 5 respectively.
All pairwise combinations of the selected terms are considered for constructing termsets. After discarding the termsets with support less than three, the remaining pairs are ranked using given in Eq. (4) . For SVM, the top K A f 1 ; 5 ; 10 ; 25 250 ; 500 ; 1000 ; 2000 ; 4000 ; 5000 ; 10000 g termsets are then conca-tenated with the BOW-based representation. For k NN, the top K
A f 1 ; 5 ; 10 ; 25 ; 50 ; 100 ; 150 ; 200 ; 250 ; 500 ; 1000 utilized for this purpose. 4.3. Simulations
Figs. 2  X  4 present the macro F 1 and micro F 1 scores achieved using RF as the collection frequency factor for the term weights and c RF for the termset weights on Reuters-21578, 20 Newsgroups and OHSUMED respectively where SVM is employed as the classi fi scheme. The terms selected using  X  2 are employed as BOW-based features and ^  X  2 is utilized for termset selection. The reference scores obtained using the baseline BOW-based representation are shown by the dashed lines. It can be seen in the fi gures that the termsets are able to contribute to the scores on all three datasets, even when a few of them are considered. Although the performance of the proposed framework is higher than that of BOW for large number of termsets such as twice the number of terms in the BOW-based representation (i.e., 10000), there are some dataset based differences. For instance, the macro F 1 curves approach a plateau when 250 termsets are employed on Reuters-21578 and 20 News-groups datasets whereas further improvements are achieved as the number of termsets increases on OHSUMED. This clearly shows that the number of discriminative termsets is dataset dependent.
For k NN, the macro F 1 and micro F 1 scores achieved using RF as the collection frequency factor for the term weights and c termset weights on Reuters-21578, 20 Newsgroups and OHSUMED are presented in Figs. 5  X  7 . As in the case of SVM, the termsets contribute to the scores on all three datasets. However, the highest scores are achieved using smaller numbers of features compared to SVM. The performance drops that occur as the number of termsets increase are mainly due to the inability of k NN to handle large feature spaces ( Lan et al., 2009 ). Comparing the performances of SVM and k NN, it can be seen that SVM provides superior scores than k NN in both BOW-based and proposed representations. Because of this, the experiments presented in the following context are conducted using SVM.

In order to verify the importance of using individual occurrence of one of the members but not the other, the termsets where both terms occur are discarded. Eq. (5) is modi fi ed for this purpose as follows: c RF Since the weight assigned to the co-occurrence of t i and t we named these features as termsubsets . Figs. 8  X  10 present the macro F 1 and micro F 1 scores achieved using c RF ind for the term-subset weights on Reuters-21578, 20 Newsgroups and OHSUMED respectively. The F 1 scores obtained using c RF f t i ; t presented for comparison. The fi gures clearly demonstrate that the use of individual occurrences is fruitful on all three datasets. Considering the co-occurrences in addition to the individual occurrences provides further improvement on Reuters-21578 and OHSUMED. Because of this, in the following context, c RF will be considered for termset weighting.

The experiments are repeated by using MOR and d MOR as the collection frequency factors. Figs. 11 and 12 present the macro F and micro F 1 scores achieved where the BOW-based representation employing MOR as the collection frequency factor is also presented as a reference. In the fi gures, it can be seen that improvements in F scores are achieved as in the case of c RF . On 20 Newsgroups dataset, the macro F 1 score decreases below the reference as the number of termsets increases to 4000. It should be noted that MOR is a symmetric scheme which considers the terms in the negative class as valuable as those in the positive. Hence, as more termsets are considered, it is likely that a large number of termsets which mainly appear in the negative class are employed. In order to verify this, the average values of  X  ^ A = ^ C  X  are computed for each dataset over all categories. It should be taken into consideration that the value of this expression decreases as more termsets are selected from the negative class. Table 5 presents the values obtained using the top ranked 1000 termsets and the termsets ranked between 9001 and 10000. It can be seen that the lower ranked termsets have lower values which means that they appear more frequently in the negative class compared to the higher ranked ones. For 20 News-groups dataset,  X  ^ A = ^ C  X  o 1 means that the termsets ranked between 9001 and 10000 appear in the negative class more frequently compared to the positive.

Remembering that the negative class includes documents from several categories that may not have common characteristics, it can be argued that the co-occurrence statistics of the member terms that mainly appear in the negative class may not always be reliable, leading to such a degradation. In fact, the degradation is mainly in the recall due to the increased number of false negatives. More speci fi cally, when the use of 1000 and 10000 termsets together with BOW is compared, the macro recall is decreased from 67.47 to 64.32 due to the increase in the number of false negatives (from 119.30 to 130.55, on the average over all cate-gories) where the macro precision remained almost unchanged. It can be concluded that the use of more negative features leads to the misclassi fi cation of increased number of positive documents. We also studied the use of 25,000 termsets for MOR. Both macro and micro F 1 scores slightly decrease for all three datasets when compared to 10,000 termsets. In particular, the macro and micro F scores are obtained as 90.26 and 94.89 for Reuters-21578, 72.69 and 75.88 for 20 Newsgroups and, 59.66 and 64.98 for OHSUMED. However, the F 1 scores are still above the baseline in both Reuters-21578 and OHSUMED.

The experimental results presented above clearly demonstrate the effectiveness of the proposed framework. We conducted further experiments to investigate the relative performances of the selection schemes  X  2 and ^  X  2 . Fig. 13 presents the macro F scores achieved by utilizing these schemes for termset selection. RF and c RF are selected as the collection frequency factors for terms in BOW and termsets respectively. As it can be seen from the fi gures, better scores are provided by ^  X  2 where the difference is less remarkable on Reuters-21578 dataset. In order to interrogate the comparable performance on this dataset, further experiments are performed. The ^  X  2 values of top 500 termsets selected by and ^  X  2 are presented in Fig. 14 . It can be seen in the Reuters-21578, the termsets selected by  X  2 achieve higher scores (around 1000) when compared to the other datasets. Because of this, they contribute to BOW-based representation on a similar order as those selected using ^  X  2 . It can be concluded that, for the proposed document representation framework, ^  X  2 ranking the termsets in a better way than  X  2 .

The termsets selected using  X  2 and ^  X  2 are studied in terms of the number of times each word is employed in their construction. Fig. 15 presents the average number of times that the most frequently used ten terms appear as members when 5000 term-sets are employed. It can be seen in the fi gure that a small set of terms are members in a large number of termsets when ^  X  2 In other words, ^  X  2 emphasizes the co-occurrences of a small set of terms with the remaining ones. It can be seen in the fi gure that the terms ranked fi fth or above are used a much fewer times, and hence a corresponding bar does not even appear. On the other means that ^  X  2 employs a wider set of different terms as members in the termsets.
The termsets selected using ^  X  2 are also investigated in terms of the total number of different terms utilized as a function of the number of termsets. Fig. 16 presents the average number of different terms used in the termsets selected over all categories using ^  X  2 as the termset selection scheme. On all three datasets, the average numbers of different terms employed increase almost linearly up to 500 termsets. The rate decreases as the number of termsets increases. For instance, on all datasets, approximately 500 different terms are employed in top ranked 500 termsets whereas, in the case of 5000 termsets, the number of different terms employed is approximately 3500 in 20 Newsgroups and
OHSUMED.
In our simulations, all 5000 terms utilized for BOW-based representation are considered for termset generation. This leads to  X  5000 4999  X  = 2 termsets which are more than twelve million possibilities. Although termset selection is done off-line during training, we studied the effect of using smaller number of terms for termset generation. More speci fi cally, the use of 500, 1000, 2000, 3000 and 4000 terms that are top ranked using  X  2 is also studied for termset generation. It should be noted that, for 500 terms, the total number of different termsets are reduced to be  X  500 498  X  = 2  X  124 ; 750 which is a much smaller number. Fig. 17 presents the macro F 1 scores achieved on three datasets. It can be seen that employing a large set of terms is bene fi cial where 4000 is the best-fi tting number for all three datasets. We studied the training time required for termset selection when 5000 terms are utilized. On a 3.1GHz i 5 processor, the total number of minutes needed for computing and ranking the termsets are computed as 38, 44 and 50 for the largest categories in Reuters-21578, 20 Newsgroups and OHSUMED respectively.

As stated in Section 2.2 , the binary term weighting is generally considered when termsets are employed. We compared the performance of the proposed framework with the binary repre-sentation where the conventionally used scheme,  X  2 is utilized for termset selection. The results are presented in Fig. 18 . The results for the proposed system using RF and c RF as the collection frequency factors and ^  X  2 for termset selection (denoted by BOW  X  Termsets(RF)) are also presented for comparison. It can be seen in the fi gure that, when binary representation is employed for term weighting, the use of termsets contributes to the BOW-based representation on two datasets, namely 20 Newsgroups and OHSUMED. However, the proposed scheme surpasses the binary representation based system for all different numbers of termsets considered on all datasets.

In order to asses the statistical signi fi cance of the improve-ments in the macro F 1 scores provided by the proposed approach, hypothesis tests are performed using the t -test approach. The null hypothesis is de fi ned as  X  H0: mean of the improvement is equal to zero  X  and the alternative hypothesis is de fi ned as  X  H1: mean of the improvement is greater than zero  X  . The tests are performed for RF based weighting scheme using 500 termsets and BOW-based baseline system. The null hypothesis is rejected at signi levels of 0.05, with p -values 0.0400, 0.0035, 2 : 88 10 6 tively for Reuters-21578, 20 Newsgroups and OHSUMED datasets. The entire Reuters collection consist of 115 categories where Reuters-21578 is the subset of ten most frequent ones. In order to investigate the performance of the proposed scheme on less frequent classes, the experiments are repeated for all 115 cate-gories. The experimental settings are the same as in the case of Reuters-21578. Fig. 19 presents the macro F 1 and micro F achieved using RF as the collection frequency factor for the term weights and c RF for the termset weights on the entire Reuters collection. Comparing Figs. 2 and 19 , it can be seen that consistent improvements are achieved when less frequent categories are also considered. Fig. 20 presents the macro F 1 and micro F 1 scores achieved using c RF ind for the termsubset weights on the entire
Reuters collection. The F 1 scores corresponding to using termset weighting is also presented for comparison. The results clearly demonstrate that the use of individual occurrences is fruitful when less frequent categories are also considered.
The relative performances of the selection schemes  X  2 and also investigated on the entire Reuters collection. Fig. 21 presents the macro F 1 scores achieved by utilizing these schemes for termset selection. RF and c RF are selected as the collection frequency factors for terms in BOW and termsets respectively. As it can be seen from the fi gures, better scores are provided by ^  X  2 . It should be noted that the difference between  X  2 and ^  X  2 is less remarkable on Reuters-21578 dataset when compared to 20 Newsgroups and OHSUMED as illustrated in Fig. 13 . However, larger differences are observed when less frequent categories are also considered.
We compared the performance of the proposed framework with the binary representation for the entire Reuters corpus. The results are presented in Fig. 22 . The results for the proposed system using RF and c RF as the collection frequency factors and for termset selection (denoted by BOW  X  Termsets(RF)) are also presented for comparison. It can be seen in the fi gure that, when binary representation is employed for term weighting, the use of termsets has only slight contributions to the BOW-based repre-sentation. However, the proposed scheme provides remarkable improvements in the macro F 1 scores compared to the binary representation based system on the entire corpus. 5. Conclusions and future work
A novel framework is proposed to employ termsets having two member terms for binary text categorization. The joint occurrence statistics of the member terms are employed for termset selection and weighting. Eight new information elements are de fi ned for this purpose. The experiments conducted on three widely used datasets have shown that, other than the co-occurrence of terms, the occurrence of only one of the members but not the other conveys discriminative information that helps to improve the performance of the BOW-based representation. It is also emphasized that the proposed approach can bene fi t from existing selection and weight-ing schemes simply by considering the proposed information elements.

The best-fi tting number of termsets is observed to be dataset dependent. On the other hand, a few hundred termsets is observed to provide remarkable improvements on all three datasets.
In particular, when 500 termsets are employed, it is shown that statistically signi fi cant improvements are achieved on all three datasets. It is also shown that using more terms for termset generation does not necessarily lead to better scores. The best-fi tting numbers of termsets and the corresponding macro F 1 are presented in Table 6 when SVM is used as the classi fi fi rst column shows the design parameters that are common to all datasets. The numbers of termsets providing the highest macro F scores are presented in the third column. It can be seen that, with the use of the proposed scheme, more than 2% improvement can be achieved in all three datasets.
 There are several points that need to be further explored.
In particular, choosing the best-fi tting number of terms to com-pute the termsets is an important problem. Similarly, tuning the number of termsets to be employed is rather critical. On the other hand, the selection of terms and termsets is generally done independently. However, as emphasized by some researchers mentioned in Section 1 , correlations between terms and termsets may exist, leading to a deterioration in the system performance.
Taking into account the computational power available nowadays, it can be argued that developing better schemes for choosing the best subset of features from a bag of all terms and termsets should be one of the next endeavors of the researchers in this fi References
