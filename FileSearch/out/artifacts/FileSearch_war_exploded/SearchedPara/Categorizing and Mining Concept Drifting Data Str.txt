 Mining concept drifting data str eams is a defining challenge for data mining research. Recent years have seen a large body of work on detecting changes and building prediction models from stream data, with a vague understanding on the types of the concept drifting and the impact of different types of concept drifting on the mining algorithms. In this paper, we first categorize concept drifting into two scenario s: Loose Concept Drifting (LCD) and Rigorous Concept Drifting (RCD), and then propose solutions to handle each of them separately. For LCD data streams, because concepts in adjacent data chunks are sufficiently close to each other, we apply kernel m ean matching (KMM) method to minimize the discrepancy of the data chunks in the kernel space. Such a minimization process will produce weighted instances to build classifier ensemble and handl e concept drifting data streams. For RCD data streams, because genuine concepts in adjacent data chunks may randomly and rapidl y change, we propose a new Optimal Weights Adjustment ( OWA) method to determine the optimum weight values for classifiers trained from the most recent (up-to-date) data chunk, such that those classifiers can form an accurate classifier ensemble to predict instances in the yet-to-come data chunk. Experiments on synthetic and real-world datasets will show that weighted instance approach is preferable when the concept drifting is mainly caused by the changing of the class prior probability; whereas the weighted classifier approach is preferable when the concept drifting is mainly triggered by the changing of the conditional probability. Classification, ensemble learning, data streams, concept drifting Recent developments in storage technology and networking architectures have made it possible for broad areas of applications to rely on data streams for quick response and accurate decision making. The challenge facing da ta mining is to properly and rapidly digest massive volumes of data produced from such data streams. This issue is further complicated by the reality that data distributions and decision concepts underlying the data may be subject to continuous changes, i.e. , so called concept drifting. An effective stream data mining algorithm should therefore be capable of dealing with such changing concepts and producing accurate models [1-4]. In practice, this issues can be addressed by detecting changes in the data streams [7, 24], and continuously updating the prediction models according to the most recently arrived data [5-6, 8]. All these methods perform well when the up-to-date data chunk has identical (or similar) distributions to the yet-to-come data chunk, which is called stationary assumption in data streams. However, sta tionary assumption ignores the uncertainty and instability nature of data streams when concept drifting occurs frequently. A recent research work [9] analyzed the KDDCUP X 99  X  X ntrusion detec tion X  dataset and proposed a relaxed  X  X earnable Assumption X . In short, if we denote the feature infinite sequence of (, ) x y . Stationary Assumption and Learnable Assumption can be written as: Stationary Assumption: Up-to-date data chunk U has the same distribution (, ) pxy as the yet-to-come data chunk Y, so classifiers performing well on U will also perform well on Y. Learnable Assumption: The distributions of up-to-date data chunk U and yet-to-come data chunk Y are similar to the degree that the model trained from U has higher accuracy on Y than both random guessing and predicting all examples to one single class. evolution of (, ) pxy in data chunks. This is, of course, the ideal situation for stream data; on the other hand, learnable assumption assumes that there is no specific relationship between data chunks, and the purpose of mining data streams is to produce models better than random guess or simp ly assigning all yet-to-come instances to one class. We believe that these two assumptions are too extreme for stream data: Sta tionary assumption is too loose (there is nearly no change of (, ) pxy ) while learnable assumption is too rigorous ( (, ) pxy changes rapidly and randomly, and data mining can only do better than random guess). Although evolves continuously, (, ) pxy can be decomposed into two parts the data samples, it usually under goes gradual changes, and such change introduce the first type of concept drifting in data streams. More specifically, we say that () px evolves with time stamp t and can be rewritten as (|) px t . (|) py x is the conditional probability of instance x , which is the second source of the concept drifting in data streams. If we consider (|) px t and (|) py x as the main driven forces of the concept drifting in data streams, we can categorize our analysis by using the following loose/rigorous concept drifting definitions: Loose Concept Drifting (LCD) is defined as if the distribution of continuously flowing data samples (|) px t is systematically localized, and (|) py x may gradually and locally evolve as time elapse. Being loose, it means that the genuine concepts of the data streams remain relatively stable, whereas the vision of the drifting is mainly caused by inadequacy or biased observation of the instances (|) px t . both (|) px t and (|) py x change and evolve randomly, which makes the existing classifier conti nuously be old-fashioned. Being rigorous, it means that the genuine concepts of the data streams are undergoing continuous change , and such changes can be worsen by inadequate or biased observation as well. Figure 1: Example of LCD vs. RCD definition w.r.t. four time stamps in a one dimensional data stream. The genuine concept of the data stream changes at time point t*. Therefore instances before and after t* are with two distinct distributions y = f ( x ) and y =g( x ) respectively. From t 1 to t 2 , the data stream is experiencing loose concept drifting, whereas from t 2 to t 3 , the data stream is experiencing rigorous concept drifting.
 We now explain the above LCD and RCD definitions in details by using a toy data stream shown in Figure 1: (1) In LCD definition,  X  (|) p xt is systematically localized X  (2)  X  X onditional probability (|) py x evolves as time elapse X  (3) The loose concept drifting can be further split into two (4) In RCD definition,  X  (|) px t and (|) py x change and two ensemble frameworks on stream data mining and argue that  X  X ertical ensemble X  framework is suitable for our new assumptions. Section 3 introduces a weighted instance approach to mine LCD data streams. In s ection 4, we propose a weighted classifier approach to mine RCD data streams. Experimental results and analysis are reported in Section 5, and we conclude our paper in Section 6. The main challenge of stream data mining is to accurately capture the continuous changing decision concepts and scale up to large volume of stream data. Existing re search in the area has proposed a set of ensemble frameworks for stream data mining. Under this framework, one can build classifi ers on rather small data chunks without missing major patterns. Street [10] proposed a SEA algorithm, which combined all the decision tree models using majority-voting. Kolter [11] proposed an AddExp ensemble method by using weighted online learners to handle drifting concepts. Scholz[12] proposed a boosting-like method to build ensemble classifiers. Wang et al. [13] proposed an accuracy-weighted ensemble, in which they assign each classifier a weight reversely proportional to the classifier X  X  accuracy on the up-to-data chunk. Zhu et al. [14] proposed an active learning framework to selectively label instances from concept drifting data streams. All these methods build classifi ers on different data chunks by using the same or different learning models, then combine the models (with or without weighti ng) to predict instances in the upcoming data chunk. For LCD data streams, these methods can work perfectly, as if decision concepts gradually evolve, aggregating modes from historical da ta is indeed an effective way to handle stream data. For RCD data streams, these methods will deteriorate significantly, because decision concepts rapidly and randomly change, with little or no correlation of the decision concepts between data chunk. The recent learnable assumption [9] brought out a majority voting ensemble framework that integrates all classifiers using different learning algorithms, e.g., Decision Tree, SVM, LR, and then builds prediction models on and only on the most recent (up-to-date) data chunk. For clarification, we name the first type of classifi er ensemble which builds upon a set of buffered data chunks as  X  X or izontal ensemble framework X . Similarly, a classifier ensemble which builds on the most recent data chunk only is called a  X  X er tical ensemble framework X . Existing research [9] has shown that this vertical ensemble framework is robust when the up-to-date data chunk has a completely different distribution to the yet-to-come data chunk. For loose concept drifting data streams, because genuine concepts across different data chunks are subject to little change, the horizontal ensemble framework is shown to be effective. For rigorous concept drifting data streams, the concepts in neighboring data chunks can be si gnificantly different, and a data chunk can conflict with some dated data chunks, if we still follow the horizontal ensemble framework. Therefore, in this paper, we will use vertical ensemble frame work as a baseline and propose two solutions for LCD and RCD data streams. Figure 2: Vertical vs. horizontal ensemble framework. For loose concept drifting data streams, because conditional probability of the instances in adjacent data chunks remains relatively stable, we can simplify concept drifting as the sample selection bias problem [19, 20] . So the stream data mining challenge is to assign proper weight values to instances in the up-to-date data chunk, such that the mining algorithm can properly reveal the data distribution in the yet-to-come data chunk. Assuming we have buffered a data chunk at time stamp t , and want to build classifier that minimizes the loss function ((),) lfx y on the yet-to-come data chunk at time stamp t+1 , that is minimize on the t by minimizing well on the data collect at t+1 . If we have some prior knowledge of the yet-to-come instances, we can utilize transfer Adaboost learning framework (TrAdaBoost) [15], in which the authors argue that when the training and testing distribution are different, TrAdaBoost can adjust the weights of the training instances to make sure that instances sharing similar distribution as the testing samples to have larger weight values in building the prediction model [16]. For this purpose, we first add weights (| 1) to the training samples at time stamp t , then resample the training set by using the weight values and generate prediction model to classify instances in data chunk t+1 . Because weight values relies time consuming process and may al so introduce extra errors in probability estimation [18-20], we employ a nonparametric Kernel Mean Matching (KMM) [21] method with a key assumption that if conditional probability (|) py x remains unchanged while () px changes, KMM can match trai ning and testing distributions in feature space to reduce the error. Experimental results [21] also show that KMM can perform reasonably well even if (|) py x changes within a certain degree. This is extremely important because it means that KMM method can be applied to handle LCD data streams. algorithm. Given a training set with m instances and a test set with m  X  instances, KMM first maps all th e training and testing instances into a kernel space using kernel function ()  X  X  , then it adds weight value m R  X   X  to each mapped training instance to minimizes the discrepancy between the distribution centers of the training and testing instances in the feature space. The objective function of KMM is denoted as follow. where [0, ] Since 11 || () ()|| mm where (, ) objective function can be denoted by a quadratic programming model given by Eq. (2). significantly impacted by parameter B under linear kernel setting, which bounds the distance of the two distribution centers. In stream data environment, the prior probability (|) px t can gradually change. Therefore, when applying KMM to stream data, the distribution centers in kernel space should be changed automatically. Assume that a data stream is flowing forward with constant velocity v and time stamp is T  X  , we can set the parameter B as vT  X  to adjust discrepancy of the distribution centers between the training and the test sets (in stream data environments, it means data in the up-to-date and the yet-to-come data chunks). Consequently, th e KMM model under stream data environment can be rewritten as: s.t.: Because the problem defined by Eqs. (4) and (5) is a standard quadratic optimization problem (QP), it can be solved by following the general procedures similar to the support vector machines (we use linear kernel f unctions in our experiments). In our system, we use the calculated weight values to resample instances from the up-to-date data chunk, and then build a set of prediction models from the sampled instances. The class labels of the instances in the yet-to-come data chunk are thus determined by the majority vote of the predictions from all base models. For rigorous concept drifting data streams, because both prior and rapidly change, we will seek to assign weight values to a set of base classifiers generated from the up-to-date data chunk to classify instances in the yet-to-come data chunk. A challenge issues here is how to determine the proper weight values for the base classifiers, such that they can indeed form an accurate classifier ensemble model. Alt hough weighted classifier ensemble model has been popularly used for stream data mining [8, 11-13], the weight values are mostly empirically determined and lack strong theoretical foundation (e.g., using error rates of the base Weights Adjustment (O WA) method to calculate optimum weight values, such that the weighted classifiers can form an accurate classifier ensemble to mine RCD data streams. candidate classes. We assume that a given classifier X  X  probability of classifying x to a class c i deviates from ) | ( x c p associated error ) ( x terms: specific test instance x , the probability that classifier f assign label c to x is () posterior probability (|) i p cx , however, it actually adds some extra errors of bias and va riance as follows [13-14, 22]: where (|) instance x . 
Figure 3: Decision boundaries and error regions associated with approximating the a posteriori probabilities (revised from [22]) loci of all points x * such that p ( c i | x *)= p ( c classifier deviates from the op timum decision boundary, as shown in Figure 3. In this figure, the actual decision boundary is denoted by x b , the Bayes optimum boundary is denoted by x *, and b = x denotes the amount by which the boundary of the classifier differs from the optimum boundary. The dark ly shaded region represents the area that is erroneously classified by the classifier f . Under mild regularity conditions, we can perform a linear approximation of p ( c k | x ) around x *, and express the density function f classifier f is given by Eq. (7), where A ( b ) is the area of the darkly shaded region, and f b is the density function for b . classifier f can be expressed by Eq. (8)  X  denotes the variance of () expected added error of a classifier is proportional to its classification bias 2  X  and variance 2 terms reduces the classifier X  X  expected error rate. instance x , the output probability will be: The second part of Eq. (9) denotes the averaged bias of all base classifier forming a classifier ensemble. Because a classifier X  X  bias on a class c i is independent of individual instance x , we can rewrite Eq. (9) as The third term of Eq. (10) is a random variable produced from the variance of classifier ensemble in classifying instance x , we mark it as Which can be rewritten as Eq. (13) Assuming that m c pairs C g and C m ( g  X  m ), then cov( , ) 0 rewrite Eq. (13) as follows: Assume there are l classes denoted as { 12 , ,..., variance of a k -classifiers ensemble on all l classes will be: Under the assumption of the classifier independence for any classifier pairs C g and C m ( g  X  m ), the squared value of the classifier ensemble bias is denoted by Eq. (16) The total squared bias value of a k -classifier ensemble on all classes can be denoted by Eq. (17). Considering Eqs. (15) and (17), and the conclusion drawn from Eq. (8), we know that in order to minimize the classifier ensemble error rate, we should find weight values, such that the ensemble variance and bias can be minimized, as defined by Eq. (18). To find solution for the optimization problem given in Eq. (18), we define that the weight of every base classifier proportional to the sum of the variance 2 m all of the l labels as follows: 
Let where (18) can be formulated as th at finding the optimal solution p such that: This mathematical programming m odel can be easily solved by Lagrange multiplier as: Taking partial on 12 ( , ,..., ) mm m l p pp , we have: Let 2 2 ) ( E c m i to: According to Eq. (24) and the constrain l i p k Eq. (21). We can easily find solution to m i p , as shown in Eq. (25). Rearrange Eq. (25), we have Substitute m i h with 2 2 ) ( m c m i where m =1,2, ..., k which is the index of the classifiers, and i =1,2,.., l , which is the index of the classes. Based on the solutions to m i p , we can calculate the weights values m i w , and then form a weighted classifier ensemble to predict instances from the yet-to-come data chunk. According to the solution given by Eq.(27), the calculation of the weight values requires that our algorithm should be able to calculate the bias m c m , m =1,.., k . In our algorithm, the calculation of given by Eqs. (28) and (29) respectively. Where x  X  denotes a number instances selected to evaluate the base classifiers, classifier m on instance x (with respect to class c average class probability of an classifier m over instances in the subset x  X  (with respect to class c i ). In our system, generated by randomly sampling a small number of instances from the yet-to-come data chunk, a nd use the instances as well as their class labels to calculate base classifiers X  bias and variance values. Notice that labeling instances from data stream is an expensive process, an alternative solution is to employ an active learning process on the data stream and selectively label a small number of instances from the yet-to-come data chunk [14]. The whole system is implemented in Java with an integration of WEKA data mining tool [23]. As mentioned in Section 2, we employed a  X  X ertical ensemble X  framework, where all base-predictions are used to classify instances from the yet-to-come data chunk. The learning algorith ms used in our experiments include Support Vector Machines (WEKA SMO implementation), Decision Trees (WEKA J4.8 im plementation) and Logistic Regression. All these base classifiers are implemented by using WEKA default parameter setting. To comparatively demonstrate the performance of KMM and OWA, as well as show the performance of the stream data mining methods under different concept dr ifting scenarios, we implement the following two benchmark me thods. The first one is the accuracy-weighted ensemble framework and the second one is the majority voting ensemble framework. Majority voting ensemble assumes we don X  X  have any prior knowledge about the yet-to-come samples, so it simply builds classifiers from the up-to-date data chunks and uses their majority vote to predict instances in the yet-to-come data chunk. We take the results from the majority voting method as the lower bound. Accuracy-weighted ensemble assumes that we have sufficient prior knowledge of the yet-to-come data chunk, i.e. , we know the class labels of all the yet-to-come instances, and then we can weight each classifier according to its accuracy on the yet-to-come instances. This assumption, of course, violates the truth. If we know all the yet-to-come samples X  labels, we need not to do data mining at all. In our system, we take accuracy-weighted ensemble method as the upper bound. After setting the lower and the upper bounds, we can get a clear picture on where KMM and OWA stand under different types of concept drifting scenarios. To simulate different types of concept drifting scenarios, we Gaussian distribution, i .e. 2 ~(, ) (|) py x at time stamp t as follows: Where the first nonlinear term generates discriminative class labels, and the second linear part simulates the evolution of (|) py x by changing b t , and the third part denotes random noise following a Gaussian distribution 2 ~(0,0.1) N  X  . stream flows with speed 2/ v  X   X  = , and t  X  continuously changes by following 1 (1) s tt vT  X  X  + =+ X  X  X  X  X  X  , where probability of 10% to be 1, which makes the streams flow reversely; T  X  is the interval of time stamps, which is set as a decision boundary follows the rules that, if ttt ybx label is  X +1 X , otherwise the class label is  X -1 X . For multi-class classification, suppose there are l classes 12 { , ,..., } assign class labels by equally dividing the nonlinear part of Eq. (30). For example, if there are four classes denoted by C ={+2,+1,-1,-2}, we divide the four cla sses by following the rules: if x is labeled as  X +1 X ; if 2 1 X ; if 2 of instances in each class is changing, the ratio between the number of instances of the classes C i and C j remains balanced for each class in a two-class problem. a variable  X  to denote  X  X ampling Width X . Comparing the number of instances in one buffered data chunk written as v ), we have the following relations: (1) If depicted in the first picture of Figure 4, the samples at time stamp t has no overlapping with instances at time stamp t +1,; (2) If v  X  = , there is no significant overlapping between each data chunk, as depicted in the second picture in Figure 4; (3) If depicted in the last picture in Figure 4. Figure 4: The left picture shows no overlapping between overlapping between (|) px t and (| 1) px t + ; the right picture shows heavy overlapping between two data chunks. To evaluate the algorithm performance on real-world datasets, we evaluate the system on KDDCUP X 99 intrusion detection dataset, which is a popularly used test bed for stream data mining [9]. This dataset consists of a series of TCP connection records, each of which corresponding to a normal connection or an intrusion including denial-of-service (DOS); unauthorized access from a remote machine (R2L); unauthorized access to local root privileges (U2R); surveillance and other probing. Because the number of attacks for R2L, U2R, and Probing are small, we will mainly focus on DOS attack instead. Meanwhile, the concepts underlying this dataset appear to be linearly separable (e.g., many classifiers can attain very high classification accuracy, over 97% or higher, on 10% sampled instan ces), we complicate the learning task by using the following four approaches to build different types of data streams: (1) Random Selection: we randomly select 100 data chunks, each of which containing 1000 instances with equal class distribution (50% of instances for each class). (2) Shuffling Selection: we repetitiv ely pick one instance from class  X  X OS X  and one instance from class  X  X ormal X , after 500 repetitions, we build a data chunk with 1000 instances. Repeating the above process for 100 times, we will build 100 data chunks, each of which containing 1000 instances. (3) Rearranged Selection: given the training set, we first find the most informative attribute by using the informa tion gain measure (provided by WEKA) (for KDD CUP X 99 dataset, we found that the 30 attribute is the most informative attribute). Then we change the label of the instance belongs to y  X   X  X OS X  then the value of the attribute 30 30 xx  X  =+ ; (2) if the class label of the instance belongs to y  X   X  X ormal X , then the value of the attribute  X  = . By doing so, we are making instances likely separable by using this attribute (the 30 th attribute) only. After that, we discard all other attributes, and sort instances based on their new attribute values. The sorted instances are finally put into 100 data chunks each of which containing 1000 instances. (4) Relationship Selection: this method is based on the Rearranged Selection. We define c  X  as the distance of distribution centers between the adjacent data chunks. Then accordi ng to the relationship between  X  and  X  in Rearranged Selection, we define three situations as follows: if 2( ) c  X   X   X &gt; + , then adjacent data chunks are nearly increase  X   X  + to 2( ) c  X   X   X &lt; + , the overlapping becomes heavier. By doing so, we are gene rating data chunks with different degrees of overlapping, so we can test the impact of the data chunk overlapping on different algorithms. Tables 1 to 4 list the results of different methods on synthetic datasets, with different data chunk sizes. The second row of the tables denotes the relation between the sampling width  X  number of instances at time stamp t , which also shows the degree of overlapping between adjacent data chunks. The first column lists the name of four methods :  X  X ve. X  denotes algorithm which uses the majority voting of the base classifiers for prediction;  X  X MM X  indicates algorithm uses KMM algorithm with weighted-instance classifier ensemble;  X  X WA X  indicates the weighted-classifier ensemble with optimal weight adjustment method; and  X  X cc. X  indicates we set classifier weights based on their accuracy on the instances from yet-to-come data chunk. As we discussed before, the  X  X ve. X  approximate ly provides lower bounder, while the  X  X cc. X  approximately provides the upper bounder. The numeric values in the tables are given as:  X  X ean  X  variance X  and the results of KMM and OWA with higher accuracy are bolded. Table 1 reports two-class classification results under the LCD scenario: (|) px t evolves while (|) py x remains stable. When each data chunk contains 100 instances, we can see that if the average accuracy of KMM for bias minimization is 74.09%, better than OWA X  X  55.04%, and even better than Acc. X  X  56.49%, which is supposed to be the upper bound of all methods; if the accuracy of OWA is 62.25%, which is better than KMM (51.38%), and under this circumstance, the lower bound accuracy (Ave.) is 51.33% and the upper bound accuracy (Acc.) is 66.26%. For v  X  &gt; , OWA also performs better than KMM, and both OWA and KMM perform better th an lower bound method (Ave.) and are inferior to Acc.. When the chunks size increases from 100 to 1000 instances, the performances of each algorithm improve performances. This is due to the f act that a smaller data chunk will contain fewer examples, and sparse training examples will generate less accurate learners in general. Table 2 shows the results on the four-class cla ssification problem under LCD scenario, where the concept drifting is involved in (|) px t . When every chunk has 100 instances, if v  X  &lt;  X  KMM performs better than OWA; if v  X  = and v  X  &gt; , OWA performs better than KMM. When the chunk size increases to ten times larger, e.g. 1000 instances, the accuracy also shares a large improvement: KMM reaches 64.78% when v  X  &lt; , OWA reaches 87.57% when v  X  = and 88.33% when v  X  &gt; . Tables 3 and Table 4 list results under RCD scenario, with both (|) px t and (|) py x evolving. We can see that KMM performs better than OWA when and OWA performs better than KMM when v  X  = and v  X  &gt; . and (| 1) px t + , KMM is still able to restore the classification model from the up-to-date chunk to predict instances in the next chunk by sampling instances from (|) px t with adjusted weights values. For RCD scenario and v  X  &gt; , although instances tend to overlap between (|) px t and (| 1) px t + , because of the dramatic changes of the conditional probability p ( y | x ), KMM performs poorly in classifying instances in the yet-to-come data chunk. OWA, on the other hand, is ab le to adjust its weight values and build accurate classifiers to predict instances in the yet-to-come data chunks. In this section, we report di fferent methods X  performance on the KDDCUP X 99 dataset. As we men tioned before, we only choose instances from class  X  X OS X  and  X  X ormal X . From Table 5, we can see that Ave. and Acc. have similar accuracies on randomly selected and shuffling selected samples. The accuracies of KMM and OWA are 99.67% and 99.06% respectively on randomly selected samples; while 97. 84% and 97.80% respectively on Shuffling selected samples. Compared with the Ave. and Acc., KMM and OWA have no additiona l advantages over Ave. and Acc.. In the Rearranged process, the percentages of each class change rapidly, and the percentage of class  X  X OS X  may decrease from 0.82706964 in the 1 st chunk to 0.03314917 in the 100 chunk, while the percentage of class  X  X ormal X  increases from 0.17293036 in the 1 st chunk to 0.9668508 of the 100 th accuracy also shows big difference, where KMM X  X  accuracy is 78.52%, while OWA is 73.84%, which is between Ave. (72.90%) and Acc. (74.77%). This mean s that KMM performs the best among all methods under this circum stance. We believe that the reason is because the data streams generated from the Rearranged process fits the LCD scenarios, although the prior probability does continuously evolve. Selection dataset, on which the distributions of the  X  X OS X  and  X  X ormal X  are not stable and con tinuously evolve (In Figure 5, we further list the actual prediction accuracies on each individual data chunks). As shown in the second column, when there isn X  X  any overlapping between adjacent c hunks, KMM reaches an average accuracy of 92.35% on 100 data chunks, while OWA is only 81.96%. When the degree of overlapping increasing, KMM marginally increases its accuracy from 92.35% to 92.53%, whereas the accuracy of OWA increases dramatically from 81.96% to 92.76% and finally reaches 94.16%. The above results show that KMM marginally improve its performances while OWA increase rapidly, with th e increasing of the overlapping between adjacent data chunks. We be lieve that the reasons are that KMM is essentially unsupervised learning method which relies on the instance matching between adjacent data chunks in feature space. Increasing the overlapping between adjacent data chunk is not helpful for KMM to build an accurate model, as long as the concepts drift rapidly in adjacent data chunks; On the other hand, OWA is guided by an evaluation set selected from the yet-to-come data chunk. Even if the concepts between adjacent data chunks vary significantly, as the increase of the data overlapping, the selected instances will help OWA build proper weight values to accurately classify instances in the yet-to-come data chunk. In this paper, we categorized the concept drifting in data streams into two scenarios: (1) Loose Concept Drifting (LCD); and (2) Rigorous Concept Drifting (RCD), and proposed solutions to handle each of them by using weighted-instance and weighted-classifier ensemble framework. More specifically, for LCD data streams, we proposed a Kernel Mean Matching (KMM) method to restore data distributions for accurate stream data mining. For RCD data streams, we derived an Optimal Weight Adjustment (OWA) method to find optimum weight values for base classifiers, such that they can form an accurate classifier ensemble with minimum error rates. We argued that if the concept drifting in the data stream is mainly caused by the gradual change of the prior using weighted-instance approach to restore the class distributions of the up-to-data data chunk and fu rther build classifier ensemble from the weighted instances is an effective solution for mining data streams. On the other hand, if the concept drifting is mainly caused by the rapid and random changes of the prior and conditional probabilities of the unde rlying data, using weighted-classifier ensemble approach is a superior solution. Experiments on synthetic and real-world data further asserted the performance of KMM and OWA under different concept drifting scenarios. The contribution of the work reported in the paper is threefold: (1) it categorized the concept drifting into two clearly defined categories (LCD vs. RCD); (2) we derived an Optimal Weight Adjustment process to a ssign weight values for a set of classifiers, such that the overall accuracy of the classifier ensemble built on them can reach the minimum; (3) we comparatively studied the performances of the instance-weighted and classifier-weighted ensemble framework, and further suggested the conditions that different methods may apply. 
Table 5: Average classification accuracies over all 100 data chunks (KDDCUP X 99 dataset) 
Table 6: Average classification accuracy over all 100 data chunks under relationship sel ection (KDDCUP X 99 dataset) Figure 5: The prediction accuracy comparison between KMM and OWA on each individual da ta chunks under different represents the prediction accuracy. This research has been supported by the National Science Foundation of China (NSFC) under Grants No. 60674109 and No. 70621001. [1] P. Domingos &amp; G. Hulten. 2000. Mining high-speed data streams, [2] G. Hulten, L. Spencer, and P. Domingos. 2001. Mining time-[3] B.Babcock, S.Babu, M.Datar, R.Motawani, and J.Widom. 2002. [4] C. Aggarwal. 2007. Data Streams: Models and Algorithms. Springer. [5] Y. Chen, G. Dong, J. Han, B. W. Wah, and J. Wang. 2002. Multi-[6] C. Aggarwal, J. Han, J. Wang, and P. S. Yu. 2004. On demand [7] R. Klinkenberg and T. Joachims.2000. Detecting concept drift with [8] Y. Yang, X. Wu, and X. Zhu. 2005. Combining proactive and [9] J. Gao, W. Fan, and J. Han, 2007. On appropriate assumptions to [10] W.Nick Street and YongSeog Kim, 2001, A streaming ensemble [11] J. Z. Kolter and M. A. Maloof. 2005. Using additive expert [12] M. Scholz and R. Klinkenberg. 2005. An Ensemble Classifier for [13] H. Wang, W. Fan, P. Yu, &amp; J. Han. 2003, Mining concept-drifting [14] X. Zhu, P. Zhang, X. Lin, and Y. Shi. 2007. Active Learning from [15] W. Dai, Q. Yang, G. Xue, and Y. Yu. 2007. Boosting for Transfer [16] H. Shimodaira, 2000. Improving predictive inference under covariate [17] M. Sugiyama, &amp; K. M X uller, 2005. Model selection under covariate [18] S. Bickel, M. Br X ckner, and T. Scheffer. 2007. Discriminative [19] Bickel, S., &amp; Scheffer, T. 2007. Di richlet-enhanced spam filtering [20] M. Dudik, R. Schapire, &amp; S. Phillips, 2005. Correcting sample [21] J. Huang, A. Smola, A. Gretton, K. Borgwardt, &amp; B. Sch X olkopf, [22] K. Tumer &amp; J. Ghosh.1996. Analysis of decision boundaries in [23] I. Witten &amp; E. Frank. 2005. Data mining: practical machine learning [24] D. Kifer, S. David, J. Gehrke . 2004, Detecing changes in data 
