 we focus on supervised distance metric learning.
 methods, and (2) robust and efficient for high dimensional da ta. Let D = { z is a vector of d dimension and y the norm of any example is upper bounded by R , i.e., sup | x  X  x  X  | 2 A = ( x  X  x  X  )  X  A ( x  X  x  X  ) .
 distance metric learning: where Let A I ( A ) denote the empirical loss , i.e., For the convenience of presentation, we also write g y light its dependence on A and two examples z true distribution, i.e., Given the empirical loss I error as uniform stability  X  if z . We further define  X  =  X /n as the uniform stability  X  behaves like O (1 /n ) . learning. This is because the example pair ( z although z constant for the regularized distance metric learning fram ework in (1). 3.1 Generalization Error Bound for Given Uniform Stability Our analysis utilizes the McDiarmid inequality that is stat ed as follows. Theorem 1. (McDiarmid Inequality) Given random variables { v R satisfying the following statement holds To use the McDiarmid inequality, we first compute E ( D following inequality for E ( D where n is the number of training examples in D .
 tion of examples that replaces z where g error by using the McDiarmid inequality.
 Theorem 2. Let D denote a collection of n randomly selected training examples, and A 1  X   X  , we have the following bound for I ( A D ) 3.2 Generalization Error for Regularized Distance Metric L earning First, we show that the superium of tr ( A sublinear in d . This is summarized by the following proposition. Proposition 1. The trace constraint in (1) will be activated only when where g Proof. It follows directly from [ tr ( A To bound the uniform stability, we need the following propos ition any examples z (b) | x | randomly selected example. Let A The proof of the above lemma can be found in Appendix A. of the Frobenius norm based regularizer.
 denoted by  X  , is bounded as follows where  X  = 32 CL 2 R 4 metric learning algorithm in (1) using the Frobenius norm re gularizer Theorem 4. Let D be a collection of n randomly selected examples, and A learned by the algorithm in (1) with h ( A ) = | A | 2 bound for the true loss function I ( A regularizer where s ( d ) = min  X  2 dg O ( s ( d ) / from the dimensionality of data. learning [2] by defining potential function  X ( A ) = | A | 2 algorithm.
 The theorem below shows the regret bound for the online learn ing algorithm in Figure 1. ( x t , x  X  t ) , y t , t = 1 , . . . , n metric M  X  S d  X  d where
 X  n ( M ) = Algorithm 1 Online Learning Algorithm for Regularized Distance Metric Learning 1: INPUT: predefined learning rate  X  2: Initialize A 0 = 0 3: for t = 1 , . . . , T do 4: Receive a pair of training examples { ( x 1 5: Compute the class label y t : y t = +1 if y 1 6: if the training pair ( x 1 8: else 10: end if 11: end for require computing  X  high dimensional data. To address this challenge, first noti ce that M  X  =  X  optimization problem M  X  = arg min  X y follows The following theorem shows the solution to the above optimi zation problem. Theorem 6. The optimal solution  X  presents the mistake bound for the separable datasets. for all the experiments according to our experience.
 puted as the inverse of covariance matrix of training sample s, i.e., ( P n different metrics. Standard deviation is included.
 RAM and Linux operation system. 5.1 Experiment (I): Comparison to State-of-the-art Algori thms which show LMNN is among the most effective algorithms for di stance metric learning. slightly better than ITML and is comparable to LMNN. online reg algorithms on the  X  X tt-face X  dataset with varying image sizes. 5.2 Experiment (II): Results for High Dimensional Data The original size of each image is 112  X  92 pixels, with 256 grey levels per pixel. k iteration, makes it unsuitable for high-dimensional data. their effect for distance metric learning. ACKNOWLEDGEMENTS thors and do not necessarily reflect the views of NSF and ARO. We define convex function N ( X ) and V and furthermore convex function T The first inequality follows from the fact that both N ( X ) and V step holds because matrix A respectively, and therefore Since d which leads to the result in the lemma.
 Proof. We denote by A  X  11.1 and Theorem 11.4 [2], we have where Using the relation A  X  By assuming | x | X we thus have the result in the theorem
