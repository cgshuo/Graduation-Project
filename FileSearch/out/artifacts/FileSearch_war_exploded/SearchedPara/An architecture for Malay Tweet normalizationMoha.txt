 1. Introduction
Social Network Services (SNS) and Microblogs are an increasingly popular form of communication. The explosion of SNS and Microblogs has drawn the attention of researchers to this topic like opinion mining and sentiment analyzing ( Thelwall, tent (UGC) is the existence of noisy text. Most of the Natural Language Processing (NLP) and text-mining algorithms are designed to apply to clean texts. Along with more and more demand for UGC processing, the research in normalization, the process of cleaning noisy text, has become an increasingly important topic.

This paper proposes an approach to normalize Malay Twitter messages. In 2012, Twitter, the most popular microblogging ing language that is practiced over Twitter ( Hong, Convertino, &amp; Chi, 2011 ).
 studied language category from a computational point of view. For example, to the best of our knowledge, currently there is no study on the Malay spellchecker and Named Entity Recognition (NER). The limited resources of the Malay language  X  inspired us to follow a specific methodology to build the Malay Tweets normalization system. The methodology articulates implementing a normalization architecture based on the results of the analyses, and (3) evaluating the system and compar-ing it with other approaches.

The remainder of this paper is organized as follows: Section 2 presents related works on normalizing noisy text. Section 3 discusses the analysis of Malay Tweets. Then, the proposed architecture is introduced in Section 4. Section 5 discusses the results of the evaluation. Finally, Section 6 concludes this paper with a brief summary and future work. 2. Related works Aw, Zhang, Xiao, and Su (2006) employed the Statistical Machine Translation (SMT) method to normalize English SMS. They considered the texting language as a source language, and the Standard English as a target language. One of the short-comings of their approach is that the system can only normalize tokens that are seen in the training set. Kobus, Yvon, and Damnati (2008) combined an ASR-like system with the SMT metaphor to boost the accuracy of the results. To have unlimited noisy-regular parallel data for the training of the SMT system, Gadde, Goutam, Shah, Bayyarapu, and Subramaniam (2011) tion, word merging, typing errors, word dropping, and de-capitalization/capitalization. The first work on normalizing Tweets performs preprocessing based on the orthographic and syntactic features of a Tweet ( Kaufmann &amp; Kalita, 2010 ). The main normalization process is to apply SMT to English Tweets. In addition, the SMT approach is considered useful to enhance Text-to-Speech (TTS) systems ( Lopez Lude X a, San Segundo, Montero, Barra Chicote, &amp; Lorenzo, 2012 ).

In 2007, a normalization method was introduced, which mimics the spelling error correction model ( Choudhury et al., 2007 ). Instead of a character level noisy channel, a word error model based on the Hidden Markov Model (HMM) was built. The HMM word error model along with a unigram language model can normalize English SMSs with considerable accuracy. However, unlike the SMT-like system, the model ignores the context around the token. Cook and Stevenson (2009) modified the supervised approach of Choudhury et al. (2007) to have a lightly-supervised method. Three word formation probabilities accuracy.

There are other studies that take completely different approaches from SMT and noisy channel metaphors. An architec-perform tokenization and de-tokenization, respectively, based on manually written rules. The second component normalizes the tokens based upon the trained phonetic model. The results of the evaluation show encouraging performance in terms of BLEU and WER scores, although SER is too high due to the phonetic similarities and complexities in French SMS.
Han and Baldwin (2011) introduced a lexical approach for normalizing Tweets. After identifying Out of Vocabulary (OOV) tokens and generating a set of candidates, the system calculates the dependency between tokens using the Stanford parser. The next step is to exploit the linear SVM classifier to determine ill-formed words. The best candidate is selected using a variety of metrics: phonemic edit distance, lexical edit distance, affix substring, Longest Common Subsequence (LCS), lan-guage model, and dependency-based frequency features. Although, the approach achieved a high F-score and BLEU score, the system performs below par in highly noisy Tweets. It has been proven that using the Han and Baldwin (2011) approach in a time sensitive Twitter search produces more appropriate results ( Wei, Zhou, Li, Wong, &amp; Gao, 2011 ).
Clark and Araki (2011) built a Trie-type dictionary that can store phrases for normalizing casual English. The dictionary consists of 1043 items and end-users can add more items to it. The Trie data structure enables us to perform a prefix search and context aware loop-up. Our proposed architecture also uses a Trie-type data structure. In 2012, Han, Cook, and Baldwin (2012) proved that dictionary based systems can outperform state-of-the-art approaches. To compile a dictionary, they gath-ered 10 million English Tweets, and selected OOV tokens, which have a high occurrence frequency. Then, for each OOV, the most similar morphophonemic In Vocabulary (IV) word was chosen from the gathered corpus. This inspired us to develop a dictionary-based module in our architecture.

To the best of our knowledge, there are two only studies on Malay normalization. The first study developed a dictionary-based system, known as NoisyTerm, to normalize the content of Malaysian online media ( Samsudin, Puteh, Hamdan, &amp; Nazri, 2012 ). Our approach solved the ambiguity problem in NoisyTerm by developing a context-aware dictionary. The second study introduced a Malay normalization approach that has not been evaluated by standard metrics ( Basri, Alfred, &amp; On, stemmer ( Kadir, Musa, Azman, &amp; Abdullah, 2011 ), which was designed to work in standard Malay, causing the loss of the original term, whilst we normalize the original words along with their affixes. 3. Analysis The design of the normalization system hinges on the results of the corpus-driven analyses. The data analysis is based on TF X  X DF schema ( Aizawa, 2003 ). Before designing the normalization system, we performed four analyzing tasks: three analyses on a Malay Twitter corpus and one analysis on the standard Malay corpus. Several studies adopted the corpus-dri-ven approach because the descriptions of the linguistic features of texting are based on the analysis of the text messages emerging through word-frequency lists, concordancing, keywords, and clusters ( Sinclair, 2004 ). 3.1. Corpus preparation
We constructed the Malay Twitter corpus due to the unavailability of a corpus. We gathered 1 million Twitter messages, consisting of 14,484,384 word instances and 646,807 vocabularies, and named it the Malay Chat-style-text Corpus (MCC). To build the corpus, which represents Malay Twitter lingo, corpus compiling criteria were envisaged: repressiveness, sampling, balance, machine readability, size of data, and definite aim of designing corpus ( McEnery &amp; Hardie, 2011 ).
It has been proven that sampling has the highest priority in achieving representativeness ( Smith, 1976 ). The sampling includes three stages: (1) defining the target population; (2) specifying the sample frame; and (3) gathering data according to the selected sampling technique. The definite population was defined as the Malay Twitting lingo. To cover a variety of language styles, the sampling frame included 4500 Twitter user-Ids, for which the profile X  X  location is set as Malaysia to cover a variety of language styles. We selected the purposive sampling technique, which makes decisions based on the pop-ulation of interest ( Tongco, 2008 ). Therefore, a linguistic expert selected 321 users from the sampling frame in order to exclude those who tweet in a language other than Malay and who tweet in formal Malay language, such as posting commer-cial and political messages. Finally, 3200 messages were fetched from each user by Twitter APIs.

Moreover, the representativeness of the corpus was evaluated from two different perspectives: (1) cartography and (2) automatic language identification. We drew the geolocation (latitude/longitude coordinates) of the Tweets on a world map in order to track the actual place of users while sending messages. The similarity between the distribution pattern of the geolocations of the MCC posts and the population density of Malaysia shows the representativeness of the corpus. obtained confidence scores show that the boundary of the desired population is well determined. 3.2. Frequency of unknown words
We analyzed the MCC in order to calculate the distribution frequency of unknown words. Table 1 shows the top 20 most frequent words in the MCC. These most frequent words indicates that there are large number of OOV words in Malay Tweets.
The number of IV words and English words are five and six, respectively. In addition, there are six abbreviated words and only one interjection.

We defined five categories of word forms: IV words, words with extra repeated letters, English words, OOV words with special characters, and other types of misspelled words. The narrow categorization was chosen because of the difficulty in distinguishing between types of misspelled words. We calculated the number of IV words by seeking them in Bahasa Word-
Table 2 refers to the percentage of each category in the corpus. Ergo, more than 60% of MCC is composed of OOV words, and a large fraction of them are English words.
 To have a more precise understanding of OOV frequencies, we divided special characters into two groups: Thin-group and Thick-group (see Table 3 ). The Thin-group represents the most common special characters (8 types). The members of the Thin-group are very common in the ordinary Malay writing system. The Thick-group includes special characters that are usually printed on ordinary computer keyboards but excludes characters in the Thin-group (24 types). Table 2 indicates that the percentage frequency of OOV words that contain one of the special characters from the Thin-group is only 0.5%; in con-trast, the percentage frequency of those OOV words that contain one of the special characters from the Thick-group is 8.1%. This shows that those words that contain one of the special characters from the Thin-group have a low probability of being detected as an OOV word. 3.3. Abbreviation patterns
Most of the frequent words used in MCC are abbreviated words. This shows that most users have the tendency to abbre-viate words. By scrutinizing abbreviated words, we discovered eight major types of abbreviation in chat-style Malay, as shown in Table 4 . The first two rows in Table 4 imply that two types of abbreviation have solid and predictable features: reduplication and negation. Reduplication is a morphological method for producing new meanings or expressing grammat-ical functions in Malay and many other languages. In Malay, the grammatical rule for producing a negative sentence is the insertion of the word tidak before the verb or adjective. 3.4. Letter repetition
One of the significant contributions of this work is that it presents a method for eliminating repeated letters from Malay words. To find the morphological features of the Malay word, we carried out an analysis on the Dewan Bahasa dan Pustaka gather and keep more than 450 loan words as a bag of words. The other eight conditions pertain to the word affixation. 3.5. Miscellaneous information
The corpus-driven analysis reveals a variety of information about Malay Tweets, such as the distribution frequency of at-signs, number-signs, asterisks, and hyperlinks; however, only periods and capitalization are described here for the sake of brevity. Only 68% of Tweets contain periods, 683,827 periods in one million Tweets. In other words, 4.72% of words contain The results of the analysis on capitalization show that only 49.4% of Tweets begin with capital letters. Table 7 shows the exact figures regarding capitalization in MCC. 4. Architecture
A set of 9000 parallel word-aligned Tweets, consisting of raw (un-normalized) messages and reference messages, was manually prepared by two project members with the inter-annotator agreement checked. We named it the Malay Parallel
Tweets (MPT), and used it for developing and testing the architecture. The architecture includes seven modules, as depicted in Fig. 1 . Each module employs a Finite-state machine to describe most of the relevant local phenomena encountered in the empirical study of Twitter language. Table 8 refers to 13 types of labels that we use in the architecture. The labels are assigned to tokens in different modules. We defined three main labels, PN, IW, and NT, referring to proper nouns, In-vocab-characters. 4.1. Enhanced tokenizing
Conventional tokenization algorithms cannot be applied to the colloquial text due to its unpredictable characteristics. The proposed tokenization module helps to detect proper nouns that contain special characters. The tokenization algorithm,
According to our OOV word analysis, these symbols are very common in standard writing style, while other symbols occur in be used to express emphasis and emotion. Therefore, identifying the end of a sentence is an arduous task, although we can detect End-of-Line (EOL) by finding the EOL special characters.

In the first step, we convert all capital letters to lowercase because uppercase does not have orthographic value in Tweets (see Section 3.4). After converting consecutive blanks to a single blank, we attend to period marks in the third step. The results of the analysis indicate that most of the sentences do not end with a period. We assume that if a period does not at the last character, the period will be eliminated and the word will be tagged with the DC label. The fourth step is to eliminate all EOL characters and tag the EL label to the words that occur before them.

After converting white spaces to a new line, we consider each line as a token. In the sixth step, if the last character of a token is an exclamation mark, colon sign, question mark, right quotation marks, comma mark, or right parenthesis, we will delete the character and tag the appropriate label (EM, CC, QP, RQM, CM, or RP) to it. The de-tokenization module needs the will be tagged with the PN label. 4.2. Finding IV words
Before starting modification of the tokens, we need to distinguish between IV and OOV words. This module takes each token, and searches for it in a Trie data structure. To have fast access to IV words, we inserted all the IV words from the do not have the PN (proper noun) label. If a token is found in the q-Trie, we will tag it with the IW label.
Trie, also known as the prefix tree, enables high speed longest-prefix matching. Since we have access to a limited number of IV words (most of them are root words), and Malay is a highly inflected language, using the Maximum-prefix-length match enhances the coverage, and boosts the recall. Therefore, we traverse through q-fast, using characters of the input token. If a token X  X  prefix matches a word, the module will store the current length, and look for a longer match, finally, threshold values were tried during the testing phase. To mention an example, suppose Fig. 3 is the data structure, where the threshold value is set to 3, and the input token is toes . The module distinguishes the token as an IV word, and tags the IW label onto it. 4.3. Search in colloquial dictionary
To build the dictionary, we first used 7500 Tweets from the Malay Parallel Tweets (MPT), including 33,878 ill-formed lation from the original words  X  from MPT along with their preceding and following words, and their translation equivalent. We then counted the occurrences of each word group (preceding word, ill-formed word, following word). Finally, word groups, that appeared two or more than two times, were inserted into an XML file.

Due to the small number of entries and simple scheme of the dictionary, the Python dictionary data structure was employed to implement the dictionary. The Python dictionary is mutable, and consists of pairs (called items) of keys and their corresponding values, where the keys are unique in a dictionary. We integrated the ill-formed words groups into to an example of the Python dictionary items. 4.4. Eliminating repeated letters
A unique elimination method is proposed to tackle the letter repetition in Malay words. However, this method is not ule uses a pattern finder to check if a word matches the patterns. Fig. 5 shows the algorithm for this module, which com-prises three steps. The first step is to check if the token has letter repetition in a row, and send distinguished tokens to tokens with NT label.

In the third step, Regular Expression (RE) comes to aid in determining if tokens conform to one of the conditions. Extra letters will be removed based on the matched pattern. If a token matches with one of the conditions, we will eliminate repeated letters in a certain way that does not disturb the pattern. For example, the term  X  X erasaaann X  will be converted to  X  X erasaan X  because it matches Condition 7. In contrast, the term  X  X ayyaaaa X  will be converted to  X  X aya X  because it does not match with any pattern, thus all the repeated letters are reduced to only one letter. 4.5. Normalizing blogger writing style
The fifth module of the architecture concentrates on two common abbreviations in the Malay lingo. This module normal-the whole reduplication abbreviation. Malay has two types of reduplication form: partial and whole. Whole reduplication is the process of adding a dash to the right side of a word and repeating the whole word after the dash (i.e. Word ? Word-Word). In order to abbreviate whole reduplication, the second word and its preceding dash is replaced with the digit  X 2 X  (i.e. Word-Word ? Word2). The second abbreviation style is the abbreviation of negation. The standard negation marker Verb ? Subject x Verb).

The input of this module is tokens, which do not have any PN, NW, IW, or NT label. Therefore, the tokens do not contain special characters or digits because it was checked and confirmed in the tokenization module. Consequently, if the last
Word2 to Word-Word (For example: buku2 ? buku-buku (books)). In addition, if the first character of a token is the letter x, the letter will be replaced with tidak followed by a white space, i.e. converting x Word to tidak Word (For example: xsenang ? tidak senang (not free)). Finally, modified tokens are tagged with the NT label. 4.6. Translating English words
As aforementioned, code-switching between Malay and English is very frequent in Malay Tweets. Therefore, this module Python dictionary data structure. Tokens, which do not have PN, NW, IW, or NT labels, are inserted into this module. We search for tokens in the dictionary, and replace them with their meaning. 4.7. De-tokenizing
The de-tokenizing module, which consists of five steps, undoes the impact of the tokenizing module. Firstly, tokens that have LQM or LP are detected and a quotation mark or open parenthesis will be added to their beginning. Secondly, the module distinguishes tokens with EM, CC, QP, RQM, CM, or RP tags, and adds the appropriate special character to the end character is created after tokens that have the EL tag. Lastly, the module removes all tags. 5. Experimental results
Aside from studies that choose extrinsic evaluations, such as Wei et al. (2011) , and Samsudin et al. (2012) , BLEU has become an acceptable evaluation metric in normalization research. We tested the architecture and SMT-like system with the same real data (MPT dataset) to discover their accuracy in terms of the BLEU score ( Papineni, Roukos, Ward, &amp; Zhu, 2002 ). 5.1. Architecture evaluation We built a prototype of the proposed architecture, using the Python programming language. Since the first 7500 Tweets of word and message, the message-aligned edition was used to test the architecture. We examined the architecture with a vari-can improve the BLEU score by 0.37.

The result shows that the prefix search reduces the accuracy of the system, and that the second module works better without a prefix search. The reason behind that is the limited size of the Bahasa Wordnet, and the alteration at the end in the Trie, the second module will mistakenly distinguish it as an IV word, whilst the correct word is terbaiknya . 5.2. SMT-like evaluation We divided the MPT dataset into 6 equal sets (1500 Tweets) for 6-fold cross validation. The experiment was done using Moses ( Koehn et al., 2007 ). Giza++ ( Och &amp; Ney, 2003 ) was employed to perform word alignment. The SMT-like system was tested with both the word-aligned version and the message-aligned version of the MPT dataset; however, higher accuracy was achieved with the gold word alignments. Although Moses allows setting the distortion limit to between 0 and 7 for reordering phrases, our experiment without reordering phrases produced better results because we were not translating a language to another language.
 The reference part of MPT, which contains 108,373 words, was fed into SRILM ( Stolcke, 2002 ) in order to build the trigram Language Model (LM) by applying Kneser X  X ey smoothing. Kaufmann and Kalita (2010) asserted that using cleaned Twitter messages instead of the conventional corpus for compiling LM could boost the accuracy of the system. From the experiments, it was found that if the LM has a weight of 0.7; the BLEU score would be increased by 0.4. Table 10 shows the results of the SMT-like system.
 5.3. Discussion
The architecture and SMT-like system attained BLEU scores of 0.83 and 0.81, respectively. This result proves that a normalization system, which was constructed based on the results of analyses, can outperform the state-of-the-art systems.
However, several limitations of our method were detected by analyzing the output of the system. The most obvious one is ing tokens are misspelled. Another shortcoming is that the English translation module can only translate the correct English words, but not the words with spelling errors. Table 11 shows that our architecture can obtain more than an 80% increase in the BLEU score. The achieved BLEU score par excellence shows that the architecture possesses a reasonable level of competence.

The experimental results in Han and Baldwin (2011) show that normalizing Tweets obtained a higher BLEU score com-pared to SMS messages. Kaufmann and Kalita (2010) indicated that the initial Tweet BLEU score is higher than SMS, that is,
Twitter messages have less OOV tokens compared to SMS. However, Kaufmann and Kalita (2010) asserted that the normal-unknown word analysis prove that, to a great extent, Malay Tweets are noisy. 6. Conclusion and future work Recent years have witnessed the explosive growth of online Social Network Services. Twitter, with nearly 400 million Tweets per day, is the most used and well-known worldwide microblogging service ( Lehmann, Castillo, Lalmas, &amp;
Zuckerman, 2013 ). The sheer volume of messages on Twitter is causing Tweets to become a valuable resource for research-ers. However, most of the NLP and text-mining methods are constructed to apply to normal text. Twitter messages contain huge ill-formed words, which are also known as noisy text. Therefore, to process the Twitter messages, normalizing the noisy text is the initial hurdle to overcome. The objective of this paper is to normalize Malay Tweets, the fourth most used language in Twitter.

We designed a normalization architecture based on the features of colloquial and standard Malay. To extract the characteristics of normal and chat-style Malay, four corpus-driven analyses were undertaken: (1) analyzing the frequency distribution of unknown words indicates that English terms are the most prevalent unknown words, followed by abbreviated words and letter repetition; (2) analyzing abbreviation pattern demonstrates that users follow certain methods for some morphology reveals that there is no sequence of repeated letters in a row in Malay morphology; and (4) inspecting the status of periods and capital letters in Malay Tweets demonstrates that most Twitter sentences do not end with a period and do not begin with a capital letter.

The normalization architecture includes seven modules in a pipeline workflow. The first one is the enhanced tokenization module, designed based on attributes of Malay Tweets. This module can also detect certain types of proper noun. The second module is concerned with distinguishing IV words to protect them against alteration in the next modules. The third module fourth module is the elimination of extra repeated letters, followed by the abbreviated words correction module. The sixth module is the English word translation module and the final module is de-tokenization.
 The architecture was implemented using the Python programming language and its accuracy was measured using the BLEU score. The system was tested over 1500 parallel Malay Tweets. According to the experimental results, the proposed malization system was implemented and evaluated. The SMT-like normalization, which considered the noisy text as a source parallel Malay Tweets, the .81 BLEU score was achieved. In conclusion, the evaluation of the approach showed that it is capable of normalizing the Malay Tweets with high accuracy. It is widely accepted that a spellchecker can improve the per-is no study on the Malay spellchecker. For future work, the architecture might be integrated with the Malay spell correction system.
 Acknowledgement References
