 Context surrounding hyperlinked semi-structured documents, externally in the form of citations and internally in the form of hierarchical structure, contains a wealth of useful but im-plicit evidence about a document X  X  relevance. These rich sources of information should be exploited as contextual ev-idence. This paper proposes various methods of accumulat-ing evidence from the context, and measures the effect of contextual evidence on retrieval effectiveness for document and focused retrieval of hyperlinked semi-structured docu-ments.

We propose a re-weighting model to contextualize (a) ev-idence from citations in a query-independent and query-dependent fashion (based on Markovian random walks) and (b) evidence accumulated from the internal tree structure of documents. The in-links and out-links of a node in the citation graph are used as external context, while the inter-nal document structure provides internal, within-document context. We hypothesize that documents in a good context (having strong contextual evidence) should be good candi-dates to be relevant to the posed query, and vice versa.
We tested several variants of contextualization and veri-fied notable improvements in comparison with the baseline system and gold standards in the retrieval of full documents and focused elements.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models ; H.3.4 [ Information Storage and Retrieval ]: System and Software X  perfor-mance evaluation ; H.2.1 [ Database Management ]: Log-ical Design X  data models ; E.1 [ Data ]: Data structures X  trees ; E.5 [ Data ]: Files X  organization/structure XML retrieval, Semi-structured data, Structural indices, Sch-ema agnostic search, Contextualization, Re-weighting, Ran-dom walks
Focused or element retrieval addresses the possiblity to utilize the hierarchical structure of documents, and hence return the most specific (and exhaustive) text units, rather than returning only full documents. One problem with this approach is that the retrieval units have varying length in textual content, as the size of elements varies with the level in the hierarchy (see Figure 3); the leaf element or descan-dent elements have less textual evidences than their ances-tors. This scant textual evidence makes matching those small text units, such as paragraphs, hard. As a conse-quence, although they are what the users (might) require, they are considered less relevant by the focused retrieval sys-tems, only because they have too few textual content, hence too little evidence to be ranked higher for the posed user query. Fortunately, this scant textual evidence can be allevi-ated significantly by a method called Contextualization [16].
Contextualization is a mechanism to estimate the rele-vance of a given structural text or document unit with in-formation obtainable from -besides the unit itself -the sur-rounding structural text or document units, i.e., from the context of the unit [16]. With contextualization, we assume that context of a retrievable unit gives hints about the rel-evance of the retrievable unit (can be document or element retrieval). Hence, it is expected in contextualization that context of a retrievable unit gives hints about the relevance of the retrievable unit.

In this study, we incorporate the idea of random walk to-gether with contextualization on citation structure of doc-uments and internal hierarchical structure of XML docu-ment. The approach is inspired by the random surfer model of [5, 10] over XML documents and relational databases re-spectively, as well as the contextualization model for XML retrieval developed by Arvola et al. [4]. The hypothesis is that contextualization together with random surfer (or walk) model will improve search effectiveness over considering re-trieval units in isolation.

Until recently, the importance of contextualization (based on hierarchical relationships of element) has been studied in several settings [1, 2, 4, 19, 22, 23, 25]. Even in a schema-agnostic environment, it has been found that by contex-tualizing the scores of the surrounding components, such as, parents, ancestors or siblings in the scoring function of the element itself, the overall precision and recall of the fo-cused retrieval system improves [4]. In document retrieval, the hyperlink structure of documents (i.e., inlinks and out-links) provides both a wider context and a wider seman-tics to the content. This far-reaching context and semantics should possibly be used to boost or reduce the documents retrieval scores. Without using the structural information (citations graph), the search system would simply ignore the documents containing a wealth of implicit information in its context as irrelevant to the query topic in question. Contex-tualization based on the bibliographic structure of scientific documents has been shown a promising direction in [22].
The models proposed in this research paper are experi-mentally evaluated using the semantically annotated Wiki-pedia XML Collection from INEX [26], both at the granu-larity of a document (document retrieval) and at the XML element level (focused retrieval). We have applied several variants of contextualization, and the results are in-line with the proposed theory about the effectiveness of contextualiza-tion. The results obtained, on both document (article level) and focused retrieval (paragraph level) tasks, exhibit clear improvements over a strong and competitive baseline system  X  itself based on data fusion over all INEX 2009 submitted runs (see Section 3), and already achieving a performance higher than any INEX 2009 official run.

Summarizing, the contributions of this study include: Section 5 concludes and highlights future work.
Contextualization is a method of exploring the features in the context of a retrievable unit [4]. In document retrieval, in turn, this means combining the evidences from a document and its context using different but plausible combination functions. The context of a document (i.e., contextualizing documents) consists of other documents which point-to or are pointed-to by the document in question ( contextualized document, P2), see Figure 1. The context of an element in focused retrieval and in this study consists of all the an-cestors of the element in question. We use random walks to induce a similarity structure over the documents based on their bibliographic relationships, and over the elements based on the containment and reverse-containment relation-ships (element, sub-element and vice versa). Hence, these re-lationships affect the weight each contextualizing document or element has in contextualization. A contextualization model is a re-scoring scheme, where the basic score, usually obtained from a fulltext retrieval model, of a contextualized document or element is re-enforced by the weighted scores of the contextualizing documents or elements.
 Figure 1: Citation structure of 5 documents and context of P2
The premise is that good context (identified by random walk and contextualization) provides evidence that a docu-ment in document retrieval and an element in focused re-trieval is a good candidate for a posed query and therefore documents and elements should be contextualized by their bibliographically similar documents and hierarchically simi-lar elements respectively. Good context is an evidence that should be used to deduce that a document or an element is a good candidate for the posed query.

In Section 2.1 we will explain the idea of contextualiza-tion based on citation structure, in Section 2.2 we elaborate on contextualization based on the internal hierarchical struc-ture of XML document (see XML document in Figure 2) and in Section 2.3 we present a contextualization model based on first the citation contextualization and then hierarchical contextualization.
There are enough empirical and intuitive support for the premise that a good document in citation graph is good be-cause it contains references to alot of good documents, and more importantly, a good document is good if it is con-tained in a good document as a reference (recursive defi-nition) [13, 17, 20]. But here, the question is, can the ev-idences, lying loosely in the context surrounding the con-textualized document, be intelligently materialized? Fortu-nately, the answer is yes, later in the section we will show a formalism that can be used to materialize and then utilize the contextual evidences for improving retrieval effective-ness.

Previous work [1, 4] presents a contextualization model where a binary vector represents the relevant context (a part of) a document. Here, we extend that work to use proba-bilistic infomation derived from a random walk over the ci-tation structure. A random walk on the citation structure of the documents independent or dependent of a query topic will populate the contextualization vector with the probab-listies that indicate authority of a document in the network of citations.

An alternative way to conceive the intuition behind the random walk model here is, to consider that authority and relevance information flows in the bibliographic structure of documents in the same fashion as that of the HITS model [17]. The authority flows in the bibliographic structure of doc-uments until an equilibirium is established which specifies that a document is authoritative if it is referenced by au-thoritative documents [20]. The bibliographic network of documents (for example, Figure 1) can be represented in matrix notation by adja-cency matrix A such that: A The reverse edge  X  , very small value, is added to ensure a unique solution to the system of linear Equations 1. For Figure 1 the corresponding adjacency matrix A can be: The random walk probabilities are then obtained by itera-tively solving the following system of linear equations 1 Here g k is the proposed contextualization vector, and k is the number of iterations. The matrix A T A constructed this way would lead to a unique solution to the system of linear Equations 1 [17].
A query independent random walk is conducted on the en-tire bibliographic structure of the documents, irrespective of any query. This walk primarily captures the authoritative-ness of documents in the collection. The adjacency matrix A becomes quite huge for the citation structure of Wikipedia collection (2 , 668 , 160  X  2 , 668 , 160, see Section 4.1). The contextualization vector g k depicts the scores of each docu-ment in the massive citation graph for the entire collection iteratively calculated using Equation 1.

A query dependent random walk is conducted on the rather smaller subset of the citation graph, corresponding to a spe-cific query topic in question. Adjacency matrix A is in this case considerably smaller then the query-independent walk. The contextualization vector g k depicts the stationary dis-tribution of random walk (scores of documents) specific to a query. The focused subgraph can be constructed from the output of per topic output of fusion run, which can be used to iteratively produce set of documents that are most likely considered to be relevant to the query topic. The Base-set S q (which is used to form A ) can be obtained by growing query results (Root-set R q ); which includes any document that pointed to by a document in Root-set R q , and any doc-ument that points to a document in R q , i.e., inlinking and outlinking documents from root-set R q respectively.
We now give a tailored re-ranking function CR , which allows the contextualizing scores to be added to the basic
Finding the dominant Eigenvector of the system of linear equations, corresponding to the dominant eigenvalue, which is 1 in this case [20]. scores. The function can be formally defined as follows: where
We can have several variants of the combination function of Equation 2, as discussed in forthcoming Sections below.
Do documents cited a lot, or documents containing more in-links or authoritative documents form a good context? Let X  X  assume that the context function C x in Equation 2 only contextualize based on the in-links. In this case the argument would be: C x  X  inlinks ( x ). The set C x only contains the in-links of the contextualizing document. The inlinks of a document x corresponds to its column in the adjacency matrix A . For example, the inlinks of document P 2 in the Figure 1 correspond to the non-zero cells of column 2 in the adjacency matrix A .

Section 4 presents experiments with two variants of con-textualization: 1. first based on random walk conducted on query inde-2. second based on query dependent random walk on ad-We have experimented with both of the approaches, see Sec-tion 4. In addition to the two variants, a third variant com-bines the query independent and query dependent random walk into a combination function: &lt;article xmlns:xlink= "http://www.w3.org/1999/xlink/" &gt; &lt;/header&gt; &lt;body&gt; &lt;section&gt;&lt;st&gt; Introduction &lt;/st&gt; &lt;/section&gt; &lt;section&gt;&lt;st&gt; Language Components &lt;/st&gt; &lt;/section&gt; &lt;section&gt;&lt;st&gt; See also &lt;/st&gt; &lt;/section&gt; &lt;/body&gt; &lt;/article&gt; where
The existence of inlinks for contextualized document is certainly a positive indication, but outlinks also happen to occur in the contextualized document X  X  context. By linking to another document, the author implicitly includes the out-linking document in its document context. Inlinks together with outlinks provide a much wider context for the con-textualized document. Combination functions, Equations 2 and 3 remain the same, only the interpretation of the con-textualization function changes now to: C x  X  ( inlinks ( x )  X  outlinks ( x )). The set C x now contains the inlinks and out-links of the contextualizing document, containing the query term. The outlinks of a document x correspond to its row in the adjacency matrix A . For example, the outlinks of document P 2 in the Figure 1 corresponds to the non-zero cells of row 2 in the adjacency matrix A .
Hierarchical contextualization model has been studied be-fore in different settings in XML retrieval [1, 4, 16, 19, 25, 27]. In hierarchical contextualization we tend to utilize the intrinsic structure within the XML document. The represen-Figure 3: XML Graph of Figure 2 with context of element  X  1.2.1.2  X  (dewey encoding) tation of documents in XML aims to follow the established structure of documents, i.e., an academic book is typically composed of  X  chapters  X  ,  X  sections  X  ,  X  subsections  X  etc., tags. This organization of document gives an intuitive start-ing point for manipulating text passages at the established hierarchy levels of text documents.

With contextualization on hierarchical structure of docu-ments we aim to rank higher an element in a good context than an identical element in a not so good context within the document. In Figure 2 the  X  article  X  ,  X  section  X  and  X  subsection  X  form different levels of context for a para-graph  X  p  X  . Hence the paragraph can be viewed in context of  X  subsection  X  ,  X  section  X  or the  X  article  X  . While the root element  X  article  X  possesses no context.

In hierarchical contextualization the weight of the element is modified by the basic weights of its contextualizing ele-ments. Each element in the context of the contextualized element, should possess an impact factor. An higher impact factor shows the importance of the contextualizing element and vice versa. The role and relation of contextualizing ele-ment are operationalized by giving the element a contextu-alizing weight. A contextualization vector is defined to cap-ture the impact factor of each contextualizing element, and this contextualization vector is represented by a g function, in a similar way as it is defined in citation contextualization.
The important research question here is: which types of el-ement context help to improve retrieval effectiveness? More specifically which types of context serves our purpose, which is, to boost the ranking of contextualized element in good context and vice versa. Sigurbj  X  ornsson et al. (2004) [27] argued that by taking the root level only (i.e.,  X  article  X  el-ement in the example case) as a context improves the overall retrieval. Camps (2007) [25] later also found that the use of article as a contextual information clearly helps to improve retrieval effectiveness. Arvola et al. (2005) [1] uses a bi-nary value to include or exclude different element types in hierarchy from the context. Ogilvie and Callan (2005) [23] utilizes the children of the element to smooth up the par-ents (smooth up tree). The smoothing up method in their hierarchical modeling is quite similar to contextualization. In it they contextualize the scores of individual keywords instead of whole elements. In the vertical contextualization approach again by Arvola et al. (2011) [4] the impact or strength of the contextualization is adjusted with a help of different parameters. Instead of considering only a specific element as a context or using the children to smooth up the parent element or using a parameter to find the impact of each of the units in the context, we propose a generalized mechanism based on the Markovian Random walk principle.
The tree-structure of the XML document is considered as a graph. Myriad of random surfers traverse the XML graphs. In particular, at any time step a random surfer is found at an element and either (a) makes a next move to the sub-element of the existing element by traversing the containment edge, or (b) makes a move to the parent-element of the existing element, or (c) jumps randomly to another element in the XML graph. As the time goes on, the expected percentage of surfer at each node converges to a limit the dominant eigen-vector of the XML graph. This limit provides the impact or strength of each element in the context of the contextu-alized element in the form of g function. We consider all the ancestors of the contextualized element in contextual-ization; where the contextualization vector g identifies the importance of each of the unit of context (see Equation 4).
Contextualization model formulated in this way, is inde-pendent of the basic weighting scheme of the elements and it could be applied on the top of any query language and retrieval systems. We have applied the contextualization model on the top of the baseline system which is the result of fusion from the INEX 2009 offically submitted runs by the participants (see Section 3.2).

In the experiments we evaluated the retrieval effectiveness at different granularity levels. We mainly tested, retrieval ef-fectiveness at article level (  X  article  X  element), and at para-graph level (  X  p  X  element); a brief intuition is explained in Section 3.3. The most improvements in retrieval are ob-served when  X  p  X  elements are retrieved. The primary rea-son is because paragraph has the most context (hierarchical depth) and most specific element in context (see Figure 3).
The re-ranking function based on the random walk prin-ciple described earlier can be formally defined as follows: where
Hybrid or twofold contextualization is when the externally accumulated evidences re-enforce the evidence accumulated from within the hyperlinked and hierarchical XML docu-ments. In this approach we first select the best documents based on the citation contextualization (Section 2.1) and later retrieve the most relevant and most specific context from the XML hierarchy using the hierarchical contextual-ization. The re-ranking functions are the same as before, first we use the re-ranking function, Equation 2 and later we use Equation 4 for better contextualization.

Contextualization with the hybrid approach provided the most benefit in the retrieval effectiveness, based on our em-pirical studies (see Section 4).
In order to study the effect of contextualization on focused and element levels, we need a suitable baseline and a test bed with adequate evaluation methods. Next, in Section 3.1 we introduce the test bed, then in Section 3.2 a baseline system based on data fusion is introduced and examined briefly in Section 3.4 with the evaluation procedure of Section 3.3.
The outcome of the present study relates to the Initiative of Evaluation for XML retrieval INEX [11] and the test bed provided by it. INEX is a forum for the evaluation of XML and focused retrieval offering a test collection with topics and corresponding relevance assessments, as well as various evaluation metrics. Aside evaluating element retrieval, pas-sage retrieval evaluation is also supported in INEX. In this study we use the data provided by the 2009 INEX ad-hoc track. The track has 68 topics with character-wise relevance assessments, and the test collection, English Wikipedia, cov-ers around 2 . 66 million XML marked articles and 50.7 Gi-gabytes of XML marked data [26].

This large, semantically marked-up, Wikipedia collection has been used in INEX since 2009 and is still in use. The reason for using the INEX 2009 test topics (instead of 2010) is the larger variety of elements in the participants X  results. This is mainly because of the existence of the thorough task, where elements are retrieved regardless of overlap, i.e., in the results a section and its sub elements, paragraphs, may be retrieved within the same results [11]. The large variety of elements is a necessity for a data fusion of results, which our baseline system is based on.
Contextualization is independent of basic scoring method, thus we are able to implement the baseline system quite freely. In this study, we use a fusion run as our baseline sys-tem for which 159 element runs out of total 173 runs from the INEX 2009 participants was used. The remaining 13 were not element runs, i.e., they contained ranges of frag-ments or file-offset-lengths (FOL) as retrievable units and were omitted from the fusion. In addition, in order to avoid noise, we made a decision to remove 61 runs having an ex-tensive number of non-existing elements. Thus, a total of 98 runs from the participants of all tasks (best-in-context, relevant-in-context, focused, fetch and browse) of the ad-hoc track were used in fusion.

The runs were fused using an acknowledged method called the reciprocal rank. The method has been found effective in document retrieval [6]. In it, every element (item) in each of the result list (candidate run) is given a score based on its ranking and the fused score for an element is the sum of their ranked scores per topic. A fusion score for an element e is calculated as follows.
 where
Before addressing the effectiveness of such approach as a baseline system, we introduce shortly our evaluation ap-proach, which aims at measuring performance of very fo-cused elements only.
One of the key issues in semi-structured retrieval is the handling of overlap in results. A partial solution has been in-troduced not to accept structurally overlapping elements in the results. Still non-overlapping elements of various gran-ularities are accepted, so that retrieval of e.g., a whole sec-tion instead of its smaller descendants separately leads to different result list than returning the descendants as indi-vidual elements. Measuring these kinds of result lists has led to numerous, typically quite complex and unintuitive metrics [9, 15, 24]. The aim of these metrics is not only to measure the matching of the text content, but also the selec-tion of granularity level at various situations. Unfortunately, retrieving elements of various granularity levels has an un-controlled effect on the evaluation results and has led to bizarreness in the true evaluation results, and favouring sys-tems retrieving large elements over focused ones [4]. Thus, as a criticism, deciding the right granularity level is based on the laboratory environment (especially metrics) rather than on true user needs.

Elements low in a hierarchy are focused answers to a query and possess more context and thus supposedly benefit more on contextualization. In order to study the effect of contex-tualization especially on those small and focused elements, and to exclude the effect of element granularity level se-lection on evaluation results, we use granulation [4], where specific types of elements are pre-selected in the collection. The search is focused on those elements only. For that pur-pose also the underlying recall base needs to be pruned so that only those selected elements are involved (see Fig 4(a)). Obviously, a semi-structured collection can be granulated in numerous ways. In this study, we focus on two types of gran-ulations: full document granulation and a granulation con-taining paragraphs (  X  p  X  -elements) only. To put it short, the former is for document retrieval and the latter is paragraph retrieval. The paragraph level elements are very frequent in the collection (on average 274 relevant paragraphs per topic) and a list containing such elements may provide satisfactory and focused answers. It is worth mentioning that, Crouch et al. [7] had similar setting and used the paragraph as the ba-sic index node. One obvious use case for paragraph retrieval is snippet retrieval.

In terms of structural query language NEXI (strict in-terpretation) [28, 29], we use the following queries //article ( .,about ( X  query  X  expr  X )) and //p ( .,about ( X  query  X  expr  X )) for full document and focused runs respectively. The  X  query  X  expr  X  stands for the title field bag-of-words query of a topic. In the full document approach only root elements (i.e. ar-ticles) are considered in the result lists and in the focused run, only elements having the name (  X  p  X  ). The correspond-ing runs are made by pruning the fusion results by basically taking out everything else but the lines corresponding to the structural conditions (i.e.,  X  article  X  and  X  p  X  ). In other words, the paragraph list is a sub list of the fusion run. Corresponding recall base is made for paragraph list. The full document recall base is provided by the initiative. The fusion run contains every element retrieved by the partici-pants. The pool was constructed from the paragraph gran-ulation by analyzing the FOLs in the recall base against the submitted paragraphs. Out of the full set of runs used, 46 runs did contain paragraphs. So the paragraph result list is a fusion of those runs.
Next, we aim to give an insight of the baseline system we want to improve using contextualization in next section. In order to avoid over tuning of the baseline system, we refer only to results, which are achieved using basic values only and leave the further analysis of the data fusion of element results for later studies. Thus, our baseline system is the bare format of reciprocal rank, i.e., k = 0. In other words, an element at the first rank of any run yields basically the score of 1 and the second yields 0 . 5, third 0 . 33 and so on.
At article level granulation, i.e., full document retrieval, the fusion run outperforms all reported official full document runs of INEX having the MAP as high as 0 . 4141. The best official INEX full document run yielded at the level of 0 . 3578 (UamsTAbi100 by the University of Amsterdam) [11]. The granulation of the run is made so that only results rows with /article [1] are considered. Similarly, at paragraph level any result row ending with /p [ n ] is considered ( n is positive integer). We did the same granulation for every 46 INEX run and compared the results with ours. Early precision was used in comparison at paragraph level for two reasons. First, the granulation results in a subset of the result, so the result list may be short. Second, early precision is in line with the nature of focused retrieval.

The runs of the Technical University of Queensland (qtau) yielded the best early precision figures, especially a run called ANTbigramsThorough. Figure 4(b) represents the recall base sizes per topic at paragraph level and the number of retrieved paragraphs of the ANTbigramsThorough run. In 21 topics the number of retrieved paragraphs of the run out-numbers the number of relevant paragraphs, so a fair com-parison can be made using r  X  precision score for those top-ics. Accordingly, the r  X  precision score for the run ANTbi-gramsThorough is 0 . 2779 and for the baseline fusion 0 . 3479. Based on these figures, we can say that the fusion approach is competitive. Next, we apply contextualization for the fu-sion and see if there still is room for improvements.
We now experimentally evaluate the propositions presented in this paper. First, we lay down the experimental settings. Later, we present some empirical evidence that our rank-ing models return intuitive results both on document and focused retrieval. We then evaluate the retrieval effective-ness of our models againts the competitive baseline systems that were introduced in Section 3. Finally, we relate the empirical evidence with the theoretical claims.
The proposed approaches are evaluated using the Wiki-pedia test collection, described in Section 3.1. The choice of experimenting with the Wikipedia collection is for the fol-lowing reasons. First, XML documents in Wikipedia 2009 collection has a very deep internal hierarchical structure, containing overall about 32 thousand different tags [26]. Sec-ond, Wikipedia has quite a huge number of inter-document references (in the form of citations). Finally because Wiki-pedia collection is quite big and extensively assessed test bed used over the years at INEX [3, 11] and at other evaluation forums.

The 2.66 million semantically marked XML documents contain a total of around 135 million citations (links), which were extracted by parsing each of the documents in the collection. We use the resultant gigantic citations graph for experimentation with the citations and hybrid contex-tualization (Sections 2.1 and 2.3). The computation of the contextualization vector g k from Equation 1 for the large Wikipedia collection was quite extensive, however this pro-cess is performed offline. The linear system of Equations 1 is usually solved iteratively, using the well known Power method [18]. The convergence of power method is acceler-ated using a technique called Extrapolation 2 . At the query
Extrapolation is a technique for constructing new data points (dominant eigenvector) outside a discrete set of known data points (known values during each iteration of time, we combine the iteratively computed random walk scores and the basic scores based on the proposed methods (Equation 3).

In the forth coming sections we will present empirical evi-dence that the contextualization vector g k together with the citation contextualization model, produces intuitive overall retrieval effectiveness (see Tables 2 and 1).

For hierarchical contextualization we index the collection and use the dewey encoding to capture the internal tree structure of the XML documents (as shown in the example, Figure 3). This way each element in the document pos-sess a unique index within the document, and together with document X  X  unique id, this becomes unique for the entire collection. The tree structure of XML documents are con-verted into a matrix, and random walk is performed on this matrix, as it is described in Section 2.1. In this case also the contextualization vector g k from Equation 4 is computed offline for each and every XML documents in Wikipedia col-lection. This suggests that computing g k vector is feasible for a reasonably large XML document collections. Again, at the query time, the scores from g k vector and basic scores are combined to produce an overall ranking score, using Equa-tion 4.
 Table 1: Ret. performance for focused retrieval N * = stat. significant than both the Fusion and QTau baselines runs at p &lt; 0.01 (1-tailed t-test), and M + = stat. significant at p &lt; 0.05 respectively.
 Table 2: Retrieval performance for document re-trieval (article level).
We have tested five different retrieval methods based on the propositions (Sections 2.1, 2.2, 2.3) and three different baseline systems (Section 3). power method) and using the properties of Markov chain;  X  1 = 1 (dominant eigenvalue) [14, 21]. Figure 6: Precision -recall performance for docu-ment retrieval (article)
We did not report results on citation contextualization based on query-dependent random walk, as the preliminary experimental analysis showed not enough or desirable re-trieval gains, apparently because of the definition of citations or links in the Wikipedia collection. Hence, we omit query-dependent citation contextualization from evaluations, and therefore investigate the usefulness of this approach in our future studies.

As defined earlier, contextualization has two general di-mensions -the magnitude of contextualization (contextual-ization force) and the impact of each contextualizing ele-ment. The impact of each contextualizing factor is identi-fied automatically with random walk principle, in contrast to the earlier studies [1, 4]. While, the contextualization force has to be parameterized. For each proposed contex-tualization model, we tuned the contextualization force and report the values leading to best overall performance. In our parameterization process we found: (i) the optimal values of contextualization force f in citation contextualization (from Equation 2) lies in: ( f  X  X  0.025, 0.055, 0.10, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75 } ); (ii) and in hierarchical contextual-ization (from Equation 4) f  X  { 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 2.00, 2.25, 2.50, 2.75 } .

These optimal values for f are obtained by using cross-validation technique 3 . We did 68-fold cross-validation (or complete cross-validation in our case) -by randomly parti-tioning the collection into 68 training and test samples based on the number of assessed topics. Of the 68 samples, a sin-gle sample is retained as the validation set for testing, and remaining 67 samples are used as training set. The cross-validation process is repeated 68 times (for each fold), with each of 68 samples used exactly only once as validation set.
Cross-validation is a technique for assessing how the re-sults of a statistical analysis will generalize to an indepen-dent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. These 68 independent or unseen samples are then combined to produce a single or a set of estimations for the parameter f .

Figures 5 illustrate the behaviour of the methods as we change the optimal values of f parameter, from Equations 2, 3 and 4, on precision-oriented measures. As can be visually observed, the proposed methods out-perform notably all the baseline systems, Fusion, QTau and UAmst (Figure 6).
Table 1 and 2 show the overview of the retrieval perfor-mance of our approaches against the baselines for focused (paragraph level) and document (article level) retrieval tasks. All the proposed contextualization models improves the per-formance over the baselines. The improvements are statis-tically significant (1-tailed t-test at p &lt; 0 . 01 and p &lt; 0 . 05) on rPrecision , P @5, P @10, P @20, P @30 and so on (Fig-ures 5). The improvements overall are surprisingly good on both focused and document retrieval.

The best overall results among the proposed methods are est mean average precision, r  X  precision and precision at N values. Documents with many and important inlinks have a higher probability of being relevant [12, 13] and hence in contextualization their role is considerable and fruitful, which is also verified in our experiments. We conclude that, context from citations, hierarchical structure of documents and their hybrid indeed improve the retrieval effectiveness, and the improvements are in-line with the theoretical antic-ipations.
Contextualization is a re-ranking model utlizing the con-text of the relevant retrievable unit for improving the over-all retrieval. We studied context from three different but related perspectives; (i) external perspective (based on cita-tions) (ii) internal perspective (hierarchical structure) and (iii) hybrid perspective (external and internal perspective). The common thread among the three ways of contextualiza-tion is the use of the graph structure originated from the doc-uments citation structure externally and hierarchical struc-ture internally. We hypothesized that context gathered from graph structure of documents (from within and outside), influence the retrieval effectiveness. The experiments vali-dated the hypothesis that utilizing the context actually en-hances the retrieval of information on article and paragraph granularity levels. The results obtained in this study are in-line with the earlier work on use of hyperlinked and hierar-chical tree (graph) structure of documents [5, 10, 12, 17] and the role of contextualization [1, 4, 19, 22, 23, 25]. However, none of these works exploits evidence accumulated from the link structure of documents with random walk as a contex-tual evidence.

The authority score  X  X n isolation X  can identify the impor-tance of each node in the graph formed from either cita-tions or hierarchical structure of documents. The usefulness of these authority scores in isolation (not in context) has been studied well over the years [5, 10, 17]. The novelty of this study is the utilization these useful sources of infor-mation not  X  X n isolation X  but  X  X n contextualization X . That means, to use the importance score of each document or ele-ment as an impact factor for identifying how essential is the role of this document or element in context. A retrievable unit (document or element) with strong context must be boosted higher in ranking than the retrievable unit with less strong context. Extensive experimentation validated this view point.
We have presented an in-depth study into the use of con-text from citations and hierarchical structure information, in order to improve retrieval performance on document and focused retrieval tasks. To the best of our knowledge, this is the first study that takes context into account by mixing two perspectives (a) the context from the citation struc-ture of documents, and (b) the context from the hierarchi-cal structure of semi-structured documents. The approaches presented are generic and can be applied to different test collections and baseline systems. Evidence is collected in a systematic way, from the surrounding context of both the document itself and the element to be ranked, in docu-ment and focused retrieval respectively. In this paper, XML documents are used as a sample case of semi-structured documents. These documents have an hierarchical struc-ture, which is often represented in a form of tree. However, the approaches could also be applicable for other generic structured (or semi-structured) test collections (e.g., Linked Data, RDF, etc.), where the structure may be represented as a general graph (with cycles). The proposed methods are particularly suited for collections that carry more types of evidence than just textual information. The importance of each single unit in the context is identified by a Markovian random walk. Most of the proposed methods are tested and found to be significantly better than the baseline system, which had an overall performance that was already better than any run submitted to INEX 2009. The proposed meth-ods both boost the rankings of the documents in good con-text and degrade the rankings of documents in not so good context.

The effectiveness of random walks to materialize the con-text has been evaluated in five different settings. We have found that the context from in-and out-links as well as a document X  X  hierarchical structure can indeed improve re-trieval results. Given that the citation structure of Wikipedia collection does not necessarily form a sound bibliographic se-mantics, because, (a) two documents can cite each other at the same time ( A cites B and B cites A ), without temporal ordering, (b) the link structure in Wikipedia is a (possibly weak) indicator of relevance [12] in isolation. Yet, when applying contextualization using weights obtained with the random walk principle, this information is found to be sig-nificantly plausible, both theoretically and empirically. Bib-liographical structure of scientific documents could lead to even better results, as their citation structure characterizes stronger semantics, and possibly a stronger indicator of rel-evance. Nevertheless, we consider our experiments on the Wikipedia test collection sufficiently promising to consider different types of evidence in future work. Specifically, we would like to investigate the effects of context derived from tweet mentions that may help improve retrieval from video collections. There are also several other venues for future work, for instance, experimenting with different granularity levels than just article and paragraph levels  X  identify the importance of each granularity level(s) and possibly auto-matically boost  X  X mportant X  ones more than other  X  X ot so important X  granularity levels. The sequential document or-dering, often referred to as the document order, where text passages follow each other in sequence, one after the other, could also be considered as a second dimenion of the struc-tural context within the random walk paradigm. Finally, graph-based methods for results list fusion may be naturally included in our current approach, where we applied random walks over result lists obtained from a separate fusion phase.
This study was supported by Academy of Finland under grant #140315. [1] P. Arvola, M. Junkkari, and J. Kek  X  al  X  ainen. [2] P. Arvola, J. Kek  X  al  X  ainen, and M. Junkkari. The Effect [3] P. Arvola, S. Geva, J. Kamps, R. Schenkel, [4] P. Arvola, J. Kek  X  al  X  ainen, and M. Junkkari. [5] A. Balmin, V. Hristidis, and Y. Papakonstantinou. [6] G. Cormack, C. Clarke, and S. Buettcher. Reciprocal [7] C. Crouch, D. Crouch, N. Kamat, V. Malik, and [8] S. Geva, J. Kamps, R. Schenkel, and A. Trotman. [9] N. G  X  overt, N. Fuhr, M. Lalmas, and G. Kazai. [10] L. Guo, F. Shao, C. Botev, and [11] W. Huang, S. Geva, and A. Trotman. Overview of the [12] J. Kamps and M. Koolen. The Importance of Link [13] J. Kamps and M. Koolen. Is Wikipedia Link [14] Kamvar, S.D. and Haveliwala, T.H. and Manning, [15] G. Kazai and M. Lalmas. eXtended Cumulated Gain [16] J. Kek  X  al  X  ainen, P. Arvola, and M. Junkkari. [17] J. Kleinberg. Authoritative Sources in a Hyperlinked [18] D. Lay. Linear Algebra and its Applications . [19] Y. Mass and M. Mandelbrod. Component Ranking [20] M. Norozi. IR Models and Relevancy Ranking.
 [21] M. Norozi. Faster ranking using extrapolation [22] M. Norozi, A. de Vries, and P. Arvola.
 [23] P. Ogilvie and J. Callan. Hierarchical Language [24] B. Piwowarski and G. Dupret. Evaluation in (XML) [25] G. Ramirez Camps. Structural Features in XML [26] Schenkel, R. and Suchanek, F.M. and Kasneci, G. [27] B. Sigurbj  X  ornsson, J. Kamps, and M. De Rijke. An [28] A. Trotman and M. Lalmas. Strict and vague [29] A. Trotman and B. Sigurbj  X  ornsson. Narrowed
