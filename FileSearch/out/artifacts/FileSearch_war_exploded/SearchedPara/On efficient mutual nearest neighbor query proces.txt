 1. Introduction
This paper studies a new form of nearest neighbor (NN) queries, namely mutual nearest neighbor (MNN) search. Given a dataset D , a query point q , and two parameters k 1 and k p 2 NN k 1 ( q ) 1 and q 2 NN k 2 ( p ), i.e., it requires each answer object tions are presented as follows.

Resource allocation. Consider that a logistic company A has six branches (labeled as p and process the branches in the order of p 1 , p 2 , p 3 , p 2 nearest branches (i.e., p 2 and p 4 ). Next, p 2 is evaluated. As p branches p 3 , p 4 , p 5 , and p 6 that is the closest to p branches, they are out of the consideration. Hence, p 3 is linked to branches p
MNN retrieval with respect to k 1 and k 2 , it means that p is one of k the running example, we utilize MNN queries to deal with the assignment of branches. Initially, both k order to retrieve the objects that are closest to each other. Consequently, p
NN objects to each other. Then, we increase either k 1 or k p is the NN object of p 2 and meanwhile p 2 is one of p 4  X  X  2 NN objects, i.e., p at p 2 . Next, we set both k 1 and k 2 to 2, and p 2 is linked to p k branches, and the only left branch is p 1 . Finally, p 1 is linked to branches p distance between a branch and its backup to 17.75, compared with the average distance 21.5 generated by k NN search. tion, etc.) specified by m , but it ignores the fact that m might not be appealing to m
NN (i.e., R2NN) retrieval can identify those members, say m again ignores the fact that m might have more interest on members other than m selecting k 1 and k 2 , MNN query can find m 0 for m such that m 0 is one of the top-k top-k 2 choices for m 0 , and thus improves the success ratio of the matching. ular clustering method. In [6,18,34,48] , the authors study applications of the k -mutual neighborhood graph consideration, and proposes several mining algorithms to detect top-n outliers [23] efficiently.
More recently, Wong et al. [45] introduce the concept of bichromatic MNN that considers two datasets and employ it to they thoroughly investigate two classes of queries, viz. MNN sets D A , D B , and return k pairs of objects ( o a , o b among all possible object pairs in D A D B .

Given an MNN search, a naive solution is to find the set of k verify whether each point p in NN k 1 ( q ) has q as one of its k
O overhead and expensive CPU cost, especially for large values of k tackling monochromatic MNN queries in spatial databases. The efficiency and effectiveness of our proposed algorithms are demonstrated through extensive experiments using both real and synthetic datasets. formalizes the MNN query and analyzes its characteristics, followed by a baseline algorithm. Four improved algorithms future work. 2. Related work veyed in this section is based on R-tree and its variants. 2.1. NN/kNN query algorithms
Points close in space (e.g., a , b , c ) are clustered in the same leaf node (e.g., N with the same principle until the top level, which consists of a single root node denoted by Root .
The algorithms for NN/ k NN search on R-trees follow the branch-and-bound paradigm and utilize some metrics to prune
Fig. 2 a illustrates these pruning metrics between q and nodes N resides in memory during search.
 that may contain/be the NN(s) of q , e.g., the entries (including Root , N ( P 1) NN(s) of q are retrieved.
 that when e refers to a point, mindist ( q , e )= dist ( q , e ), omit associated distances to q for node MBRs and data points.
 where, given two datasets D 1 and D 2 , the goal is to retrieve for each point p existing work has solved the MNN query. 2.2. RNN/RkNN query algorithms is limited to answer R k NN retrieval for a fixed k .

Inspired by the defects of pre-computation based approaches, several alternative RNN/R k NN search methods without sub-region may become the result. Consequently, the candidate set only contains 6 objects, retrieved by constraint NN TPL pruning, in the following we describe the TPL algorithm via an illustrative example. segment connecting point p and point q . The bisector ? X  p ; q  X  divides the data space into two half-planes: HP tains q and HP p ( p , q ) that contains p . Any point p 0 or MBR N falling inside HP cannot be/contain an RNN of q and can be safely pruned away. As illustrated in Fig. 3 a, the bisector ? X  p data space into two half-planes, i.e., HP q ( p 1 , q ) and HP
HP p 1 ( p 1 , q ), they are closer to p 1 than to q , and hence they for sure are not answer objects. Similarly, N planes in some cases. In Fig. 3 a, for example, N 2 can be pruned since it lies entirely in HP are closer to p 0 than q . Consequently, if a data point is inside at least kHP and thus can be safely pruned away.
 ple, the first point (i.e., the first NN of q ) evaluated is i , which is added to a candidate set S
N can be pruned, which are maintained by a refinement set S to S rfn , as it falls fully in ABFE .

After the termination of the filter step, TPL has a candidate set S step, TPL eliminates false hits by reusing the pruned points/MBRs maintained in S ple, point e 2 S c is a false hit since it is closer to d 2 S search, and so forth. 3. Preliminaries the rest of this paper.
 3.1. Problem definition
Definition 1 ( Mutual nearest neighbor query ). Given a dataset D , a query point q , and two parameters k neighbor (MNN) query retrieves the set of objects S # D , such that (i) " p 2 S , p 2 NN
Formally, MNN k 1, k 2 ( q )={ p 2 S j p 2 NN k 1 ( q ) ^ q 2 NN
As defined in Definition 1 , an MNN query returns all the objects in D that are among the k as one of their k 2 NNs. It has two important properties, which make it different from conventional NN search.
Property 1. MNN is symmetric. That is to say for any two given objects o o k NN search is asymmetric. As shown in Fig. 4 , point p 4
MNN retrieval is symmetric. For example, MNN 1,2 ( p 1 )={ p to the definition of the MNN query, p 1 2 MNN 2,1 ( p 2 ) satisfies as well.
Property 2. Given a query point q, the cardinality of q X  X  mutual nearest neighbors (MNNs), denoted by j MNN data distribution of D changes. k NN search consistently returns k answer objects, whereas MNN queries with the same k points might return result sets with different cardinalities. For instance, j MNN picted in Fig. 4 .
 saving.

Lemma 1. If the distance from each data object in D to q is unique, then the cardinality of MNN k ].
Proof. According to Definition 1 , MNN k 1, k 2 ( q )={ p 2 S j p 2 NN
Theorem 1. MNN k1,k2 (q) is a subset of NN k1 (q), i.e., MNN
Theorem 2. MNN k1,k2 (q) is a subset of RNN k2 (q), i.e., MNN 3.2. Baseline algorithm for MNN search filtering-verification framework, i.e., first conducting a k NN search to retrieve the candidate set S itate the best-first traversal paradigm.
 firms that i 2 NN 3 ( q ) is indeed an MNN of q and it is returned as an answer object; whereas points e and j in NN hits as NN 1 ( e )={ d } and NN 1 ( j )={ i }.

The correctness of SP is obvious. Observe that the candidate set returned by BF -k NN is always a superset (i.e., NN the final result set (i.e., MNN k 1, k 2 ( q )), that is, it does not incur false misses because MNN
NN k 2 ( p ). Consequently, SP can return the exact set of MNNs.
 Lemma 2. The SP algorithm loads some entries (node MBRs or data objects) of the R-tree T Proof. Since the SP algorithm requires traversing the R-tree T loads/accesses some index entries (e.g., the root of T D ) multiple times. h
MNN 3,1 query depicted in Fig. 6 a. As seen from this diagram, SP visits entries N
For the SP algorithm, let j H 1 j be the size of heap H 1 algorithm are analyzed in Theorem 3 .

Theorem 3. The time and space complexities of the SP algorithm are O (( j S Proof. The SP algorithm follows the filtering-verification framework. In the filtering step, SP takes O ( log j T candidate set S c ; in the verification step, SP incurs O ( j S
MNN of q . Thus, the time complexity of the algorithm is O (( j S 2 4 6 8 10 rithm is O ( j H 1 j + j H 2 j ). h
SP is very inefficient in terms of I/O overhead and CPU cost, especially for large values of k the performance of MNN query processing via different optimization techniques. 4. Optimizations
I/O cost) and speed up search performance accordingly. For this purpose, several enhanced algorithms for MNN search, using RNN search with pruning (RNNP), are developed. 4.1. Two-step algorithm
The SP algorithm has to verify every single object included in the candidate set S has a specified query point q as one of its k 2 NNs, we can issue an R k rithm is depicted in Fig. 7 .

Based on Definition 1 , MNN k 1, k 2 ( q )={ p 2 S j p 2 NN large and k 2 is very small, i.e., k 1 k 2 . 4.2. Reuse two-heap algorithm
Our second algorithm tries to improve the performance of SP using different optimization techniques. The SP algorithm employs two heaps: (i) heap H 1 used by the function BF -k (ii) heap H 2 utilized by the function BF -k 2 NN for verifying each candidate c in S algorithm (RTH), is proposed, which (i) attempts to fully use locally available nodes (e.g., those nodes in H didate c 2 S c may be terminated earlier without finding all the k
Similar as SP algorithm, RTH utilizes BF -k NN to retrieve the candidate set S worst-case scenario. Compared with SP, RTH reduces the traversal of the dataset from ( j S once RTH encounters a data object o such that o 2 NN k 2 ( c ) and dist ( o , c ) P dist ( q , c ), candidate c 2 S k
NNs to q , and hence the verification process of c can be terminated without finding all the k tion condition can improve the search performance further.
 ations involved in the Lines 18 and 19 of Verify algorithm.
 { i , e , j } has been identified as the candidate set S c trated in Fig. 10 : View 0 ). In the subsequent verification step, candidate objects in S due to dist ( i , j )&gt; dist ( i , q ). The situation of H and S the number of nodes accesses, as pointed out in Lemma 3 .
 Lemma 3. The RTH algorithm loads any entry (node MBR or data object) in the R-tree T been visited so far during query processing. h 4.3. Algorithm using NN search with pruning
Although TS and RTH reduce the I/O overhead by using batch processing and reusing technology, there is still room for performance improvement based on the following observations. TS, no matter what is the value of k to retrieve all the objects that have q as one of their k depicts the pseudo-code of NNP algorithm.
 and 6). The main target is to remove those candidates that definitely will not belong to RNN straightforward, we only explain the NNP-Finding algorithm, which is presented in Fig. 12 . to examine whether o can be discarded, with pruned objects preserved in the refine set S dition, i.e., whether any un-examined object o 0 2 NN k 1 to estimate j S j based on P o
NN k 1 ( q ). Consequently, NNP-Finding can be terminated.

In order to facilitate the understanding of NNP-Finding , we take an MNN
NNP-Finding accesses the root node and inserts its entries N cardinality of the candidate set S c reaches k 1 , i.e., j S longs to NN 3 ( q ).

The first de-heaped entry is N 2 , and its child nodes are inserted into H ={ N entry N 6 might contain the objects that can contribute to NN
S the candidate set with S c ={ i , e }. Since all the remaining nodes/points in H ={ j , d , N and hence the algorithm can be terminated, after which S c sure the correctness of the search.
 Theorem 4. The NNP-Finding algorithm does not miss any answer object of an MNN query. an entry e 0 2 H that cannot be pruned by TPL -k -Trim and meanwhile has ObjNum ( S ) P k dist ( e o , q ) 6 dist ( o , q ) holds for " s 2 S .As ObjNum ( S ) P k hence o R NN k 1 ( q ). This finding contradicts our assumption, and thus the proof completes. h 4.4. Algorithm using RNN search with pruning
As defined in Definition 1 , MNN k 1, k 2 ( q )={ p 2 S j p 2 NN set S c based on NN k 1 ( q ) and then verify each candidate c in S j RNN k 2 ( q ) j &lt; j NN k 1 ( q ) j , it is more beneficial to constitute the candidate set based on RNN k rithm to form the candidate set (Line 3). Subsequently, it checks whether each candidate object c 2 NN by invoking NN-Verify algorithm (Line 5).

Fig. 15 depicts the pseudo-code for RNNP-Finding algorithm. The basic idea is to retrieve those objects p whose NN ventional R k NN search, it takes the characteristic of MNN search into consideration and tries to exclude p R NN those unvisited objects) are for sure not included in NN k 1 object of MNN search.

Proof. Without loss of generality, we assume at least one answer object o 0 2 MNN nation condition of RNNP-Finding algorithm. In other words, o 0 2 NN objects in S are closer to q than o 0 , and o 0 for sure does not belong to NN tion and completes the proof. h
Whenever a data object o that is included in the candidate set S trieved (Lines 10 and 11); or (ii) all the data objects contained in S comes empty (Lines 15 and 16). Notice that NN-Verify verifies every candidate in S candidate objects in S c and all the pruned objects/nodes maintained in S
RNNP (Lines 2 and 3). 5. Experimental evaluation ments are conducted on a PC with Pentium IV 3.0 GHz CPU and 2GB main memory, running Microsoft Windows XP Profes-findings in Section 5.2. 5.1. Experimental setup 2D points representing 123,593 geometric locations in Long Beach County; Wave includes 3D points representing 60,000 various dimensions are mutually independent.
 where the I/O time is computed by charging 10 ms for each page access, as in [41]), and the maximum number of entries in the heap (as the heap storages dominate the space complexities of our proposed algorithms). Each reported value in is 0 in the experiments, i.e., the I/O cost is determined by the number of nodes accessed. 5.2. Performance study the I/O cost and the CPU cost, respectively. The cost increases with k experiments.
 is that NNP takes NN k 1 ( q ) as the candidate set, while RNNP forms the candidate set based on RNN the filtering step and verification step of NNP (RNNP), respectively. Notice that, when k filtering step cost of RNNP is lower than that of NNP for all datasets as j RNN
However, both NNP and RNNP outperform the other algorithms (including SP, TS, and RTH) by several orders of magni-pared against SP/TS/RTH for LB dataset with k 1 = 256 and k number of nodes accessed. It is worth noting that when k 1 improve the search performance.

Table 4 shows the maximum number of entries in the heap (denoted as n ) of different algorithms with respect to k which causes the major run-time memory consumption. Let dim be the dimensionality. In our experiments, we allocate 4, negligible compared with the R-tree size. As an example, for Color dataset with k niques to discard non-qualifying entries, leading to heap size saving.

Next, we fix k 1 to 16 and vary k 2 between 1 and 256 to evaluate the impact of k shown in Fig. 19 . It is observed that the cost of the algorithms increases slightly as k imum speedup of these two algorithms over RTH is about 7.5 times, occurring under Color dataset with k (Fig. 19 f). The second observation is that RNNP outperforms NNP when k is that j RNN k 2 ( q ) jj NN k 1 ( q ) j satisfies when k lower than that of NNP when k 1 = 16 and k 2 =1.

Table 5 compares the maximal number of entries in the heap as a function of k algorithms (i.e., RTH, NNP, and RNNP), especially for the large values of k and 22 as well.
 2to5. Fig. 21 plots the efficiency of different algorithms in answering MNN for the algorithms with respect to dimensionality. As expected, the memory consumption of the algorithms increases as better than RTH.
 obvious as that of dimensionality. This is because given fixed k algorithms) for the dataset cardinality. The phenomena and their explanations are the same as those in Table 6 . the entries visited in memory. Both NNP and RNNP again outperform their competitors significantly in all cases.
To summarize, from the above experimental results on both real and synthetic datasets, we can conclude that both NNP and RNNP consistently provide the best performance under all the settings, and RNNP is the best choice if k number of entries in the heap for each algorithm is negligible compared to the dataset size. 6. Conclusions respect to a line segment which contains continuous query points instead of a fixed query point.
References
