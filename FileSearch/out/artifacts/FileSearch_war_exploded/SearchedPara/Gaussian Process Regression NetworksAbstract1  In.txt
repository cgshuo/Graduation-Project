 Andrew Gordon Wilson agw38@cam.ac.uk David A. Knowles dak33@cam.ac.uk Zoubin Ghahramani zoubin@eng.cam.ac.uk  X  X earning representations by back-propagating errors X  by Rumelhart et al. (1986) is a defining paper in ma-chine learning history. This paper made neural net-works popular for their ability to capture correlations between multiple outputs, and to discover hidden fea-tures in data, by using adaptive hidden basis functions that were shared across the outputs.
 MacKay (1992) and Neal (1996) later showed that no matter how large or complex the neural network, one could avoid overfitting using a Bayesian formulation. Neal (1996) also argued that  X  X imiting complexity is likely to conflict with our prior beliefs, and can there-fore only be justified to the extent that it is neces-sary for computational reasons X . Accordingly, Neal (1996) pursued the limit of large models, and found that Bayesian neural networks became Gaussian pro-cesses as the number of hidden units approached infin-ity, and conjectured that  X  X here may be simpler ways to do inference in this case X .
 These simple inference techniques became the corner-stone of subsequent Gaussian process models (Ras-mussen &amp; Williams, 2006). These models assume a prior directly over functions, rather than parameters. By further assuming homoscedastic Gaussian noise, one can analytically infer a posterior distribution over these functions, given data. The properties of these functions  X  smoothness, periodicity, etc.  X  can easily be controlled by a Gaussian process covariance kernel. Gaussian process models have recently become pop-ular for non-linear regression and classification (Ras-mussen &amp; Williams, 2006), and have impressive em-pirical performances (Rasmussen, 1996).
 However, a neural network allowed for correlations be-tween multiple outputs, through sharing adaptive hid-den basis functions across the outputs. In the infinite limit of basis functions, these correlations vanished. Moreover, neural networks were envisaged as intelli-gent agents which discovered hidden features and rep-resentations in data, while Gaussian processes, though effective at regression and classification, are simply smoothing devices (MacKay, 1998).
 Recently there has been an explosion of interest in ex-tending the Gaussian process regression framework to account for fixed correlations between output variables (Alvarez &amp; Lawrence, 2011; Yu et al., 2009; Bonilla et al., 2008; Teh et al., 2005; Boyle &amp; Frean, 2004). These are often called  X  X ulti-task X  learning or  X  X ulti-ple output X  regression models. Capturing correlations between outputs (responses) can be used to make bet-ter predictions. Imagine we wish to predict cadmium concentrations in a region of the Swiss Jura, where ge-ologists are interested in heavy metal concentrations. A standard Gaussian process regression model would only be able to use cadmium training measurements. With a multi-task method, we can also make use of correlated heavy metal measurements to enhance cad-mium predictions (Goovaerts, 1997). We could further enhance predictions if we could use how these (signal) correlations change with geographical location. There has similarly been great interest in extending Gaussian process (GP) regression to account for in-put dependent noise variances (Goldberg et al., 1998; Kersting et al., 2007; Adams &amp; Stegle, 2008; Turner, 2010; Wilson &amp; Ghahramani, 2010b;a; L  X azaro-Gredilla &amp; Titsias, 2011). Wilson &amp; Ghahramani (2010a; 2011) and Fox &amp; Dunson (2011) further extended the GP framework to accommodate input dependent noise cor-relations between multiple output (response) variables. In this paper, we introduce a new regression frame-work, Gaussian Process Regression Networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric flexibility of Gaussian processes. This network is an adaptive mix-ture of Gaussian processes, which naturally accommo-dates input dependent signal and noise correlations between multiple output variables, input dependent length-scales and amplitudes, and heavy tailed predic-tive distributions, without expensive or numerically unstable computations. The GPRN framework ex-tends and unifies the work of Journel &amp; Huijbregts (1978), Neal (1996), Gelfand et al. (2004), Teh et al. (2005), Adams &amp; Stegle (2008), Turner (2010), and Wilson &amp; Ghahramani (2010b; 2011).
 Throughout this text we assume we are given a dataset of input output pairs, D = { ( x i , y ( x i )) : i = 1 ,...,N } , where x  X  X  is an input (predictor) variable belonging to an arbitrary set X , and y ( x ) is the corresponding p dimensional output; each element of y ( x ) is a one dimensional output (response) variable, for example the concentration of a single heavy metal at a geo-graphical location x . We aim to predict y ( x  X  ) | x and  X ( x  X  ) = cov[ y ( x  X  ) | x  X  , D ] at a test input x accounting for input dependent signal and noise cor-relations between the elements of y ( x ).
 We start by introducing the GPRN framework and discussing inference. We then further discuss related work, before comparing to eight multiple output GP models, on gene expression and geostatistics datasets, and three multivariate volatility models on several benchmark financial datasets. In the supplementary material (Wilson &amp; Ghahramani, 2012) we further dis-cuss theoretical aspects of GPRN, and review GP re-gression and notation (Rasmussen &amp; Williams, 2006). We wish to model a p dimensional function y ( x ), with signal and noise correlations that vary with x . We model y ( x ) as where = ( x ) and z = z ( x ) are respectively N (0 ,I q ) and N (0 ,I p ) white noise processes. I q and I p are q  X  q and p  X  p dimensional identity matrices. W ( x ) is a p  X  q matrix of independent Gaussian processes such that W ( x ) ij  X  GP (0 ,k w ), and f ( x ) = ( f 1 ( x ) ,...,f is a q  X  1 vector of independent GPs with f i ( x )  X  GP (0 ,k f through GP priors in W ( x ) and f ( x ), and the noise model is induced through and z .
 We represent the Gaussian process regression network (GPRN) 1 of equation (1) in Figure 1. Each of the la-tent Gaussian processes in f ( x ) has additive Gaussian noise. Changing variables to include the noise  X  f , we let  X  f i ( x ) = f i ( x ) +  X  f  X  X P (0 ,k  X  f and  X  aw is the Kronecker delta. The latent node func-tions  X  f ( x ) are connected together to form the outputs y ( x ). The strengths of the connections change as a function of x ; the weights themselves  X  the entries of W ( x )  X  are functions. Old connections can break and new connections can form. This is an adaptive net-work, where the signal and noise correlations between the components of y ( x ) vary with x . We label the length-scale hyperparameters for the kernels k w and k the weight GPs share the same covariance kernel k w , including hyperparameters. Roughly speaking, shar-ing length-scale hyperparameters amongst the weights means that, a priori, the strengths of the connections in Figure 1 vary with x at the same rate.
 To explicitly separate the adaptive signal and noise correlations, we re-write (1) as Given W ( x ), each of the outputs y i ( x ), i = 1 ,...,p , is a Gaussian process with kernel k The components of y ( x ) are coupled through the ma-trix W ( x ). Training the network involves conditioning W ( x ) on the data D , and so the predictive covariances of y ( x  X  ) |D are now influenced by the values of the observations, and not just distances between the test point x  X  and the observed points x 1 ,...,x N as is the case for independent GPs.
 We can view (4) as an adaptive kernel learned from the data. There are several other interesting features in equation (4): 1) the amplitude of the covariance put dependent); 2) even if each of the kernels k f different stationary length-scales, the mixture of the kernels k f overall length-scale is non-stationary; 3) the kernels k others squared exponential, others Brownian motion, and so on. Therefore the overall covariance kernel may be continuously switching between regions of entirely different covariance structures.
 In addition to modelling signal correlations, we can see from equation (3) that the GPRN is also a mul-tivariate volatility model. The noise covariance is  X  W ( x ) W ( x ) &gt; +  X  2 y I p . Since the entries of W ( x ) are GPs, this noise model is an example of a generalised Wishart process (Wilson &amp; Ghahramani, 2010a; 2011). The number of nodes q influences how the model ac-counts for signal and noise correlations. If q is smaller than p , the dimension of y ( x ), the model performs dimensionality reduction and matrix factorization as part of the regression on y ( x ) and cov[ y ( x )]. How-ever, we may want q &gt; p , for instance if the output space were one dimensional ( p = 1). In this case we would need q &gt; 1 for nonstationary length-scales and covariance structures. For a given dataset, we can vary q and select the value which gives the highest marginal likelihood on training data. We have specified a prior p ( y ( x )) at all points x in the domain X , and a noise model, so we can infer the pos-terior p ( y ( x ) |D ). The prior on y ( x ) is induced through the GP priors in W ( x ) and f ( x ), and the parameters  X  = {  X  f ,  X  w , X  f , X  y } . We perform inference directly over the GPs and parameters.
 We explicitly re-write the prior over GPs in terms of u = (  X  f , W ), a vector composed of all the node and weight Gaussian process functions, evaluated at the training points { x 1 ,...,x N } . There are q node func-tions and p  X  q weight functions. Therefore where C B is an Nq ( p + 1)  X  Nq ( p + 1) block diagonal matrix, since the weight and node functions are a priori independent. We order the entries of u so that the first q blocks are N  X  N covariance matrices K  X  f the node kernels k  X  f covariance matrices K w from the weight kernel k w . From (1), the likelihood is p ( D| u , X  y ) = Applying Bayes X  theorem, We sample from the posterior in (7) using elliptical slice sampling (ESS) (Murray et al., 2010), which is specifically designed to sample from posteriors with strongly correlated Gaussian priors. For comparison we approximate (7) using a message passing imple-mentation of variational Bayes (VB). We also use VB to learn the hyperparameters  X  |D . Details about our ESS and VB approaches are in Sections 1 and 2 of the supplementary material.
 By incorporating noise on f , the GP network accounts for input dependent noise correlations (as in (3)), with-out the need for costly or numerically unstable ma-trix decompositions during inference. The matrix  X  2 y I does not change with x and requires only one O (1) operation to invert. In a more typical multivariate volatility model, one must decompose a p  X  p matrix  X ( x ) once for each datapoint x i ( N times in total), an O ( Np 3 ) operation which is prone to numerical in-stability. In general, multivariate volatility models are intractable for p &gt; 5 (Gouri  X eroux et al., 2009; Engle, 2002). Moreover, multi-task Gaussian process mod-els typically have an O ( N 3 p 3 ) complexity (Alvarez &amp; Lawrence, 2011). In Section 3 of the supplementary material (Wilson &amp; Ghahramani, 2012) we show that, fixing the number of ESS or VB iterations, GPRN in-ference scales linearly with p , and further discuss the-oretical properties of GPRN, like the heavy-tailed pre-dictive distribution. Gaussian process regression networks are related to a large body of seemingly disparate work in ma-chine learning, econometrics, geostatistics, physics, and probability theory.
 In machine learning, the semiparametric latent fac-tor model (SLFM) (Teh et al., 2005) was introduced to model multiple outputs with fixed signal correla-tions. SLFM specifies a linear mixing of latent Gaus-sian processes. The SLFM is similar to the linear model of coregionalisation (LMC) (Journel &amp; Hui-jbregts, 1978) and intrinsic coregionalisation model (ICM) (Goovaerts, 1997) in geostatistics, but the SLFM incorporates important Gaussian process hy-perparameters like length-scales, and methodology for learning these hyperparameters. In machine learning, the SLFM has also been developed as  X  X aussian pro-cess factor analysis X  (Yu et al., 2009), with an empha-sis on time being the input (predictor) variable. For changing correlations, the Wishart process (Bru, 1991) was first introduced in probability theory as a distribution over a collection of positive definite covari-ance matrices with Wishart marginals. It was defined as an outer product of autoregressive Gaussian pro-cesses restricted to a Brownian motion or Ornstein-Uhlenbeck covariance structure. In the geostatistics literature, Gelfand et al. (2004) applied a Wishart pro-cess as part of a linear coregionalisation model with spatially varying signal covariances, on a p = 2 di-mensional real-estate example. Later Gouri  X eroux et al. (2009) returned to the Wishart process of Bru (1991) to model multivariate volatility, letting the noise co-variance be specified as an outer product of AR(1) Gaussian processes, assuming that the covariance ma-trices  X ( t ) = cov( y | t ) are observables on an evenly spaced one dimensional grid. In machine learning, Wilson &amp; Ghahramani (2010a; 2011) introduced the generalised Wishart process (GWP), which generalises the Wishart process of (Bru, 1991) to a process over arbitrary positive definite matrices (Wishart marginals are not required) with a flexible covariance structure, and using the GWP, extended the GP framework to account for input dependent noise correlations (mul-tivariate volatility), without assuming the noise is ob-servable, or that the input space is 1D, or on a grid. Gaussian process regression networks act as both a multi-task and multivariate volatility model. The sig-nal correlation model in GPRN differs from Gelfand et al. (2004) in that 1) the GPRN incorporates and estimates Gaussian process hyperparameters, like length-scales, effectively learning aspects of the covari-ance structure from data, 2) is tractable for p &gt; 3, 3) is used as a latent factor model (where q &lt; p ), 4) can account for input dependent length-scales and co-variance structures, and 5) incorporates an input de-pendent noise correlation model. Moreover, the VB and ESS inference procedures we present here are sig-nificantly more efficient than the Metropolis-Hastings proposals in Gelfand et al. (2004). Generally a noise model strongly influences a regression on the signal, even if the noise and signal models are a priori inde-pendent. In the GPRN prior of equation (3) the noise and signal correlations are explicitly related: through sharing W ( x ), the signal and noise are encouraged to increase and decrease together. The noise model is an example of a GWP, although GPRN scales lin-early and not cubically with p , per iteration of ESS or VB. If the GPRN is exposed solely to input dependent noise, the length-scales on the node functions f ( x ) will train to large values, turning the GPRN into solely a multivariate volatility model: all the modelling then takes place in W ( x ). In other words, through learn-ing Gaussian process hyperparameters, GPRN can au-tomatically vary between a multi-task and multivari-ate volatility model. The hyperparameters in GPRN are also important for distinguishing between the be-haviour of the weight and node functions. We may expect, for example, that the node functions will vary more quickly than the weight functions, so that the components of y ( x ) vary more quickly than the corre-lations between the components of y ( x ). The rate at which the node and weight functions vary is controlled by the Gaussian process length-scale hyperparameters, which are learned from data.
 When q = p = 1, the GPRN resembles the nonstation-ary GP regression model of Adams &amp; Stegle (2008). Likewise, when the weight functions are constants, the GPRN becomes the semiparametric latent factor model (SLFM) of Teh et al. (2005), except that the resulting GP regression network is less prone to over-fitting through its use of full Bayesian inference. The GPRN also somewhat resembles the natural sound model (MPAD) in Section 5.3 of Turner (2010), ex-cept in MPAD the analogue of the node functions are AR(2) Gaussian processes, and the  X  X eight functions X  are a priori correlated.
 Ver Hoef &amp; Barry (1998) in geostatistics and Boyle &amp; Frean (2004) in machine learning proposed an al-ternative convolution GP model for multiple outputs (CMOGP) with fixed signal correlations, where each output at each x  X  X is a mixture of latent Gaussian processes mixed across the whole input domain X . We compare GPRN to multi-task learning and multi-variate volatility models. We also compare between variational Bayes (VB) and elliptical slice sampling (ESS) inference within the GPRN framework. In the multi-task setting, there are p dimensional observa-tions y ( x ), and the goal is to use the correlations be-tween the elements of y ( x ) to make better predictions of y ( x  X  ), for a test input x  X  , than if we were to treat the dimensions independently. A major difference be-tween GPRN and alternative multi-task models is that the GPRN accounts for signal correlations that change with x , and input dependent noise correlations, rather than fixed correlations. We compare to multi-task GP models on gene expression and geostatistics datasets. In the multi-task experiments, the GPRN accounts for both input dependent signal and noise covariance ma-trices. To specifically test GPRN X  X  ability to model in-put dependent noise covariances (multivariate volatil-ity), we compare predictions of cov[ y ( x )] =  X ( x ) to those made by popular multivariate volatility models on benchmark financial datasets.
 In all experiments, GPRN uses squared exponential covariance functions, with a length-scale shared across all node functions, and another length-scale shared across all weight functions. GPRN is robust to initial-isation. We use an adversarial initialisation of N (0 , 1) white noise for Gaussian process functions. 5.1. Gene Expression Tomancak et al. (2002) measured gene expression lev-els every hour for 12 hours during Drosophila embryo-genesis; they then repeated this experiment for an in-dependent replica (a second independent time series). Gene expression is activated and deactivated by tran-scription factor proteins. We focus on genes which are thought to at least be regulated by the transcription factor twi , which influences mesoderm and muscle de-velopment in Drosophila (Zinzen et al., 2009). The assumption is that these gene expression levels are all correlated. We would like to use how these correlations change over time to make better predictions of time varying gene expression in the presence of transcrip-tion factors. In total there are 1621 genes (outputs) at N = 12 time points (inputs), on two independent repli-cas. For training, p = 50 random genes were selected from the first replica, and the corresponding 50 genes in the second replica were used for testing. We then repeated this experiment 10 times with a different set of genes each time, and averaged the results. We then repeated the whole experiment, but with p = 1000 genes. We used exactly the same training and testing sets as Alvarez &amp; Lawrence (2011).
 We use a smaller p = 50 dataset so that we are able to compare with popular alternative multi-task meth-ods (LMC, CMOGP, SLFM), which have a complex-ity of O ( N 3 p 3 ) and would not scale to p = 1000 (Al-varez &amp; Lawrence, 2011). 2 For p = 1000, we compare to the sparse convolved multiple output GP methods (CMOFITC, CMODTC, and CMOPITC) of Alvarez &amp; Lawrence (2011). In both of these regressions, the GPRN is accounting for multivariate volatility; this is the first time a multivariate stochastic volatility model has been estimated for p &gt; 50 (Chib et al., 2006). We assess performance using standardised mean square er-ror (SMSE) and mean standardized log loss (MSLL), as defined in Rasmussen &amp; Williams (2006) on page 23. Using the empirical mean and variance to fit the data would give an SMSE and MSLL of 1 and 0 re-spectively. The smaller the SMSE and more negative the MSLL the better.
 The results are in Table 1, under the headings GENE (50D) and GENE (1000D). For SET 2 we reverse train-ing and testing replicas in SET 1 . GPRN outperforms all of the other models, with between 46% and 68% of the SMSE, and similarly strong results on the MSLL error metric. 3 On both the 50 and 1000 dimensional datasets, the marginal likelihood for the network struc-ture is sharply peaked at q = 1, as we might expect since there is likely one transcription factor twi con-trolling the expression levels of the genes in question. Typical GPRN (VB) runtimes for the 50D and 1000D datasets were respectively 12 seconds and 330 seconds. These runtimes scale roughly linearly with dimension ( p ), which is what we expect. GPRN (VB) runs at about the same speed as the sparse CMOGP meth-ods, and much faster than CMOGP, LMC and SLFM, which take days to run on the 1000D dataset. The GPRN (ESS) runtimes for the 50D and 1000D datasets were 40 seconds and 9000 seconds (2.5 hr), and re-quired respectively 6000 and 10 4 samples to reach con-vergence, as assessed by trace plots of sample likeli-hoods. In terms of both speed and accuracy GPRN (ESS) outperforms all methods except GPRN (VB). GPRN (ESS) does not mix as well in high dimen-sions, and the number of ESS iterations required to reach convergence noticeably grows with p . However, ESS is still tractable and performing relatively well in p = 1000 dimensions, in terms of speed and predictive accuracy. Runtimes are on a 2 . 3 GHz Intel i5 Duo Core processor. 5.2. Jura Geostatistics Here we are interested in predicting concentrations of cadmium at 100 locations within a 14.5 km 2 region of the Swiss Jura. For training, we have access to measurements of cadmium at 259 neighbouring loca-tions. We also have access to nickel and zinc concen-trations at these 259 locations, as well as at the 100 locations we wish to predict cadmium. While a stan-dard Gaussian process regression model would only be able to make use of the cadmium training mea-surements, a multi-task method can use the correlated nickel and zinc measurements to enhance predictions. With GPRN we can also make use of how the correla-tions between nickel, zinc, and cadmium change with location to further enhance predictions.
 The network structure with the highest marginal like-lihood has q = 2 latent node functions. The node and weight functions learnt using VB for this setting are shown in Figure 1 of the supplementary material (Wil-son &amp; Ghahramani, 2012). Since there are p = 3 out-put dimensions, the result q &lt; p suggests that heavy metal concentrations in the Swiss Jura are correlated. Indeed, using our model we can observe the spatially varying correlations between heavy metal concentra-tions, as shown for cadmium and zinc in Figure 2. Although the correlation between cadmium and zinc is generally positive (with values around 0.6), there is a region where the correlations noticeably decrease, perhaps corresponding to a geological structure. The quantitative results in Table 1 suggest that the ability of GPRN to learn these spatially varying correlations is beneficial for predicting cadmium concentrations. We assess performance quantitatively using mean ab-solute error (MAE) between the predicted and true cadmium concentrations. We restart the experiment 10 times with different initialisations of the parame-ters, and average the MAE. The results are marked by JURA in Table 1. The experimental setup follows Goovaerts (1997) and Alvarez &amp; Lawrence (2011). We found log transforming and normalising each dimen-sion to have zero mean and unit variance to be ben-eficial due to the skewed distribution of the y -values (but we also include results on untransformed data, marked with *). All the multiple output methods give lower MAE than using an independent GP, and GPRN outperforms SLFM and the other methods.
 For the JURA dataset, the improved performance of GPRN is at the cost of a slightly greater runtime. However, GPRN is accounting for input dependent sig-nal and noise correlations, unlike the other methods. Moreover, the complexity of GPRN scales linearly with p (per iteration), unlike the other methods which scale as O ( N 3 p 3 ). This is why GPRN runs relatively quickly on the 1000 dimensional gene expression dataset, for which the other methods are intractable. These data are available from http://www.ai-geostats.org/ . 5.3. Multivariate Volatility In the previous experiments the GPRN implicitly ac-counted for multivariate volatility (input dependent noise covariances) in making predictions of y ( x  X  ). We now test the GPRN explicitly as a model of mul-tivariate volatility, and assess predictions of  X ( t ) = cov[ y ( t )]. We make 200 historical predictions of  X ( t ) at observed time points, and 200 one day ahead fore-casts. Historical predictions can be used, for example, to understand a past financial crisis. The forecasts are assessed using the log likelihood of new observa-tions under the predicted covariance, denoted L Fore-cast. We follow Wilson &amp; Ghahramani (2010a), and predict  X ( t ) for returns on three currency exchanges (
EXCHANGE ) and five equity indices ( EQUITY ) processed as in Wilson &amp; Ghahramani (2010a). These datasets are especially suited to MGARCH, the most popu-lar multivariate volatility model, and have become a benchmark for assessing GARCH models (Poon &amp; Granger, 2005; Hansen &amp; Lunde, 2005; Brownlees et al., 2009; McCullough &amp; Renfro, 1998; Brooks et al., 2001). We compare to full BEKK MGARCH (Engle &amp; Kroner, 1995), the generalised Wishart process (Wil-son &amp; Ghahramani, 2010a), and the original Wishart process (Bru, 1991; Gouri  X eroux et al., 2009). We see in Table 1 that GPRN (ESS) is often out-performed by GPRN (VB) on multivariate volatility sets, suggesting convergence difficulties with ESS. The high historical MSE for GPRN on EXCHANGE is essen-tially training error, and less meaningful than the en-couraging step ahead forecast likelihoods; to harmo-nize with the econometrics literature, historical MSE for EXCHANGE is between the learnt covariance  X ( x ) and observed y ( x ) y ( x ) &gt; . See Wilson &amp; Ghahramani (2010a) for details. Overall, the GPRN shows promise as both a multi-task and multivariate volatility model, especially since the multivariate volatility datasets are suited to MGARCH. These data were obtained using Datastream ( http://www.datastream.com/ ). A Gaussian process regression network (GPRN) has a simple and interpretable structure, and generalises many of the recent extensions to the Gaussian pro-cess regression framework. The model naturally ac-commodates input dependent signal and noise corre-lations between multiple output variables, heavy tailed predictive distributions, input dependent length-scales and amplitudes, and adaptive covariance functions. Furthermore, GPRN has scalable inference proce-dures, and strong empirical performance on several real datasets.
 In the future, it would be enlightening to use GPRN with different types of adaptive covariance structures, particularly in the case where p = 1 and q &gt; 1; in one dimensional output space it would be easy, for in-stance, to visualise a process gradually switching be-tween brownian motion, periodic, and smooth covari-ance functions. It would also be interesting to ap-ply this adaptive network to classification, or to use a GPRN where the weight functions depend on a differ-ent set of variables than the node functions. We hope the GPRN will inspire further research into adaptive networks, and further connections between different areas of machine learning and statistics.

