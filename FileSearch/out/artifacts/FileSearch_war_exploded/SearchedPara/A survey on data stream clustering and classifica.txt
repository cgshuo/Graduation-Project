 Hai-Long Nguyen  X  Yew-Kwong Woon  X  Wee-Keong Ng Abstract Nowadays, with the advance of technology, many applications generate huge amounts of data streams at very high speed. Examples include network traffic, web click streams, video surveillance, and sensor networks. Data stream mining has become a hot research topic. Its goal is to extract hidden knowledge/patterns from continuous data streams. Unlike traditional data mining where the dataset is static and can be repeatedly read many times, data stream mining algorithms face many challenges and have to satisfy constraints such as bounded memory, single-pass, real-time response, and concept-drift detection. This paper presents a comprehensive survey of the state-of-the-art data stream mining algorithms with a focus on clustering and classification because of their ubiquitous usage. It identifies mining constraints, proposes a general model for data stream mining, and depicts the rela-tionship between traditional data mining and data stream mining. Furthermore, it analyzes the advantages as well as limitations of data stream algorithms and suggests potential areas for future research.
 Keywords Data stream mining  X  Clustering  X  Classification  X  Survey 1 Introduction Traditional data mining research mostly focused on mining resident and static data reposi-tories. However, technological developments give rise to the emergence of data streams and changed the way people store, communicate, and process data. Nowadays, many organi-zations generate large amounts of data at higher speed than ever. For example, on a daily basis, Google handles more than 3.5 billion searches, 1 NASA satellites generate around 4TBimages, 2 and WalMart records more than 20 million transactions. 3 The new  X  X ntensive data X  research problem is: How can one model an infinite amount of continuous, rapid, and time-evolving data streams with a time-critical requirement?
These datasets are too large to fit in main memory and are alternatively stored in secondary storage devices. Therefore, random access to these datasets, which is commonly assumed in traditional data mining, is prohibitively expensive. One goal of data stream mining is to create a learning process that linearly increases according to the number of examples. Moreover, as data continuously arrive with new information, the model that was previously induced not only needs to incorporate new information, but also eliminates the effects of outdated data. Simply retraining the model with new examples is ineffective and inadequate; therefore, another goal of data stream mining is to update its model incrementally as each example arrives.

In this survey, we focus on clustering and classification of data stream as they are the two most frequent forms of data mining. Clustering aims to group a dataset into subsets (clusters), where data objects within a cluster are  X  X imilar X  and data objects in different clusters are  X  X issimilar X  with respect to a given similarity measure. Classification is the process of finding a general model from known data and then using this model to predict class labels for new data objects. Readers may find a good introduction of traditional data mining in a book of Jiawei Han [ 51 ].

There are some stream mining surveys that give overviews of various data stream algo-rithms. Gaber et al. [ 38 ] introduced a survey with theoretical foundations and basic algorithms for various tasks of data stream mining, including classification, clustering, frequency count-ing, and time series analysis. For stream clustering, it covers STREAM algorithm that uses a divide-and-conquer strategy to incrementally cluster data streams [ 75 ], and CluStream [ 3 ] that maintains its online summarized statistics and performs clustering in an offline manner. For stream classification, the survey consists of a decision tree algorithm for data streams [ 35 ], an ensemble-based classifier et al. [ 99 ], and a nearest neighbor classifier On-Demand clas-sification [ 5 ]. However, it may not be easy to understand the survey as the authors did not provide clear distinctions among these algorithms. In addition, many new and interesting algorithms have emerged since then.

Another survey of Gama and Rodrigues [ 42 ] focused on illustrating data stream mining with specific algorithms and applications. The study is interesting with a real-life example of an electrical network problem. Data streams are continuously generated by around 4,000 sensors spread over the electrical network. To make hourly/daily/weekly forecasts on the electronic load, several data stream algorithms are applied, including the PiD algorithm (Par-tition Incremental Discretization) for data preprocessing [ 41 ], ODAC clustering algorithm with incremental correlation measure [ 80 ], and VFDT decision tree classifier [ 35 ]. Although the study gives a practical viewpoint of data stream mining, it is lacking in in-depth analy-sis and the covered algorithms are quite outdated. Later, Aggarwal [ 2 ] attempted to give a broader overview of data stream mining, where more mining tasks and real applications were discussed, but it still does not give readers a sufficient understanding of current data stream mining methods.
In a recent survey paper, Silva et al. [ 86 ] introduced a taxonomy to classify data stream clustering algorithms. In addition, the authors also examined temporal aspects of data stream clustering, which may obscure the boundary separating outliers from possibly new clusters. Our work dovetails this paper by creating a framework to better understand the evolution of classical clustering algorithms into their stream versions, which will allow future algorithms to be more systematically designed to cope with specific temporal effects. We also review some newly emerged algorithms as well as stream validation methods.

In this paper, we introduce a comprehensive survey of the latest data stream clustering and classification algorithms. We present preliminaries and overview of data stream mining in Sect. 2 . Then, we discuss state-of-the-art algorithms in detail, including their merits and limitations.Weclassifythemintodifferentcategoriesbasedontheirapproachesandderivethe relationships between traditional mining algorithms and stream mining algorithms in Sects. 3 and 4 . We also analyze capabilities of each algorithm in terms of addressing constraints in a data stream setting. Finally, we discuss future research in Sect. 5 and conclude the survey in Sect. 6 . 1.1 Data stream applications Here, we introduce several data stream applications and requirements of performing data analytics on this special kind of data.  X  Mining query streams : Searching the web to retrieve information has become an essential  X  Network monitoring : Internet includes many routers that are connected and communicate  X  Sensor networks : A sensor network consists of spatially distributed autonomous sensors  X  Social network streams : Online social networks (OSNs) have become more and more 1.2 Software for data stream mining There is some useful, open-source software for data stream mining research.  X  WEKA: 4 WEKA is the most well-known data mining software within the academic envi- X  Massive Online Analysis (MOA): 5 MOA is based on the WEKA framework that is  X  RapidMiner: 6 RapidMiner is another open-source software for data mining. RapidMiner 2 Overview of data stream mining We define a data stream DS as a sequence of data objects or samples: DS = { x y  X  Y ={ y 1 , y 2 ,..., y c } when classifying data stream, and there is no label when clustering data stream. 2.1 Mining constraints Data streams have intrinsic characteristics, such as possibly infinite volume, chronological order, and dynamical changes. For example, Google processes million searches daily, each of which is attached with a time stamp; and these searches are changed according to different hot topics at different times.
 Table 1 shows comparisons between traditional data mining and data stream mining. Traditional data mining is able to scan datasets many times; executes with unlimited time and memory; has only one concept; and needs to produce fairly accurate results. On the other hand, data stream mining may produce approximate results and has to satisfy constraints, such as single-pass, real-time response, bounded memory, and concept-drift detection :  X  Single-pass: Unlike traditional data mining that may read static datasets repetitively  X  Real-time response: Many data stream applications such as stock market prediction  X  Bounded memory: The amount of arriving data is extremely large or potentially infinite.  X  Concept-drift detection: Concept drifts refer to the situation when the discovered patterns Definition 1 ( Concept ) A concept or a data source is defined as a set of the prior probabilities (definition from [ 57 ]): Traditional data mining only works with a single concept, which means the distributions of training dataset and testing dataset are the same. Meanwhile, data streams are dynamic and have many concepts. An example is the problem of preventing malicious attacks in network monitoring. The descriptions of the two class  X  X ormal X  and  X  X ttack X  evolve over time. Moreover, attackers are active and always try to devise new attacking methods, thereby changing the definition of class  X  X ttack. X 
Suppose data stream DS consists of a set of k data sources S i with known distributions, i are the influence of data source S i at a time stamp t ,where k i = 1 w i ( t ) = 1. The data distribution of the data stream DS at a time stamp t is characterized as follows: Definition 2 (Concept Drift) . Suppose a data stream DS consists of a set of k data sources S with influence w i . The underlying data distribution of data stream DS is DS ( t ) = { w stamps t 1 and t 2 that DS ( t 1 ) = DS ( t 2 ) , we say there is a concept drift. The term concept drift indicates the change of the influence of data sources over time. Figure 1 illustrates an example of a concept drift, where the data distribution of a data stream DS gradually changes from a data source S 1 toadatasource S 2 .

To help readers understand an overview of data stream mining, we propose a general model of data stream algorithms in Fig. 2 . When a data stream comes, a buffer is used to store the most recent data. The stream mining engine reads the buffer to create a synopsis of the data in memory. In order to maintain the synopsis, the system may apply different time window and computational approaches. When certain criteria are triggered, for example, a user X  X  request or after a certain time lapse; the stream mining engine will process the synopsis and output approximate results. In general, most data stream algorithms are derived and adapted from traditional mining algorithms. Lastly, stream validation methods are applied to evaluate the performance of data stream algorithms. 2.2 Time windows As data streams are potentially infinite, it is possible to only able to process a portion of the entire data streams. This interesting portion is defined as a time window of data objects. W [ i , j ]= ( x i , x i + 1 ,..., x j ) ,where i &lt; j . There are different types of time windows: landmark window, sliding window, fading window, and tilted time window. 2.2.1 Landmark window In the landmark window, we are interested in the entire data stream from starting time the landmark window, all transactions in the window are equally important; there is no difference between past and present data. However, as data stream evolves continuously, the model built with old data objects may become inconsistent with the new ones. In order to emphasize recent data, one may apply the sliding window, tilted window, or fading window variants. 2.2.2 Sliding window In the sliding window variant W [ t c  X  w + 1 , t c ] , we are only interested in the w most is dependent on the size of the window w .If w is too large and there is a concept drift, the window possibly contains outdated information, and the accuracy of the model decreases. If w is small, the window may have deficient data and the model over-fits and suffers from large variances. Previous work considers a fixed value for the size of the sliding window specified by users or an experimental value. Recently, there are proposals for flexible sliding windows where the size of the window changes according to the accuracy of the model [ 16 , 65 ]. When the accuracy is high, the window extends; and when the accuracy is low, the window shrinks. 2.2.3 Fading window In the fading window variant, each data object is assigned a different weight according to its arrival time so that new transactions receive higher weights than old ones [ 22 , 24 , of old and outdated transactions on the mining results. A decreasing exponential function f ( t ) =  X  t ( 0 &lt; X &lt; 1 ) is usually used in the fading model. In this function, t is the age of a data object that is equal to time difference between the current time and its arrival time. The fading window needs to choose a suitable fading parameter  X  , which is typically set in the range [0.99, 1] in real applications. 2.2.4 Tilted time window The tilted time window variant is somewhere between the fading window and sliding window variants [ 3 , 5 ]. It applies different levels of granularity with regard to the recency of data. One is more interested in recent data at fine scale than long-term data from the past at coarse scale. Tilted time window approximately stores the entire dataset and provides a nice trade-off between storage requirements and accuracy. However, the model may become unstable after running for a long time. For example, the tree structure in FP-Stream [ 29 ] will become very large over time, and the process of updating and scanning over the tree may degrade its performance. Similarly, the microstructures in On-Demand Classification [ 5 ] will become larger and larger that may give rise to the problem of low-purity clustering with large micro-clusters [ 110 ].

Figure 3 shows examples of four different time windows. For fading window,  X  is set to 0 . 99; the weights of data objects decrease. For tilted time window, we store the four most recent quarters of an hour, then the last 24h, and last 31days. 2.3 Computational approaches Besides various time window variants, there are two computational approaches to process the data streams. 2.3.1 Incremental learning Incremental learning is another computational approach for data streams [ 32 , 44 , 46 , 53 , 68 , 69 , 82 , 87 , 90 ]. In this approach, the model incrementally evolves to adapt to changes in incoming data. There are two schemes to update the model: by data instance and by window. For example, Street et al. [ 90 ] deployed an ensemble of classifiers for data stream. It evaluated a window of incoming data and adapted the model by adjusting the weight of each classifier or replacing an old classifier with an updated one. Figure 4 a illustrates the incremental learning approach. It has the advantage of providing mining results instantly, but requires more computational resources. 2.3.2 Two-phase Learning Two-phase learning, also known as online X  X ffline learning, is a common computational process into two phases. In the first phase (online phase), a synopsis of data is updated in a real-time manner. In the second phase (offline phase), the mining process is performed on the stored synopsis whenever a user sends a request. For example, Aggarwal et al. [ 3 ] proposed an online X  X ffline clustering method for data streams. Its online component summarizes the statistical information of the data stream in a real-time manner. Meanwhile, the offline com-ponent uses the summary statistics to perform clustering at a high level whenever required. The two-phase learning approach is depicted in Fig. 4 b. This approach is able to process data streams at high speed. However, its limitation is that users must wait until the mining results are available. 2.4 Stream validation In traditional data mining with limited amount of data, its validation process focuses on maximizing the use of data. Hold-out, cross-validation, and leave-one-out are standard vali-dation methods. The holdout method randomly divides the dataset into two subsets, one for and ( 2 3 , 1 3 ) .The k -fold cross-validation segments the data into k independent and equal-size subsamples. A single subsample is used for testing while the remaining ( k  X  1 ) subsamples are merged and used as training data. The validation process is repeated k times so that each subsample is used exactly once as testing data. The leave-one-out method is a variant of the cross-validation method where the number of folds k is equal to the data size.

In a data stream environment, as data are potentially infinite, validation focuses on eval-uating the model at various stages. A well-known approach is to plot a learning curve by recording the model X  X  performance over time, which will show how much the model improves with additional training data and how well it adapts to concept drifts. An algorithm is said to be more superior than another if its learning curve is above the other X  X  curve most of the time.

Hold-out and prequential are two popular approaches for stream validation. In the hold-out method, data examples are collected into chunks. Each data chunk is first used as a test example and then used to update the model. The hold-out method is preferred in scenarios with concept drifts since it allows the model to adapt to latest changes of the data. Prequential (or Interleaved Test-Then-Train) is another validation method for data streams [ 17 , 43 ]. Each data instance is used for testing the model before it is used to incrementally update the model. This method can be considered as a special case of the hold-out method, where chunk size is equal to one. It has the advantage of not needing a predefined chunk size; however, it obscures the algorithm X  X  performance at a certain time since the model X  X  early mistakes will quickly diminish over time. 2.4.1 Evaluation measures Evaluation measures for traditional data mining can generally be applied for data stream mining. For data clustering, some widely used measures are purity, precision, sum of squared distance, and F-measure. There are some constraints in data clustering such as the cluster homogeneity constraint, cluster completeness constraint, rag bag constraint, and cluster size versus quantity constraint. Unfortunately, these clustering metrics cannot simultaneously satisfy the above constraints. The BCubed measure, which computes the average correctness over the dataset, is stated to meet the above constraints at the same time [ 13 ]. Recently, Kremer et al. [ 62 ] proposed an effective evaluation measure for clustering data streams, named Cluster Mapping Measure (CMM). CMM is based on the concept of connectivity between points and clusters, which indicates how well a point fits the distribution of the cluster. It is able to handle different fault types caused by evolving data streams, including cluster aging, cluster joining, and cluster diminishing.

For data classification, accuracy and 0 X 1 loss function are two popular measures. To con-tinuously assess the performance of data stream classification, Game et al. [ 39 ] proposed the prequential loss method cooperating with different time windows. The forgetting prequential error is proven to converge to the Bayes error in case of stationary data, which makes it useful for concept drift detection. This method can easily apply to other performance measures. 2.5 Data repositories There is a shortage of available real-world data for data stream evaluation. One possible reason is that researchers from traditional data mining domains usually keep their data small enough to accommodate batch learning. Another reason is the privacy issue while publishing very large datasets; researchers often use their private data to demonstrate their systems that cannot be reproduced. To overcome this shortage, some synthetic datasets with unlimited number of examples are created, for example, Random Tree Generator [ 35 ], SEA Concepts Generator [ 90 ], and Rotating Hyperplane [ 53 ]. These data generators are implemented in the MOA software package [ 17 ]. We also recommend some data repositories with large datasets for stream evaluation:  X  UCI Machine Learning Repository  X  X n online repository for the empirical analysis of  X  KDD Cup Center  X  X nnual Data Mining and Knowledge Discovery competition orga-3 Clustering Clustering or data segmentation is the process of grouping objects into different sets called clusters. The goal is that data objects in the same cluster are similar and are dissimilar to data objects in other clusters. Clustering is a well-studied problem, and many clustering methods have been proposed in the literature [ 54 ]. As mentioned in the general model (Fig. 2 ), data stream algorithms typically maintain synopses of data streams using different time window and computational approaches. They generally extend traditional algorithms to work for data streams with the goal to satisfy constraints, such as bounded memory, single-pass, real-time processing, and concept drifts. Similar to traditional data clustering, data stream clustering methods can be classified into five categories: partitioning methods, hierarchical methods, density-based methods, grid-based methods, and model-based methods . Moreover, it requires a measure distance between clusters as two clusters may be merged while clustering. Basi-cally, there are four types of distance measures, including minimum distance (single-linkage), maximumdistance(complete-linkage),meandistance,andaveragedistance.Sincemaximum distance and average distance require expensive computation, they are rarely used in data stream settings. Minimum distance and mean distance are more popular for data streams, for example, D-Stream [ 24 ], Den-Stream [ 22 ], and MR-Stream [ 97 ] use minimum distance for cluster merging, while STREAM [ 75 ], ClusStream [ 3 ] use mean distance for cluster merging. 3.1 Partitioning methods A partitioning algorithm groups dataset into k clusters, where k is a predefined parameter. It iteratively reassigns objects from one group to another group in order to minimize its objective function. For traditional clustering, the most popular methods are k-means and k-medians [ 56 ].  X  X TREAM [ 75 ]: STREAM is one of the first data stream algorithms, which extends the k-medians algorithm. To address the bounded memory and single-pass constraints, it uses a divide-and-conquer strategy and performs clustering incrementally. The STREAM algorithm breaks the data stream into chunks D 1 ,..., D r ,... , each of which has a manageable size m and fits into main memory. For each D i with at most m data points { x 1 ,..., x m } , STREAM uses a k -medians algorithm to select k representatives (medians) { c 1 ,..., c k } from D i and assigns each data point to its closest representatives. The objective function is to minimize the sum of squared distance (SSQ) measure: where the assignment operator is denoted by  X  , and the distance function between two data points is denoted by dist (., .) .

After a chunk has been processed, we only store k medians and their weights. The process is repeated for the next chunks. When the number of representative points exceeds m ,a second level of cluster is applied to select level-2 representatives. In general, the algorithm is performed in a multi-level manner. Whenever the number of representatives at level-l reaches m , they are clustered to select k representatives of level-( l + 1 ) . Together with a sampling method, STREAM is able to perform clustering with limited time and memory. Obviously, the STREAM algorithm is sensitive to parameter k due to the intrinsic properties of k -medians and is only able to discover spherical clusters. Moreover, it fails to detect concept drifts, where the underlying stream may evolve and change significantly. 3.2 Hierarchical methods A hierarchical method aims to group data objects into a hierarchical tree of clusters. Hierar-chical clustering methods can be further classified as either agglomerative or divisive, where the hierarchical decompsition is formed in a bottom-up/merging or top-down/splitting fash-ion, respectively. Some traditional hierarchical algorithms are BIRCH [ 108 ], CURE [ 47 ], ROCK [ 48 ], and CHAMELEON [ 55 ].  X  X luStream [ 3 ]: CluStream extends the traditional clustering method BIRCH [ 108 ]for data streams. CluStream uses micro-clusters to capture the summary information about the data streams. A micro-cluster is defined as a temporal extension of the clustering feature vector in BIRCH as follows: Definition 3 (Micro-Cluster) . A micro-cluster for n data points { x i 1 ,..., x i n } with time time stamps, i.e., n j = 1 i j ,and n is the number of data points.

CluStream follows the online X  X ffline approach, which is similar to the multi-phase clus-tering technique in BIRCH. In the online phase, CluStream continually maintains a set of q micro-clusters in the data stream. When a new micro-cluster is created, an outlier micro-cluster is deleted or two neighbor micro-clusters are merged. In the offline phase, it performs k -means to cluster the stored q micro-clusters. CluStream analyzes the evolution of clusters by using additional property to extract information of micro-clusters during a specific time range. Moreover, it applies the tilted time window to optimize the number of stored snapshots (the status of micro-clusters in the data stream) at differing levels of granularity.
Based on CluStream X  X  framework, many improvements have been proposed. HPStream addresses the problem of high-dimensional data streams by deploying a projection technique to select the best attribute set for each cluster (subspace clustering) [ 4 ]. Similar to CluS-tream, HPStream maintains micro-clusters to capture the summary information about the data stream. Furthermore, each micro-cluster consists of a set of relevant attributes, which can be considered its subspace. When a new data instance arrives, the average Manhattan distance between the new instance and each cluster is computed. Only relevant attributes of the clusters are utilized in the distance computation. Then, the new instance is assigned to the closest cluster if their distance does not exceed a limiting range, a multiple of the cluster X  X  radius. Moreover, the statistical properties of the closest cluster are also updated. HPStream only maintains a fix number of micro-clusters. When the number of clusters reaches a max-imum value, it removes the oldest cluster to give space for a new one.

SWClustering identifies a problem that the clustering results of CluStream may degrade after running for a long time [ 110 ]. For example, when the center of a micro-cluster gradually shifts, CluStream maintains the micro-cluster with growing radius, instead of splitting it into many micro-clusters. To summarize data streams, SWClustering creates a temporal cluster feature (TCF) for a sliding window. The TCF is similar to a micro-cluster; the only difference is that TCF stores the latest timestamp, while micro-cluster stores the sum of timestamps. An exponential histogram of cluster feature (EHCF), a collection of TCFs, is used to capture the evolution of individual clusters. SWClustering not only produces more qualified clustering due to the fine granularity of EHCFs; it also has better performance than CluStream in terms of running time and memory usage.

E-Stream classifies cluster evolution into five categories: appearance, disappearance, self evolution,merging,andsplitting.Itutilizesthefadingmodelandclusterhistogramstoidentify the type of cluster evolution [ 95 ]. ClusTree uses an R*-tree structure [ 15 ]toindexmicro-insertion, which iteratively evaluates for better decisions as long as time permits. Thereby, the algorithm automatically adapts to the speed of the incoming data streams.  X  X EPSTREAM [ 69 ]: Inspired by CHAMELEON [ 55 ], REPSTREAM is a graph-based hierarchical clustering approach for data streams. To identify clusters, REPSTREAM updates two sparse graphs that are formed by connecting each vertex to its k -nearest vertices. The first graph captures the connectivity relationship among coming data points and is used to select a set of representative vertices. The second graph of representative vertices helps to make clustering decisions at a higher level. REPSTREAM applies the fading window to diminish the effect of old data. REPSTREAM keeps track of the connectivity between the representative vertices and performs merging or splitting according to their connectivity. 3.3 Density-based methods Density-based methods build up a density profile of data for clustering purposes. Thereby, clusters are considered as dense regions of objects and are separated by sparse regions with low density in the data space. Density-based clustering is able to discover arbitrary-shaped clusters and does not require the predifined number of clusters. DBSCAN [ 71 ], OPTICS [ 14 ], and PreDeCon [ 18 ] are some well-known, traditional density-based clustering methods.  X  X enStream [ 22 ]: DenStream is a density-based stream clustering algorithm that extends the DBSCAN algorithm. Similar to CluStream, DenStream uses micro-clusters to capture synopsis information of data streams; its online component continually updates the micro-clusters collection. Each micro-cluster has a center and a radius that are derived from its clustering feature vector. DenStream applies the fading model where elements of its feature vector decrease over time. Given threshold values for the weight and radius, there are three types of micro-clusters: a core micro-cluster, a potential core micro-cluster, and an outlier micro-cluster. For the offline components, it applies DBSCAN on these kinds of micro-clusters; a cluster is created as a group of micro-clusters that are dense and close to another.  X  X PTICS-Stream [ 92 ]: OPTICS-Stream is an extension of the OPTICS algorithm for data streams. Similarly to DenStream, it uses micro-clusters and the fading model to construct the synopsis. The offline component performs clustering by using the definitions of core-distance and reachability distance in OPTICS [ 14 ]. A micro-cluster is a core micro-cluster if its weight is greater than or equal to  X  , and its radius is less than or equal to . A micro-cluster c q is density-reachable to a core micro-cluster c p if the Euclidean between their centers is less than or equal to 3  X  . By the order of data points with regard to the reachability distance, the 3-D reachability plot with one more time dimension can be used to visualize the changes of the cluster structures in the data stream over time.  X  X ncPreDecon [ 63 ]: incPreDecon is an incremental version of the PreDeCon [ 18 ] algo-rithm, which is designed to work for dynamic data. The algorithm supports two types of updates, a single instance update and batch update. Both updating methods share the same strategy. They first identify a group of affected objects, whose properties may be changed into one of the following cases: (i) core  X  non-core, (ii) non-core  X  core, and (iii) core  X  core but under different preferences. Given new arriving data p , the group of affected objects consists of reachable objects from p . Properties of these objects, as well as their clusters, are updated according. The ability of incPreDecon to work for data streams is still unclear as only experiments with relatively small datasets have been performed. 3.4 Grid-based methods Grid-based clustering methods quantize data space into a multi-resolution grid structure. The grid structure contains many cells, each of which has a subspace and stores summary infor-mation of data objects within the subspace. Then, clusters are determined by dense regions of nearby dense cells. There are some popular, traditional grid-based clusters algorithms, for example, DENCLUE with a fix-sized grid [ 11 ], STING with a multiresolution grid [ 100 ], and WaveCluster with wavelet transformation method to make the clusters more salient in the transformed space [ 85 ].  X  X -Stream [ 24 ]: D-Stream is a density-based clustering method for data streams. It can be considered as an extension of the DENCLUE algorithm [ 11 ]. In D-Stream, each dimension is divided into p segments. With this, a grid of equal size hyper-rectangular cells is created. Similar to a micro-cluster, a grid cell in D-Stream is used to store synopsis information of data objects falling into it. As D-Stream uses the fading model to decrease the weight of cell over time, it periodically removes sparse grid cells to save memory and accelerate the mining process. D-Stream performs clustering upon a user request. A cluster is defined as a group of adjacent dense grid cells.  X  X R-Stream [ 97 ]: MR-Stream is a multi-resolution density-based clustering method for data stream. MR-Stream takes advantages of both D-Stream [ 24 ]andSTING[ 100 ]. MR-Stream proposes a tree of grid cells to capture the hierarchical structure of the data space. A deeper-level tree node has higher granularity level. Similar to D-Stream, MR-Stream applies the fading model and periodically prunes sparse grid cells to save memory. Additionally, MR-Stream significantly reduces the number of tree nodes by merging sibling nodes of a parent node if they are all dense or sparse. Thus, MR-Stream preserves more memory than D-Stream and accelerates the clustering process. Moreover, MR-Stream provides a better cluster result by extending the neighborhood range concept and supports a memory sampling method that helps users to detect when concept drifts occur.  X  X ellTree [ 77 ]: CellTree is another grid-based algorithm for data streams. It starts by partitioning the data space into a set of mutually exclusive equal size cells. When a weight of a cell is greater than a threshold value, the cell is dynamically divided into two intermediate cells using a hybrid-partition method that selects a better method between  X  -partition and  X  -partition methods. The  X  -partition divides a dimension with the largest standard deviation, while the  X  -partition method choose to split a dimension with the smallest standard deviation. To save memory, CellTree prunes sparse cells with density less than the threshold value. CellTree has been extended to a better version Cell*Tree [ 78 ]thatusesa B+Tree to store the synopses of data streams. The hybrid-partition method has a drawback that CellTree is not able to employ any indexing structure to access a specific grid cell immediately. In Cell*Tree, a dense grid cell is divided into a fixed number of equal size grid cells, which are indexed easily based on its order. Moreover, Cell*Tree applies the fading model to emphasize the latest change of information in a data stream on the clusters. 3.5 Model-based methods Model-based clustering methods attempt to optimize the likelihood between data and some statistic models. For traditional model-based clustering, Expectation-Maximization (EM) algorithm is a soft clustering method [ 34 ], and Self-Organizing Map (SOM) is a popular neural network method for clustering [ 59 ].  X  X WEM [ 32 ]: SWEM is an EM-based clustering algorithm for data streams using a sliding window. In SWEM, each micro-component is represented by a tuple consisting of a weight, a mean, and a covariance matrix. For the first data window, SWEM applies the EM algorithm to obtain the converged parameters. Then, in the incremental phase, SWEM utilizes the converged parameters in the previous window of data object as the initial values for the mixture models X  parameters. If the two sets of parameters are significantly different, SWEM redistributes components in the entire data space by splitting those micro-components with large variance and merging neighbor micro-components. SWEM also deploys the fading model to expire the statistic summarization of the micro-components. In short, SWEM may be considered as an EM clustering using Mahalanobis 7 distance with fading window.  X  X CPSOM [ 87 ]: There are two important extensions of SOM: Growing self-organizing map (GSOM)[ 10 ] and cellular probabilistic self-organizing map (CPSOM) [ 28 ]. In GSOM, there is no need to prespecify the size of the output map; it dynamically grows nodes at the boundary of the map whenever its accumulated error exceeds a threshold. CPSOM is an online algorithm and is suitable for large datasets. CPSOM uses a fading window to reduce the weight of the neuron state. Thus, CPSOM may forget old patterns and adapt to new patterns as they appear. GCPSOM is a hybrid algorithm that aggregates the advantages of both the GSOM and CPSOM. Therefore, GCPSOM dynamically grows the feature map for clustering data streams and keeps track clusters as they evolve. 3.6 Overall analysis Previous sections have introduced many aspects of data stream mining, including constraints in data stream mining, time windows, and computational approaches. Many traditional and data stream clustering algorithms are also presented. Furthermore, we perform an overall analysis to summarize the above ideas and give readers a coherent view of data stream clustering.InFig. 5 ,weplotanintuitivediagramtooutlinetherelationshipbetweentraditional clustering algorithms and data stream clustering algorithms. Traditional clustering methods are shown on the left of the diagram, and data stream clustering methods are on the right. Two schemata for categorizing data stream clustering methods are in the middle with two computational approaches and four time windows.

Based on this diagram, we observe that most data stream clustering methods are adapted from traditional clustering methods but apply different computational approaches and time windows. For example, STREAM extends the k -means algorithms with incremental compu-tational approach and landmark window. The key idea of STREAM is to apply the LSEARCH technique to perform k -means incrementally and hierarchically. CluStream extends the BIRCH algorithm and applies the two-phase learning approach and tilted time window. HPStream improves CluStream to work for high-dimensional data streams; SWClustering enhances CluStream on long-term running; E-Stream extends CluStream to classify different types of concept drifts; and ClusTree indexes micro-clusters for automatical adaptation to the speed of data streams. REPSTREAM extends CHAMELEON algorithm with the incremental learning approach and fading window. Similarly, DenStream is an extension of DB-SCAN with the two-phase learning approach and fading window. incPreDeCon combines it with the preference distance measure to work with data streams. D-Stream inherits from DENCLUE; it has been extended into a multi-resolution approach with merging and splitting operations in MR-Stream. Moreover, CELL-TREE, XWAVE, SWEM, and GCPSOM are extensions of STING, WaveCluster, EM, and SOM, respectively.

Although many clustering methods have been proposed, they are often unable to address all data stream mining constraints simultaneously, which are discussed in Sect. 2 .Forthe concept-drift constraint, we establish two levels of how the algorithms respond to concept drifts. The first level, concept-drift adaptation , means that an algorithm may update with new concepts and remove outdated concepts. Most algorithms deploy the time windows to update fresh information; therefore, they satisfy this criterion. However, those applying the landmark window do not.
The second level, concept-drift classification , indicates that an algorithm may detect and adapt properly to different types of concept drifts. For example, with cluster shifting or recurrence, we should extend the sliding window or retrieve historical information of recur-rence clusters. Another example with cluster appearance/disapperance, a clustering algorithm should remove the least informative clusters (the least weight cluster or the farthest cluster from the new one), not the oldest one.

Furthermore, we evaluate whether these algorithms can work for high-dimensional data streams. Table 2 summarizes the capabilities of previously reviewed data stream cluster-ing techniques. We observe that only HPStream and incPreDecon can work with high-dimensional data streams. E-Stream partly satisfies the concept-drift classification constraint as it can distinguish different types of cluster evolutions; however, it does not consider high dimensionality.

Besides the trade-offs of time windows and computational approaches in Sect. 2 ,itis worthy to note that traditional and data stream clustering algorithms share similar advantages and limitations. Table 3 illustrates the trade-offs of each category of clustering techniques. For example, partitioning clustering algorithms are simple and relatively efficient; however, they need to specify the number of clusters and are unable to discover non-spherical clusters. Grid-based clustering methods are quite fast; they are able to discover arbitrary-shaped clusters. Nevertheless, their clustering quality depends on grid granularity, and they are unsuitable for high-dimensional data streams. 4 Classification Classification is the process of finding a general model from past data to apply to new data. Classification is performed in two steps: learning step (training) and testing step. In the learning step, the system tries to learn a model from a collection of data objects, called the training set. In the testing step, the model is used to assign a class label for unlabeled data objects in the testing set. There are many data stream classification techniques in the literature, such as the decision tree, Bayesian classification, neural networks, support vector machines, k -nearest neighbor, and ensemble classifiers. 4.1 Decision Tree Hoeffding tree is a decision tree classifier for data streams [ 35 ]. Traditional decision trees need to scan the training data many times to select the splitting attribute. However, this requirement is infeasible in the data stream environment. To overcome this limitation, the Hoeffding bound is used to choose an optimal splitting attribute within a sufficient amount of receiving data objects. Given N independent observations of a random variable r with range R and computed mean r , the Hoeffding bound guarantees that the true mean of r is at least r  X  with probability 1  X   X  ,where  X  is a user-specified parameter.

Let G ( X i ) be a heuristic measure to select a splitting attribute. After receiving N observations, X a and X b are the best and the second best splitting attribute. In case of Hoeffding tree, the variable r is now considered as r = G = G ( X a )  X  G ( X b ) .If r = G = G ( X a )  X  G ( X b )&gt; ,where is computed from the above equation, we say that this difference is larger than ( r  X  ) &gt; 0 with confidence 1  X   X  . Then, the attribute X a is selected to build the tree.

Hoeffding tree algorithm is an incremental algorithm, which satisfies the single-pass con-straint of data stream mining. For each new arriving data, Hoeffding tree algorithm uses Hoeffding bounds to check whether the best splitting attribute is confident enough to create the next level tree node.

Hoeffding tree algorithm has high accuracy and works well with large datasets. However, it is not able to handle concept drifts in data streams as no node can be changed once created. CVFDT [ 53 ] is an extension of the Hoeffding tree to address concept drifts in data streams. CVFDT maintains sufficient statistics at every tree node to monitor the validity of its previous decisions. When data come, it continually updates the statistics stored in tree nodes. Using the sliding window, CVFDT removes the effect of outdated data by decreasing the corresponding statistics at the tree nodes. It periodically scans the tree nodes to detect concept drifts. If concept drift appears, CVFDT concurrently grows alternative branches with the new best attribute and removes the old branches with alternative branches if it becomes less accurate. 4.2 Bayesian classification Seidl et al. proposed a novel index-based classifier called Bayes tree [ 84 ]. Adapted from the R*-tree [ 15 ], Bayes tree generates a hierarchical Gaussian-mixture tree to represent the entire dataset. Each tree node contains statistics of the data objects within, including a minimum bounding rectangle, the number of data objects, linear sum, and quadratic sum of all data objects.

To solve a multi-labeled classification problem, a single Bayes tree is constructed for each class. For each testing data object x , the algorithm tries to find a set of prefix-closed nodes E , called frontier, in every Bayes tree. The probability that x belongs to class c i is computed as follows: data object, the center and the deviation of tree node e s . Testing object x is labeled with the class having the maximum probability. Bayes tree is an anytime classifier that can provide a decision after a very short initialization and later provide more accurate decisions when more time is available by selecting the more precise frontier.

Bayes tree is later extended to the MC-tree [ 61 ] by Kranen et al. The MC-tree combines all classes in a multi-Gaussian tree and needs only one step to refine all class models simul-taneously rather than | C | steps in Bayes tree, where | C | is the number of classes. Moreover, MC-Tree optimizes tree construction by using multi-dimensional scaling (MDS) [ 33 ]to transform the data space. It is claimed to outperform Bayes tree with higher accuracy of up to 15%. 4.3 Neural Network Leite etal. proposed an evolving granular neuralnetwork (eGNN) supported by granule-based learning algorithms to classify data streams. There are two phases in eGNN. In the first phase, eGNN uses T-S neurons to construct information granules of incoming data. Then, the neural network is built on the information granules rather than the original data in the second phase. A granule associated with a class label is defined by triangular membership functions, which are later evolved to accommodate new data. The weights are decreased by a decay constant; this process helps to reduce the degree of importance of outdated granules. Basically, eGNN uses class exemplars in the form of information granules to perform classification tasks. When testing data comes, a max-neuron selects the best-fit granules and assigns their labels as the prediction labels of the testing data. eGNN is able to tackle classification problems in continuously changing environments. However, the requirement of long training time is still a cost that limits the ability of eGNN to work with a massive dataset. Therefore, only small datasets are used to evaluate the performance of eGNN in the experiment section of this paper. The authors later extend this work to a more general and efficient semi-supervised approaches [ 67 ] to work with partially labeled dataset. 4.4 Support vector machines (SVMs) Support vector machines have shown its prominent performance in many machine learning problems with static datasets. However, it is very expensive to use SMVs in large-scale application due to its time complexity O ( N 3 ) and memory complexity O ( N 2 ) ,where N is the number of data objects. To work with a very large dataset, Tsang et al. proposed the Core Vector Machine (CVM) algorithm that uses Minimum Enclosing Ball (MEB) to reduce its complexity [ 94 ]. A MEB is a hyper-sphere that represents the set of data objects inside it. The algorithm first finds a representative MEB set that is a good approximation of the original dataset. Then, the optimization problem of finding the maximum margin is directly performed on this MEB set.

Raietal.[ 79 ] proposed StreamSVM, an extension of CVM with a single scan, to work with data streams. In StreamSVM, a MEB has a flexible radius that is increased whenever a new training is added. The algorithm is competitive by providing an approximation result to the optimal one. However, StreamSVM is still unable to detect concept drifts in data streams. 4.5 k-Nearest-Neighbor Classifier (k-NN) On-Demand-Stream is a k -NN data stream classifier that extends the CluStream method [ 5 ]. It inherits most of the good features in CluStream such as the micro-cluster structure, the tilted time window, and the online X  X ffline approach. A micro-cluster in On-Demand-Stream is extended with a class label, and it only takes data objects with the same class label. Its offline classification process starts to find the best window of data objects, called the best time horizon. These micro-clusters in the best time horizon are extracted using the addition property of micro-clusters. On-Demand-Stream performs 1-NN classification by assigning a testing data object to the label of the closest micro-clusters.

Inspired by M-tree [ 30 ], Zhang et al. [ 104 ] proposed a Lazy-tree structure to index micro-clusters (which are called exemplars in the paper). This helps to significantly reduce k -NN X  X  classification time from O ( N ) to O ( log ( N )) ,where N is the total number of exemplars in the Lazy-tree. The tree consists of three main operations: search, insertion, and deletion operations.Theinsertionanddeletionoperationsaddnewnodesandremoveoutdatednodesin a way that guarantees that the tree is balanced. The search operation is used to classify testing data. A branch-and-bound technique based on the triangle inequality filters unnecessary checking exemplars; therefore, it minimizes the number of comparison and accelerates the searching operation. 4.6 Ensemble classifiers Bagging and Boosting have shown their superior through extensive experiments on traditional dataset. Therefore, many researchers have tried to adapt the methods to work on data streams.  X  Online Bagging &amp; Boosting : Oza et al. [ 76 ] proposed the Online Bagging &amp; Boosting , which is one of the first work of adapting traditional bagging and boosting. From a statistical view, each training data object appears k times in training datasets of classifier members with the probability where k is the size of training set and N is the size of dataset. On data streams, we can assume that the number of data objects is unlimited, N  X  X  X  . Therefore, the probability P ( k ) tends to a Poisson ( 1 ) distribution, where Poisson ( 1 ) = exp (  X  1 )/ k ! . Within this observation, Oza et al. proposed Online Bagging to assign each data object a weight according to Poisson ( 1 ) distribution, which is considered as a replacement sampling method. In Online Boosting, the weights of coming data objects and classifier members are adjusted according to the error rates of classifier members at the current time.  X  Weighted Ensemble Classifiers : With the observation that the expiration of old data should rely on data distribution instead of their arrival time, Wang et al. [ 98 ] proposed an accuracy-weighted ensemble (AWE) classifier for mining concept-drifting in data streams. The algorithm constructs and maintains a fix k number of classifiers, which can be C4.5, RIPPER, or naive Bayesian. Processing in a batch-mode manner, it use each new chunk of coming data objects to train a new classifier. Then, the ensemble is formed by selecting the k most accurate classifiers, and the weight of each classifier is set according to its accuracy. This method is stated to be better than a single data stream classifier, such as VFDT and CVFDT. However, it is quite sensitive to the chunk size and the number of classifier members k .
In real applications, data stream may contain noise where data instance may be mislabeled or have erroneous values. Zhang et al. [ 106 , 107 ] proposed an aggregated ensemble algorithm to tackle the problem of learning from noisy data stream. This approach is a combination of horizontal and vertical ensemble frameworks. The horizontal framework builds a different classifier on each data chunk, while the vertical framework builds different classifiers on the up-to-date data chunk with different learning algorithms. The horizontal framework is robust to noise and can reuse historical information; however, it is unsuitable with sudden drifts, where the concepts of data stream change dramatically. On the other hand, the vertical frame work can produce good results even in case of sudden drifts; nevertheless, it is sensitive to noise. Building classifiers on different data chunks using different learning algorithms, the aggregated ensemble constitutes a Classifier Matrix. The average weighting method is used on this matrix to predict the label for testing data. The author theoretically proved that the aggregate ensemble has less or equal mean squared error of the vertical and horizontal frameworks in average. However, this framework suffers high time complexity.

Nguyen et al. [ 74 ] addressed the problem of learning from high-dimensional data streams, where only a small subset of data features are important for learning process. Moreover, in this context, the definition of relevant features is temporary and limited to a certain period of time. Informative features may become irrelevant afterward, and previously insignificant features may become important features. Proposing a definition of feature drifts, i.e., a change in the set of important features, the authors combined this concept with a weighted ensemble classifier to tackle this problem. A multivariate feature selection method [ 66 ] is adapted with a sliding window technique to detect feature drifts. Then, an ensemble learner consists of selected online learners and is constructed with an optimal weighting method. When a gradual drift occurs, classifier members of the ensemble are updated, and theirs weights are also adjusted according to their error rates. When a feature drift occurs, the ensemble replaces an old-fashioned learner with a updated classifier, which is trained with a new set of important features. Experiment results show that the algorithm is effective and efficient with high-dimensional data streams.  X  Adapted One-vs-All Decision Trees (OVA) [ 82 ]: OVA is a recent ensemble method for data streams. It learns k binary CVFDT classifiers, and each classifier is trained to classify instances between a specified class and all remaining classes. To classify a new data object, each classifier is run and the one with the highest confidence is returned. The confident of the classification is set as the proportion of the dominant class at the leaf where the testing object has reached. To archive a high accuracy, OVA aims to construct an ensemble of CVFDT classifiers with a low error correlation and a high diversity. Moreover, OVA quickly adapts to concept drifts as it only needs to update two component classifiers related to the evolving class and can work well with imbalance data streams.  X  Meta-knowledge Ensemble [ 105 ]: The high complexity of ensemble learning limits it to be applicable to many time-critical data stream applications in the real world. Zhang et al. proposed a meta-knowledge ensemble algorithm that selects the best suit classifier for testing data. An Ensemble-tree (E-tree) is constructed to organize base classifiers, each of which occupies a weight and a closed space in the whole data space. Similar to R-tree [ 49 ], the E-tree has three key operations, including search, insertion, and deletion operation. Hence, it is height-balanced and guarantees a logarithmic time complexity for prediction. The insert operationisusedtointegrateanewclassifierintotheensemble.Whenthenumberofclassifiers in a node exceeds a predefined value, the node is split into two nodes with a principle that the covering area of the two nodes should be minimized. The deletion operation removes outdated classifier when E-tree reaches its capacity; the tree may need to be reorganize to guarantee its balance. The search operation is used to classify a testing instance x . Classifiers whose close space contains x are invoked; a weighted voting method is applied to decide a class label for x . 4.7 Overall analysis After presenting many data stream classifiers, we introduce an overall analysis to summarize all related issues and give readers a broader view of data stream classification. Figure 6 shows the relationship between traditional and data stream classifiers. Traditional classifiers are on the left; data stream classifiers are on the right. To categorize data stream classifier, the two schemata, computational approaches and time windows, are in the middle.
Similarly, we observe that data stream classifiers are inherited from traditional classifiers and apply different computational approaches and time windows. For example, VFDT is an extension of the decision trees for data streams. VFDT uses Hoeffding bound to create a tree node when having a sufficient amount of data. It follows the incremental learning approach and landmark window. CVFDT is an enhanced version of VFDT that can adapt to concept drift by constructing alternative trees. Bayes tree is an extension of Bayesian classifier with the two-phase learning approach and landmark window. MC-Tree improves the Bayes tree with multi-label nodes. eGNN is a neural network algorithm that designed to work for data streams. CVM, derived from SVM classifier, follows the two-phase learning approach and fading window. StreamSVM extends CVM by making the radius of minimum enclosing balls flexible. On-Demand, an extension of k -NN classifier, applies the two-phase learning approach and tilted time window. Lazy-Tree is another improved version of k -NN classifier with the two-phase learning approach and sliding window. Many ensemble classifiers are designed to work for data streams. Online Bagging &amp; Boosting are extensions of traditional Bagging &amp; Boosting with incremental learning approach and sliding window. There are many weighted ensemble classifiers with different weighting strategies. For example, aggregated ensemble uses average weighting method (voting), while HEFT-Stream, AWE, OVA, and Ensemble-tree apply weighting method based on the accuracy of classifier members. They follow the incremental learning approach and can be loosely considered of applying sliding window as they typically remove the least accurate clarifier member.

Moreover, we evaluate capabilities of data stream classifiers into the Table 4 . Although many classifiers have been proposed, they are typically unable to satisfy all constraints of data stream mining at the same time, which are discussed in Sect. 2 . Similarly, we propose two levels of concept-drift constraint, concept-drift adaptation, and concept-drift classification. We also assess whether these classifiers can work for high-dimensional data streams. We observe that all classifiers satisfy the bounded memory and single-pass constraints. eGNN, CVM, and StreamSVM cannot response in a real-time manner, as they require much time for training process. HEFT-Stream can distinguish two kinds of concept drifts (gradual drifts and feature drifts) and adapts properly to them. VFDT, CVFDT, AWE, OVA, and Ensemble-tree can work for high-dimensional data streams, as they deploy the decision tree as their primitive classifiers.

When a classifier follows a computational approach and time window, it will have some specific trade-offs, which is discussed in Sect. 2 . Moreover, traditional and data stream classifiers also have common advantages and limitations in Table 5 . For example, decision trees are easy to understand, robust to noise, efficient, and can remove redundant attributes; however, it suffers an over-fitting problem when the tree has many levels, and the classification rules become difficultto interpret. The lazy learner can be implemented easily, butitconsumes much memory and is susceptible to high-dimensional data streams. Ensemble is highly accurate and easy to implement; however, it is mostly based on heuristics and lacks of solid theory. 5 Future research Although significant data stream research has been performed for more than a decade, there are still many research issues that are required further exploration. The most important topics are described follow: 5.1 Dynamic feature selection In high-dimensional data, not all data features (attributes) are important to the learning process. There are three common types of features: (i) irrelevant features, (ii) relevant but redundant features, and (iii) relevant and non-redundant features. The critical task of feature selection techniques is to extract the set of relevant and non-redundant features so that the learning process is more meaningful and faster. In the literature, feature selection techniques can be classified into three categories: filter, wrapper, and embedded models [ 70 ]. The filter model applies an independent measure to evaluate a feature subset; thus, it only relies on the general characteristics of data. The wrapper model runs together with a learning algorithm and uses its performance to evaluate a feature subset. A hybrid model takes advantage of the above two models.

Furthermore, the importance of a feature evolves in data streams and is restricted to a certain period of time. Features that are previously considered as informative may become irrelevant and vice versa; those rejected features may become important features in the future. Thus, dynamic feature selection techniques are required to monitor the evolution of features. Figure 7 illustrates the dynamic nature of key features. Let us suppose that a data stream has three attributes {x,y,z} and two classes: the black and brown dots represent the positive and negative classes, respectively. At timestamp t 1 , the important feature set is {x,y} since data examples are located on the plane {x,y}. Consequently, as the data distribution evolves over time, the key feature set changes to {y,z} at timestamp t 2 .

After investigating many clustering and classification algorithms, we observe that only a few algorithms are able to work with high-dimensional data streams, and they have many limitations. For clustering, HPStream [ 4 ] applies the projected approach and only removes irrelevant features but no redundant features. It requires a predefined value of the average dimensional degree and is just suitable for discovery spherical-shaped clusters. Similarly, incPreDeCon [ 63 ] only takes out irrelevant features and suffers from high complexity. For classification, some classifiers work with high-dimensional data streams as they are simply derived from or deployed decision tree as primitive classifiers [ 35 , 53 , 82 , 98 , 105 ]. In this scenario, they are considered to be deploying a filter model that uses decision tree X  X  infor-mative measures (e.g., information gain) to select relevant features. Hence, these algorithms strictly use decision tree as primitive classifiers and are not able to remove redundant features. HEFT-Stream [ 74 ] is the only ensemble classifier that removes both irrelevant and redundant features and that works with any type of classifier. However, HEFT-Stream still uses the filter model, which is independent of classifier members. Therefore, the problem of dynamic feature selection in data streams is open and requires further exploration. A dimension reduc-tion technique that follows the hybrid model, captures features X  evolution dynamically, and removes both irrelevant and redundant features is much expected. 5.2 Tracking cluster evolution In many applications, it is necessary to keep track of cluster evolution over time so that analysts can gain more valuable insights into the nature of the data streams. For example, in customer relationship management, a company may want to know whether an emerging cluster is a new group of customers or rather a shift of behavior of existing customers. Figure 8 illustrates an example of cluster evolution. At timestamp t 1 , there are four dynamic clusters, C1, C2, C3, and C4. The clusters C1 and C2 are merged into cluster C12 at timestamp t 2 . Meanwhile, the cluster C3 disappears and cluster C5 emerges. Moreover, cluster C4 is shifted to the right.

To address this problem, Spiliopoulou et al. proposed the MONIC algorithm to detect changes in clusters by checking their overlaps [ 89 ]. There are two types of cluster transi-tions: internal transitions (e.g., cluster shrinks, cluster expands, cluster becomes compacter or many clusters, cluster is merged with other cluster, cluster disappears, and cluster emerges). MONIC is independent of any clustering algorithm; however, it is unsuitable for data streams. It requires time to post-process clustering results; thus, it cannot show relationships among clusters in a real-time manner.

C-TREND constructs a temporal cluster graph to capture temporal order among clus-ters [ 1 ]. In this graph, each node represents for a cluster and is labeled with the size of the cluster. Edges connect adjacent node and are labeled with a distance value (similarity) between two nodes. Althought C-TREND is able to visualize and detect trends in multi-attribute temporal data, it suffers from many limitations. First, C-TREND is not suitable for data streams as it requires a predefined number of clusters. C-TREND is also computationally expensive as it deploys hierarchical clustering and constructs a dendrogram as a preprocess-ing step. Moreover, it only considers transitions among clusters while other internal cluster transition is ignored.

TRACDS takes advantage of both MONIC and C-TREND [ 50 ]. It is independent of clustering algorithms and records transitions among clusters into a transition-count matrix. The matrix can be easily converted to a Markov Chain directed graph to learn about temporal relationship between clusters. This information, for example, can be used to predict the cluster a future record will belong to. Unlike MONIC that performs post-process clustering results to detect cluster transitions, TRACDS incrementally constructs the transition-count matrix whenever each data instance arrives and is assigned to a specified cluster. Therefore, TRACDS only needs a lightweight interface and is able to work for data streams.

OPTICS-Stream is one of the first work that attempts to visualize the temporal order of clusters [ 92 ]. Inheriting the OPTICS algorithm [ 14 ], OPTICS-Stream uses micro-cluster structure to build a synopsis of data streams and further orders reachability distance among micro-clusters. It produces a 3-D reachability plot that can be used to visualize the clustering structure and structure changes in data streams. However, this plot fails to provide a clear view of borders among clusters and cannot extract external transitions clusters, such as splitting or merging clusters.

The above research work tracks evolution of clusters; however, they typically ignore the historical clustering that is important to producing better clustering results. Evolutionary clustering that considers this research issue has attracted much research work recently. Its goals is to optimize the trade-off between preserving the faithfulness of current data and preventing dramatic shifts from historical clustering to current clustering. Chakrabarti et al. first introduced the problem and proposed a framework for evolutionary clustering [ 23 ]. Its objective function is as follows: parameter to penalize the important of historical clusters. Most evolutionary clusterings differ from snapshot quality and history cost functions. The authors deployed agglomerative hierarchical clustering and k -means as examples of the framework. In hierarchical clustering, snapshot quality is the quality of all merges performed to create C t ; history cost is the distance between two trees. In k -means clustering, snapshot quality is the sum of distance between each data instance and its center; history cost is the sum of distance between each pairing clusters between time t and ( t  X  1 ) . Extending this idea, Chi et al. proposed two frameworks for evolutionary spectral clustering, called Preserving Cluster Quality (PCQ) and Preserving Cluster Membership (PCM) [ 25 , 26 ]. PCQ penalizes clustering results that disagree with past similarities, while PCM penalizes clustering results that disagree with past clustering results.
Although evolutionary clustering and data stream clustering have a close relationship, they have different objectives. Working with large amounts of high-speed data, data stream clustering much focuses on single-pass and scalability issues. While evolutionary clustering aims to obtain clusters that evolve smoothly over time, most evolutionary clusterings are spectral clustering algorithms that require matrix and graph processing operations; thus, they are computationally expensive. Therefore, tracking cluster evolution in data stream setting while preserving their smooth transitions remains a challenge. Moreover, any clustering algo-rithm should support user-friendly visualization so that users can easily explore transitions among clusters and understand the major reasons that trigger these evolutionary transitions. 5.3 Mining text streams Recently, a wide range of web-related applications have generated massive amounts of text streams. For example, users in social networks continuously communicate with others via text messages; many web portals categorize and provide real-time news according to readers X  interests; and web crawlers harvest millions of webpages for indexing. Mining text streams is also relevant to other important tasks, such as email spam filtering and target marketing for electronic commerce. Readers may find details about traditional text mining in a survey paper of Sebastiani [ 83 ].

In general, high-dimensional stream mining techniques can be generalized to text streams after text data are performed with preprocessing steps, including removing stop-words, stem-ming,mappingtointernalrepresentations(suchasbagsofwords,TF*IDFscheme,andphrase segmentation). However, text data are more complicated than high-dimensional data as it is unstructured, contains a high level of noise, and exists on different formats. Furthermore, text streams are rich with surprising events and complex topic evolution over time so that mining text stream is still a daunting task. 5.3.1 Clustering text streams In the past, there are some efforts on clustering for very large document database. Van Rijsbergen proposed a single-pass clustering algorithm, which requires a single sequential scan over the sentences [ 96 ]. The algorithm is likely to k -mean algorithm [ 56 ] by assigning a sentence to its closest cluster regarding to cosine-similarity measure. Can et al. introduced an cover-coefficient-based clustering method C 3 M for text clustering, which can give a priori knowledge of the number of clusters [ 21 ]. After computing the cover-coefficient matrix for each documents, C 3 M algorithm selects seed documents having highest seed power. Then, it forms clusters through grouping non-seed documents around seed documents. The author later presented an incremental version of the C 3 M algorithm, named C 2 ICM algorithm [ 19 ]. For text stream clustering, Shi Zhong proposed one of the first algorithm, referred as the Online Spherical k -Means algorithm (OSKM) [ 109 ]. Similar to the STREAM algorithm [ 75 ], OSKM divides text streams into small chunks, each of which can be processed effectively in main memory. It then applied a number of k -mean iterations to cluster each chunk. The centroids with counts of previous chunks are used as inputs of the next iteration. Moreover, it applies a fading model so that old documents will be forgotten at an exponential rate. Experimental results show that OSKM can very effective in clustering text streams.
Extending the concept of micro-cluster, Aggarwal et al. [ 6 ]defineda cluster droplet structure to store statistics information of text streams. A cluster droplet is a tuple of (
DF 2 , DF 1 , n ,w( t ), l ) ,where DF 2 maintains the counts of each word pair in the clus-ter, DF 1 maintains the count of each word, n is the number of data points, w( t ) is the decayed weight of data points, and l is the time stamp of the last update. Since two vectors DF 2and DF 1 are huge, the algorithm only maintains two lists of nonzero counts to save spaces. When a document comes, it would be assigned to a suitable cluster, whose cluster droplet is correspondingly updated. In the offline clustering process, the additional property is used to extract a set of droplets within a time range. Then, each droplet is treated as a pseudo point, and the k -means algorithm is performed for clustering purpose. The statistical information of cluster droplets can be used for further analysis, such as higher-level clusters and correlation analysis. 5.3.2 Online event detection Topic detection and tracking (TDT), an important research in text mining, aims to organize documents into ordered stories/topics. Since a topic is defined as  X  X  seminal event or activity with all directly related events and activities X  X , event detection has become an key task in TDT. There is a large number of important research work on event detection, topic tracking, and novelty detection [ 9 , 12 , 20 ]. Online event detection (OED) is more challenging since it is required to detect and trace surprising events and emerging trends over text streams in an online fashion, for example, a sequence of news articles, a sequence of messages or dialogues in social networks. This problem is closely relevant to text stream clustering as the events can be inferred from clusters of related documents.

Most popular OED algorithms are based on the initial work of Yang et al. [ 101 ]. In this work, the authors proposed an agglomerative clustering to extract events in the text corpus. To be able to work with text streams, the algorithm uses an incremental version of inverse document frequency (IDF) and applied an iterative bucketing and re-clustering techniques. It also employs a decaying model to lessen the effect of old documents on the current decision. When a new document arrives, it is compared to all existing clusters. If none of their similarity exceeds a threshold, a new cluster is created, and a new event is triggered. There are several extensions of this approach, for example, using name entities [ 64 ] and reweighing terms [ 102 ] to improve accuracy.

Another research direction is feature-pivot approach. It first aims to identify the representa-tive features of the hidden events; the events later are detected by clustering these presentative features. Fung et al. proposed a parameter-free, probabilistic approach to detect bursty fea-tures. The frequency of each feature is modeled with a binomial distribution. Bursty features are extracted with a threshold-based heuristic [ 37 ]. The bursty events are detected by maxi-mizing the co-occurrences of the bursty features. Similarly, He et al. utilized the Kleinberg X  X  concept of burstiness [ 58 ] to identify bursty features and then applies a standard k -means to construct the clustering [ 52 ]. A bursty feature is defined by a 2-state finite automaton model, where the feature X  X  frequency at a certain time point is greater or equal to ( s &gt; 1 ) times of its average frequency.

Recently, online event detection on social networks (such as Twitter, Facebook, MySpace and various blogging sites) has become a hot research topic. Sakaki et al. [ 81 ] proposed one of the earliest algorithms of online event detection from Twitter posts. They considered each Twitter user as a sensor and applied Kalman filter to estimate the location of earthquakes and typhoons. This approach monitors the burstiness of a predefined set of keywords (such as earthquake, shaking, and typhoon) and applies a support vector machine (SVM) to classify it as a real event or not. Petrovic et al. tracked events on Twitter streams by utilizing locality sensitive hashing (LSH) to limit the search space. To further reduce time complexity, they limited the number of documents inside a single bucket and the maximum number of com-parisons for a new document. Based on cosine measure, similar tweets are grouped together as events.

Aggarwal et al. used a graph-based algorithm that considers both the text context and the network structure for online event detection [ 7 ]. In the graph, a node is a user and an edge corresponds to an activity between two users, for example, sending message. This approach aims to discover highly dense clusters in the graph, each of which is defined by a set of nodes and a set of content-summary words. The similarity between a stream object and a cluster is a linear combination of the structural and content-based similarity values. A stream object is considered as a novel event if it is placed in a newly created cluster. With a different approach, Li et al. applied phrase segmentation for identifying the bursty features and then performed k -nearest-neighbor clustering for event detection. They further exploited Wikipedia as an external information source to identify the realistic events (i.e., phrases that appear as anchor text in Wikipedia articles are more newsworthy than those which do not).

To better understand evolving topics in text streams, Cui et al. [ 31 ] proposed TextFlow, an interactive visual analysis tool for analyzing evolution patterns from multiple topics. The core component of TextFlow is a three-level Directed Acyclic Graph (DAG) whose first level corresponds to the topic flow, the second level encodes keyword bundles (critical events), and the third level represents keyword thread. First, text documents are clustered so that topics represented by document clusters and their connections are extracted. The Hierarchical Dirichlet Processes [ 93 ] are used to trace spitting/merging events between topics. These critical events are likely to involve intense keyword changes as a topic is summarized by keywords. Acknowledging the limitation of stack-based graph [ 36 ] that cannot well-present topic merging and splitting, TextFlow applies a river-flow-based visualization that help users understand and analyze the evolution topics easily. The publication and Bing news datasets are used to demonstrate this framework. 6 Conclusions With the proliferation of advanced data collection systems, data streaming mining has emerged as a new active and exciting area of research, which aims to extract knowledge from massive amounts of continuously generated data. Unlike traditional data mining, data stream mining is a continuous learning process while coping with time and memory limita-tions. Scalability and concept drift adaptation are two key issues of data stream mining. In order to achieve scalable performance, a data stream algorithm must minimize the number of passes over the data while fitting the data synopsis into the main memory. To address concept drifts, data stream algorithms need novel strategies to efficiently detect them at varying time windows.

This survey aims to provide a systematic analysis of the state-of-the-art data stream clus-tering and classification algorithms so as to enable a more informed decision in choosing the best algorithm to cope with today X  X  highly dynamic data streams. Our specific focus on clustering and classification stems from their practical relevance in solving real-world problems.

First, we explain the key differences between traditional data mining and data stream mining. We identify four key constraints: single-pass, real-time response, bounded memory, and concept drift detection and propose a general model for data stream mining. Various computational approaches and their trade-offs are also presented. Next, we discuss in detail various state-of-the-art algorithms data stream clustering and classification. The inheritance of stream algorithms from traditional algorithms is intuitively illustrated so that readers can quickly understand and appreciate their relationships. We also examine their capabilities on satisfying stream mining constraints and analyze their pros and cons.

Finally, we explore future research directions which will continue to push boundaries and enhance the impact of data stream mining.
 References
