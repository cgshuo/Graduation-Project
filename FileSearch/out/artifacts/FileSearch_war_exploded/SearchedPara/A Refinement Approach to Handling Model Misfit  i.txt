 Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents. This problem has been studied in information retrieval, machine learning and data mining. So far, many effective techniques have been proposed. However, most techniques are based on some underlying models and/or assumptions. When the data fits the model well, the classification accuracy will be high. However, when the data does not fit the model well, the classification accuracy can be very low. In this paper, we propose a refinement approach to dealing with this problem of model misfit. We show that we do not need to change the classification technique itself (or its underlying model) to make it more flexible. Instead, we propose to use successive refinements of classification on the training data to correct the model misfit. We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers, the Rocchio classifier and the na X ve Bayesian classifier. These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier. Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically. In particular, our refined model is able to improve the na X ve Bayesian or Rocchio classifier X  X  prediction performance by 45% on average. Text categorization, na X ve Bayesian classifier, Rocchio algorithm With the ever-increasing volume of text data from various online sources, it is an important task to categorize or classify these text documents into manageable and easy to understand categories. Text categorization or classification aims to automatically assign categories or classes to unseen text documents. The task is commonly described as follows: Given a set of labeled training documents of n classes, the system uses this training set to build a classifier, which is then employed to classify new documents into the n classes. The problem has been studied extensively in information retrieval, machine learning and natural language processing. Past research has produced many text classification techniques, e.g., the na X ve Bayesian classifier [27, 29], the Rocchio algorithm [19], and support vector machines [18]. These existing techniques have been used to automatically catalog news articles [26], classify Web pages [8] and learn the reading interests of users [24]. An automatic text classifier can save considerable time and human effort, particularly when aiding human indexers who have already produced a large database of categorized document collection. However, most classification techniques are based on some underlying models and/or assumptions. When the data fits the model well, the classification accuracy can be very high. However, when the underlying model does not fit the data well, the performance of the resulting classifiers can be quite poor. For instance, the na X ve Bayesian classifier assumes that text documents are generated from a mixture model and there is a one-to-one correspondence between the mixture components and the classes. However, this assumption can be seriously violated in many real world applications, e.g., the prevention of junk mails, where junk mails generally contain multiple sub-topics such as adult content and various unrelated business letters. Thus the one-to-one correspondence assumption does not hold. In such cases, the na X ve Bayesian classifier suffers, which is the problem of model misfit. On the other hand, we may explain that the model misfit is due to feature-space heterogeneity . Feature-space heterogeneity [2] occurs when the best features to base the classification on are different in different regions of the feature space. Since most widely used classification techniques, such as the na X ve Bayesian algorithm, rely on measures computed over features of the entire training data to build classifiers, their performances are inevitably affected by an averaging effect over the entire training space. The resulting classifiers are likely to be sub-optimal due to the complex nature of the training data. Serious model misfit often leads to poor classification performance. For unstable classifiers such as decision trees and neural networks, boosting algorithms [12, 31, 35] can be used to enhance the basic weak learners. However, it is generally believed that boosting is not so effective on stable classifiers such as linear classifiers (e.g., the Rocchio algorithm). The na X ve Bayesian classifier is also relatively stable with respect to small changes in training data [37]. Other techniques such as meta-learning can be used to complement and improve individual classifiers by combining multiple classification techniques with relatively complicated mechanisms [5, 6, 23]. In this paper, we present a novel and yet simple method to deal with the problem of model misfit. In the proposed technique, we make use of training errors to successively refine the classification model on the training data. In learning, model misfit generally leads to high training error of classifiers such as the na X ve Bayesian and the Rocchio algorithms. Based on the prediction errors on the training data, we retrain a sub-classifier using the training examples of each predicted class with the same learning method. In this way, we force the classifiers to learn from refined regions in the training data, making the model stronger and fit the training data better. Our technique is very flexible, which only needs one classification method and there is no change to the method in any way. The original classification model is improved by successively splitting the training data and re-learning sub-models. Moreover, when applying our technique to enhance the probabilistic classifiers such as the na X ve Bayesian classifier and linear classifiers such as Rocchio, overfitting is generally not a problem. When the training data is basically consistent with the test data, as the training classification model becomes better, the corresponding classification performance on test instances also improves. We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers, the Rocchio classifier and the na X ve Bayesian classifier. Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically. These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a classifier. The resulting classifiers of our technique are very efficient, much faster than many state-of-the-art approaches, e.g., SVM and Adaboost, while also gaining in prediction performance. Our results show that the refined na X ve Bayesian classifier outperforms the proven superior classifier SVM [41], which has very high memory requirements and needs the data in memory. Furthermore, SVM converges slowly for large data sets [30]. The rest of this paper is organized as follows. Section 2 discusses related work and Section 3 reviews some existing text categorization techniques related to this work. Section 4 describes the proposed technique in detail. We present the experimental results in Section 5, which is followed by some concluding remarks in the last section. Improving prediction accuracy of text classifiers has been an important issue. Many studies have been conducted in this area. One of the popular frameworks is meta-learning or classifier committees [6, 11, 14, 16, 22, 23, 25, 42], which is an integration of multiple learning models to achieve higher accuracy. This is because a combination of different learning approaches can represent an integration of different learning biases that could complement each other on their inefficient characteristics. One pioneer work on applying meta-learning to data mining is done by Littlestone &amp; Warmuth [28]. They proposed several weighted majority algorithms for combining different classifiers. Chan and Stolfo [5, 6] adapted their methods to learn the weights using a validation set. They presented techniques that learn an arbiter to arbitrate among predictions generated by different classifiers, and a combiner to merge the predictions of several classifiers. Similar work in this direction also includes the stacked generation [36, 39], and combining multiple rule sets using Bayesian utility theory [1]. Our technique is different from the meta-learning approach. We simply refine a classifier using the training data. Unlike meta-learning, no voting is involved in our process. We do not require multiple classification techniques. Another popular framework is adaptive resampling [3, 17, 31], which adaptively selects instances from a labelled training set to improve classification accuracy. The selection process biases in favour of misclassified data. In particular, the boosting algorithm [12, 32, 33, 34, 35] adaptively resamples or reweights data biasing towards the misclassified examples in the training set and then combine the predictions of a set of classifiers. During the course of its execution, it assigns different importance weights to different training tuples. A weak learning algorithm takes these weights into consideration, and as the algorithm progresses, training documents that are hard to be classified correctly get incrementally higher weights while documents that are easy to classify get lower weights. This, in effect, forces the weak learning algorithm to concentrate on documents that have been misclassified most often previously. Freund and Schapire X  X  Adaptive Boosting (AdaBoost) algorithm [34, 35] has been reported to be a superior voting method. Several studies have been made on implementing AdaBoost with various classifiers. Elkan [10] provided a framework to apply boosting to the na X ve Bayesian classifier. Recently, Kim [20] presented BayesBoost, which uses the na X ve Bayes classifier as the weak learner for boosting by allowing the boosting algorithm to utilize term frequency information while maintaining probabilistic accurate confidence ratio. However, there is no clear evidence in how much BayesBoost can improve the na X ve Bayes classifier. On the other hand, Ting [37] reported that boosting does not work well for the na X ve Bayesian classifier. However, we will show that our proposed technique can remarkably improve the performance of the na X ve Bayesian classifier. The refined NB classifier is much superior to AdaBoost (with decision stumps) [35]. In terms of using tree strategy to handle the data heterogeneity problem, Apte et. al. [2] introduced the Importance Profile Angle (IPA) to split the feature space. They compute the IPA value for each feature. If the value exceeds a suitable threshold, it is an indication of heterogeneity. Hence, training data are recursively split according to the feature that gives the largest IPA value. Friedman [13] introduced a hybrid approach to classification by combining aspects of both the K-nearest-neighbor and tree-structured recursive partitioning techniques. It consists of two strategies, machete and scythe . The machete is a successive splitting procedure. It begins with the entire input measurement space and divides it into two regions based on one of the input variables (attributes). The splitting criterion is one that maximizes the estimated relevance. The scythe employs an alternative splitting strategy in which the respective variables influence each split in proportion to their estimated relevance, rather than the winner-takes-all approach of the machete. Our method is different, as we do not need sophisticated extra mechanisms to select variables to split the data. Kohavi [21] reported a hybrid system of na X ve Bayesian (NB) and decision tree. The algorithm is similar to the decision tree-building algorithm, except that the leaf nodes created are NB classifiers instead of nodes predicting a single class. It uses a validation set to determine when NB will form a leaf. Our approach differs in the aspect that we do not require a complex combination mechanism and integration of different classification techniques at the algorithm level. We only use the classification results to partition the training data, and apply only a single classification technique. We also do not need a validation set. We review four commonly used text classification methods. We will apply our technique on them or use them as baseline algorithms for comparison in our experiments later. An early text classification technique from information retrieval is the Rocchio algorithm, which was originally designed for relevance feedback. It has also been widely used for document classification [19]. In this algorithm, documents are represented with the popular vector space representation. Building a classification is achieved by constructing document vectors into a prototype vector each class c j . Both the normalized document vectors of the relevant examples for a class as well as those of the irrelevant examples for a class are first summed up. Next, the prototype vector is computed as a weighted difference of each summation.  X  and  X  are parameters that adjust the relative impact of relevant and irrelevant training examples. Buckley et al [4] recommended  X  = 16 and  X  = 4. In classification, for each test document ' d simply use the cosine measure to compute the similarity of ' d with each prototype class vector to ' d The naive Bayesian (NB) method is another effective technique for text classification. It has been shown to perform extremely well in practice by many researchers [9, 15, 27, 29]. Given a set of training documents D , each document is considered an ordered list of words. We use w d position k of document d i , where each word is from the all words we consider for classification. We also have a set of pre-defined classes, C = { c 1 , c 2 , ... , c | C | consider two class classification, so, C = { c 1 , c perform classification, we need to compute the posterior probability P ( c j | d i ), where c j is a class and d on the Bayesian probability and the multinomial model, we have and with Laplacian smoothing, where N ( w t , d i ) is the count of the number of times the word w occurs in document d i and P ( c j | d i )  X  {0,1} depending on the class label of the document. Finally, assuming that the probabilities of words are independent given the class, we obtain the na X ve Bayesian classifier: In the naive Bayesian classifier, the class with the highest P ( c is assigned as the class of the document. Over the recent years, S upport Vector Machine (SVM) has been shown to be an accurate classification met hod for text documents. SVM is a relatively new approach introduced by Vapnik [7, 38] to solving two-class pattern recognition problems. It is based on the Structural Risk Minimization principle for which error-bound analysis has been theoretically motivated. This method is defined over a vector space in which the problem is to find a decision surface that  X  X est X  separates the data vectors into two classes. Joachims [18] provides both theoretical and empirical evidences that SVM is very suitable for text categorization. He compared SVM with other classification methods and showed that SVM outperformed all the other methods tested in his experiments. AdaBoost, proposed by Schapire [34, 35], is a learning algorithm that generates multiple classifiers and uses them to build an ultimate classifier. It is well suited to the text categorization problem. Given an instance x and a set of classifiers (  X  with a weak learner, AdaBoost produces the final classifier F ( x ) through combining the hypotheses of the weak classifiers: where T is the number of iterations and  X  t is the weight for hypothesis  X  t ( x ), which can be calculated by using the equation of the original AdaBoost. Schapire and Singer [35] introduced the BoosTexter system for text classification, which employs AdaBoost on a one-level decision tree (decision stumps) as the weak learner and reported excellent results. In this section, we first describe the framework of our proposed technique, we then analyse how our approach can solve the model misfit problem when using the na X ve Bayesian classifier and the Rocchio classifier. In this work, we consider binary text classification that assigns each document d j either to the positive class C complement negative class C -. Theoretically, binary text classification is more general than the multi-class one and a multi-class classification problem can be transformed into a set of independent binary ones. For a text classification task, given a set D of pre-labelled training examples, we choose a classification technique Cl (e.g., the na X ve Bayesian classifier) as the base classifier of the proposed technique. We begin with the entire training set D , learning an initial classifier Cl 0 from D , and then classify D into positive and negative classes respectively. We thus split D into two subsets, D and D N , consisting of the predicted positive documents and negative documents respectively. The resulting predicted examples generally contain errors, e.g., the predicted positive class is made up of true positive and false positive documents. We examples in the predicted positive class. The sub-classifier Cl could be a refiner of the original classifier Cl documents produced by Cl 0 . We denote their combined classifier as Cl 0+P . The contribution of Cl P can be evaluated by comparing the classification performances of Cl 0 and Cl 0+P data. If Cl 0+P outperforms Cl 0 , we keep the sub-classifier Cl the refiner of the original classifier Cl 0 ; otherwise, we discard Cl A similar refining process is also done on the predicted negative documents produced by Cl 0 and thus can output another refined classifier Cl N . We can recursively carry out the above learning process to build a refinement tree, which is illustrated by Figure 1. The effectiveness of text classifiers is commonly evaluated by (which is computed with r and p ). In our technique, we use F score, as it is often employed in text classification. We also use F score on the training data to evaluate the resulting sub-classifiers (e.g. Cl P , and Cl N ) during the process of building a refinement tree. Let: Note that in this work we are only interested in the positive class. Thus, the recall and the precision are defined on the positive class as: The F-score of the positive class is computed as follows (which gives the equal weight to recall and precision): In the proposed technique, we recursively build a refinement tree from the root node. The root node comprises all example documents in the training set D . At any node, we first build a base classifier Cl 0 by using the selected classification technique on all the training data of this node. Then, according to the predictions of Cl 0 , we split this node into positive and negative children nodes child ( P ) and child ( N ), and compute the F score of the training data of this node with the class assignments of Cl 0 : Next, we build two sub-classifiers, Cl P and Cl N , using the training data of the positive and negative children nodes respectively, and then compute two new F scores by adding the contributions of Cl and Cl N accordingly: This algorithm is a heuristic one, which tries to refine the classification model by improving the F score on the training data. In the resulting refinement tree, the F score of the base classifier of a particular node is always smaller than that of the combined classifier produced by merging the node X  X  base classifier with a child node X  X  classifier. Since the node splitting process will stop when the F measure does not increase, our approach may reach a local maximum rather than a global maximum. The experimental results show that the proposed technique is already extremely effective. We present the algorithm in Figure 2. In this sub-section, we show why the proposed approach is able to deal with the problem of model misfit in the contexts of the na X ve Bayesian classifier and the Rocchio classifier. In devising the Bayesian method for text classification, two assumptions are made: (1) text documents are generated by a mixture model and there is a one-to-one mapping between mixture components and classes; (2) document features are independent given the class. Many researchers have shown that the Bayesian classifier performs surprisingly well in obvious violation of (2). However, (1) often causes difficulty when it does not hold. In many real-life situations, one-to-one correspondence of mixture components and classes does not hold. That is, a class (or category) may cover a number of sub-topics. In the proposed technique, after building the original classifier using the entire training data, we build a sub-classifier on the predicted positive training examples that contain false positive examples (misclassified negative ones) and true positive ones. The predicted positive training examples are similar to each other according to their probabilities in the original model. When we re-train on the predicted positive training examples, we have gotten rid of most of the mixture components in the training data, which have been classified into the negative class. When we perform this process recursively, we can refine the classification and make it closer to one-to-one correspondence of mixture components and classes. Figure 3 illustrates the problem of model misfit when using the negative prototype vectors respectively. The Rocchio classification model is based on the assumption that a given document should be assigned to a particular class if the similarity between this document vector and the prototype vector of the class is the largest. However, when the data does not fit the model well, the Rocchio classifier suffers (see the data points in regions 1 and 2 of Figure 3). Training instances within region 2 will be misclassified as positive documents, while instances within region 1 will be misclassified as negative documents. In the proposed technique, after splitting the entire training data by the initial classifier, we obtain the situation in Figure 4 for the predicted positive class. We can see that it fits the Rocchio model very well. The original region 2 can now be classified correctly using a Rocchio sub-classifier. 
Figure 4: The refined model for the predicted positive class The empirical evaluation is done on two data corpora. First, we will give the dataset descriptions, experimental settings and evaluation measures. Next, we present the empirical results of the experiments on the two datasets. from the Reuters newswire. The  X  X odApte X  split is used, leading to a corpus of 9,603 training documents and 3,299 test documents. Of the 135 potential topic categories, only the most populous 10 are used as positive classes. Binary text classification is performed in the experiments. Lang (1995) from 20 different newsgroups. 1000 articles were collected for each newsgroup. There are four main categories within the newsgroups, namely, Computing (comp) , Recreation (rec) , Science (sci) , and Talk (talk). We perform classification on the sub-categories within them. The detailed data compositions are shown in Table 11 in the appendix. For example, we take graphics category as the positive class and the rest of the comp categories ( os-ms , ibm-pc , mac and x-win ) as the negative class. We withhold the most recently posted articles (40% of the entire dataset) for testing. Two classification techniques, the na X ve Bayesian classifier (NB) and the Rocchio classifier, have been tested in the proposed approach as the base classifier. We also attempted to apply our algorithm on SVM. SVM has been previously proven to be a superior classifier by Yang [41] and fits training data very well in general. In most categories, SVM does not generate substantial training error, therefore our technique fails to refine it much further and the improvement is minimal. Nevertheless, we use SVM as a baseline for comparison. Our technique works well in different feature settings. In particular, the performance of our refined classifiers is more outstanding when fewer features are used. Hence, we select the first 1000 words with the highest information gains as features for our classifiers during refinement. Experiments are conducted on package. The other experiments are conducted using the publicly We use F score (of the positive class) and error as the evaluation measures for our system. F score takes into account of both recall and precision, which makes it a more reliable and suitable measure. Error of classification is the fraction of all documents that are classified wrongly with respect to both classes. It is the ratio of the sum of the numbers of false positives and false negatives to the total number of documents. With these metrics distributed over all the categories, some kind of averaging is needed to get global performance measures. There are essentially two methods to perform the averaging, micro-averaging and macro-averaging . In micro-averaging , the 2 http://www.cs.cmu.edu/afs/cs/projec t/theo-11/ www/naive-bayes/20_newsgroups.tar.gz individual numbers for true/false positives and true/false negatives are summed up and the evaluation measures are computed on them. In macro-averaging , the individual measures (e.g.: F score or error ) for every category are computed and the average is calculated over the total number of categories. Consequently, macro-averaging treats all categories equally, while micro-averaging treats all documents equally. The denominator of error rate is a constant for all the categories, resulting in the same value for macro-averaging and micro-averaging. Table 1 shows the experimental results on the Reuters data set. In Table 1, Columns 2 and 3 contain the error and F score of using the NB classifier on each of the top 10 categories of the Reuters-21578 collection respectively. Columns 4 and 5 show the results of the refined NB classifier with our technique. Column 6 provides the classification improvement (I + ) on the F score of Column 5 over Column 3, which is the improvement percentage of our refined NB classifier over the initial NB classifier. Similarly, Table 2 shows the classification performance of the refined Rocchio classifier. For all these experiments, 1000 top features (or words) are used. According to the results, our algorithm works very well when applied to the NB or Rocchio classifiers. For example, in datasets that NB performs badly (e.g., ship and corn ), our system successfully enhances NB by an overwhelming improvement. The refined classifiers are on average 45% better than the original NB classifiers and 44% better than the original Rocchio classifiers in terms of macro-averaged F score. In terms of micro-averaged F score, the refined classifiers are also 25% and 31% better than the original NB classifiers and the original Rocchio classifiers respectively. In addition, our technique also reduces all the error rates substantially. We observe that the refined classifiers are able to improve on every dataset. Although both refined classifiers show outstanding performances the refined NB classifier gives better results than the refined Rocchio classifier. Hence we use it as our representative technique for the performance comparison in Figure 5. Figure 5 illustrates the comparison of our techniques with several benchmarks in terms of F score. They include AdaBoost (with decision stumps) in BoosTexter, NB and linear SVM. Yang [41] reported that linear SVM provides slightly better results than non-linear models on the Reuters dataset. Thus, we use the linear version as the representative of SVM in our comparison. For this comparison, we use all features for NB and SVM. These classifiers perform better with all the features. From the diagram, it is clear that our refined NB classifiers outperform AdaBoost greatly for all categories. In general, it also performs better than SVM. As for the Rocchio-refiner, it is slightly worse than SVM. In terms of running time, our system is much faster than SVM and AdaBoost, requiring only linear scans and thus incurring significantly less computational cost. For most categories in the Reuters dataset, the optimal results of our algorithm are obtained after reaching 2-3 levels of the refined classification tree. As the training error decreases, the test error also decreases, apparently without overfitting. This behavior is also observed in the Usenet dataset experiments, which demonstrates the flexibility of our technique. As above, Tables 3, 5, 7 and 9 show the refinement results obtained from the NB classifier from the four main newsgroups. Tables 4, 6, 8 and 10 show the corresponding results using Rocchio as the base classifier. For these results, 1000 top features (or words) are used. Figures 6, 7, 8 and 9 give the comparison with the various benchmark techniques (all features are used for NB and SVM). From the experiments, we observe that our technique clearly outperforms all the other methods by a significant margin. BoosTexter did not perform well for the Usenet data set, which is in accordance with the results reported in [35]. SVM also does not perform well in the Usenet data set. One possible reason is that SVM overfits the training data because the training errors were very low for most datasets. However, our refined NB classifier and our refined Rocchio classifier do not seem to suffer from overfitting. Regarding running efficiency, SVM is much slower than our system because of its quadratic optimization. AdaBoost using a large number of rounds of boosting is also time-consuming. Using NB or Rocchio as the base classifier, our refined system requires only linear scans of the data and is on average more than 1000 times faster than SVM using the Rainbow package. money-fx 0.072 0.59 0.018 0.828 40.3 Table 2: Results after refining Rocchio classifier for Reuters Table 3: Results of refining NB classifier for UseNet (comp) Table 4: Results of refining Rocchio classifier for Usenet (comp) Table 5: Results of refining NB classifier for UseNet (rec) motorcycle 0.069 0.88 0.010 0.98 11.4 Table 6: Results of refining Rocchio classifier for UseNet (rec) motorcycle 0.074 0.87 0.013 0.97 11.5 electronics 0.043 0.91 0.030 0.94 3.3 Table 8: Results of refining Rocchio classifier for Usenet (sci) electronics 0.072 0.86 0.067 0.89 3.5 Table 9: Results of refining NB classifier for Usenet (talk) Table 10: Results of refining Rocchio classifier for Usenet (talk) In this paper, we proposed a refinement approach to handle the problem of model misfit evident in some existing text categorization techniques. Our approach successively refines the classification model based on the prediction errors of the training data. Extensive experiments conducted on Reuters-21578 and Usenet newsgroups corpora confirm that our technique could make an inflexible classification model versatile, and outperform the state-of-the-art techniques like SVM and AdaBoost significantly. The results reported here are not necessarily the best that can be achieved. There is still room for improvement by finding better ways to identify the performance characteristics of the base classifier during the initial training phase. Nevertheless, the approach described is already able to achieve outstanding prediction performance without using any complex strategies or incurring significant computational costs. We gratefully acknowledge the support of A-STAR and 
National University of Singapore Academic Research Fund grants R252-000-041-112 and R252-000-041-303. We would also like to thank Wee Sun Lee for many useful discussions. [1]. Ali, K. and Pazzani, M. Error reduction through learning [2]. Apte, C., Hong, S., Hosking, J., Lepre, J., Pednault, E. and [3]. Breiman, L. Bagging predictors. Machine Learning 1996 . [4]. Buckley, C., Salton, G. and Allan, J. The effect of adding [5]. Chan, P. and Stolfo, S. Comparative evaluation of voting [6]. Chan, P. and Stolfo, S. Learning arbiter and combiner [7]. Cortes, C. and Vapnik, V. Support vector networks. [8]. Craven, M., DiPasquo, D., Freitag, D., MaCallum, A., [9]. Duda, R. and Hart, P. Pattern Classification and Scene [10]. Elkan, C. Boosting and naive Bayesian learning. In [11]. Freitag, D. Multistrategy learning for information [12]. Freund, Y. and Schapire, R. Experiments with a new [13]. Friedman, J. Flexible metric nearest neighbor [14]. Guo, Y. and Sutiwaraphun, J. Knowledge probing in [15]. Hand, D. and Yu, K. Idiot X  X  Bayes -Not so Stupid After All? [16]. Hull, D., Pedersen, J. and Schutze, H. Method [17]. Iyengar, V., Apte C. and Zhang, T. Active learning using [18]. Joachims, T. Text categorization with support vector [19]. Joachims, T. A probabilistic analysis of the rochhio [20]. Kim, Y., Hahn, S. and Zhang, B. Text filtering by [21]. Kohavi, R. Scaling up the accuracy of na X ve-Bayes [22]. Kumar, S. A hierarchical multi-classifier system for [23]. Lam, W. and Lai, K. A meta-learning approach for text [24]. Lang, K. Newsweeder: Learning to filter netnews. In [25]. Larkey, L. and Croft, W. Combining classifiers in text [26]. Lewis, D. &amp; Gale, W. A sequential algorithm for training [27]. Lewis, D. &amp; Ringuette, M. A comparison of two learning [28]. Littlestone, N and Warmuth, M. The weighted majority [29]. McCallum, A., &amp; Nigam, K. A comparison of event [30]. Pavlov, D. and Mao, J. Scaling-up Support Vector [31]. Quinlan, J. Bagging, boosting and C4.5. In Proceedings [32]. Schapire, R., Freund, Y. Bartlett, P. and Lee, W. Boosting [33]. Schapire, R., Singer, Y and Singhal, A. Boosting and [34]. Schapire, R. and Singer, Y. Improved boosting algorithms [35]. Schapire, R. and Singer, Y. BoosTexter: A boosting-based [36]. Ting, K. and Witten, I. Stacked generalization: when does [37]. Ting, K. and Zhen, Z. Improving the performance of [38]. Vapnik, V. The Nature of Statistical Learning Theory, [39]. Wolpert, D. Stacked generalization. Neural Networks , [40]. Yang, Y. An evaluation of statistical approaches to text [41]. Yang, Y. and Liu, X. A re-examination of text [42]. Yang, Y., Ault, T. and Pierce, T. Combining multiple 
