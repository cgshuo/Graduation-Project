 We propose a method for predicting the cardinality distri-bution of a multi-dimensional query. Compared to conven-tional  X  X oint-based X  estimates, distribution-based esti mates enable the query optimizer to predict the cost of a query plan more accurately, as we show experimentally. Our method is computationally efficient and works on top of a histogram al-ready in place. It does not store any information additional to the histogram. Our experiments show that the quality of the predictions with the new method is high.
 H.2.4 [ Information Systems ]: Database Management X  Query processing; Relational databases Theory Cardinality Estimation
Accurate query result-size estimation is essential to query optimization. Using such estimates, the optimizer can com-pute the costs of different query plans and choose the better one (with lower cost). When doing this, optimizers typi-cally use the assumption that the query cost is linear against the cardinality. This assumption rarely holds in practice, and can lead to inaccurate cost estimates and bad plan choices. A distribution-based estimate is a probability di s-tribution over possible cardinality values. Previous work [ 5, 1] addresses this problem only partially, focusing on uni-dimensional predicates or on special classes of queries. [4, 3] discusses query optimization without the linear cost as-sumption (coined least-expected cost optimization) in gen-eral terms, but without focusing on how to derive cardinalit y distributions.
 We propose a method, called the Sample-based method, which estimates cardinality distributions for multi-dime nsional queries. The Sample-based method operates on top of a multi-dimensional histogram. We for our part have used the STHoles histogram [2, 6]. It has generic bucket layout which subsumes most other histograms. Our method uses past query execution results to come up with a cardinality distribution. Our experiments show that the Sample-based method offers better cost estimates than the estimates ob-tained using the STHoles histogram and the linear cost as-sumption.
We model cardinality estimates as random variables. For-mally, the randomness is due to compression needed to ob-tain the histogram. Due to the compression we have im-perfect information about the initial data set. A cardinali ty distribution instead of a point estimate is a way to model the uncertainty which comes from such imperfect informa-tion.
 We write card ( q ) to denote the cardinality of the query q . F (  X  ) denotes the cumulative distribution function of a ran-dom variable.
 A query plan  X  has an associated cost function v  X  (  X  ). For a query q , we define the cost of the plan  X  as This is the expected cost according to the random vari-able card ( q ). Histogram buckets and queries are hyper-rectangles in the attribute-value space. The volume of a hyper-rectangle is denoted by vol (  X  ). The cardinality of a region divided by its volume is its selectivity  X  sel (  X  ).
The STHoles histogram stores a tree of non-overlapping buckets. The goal is to obtain a partitioning of the data do-main into regions with close to uniform density. The bound-ing box of a histogram bucket, denoted by box (  X  ), is a hyper-rectangle in the attribute-value space. Each bucket stores the number of tuples in it, excluding child buckets. The volume vol (  X  ) of a bucket is the volume of its rectangular bounding box, excluding the volume occupied by the child nodes.
 The histogram estimates the cardinality of a query, using the uniformity assumption, i.e., it assumes that the tuples are uniformly distributed inside a bucket.
Example 1. In Figure 1 the histogram has three buckets: b is a child of b which is a child of b root . The query q inter-sects with all three buckets. According to (2) the cardinali ty of query q is: n ( q ) = n ( b c ) + n ( b )  X  vol ( q  X  b ) Note that we can write vol ( b  X  q ) because the region covered by b does not include b c . The same holds true with b root
The Sample-based method treats the past query execu-tion results as a sample to approximate the random variable card ( q ). Let X be a random variable; { x 1 , . . . , x ple for X . Then the cumulative distribution function of X can be approximated as follows: where I ( P ) is the  X  X ndicator X  function, it equals 1 if the predicate P is true and 0 otherwise. [7] shows that F m converges to F .

Example 2. Let m = 5, and x 1 = 0 . 4, x 2 = 3, x 3 = 1 . 2, x 4 = 1 . 3 and x 5 = 1 . 9. We now want to estimate the probability that X  X  1 . 2. According to Equation (3), Let b be the bucket which encloses q , and b 1 , . . . , b child buckets of b . We use the already observed selectivities within the region of b to approximate the distribution of selectivities. These are:
We approximate the cumulative distribution of selectiv-ities inside the bucket using { s, sel ( b 1 ) , . . . , sel ( b sample, using Formula (3). The buckets have different vol-umes, thus we weight the  X  X vidence X  X ith the relative volume of the bucket. For the selectivity s this is (1  X  P vol ( b the cardinality distribution becomes:
Example 3. Figure 2 shows a histogram which has 7 buck-ets (the root bucket has the largest bounding box) and a query which intersects with all buckets. Each of the child buckets spans  X  5% of the parent-bucket area. Three buck-ets have selectivity 0, three buckets have selectivity 20. The root bucket has selectivity 5. The probability that the se-lectivity is less than or equal to 7, according to (4), is: The Sample-based method uses only the information which is contained in the underlying histogram  X  it does not incur any additional storage costs. According to (4), the cumu-lative probability at any point can be computed at O ( m ) cost.
Conventional metrics used to evaluate point-estimation methods compare real and estimated cardinalities. This im-plicitly assumes a linear cost model. Instead, we adopt a metric based on the difference of the estimated and the real costs. To assess the quality of an estimator which issues cardinality distribution X , we compare the expected cost according to X to the real cost: If we have two methods which yield distributions X and Y respectively, we compare X and Y . The smaller number indicates that the corresponding distribution is better. We now describe an evaluation of the Sample-based method. We first describe our experimental setup in general terms; in Section 4.2 we describe the technical details; the experimen ts themselves are in Section 4.3.
In the following, the cost of a physical operation is the number of I/O accesses performed. Because the metric in (5) depends on a cost function, we fix it to one that is both non-linear and common. Namely, the cost of a multi-pass hash-based join (HJ) of two relations, both having approximately size M &gt;&gt; B , is [8]: For the evaluation we use a set of queries Q (more in Section 4.2) and calculate the normalized average error of the query-cost estimation for the Sample-based method and the STHoles: where q is given by: The method which produces the smaller normalized aver-age error is better. For the distributions produced by the Sample-based method, the expected cost can be calculated using the following formula: The original estimation method based on STHoles histograms issues point-based estimates. This means that the probabil -ity distribution consists of one value, E [ X ], with probability 1. So, for STHoles E [  X  ( X )] =  X  ( E [ X ]), and we adjust (8) accordingly.
In our experiments, we vary the data sets, the query gen-eration pattern, and the volume of the queries. Below, we describe these in detail. Overall our setup is similar to the one in [2].

We use three data sets, two of them are synthetic, one is based on the U.S. Census Bureau data set. The Census data set contains little over 210,000 tuples. Table 1 provide s the parameters of the distributions used to obtain the syn-thetic data sets. The Array data set draws tuples from a Zipfian distribution. The data set contains 500,000 tuples, there are 50 distinct values per dimension, and the Zipfian skew equals 1. Because we use only 50 distinct values per dimension to generate 500,000 tuples, the Array data set has a lot of duplicate values.
 The Gauss data set is a highly correlated, multi-dimensional real-valued data set. It consists of multi-dimensional, ov er-lapping Gaussian bells with varying number of tuples in each bell. The queries in our workload span a certain percentage of the overall data domain. We vary the distribution of query centers. We write U nif orm to denote uniformly distributed query centers and Data for a distribution that follows the data distribution. Data means that we sample the data set to obtain the query centers. So Gauss [ U nif orm, 1%] means we are using the Gauss data set, and we generate query cen-ters which are distributed uniformly over the data domain, each query spanning 1% of the overall data space. One run of our simulations consists of 1000 queries. We vary the maximal number of buckets in the histogram from 100 to 300. We plot the -metric for the Sample-based method and STHoles. The X -axis is the maximal number of histogram buckets, the Y -axis is the value of the -metric.
Figures 3, 4, 5, 6 show the epsilon -measures for differ-ent settings. Throughout the experiments, the Sample-Figure 3: -measures for the Gauss [ U nif orm, 1%] set-ting Figure 4: -measures for the Array [ U nif orm, 1%] set-ting based method performed better than the STHoles. In Fig-ures 3 and 4 it has been a clear winner over the baseline point-estimation approach (the original STHoles estimati on method). Figure 6 is the only setting where the refernce ap-proach has slightly outperformed the Sample-based method. In all plots there is a slight slope noticeable  X  -measures start higher for 100 histogram buckets and decrease as the number of the buckets increases. This effect is expected, as the histograms usually fire better with more buckets. Summing up, the evaluation of the Sample-based method against a point-estimation method shows a significant im-provement. Figure 5: -measures for the Census [ Data, 1%] setting Figure 6: -measures for the Census [ U nif orm, 0 . 5%] setting
