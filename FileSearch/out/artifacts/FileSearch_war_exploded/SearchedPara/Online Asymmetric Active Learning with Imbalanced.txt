 This paper considers online learning with imbalanced stream-ing data under a query budget, where the act of querying for labels is constrained to a budget limit. We study dif-ferent active querying strategies for classification. In par-ticular, we propose an asymmetric active querying strategy that assigns di ff erent probabilities for query to examples pre-dicted as positive and negative. To corroborate the proposed asymmetric query model, we provide a theoretical analysis on a weighted mistake bound. We conduct extensive eval-uations of the proposed asymmetric active querying strat-egy in comparison with several baseline querying strategies and with previous online learning algorithms for imbalanced data. In particular, we perform two types of evaluations ac-cording to which examples appear as  X  X ositive X /  X  X egative X . In push evaluation only the positive predictions given to the user are taken into account; in push and query evaluation the decision to query is also considered for evaluation. The push and query evaluation strategy is particularly suited for a recommendation setting because the items selected for querying for labels may go to the end-user to enable cus-tomization and personalization. These would not be shown any di ff erently to the end-user compared to recommended content (i.e., the examples predicated as positive). Addi-tionally, given our interest in imbalanced data we measure F -score instead of accuracy that is traditionally considered by online classification algorithms. We also compare the querying strategies on five classification tasks from di ff er-ent domains, and show that the probabilistic query strategy achieves higher F -scores on both types of evaluation than de-terministic strategy, especially when the budget is small, and the asymmetric query model further improves performance. When compared to the state-of-the-art cost-sensitive online learning algorithm under a budget, our online classification algorithm with asymmetric querying achieves a higher F -score on four of the five tasks, especially on the push evalu-ation.
 Online Learning; Query Budget; Imbalanced Data; F-score
Traditional batch classification algorithms that have been broadly applied in various data mining domains, such as document filtering [29], news classification [23], spam detec-tion [2], and opinion mining [25], are challenged by appli-cations characterized by large-scale, streaming data as for instance in web mining applications. Large amounts of data are continually generated by the web such as through social media and news. These datasets are often also highly imbal-anced with respect to classes of interest. A key consequence of scale is that getting adequate training data becomes non trivial in e ff ort and costly. To address these problems, we study online classification algorithms where the act of query-ing for labels is constrained to a budget limit.

Lowering cost is always a goal in algorithms processing large-scale and real-time data. In classification, there are two major sources of cost. The first is the obvious cost incurred due to errors in performance (false positive and false negative decisions). The second is the cost of labeling the data used to initially build or retrain the model over time. Focusing on the second cost for the moment, labelled data may be collected in two general ways with cost di ff er-ences that are both subtle and explicit. We may directly ask the client (end-user) to provide labels. This approach is particularly useful for personalized recommendation [8, 1]. As a consequence, in addition to giving the client the high confidence positive predictions made by the system we also show instances of low confidence to label. While risky these low confidence instances are likely to be the most useful for improving the performance of the classifier. However, over time the client may become disappointed if the system takes big risks with too many false positives shown. Therefore, it makes sense to limit the amount of queries to label sent to the end-user. In this paper, we tackle this issue in an online setting for streaming data. We aim to maximize the per-formance subject to a budget limit for querying the labels. There are several fold of entangled di ffi culties: (i) how to decide which examples to query for the true labels given a budget limit; (ii) which performance measure should we tar-get? (iii) how to evaluate the performance of the system? These di ffi culties become severe in the presence of imbal-anced data. For example, if there are many more negative examples than positive examples, treating them equally for making the query decisions can be sub-optimal. A  X  X ad X  query may even harm performance. For example, Figure 2
Figure 1: Illustration of two evaluation methods. shows the results of two algorithms (the red and blue lines) on the OHSUMED dataset as discussed in the experiment section. The blue line queries the first 3,000 instances for labels then it stops querying. In contrast the red line al-gorithm keeps querying for labels of the examples that are predicted as positive. It shows that although the red line algorithm queries more labels it performs worse. The rea-son will be discussed in the experiment section. Moreover, the traditional performance measure by error rate is not ap-propriate for imbalanced data. Furthermore, the examples for label querying may also be pushed to the end-user in the same way as the recommended items and thus should be also taken into account in evaluation.

The contributions of the paper are three-fold. Firstly, we propose an asymmetric active querying strategy, which is randomized in nature and assigns di ff erent probabilities for querying to examples that are predicted as positive and neg-ative. We also provide a rigorous theoretical analysis of the proposed asymmetric query strategy comparing with previ-ous symmetric query strategy. Secondly, we evaluate the performance of di ff erent algorithms by two methods. In the first performance is assessed both in terms of instances pre-dicted as positives (pushed to client) and instances shown for labeling (querying the client). I.e., both instances that are positive predictions by the classifier and instances se-lected for label querying are shown to the user as positive predictions. This evaluation is particularly suited when the client is the source of the labeling. In the second evaluation, performance is only based on the positive predictions made by the classifier. This traditional evaluation method is more suitable when the queried labels are from another source, e.g., internal workers (or crowd-workers). We refer to the first evaluation as  X  X ush and query X  and the second as  X  X ush X  evaluation. These two evaluations are in essence based upon what the user  X  X ees X , and are illustrated in Figure 1. In our experiments, we find that the proposed algorithm per-forms well on both evaluations. Lastly, we evaluate the performance by F-1 score, which is more suited for imbal-anced data. Most existing studies of online classification or learning mainly minimize error rate (or maximize accuracy). Lower error rate is always good. However, we are interested in working with highly imbalanced data where error rate is not meaningful. If 90% of the examples are negative, simply classifying all of the dataset as negative will give an error rate of 0 . 1, which looks excellent but is actually meaningless. Therefore, we choose to use the F score as the performance metrics. In the paper, we will also show that the proposed asymmetric querying method also favors the F score.
The remainder of the paper is organized as follows. We first discuss the related work of online learning and online ac-tive classification in Section 2. Then we describe and analyze the algorithms in Section 3, including four algorithms with distinct query strategies and the variant with asymmetric query on positive and negative examples. The experimental design and results are discussed in Section 4 and the last section will be the discussion and conclusions. Many online algorithms have been developed, e.g., the Perceptron algorithm [27], the exponentially weighted av-erage algorithm [3, 22] and the online gradient descent [35, 16]. In the last ten years, we observe substantial applications of these algorithms in machine learning and data analytics, e.g., online classification [21, 15, 10, 19, 9]. Traditional on-line classification aims to minimize the number of mistakes by minimizing a cumulative convex loss function. However, most of the algorithms have innocently ignored both the im-balanced data distribution and the query budget constraint.
Learning with imbalanced data has attracted much atten-tion from the machine learning and data mining community for many years. Most studies cast the problem into cost-sensitive learning that assigns di ff erent costs to mistakes of di ff erent classes [32, 14, 24, 28]. Batch learning with cost-sensitivity has been studied a lot, while few studies are de-voted to online learning with imbalanced data [9, 31, 34]. These few studies either modify conventional loss functions to incorporate the given/defined cost matrix or use a dif-ferent loss function to optimize a measure that is suited for imbalanced data. However, they also assume full knowledge of the label information for all received examples. This ren-ders them unattractive for mining massive streaming data where querying for the labels is subject to a cost.
Online learning under a query budget has received lit-tle attention. Several papers have studied a similar prob-lem in a context also known as label-e ffi cient online learning or online active learning. Cesa-Bianchi, Lugosi, and Stoltz [6] studied the problem of learning from expert advice un-der a query budget. They proposed a simple strategy that uses an independent and identically distributed (i.i.d.) se-quence Z 1 ,...,Z T of Bernoulli random variables such that Pr( Z t =1)=  X  and asks the label to be revealed whenever Z t = 1. The limitation with the pure random query strategy is that it does not di ff erentiate between examples with high confidence score and low confidence score for classification. Later on, the same authors [7] designed a new strategy of query that makes the probability of querying dependent on the absolute value of the prediction score. This query algo-rithm has also been used in a recent work for cost-sensitive online learning [33]. Active learning for querying the labels has also been considered in di ff erent works for di ff erent al-gorithms from the perspective of sample complexity [11, 12, 4]. [11] analyzed the perceptron-like algorithm under an ac-tive setting and provided a complexity on the number of queried labels for achieving a certain generalization perfor-mance. [4, 12] studied online ridge-regression type of algo-rithms for classification for di ff erent querying strategies and established the sample complexity bound. However, these works have not consider the asymmetry between positive examples and negative examples for imbalanced data.
One of our goals in this paper is to compare di ff erent query strategies for learning with imbalanced data. Moreover, we note that the symmetric query model where the probabil-ity of querying is independent of the positive/negative de-cision is not well suited for imbalanced data. To under-stand this, consider that the number of positive examples is much smaller than the number of negative examples. As a result, there would be more false positives than false nega-tives. If we assign equal probabilities to these false predicted examples for querying the label, the algorithm would favor the negative class more than the positive class, consequen-tially harm the performance. Therefore, we propose a novel asymmetric query model that is demonstrated to be sound in theory and e ff ective in practice as well. Moreover, the comparison of two di ff erent strategies to handle imbalanced data under a query budget, namely (i) an asymmetric query model plus a symmetric updating rule of the proposed al-gorithm, and (ii) an asymmetric updating rule plus a sym-metric query model of a state-of-the-art algorithm [33], also demonstrate the proposed algorithm is very useful.
We first introduce some notations. We denote by x t  X  R d the feature vector of the example received at the t -th round, and by y t  X  { 1 ,  X  1 } the label of x t .Let f ( x )denotea prediction function. In the sequel, we will focus on the pre-sentation using the linear model f ( x )= w  X  x ,whileonecan easily generalize it to a non-linear function in a reproducing kernel Hilbert space. Let B&gt; 0denoteabudgetlimiton the number of queries.

The framework of online classification under a query bud-get is presented in Algorithm 1. We let w t and B t denote the model available before the ( t +1)-th round and the bud-get used before the ( t +1)-thround. Initially, themodel is w 0 =0and B 0 =0. Inline5,thealgorithmcomputes p = w  X  t  X  1 x t and in line 6 it makes a decision about the bi-nary label of x t by a function  X  y t = Predict ( p t )basedon p The simplest Predict function is  X  y t = sign ( p t ), i.e., using the sign of p t to determine the label. More generally, we can use a threshold  X  and and let  X  y t = sign ( p t  X   X  ). For imbalanced data, we observe that using a threshold always yields better performance. We discuss how to set the value of  X  in the experiment section.

After the binary decision is made, the algorithm enters the query stage, where it decides whether to query the label and update the model. When the algorithm reaches the budget limit, the model will not be updated because query is not allowed and there are two options for making future predictions. The Option I is to use the last updated model and the Option II is to use the averaged model before the iteration T B when the budget is used up. Many previous studies have found that the averaged model might give more robust predictions than the last model [30].

If the remaining budget is not zero, i.e., B t  X  1 &lt;B ,we use the output of a query function Z t = Query ( p t ), which might depend on p t and some other parameters, to deter-mine query ( Z t =1)ornot( Z t =0). If Z t is 1, then the algorithm queries the label of current example denoted by y  X  { 1 ,  X  1 } from a source. Then the algorithm proceeds to update the model using w t = Upate ( w t  X  1 ,y t , x t )inline13. Function Upate depends on what optimization method is employed and what surrogate loss function is assumed. In-deed, many di ff erent updating schemes can be used, includ-ing the margin-based updating rules (e.g., Perceptron [27], online passive-aggressive algorithm [9], confidence-weighted learning algorithm [13], etc) and online gradient descent up-dates for di ff erent types of loss functions [35]. Take the Perceptron for example, w t is updated by where I ( v )isanidentityfunctionthatoutputs1if v is true and otherwise outputs zero.

The query model is the key concern in this paper. Be-low we first discuss some baseline models that either arise straightforwardly or appear in previous work. Then we present the proposed query model for imbalanced data.
We discuss several baseline query models below and com-ment on their deficiencies. Algorithm 1 Online Binary Classification Under A Query Budget 1: Input: budget B 2: Initialize w 0 =0, B 0 =0 3: for t =1 ,...,T do 4: Receive an example x t 5: Compute p t = w  X  t  X  1 x t 6: Make a push decision  X  y t = Predict ( p t )  X  { 1 ,  X  1 } 7: if B t  X  1  X  B then 8: Set w t as Option I: w 9: else 10: Compute a variable Z t = Query ( p t )  X  { 1 , 0 } 11: if Z t =1 then 12: Query the true label y t from a source S 13: Update the model 14: Update B t = B t  X  1 +1 15: end if 16: end if 17: end for Comparing the above query models, we can see that Q F and Q R are prediction independent, and therefore will waste many budget on those easy examples with the model intact. Moreover, if the data is imbalanced and the first B examples are negative, then the model learned by using Q F will pre-dict all the following examples to be negative. Similarly, the Q
R model will also query more negative examples. In con-trast, Q D and Q S are prediction dependent and therefore will likely query more uncertain examples facilitating the learning of the model w t .Q S uses randomization in query that tends to be more robust. More importantly, it has been analyzed theoretically in [5] about the mistake bound. To facilitate the comparison between the symmetric query model and the proposed asymmetric query model in subsec-tion 3.2, we present the mistake bound of Algorithm 1 using the symmetric query model Q S in the following theorem. Theorem 1. Let T B be the smallest number that B T B  X  1 = B . If we run Algorithm 1 using Eqn. (3) as the query model, then for all u  X  R d and for all  X  &gt; 0 ,theexpectednumber of mistakes up to T B satisfies where  X   X  ( z )=max(0 ,  X   X  z ) is a hinge loss parameterized by amarginparameter  X  &gt; 0 ,and  X  = c + R 2 / 2  X  with R being the upper bound of data norm, i.e., max t  X  x t  X  2  X  R .
Remark: Since the upper bound holds for any u ,we can minimize the upper bound by choosing the best u .Note that we only establish the number of mistake bound up to T
B since the model will keep the same after that and its performance is determined by examples received before iter-ation T B .Theproofcanbefoundin[5].Forcompleteness, we include a proof in the appendix.
The issue of the randomized symmetric query model is to treat the positive examples and the negative examples equally. For imbalanced data, this will be vulnerable to the majority class (e.g., the negative class). If the negative class is the majority class, positive examples will be more likely to be predicted as negative. Therefore, intuitively for the sake of learning the model, it is better to query more false negative examples than false positive examples, i.e., making the query asymmetric. To quantify this, we propose the following asymmetric query model, referred to as Q A : We establish below the weighted mistake bound of Algo-rithm 1 using the asymmetric query model. The proof is deferred to the supplement due to the limits of space. Theorem 2. Let T B be the smallest number that B T B  X  1 = B . If we run Algorithm 1 using Eqn. (4) as the query model, then for all u  X  R d and for all  X  + ,  X   X  &gt; 0 ,theexpected weighted number of mistakes up to T B satisfies
E  X   X  bound of data norm, i.e., max t  X  x t  X  2  X  R .
 Remark: Compared to Theorem 1, there are two key dif-ferences: (i) Theorem 2 is bounding the weighted number of mistakes, where the false negative is weighted by c  X  and false positive is weighted by c + ;(ii)themistakeboundin Theorem 2 is compared to the optimal loss that is defined using di ff erent margin  X  + and  X   X  for positive and negative examples, respectively. It is these di ff erences that render the flexibility of Algorithm 1 in balancing between false negative and false positive for imbalanced data. Hence, it achieves the similar a ff ect as using di ff erent costs for false negative and false positive, which has been widely adopted in previ-ous studies on learning from imbalanced data. In particular, if the negative class is the dominant class, then it is ex-pected that c  X  should be set to a larger value than c + .This phenomenon has been observed in our experiments, which validates the result in Theorem 2.
 Proof. We denote by  X  y t = sign ( p t )andintroducethe Bernoulli random variable M t = I  X  y t  X  = y t .Considernowa round t where the algorithms queries a label and makes a mistake, i.e., M t Z t =1. Weconsidertwoscenarios. Firstif p  X  0, then we have for any u  X  R d and  X   X  &gt; 0, where we use the fact w t = w t  X  1 + y t x t for M t Z t y and reorganize the inequality we have We note that the above inequality holds for all rounds such that  X  y t =1. If M t Z t =0,then w t = w t  X  1 ,andtheabove inequality holds because  X  X   X  + ( y t u  X  x t )  X  0.
Similarly, if p t &lt; 0, then for any u  X  R d and  X   X  &gt; 0, The above inequality holds for all rounds such that  X  y t Summing the above inequality over t =1 ,...,T B ,wehave In the first inequality, on the right hand side, we use the summation over y t =1and y t =  X  1. The inequality holds because when M t Z t =1, X  y t =1indicates y t =  X  1, and  X  y =  X  1indicates y t =1. Then, !  X   X  R / 2and  X  X   X   X  c  X  + R 2 / 2, then have !  X   X  By taking expectation over randomness in Z t and noting that E[ Z t ]= c + c  X  y =  X  1, we have
E  X   X  i.e.,
E " From the result in Theorem 2, we can see the proposed asym-metric query strategy aims to minimize the cost-sensitive error. Next, we leverage the previous results to show that minimizing the cost-sensitive error with appropriate costs is equivalent to the F-measure maximization. To present the results, we first give some notations. Let h ( x )  X  H : R d { 1 ,  X  1 } denote a classifier and e ( h )=( e 1 ( h ) ,e the false negative (FN) error and false positive (FP) error of h ( x )onthepopulationlevel,respectively,i.e., where Pr(  X  )denotestheprobabilityover( x ,y ). When it is clear from the context, we write e = e ( h )forshort. Let P 1 denote the marginal probability of the positive class, i.e., P 1 =Pr( y =1).ThentheF-measure(i.e.,F-1score)of h (  X  ) on the population level can by computed by [26] Let c ( t )=(1  X   X  2 ,  X  2 )  X  .Thefollowingpropositionexhibits that maximizing the F-measure is equivalent to minimizing acost-sensitiveerror.
 Proposition 1. (Proposition 4 [26]) Let F  X  =max e F ( e ) . Then we have e  X  =argmin e c ( F  X  )  X  e  X  F ( e  X  )= F  X  The above proposition indicates that one can optimize the following cost-sensitive error to obtain an optimal classifier h  X  ( x ), which will give the optimal F-measure, i.e., F ( h  X  )= F  X  .However,thecost-sensitive error in (5) requires knowing the exact value of the optimal F-measure. To address this issue, we discretize (0 , 1) to have a set of evenly distributed values {  X  1 ,...,  X  such that  X  j +1  X   X  j =  X  0 / 2. Then we can solve for a series of K classifiers to minimize the cost-senstive error h The following proposition shows that there exists one classi-fier among { h  X  j ,  X  X  X  ,h  X  K } that can achieve a close-to-optimal F-measure as long as  X  0 is small enough.

Proposition 2. Let {  X  1 ,...,  X  K } be a set of values evenly distributed in (0 , 1) such that  X  j +1  X   X  j =  X  0 / 2 .Thenthere exists h  X  j  X  { h  X  j ,  X  X  X  ,h  X  K } such that where B =max e  X  e  X  2 .
 Remark: The above proposition is a corollary of Proposi-tion 5 in [26]. Note that the cost-sensitive error in (6) is just the population level counterpart of the cost-sensitive error in Theorem 2, which further justifies the proposed asymmetric query model.

The above analysis implies that we can try di ff erent values for the costs associated with the false negative error and false positive error, and use the cross-validation approach to choose the best setting.
In order to investigate the algorithms on data of di ff erent types, dimensionality, and proportion of positive examples, we conduct the experiment on 5 binary classification tasks from 3 datasets. All datasets are split as 2:1 for validation and testing respectively, with more information listed in Ta-ble 1. The validation data is used to tune the parameters in the compared algorithms. The testing data is used to evaluate the performance of di ff erent algorithms. On each collection, we evaluate the performance mainly through F 1 score across the number of received examples. Also, we pro-vide both  X  X uery evaluation X  and  X  X uery and push evalua-tion X  according to the potential two types of label sources.
One collection is cover type dataset from the UCI repos-itory of Machine Learning databases. It contains 581,012 examples and 7 classes of forest type, namely, Spruce-Fir, Lodgepole Pine, Ponderosa Pine, Cottonwood/Willow, As-pen, Douglas-fir, and Krummholz. Since the 7 classes have distinct rates of positive examples, we conduct the exper-iment on three of them with di ff erent level of imbalance to investigate the performance of algorithms. The three binary classification problems are Ponderosa Pine vs Non Ponderosa Pine, referred to as  X  X ov1 X , Spruce-Fir vs Non Spruce-Fir, referred to as  X  X ov2 X , and Lodgepole Pine vs None Lodgepole Pine, referred to as  X  X ov3 X . From cov1, cov2 to cov3, the level of imbalance decreases. Detailed in-formation about the three tasks can be seen in Table 1. Table 2: The best parameter values of di ff erent al-gorithms using PE
We also evaluate our algorithm on two more datasets from di ff erent domains, OHSUMED -a dataset of biomedi-cal publications, and 2 days X  tweets collected from Twitter. OHSUMED [17] is a well-known dataset, collecting 348, 543 medical documents from MEDLINE from the year 1987 to 1991. Each document consist of all or some of following fields: MEDLINE identifier, MeSH terms, title, publication type, abstract, author, and source. Each document is asso-ciated with one or more MeSH terms, the medical subject heading assigned by human. Since the MeSH term is orga-nized in a tree structure, we pick a subtree rooted at the MeSH term  X  X eart Disease X  to be the positive class, and leave all the terms not in the subtree as negative. Specifi-cally, a document is a positive example if and only if it con-tains at least one MeSH term in the X  X eart Disease X  X ubtree. The task is referred to as hd.

The two days X  tweets, referred as 2days in the following discussion, is a collection related to life satisfaction of the author of the tweets. It is collected by keywords such as  X  X  X ,  X  X y X , etc. And each tweet is manually labeled as satisfy, dissatisfy, or irrelevant. We consider the binary classification problem of  X  X elated to the topic of life satisfaction X  (positive example) or not relevant (negative example). Due to the nature of the data (e.g., tweet is a short text). this is a di ffi cult task.
As we mentioned in the introduction section, we mainly evaluate the algorithms by F 1 score. Specifically, we plot the accumulative F 1 score along with the increase of the iteration. We also conduct two types of evaluation accord-ing to two types of label source. A push evaluation (PE) means that the labeling is independent from the use of the application, while push and query evaluation (PQE) means the labels are completely obtained from the user feedback. Specifically, in PE, only the examples such that  X  y t =1are counted as positive predication, and in PQE the queried ex-amples are also considered as positive predication. Thus, PQE may lead to a lower precision and higher recall, and it is related to the budget and data as well. We compare the proposed online classification algorithm with the asymmetric query model Q A to (i) di ff erent query strategies as discussed in subsection 3.1; and (ii) a state-of-the-art cost-sensitive online active learning algorithm under a query budget, re-ferred to as CSOAL [33]. Di ff erent from our algorithm Q
A ,CSOALusesanasymmetricupdatingruleinsteadof an asymmetric query model. In experiments for the pro-posed framework (with di ff erent querying strategies), we use the Perceptron update in (1) for updating the model. The comparison between Q A ,CSOALand Q S can help demon-strate which method is more e ff ective in the context of online learning with imbalanced data under a query budget. For all randomized algorithms, we repeat 10 times and show the average and the error bar in all figures. For the presented algorithms with di ff erent query strategies, we use the same option I in Algorithm 1.
As mentioned above, each collection is split into a valida-tion and a testing subset. To be fair, we tune the parameters on the validation set for all the algorithms. Due to lim-its of space, we only report the best parameter values using PE in Table 2. For the three tasks on the covtype data, we fix the budget to B =1250andmainlyinvestigatehowthe performance changes as the ratio of the number of positive examples change. For hd and 2days, we try two di ff erent values for the budget, a lower budget and a higher budget.
We show the results on covtype data in Figure 3 that com-pares di ff erent query strategies and Figure 4 that compares Q
A to CSOAL. We only report the results with a lower bud-get value B =1 , 250, and the results with a larger budget value B =5000areincludedinthesupplementarymaterial due to limits of space. The results on hd are shown in Fig-ure 5 for two di ff erent values of budget. Figure 6 shows the results on 2days data for two di ff erent values of budget.
We first discuss the best parameter values of the proposed algorithm Q A .ItcanbeseenfromTable2thevalueof c  X  is larger than c + on imbalanced data (cov1, cov2, hd, and 2days), and they are the same for the balanced data cov3. This is consistent with our theoretical findings.
The comparison between di ff erent query strategies on cov-type data (Figure 3) clearly demonstrate the e ff ectiveness of the proposed asymmetric query model. In particular, when the data is imbalanced (cov1, cov2), the asymmetric query model is better than other baseline query models. When the data is balanced, the asymmetric query model reduces to the symmetric query model. Therefore, the line of Q S overlaps with the line of Q A in Figure 3(e), (f) and the line of Q Figure 3(e) as well. Moreover in this case, the probabilistic query model does not have any advantage over deterministic query strategy. On 2days data, we have similar observations. On hd data, the comparison between di ff erent query strate-gies (Figure 5 (a)  X  (d)) shows that using a smaller query budget favors the asymmetric query model more than using alargervalueofbudget. Fromtheresults,wealsoobserve that using PE favors Q A more than using PQE.

Next, we discuss the comparison to CSOAL. On cover type dataset, Q A performs better than CSOAL on all the classes and both evaluations. On the hd data, the CSOAL performs better when B =5 , 000 especially on PQE, however, it loses to Q A when the budget is low ( B = 100). On 2days data, Q A performs consistently better than CSOAL.

Finally, it is worth mentioning that when only querying those examples predicted as positive the algorithm performs extremely bad as seen in Figure 2. We conjecture that the reason is that many of the positive predictions are di ffi cult, i.e. their features look like positive examples but they are actually negative. There can be many such examples as the dataset is imbalanced. And querying those examples will push the decision boundary to that side that includes fewer positive examples and consequentially harm performance. Therefore, using a probabilistic query model with a smaller value of c + will reduce querying for such examples, which is consistent with observation both in theory and practice.
In this paper, we have considered online classification with imbalanced data under a query budget. We compare and in-vestigate di ff erent query strategies in an online classification algorithm. We also propose a novel asymmetric query model and provide a theoretical analysis of the weighted mistake bound. We conducted extensive experiments on five clas-sification tasks from three real datasets. The experimental results demonstrate the usefulness of the proposed online classification algorithm with an asymmetric query model.
This work is supported by the National Science Founda-tion under grant 1545995. T. Yang was also partially sup-ported by NSF (1463988). Any opinions, findings, and con-clusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the National Science Foundation. [1] X. Amatriain. Mining large streams of user data for [2] C. Castillo, D. Donato, A. Gionis, V. Murdock, and [3] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. [4] N. Cesa-Bianchi, C. Gentile, and F. Orabona. Robust [5] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, [6] N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Minimizing [7] N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Minimizing [8] W. Chu and S.-T. Park. Personalized recommendation [9] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [10] K. Crammer and Y. Singer. Ultraconservative online [11] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis [12] O. Dekel, C. Gentile, and K. Sridharan. Selective [13] M. Dredze, K. Crammer, and F. Pereira.
 [14] C. Elkan. The foundations of cost-sensitive learning. [15] C. Gentile. A new approximate maximal margin [16] E. Hazan, A. Agarwal, and S. Kale. Logarithmic [17] W. Hersh, C. Buckley, T. Leone, and D. Hickam. [18] S. Huang, R. Jin, and Z. Zhou. Active learning by [19] J. Kivinen, A. Smola, and R. Williamson. Online [20] J. Langford and T. Zhang. The epoch-greedy [21] Y. Li and P. M. Long. The relaxed online maximum [22] N. Littlestone and M. K. Warmuth. The weighted [23] J. Liu, P. Dolan, and E. R. Pedersen. Personalized [24] H. Masnadi-Shirazi and N. Vasconcelos. Risk [25] A. Pak and P. Paroubek. Twitter as a corpus for [26] S. Puthiya Parambath, N. Usunier, and [27] F. Rosenblatt. The perceptron: A probabilistic model [28] C. Scott. Surrogate losses and regret bounds for [29] F. Sebastiani. Machine learning in automated text [30] O. Shamir and T. Zhang. Stochastic gradient descent [31] J. Wang, P. Zhao, and S. C. H. Hoi. Cost-sensitive [32] G. M. Weiss, K. McCarthy, and B. Zabar.
 [33] P. Zhao and S. C. H. Hoi. Cost-sensitive online active [34] P. Zhao, S. C. H. Hoi, R. Jin, and T. Yang. Online auc [35] M. Zinkevich. Online convex programming and Proof. We denote by  X  y t = sign ( p t )andintroducethe Bernoulli random variable M t = I  X  y t  X  = y t .Consideraround t where the algorithms queries a label and makes a mistake. Then Z t =1and M t =1. Thenwehaveforany u  X  R d where we use the fact w t = w t  X  1 + y t x t for Z t =1and M t =1. Since y t  X  = X  y t ,then y t w  X  t  X  1 x t  X  0. Replacing u by  X  u with  X  &gt; 0andreorganizetheinequalitywehave We note that the above inequality holds for all t =1 ,...,T . If M t Z t =0,then w t = w t  X  1 ,andtheaboveinequality holds because  X  X   X  ( y t u  X  x t )  X  0. Summing the above in-equality over t =1 ,...,T B ,wehave ! + 1  X   X  Therefore ! By taking expectation over randomness in Z t and noting
