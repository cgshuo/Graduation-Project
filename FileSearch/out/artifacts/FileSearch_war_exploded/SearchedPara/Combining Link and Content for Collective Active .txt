 In this paper, we study a novel problem Collective Active Learning , in which we aim to select a batch set of  X  X nfor-mative X  instances from a networking data set to query the user in order to improve the accuracy of the learned clas-sification model. We perform a theoretical investigation of the problem and present three criteria (i.e., minimum re-dundancy, maximum uncertainty and maximum impact) to quantify the informativeness of a set of selected instances. We define an objective function based on the three criteria and present an efficient algorithm to optimize the objective function with a bounded approximation rate. Experimental results on a real-world data sets demonstrate the effective-ness of our proposed approach.
 H.3.3 [ Information Search and Retrieval ]: Text Mining; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation collective active learning, link, document classification
Machine learning algorithms suffer from insufficiently la-beled training data. The goal of active learning is, as usual to construct an accurate classifier, but also to minimize the number of labeled instances by actively selecting a few num-ber of instances to query the user. Traditionally, this prob-lem is usually addressed in a single mode, i.e., the active learning algorithm queries the user k times, with each time querying one instance for its label. Following this thread, considerable research has been conducted on how to select the best example to query in each time [7, 11].

Recently, there has seen a new direction of machine learn-ing field, that is how to learn an accurate model to clas-sify networking/graphical data, e.g., the linked Web pages. Quite a few models have been proposed such as Conditional Random Fields [2], Continuous Bayesian network [6], Collec-tive Learning [1], and Semi-supervised Learning over graphs [10]. A few works also try to combine the networking in-formation under single mode active learning framework [3, 8, 11]. However, two important issues have been largely ignored by most existing works. First, almost all learning algorithms for the networking data are computationally in-tensive. Suppose a machine needs to query the user k times, when the user inputs a label for the queried instance, she/he may have to wait for a very long time for the next query, which is obviously undesirable. Second, these methods are in a single mode, the selected instances in different iterations may have a undesirable information overlap.

Ideally, we hope that an algorithm can actively select a set of instances with minimal redundancy to query the users in a batch mode, which we refer to as the col lective active learning (CAL) problem for networking data. The problem posts several unique challenges. First, as the optimization problem of selecting the most informative instances is NP-hard, it is unclear how to formulate the problem in a princi-pled framework. Second, to design criteria to quantitatively measure the informativeness of the data is not an easy task. Third, the active learning algorithm should be efficient, in particular considering the rapidly increasing scale of the net-workingdataontheWeb.

To this end, we formally formulate the problem and pro-pose a general framework for collective active learning. Specif-ically, we propose three criteria to respectively capture the maximum uncertainty, maximum impact, and minimum re-dundancy (which will be explained in section 3.1). We design an objective function based on the criteria and further pro-pose an efficient algorithm to solve the objective function. A theoretical analysis for the approximation rate of the algo-rithm is presented. We conduct experiments on a real-world data set to validate the effectiveness and efficiency of the proposed approach. Experimental results show that our ap-proach clearly outperforms (+6%) the baseline methods of single mode active learning and batch mode active learning on linked data sets.
The collective active learning problem can be defined as follows: given an (un)directed graph G =( V,E ), where V indicates a set of data points and E  X  V  X  V represents a set of edges between the data points. For example, in a so-cial network, the edge can represent the friendship between users; while in the citation network, the edge presents the citation relationship between papers. Suppose there are n unlabeled data U = { x 1 , x 2 ,  X  X  X  , x n } ,where U X  V and l labeled data L = { ( x n +1 ,y n +1 ) ,  X  X  X  , ( x n + l L X  V . In most circumstances, we have l n . Further let x i denote the observation vector, e.g., it can be the feature vector in most applications. Without loss of the general-ity, we associate each data point with a binary classification label, y i  X  X  0 , 1 } .

A general classification problem is to learn a mapping function f from the labeled data points L to predict the labels of the unlabeled data points U . However, as label-ing is always tedious and time consuming, the labeled data points are usually insufficient. The problem of collective ac-tive learning is to select k n unlabeled data points, i.e., S  X  X  with | S | = k , to query their labels, in order to improve the prediction accuracy of the learned function f . The goal is to maximize the improvement on the accuracy by query-ing the k data points. Formally, we can define the following objective function: And the goal is to select S to maximize the function Q :
In this way, the collective active learning problem can be also considered as a set function optimization problem. The following task is how to instantiate the objective function Q ( S ) and how to efficiently solve the function Q ( S ). Three Criteria In our collective active learning problem, one key challenge is how to measure the informativeness of a set of selected instances. In this work, we propose three criteria to measure the informativeness of instances.
Based on the defined criteria, we give an instantiation of the objective function. This is just a possible meethod, but not the only way to instantiate the objective function. Basically, the objective function is defined as a linear com-bination of the two terms, i.e., C ( S )and H ( S ): where H ( S ) corresponds to the maximum uncertainty and C ( S ) corresponds to the maximum impact.
 Maximum Uncertainty We use entropy to measure the uncertainty of selected samples. Joint entropy is very hard to compute, so we use the summation of entropies over single data points. In summary, the maximum uncertainty part is defined as the H ( S ) function in Q ( S ):
H ( S )= Maximum Impact The motivation of our maximum im-pact measurement comes from the classical nearest neigh-borhood classifier. The classifier classifies data point x the same class with labeled data point x j which has the highest impact on x i :
From the view of Nearest Neighbor classifier, the clas-sification result is more guaranteed if the impact is higher. That gives a direct motivation on the maximum impact mea-surement: to maximize the impact on a single unlabeled data point x i , we can choose the data points with the max-imum impact over x i from the candidates. So we can have a weighted function of summations over all these maximum values to measure the impact: where s i serves as a weight factor when counting the impact over points in the unlabeled data set. It has no problem to choose s i = 1 for all unlabeled data points, but there may be better choices. We suggest to use entropy as the weight. Specifically,
The point is that the use of entropy information here does not overlap with the entropy in H ( S ): different examples are checked by C(S) and H(S) in terms of entropy. To achieve a higher flexibility, we can introduce a balancing factor  X  to give a strengthened definition of C ( S ) as(for i = j , w Minimum Redundancy In equation 1, we do not have a term explicitly demonstrating the redundancy over the selected set. In this section, we X  X l prove that the minimum redundancy criterion has already been implicitly satisfied in the definition of Q ( S ).

The following is an explanation why maximizing Q ( S ) will also minimize the diversity. Specifically, C ( S )iscloselyre-lated to redundancy. Given a data point i  X  U  X  S ,letus define the dominant point dp ( i )as
The maximization of Q ( S ) will cause the dominant points get diversely distributed. If two dominant points in S are very close to each other, it is likely that they may have sim-ilar impact on other points, thus removing one of them will not let C ( S ) decrease much. In other words, if we already have vertex i in the selected set S , we X  X l not choose another vertex j similar to i in the future, because the improvement on Q ( S )islittle.
It is flexible in our framework that link information is nat-urally integrated into the definition of similarity matrix, by extending it using a similar method as page rank. That is reasonable because edges indicate the similarity and impact between the two ends. It is introduced that the similarity matrix W is used as transformation probability matrix in random walk[11]. Page rank is a way to introduce the graph structure into the transformation matrix. Generally speak-ing, under the page rank model, a particle may transit in one of the following cases:
Suppose there X  X  a well defined similarity matrix W which measures the impact solely in feature vector space, now we want to integrate link information into it, to get a new def-inition W . where 0  X   X  1, I ( i, j ) is an indicator function whether there is an edge between point i and j : and d i is the degree of i , d i = ( i,j )  X  E 1.
It can be proved that our algorithm is montonic submod-ular. There have been many works on finding good approx-imation algorithms for monotone submodular function opti-mization. For simplicity and efficiency, we X  X l use the greedy algorithm [5]. Algorithm 1 shows a structure of this algo-rithm. The outline of the algorithm is repeatedly enlarging set S by a new point v , such that Q ( S  X  X  v } )  X  Q ( S )ismax-imized. This algorithm have a guaranteed approximation rate 1  X  1 e .
 Algorithm 1 Maximize Q ( S )
The experiments are performed in text categorization data sets with citation information. We test the proposed method on the following three data sets, which are the most widely used data sets for text classification with link information:
Cora Data Set [9] contains 2708 scientific publications, and they are classified into seven fields, which are Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, and The-ory. After stemming and removing stop words, we are left with a vocabulary size of 1433 unique words, all of which appear at least 10 times in the documents. There are 5429 citations between the documents. We construct a binary classification problem by combining the first 4 classes into a category, and the others as another category.

Citeseer Data Set [9] contains 3312 publications, la-beled into 6 classes: Agents, AI, DB, IR, ML, and HCI. There are 3703 unique words after processing, and the num-ber of citations is 4732. In the same way as in Cora data set, we construct it into the binary classification by grouping some classes into a category.

WebKB Data Set [9] contains web pages from four computer science departments, categorized into five topics: course, faculty, student, project, and staff. The webKB data set contains 877 data points and 1703 unique words. There are 2868 total links between these pages. We construct it to binary classification by letting class  X  X ourse X  and  X  X roject X  be one class and the others be another class.
For the linked data set, we use the following methods as baselines:
Random selects the samples set randomly, giving each unlabeled point equal probability to be selected.
Most uncertainty selects the set with the largest en-tropy. Specifically, it is the function when  X  =0.
Active Learning using Gaussian Fields is an ap-proach suggested by [11] based on a semi-supervised learning framework using Gaussian fields and harmonic functions. It is single-mode, so we run this algorithm k times to select a set of size k . Note that in this framework, the link informa-tion can be similarly introduced using the proposed method. We will utilize the link information in the tests.
Hybrid is suggested by [3]. It asks for uncertainty ap-proach and two graphical metrics (betweenness and cluster-finding) to find a selected set, and using empirical risk to pick the best set among the union of the data points se-lected by the three strategies. k -means is suggested by [8]. In the article some active in-ference methods are compared with each other and k -means is found to be the best one among them. Here we employ the same strategy for active learning, that is, we find ver-tices using k -means as the labeled set and then train the classifier.
 For the purpose of simplicity, we will use Random, MU, GF, Hybrid, K-M short for the methods above respec-tively. We refer to our model as CAL (Collective Active Learning).
We set the  X  parameter to be 0.5, meaning each criterion are roughly equally balanced in our experiment. For the cora data set, we randomly pick 5 points as the initially labeled set L ; for the other two data sets, this number is 10 due to the size of the data set and the learning difficulty. For the same reason, we define the batch size k =5 , 10 , 5 for Cora, Citeseer and WebKB data sets respectively. For each data set, the batch mode active learning and collective active learning methods first select k samples based on L and then repeatedly select k samples based on the union of initialized labeled and selected samples; the single mode active learning iteratively select k samples, and repeatedly select and update the model as the batch mode methods do. After the selecting process, we learned the prediction based on the samples selected by different active learning methods using the same semi-supervised learning method for fair. Here we use the famous NetKit-SRL toolkit[4] for learning in networked data set. For each data set, we run the experiment 30 times with different initially labeled sets, and both the average and variance of the accuracy is used for final evaluation.

Figure 1 shows the results on each of the data set. Due to space limitations, we only draw the variance of proposed method in the figure. We can show from the results that maximum entropy does not have a good performance over all the data sets. For all the three data sets, our method outperforms the strategies based on graph metrics: the hy-brid method and the k -means method. The Gaussian ran-dom field based method X  X  performance in the three results was somewhat erratic: it does not perform well in Cora and Citeseer data set but in the webKB data set, the accuracy of it is very near to the proposed method. In webKB data set, the gap between random selection and these methods are not as high as other data sets. It is probably because it is not so easy in this web-linked data set to perform collective active learning. Also, from the view of variance, our meth-ods have average variances of 0.005, 0.009, 0.01 on the Cora, Citesser and WebKB data set, which are much smaller than the other methods. Generally speaking, from the experi-ment results of the three linked data set, we can conclude that our method is the best, or at least close to the best.
In this paper, we present a novel framework for collective active learning, which utilize both link and content infor-mation. An objective function is defined based on three proposed criteria. Experiments on a real-world data set shows that our approach outperforms other state-of-the-art methods in both linked and regular data sets. Although our model concentrates on the binary classification problem, it can be easily extended to the multi-class classification prob-lem.
The work is supported by the Natural Science Foundation of China (No. 60703059), Chinese National Key Foundation Research (No. 60933013), National High-tech R&amp;D Pro-gram (No. 2009AA01Z138).
