 ORIGINAL PAPER Huaigu Cao  X  Venu Govindaraju  X  Anurag Bhardwaj Abstract With the ever-increasing growth of the World uments when presented with user queries. However, uncon-strained handwriting recognition remains a challenging task with inadequate performance thus proving to be a major hur-dle in providing robust search experience in handwritten doc-uments. In this paper, we describe our recent research with focus on information retrieval from noisy text derived from imperfect handwriting recognizers. First, we describe a novel term frequency estimation technique incorporating the word segmentation information inside the retrieval framework to improve the overall system performance. Second, we out-line a taxonomy of different techniques used for address-ing the noisy text retrieval task. The first method uses a novel bootstrapping mechanism to refine the OCR X  X d text and uses the cleaned text for retrieval. The second method uses the uncorrected or raw OCR X  X d text but modifies the standard vector space model for handling noisy text issues. The third method employs robust image features to index the documents instead of using noisy OCR X  X d text. We describe these techniques in detail and also discuss their performance measures using standard IR evaluation metrics. 1 Introduction and contemporary handwritten documents (e.g. handwritten medical records, historical manuscripts, personal notes) has led to an ever-increasing demand for an efficient information ten documents when presented with user queries. Over the recent years, the retrieval methods have improved the quality of document search in a dramatic fashion. However, these methods are predominantly applicable to documents with ASCII text or machine-printed document images where the interested in recognition of unconstrained handwritten doc-uments, which is a significantly more challenging task due to the large variations in writing styles and document image quality. The methods developed for ASCII text and machine-printed documents are rendered ineffectively due to low OCR accuracy. In this paper, we present an overview of our own research in information retrieval and extraction from noisy OCR X  X d text extracted from unconstrained handwritten doc-uments. Specifically, our focus is on handwritten medical lenging domain for any automatic recognition or retrieval system. The challenges of automatic transcription lies in three respects: (1) large variability in handwriting samples given the multiple authors even with a single document, dif-gency medical conditions, (2) poor image quality, and (3) a large lexicon (dictionary) of medical words that can be around 5,000 words. In this paper, we outline three separate research directions for tacking this problem in form of OCR correction-based retrieval methods, modified vector model-based methods and keyword spotting-based methods.
The rest of the paper is organized as follows. Sec-tion 2 describes the related work in the area of infor-mation retrieval from handwritten documents. Section 4 presents OCR model-based document retrieval techniques and describes a novel term frequency estimation technique in detail. Keyword spotting-based techniques are presented in Sect. 5 . Conclusions are outlined in Sect. 6 . 2 Background Several works have tried to improve the quality of infor-mation retrieval on OCR X  X d text. Previous work [ 2 , 11 ] has shown that the IR performance is adversely affected by the noise present in the OCR output due to low recognition per-formance. A typical solution to this problem is to correct OCR errors using post-processing techniques. [ 14 , 19 , 20 ] propose different methods of OCR correction for improv-ing the information retrieval performance. Mittendorf et al. to design a term-weighting scheme for information retrieval from document images. In Ohta et al. [ 20 ], specific charac-ter transformations and character occurrence bi-grams were used to generate candidate terms for each  X  X rue X  search term. Documents retrieved by each candidate term are then eval-uated for inclusion into the final result set. This approach results in minor improvements in recall for moderate quality OCR documents. Jing et al. [ 14 ] build a language model that takes OCR errors into account. This model approximates an  X  X ncorrupted X  version of a particular document for efficient retrieval.

Due to the poor recognition results in handwritten docu-ments, it is not feasible to apply probabilistic modeling of OCR output. Recently, there has been much focus on infor-propose to use top-k hypothesis from the OCR instead of using just the top choice. They report that using multiple recognition choices for retrieval improves the overall recall of the system. Rath et al. [ 22 ] propose an IR model that assumes independence between each term of the query for the purpose of computing its similarity with a given doc-ument. The frequency of each term is computed using the posterior probability estimated from the word image features. not use word recognition probabilities. Instead, they model the ranking as a zipfian distribution where word recognition probability is inversely proportional to its rank.
As an alternative to OCR-based document indexing and retrieval techniques, keyword spotting from handwriting has gained significant research attention lately for solving the document retrieval task. Keyword Spotting is defined as an image matching task where the input query is matched against candidate word images of the document. The word-level matching scores can then be combined to generate a document level score that can be used for ranking the documents in order of query similarity. There are two major classes of keyword spotting: (i) Unconstrained Keyword Spotting: also known as  X  X ecognition-based keyword spot-ting X  that relies on an OCR (i.e. HMM) to provide prob-abilistic values for each keyword in the lexicon that can later be integrated into a document-level ranking score  X  X ecognition free keyword spotting X  that is the typical image matching task for generating the document relevance score for an input query [ 5 , 18 , 24 , 26 , 27 ]. 3 OCR correction-based IR This technique can be described as a multi-pass technique to boost recognition and then perform retrieval on the refined OCR X  X d text. In the first pass, an OCR correction model is employed, which improves the recognition rate of the hand-text of handwritten word recognition, reducing the size of lexicon translates to an improvement in word recognition performance. In our work by Milewski et al. [ 17 ], we pro-pose a lexicon reduction-based strategy for OCR correction in context of handwritten medical forms. It can be under-stood as a novel bootstrapping algorithm, where a sequence of confidently recognized characters from word recognizers are used to encode the document category that is further used specific terms.

First, all the forms that are manually categorized under a specific topic are used to generate a lexicon of words. These lexiconsarethenusedtoextractphrasesfrominitialwordrec-ognitionoutputusingacohesion-basedmetric.Thesephrases are then encoded into a phrase-category matrix on which singular valued decomposition (SVD) is performed. The resultant matrix is then used to compute a topic or category distribution for every test document. Finally, a second phase of recognition is performed that uses a reduced lexicon, cor-responding to document category. In a similar work on OCR correction, Bhardwaj et al. [ 3 ] also use the document topic since instead of reducing the initial lexicon, they create a re-weighted lexicon for word recognition. In the retrieval phase, they utilize a two-pass document scoring method for ranking each document in the collection corresponding to the input query. First, each document is ranked according to the frequency of query terms present in it. In cases, where sim-ilar number of query terms are present, a secondary phase re-scores these documents using a distance metric d ( a i that computes the distance between two matched words, a i and b j such that i and j , respectively, represent the word position in the document. w ij represents a weight based on the frequency of occurrences of words a and b in the document. d ( a i , b j ) = w ij  X  3.1 Performance measures The retrieval performance is evaluated using standard MAP(mean average precision) and R-precision metrics. MAP denotes the mean value of the average precision of all queries in the set. R-precision measures the precision of the retrieval system at a point when the total number of doc-metrics for different search experiments.  X  X R X  experiments refer to methods when either of the query terms are searched.  X  X ND X  X xperimentsrefertomethodswhenallquerytermsare searched.  X  X L X  and  X  X L X  methods refer to reduced lexicon-and complete lexicon-based methods. A complete lexicon of 4,570 words is used in the experiment of [ 17 ]. Through the lexicon reduction, the complete lexicon is decomposed 1,200 words. The average size of the reduced lexicons is 574. As seen from the figure, reduced lexicon-based search meth-ods outperform the complete lexicon-based searches due to better quality of the recognized text.
 4 Adapted vector model-based IR 4.1 Classic vector model In the classic Vector Model [ 1 ], the documents are repre-sented by the vector space of terms. A term is a word from the vocabulary of all of the documents. Given the vocabulary { t } , 1  X  i  X  N , the term frequency tf tf of occurrences of all the terms in document d j .Theinverse document frequency (IDF) of a term is defined by id f i = log where # { X } denotes the number of elements in set { X } .The IDF of a term shows the importance of the term: a term that appears in most documents is less important than a term that appears in only a few documents. A query is also represented by the vector of terms. The query term frequency (QTF) of query q is defined as tf and the query is represented by vector tf 1 , q , tf 2 , q tf
The similarity between document d j and query q is defined as sim ( d j , q ) = 4.2 Modified vector model document image and need to be estimated. Thus, we mod-modified TF is tf and the modified IDF id f i = log function of the expected value of freq i , j , i.e., round ( value: freq i , j = E { freq i , j } , (8) The text length in Eq. ( 6 ) is estimated by L The similarity between document image d j and the query q is given by sim ( d j , q ) =
Suppose document d j is represented by the observed image features arbitrary segmentation of images. The expected value of freq i , j is given by E { freq i , j }= where the probability that is the word sequence recognition probability. # t i ( number of term t i occurring in sequence
Equation ( 11 ) can be simplified in some special situations.  X   X  segmentation E { freq i , j }= In addition to the assumption of knowing the correct segmen-tation, assuming the independence of terms  X  1 , X  2 ,..., X  i.e., Pr (  X   X   X  |  X   X  w) = then Eq. ( 12 ) is equivalent to E { freq i , j }= tation and independence of terms. In the general case, given the probability of every single segmentation point and a lan-guage model ( n -gram), we can solve Eq. ( 11 ) by dynamic programming. 4.3 Estimating term count freq i , j The observational sequence of a document image can be rep-resented by a sequence of connected components sorted in the reading order. Since the following discussion focuses on a single document, we can omit the subscript j of d j from notations like freq i , j without ambiguity.

Given N consecutiveconnectedcomponents c 1 ,... c n and the set of terms t 1 ,... t N , we use a dynamic programming-based algorithm to compute the term count. We assume a wordimageiscomposedofatmost C connectedcomponents. The term count of t i in sequence c 1 ,..., c k (0 &lt; k word gap is denoted by  X  k . When we define freq k i and  X  a sequence c 1 ,..., c k , we assume  X  0 =  X  k = 1. Next, we will present the formulae of estimating term counts when the language model is a bi-gram. The formulae for higher-order LM X  X  can be derived similarly.
 When k = 0, the sequence is empty, and thus E ( freq 0 i ) = 0 (15)
When k = 1, the only possible segmentation is that c 1 is a word image, and thus E ( freq 1 i ) =
When k = 2, the last word image can be either c 2 or c 1 c The probability that c 2 is recognized as t i is t as t i is Thus, E ( freq 2 i ) =  X 
For an arbitrary k &gt; 0, we can prove that the number of terms and Similarly, we can prove that  X  + if 1  X  k  X  C ; (22) and if k &gt; C . (23) Eqs. ( 15 ) X ( 22 ). 4.4 Estimating word segmentation probability Word segmentation is defined as the process of segmenting a line into words. In handwritten lines, the space between words is uneven. Moreover, the same amount of space may be present between words and between characters within a word. Such cases arise due to differences in writing styles and space constraints.

In our word segmentation method, for every gap between any two consecutive connected components, the probability of that gap being a between-word gap is estimated. A gap between two connected components is represented by three features: 1. Euclidean Distance. This feature is defined as the hor-2. Minimum Run Length. This feature represents the min-3. Convex Hull Distance. The Euclidean distance between To eliminate the effect of different text sizes, we compute the average height of all the components and normalize the extracted features by dividing them by the average height of all components in the same line.
 The segmentation probability of a gap g is given by the Bayes X  Rule  X  word gaps and within-word gaps, respectively. f represents density of the features of within-word gaps. Given a set of gap features with the annotation of  X  X etween-word X  and  X  X ithin-word X  , we can estimate Pr ( g ) ,Pr (  X  g ) , p p ratio of the numbers of between-word and within-word gaps in the training set.
 Pr ( g ) = # Pr (  X  g ) = 1  X  Pr ( g ) (26) p Parzen window technique with a Gaussian kernel function. 4.5 Estimating word recognition likelihood We use a lexicon-driven word recognition algorithm [ 15 ] based on character segmentation and dynamic programming to find the best matching path. First, a word image is seg-mented into candidate character images. Then the directional features are extracted from the contours of character images and matched to every word in the lexicon by searching all possible segmentations for the minimum sum of Euclidean distances from the features of the test image and the char-acter templates in the training set. The minimum Euclidean distance indicates the similarity between the word image and by s (w, t i ) . We can use the Bayes X  rule to verify, if t genuine match of w : bilities of genuine and imposter matches, respectively. For ferent term t i . Thus, can be denoted by a function g of s (w, t i ) .
 scores of all of the terms. We model p ( s | G ) and p ( s | I ) as Gamma distributions. Actually, the matching score s is a squared sum of distances between character-level fea-In other words, s = where D l is a character matching distance. If we assume all the clusters of the training feature vector space are independent normal distributions, then the squared sum of the distances can be modeled as a gamma distribution. The probability density function of the gamma distribution can be represented by f ( s ; k , X ) = s k  X  1 e where ( k ) is the gamma function: ( k ) = closed-form solution for the maximum likelihood estimation of k and  X  [ 9 ]. However, we can use a simple way to esti-mate the Gamma distribution. First, we can prove that the mean and variance of the Gamma distribution are k  X   X  and k  X   X  2 , respectively. Then, given N genuine matching scores s , s and variance:  X   X   X   X   X   X   X  Let  X  k  X   X   X  =  X   X  and  X  k  X   X   X  2 =  X   X  2 , then  X   X   X   X   X   X   X  k =  X  =
A Genuine probability/score curve estimated from 5461 genuine matching scores and 1,226,022 imposter matching scores is shown in Fig. 4 .
 By Bayes X  rule, the likelihood Pr ( t
On the other hand, we approximately compute the poster-ior probabilities using Pr ( t From Eqs. ( 34 ) and ( 35 ), Pr (w | t Thus, we can replace the likelihood Pr ( c k  X  c + 1 ... c g ( s ( c k  X  c + 1 ... c k , t i )) in Eqs. ( 15  X  22 ). 4.6 Search engine based on modified vector model A search engine for handwritten document is built using the modified vector model and term count estimation method discussed in the previous subsections. The flowchart of the processing, indexing, and document retrieval.

In the preprocessing phase, image enhancement such as are identified by page segmentation.

Indexing includes word segmentation and recognition with the estimation of probabilities. We use these probabili-tiestoestimatethetermfrequency(TF)andinversedocument frequency (IDF) and store the estimated TF and IDF values for retrieval.

When searching the database for relevant documents, the user input query is converted to a query vector and the sim-ilarity of the vector model is calculated for each document. Documents are ranked in the decreasing order of similarity, and top documents are returned. 4.7 Computational issues Only the non-zero values of the TF matrix are needed to be time complexity of retrieval are both linear in the number of non-zero values in the TF matrix. Since the TF matrix for sparse when indexing document images (using the proposed method). Practically, we can convert the TF matrix into a sparse one without affect performance much: we can choose a threshold THR sparse and turn those elements from the TF matrixthatarelessorequaltoTHR sparse (seecircledelements in Fig. 6 b). We set THR sparse to 0.002 in our experiments. 4.8 Test corpus Our test corpus consists of the New York State Pre-hospital Care Reports (PCR forms). In New York State, all patients who enter the emergency medical system (EMS) are tracked through their pre-hospital care to the emergency room using the PCR. The PCR is used to gather vital patient infor-mation. Retrieval on this data set is quite challenging for several reasons: (i) handwritten responses are very loosely constrained in terms of writing style, format of response, and choice of text due to irrepressible emergency situations, (ii) images are scanned from noisy carbon copies and color background leads to low contrast and low signal-to-noise (  X  5,000 entries). This leads to difficulties in the automatic using Word Model Recognizer (WMR) [ 15 ] is below 30%. Each PCR contains only about 100 handwritten words on average, so the content is very short and ordinary IR meth-ods perform badly, since some of the terms are often absent from the OCR result. 4.9 Preprocessing and recognition of PCR form images First, we detect and remove the skew of every PCR form image as follows. 1. We manually de-skew a form and take it as a template. 2. We use the anchors to perform registration between the 3. The skew angle of the test image is obtained by the rel-
By aligning the test image to the template image, we can also obtain the position of each form cell containing a line of text. The template-matching-based de-skewing and page segmentation work well on the PCR form images, since they have a fixed layout and are scanned at the same resolution. Our approach is applicable to other types of forms as well.
We use the MRF-based document image preprocess-ing algorithm [ 6 ] to binarize the form image and remove the grid lines from the image. Assuming the binarized objective image is x and the grayscale image is y ,we solve the maximum a posteriori (MAP) estimation  X  x = argmax An example of binarization and line removal result is shown in Fig. 8 . The MRF-based preprocessing method improves the word recognition accuracy from 18.7% (obtained by the PCR form preprocessing algorithm in [ 17 ]) to 28.6%.
We use 1,099 between-word gaps and 5,138 within-word gaps to train the word gap classifier using the method pre-sented in Sect. 4.4 . The classifier is evaluated on a test set of 791 between-word gaps and 4,369 within-word gaps. If we take probability p thr as a threshold to determine the category of a gap, we can compute the recall and precision values obtained from the given test deck. Thus, a precision-threshold, p thr .

The WMR handwritten word recognizer is trained using 21,054 character images collected from the handwriting on of 4,551 English words is generated from the ground truth of 783 PCR forms.

Abi-gramLMistrainedfromtheabove783forms.Aword recognition rate of 28.6% is obtained on the PCR forms. 4.10 Evaluation metrics of IR test The IR tests are evaluated in terms of mean average precision (MAP) and R-Precision [ 1 ]. The mean average precision is obtained in the following way: 1. For each query, check the returned documents starting 2. The mean value of the average precisions of all the que-
R-Precision of a query is the mean value of precisions computed for each query when R documents are retrieved, where R is the number of relevant documents. The mean value of the R-Precisions of all queries is the R-Precision of all of the queries. For example, suppose 100 documents are relevant to query q 1 , and 30 of the top 100 retrieved documents are relevant to the query, then the R-Precision of query q 1 is 30/100 = 30%. Suppose the R-Precision of another query q 2 is 20%, then the R-Precision of q 1 and q is (30 + 20%)/2 = 25%.

In addition to the mean average precision and R-Precision, the performance of the IR system can also be visualized using is calculated. Finally, we get 11 precisions. 4.11 IR tests The document images used in our IR tests are 342 PCR forms with manually transcribed ground truth and coordinates of each word. We have 28 queries and manual annotation of relevance of the 342 forms to these queries. These 342 PCR forms are different from the 791 forms used in the training of the word recognizer and LM. The queries used in our IR tests are shown in Table 1 .
 We compare the performances of the following 7 IR tests: Tests 1 X 4: IR tests on OCR X  X d text
The MAP and R-Precision values of the above IR tests are compared in Fig. 10 a. A trivial average precision of 4.76% is obtained by generating random retrieval results for the 28 queries. We amend the metrics by subtracting the triv-ial AP from the MAP and R-Precision values. The amended metrics show the incremental improvement from the triv-ial result. The amended MAP and R-Precision values of the above IR tests are compared in Fig. 10 b. Tests 1 X 4 show that the improvement of using more word recognition candidates word recognition text is very slight. Even a naive estimation of the term counts (Eq. 37 ) improves the IR performance compared to the tests based on OCR X  X d text. But the use of the word segmentation probabilities and the language model method that only uses isolated word recognition results.
The interpolated 11-point precision curves of tests 1 (OCR X  X d text, S = 1), 5 (VM+isolated word estimation) and 7 (VM+word sequence estimation) are shown in Fig. 11 a. The IR performance of building the index on the ground truth text is also shown in Fig. 11 a. Tests 5 and 7 produce simi-lar precisions at low recall (around 0), but Test 7 produces significantly higher precisions at higher recalls.
For better comparison, the above 11-point precision truth precision, and then normalize the recall-precision coordinates so that the trivial precision is always 0 and the ground-truth precision is always 1. The trivial precision is defined as the precision obtained by ranking all the docu-ments randomly: precision obtained by IR test performed on the index built on ground-truth text. The amended precision of an original precision p is defined as
The amended 11-point precision curves in Fig. 11 bshow that the proposed method obtained improvement at almost all recall levels but especially improved the precisions at high recall rates (&gt;50%). The two existing methods perform very But the proposed method still obtained about 10% precision at the recall level of 100%. 5 Word spotting-based IR The notion of word spotting [ 22 ] has been introduced as an alternative to OCR-based information retrieval solutions. It occurrences of a typed query word in a set of handwritten or machine-printed documents. This section presents some of our keyword spotting approaches for handwritten medical forms as well as multilingual documents.
 5.1 Probabilistic word spotting model vides an improved retrieval performance by combining the word recognition likelihood and word segmentation prob-ability in a probabilistic framework. Given a series of con-word image w represented by c i , c i + 1 ,... c j (1  X  i their model represents the similarity between w and a query word q by: sim (w, q ) =  X  where  X  k ( 1  X  k  X  n  X  1 ) is the probability of the gap between c k  X  1 and c k being a between-word gap,  X  0 =  X  and Pr ( q | w) is the word recognition probability. The gaps are assumed to be independent, and therefore, the word segmentation probability can also be represented as  X  i  X  ( 1  X   X 
Figure 12 shows the average precision curve of the pro-posed method that outperforms traditional keyword spotting approaches assuming perfect word segmentation. 5.2 Feature-based word spotting model Bhardwaj et al. [ 4 ] propose an image feature-based keyword spotting solution for multilingual documents. They describe an indexing process that extracts moment features from input word images and stores the feature values as indexes. The features are computed using a geometrical moment equation that is invariant under image translation and scale transfor-mations. m
For the retrieval process, they represent all the query words and candidate words as traditional vector space model. Cosine similarity is used to compute similarity between the query images and indexed images on moment feature space. Finally, all the candidate word images are ranked in order of their similarity with the query image. Since the similar-ity values are computed on feature space, it X  X  not robust to larger image variation and lower image quality. To address this issue, they use relevance feedback mechanism to re-rank all the candidate word images. This mechanism re-formu-lates the query feature vector by adjusting the values of the individual moment orders present in the query vector. The relevance feedback mechanism assumes a user input after denoting a result to be relevant or 0 denoting a result to be irrelevant. The new query vector is computed as follows: q a relevant result set, and NR denotes a non-relevant result set.

Table 2 describes their results on 3 different scripts before and after applying relevance feedback (RF). 6 Conclusion Information retrieval from handwritten documents is a chal-lenging task primarily due to lower word recognition rates in the case of unconstrained handwritten documents when compared to machine-printed document images. Traditional informationretrieval techniques thereforefail toperformeffi-ciently in case of noisy OCR X  X d text. In this paper, we pre-sented some of our existing methods that deal with retrieval from noisy OCR X  X d text. We discussed three approaches that OCR output and then performs retrieval over the cleaned text. The second approach uses the uncorrected OCR X  X d text, but modifies the traditional retrieval model to account for OCR errors. The third approach uses image-processing techniques to compute similarity between query and word images and retrieves them accordingly. Each of the discussed approaches ferent applications. Our future work will focus on exploring techniques leading to higher IR performance on handwrit-ten documents including combining the benefit of all OCR correction methods with modified retrieval, integrating the stemming technique into the language model used in OCR, as well as broader applications such as the detection of out-of-vocabulary (OOV) items X  X ame identities and so on X  X n noisy text.
 References
