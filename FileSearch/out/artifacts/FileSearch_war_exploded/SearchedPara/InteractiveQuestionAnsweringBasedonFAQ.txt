 1 Introduction
A question answering system receives the user  X  s question in nature language, and answers it in a c oncise, accurate and natural way. Question answering system analyzes the question to acquire the information requirement of user. And then based on the knowledge question answering system convert the information requirement into constraint condition while searching answers in knowledge space. The interaction between user and question answering system is introduced in the interactive question answering. There are two differences between interactive question answering and single round question answering. The first difference is that interactive question answering is a continuous question answering. How to use the context information is one key point. The second difference is that the question answering system interacts with users besides answering questions. It is the second key point of interactive question answering.

In this work, the answers are from the FAQ. FAQ provides information from the question-answer pair. The community question answering in web portals brings out abundant FAQs. In this work the FAQ is from the Baiduzhidao. For the first key point of interactive question answering, the ranking learning method is used to train a statistic model to predict answers. The features that describe the training and testing instances are from the question, related FAQ and question answering context. And the syntactic, semantic and pragmatic information is concerned to extract the features.
For the second key point, the interaction between the question answering system and user is designed as that after answering a user  X  s question the system will request the user to feedback whether he is satisfied with the answer. This interactive mode has two advantages. First is that the white or black feedback is easy to be caught and fully used by QA system. Secondly this kind of feedback matches FAQ. If the feedback is positive the question and answer will be added into the FQA base. If the feedback is negative, QA system will provide another answer based on the question and repudiated answer.
 The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 is about the model and syntactic, semantic and pragmatic features in context question answering. Section 4 describes the interaction between user and QA system. Section 5 is the experiments and results. Section 6 is conclusions. 2 Related work get answers in continuous question answering. The second is the interaction between QA system and users. To the first point, since 2004 in TREC QA task [ 1], a group of questions were around one topic. Hence a question was related to the question answering context. In real interaction the topic transformation is possible. The questions may not be around one topic. Yang estimated whether the topic is transferred through anaphora and ellipsis by decision tree [ 2]. Sun used center theory to deal with the anaphora in context question answering [ 3]. Sun [ 4] and Chai [5] used discourse theory in context question answering. Kirschner adopted logistic regression with contextual information to find answers in context QA [ 6]. There are three methods to obtain the data of context question answering. The first was getting questions from the QA evaluation such as TREC QA task [ 3]. However there was a gap between this kind of data and the realistic context question answering. The second method is wizard of OZ [7] which simulated the interaction between user and QA system to get the question answering data. The third method was collecting the question answering data while using the question answering system. This kind of data is the most realistic data. However, this kind of data is limited by the ability of the QA system.
 The second key point is the method of interaction between user and QA system. The TREC QA task also explored the interaction between QA system and user. Interactive QA was first introduced in TREC 2006[9]. And in TREC 2007 QA task[10] reviewers interact with the QA system online. As reported, for most systems the answers are improved after interaction. However the improvements were not significant. Hickl employed CRF model to construct question-answer pairs. And then it showed the question answering pair to user to impact the following question that users will ask in the next round [ 11]. Misu used reinforcement learning to learn the interacting strategy in the interactive process [ 12]. S imilarly , adaptive learning was also used in an interactive question answering [ 13].
 The FAQ is a kind of knowledge source that is easy to use by QA system. In 1997 Burke researched the FAQ question answering in semantic level [ 14]. Kolkata discussed how to get answers when there are no related questions in question-answer pairs [ 15]. Cong used sequence pattern based classification method to extract FAQ pairs form forum and a c ommunity question answering [ 16]. In this work ranking learning method is used to select answers [ 17]. Ranking learning methods are widely used in information retrieval [ 18] and question answering [ 19]. 3 Context question answering in Interactive question answering how to extract answers in context question answering. The answer extraction is converted to a ranking learning process. It is supervised learning that uses labeled instances to train the statistic model . In the following part of this section the support vector based ranking learning methods and the syntactic, semantic and pragmatic features in context question answering are introduced. 3.1 Support vector based ranking learning method SVM-MAP[21] are used to rank the candidates question-answering pairs. The idea of support vector was first exploited in support vector machine (SVM) [22]. The core of data correctly classified and the g eometric interval s maximize. The optimization of SVM is described in the following equations. i  X  is the slack variable for l inearly inseparable conditions. The parameter C is the p enalty parameter . classification. The mainly difference is that o utput Space of ranking learning is an ordered sequence space. The optimization target of support vector based ranking learning is that the order of the output sequence is correct and maximizes the interval of distances between the elements which are mapped on the plane.
 is Ranking SVM which is a pair-wise approach. The second is SVM-MAP which is a list-wise approach. The mainly difference between the two approaches is the difference between their lose/risk functions.
  X  is a metric to measure the consistency of two finite st rict orderings . For two finite st rict orderings ra and rb which are in the same space, r a  X  D  X  D and r b  X  D  X  D, the Kendall  X  s  X  is defined as: In the two orderings. If a pair d i  X  d j has the same order in ra and rb, the pair is concordant. Otherwise the pair is discordant. P is the number of the concordant pairs and Q is the number of the discordant pairs. And m is the number of the elements in ordering. The summation of P and Q is consistency the two orderings are.
 risk function of SVM-MAP is described by mean average precision (MAP) [24]. MAP is also a metric which measure the consistency of two finite st rict orderings . It is widely used in the evaluation of information retrieval and question answering. Average precision is the basis of MAP, which is calculated as equation 9. In the equation p(i) is the prec ision of the elements from the first to the ith . And  X  r(i) is the variation of recall in the ith position. It is the difference between r(i-1) and r(i). And r(i) is the prec ision of the elements from the first to the ith . On the right of equation 9 rel(i) represents whether the ith element is a correct answer for the question. Average precision describes the consistency fo the two orderings from precision and recall. MAP is the mean of APs for several groups of orderings. SVM-MAP has been introduced. The mainly differences between them are the definitions of the risk function. And the two ranking learning approaches will tested in the experiments. 3.2 The features for context question answering syntactic, semantic and pragmatic level. Here the syntactic, semantic and pragmatic from question Q and candidate question-answer(Q  X  , A  X  ) pairs are first introduced. 3.2.1 Syntactic feature question-answer pair in grammatical form . The overlap of words are used to calculate the syntactic similarity[19]. The overlap of words is the proportion of the concurrence words in the sentence. The overlap of words between question Q and the question Q  X  and answer A  X  of candidate question-answer pair(Q  X  , A  X  ) are calculated separately. Firstly the question Q and QA pair (Q  X  ,A  X  ) are segmented and tagged the POS and the entities. Then the verbs, nouns, adjective s and entities are retained to calculate the overlap of words between question Q and question Q  X  and answer A  X  in candidate QA pair, as the equation 7 and 8 . C is the number of the overlap words in sentence. And n is the number of words of a sentence. 3.2.2 semantic features QA pairs (Q  X  , A  X  ) in content. The semantic similarity between two sentences is calculated based on the word semantic similarity. Here the word semantic similarity is calculated based on Howent [25]. We refer Liu  X  s method[26] to calculate the word semantic similarity between words based on sememes. The similarity between two sememes is calculated based on the distance of the two sememes in the sememes tree(equation 9). And the similarity between concepts is calculated based on the sememes similarity. There are four kinds of sememes describing a concept. They are first sememe, basic sememe, relational symbol and relational sememe. The similarities of the four kinds of sememes between two concepts are calculated separately. And then the semantic
The semantic similarity between sentences is calculated based on the word semantic similarity. Equation 20 shows how to calculate the sentence semantic similarity. The sentence similarities between question Q and question Q and answer A in the candidate QA pair are calculated as equation 12 and 13. 3.2.3 Pragmatic features question answering system the subject is the user. And the goal of the user is obtaining the answer that satisfies the user  X  information requirements . In question answering the pragmatic information is the information that indicate s the whether the answer can meet the user  X  s information requirement. In this work , the pragmatic information is from the question Q and Q  X  in candidate QA pair. The pragmatic feature of question Q describes the expected speech act of answers. The user has expectation on the speech act of answers. For example, when a user asks  X  Why France rejected the EU constitution treaty?  X  , the expected speech act of answer is  X  explanation  X  . From the expected act of answer, questions are classified in five categories: statement, instruct , explanation, v erifying and o pining . A maximum entropy [27] method is used to classify the expect ed speech act s of answer s . The classification features include n-gram, i nterrogative , the words modified by i nterrogative and syntactic structure . We collect 3124 questions by a search engine based on Chinese interrogatives and label the expec ed t speech act of answer. And 70% data is used to train the model and 30% data is used to test it. The results are shown in table 3. And the average F score is 91.3% classified as another pragmatic feature. And if the two expected speech acts are matched, there is more possibility that the candidate QA pair is the answer for the question. 3.3 Context features context. The context features include whether the topic of QA is continuous and the syntactic and semantic similarities between the candidate QA pair and context. topic transfers, the question has no relations with the context, and the context information cannot assist to find answers. Hence whether the QA topic is continuous is an important feature when using the context information.
 an effective classification method in dichotomy problem. Here the tool libsvm[28] is used. RBF kernel is chosen and the parameter c and r are confirmed by the tool grip. And the features for classification include the features from the current question and the features from context. Firstly the features from the question are introduced. The anaphora is an important feature. If the question contains anaphora , it is high possibility the question has relation with context. And some conjunctions and adverb s such as  X   X  X  X   X  ( since ) and  X   X  X  X   X  (then) also show the continuous relation. E llipsis is also an important feature. We use the dependency of the question to judge the ellipsis in question. If the subject is lacking in the question, the e llipsis are confirmed. The second kind of features is from the question and context. The syntactic and semantic similarities between question and question answer pair in the p revious round are introduced as features.
 community . Second is the Confucius and analects of Confucius question answering in Baiduzhidao. We collect 400 group context question answering and tag the continuity manually. And 5 times cross validation is used. Table 4 shows the result. features. These features are: (1) the syntactic and semantic similarities between the question Q  X  in candidate QA pairs and the last user question Qp, (2) the syntactic and semantic similarities between the question Q  X  in candidate QA pairs and the last answer Ap, (3) the syntactic and semantic similarities between the answer A  X  in candidate QA pairs and the last user question Qp, (4) the syntactic and semantic similarities between the answer A  X  in candidate QA pairs and the last answer Ap. 4 The interaction between QA system and user The second key point of interactive question answering is the interaction mode. As presented by the TREC CIQA task, complex interaction did not assist to get answers signally. In this work a naive and effective interaction mode is adopted. The interactive mode is that after providing answers, the question answering system will ask user for feedback whether he is satisfied with the answer. It is the direct representation of the answer effect. In this interaction mode the form of user  X  s feedback is restrict ed. Hence the feedback information is easy to be obtained and used.

There are positive and negative feedbacks from the user. For the positive feedbacks, user  X  s question and system  X  s answer are combined together as the QA pair and is stored in the FAQ knowledge base. It makes the knowledge of QA system increases in the process that the user uses a question answering system. For the negative feedbacks, QA system should supply a new answer for the user. When finding the new answer, the information from question and previous answer is used. The syntactic and semantic similarity between question Q and new answer are calculated. And the semantic similarity negative answer and new answer are also calculated. Then the score for new answer is calculated as equation 14. It follows the hypothesis that the more related the new with the question and the less related to the negative answer, the more possibility the new answer is correct. 5 Experiment paper the QA system is FAQ based QA system about t he Analects of Confucius . The FAQ is from Baiduzhidao. Baiduzhidao is a portals community of question answering. In community question answering the answers are also from users which contain the domain experts. And users can vote, commit the answers to filter the best answers. Hence the community question answering is an effective source to get the FAQs. Here we crawled more than 26000 questions about the t he Analects of Confucius . And about 7100 QA pairs that are labeled  X  best answer  X  are restored in the FAQ knowledge base.

Baiduizhiao is also used to acquire the training and testing data. Volunteer s are required to supply 10 questions about t he Analects of Confucius . volunteer s can get the answers from Baiduzhidao, and consider the next question based on the answer. Form 10 volunteer s 100 questions are collected. And after filtering the improper questions 10 groups 90 questions are retained. This method simulates the interaction process between users and QA system, so that it is a Wizard of oz method. The 10 groups questions are divided into 5 parts. And 5 cross validation is used to evaluate the results. The evaluation metrics are MAP and p@1. MAP has introduced in section 3.1. P@1 is the precision of the first answer. In this experiment the effect of context features is evaluated. The results with and without context features are compared.
 is consistent with the related works [ 21]. And after adding the context features the MAP and p@1 using SVM MAP are both improved. And for Ranking SVM the MAP decreases slightly . These prove that the context features are important for continuous question answering. And after feature selection the MAP and p@1 are both increase for SVM MAP and Ranking SVM. And the results are also comparable with recent related work [ 15].
 system and user to verify the effect of the interaction method. Based on the previous experiments, there are 62 answers are correct for 90 questions. For the correct answers users give positive feedbacks. These question answer pairs are stored in the FAQ knowledge base. And for the rest 28 answers with negative feedback, the method in section 4 is used to find a new answer for the user. And 15 new answers are correct. The precision of the second round answers is 0.536. And combining the first round answers and second round answers the total precision is 0.856. It proves that the interaction between QA system and user is effective and helpful to find the collect answer. 6 Conclusions interactive answering faces two major problems . The first is how to answer the user question in the process of continuous answering, which is mainly manifested as how to use context information . The S econd is the interaction mode between QA system and the user. For the continuous question answering , this paper adopts the frequently answers to questions (FAQ) as the source of the answer and uses ranking learning methods based on support vector to build model. The features of describing training and test instances mainly come from two sources : one is the syntax, semantic and pragmatic features of the candidates question answer pairs , the other is the continuity of the question answering and the syntax and semantic features from context . As to the interaction mode between QA system and the user, the QA system asks user s whether they are satisfied with the answer. This interaction is simple and effective, because the information obtained is easily understood and used by the QA system . I t can provide the basis for a QA system to get new answers or adding correct answers to the questions to the knowledge base. Experiments show that it is effective to answer user questions by using the ranking learning method and multiple features. And the interaction between QA system and user s further significantly improve the accuracy of the QA system to answer user questions. Future studies need to address the following questions. Firstly more features describing the relations between questions and answers are need ed to understand the questions and answers and to improve the performance of QA system. The second is the interaction way between QA system and the user. A naive interactive way is used in this paper, by this way, the QA system is easy to understand and use the information. However, the expression of the user's information is limited, so in future studies it should also be concerned about the way that enables users to more freely express their intent ion . Reference
