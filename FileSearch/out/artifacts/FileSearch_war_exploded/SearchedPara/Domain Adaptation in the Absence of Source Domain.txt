 The overwhelming majority of existing domain adaptation meth-ods makes an assumption of freely available source domain data. An equal access to both source and target data makes it possible to measure the discrepancy between their distributions and to build representations common to both target and source domains. In re-ality, such a simplifying assumption rarely holds, since source data are routinely a subject of legal and contractual constraints between data owners and data customers. When source domain data can not be accessed, decision making procedures are often available for adaptation nevertheless. These procedures are often presented in the form of classification, identification, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other cases, the owner of a source data is allowed to share a few representative examples such as class means.

In this paper we address the domain adaptation problem in real world applications, where the reuse of source domain data is lim-ited to classification rules or a few representative examples. We ex-tend the recent techniques of feature corruption and their marginal-ization , both in supervised and unsupervised settings. We test and compare them on private and publicly available source datasets and show that significant performance gains can be achieved despite the absence of source data and shortage of labeled target data. Domain adaptation; classification; machine learning; marginaliza-tion; emerging applications
While huge volumes of unlabeled data are generated and made available in many domains, the cost of acquiring data labels re-mains high. Domain Adaptation (DA) problems arise each time when we need to leverage labeled data in one or more related source domains, to learn a classifier for unseen data in a target domain. The domains are assumed to be related, but not identical in which case it becomes a standard machine learning problem. Such a sit-uation occurs in multiple real world applications. Examples in-clude named entity recognition across different text corpora, object recognition in images acquired in different conditions (like back-ground scene, object location and pose, view angle changes), and many others (see [25, 34, 39] for a survey on transfer learning and domain adaptation methods).

Domain adaptation is particularly critical for service companies operating customer business processes in customer care, human re-source and finance. All machine learning components deployed in a given service solution should be customized to a new customer. This entails re-annotating data or calibrating the models in order to achieve a contractual performance. For example, in vehicle classi-fication, when an image classification algorithm is re-deployed in a new location, better performance is obtained by annotating samples collected on the field. In the brand sentiment management, when a sentiment classifier is re-deployed for a new customer, it is crit-ical to tune the classifier to the manner how users talk about their experience with a particular product, etc.

Numerous approaches have been proposed in the last years to address the text and visual domain adaptation to cite just a few [3, 6, 18, 19, 20, 22, 23, 24, 30, 32, 36]. The majority of existing methods makes an assumption of freely available source domain data. An equal access to both source and target data allows to mea-sure the discrepancy between their distributions and 1) either build representations common to both target and sources, or 2) directly reuse some source instances for a better target classification [43].
In reality, such a simplifying assumption rarely holds. Source data are often a subject of legal, technical and contractual con-straints between data owners and data customers. Beyond privacy and disclosure obligations, customers are often reluctant to share their data. When operating customer care, collected data may in-clude information on recent technical problems which is a highly sensitive topic that companies are not willing to share.
It is more common that just decision making procedures but not source domain data are available for adaptation. These pro-cedures are often presented in the form of classification, identifica-tion, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other cases, the owner of a source data is not allowed to share the entire dataset, but some representative source examples such as class means.

In this paper we address the domain adaptation problem in real world applications, where the reuse of source data is limited to classification rules or a few representative examples. We adopt the recent techniques of feature corruption and their marginalization , both in the supervised (the Marginalized Corrupted Features [31]) and unsupervised (stacked Marginalized Denoising Autoencoder [7]) frameworks. We propose several extensions to MCF ad sMDA methods in order to address the adaptation needs with no access to source domain data. We cope with three most typical cases of what is available for adaptation: 1) classifiers as a black box, 2) classifiers with known models and parameters, and 3) class repre-sentatives.

The remainder of the paper is organized as follows. In Section 2 we present several industrial applications on customer data with a critical need for domain adaptation. In Section 3 we revise the prior art and Section 4 presents how to extend MCF ad sMDA methods to address these adaptation needs, in both supervised and unsuper-vised settings. Publicly available datasets of Office and Amazon, as well on Xerox X  X  Smart Document Management data are described in Section 5.1. In Section 5, we describe the experimental setting and report the evaluation results of adapting the source classifiers and representatives on the selected datasets. Section 6 discusses the open questions and concludes the paper.
In this section, we present three real world applications with a strong domain adaptation need and no access to source domain in-stances.
Upon a strong demand for vehicle occupancy count (VOC) au-tomation in the US tolling and European border crossing, Xerox has developed Vehicle Passenger Detection System (VPDS) 1 Figure 1: VPDS annotation/classification interface at French-Swiss border.

The VPDS is a two-camera system that takes one photograph through the front windscreen of the car, and another one on the side of the car, looking into the back seat. Using the two photos, the system calculates whether the car contains just the driver, the driver plus one passenger, or the driver plus two or more passengers. The project started with one week of "training" the system on the field. Compliance with data protection law is ensured by applying non-reversible blurring techniques to the photographs to prevent facial and number plate recognition; and by retaining the photographs for no more than five days. The results proved the system to be 95% accurate in counting occupants.

The technical solution consists of the car detection, car location in the image, and the VOC classifiers. When the front and side cameras take a photo, the object detectors first localize the infor-mation zones in these images. In particular, the visor breeze to the front image and the rear window to the side image. The front and side classifiers then estimate probability of passenger presence in the car. The last module aggregates these probabilities into a final estimate of the vehicle occupancy rate. http://www.xerox.com/downloads/usa/en/bpo/casestudies/bpo_ casestudy_jougne.pdf
All classifiers are learned during the training period, when the image annotation tool (see Figure 1) is used to screen the images and to associate each car image with one of the labels, 1, 2 or "3 or more". The training of the algorithm includes annotating the image pairs and estimating optimum parameters to classify the new images.

When a new VPDS installation is requested, the "training" period and annotation cost is expected to be reduced by emerging domain adaptation techniques. As all images are destroyed after five days, the VOC classifiers are the only components to be reused from one VPDS installation to another.
EMPATH is a product developed by Xerox for customer analyt-ics 2 It aims to understand customers X  experiences, needs, and opin-ions in order to improve operations, products and services offered to customers. EMPATH starts by gathering data from a range of text sources (social media, news feeds, emails, chat and surveys). Collected data is then analyzed by an operator with the help of a text analytics engine. The operator tags one to two hundred posts, which are then used for training statistical models. This analysis helps operators better answer customer requests, to report upcom-ing issues and to provide valuable insights on product development.
In the text analytics engine, topic models, categories and senti-ment classifiers are treated as domain and customer dependent. For some domains (such as IT and mobile phone ), major concerns are similar and domain adaptation has been shown to reduce the anno-tation burden.

The main text analytics GUI is presented in Figure 2. Using the interface, the operator extracts topics from the text, creates cate-gories to detect coming issues and monitors customer satisfaction with the sentiment predictions. As customers disallow to share their data, operators face the problem of wasting their time and resource by starting the analytics processes from scratch. As certain com-ponents, topics, categories and sentiments are common to many brand customers, the adaptation of classifiers across domain and customers represents a strong opportunity for the EMPATH prod-uct.
As a part of Print and Document Management Services, Xerox offers to their customers facilities to automate and simplify the management of paper and electronic documents. The demateri-alization services and process automation offered by Xerox help http://www.wds.co/product/contact-center/social-media/ streamline client X  X  business processes to drive growth, and to re-duce costs.

As documents vary a lot by their origin, layout and content, there exists a strong need to evolve a workflow for the same client, as well as to redeploy the workflow for new clients, under all legal obligations.

In Section 5, document collections from three customers and evaluation results of domain adaptation are presented in more de-tails.
Domain adaptation for text data has been studied for more than a decade, with applications in statistical machine translation, opinion mining, part of speech tagging and document ranking [15, 32, 44]. Most effective techniques include feature replication [14, 15], pivot features [4, 32] and finding topic models shared by source and tar-get collections [9, 10]. Domain adaptation has equally received a lot of attention in computer vision [3, 6, 17, 18, 19, 20, 22, 23, 24, 30, 32, 36, 38] where domain shift is a consequence of changing conditions, such as background, location and pose, etc. and, more generally, a strong bias between datasets.

A considerable effort to systematize different domain adaptation and transfer learning techniques has been undertaken in [25, 34, 39]. These studies distinguish three main categories of domain adaptation methods. The first category aims at correcting sampling bias and this family of models gives more weight in the classifi-cation loss to instances more similar to the target collection [43]. The second category is in the line with multi-task learning where a common predictor is learned for all domains, which makes it ro-bust to domain shift [6]. The third family seeks to find common representation between source and target examples so that the clas-sification task is eased [33]. In a nutshell, models of this family aim at inferring domain invariant features . Finally, an important research direction deals with the theory of domain adaptation; it studies conditions and assumptions for which the adaptation can be effective and generalization bounds can be provided [2].
More recently, deep learning has been proposed as a generic so-lution to domain adaptation and transfer learning problems [11, 22, 29]. One successful method which aims to find common features between source and target collection relies on denoising autoen-coders . In deep learning, a denoising autoencoder is a one-layer neural network trained to reconstruct input data from partial ran-dom corruption [40]. The denoisers can be stacked into multi-layered architectures where the weights are fine-tuned with back-propagation. Alternatively, outputs of intermediate layers can be used as input features to other learning algorithms. This learned feature representation has been applied to domain adaptation [22], where stacked denoising autoencoders (SDA) achieved top perfor-mance in sentiment analysis tasks. The main drawback of SDAs is the long training time and Chen et al. [7] proposed a variation of SDA where the random corruption is marginalized out . This crucial step yields a unique optimal solution which is computed in closed form and eliminates therefore the back-propagation. In ad-dition, features learned with such an approach lead to classification accuracy comparable with SDAs, with a remarkable reduction of the training time [7, 42].

Several deep learning approaches have demonstrated their ability to learn robust features and that good transfer performance could be obtained by just fine tuning the neural network on the target task [11]. While such solutions perform relatively well on some tasks, the refinement may require a significant amount of new la-beled data. Recent works by Ganin et al. [20, 21] have proposed better strategies than fine tuning: adding a domain prediction task while learning the deep neural network leads to better domain-invariant feature representation. Their approach obtained a signif-icant performance gain which shows that transfer learning is not completely solved by fine tuning and that transfer tasks should be addressed by appropriate deep learning representations.

Nevertheless, the majority of domain adaptation methods makes an assumption of largely available source collections. Such as-sumption rarely holds in reality. The source instances may become unavailable for technical reasons, or are disallowed to store for le-gal and contractual reasons. When objects are to be removed from a database, it also makes privacy preserving machine learning meth-ods inapplicable [1, 5].
 The case of available source classifiers have been considered by Duan et al. in [17], where their contribution is limited to regular-izing the supervised target classifier. On the other hand, this case has been studied by domain adaptation theory [28]. Particularly, a theoretical analysis was conducted by considering the algorith-mic stability of source classifiers. It was shown that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out (LOO) error to the generalization error, thus en-abling the use of the LOO error to stop the adaptation learning. In case of unrelated domains, it also suggests how to prevent negative transfer, so that in the limit it does not hurt the performance in the target domain. We define a domain D as composed of a feature space X  X  R and a label space Y . Any given task in domain D (classification, regression, ranking, etc.) is defined by a function h : X  X  X  . In the domain adaptation setting, we assume working with n domains, including n S source domains D s ,s = 1 ,...,n S target domain D t . In conventional setting the source data X [ X 1 ,...,X n S ] , with the corresponding labels Y s = [ Y 1 are available. In the absence of source data, we assume to have at our disposal either 1) a set F of classifiers f k , k = 1 ,...,K , each being trained on source data where at least one classifier is available for source domain D s , or 2) a set of representatives for each D s , s = 1 ,...,n S .

A close analysis of target applications (see Section 2) reveals three important cases to consider; they reflect different situations and complexity levels, as follows: 1. Models and parameters  X  k of source classifiers f k  X  F are 2. Models of classifiers f k  X  F are unknown; they are used 3. In the target domain, we may have access to both labeled and
The domain adaptation problem then consists of leveraging the source labeled and target unlabeled data to derive a hypothesis per-forming well on the target domain. To achieve this goal, most DA methods compute covariance or correlation between features from source and target domains. With no or limited access to source data, we argue that the above principle can be extended to the correla-tion between target features and the source class decisions . We thus tune the adaptation trick by considering predicted class probabili-ties as augmented features for target data. Domain adaptation al-gorithms can then be used to find relations between features which were related in the source with features which seem also related in the target collection. In other words, we use the source classifiers as a pivot to transfer knowledge from source to target.

In the following section, we propose solutions to address the three cases mentioned above: 1. When source classifiers are known and they are linear, we ex-2. When source classifiers are used as a black box, we can 3. When class means are available as source representatives, we
In the following we will discuss the unsupervised setting where no target instance is labeled, X t l =  X  , and the supervised setting where a few target examples (for each class) are labeled. To address these problems, we adapt on one hand the Marginalized Corrupted Features that allows to address the problem when target labels are available and, on the other hand, the stacked Marginalized Denois-ing Autoencoder (sMDA) [7] framework to address the case when no labeled target instances are available.
First, we assume that a set of target instances were manually la-beled and we consider the Marginalized Corrupted Features frame-work to address our specific domain adaptation cases.

To marginalize the corrupted features in the MCF, a corrupting distribution is first defined to transform observations x into cor-rupted versions  X  x . The corrupting distribution is assumed to fac-torize over all feature dimensions and, each individual distribu-tion is assumed to be a member of the natural exponential fam-ily, p (  X  x | x ) = Q D d =1 P (  X  x d | x d ;  X  d ) , where x = ( x  X  ,d = 1 , ...,D is a parameter of the corrupting distribution on dimension d .

The corrupting distribution can be unbiased ( E [  X  x ] p (  X  x | x ) biased. Known examples of distribution P are the blankout [40], Gaussian, Laplace and Poisson noise [31]. For example, in the unbiased blankout noise, p (  X  x = 0) = q and p (  X  x = x with expectation E [  X  x ] = x and variance V ar [  X  x ] =
For the generalization of a classifier using corrupting distribu-tion P , the direct approach is to corrupt each element of the target training set D t = { ( x n ,y n ) } ,n = 1 ,...,N . The extended data set  X  D t can be used for training the model by minimizing where  X  x nm  X  p (  X  x nm | x n ) is the m -th corrupted observation of x n and  X  is the set of model parameters. As explicit corruption comes at a high computational cost, the marginalization trick is to consider the limiting case M  X   X  , where the weak law of large numbers is applied to rewrite 1 M P M m =1 L (  X  x its expectation E [ L (  X  x nm ,y n ;  X )] . It has been shown that dropout marginalization plays the role of regularization [41]. Classifiers can be trained efficiently with quadratic, exponential, logistic [31] and hinge loss [8], they generalize better and are more robust to missing feature values at test time. In this paper we focus on the quadratic loss which allows a closed form solution 3 .

Driven by our target applications in Section 2, we consider the multi-class case where label space Y includes C classes. We denote by y n the vector of size C , where the component c corresponding to the class label of x n is set to 1, and all others set to 0.
The expected value of the quadratic loss under corrupting distri-bution p (  X  x | x ) is given by 4
L ( D ;  X  ) = E [ P n k  X  &gt;  X  x n  X  y n k 2 ] where  X  is a D  X  C parameter matrix with column c seen as a linear classifier corresponding to the class c , and tr (  X  ) is the matrix trace. The optimal solution for  X   X  can be given in a closed form [31]
As the corruption distribution factorizes over all dimensions,  X  depends only on the expectations E [  X  x nd ] and variances V ar [  X  x over individual dimensions d . Hence, for the unbiased distribution P we can write  X   X  = Q  X  1 P , where P = and diag ( V ar [ x n ]) denotes the D  X  D diagonal matrix constructed with the variances V ar [ x n ] .
In the case of available source classifiers, the explicit corruption (1) takes into account their predictions on corrupted instances, as L (  X  x nm ,y n , F ;  X  ) .
 In the limiting case M  X  X  X  , we obtain the loss expectation where the corrupting distribution p (  X  x n | x n ) factorizes over d di-mensions.

Minimizing the expected loss value under the corruption model leads to learning with marginalized corrupted features and source predictors . The tractability of (5) depends on the choice of the loss function L and the corrupting distribution.

Let us denote the vector of all the predictions for a given cor-rupted target instance  X  x n by f (  X  x n ) and let  X  be the corresponding parameter matrix to be learned. The quadratic loss can then be written as
L ( D ;  X  ,  X  ) = P n E h  X  &gt;  X  x n +  X  &gt; f (  X  x where source predictions f (  X  x n ) can be seen as augmented cor-rupted features under the same distribution p (  X  x n | x
Other losses have been tested as well; the minimization with the exponential loss requires a gradient descent technique, the logistic and hinge losses are approximated by the upper bounds; they will be included in the extended version. For the notation simplicity, we omit subscript p (  X  x | x ) in The corresponding closed form solution can be written as: Linear source classifiers. If source classifiers are known to be linear functions (Case 1 in Section 4), we can marginalize them out jointly with the corrupted features. If we know that f ( x ) = A then we can estimate their expectation on a corrupted instance x , E ( f (  X  x )) = E ( A &gt;  X  x ) = A &gt; E (  X  x ) = f (
E [ f (  X  x n ) f (  X  x n ) &gt; ] = E [ A  X  x n  X  x &gt; Hence, we can derive that and P F = BP , where B denotes [ I D A ] .
 Algorithm 1 Marginalized corrupted features and source predic-tions (MCFA).
 Require: Labeled target dataset X t l  X  R N l  X  D with labels Y Require: Unlabeled target dataset X t u  X  R N u  X  D Require: Source classifiers f k  X  R D  X  [0 .. 1] C 1: Generate class predictions f ( x t n ) for labeled x t 2: if f k are known linear functions then 3: Estimate [  X   X  ;  X   X  ] by using (7) 4: else 5: Compose an augmented dataset with u t n = [ x t n ; f ( x 6: Estimate  X   X  ;  X   X  by using (4) with u t n . 7: end if 8: Generate class predictions f ( x t n ) for unlabeled x 9: Estimate class labels as y t = [  X   X  ;  X   X  ] &gt; [ x 10: Label each x t with c  X  = argmax c { y t c | y t } 11: return Labels for X t u Classifiers as a black box. If the classifier parameters are unknown or they are complex nonlinear functions, it is hard to directly esti-mate their expectation and variance. Used as black boxes, they can generate class prediction for target instances.

In such a case, we approximate (6) by considering any f k an additional feature for x n . We generate an augmented representa-tion of the target instance as u n = [ x n ,f 1 ( x n ) ,...,f stead of corrupting features  X  x n and marginalizing over known clas-sifiers f k , we assume that the corruption distribution P factorizes over augmented representation u n as p (  X  u | u ) = Q D + K In this case, (4) can be directly applied, where  X  x n is replaced by  X  u Algorithm 1 summarizes all steps of joint marginalization of cor-rupted features and source predictions (MCFA). Figure 3 schemati-cally shows the diagrams of training MCFA with known linear and black box classifiers.
 Figure 3: MCFA diagrams. Training with a) known linear clas-sifiers and b) black box classifiers.
The main drawback of supervised setting presented in the pre-vious section is that it requires target labels and does not exploit the unlabeled target instances X t u , which is generally much larger set than X t l . In this section we address the unsupervised domain adaptation case by marginalizing corrupted features in the denois-ing autoencoder framework.

The stacked Marginalized Denoising Autoencoder (sMDA) is a version of the multi-layer neural network trained to reconstruct in-put data from partial random corruption [40] proposed by Chen et al . [7] where the random corruption is marginalized out yielding the optimal reconstruction weights in the closed form.

The basic building block of the method is a one-layer linear de-noising autoencoder where a set of sample inputs x n are corrupted M times by random feature dropout with the probability p and re-constructed with a linear mapping W : R D  X  R D by minimizing the squared reconstruction loss 5 :
By applying the marginalization trick described for MCF, the mapping W can be expressed in closed form as W = E [ P ] E where
E [ Q ] ij = S ij q i q j , if i 6 = j, S where q = [1  X  p,..., 1  X  p, 1]  X  R D +1 , p is the dropout proba-bility, D is the feature dimension and S = XX T is the covariance matrix of the uncorrupted data X .

Stacking together several MDA layers can create a deep architec-ture where we feed the representations of the ( l  X  1) th layer as the input to the l th layer and learn the transformation W to reconstruct the previous output from its corrupted equivalent. In order to extend the mapping beyond a linear transformation, be-tween layers we apply on each output either a hyperbolic tangent function h l = tanh ( W l h l  X  1 ) or rectified linear units (RELU) h = max ( W t h l  X  1 , 0) . We will denote the final output h sponding to the input x n by  X  x n .

The main advantage of sMDA is that it does not require class labels; hence we can take advantage of the unlabeled target data and apply it for unsupervised domain adaptation. Indeed, [7] applied sMDA to the union of target and source datasets in order to learn domain invariant features.

In our case, X S is unavailable, we have either access to source classifiers F or a set of representative source examples. We are
A constant is added to the input, x n = [ x n ; 1] , and an appropriate bias incorporated within W which is is never corrupted. interested in one particular case, when the representative examples are the class means for each domain X s . Below we propose two different extensions of sMDA to these two cases. Class Means. If class means are available, they can be used both as classifiers and as data points. Used as classifiers, the class means can directly predict labels with a weighted softmax distance [13]. Used as representatives of source data, the class means can be con-catenated with the target data and fed to sMDA for a joint denois-ing. The denoised class means can then classify the target test instances [12]. These two ways of using the class means are de-scribed in what follows.

The DSCM [13] classifier (see Figure 4) generates the class means per domain as where N c d is the number of instances from class C c in the domain D . Then it can predict the class label for an unlabeled target in-stance using a weighted softmax distance to these domain-specific class means: where w d is the domain weight. In unsupervised domain adapta-tion, the DSCM make predictions with source class means  X  some target instances are labeled, the classifier can additionally use the target class means  X  t c .

The Adapted Class Means [12] classifier (ACM) is an extension of DSCM to unsupervised and semi-supervised settings. First, a sMDA is used to denoise the source class means. Instead of full source data X S , the source class means [  X  s 1 ,..., X  s with the unlabeled target data X t u are fed into the sMDA. The trans-formation matrix W obtained by (9) (or the set of stacked transfor-mation matrices W l ) is then used to jointly denoise the source class means  X   X  s c and the target examples  X  x t n . The denoised class means  X   X  are then used by the classifier to predict labels for denoised target data.

Algorithm 2 reports all steps for the case of a single source; the generalization to multiple sources is straightforward.
In the previous section we addressed the case of source repre-sentatives, available in the form of class means. Now we address the case of source classifiers available as a black box. They can be solely used for predicting labels, including for target domain in-stances. Such an unsupervised domain adaptation is of high interest Algorithm 2 Adapting Domain Specific Class Means (ACM).
 Require: A large set of unlabeled target dataset X t u and optionally Require: A set of source class means  X  s 1 ,... X  s C 1: Compute W using (9) with X = [  X  s 1 ,... X  s C , x t 1 2: (Optionally) repeat l times, while alternating with h 3: Decompose h l into reconstructed class means  X   X  s c and recon-4: if Labeled target examples then 6: else 8: end if 9: Label x t n with c  X  = argmax c p ( c | x t n ) 10: return Labels for X t u . for us, and we propose a Transductive Domain Adaptation (TDA) method which also relies on sMDA but uses it in the way different from the ACM.

Like in Section 4.1, we consider class predictions f k ( x evant but corrupted by the domain shift. Hence, we can exploit the correlation between the target data x n and source predictions f ( x t n ) to reconstruct (or denoise) both the target data and the source classifiers predictions of these target data. We apply the sMDA to the augmented dataset U t = [ u 1 ,... u N ] and compute W ; or stack several layers interleaved with nonlinear functions. We de-note by  X  u n the final output corresponding to the input u
In this case, we can directly use the reconstructed class predic-tions  X  f ( x t n ) to make the classification decisions. If the set F in-cludes a single source classifier f 1 with one prediction per class, the class with the maximum reconstructed value, c  X  = argmax c is assigned as label to x t n . In the case of multiple classifiers f F , the maximum of the averaged predictions should be considered. Algorithm 3 summarizes all the steps of this transductive version of domain adaptation.
 Algorithm 3 Transductive domain adaptation with mSDA (TDA). Require: Target dataset X t  X  R N  X  D without labels Require: Source classifiers f k  X  R D  X  [0 .. 1] C 1: Generate class predictions f ( x t n ) for all x t n  X  X 2: Compose an augmented dataset U with u t n = [ x t n ; f ( x 3: Use sMDA to estimate W = min W || U  X  W  X  U || 2 4: Get denoised class predictions for x t as y t = 5: Label x t with c  X  = argmax c { y t c | y t } . 6: return Labels for X t u
In this section we present a series of experiments to cope with domain adaptation tasks, along several important dimensions: 1. The amount of supervision in target domain; we consider un-2. Information available from the sources: source linear classi-3. When the classifier is a black box, we test different classifica-
We recall briefly all algorithms proposed in the previous sec-tions:
MCFA (Section 4.2) is primarily supervised domain adaptation
ACM (Section 4.4) assumes that class means are available from
TDA (Section 4.5) is the transductive algorithm to denoise source
The experiments are organized by the supervision type (unsu-pervised, supervised and semi-supervised); they aim to test perfor-mance of all algorithms on different datasets. Below we describe two image and one text datasets used in the experiments. XS3. This collection is a part of the Xerox X  X  Smart Document Management data; it contains printed/scanned documents from three Xerox customers denoted by C 1 ,C 2 and C 3 . The three datasets contain different types of documents such as invoices, contracts, IDs, filled forms, etc. All the documents have been grouped into 21 different classes, all well represented in each dataset (see examples in the Figure 5). Amongst these classes, some refer to generic con-cepts such as printed emails, computer programs, drawings, hand-written letters; some others ( contracts and filled forms ) form a more fine-grained class hierarchy, according to the customers X  require-ments. We manually aligned between the fine-grain classes have been manually aligned by their visual similarities as the correspon-dences is often not obvious.

The number of documents per class and per customer varies from 24 to 859; around 2.5K documents are available for each cus-tomer. As image representations we use deep convolutional acti-vation features [16]. These features are obtained using publicly available Caffe CNN models 6 [27] trained on the 1000 classes of ImageNet [35] and fine-tuned for document images on the RVL-CDIP document image dataset 7 [26]. In our experiments we use the fully connected layer (caffe_fc6) of dimensionality 4096.
Due to the confidentiality clauses on X3S collection, we also run experiments on two publicly available benchmark datasets:
OFF31. One of the most popular datasets for comparing visual domain adaptation methods is the Office31 collection [36]. It con-sists of four domains: Amazon ( A ), Caltech ( C ), dslr ( D ) and We-bcam ( W ) with images of 31 products (classes). Each domain is considered in its turn as a target , with the other domains consid-ered as sources . For the target set we select 3 instances per class to https://github.com/BVLC/caffe/ http://scs.ryerson.ca/~aharley/rvl-cdip/ Figure 5: Examples from the XC3 datasets. Each line corre-sponds to one customer and each column refers to one doc-ument class (documents are intentionally blurred for privacy reasons). form the training set, all other instances form the test set. We use the caffe_fc6 deep convolutional activation features [16] we obtain with the publicly available VGGNET (16 layer) CNN model [37] with no fine-tuning.

AMT. The Amazon text dataset consists of products reviews in four domains: Books ( b ), Dvd ( d ), Kitchen ( k ) and Electronics ( e ). While a book review can be quite different from a kitchen item review, there are nevertheless some common features to as-sess whether the users were satisfied with their purchase. A part of this collection was preprocessed by Blitzer et al. [3] and used subsequently in several domain adaptation studies. The task is to predict whether a customer review is positive or negative, where a review with more than 3 stars is considered as positive and (strictly) less than 3 as negative. Documents are represented by a bag of uni-grams and bigrams. In our experiments, we only considered the top 10,000 most-frequent features for each domain. In all experiments, we use the whole source and target collections whereas other stud-ies use a predefined split of 2000 documents per domain.
Finally, we list four classification models tested as source classi-fiers:
DSCM classifier [13] uses a softmax distance to the class specific
MCF [31] is a multi-class linear classifier with dropout noise
LR refers to a l 2 regularized Logistic Regression, cross-validated
SVM is used with the RBF kernel, two main hyperparameters C
In this setting, the baseline consists in classifying target data without any domain adaptation. We compare the performance of the unsupervised ACM with available class means to the TDA with the black box classifier. http://scikit-learn.org/stable/index.html Table 1: Unsupervised adaptation in XS3 and OFF31 datasets with DSCM and TDA methods. Underline indicates improve-ment over the baseline and bold indicates the best performance per task.
Table 1 reports results for 6 domain adaptation tasks in XS3 and 6 tasks in OFF31 dataset. Each line indicates an adaptation task S  X  T , from source S to target T . Column 1, sMDA  X  , refers to the dream case of fully available source data; it shows classification accuracy of sMDA on full source and target data. Columns 3 to 5 correspond to cases where the source classifier f s is a DSCM classifier. Column 3 reports the baseline accuracy when the source classifier is applied on the target test without adaptation, f Columns 4 (ACM) and 5 (TDA) show the results of applying the respective domain adaptation. Both methods in general outperform the baseline and achieve the accuracy values comparable with the dream case with full access to source data, and in general ACM outperforms TDA, showing that it is more interesting to adapt the class means (if available) than the softmax scores we obtain with. Finally, columns 6 and 7 show the classification accuracy when the source classifier is the MCF classifier and the source class means are unavailable. The TDA method outperforms the baseline MCF in most cases except the adaptation between D and W , that can be explained by the fact we have almost no domain shift between these two domains.

Table 2 shows the performance of the TDA model on the 12 adaptation tasks of the AMT dataset. We include accuracy values for MCF and LR source classifiers, but exclude results for DSCM and ACM as the class means classifier performs poorly on text data. On average, the LR classifier performs better. Compared to the baseline f s ( X t ) and the dream case sMDA  X  , TDA always improves over the baseline and is often close to the dream case.
To sum up, domain adaptation with TDA is successful when the classification predictions are obtained with DSCM, MCF or LR. When we have access to the class means, adapting them with ACM seems to be a better strategy for image datasets. For the text dataset, TDA combined with LR predictions performs the best. In all cases, the performance of TDA and ACM is close to the dream case.
We assume now that some of the target instances are labeled, but no unlabeled target instances are given at the training time. For each adaptation task, there are two baselines . The first one consists in training the classifier on the labeled target instances; the second Table 2: Unsupervised adaptation in AMT dataset with TDA method and MCF/LR source classifiers. Underline indicates improvement over the baseline and bold indicates the best per-formance per task. one relies on the logistic regression model 9 trained on the concate-nation of the raw target features and the source predictions. Both baselines are shown in italic in the tables. We run experiments with different source classifiers (MCF, SVM, DSCM) and evaluate the two proposed MCF extensions: MCFAl and MCFAb.
 Table 3 presents evaluation results on XS3 and OFF31 datasets. Each column in the table shows the accuracy of the target classifica-tion using linear classifiers (Column 1) and the black box classifiers (Columns 2 to 4). For all tasks, the first supervised baseline is de-noted as T  X  T (for ex., C 1  X  C 1 ), it refers to learning from labeled target data only. The second baseline is reported in the last column, it uses LR as the target classifier ( f t ) that is trained on the target features concatenated with the source classifier predictions obtained with DSCM ( f s ).

We can observe that both MCF extensions outperform the base-line LR trained on the concatenated features. Given the small amount of labeled data, the MCF is more appropriate to handle the noise and variability in data, which makes it better suited for domain adaptation and transfer tasks. We can see that MCFAb with SVM or DSCM predictions provide even better results and outperform both baselines; this witnesses a successful adaptation independently of which classifier is used in the black-box ( f s ).

We have also conducted experiments on the AMT dataset where 10 target labels per class were used to compare several source clas-sifiers. We do not include results here but report the main findings. MCF shows a better classification performance than LR when us-ing the target features only (similarly to the image datasets). How-ever, when the target data get combined with the source predictions, the LR baseline obtains better results than MCFAb. Adding the source classifier X  X  predictions to the original target features brings an improvement over the supervised T  X  T baseline, for both LR and MCFA. However, due to a small amount of target labels, the T  X  T baseline is worse than the S  X  T one, and none approach is able to outperform the results obtained with the source classifier X  X  predictions ( f s ).

On the other hand, as shown in Table 2, combining the source predictions with the unlabeled target data outperforms significantly the results obtained with source predictions f s . It points out that using the source predictions and the unsupervised denoising with TDA is a much better option than using any supervised method
Experiments with SVM give very similar results. Table 3: Supervised adaptation in XS3 and OFF31 datasets with MCFA, using 3 labels per class. Underline indicates im-provement over both baselines and bold indicates the best per-formance per task. including the baselines. This clearly demonstrates the importance of having access to the unlabeled target data. It shows that the unsupervised domain adaptation can be highly beneficial for the accuracy gain and can considerably reduce the annotation cost. Lastly, we consider here a transductive semi-supervised setting. We assume that a large set of unlabeled target set X t u is available at the training time. The supervised baseline which uses only tar-get labeled data and our algorithm MCFAb are used as baselines. They are compared to the semi-supervised version of ACM, which assumes available source class means. Despite this requirement, it is still interesting to compare these approaches to understand which classifiers can be more amenable to transfer.

Table 4 shows the evaluation results for 12 domain adaptation tasks in XS3 and OFF31 datasets. For each adaptation task, the supervised baseline is denoted T  X  T ; it trains a classifier on target labeled data only. Column 2 in the table reports the results of DSCM classifier in the supervised setting. Column 3 indicates the performance of sMDA with a DSCM classifier with full ac-cess to source data . The sMDA results help us measure the per-formance or adaptation methods under the limited or no access to the source data. Column 4 indicates the performance of the ACM model and Column 5 reports the supervised baseline MCFAB. The table reveals two important findings. First, the semi-supervised ACM brings additional gains comparing to the baselines. But the most important finding is that methods without access to source data (ACM, MFCAb) achieve performances close to the methods with access to source data (sMDA+DSCM, Column 2). This indi-cates that the classifier contains enough information for adaptation and upholds the result in unsupervised adaptation.
In this paper we address the problem of domain adaptation in real world applications, where the reuse of source data is limited, Table 4: Semi-Supervised adaptation in XS3 and OFF31 datasets. due to legal and contractual obligations, to classification rules or a few representative examples. We adapt the recent techniques of feature corruption and their marginalization developed for the su-pervised (MCF) and unsupervised (sMDA) settings. We propose several extensions to MCF and sMDA methods in order to address cases when the source classifiers are available either as a black box or with known parameters, or a few class representatives are from a source domain. We conduct a series of experiments on two public and one in-house datasets in supervised and unsupervised settings. We present the evaluation results and show that significant perfor-mance gains can be achieved despite the absence of source data and shortage of labeled target data. This result is an highly encourag-ing for a range of applications with machine learning components, whose adaptation to new domains and customers is expected to be low and without sacrificing the stated performance. [1] R. Agrawal and R. Srikant. Privacy-preserving data mining. [2] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. [3] J. Blitzer, S. Kakade, and D. P. Foster. Domain adaptation [4] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation [5] K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic [6] M. Chen, K. Q. Weinberger, and J. Blitzer. Co-training for [7] M. Chen, Z. Xu, K. Q. Weinberger, and F. Sha. Marginalized [8] N. Chen, J. Zhu, J. Chen, and B. Zhang. Dropout training for [9] Z. Chen and B. Liu. Topic modeling using topics from many [10] Z. Chen, A. Mukherjee, B. Liu, M. Hsu, M. Castellanos, and [11] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep [12] G. Csurka, B. Chidlovskii, and S. Clinchant. Adapted domain [13] G. Csurka, B. Chidlovskii, and F. perronnin. Domain [14] H. Daum X . Frustratingly easy domain adaptation. CoRR , [15] H. Daume III and D. Marcu. Domain adaptation for [16] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, [17] L. Duan, I. W. Tsang, D. Xu, and T.-S. Chua. Domain [18] L. Duan, D. Xu, and S.-F. Chang. Exploiting web images for [19] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. [20] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation [21] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, [22] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for [23] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow [24] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for [25] R. Gopalan, R. Li, V. M. Patel, and R. Chellappa. Domain [26] A. W. Harley, A. Ufkes, and K. G. Derpanis. Evaluation of [27] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet [28] I. Kuzborskij and F. Orabona. Stability and hypothesis [29] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning [30] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer [31] L. v. d. Maaten, M. Chen, S. Tyree, and K. Weinberger. [32] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen. [33] S. J. Pan, J. T. Tsang, Ivor W.and Kwok, and Q. Yang. [34] S. J. Pan and Q. Yang. A survey on transfer learning. [35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, [36] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting [37] K. Simonyan and A. Zisserman. Very deep convolutional [38] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy [39] S. S. Sun, H. Shi, and Y. Wu. A survey of multi-source [40] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. [41] S. Wager, S. I. Wang, and P. Liang. Dropout training as [42] Z. Xu, M. Chen, K. Q. Weinberger, and F. Sha. From sBoW [43] Z. Xu and S. Sun. Multi-source transfer learning with [44] M. Zhou and K. C. Chang. Unifying learning to rank and
