 and their references), it still remains an active research a rea. non-negative weights w convergence and correctness of algorithms like max-produc t. converge.
 b edges touching node i (setting all b provide proofs only for the conceptually easier case of simp le matchings where b graphs, the LP relaxation is always tight.
 Suppose that we are given a graph G with weights w to any node i is at most b all b Here E problem is to replace the constraint x which all x program IP . Associate a binary variable x following probability distribution: which contains a factor  X  ( x P of p , and e to refer both to the edges of G and variables of p . The factor  X  ( x E for any x , p ( x ) = exp( P sets A and B the set difference is denoted by the notation A  X  B . Max-Product for Weighted Matching (INIT) Set t = 0 and initialize each message to be uniform. (ITER) Iteratively compute new messages until convergence as foll ows: (ESTIM) Upon convergence, output estimate  X  x : for each edge set  X  x Remark: If the degree | E this step easy even for large degrees: for each edge e  X  E if all b The simplification for general b is as follows: let F with the largest values of a These updates are linear in the degree | E The Computation Tree for Weighted Matching we now describe for the special case of simple matching ( b T T its neighbors in G , except for the neighbor that is already present in T edges in T Suppose M is a matching on the original graph G , and T of illustrate the ideas of this section with a simple example. the center plot we show T which is the image of M  X  onto T that any matching on T we depict M , the max-weight matching on the tree T that n 4 can lead to incorrect beliefs, and estimates. Theorem 1 Let G = ( V, E ) be a graph with nonnegative real weights w holds: proofs are presented for the conceptually easier case of sim ple matchings, in which all b such that the maximizing assignment arg max Proof of Part 1: Max-Product is as Powerful as LP Relaxation below as DUAL .
 ex. 4.20]).
 is the optimal matching, then there exists an optimal dual so lution z to DUAL such that incorrect, i.e n t Recall that n t (b) M is a max-weight matching on T then add all edges adjacent to e that are in M  X  maintain the alternating structure, or a leaf of T because M and M  X  it in each of the two matchings.
 For illustration, consider Example 1 of section 3. M  X  We now show that w ( P  X  M  X   X  above. Suppose first that neither endpoint of P is a leaf of T Lemma 1 it follows that On the other hand, from part 2 of Lemma 1 it follows that Now by construction the root e  X  P  X  S , and hence w ( P  X  M  X  these cases we would need to make explicit use of the fact that t  X  2 w max We now show that M cannot be an optimal matching in T P to obtain a matching with higher weight. Specifically, let M 0 = M  X  ( P  X  M ) + ( P  X  M  X  the choice of M , and shows that for e /  X  M  X  the beliefs satisfy n t max-product converges globally to the correct M  X  .
 Proof of Part 2: LP Relaxation is as Powerful as Max-Product max-product does not converge. As a first step we have the foll owing lemma. Lemma 2 If Max-Product converges, the resulting estimate is M  X  . edge weights satisfy
Example: On the right is a bad blossom: bold edges are in M  X  , numbers are edge weights and alphabets are node labels. Cycle
C in this case is abcdu , and path R is cf ghi . 3 cycles C weights satisfy Example: On the right is a bad dumbbell.

Cycles C ( c, f ) is the path R . 3 dumbbell.
 appendix submitted along with the paper).
 blossom B that near the root e , M will be the image of M  X  onto T than | V | from the root, e 0  X  M if and only if its copy in G is in M  X  . This means that the copies in T a new matching M 0 on T ( P  X  M ) . Then, it is easy to see that By assumption B proposed in [7]. Sparse network obtained by b -matching with b = 5 . bound on optimal cost obtained using LP relaxation and max-p roduct. We set p = 3 for concreteness, and let the edge weights be w be as N increases, nodes have increasingly higher degrees.
 N = 100 averaged over 100 trials. subgraph got disconnected twice over 500 trials.
 exceeds 99 percent.

