 H.3.3 [ Information Systems ]: Information Search and Retrieval  X  retrieval models, search process Algorithms, Design Tag Clouds, Automated Annot ation, Word Clustering Traditional search engines, based on exact keyword match, return too many documents in response to a user query, and most of the returned documents are irrelevant. Very often the user simply doesn't know how to formulate the query in a way that expresses his intention. Furthermore, as observed by the authors of Scatter/Gather [1], users may not only search documents but also browse through the collection to discover the general information interfaces for formulating advanced queries, and demand a fast response time. To overc ome this problem, major search engines take a lot of trouble to provide the user with an intuitive interface to: Several methods based on Document Clustering [1, 2, 8], Faceted by the Blog community, are used to satisfy these needs. Google Labs Suggestion [4], Yahoo! Search Assistant [11] or Clusty remix clustering [10] are exampl es of this kind of interface. In this paper, we describe TopicRank, a Word Clustering based approach that automatically and dynamically generates an interactive Tag Cloud related to the user query where the layout of presented keywords relies on a semantic closeness metric. Used in this way, in contrast to [6], we found that Tag Clouds are both an efficient navigational tool an d a good tool for understanding abstract information. As in SHOC [2], the TopicRank approach meets the Semantic and Online Clustering requirements but doesn't require the clustering algorithm to produce a Hierarchical output, and the clustering is done on words rather than documents. TopicRank focuses primarily on producing semantically related clusters of words but does not try to name clusters, thus mitigating the difficulty of the automatic labeling problem that faces the Document Clustering approaches. The proposed algorithm follows the following 3 steps: For a given query 'K' provided by the user, the query is submitted to our search engine, which returns a list of documents from which we extracts the potential keyword candidates. For each keyword 'w' from the returned top D (we set D=300) documents, the following 'relatedness measure' , denoted 'TR' (TopicRank), is calculated: where df(K,w) denotes the numbe r of documents where  X  X  X  and  X  X  X  appear together, df(w) de notes the number of documents where keyword  X  X  X  appears and p(w) denotes probability of keyword  X  X  X .  X  and  X  are two weighting factors. We set  X  = 3 and  X  = 2. In many approaches, Mutual In formation or Information Gain serves as the base for building a similarity/distance measure to evaluate how two words are related from their co-occurrence, but this simple 'relatedness measure' (which is neither a distance nor a similarity measure, as it doesn't have symmetrical and transitivity properties) performs extraordinar ily well and returns a very good set of keywords related to the query. The first part of TopicRank number of times a word  X  X  X  app ears with the given keyword  X  X  X  vs. the number of times it appears al one. It can be also view as an approximation of the conditional probability: This quantity has the drawback of overestimating the importance of 'rare' keywords, where the extreme case is when a keyword appears only one time (df(w) =1) at the same time as the given keyword 'K' (df(K,w) = 1). As in the traditional IR 'bag-of-words' model, where the local term freque ncy (tf) part is moderated by the global Invert Document Fr equency (IDF), we smooth the overestimating effect of the first part by the entropy of the word w Because entropy(p=0) = entropy(p=1) = 0, the entropy eliminates words with too small or too high Document Frequency (df). clustering algorithm. Document Vector Representa tion. For a given set of N words TopicRank between words i w and j w as defined in 2.1 We then define a similarity measure between two words vector features as: With this Word Vector Representation, we apply the Complete Link Clustering algorithm which tends to produce good clusters of relatively similar size. Because of the relatively low dimension (N=100), running even a O(N^3) clustering algorithm is done in less than 1 second on a standard notebook. remains of how to rank keywords within a cluster, and how to rank the clusters. For the ranking of keywords within a cluster, we clusters themselves we attribute to a cluster the average TopicRank of the keywords it contains. We have created a prototype to demonstrate the effectiveness of TopicRank. A corpus of news has been crawled from major Korean newspapers, and divided into 8 categories. For each day of the year 2007 and for each category we collected about 25 articles so the total corpus size is around 80,000 articles, which after html stripping represents about 224 MB of pure text. We only indexed nouns extracted from the morphological analyzer without making any stop-word removal. Both title and content have been used equally without any particular weighting scheme. Figure 1 shows the output for the query  X  X  X  X  X  (Samsung). The (baseball, soccer...), another to Renault Samsung including other automobile brands, another to Samsung Electronics including cell phone related companies or brands. result of a survey of 100 users asking for the usability, usefulness more than 80% of them were very satisfied. We have proposed an efficient interface and fully automatic algorithm doing online clustering of words to return dynamically highly semantic related topics to a user query, helping the user to method presents a number of advantages: -it can be applied to any search engine and any collection of texts -it does not need any particular external resources like stop-words or user tags explanation as to why they are related, and in future work we plan to use lightweight ontologies to enrich the user experience. [1] D. R. Cutting, D. R. Karger, J. O. Pederson, and J. W. Tukey [2] Dell Zhang and Yisheng Dong. 2001 Semantic, Hierarchical, [3] G. Begelman and P. Keller and F. Smadja 2006 Automated [4] Google LABS Suggest: [5] Hassan-Montero, Y., Herrer o-Solana, V. 2006 Improving [6] Hearst M. 2006 Clustering vers us Faceted Categories for [7] Hearst, M.A., and Rosner, D. 2008 Tag Clouds: Data [8] H.-J. Zeng, Q.-C. He, Z. Ch en, W.-Y. Ma and J. Ma. 2004 [9] Ido Dagan, Shaul Marcus, et al. 1995 Contextual Word [10] Vivisimo Inc. http://vivisimo.com/ [11] Yahoo! Inc. http://yahoo.com Figure 1. Result from TopicRank for the query  X   X  X  X   X . 
