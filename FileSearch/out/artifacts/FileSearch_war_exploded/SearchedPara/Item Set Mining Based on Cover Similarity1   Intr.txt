 Frequent item set mining and association rule induction are among the most intensely studied topics in data mining and knowledge discovery in databases. The enormous research effo rts devoted to these tasks have led to a variety of sophisticated and efficient algorithms, among the best-known of which are Apri-ori [1], Eclat [27,28] and FP-growth [13]. However, these approaches, which find item sets whose support exceeds a user-s pecified minimum in a given transac-tion database, have the disadvantage that the support does not say much about the actual strength of association of t he items in the set: a set of items may be frequent simply because its elements are frequent and thus their frequent co-occurrence can even be expected by chance. As a consequence, the (usually few) interesting item sets drown in a sea of irrelevant ones.

In order to improve this situation, we propose in this paper to change the selection criterion, so that fewer irre levant items sets are produced. For this we draw on the insight that for associat ed items their covers X  X hat is, the set of transactions containing them X  X re more similar than for independent items. Starting from the Jaccard index to illustrate this idea, we explore a total of twelve specific similarity measures that can be generalized from pairs of sets (or, equivalently, from pairs of binary vectors) as well as a generalized form. By applying an Eclat-based mining algorithm to standard benchmark data sets and to the 2008/2009 Wikipedia Selection for schools, we demonstrate that the search times are bearable and that high quality item sets are produced. Frequent item set mining was originally developed for market basket analysis, aiming at finding regularities in the shopping behavior of the customers of su-permarkets, mail-order companies and online shops. Formally, we are given a set B of items , called the item base , and a database T of transactions .Eachitem represents a product, and the item base represents the set of all products on offer. The term item set refers to any subset of the item base B . Each transac-tion is an item set and represents a set of products that has been bought by an actual customer. Note that two or even more customers may have bought the exact same set of products. Note also that the item base B is usually not given explicitly, but only implicitly as the union of all transactions.

We write T =( t 1 ,...,t n ) for a database with n transactions, thus distinguish-ing equal transactions by their positio n in the vector. In order to refer to the index set, we introduce the abbreviation IN n := { k  X  IN | k  X  n } = { 1 ,...,n } . Given an item set I  X  B and a transaction database T ,the cover K T ( I )of I w.r.t. T is defined as K T ( I )= { k  X  IN n | I  X  t k } , that is, as the set of indices of transactions that contain I .The support s T ( I )ofanitemset I  X  B is the num-Given a user-specified minimum support s min  X  IN , a n i t e m s e t I is called fre-quent in T iff s T ( I )  X  s min . The goal of frequent item set mining is to identify all item sets I  X  B that are frequent in a given transaction database T .
A standard approach to find all frequent item sets w.r.t. a given database T and a support threshold s min , which is adopted by basically all frequent item set mining algorithms (except those of the Apriori family), is a depth-first search in the subset lattice of the item base B . Viewed properly, this approach can be interpreted as a simple divide-and-conquer scheme. All subproblems that occur in this scheme can be defined by a conditional transaction database and a prefix . The prefix is a set of items that has to be added to all frequent item sets that are discovered in the conditional database, from which all items in the prefix have been removed. Formally, all subproblems are tuples S =( T C ,P ), where T C is a conditional transaction database and P  X  B is a prefix. The initial problem, with which the recursion is started, is S =( T,  X  ), where T is the given transaction database to mine and the prefix is empty. A subproblem S 0 =( T 0 ,P 0 )isprocessedasfollows:Chooseanitem i set of items occurring in T 0 . This choice is arbitrary, but usually follows some predefined order of the items. If s T 0 ( i )  X  s min , then report the item set P 0  X  X  i } as frequent with the support s T 0 ( i ), and form the subproblem S 1 =( T 1 ,P 1 )with P in T 0 that contain the item i , but with the item i removed. This also implies that transactions that cont ain no other item than i are entirely removed: no empty transactions are ever kept. If T 1 is not empty, process S 1 recursively. In any case (that is, regardless of whether s T 0 ( i )  X  s min or not), form the subproblem S 2 =( T 2 ,P 2 ), where P 2 = P 0 and the conditional transaction database T 2 comprises all transactions in T 0 (including those that do not contain the item i ), but again with the item i removed. If T 2 is not empty, process S 2 recursively. We base our item set mining approach on the similarity of item covers rather than on item set support. In order to measure the similarity of a set of item covers, we start with the Jaccard index [16], which is a well-known statistic for comparing sets. For two arbitrary sets A and B it is defined as J ( A, B )= | A  X  B | / | A  X  B | . Obviously, J ( A, B ) is 1 if the sets coincide (i.e. A = B ) and 0 if they are disjoint (i.e. A  X  B =  X  ). For overlapping sets its value lies between 0 and 1. The core idea of using the Jaccard index for item set mining lies in the insight that the covers of (positively) associat ed items are likely to have a high Jaccard index, while a low Jaccard index indicates independent or even negatively a ssociated items. However, since we consider also item sets with more than two items, we need a generalization to more than two sets (here: item covers). In order to achieve this, we define the carrier L T ( I )ofanitemset I w.r.t. a transaction database T as The extent r T ( I )ofanitemset I w.r.t. a transaction database T is the size of its carrier, that is, r T ( I )= | L T ( I ) | . Together with the notions of cover and support (see above), we can define the generalized Jaccard index of an item set I w.r.t. a transaction database T as its support divided by its extent, that is, as Clearly, this is a very natural and straightforward generalization of the Jaccard index. Since for an arbitrary item a  X  B it is obviously K T ( I  X  X  a } )  X  K T ( I ) and equally obviously L T ( I  X  X  a } )  X  L T ( I ), we have s T ( I  X  X  a } )  X  s T ( I )and r thus that the generalized Jaccard i ndex w.r.t. a transaction database T over an item base B is an anti-monotone function on the partially ordered set (2 B ,  X  ). Given a user-specified minimum Jaccard value J min ,anitemset I is called Jaccard-frequent if J T ( I )  X  J min . The goal of Jaccard item set mining is to identify all item sets that are Jaccard-f requent in a given transaction database T . Since the generalized Jaccard index is anti-monotone, this task can be addressed with the same basic scheme as the task of frequent item set mining. The only problem to be solved is to find an effici ent scheme for computing the extent r T ( I ). Since we will draw on the basic scheme of the well-known Eclat algorithm for mining Jaccard item sets, we briefly revie w some of its core ideas. Eclat [27] uses a purely vertical representation of conditional transaction databases, that is, it uses lists of transaction indices, which repres ent the cover of an item or an item set. which allows to extend an item set by an item. This is used in the recursive divide-and-conquer scheme described abov e by intersecting the list of transaction indices associated with the split item wi th the lists of transaction indices of all items that have not yet been co nsidered in the recursion.

An alternative to the intersection approach, which is particularly useful for mining dense transaction databases, relies on so-called difference sets (or diffsets for short) [28]. The diffset D T ( a | I )ofanitem a w.r.t. an item set I and a transaction database T is defined as D T ( a | I )= K T ( I )  X  K T ( I  X  X  a } ). That is, a diffset D T ( a | I ) lists the indices of all transactions that contain I , but not a .Since s T ( I  X  X  a } )= s T ( I )  X  X  D T ( a | I ) | , diffsets are equally effective for finding frequent item sets, provided one can derive a formula that allows to compute diffsets with a larger conditional item set I without going through covers (using the above definition of a diffset). However, this is easily achieved, because D T ( b | I  X  X  a } )= D T ( b | I )  X  D T ( a | I ) [28]. This formula allows to formulate the search entirely with the help of diffsets. The diffset approach as it was reviewed in the previous section can easily be transferred in order to find an effi cient scheme for computing the carrier and thus the extent of item sets. To this end we define the extra set E T ( a | I )as That is, E T ( a | I ) is the set of indices of all transactions that contain a , but no item in I , and thus identifies the extra transaction indices that have to be added to the carrier if item a is added to the item set I . For extra sets we have E
T ( a formula for diffsets reviewed above. This relation is easily verified as follows: In order to see how extra sets can be used to compute the extent of item sets, let I = { i 1 ,...,i m } , with some arbitrary, but fixed order of the items that is indicated by the index. This will be the order in which the items are used as split items in the recursive div ide-and-conquer scheme. It is and since the terms of the last union are clearly all disjoint, we have immediately Thus we have a simple recursive scheme to compute the extent of an item set from its parent in the search tree (as defined by the divide-and-conquer scheme).
The mining algorithm can now easily be implemented as follows: initially we create a vertical represe ntation of the given transaction database. The only difference to the Eclat algorithm is that we have two transaction lists per item i : one represents K T ( { i } ) and the other E T ( i | X  ), which happens to be equal to K which, however, will obviously not be maintained in the recursive processing.) In the recursion the first list for the split item is intersected with the first list of all other items to form the list representing the cover of the corresponding pair. The second list of the split item is subtracted from the second lists of all other items, thus yielding the extra sets of transactions for these items given the split item. From the sizes of the resulting lists the support and the extent of the enlarged item sets and thus their gene ralized Jaccard index can be computed. Up to now we focused on the (generalized ) Jaccard index to measure the simi-larity of sets (covers). However, there is a large number of alternatives. Recent extensive overviews for the pairwise case include [5] and [6].

The JIM algorithm (as presented above) allows us to easily compute the quantities listed in Table 1. With these quantities a wide range of similarity measures for sets or binary vectors can be generalized. Exceptions are those measures that refer explicitly to the number of cases in which a vector x is 1 while the other vector y is 0, and distinguish this number from the number of cases in which y is 1 and x is 0. This distinction is difficult to generalize beyond the pairwise case, because the number of possible assignments of zeros and ones to the different vectors, each of which one would have to consider for a generalization, grows exponentially with the number of these vectors (here: covers, and thus: items) and theref ore becomes quickly infeasible.

By collecting from [6] similarity meas ures that are specified in terms of the quantities listed in Table 1, we compiled Table 2. Note that the index T and the argument I are omitted to make the formulas more easily readable. Note also that the Hamann measure S H = x + z  X  s n = n  X  2 s n [14] listed in [6] is equivalent to the Sokal&amp;Michener measure S M , because S H +1=2 S M , and hence omitted. Likewise, the second Baro ni-Urbani&amp;Buser measure S U = is equivalent to the one given in Table 2, because S U +1 =2 S B . Finally, note that all of the measures listed in Table 2 have range [0 , 1] except S K (Kulczynski) and S O (Sokal&amp;Sneath 3), which have range [0 ,  X  ).

Table 2 is split into two parts depending on whether the numerator of a measure refers only to the support s or to both the support s and the number z of transactions that do not contain any of the items in the considered set. The former are often referred to as based on the inner product, because in the pairwise case s is the value of the inner (or scalar) product of the binary vectors that are compared. The latter measures (that is, those referring to both s and z ) are referred to as based on the Hamming distance, because in the pairwise case q is the Hamming distance of the two vectors and n  X  q = s + z their Hamming similarity. The decision whether for a given application the term z should be considered in the numerator of a similarity measure or not is difficult. Discussions of this issue for the pairwise case can be found in [22] and [9].

Note that the Russel&amp; Rao measure is simply normalized support, demon-strating that our framework comprises standard frequent item set mining as a special case. The Sokal&amp; Michener measure is simply the normalized Hamming similarity. The Dice/S X rensen/Czekanowski measure may be defined without the factor 2 in the numerator, changing the range to [0 , 0 . 5]. The Faith measure is equivalent to the AZZOO measure (alter zero zero one one) for  X  =0 . 5and the Sokal&amp;Michener measure results for  X  = 1. AZZOO is meant to introduce flexibility in how much weight should be placed on z , the number of transactions which lack all items in I (zero zero) relative to s (one one).

All measures listed in Table 2 are anti-monotone on the partially ordered set (2 B ,  X  ), where B is the underlying item base. This is obvious if in at least one of the formulas given for a measure the numerator is (a multiple of) a constant or anti-monotone quantity or a (weighted) sum of such quantities, and the numerator is (a multiple of) a constant or monotone quantity or a (weighted) sum of such quantities (see Table 1). This is the case for all but S D , S N and S B . That S D is anti-monotone can be seen by considering its reciprocal value S monotone and thus S D is anti-monotone. Applying the same approach to S B , we arrive at S  X  1 B = both s and Since q is monotone, the numerator is monotone, and since n is constant and s and z are anti-monotone, the denominator is anti-monotone. Hence the fraction is monotone and since it is subtracted from 1, S N is anti-monotone.
 Note that all measures in Table 2 can be expressed as by specifying appropriate coefficients c 0 ,...,c 7 . For example, we obtain S J for c 0 = c 6 =1, c 5 = Similarly, we obtain S O for c 0 = c 1 = c 6 =1, c 4 = c 5 =  X  1and c 2 = c 3 = c 7 =0, various similarity measures. Note, howeve r, that not all selections of coefficients lead to an anti-monotone measure and hence one has to carefully check this property before using a measure that differs from the pre-specified ones. We implemented the described item set mining approach as a C program that was derived from an Eclat implementation by adding the second transaction identifier list for computing the extent of item sets. All similarity measures listed in Table 2 are included as well as the general form (1). This implementation has been made publicly available under the GNU Lesser (Library) Public License. 1
In a first set of experiments we applied the program to five standard bench-mark data sets, which exhibit different c haracteristics, a nd compared it to a standard Eclat search. We used BMS-Webview-1 (a web click stream from a leg-care company that no longer exist s, which has been used in the KDD cup 2000 [17]), T10I4D100K (an artificial data set generated with IBM X  X  data gen-erator [29]), census (a data set derived f rom an extract of the US census bureau data of 1994, which was preprocessed by discretizing numeric attributes), chess (a data set listing chess end game positions for king vs. king and rook), and mushroom (a data set describing poisonous and edible mushrooms by different attributes). The first two data sets are available in the FIMI repository [11], the last three in the UCI machine learning repository [2]. The discretization of the numeric attributes in the census dat a set was done with a shell/gawk script that can be found on the web page given in footnote 1 (previous page). For the experiments we used an Intel Core 2 Quad Q9650 (3GHz) machine with 8 GB main memory running Ubuntu Linux 10.4 (64 bit) and gcc version 4.4.3.
The goal of these experiments was to determine how much the computation of the carrier/extent of an item set aff ected the execution time. Therefore we ran the JIM algorithm with J min = 0, using only a minimum support threshold. As a consequence, JIM and Eclat always found exactly the same set of frequent item sets and any difference in execution time comes from the additional costs of the carrier/extent computation. In addition, we checked which item order (ascending or descending w.r.t. their frequency) yields the shortest search times.
The results are depicted in the diagrams in Figure 1. We observe that pro-cessing the items in increasing order of frequency always works better for Eclat (black and grey curves) X  X s expected. F or JIM, however, the best order depends on the data set: on census, BMS-Webview-1 and T10I4D100K descending order is better (red curve is lower than blue), on chess ascending order is better (blue curve is lower than red), while on mushroom it depends on the minimum support which order yields the shorter ti me (red curve intersects blue).

We interpret these findings as follows: for the support computation (which is all Eclat does) it is better to process the items in ascending order, because this reduces the average length of the transaction id lists. By intersecting with short lists early, the lists processed in t he recursion tend to be shorter and thus are processed faster. However, for the extent computation the opposite order is preferable. Since it works on extra sets, i t is advantageous to add frequent items as early as possible to the carrier, because this increases the size of the already covered carrier and thus reduces the average length of the extra lists. Therefore, since there are different preferences, i t depends on the data set which operation governs the complexity and thus which item order is better.

From Figure 1 we conjecture that dense data sets (high fraction of ones in a bit matrix representation), like chess and mushroom, favor ascending order, while sparse data sets, like census, BMS-Webview-1 and T10I4D100K, favor descending order. This is plausible, because in dense data sets intersection lists tend to be long, so it is important to reduce them. In sparse data sets, however, extra lists tend to be long, so here it is more important to focus on them.
Naturally, the execution times of JIM are always greater than those of the corresponding Eclat runs (with the same order of the items), but the execution times are still bearable. This shows that even if one does not use a similarity measure to prune the search, this additional information can be computed fairly efficiently. However, it should be kept in mind that the idea of the approach is to set a threshold for the similarity measur e, which can effectively prune the search, so that the actual execution times found in applications are much lower. In our own practice we basically always achiev ed execution times that were lower than for the Eclat algorithm (but, of course, with a different output). In another experiment we used an extract from the 2008/2009 Wikipedia Selection for schools 2 , which consisted of 4861 web pages. Each of these web pages was taken as a transaction and processed with standard text processing methods (name detection, stemming, sto p word removal etc. )toextractatotal of 59330 terms/keywords. The terms occurring on a web page are the items occurring in the corresponding transaction. The resulting data file was then mined for Jaccard item sets with a threshold of J min =0 . 1. Some examples of found term associations are listed in Table 3.

Clearly, there are several term sets wit h surprisingly high Jaccard indices and thus strongly associated terms. For example,  X  X eptiles X  and  X  X nsects X  always appear together (on a total of 12 web pages) and never alone. A closer inspection revealed, however, that this is an artifact of the name detection, which extracts these terms from the Wikipedia category title  X  X nsects, Reptiles and Fish X  (but somehow treats  X  X ish X  not as a name, but as a normal word). All other item sets contain normal terms, though (only  X  X rand Slam X  is another name), and are no artifacts of the text processing st ep. The second item set captures several biology pages, which describe different v ertebrates, all of which belong to the phylum  X  X hordata X  and the kingdom  X  X nimalia X . The third set indicates that this selection contains a surprisingly high number of pages referring to magnolias. The remaining item sets show that term sets with five or even six terms can exhibit a quite high Jaccard index, even though they have a fairly low support.
An impression of the filtering power can be obtained by comparing the size of the output to standard frequent item set mining: for s min =10thereare 83130 frequent item sets and 19394 closed item sets with at least two items. A threshold of J min =0 . 1 for the (generalized) Jaccard index reduces the output to 5116 (frequent) item sets. From manual inspection, we gathered the impression that the Jaccard item sets contained mor e meaningful sets and that the Jaccard index was a valuable additional piece of information. It has to be conceded, though, that whether item sets are more  X  X eaningful X  or  X  X nteresting X  is difficult to assess, because this requires an objective measure, which is not available.
However, the usefulness of our method is indirectly supported by a successful application of the Jaccard item set min ing approach for concept detection, for which standard frequent item set mining did not yield sufficiently good results. This was carried out in the EU FP7 project BISON 3 and is reported in [18]. We introduced the notion of a Jaccard item set as an item set for which the (generalized) Jacca rd index of its item covers exceed s a user-specified threshold. In addition, we extended this basic idea to a total of twelve similarity measures for sets or binary vectors, all of which can be generalized in the same way and can be shown to be anti-monotone. By exploiting an idea that is similar to the difference set approach for the well-known Eclat algorithm, we derived an efficient search scheme that is based on fo rming intersections and differences of sets of transaction indices in order to compute the quantities that are needed to compute the similarity measures. Since it contains standard frequent item set mining as a special case, mining item sets based on cover similarity yields a flexible and versatile framework. Furthermore, the similarity measures provide highly useful additional assessments of found item sets and thus help us to select the interesting ones. By running experiments on standard benchmark data sets we showed that mining item sets based on cover similarity can be done fairly efficiently, and by evaluating the results obtained with a threshold for the cover similarity measure we demonstrated that the output is considerably reduced, while expressive and meaning ful item sets are preserved.
 This work was supported by the European Commission under the 7th Framework Program FP7-ICT-2007-C FET-Open, contract no. BISON-211898.

