 Open-domain Question Answering (QA) systems are expected to response exact most existing QA systems consist of the following components: question analysis, information retrieval, and answer extraction. Question analysis accounts for the processes of analyzing a given question, extracting keywords, and determining the question type. The process of determining the type of a given question is usually called question classification. According to error analysis about QA systems [1], 36.4% of the errors were generated by the question classification. Thus, in this paper, we focus on classification for a Chinese QA system. 
Traditional regular expression methods using a set of handcrafted patterns are time (untrained) patterns. To improve such drawbacks, Li [2] propose a statistical language model (LM) to enhance the performance of question classification. However, the LM approach only using a small number of questions as training data (about 700 questions in Li X  X  report) suffers from the problems of insufficient training data. Recently, a few approaches to classifying factoid questions perform well by using effective machine learning techniques [3] like support vector machines (SVM) [4], [5], [6], [7]. However, these approaches using machine learning techniques heavily depend on the while facing various real questions from the Web users. In fact, it is still difficult to collect sufficient training questions because most questions currently available is generated manually. 
Brill et al. (2002) take advantage of the Web as a tremendous data resource and employ simple information extraction techniques for question answering [8]. Ravichandran and Hovy (2002) tried to utilize Web search results to learn surface patterns for finding correct question answers [9]. Solorio et al. and others improve the performance of question classification based on the numbers of the returned search results by submitting keywords in questions in combination with all the possible question types [5]. 
Different from the aforementioned works, in this paper, we present a simple learning method that explores Web search results to collect more training data (sentences) automatically by a few seed terms (question answers) of each question type. In addition, we propose a novel semantically related feature model (SRFM), which takes advantage of question focus (QF) [10] and semantically related features learned from better performance than the bigram LM approach while mitigating the problem of insufficient training data. 2.1 Problems According to our analyses on the 400 Chinese questions from the task of NTCIR-5 1 Cross-Language Question Answering, some interrogative (question) words can be used to determine question type directly. For examples, a Chinese question with a interrogative word  X   X  (who) can be classified as question type PERSON, and  X   X  (where) can be used to determine question type LOCATION. However, some interrogative words can not be used to determine question type, such as  X  /  X  (which),  X   X  (what), because these interrogative words always result in ambiguities of question types. Two examples are shown in Table 1. Both questions contain the interrogative word  X   X  but have different question types: the first one is type PERSON, and the other type LOCATION. (QF), that can be used to disambiguate the sense of the interrogative words and determine the type of a question. For the examples in Table 1, the QF of the question in the first example is  X  (company), and the QF of the question in the second example is  X  know the QF in a new question, the type of the question may not be determined. Chinese Question English Translation Interrogative Word  X   X  (which)  X   X  (which) Question Type ORGANIZATION LOCATION 
Chinese Question English Translation Which writer won the Nobel Prize in Literature in 2001? Question Focus (QF) (writer) Related Verb (RV) (won) Related Quantifier (RQ) (a Chinese quantifier, no English equivalent) 
Related Noun (RN) (Prize in Literature) 2.2 Ideas information. Our idea is that first, some words which have semantically related information to QFs in questions may support the lacking QF information like the verbs, quantifiers, and nouns in a question. We call these words related verbs (RV), related features may be easier to be learned using our proposed SRFM based on the abundant training data from Web search results (described in Section 3.3). In Table 2, if we didn X  X  learn the word  X   X  (writer) which belongs to type PERSON, then we can use the three semantically related features: the RV  X   X  (won), the RN  X   X  (Prize in Literature), and the RQ  X   X  (a Chinese quantifier unit for type PERSON, no English equivalent) to easily obtain the information of type PERSON for determining the type of this question. It is quite possible that they can be trained in advance if these verbs, nouns, and quantifiers frequently co-occurred with those name entities labeled by type PERSON in Chinese texts. stage, we use a few basic classification rules to handle the simple questions without the ambiguous interrogative words. Second, the LM or our proposed SRFM is used to deal following, we describe the basic classification rules, the LM and the SRFM in details. 3.1 Basic Classification Rules classification rules for four types of questions. These basic rules are shown in Table 3. The first question  X  ?  X  (Who is the Finland's first woman president?) can be easily determined as the type PERSON according to its interrogative word  X   X  (who). However, these basic rules are too simple to classify many complicated questions correctly as mentioned in Section 2, whereas the complicated questions are able to be handled by the language model or our proposed semantically related feature model. Who is the Finland's first woman president? Original 
Quesiton 
Question Original 
Quesiton 
Question 3.2 Language Model To handle the problems of classifying some complicated questions, we initially refer to the language modeling approach proposed by Li (2002). The major advantage of select the best possible C with the highest probability. Here we construct the unigram and bigram language models for each question type C of training questions: and 3.3 Learning Question Focus and Semantically Related Features We try to use a different training method of only using a certain type of few seed terms and explore Web search results to obtain the same performance. 3.3.1 Collecting Training Data from Web Search Results features from Web search results. 1. Collect a few seed terms (e.g. question answers) for each question type. (For 2. Submit each seed term to search engines (e.g., Google), and then retrieve a number 3. Extract the sentences containing the seed term, and then use the CKIP tagger to label 4. Extract the terms labeled with the POS tag: verbs, quantifiers, and nouns, and then 3.3.2 Question Focus Identification The QF of a question provides important information to determine the type of a certain of interrogative words, e.g., (what), (what), (which), (which), (what), (is), (is). Our algorithm of question focus identification is described below: Input: Question Output: Question focus 1. Use the CKIP tagger to obtain the POS tags of all words in a question. 2. Determine whether a question contains an interrogative word, if it does, then go to 3. Seek a word with the tag  X  X a X  or  X  X c X  following the interrogative word, and take it 4. Seek a word with the tag  X  X a X  or  X  X c X  preceding the end of the question until the 
Please note that during the identification process, if two consecutive words with the same tag  X  X a X  or  X  X c X , we identify the latter word as the QF. If the first one is  X  X a X , and the second one is  X  X c X , then we identify the word with  X  X a X  to be the QF. If the first word is  X  X c X , and the second one is  X  X a X , then the word with  X  X a X  is considered the QF. Chinese Question EnglishTranslation Who was the prime suspect behind the 911 attack? 
Tagged Question Interrogative Word (Which) Question Focus (person) Chinese Question ? EnglishTranslation What is the name of the world's second largest desert? 
Tagged Question Interrogative Word (Which) Question Focus (desert) 
In Table 5, we can find the term  X   X  (person) tagged with  X  X a X  following the interrogative  X   X  (which),, thus it can be identified as the QF. Table 6 shows that the term  X   X  (desert) with the tag  X  X a X  can be identified as the QF by seeking a noun preceding the interrogative word  X   X  (which). 3.3.3 Semantically Related Feature Model To mitigate the problems of lacking sufficient training questions in LM approach, we intend to exploit the abundant Web search results to learn more QFs frequently occurring in questions and their semantically related features, and then train an effective model for question classification. Fig. 2 illustrates our SRFM. We assume that some QFs and semantically related features in question Q possibly occur in Web search results frequently and thus is easier to be trained. first, each retrieved training search results D C can be generated under the constraint of according to the distribution P ( Q | D C , C ). 
The semantically related feature model generates the probability of the query P ( Q | C ) over all training texts related to question type C . The model is formulated as: 
To make the model of Equation (3) tractable, we assume that the probability P ( D C | C ) following: 
We assume that C is determined based on the two kinds of important features in Q : the For convenience of training parameters, we assume these features are mutually independent in this initial work. Therefore, Equation (4) can be decomposed as follows: 
We substitute Equation (5) into Equation (3), and then get the final form: Finally, we consider that Q Therefore, we may give QFs and each kind of semantically related features different weights in the parameter estimation. 4.1 Data Sets The data set of 680 Chinese questions used in this work are collected from two sources: 400 Chinese questions from the NTCIR-5 CLQA Task and 280 translated Chinese questions for a few rare question types from TREC 3 2005, 2004, 2002, and 2001 English questions. They are classified into six different question types: PERSON, LOCATION, ORGANIZATION, NUMBER, DATE, and ARTIFACT. We use the evaluation technique of four-fold cross-validation which divides the whole question set into four equally-sized subsets and perform four different experiments. For evaluation considered the training set. Table 7 shows the distribution of six question types in the results and learning QFs and semantically related features (see Section 3.3.1). 4.2 Results We conduct the following experiments to realize the effectiveness of our proposed SRFM and compare classification performance with the LM. Basically, these two easily learn reliable probabilities for QFs of questions. However, for the unseen number of Web search results can utilize the semantically related features (related verbs, effectiveness of semantically related features to support the lack of QFs, we thus divided the testing data into three testing classes: the first class, Unneeded_Question_Focus, is that test questions can be directly classified according the basic classification rules, such as the questions containing interrogative words  X   X  (who) or  X   X  (where) (see Section 3.1). The second class, Trained_Question_Focus, is that the QFs of test questions have been trained by the LM. The third class, Untrained_Question_Focus, is accuracy for the testing class Unneeded_Question_Focus is about 98%. Then, the second and third testing classes are particularly used to compare classification performance for these two models. Some preliminary results are shown in Table 9. For the testing class Trained_Question_Focus, the LM (word_unigram) (71%) performs better than the SRFM (QF, RV, RQ, RN) (59%). The major reason might be that the training questions are suitable to train a good bigram LM, but currently our collected training data may Untrained_Question_Focus, we can see that the SRFM (QF, RV, RQ, RN) (54%) performs better than the LM (word_unigram) (49%). It shows that our hypothesis of supporting untrained QFs with the related verbs, related quantifiers, and related nouns our SRFM performs better than the LM for types LOCATION and ORGANIZATION, but is worse than the LM for types PERSON and ARTIFACT. PERSON 11/19=58% 12/19=63% 7/13=54% 8/13=62% LOCATION 53/85=62% 68/85=80% 10/14=71% 6/14=43% ORGANIZATION 37/53=70% 33/53=62% 17/27=63% 12/27=44% NUMBER 2/2=100% 0/2=0% 2/4=50% 2/4=50% ARTIFACT 19/41=46% 27/41=66% 38/79=48% 39/79=49% Total 127/216=59% 153/216=71% 74/137=54% 67/137=49% Table 11 shows some examples of question classification in the testing class Trained_Question_Focus to explain why some classifications are effective based on the LM or SRFM. For the first test question, the LM is effective since the QF  X   X  (country) has been trained on the training questions, but the SRFM is ineffective because the QF  X   X  (country) was trained among different question types, and there are not enough semantically related features in this short question to support the QF. For the second question, the SRFM is still ineffective. Although the QF  X   X  (time) has been trained by the SRFM, the semantically related feature  X   X  (shooting), which has not been trained in type DATE but trained in other question types, causes effective due to that the QF  X   X  (company) and the semantically related feature the RV  X   X  (provide) is closely related to type ORGANIZATION, and has been trained by the SRFM. For the last question (with long length), the QF  X   X  (amount of money) and the RV  X   X  (pay out) have been trained by the SRFM and thus make the classification succeed, whereas the LM suffers from a difficult problem that is caused by too much noise on long questions. 
In Table 12, some results of question classification are demonstrated for the testing class Untrained_Question_Focus. The first question contains the QF  X   X (river), the RV  X   X  (renovate) and the RQ  X   X  (no English equivalent), which are related to type LOCATION, and trained by the SRFM, therefore it is classified correctly. On the questions. For the second question, the QF  X   X  (company) has been trained by the SRFM, but the QF was also not trained by the LM. However, the third question with the 
LOCATION 
DATE 
ORGANIZATION 
NUMBER QF  X   X (doctor) trained by the SRFM is classified incorrectly because the QF and some semantically related features have also been trained in other question types and cause incorrect noise. Oppositely, the QF has not been trained by the LM, but because some of the semantically related features in this question have been trained, thus the question is classified correctly using the LM. 
LOCATION 
ORGANIZATION 
PERSON 4.3 Discussions According to the above analyses on our experimental results, we can see that the LM is effective to determine question type for the questions with trained QFs from the training questions. However, the LM is not appropriate to handle the classification of questions with untrained QFs, but our proposed SRFM performs better than the LM by employing the QFs and their semantically related features trained from Web search results. In fact, we think that these two models are complementary, and it is worthy to investigate an integrated technique to improve both models in the future. We have presented a novel approach to learning QFs and semantically related features starting with a few seed question answers, and to extract QFs and semantically related features from the collected training data. Our contribution in this work is to mitigate the problems of lacking sufficient training data while training a classifier by using machine learning techniques. Our experimental results show that for the testing questions with untrained QFs using our proposed SRFM, the classification accuracy can be increased from 49% (LM) to 54%. collected Web search results might be exploited to train better LMs and SVM classifiers, or even semantically related features would be beneficial to the SVM classifiers which always require good features for training. We are trying to propose a good bootstrapping technique to effectively collect more and more training data from the Web for various question types, and some new methods to improve our SRFM. Acknowledgments. We would like to thank Yi-Che Chan for his support to collect remarks and suggestions. 
