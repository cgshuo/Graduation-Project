 Subgroup discovery is a learning task that aims at nd-ing interesting rules from classi ed examples. The search is guided by a utility function, trading o the coverage of rules against their statistical unusualness. One shortcoming of existing approaches is that they do not incorporate prior knowledge. To this end a novel generic sampling strategy is proposed. It allows to turn pattern mining into an iterative process. In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns. The re-sult of this technique is a small diverse set of understandable rules that characterise a speci ed property of interest. As another contribution this article derives a simple connection between subgroup discovery and classi er induction. For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of strati ed resampling. The proposed techniques are empirically compared to state of the art sub-group discovery algorithms.
 H.2.8 [ Database Applications ]: Data Mining; I.2.6 [ Learning ]: Induction Algorithms, Performance subgroup discovery, sampling, prior knowledge
The discipline of Knowledge Discovery in Databases is about nding useful and novel patterns, hidden in huge amounts of real-world data. Common problems are that the applied Data Mining techniques either nd an unman-ageable number of patterns, e.g. frequent itemsets, or that they are limited to nding \obvious" patterns only, which are already known to domain experts.

This work presents an approach towards mining interest-ing patterns sequentially. The most obvious patterns may either be elicited from domain experts beforehand, or they may be the result of a rst application of Data Mining tools. The main question that arises in an iterative Data Mining framework is how to pre-process the data, so that subse-quent steps do not report previously found patterns again, but focus on uncorrelated new patterns.

The interestingness of patterns is a crucial notion when formalising this task. A basic assumption underlying this work is that patterns are interesting to the degree to which they deviate from the user's expectation. Hence, a straight-forward heuristic for rule interestingness is the degree of deviation from the user's speci ed domain knowledge and from previously found patterns.

For simplicity this work con nes itself to probabilistic rules as the representation language. The main ideas from the literature on subgroup discovery are adopted, but ex-tended to respecting prior knowledge. In subgroup discovery the interestingness of rules is evaluated by a utility function. This function can be regarded as a user speci ed parame-ter of the learning task. The goal of subgroup discovery is to identify subsets of the population that show an unusu-ally high frequency of a speci ed property of interest. The task of characterising groups of car drivers with an unusu-ally high risk of accidents is considered as an intuitive toy example. Assuming that the default probability, computed from all registered drivers, is about 1% per year, suciently large subgroups having a risk of 5% might be interesting. This illustrates a major di erence to classi cation, where rules are only useful if they predict the most probable class.
In this work subgroups are identi ed with their corre-sponding population in the database at hand, rather than with their syntactical descriptions. This makes a di erence, since many databases allow to identify the same or similar populations using correlated attributes, but these correla-tions are often not subject to the Data Mining step. For instance the subgroup of young drivers is almost identical to the subgroup of people that recently acquired their driv-ing license. If the former subgroup is known to have a high risk of accidents then the same is expected for the latter. Hence, there is no need to report both rules.

The presented iterative Data Mining procedure will iden-tify a new subgroup in each iteration, favouring subgroups that are new with respect to formalised prior knowledge and all previously found patterns.
The remainder of this paper is organised as follows: Af-ter the formal framework and basic de nitions are given in section 2, existing work on subgroup discovery is described in section 3. As the main contribution a generic sampling technique to incorporate prior knowledge into subgroup dis-covery is presented in section 4. An algorithm operational-ising this idea is presented in section 5, which is empirically evaluated in section 6. Section 7 concludes the paper.
Two di erent learning tasks are subject to this paper, subgroup discovery and classi er induction. Both tasks are supervised, so the learning step is based on a classi ed sam-ple. Examples are de ned as classi ed elements of an in-stance space X , assumed to be sampled i.i.d with respect to a distribution D : X ! IR + . To simplify formal aspects X is assumed to be nite, although all results are easily gen-eralised to continuous domains. The probability to observe an instance x 2 X under D is denoted as Pr x D ( x ). The probability to observe an instance from a subset W X is denoted as Pr D [ W ]. If the underlying distribution is clear from the context the subscripts are omitted. Each example is assigned a label from Y , the set of all possible labels by the target function C : X ! Y . This work considers only supervised learning, unless noted otherwise with a boolean target attribute Y = f 0 ; 1 g . C is assumed to be xed but unknown to the learner, whose task is to nd a good approx-imation. For subgroup discovery, the main learning task in this work, Horn logic rules are the main representation lan-guage for patterns.

De nition 1. A Horn logic rule consists of a body A, which is a conjunction of atoms over attributes describing X ,and ahead B , predicting a value for the target attribute. It is notated as A ! B . If the body evaluates to true the rule is said to be applicable , if the head also evaluates to true it is called correct .
 The focus of this work lies on propositional logic, so the bod-ies of rules are conjunctions of attribute-value pairs. Rules can be identi ed with the subset they apply for, de ned by an indicator function h : X !f 0 ; 1 g , and the label they pre-dict. To ease notation the following abbreviations are used whenever Y = f 0 ; 1 g : Using this notation, the Horn logic rules predicting a boolean target are of the form h ! Y + and h ! Y  X  . The former rule states that the target class is positive ( Y + )iftherule is applicable ( h ), the latter that it is negative in this case.
As illustrated by the example of road accidents, rules are not expected to match the data exactly. It is sucient if they point to interesting regularities in the data, for exam-ple that the subgroup of young drivers faces an unusually high risk of accidents. Hence, the intended semantics is that the conditional probability Pr ( Y + j h )(or Pr ( Y higher than the class prior P ( Y + )(or P ( Y  X  )). Probabilistic rules are often annotated by their corresponding conditional probabilities: h ! Y + [5%] : , Pr x D [ C ( x )=1 j h ( x )=1]=5% For now any form of prior knowledge is assumed to be rep-resented by rules of this form. Subsection 4.3 extends the presented techniques to more general forms of prior knowl-edge.

Performance metrics are functions that heuristically as-sign a utility score to each rule under consideration. For the notion of rule interestingness di erent formalisations have been proposed in the literature (e.g. [24]). In this work in-terestingness is considered equal to unexpectedness. The following paragraphs discuss a few of the most important metrics for rule selection.

The goal when training classi ers is to select a predictive model that separates positive and negative examples accu-rately.
 De nition 2. The accuracy of rule A ! B is de ned as
De nition 3. The precision of a rule is de ned as its prob-ability of being correct, given that it is applicable: Subgroup discovery has a di erent focus. Rules are inter-esting only, if for the covered subset the target attribute's distribution deviates from the default distribution.
De nition 4. The Bias of a rule is the di erence between conditional and default probability (prior) of the target: The following metric has been used to measure interest in the domain of frequent itemset mining [3]. In the supervised context it measures the change in the target attribute's fre-quency for the subset covered by a rule.
 De nition 5. For any rule A ! B the Lift is de ned as The Lift is the multiplicative counterpart to the Bias ,and is often more convenient in the scope of this work. Both metrics capture the value of \knowing" the prediction for estimating the probability of the target attribute. For in-dependent events A and B we have Lift ( A ! B )=1and Bias ( A ! B ) = 0, for positively correlated events A and we have Lift ( A ! B ) &gt; 1and Bias ( A ! B ) &gt; 0. In the latter case the conditional probability of B given A is higher than the default probability Pr [ B ].

If h characterises the subgroup of young drivers, Y + de-notes the event of having an accident, and if Pr [ Y + ]=1% is the default probability of this event, then the above rule h ! Y + [5%] has a Bias of 4%, which is the absolute dif-ference between conditional and default probability, and a Lift of 5, reflecting the increase in terms of a relative factor.
Knowing the Lift of rules allows to combine them in a simple way to predict the conditional probability of a tar-get class. If a new attribute is de ned according to the prediction of each rule, then predictions can be combined by means of classi er induction techniques. The underly-ing assumption of Na  X  ve Bayes [11] is that all attributes are conditionally independent given the class. These classi ers work surprisingly well in practice, often even if the under-lying assumption is known to be violated. When mining rules iteratively, using the sampling technique proposed in section 4, the conditional independence assumption is not as unrealistic as one might expect. The reason is that all correlations \reported" by previously found patterns are \re-moved" from subsequently constructed samples.

Let f h i ! Y + =  X  j 1 i n g denote a set of rules, h h 1 ;:::;h n i ( x ) the corresponding vector ^ y = h ^ y 1 ^ y 2 Y n , of predictions. Then for a given example x 2 X and class Y c the Na  X  ve Bayes classi er estimates = = De ning allows to rewrite this term to For b o olean Y it is easier to consider the odds as ( x ) cancels out, but it is still possible to recalculate based on eqn. (1). So following the conditional indepen-dence assumption it is possible to combine rules to predict class probabilities, just knowing their Lift and the class pri-ors. Eqn. (1) has an intuitive interpretation. The rst fac-tor reflects the positive-negative ratio, which is 1 = 99 for the prior Pr [ Y + ]=1%. The Lift -ratios specify how to update these positive-negative ratios in relative terms, if it is known whether a speci c rule is applicable or not. If the driver un-der consideration is young, then there is a known factor as-sociated to the corresponding rule, which allows to update the previously computed ratio, increasing it from 1 = 99 to 5 = 95. The same holds for further, subsequently discovered rules, each of which may be annotated by an empirically estimated Lift -ratio.

It is not necessary to restrict rules to the case in which they are applicable. Please note that but the precisions of both rules may di er. This is easily seen considering an example. If, compared to the average driver, young drivers have a higher risk of accidents, then the average risk of the remaining drivers has to be lower, since removing a subgroup with higher risk always decreases the overall risk. So each rule ( h ! Y + =  X  ) should rather be con-sidered to partition the instance space into h and h ,making a prediction for both subsets. As a consequence any two rules overlap. Thus, for any known degree of overlap be-tween a rule R 1 that is part of the prior knowledge and a rule candidate R 2 under consideration, there is an expec-tation for Lift ( R 1 ) based on Lift ( R 2 ). This expectation reflects the assumption that R 2 does not introduce a Lift of its own, but simply shares a biased subset with R 1 .Ifthis assumption is met, then the rule candidate is redundant and should be ranked low. The Lift of a rule should change in the presence of prior knowledge. A corresponding technique is introduced in section 4.
Subgroup discovery aims at nding interesting subsets of the instance space that deviate from the overall distribution. Di erent search strategies led to several algorithms, which are briefly described in subsection 3.1. In all cases the search is guided by a utility function , a speci c type of rule selection metric, which can be regarded as a parameter of the learning task itself. Choosing this function carefully allows to direct the search towards di erent kinds of interesting rules, e.g. rules having a lower bias but a higher coverage compared to those found via standard classi er induction. For a speci c utility function some recently proposed subgroup discovery algorithms are limited to, there is a simple way to transform the corresponding formal Data Mining problem into a clas-si er induction problem. This transformation is presented in subsection 4.4 and it will be used later on to evaluate the proposed techniques empirically. The bene ts of incor-porating prior knowledge into subgroup discovery and some existing approaches are discussed in subsection 3.2, before the generic knowledge-based sampling approach is presented in section 4.
The common goal of all subgroup discovery strategies is to nd interesting and novel patterns in datasets. To this end utility functions are used that formalise a trade-o between the size of the subgroup and its unusualness in terms of a target attribute's observed frequency. Each subgroup is represented by a Horn logic rule. A popular utility function is the weighted relative accuracy [13]. It is used for subgroup discovery since EXPLORA [12], and it is the default utility function of MIDOS [27].

De nition 6. The weighted relative accuracy ( WRAcc ) of a rule A ! B multiplies coverage ( Pr [ A ]) with bias: Several other functions have been suggested in the litera-ture [12], basically putting more emphasis on either coverage or bias. This work only makes use of the commonly used WRAcc metric for mainly two reasons. First of all it allows the underlying formal Data Mining problem to be tackled with approved rule induction algorithms (subsection 4.4), which simpli es the evaluation in practice. The second rea-son is that it is sucient to compare the presented approach to the reweighting schemes that have recently been proposed in the scope of subgroup discovery [14].

There are two di erent strategies of searching for interest-ing rules: exhaustive and heuristic search. EXPLORA [12] and MIDOS [27] tackle subgroup discovery by exhaustively evaluating the set of rule candidates. The set of rules are ordered by generality, which allows to prune large parts of the search space. The advantage of this strategy is that it allows to nd the n best subgroups reliably. Finding sub-groups on subsamples of the original data is a straightfor-ward method to speed up the search process. As shown in [22] most of the utility functions commonly used for sub-group discovery are well suited to be combined with adaptive sampling. This sampling technique reads examples sequen-tially, continuously updating upper bounds for the sample errors, based on the data read so far. In this way, the re-quired sample size allowing to give a probabilistic guarantee of not missing any of the n best subgroups can be reduced.
Heuristic search strategies are fast, but do not come with any guarantee to nd the most interesting patterns. One recent example implementing a heuristic search is a variant of
CN2 . By adapting its rule selection metric to WRAcc the well known CN2 classi er has been turned into CN2-SD [14]. As a second modi cation the iterative cover approach of CN2 has been replaced by a heuristic weighting scheme. Example weights are either changed by a constant factor or by an additive term each time the example has been cov-ered by a rule. In section 4 a new generic weighting scheme is proposed that allows to overcome some shortcomings of CN2-SD .
A drawback of classical subgroup discovery lies in a lack of expressiveness. Especially interesting exceptions to rules are hard to be detected using standard techniques, for mainly two reasons. First of all, due to the syntactical structure imposed by Horn logic it is often hard to exclude excep-tions from rules, even if this improves the score assigned by the utility function. The syntactical bias is important, however, because results are required to be understandable and because it is the main reason for diversity within the n best subgroups. The syntactical bias might not be su-cient to avoid sets of similar rules. Redundancy lters are a common technique to overcome this problem [12]. Overlap-ping patterns like exceptions to rules are not found reliably that way. Exceptions could still be represented by separate rules. This fails for the second reason, namely that utility functions evaluate rules globally. Interactions between rules do not a ect their scores.

For the task of nding exception rules some ecient al-gorithms have been developed [25]. They focus on mining pairs of a strong rule and a corresponding exception, which is too speci c for subgroup discovery in general. Subgroups do not necessarily have exceptions, and they may overlap in arbitrary ways.

As a strategy for pruning rulesets to cover di erent as-pects, ROC analysis was suggested in [14]. According to the false positive and false negative rates all rules are plot-ted in ROC space [5]. Only rules lying on the convex hull are deemed relevant and may be turned into a single classi er by weighted majority vote. A major drawback of this lter is that it systematically discards one of two rules covering dis-joint subsets and having almost the same performance. As soon as one of these rules is superior in both true positive and false negative rates the other rule is considered to be redundant. This is not desirable in descriptive scenarios, as the only rule covering a speci c subset of the instance space should not easily be discarded, nor for predictive settings, as diversity of base classi ers is crucial for reaching high predictive accuracy. The latter has empirically been shown by the success of Random Forests [2] and similar ensemble methods.

A way to improve the interestingness and diversity of rule-sets is to make use of previously found patterns and for-malised prior knowledge during construction. Incorporating prior knowledge like Bayesian Networks into existing Data Mining techniques is an active eld of research. Some ap-proaches like [28, 20] try to utilise prior knowledge to com-pensate for a lack of data. In these scenarios the models are tted to both, the prior knowledge and the dataset at hand. In contrast, the goal of subgroup discovery is to nd rules that contradict expectation, as this is assumed to indicate interestingness. In such a scenario any available information that allows to compute estimates of the user's expectation may help to re ne the metric for selecting interesting rules. A similar idea has recently been proposed in the scope of frequent itemset mining [10].

Following this idea, a subgroup pattern may be interest-ing relative to prior knowledge, only, as illustrated by the following example: Y is distributed in A just as in the overall population, so this rule would not be deemed interesting by any reasonable utility function. Now assume that in the prior knowledge there is a statement about a superset of A : This rule predicts a higher conditional probability of given B . In this context the rule A ! Y + becomes inter-esting as an exception to the prior knowledge, because one would rather expect Pr [ Y + j A ]= Pr [ Y + j B ]. The reason is that the prediction for B X is more speci c than the general class priors.

To the best of the author's knowledge the only approach towards incorporating available knowledge into subgroup discovery reported in the literature so far, is the ILP sys-tem RSD [15]. It uses background knowledge exclusively to propositionalise relational data, a step which is out of the scope of this work. For the learning step itself CN2-SD is used. The next section shows a generic technique to incor-porate prior knowledge into subgroup discovery and similar supervised learning tasks.
This section introduces a sampling-based technique to in-corporate prior knowledge into supervised Data Mining al-gorithms. Subsection 4.1 discusses the overall idea before some constraints for sampling are de ned in subsection 4.2. These constraints de ne a unique distribution as shown in 4.3. The last subsection 4.4 shows how to use strati ed sam-pling in order to solve speci c subgroup discovery tasks by means of classi er induction algorithms.
The most crucial question in an iterative Data Mining framework is how to pre-process the training data so that subsequent learning steps do not yield previously found pat-terns again. The goal is to nd uncorrelated new patterns, so that the resulting ruleset is compact, but still allows for a precise characterisation of the target attribute. The two example subgroups mentioned in the introduction, one containing all young drivers, and the other containing all persons who recently acquired their driving license, illus-trates how a stand-alone evaluation of each rule may result in highly overlapping rulesets. This bears the risk of almost redundant rules.

The algorithm proposed in this work allows to focus on previously undiscovered patterns by means of sampling. Tar-get samples are constructed in a way that does not allow to rediscover the available prior knowledge, because the tar-get attribute is made independent of the available predic-tions. At the same time it is taken care, that the remaining patterns part of the original data remain intact. Similar techniques are found in the boosting literature [7, 8, 21]. Please note, that boosting was rst introduced in terms of altering an initial distribution function and a corresponding sampling technique [19].

The technique which is shown to be capable of sampling out prior knowledge in the following sections is called rejec-tion sampling [16]. It allows to sample with respect to a distribution D 0 , given a procedure to sample from another distribution D : Assume that an example set of size n at hand has been sampled from distribution D n .Theneach example x is assigned a weight rather than sampling directly with respect to D 0 , which may be infeasible. A sample may then be constructed by weight-proportionate resam-pling. Alternatively, the weights may be interpreted as fac-tors of being over-or underrepresented by all subsequently applied algorithms.

Rejection sampling has also been approved as a generic way to incorporate costs into the Data Mining step. In [29] a proof is given, that this kind of sampling does not in-crease the sample complexity in the agnostic PAC learning framework for cost sensitive classi cation. As illustrated in the next subsections, rejection sampling even allows to generically incorporate a user-given or previously discovered probabilistic model into the Data Mining step. For this pur-pose a knowledge-based sampling scheme based on altering the original distribution underlying a dataset is introduced. Depending on the application, examples may be weighted or resampled.
Before going into detail the idea of removing prior knowl-edge by means of sampling is formulated in terms of con-straints. Formally, this step means to de ne a new distribu-tion D 0 , as close to the original function D as possible, but independent of the estimates produced by available prior knowledge. Switching from the initial distribution to the re-sampled data is a step of applying prior knowledge by means of sampling. As a result the previously discussed rule selec-tion metrics { when applied to these kind of samples { are \blinded" regarding the parts of rules that could already be concluded from prior knowledge. All that is accounted for is the unexpected component of each rule.

The scenario is discussed along the simpli ed case of a single rule as available prior knowledge: The distribution to be constructed should no longer support rule R ,so h and Y + should be independent events: If
R predicts a higher accident probability for young drivers, for example, then in the constructed sample this subgroup should share the default probability of accidents.
As further constraints the probabilities of events part of the rule should not change, since it is sucient to remove their correlation. This means that the class priors and the probability of R being applicable to a randomly drawn in-stance are equal for both distributions: For the example rule the probability of accidents and the probability of seeing a young driver will not change from the original training set to the constructed sample.
Finally, within each partition sharing the same class and prediction of R the new distribution is de ned proportion-ally to the initial one. The simple reason is that having just R as prior knowledge all instances within one partition are indistinguishable. Changing the conditional probabil-ities within one partition would mean to prefer some in-stance over others despite their equivalence with respect to the available prior knowledge. For the boolean rule R these constraints translate into the following equalities: Hence, if the probability of seeing a speci c driver halves from the original to the new distribution, then the same will happen to all other drivers sharing both, the property of being young or not, and the property of having had an acci-dent or not. All that changes are the marginal probabilities of the partitions.

Given a database and pattern R it is possible to apply any Data Mining technique after sampling with respect to . Further interesting patterns, even if they are overlap-ping with R , are still observable in the new sample. For instance subgroups that are subsets of h and have an even signi cantly higher or much lower Lift than rule R are just rescaled proportionally. For instance, if unexperienced per-sons driving a speci c kind of car tend to be involved in ac-cidents even more frequently than young drivers in general, then this more speci c rule can be found in a subsequent step. As motivated in subsection 3.2, various exceptions to previously found rules and patterns overlapping in some other way can be found, analogously.
In subsection 4.2 the idea of sampling with respect to an altered distribution function has been presented. Intuitively, prior knowledge and known patterns are \ ltered out". This subsection proves that the proposed constraints (3)-(9) in-duce a unique target distribution.
De nition 7. The Lift of an example x 2 X for a rule h ! Y + is de ned as
Lift ( h ! Y + ;x ):= Theorem 1. For any initial distribution D and given rule R the constraints (3)-(9) are equivalent to Up to a constant factor they induce D 0 : X ! IR + uniquely.
Proof. The proof is exemplarily shown for the partition ( h \ Y + ), in which the rule under consideration is both ap-plicable and correct. Assuming that the constraints hold D 0 can be rewritten in terms of D and Lift ( R; x ): ( 8 x 2 h \ Y + ): Pr D 0 ( x ) The other three partitions can be rewritten analogously. On the other hand, it can easily be validated that D 0 as de ned by theorem 1 is in fact a distribution satisfying the con-straints: and analogously for the other partitions. This directly im-plies constraints (3)-(5) by marginalising out. Constraints (6)-(9) are met, because for all four partitions D 0 is de ned proportionally to D .
 Please recall, that the Lift simply reflects the factor by which a label is overrepresented in a considered subset, com-pared to the label's prior. For the example rule the Lift is 5, since the risk for young drivers is 5 times higher than for the average driver. Hence, the probability to see a speci c young driver who had an accident in the target sample is reduced by a factor of 1 = 5 compared to the original data. Each of the four partitions that are de ned by a combination of prediction and true label is rescaled in the same fashion.
Theorem 1 de nes a new distribution to sample from, given a single rule R as prior knowledge. The same strat-egy may be applied iteratively, de ning a new distribution after each selected rule. Section 6 introduces an appropriate algorithm and evaluates it empirically.

Please note, that without any changes to theorem 1 more complex forms of prior knowledge may be incorporated. En-sembles of base learners like propositional rules are valid background theories, for example, as long as the predictions are discrete. Straightforward generalisations of constraints (3)-(9) allow to incorporate probabilistic predictions: Let the prior knowledge be associated to a function estimating the target distribution for each h x; y i2 X Y . Assuming the class priors Pr [ C ( x )= y ] to be known for each y 2 Y and applying the de nition of the Lift the cor-responding estimated Lift can easily be computed as Given a procedure for sampling examples x D i.i.d., the following distribution that generalises theorem 1 can be used to weight examples: To remove prior probabilistic knowledge, for example from a data stream, it is sucient to assign to each example x a weight of ( d Lift ( x ! C ( x ) j ))  X  1 . This strategy is surpris-ingly simple and well suited to apply rejection sampling.
Up to here no assumptions about the utility function for evaluating rule candidates were made. In fact any utility function can be used in combination with knowledge-based sampling.
This subsection shows a simple connection between sub-group discovery limited to the utility function WRAcc and the better known task of classi er induction.

The goal when inducing a classi er from data generally is to select a predictive model that separates positive and negative examples with high predictive accuracy. Many al-gorithms and implementations exist for this purpose [18, 26], basically di ering in the set of models (hypothesis space and search strategies. Subgroup discovery demands the def-inition of a property of interest, which can be assumed to be present in the form of a target attribute. In this sense this task is also supervised. The process of model selection is guided by a utility function. In the following de nition sub-group discovery is simpli ed to nding the most interesting rule.

De nition 8. Let H denote the set of models (rules) valid as output and D denote a distribution function over X . The task of classi er induction is to nd For a given utility function q : H ! IR t h e t a s k o f subgroup discovery is to nd For boolean target attributes common classi er induction algorithms do not bene t from nding rules with a preci-sion below 50%. In contrast, for subgroup discovery it is sucient if the precision of a rule is higher than the corre-sponding class prior. Choosing the utility function WRAcc we can transform subgroup discovery as de ned above into classi er induction by a simple sampling technique to over-come imbalanced class distributions.

De nition 9. For D : X ! IR + , C : X ! Y the strati ed random sample distribution D 0 of D (and C ) is de ned by D 0 is de ned by rescaling D so that the class priors are equal. This de nition allows to state the following theorem. Input: -labelled example set E = h x 1 ;y 1 i ;:::; h x m ;y m i -integer n Output: -Setof n (probabilistic) Horn logic rules KBS(E, n): 1. Let D 0 denote the uniform distribution over E . 2. For each c 2 Y de ne ( c ):= Pr x D 0 ( C ( x )= c ). 3. Let D 1 ( x i ):= ( y i )  X  1 for i 2f 1 ;:::;n g . 4. For k =1to n do 5. Output the set of rules f r 1 ;:::;r n g and their Lift s.
Theorem 2. For every rule h ! Y + the following equali-ties hold if D 0 is the strati ed random sample distribution of D :
Proof. The rst equality can be proved by rewriting ac-curacy in terms of WRAcc , exploiting P D 0 ( C )= P D 0 ( The second equality follows by applying the de nition of D 0 to WRAcc D 0 ( h ! Y + ), reaching at a reformulation in terms of the original distribution D .

For a full proof please refer to [23], for a broader discus-sion of rule selection metrics to [6, 9]. As a consequence of theorem 2 subgroup discovery tasks with utility function WRAcc can as well be solved by rule induction algorithms optimising predictive accuracy after strati ed resampling. The induced rankings of rules are equivalent.
This section discusses three subgroup discovery algorithms, which have been integrated into the learning environment YALE [17]. The rst of these is the knowledge-based sam-pling algorithm ( KBS ) shown in gure 1. It applies sampling as presented in subsection 4.3. More precisely, the imple-mentation allows to use example reweighting if the training data ts into main memory. As discussed in subsection 4.1 the probabilities computed by rejection sampling procedures may as well be interpreted as example weights.

The KBS algorithm iteratively selects a single rule cor-responding to a subgroup with high WRAcc ,updatesthe distribution according to theorem 1, and selects the next rule according to the updated distribution. Theorem 2 implies that high predictive accuracy on strati ed samples directly translates into high WRAcc on the original data. To this end lines 1-3 de ne a distribution D 1 by reweighting the training examples E with respect to de nition 9. Each loop in line 4 induces a single rule with high predictive accuracy, characterising a new subgroup. The rule induction algo-rithm used in the experiments is ConjunctiveRule ,part of the WEKA learning environment [26]. It iteratively con-structs the body of rules comparing the information gain of each candidate literal, and it prunes rules applying the reduced error pruning heuristic.

As distribution updates are computed according to the-orem 1 (line 4c) all constraints de ned in subsection 4.2 hold. Constraint (5) implies that all subsequently de ned distributions share the strati cation property of D 1 .The corresponding distributions without strati cation are very similar to the distributions actually used. They can be re-constructed by rescaling with respect to the original class priors.

In a prediction scenario signi cance tests help to avoid over tting, in a descriptive setting they avoid to report rules which could easily be valid just by chance. For simplicity signi cance tests are avoided in the experiments, but the mining step is restricted to n iterations. Rules are selected by WRAcc , a metric proportional to coverage, so rules are not expected to over t if n is chosen small enough.
The selected rules annotated by the Lift s allow to com-pute predictions for the target attribute. If the property of interest is boolean 1 , then all rules r i are of the form h i ! Y + =  X  . An application of the Na  X  ve Bayes strategy for combining predictions (see section 2) yields for the odds (eqn. (2)). Estimating each i with respect to D i prevents poor approximations in case of violated condi-tional independence. If the precision of a selected rule is 1 then eqn. (11) is not applicable, but the covered subset may simply be removed during training.

The reweighting performed by KBS is very similar to eqn. (10). Enumerator and denominator of the product in (11) approximate the Lift s for positives and negatives, re-spectively, e.g.: where denotes the set of probabilistic rules f r 1 ;:::r k It can easily be seen that after strati cation the algorithm weights examples inverse proportionally to these estimates.
In the next section KBS is compared to the only two other reweighting strategies reported in the subgroup dis-covery literature so far [14]. These strategies just a ect the reweighting step of the algorithm shown in gure 1: After a positive example e has been covered by i rules its new weight is computed as either additive update: w i ( e ):= 1 i +1 or multiplicative update: w i ( e ):=  X  i for given  X  2 [0 ; Accordingly, two versions of subgroup discovery ruleset in-duction ( SDRI ) have been implemented, which are similar
This restriction is only made for simplicity, as it is always possible to estimate each class against all the others. Dataset Examples Discr. Cont. Minority KDD Cup 10.000 { 71 50 : 0% Adult 32.562 8 6 24 : 1% Ionosphere 351 { 34 35 : 8% Credit Domain 690 6 9 44 : 5% Voting-Records 435 16 { 38 : 6% Mushrooms 8.124 22 { 48 : 2% Table 1: Data characteristics: size, number dis-crete/continuous attributes, frequency min. class to CN2-SD . The variant that applies ConjunctiveRule on strati ed samples after additive updates is referred to as SDRI + , the one with multiplicative updates as SDRI . Weights are updated after each iteration. The class explic-itly predicted by a rule is de ned to be the positive one, as xing one of the classes as positive gave worse experimen-tal results. Multiple occurrences of rules in the ruleset are allowed, reflecting the importance of patterns that are still observable after reweighting. The rulesets constructed by SDRI are combined as in CN2-SD rather than by applying Na  X  ve Bayes: The predicted target class distributions of all applicable rules are averaged.
A primary goal of subgroup discovery is to nd a small set of understandable rules that characterise a target variable. In more formal terms the probabilistic classi ers built from the rulesets should be accurate. This property is commonly measured by the area under the ROC curve metric (AUC).
The proposed idea of sequential sampling-based subgroup discovery has been evaluated on ve datasets from the UCI Machine Learning Library [1] and a 10K sample taken from the KDD Cup 2004 Quantum Physics dataset 2 . All datasets have boolean target attributes. Further characteristics are listed in table 1.

Figure 2 to 7 show how the AUC performance changes with an increasing number of iterations. All values have been estimated by 10fold cross-validation 3 . The columns n and auc in table 2 list the average performances of rule-sets from the cross-validation experiments for the empiri-cally best choice of n . For the KDD Cup data and the adult dataset the number of iterations were limited. To fur-ther evaluate the di erences between the algorithms another ruleset was induced for each variant, using the same value for parameter n . For evaluation the same set was used as for training, as common for descriptive learning tasks. Ta-ble 2 shows the resulting average coverages ( cov )andaver-age weighted relative accuracies ( wracc ). The ROC lter for rulesets discussed in subsection 3.2 was applied to both SDRI variants, denoted as RF in table 2.

The column div reflects the diversities of rulesets. The entropy of predictions is an appropriate measure for diver-sity of classi er ensembles in general [4]. Each rule can be considered to predict the conditional distribution of the tar-get, given whether it is applicable or not. For a boolean target, a set of n rules with p i ( x ):= Pr ( Y + j h i ( http://kodiak.cs.cornell.edu/kddcup/
For SDRI results are reported for the empirically best  X  from the candidate set f : 1 ;: 2 ;: 3 ;: 4 ;: 5 ;: 6 ;: 7 set of m examples the diversity was computed as 1 m after removing multiple occurrences of rules.

Throughout the gures 2 to 7 the KBS algorithm outper-forms SDRI with both reweighting strategies, while none of the SDRI variants is clearly superior to the other one. In gure 2 all three algorithms manage to nd useful rules re-peatedly. SDRI + performs best for sets of 3 to 6 rules, but for larger rulesets and for any other dataset and number of iterations KBS is superior. In gures 3 to 5 KBS improves AUC much quicker than SDRI , although for the smallest dataset ( g. 4) it over ts after the 3rd iteration. For the credit domain data ( g. 5) the AUC values of the SDRI rulesets improve non-monotonically. Inspecting the rulesets reveals many duplicates. For the voting-records ( gure 6) SDRI e ectively nds just 2 useful rules with both reweight-ing strategies, improving AUC by about 1% compared to the rst iteration. KBS selects 6 rules and improves AUC by about 4%. Finally, in the experiment shown in gure 7 KBS reaches 100% AUC with just 12 rules, while SDRI hardly improves over the performance of the rst rule at all. For the smaller datasets the ROC lter basically just removes dupli-cates from the rulesets, which has a marginal impact on the performance metrics. For the large datasets the lter prunes the ruleset at the price of a reduced AUC performance and diversity.

Although the KBS rulesets often have a smaller coverage and WRAcc their predictions outperform those of the other algorithms. It is interesting to note that for all datasets the KBS rulesets have the highest diversity, but according to the standard deviation of the AUC performance (column \ ") they are nevertheless most robust against minor changes to the data.
In this paper the idea of knowledge-based sampling has been presented, a generic technique of making rule selection metrics sensitive to prior knowledge. The interestingness of rules is often relative to a user's expectation or previ-ously found patterns. A set of intuitive constraints has been proposed, that formalise how to construct samples which are independent of prior knowledge, so that subsequently applied rule induction techniques focus on novel patterns. The constraints have been shown to uniquely de ne a new distribution, which can easily be operationalised by either resampling or de ning example weights.

Incorporating prior knowledge has been shown to be bene-cial for the learning task of subgroup discovery. Evaluating rules globally results in overlapping patterns. To cover var-ious aspects of a dataset it is more appropriate to construct sets of smaller rules, each of which captures a new pattern. Knowledge-based sampling is a way to shift the focus of subgroup discovery to undiscovered patterns, which allows to construct small sets of rules with high diversity. A new subgroup discovery technique based on strati cation, itera-tive reweighting, and an arbitrary embedded rule learner has been presented. Experiments with six real world datasets in-dicate that the algorithm outperforms state of the art sub-group discovery algorithms which are based on alternative reweighting strategies. + 20 86 : 12 : 8 47 : 0% 0 : 053 0 : 703 + ,RF 7 83 : 62 : 6 49 : 8% 0 : 055 0 : 703 + 31 88 : 44 : 2 56 : 8% 0 : 156 0 : 796 + ,RF 3 87 : 05 : 3 66 : 9% 0 : 139 0 : 668 + 6 98 : 70 : 2 43 : 4% 0 : 195 0 : 470 + ,RF 1 98 : 60 : 1 43 : 4% 0 : 195 0 : 470
Thanks to Timm Euler for carefully proof-reading some drafts, and to the anonymous reviewers for several useful hints. [1] C. Blake and C. Merz. UCI repository of machine [2] L. Breiman. Random forests. Machine Learning , [3] S. Brin, R. Motwani, J. Ullman, and S. Tsur. Dynamic [4] P. Cunningham and J. Carney. Diversity versus [5] T. Fawcett. ROC Graphs: Notes and Practical [6] P. A. Flach. The Geometry of ROC Space: [7] Y. Freund and R. R. Schapire. A decision{theoretic [8] J. H. Friedman, T. Hastie, and R. Tibshirani. [9] J. F  X  urnkranz and P. Flach. ROC 'n' Rule Learning { [10] S. Jaroszewicz and D. A. Simovici. Interestingness of [11] G. H. John and P. Langley. Estimating continuous [12] W. Kl  X  osgen. Explora: A Multipattern and [13] N. Lavrac, P. Flach, and B. Zupan. Rule Evaluation [14] N. Lavrac, B. Kavsek, P. Flach, and L. Todorovski. [15] N. Lavrac, F. Zelezny, and P. Flach. RSD: Relational [16] D. Mackay. Introduction To Monte Carlo Methods. In [17] I. Mierswa, R. Klinkberg, S. Fischer, and O. Rittho . [18] T. M. Mitchell. Machine Learning . McGraw Hill, New [19] R. E. Schapire. The Strength of Weak Learnability. [20] R. E. Schapire, M. Rochery, M. Rahim, and N. Gupta. [21] R. E. Schapire and Y. Singer. Improved Boosting [22] T. Sche er and S. Wrobel. Finding the Most [23] M. Scholz. Knowledge-Based Sampling for Subgroup [24] A. Silberschatz and A. Tuzhilin. What makes patterns [25] E. Suzuki. Discovering Interesting Exception Rules [26] I. Witten and E. Frank. Data Mining { Practical [27] S. Wrobel. An Algorithm for Multi{relational [28] X. Wu and R. Srihari. Incorporating Prior Knowledge [29] B. Zadrozny, J. Langford, and A. Naoki.

