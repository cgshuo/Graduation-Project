 Jason Weston jweston@google.com Ameesh Makadia makadia@google.com Google Inc., 76 9th Avenue, New York, NY 10011 USA. Hector Yee hyee@google.com Google Inc., 901 Cherry Avenue, San Bruno, CA 94066 USA There are many tasks where the goal is to rank a huge set of items, documents, or labels, and return only the top few to the user. For example, in the task of recommendation, e.g. via collaborative filtering, one is required to rank large collections of products such as movies or music given a user profile. For the task of annotation, e.g. annotating images with keywords, one is required to rank a large collection of possible annotations given the image pixels. Finally, in infor-mation retrieval a large set of documents (text, im-ages or videos) are ranked based on a user supplied query. Throughout this paper we will refer to the en-tities (items, documents, etc.) to be ranked as labels, and all the problems above as label ranking problems. Many powerful algorithms have been proposed in the machine learning community for the applications de-scribed above. A great deal of these methods typically rank the possibilities by scoring each label in turn, for example SVMs, neural networks, decision trees, and a whole host of other popular methods are used in this way. We refer to these methods as label scorers. Many of these methods, due to scoring labels independently, are linear in the number of labels. Thus, unfortunately, they become impractical when the number of labels goes into the millions or more as they are too slow to be used at serving time.
 The goal of this paper is to make these methods us-able for practical, real-world problems with a huge numbers of labels. Rather than proposing a method that replaces your favorite algorithm, we instead pro-pose a  X  X rapper X  approach that is an algorithm for making those methods tractable while maintaining, or in some cases even improving, accuracy. (Note, our method improves testing time, not training time, and as a wrapper approach is in fact not faster to train.) Our algorithm works by first partitioning the input space, so any given example can be mapped to a par-tition or set of partitions. In each partition only a subset of labels is considered for scoring by the given label scorer. We propose algorithms for optimizing both the input partitions and the label assignment to the partitions. Both algorithms take into account the label scorer of choice to optimize the overall precision at k of the wrapped label scorer. We show how vari-ants that do not take into account these factors, e.g. partitioning independent of the label scorer, result in worse performance. This is because the subset of la-bels one should consider when label partitioning are the ones that are both the most likely to be correct (according to the ground truth) for the given inputs, and the ones that the original label scorer actually performs well on. Our algorithm provides an elegant formulation that captures both of these desires. The primary contributions of this paper are:  X  We introduce the concept of speeding up a base  X  We provide an algorithm for input partitioning  X  We provide an algorithm for label assignment that  X  We present results on real-world large scale There are many algorithms for scoring and ranking labels that take time linear in the size of the label set. That is because fundamentally they operate by scoring each label in turn. For example, a one-vs-rest approach (Rifkin &amp; Klautau, 2004) can be used by training one model for each label. The models themselves could be anything from linear SVMs, ker-nel SVMs, neural networks, decision trees, or a battery of other methods, see e.g. (Duda et al., 1995). For the task of image annotation, labels are often ranked in this way (Perronnin et al., 2012). For collabora-tive filtering, a set of items is ranked and a variety of algorithms have been proposed for this task which typically score each item in turn, for example item-based CF (Sarwar et al., 2001), latent ranking models (Weimer et al., 2007), or SVD-based systems. Finally, in information retrieval where one is required to rank a set of documents, SVMs (Yue et al., 2007; Grangier &amp; Bengio, 2008) and neural networks like LambdaRank and RankNet (Burges, 2010) are popular choices. In this case, unlike for annotation, typically only a single model is trained that has a joint representation of both the input features and the document to be ranked, thus differing from the one-vs-rest training approach. However, documents are still typically scored indepen-dently and hence in linear time. The goal of our paper is not to replace the user X  X  favorite algorithm of choice, but to provide a  X  X rapper X  to speed up these systems. Work on providing sublinear rankings has typically focused on proposing a single approach or speeding up a specific method. Probably the most work has gone into speeding up finding the nearest neighbors of points (i.e. for k -nearest neighbor approaches), which is a setup that we do not address in this paper. The algorithms typically rely on either hashing the input space e.g. via locality-sensitive hashing (LSH) (Indyk &amp; Motwani, 1998) or through building a tree (Bent-ley, 1975; Yianilos, 1993). In this work we will also make use of partitioning approaches, but with the aim of speeding up a general label scorer method. For this reason the approaches can be quite different because we are not required to store the examples in the parti-tion (to find the nearest neighbor) and we also do not need to partition the examples, but rather the labels, so in general the number of partitions can be much smaller in our method.
 Several recent methods have proposed sublinear clas-sification schemes. In general, our work differs in that we focus on ranking, not classification. For example, label embedding trees (Bengio et al., 2010) partition the labels to classify examples correctly, and (Deng et al., 2011) propose a similar, but improved algo-rithm. Other methods such as DAGs (Platt et al., 2000), the filter tree (Beygelzimer et al., 2009), and fast ECOC (Ciss  X e et al., 2012) similarly also focus on fast classification. Nevertheless, we do run our algo-rithm on the same image annotation task as some of these methods in our experiments. We are given a dataset of pairs ( x i ,y i ), i = 1 ,...,m . In each pair, x i is the input and y i is a set of labels (typically a subset of the set of possible labels D ). Our goal is, given a new example x  X  , to rank the entire set of labels D and to output the top k to the user which should contain the most relevant results possible. Note that we refer to the set D as a set of  X  X abels X  but we could just as easily refer to them as a set of documents (e.g. we are ranking a corpus of text documents), or a set of items (e.g. we are recommending items as in collaborative filtering). In all cases we are interested in problems where D is very large and hence algorithms that scale linearly with the label set size are unsuitable at prediction time.
 It is assumed that the user has already trained a la-bel scorer f ( x,y ) that for a given input and single label returns a real-valued score. Ranking the labels in D is performed by simply computing f ( x,y ) for all y  X  X  , which is impractical for large D . Furthermore, after computing all the f ( x,y ), you still have the added computation of sorting or otherwise computing the top k (e.g. using a heap).
 Our goal is given a linear time (or worse) label scorer f ( x,y ), to make it faster at prediction time whilst maintaining or improving accuracy. Our proposed method, label partitioning, has two components: (i) an input partitioner that given an input example, maps it to one or more partitions of the input space; and (ii) label assignment which assigns a subset of labels to each partition. For a given example, the label scorer is applied to only the subset of labels present in the corresponding partitions, and is therefore much faster to compute than simply applying it to all labels. At prediction time, the process of ranking the labels is as follows: 1. Given a test input x , the input partitioner maps 2. We retrieve the label sets assigned to each par-3. We score the labels y  X  L with the label scorer The cost of ranking at prediction time is additive in the cost of assigning inputs to their corresponding par-titions (computing p = g ( x )) and scoring each label in the corresponding partitions (computing f ( x,y ) ,y  X  L ). By utilizing fast input partitioners that do not depend on the label set size (e.g. using hashing or tree-based lookup as described in the following sec-tion) and fixing the set of labels considered by the scorer to be relatively small (i.e. | L | |D| ), we en-sure the whole prediction process is sublinear in |D| . In the following sections we describe both components of the label partitioner, the input partitioner and the label assignment. 3.1. Input Partitioner We consider the problem of choosing an input parti-tioner g ( x )  X  p  X  X  , which maps an input point x to a set of partitions p , where there are P possible par-titions, P = { 1 ,...,P } . It is possible that g always maps to a single integer, so that each input only maps to a single partition, but this is not required. There is extensive literature suitable for our input par-titioning task. For example, methods adapted from the nearest-neighbor approaches could be used as the input partitioner, such as a hash of the input x (e.g. (Indyk &amp; Motwani, 1998)), or a tree-based cluster-ing and assignment (e.g. hierarchical k -means (Duda et al., 1995) or KD-trees (Bentley, 1975)). Those choices may work well, in which case we simply need to worry about label assignment, which is the topic of section 3.2. However, the issue with those choices is that while they may be effective at performing fully unsupervised partitioning of our data, they do not take into account the unique needs of our task. Specifically, we want to maintain the accuracy of our given label scorer f ( x,y ) whilst speeding it up. To summarize our goal here, it is to partition the input space such that examples that have the same relevant labels highly ranked by the label scorer are in the same partition. We propose a hierarchical partitioner that tries to op-timize precision at k given a label scorer f ( x,y ), a training set ( x i ,y i ), i = 1 ,...,m , and a label set D as defined earlier. For a given training example ( x i ,y i and label scorer we define the accuracy measurement of interest (e.g. the precision at k ) to be  X  ` ( f ( x and the loss to be minimized as ` ( f ( x i ) ,y i ) = 1  X   X  ` ( f ( x i ) ,y i ). Here f ( x ) is the vector of scores for all labels f ( x ) = f D ( x ) = ( f ( x, D 1 ) ,...,f ( x, D where D i indexes the i th label from the entire label set. However, to measure the loss of the label parti-tioner, rather than the label scorer, we need to instead consider ` ( f g ( x ing only the set of labels in the partitions of x f g ( x ) ( x ) = ( f ( x,L 1 ) ,...,f ( x,L | L | ))). We can then define the overall loss for a given parti-tioning as: Unfortunately, when training the input partitioner the label assignments L are unknown making the compu-tation of the above objective infeasible. However, the errors incurred by this model can be decomposed into several components. For any given example, it receives a low or zero precision at k if either:  X  It is in a partition where the relevant labels are  X  The original label scorer was doing poorly in the Now, while we don X  X  know the label assignment, we do know that we will be restricting the number of labels per partition to be relatively very small ( |L j | |D| ). Taking this fact into consideration, we can translate the two points above into tangible guidelines for de-signing our label partitioner:  X  Examples that share highly relevant labels should  X  Examples for which the label scorer performs well Based on this, we now propose approaches for input partitioning. To make our discussion more specific, let us consider the case of a partitioner that works by using the closest assigned partition as defined by partition centroids c i ,i = 1 ,...,P : This is easily generalizable to the hierarchical case by recursively selecting child centroids as is usually done in hierarchical k -means and other approaches (Duda et al., 1995).
 Weighted Hierarchical Partitioner A straight-forward approach to ensuring the input partitioner pri-oritizes examples which already perform well with the given label scorer is to weight each training example with its label scorer result: In practice, for example, a hierarchical partitioner based off of this objective function can be implemented as a  X  X eighted X  version of hierarchical k -means. In our experiments we simply perform the  X  X ard X  version of this: we only run the k -means over the set of training Note that we would rather use ` ( f g ( x than ` ( f ( x i ) ,y i ) but it is unknown. However, we employ upper bounds the true one, because we have strictly fewer labels than the full set so the precision cannot decrease  X  unless the true label is not in the partition. To help prevent the latter we must ensure examples with the same label are in the same parti-tion, which we do by learning an appropriate metric in the following subsection.
 Weighted Embedded Partitioners Building off of the weighted hierarchical partitioner above, we can go one step further and incorporate the constraint that examples sharing highly ranked relevant labels are mapped similarly in the partitioner. One way of encoding these constraints is through a metric learning step as in (Weinberger et al., 2006).
 One can then proceed with learning an input parti-tioner by optimizing the weighted hierarchical parti-tioner objective above but in the learnt  X  X mbedding X  space: However, some label scorers already learn a latent  X  X mbedding X  space, for example models like SVD and LSI (Deerwester et al., 1990) or some neural networks (Bai et al., 2009). In that case, one could consider per-forming the input partitioning directly in that latent space, rather than the input space, i.e. if the label scorer model is of the form f ( x,y ) =  X  x ( x ) &gt; then the partitioning can be performed in the space  X  x ( x ). This both saves the time of computing two em-beddings (one for the label partitioning, and one for the label scorer), and further partitions in the space of features that are tuned for the label scorer, so is thus likely to perform well. 3.2. Label Assignment In this section we consider the problem of choosing a label assignment L . To recap, we wish to learn this given the following:  X  A training set ( x i ,y i ), i = 1 ,...,m with label set  X  An input partitioner g ( x ) built using the methods  X  A linear time label scorer f ( x,y ).
 We want to learn the label assignment L j  X  D which is the set of labels for the j th partition. What follows is the details of our proposed label assignment method that is applied to each partition. Let us first consider the case where we want to optimize precision at 1, and the simplified case where each example has only one relevant label. Here we index the training examples with index t , where the relevant label is y t . We define  X   X  X  0 , 1 } |D| , where  X  i determines if a label D i should be assigned to the partition (  X  i = 1) or not (  X  i = 0). These  X  i are the variables we wish to optimize over. Next, we encode the rankings of the given label scorer with the following notation:  X  R t,i is the rank of label i for example t : We then write the objective we want to optimize as: subject to: where C is the number of labels to be assigned to the partition. For a given example t , to maximize precision at 1 two conditions must hold: (1) the true label must have been assigned to the partition and (2) the true label must be highest ranked of all assigned labels. We can see that equation 1 computes exactly precision at 1 these two conditions respectively. Thus our proposed objective function counts the number of training ex-amples for which we have precision at 1 of 1.
 Interestingly, note that the nature of label partitioning means we can have situations where: (i) the training example t was labeled incorrectly in ii) the original label scorer labeled the example cor-It is thus the job of this optimization problem to in-clude as many of the relevant labels in the partition as possible, and to eliminate as many confusing labels (high-ranking but incorrect) as possible, if by remov-ing them more examples are then correctly labeled. See figure 1 for an example.
 Unfortunately, the binary constraint (eq 2) renders the optimization of eq 1 intractable, but we relax it to: max Now, the values of  X  are no longer discrete and we do not use the constraint of eq. 3 but instead must rank the continuous values  X  i after training, and take the largest C labels to be the members of the partition. We now generalize the above to the case of measuring precision at k &gt; 1. Instead of seeing if there is at least one  X  X iolating X  label above the relevant label, we must now count the number of violations above the relevant label. Returning to an un-relaxed optimization prob-lem, we have: subject to: Here, we can simply take  X ( r ) = 0 if r &lt; k and 1 otherwise to optimize precision at k .
 Up to now, we have only considered having a single relevant label, but in many settings it is common for examples to have several relevant labels, which makes computation of the loss slightly more challenging. Let us go back to considering precision at 1 for simplicity. In this case the original objective function (eq 1) would generalize to: subject to: Here, y t contains several relevant labels y  X  y t , and if any of them are top-ranked then we get a precision at 1 of 1, hence we can take the max y  X  y t .
 We can combine our developments in equations 5 and 7 to formulate a cost function for precision at k with multi-label training examples. To make it suitable for optimization, we relax the max y  X  y t in eq 7 to a mean and approximate  X ( r ) with a sigmoid:  X ( r ) = subject to For a single example, the desired objective is that a rel-evant label appears in the top k . However, when this is not the case the penalty does not reflect the actual ranking position (i.e. our original cost is equivalent for being ranked in position k + 1 or being ranked at posi-tion |D| ). As earlier we wish to deemphasize examples for which the label scorer was already performing very poorly. In order to reflect this we introduce a term weighting examples by the inverse of the rank of the relevant label using the original label scorer, following (Usunier et al., 2009). Equations 4 and 9 now become, respectively: Here for simplicity we take w ( R t,y ) = ( R t,y )  X  , X   X  0 and in our experiments we set  X  = 1 . 0 (higher values would further suppress examples with lower-ranked relevant labels). These equations represent the final relaxed form of our label assignment objective, which we optimize using stochastic gradient ascent (Robbins &amp; Monro, 1951).
 Optimization Considerations We consider the case where the input partitioner g ( x ) is chosen such that each input x maps to a single partition. Each partition X  X  label assignment problem is thus indepen-dent, which allows them to be easily solved in parallel (e.g. using the MapReduce framework). In order to further reduce the training time, for each partition we can optimize over a subset of the full label set (i.e. choose  X  D  X  D ,C &lt; |  X  D| &lt; |D| ). For each partition, we choose the |  X  D| labels that are the highest ranked relevant labels among the partition X  X  training exam-ples using the original label scorer. In all of our ex-periments presented in the following section, we set |  X 
D| = 2 C . Note, in our experiments we found a neg-ligible effect of reducing the parameter set size from |D| to 2 C . This might be explained by the fact that for any partition the majority of labels in D will not appear as relevant labels in any of the training exam-ples. Since such labels would not receive any positive gradient updates in our formulation, leaving them out of the parameter set would have little effect. Counting Heuristic Baseline By way of compar-ison with our proposed label assignment optimization, in our experiments we also consider a much simpler heuristic: consider only the first term of equation (1), i.e. max  X  P t  X  y t . In that case the optimization re-duces to simply counting the occurrence of each true label in the partition, and keeping the C most fre-quent labels. This counting-based assignment provides a good baseline for comparison with our proposed op-timization. We show in our experiments that our com-plete optimization outperforms this heuristic, presum-ably because the second term also takes into account the label scorer itself. 4.1. Image Annotation We first considered an image annotation task us-ing the publicly available ImageNet dataset. Ima-geNet is a large image dataset that attaches quality-controlled human-verified images to concepts from WordNet (Fellbaum, 1998). We used the Spring 2010 version, which contains about 9M images, from which we kept 10% for validation, 10% for test, and the re-maining 80% for training. The task is to rank the 15,589 possible labels that range from animals ( X  X hite admiral butterfly X ) to objects ( X  X efracting telescope X ). We used a similar feature representation as in (Weston et al., 2011) which combines multiple feature represen-tations which are the concatenation of various spatial and multiscale color and texton histograms for a total of about 5  X  10 5 dimensions. Then, KPCA is performed (Schoelkopf et al., 1999) on the combined feature rep-resentation using the intersection kernel (Barla et al., 2003) to produce a 1024 dimensional input vector. In (Weston et al., 2011) it was shown that the ranking algorithm Wsabie performs very well on this task. Wsabie builds a label scorer of the form f ( x,y ) = xU &gt; V y by optimizing a ranking loss that (approxi-mately) optimizes precision at k by weighting the indi-vidual ranking constraints per example based on their rank. Wsabie was shown to be superior to unbalanced one-vs-rest (see also (Perronnin et al., 2012)), PAMIR (Grangier &amp; Bengio, 2008), and k -nearest neighbors. We therefore use Wsabie as our baseline linear time label scorer that we want to speed up with label par-titioning (although we could easily apply it to another algorithm, such as one-vs-rest).
 The result we obtained for Wsabie is a P@1 of 8.32%, which is in line with the results reported in (Weston et al., 2011) (there, 8.83% is reported at best, but on a different train/test size and split). The main results comparing the baseline label scorer with our label par-titioning algorithm are given in Table 1.
 We report several variants of our approach: with differ-ent input partitioners ( k -means, hierarchical k -means, and LSH), different numbers of partitions and labels per partition, resulting in different performance/speed tradeoffs. We can achieve very similar accuracies to the original label scorer ( &gt; 8 . 0%) whilst being up to 21x faster. We can be faster still, but with slightly less accuracy. LP-k -means (where we weight examples when training k -means as explained in section 3.1) is also compared to k -means and brings some small gains. Further, compared to the counting heuristic described in section 3.2 for label assignment, we see consistent larger improvements with our optimization method. Our method reported in the table uses the  X  X eighted embedded partitioning X  described in section 3.1 whereby the input partitioning is performed in the la-tent space  X ( x ) of the label scorer ( Wsabie ) as this gave the best results. However, we also ran the k -means input partitioner in the original input space, which is consistently worse. For example, for P = 1024 and C = 250 this gives 7.48% (compared to 8.07% in Table 1) with label assignment optimization, or 6.76% (compared to 7.61%) with the counting heuristic. Gen-erally, however, the same trends apply when using a partitioner in the input space, i.e. we see large gains from label assignment optimization, and small gains from input partition optimization (e.g. with the same parameters the results go from 6.76% to 7.01%). Further, as this particular label scorer operates in an embedding space (which is not true in general) meth-ods like LSH can be directly applied to partition both inputs and labels, i.e. put the labels in the partitions using V y and the inputs using xU . We apply a ran-dom projection matrix W and hash using the signs of the resulting hyperplanes, as is common. We fur-ther implement  X  X pilling X  by taking labels that differ only by a few hash bits from the partition: we rank the labels by the number of differing bits, and take up to 250, 500, or 1000 labels. This results in a perfor-mance of 2.10%, 3.27% and 4.32% respectively. This is inferior to our method even when it has less labels per partition (bottom two rows of Table 1). With more bits LSH+spilling improves, e.g. with 16 bits and 1000 labels it gets 5.07% but this is still worse than our method with only 10 bits. Note that LSH without spilling performs more poorly (e.g. even 4 bits gives 2.91%, and more bits gets worse) indicating the importance of labels appearing in multiple partitions, which is a feature of our method. Some example label assignments of our method are shown in Table 3. Finally, although we optimize precision at k , recall is an important and often reported metric for ImageNet. As an example of recall performance, the baseline lin-ear time Wsabie model has R@5 of 19.7%. In com-parison, the label partitioner optimized for precision at 1 with 2048 k -means input partitions and 250 labels per partition (i.e. the same model as in the first row of Table 1) has R@5 of 18.3% (18.0% for the counting heuristic). 4.2. Video Recommendation We next considered a much larger scale problem, that of recommending videos from a large online video com-munity. The million most popular videos are consid-ered as the set D and our aim is to rank these videos for a given user, to suggest videos that are relevant to the user. The training data is thus of the form where each training pair is based on an anonymized user. For each user the input x i is a set of features which indicate their preferences. These features are generated by aggregating over each user the topics of videos they are interested in. These collection of top-ics are then clustered. There are 2069 of these clusters that represent the user, of which 10 are active at any time. The label y i is a set of known relevant videos. The dataset consists of 100s of millions of examples where each example thus has 2069 input features, and on average on the order of 10 relevant videos. We set aside 0.5M examples for validation, and 1M for test. Our baseline label scorer Wsabie had a P@10 that gives an 108% improvement compared to applying Naive Bayes (i.e. gives around double the perfor-mance) 1 so the baseline seems relatively strong. We then ran the label partitioner using hierarchical k -means with 10000 partitions and various label assign-ment set sizes, and results are given in Table 2. Our method achieved a speedup of 990x whilst actually im-proving over the label scorer P@10 by 13%. This re-sult is not as impossible as it might seem: the label scorer we are using is a linear model whereas the la-bel partitioner is somehow a  X  X on-linear X  one: it can change the label sets in different partitions of the input space  X  and thus correct the mistakes of the original scorer (in a way, this is like a re-ranker (Collins &amp; Koo, 2005)). Note the optimization-based label partitioner consistently outperforms the counting heuristic again. Our label partitioner was used in the fielded video rec-ommendation system where we attempted to improve an already strong baseline machine learning system (Davidson et al., 2010). In our experiments above we used precision, but precision is merely a proxy for the online metrics that matter more, such as video click through rate and duration of watch. When evaluating the label partitioner in the real-world system, it gave statistically significant increases in both click through rate and watch duration (approx 2%). Note that we cannot compare it to the original label scorer in that case as it is simply not feasible to use it. We have presented a  X  X rapper X  approach for speeding up label scoring rankers. It employs a novel optimiza-tion to learn an input partitioning and label assign-ment that outperforms several baselines. The results are either similar to, or better than, the original label scorer, whilst being orders of magnitude faster. This allowed our technique to be usable in a real-world video recommendation system, which would otherwise not have been feasible. Finally, while we feel that our pro-posed label assignment is a good solution to the prob-lem, the large performance differences between input partitioners indicates that this remains an important problem for future work.

