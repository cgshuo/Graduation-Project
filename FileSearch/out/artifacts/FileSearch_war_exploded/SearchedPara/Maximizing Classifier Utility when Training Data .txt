 Classification is a well-studied problem in machine learning and data mining. Classifier performance was originally gauged almost exclusively using predictive accuracy. However, as work in the field progressed, more sophisticated measures of classifier utility that better represented the value of the induced knowledge were introduced. Nonetheless, most work still ignored the cost of ac-quiring training examples, even though this affects the overall utility of a classifier. In this paper we consider the costs of acquir-ing the training examples in the data mining process; we analyze the impact of the cost of training data on learning, identify the optimal training set size for a given data set, and analyze the per-formance of several progressive sampling schemes, which, given the cost of the training data, will generate classifiers that come close to maximizing the overall utility. Data mining, machine learning, induction, decision trees, utility-based data mining, cost-sensitive learning, active learning Classification is an important a pplication area for data mining. The quality of a classifier is almost always measured exclusively by its performance on new exampl es. Originally only simple measures like predictive accuracy were used. However, as the field advanced and more complex problems were addressed, more sophisticated perform ance measures were introduced X  X easures that more accurately reflect how the classifier will be used in its target environment. Thus, it is now not uncommon for misclassi-fication cost information to be considered when evaluating classi-fier performance or for profit/loss metrics to be used. The ultimate goal of utility-based da ta mining [12] is to consider all utility considerations in the data mining process and maximize the utility of this entire process. In the case of classification, this translates to considering the costs associated with building the classifier and the costs/benefits associated with applying the clas-sifier. As just mentioned, there has been a substantial amount of work in properly measuring the costs and benefits of applying the classifier. However, with the exception of some work from the active learning community, the costs associated with building the classifier are often ignored. This is a mistake, since the cost of building a classifier can be quite substantial. These costs may include the cost of acquiring the training cases, the cost of clean-ing and preparing the training data, the cost of labeling the train-ing data and the CPU time and hardware costs associated with building the classification model. These costs are described in more detail in Section 2. In this paper we focus on the cost of acquiring complete, usable, training cases, where one has no control over which specific train-ing examples can be acquired (t his differentiates our work from the work on active learning). Thus, the value/utility of a classifier is the value associated with using the classifier minus the cost of the training data used to build the classifier. With this notion of utility, if classifier A performs only slightly worse than classifier B but is much less costly to gene rate, classifier A will be consid-ered the better classifier. In Section 3.3 we formalize this notion of total utility so that we can precisely determine when one classi-fier is  X  X etter X  than another. The main contribution of this paper is that we analyze the trade-off of between acquiring more training data (and the concomitant increase in predictive accuracy) and the cost of acquiring this data. We show that for each data set and learner there is an optimal training set size that maximizes the overall utility of the classifier. We then propose two progressive sampling schemes and demonstrate th at they can be used to gen-erate classifiers with near optimal overall utility. This paper focuses on the cost of training data and how it impacts the overall learning/data mining pro cess. In this section we de-scribe in some detail what we m ean by the cost of training data. We then motivate this research by describing three data mining scenarios. In this paper we are concerned with the cost of acquiring labeled examples that can be used to train a classifier. In this section we describe several of the costs associated with acquiring training data. The primary cost we are concerned with is the cost of ac-quiring the raw, but labeled, tr aining examples. Specifically, we assume that a data-mining practitioner can request a batch of b examples with some cost c . We assume no restrictions on the value of b , although depending on the domain, there may be re-strictions on b (i.e., you may not have total flexibility in specify-ing the batch size). We also assume that c does not depend on the specific examples, although our analysis is not highly dependent on this assumption. Turney [9], who provides a fairly comprehen-sive list of costs associated with classifier learning, refers to this cost as the  X  X ost of cases. X  Training data often needs to be prepared before it can be used for learning. Data preparation may include cleaning the data, remov-ing outliers and transforming the data into a format suitable for mining. These data preparations steps often have a cost associated with them X  X specially if some manual effort is required. Thus, even if training cases are free, th ere still may be a cost for gener-ating usable training cases. Thus the re search described in this paper is relevant when there are data preparation costs. In many situations unlabeled training cas es m ay be freely avail-able but there m ay be a cos t for labeling them . Turney refers to this situation. However, when there is a cos t of teacher one may selectively label exam ples , which is an exam ple of active learning [1] . Becaus e we as sume that the us er has no control of the exam -ples that are requested, we do not cover active learning in this paper and hence our work does not apply for situations where active learning is used. Nonetheles s, in practice active learning is not alway s utilized when there is a cost of teacher. Active learn-ing can als o be us ed when there is a cos t as sociated with meas ur-ing specific feature values. We do not consider this ty pe of cost. In summary , our work differs from the work on active learning in that we focus on the cos t of acquiring com plete cas es (cos t of cases), in which one has little or no control over which specific cas es are acquired. We believe that for m any dom ains there are cos ts associated with acquiring training data and, just as importantly , one has some choice in the number of training exampl es that can be acquired. In these situations, it is essential to consider the cost of data acquisi-tion if one is to m axim ize the overall utility of the classifier. Three exam ples are provided in this section. Based on past experience in industry , we know that it is quite common to acquire training data from an external vendor, whose business relies on selling inform ati on. For example, in order to build classification m odels to cl assify busine sse s, we acquire d summa ry busine ss da ta from D&amp;B and detailed survey data from Ziff Davis. Com panies that sell data ty pically do not require their customers to buy either  X  X ll or nothing. X  Depending on the com-pany , a customer may be allowed to choose a number of records and pay based on that number or choose from a set of predeter-mined levels of data coverage a nd pay based on that level of cov-erage. A second example comes from the task of classify ing a phone line as belonging to a residential or business customer based on the pattern of phone usage. Informa tion describing every phone call is recorded as a call-detail record, which is generated in real-tim e and s tored s equentially . Becaus e the clas sification tas k requires examples to be at the phone-line level, all of the call detail re-cords associated with each phone num ber m ust be aggregated. Given that billions of call-detail records are generated every month and becaus e the aggregation s tep requires sorting all of thes e records , this data preparati on step is very expensive in terms of both dis k space and CP U tim e. Thus , in this dom ain the train-ing examples were expensive ev en though the raw data was essen-tially free. The third example comes from the domain of game play ing. If our goal is to learn something about an opponent so that we can de-sign a game-play ing strategy tailored to this opponent, the train-betting is involved. For example, if y ou want to learn something about an opponent in poker  X  X  ou may play only 50 or 100 hands against a given opponent and want to quickly learn how to exploit them  X  [3] . We hope that the descriptions of the various costs and these two simple exam ples motivate the need to factor in the training data cos ts when building a clas sifier. It is interes ting to note, however, that except for the work on active learning, as des cribed in more detail in Section 6, very little res earch has addressed this issue. Specifically , pas t res earch does not addres s the  X  cost of cas es X  at all. This is true even though l earning curves, which describe the relationship between training set size and clas sifier perform ance, are well known and frequently studied X  X hey are just rarely used in practice. The experiments in this paper va ry the training s et s ize and then track the accuracy of the induced classifier. This perform ance inform ation is then com bined with cost inform ation in order to determ ine the overall utility of the classifier. W e outline our ex-perimental methodology in Secti on 3.1, summarize the data sets we analy ze in Section 3.2 and discuss how we m easure total util-ity in Section 3.3. All of the experiments in this paper use C4.5 [8] , a popular deci-sion tree learner that is a des cendant of ID3. In order to determine the relations hip between training s et size and predictive accuracy , training sets are generated with a variety of s izes . The data is partitioned via random sam pling as follows. For each experim ent, 25% of the data is randomly selected and allocated to the test set, while the rem aining 75% of the data is potentially available for training. However, becaus e we want to vary the training set size, for m ost experim ents only a porti on of this 75% will be assigned to the training set (the rem ainder is not used). All results in this paper are based on averages over 20 runs, in order to increase the statis tical s ignificance of our res ults. Becaus e it does not take much CPU tim e to build the m odels for any of the data s ets ana-lyzed in this paper, the use of m ultiple runs does not seriously lim it our work. In future work we m ay investigate the use of sin-gle runs if the size of the training sets warrants it. We investigate two sim ple sa mpling schedules. Sampling sched-ule S1 uses the following training set sizes: 10, 50, 100, 500, 1000, 2000, ..., 9000, 10000, 12000, 14000, 16000, etc. Specifi-increm ented by 1,000 until the training set size reaches 10,000, after which the training set size is incremented by 2,000. Our second sampling schedule, S2, starts with a training set size of 50 and then successively doubles the training set size. This geometric sam pling schem e is motivated by previous work on progressive sampling, which shows that give n certain assumptions (which do not hold in this paper), this schedul e is asy mptotically optim al [7] . This previous work on progressive sampling is described in Sec-tion 6. For sampling schedules S1 and S2, in addition to evaluat-ing the training set sizes just desc ribed, the largest possible train-some of the plots in our results s ection, Section 4, some additional training s et sizes were evaluated in order to improve the granular-ity of our results. These additional training set sizes were not used in Section 5, where we discuss a progressive sampling strategy . We analy ze the ten data sets described in Table 1. In order to improve the presentation of our re sults, the data sets are parti-tioned into two groupings based on their relative sizes: large and small. Table 1 als o lis ts the total num ber of exam ples in each data set. The data s ets were obtained from the UCI Machine Learning Repos itory [6] , except for thos e m arked with an as teris k, which were originally made available from res earchers at AT&amp;T (thes e data s ets are available from the author). We evaluate the perform ance of the induced clas sifiers bas ed on total utility because we need to take the cost of training data into consideration. Thus, our utility m etric m ust take into account the cos t of training data (data cos t) and the cos t of clas sification er-rors (error cost). We do not include othe r possible costs, suc h as the CPU time cost associated with training the classifier, although we do record thes e CP U tim es and com ment on their potential im pact on total utility . Total cost is defined below in equation 1. Total Cos t = D ata Cos t + Error Cos t [1] Before we can understand the erro r cost term, some background is required. For our experim ents, a classifier is built from training data and its accuracy is evaluated us ing s eparate tes t data. The purpos e of any clas sifier is to clas sify new, unlabeled, exam -ples X  X ot the test set. We thus assume the existence of a score data set, S, which the user will apply the classifier to, over som e period of tim e. The error cost will be based on the num ber of errors we expect to get when clas sifying the s core data set, which can be estim ated as the error rate on the test set m ultiplied by the size of S, denoted | S|. Thus error cost is directly proportional to |S|. Although we do not know the value of | S| for any of the data sets in this paper, a domain expe rt should be able to estimate its value, although this may not alway s be a simple task (e.g., it may depend on how long the classifier is used, how successful it is, etc.). For each experim ent we know the num ber of training exam ples, n , and the es tim ated error rate, e , based on the performance of the clas sifier on the tes t set. W e as sume that there is some fixed cos t C tr for acquiring each training exam ple and s ome fixed cos t C err for each error m ade on an exam ple in the s core data set. Data cost will then equal n  X  C tr and error c ost will be estim ated as e  X |S| X  C err . The total cos t for a clas sifier then is given by equation 2, which is our m easure of total utility . Total Cost = n  X  C tr + e  X |S| X  C err [2] With specific domain knowledge we would be able to estimate C tr , C err , and |S| and thus calculate total cost. However, in our case we do not know these values. Therefore we need to treat them as variables and analy ze a wide range of values in order to properly analy ze a data set. The probl em with this situation is that three variables make a thorough analy sis difficult. However, we can reduce this to two variables by arbitrarily assuming | S| is 100. This does not reduce the generality of our results because we can easily account for other values of | S| via a simple calculation. Namely , error cost is proportional to the product |S| X  C err so that if we find that | S| is 100,000 instead of 100, we can simply look at the experiment results for C err /1,000 rather than C err . In a s ense, we are measuring error cost in terms of every 100 score examples and then adjusting for different score set sizes. Given that we now only need to track C tr and C err , for analy sis purposes we can simplify things further by only tracking the ratio of these two variables. W hile the real total cost will depend on the actual constants, the optim al training set size, for exam ple, will only depend on the ratio of the costs. So, in our results we simply cos t, and C err  X  1. Becaus e we want to plot our res ults us ing nu-merical values, we often use the relative cost or relative cost ratio instead, which is sim ply C err / C tr . F or exam ple, if the cos t ratio is 1:100 then the relative cost ratio is 100. Note that in this case we can say that from a utility perspec tive it is an even trade-off to purchase 100 training exam ples if it will reduce the num ber of errors by 1, assum ing | S| is 100. We can rem ove the condition on |S| by stating things in a sligh tly different manner: purchasing 100 training exam ples leads to an even trade-off if it res ults in a 1% reduction in error rate. As an exam ple, we can com pute the total cos t as sociated with one of the experiments reported in th is paper, which uses the adult data set. For this particular experim ent, n is 1500 and the error rate of the resulting classifier is 15.8%. The cost ratio is 1:1000 and, as discussed, for now we presum e the score set will have 100 exam ples. Using equation 2, the total cost is then: Total cost = 1500 X  1 + .158 X  100 X  1000 = 1500 + 15800 = 17,300 One potential issue with the utility measure in equation 2 is that if |S| is sufficiently large then the second term will dom inate the first, in which case the cost of acquiring the training data is not im portant. W ill the error cost term alway s dom inate the data cost term? We do not believe so for se veral reasons. First, for some domains the cost of acquiring trai ning data is very significant and once the learning curve begins to flatten out it may take tens or hundreds of thousands of training exam ples to im prove accuracy by even a tenth of a percent. In this region, even if |S| is very that region that we expect our cost model to be most useful. In addition, | S| need not alway s be extrem ely large. F or exam ple, in the poker exam ple mentioned in Section 2.2 one will not ty pically play a large number of poker hands against a single opponent. Finally , other work in the field seem s to support our intuition that data cos t is im portant. For exam ple, the entire field of active learning is based on the assum ption that error cost will not totally dominate the data cost X  X f it did then active learning would be unnecessary . We conclude this section on m easuring total utility by noting that total cost is not the only m etric we could have used to measure utility . W e could alternatively have factored in a benefit for each correctly clas sified exam ple and a cos t for each incorrectly clas si-fied example. However, given th e goals of this paper we do not believe that there is m uch value in also evaluating this perform -ance m etric, although we recom mend that practitioners use this alternative m etric if it m akes sense for a specific dom ain. This section includes our main re sults. Section 4.1 describes how we us e the cos t ratio inform ation to analy ze the learning curve data generated by our experiment s. Section 4.2 presents detailed results for a representative data set and then Section 4.3 provides sum mary results for the rem aining data sets. The basic experiments in this paper involve generating the learn-ing curves for each data s et. In order to analy ze thes e res ults, we vary the cost ratios and then see how this im pacts the total utility of the classifier. In particular, we want to determ ine the optim al training set size for any cost ratio and we would like to see how this optim al training set size change s as the cos t ratio is varied. In our analy sis we examine a wide range of cost ratios. We cannot focus on the m ost realis tic values since thos e values are dom ain specific and we do not have the requisite domain knowledge. Rather, we try to exam ine a s ufficient range of cos t ratios so that we hit the  X  X wo extrem es X  and s ample s ome points in between. Specifically , for each data s et we s trive to analy ze cos t ratios such that for one of the cost ratios the optim al strategy is to acquire almost no training data (  X  10 examples) and for another cost ratio the optimal strategy is to acquire all possible training data. The cos t ratio that leads us to acquire all of the training exam ples may be quite high, such as 1:50,000 (the cost ratio required by the adult data set). This cost ratio, which say s that the cost of an error is 50,000 times that of the cost of a training example, may appear to be unrealistic, but that is not necessarily so. For exam ple, if we have a direct marketing campaign where it costs $1 to produce and mail a catalog and the demographic information that is pur-chased to help build a predictive model is $100 per 100,000 households, one can see that a tr aining example is very cheap relative to the cost of an error ($.001 per training example versus $1 for each household that is m istakenly predicted to m ake a pur-chase). Also recall that the erro rs are per 100 score examples. The cost ratio of 1:50,000 with 100 scor e examples is equivalent, as described in Section 3.3, to a cost ratio of 1:50 if there are 100,000 examples in the score set. Stated more generally , a cost ratio of 1:50,000 means that pur chasing 50,000 training examples leads to an even trade-off if the error rate is reduced by 1%. This certainly seem s like it could be a reas onable trade-off. Our analy sis of the adult data se t begins with its learning curve, shown in Figure 1. As is common for learning curves , there is a very steep increas e in accuracy at firs t which then dim inis hes as more training data becomes availabl e. It is worth noting that the learning curve for this data s et never reaches a plateau, even when there are more than 15,000 traini ng examples and that the curve shows a s mall but s teady increas e in accuracy for an extended period of time. We also see that the learning curve in Figure 1 is not smooth and in a few cas es shows a decreas e in accuracy as the training set size increas es. W e expect that thes e are statis tical aberrations would disappear given an infinite number of runs. Given the learning curve data it is straightforward to compute the total cost for a variety of cost ratios, by using equation 2 (recall training set size for six different cost ratios ( C tr: C err ). If we look at the 10:1 cost ratio, which places the highest relative cos t on the training data (and is the only cas e where C tr&gt; C err ), we see that the curve is linear. The reas on for this is that in this situa-tion data cost com pletely dom inates error c ost , so that total cost is directly proportional to the size of the training set. When the becaus e the total cos t is now approxim ately one-tenth the cos t. As the cost ratio continues to shift toward a higher relative cost for errors, the curve becomes non-linear and the minimum total cost (identified for each curve by the large square m arker) no longer is at the minim um training set size, but rather shifts towards the larger and larger training set sizes. At a relative cost of 1:7500 the lowest cost is achieved with a training size of 6,500. ratio becom es m ore s kewed, which obs cures some of the changes of the curves with lower total co st. To fix this problem we nor-malize each curve by dividing the total cos t by the m axim um total cos t as sociated with the curve. The res ults for norm alized cos t are shown in Figure 3. The method for representing the re sults in Figure 3 also permits us to exam ine higher cos t ratios and shows us that for a cost ratio of 1:50,000 the optimum strategy is to use all of the training data. Figure 3, in conjunction with Figure 1, shows that once the learn-ing curve begins to flatten out, a great increas e in the cos t ratio is required for it to be worthwhile to use more training data. This is encouraging in that once we get past a certain point, the optimal training set size is not overly sensitive to the exact value of the cost ratio; hence a good estimate of this ratio should be adequate. Figure 3 also makes it clear that using all of the potentially avail-able training data is not a good strategy  X  X or most relative cost ratios the total (norm alized) cost is much lower for the optim al training set size than when the maximum number of training ex-am ples are us ed. Figure 4 provides the most highl y summarized information con-( C err / C tr ), the optimum training set size. The optimum training set size curve can be used by a practitioner to determ ine the amount of training data to obtain even if the precise cost ratio is not known (in Section 5 we introduce a progressive sampling strategy to find this optimum w ithout first requiring all of the potentially available training data). At a m inim um, curves like those in Figure 4 can inform a data m ining practitioner of the trade-offs involved. We provide the associated accuracies beneath some of the data points, to help correlate these results with the learning curve results in Figure 1. The s hape of the optim al training s et s ize curve in Figure 4 de-serve s some disc ussion. This c urve is not smooth and this is not becaus e we only calculate the optim al training s et size for certain relative cos ts. The curve is not s mooth becaus e the learning curve in Figure 1 is not smooth. You may also note that there is a sharp increase in slope when the rela tive cost increases from 4,000 to 5,000 (between these two values the optimal training set size jumps from 1,400 to 5,000 examples). This is due to the fact that the learning curve in F igure 1 temporarily shows a small decr eas e in accuracy when the training set size increases bey ond 1,400 and hence the cos t ratio m ust increas e significantly to overcom e the  X  X urden X  of purchasing training exam ples which do not increas e the accuracy of the clas sifier. In this section we present sum mary results for all of the data sets. Figures 5 and 6 show the learning curves for the large and small data sets, respectively . Note that for m any of the data sets a pla-teau is not reached us ing the available training data. F or some, like coding, the perform ance is still im proving relatively rapidly , while for others, like adult, it is improving only slowly . Accur a cy ( % ) Ac c u ra c y Figures 7 and 8 show the optimal training s et s izes for the large and small data sets, respectively (the curve for adult is not pro-vided again). Note that once the relative cos t is sufficiently high, all of the potentially available training data will be used and then the optim al training set size curve will flatten out com pletely . The results in Section 4 demonstr ate that one can improve overall classifier utility by properly trading-off the cost of acquiring addi-tional training data with its benef its (i.e., fewer errors). However, to be of practical use, we need a strategy that identifies a good (near-optim al) training s et s ize g without acquiring/using more than g training exam ples . That is , we m ust  X  X ay X  for training ex-am ples  X  up front X , so once we acquire them we will alway s use them. The strategy used in Section 4 of try ing a variety of training set sizes up until the m axim um num ber available makes no sense in this context. W hat we need is a progressive sampling strategy to identify g while purchasing only g exam ples . The general outline of a progressive sam pling strategy is sim ple. You begin with som e initial am ount of training data and then, iteratively , build a clas sifier, evaluate its perform ance and acquire additional training data. There are two key decis ions faced by such a progressive sampling algorithm: 1) when to terminate the loop and stop acquiring training da ta, and 2) how many training exam ples to acquire at each iteration (i.e., the batch s ize). In our progressive sampling expe riments we use a simple stopping strategy : we stop obtaining more training data after the first ob-served increase in total cost. Note that this guarantees that we will not achieve the optim al cos t becaus e, at m inim um, there is one better training set size (i.e., the one observed before the increas e). That is, once we have acquired a dditional training data we have incurred the cos t as sociated with purchas ing it, and m ust include this cos t we analy zing the perform ance of the progres sive sam-pling strategy . If the accuracy of the learning curve is non-decreasing, then this stopping condition will lead to a training set size that is very clos e to optim al. Our res ults show that the actual learning curves are not alway s non-decreasing, but this does not usually have a big impact on the results. The choice of how much additional training data to acquire at each iteration is decided by a s ampling s chedule. In this paper we only evaluate very simple, non-adaptive sampling schedules, al-though more sophisticated ones ar e described later as possible future work. W e utilize the sam pling strategies S1 and S2 that were described in Section 3.1 as our progressive sampling strate-gies. With a few exceptions, S1 samples every 1000 examples whereas S2 us es a geom etric s ampling s chem e that starts with 50 examples and then repeatedly doubles the training set size. This section presents the results of the two progressive sampling strategies , S 1 and S 2. As des cribed earlier, each s trategy term i-nates after the firs t obs erved increas e in total cos t. Becaus e we want to see how well thes e s trategies perform , we com pare them to the  X  optim al strategy  X  that alway s selects the optim al training set size and cost based on S1 (w hich samples more frequently than S2). We als o com pare thes e progres sive s ampling s trategies to our  X  straw m an X  strategy , which sim ply uses all of the avail-able training data. 1 This second comparison quantifies the benefit of considering the training data cost when building a classifier, since without such knowledge a reasonable strategy would be to use all potentially available training data. We begin by presenting detailed re sults for the adult data set. Table 2 presents the results for the progressive sampling strategies S1, S 2 and the  X  optim al X  vers ion of S1, Optimal-S1. We report the res ults for a variety of relative cos t ratios . For each cos t ratio and strategy , we report the selected training set size, the total cost and the CPU time, in seconds, asso ciated with all of the experi-ments used to identify that training s et s ize. F or exam ple, for a relative cost ratio of 10,000 a training set size of 9,000 yields the optimal cost, which is 152,900. The total CPU time required is 9.15 seconds, which is the time to build all of the classifiers using the sam pling schedule, up until the training set size of 10,000. Table 2: Progressive Sampling Strategy Comparison for Adult Table 2 shows us that S1 and S2 are quite effective s trategies , since they com e relatively clos e to achieving the optim al cos t. The S2 strategy seems to outperform S1, although if more training data were available we would expect S 1 to do better X  X  ince each strategy stops one iteration after the lowest cost that extra step would be m ore cos tly for S 2, which geom etrically increas es the training set size. Our total cost m etric does not factor in CPU tim e, but the results for the adult data set indicate that this is probably okay , since the CPU times are all quite sm all. However, this might not be true for much la rge r data sets. We disc uss e xten-sions to the total utility m etric to factor in the cost of com putation in Section 7. Figure 9 com pares the perform ance of the S 1 and Straw Man strategies for cost ratios below 1: 10,000. Note that the x-axis is not scaled in this cas e, in order to m ake the results easier to read. There may not alway s be a maximum  X  X mount of available training data X  for a data set, but in m any cases there will be (e.g., the number of records describing businesses is limited by the number of businesses). In this paper we assume that the only data available is in the original data set and the m axim um amount available for training is e qual to 75% of this amount. We see that the straw man strategy of just using all of the training data independent of the relative cost ratio leads to very poor re-sults until a relative cost ratio of about 1:10,000 is reached. This motivates the need and benefit of factoring in the training data cos t when building a clas sifier. 
Figure 9: Comparison of S1 and Straw Man Strategies for Adult A com paris on of the perform ance of the S 1 strategy with the S 1-optimal strategy , for the five large data sets, is provided in Table 3. The results show that the S1 progressive sampling strategy is quite effective, except for very lo w relative costs. In these situa-tions the training data is relatively expensive and the stopping criteria, which requires that th e sampling strategy go past the optim al training s et s ize, is heavily penalized. An adaptive sam-pling s trategy that reduces the batch size as increas es in training set size lead to sm aller im provem ents in total utility would likely reduce the im pact of this problem . The results for the S2 strategy are not included in this section due to space cons iderations and becaus e it is not that different than the S 1 sampling s trategy given the amount of available training data (it would be very different if the data sets were m uch larger). Re lative Cost Ratio Adult Blackjack Boa1 Codi ng Network1 Table 4 com pares the straw man strategy with the S1 progressive sampling strategy for the five larg e data sets and shows that, con-sistent with the results presented in Figure 9 for the adult data set, the straw man strategy performs very poorly when the relative cost ratio is below a certain threshold. After a point they perform sim ilarly and the straw m an strategy is even som etim es superior, becaus e the S 1 strategy sometim es stops prem aturely , due to a tem porary decreas e in accuracy in the learning curves . If m ore training data were available, s uch that the learning curves reached a plateau with only a fraction of this training data, we would then expect the straw man strategy to perform poorly even for these higher cost ratios. Previous research, to the best of our knowledge, does not directly addres s the cos t of cas es that we addres s in this paper. However, there is a substantial amount of re search that studies related costs and issues. W e des cribe thes e res earch efforts in this section and comment on how they relate to our work. Work on progressive sampling ha s focused on efficiently finding the training s et s ize where the learning curve reaches a plateau [7] . The motivation for that work is to reduce the cos t of com puta-tion X  X he training data cost is not taken into account. If we ignore the cost of computation, the past work on progressive sampling is a s pecial cas e of our work, where the cos t of training data is arbi-of the available training data even if a plateau is reached). This previous work showed that a ge ometric sampling scheme, similar to our S2 strategy , is asy mptoti cally optim al, with respect to the cost of computation (not with re spect to training data cost). As described shortly , we plan to generalize our work to include the cost of com putation, at which point our work will subsum e this previous work on progressive sampling. Weiss and Provost [11] factored in the cost of cases, but only in a lim ited way . The assum ption in that work was that the cost of cases limited the amount of traini ng data and this amount is al-ready specified. The only decis ion in that work was what clas s distribution should be used for tr aining in order to maximize clas-sifier perform ance. That work als o us ed a progres sive sampling strategy , although the goal in that case was to identify the optimal class distribution. In this paper we assum e that one has no control over which exam-ples are added to the training data. That is , if there is a cos t as so-ciated with labeling an example, we cannot selectively choose which exam ples to label and if there is a cos t associated with measuring feature values, we cannot determine which examples to add based on these feature acquisition costs. Thus, we do not consider active learning methods which have been used in other work [1, 4, 10, 13] . Nonethele ss, one does not alway s have the freedom to use active learning me thods (e.g., in the scenario where one is purchas ing data from an external source). Als o, much of the work on active learning assumes a fixed budget [4] , in which cas e the decis ion is jus t which exam ples are bes t to label or features to m eas ure, and there is no need to determ ine when to stop acquiring more training data. The closest match to our re-search from the active learning com munity is res earch where the marginal utility of each exam ple is estim ated and this is used to determine how many exam ples to label [5] . One of the contributions of our re search is that it shows how the optimal training set size varies based on the relative cost of train-may be useful even if the specific cost ratios are not known. The cost curves of Drummond and Holte [2] are analogous to our op-tim al curves , except that their cu rves show the optimal perform-ance based on the ratio of the cost of a false positive to a false negative clas sification error . Both their curves and ours can aid a practitioner who m ust m ake decisi ons about how to generate the best c lassifie r. [1] Cohn, D., Atlas, L., and La dner, R. Improving Generaliza-[2] Drum mond, C. and Holte, R. Explicitly Representing Ex-[3] Hoehn, B., S outhey , F., H olte, R., and Bulitko, V . Effective The work described in this pape r has several limitations and can be extended in many way s. In this section we describe some of the lim itations and possible future extens ions . W e expect to ad-dres s m any of thes e is sues in the near future. One of the lim itations of our work concerns the s ize of the data sets. Ideally we would have sufficient training data for all of our data s ets so that the learning curves would alway s reach a plateau. If that were the case then additional data would not be of any benefit and then we could completely analy ze the behavior of the data set with respect to training set size. Unfortunately , for m any of our data sets a plateau is not reached. It would therefore be valuable to analy ze much larger data s ets, es pecially thos e that are complex enough to require a great deal of training data in order for the learning curve to reach a plateau. [4] Kapoor, A., and Greiner, R. Learning and Classify ing under [5] Melville, P., Saar-Tsechansky , M., Provost, F. and Mooney , [6] Newm an, D.J., Hettich, S., Blak e, C.L. and Merz, C.J. UCI Our utility m etric considers the cost of data but not the cost of com putation (i.e., CPU tim e). W e intend to include the cost of com putation in future analy ses. However, since both progressive sampling strategies required le ss than one minute of CPU time when applied to each of the ten data s ets, it is im portant that we first obtain much larger and more complex data sets. In addition, we intend to analy ze more sophisticated sampling schedules, in-cluding adaptive schedules, where the amount of training data reques ted in each  X  batch X  varies bas ed on the expected change in total cost (which could be extr apolated based on the changes in total cost for the previous batc hes). These more sophisticated schemes would be more likely to find the  X  X ptimal X  training set size, by reducing the batch size as the m arginal utility of adding training data approaches zero. Note that this behavior is the oppo-site of what happens when the co st of computation is the main cost; the past work on progressive sampling increases the batch size over time since it uses a geometric sampling scheme [7] . [7] Provost, F., Jensen, D., and Oate s, T. Efficient Progressive [8] Quinlan, J. R., C4.5: Programs for Machine Learning. San [9] Turney , P. Ty pes of Cost in Inductive Concept Learning. [10] Veeram achaneni, S ., and Aves ani, P . Active S ampling for This paper analy zed the im pact of training data cos t on total clas -data as well as the perform ance of the clas sifier on clas sifying new exam ples . We introduced a variety of charts to help vis ualize the relationship between training data cost, the cost of errors and total utility . W e also identified the optim al training set size for different data sets and different cost ratios and showed that over-all utility can be substantially im proved by not using all of the training data that is potentially av ailable. Two simple progressive sampling stra tegie s we re also in troduced and were shown to be relatively effective in finding the optim al training set size and optim al total utility . Furtherm ore, one of these progressive sam -pling strategies was shown to out perform the  X  X traw man X  strat-egy of using all potentially available training data. The research described in this paper fills in a  X  hole X  in the area of Utility -Based Data Mining by considering the cost of training cases in the data mining process. [11] Weiss, G. M. and Provost, F. Learning when Training Data [12] Weiss, G. M ., S aar-Ts echans ky, M . and Zadrozny , B., P ro-[13] Zheng, Z., and Padmanabhan, B. On Active Learning for 
