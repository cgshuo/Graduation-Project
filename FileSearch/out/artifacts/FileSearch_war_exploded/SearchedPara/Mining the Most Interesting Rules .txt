 Several algorithms have been proposed for finding the  X  X est, X   X  X pti-mal, X  or  X  X ost interesting X  rule(s) in a database according to a vari-ety of metrics including confidence, support, gain, chi-squared we show that the best rule according to any of these metrics must reside along a support/confidence border. Further, in the case of conjunctive rule mining within categorical data, the number of rules along this border is conveniently small, and can be mined efft-ciently from a variety of real-world data-sets. We also show how this concept can be generalized to mine all rules that are best according to any of these criteria with respect to an arbitrary subset of the population of interest. We argue that by returning a broader set of rules than previous algorithms, our techniques allow for improved insight into the data and support more user-interaction in the optimized rule-mining process. There are numerous proposals for mining rules from data. Some are constrainf-based in that they mine every rule satisfying a set of hard constraints such as minimum support or confidence (e.g. [1,2,6]). Others are heuristic in that they attempt to find rules that are predictive, but make no guarantees on the predictiveness or the completeness of the returned rule set (e.g. decision tree and covering algorithms [9,15]). A third class of rule mining algorithms, which are the subject of this paper, identify only the most interesting, or optimal, rules according to some interestingness metric [ 12,18,20,24]. Optimized rule miners are particularly useful in domains where a constraint-based rule miner produces too many rules or requires too much time. It is difficult to come up with a single metric that quantifies the  X  X nterestingness X  or  X  X oodness X  of a rule, and as a result, several different metrics have been proposed and used. Among them are confidence and support [I], gain [ 121, variance and chi-squared value [17,18], entropy gain [16,17], gini [16], laplace [9,24], lift algorithms are known to efficiently find the best rule (or a close approximation to the best rule [ 161) according to a specific one of simple concept of rule goodness captures the best rules according to any of them. This concept involves a partial order on rules defined in terms of both rule support and confidence. We demonstrate that the set of rules that are optimal according to this partial order includes all rules that are best according to any of the above metrics, even given arbitrary minimums on support and/or confidence. details, but the problem can be formally stated in an identical manner1 : 
PROBLEM (OFVMIZED RULE MINING): Find a set A, L U such that 
Any rule A + C whose antecedent is a solution to an instance I of the optimized rule mining problem is said to be I -optimal (or just optimal if the instance is clear from the context). For simplicity, we sometimes treat rule antecedents (denoted with A and possibly some subscript) and rules (denoted with r and possibly some subscript) interchangeably since the consequent is always fixed and clear from the context. 
We now define the support and confidence values of rules. These values are often used to define rule constraints by bounding them above a pre-specified value known as minsup and minconf respectively [I], and also to define total orders for optimization 112,201. The support of a condition A is equal to the number of records in the data-set for which A evaluates to true, and this value is denoted as sup(A). The support of a rule A + C, denoted similarly as sup(A + C) , is equal to the number of records in the data-set for which both A and C evaluate to true.* The antecedent support of a rule is the support of its antecedent alone. The confidence of a rule is the probability with which the consequent evaluates to true given that the antecedent evaluates to true in the input data-set, computed as follows: 
Many previously proposed algorithms for optimized rule mining solve specific restrictions of the optimized rule mining problem. 
For example, Webb [24] provides an algorithm for mining an optimized conjunction under the following restrictions: l U contains an existence test for each attribute/value pair appear-ing in a categorical data-set outside a designated class column; 5 orders rules according to their laplace value (defined later); N is empty. 
Fukuda et al. [ 121 provide algorithms for mining an optimized dis-junction where: l U contains a membership test for each square of a grid formed by discretizing two pre-specified numerical attributes of a data-set (a record is a member of a square if its attribute values fall within the respective ranges); &lt; orders rules according to either confidence, antecedent sup-port, or a notion they call gain (also defined later); 
N includes minimums on support or confidence, and includes one of several possible  X  X eometry constraints X  that restrict the allowed shape formed by the represented set of grid squares: 
Rastogi and Shim [20] look at the problem of mining an optimized disjunction where: 
U includes a membership test for every possible hypercube defined by a pre-specified set of record attributes with either  X  issue for simplicity of presentation. corresponds to our notion of antecedent support. 
Consider also the similar partial order SS, such that rl cSyC r2 if and only if: sup(rl) I sup(rJ A conf(r,) &gt; conf(rJ , or sup(r,) &lt; sup(r*) A conf(r,) 2 conf(r2). 
The equivalence condition is the same as before. An optimal set of rules according to this partial order forms a lower border. 
In this section, we show that for many total orders &lt;, intended to rank rules in order of interestingness, we have that is useful due to the following fact: 
LEMMA 3.1: Given the problem instance I = (U, D, I,, C, N) such that I, is implied by I,,, an I -optimal rule is contained within any I,, -optimal set where I,, = (U, D, I,,, C, N&gt; . 
Proofi Consider any rule r, that is not I,, -optimal (for simplicity we will ignore the presence of constraints in N). Because rl is non-optimal, there must exist some rule r2 that is optimal such that r, &lt;SC r2. But then we also have that r1 I, r2 since I, is implied by ssc. This implies that any non-l,, -optimal rule is either non-l -optimal, or it is equivalent to some Z-optimal rule which resides in an I,, -optimal equivalence class. At least one 
Is, -optimal equivalence class must therefore contain an I -opti-mal rule. Further, because = , is implied by =sc, every rule in this equivalence class must be I -optimal. By definition, an I,, -optimal set will contain one of these rules, and the claim fol-lows. 0 
Put simply, mining the upper support/confidence border identifies optimal rules according to several different interestingness metrics. 
We will show that these metrics include support, confidence, conviction, lift, laplace, gain, and an unnamed interest measure proposed by Piatetsky-Shapiro [ 191. If we also mine the lower trivially monotone in support as long as c 2 0. We can ignore the case where c &lt; 0 if we assume there exists any rule r satisfying the input constraints such that conf(r) 2 0. This is because for any pair of rules rt and r2 such that conf(r,) 5 0 and conf(r.$ &lt; 0, we know that gain(r)) 2 gain(rz) irrespective of their support. Should this assumption be false, the gain criteria is optimized by any rule with zero support should one exist (e.g. in the conjunctive case, one can simply add conditions to a rule until its support drops to zero). The gain function is monotone in confidence among rules with equivalent support for reasons similar to the case of the Laplace function. If support is held constant, then an increase in gain implies a decrease in the subtractive term. The subtractive term can be decreased only by reducing antecedent support, which implies a larger confidence. Note that, like k from the Laplace function, after identifying the optimal set of rules, the user can vary 8 and view its effect on the optimal rule without additional mining. Another interestingness metric that is identical to gain for a fixed value of 0 = sup(C)/ID] was introduced by Piatetsky-Shapiro [193: Consider next conviction [8], which was framed in [6] as a function of confidence: Conviction is obviously monotone in confidence since confidence appears IO a subtractive term within the denominator. It is also unaffected by variations in rule support if confidence is held constant, which implies monotonicity. Lift, a well-known statistical measure that can be used to rank rules strength [lo]), can also be framed as a function of confidence [6]: Like conviction, lift is obviously monotone in confidence and unaffected by rule support when confidence is held fixed. The remaining interestingness metrics, entropy gain, gini, and chi-squared value, are not implied by 5,,. However, we show that the space of rules can be partitioned into two sets according to confidence such that when restricted to rules in one set, each metric is implied by ssc, and when restricted to rules in the other set, each metric is implied by &amp;. As a consequence, the optimal rules with respect to entropy gam, gini, and chi-squared value must reside on either the upper or lower support confidence border. This idea is formally stated by the observation below. .I OBSERVATION 3.4: Given instance I = (U, D, I ,, C, N&gt; , if I,, implies I, over the set of rules whose confidence is greater than equal to some value y. and &amp; implies I, over the set of rules whose confidence is less than or equal to y , then an I -optimal rule appears in either (a) any Is, optimal set where I,, = (U, D, ssc, C, N) , or (b) any Is, -optimal set where I,-,c = ( U, D, &amp;., C, A9 . To demonstrate that the entropy gain, gini, and chi-squared values satisfy the requirements put forth by this observation, we need to know when the total order defined by a rule value function is implied by &amp;. We use an analog of the Lemma 3.3 for this purpose: its interior region, such a value of 6 is guaranteed to exist unless x2 = 0, which is a trivial boundary case. Now, consider the line [(x,, y,), (x2 -6, y2)] . This line must contain a point (xs, ys) such that xj and ys are non-negative, and one or both of xg or y3 is non-zero. But because f is convex and minimum at (x3, y3), we have that fix,, y,) Iflx2 -6, y2) , which is a contradiction, Cl 
LEMMA 3.7: For a convex function Jlx, y) which is minimum at conf(x, y) = c , Jlx, y) is (1) anti-monotone in conf(x, y) for fixed y , so long as conf(x, y) I c , and (2) monotone in y when conf(x, y) = A for any constant A I c . Proof Similar to the previous. 0 
The previous two lemmas and Observation 3.4 lead immediately to the following theorem, which formalizes the fact that mining the upper and lower support-confidence borders identifies the optimal rules according to metrics such as entropy gain, gini, and chi-squared value. Conveniently, an algorithm specifically optimized for mining the upper border can be used without modification to mine the lower border by simply negating the consequent of the given instance, as stated by the subsequent lemma. 
THEOREM 3.8: Given instance I = (U, D, I,, C, N) , if sr is defined over the values given by a convex function Ax, y) over rules A + C where: (1)x = sup(A)-sup(AuC) andy = sup(AuC),and (2) Ax, y) is minimum at conf(x, y) = sup(C)/]D] , then an I -optimal rule appears in either (a) any Is, optimal set where I,, = (U, D, ssc, C, N&gt; , or (b) any I,, -optimal set where I,, = (U, D, I,,, C, N&gt; . LEMMA 3.9: Given an instance Is, = (U, D, &amp;, C, N&gt; , any 
I,,-optimal set for Is, = (U, D, Ssc, -C, iV) (where -C evaluates to true only when C evaluates to false) is also an I,,, -optimal set. 
Proof Idea: Note that conf(A + -CJ = 1 -conf(A + C) . Thus, maximizing the confidence of A + -C minimizes the confi-denceofA+C. Cl 
Before ending this section we consider one practical issue --that of result visualization. Note that the support-confidence borders as displayed in Figure 1 provide an excellent means by which optimal sets of rules may be visualized. Each border clearly illustrates the trade-off between the support and confidence. Additionally, one can imagine the result visualizer color-coding points along these borders that are optimal according to the various interestingness metrics, e.g. blue for Laplace value, red for chi-squared value, green for entropy gain, and so on. The result of modifying minimum support or confidence on the optimal rules could be displayed in real-time as the user drags a marker along either axis number of database passes while still substantially reducing the search space. For this purpose, a node is better than another if the rule it enumerates has a higher confidence value. The remaining modification is the incorporation of inclusive pruning as proposed by Webb [23]. This pruning strategy avoids enumerating a rule when it can be determined that its antecedent can be extended with an additional condition without affecting the support of the rule. In the absence of item constraints, this optimization prunes many rules that are either non-optimal or equivalent to some other optimal rule to be enumerated. Unfortunately, when there are item constraints in N (e.g.  X  X ules must contain fewer than k conditions X ), this pruning strategy cannot be trivially applied without compromising completeness, so it must be disabled. Full details of this pruning strategy are provided in Appendix B. We evaluated our algorithm on the larger of the categorical data-sets from the Irvine machine learning database repository,6 including chess, mushroom, letter, connect-4, and dna. We also used the pums data-set from [6] which is compiled from census data (a similar data-set was used in [8]). For the Irvine data-sets, we used each value of the designated class column as the consequents. For the pums data-set, we used the values of the RELATl column (13 in a11)7. Each of these data-sets is known  X  X o be difficult for constraint-based rule mining algorithms such as Apriori, even when specifying a strong minimum support constraint [4,6,8]. Experiments were performed on an IBM IntelliStation with 400 MHZ Intel Pentium-II processor and 128 MBytes of main memory. Execution time and the number of rules returned by the algorithm appear in Table 1; characteristics of each data-set appear in Table 2. For the Irvine data-sets, with the exception of connect-4, our algorithm identified an optimal set of rules within 30 seconds in every case, with many runs requiring less than 1 second. Connect-4 substantially more records and more columns than many of the other data-sets. But a stronger contributor to this discrepancy was the fact that rules with high confidence within the connect-4 data-set have very low support. For example, with the  X  X ie X  class as the consequent, rules with 100% confidence have a support of at most 14 records. This property greatly reduces pruning effectiveness, resulting in almost one hour of execution time given this consequent. In cases like these, modest settings of the minimum support or confidence constraint can be used to improve nmtime considerably. For example, a minimum support of 676 records (1% of the data-set) reduces execution time to 6 minutes. The number of rules in each optimal set was on the order of a few hundred at most. Of the Irvine data-sets, connect-4 contained the We plot the upper support-confidence border for each of these consequents in Figure 3. Rule support is normalized according to consequent support so that each border ranges from 0 to 100% along the x axis. While sc-optimality is a useful concept, it tends to produce rules that primarily characterize only a specific subset of the population 
The values 1-12 for the RELATI colum~corres~~td to items O-12 in the apriori binary format of this data. only if: poWI) E pop(r2) A conf(r,) &lt; conf(r*) , or pop(r,) c pop(r.J A conf(r,) I conf(rz) . 
Two rules are equivalent according to this partial order if their population sets are identical and their confidence values are equal. 
One can analogously define the partial order 5,,* where r, &lt;,,* r2 if and only if: pop@,) E pop(rJ A conf(r,) &gt; conf(r$ or pop(r,) c pop@&amp; A conf(r,) 2 conf(rz) . 
It is easy to see that I,, is implied by 5 all the claims from Section 3 also IT optimality. Note that &lt;PC results in more incomparable rule pairs than ssc due to the population subsumption requirement. This implies there will be many more rules in a pc-optimal set compared to an SC-optimal set. The consequence of these additional rules, as formalized by the observation below, is that a pc-optimal set always contains a rule that is optimal with respect to any of the previous interestingness metrics, and further with respect to any constraint that requires the rules to characterize a given subset of the population of interest. 
OBSERVATION 4. I : Given an instance I = ( U, D, S ,, C, N) where (1) I, is implied by 5,,,, and (2) N has a constraint R on rules r stating that P c pop(r) for an Z-optimal rule appears in any Z,,c -optimal set where I,,c = (U, D,I,,, C, N-(n)). We note that this generalization of sc-optimality is quite broad. 
Given the large number of rules that can appear in a pc-optimal set (see next subsection), a more controlled generalization might be desirable. One idea is to have the rule set support any constraint that requires a rule to characterize a given member t (in place of a broad characterization, but potentially with much fewer rules. This notion designates a rule r as uninteresting if there exists some set of rules R such that (1) each rule in R satisfies the input constraints, (2) every member of pop(r) is characterized by at least one rule in R and (3) each rule in R is equal to or better than r according to I,,. (Note that pc-optimality designates a rule as uninteresting only if there exists such a set R containing exactly one rule.) This notion of uninterestingness cannot, to our knowledge, be expressed using a partial order on rules. Instead, we are examining how the concept of pc-optimality combined with constraints can successfully express (and exploit) this notion. 
Producing an algorithm that can efficiently mine an optimal set of conjunctions with respect to I,, may seem an impossible proposition in large data-sets due to the need to verify subsumption Figure 5. Algorithm for mining a pc-optimal set of conjunctions. OBSERVATION 4.4: Given an instance Ipc = ( U, D, Spc, C, N) , a Using the terminology from [6], this observation simply states that a pc-optimal rule must have a non-negative improvement value. We can in fact require that improvement be positive since we only need one member of each equivalence class. The Dense-Miner algorithm already provides pruning functions that bound the improvement value of any rule derivable from a given node. We thus modify Dense-Miner to prune nodes whose improvement bound is 0 (see Appendix B for the simplified pruning functions that can be used for this special case). We also again exploit Webb X  X  inclusive pruning strategy for the case where there are no item constraints in N . We can now fully describe an algorithm for mining an Zoc -optimal set of rules without performing any explicit subsumptton checks between rule populations (Figure 5). We use the above-described variant of Dense-Miner to implement step 1 of this algorithm. Step 3 applies Theorem 4.3a to identify non-optimal rules for removal. This step requires we first associate each rule with its a-maximal rule (step 2). To find the a-maximal rules, we apply the algorithm For a data-set such as connect-4, our implementation of this step requires less than 3 seconds for a set with up to 10,000 rules. Finally, step 4 applies Theorem 4.3b in order to identify equivalent rules so that there is only one representative from each equivalence class in the returned result. This algorithm could be simplified slightly when the input constraints N have the property that amax satisfies N whenever r does. In this case, the set R, could be returned an end-user. We lastly generalized this concept using another partial order I,, in order to ensure that the entire population of interest is well-characterized. This generalization defines a set of rules that contains the most interesting rule according to any of the above metrics, even if one requires this rule to characterize a specific subset of the population of interest. 
These techniques can be used to facilitate interactivity in the process of mining most-interesting rules. After mining an optimal the most-interesting rule according to any of the supported interestingness metrics without additional querying or mining of the database. Minimum support and confidence can also be modified and the effects immediately witnessed. After mining an optimal set of rules according to the second partial order, in addition to the above, the user can quickly find the most-interesting rule that characterizes any given subset of the population of interest. This extension overcomes the deficiency of most optimized rule miners where much of the population of interest may be poorly characterized or completely uncharacterized by the mined rule(s). We are indebted to Ramakrishnan Srikant and Dimitrios Gunopulos for their helpful suggestions and assistance. [31 141 I71 [91 [IO] Dhar, V. and Tuzhilin, A. 1993. Abstract-driven pattern dis-Since C and D are constant for a given instance of the problem, terms such as p(0) = sup(C)/ID] are constants. Any of the above variable terms can be expressed as functions of x = sup(A) -sup(A -+ C) and y = sup(A + C) , and hence so can the functions themselves. For example, sup(A) = y +x, sup(+ = IDI -(Y +x) , p(A) = Y/(Y + x) , P(A) = X/(Y + X) , and so on. The set-enumeration tree search framework is a systematic and complete tree expansion procedure for searching through the on the elements of U . The root node of the tree will enumerate the empty set. The children of a node N will enumerate those sets that can be formed by appending a single element of U to N, with the restriction that this single element must follow every element already in N according to the total order. For example, a fully-expanded set-enumeration tree over a set of four elements (where each element of the set is denoted by its position in the ordering) appears in Figure 6. 1,2\ lj3 1,4 213 2,4 3,4 1,2,3 1,2,4 1,3,4 23 Figure 6. A complete set-enumeration tree over a 4 item set. 
