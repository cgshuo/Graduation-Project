 fi cation results were carried out using AWNN implemented in Matlab 1. Introduction
Simulation techniques are becoming more present in automotive development process each day. They are not covering only single components, but frequently the system approach is observed. The automotive system complexity and the aim on high model accuracy lead to a nonlinear multivariable basis. White-box modeling is being one of the main tools used by the development engineers to represent the system interactions and laws however; the level of detail and the knowledge needed to build up models are still issues ( Yu and Cheng, 2011; Cieslar et al., 2014; Maghbouli et al., 2013;
Pariotisa et al., 2012 ). On the other hand, system identi black-box modeling is being proved to be reliable and powerful on system representations for different classical problems.
System identi fi cation is basically the process of developing or improving a mathematical repres entation of a system/process on forecasting the behavior of the rea l system under different operating conditions. Appropriate mathematical modeling through systems identi fi cation plays a signi fi cant role in natural sciences and engi-neering fi elds including simulation, automatic control, fault tolerant analysis, forecasting, fi ltering, among others.

Many linear system identi fi cation methods have been devel-oped in the past decades ( Ljung, 1999; S X derstr X m and Stoica, 1989; Wang et al., 2012 ) and attained a state of maturity. On the other hand, identi fi cation of nonlinear systems is a relatively new topic of interest ( Haber and Unbehauen, 1990; Leontaritis and have been made to solve the nonlinear identi fi cation problems and several ef fi cient identi fi cation methods and algorithms ( Gregor and Lightbody, 2008; Su et al., 2013; Kou et al., 2011; Tijani et al., 2014; Ko, 2012 ) have been proposed.

System identi fi cation approaches that range from a simple linear model to a more complicated nonlin ear one are not easy to handle. In this context, the black-box modeling techniques to nonlinear identi-fi cation can be useful in the automotive models on and off-board when experimental data is available. Some attempts have been made to develop nonlinear identi fi cation approaches to automotive appli-cations. In this context, see examples in ( Keen and Cole, 2012; Ye, 2007; Liu and Bewley, 2003; Zito and Landau, 2005; Wu and Liu, 2009; Worden et al., 2001; Tan and Saif, 2000; Togun et al., 2012; Antory, 2007; Coelho et al., 2014 ).

Recently, natural computing techniques, such as arti fi cial neural networks (ANNs) ( Kilic et al., 2014; Gil et al., 2013 ), fuzzy systems ( Babu  X  ka and Verbruggen, 1996; Dov  X  an and  X  krjanc, 2010 )and combined neuro-fuzzy systems ( Salahshoor et al., 2012 )havebecome those systems whose mathematical models are dif fi cult to obtain. In this context, ANNs are good on tasks such as pattern matching and classi fi cation, function approximation, optimization and data cluster-ing. The feed-forward ANNs can approximate the behavior of an arbitrary continuous or  X  otherwise reasonable  X  function within arbitrary accuracy on a compact domain (this result is called universal approximation theorem, see details in ( Tikk et al., 2003 ; Scarselli and Tsoi, 1998 )).

Since the emergence of ANN, this fi eld gains a great interest by the researchers and new structures of ANNs have been proposed.
The arti fi cial wavelet neural network or wavenet (AWNN), which logically connects an ANN with wavelet decomposition, is based on an ANN structure and involves the wavelet transform. The main merit of wavelet transform over Fourier transform is the ability of specifying the time  X  frequency position. In other words, AWNNs combine the capability of arti fi cial neural networks in learning from processes and the capability of wavelet decomposition.
On the other hand, swarm intelligence (Eberhart et al., 2001) is a category of stochastic search optimization algorithms of natural computing fi eld based on procedures inspired by collective beha-vior and emergent intelligence in natural environment. The intelligence emerges from a chaotic balance between sociality and individuality. The arti fi cial bee colony (ABC) algorithm is one of these swarm intelligence algorithms that has shown potential and good performance for solving various optimization problems ( Karaboga and Basturk, 2008 ). ABC algorithm, described in
Karaboga (2005) , is inspired from the foraging behavior of honey bee colonies include three groups of bees: employed bees, onloo-kers and scout bees. Recently, several ABC approaches have been proposed in optimization applications (see Zhang et al., 2012; Li et al., 2014b; Tasgetiren et al., 2013 ).

This paper proposed an AWNN approach combined with an arti fi cial bee colony approach to tune the spread of wavelet functi-ons and the maximum number of neurons of the hidden layer in the network. This work attempts to demonstrate the feasibility of adapting the proposed AWNN to model the behavior of a real truck engine in an unloaded operation. The engine is modeled in the torque and crankshaft speed generation perspectives, meaning that the multivariable system representation shall deal with at least 15 input variables and nine modeled outputs. The technique shall use as input an experimental database with around 18,000 samples. The model quality will be evaluated by using control parameters like the multiple correlation coef fi cient, the mean squared error, the mean absolute percentage error and the time to training of AWNN model.
The fi nal usage for this crank-resolution engine is intended to hardware-in-the-loop simulations and embedded systems where actually the CFD, white and gray models are the most common alternatives. The wavenets' outcome is also compared to three classic algorithms: Elman neural network, Jordan network and kernel adaptive fi ltering, KAF, in order to check their performance.
Elman and Jordan networks are well-known recurrent neural net-works that have been applied to dynamic modeling ( Pham et al., 1999; Muldera et al., 2015; Moghaddamniaa et al., 2009 ). The kernel based methods are also very powerful with applications from the support vector machines up to the fi ltering ( Constantina and Lengell X b, 2013 ).

Finally, the black-box model generated can be applied to engine calibration purposes, fuel consumption analysis or even for com-ponent behavior study.

The remainder of this paper is organized as follows. In Section 2 , the fundamentals of system identi fi cation are described. The back-ground of the AWNN and the proposed AWNN based on ABC tuning are summarized in Section 3 . Description of case study of a real truck respectively. Benchmarks are presented in Sections 6  X  8 .Finally,the conclusion and further re search are reported in Section 9 . 2. Brief fundamentals of system identi fi cation
The input  X  output model is a mean of describing the dynamics of a system. System identi fi cation is the process of creating models of dynamic process from input  X  output signals. Nonlinear system iden-ti fi cation, which is to estimate models of nonlinear dynamic systems
In general, a good model of the system dynamics can facilitate model-based design, allowing applications of control design and analysis tools. In many practical situations, designing an appropriate model of a system/process is a challenging task due to: (i) the structural complexity and nonlinearity of the plant, (ii) the degree of unknown dynamics, typically noise and outliers associated with the system/process, and (iii) the requirement for selection of proper model structure along with the accuracy and ef fi ciency of the
In general, the procedure of identi fi cation adopted is summar-ized by the following steps:
Step (i) Design an experiment to obtain the process input/ output data sets pertinent to the model application.

Step (ii) Examine the quality of measured data, removing trends and outliers.

Step (iii) Determine a class of models and construct a set of candidate models based on information from the experimental data sets (or simulation data sets). This step is the model structure identi fi cation.

Step (iv) Select a particular model from the set of candidate models and estimate the model parameter values using the experimental data sets (or simulation data sets).

Step (v) Evaluate how good the model is using a performance criterion. This step is the validation of the modeled system.
Step (vi) If a satisfactory model is still not obtained in Step v then repeat the procedure either from Step (i) or Step (iii), depending on the problem. 3. Arti fi cial wavelet neural networks
This section presents an overview of the wavelet transform and its features. After, we present in details the AWNN design and the proposed AWNN model based ABC tuning 3.1. Wavelet transform
Among almost all the functions used for approximating arbitrary signals or functions, none has ha d such an impact and spurred so much interest as wavelets. Multiresolution wavelet expansions out-perform many other approximation schemes and offer a fl exible capability for approximating arbitrary functions. Wavelet basis func-tions have the property of localization in both time and frequency.
Due to this inherent property, wavelet approximations provide the foundation for representing arbitrary functions economically, using just a small number of basis functio ns. Wavelet algorithms process data at different scales or resolutions. Wavelet analysis is based on a wavelet prototype function, called the analyzing wavelet, mother wavelet, or simply wavelet. Temporal analysis is performed using a contracted, high-frequency versi on of the same function. Because the signal or function to be studied can be represented in terms of a wavelet expansion, data operations can also be performed using the corresponding wavelet coef fi cients ( Wei and Billings, 2002 ).
The best way to introduce wavelet transforms is through their comparison to Fourier transforms, a common signal analysis tool.
The Fourier transform converts a signal expressed in the time domain to a signal expressed in the frequency domain. The transform works well if stationary signals are considered. In other words, Fourier analysis is not effective when used on non-stationary signals because it does not provide frequency content information localized in time: the frequency component of a signal can be known but its location in time is not known. To obtain time information about a signal, in addition to frequency information, the short time Fourier transform was developed by Gabor. It analyses a small section of the signal at a time, which is known as windowing. The window is a square wave, which truncates the sine or cosine function to fi t a window of particular width.
The wavelet transform was developed as a method to obtain simultaneous, high resolution time and frequency information about a signal. It presents an improvement over the short time Fourier transform because it obtains good time and frequency resolution simultaneously by using a variable sized window region (the wavelet) instead of a constant window size. Because the wavelet may be dilated or compressed, different features of the signal are extracted. While a narrow wavelet extracts high fre-quency components, a stretched wavelet picks up on the lower frequency components of the signal ( Lardies, 2007 ). 3.2. Design of AWNNs
The basis functions ANNs are a class of ANNs, in which the output of the network is a weighted sum of a number of basis functions. The usually used basis functions include Gaussian radial basis functions, B-spline basis functions, wavelet basis functions and some neuro-fuzzy basis functions.

An AWNN was fi rst proposed by Zhang and Benveniste (1992) as an alternative to the classical feed-forward ANN for approx-imating arbitrary nonlinear functions, inspired by both the feed-forward ANN and wavelet theory. In the WNNs structure, the layers are arranged to have a three-layer feed-forward design, which is comprised of an input layer, a wavelet layer and an output layer.

The main mathematical representation for a wavenet is giving according to the following equation: gx  X  X  X  where g ( x ) stands for the result functions, t i , D i and R respectively translation, dilatation and rotation matrices. the wavelet function and  X  i for the gain applied to the network branch. Finally, g 0 is used in order to ease the non-null average function's approximation.

The wavenet topology proposed by Zhang and Benveniste (1992) is shown in Fig. 1 . Each network branch, shown in Fig. 1 , that has rotational, dilatation and translation matrices combined with a wavelet function is called wavelon.

According to Righeto et al. (2004) , the combination between wavelet theory and neural networks creates a new modeling method called adaptive wavelet neural network or wavenet. This arrangement provides reliable signal processing behavior focused on system modeling and especially for nonlinear systems. Part of the modeling success is relying on the correct wavelet choice for each of the wavelon layers as well as the number of the neurons applied.

The selection of transfer function is crucial for the approxima-tion property and the convergence of an AWNN. This paper presents the AWNN modeling based on two types of wavelet functions: the Morlet wavelet and the Rickert wavelet.
The Morlet wavelet was formulated by Goupillaud et al. (1984) as a constant subtracted by a planar wave in a Gaussian envelop. It is consider a continuous wavelet de fi ned as follows:  X   X  t where the admissibility criterion is de fi ned by the following equation:  X   X  and the normalization constant is given by the following equation: c  X  X  The Morlet wavelet pro fi le is presented in Fig. 2 .

Rickert wavelet is de fi ned as the negative normalized second derivative of a Gaussian function or second Hermite function. It is also called as Mexican hat due to its shape similarity with the typical Mexican object. It is considered a continuous wavelet and it is de fi ned as follows (see details in Fig. 3 ):  X   X  t 3.3. The proposed arti fi cial neural network (ANN) approach based on ABC tuning It is adopted an ANN modeling with the following procedure: the
ABCalgorithmrunsandselectsthe fi rst parameter candidates. Each single candidate is applied to the ANN training, then the trained network is simulated and its fi tness is recorded. When the training and validation for all candidates is fi nished then all fi meters are handled to the ABC algorithm in order to rank the best ones and create a new population up to the maximum number of generations or if the desired accuracy is reached. The whole process is developed individually using the Morlet and then the Rickert wavelets in the AWNN design. The benchmark processes are carried afterwards keeping the same setup of optimization. Finally, the best results from Morlet and Rickertwavenets as well as from the bench-mark methods are compared in order to check the best of them.
In the proposed arrangement, an ABC approach is used to tune the spread of wavenet function and the maximum number of the hidden layer of the network. For Elman and Jordan, the maximum number of neurons and the maximum delays are tuned by ABC. The dictionary size, kernel parameter and regularization parameter are tuned by ABC. The main idea of ABC is inspired on the natural bee's behavior while searching for new food supplies, selecting the known sources and extracting food. Recently, many research activities have been devoted to design, implementation and valida-tion of ef fi cient ABC approaches in engineering fi eld ( Chaves-Gonz X lez et al., 2013; Sabat et al., 2010; Li et al., 2014a; Karaboga and Latifoglu, 2013; Mohammadi and Abadeh, 2014; Chakrabarty et al., 2013; Banharnsakun et al., 2012 ).

The ABC algorithm was chosen once it deals with a small number of parameters to be set and performs well for benchmark problems. There are several applications already showing its performance in known science areas like in ( Li et al., 2014b; Tasgetiren et al., 2013 ) and ( Karaboga, 2005; Karaboga et al., 2012;
Chaves-Gonz X lez et al., 2013 ). In the optimization process, the population size, number maximum of searching loops, minimum and maximum boundaries were selected.

In a real honey bee hive, there are basically three types of bees: the employed, the onlooker and the scout bees. The fi rst ones are those bees exploiting some food source and bringing to the hive food (nectar) and information about the food source. The onlooker bees wait for information of good food sources and then go to exploit some nectar source and fi nally, the scout bees search in the environment around the nest looking for new food sources regardless to the information brought by the employed bees.
These concepts are used in the ABC algorithm in order to fi good solutions for a problem. Possible solutions represent posi-tions of food sources and the fi tness function represents the goodness of the food source.

A combination method model the exploitation of employed bees. In other words, the ABC algorithm runs according to the following steps:
Initialization  X  the bees position are set randomly as well as the execution parameters as the maximum loop number.

Employed bees phase  X  each employed bee have the function value calculated in order to be evaluated.

Onlooker bees phase  X  the better food sources are passed to other onlooker bees in a probabilistic way.

Scout bees phase  X  scout bees are searching the working space randomly for new food sources. The new sources cost function are calculated and ranked.

If a bee cannot improve its food source for a x number of interactions then it will become a scout bee leaving its food source.
Finally the best results are stored and a new round is initiated up to the maximum loop numbers. 4. Description of case study and performance indices
All data collected for this paper is a private property of the manufacturer that gently authorized the usage if the fi nal variable naming was not published, based on the secrecy agreement. The modeling data were acquired in a real experiment using a truck engine with a cubic displacement greater than seven liters. The fuel injection system has a cylinder independent design thus no data from common rail was gathered. All variables were collected directly on the engine control unit using a Kvaser interface. The computer software applied was developed by Accurated Technol-ogies Inc. and it was loaded with the pertinent keys in order to access the desired information. Fig. 4 illustrates the used instrumentation.

The engine data was collected during the heating up time and sometime after the temperatures had reached the operational point. No load was applied to the vehicle in this condition and no road operation was set. The sampling time for all variables was set on 100 ms. The total amount of 18,009 samples for 28 variables was recorded in this experiment. All recorded variables are described in Table 1 .

After the data acquirement all variables were cross correlated and the number of working elements was reduced. This reduction aims to lower the computational cost and the cross correlation could assure that no system information is lost. All elements were normalized in an unitary range in order to reduce the searching fi eld when adjusting the AWNN weights and biases. Fig. 5 shows the fi nal variable topology, where white arrows represent the source of the variables, the dashed gray boxes stand for control units, the engine itself and the system outputs and fi nally the variable names are in the light gray boxes.

Finally the proposed AWNN model will deal with 16 input variables, and their historic values, estimating nine different outputs.
In order to compare the model to the real system, or even to models created by other techniques, some indices are used. In this development where used: the multiple correlation coef fi cient ( R the mean squared error (MSE) and the mean absolute percentage error (MAPE).

The R 2 returns a value in an unitary range. Unity means a perfect match model and null stands for a total discrepancy. The index is calculated using the following equation: R 2  X  1 where y real stands for a real system output, y est represents the model estimated output and y mean the real average system output.
The MSE index returns a reference value according to the model and real system output matching. This value is affected by the measuring unit and can be calculated using the following equation: MSE  X  where y real stands for a real system output, y est represents the model estimated output and N is the amount of samples.
The MAPE correlates the real signal and the estimated one returning the mean absolute percentual error. This index is also affected by the measuring unit and can be calculated using the following equation: MAPE  X  100 where y real stands for a real system output, y est represents the model estimated output and N is the amount of samples. 5. Setup of the AWNN and identi fi cation results
All wavenet system modeling was developed in Matlab envir-onment from Mathworks, running in an Athlon dual core 2.5 GHz computer with 4 GB of random access memory (RAM) and 64-bit operational system. Models are multi input multi output (MIMO) ones, that means that the parameter tuning shall affect all the variables together.

A wavenet generic toolbox was not available for this research then an adaptation on the radial basis network toolbox of Matlab took place. This adaptation process was basically to change the radial basis function core for one of the selected wavelets. The success is possible once the radial basis function needs the same kind of operations, like dilatation and translations, as the wavelets.
The fi nal toolbox have only two parameters: spread of wavelet functions and maximum neuron number. The network is basically built by an output neuron layer activated by a linear function and an intermediate layer based on wavelet functions with a variable number of neurons. The toolbox tuning procedure respects the following steps: i) the intermediate layer has no neuron; ii) the AWNN model is simulated the estimated output is generated; iii) the input vector with the greatest error is identi fi iv) a new intermediate neuron is added with a weight equal to the input vector from last step; v) the output layer weights are adjusted in order to minimize the output error; and vi) the steps from (ii) to (v) are repeated up to the global error meets the desired target.

Besides the two constructive parameters cited, spread and maximum neuron number, more one basic parameter was set to be adjusted, the variable feedback delays. This parameter is related to the history of the system variables, meaning that in each modeling time there is a number of historic variables fed to the model. This parameter has a tradeoff once handling a great historic data bank will cause the model to be computational heavy and small data bank could bring no accuracy pro fi t.

Once the parameter tuning is not intuitive and no previous modeling test was held then the optimization algorithm was used in order to fi nd a suitable combination for the constructive parameters. The ABC was elected to be applied once it is being proved as a high ef fi ciency alternative for several classical optimi-zation problems. The following setup was checked for the optimi-zation algorithm to tune the AWNN parameters: the bees num-ber equal to 20, the maximum searching loop equal to 5, number of searching elements is set to 3, the searching range between [1 10 10 , 1], cost function set to the inverse sum of all R (minimization problem) for each modeled variable.

Besides to the setup of the ABC, the constructive elements have a multiplier set to achieve the real variable searching range. The variable feedback delay was set as a discrete number between one and four. The spread range was set between 1 and 200 as well as the maximum number of neurons that was set as a discrete number between 1 and 200. The fi nal best parameter set found for AWNN by ABC can be seen in Table 2 .

The fi nal models shall replace the real engine for numerical simulation. The real engine main inputs are present but the main outputs are given by the model. There are feedback for the last j times for the simulation results. There is no transfer function available once the neural network is applied, however a model arrangement is presented ( Fig. 6 ):
Once Morlet models were built with three feedback delays, the fi nal model structure shall be: Y where U [1  X  16] ( k ) represent the real engine input signals and Y [1  X  9] ( k ) stand for the real engine output signals those in the model are simulated.

Rickert models required less feedback information with only one feedback delay, the fi nal model structure is according to the following equation: Y  X  k  X  X  X  fU  X  1 16 k  X  X  ; Y  X  1 9 k 1  X  X  ;  X  10  X  where U [1  X  16] ( k ) represent the real engine input signals and Y [1  X  9] ( k ) stand for the real engine output signals those in the model are simulated. 5.1. Identi fi cation results using AWNN with Morlet wavelets
The building parameters, shown in Table 2 , were applied to the training and validation procedure of an AWNN. The training was done using 55% of the available experimental data and the validation process was conducted on the remaining 45%. Models were trained in an average time of 30 s, running in an Athlon dual core computer with 4 GB of RAM, showing a mean coef fi cient of multiple determination between all modeled variables around 91%. Ten models were trained with same set of data and para-meters in order to check the output deviation due to an initial random seed as usual in a classical ANN structure. Modeling results can be seen in Table 3 .

The very low standard deviation for all performance indices can assure that the process is not susceptible to an initial seed and so no variation on result shall be seen for two models with same structure and data set. Checking the coef fi cient of multiple determination ( Schaible et al., 1997 ) one can see that variable two has the worse approximation and even this worse one is standing for a 78% of accuracy, which is not far from an engineer modeling target of 80%. Looking for the mean squared error it is possible to check that the worst case was achieved by two speed related variables, the one and the six. This latest analysis could lead to a model depreciation. However when combining to the mean absolute percentage error analysis is possible to assure that the errors are proportionally low and the model shows a suitable behavior for engineering simulations.

The fi rst built model was simulated in order to check the output validation. Fig. 7 shows on the right curve the best modeled variable and on the left the worst one. Black line stands for the estimation and the real system output is in red.

The visual modeling error analysis was also conducted as a check. Fig. 8 shows the modeling error for the best modeled variable on the right and the worst one on the left. A remind is made at this point once the error is susceptible to variable scaling factor. Variables have coherent plot errors when relating to the calculated performance indices.

Taking in consideration that the worse modeling case achieved 78% of approximation on the coef fi cient of multiple determination and no discrepancies were veri fi ed on the other analysis then the model based on experimental data and Morlet wavenet is able to cope with simulation that affords accuracies around 80%. Accuracy and results are validated for the garage operating point then one cannot assure that the same training set is valid for road operation.
In this case a new training data set, based on the desired operational point is recommended. 5.2. Identi fi cation results using AWNN with Rickert wavelets
The building parameters related to the Rickert structure, shown in Table 2 , were applied to train the AWNN with Rickert wavelets.
Comparing the building parameters between the two chosen wavelet functions one can see that Rickert has a greater spread but lowered number of neurons and feedback delay. This fact shows that the actual structure is computational softer then the Morlet one.

The training procedure of the proposed AWNN used the same amount of data as used before on Morlet structure, meaning that 55% of the available data was applied to train the network and the validation process was performed on the remaining 45%. Ten models were trained with same set of data and network structure in order to check the initial random seed impact. Each model took around of 11 s to be trained and the result was better than the one achieved with Morlet wavelets. Modeling results can be seen in Table 4 .
The process proved to not be susceptible to an initial seed and any model generated by the same structure shall present the same results. The coef fi cient of multiple determination calculated for all variables presented approximation greater than 84%. As seen in the Morlet analysis some mean squared errors were higher, demanding a more deep analysis. Checking the MAPE values is possible to see that the percent errors are smaller and then the precision is acceptable.

The fi rst generated model has its outputs, simulated and real ones, graphically represented for a visual veri fi cation. Fig. 9 shows all 18,009 samples for the simulated and real outputs for the best, on the right, and the worst variable, on the left.
 The output modeling errors were plotted in Fig. 10 .

By the fact that the Rickert structure is computationally softer than the Morlet's and looking to the little improvement on the estimation then this method is taken as the best choice in this paper. The model based on experimental data and Rickert wavelets is able to cope with simulation that needs accuracies around 84%. Accuracy and results are validated for the garage operating point then one cannot assure that the same training set is valid for road operation. In this case a new training data set, based on the desired operational point is recommended. 6. Benchmark with Elman neural network
The benchmark with Elman network was developed in Matlab environment from Mathworks, running in the same computer as the wavenets. To keep the level of comparison, the Elman models are multi input multi output (MIMO), like those used for AWNN. The standard toolbox, from Matworks, was used to model the system. The only constructive parameter to be adjusted in the Elman ANN was the number of neurons, besides this, the variable feedback delays was also used as a parameter in order to keep the comparison level. This last parameter is related to the history of the system variables, meaning that in each modeling time there is a number of historic variables fed to the model. This parameter has a tradeoff once high historic can turn the model computationally heavy and less historic can create a less accurate model.
The ABC setup for the Elman's parameters optimization was: the bees number equal to 20, the maximum searching loop equal to 5, number of searching elements is set to 2, the searching range between [1 10 10 , 1], cost function set to the inverse sum of all R (minimization problem) for each modeled variable.

Besides to the setup of the ABC, th econstructiveelementshavea multiplier set to achieve the real variable searching range. The variable feedback delay was set as a discrete number between one and four. The number of neurons was set as a discrete number between 1 and 11. The fi nal best parameters found by the ABC can be seen in Table 5 . 6.1. Identi fi cation results using Elman ANN
The building parameters, shown in Table 5 , were applied to the training and validation procedure of the Elman ANN. The training was done using 55% of the available experimental data and the validation process was conducted on the remaining 45%. Models were trained in an average time of 2627 s, running in an Athlon dual core computer with 4 GB of RAM. Ten models were trained with same set of data and parameters in order to check the output deviation due to an initial random seed as usual in a classical ANN structure. Modeling results can be seen in Table 6 .

The standard deviation for all performance indices presented situations with low values, comparing them to the mean value, and with values in the same level of the mean. This fact shows that the training process is dependent of the initial state and the situation could vary.

Checking the coef fi cient of multiple determination ( Schaible et al., 1997 ) one can see that variables two and four have the worse approximations out of the calculation range. Variables three and fi ve presented values between 40% and 75% of representation.

The best model built by the method, the second one, was simulated in order to check the output validation. Fig.11 shows the best model result on the right and the worst on the left. In black there is the estimated values and the real system output in red.
The visual modeling error analysis was also conducted as a check. Fig. 12 shows the respective modeling error for the variables represented in Fig. 11 . 7. Benchmark with kernel adaptive fi ltering algorithms
The second benchmark, using kernel adaptive fi ltering algo-rithm, was performed using a toolbox obtained from the Matlab Central (Steven Van Vaerenbergh). The toolbox is developed in Matlab environment, from Mathworks, and ran in the same computer as the previous methods. The same modeling structure was kept from previous techniques and so a single model, multi input multi output (MIMO), was built.

There are several parameters that are possible to be adjusted in the algorithm. The kernel type was kept fi xed as Gaussian, however the dictionary size, regularization and kernel parameter were adjustedbytheABCalgorithm.Thevariablefeedbackdelayswas also used as a parameter in order to keep the comparison level.
The ABC setup for the kernel adaptive fi ltering parameters optimization was: the bees number equal to 20, the maximum searching loop equal to 5, number of searching elements is set to 4, the searching range between [1 10 10 , 1], cost function set to the inverse sum of all R 2 (minimization problem) for each modeled variable. The possible results from ABC were then scaled to achieve the real variable searching range. The variable feedback delay was set as a discrete number between one and four. The regularization remained in the unitary scale, dictionary size was set to a range between 150 and 450 and the kernel parameter from 5 to 20. The scaling factors were selected by some previous testing.
The fi nal best parameters found by the ABC can be seen in Table 7 . 7.1. Identi fi cation results using kernel adaptive fi ltering
The best set of parameters found by ABC and shown in Table 7 , was applied to the training and validation procedure. The data split for training and validation was done according to the previous techniques. The average time needed to train a model was 33 s, running in an Athlon dual core computer with 4 GB of RAM. Ten models were trained with same set of data and parameters in order to check the output deviation due to an initial random seed as usual in a classical ANN structure. Modeling results can be seen in Table 8 .
The standard deviation for all performance indices presented extremely low values, pointing that the technique is stable. The model could not succeed for three simulated variables once they present no representativeness. In other hand the performance for four simulated variables were high.

Once the standard deviation was quite low for all performance indices, the fi rst model was selected. This model was simulated in order to check the output validation. Fig. 13 shows the best simulated variable result on the right and the worst on the left.
In black there is the estimated values and the real system output in red.

The visual modeling error analysis was also conducted as a check. Fig.14 shows the respective modeling error for the variables represented in Fig. 13 . 8. Benchmark with Jordan network
The third benchmark technique is the Jordan network. This is a recurrent neural network that could cope with dynamic modeling. The toolbox was obtained from Matlab Central web site, built by
Arnav Goel. This toolbox is a modi fi ed version of the Elman Matlab toolbox. In order to keep the comparison level, the same computer, model structure, data availability and optimization algorithm is applied.
 The only constructive parameter to be adjusted in the Jordan
ANN was the number of neurons, besides this, the variable feed-back delays was also used.

The ABC setup for the Jordan's parameters optimization was: the bees number equal to 20, the maximum searching loop equal to 5, number of searching elements is set to 2, the searching range between [1 10 10 , 1], cost function set to the inverse sum of all
R (minimization problem) for each modeled variable.

Besides to the setup of the ABC, the constructive elements have a multiplier set to achieve the real variable searching range. The variable feedback delay was set as a discrete number between one and four.
The number of neurons was set as a discrete number between 500 and 3000. The scaling factors were chosen by preliminary tests when it was also decided to use a double layer network because the result with only one neuron layer were far from the target. The fi parameters found by the ABC can be seen in Table 9 . 8.1. Identi fi cation results using Jordan network
Using the best set of parameters shown in Table 9 the network was trained and validated. The training was done using 55% of the available experimental data and the validation process was con-ducted on the remaining 45%. Models were trained in an average time of 0.087 s, running in an Athlon dual core computer with 4 GB of RAM. Ten models were trained and the output dispersion was checked. Modeling results can be seen in Table 10 .
The standard deviation for all performance coef fi cients and simulated variables are high meaning that the ANN is directly affected by the initial stat e and random processes. All fi could not reach the desired levels of precision and the best average one, according to the multiple correlation coef fi cient, was the third model. For this model, the best simulated variable, the ninth, and the worst one, the third, were represented graphically in Fig. 15 .Inblack there is the estimated values and the real system output in red.
The visual modeling error analysis is presented in Fig. 16 . The curves are related to the same variables in Fig. 15 . 9. Conclusion and future research Taking in consideration that the generated models were all MIMO, when comparing the results from the Elman, Jordan and kernel adaptive fi ltering methods to the wavenets, one can conclude that wavenet models could be build faster and presented stable outputs regardless to the initial states. The only faster method than wavelets in this paper, Jordan ANN, could not reach the desired target objectives.

Comparing the results for the multiple correlation coef fi wavenets worst approximation was around 78% and the worst result for Elman, Jordan and KAF were out of the coef fi cient range.
Taking the mean squared error as reference, the only benchmark technique that could reach comparable results was KAF. Using the proposed setup and the experimental data, only wavenets could present a model valid within the desired range of accuracy for all simulated variables.

The modeling and identi fi cation of linear and nonlinear dyn-amic systems through the use of measured experimental data is a problem of considerable importance in automotive applications.
However, many dynamical systems in the automotive fi eld contain nonlinear relations which are dif fi cult to model with conventional techniques. In this context, combining arti fi cial neural networks and wavelet based functions is possible to have a valuable tool to represent the nonlinear multivariable systems in automotive perspective.

In this paper, two different AWNN con fi gurations were eval-uated. An experimental case of an engine like a system was considered in order to investigate the effectiveness of the pro-posed AWNN based on ABC. An optimization algorithm based on arti fi cial bee colony was applied and the AWNN tuning was performed in a suitable way. Both fi nal AWNN models presented accuracies around 80% when compared to real engine data, showing that any simulation that can afford this level of precision can be handled by the method. Tasks like engine calibration, consisting of adjust the running engine parameters aiming an engine response, could use models built by this framework in order to be developed. The usage of the model on the calibration task can provide the fi eld engineer the ability to work remotely, speed up the deliveries and reduce the costs by avoiding the usage of many engines for numerous calibration engineers working on the same project, for example. Additionally, the modeling techni-que does not demand an expert knowledge on the real system, proving to be a useful tool for the actual automotive engineers.
As reminder one is advised that the accuracy and results are validated for the garage operating point then one cannot assure that the same training set is valid for road operation or even another engine. In this case a new training data set, based on the desired test object is recommended.

In general, this study successfully demonstrates the application of an AWNN model approach to an automotive application. The result shows that AWNN model can be used for model-based fault detection in future works. Furthermore, the AWNN model can be applied in simulation and designing of model-based predictive controllers.
 Acknowledgments
The authors would like to thank National Council of Scienti and Technologic Development of Brazil  X  CNPq (Grant numbers 307150/2012-7/PQ and 479764/2013-1) and  X  Funda X  X o Arauc X ria (Project: 117/2014) for its fi nancial support of this work. References
