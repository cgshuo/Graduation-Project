 Entity matching (EM) is the task of identifying records that refer to the same real-world entity from different data sources. While EM is widely used in data integration and data clean-ing applications, the naive method for EM incurs quadratic cost with respect to the size of the datasets. To address this problem, this paper proposes a scalable EM algorithm that employs a pre-materialized structure. Specifically, once the structure is built, our proposed algorithm can identify the EM results with sub-linear cost. In addition, as the rules evolve, our algorithm can efficiently adapt to new rules by se-lectively accessing records using the materialized structure. Our evaluation results show that our proposed EM algo-rithm is significantly faster than the state-of-the-art method for extensive real-life datasets.
 H.2.m [ Database Management ]: Miscellaneous.data clean-ing; H.2.8 [ Database Management ]: Database Applica-tions.data mining Algorithms, experimentation, performance Entity matching, resolution, materialization, rule evolution
Entity matching (EM) is the process of identifying records that come from different data sources but represent the same real-world entity. EM is also known as entity resolution [1], reference reconciliation [3], record linkage [5], or object iden-tification [10]. EM is widely used in various real-world ap-plications such as data integration and data cleaning: (1) When entities are stored in a distributed manner across het-erogeneous databases, we need to maintain those databases for data integration. (2) When many records in a single database may indicate the same entity, we need to eliminate redundant records for data consistency.

We first introduce basic notations to define the EM prob-lem. We assume two finite sets P and Q of records from dif-ferent sources, i.e. , P = { p 1 , . . . , p m } and Q = { q as input. Each record has a finite set of fields (or attributes) with various data types such as numbers and strings. For simplicity, we assume that P and Q have compatible schemas: the records from P and Q have corresponding values for some fields.

We then formalize a match function (or a matcher ) to compare records in a pair-wise manner. Specifically, we fol-low a join model over P  X Q . A join operation takes two sets P and Q as input, and returns a set of record pairs ( p, q )  X  X  X Q , referring to the same real-world entity, along with a match function M . The match function can thus be represented by a Boolean condition for the same fields of P and Q if M ( p, q ) is true, p and q indicate the same entity; otherwise, they are different. Without loss of generality, we define the match function M as: where F ( p, q ) is a user-specific function calculating the sim-ilarity between records and  X  is a threshold parameter.
Considering the match function M as a user-specific rule , we can define the rule-based EM problem [1, 7, 9, 11]. Definition 1 (Entity matching) Given two sets P and Q of records and a match function M , entity matching is to identify a set S of record pairs ( p, q )  X  P  X Q such that S = { ( p, q ) | p  X  X  , q  X  X  , M ( p, q ) = 1 } .

To support a sophisticated matching rule M we exploit F by using an aggregate function combining multiple fields, as commonly adopted in the EM literature [7]. For simplicity, we assume that F is monotone [4].
 where  X  is a space of all possible similarity functions, and f ( p, q ) is the similarity function between p and q . Also, w a weight vector reflecting the importance of function f ( p, q ).
For example, we describe the toy datasets related to restau-rants (Table 1). For the first three fields name , address and type , we can exploit well-known string similarity func-tions such as Levenshtein distance and Jaro-Winkler dis-tance. For the fourth field, coordinate , we can make use of numeric similarity functions such as Manhattan distance and Euclidean distance.
Once the match function M is determined, we identify a pair of records that refer to the same entity over P X Q . The brute-force approach for EM first enumerates all possible record pairs ( p, q )  X  X  X Q , and then checks whether p and q match: if M ( p, q ) is true, ( p, q ) is inserted into an EM result set; otherwise, ( p, q ) is skipped.
 We describe how this naive method works using toy datasets. We generate nine possible record pairs over P  X  Q (Ta-ble 2). Suppose that similarity functions f name and f type are defined as Levenshtein distance, f addr is defined as Jaro-Winkler distance, and f coord is defined as Manhattan dis-tance. On the basis of some background knowledge, we set the weight vector w used in F as h 0 . 1 , 5 , 0 . 02 , 0 . 1 i and the threshold  X  as 1. The aggregate function values for each pair are given in the last column in Table 2. Finally, we can and  X  X hilippe The Original X  and are correctly resolved. Table 2: All possible record pairs and their aggre-gated scores with weight vector h 0 . 1 , 5 , 0 . 02 , 0 . 1 i
To provide exact EM results, it is critical to determine meaningful similarity functions f  X   X  and tune the user-specific parameters w and  X  . To address these needs, rules can continuously evolve either by adding/removing functions from  X  or adjusting w and  X  . Efficient support for evolv-ing rules has also been emphasized by Whang and Garcia-Molina [11]. However, their proposed solution makes re-strictive assumptions, e.g. , rule monotonicity, and suffers in performance when rules evolve significantly. Even when their assumptions hold (as we empirically study later), our proposed algorithm significantly outperforms the other so-lution, which still incurs a prohibitive cost for intermediate EM results.
We propose a scalable EM algorithm using a materializa-tion structure. Specifically, we observe that the process of finding EM results satisfying a threshold condition is similar to that of top-k queries [2, 4, 6], which identify the best k records with the highest scores. We thus adopt the mate-rialized lists used for top-k processing without accessing all records. The materialized lists for EM store the similarity values of all record pairs for single-field functions as a basic access unit for EM. By accessing records in materialization lists at a time, we can identify a final EM result with sub-linear cost. Furthermore, we can interactively evolve various rules and efficiently obtain EM results with high accuracy.
This section proposes a scalable EM algorithm that identi-fies EM results using a pre-materialized structure. Towards this goal, we first introduce a threshold algorithm ( TA ) [4] that identifies the top-k results by performing a fuzzy join with sub-linear cost. With similar intuition, we then adopt the materialization used in TA-family algorithms, and de-velop an EM algorithm that efficiently evolves for new rules. We first provide the overview of top-k query computation. A top-k query aims to identify the best k records with re-spect to a user-specific scoring function F . Suppose that the function F is represented by a linear combination of d attributes, i.e. , F ( f 1 ( A 1 ) , . . . , f d ( A d )), where f scoring function of attribute A i . The naive approach to finding the top-k results requires to access all records in the worst case.

To address this problem, TA exploits a pre-computed data structure. Specifically, they first maintain d sorted lists in increasing order of f i ( A i ), where smaller scores are better. On these sorted lists, they then access records by performing sorted access or random access until the top-k records with the highest scores are identified.

TA keeps the current top-k records found thus far while accessing the sorted lists. When the aggregated score of the k th ranked record is larger than the aggregated score of the last seen attribute values in the sorted lists, TA stops attempting to access further records and returns the current top-k records as an answer. Because this early-termination condition avoids searching the whole dataset, it is enough to consider few records distributed at the heads of sorted lists.
This key idea of TA can be applied to the EM problem in the sense that both problems only deal with some of the records satisfying a threshold condition. Therefore, we can get the EM results quickly by probing a small number of record pairs. We first devise a materialization structure inspired by TA. Before starting an EM algorithm, we construct lists for each similarity function f i ( p, q ). These lists are sorted in increas-ing order of function values, i.e. , the most similar record pairs are accessed first. We call them materialized lists , de-noted as M i for f i ( p, q ). Algorithm 1 TEM ( M , w,  X , H ) 2: S  X  X } 3: if H .w  X  w then 4: S  X  X  . S // start from the result of history 7: end for 8: end if 15: end for 17: S  X  X  X  ( p, q ) // ( p, q ) is a matched record pair 18: end if 19: end for 20: end while 21: return S
For example, we describe the materialized lists using the toy datasets in Table 2. For ease of representation, suppose that three similarity functions on the fields of name , address , and coordinate are used for EM. We calculate all possible pair-wise similarities and sort them (Table 3).
 Table 3: Materialized lists built from 3 similarity function values of pair-wise records in Table 2
We then develop a threshold-based EM algorithm ( TEM ) using the materialized lists. In a method similar to TA, we use two access modes: sorted access and random access. Algorithm 1 describes the pseudo code of TEM . Let d denote the number of fields used in EM. Specifically, the overall procedure of TEM is as follows: 1. We do sorted access on each materialized list. If a new 2. For each list M i , let t i be the last similarity score seen 3. Return all matched record pairs as EM results.

In addition to the above basic procedure, we can also ex-ploit the EM history H and evolve from an old rule. If a weight vector of H is dominated by w (line 3), the result in the history will also be a part of the result for w . Therefore, we reuse results for the old rule (line 4) and save access costs by restarting from the last access points of H (lines 5 X 7). We prove the correctness of TEM as follows.
 Theorem 1 (Correctness of TEM ) Given a match func-tion M , TEM correctly returns all record pairs such that M ( p, q ) = 1.

Proof. Suppose there is a matched record pair ( p  X  , q  X  ), but TEM does not return it as an answer. That is, ( p  X  , q  X  ) is not applied to a match function, since it is not visible by sorted access on the materialized lists. This means that the corresponding value of the last seen similarity score t By the definitions of the aggregate and match functions, does not exist a false negative such as ( p  X  , q  X  ), and TEM correctly returns all matched record pairs.

We explain TEM using materialized lists (Table 3). Sup-pose an aggregation function F consists of w = h 0 . 1 , 5 , 0 . 1 i , and  X  is 1. The threshold vector t is initially a zero vec-tor. Then, we access the materialized lists from M name to M coord . The first sorted access steps for the three material-ized lists retrieve ( p 1 , q 2 ), whose aggregate score F is smaller than the threshold  X  = 1, and we insert ( p 1 , q 2 ) into an EM result set. Then t is updated to h 0 , 0 . 091 , 0 i . The second sorted access and random access steps calculate F ( p 2 , q and F ( p 3 , q 1 ). The ( p 2 , q 3 ) pair is matched and inserted into the result set. Now, t is updated to h 2 , 0 . 11 , 0 . 052 i and F ( t ) &lt;  X  , so we continue. The third access steps get ( p and ( p 2 , q 1 ). The pair ( p 3 , q 1 ) is was already addressed at the second step, and F ( p 2 , q 1 ) &gt;  X  ; thus, we dismiss them. Finally, we stop accessing the materialized lists and return the result set { ( p 1 , q 2 ) , ( p 2 , q 3 ) } . This section shows experimental results for our proposed EM algorithm using extensive real-life datasets. Specifically, we exploited four datasets used in existing EM experimen-tal study [8] (Table 4). Two dataset pairs, Abt-Buy and Amazon-GoogleProducts are used in e-commerce applica-tions, where they consist of four fields, product name , de-scription , manufacturer , and price . The other pairs DBLP-ACM and DBLP-Scholar are used in the bibliographic do-main, where they consist of four fields, title , authors , venue , and year . For simplicity, we used Levenshtein distance as the similarity function for each field. (By carefully design-ing similarity functions, we can further improve the accuracy of EM results.)
Our experiments were conducted using a 64-bit Linux server with an Intel Core i7 Processor (2.8 GHz) and 24 GB of RAM. All the algorithms were implemented in Java. Figure 1 depicts the execution time for two EM tasks. Note that these figures have a log-scaled y -axis. When con-structing the initial EM task (Figure 1a), the TEM execu-tion time is the sum of those for both building material-ization lists and computing EM results. Nevertheless, our algorithm is two to seven times faster than the state-of-the-art EM algorithm R-Swoosh [1]. Although R-Swoosh focuses on reducing the search space for all possible pairs without additional indices, as a hierarchical tree structure it inher-ently requires a large number of comparisons to obtain in-termediate EM results. These comparisons can deteriorate the overall performance. In contrast, after building sorted lists, our algorithm selectively accesses records to minimize unnecessary computation.
 Figure 1: Execution times to perform an EM task from scratch 1 and to evolve for a new rule 2
We also report on the execution time for evolving rules af-ter the materialized lists are built up. We compared our al-gorithm with the rule evolution algorithm HC DS [11] based on R-Swoosh . We generated evolving rules satisfying some constraints, i.e. , rule monotonicity and rule-context free , mentioned the previous study [11]. Note that these rule generation constraints are unfavorable for our work, which does not have these restrictions. In addition, the TEM used in this experiment does not exploit the history. However, for all datasets, our algorithm is up to two orders of mag-nitude faster than the existing algorithm. This implies that our algorithm selectively accesses small portions of the ma-terialization lists over an extensive set of rules. Table 5: Ratio of sorted accesses in materialized lists
Specifically, Table 5 reports on the ratio of accessed tuples in the materialized lists. This ratio determines the execution time to resolve entities for our proposed method. Empiri-cally, sorted access of less than 0.5% of the materialized lists is sufficient to obtain EM results. Finally, we report on the accuracy of our algorithm using F 1 score (Table 6). We evolved rules by changing a weight vector w in a simple hill-climbing manner, where the func-tions and threshold  X  are fixed. In clear contrast to previous methods, our algorithm achieved comparable accuracy from D 3 and D 4 in the benchmark set in addition to adapting to rule changes efficiently. These algorithms require complete re-training to try out new matching techniques, which can also be repeated until convergence.
 Table 6: Accuracy comparisons ( F 1 score ) for ours and ML-based EM algorithms
This paper has addressed the scalability of EM. Towards this goal, we exploited a materialization structure inspired by top-k query processing. Using the materialization, we developed a scalable EM algorithm for evolving rules. Our evaluation demonstrated that our proposed algorithm is two to seven times faster than existing ER algorithms. In addi-tion, when rules are evolved, our algorithm is two orders of magnitude faster than existing algorithms.
 This work was supported by Microsoft Research Asia and the Ministry of Knowledge Economy (MKE), Korea under Information Technology Research Center (ITRC) support program supervised by the National IT Industry Promotion Agency (NIPA-2011-C1090-1131-0009).
