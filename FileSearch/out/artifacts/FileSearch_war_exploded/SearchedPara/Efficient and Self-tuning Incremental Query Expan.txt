 We present a novel approach for efficient and self-tuning query expansion that is embedded into a top-k query pro-cessor with candidate pruning. Traditional query expansion methods select expansion terms whose thematic similarity to the original query terms is above some specified threshold, thus generating a disjunctive query with much higher di-mensionality. This poses three major problems: 1) the need for hand-tuning the expansion threshold, 2) the potential topic dilution with overly aggressive expansion, and 3) the drastically increased execution cost of a high-dimensional query. The method developed in this paper addresses all three problems by dynamically and incrementally merging the inverted lists for the potential expansion terms with the lists for the original query terms. A priority queue is used for maintaining result candidates, the pruning of candidates is based on Fagin X  X  family of top-k algorithms, and option-ally probabilistic estimators of candidate scores can be used for additional pruning. Experiments on the TREC collec-tions for the 2004 Robust and Terabyte tracks demonstrate the increased efficiency, effectiveness, and scalability of our approach.
 Categories and Subject Descriptors: H.3.3: Search process.
 General Terms: Algorithms, Performance, Experimenta-tion.
 Keywords: Top-k Ranking, Query Expansion, Incremental Merge, Probabilistic Candidate Pruning.
Query expansion is an indispensable technique for evalu-ating difficult queries where good recall is a problem. Exam-ples of such queries are the ones in the TREC Robust track,
Partially supported by the EU within the 6th Frame-work Program under contract 001907  X  X ynamically Evolv-ing, Large Scale Information Systems X  (DELIS) e.g., queries for  X  X ransportation tunnel disasters X  or  X  X hip losses X  on the Aquaint news corpus. Typical approaches use one or a combination of the following sources to gener-ate additional query terms: thesauri such as WordNet with concept relationships and some form of similarity measures, explicit user feedback or pseudo relevance feedback, query associations derived from query logs, document summaries such as Google top-10 snippets, or other sources of term correlations. In all cases, the additional expansion terms are chosen based on similarity, correlation, or relative en-tropy measures and a corresponding threshold. For difficult retrieval tasks like the above, query expansion can improve precision@top-k, recall, as well as uninterpolated mean av-erage precision (MAP) by a significant margin (see, e.g., [20, 21]). However, in contrast to a mere benchmark set-ting such as TREC, applying these techniques in a real ap-plication with unpredictable ad-hoc queries (e.g., in digital libraries, intranet search, or web communities) faces three major problems [4, 5]: 1) The threshold for selecting ex-pansion terms needs to be carefully hand-tuned, and this is highly dependent on the application X  X  corpus and query workload. 2) An inappropriate choice of the sensitive expan-sion threshold may result in either not achieving the desired improvement in recall (if the threshold is set too conserva-tively) or in high danger of topic dilution (if the query is expanded too aggressively). 3) The expansion may often re-sult in queries with 50 or 100 terms which in turn leads to very high computational costs in evaluating the expanded queries over inverted index lists.

This paper addresses the above three issues and provides a practically viable, novel solution. Our approach is based on the paradigm of threshold algorithms, proposed by Fa-gin and others [13, 14, 16, 17, 27], with a monotonic score aggregation function (e.g., weighted summation) over in-verted index lists. In this framework, each per-term list is precomputed and stored in descending order of per-term scores (e.g., some form of tf  X  idf-style index-term weights or the term X  X  contribution to a probabilistic relevance measure such as Okapi BM25 [31, 32]). The specific algorithm that we employ processes index lists using efficient sequential disk I/O, prunes top-k result candidates as early as possible, and terminates without having to scan the entire index lists (of-ten after processing only fairly short prefixes of the lists). In addition, we use score distribution statistics to estimate ag-gregated scores of candidates and may prune them already when the probability of being in the top-k result drops below an acceptable error threshold (e.g., 10 percent).
Our key techniques for making query expansion efficient, scalable, and self-tuning are to avoid aggregating scores for multiple expansion terms of the same original query term and to avoid scanning the index lists for all expansion terms. For example, when the term  X  X isaster X  in the query  X  X rans-portation tunnel disaster X  is expanded into  X  X ire X ,  X  X arth-quake X ,  X  X lood X , etc., we do not count occurrences of several of these terms as additional evidence of relevance. Rather we use a score aggregation function that counts only the maximum score of a document for all expansion terms of the same original query term, optionally weighted by the similarity (or correlation) of the expansion term to the orig-inal term. Furthermore and most importantly for efficiency, we open scans on the index lists for expansion terms as late as possible, namely, only when the best possible candidate document from a list can achieve a score contribution that is higher than the score contributions from the original term X  X  list at the current scan position or any list of expansion terms with ongoing scans at their current positions. The al-gorithm conceptually merges the index lists of the expansion terms with the list of the original query term in an incre-mental on-demand manner during the runtime of the query. For further speed-up, probabilistic score estimation can be used, considering score distributions and term selectivities. The novel contributions of the paper are threefold: 1) the Incremental Merge algorithm for highly efficient query ex-pansion, 2) its integration with top-k query processing and probabilistic prediction of aggregated scores and selectivi-ties, and 3) a comprehensive experimental evaluation using the Robust track and Terabyte track benchmarks of TREC 2004.
There is a rich body of literature on query expansion (see, e.g., [3, 8, 11, 18, 20, 21, 24, 30, 33, 34, 36, 37]). All methods aim to generate additional query terms that are  X  X emanti-cally X  or statistically related to the original query terms, often producing queries with more than 50 or 100 terms and appropriately chosen weights. Given the additional un-certainty induced by the expansion terms, such queries are usually considered as disjunctive queries and incur very high execution costs for a DBMS-style query processing [4, 5]. The various methods differ in their sources that they ex-ploit for inferring correlated terms: explicit relationships in thesauri, explicit relevance feedback, pseudo relevance feed-back, query associations derived from query logs and click streams, summary snippets of web search engine results, ex-tended topic descriptions (available in benchmarks), or com-binations of various techniques. In all cases some similarity, correlation, or entropy measure between the original query terms and the possible expansion terms should be quanti-fied (usually in some statistical manner), and a carefully tuned threshold needs to be determined to eliminate expan-sion candidates that are only weakly related to the original query. While such manual tuning is standard in benchmarks like TREC, it is almost black art to find robust parameter settings for real applications with highly dynamic corpora and continuously evolving query profiles [4] (e.g., intranets, web forums, etc.). This calls for automatic and self-adaptive query tuning. [20] generate Google queries from the original query and use the summary snippets on the top-10 result page to gener-ate alternative query formulations and performed very suc-cessful in recent TREC benchmarks. The final query expan-sion is a weighted combination of the original and the alter-native queries. [21] uses a suite of techniques for extracting phrases and word sense disambiguation (WSD), with Word-Net [15] as a background thesaurus and source of expansion candidates. Both of these techniques seem to require sub-stantial hand-tuning for achieving their very good precision-recall performance. In the current paper, where the em-phasis is on efficiency rather than effectiveness, we use a rather simple form of WSD and WordNet-based expansions. The techniques of the current paper could be easily carried over to more sophisticated expansion methods as mentioned above.
There is also a good amount of literature on efficient top-k query processing for ranked retrieval of text, web, and semistructured documents as well as multimedia similarity search, preference queries on product catalogs, and other applications (see, e.g., [13] for an overview). Most algo-rithms scan inverted index lists and aggregate precomputed per-term or per-dimension scores into in-memory  X  X ccumu-lators X , one for each candidate document. The optimizations in the IR literature aim to limit the number of accumulators and the scan depth on the index lists in order to terminate the algorithm as early as possible [1, 2, 7, 6, 22, 25, 29, 28]. This involves a variety of heuristics for pruning potential result candidates and stopping some or all of the index list traversals as early as possible, ideally after having seen only short prefixes of the potentially very long lists. To this end, it is often beneficial that the entries in the inverted lists are kept in descending order of score values rather than being sorted by document identifiers [13, 28]. In the current paper, we assume exactly this kind of index structure.

A general framework for this kind of methods has been developed by Fagin et al. in [14] and others [16, 17, 27], in the context of similarity search on multimedia and struc-tured data. This family of so-called threshold algorithms (TA) operates on score-sorted index lists and assumes that the score aggregation function is monotonic (e.g., a weighted summation). It uses a threshold criterion that is provably optimal on every possible instance of the data [14]. Many variations and extensions of the TA method have been devel-oped in the last few years [9, 10, 12, 19, 23, 26, 35, 38]. The current paper builds on the particular variant developed in [35], with sequential index accesses only and a probabilistic score prediction technique for early candidate pruning, when approximate top-k results (with probabilistic error bounds) are acceptable by the application. The rationale for avoid-ing, minimizing, or limiting random accesses is that for very large corpora index lists are disk-resident and sequential disk I/O is an order of magnitude more efficient than random I/O (because of lower positioning latencies).
The basic query processing on which this paper builds is the Prob-k algorithm developed in [35]. A query scans precomputed index lists, one for each query term, that are sorted in descending order of scores such as tf  X  idf or BM25 weights of the corresponding term in the documents that contain the term. Unlike the original TA method of [14], Prob-k avoids random accesses to index list entries and per-forms only sequential accesses. The algorithm maintains a pool of candidate results and their corresponding score ac-cumulators in memory, where each candidate is a data item d that has been seen in at least one list and may qualify for the final top-k result based on the following information (we denote the score of data item d in the i -th index list by s and we assume for simplicity that the score aggregation is summation):
The algorithm terminates and is guaranteed to obtain the exact top-k result, when the worstscore of the k th rank in the current top-k result is at least as high as the highest bestscore among all other candidates. The Prob-k algorithm implements the candidate pool using a priority queue. In ad-dition, it can optionally replace the conservative threshold test by a probabilistic test for an approximative top-k al-gorithm with probabilistic precision and recall guarantees. The most flexible implementation uses histograms to cap-ture the score distributions in the individual index lists and computes convolutions of histograms 1 in order to predict the probability that an item d has a global score above some high j ] , where S j ( d ) denotes the random variable for the score of d in list j . Here,  X  is set to the score threshold that the candidate has to exceed, namely to the worstscore of the k th rank in the current top-k result (coined min -k ). When this probability for some candidate document d drops below a threshold (e.g., set to 0.1), d is discarded from the candidate set. The special case = 0 . 0 corresponds to the conservative threshold algorithm.

Figure 1 shows pseudo code for this algorithm. The can-didates and their accumulators are kept in a hash table, whereas only a small number of top candidates has to be or-ganized in a priority queue in order to maintain the thresh-old condition for algorithm termination, using their best -score values as priorities. The probabilistic pruning test is performed only every b index scan steps, where b is chosen in the order of a few hundred. At these points the prior-ity queue is rebuilt, i.e., all candidates X  bestscore values are updated taking the current high i values into account.
We generate potential expansion terms for queries using a thesaurus based on WordNet [15]. WordNet concepts (i.e., word senses) and their hypernymy/hyponymy relation-ships are represented as a graph. The edges in the graph are weighted by precomputing statistical correlation mea-sures on large text corpora (e.g., the various TREC cor-pora). More specifically, we compute the Dice coefficient for each pair of adjacent nodes, counting a document as a Algorithm 1 Prob-k(query q = ( t 1 , . . . , t m ) ): co-occurrence of the two concepts, if it contains at least one term or phrase denoting a concept from each of the two sets of synonyms (synsets) that WordNet provides for the concepts. For non-adjacent nodes the similarity weight is computed on demand by multiplying edge weights along the best connecting path, using a variant of Dijkstra X  X  shortest path algorithm.

A query term t is mapped onto a WordNet concept c by comparing the textual context of the query term (i.e., the description of the query topic or the summaries of the top-10 results of the original query) against the synsets and glosses (i.e., short descriptions) of concept c and its hyponyms (and optionally siblings in the graph). This comparison com-putes a cosine similarity between the two contexts con ( t ) and con ( c ) for each potential target concept c whose synset contains t . The mapping uses a simple form of word sense disambiguation by choosing the concept with the highest cosine similarity. Here con ( t ) and con ( c ) are basically bags of words, but we also extract noun phrases (e.g.,  X  X iber op-tics X  or  X  X rime organization X ) and include them as features in the similarity computation by forming 2-grams and 3-grams and looking them up in WordNet X  X  synsets. Once we have mapped a query term t onto its most likely concept c , we generate a set exp ( t ) of potential expansion terms. Each t  X  exp ( t ) has a weight sim ( t, t j ) set to the Dice coefficient for adjacent concepts c and c j (or 1 for the special case of t and t j being synonyms) or an aggregation of edge weights on the shortest connecting path between c and c j as described above.
At query run-time, for a query with terms t 1 . . . t m we first look up the potential expansion terms t ij  X  exp ( t i ) for each t with sim ( t i , t ij ) &gt;  X  , where  X  is a fine-tuning threshold for limiting exp ( t i ) . Note that  X  merely defines an upper bound for the number of expansion terms; in contrast to a static expansion, our algorithm does not necessarily exhaust the full set, such that  X  could even be set to 0 in the incremental expansion setup.

The top-k algorithm is extended such that it now merges multiple index lists L ij in descending order of the combined score that results from the local score s ij ( d ) of an expansion term t ij in a document d and the thesaurus-based similarity sim ( t i , t ij ) . Moreover, to reduce the danger of topic drift, we use the following modified score aggregation for a query t . . . t m that counts only the best match for any one of the expansion terms per document: with analogous formulations for the worstscore ( d ) and best -score ( d ) bounds.

The Incremental Merge algorithm provides sorted access to a  X  X irtual X  index list that results from merging a set of stored index lists in descending order of the combined sim ( t i , t ij )  X  s ij ( d ) values. The index lists for the expan-sion terms for a given query term are merged on demand (hence, incrementally) until the min -k threshold termina-tion is reached, by using the following scheduling procedure for the index scan steps: the next scan step is always per-formed on the list L ij with the currently highest value of sim ( t i , t ij )  X  high ij , where high ij is the last score seen in the index scan (i.e., the upper bound for the unvisited part of the list). This procedure guarantees that index en-tries are consumed in exactly the right order of descending sion terms are opened and added to the set of active expan-sions activeExp ( t i ) for term t i , only if they are beneficial for identifying a top-k candidate. Often the scan depth on these lists is much smaller than on the index lists for the original query terms, and for many terms in exp ( t i ) the index scans do not have to be opened at all, i.e., activeExp ( t i )  X  exp ( t
The precomputed sim ( t i , t ij ) values are stored in a meta-index table which is fairly small and typically fits into main memory (e.g., all WordNet nouns together with their initial high scores in the inverted lists). The sim ( t i , t i ) is defined to yield the maximum similarity value of 1 , such that the Incremental Merge scans are initialized on the index lists for the original query conditions t i first. The meta-index also contains the maximum possible scores of all expansion candidates, i.e., the initial high ij values at the start of the query execution. This way, we are in a position to open the scans on the expansion-term index lists as late as possible , namely, when we actually want to fetch the first index entry from such a list. Fig. 2 gives pseudo code for the Incremental Merge algorithm. Fig. 1 illustrates the merged index scans for an expansion exp ( t ) .

When we want to employ probabilistic pruning based on score predictors, we face an additional difficulty with query expansion using the Incremental Merge technique. Before computing the convolution over the still unknown scores for a subset of the original query X  X  terms, we need to con-sider the possible expansions for a query term. For term t with a random variable S i capturing the score distribution in the not yet visited part, we are interested in the proba-bility P [ S i  X  x | S i  X  high i ] . With expansion into exp ( t { t i 1 , ..., t ip } , this becomes P [ max { sim i 1  X  S i 1  X  x | S i 1  X  high i 1  X  X  X  X  X  S ip  X  high ip ] , where sim is shorthand for sim ( t i , t ij ) . As the individual factors of this product are captured by the precomputed histograms Algorithm 2 IncrementalMerge(query q = ( t 1 , . . . , t m meta-index with exp ( t 1 ) , . . . , exp ( t m ) and sim ( t per index list (with the constant sim ( t i , t ij ) coefficients sim-ply resulting in a proportionally adjusted distribution), we can easily derive a new combined histogram for the max-distribution. This meta histogram construction is performed prior to query execution, and it is updated whenever a new scan on another index list is opened with linear costs in the number of histogram cells and the current number of active query expansions. Then this meta histogram is fed as input into the convolution with other (meta) histograms for the original query terms (or their expansion sets). In our experiments the overhead for this dynamic histogram construction was negligible compared to the costs saved in physical disk accesses. Figure 1: Example schedule for Incremental Merge
For phrase matching and computing phrase scores, we cannot directly apply the algorithms described so far, be-cause it is rarely feasible to identify potentially query-rele-vant phrases in advance and create inverted index lists for entire phrases. We store term positions in a separate index, not in the inverted index lists to keep the inverted index as compact as possible and minimize disk I/O when scanning these lists. Thus, testing the adjacency condition (or other kinds of proximity condition and computing an additional proximity factor for the overall score contribution) requires random I/O. We therefore treat the adjacency tests as ex-pensive predicates in the sense of [9] and postpone their evaluation as much as possible.

In our approach we assume that the score for a phrase match of a phrase t 1 . . . t p is the sum of the scores for the individual words, possibly normalized by the phrase length. The key idea to minimizing the random I/Os for adjacency tests is to eliminate many candidates based on their upper score bounds (or estimated quantile of their scores) early before checking for adjacency. Consider a phrase t 1 . . . t and suppose we have seen the same document d in the in-dex lists for a subset of the phrase X  X  words, say in the lists for t 1 . . . t j ( j &lt; p ) . At this point we know the bestscore with regard to this phrase match alone (which is part of a broader query), and we can compare this bestscore 0 ( d ) or the total bestscore ( d ) for the complete query against the worstscore ( d 0 ) of other candidates d 0 that have been collected so far. In many cases we can eliminate d from the candidate pool without ever performing the adjacency test. Even if we have seen d in all p index lists for the phrase X  X  words, we may still consider postponing the adja-cency test further, as we gain more information about the total bestscore ( d ) for the entire query (not just the phrase) and the evolving min -k threshold of the current top-k docu-ments.

This consideration is implemented using a top-k algorithm for both the phrase matching and the entire query, leading to a notion of nested top-k operators that dynamically com-bine the index lists for each of several phrases of the full query. As indicated by the example expansion Fig. 2, this method allows a very fine grained modeling of semantic sim-ilarities directly in the query processing. The algorithm works by running outer and inner top-k operators in sep-arate threads, with inner operators periodically reporting their currently best candidates along with the correspond-ing [ worstscore i ( d ) , bestscore i ( d )] intervals. Then the ag-gregated score interval of a candidate d at each operator sim-ply becomes the sum of the score interval boundaries propa-gated by each of the subordinate operators [ P i =1 ..m worst -score i ( d ) , P i =1 ..m bestscore i ( d )] for a top-k join or the maximum of the respective interval boundaries [max i =1 ..m worstscore i ( d ) , max i =1 ..m bestscore i ( d )] for an Incremen-tal Merge join which remains a monotonous score aggrega-tion at any level of the operator tree. At the synchronization points, the outer operator integrates the reported informa-tion into its own candidate bookkeeping and considers prun-ing candidates from the outer priority queue. This technique also allows a lazy scheduling of random I/Os for phrase tests by the top-level operator only which can further decrease the amount of phrase tests by an order of magnitude for large phrase expansions.
The presented algorithms are implemented in a Java pro-totype using JDK 1.4. Inverted index lists are stored as tables with appropriate B + -tree indexes in an Oracle 10g database; in addition, a term-to-position index is kept in a second table for phrase matching. The top-k engine accesses the index lists via JDBC, with a large prefetching buffer. In-dex scans are multi-threaded, with periodic thread synchro-nization and queue updates after every b = 500 index scan steps. All experiments were run on an Intel Dual XEON with 4 GB RAM and a large RAID-5 disk array.
We performed experiments with two different TREC data sets and ad-hoc query tasks: (1) The distinct set of 50 queries of the TREC 2004 Robust track marked as hard (ex-plicitly identified by TREC) on the Aquaint news corpus, Figure 2: Phrase matching with multiple nested top-k operators consisting of 528,155 text documents and a raw data size of 1.9 GB and yielding about 86 million distinct (term, do-cid, score) tuples; and (2) the 50 queries of the TREC 2004 Terabyte track on recently crawled web documents of the .gov Internet domain, consisting of 25,205,179 documents and a raw data size of 426 GB and yielding about 2.9 billion distinct (term, docid, score) tuples.

For both collections we used a standard Okapi BM25 [31, 32] model with the exact parameters k 1 and b set to 1.2 and 0.75, respectively. Query expansion was based on WordNet concepts and the statistically quantified hyper-nym/hyponym relations in addition to the concept synsets (see Sec. 4.1). We used both the benchmark query titles, which are merely 2 to 5 keywords, and the descriptions, which are a few sentences describing the query topic. We mostly focus on results for the 50 hard queries of the Ro-bust track data as these have become the gold standard for query expansion; we will discuss results for the Terabyte corpus only in Sec. 6.4 on scalability. We compared both efficiency and effectiveness of the following methods: a) a baseline method without query expansion, with the b) a static expansion method, where we generate a large c) the Incremental Merge method for dynamic on-demand
With static expansion the score aggregation function was the summation of scores for the whole expansion; with In-cremental Merge we used the score aggregation function in-troduced in Sec. 4.2. For both expansion methods we varied the control parameter for probabilistic candidate pruning. In addition to these methods under comparison, we also in-dicate the number of relevant entries of index lists for a DBMS-style Join &amp; Sort query processing, where all inverted index lists that are relevant to a query are fully scanned (however, disregarding random I/Os for phrase matching); this merely serves as a reference point. We measured the following metrics for assessing efficiency and effectiveness:
The first part of Tab. 1 shows the results for the baseline run, i.e., without query expansion. The maximum query dimensionality M ax ( m ) was 4, the average query length Avg ( m ) was 2.5 terms; there was no consideration of phrases. We see that top-k query processing is generally much more efficient than the Join&amp;Sort technique could possibly be, in terms of inverted index lists accesses. The rows for the top-k baseline method differ in their settings for the control pa-rameter. Allowing a modest extent of probabilistic pruning led to noticeable performance gains, while it hardly affects effectiveness. This makes the Prob-k method an excellent choice for a system where high precision at the top ranks is required and efficiency is crucial.
The second part of Tab. 1 shows the efficiency and effec-tiveness results for the 50 hard queries of the TREC Ro-bust track, comparing the Incremental Merge method with static query expansion. A fixed expansion technique using only synonyms and first-order hyponyms of noun-phrases from titles and descriptions already produced fairly high-dimensional queries, with up to 118 terms (many of them marked as phrases); the average query size was 35 terms.
Compared to the baseline without query expansion, all ex-pansion techniques significantly improved the result quality in terms of precision@10 and MAP. The quality is still be-low the very best TREC runs [20, 21] which achieved about 0.37 precision@10, but the results are decent. Recall that our basis for query expansion, WordNet, is certainly not the most suitable choice for ad-hoc query expansion (at least not unless combined with other sources and techniques), and the emphasis of this paper is on efficiency with good (but not hand-tuned) effectiveness. The best result quality in our experiment, 0.31 precision@10, was achieved by the Incremental Merge technique with set to 0.0. Probabilistic pruning with = 0 . 1 reduced the number of sorted accesses by about 25 percent, but in terms of run-time (in CPU sec-onds) it gained a factor of 2 as it also incurred fewer random accesses (mostly for phrase matching) and lower memory overhead. For the static expansion technique, the cost sav-ings by the probabilistic pruning were much higher, more than a factor of 4 in all major efficiency metrics, but this came at the expense of a significant loss in query result qual-ity. In terms of run-time, however, both Incremental Merge and static expansion performed almost equally well when probabilistic pruning was used. The absolute performance numbers of less than 2 seconds per query, with an academic prototype in Java and the high overhead of JDBC sessions, are very encouraging. It is particularly remarkable that for the Incremental Merge method, the probabilistic pruning affected the effectiveness only to a fairly moderate degree, reducing precision@10 from 0.310 to 0.298. This seems to be a very low price for a speed-up factor of 2.

Fig. 3 and 4 show how the effectiveness and efficiency measures change as the control parameter is varied from 0.0 and 0.1 towards higher, more aggressive values (with the extreme case 1.0 corresponding to a fixed amount of index-scan work, looking only at the b = 500 highest-score entries of each index list). We see that the relative precision of most variants drops linearly with , which is just the expected be-havior (see [35]), but in terms of objective result quality, as measured by the TREC relevance assessments, the pruning technique performed much better: both precision@10 and MAP decrease only very moderately with increasing . For example, for = 0 . 5 the best Incremental Merge method could still achieve a precision@10 of about 0.27 while its run-time cost, in terms of sorted accesses, was reduced by a factor of more than 3. In Fig. 3 and 4 the curves for static expansion with query titles and descriptions reveal that the score predictor degenerates for very high-dimensional dis-junctive queries because of neglecting feature correlations. This led to the sudden drop in the number of sorted ac-cesses already for being as low as 0.1, but this came at the expense of a significant loss in retrieval quality. This phe-nomenon did not occur for the Incremental Merge method, where expansions are grouped into multiple nested top-k operators, such that the maximum number of query dimen-sions at the top level was only 22 compared to 118 for the static expansion (including phrases).

Fig. 5 and 6 show the efficiency and effectiveness results as a function of varying the  X  parameter, i.e., the aggressiveness of generating candidate terms and phrases for query expan-sion. Tab. 2 gives detailed figures for various combinations of  X  and values. The charts demonstrate the robustness and self-tuning of the Incremental Merge method: even with very low  X  , meaning very aggressive expansion, both preci-sion@10 and MAP values stayed at a very good level. The execution cost did significantly increase, however, but this is not surprising when you consider that the expansion can-didate sets for some queries had up to 876 terms or phrases  X  quite a stress test for any search engine. In combination with moderate probabilistic pruning, the Incremental Merge method was still able to answer all 50 queries in a few min-utes, less than 4 seconds per query.
The evaluation on the Terabyte corpus and queries served as a proof of scalability for the developed top-k query pro-cessing and expansion methods. The third part of Tab. 1 shows the results of some of our runs, using the same fixed expansion technique as in Sect. 6.3 for the Robust track. The performance gap for static expansions without vs. the one with probabilistic pruning indicates the same overly ag-gressive behavior of the score predictor as for the high di-mensional queries in the Robust setup  X  however, again at the expense of retrieval quality. The overall efficiency gain in terms of access rates for the Incremental Merge method compared to the static expansion without probabilistic prun-ing is even more impressive by a factor of more than 4 and a factor of 8 compared to full scans, respectively, while achiev-ing higher precision@10 and MAP values. This makes the Incremental Merge approach the method of choice in terms of both retrieval robustness and efficiency.
This paper has presented a suite of novel techniques for efficient query expansion and tested them on the TREC Ro-bust and Terabyte benchmarks. The Incremental Merge technique clearly outperformed the traditional method of static expansion, and proved that it can achieve decent query-result quality while exhibiting very good execution cost and run-time behavior. Especially in combination with proba-bilistic score prediction and candidate pruning, it is a very efficient and effective, scalable method that could be of in-terest for industrial-strength search engines. Our emphasis has been on efficiency, so we used only a relatively simple basis for generating expansion terms and phrases. Our fu-ture work may consider combining our methods with more advanced techniques for expansion, using, for example, doc-ument summaries from Google top-10 snippets and query associations from query logs. A second line of ongoing and future work is to study more sophisticated scheduling strate-gies for random accesses to inverted index lists and position indexes. Our current strategies all have a greedy-heuristics flavor; a more advanced approach could be to drive the scheduling of sorted and random accesses by considering score distributions and an explicit cost model.
