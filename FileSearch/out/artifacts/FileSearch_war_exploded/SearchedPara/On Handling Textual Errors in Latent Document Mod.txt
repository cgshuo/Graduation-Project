 As large-scale text data become available on the Web, tex-tual errors in a corpus are often inevitable (e.g., digitizing historic documents). Due to the calculation of frequencies of words, however, such textual errors can significantly impact the accuracy of statistical models such as the popular La-tent Dirichlet Allocation (LDA) model. To address such an issue, in this paper, we propose two novel extensions to LDA (i.e., TE-LDA and TDE-LDA): (1) The TE-LDA model in-corporates textual errors into term generation process; and (2) The TDE-LDA model extends TE-LDA further by tak-ing into account topic dependency to leverage on semantic connections among consecutive words even if parts are ty-pos. Using both real and synthetic data sets with varying degrees of  X  X rrors X , our TDE-LDA model outperforms: (1) the traditional LDA model by 16%-39% (real) and 20%-63% (synthetic); and (2) the state-of-the-art N-Grams model by 11%-27% (real) and 16%-54% (synthetic).
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms, Experimentation Topic Models, Textual Errors, Topic Dependency
Using topic models for representing documents has re-cently been an area of tremendous interests in data min-ing and machine learning. Probabilistic topic models are stochastic models for text documents that explicitly model topics in document corpora. Because probabilistic topic models are  X  X enerative X , they describe a procedure for gen-erating documents using a series of probabilistic steps. One Figure 1: Three examples of erroneous OCR outputs for a poor quality typewritten text (taken from [21]). Erroneous outputs are underlined. of the popular paradigms of topic models, characterized by the Latent Dirichlet Allocation (LDA) model, consists of a series of probabilistic document models and extensions where topics are modeled as hidden random variables. The LDA model is a widely used Bayesian topic model which can model the semantic relations between topics and words for document corpora. The LDA model assumes that text documents are mixtures of hidden topics and applies Dirich-let prior distribution over the latent topic distribution of a document having multiple topics. In addition, it assumes that topics are probability distribution of words and words are sampled independently from a mixture of multinomials. Since the LDA model was introduced in [4], it has quickly be-come one of the most popular probabilistic document mod-eling techniques in data mining and also has inspired a series of extensions (e.g., [18, 6, 12, 15, 20, 1, 14]).

Despite tremendous advancement in document modeling, however, we believe that two major limitations still remain during the document modeling process.

First, the LDA model assumes that the entire document corpus is error-free to ensure accurate calculation of fre-quencies of words . However, an increasing number of new large-scale text data are often machine-generated, and thus inevitably erroneous. For instance, speech recognition soft-wares can turn audio data into textual transcripts with vary-ing error rates. Similarly, Optical Character Recognition (OCR) engines, despite great success in recent attempts such as Google Books or Internet Archive, are not without prob-lems, and often produce error-abundant text data. [13] pointed out that although researchers are having increas-ing levels of success in digitizing hand-written manuscripts, Table 1: Top words (selected by LDA) for five topics of a small sample of Unlv OCR data set (erroneous words are in italic).
 mcknight, vista, de , fleetwood, brown, davi , san, democr error rates remain significantly high. Consider our illustra-tions below.
 Example 1. As an illustration, consider Figure 1 that shows three examples of OCR outputs for a poor-quality type-written text  X  X AILWAY TRANSPORT. X  All three popu-lar OCR engines (i.e., ABBYY FineReader, OmniPage Pro, and Google Tesseract) generated outputs with one erroneous word for each. It is known that the accuracy of the LDA model often declines significantly as word error rates in-crease [21]. Now, consider Table 1 that shows some top words (selected by the LDA model) for five topics of a small sample of Unlv 1 OCR data set. From the list, we can see that there exist a lot of erroneous words in the selected top words. In addition, the words are not representative and the differences between the topics are difficult to iden-tify. This example shows that the performance of tradi-tional LDA model greatly suffers when documents contain erroneous words. 2
Second, since the LDA model does not consider the order of the topics and words, during parameter estimation and inference, the topics and the words are assumed to be ex-changeable . The LDA model relies on the bag-of-words doc-ument prototype. It assumes each word in a document is generated by a latent topic and explicitly models the word distribution of each topic as well as the prior distribution over topics in the document. However, we argue that the ordering of words and phrases are often critical to capture the meaning of texts in data mining tasks. Successive words in the same document are more likely to belong to the same topic. For example, a phrase  X  X ocial network X  is a term in modern information society under Web 2.0 while  X  X ocial X  is a term from traditional sociology and  X  X etwork X  refers to a particular term in computer science. Often, the ordering of terms carries special meanings in addition to the appearance of individual words. Therefore, incorporating topic depen-dency is important to learn topics and also to disambiguate words which may belong to different topics. More impor-tantly, considering the ordering of consecutive terms can of-ten help in dealing with errors found in parts. For instance, despite the typo  X  X etwork X  in the middle from a phrase  X  X o-cial betwork analysis X , surrounding correct words  X  X ocial X  and  X  X nalysis X  still have common semantic connections that could be exploited.

Motivated by the above two observations, in this paper, we introduce our novel models to tackle the issues of noisy data. In particular, we propose a new LDA model termed as TE-LDA to deal with textual errors in document corpora. We further extend it to a new TDE-LDA model in order to take http://code.google.com/p/ isri-ocr-evaluation-tools/updates/list into account topic dependency in the document generation process. Through a set of comprehensive experiments, the efficacy of our proposed models is validated using both real and synthetic data sets.

In summary, with respect to the document modeling prob-lem with varying degrees of noisy corpora and using the per-plexity as an evaluation metric, our second proposal with a better result, TDE-LDA, outperforms: (1) the traditional LDA model by 16%-39% using real data and by 20%-63% using synthetic data; and (2) the state-of-the-art N-Grams model [23] by 11%-27% using real data and by 16%-54% using synthetic data.

Our contributions are as follows:
The rest of this paper is organized as follows. Section 2 briefly reviews the related research work in document mod-eling. Section 3 gives a general overview of the traditional LDA model. Section 4 introduces our proposed models and presents the detailed model formulation. Section 5 presents the results of extensive experimental evaluations of apply-ing our document models to both real and synthetic data sets. Finally, section 6 concludes the paper and discusses the directions of future work.
Probabilistic document modeling has received tremendous attention in the data mining community. A series of prob-abilistic models have been introduced to simulate the doc-ument generation process. These models include the Naive Bayesian model and the Probabilistic Latent Semantic In-dexing (PLSI) model [11]. The LDA model has become most popular in the data mining and information retrieval com-munity due to its solid theoretical statistical foundation and promising performance. A wide variety of extensions of LDA model have been proposed for different modeling purposes in different contexts. For example, the author-topic model [18, 20] uses the authorship information with the words to learn topics. The correlated LDA model learns topics simultane-ously from images and caption words [6]. The Link-LDA model and Topic-link LDA model [12] represent topics and author communities using both content words and links be-tween documents.

Most topic modeling techniques require the bag-of-words assumption [4]. They treat the generation of all words inde-pendently from each other given the parameters. It is true that these models with the bag-of-words assumption simpli-fied the problem domain and enjoyed a big success, hence attracted a lot interests from researchers with different back-grounds. Some researchers tried to drop this assumption to assume that words are generated dependently. For exam-ple, [22] developed a bigram topic model on the basis of the hierarchical Dirichlet language model, by incorporating the concept of topic into bigram models. [23] proposed a topical n -grams model to automatically determines whether to form an n -gram based on the surrounding context of words. [1] developed a probabilistic time series model to capture the evolution of topics in large document corpora. [10] proposed a hidden topic Markov model (HTMM) to incorporate a hidden Markov structure into LDA. However, their model is based on the assumption that all words in the same sentence must have the same topic and imposes a sentence boundary for words. [2] proposed a correlated topic model which al-lows for correlations between topic assignments and draws a topic proportion from a logistic normal instead of a Dirichlet distribution. [9] proposed the HMMLDA model as a genera-tive composite model which considers both short-range syn-tactic dependencies and long-range semantic dependencies between words. [5] proposed a probabilistic model to match documents at both general topic level and specific word level in information retrieval tasks.

Recently, a number of researchers proposed topic segmen-tation models which are closely related to topic models. Topic segmentation is to split a text stream into coherent and meaningful segments. For example, the aspect hid-den markov (HMM) model proposed in [3] models unstruc-tured data which contains streams of words. In the aspect HMM model, documents are separated into segments and each segment is generated from a unique topic assignment and there is no mixture of topics during the inference. [17] proposed a Bayesian approach to linear topic segmentation which assumes some numbers of hidden topics are shared across multiple documents. [8, 7] further extended this work by marginalizing the language models using the Dirichlet compound multinomial distribution, and applied the model to both linear topic segmentation and hierarchical topic seg-mentation for the purpose of multi-scale lexical cohesion. [19] proposed a statistical model that combines topic identifica-tion and segmentation in text document collections, and the model is able to identify segments of text which are topically coherent and cluster the documents into overlapping clusters as well. Note that the Markov transition is based on seg-ments with each being generated from a linear combination of the distributions associated with each topic.

Most topic modeling techniques require clean document corpora. This is to prevent the models from confusing pat-terns which emerge in the noisy text data. Recent work in [21] is the first comprehensive study of document cluster-ing and LDA on synthetic and real-word Optical Character Recognition (OCR) data. The character-level textual errors introduced by OCR engines serve as baseline document cor-pora to understand the accuracy of document modeling in erroneous environment. As pointed out by these researchers, even on clean data, LDA will often do poorly if the very simple feature selection step of removing stop-words is not performed first. The study shows that the performance of topic modeling algorithms degrades significantly as word er-ror rates increase. Our work in this paper is a substantial ex-tension of our preliminary work [25] with a novel model pro-posed, a comparison to state-of-the-art model, and a much more comprehensive empirical study. In this section, we give a brief overview of the Latent Dirichlet Allocation (LDA) model. [4] introduced the LDA model as a semantically consistent topic model, which at-tracted considerable interest from both the statistical ma-chine learning and natural language processing communities. LDA models documents by assuming that a document is composed by a mixture of hidden topics and that each topic is characterized by a probability distribution over words.
The model is known as a graphical model for topic dis-covery. The notations are shown in Table 2.  X  d denotes a T -dimensional probability vector and represents the topic distribution of document d .  X  t denotes a W -dimensional probability vector where  X  t,w specifies the probability of gen-erating word w given topic t . Multi ( . ) denotes multinomial distribution. Dir ( . ) denotes Dirichlet distribution.  X  is a T -dimensional parameter vector of the Dirichlet prior distri-bution over  X  d , and  X  is a W -dimensional parameter vector of the Dirichlet prior distribution over  X  t . The process of generating documents is shown in Algorithm 1.

Algorithm 1: The LDA Model .
For each of the T topics t , sample a distribution over words  X  t from a Dirichlet distribution with hyperparameter  X  ;
For each of the D documents d , sample a vector of topic proportions  X  d from a Dirichlet distribution with hyperparameter  X  ;
For each word w d,i in document d , sample a topic z d,i from a multinomial distribution with parameters  X  d ;
Sample word w d,i from a multinomial distribution with parameters  X  z d,i .

Performing exact inferences for the LDA model is intractable due to the choice of distribution and the complexity of the model. The existing approximate algorithms for parameter estimation and inference of the LDA model include varia-tional methods [4], EM algorithm [11] and Markov Chain Monte Carlo (MCMC) [16]. One assumption in the genera-tion process above is that the number of topics is given and fixed. LDA model considers documents as  X  X ags of words X , i.e., there is no ordering between words and all words as well as their topic assignments in the same document are as-sumed to be conditionally independent. Furthermore, find-ing good estimates for the parameters of LDA model re-quires accurate counts of the occurrences and co-occurrences of words, which in turn requires a  X  X erfect X  corpus with er-rors as few as possible.
To account for textual errors in the traditional LDA topic model, in this section, we propose a new LDA model termed as TE-LDA (LDA with T extual E rrors) to take into account noisy data in the document generation process. We further extend it to a new TDE-LDA (LDA with T opic D ependency and textual E rrors) model in order to take into account topic dependency in the document generation process. We explain the details of our proposed models in the following.
In this model, we distinguish the words in the documents and separate them as tokens and typos. Given a document, each word has a probability to be an error and we want to capture this probability structure in the term generation process. In order to reflect the nature of textual errors in the generative model, we adopt a switch variable to control the influence of errors on the term generation.

The proposed model is illustrated in Figure 2(a). Here we introduce some notations used in the graphical model: D is the number of documents, T is the number of latent topics, N d is the total number of words in document d (with N d = N term + N typo , the sum of all the true terms and ty-pos).  X  ,  X  and  X  0 are parameters of Dirichlet priors,  X  the topic-document distribution,  X  t is the term-topic distri-bution.  X  typo is the term distribution specifically for typos. We include an additional binomial distribution  X  with a Beta prior of  X  which controls the fraction of errors in documents.
For each word w in a document d , a topic z is sampled first and then the word w is drawn conditioned on the topic. The document d is generated by repeating the process N d times. To decide if each word is an error or not, a switch variable X is introduced. The value of X (which is 0 or 1) is sampled based on a binomial distribution  X  with a Beta prior distribution of  X  . When the sampled value of X equals 1, the word w is drawn from the topic z t which is sampled from the topics learned from the words in document d . When the value of X equals 0, the word w is drawn directly from the term distribution for errors. Overall, the generation process for TE-LDA can be described in Algorithm 2.
As we mentioned in the introduction section, LDA relies on the bag-of-words assumption. However, in many data mining tasks, words are often connected in nature and suc-cessive words in the document are more likely to be assigned the same topic. Therefore, incorporating topic dependency
A lgorithm 2: The TE-LDA Model . F or each of the D documents d , sample  X  d  X 
Dir(  X  )and  X  d  X  Beta(  X  );
For each of the T topics t , sample  X  t  X  Dir(  X  );
Sample  X  typo  X  Dir(  X  0 ); foreach N d words w d,i in document d do 5 S ample a flag X  X  Binomial(  X  d ); 6 if X = 1 then 7 S ample a topic z d,i  X  Multi(  X  d ); 8 Sample a word w d,i  X  Multi(  X  z d,i ); 9 end if 10 if X = 0 then 11 S ample a word w d,i  X  Multi(  X  typo ); end foreach i s important to capture the semantic meaning of texts and also to disambiguate words which may belong to different topics. Even in noisy text corpora, consecutive words may be dependent to each other regardless of textual errors. For example, in a phrase  X  X ext dat mining X  with textual error  X  X at X  as typo of word  X  X ata X , the correct word  X  X ext X  and  X  X ining X  still have semantic connections and both words be-long to the same topic of data mining. Hence, incorporating this correlation gives a more realistic model of the latent topic structure and we expect to obtain better generaliza-tion performance quantitatively. To apply topic dependency and drop the bag-of-words assumption, we assume the top-ics in a document form a Markov chain with a transition probability that depends on a transition variable Y . When Y equals 0, a new topic is drawn from  X  d . When Y equals 1, the current topic of word w i is equivalent to the previous topic of word w i  X  1 . determine whether to form an n -gram based on the sur-rounding context of words. The n -grams model is an ex-tension of the bigram topic model, which makes it possible to decide whether to form a bigram for the same two con-secutive words depending on the nearby context. As a re-sult, the n -grams model imposes a Markov relation on the word set. In contrast, topic dependency considers the rela-tion between consecutive topics instead of words. That is, the Markov relation is on the topic set instead of the word set. Figure 3(a) shows an alternative graphical model for applying topic dependency to LDA. The n -grams model is illustrated in Figure 3(b). We incorporate topic dependency in our proposed TE-LDA model in the following.
We extend our TE-LDA model and further incorporate topic dependency into one unified model, named as TDE-LDA. The proposed model is illustrated in Figure 2(b).
For each word w in a document d , a topic z is sampled first and then the word w is drawn conditioned on the topic. The document d is generated by repeating the process N d times. To decide if each word is an error or not, a switch variable X is introduced. The value of X (which is 0 or 1) is sampled based on a binomial distribution  X  with a Beta prior distribution of  X  . When the sampled value of X equals 1, the word w is drawn from the topic z t which is sampled from the topics learned from the words in document d . To decide if the current topic is dependent to the previous topic  X  Figure 3: Comparison of topic dependency and term dependency. or not, a switch variable Y is introduced. The value of Y (which is 0 or 1) is sampled based on a binomial distribution  X  with a Beta prior distribution of  X  . When the sampled value of Y equals 1, the topic z i is assigned to be identical to the previous one z i  X  1 to reflect the dependency between them. When the value of Y equals 0, the topic z i is sampled from the topics learned from the words in document d . And the word w is drawn from the topic z t . When the value of X equals 0, the word w is drawn directly from the term distribution for errors. The generation process for TDE-LDA can be described in Algorithm 3.

Algorithm 3: The TDE-LDA Model.
For each of the D documents d , sample  X  d  X  Dir(  X  )and  X  d  X  Beta(  X  );
For each of the T topics t , sample  X  t  X  Dir(  X  );
Sample  X  typo  X  Dir(  X  0 ); foreach N d words w d,i in document d do 5 Sample a flag X  X  Binomial(  X  d ); 6 if X = 1 then 7 Sample a flag Y  X  Binomial(  X  d ); 8 if Y = 1 then 11 if Y = 0 then 12 Sample a topic z d,i  X  Multi(  X  d ); 14 Sample a word w d,i  X  Multi(  X  z d,i ); 16 if X = 0 then 17 Sample a word w d,i  X  Multi(  X  typo ); end foreach
In this section, we discuss two important issues on our proposed models.
 Rare Words vs. Textual Errors In terms of frequency of words, note that it is difficult to dif-ferentiate between rare-but-correct-English words and typos because both appear rather seldom in the corpus. Without prior knowledge of grammar and syntax of human language or helps of dictionary, that is, machines cannot solely rely Figure 4: Comparison of percentages of typos and rare words. on the word frequency to tell the difference between a tex-tual error and a rare word. To illustrate this point, we se-lected the Reuters newswire data set (to be explained in Section 5.1) and combined two OCR Magazine data sets. We calculated the percentages of words that appear from once to five times in the corpus. In Figure 4, the percent-age curves of both typos and rare words exhibit very similar patterns in both corpora, making a computation-based dif-ferentiation hard. Therefore, our models adopt a supervised approach to distinguish rare words and textual errors in the document modeling process. One may use linguistic char-acteristics to differentiate typos in an unsupervised fashion. However, since the immediate goal of this paper is first to evaluate the validity of incorporating textual errors into doc-ument modeling process, we rather leave the development of more sophisticated modeling methods for future work. Topic vs. Term Dependency The bigram topic model and n -grams model we mentioned in section 2 determine whether to form a bigram or an n -gram based on the surrounding words in the document. Although these models show better generalization performance over LDA, we argue that incorporating term dependency is not suitable in noisy text data for two reasons. First, in noisy document corpora, simply forming bigram or n -gram be-tween consecutive words will increase the overall error rate. This is because an erroneous word will impact both the pre-vious word and the succeeding word in terms of term com-bination. But it only impacts itself under the bag-of-words assumption for documents. Secondly, even though our TE-LDA model has a mechanism to distinguish between textual errors and correct words, by skipping typos the document model may generate incorrect bigram or n -gram word combi-nations which, in turn, decreases the accuracy of generaliza-tion performance. Therefore, we only consider topic depen-dency in order to capture the semantic relation of words. As a result, in this paper, we select the traditional LDA model and the n -grams model without error modeling as baselines.
In order to validate our proposed models, we applied it to the document modeling problem. We trained our new mod-els as well as the traditional LDA model on both synthetic and real text corpora to compare the generalization perfor-mance of these models. The documents in the document corpora are treated as unlabeled and the goal is to achieve high likelihood on a held-out test data [4]. In our experi-ments, each model was trained on 90% of the documents in each data set with fixed parameters  X  =0.5,  X  =0.01,  X  0 =0.01 and  X  =0.1 for simplicity and performance. The trained model was used to calculate the estimate of the marginal log-likelihood of the remaining 10% of the documents.
Table 3 shows the summary of both real and synthetic data sets that we used in our experiments.

First, we prepared real data sets that contain varying degrees of errors in texts. From the PDF images in the data set, Unlv , using one of the most popular OCR engines (Google Tesseract), we converted PDF images to a textual document corpus. Since Unlv has the full texts as the ground truth, by comparing the transcript generated from OCR, we can exactly pinpoint which words are errors. In the end, we prepared five subsets: Business , Magazine , Legal , Newspaper , Magazine2 . Similarly, we prepared another real corpus called BYU 2 which consists of 600 of the Eisenhower World War II communiques. This data set contains the daily progress of the Allied campaign until the German surrender. Example documents from Newspaper data set and BYU data set are shown in Figure 5. The quality of these originals is quite poor, hence the error rate is pretty high for the out-puts of OCR engine. Note that in these real data sets, we cannot control the degrees of errors, and the error rates are determined by the OCR engine.

Second, to conduct more controlled experiments, we also prepared synthetic data sets. In particular, we used three well-known benchmark data sets in the document modeling literature: TREC AP , NIPS , and Reuters-21578 . The TREC Associate Press (AP) data set 3 contains 16,333 newswire ar-ticles with 23,075 unique terms. The NIPS data set 4 consists of the full text of the 13 years of proceedings from 1988 to 2000 Neural Information Processing Systems (NIPS) Con-ferences. The data set contains 1,740 research papers with 13,649 unique terms. The Reuters-21578 data set 5 consists of newswire articles classified by topics and ordered by their date of issue. The data set contains 12,902 documents and 12,112 unique terms. http://www.lib.byu.edu/dlib/spc/eisenhower http://www.daviddlewis.com/resources/ testcollections/trecap/ http://www.cs.nyu.edu/~roweis/data.html http://kdd.ics.uci.edu/databases/reuters21578/ reuters21578.html Figure 5: Example documents from UNLV and BYU data sets.

For all the above synthetic data sets, we generated er-roneous corpora to simulate different levels of Word Error Rates (WER)  X  i.e., the ratio of word insertion, substitu-tion and deletion errors in a transcript to the total number of words. Then, we closely studied the impact of textual errors in document modeling. In our experiments, we used three types of edit operations (i.e., insertion, deletion and substitution) in all the documents as follows: (1) insertion: a number of terms are randomly selected in a uniform fash-ion to insert a single character into the terms; (2) deletion: a number of terms are randomly selected in a uniform fashion to delete a single character from the terms; (3) substitution: a number of terms are randomly selected in a uniform fash-ion to change a single character of the terms. Note that multiple edit operations are not allowed for a single word. Let S , D and I denote the number of substitution, deletion and insertion operations, and let N denote the total num-ber of words. Then, WER is calculated as follows. The procedure repeats until the desirable WER is achieved.
The purpose of document modeling is to estimate the den-sity distribution of the underlying structure of data. The common approach to achieve this goal is by evaluating the document model X  X  generalization performance on new un-seen documents. In our experiments, we calculated the per-plexity of a held-out test set to evaluate the models. In lan-guage modeling, the perplexity quantifies the goodness of measuring the likelihood of a held-out test data to be gen-erated from the learned distribution of the trained model. In particular, it is monotonically decreasing in the likeli-hood of the test data, which means a lower perplexity score corresponds to better generalization performance of the doc-ument model. Formally, for a test data of D test documents the perplexity score is calculated as follows [4, 16]: In the above equations, the probability p ( w d | z k ) is learned from the training process and p test ( z k | d ) is estimated through an additional Gibbs sampling process on the test data based on the parameters  X  and  X  learned from training data.
We first examine the performance of our TE-LDA model on real OCR data sets. Note that our immediate objective is to evaluate the validity of incorporating textual errors into document modeling process. This is based on the fact that most large-scale text data are machine-generated and thus inevitably contain many types of noise. As a novel solu-tion, our TE-LDA model is developed from the traditional LDA model by adding a switch variable into the term gener-ation process in order to tackle the issue of noisy text data. Hence, in this experiment, we compare the generalization of our TE-LDA model with the traditional LDA on various erroneous OCR text data. For example, each subset of real OCR data Unlv has a fixed WER, determined by the OCR engine. Due to the poor quality of PDF images and imper-fect OCR process, WERs range from 0.2856 to 0.3739. That is, about 28 X 37% of words in the corpus could be erroneous words. Similarly, the WER of real OCR data BYU is as high as 48%.

Recently, [24] proposed an algorithm for applying topic modeling to OCR error correction. The algorithm builds two models on an OCR document. One is a topic model which provides information about word probability and the other is an OCR model which provides the probability of charac-ter errors. The algorithm can reduce OCR errors by around 7%. We use the same error detecting technique to further correct our six real OCR data sets and then compare the per-formance of our TE-LDA model with the traditional LDA again. By doing so, we aim at finding out how the behav-ior of both topic models changes as the error rate changes on real OCR data. Figure 6 shows the perplexity of TE-LDA and LDA as a function of the number of hidden topics (e.g., 10, 20, 40, and 80). As we can see, our proposed TE-LDA model consistently outperforms the traditional LDA model on both original and improved Unlv as well as BYU data sets. An interesting finding is that LDA performs bet-ter on improved corpora while TE-LDA performs better on original corpora. This is reasonable because our model is specifically designed to deal with textual errors in modeling noisy text documents and can achieve better generalization performance as the word error rates increase.
In this section, we systematically evaluate the performance of different models using various real and synthetic data sets. Since our purpose is to understand the performance of doc-ument modeling in erroneous environment, we compare the performance of our proposed models and the baseline models without removal of typos in text corpora.
 Results using Real Data Sets We first compare the performance of our proposed models with the traditional LDA model and Wang X  X  n -grams model on the real OCR data sets. Figure 7 shows the perplexity of TE-LDA and TDE-LDA models as a function of the num-ber of hidden topics (e.g., 10, 20, 40, and 80) on the five subsets of Unlv corpus and the BYU corpus. As we can see, despite high WERs and different document themes among these data sets, our proposed TE-LDA and TDE-LDA mod-els consistently outperform the traditional LDA model and the n -grams model. Note also that TDE-LDA is the best among the proposed models and the baseline models, which demonstrates that considering topic dependency improves the generalization performance of topic models in the con-text of noisy data.
 Table 4: Comparison of the selected top words us-ing LDA vs. N-grams vs. our proposed models on a small sample of Unlv OCR data set. OCR-introduced erroneous words are in italic .

T able 4 shows examples of top words selected by LDA and the n -grams model as well as our models on the topic 3 of Table 1. From the table, note that LDA suffers from choos-ing many OCR-introduced erroneous words as top words. Furthermore, the n -grams model tends to select several erro-neous n -gram words as well. On the contrary, both TE-LDA and TDE-LDA models selected no erroneous top words, high-lighting the superiority of our models in dealing with noisy text data. Overall, compared to others, our TDE-LDA model can select meaningful and generic top words or highly related words and make the topic more understandable. Results using Synthetic Data Sets We then systematically compare the performance of our pro-posed models with the traditional LDA model as well as Wang X  X  n -grams model on the synthetically generated er-roneous corpora. In this comparison, we simulate differ-ent levels of WER (e.g., 0.01, 0.05, 0.1). Figures 8(a)-(c) show the perplexity of TE-LDA and TDE-LDA models as a function of the number of hidden topics in the TREC AP cor-pus. As we can see from Figures 8(a)-(c), at different levels of WER, our TE-LDA and TDE-LDA models consistently outperform the traditional LDA model. Furthermore, as WER increases, the margin of improvement increases. This is due to the incorporation of textual errors into the gen-eration of terms in the document modeling process. We can also see that the models with consideration of topic or term dependency outperform the ones without that, regard-less of whether we take into account textual errors during term generation. However, TDE-LDA is the best among the models and show better generalization of incorporating topic dependency in noisy text data. This demonstrates the improved performance of topic models with the removal of bag-of-words assumption.

In Figures 8(d)-(f), we fix the number of topics K and demonstrate how the different models perform as the WER increases in the TREC AP corpus. An interesting finding here is that the perplexity of both LDA and n -grams models in-creases as the word error rates increase. This is because these two models do not consider the errors in the term gen-eration where the accuracy of calculation of word frequencies is affected. In contrast, our TE-LDA and TDE-LDA models outperform the other two and the margin of improvement increases as the word error rates increase. The experimen-tal results in the NIPS (Figures 8(g)-(l)) and Reuters (Fig-ures 8(m)-(r)) corpora show similar perplexity patterns.
In this paper, we have proposed two extensions to the tra-ditional LDA model to account for textual errors in latent document modeling. Our work is motivated by the facts that textual errors in document corpora are often abundant and separating words cannot completely capture the meaning of texts in data mining tasks. To overcome these constraints, we proposed our TE-LDA and TDE-LDA models to incorpo-rate textual errors into the term generation process. Both TE-LDA and TDE-LDA adopt a switching mechanism to explicitly determine whether the current term is generated from the topic-document distribution through the general topic generation route or from a special word distribution through the typo processing route. However, TDE-LDA models the transition of topics between consecutive words as a first-order Markov process. Through extensive exper-iments, we have shown that our proposed models are able to model the document corpus in a more meaningful and realistic way, and achieve better generalization performance than the traditional LDA model and the n -grams model.
Many directions are ahead. First, we plan to infer more complex topic structures and conduct tests of statistically significant differences across all the models. Second, we plan to apply our proposed models to handling textual errors in user-generated contents on social media. This research was in part supported by NSF awards of DUE-0817376, DUE-0937891, and SBIR-1214331. [1] D. M. Blei and J. D. Lafferty. Dynamic topic models. [2] D. M. Blei and J. D. Lafferty. A correlated topic [3] D. M. Blei and P. J. Moreno. Topic segmentation with [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] C. Chemudugunta, P. Smyth, and M. Steyvers.
 [6] X. Chen, C. Lu, Y. An, and P. Achananuparp.
 [7] J. Eisenstein. Hierarchical text segmentation from [8] J. Eisenstein and R. Barzilay. Bayesian unsupervised [9] T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. [10] A. Gruber, M. Rosen-Zvi, and Y. Weiss. Hidden topic [11] T. Hofmann. Probabilistic latent semantic analysis. In [12] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link [13] W. B. Lund and E. K. Ringger. Improving optical [14] R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. [15] D. Newman, C. Chemudugunta, and P. Smyth.
 [16] I. Porteous, D. Newman, A. Ihler, A. Asuncion, [17] M. Purver, T. L. Griffiths, K. P. Kording, and J. B. [18] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and [19] M. M. Shafiei and E. E. Milios. A statistical model for [20] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [21] D. D. Walker, W. B. Lund, and E. K. Ringger. [22] H. Wallach. Topic modeling: Beyond bag-of-words. In [23] X. Wang, A. McCallum, and X. Wei. Topical n-grams: [24] M. Wick, M. Ross, and E. Miller. Context-sensitive [25] T. Yang and D. Lee. Towards noise-resilient document data sets respectively.
