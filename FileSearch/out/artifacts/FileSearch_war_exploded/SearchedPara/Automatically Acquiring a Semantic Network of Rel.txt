 We describe the automatic construction of a semantic net-1,500 and 100,000 times) are linked to their semantically related concepts in the WordNet noun ontology. Related-ness between nouns is discovered automatically from co-occurrence in Wikipedia texts using an information theoret ic inspired measure. Our algorithm then capitalizes on salien t sense clustering among related nouns to automatically dis-ambiguate them to their appropriate senses (i.e., concepts ). Through the act of disambiguation, we begin to accumulate relatedness data for concepts denoted by polysemous nouns, as well. The resultant concept-to-concept associations, c ov-ering 17,543 nouns, and 27,312 distinct senses among them, constitute a large-scale semantic network of related conce pts that can be conceived of as augmenting the WordNet noun ontology with related-to links.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing; I.2.4 [ Artificial Intelligence ]: Knowledge Represen-tation Formalisms and Methods X  semantic networks ; I.2.6 [ Artificial Intelligence ]: Learning X  concept learning, con-nectionism and neural nets, knowledge acquisition Algorithms, Experimentation, Measurement semantic relatedness, semantic networks, lexical semanti cs, common sense knowledge, knowledge acquisition 1 Available online: http://www.cs.ucf.edu/ ~ seansz/sem not Wikipedia.

While the contemplation of semantic networks and their usefulness for natural language understanding tasks dates back to the work of Quillian [26], we have yet to see the creation of a large-scale, viable model of semantic memory. The WordNet ontology [8] constitutes a partial realization of Quillian X  X  dream through its instantiation of a variety o f labeled edges indicating, inter alia , subsumptive is-a rela-constitute a rich taxonomy of semantic similarity.
Absent from the ontology, however, is a more general indi-cation of semantic relatedness. From WordNet we can infer, for example, the similarity between penguins and flamingos : both share the superordinate concept aquatic bird (and are, by virtue of their similarity, also related). In contrast, t he relatedness between penguins and icebergs (which certainly are not similar entities, although they are conceptually re -lated), is not articulated in the ontology.

This distinction between similarity and relatedness is wel l established in the literature [28], and augmenting WordNet with related-to links would open new vistas for applications and research using the ontology (see Section 8). To un-derstand the role of semantic relatedness in natural lan-guage understanding, consider, for example, the following sentences: (1) The astronomer photographed the star. (2) The paparazzi photographed the star.

Despite the syntactic equivalence of the two sentences, it i s clear that the  X  X tar X  in (1) denotes a celestial body, wherea s the  X  X tar X  in (2) refers to a celebrity. While it is conceiv-able that an astronomer would photograph a celebrity, or that a paparazzo would photograph a celestial object, the  X  X tars X  here are preferentially disambiguated by the stron g semantic relatedness between astronomer and the celestial body sense of  X  X tar, X  and paparazzi and the celebrity sense of  X  X tar, X  respectively. Notice that if we relied on semantic s im-ilarity to disambiguate (1), the path in WordNet connecting astronomer and the celebrity sense of  X  X tar X  (in that both are people ) would lead us astray.

Implicit to this discussion so far is the assumption that the semantic network relates not just words, but concepts.  X  X oun senses X  interchangeably. In distinguishing between words and the concepts they denote, we quote the former and italicize the latter. Concept-level association is critical for natural languag e un-derstanding, although we see in much of the existing litera-ture a focus on surface-level relationships between words.
In this work, we automatically acquire a semantic net-work of related concepts . For our concepts, we use the noun senses defined in WordNet 3.0. This seems an obvious choice given the sophistication of WordNet X  X  noun ontology and its ubiquitous use in computational linguistics and artificial in-telligence. Rather than tie edges to weights that we derive from co-occurrence data, which are susceptible to corpus bi -ases, we create a network in which relatedness is represente d categorically, without weight. However, this network coul d presumably be used as a kernel to infer quantitative relat-edness scores, in the same way that WordNet has been used to derive semantic similarity scores between concepts.
We first acquire relatedness between nouns by applying a novel adaptation of an information theoretic measure to co-target corpus for its large size and coverage of the English language. However, our approach is not specific to Wikipe-dia; it can be applied to any large corpus, either to augment our existing semantic network or to create a new one.
Once we have established relatedness between nouns, we automatically disambiguate them to their corresponding no un senses in WordNet, capitalizing on sense similarity cluste r-ing and high degrees of inter-relatedness that we have found to occur among related nouns.

We should note that, when faced with two polysemous nouns that are related, disambiguation often requires that we already have a related-to link established between the ap-propriate word senses. Consider, for example, the relation between X  X us X  X nd X  X orn. X  X ere we think not of a computer X  X  front-side bus, or of a rhinoceros X  X  horn, but of the automo-bile and its car horn. This automatic disambiguation result s from the semantic relatedness between the two senses de-noted by these words. Of course, if our goal is to instantiate such a related-to link, then the link cannot be used for this initial act of disambiguation. Another approach is called f or.
We instead focus on disambiguating words related to mo-nosemous nouns. In doing so, the monosemous noun to which a polysemous noun is related provides an unequiv-ocal context in which that disambiguation can take place (cf.  X  X orn X  and the monosemous  X  X hinoceros X ). This makes disambiguation significantly more achievable. That is not to say, however, that we are not accumulating relatedness data for polysemous nouns. By disambiguating the  X  X orn X  to which  X  X hinoceros X  is related and the  X  X orn X  to which  X  X boe X  is related, we begin to accumulate relatedness data for individual senses of the polysemous  X  X orn. X  Addition-ally, once this initial partition is formed, we can recover a nd disambiguate polysemous nouns related to individual sense s of  X  X orn, X  although a detailed discussion of this process is beyond the scope of this paper.

Thus, the contributions of this work are twofold: we of-fer (1) a novel approach to discovering semantic relatednes s based on lexical co-occurrence data from a large corpus and (2) a first iteration of a semantic network of related concept s, automatically acquired by considering relatedness to over 3000 of the most frequently occurring monosemous nouns in Wikipedia, and currently relating 27,312 distinct sense s from among 17,543 nouns. http://www.wikipedia.org
The rest of this paper proceeds as follows. In Section 2, we discuss our approach in the context of related work. In Sections 3 through 6, we explicate our approach to automat-ically acquiring the semantic network. At the end of each of these sections, we pause to present results and an evalu-ation of our algorithm X  X  performance up to that point. We discuss two excerpts from the semantic network in Section 7, and present our conclusions and directions for future work in Section 8.
Our work bears strong relation to ConceptNet [19]. Not-withstanding the name, ConceptNet is a semantic network in which the nodes stand for words that are not disambiguated (not concepts). The network is constructed using a set of 20 predefined semantic relations (e.g., EffectOf , CapableOf , Lo-cationOf ) coupled with regular expression pattern matching to extract categorical relatedness between words from the Open Mind Common Sense project. The strengths of Con-ceptNet compared to our work are that it relates not only nouns, but also verbs, adjectives, and prepositional phras es, and it indicates the semantic relation that associates each pair of nodes.

However, its dependence on a finite set of predefined se-mantic relations precludes ConceptNet from discovering re -latedness between words in the general case; as Quillian aptly points out,  X  X n natural language text almost anything can be considered as a relationship, so that there is no way to specify in advance what relationships are to be needed X  (emphasis in original) [26]. Furthermore, the constructio n of ConceptNet is not fully automated, as it relies on com-mon sense facts that are manually entered into its training corpus, and cannot discover relatedness from a corpus that is not hand-tailored for the purpose.

Several other methods have used pattern matching to dis-cover specific semantic relations. Turney X  X  Latent Relatio nal Analysis (LRA) [34, 33] induces patterns automatically fro m a pair of relationally similar words, and solves SAT analo-gies, while Davidov and Rappoport X  X  [7] unsupervised pat-tern clustering algorithm has been used to create categorie s of semantically similar words. Pantel and Pennacchiotti X  X  Espresso algorithm induces search patterns automatically from a corpus, given small seed sets of related nouns, and has successfully discovered hyponymic ( is-a ) and meronymic ( part-of ) relations between nouns, several relations specific to the domain of chemistry (such as chemical reaction and production relations), among others [23].

Manually defined lexico-syntactic patterns have also been used to harvest relations from large corpora. Hearst [13] fir st used such patterns to automatically discover hyponymic re-lations not present in WordNet. For example, the pattern NP { , NP } * { , } or other NP was used to establish all the for-mer NPs as hyponyms of the latter, as in  X ...temples, trea-suries, and other important civic buildings, X  where we see that  X  X emple X  and  X  X reasury X  are hyponyms of  X  X ivic build-ing. X  In a similar vein, Girju et al. [10] and Berland and Charniak [2] used manually defined lexico-syntactic patter ns to mine large corpora for meronymic relations.

The major difference between these approaches and ours is that (with the exception of [7]) the pattern-based method s require a predetermination of the specific types of relation s to be mined, whether through the articulation of exemplar seed sets, target noun pairs, or lexico-syntactic patterns , and are not designed for the more general discovery of semantic relatedness that we are interested in.

Other approaches in the literature typically measure re-latedness between two concepts or nouns quantitatively, an d are distinct from our work in that they do not build data-bases or discover categorical relatedness. Whereas we at-tempt to answer the question,  X  X hat concepts are related to X ? X  the quantitative approaches attempt to answer the question, in which both concepts (or nouns) are given as pri-ors,  X  To what degree are X and Y related? X  (which clearly cannot be precomputed for every possible X and Y in the English language).

Some of these quantitative approaches attempt to measure relatedness using only information available from WordNet , such as is-a relations and sense glosses [24, 15]. These meth-ods are inherently limited by the fact that, while Word-Net serves as a rich taxonomy of semantic similarity, it lacks general indications of semantic relatedness (with it s articulation of holonymic relationships being the notable ex-ception). Consider, for example, how WordNet-based ap-proaches would discover the strong semantic relationship, as our system does, between penguin and tuxedo . For this purpose, the minimalistic glosses of WordNet are simply in-sufficient; if we want to discover relatedness beyond semanti c similarity, beyond the most obvious examples of relatednes s, we need the assistance of a sizeable corpus.

For this reason, many quantitative measures have turned to large corpora to measure relatedness, often relying on di s-tributional similarity to establish synonymy and hypernym relations between nouns [12, 11]. Some measures have used the underlying structure of Wikipedia (i.e., disambiguati on pages and links between articles) to measure semantic re-latedness between nouns or concepts, sometimes grounding their work in the folksonomy of concepts constituted by ti-tles of Wikipedia articles rather than measuring relatedne ss between WordNet synsets [30, 9, 35]. Suchanek et al. [31] derived a semantic network called YAGO from the underly-ing structure of Wikipedia articles. Over 73% of the facts in YAGO are encompassed by its isCalled , type , and means re-lations, which are indicative of semantic similarity. Amon g its most frequent relations beyond those indicating simila rity are specific ones such as bornOnDate , diedOnDate , hasPopu-lation , bornInLocation , actedIn , directed , and writtenInYear .
Augmenting the structure of Wikipedia itself has been the subject of research, as well, and involves the discov-ery of relations between articles. Mihalcea and Csomai [20] augmented the underlying structure of Wikipedia by adding links between pages after automatically identifying keywo rds in each article and disambiguating those words to their ap-propriate Wikipedia concepts (article titles). Ponzetto a nd Navigli [25] used graph theoretic approaches to augment the taxonomic organization of Wikipedia articles.

Other quantitative approaches have leveraged the large amounts of data available on the Web to discover related-ness. Agirre and de Lacalle [1] employed web queries to as-sociate WordNet synsets with representative context words , known as topic signatures. On average, a topic signature from their collection contains 6877 words and their associ-ated weights. Cuadros and Rigau [6] have used these data to construct four KnowNets, semantic knowledge bases de-rived by disambiguating the top 5, 10, 15, and 20 nouns, respectively, from the topic signatures of Agirre and de La-calle. Similarly, Navigli [22] has developed a semi-automa ted method for creating a semantic network by disambiguating terms in collocations extracted from various semantically annotated resources, including WordNet and the Longman Language Activator.

In our approach, we rely solely on lexical co-occurrence between two nouns in a large corpus to discover semantic re-latedness, rather than drawing on predetermined relations , lexico-syntactic patterns, distributional similarity (c ontext), the underlying structure of Wikipedia or WordNet, or other semantically annotated resources. Because our approach is fully automated and we avoid relying on structured or se-mantically annotated resources, it can be applied to any large corpus, in any language, to discover new semantic re-lations, build new semantic networks, and augment existing ones with related-to links. Because we relate concepts cat-egorically rather than quantitatively, and because we rela te concepts, not nouns, the large-scale resource we have devel -oped has potential for use in a wide variety of semantically driven tasks.
Our algorithm for acquiring the semantic network unfolds in three stages. First we measure the relational strength be -tween nouns co-occurring in Wikipedia using an information theoretic measure. We then use this quantitative measure to make categorical assertions about relatedness between nouns. Finally, we disambiguate related nouns automati-cally, giving rise to a semantic network of related concepts . To facilitate the extraction of co-occurrence data from Wikipedia, we have part-of-speech tagged the entire Wi-kipedia corpus (stripped of markup and metadata) using Brill X  X  tagger [3]. Throughout the remainder of this work, co-occurrence, and only between noun stems, rather than extracting separate data for distinct inflected forms. Any noise that results from considering co-occurrence at the se n-tence level, rather than adopting a smaller or variable size d window, is generally quashed by the sheer magnitude of co-occurrence data available from the corpus.
We now adopt the following terminology. A target is any noun for which we would like to extract relatedness data. Nouns co-occurring with a target are called its co-targets , all of which are potentially semantically related to the tar -get.

We define relational strength as a quantitative measure of the semantic relatedness of a target, t , to one of its co-targets, c . For this purpose, we adapt Resnik X  X  selectional association metric [28], given here in the form of S rel ( t, c ), the relational strength of t to c : where P ( c ) is the relative frequency of c  X  X  occurrence in the corpus (the number of times c occurs, divided by the number mal, we only consider common nouns here. of noun tokens counted in the corpus). Similarly, P ( c | t ) is the probability of encountering c in a sentence containing t (the number of times c occurs in sentences containing t , divided by the total number of nouns tokens co-occurring with t ).

D KL is the relative entropy, or Kullback-Leibler diver-gence, between probability distributions P ( C | t ) and P ( C ), where C is the class of all co-targets of t :
Intuitively speaking, D KL indicates how likely we are to encounter c as a consequence of encountering t . Its high-est values are assigned when c  X  X  relative frequency of co-occurrence with t is significantly higher than c  X  X  relative fre-quency of occurrence in the corpus.

We are primarily interested in using S rel ( t, c ) to measure the relatedness of t to c relative to all other co-targets of t , rather than measuring relational strength in a global fashi on. Accordingly, the metric is used only to sort the list of t  X  X  co-targets in order of decreasing relational strength, after w hich the usefulness of the metric is exhausted, and its values are discarded. Thus, D KL , which is constant with respect to c , can be dropped from the definition of S rel ( t, c ); the ordering of t  X  X  co-targets remains the same. This leaves us with:
We also make this pragmatic change to our metric: to account for the relatedness of c to t , which certainly plays some role in the relational strength of t to c , we multiply S rel ( t, c ) by P ( t | c ). This is particularly useful in suppressing words like  X  X rticle, X  which tends to appear frequently with nouns that serve as titles of Wikipedia articles, despite th e fact that those nouns are not generally semantically relate d to  X  X rticle X  at all 6 . With this final modification, S rel becomes:
Given a target of interest, we assemble its co-occurrence data (if it has not already been cached) and sort all co-targets by descending order of S rel ( t, c ). The notable ex-ception is that if P ( c | t ) &lt; 0 . 07%, we exclude c from con-sideration outright. This is done largely as a computationa l requires us to have the co-occurrence data for both t and c to compute S rel ( t, c ), and, as there are often thousands of nouns co-occurring with a target below this frequency threshold, we save a considerable amount of processing time by eliminating them. This also protects us from false indi-cations of relatedness that would arise if an incredibly rar e word from the corpus were to co-occur with a semantically unrelated target just once or twice, as a matter of happen-stance. We have found that lowering this threshold below choice of corpus, our method for quashing them retains its generality for use with any corpus.
 Table 1: Coefficients of correlation with human sim-ilarity judgments. Starred rows are presented in [4]. 0.07% dramatically increases runtimes while producing neg -ligible changes in our overall results. This makes intuitiv e sense, as we are effectively only eliminating from consider-ation those co-targets that account for fewer than 7 out of every 10,000 nouns co-occurring with a target.
Although our aim is not to develop a quantitative measure of semantic relatedness, an objective evaluation is in orde r. In the relatedness literature, a standard approach is to mea -sure correlation with mean similarity scores elicited from human subjects by Rubenstein and Goodenough [29] and Miller and Charles [21] (henceforth R&amp;G and M&amp;C, respec-tively). In these studies, participants rated the  X  X imilar ity of meaning X  of noun pairs on a scale of 0.0 ( X  X emantically unrelated X ) to 4.0 ( X  X ighly synonymous X ). In R&amp;G, partic-ipants evaluated 65 word pairs. M&amp;C then replicated the experiment using 30 pairs from the 65 used in R&amp;G.
Given that our S rel function is used only to rank co-targets by their relative relatedness to a particular target, for th is task we score relatedness between two words, a and b , as follows: where rank t ( c ) is the numerical rank of c among t  X  X  co-targets, as sorted by increasing value of relational strength to t , and | C t | is the number of t  X  X  co-targets. That is, the least related co-target of t has rank t ( c ) = 1, and the most strongly related has rank t ( c ) = | C t | .

If neither rank is defined, then score ( a, b ) = 0. If exactly one of these ranks is defined, we take 75% of the defined term, rather than allowing it to be averaged with zero.
In Table 1, we compare our correlation results with those presented in a review by Budanitsky and Hirst [4] as well as three state of the art studies published since then. Higher values indicate better correlation with the human-assigne d scores; 1.0 would indicate a perfect fit. The average correla -tion of ten individual human evaluations to the M&amp;C scores comes from a replication of the study by Resnik [27].
Our lexical co-occurrence method produces results that are competitive with methods that draw on rich semantic resources like WordNet and the underlying structure of Wi-kipedia, and is comfortably within the realm of human per-formance. We caution, however, that high correlation on this task, and particularly scores that exceed average hu-man correlation, might indicate that a measure is failing to capture semantic relatedness beyond that of similarity.
We now present an algorithm for categorically determin-ing semantic relatedness between nouns. We will write pairs of related nouns as, e.g., (astronomer, star), which indica tes the relatedness of  X  X stronomer X  to  X  X tar; X  the former is our target, and the latter is a co-target that we have found to be semantically related. The collection of all such word pairs constitutes a semantic network of related nouns.

Intuitively speaking, the idea behind our algorithm is this : if t is strongly related to c and, conversely, c is strongly re-lated to t , we include ( t, c ) in our semantic network. For this purpose we rely on our measure of relational strength: once we have sorted a list of co-targets by decreasing value of their relational strength to some target, we have an ex-ceptionally good idea of which nouns are strongly related to the target (those at the top of the list) and those which are strongly unrelated to the target (those at the bottom).
More formally, we introduce the notion of mutual relat-edness between nouns, defined as follows: if c is in the top x % of t  X  X  most strongly related co-targets (sorted by S and t is in the top x % of c  X  X  most strongly related co-targets, we say that t and c are mutually related within x %. The set of all nouns mutually related to t within x % is denoted m x ( t ).

To find the nouns categorically related to a target, t , we let x = 20 and find the initial set, m x ( t ). We then expand this set by incrementing x until 5 iterations pass without t being related to any additional co-targets (see Algorithm 1). Our experiments have shown that varying these parameters has negligible effects on the results of our algorithm, even if we allow the algorithm to proceed until as many as 10 iterations have passed without any new relations being discovered. Algorithm 1 FindRelatedNouns ( t ) Require: A target noun t .
 Ensure: Set of pairs ( t, c ) such that t and c are semantically 1: S 0  X  X  X  2: noGain  X  0 3: for n = 20 to 100 do 4: S  X  X  ( t, c ) | c  X  m x ( t ) } 5: if | S | &gt; | S 0 | then 6: noGain  X  0 7: else 8: noGain  X  noGain + 1 9: end if 10: if noGain  X  5 then 11: break 12: end if 13: S 0  X  S 14: end for 15: return S 0 Table 2: Summary of Statistics for the Semantic Network of Related Nouns
Target Nouns 7,593 (number of nouns occurring between 1,500 and 100,000 times in Wikipedia)
Nodes 25,142 (number of nouns represented in network; includes both targets and co-targets)
Edges 120,588 (number of related word pairs; ( a, b ) and ( b, a ) are not counted as distinct word pairs)
Average Degree of Target Nodes 30.74 (average number of nouns to which each target is related)
Average Threshold of Target Nouns 28.19% (average of target thresholds determined in Algorithm 1)
Upon termination of the algorithm, we admit all ordered pairs in S 0 to the network.

The algorithm exhibits several important properties worth mentioning. First, the algorithm accounts for the fact that some nouns are more promiscuous with their semantic re-latedness than others, and relates each target to as many or as few nouns as it deems fit rather than using a single, arbitrary threshold to restrict relatedness to all targets .
Secondly, the algorithm is resilient to the gradated nature of the relational strength of a target to its co-targets. Thi s gradation makes it impossible even for human judges to find a clear cutoff above which we can consider all nouns to be related to the target, and below which we can comfortably exclude their relatedness. However, our algorithm makes in -cisive decisions about relatedness without being lured dow n the slippery slope of over-inclusiveness.

A third notable feature of our algorithm is that it admits ( t, c ) only when the strength of t  X  X  relatedness to c is recipro-cated from c to t (as with  X  X enguin X  and  X  X ceberg X  which are strongly related in both directions; compare this with  X  X ce  X  and X  X enguin, X  X hich are far more strongly related in one di-rection (penguin to ice) than the other (ice to penguin) and are therefore excluded from relation in the network). This stringent requirement causes us to miss some related noun pairs, but provides very strong evidence for the relatednes s of pairs that do gain admission to the network.
We have constructed a semantic network of related nouns with this algorithm, using as our target nouns all those oc-curring between 1,500 and 100,000 times in Wikipedia. An overview of the resultant network is given above (Table 2).
For the 7,593 target nouns in our restricted range, our algorithm produces a semantic network relating 25,142 dis-tinct nouns (most of which appear as co-targets, but not targets themselves, because of their low frequency of occur -rence in the corpus), derived from 237,584 noun pairs. Of these noun pairs, 116,996 are redundant, in that they are the symmetric images of pairs already included in the net-work. Thus, the network has 120,588 distinct undirected edges. Each target noun is related, on average, to 30.74 other nouns.

To evaluate the precision of these relations, we asked three Table 3: Judges X  Evaluations of Accuracy on Related and Unrelated Noun Pairs judges with backgrounds in computational linguistics to ev al-uate 150 noun pairs and determine whether they would con-sider the nouns in those pairs to be semantically related or not. To prepare them for this task, we presented the judges with the following exemplars of semantic relatedness, whic h we hand picked from the network: (astronomer, observa-tory), (crime, prevention), (automobile, gasoline), (pho ne, signal), (penguin, tuxedo), (prison, lawyer), (tendon, ca rti-lage), (string, output), and (desert, habitat).

Of the 150 noun pairs presented to the judges for eval-uation, 100 were chosen at random from the related pairs in our network. Additionally, 50 pairs of unrelated nouns were generated at random from among the nouns currently represented in the network. The 150 pairs were presented in random order to the judges, none of whom had direct ties to this research. The results of their evaluations are summarized above in Table 3.

On average, the judges evaluated 95.66% of the pairs from our network to be semantically related. They also judged 80.66% of the unrelated pairs to be unrelated. (That is, they identified an average of 19.34% of the unrelated (randomly paired) nouns as being related.)
This domain is too open-ended for there to be any feasible measure of recall. However, the fact that our target nouns are related to an average of 30.74 nouns while maintaining precision in excess of 95% is indicative of broad and accurat e coverage of semantic relatedness.

To illustrate the quality of the relations discovered by our algorithm, we have included a discussion of the semantic network surrounding the monosemous nouns (concepts) as-tronomer and tennis in Section 7.
Once we have established relatedness between nouns, we automatically disambiguate them to their corresponding no un senses in WordNet 3.0. For this purpose, we use a complex suite of disambiguation methods that work in tandem to support or refute one another X  X  results.

Because each of these methods has certain weaknesses, a noun sense has to be verified by at least two of them in order to be admitted to the network when the methods produce conflicting results. Preference is given to results produce d by these methods in order of their presentation below. If all three methods described below fail to disambiguate a noun, we default to its most frequent sense in WordNet.
Our first disambiguation method capitalizes on the sense similarity clustering that we have found to occur among re-lated nouns. For example, concepts related to astronomer form one cluster beneath the umbrella of celestial body in WordNet (planet# { 1 , 3 } , star# { 1 , 3 } , minor planet#1, qua-sar#1), another under the purview of scientist (mathemati-
Accordingly, we determine the most frequently occurring immediate hypernyms for all the senses of the nouns re-lated to a given target, and allow them to disambiguate the concepts they subsume. Although accidental inclusion of fringe senses categorized by common hypernyms occurs in rare cases, this is the strongest of our methods for disam-biguation.
Our gloss method gathers all monosemous nouns related to a target, as well as the target itself, and searches for the se terms in the WordNet glosses of the target X  X  polysemous related nouns. Search terms may be pluralized, and suffixes from the set { -y, -er, -ist, -ing } may be replaced with any suffix from the set { -s, -es, -ies, -y, -er, -ist, -ing } , so that, e.g.,  X  X iologist X  can also be matched by the occurrence of  X  X iology, X  or  X  X ngineering X  by  X  X ngineers. X 
This method returns a list of all noun senses with at least one of the search terms occurring in their glosses. Even with target nouns that have a large number of related terms, this list is surprisingly concise, although the results are less reliable than those of the previous method.

However, these results do not require verification by an-other method if a search term matches a topic word in a sense gloss, as with  X  X stronomy X  in the gloss for star#1:  X (astronomy) a celestial body of hot gases that radiates en-ergy derived from thermonuclear reactions in the interior.  X 
Next we use Resnik X  X  selectional association measure [28] to build selectional preferences for the nouns related to a given target. Formally, we define the selectional associati on, A ( t, c ), of a target noun t with a WordNet class c as: As before, D KL is the Kullback-Leibler divergence between probability distributions P ( C | t ) and P ( C ): Here, however, C is no longer the class of nouns co-occurring with t . Rather, C is the set of concepts in WordNet denoted by the monosemous nouns that are related to t , along with all the concepts in their hypernymic traces (all hypernyms o f those concepts up to and including the root of the hierarchy, entity#1).

The posterior distribution, P ( C | t ), derives from the fre-quency of co-occurrence of t  X  X  monosemous related nouns. To compute the prior distribution, P ( C ), we use the fre-quency data for all monosemous nouns occurring between 1,500 and 100,000 times in Wikipedia. This is a depar-ture from the approach of Resnik, who includes polysemous nouns in both probability distributions and apportions cre dit for a noun evenly across all its senses. By focusing only on 7 We denote sense n of a noun by noun# n , or multiple senses with, e.g., noun# { m, n } . Table 4: Selectional Preferences Derived from Mo-nosemous Co-Targets of  X  X nicorn X  monosemous nouns in this approach, we eliminate the noise introduced by the ambiguity of polysemous nouns. Once we have the selectional preferences derived from our target X  X  monosemous nouns, we use them to preferentially disam-biguate our polysemous nouns.

Consider, for example, the categories in WordNet with the highest selectional association with the monosemous noun  X  X nicorn X  (Table 4). Among these selectional preferences we find mythical monster#1, imaginary being#1, and spir-itual being#1, which do not appear as co-targets of  X  X ni-corn, X  but do categorize many of the monosemous co-targets of  X  X nicorn, X  such as  X  X riffin, X   X  X oblin, X   X  X ermaid, X   X  X ep-rechaun, X  and  X  X inotaur, X  among others.

These selectional preferences are applied, in decreasing o r-der of selectional strength, to each sense of the target X  X  po ly-semous related nouns, which are disambiguated to the sense or senses categorized by the first such selectional preferen ce that subsumes them. Thus,  X  X hoenix X  (as it relates to  X  X ni-corn X ) is disambiguated to phoenix#3 in WordNet ( X  X  leg-endary Arabian bird said to periodically burn itself to deat h and emerge from the ashes as a new phoenix X ) by virtue of its subsumption by mythical being#1. The three senses of  X  X hoenix X  that are excluded here are phoenix#1 (the capi-tal city of Arizona), phoenix#2 (the taxonomic group genus Phoenix ), and phoenix#4 (a constellation). These selec-tional preferences similarly succeed in disambiguating th e polysemous  X  X ion X  to lion#1 (a feline, as opposed to the celebrity, astrological categorization of a person, or sig n of the zodiac denoted by senses 2, 3, and 4 of  X  X ion, X  respec-tively),  X  X east X  to beast#1 (the animal, as opposed to a cruel person, which is sense 2 of  X  X east X ), and  X  X atyr X  to satyr#2 (the mythical woodland deity, as opposed to sense 1 of  X  X atyr, X  which refers to a lecherous man).

If an upper-level ontological concept like physical entity#1 or abstract entity#1 performs the disambiguation in this method, we automatically dismiss the result as being too general to be reliable. More specifically, if c 1 is the strongest selectional preference from our list that disambiguates so me Table 5: Summary of Statistics for the Semantic Network of Related Concepts polysemous noun related to t , and A ( t, c 1 ) is less than the average value of A ( t, c ) for all c  X  C , then we discard the result and this method fails to disambiguate the polysemous noun in question.

This method sometimes assigns disproportionately strong selective power to hypernyms that are particularly rare in the prior distribution. As such, this method defers to the subsumption and gloss methods when its results conflict with theirs.
We have used these methods to disambiguate the poly-semous nouns related to monosemous targets occurring at least 1,500 times in the corpus. There are 3,024 such tar-get nouns, heading up 76,264 of our related noun pairs from the previous section. 36,385 of these pairs associate two monosemous nouns. The remaining 39,879 connect our mo-nosemous targets to polysemous nouns that must be disam-biguated. Statistics for the resulting semantic network of related concepts are given above in Table 5.

To test our precision at disambiguation, we randomly se-lected 50 pairs from among those used to build this net-work and presented them to our three judges with the gloss and taxonomic categorization of each sense of the polyse-mous nouns. The judges were asked to grade the relation of each sense to its monosemous target, using the following scale: (4) Primary intended sense or one of its synonyms. (3) Strongly related sense, but not the primary intended mean-ing. (2) Weakly related sense; could reasonably be included or excluded from relation to the target. (1) Unrelated sense .
We then measured how often the senses chosen by our dis-ambiguation algorithm fell into each of these categories, a nd compared our results to the standard baseline of randomly selecting noun senses (see Table 6, below).

The first column ( grade  X  4) indicates how frequently our system disambiguated to senses the judges considered to be the primary intended meanings of the related nouns. The last column ( grade = 1) indicates how often our system selected senses that were unacceptable to the judges. The next-to-last column ( grade  X  2) indicates how frequently our system chose senses that were acceptable to our judges.
Given that 47.7% of the edges in our network connect two tennis , explicated in Section 7.
 Table 6: Precision of Our System X  X  Disambiguation Results, as Compared with Judges X  Manually Dis-ambiguated Senses monosemous nouns (where there is no room for disambigua-tion error) and the remaining 52.3% have an average rate of acceptability of 85% as evaluated by our judges, we esti-mate the accuracy of the concept-to-concept associations i n our semantic network to be 92.15%.
The graphs in Figure 1 are abbreviated excerpts from the semantic network of related concepts for the monosemous nouns  X  X stronomer X  and  X  X ennis. X  Astronomer is related to 44 distinct concepts in our semantic network (listed in full in Table 7, below), and tennis is related to 80. For the sake of clarity, we present only a small sampling of those related nouns graphically. Furthermore, to avoid messy edg e crossings in the graphs, we do not show the inter-relatednes s between the concepts related to each of our targets. (For example, astronomy#1 and astrologer#1 are both related to astrology#1, but we instantiate the latter node twice in the graph to preserve clarity.)
The target concepts X  nodes in the graph are dark gray (astronomer#1 and tennis#1). We provide a sampling of their related terms in medium gray. In turn, those concepts are related to concepts in light gray, and those terms are related to concepts in white. This gives an idea of spreading activation through the semantic network.

In all cases, solid edges indicate that the target is re-lated to the smaller node incident to that edge. For ex-ample, the solid edge from star# { 1,3 } to sky#1 indicates that astronomer#1 is related to sky#1, too. The dotted edge from astrology#1 to horoscope# { 1,2 } indicates that astronomer#1 is not related to horoscope# { 1,2 } .
Some nouns are not yet disambiguated because they are related to concepts denoted by polysemous nouns. We have included a sampling of these nouns to give a fair indica-tion of the current state of the network. We also see how these might be easily disambiguated. Notice, for example, that tennis#1 is related to softball#2 (the game of softball, as opposed to the ball itself), which is in turn related to some (as yet undetermined) sense of  X  X olleyball. X  Because tennis#1 is related to volleyball#1 (again, the game as op-posed to the ball), this can be propagated through the net-work to disambiguate the relation between softball#2 and  X  X olleyball X  as (softball#2, volleyball#1).

There are also cases in which polysemous nouns are re-lated to disambiguated concepts in the graph, such as with the relation of star# { 1,3 } to solar system#1.  X  X olar sys-tem X  is monosemous in WordNet, and our disambiguation algorithm found it to be semantically related to star# { 1,3 } .
We note that while our algorithm discovers some relations of semantic similarity (e.g., the relation of astronomer#1 to mathematician#1 and astrophysicist#1), it also discov-ers many relations beyond similarity, including concepts r e-minor_planet#1 astrophysicist#1 physicist#1 geographer#1 biologist#1 chemist#1 theologian#1 black_hole#1 star#{1,3} astronomy#1 astrology#1 discovery#1 quasar#1 mathematician#1 moon#6 telescope#1 observatory#1 geologist#1 cartographer#1 philosopher#1 galaxy#3 comet#1 orbit#{1,4} redshift#1 planet#{1,3} cosmologist#1 amateur#2 sky#1 supernova#1 cosmology#2 discoverer#1 nebula#3 eclipse#1 constellation#2 observation#1 treatise#1 astrologer#1 solar_system#1 dwarf#2 asteroid#1 meteorologist#1 lated through collocation (as with amateur#2, which, inci-dentally, is incorrectly disambiguated) and more general s e-mantic relatedness (telescope#1, star# { 1,3 } , planet# { 1,3 } , galaxy#3, observatory#1, redshift#1, etc.).

Equally important is the absence of relations to semanti-cally similar concepts to which the targets are not strongly semantically related. Consider, for example, the fact that astronomer#1 is related to some hyponyms of scientist#1 (physicist#1, mathematician#1, chemist#1), but not oth-ers (linguist#1, psychologist#1, medical scientist#1, etc.), despite the fact that quantitative relatedness measures ba sed on the WordNet ontology would erroneously associate as-tronomer#1 to all these terms with nearly equal strength.
The network also associates astronomer#1 with astrolo-ger#1, which is clearly related, but is surprisingly far re-moved from astronomer#1 in WordNet. (Their first shared hypernym in the ontology is person#1.)
Finally, notice the relation of astronomer#1 to astrophysi -cist#1 and mathematician#1, but neither astrophysics#1 nor mathematics#1, although it is transitively related to t he latter concepts by way of the former, as well as by way of as-tronomy#1. Similarly, mechanisms of spreading activation transitively relate astronomer#1 to additional concepts l ike light year#1 by way of star# { 1,3 } , radio astronomy#1 by way of astronomy#1, and so on. This is arguably quite on-tologically sound. The astronomer himself is more strongly related to the astrophysicist and the celestial body senses of  X  X tar X  than to the light year or the study of astrophysics , although he is indirectly related to the latter concepts.
We have automatically acquired a semantic network of related concepts. The network is derived from relatedness between nouns co-occurring in Wikipedia texts, which are automatically disambiguated to their corresponding Word-Net 3.0 noun senses (i.e., concepts). At present, monose-mous noun targets form the basis of the network, each be-ing related to an average of 27.81 concepts (denoted both by monosemous and polysemous nouns). The network cur-rently relates 17,543 nouns, with 27,312 distinct noun sens es among them, and is available for download on-line.
There are several potential applications for this resource , including semantic interpretation, exploration of spread ing activation mechanisms [5], contextual frameworks for com-puter vision (cf. Torralba et al. [32]), noun sense dis-ambiguation, question answering systems, query predictio n, and user profiling for providing recommendations in multi-media content delivery systems.

In future work, we expect to continue expanding and re-fining the semantic network. Polysemous targets and tar-gets that occur fewer than 1,500 times in Wikipedia need to be incorporated into both the network of nouns and the network of related concepts. We are investigating the feasi -bility of applying our algorithm to these targets and using the existing semantic network to guide (i.e., bootstrap) th e process, which is more error prone with nouns that occur infrequently in the corpus and does not currently resolve ambiguity of polysemous-to-polysemous noun relations.
This research was supported in part by the NASA Engi-neering and Safety Center under Grant/Cooperative Agree-ment NNX08AJ98A. [1] E. Agirre and O. L. de Lacalle. Publicly available [2] M. Berland and E. Charniak. Finding parts in very [3] E. Brill. Transformation-based error-driven learning [4] A. Budanitsky and G. Hirst. Evaluating [5] A. M. Collins and E. F. Loftus. A spreading-activation [6] M. Cuadros and G. Rigau. KnowNet: building a large [7] D. Davidov and A. Rappoport. Efficient unsupervised [8] C. Fellbaum, editor. WordNet: An Electronic Lexical [9] E. Gabrilovich and S. Markovitch. Computing [10] R. Girju, A. Badulescu, and D. Moldovan. Automatic [11] J. Gorman and J. R. Curran. Scaling distributional [12] Z. S. Harris. Distributional structure. In J. J. Katz, [13] M. A. Hearst. Automatic acquisition of hyponyms [14] G. Hirst and D. St-Onge. Lexical chains as [15] T. Hughes and D. Ramage. Lexical semantic [16] J. J. Jiang and D. W. Conrath. Semantic similarity [17] C. Leacock and M. Chodorow. Combining local [18] D. Lin. An information-theoretic definition of [19] H. Liu and P. Singh. ConceptNet  X  a practical [20] R. Mihalcea and A. Csomai. Wikify!: linking [21] G. A. Miller and W. G. Charles. Contextual correlates [22] R. Navigli. Semi-automatic extension of large-scale [23] P. Pantel and M. Pennacchiotti. Espresso: Leveraging [24] S. Patwardhan and T. Pedersen. Using WordNet-based [25] S. P. Ponzetto and R. Navigli. Large-scale taxonomy [26] M. R. Quillian. Semantic Memory . In M. Minsky [27] P. Resnik. Using information content to evaluate [28] P. Resnik. Semantic similarity in a taxonomy: An [29] H. Rubenstein and J. B. Goodenough. Contextual [30] M. Strube and S. P. Ponzetto. Wikirelate! Computing [31] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago : a [32] A. Torralba, K. P. Murphy, W. T. Freeman, and [33] P. D. Turney. Expressing implicit semantic relations [34] P. D. Turney. Similarity of semantic relations. [35] H. Zaragoza, H. Rode, P. Mika, J. Atserias,
