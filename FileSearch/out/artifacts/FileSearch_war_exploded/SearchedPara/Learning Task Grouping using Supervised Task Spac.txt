 Lifelong multitask learning is a multitask learning frame-work in which a learning agent faces the tasks that need to be learnt in an online manner. Lifelong multitask learning framework may be applied to a variety of applications such as image annotation, robotics, automated machines etc, and hence, may prove to be a highly promising direction for fur-ther investigation. However, the lifelong learning framework comes with its own baggage of challenges. The biggest chal-lenge is the fact that the characteristics of the future tasks which might be encountered by the learning agents are en-tirely unknown. If all the tasks are assumed to be related, there may be a risk of training from unrelated task resulting in negative transfer of information. To overcome this prob-lem, both batch and online multitask learning algorithms learn task relationships. However, due to the unknown na-ture of the future tasks, learning the task relationships is also difficult in lifelong multitask learning. In this paper, we propose learning functions to model the task relation-ships as it is computationally cheaper in an online setting. More specifically, we learn partition functions in the task space to divide the tasks into cluster. Our major contri-bution is to present a global formulation to learn both the task partitions and the parameters. We provide a supervised learning framework to estimate both the partition function and the model. The current method has been implemented and compared against other leading lifelong learning algo-rithms using several real world datasets, and we show that the current method has a superior performance.
 Lifelong Multitask Learning, Online Learning, Feature Se-lection, Partition Models
Lifelong multitask learning is a multitask learning frame-work where a learning agent transfers the knowledge from the already learnt tasks to the new tasks which it encounters c  X  for learning during its lifetime. Lifelong multitask learning can be viewed as a subset of online multitask learning prob-lem where the framework is online in terms of the tasks. The inspiration for lifelong multitask learning comes from the natural process used by human beings for learning [23], as they have a natural tendency to transfer the knowledge from previously learnt tasks to new tasks. The lifelong mul-titask learning setting is of great utility for many real world problems where one needs to make predictions related to new tasks regularly. Examples of such applications may arise in online image annotation where new images are con-stantly being added by the users and new objects that need to be identified are repeatedly added to the list, or in the au-tonomous robots sent on exploratory mission in outer space or under water to explore and identify a new surrounding. Lifelong multitask learning has been primarily studied by Eaton et al [7], where the authors develop an algorithm which can handle new tasks as they are faced by the learning agent during its lifetime of training. The authors assume that all the tasks are related in a sub-dimensional space which can be represented by a set of basis vectors. The au-thors aim to learn these basis vectors and the coefficients of these basis vectors for each task. This work has been ex-tended by the authors in multiple ways as in [1, 15, 16, 17]. These methods only suffer from one inefficiency, and that is entire basis vector needs to updated when a new task ar-rives. Also, this method cannot do feature selection while learning the tasks.
 In this paper, we focus on learning the structure of the tasks in lifelong multitask learning along with feature se-lection. Learning the task structure is especially hard in lifelong learning setting because there is no previous knowl-edge of the nature of new tasks that may be arriving. Thus, the task learning mechanism needs to be dynamic and able to evolve with changing nature of the task sets. For this purpose, we propose learning partition functions rather than task structure matrix for learning the task relationships. In this paper, we learn the task clusters by imposing a series of partitions on the task space in a supervised manner. We also assume that the similar tasks depend on the similar features. Therefore, we incorporate feature selection in our model as well.
 Our main contribution in this paper is to present a global formulation for learning both partition models in task space and the task models using novel supervised learning formu-lation. This formulation partitions the task space such that the similar tasks remain in the same region using supervised learning and enforces similar tasks to depend on similar fea-tures. Learning both the task parameters and relationships is done in a supervised manner. We present the solution of this formulation using dual averaging technique for regu-larized stochastic gradient methods to solve for the sparsity constraints.
This paper is essentially a framework for multitask learn-ing in a lifelong learning framework. The core idea of the present approach is to learn both the task relationships and task models using supervised learning in a lifelong learning framework.
 The literature related to lifelong learning is very limited. The idea of lifelong learning has been known since 1996 when Thrun [23, 24] introduced the concept first in the machine learning community. Since then multiple authors have con-tributed to lifelong multitask learning in supervised setting [20, 21, 22]. These papers are about methods to incorpo-rate previously learnt neural network so that the knowledge learnt may be used for future tasks and do not deal with identifying the task relationships. Recently, Eaton et al [7] published a study on lifelong multitask learning which as-sumes that the tasks are related in a sub dimensional latent space. This study is based on [13]. The authors learn the basis of the latent space in which the tasks are related along with the coefficients of the basis for each tasks. Since the authors assume sparse coefficients, an implicit task relation-ship is assumed in their algorithm, but the explicit task re-lationships may not be identified using this algorithm. Even though learning task relationships is a new concept in life-long learning, it has been much explored in multitask learn-ing algorithms.
 In the traditional multitask learning algorithms, where all the tasks were initially assumed to be related to each other [2, 3, 4, 8], researchers observed that training unrelated tasks together may lead to negative transfer of information, which results in degradation of generalization error [12, 13]. To avoid this problem, researchers investigated learning task relationships such that the information is only transferred among the related tasks. The most common approach for solving this problem is to learn a positive semi-definite ma-trix to represent the task relationships [28, 9]. This positive semidefinite matrix is often the task covariance matrix or task kernel matrix [28], or Laplacian matrix or indicator ma-trix mapping each task to other related tasks [9]. Although task relationships are learnt using these fixed sized matri-ces, there are inherent problems in extending this method to lifelong learning. When a new task arrives, the knowl-edge learnt regarding existing task relationships is not di-rectly transferrable to the new task. Also, each time a new task arrives, a new row and column needs to be added to task relationship matrix which is computationally expen-sive. Another method for learning task relationships is to use k nearest neighbors to identify the task clusters [11, 30, 29]. These methods use an unsupervised step to learn the task relationships. Some authors assume that the tasks are related in sub dimensional latent space, and learn the re-lationships in that latent space [13, 14, 10]. This method does not learn the explicit task relationships in the physical space. Kang et al learn an indicator matrix whose element in i th row and j th column takes the value 1 if the task in row i belongs to the group in column j otherwise 0. This algorithm does an explicit task relationship modeling, and does feature selection as well while learning the task param-eters and thus bears closest resemblance to our algorithm. However, the information learnt regarding task relationships is non-transferable to the new tasks. All of the above al-gorithms learn the task relationship using an unsupervised step.
 In this paper, we propose using supervised learning to learn explicit task relationship modeling. More specifically, we learn a global formulation for learning partition functions in a supervised fashion to group the tasks together and then learn the task models. Learning functions in the task space which divide the tasks into clusters are easier to learn in terms of optimization, and they provide an easy transfer of information from previously learnt task to the new task. Us-ing a global formulation to learn the task clusters and task model ensures maximum coupling between the two learning assignments ensuring higher accuracy.
In this paper, we learn a model for lifelong multitask learn-ing scenario which learns the cluster of tasks as well as does feature selection. Before we give a description of our model, we would first like to describe the notation used, followed by a brief overview of our work. Proceeding which, we provide the details of our algorithm.
Throughout this text, we represent the matrices by cap-ital letter, the vectors by bold faced lowercase letters and scalar values by lowercase letters. Given a matrix X , its j column is represented by x  X  ,j . For the same matrix X , the element at the i th row and j th column is x i,j .
 The dataset being provided is represented by D , which con-sists of the input features x i,t and the output label y where i ranges from 1 ...N t and t ranges from 1 ...T . N resents the number of samples in task t and T represents the total number of tasks. In a lifelong multitask setting, the values of N t and T are undetermined.
 The given dataset D consists of 2-tuple elements, where each entry x i,t belongs to a d dimensional real valued vec-tor, x i,t  X  R d . The values of outputs depend on the tasks. For regression problems, the values of y i,t is a real num-ber, y i,t  X  R . For classification problems, y i,t is either 1 or 0, y i,t  X  { 0 , 1 } . We assume linear models for prediction throughout. Thus, the objective of the algorithm would be to learn the vector w t , where  X  y i,t = x i,t w t for linear re-gression and  X  y i,t =  X  ( x i,t w t ) for classification problem.  X  represents any monotonic function.
 Given the above framework, the average empirical risk for all the tasks in this case may be given as
Here, l is the loss function used. Usually, l belongs to the sigmoid family of functions for classification problems and squared loss function for regression problems. If we minimize the above equation to solve for w t , then the tasks are not regularized and the objective is same as learning each task separately. Therefore, for multitask learning problems, a regularization term is added to the empirical risk to ensure that the tasks lie close together.
 where  X  is a regularization constant, and F is a positive monotonic function of W . The common choices of F are L 1 norm, L 2 norm and trace norm.
In most multitask learning formulations, all the tasks are regularized together under the assumption that they are re-lated similarly. There are multiple applications where all the tasks are not related to each other and it is beneficial to identify the related tasks. The existing multitask learning approaches alternate between unsupervised step and super-vised step to learn the task relationships and task models. We introduce a new method to learn both the task relation-ships and task models in a supervised manner which may also be used in lifelong multitask setting.
 In this paper, we propose learning partition functions to divide the task space into regions. As a result, the tasks in the same region are effectively clustered together. There are multiple benefits of using partition functions instead of indicator matrices to learn task relationships. Functions are easier to update in an online setting as opposed to indica-tor matrices. Therefore, learning functions results in more adaptive algorithm to lifelong learning setting. Second ben-efit of using function is that it is easier to transfer knowl-edge of previously learnt tasks to the new incoming tasks. The learnt function can easily provide rules for clustering the new incoming task. Third benefit lies in the fact that learning a discrete valued binary matrix using optimization techniques is much harder than learning a real valued con-tinuous function. Therefore, we present this novel idea of learning partition functions in a supervised manner to clus-ter the task.
 Let us first describe our algorithm for including a single partition function, and then we later extend the idea for creating multiple partition functions for creating r groups of tasks. Let us assume a function g ( X, y ) which divides the task space or the model space into two regions, region 1 and region 2. Also, we assume that the task parameters, w t can be given as either u 1 + v t or u 2 + v t depending on if the given task belongs to the region 1 or 2. Here, u 1 is the common model for all the tasks which lie in region 1, and u 2 is the model common for all the tasks in region 2. v t are task specific models.
 In this case, the empirical loss function can be written as R = min 1
Here, I is the indicator function and  X  t is a function that maps the input data X t to the output y t for a given task t , and thus, can be seen as the representative of the task t . Henceforth, we will represent g (  X  t ( X t , y t )) as simply g . The function l ( u 1 , v t , x i,t ,y i,t ) is the loss function and mea-sures the mean squared error between x i,t ( u 1 + v t ) and y For classification problems, the loss function may belong to either zero-one loss function or sigmoidal family of functions. Also, let us assume Thus, the empirical risk described in equation 3 is
Now, let us consider the case where there are more than one partitions of the task space. Since, each partition func-tion divides the task space into two regions, we adapt the framework of using a series of partition functions which re-cursively partition the task space hierarchically. In other words, we establish a tree structure in the task space with a partition function g k at each node k of the tree. The func-tion g k divides the region at that node of the task space into two more regions. There is no partition function at the leaf nodes. Thus, each leaf node is associated with a region of the task space which is not partitioned any further. For each leaf node r , there exists a common task specific model u which is common for all the tasks that belong in the region associated with leaf node r . As mentioned before, there also exists task specific model v t , thus, the task model is given by w t = u r + v t , assuming task t belongs to region or leaf node r .
 Let the left child of k th node of the partition tree be k and right node be k R . For any node k, the value of empiri-cal loss for a task t would then be
This formulation of the loss function is recursive. Thus, the loss function of the entire tree may be given by the loss function of the root node. The loss function of the root node is the sum of the total loss of its left child and right child. The left child loss and right child loss may further be broken down in the same way till we reach the leaf node. Thus, the loss function of the root node is same as the loss functions of the sum of leaf nodes. Therefore, if the root node is 1, the cumulative empirical risk is Additionally, we also impose the models to be sparse. That is, the common model in region r, u r should only select some features relevant to all the tasks in the region. Simi-larly, the task specific models, v t are assumed to be sparse too. Sparsity assumption is especially useful for tasks with large dimensionality. The empirical loss function is regu-larized using L 1 norm on regional and task specific models. Thus, the empirical loss function may be written as assuming there are p l leaf nodes. We need to solve for the values of u r , v t and g k . Henceforth, we will refer to all g k as partition functions, u r as region specific models and v t as task specific model. We use alternative minimization to solve for each of the values. First, let us discuss how we can solve for a single partition function. Later, we will show how the same framework is extended for multiple partition functions.
To find the solution for g for a single partition case, let us assume that the values of u 1 , u 2 and v t is fixed. Then, the risk function is
The regularization terms need not be included as the val-ues of u 1 , u 2 and v t are fixed. Also, because u 1 , u are fixed, the values of l 1 t and l 2 t can be easily computed. Theorem 1. For a fixed values of vectors u 1 and u 2 solution for finding the optimal partition function g which minimizes Equation 8 is same as finding the optimal classi-fier which minimizes the following empirical loss
Proof. Let us define a set S consisting of all the indices a &gt; b , the result is 1 if a is greater than b and 0 otherwise. We can rewrite equation 8 as.
 Since, for all t  X  S , the value of l 1 t &gt; l 2 t is 0. Similar argu-ment holds for other terms. Rearranging terms, R = 1
Here, the last term is a constant and does not depend on function g . Minimization of R with respect to g will not be effected by the third term, and hence, can be removed. Thus, empirical risk may be given by
From theorem 1, it can be deduced that the optimal par-tition of the task space is obtained when each task is given the label 0, if model in partition 0 gives a lower error, and 1, if model in partition 1 gives a lower error. The empiri-cal risk is then the weighted error of classifying these tasks, with weights for each error given as | l 1 t  X  l 2 t | . The partition function in the task space, g , can be obtained by using any cost sensitive classifier. Here, we assume that the classifier g is a linear classifier. The value of  X  t used here is the sin-gle task learners obtained from solving logistic regression or linear regression problem. The details for  X  t are provided in section 3.2.3.
 To compute the weights of linear classifier g , we use the cost sensitive classifier developed by Zhang and Garcia [27]. For each incoming task t, the classifier g may be updated as Here, the value of label is the label given to task t according to previous paragraph. The value of  X  t is defined as Here,  X  is a constant determining the size of step, and c the weight associated with each task. Here, the value of c is | l 1 t  X  l 2 t | .
The above theorem can be easily extended to a multi-region partitioned task space as well. For a given node k in the partition tree and a given task t , the empirical loss and the global empirical risk is given in equations 5 and 7. In order to solve for each of the g k for fixed task and model specific models in equation 7, we use alternate minimiza-tion. Thus, for each task, we first fix all the u r and g k in all nodes of the partition tree. Then, we pick one par-tition function g k at a time and solve for that. Therefore, for each node k not in leaf node, the values of g k is given by minimizing Minimizing the above equation is same as minimizing g k = min  X  1 This is a direct extension of Theorem 1. It is also easy to infer from theorem 1 that the value of L k L t is the smallest value of l r kL t where r kL represents all the leaf nodes under the left child of node k . Similarly, L k R t is the minimum value of l r kR t . Since, all other partition functions and region models are fixed, the value of L k L t and L k R t may easily be computed. Thus, the value of each partition function may be computed using any online cost sensitive classifier. After updating the partition functions, we solve for each u r and v as described in the next section.
Given the value of the partition function, we know the region a given task belongs to. Let this region be depicted by r , and the indices of all tasks in this region belong to set S . The equation 7 then reduces to The major inefficiency in the above equation is that it de-pends on all the samples from previous data owing to the inner summation in the equation. In order to overcome this efficiency, we use the trick used in [7]. We expand the in-ner summation term, 1 N  X  the single task learner of task t . Using second order Taylor series expansion and ignoring all the constant terms, we get R = 1 where, and, k K k D is defined as K T DK .
 We can obtain the value of  X  t using linear or logistic re-gression. The value of  X  t may be easily updated online as well.
 In order to find v t and u r , we first fix u r and solve for v The solution for v t can be obtained by solving the following equation
This equation can be solved using any off the shelf algo-rithm for solving L 1 norm regularization.
 Then, we keep the values of v t fixed, and find the value of u r . Therefore, we minimize the following equation
The above equation depends on all the tasks in the given region. In this case, using simple stochastic gradient algo-rithm to find the values of u r in an online fashion may be used. However, it is very difficult to find the sparse solution in online framework because it is hard to find summation of different values which add to zero. For this purpose, we base our solution to find u r on Lin Xiao X  X  regularized dual averaging method [25]. The regularized dual averaging method updates the param-eters by the average sub gradient of the function. Here, the value of the sub gradient d t is where tr ( D t ) is the trace of matrix D t . Then, the average sub gradient is computed as
Thus, the value of u r is updated as u
The value of  X  RDA t =  X  2 +  X   X  t and  X  and  X  are regularizing parameters, u r,j is the j th element of vector u r . There is one more inefficiency that needs to be discussed. Since this algorithm is online algorithm, the number of par-titions or the size of the partition tree is not fixed. In-fact, we start out with the entire task space and partition the task space recursively as we encounter more and more tasks. There are two conditions that still need to be dis-cussed. When do we decide to split a given region into two parts, and when do we stop growing the partition tree. The answer to the second question is to choose a maximum depth of the tree and do not grow the partition tree beyond the maximum depth. To evaluate the criteria for deciding when to split a region, we use a common intuition. All the tasks in a given region r share the same model u r . Thus, the loss function of the models in the same region will be in similar range. If a new task comes in, and the resulting loss l r new task t is significantly different in value than losses of other tasks, then new task t probably belongs to a separate group. Hence, the region must be split.
 In order to detect the significant difference in values of losses, we use theory from change point detection. Change point detection estimation is a problem in machine learning which is associated with detecting change of events in a given sig-nal. More specifically, we implement Shewart Control Chart [18, 5] to decide if change point has occurred and given re-gion needs to be split. Here, we assume that the losses of the tasks in a given region are the samples belonging to gaus-sian distribution with mean m and variance  X  . The values of m and  X  can easily be estimated. Let us assume that the new task t belongs to a gaussian distribution with mean l and variance  X  . Then the value of decision function for Shewhart Control Chart is given by df = ( m  X  l r t Refer to [5] for derivation of decision function in Shewhart Control Chart for two gaussian functions. If df is greater than a constant h , then the region is split.
 The summary of the entire algorithm is described in Al-gorithm 1.
 Algorithm 1 Supervised Clustering in Lifelong Multitask Learning Require: Data { X t , y t ,  X  1 ,  X  2 , Maximum depth of parti-tion tree p ,  X 
Initialize values of u , v and g as 0 for all incoming task t do end for
In this paper, we developed a lifelong multitask learning algorithm which learns groups of tasks and selects a common set of features for each group of tasks. The description of the algorithm has been provided in the previous section. Now, we demonstrate the performance of our algorithm with re-spect to other state of the art similar algorithms. We name our algorithm SUPART (Supervised partitioning of Tasks) and show that SUPART performs better than or equivalent to other methods. The list of the different methods to which we compare our algorithm is provided below: BatchMTL : This algorithm is a multitask learning algo-rithm developed by Kang et al [12]. Kang et al also devel-oped an algorithm for clustering the tasks in multitask learn-ing framework and using feature selection to enforce that similar tasks depend on the similar features. BatchMTL is the closest multitask learning algorithm to our frame-work, except that BatchMTL is implemented in batch set-ting where entire data is available. Therefore, we use Kang et al X  X  method to compare the performance of our algorithm. ELLA : Efficient Lifelong Learning Algorithm, or ELLA, is a lifelong multitask learning algorithm [7]. In this algorithm, the authors assume that all the tasks are related in a sub dimensional space, and learn a set of basis functions rep-resenting the sub dimensional space in which the tasks are related. For each task, the coefficients for the basis func-tion are also learnt. Since the coefficients are assumed to be sparse, sometimes an overlapping clustering effect on the tasks is also observed. We pick this algorithm for compari-son because it is based in lifelong setting. We would like to thank the authors for providing us their code.
 TREE : This algorithm is a baseline method. Here, we build the clustering structure of the tasks using hierarchical clus-tering and enforce that the tasks at each node remain close to each other by using a gaussian prior around the mean of the tasks belonging to that node. Algorithm 2 Update the partition functions Require:  X  t , t , h , p if t is the first task then else end if LANDMINE 29 9 14,820 STOCK Market 25 16 24,452 STL : The single task learning algorithm which we use to compare our algorithm with bayesian online regression and classification methods as described in [6].
 SUPART : This is the algorithm described in current paper. In this paper, we propose a global formulation for simulta-neously partitioning and learning the tasks. In this way, both the task partitions and task parameters are learnt in a supervised setting.
In order to evaluate the performance of our dataset, we use several real world datasets. The description of the dataset is provided below.
 MNIST Dataset : The MNIST dataset is a collection of handwritten digits. We have downloaded this dataset from Kang X  X  webpage [12]. This dataset is basically a subset of the original MNIST dataset which has already been prepro-cessed. In this dataset, principal component analysis was used to extract the top 64 features of each image. The data has been provided for only 2,000 image, and each image needs to be classified into one of the digits between 0 and 9. We binarize the tasks, and thus, each task description includes identification of a single digit out of the ten digits. Thus, there are 10 tasks, and all of the 2,000 images serve as the samples for each task. Therefore, there are effectively Table 3: The performance comparison of SUPART with Batch Method. The mean accuracies are reported here. The performance of batch algorithms are often seen as the max-imum performance which an online algorithm may reach. Here, SUPART has a performance almost equivalent to the Batch Multitask Learning.
 20,000 samples belonging to the total of 10 tasks. The di-mensionality of each of the samples is 64.
 USPS Dataset : This dataset is also downloaded for Kang X  X  webpage [12]. It is also a handwritten digit dataset which has been preprocessed using a similar procedure as the MNIST dataset. In USPS dataset, top 87 features were used. The task was again to identify the digit between 0 and 9 in the image. Again, we binarized the task, and thus, each task was to identify if an image contains a particular digit or not. The number of samples were 2,000 for each of the 10 task, making the effective number of samples to be 20,000, and the number of features in each sample was 87.
 Stock Market Dataset : Stock market dataset is a com-pilation of the stock market prices which we compiled from Yahoo X  X  financial services website. We selected 25 differ-ent companies from various market domains such as oil and gas industries, computers and electronics industries, phar-maceutical industries and finance. The weekly stock market prices from the period of December 1994 to December 2014 were obtained. Each task consisted of predicting the cur-rent stock market prices of each of the company based on the stock market prices of past sixteen weeks. There were a total of 25 tasks and 24,452 samples, and each sample hav-ing 16 features.
 Landmine Detection Dataset : This dataset is about pre-dicting the presence of a mine based on the radar images of the location. Originally, this dataset is from [26], how-ever, we obtain the data from [7]. The dataset was available with the software for ELLA. The radar images were prepro-cessed and 9 features were extracted from the images, out of which 4 features were moment-based, 3 correlation-based, one energy-ratio feature and one spatial variance feature. The problem formulated as binary classification having 29 tasks, nine features and 14,820 samples.
 Table 1 presents a summary of all the datasets.
All the models selected consisted of some hyper-parameters which need to be fixed by the user. The hyper-parameters were chosen using five fold crossvalidation on around fifty percent of the dataset. We also use a greedy search proce-dure if the algorithm has multiple hyper-parameters which need to be selected. Thus, we first fix all the hyper param-eters to a constant value, and vary one at a time. We pick the value of the hyper parameter which minimizes the cross is not statistically significant.
 validation error. The values of the hyper parameters thus selected are kept constant across all the experiments. For Kang et al X  X  method, the hyper parameters which needed to be selected were the number of groups to use, the regular-ization coefficient for the task parameters and task correla-tion matrix. The value of the number of groups were chosen from a pool of { 2 , 3 , 4 , 5 } and the values of regularization parameters were picked from { 10  X  3 , 10  X  2 , 10  X  1 , 1 , 10 , 100 } . In ELLA, there were five different parameters which needed to be selected, namely, the number of basis selected, the reg-ularization parameter for the basis,  X  1 , regularization pa-rameter for sparsity constraint of the basis coefficients in each task,  X  , the ridge term for single task learner,  X  2 the L 2 regularization component for the single task specific components,  X  2 . The values of the number of basis func-tion were chosen from a pool of { 2 , 4 , 6 , 8 , 10 } . The values of  X  , X  and  X  2 were chosen from the pool of { exp(-12), exp(-8), exp(-4), exp(0), exp(4) } . The value of  X  2 was also selected from the same pool except the value of infinity was added to the pool according the author X  X  guidelines.
 In the TREE algorithm, the online bayesian methods for classification and regression were used as single task learn-ers. The single task parameters were clustered using online top-down hierarchical clustering, and the mean of the tasks at each node was applied as the prior of each task. The same online single task learners were used for predicting on-line STL algorithm. There were two parameters used for online single task learners, the variance of the noise in the data,  X  , and the variance of the zero mean gaussian prior,  X  . The values of  X  and  X  were chosen from the pool of { 0.001, 0.01, 0.1, 0, 1, 10, 100 } . The gaussian prior was again used on each node of the hierarchical structure to regularize the related tasks together. The mean of all the tasks belonging to the node was used as the mean of the prior. The variance of the prior is chosen as the variance of the prior of its child node times the decay factor. The decay factor was picked from the pool of { 1, 2, 5, 10, 20 } .
 Finally, in the current algorithm, SUPART, there were five parameters which needed to be selected, namely, the max-imum depth of the tree, p , regularization constant of task specific parameters,  X  1 , regularization constant for regional classifier,  X  2 , the sparsity constraint,  X  and parameter dic-tating the step size,  X  . The number of reject classifiers was picked from { 2, 3, 4, 5 } , and the values of  X  1 ,  X  and  X  were picked from { 0.001, 0.01, 0.1, 1, 0, 10, 100 } , and  X  { 10  X  6 , 10  X  5 ,10  X  4 ,10  X  3 } . For deciding when to split the nodes, we need to choose the value of constant h . The value of h was picked from a pool of { 0 . 05 , 0 . 1 , 0 . 15 , 0 . 2 } .
For all the algorithms tested, datasets were randomly di-vided into training and testing. All the algorithms were trained on the training dataset and evaluated on the test-ing dataset. For the MNIST and USPS dataset, 1000 sam-ples from each tasks were picked for training and remaining for testing. Similarly, for Landmine Dataset, 300 samples and for Stock Market Dataset, 100 samples from each task were kept for training. The splits were consistent across all the models. The hyper parameters were fixed to the values that minimized the cross-validation error as found previ-ously. The division of the dataset into training and testing was repeated 100 times. The samples from each task were fed into the algorithm one at a time and their performance on the test samples was measured. The order of the tasks were randomized between each run. The values of the hyper parameters were used based on cross-validation for all the algorithms and the task specific, region specific and the par-tition parameters were initialized with values of zeros. The test samples were only used for testing the model and did not contribute to the training. The results reported are on test dataset.
 The results recorded are the average accuracy obtained from each randomization. The accuracy for the regression task is defined as 1  X  nmse , where nmse is the normalized mean squared error. For the classification tasks, we define the ac-curacy as the ratio of the correctly classified samples to the total number of samples. The results are reported in Table 2. As can be seen in the Table 2, the performance of SUPART is superior to all other algorithms, except for stock market dataset, where there is not significant difference in the per-formance of SUPART versus others.
 We also compare SUPART with a batch multitask learn-ing algorithm. We use Kang X  X  method [12], Batch MTL, as the batch multitask method as this algorithm groups the tasks explicitly into groups and does feature selection as well. The comparison of SUPART with Batch MTL is provided in Table 3. Usually, online algorithms do not have as good performance as the batch algorithms and batch algorithms are often viewed as the maximum performance that may be achieved by any online algorithm. However, our algorithm performs almost as good as the Batch MTL. In Landmine and Stock Market data, it performs even better. The reason may be that both Landmine and Stock Market data have few number of features, and hence, do not benefit by the feature selection. Figure 1: The average time taken in seconds to train plus test each algorithm. The vertical axis is the time in seconds on logarithmic scale and the horizontal axis represents the dataset. SUPART performs orders of magnitude faster than the batch MTL and significantly faster than other online algorithms.
The time taken for training and testing each of the model was also recorded. We present the average time taken for each of the 100 randomizations along with the standard de-viation. All the experiments were conducted on Intel(R) Xeon(TM) 3.2 GHz Linux Machine. The time is measured in seconds. The results are reported in Figure 1. Thus, it can be observed that SUPART has significantly faster per-formance than other methods and even faster than ELLA. The major reason for our model to be faster than ELLA is that in ELLA, all the basis vectors needs to be updated each time a new task arrives. In our model, we only update the vectors associated with the region the task belongs to.
In the current paper, the major contribution is to develop the framework to learn the task relationship modeling in a lifelong learning setting. In our framework, we partitioned the task space using supervised learning. We could have also chosen to partition the task space using unsupervised learn-ing by learning linear functions such that tasks closest to each other are grouped together. Most common formulation would be to use of k-means to find the partition function. In order to show that supervised learning is better for parti-tioning task space, we use two methods to partition the task space in the current framework. The first method is using k-means algorithm to cluster the tasks into two groups at each node of the partition tree being developed in this al-gorithm and then defining the partition function as the line bisecting the line joining the mean of the two groups. We use the online k-mean algorithm presented by Shindler [19]. This algorithm is both fast as it requires only one pass of the dataset and has shown a good performance in unsuper-vised learning. The second method is to use our supervised learning approach to partition the task space as described in section 3.2.1. All other parameters were kept exactly the same in both the methods. The data was divided into training and testing and the performance was recorded 100 times. The same division of the data as used in previous sections were used here as well. The performance of using k-means versus the current algorithm is shown in table 4. The time comparison of the two methods is shown in table 5. As can be seen, the current method is faster and has sim-ilar performance than using k-means to partition the tasks. Table 4: The performance comparison of using supervised learning versus using k-means to learn partition function. Results reported are average accuracies.
 Table 5: The time comparison of using supervised learning versus using k-means to learn the partition functions. Re-sults reported are average time in seconds.
 LANDMINE 0.4766  X  0.0206 0.2768  X  0.0320 It is observed that the k-means algorithm takes significantly more time for datasets with larger number of tasks such as landmine and stock dataset. We would like to make a note here that both the algorithms tested here are two different forms of the algorithm presented in this paper. In one algo-rithm, we use supervised method to learn partition functions whereas in other, we use k-means to learn partition function.
The last experiment which we conducted was to measure the error as a function of task. For this experiment, we plot the average error on existing tasks over 100 runs, each time a new task is made available. The tasks were randomized in each of the 100 runs. The errors were plotted against the position of the task in the sequence in which it was en-countered. We also plot the best exponential fit curve. The figure is shown in Figure 2. As can be seen in the figure, the overall trend of the error is decreasing in all the tasks. This shows that negative transfer does not occur in general as new tasks arrive and the new tasks arriving improve the overall performance.
 Figure 2: The decrease in the overall error as a function of position in task sequence. The red line indicates best exponential fitting curve.
In this paper, we develop a novel algorithm for lifelong multitask learning in which we use partition functions to cluster the related tasks and learn the task parameters. We present a global formulation which partitions the task space and learns the task models using supervised learning. We also assume that the related tasks depend on similar set of features, and thus, regularize the similar tasks together us-ing L 1 norm to implement feature selection.
 The current algorithm, SUPART, is implemented and its performance is compared against leading lifelong learning algorithm. We observe that SUPART performs better than the leading algorithms in terms of both speed and accuracy. [1] H. B. Ammar, E. Eaton, P. Ruvolo, and M. Taylor. [2] C. Archembeau, S. Guo, and O. Zoeter. Sparse [3] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [4] B. Bakker and T. Heskes. Task clustering and gating [5] M. Basseville, I. V. Nikiforov, et al. Detection of [6] C. M. Bishop et al. Pattern recognition and machine [7] E. Eaton and P. L. Ruvolo. Ella: An efficient lifelong [8] T. Evgeniou and M. Pontil. Regularized multi X  X ask [9] H. Fei and J. Huan. Structured feature selection and [10] S. Gupta, D. Phung, and S. Venkatesh. Factorial [11] L. Jacob, F. Bach, and J.-P. Vert. Clustered [12] Z. Kang, K. Grauman, and F. Sha. Learning with [13] A. Kumar and H. Daume III. Learning task grouping [14] A. Passos, P. Rai, J. Wainer, and H. Daume III. [15] P. Ruvolo and E. Eaton. Active task selection for [16] P. Ruvolo and E. Eaton. Scalable lifelong learning [17] P. Ruvolo and E. Eaton. Online multi-task learning [18] W. A. Shewhart. Economic control of quality of [19] M. Shindler, A. Wong, and A. W. Meyerson. Fast and [20] D. L. Silver and R. Poirier. Sequential consolidation of [21] D. L. Silver, R. Poirier, and D. Currie. Inductive [22] R. J. Solomonoff. A system for incremental learning [23] S. Thrun. Learning to learn: Introduction. In In [24] S. Thrun. Lifelong learning algorithms. In Learning to [25] L. Xiao. Dual averaging method for regularized [26] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. [27] J. Zhang and J. Garc  X  X a. Online classifier adaptation [28] Y. Zhang and J. G. Schneider. Learning multiple tasks [29] W. Zhong and J. Kwok. Convex multitask learning [30] J. Zhou, J. Chen, and J. Ye. Clustered multi-task
