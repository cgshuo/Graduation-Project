 Many school districts have developed successful interven-tion programs to help students graduate high school on time. However, identifying and prioritizing students who need those interventions the most remains challenging. This paper de-scribes a machine learning framework to identify such stu-dents, discusses features that are useful for this task, applies several classification algorithms, and evaluates them using metrics important to school administrators. To help test this framework and make it practically useful, we partnered with two U.S. school districts with a combined enrollment of approximately 200,000 students. We together designed several evaluation metrics to assess the goodness of machine learning algorithms from an educator X  X  perspective. This pa-per focuses on students at risk of not finishing high school on time, but our framework lays a strong foundation for future work on other adverse academic outcomes.
 I.2.1 [ Artificial Intelligence ]: Applications and Expert Systems evaluation metrics; applications; education; risk prediction;  X 
Work done as part of the Eric &amp; Wendy Schmidt Data Sci-ence for Social Good Summer Fellowship at the University of Chicago [27].
 c  X 
One of the perennial challenges faced by school districts is to improve student graduation rates. Though the mag-nitude of this problem has reduced due to a steady rise in high school graduation rates over the past few years, nearly 730,000 students in the United States do not finish high school on time every year [28]. A myriad of reasons ranging from economic problems, lack of motivation, and unexpected life changes can delay students X  graduation or cause them to drop out [7, 13, 24]. Studies have shown that not gradu-ating high school on time impacts a student X  X  future career prospects immensely [1, 18]. In addition, students who do not graduate on time can strain school districts X  resources. To address this issue, school districts have been heavily in-vesting in the construction and deployment of intervention programs to better support at-risk students and their indi-vidual needs.

The success of these individualized intervention programs depends on schools X  ability to accurately identify and pri-oritize students who need help. Traditionally, schools re-lied on feedback from instructors and used heuristic rules based on metrics such as GPAs, absence rates, and tardi-ness to identify at-risk students [8]. Though human judg-ment and heuristics can often be accurate, they serve as rules of thumb, are static, expensive to maintain, and often error prone [25]. Further, the set of heuristics which might help in identifying at-risk students for a particular cohort of students within one school district might not generalize or transfer to other cohorts and schools.

As alternatives to manually created rule-based systems, recent research has indicated the potential value of machine learning approaches such as Logistic Regression, Decision Trees, and Random Forests [8, 26, 3]. Trained using tra-ditional academic data, these machine learning approaches can often identify at-risk students earlier and more accu-rately than prior rule-based approaches [3].

Nevertheless, the application of such methods to this par-ticular context is still in its early stages, even for schools with state-of-the-art technology and analytics teams. To build more robust and comprehensive early warning systems, we partnered with two large U.S. school districts with a com-bined enrollment of approximately of 200,000 students. Fol-lowing a number of discussions with the district officials who oversee the implementation of early warning systems, we de-veloped an outline of their expectations:
Our interactions with educators revealed that there were several deeper and interesting challenges in this setting, and helped us quickly understand that evaluating algorithms simply using AUC and precision/recall metrics would not be sufficient. In this work, our goal is to investigate how to evaluate the suitability of any given algorithm for the prob-lem at hand so as to ensure that it meets the expectations of educators and school officials. To this end, we apply sev-eral off-the-shelf machine learning algorithms to identify at-risk students and analyze their behavior according to several evaluation techniques. To summarize, our major contribu-tions are:
Our interdisciplinary work benefited from prior research at the intersection of educational research and data min-ing. Educational research provided a basis for selecting and understanding features indicative of adverse academic out-comes, and data mining research helped us use these features in conjunction with statistical techniques to develop robust early warning systems.

Educational research has found that some specific features such as students X  grades and attendance are particularly rel-evant to predicting on-time high school graduation [24, 8]. Bowers et al. [8] systematically reviewed this vast literature, finding that these commonly recorded student features can predict future student outcomes with high accuracy. Most of these studies predominantly advocated usage of rule based models. These studies widely varied in how they combined individual student features when developing such rule based models. For instance, one of the studies suggested that a student should be flagged as at-risk if he/she has both low grades AND low attendance rates, while another study sug-gested that a student should be flagged if he/she has either low grades OR low attendance rates. Moreover, many stud-ies focused on developing high precision rules, but at the cost of low recall. A major drawback of such rule-based models is that they are not generalizable in the sense that they might work well for a specific district and cohort, but result in poor performance when applied elsewhere.

Recent research in data mining addresses the limitations of such rule based models by advocating the usage of auto-mated learning methods. Several well-known machine learn-ing algorithms such as Random Forests, Logistic Regres-sion, Decision Trees etc. were used to predict student out-comes [26, 12, 3, 2, 11, 22, 31, 12]. These models consistently outperformed rule based models on traditional metrics such as precision, recall, and AUC. In addition, models such as Bayesian networks were employed to identify students who were likely to fail in mathematics courses [29]. Further, ma-chine learning models were also employed to predict trajec-tories of future learning performance using past history [15].
Though machine learning models are very useful in prac-tice, they are essentially black boxes from an educator X  X  per-spective. Furthermore, traditional model evaluation metrics such as AUC are both difficult to interpret for educators and are not well suited to address all the issues at hand [30]. We therefore worked closely with two school districts to identify how to best interpret and evaluate machine learning meth-ods so that they address the districts X  educational goals.
The work we describe in this paper is being done in col-laboration with two school districts in the United States. One of the districts is among the largest districts in the mid-Atlantic region with over 150,000 students enrolled across 40 schools (District A). The other is a medium-sized district on the east coast with an enrollment of approximately 30,000 students across 39 schools (District B). Both of these dis-tricts are instituting several measures to help students and had already recognized the importance of early warning indi-cator systems for identifying at-risk students. District A had a rule-based early warning system in place which employed several important indicators such as academic performance, behavior, mobility and few demographic attributes. Our partnership with these school districts has been critical in developing a machine learning system that is not only based on real data but also designed for the needs and priorities of educators.

We obtained data from each of these school districts. The dataset provided by District A comprises of two cohorts of 10884 and 10829 students, expected to graduate in 2012 and 2013 respectively. Most of the students in these cohorts were tracked from 6 th -12 th grade, while some arrived throughout the study. Students belonging to the latter group have miss-ing data fields for all the years prior to their enrollment in the school district since the school only starts collecting data when students enroll. The data contains several attributes for each of these students such as their GPAs, absence rates, tardiness, gender etc. About 90% of the students in each of these cohorts graduated high school within 4 years of enroll-ment.

The dataset obtained from District B comprises of two cohorts of 1499 and 1575 students, with expected graduation dates in 2012 and 2013 respectively. In this dataset, most of the students were tracked from 8 th -12 th grade and several academic and behavioral attributes of these students were recorded. However, some arrived throughout the study and subsequently have missing data fields for years prior to their enrollment. About 95% of the students in each of these cohorts completed high school on time.

While there could be a variety of reasons for academic difficulties ranging from lack of motivation to economic con-cerns, recent research has demonstrated that these diverse causes often manifest themselves through a common set of indicators such as academic performance, behavior, and at-tendance [4, 6]. The data used for this analysis captured most of these indicators. Table 1 provides an exhaustive list of all attributes that we used in the analysis. The availabil-ity of each of these attributes in a given dataset is indicated by the two rightmost columns of the table. It can be seen that there are minor variations in the ways data is recorded for the two districts. For instance, GPA is recorded on a quarterly basis for District A and on an yearly basis for Dis-trict B. Our analysis is not sensitive to such representational variations. In fact, the framework proposed in this paper is generic enough to be applicable to any given set of features.
In this section, we present an overview of the models that we will be using through out this study. In addition, we also describe in detail the experimental setup that we use for all the prediction tasks. Our experimental setting is designed to match the real world setting as closely as possible. Student Attributes District A District B Gender XX Age X X Ethnicity X X City X X Street X X School Code XX Absence Rates XX
Tardiness Rates XX # of Suspensions X X # of Unexpected Entries/Withdrawals XX Quarterly GPA X X Cumulative GPA X X Cumulative Math GPA X X Cumulative Science GPA X X Cumulative Social Science GPA X X Cumulative English GPA X X MAP-R National Percentile Ranks X X Math Proficiency Scores (MPS) X X PSAT Critical Reading X X PSAT Math X X Limited English Proficiency X X Economically Disadvantaged (EDS) X X Is student new to the school district? XX Is student disabled? X X Was student ever retained? XX Did student graduate on time? XX Table 1: List of student attributes and their avail-ability in Districts A and B.

Problem Setting: In order to provide assistance to stu-dents who are at risk of not graduating on time, we first need to accurately identify such students. This can be achieved by using algorithms that can learn from the outcomes of stu-dents in the earlier cohorts. Schools have records on which of the students from prior cohorts failed to graduate high school within 4 years. From Table 1, it can be seen that the flag Did the student graduate high school on time? captures this aspect and hence can serve as the outcome variable. We compute the complement of this flag which takes the value 1 if the student failed to graduate on time and 0 otherwise. We use the term no_grad to refer to this complement vari-able and use it as the response variable for all our prediction tasks. The problem of identifying students who are at risk of not graduating on time can thus be formulated as a binary classification task with no_grad as the outcome variable. All the other variables in Table 1 can be used as predictors.
Models: To predict if a student is at risk of not gradu-ating on time, we experiment with Random Forests (RF), Adaboost (AB), Logistic Regression (LR), Support Vector Machines (SVM), and Decision Trees (DT). We use scikit-learn implementations of all these models.

Experimental Setup: Our datasets comprise of cohorts of students graduating in 2012 and 2013. Recent research that deals with the problem of predicting student perfor-mance evaluated the models via cross validation on a single cohort [26, 3]. Though this is an acceptable way of estimat-ing any algorithm X  X  performance in general, it is not ideal for the current setting. To illustrate, school districts often have access to outcomes and other features from previous cohorts. The goal here is to predict the future outcomes accurately by training on data from previous cohorts. Thus, an apt way of evaluating an algorithm in this setting is to train a model using data from previous cohorts and use a later co-hort as the test set. We carry out all the evaluations in this manner, using the cohort of students graduating in 2012 as the training set and the later cohort of students graduating in 2013 as the test set.
 Some of the models that we employ such as Random Forests involve sampling random subsets of data. This cre-ates a certain degree of non-determinism in the estimated outcomes. In order to account for this, we carry out 100 runs with each of these models and average the predictions (and/or probabilities) to compute the final estimates. Dur-ing our analysis, we also experimented with the leave-k-out strategy. As a part of this approach, we executed 100 iter-ations for each classification model. During each iteration, every model is trained on N  X  k randomly chosen data points, where N is the size of the entire dataset and k = 0 . 01  X  N .
With this framework in place, we now proceed to present how we evaluate each of the models while taking into account educators X  requirements.
School districts are interested in identifying those students who are at risk of not graduating high school on time before they reach the end of middle school. This helps them plan their resource allocation ahead of time. In this section, we focus on this setting by predicting if a student is at risk of not graduating high school on time using the data available prior to the end of middle school. More specifically, we use GPAs, absence rates, tardiness, other scores and flags (listed in Table 1) up until grade 8 along with other demographic attributes and predict the outcome variable no_grad . In this section, we address the following questions:
Our goal here is to evaluate the performance of various models on the task of predicting if a student is likely to graduate high school on time. Since we are dealing with the prediction of a binary outcome, several standard metrics such as accuracy, precision, recall, and AUC can be readily used. We evaluate the performance of all the models us-ing these standard metrics. Figure 1 shows the ROC curves corresponding to various classification models for districts A and B. It can be seen that the Random Forest model out-performs all the other models for both school districts, with AdaBoost and Logistic Regression being the next best per-forming solutions for both the datasets. SVMs and Decision Trees exhibit varying performance across the two datasets. While SVM performs on par with Logistic Regression and AdaBoost models on District A, it performs much more poorly when applied to District B.

Usage of metrics such as AUC for a binary classification task is relatively common in machine learning. Educators on the other hand think about the performance of an al-gorithm in this context slightly differently. The educators perspective stems from the fact that school districts often have limited resources for assisting students. Furthermore, the availability of these resources varies with time. Due to factors such as the number of students enrolled and budget allocated, the availability of these resources widely varies across districts. For example, District A might have the re-sources to support 100 students in 2012, however they might be able to target only 75 students in 2013. Further, District B might be able to assist 300 students in 2012 and 500 stu-dents in 2013. Building algorithms that can cater to these settings is extremely crucial to address the problem at hand.
After various discussions with our school district partners, we understood that an algorithm that can cater to their needs must provide them with a list of students ranked ac-cording to some measure of risk such that students at the top of the list are verifiably at higher risk. Once educa-tors have such a ranked list available, they can then simply choose the top K students from it and provide assistance to them. For instance, if District A can only support 75 stu-dents in 2013, educators in that district can just choose the top 75 students from this rank ordered list and assist them. Furthermore, as more resources become available, they can Figure 2: Empirical Risk Curves. The ranking qual-ity of an algorithm is good if this curve is monoton-ically non-decreasing. choose more students from this list according to the rank ordering and provide support to those students too.
The challenge associated with ranking students is that the data available to school districts only has binary ground truth labels (i.e., graduated/not-graduated). This effectively means that we are restricted to using binary classification models because other powerful learning to ranking tech-niques [20] require ground truth that captures the notion of ranking. Fortunately, most of the classification models assign confidence/probability estimates to each of the data points and we can use these estimates to rank students. However, before we begin using these estimates to rank stu-dents, we need to ensure that these estimates are indeed correct.
We begin this section by understanding how to use the confidence scores or probability estimates output by algo-rithms to rank order students. Then, we discuss how to evaluate the goodness of such estimates produced by vari-ous algorithms.

From models to risk estimates: Binary classification approaches output a 0/1 value for each data point. However, most of the classification algorithms involve computation of some form of confidence scores for each data point before the algorithm even assigns a label to it. In this work, we use the probability of not graduating on time as a proxy for estimating risk. While Logistic Regression estimates these probabilities as a part of its functional form, all the other algorithms output proxies to these probabilities. We obtain these proxy scores and convert them into probabilities.
Decision tree assigns each data point to one of its leaf nodes and the probability of not graduating on time for any given data point is equivalent to the fraction of those stu-dents assigned to the corresponding leaf node who do not graduate on time [10]. Random Forests involve training a forest of trees on data points and the probability of not grad-uating on time for a particular data point is computed as the mean of the predicted class probabilities of the trees in the forest [9]. The class probability assigned by any single tree is computed in the same manner as that of a decision tree. Similarly, in the case of AdaBoost which involves multiple learners, the probability assigned to a particular student is computed as the weighted mean of the predicted class prob-abilities of the classifiers in the ensemble [21]. Support Vec-tor Machines estimate the signed distance of a data point from the nearest hyperplane and Platt scaling can be used to convert these distances into probability estimates [19].
Next, we describe the process of evaluating the goodness of these probabilistic estimates of risk. We use the term risk scores to refer to these probabilities from here on.
Measuring the goodness of risk scores: In order to understand the accuracy of the risk scores estimated by var-ious algorithms for ranking students, we propose a simple solution. We first rank students in descending order of their estimated risk scores. We then group students into bins based on the percentiles they fall into when categorized using risk scores. For example, if we choose to create 10 bins, the bottom 10% of students who have the least risk are grouped into a single bin. Students who rank between 10 th and 20 percentile are grouped into the next bin and so on. For each such bin, we compute the mean empirical risk , which is the fraction of the students from that bin who actually (as per ground truth) failed to graduate on time. We then plot a curve where values on the X-axis denote the upper per-centile limit of a bin and values on the Y-axis correspond to the mean empirical risk of the corresponding bins. We call this curve an empirical risk curve .

An algorithm is considered to be producing good risk scores and consequently ranking students correctly if and only if the empirical risk curve is monotonically non-decreasing. If the empirical risk curve is non-monotonic for some algo-rithm, it implies that the ranking using the algorithm X  X  risk scores may result in scenarios where students with lower risk scores are more likely to not graduate on time compared to students with higher risk scores. Figure 2 shows these curves with 10 student bins for districts A and B respectively. It can be seen that most algorithms exhibit monotonically non-decreasing empirical curves in the case of District A. How-ever, decision tree exhibits some degree of non-monotonicity. On the other hand, for District B, all the models except for Random Forest exhibit non-monotonicity consistently. Therefore, students should be ranked using the scores pro-vided by Random Forest model for District B.
In the previous section, we discussed how to evaluate the goodness of rankings produced by various models. Here, we continue the discussion and present two metrics which are far more informative to educators than traditional pre-cision recall curves. We already emphasized on the fact that school districts have limited resources and can assist only a certain number of students every year (let us denote this number as K ). Consequently, there is a strong need for algorithms which can produce good risk scores to rank students. Given this setting, it would be much more in-formative to provide precision and recall values of various algorithms at different values of K . We call these curves precision at top K curve and recall at top K curve re-spectively. These curves help educators in readily inferring the precision and recall of various algorithms at a threshold K of their choice.
 Figure 3 shows the precision at top K curves for districts A and B respectively. It can be seen that there are huge dif-ferences in the precision of algorithms at smaller values of K, particularly for District B. Note that resource constraints of-ten force educators to set K to small values. Random Forests consistently outperform their counterparts across all K for both districts A and B. The precision of other algorithms, however, varies with K . For instance, we can observe that Logistic Regression has lower precision compared to the de-cision tree when K  X  150 on District B. Beyond this thresh-old, Logistic Regression has a higher precision than the de-cision tree.

Figure 4 shows the recall at top K curves for both dis-tricts. Again, Random Forests outperform all other models at all values of K . It can be seen that there is a higher variation in the recall values of algorithms for District B. Further, Support Vector Machines exhibit consistently low recall on District B. The performance of other algorithms depends on the threshold K .
While the construction of models that can precisely iden-tify students at risk is an important step to the design of early warning systems, it is equally important to analyze the output produced by these algorithms to make sure it aligns with the prior knowledge and/or findings of educators. In this section, we study in detail: Each of these aspects allows us to obtain a better under-standing of the model behavior.
In order to ensure that the output of the prediction mod-els can be converted into actionable insights, it is essential to understand which factors contribute most heavily to the predictions. To answer this question, we make use of a vari-ety of feature selection techniques that can be used to rank features according to their level of importance.

The setup we use to evaluate feature importances is simi-lar to that previously described in section 5. We specifically chose to threshold our datasets at the end of 8 th grade, as that time stamp marks the students X  transition into high school, and has been shown to be an especially opportune moment for targeted interventions [5].

The approaches that we use to compute feature impor-tances are strictly dependent on the algorithms being used. We compute feature importances using both Gini Index (GI) and Information Gain (IG) for Decision Trees [23]. In the case of Random Forest, we consider feature importance to be the ratio of the number of instances routed to any deci-sion tree in the ensemble that contains that feature over the total number of instances in the training set. For AdaBoost, we simply average the feature importances provided by the base-level classifier  X  CART decision tree with maximum depth of 1  X  across all iterations. In the case of Logistic Re-gression and SVM models, feature importances were simply considered to be the absolute values of feature coefficients.
As discussed earlier, we ran each classification model 100 times and subsequently averaged the importance scores as-signed to each feature across all the iterations. We then ranked all the features in descending order of their impor-tance scores derived from the appropriate classifiers. Figure 5 illustrates how features are ranked by various algorithms. Due to space constraints, we only present a subset of 5 fea-tures in Figure 5. Table 2 lists the top 5 features as ranked by each of the algorithms.

It can be inferred from Table 2 and Figure 5 that GPA at 8 th grade is highly ranked according to most of the algo-rithms, indicating that academic performance at that par-ticular time stamp is predictive of on-time high school grad-School Code=317 Science Credits 08 Gender 07 =Female Figure 5: Feature ranking by various algorithms. GPA 08 is ranked consistently high in both the dis-tricts by most of the models. uation. Interestingly, gender was highly ranked by Logistic Regression and SVM methods for one of the cohorts. In addition to GPA, absence rates at 8 th grade also show up as prominent features for both districts A and B. Further, some of the algorithms rank features such as economically disadvantaged (EDS) and disability flags highly.
School district administrators and educators are often in-terested in understanding the patterns of mistakes made by algorithms, which in turn helps them decide whether to use that model. For instance, if an algorithm is misclassify-ing certain kinds of students and educators consider such patterns of misclassifications unacceptable, then they can choose not to use it in spite of the fact that the algorithm might be achieving a high precision and recall.

In order to identify such patterns for any given classifi-cation model, we use a simple technique involving frequent itemset extraction. Below is a description of the technique: 1. Identify all frequent patterns in the data using the FP-2. Rank students based on risk score estimates from the 3. Create a new field called mistake . Set the value of this 4. For each frequent pattern detected in Step 1, compute 5. Sort the patterns based on their probability of mistake
The above procedure helped us identify several interest-ing mistake patterns for various algorithms. Due to space constraints, we present the patterns for just two of the al-gorithms (refer Table 3). It can be seen that the models are making mistakes when a student has a high GPA and a high absence rate/tardiness or when a student has a low GPA and low absence rate/tardiness. It is also interesting to note that the Adaboost model is less accurate with re-spect to students who are economically disadvantaged but do well in Math and Science. This indicates that classifi-cation models are prone to making mistakes particularly on those data points where certain aspects of students are posi-tive and others are negative. We found similar patterns with most other algorithms.
Our discussions with school districts revealed that edu-cators placed a lot of importance on exploratory aspects of models. When we present educators with a suite of al-gorithms, they are keen on understanding the differences in rank orderings produced by each of these algorithms. Here, we address the question: How similar or dissimilar are the rank orderings produced by any two given models ? This question can be answered by computing rank correla-tion metrics such as Spearman rank correlation coefficient, Figure 6: Jaccard Similarity of students at-risk as identified by various algorithms.
 Kendall X  X  Tau, and Goodman and Kruskal X  X  gamma [17] for every pair of algorithms. While this is a perfectly reasonable strategy, recall that educators are typically interested in un-derstanding all the metrics as a function of K (the number of students that can be targeted using the available resources).
In order to measure the similarity of rank orderings for various values of K , we use Jaccard similarity metric. Given two sets A and B, Jaccard similarity is the ratio of the num-ber of elements in the intersection of A and B to the number of elements in the union of A and B. The higher the value of Jaccard similarity, the more similar the sets. For a given K , all the algorithms return a set of K students who are likely to not graduate on time based on the risk scores they produce. Similarity between rank orderings of algorithms can now be estimated by computing the Jaccard similarity metric between the set of K students returned by various algorithms.

Figure 6 shows the Jaccard similarity values that we com-puted for every pair of algorithms at various values of K for Districts A and B respectively. It can be seen that Lo-gistic Regression and SVM produce highly similar outputs for all values of K in District A. Furthermore, the ranking produced by most models is not similar to Decision Trees (RF-DT, AB-DT, LR-DT, SVM-DT curves). In the case of District B, there are interesting variations in the similarity between algorithms as K changes. For small values of K , AdaBoost and Decision Trees flag an identical set of stu-dents as at-risk. However, as K increases, Random Forest and AdaBoost produce similar outputs. The rank orderings output by Random Forest and Logistic Regression models are also quite similar. Lastly, we see that results from SVM are least similar to that of any other algorithm in the case of District B (RF-SVM, AB-SVM, LR-SVM, SVM-DT curves).
This analysis helps school districts in understanding which algorithms to retain in their suite and which ones to discard. Figure 7: Precision at top 5% across various grades. For instance, if they find that two algorithms are consistently similar in the rankings they produce, they may choose to retain just one of these algorithms based on parameters such as ease-of-use, computational efficiency etc.

Next, we focus on the importance of predicting risk at early stages. We describe in detail an evaluation procedure that helps us determine if an algorithm is able to predict student risk at early stages.
Beyond being able to accurately identify students who are at risk of not graduating on time, it is important to make these predictions early so that educators and administrators have enough time to intervene and guide students back on track. In addition, our interactions with school districts re-vealed that once a student is retained in a grade, it becomes much harder to ensure timely graduation. Therefore, it is important to identify a student who is at risk before he/she starts failing grades and/or drops out. In this section, we discuss evaluation procedures which help us determine if an algorithm is making timely predictions.
 Predicting risk early: Here, we address the questions: How precise is any given model at the earliest grade for which we have data? How does this performance change over time? These questions can be answered by examining the perfor-mance of the models across all grades. The metrics that we use to evaluate the performance of our models are: preci-sion at top K and recall at top K . Working with our school district partners revealed that a majority of school districts can afford resources to assist at least 5% of their student population. Therefore, we set K to 5% of the student pop-ulation for each of the districts.

We evaluate the performance of the models across all grades. Figure 7 depicts the precision at top 5% for each of the algorithms on districts A and B respectively. Random Forest consistently outperforms all other models in both the districts. In the case of District A, the performance im-proves steadily from 6 th to 11 th grade and then plateaus. AdaBoost and Decision Tree algorithms exhibit poor per-formance compared to other models across all grade levels for district A. The performance of SVM is consistently poor through out all grades for District B. The corresponding re-call curves (omitted due to space constraints) exhibit similar patterns.

Identifying risk before off-track: Recall that an im-portant requirement in this setting is for a model to be able to identify students who are at risk of not graduating on time even before the student begins to fail grades and/or drops out. It is ideal to provide interventions to students before either of these undesired outcomes materialize, as opposed to taking a more reactive approach. A student can be cat-egorized as off-track if he or she is retained or drops out at the end of a given grade. An ideal algorithm should be able to predict risk even before students go off-track .
Here, we investigate if our models succeed in identifying students before they go off-track . In order to do determine this, we use a metric called identification before off-track . This metric is a ratio of the number of students who were identified to be at risk before off-track to the total number of students who failed to graduate on time. For instance, if there are 100 students in the entire dataset who failed to graduate on time, and if the algorithm identifies 70 of these students as at-risk before they fail a grade or drop out, then the value of identification before off-track is 0.7. The higher the value of this metric, the better the algorithm is at diagnosing risk before any undesirable outcome occurs. Note that we exclude all those students who graduate in a timely manner from this calculation.

Figure 8 presents the values for identification before off-track metric across varying K for districts A and B. The findings here match our earlier results in that Random For-est model outperforms all the other models for both districts. While Decision Tree exhibits poor performance on District A, SVM turns out to be the weaker model for District B.
In this paper, we outlined an extensive framework that uses machine learning approaches to identify students who are at risk of not graduating high school on time. The work described in this paper was done in collaboration with two school districts in the US (with combined enrollment of around 200,000 students) and is aimed at giving them (as well as other schools) proactive tools that are designed for their needs, and to help them identify and prioritize students who are at risk of adverse academic outcomes. Although the work in this paper is limited to predicting students who are likely to not finish high school on time, we believe that the framework (problem formulation, feature extraction process, classifiers, and evaluation criteria) applies and generalizes to other adverse academic outcomes as well, such as not apply-ing to college, or undermatching [16]. Our hope is that as school districts see examples of work such as this coming from their peer institutions, they become more knowledge-able, motivated and trained to use data-driven approaches and are able to use their resources more effectively to im-prove educational outcomes for their students. This work was supported in part by the Eric &amp; Wendy Schmidt Data Science for Social Good Summer Fellowship, and a Robert Bosch Stanford Graduate Fellowship. The authors would like to thank Amy Hawn Nelson for her help in coordinating with one of the school districts, Ben Yuhas for insightful discussions, and Shihching Liu and Marilyn Powell for assisting us with data logistics. [1] Building a Grad Nation. http: [2] E. Aguiar, G. A. Ambrose, N. V. Chawla, [3] E. Aguiar, H. Lakkaraju, N. Bhanpuri, D. Miller, [4] E. M. Allensworth and J. Q. Easton. What matters [5] E. M. Allensworth, J. A. Gwynne, P. Moore, and [6] R. Balfanz, L. Herzog, and D. J. Mac Iver. Preventing [7] A. J. Bowers and R. Sprott. Why tenth graders fail to [8] A. J. Bowers, R. Sprott, and S. A. Taff. Do we know [9] L. Breiman. Random forests. Machine learning , 45(1), [10] N. V. Chawla and D. A. Cieslak. Evaluating [11] G. W. Dekker, M. Pechenizkiy, and J. M.
 [12] E. Er. Identifying at-risk students using machine [13] D. C. French and J. Conrad. School dropout as [14] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [15] A. Hershkovitz, R. Baker, S. M. Gowda, and A. T. [16] C. Hoxby, S. Turner, et al. Expanding college [17] M. Kendall. Rank correlation methods . Griffin, [18] H. M. Levin and C. Belfield. The price we pay: [19] H.-T. Lin, C.-J. Lin, and R. C. Weng. A note on [20] T.-Y. Liu. Learning to rank for information retrieval. [21] A. Niculescu-mizil and R. Caruana. Obtaining [22] K. Pittman. Comparison of data mining techniques [23] J. Quinlan. Induction of decision trees. Machine [24] R. W. Rumberger and S. A. Lim. Why students drop [25] J. Soland. Predicting high school graduation and [26] A. Tamhane, S. Ikbal, B. Sengupta, M. Duggirala, and [27] University of Chicago. The Eric &amp; Wendy Schmidt [28] U.S. Department of Education, National Center for [29] A. Vihavainen, M. Luukkainen, and J. Kurhila. Using [30] K. L. Wagstaff. Machine learning that matters. In [31] S. K. Yadav, B. Bharadwaj, and S. Pal. Data mining
