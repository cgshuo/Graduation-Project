 Scientific models typically depend on parameters. Preserv-ing the parameter dependence of models in the pattern min-ing context opens up several applications. Within associa-tion rule mining (ARM), the choice of parameters can be studied with more flexibly then in traditional model build-ing. Studying support, confidence, and other rule metrics as a function of model parameters allows conclusions on as-sumptions underlying the models. We present efficient tech-niques to handle multiple model output data sets at little more than the cost of one. We integrate output from hidden Markov models into the association rule mining framework, demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation. Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications X  X ata Mining General Terms: Algorithms Keywords: association rule mining, profile hidden Markov model, model mining
Progress of science depends heavily on the effective use of existing data. Data mining has established itself as a leading discipline in the extraction of knowledge from large data repositories. Interestingly, data mining techniques have rarely been applied to some of the oldest types of data that are processed in computers: The results of computational scientific models. These results constitute abstractions of real-world mechanisms and observations that can contribute to our understanding of phenomena far beyond the original purpose of the models.  X 
This material is based upon work supported by the Na-tional Science Founda tion under Gra nt No. #01322899  X  Corresponding Author Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. riods of time through experiments such as experimentally determined localization, function, and phenotype informa-tion for proteins. This paper demonstrates how a variety of experimental data can be mined in combination with mul-tiple model data sets.

The remainder of the paper is organized as follows. Sec-tion 2 discusses generation of ARM items from models and defines terms relevant to the problem of using multiple mod-els. Profile hidden Markov models are described in Section 2.2 and related work in 2.3. Section 3 explains the concept of cumulative sequence ARM that underlies our algorithm and describes our implementation. Section 4 gives a per-formance analysis and presents results. Finally, Section 5 concludes the paper.
Association rule mining is based on the concept of trans-actions involving items. In the original market basket set-ting [1], a transaction corresponded to one shopping event. Transactions focus on entities such as customers or proteins. In our work, experimental properties form a central part of the analysis. These experimental properties are associated with proteins in much the same way as items associate with objects in standard association rule mining. Proteins can be characterized not only by experimental properties but also by sequences. Sequence information in itself is not suit-able for basic ARM analysis. Different models have been developed to extract meaningful subsequences from gene se-quences. These subsequences are expected to be related to observable behavior in the proteins. While some model tech-niques are computational [3], most are based on biological knowledge of which sequences are similar [12]. Many involve model parameters [15]. In the model-building process, pa-rameters are set to optimize performance. Our approach allows more flexibility in identifying relationships between model parameters and properties of the entities or proteins.
We partition the data in two parts. Target items represent experimental data and are independent of model parame-ters. Only one set of target items is given for each entity. Model items occur in entities for each model. Each model transaction corresponds to a parameter or set of parameters of the underlying model. Model items are mined together with target items with the goal of studying how to conclude from model items to target items. The background to this focus is the relative difficulty of obtaining experimental data compared with the relative abundance of model output data.
Similar to the standard association rule mining (ARM) and sequence mining framework [1, 2, 4] we present our ap-called the target items that are independent of model param-eters. We call a subset X t  X  I t a target itemset and we call set of items called model items . I M defines the item space covered by the output from models. Note that the model item space itself is independent of the particular model pa-rameters. Target itemsets a re non-model itemsets and we will use the terms target and non-model interchangeably.
A Target Transaction is the target item partition of a tuple ( tid , X t ) that is identified by a transaction identifier where the itemsets of each t t k .X t  X  I t .
 score threshold is a parameter that is associated with a cu-mulative model sequence since each score within the thresh-old creates a model. Since the threshold does not affect the model definition as such, the models themselves do not have to be reconstructed.
HMMs have been extensively studied in combination with clustering [26, 16, 21]. In these approaches, HMMs are used to provide a similarity measure. Mannila et al. discuss min-ing of clusters formed by a model using global techniques such as probabilistic modeling and local techniques such as frequent itemset mining [17]. Hollmen, Seppanen and Mannila present the use of frequent itemset mining on clus-ters formed by probabilistic means using Bernoulli models in [13]. Hollmen et al. focus on the use of a single model to partition and mine the data.

The cumulative model sequence concept involves taking advantage of monotone properties as they hold in sets. Stud-ies on constrained ARM mining and the monotone prop-erty have addressed such constraints in different contexts. One such work [7] considers the monotone properties of the itemsets in a preprocessing step. In our setting, data have an additional dimension associated in the form of a se-quence of models, i.e., we have transactions with items over asequence . The monotone property, Lemma 1, applies to the database transaction level rather than the itemset level where typical monotone constraints operate.

Work on constraints in ARM is further of relevance. Stan-dard ARM generates rules involving any subset of the parent item set with only the following conditions imposed: For parent itemset X and rules of the form A  X  C the sets A, C  X  X have to be non-overlapping and the rules have to exceed the given thresholds on the rule evaluation crite-ria. Further constraints have been introduced into ARM [27, 22] to eliminated unwanted results and offer special means to perform additional pruning of the itemset search space. Two types of rule constraints are useful when dealing with models. We constrain rules so that the antecedent is formed by model items exclusively and the consequent is formed by target items exclusively ( A  X  I M and C  X  I t ). We thereby achieve the goal of characterizing the model output in terms of independently gathered information.
We introduce our algorithm for cumulative model sequences, CMS-ARM. When models generate cumulative sequence attributes, the performance of association rule min-ing can benefit from exploiting the cumulative property. To offer comparative analysis of models in a cumulative se-quence, we mine the model items combined with target items and form rules that have model items as antecedents and tar-get items as consequents. We present an efficient approach to ARM that makes use of the cumulative property of the model data.
 Lemma 1. (Monotone Property of Cumulative Sequence Itemsets) The support of an itemset increases monotonically for a cumulative model sequence.

Proof. Through Definitions 2 and 3, we see that the sup-port of any itemset generated from M a as i M a k is at most that The profile HMM database input for HMMER is the pfam ls A families at the pfam database [5] which consists of  X  7,500 protein domains.
Our implementation of cumulative model sequence ARM (CMS) is shown to be more efficient than basic ARM. Figure 1 shows the number of itemsets joined by ARM, considered candidates by CMS, counted by ARM, counted by CMS, and found frequent by both. The figure uses E-value instead of score, which is an alternative measure of sequence similarity. A large E-value corresponds to a small score. All frequent itemsets found by CMS are the same as the itemsets found by ARM, which follows from Theorem 1. The largest contri-bution to the work of CMS comes from the initial run. The sum of all later points in the graph is smaller than the initial one by itself (initial point is  X  1 . 73 e 5, the sum of all other points is  X  3 . 15 e 3). We clearly see that fewer itemsets have to be counted in CMS when compared to ARM (by almost two orders of magnitude after the initial point). In addition, there is no need for a joining phase with CMS as is needed in ARM. The plot shows that the ARM join operation adds significantly to the number of operations in the algorithm. In CMS, additional item sets have to be considered but this process adds very little work to the CMS algorithm.
It can be shown that CMS-ARM has a performance that comes close to the Oracle method described in [24]. An Oracle is defined as an algorithm, in which it is already known which itemsets will be frequent and the only process-ing needed is to count them. To see how far CMS is from the ideal Oracle algorithm we can compare the CMS fre-quent itemsets, which are the only ones the Oracle needs to count, to CMS counted itemsets in Figure 1. For the points after the initial one, there are  X  5% more counts in CMS than in Oracle. The Oracle counts 2,993 itemsets total (as the sum of all points after the initial one) while the CMS counts 3,146. Standard ARM, on the other hand, is several orders of magnitude worse than the Oracle. Initially, we evaluate results based on individual rules. This allows insight into the usefulness of parameter-depen-dent ARM as a pattern discovery technique. Rule volume does not increase beyond standard ARM for a single set of rule output. Rather, the detail provided with rules is in-Figure 3: Confidence of rules with a subcategory of phenotype as consequent creased score threshold. Since phenotypes are well-defined experimental properties, it is to be expected that domains associated with them have to be conserved well. This ob-servation raises the question if there is a quantitative way of testing the relationship between slope of the confidence curveandthetypeofconsequent.
Visual inspection of individual rules can give clues for po-tentially interesting rules. To test hypotheses on behavior of rule metrics as a function of model parameters requires quantitative analysis. We test whether slope is related to the rule consequent. In the previous section, we saw examples of rules for which the confidence increased for increasing score cut-off. We would expect that rules that have specific conse-quents, such as phenotype, should show a stronger increase than rules that show associations with common annotations, such as localization in the cytoplasm or nucleus. These lo-calizations are so frequent that they may be present even if the domain is not recognized appropriately by the hid-den Markov model. Domains that are only associated with broad localizations may not have a well-known function and they may not be well conserved.

Well-conserved domains are typically easiest to study ex-perimentally. To test this we use ROC analysis [23] with av-erage slope as classifier and any one of the main categories of experimental results as class label. If slope were unre-lated to the categories of the consequent, the ROC curve should be a diagonal with an area of 0.5. For this study, we limit the rule output to single antecedent and single conse-quent. The result can be seen in Figure 4. Slope clearly is a much poorer predictor of localization than any of the other categories are. The area under the ROC curve, AROC, con-firms this impression, see Table 1. Note that localization is much more frequent than any of the other categories are. We can conclude that domains with high average slope of the confidence curve are more likely to be associated with one of the categories that indicate specific knowledge of the protein. This observation is in accordance with the concept that high slope is indicative of sequence conservation.
Another interesting question is if our analysis helps iden-tify appropriate model parameters for individual models. A natural choice for a score threshold is the point at which the confidence is maximized. We compared this value with the GA score that is provided by PFAM as a suggested cut-off. Using optimal values the confidence was improved for 75.61% of the rules. For the remaining ones the confidence stayed equal. The average increase in confidence was 9.15% and the maximum increase 49.04%. This test is not conclu-sive since we had to test the confidence on the same data we used for identifying the maximum. We did, however, check if the score thresholds could be supported by using the ROC analysis described for slope, see Figure 4 and Table 1. We found that the GA-score was not noticeably predictive of categories, while the score at maximum confidence showed similar behavior as the slope, albeit less clearly. This gives another indication that the score at maximum confidence is more biologically relevant than the GA-score.

These studies demonstrate that the results of ARM on model sequences can lead to insights beyond the analysis of single rules. Results from global studies can help determin-ing appropriate model parameters as well as understanding relationships between information available for entities (in our case proteins) and the properties of model output.
Integrating hidden Markov model output with external data to perform association rule mining has been shown to provide information that significantly exceeds what can be gained from traditional techniques. Including model in-stances from several parameter choices allows finding depen-dencies that are more flexible than the standard optimiza-tion strategies that lead to one-fits-all results. Rather than focusing on a single target, our approach allows inclusion of a variety of experimental results. We have demonstrated how our approach helps researchers in the task of identi-fying suitable model parameters and in understanding the relationship between model parameters and other proper-ties of the objects in question. The additional information that our algorithm provides at very little cost is important in science, where individual rules can have a strong impact on further research. We presented an efficient algorithm, CMS-ARM, for mining a class of model sets called cumula-tive model sequences. The use of ROC-based analysis and other functional plotting methods offered visual exploitation of the set of information ARM was able to provide. In fur-ther work, we will expand this research to other model types and refine the visualization techniques. Thanks go to resources provided by the NDSU Center for High Performance Computing.
