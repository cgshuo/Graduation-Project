 The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus generated from word-based models (Brown et al., 1993), proceeds with step of induction of phrase table (Koehn et al., 2003) or synchronous gram-mar (Chiang, 2007) and with model weights tun-ing step. Words are taken as inputs to PB-SMT at the very beginning of the pipeline. But there is a deficiency in such manner that word is too fine-grained in some cases such as non-compositional phrasal equivalences, where clear word align-ments do not exist. For example in Chinese-to-English translation,  X   X   X  and  X  X ould like to X  constitute a 1 -to-n phrasal equivalence,  X   X  X  X   X   X  and  X  X ow much is it X  constitute a m -to-n phrasal equivalence. No clear word alignments are there in such phrasal equivalences. Moreover, should basic translational unit be word or coarse-grained multi-word is an open problem for opti-mizing SMT models. 
Some researchers have explored coarse-grained translational unit for machine translation. Marcu and Wong (2002) attempted to directly learn phrasal alignments instead of word align-ments. But computational complexity is prohibi-tively high for the exponentially large number of decompositions of a sentence pair into phrase pairs. Cherry and Lin (2007) and Zhang et al. (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. Blunsom et al. (2008; 2009) induced phrasal synchronous grammar, which aimed at finding hierarchical phrasal equiva-lences. 
Another direction of questioning word as basic translational unit is to directly question word segmentation on languages where word bounda-ries are not orthographically marked. In Chinese-to-English translation task where Chinese word boundaries are not marked, Xu et al. (2004) used word aligner to build a Chinese dictionary to re-segment Chinese sentence. Xu et al. (2008) used a Bayesian semi-supervised method that com-bines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine trans-lation. There are also researches focusing on the impact of various segmentation tools on machine translation (Ma et al. 2007; Chang et al. 2008; Zhang et al. 2008). Since there are many 1 -to-n phrasal equivalences in Chinese-to-English trans-lation (Ma and Way. 2009), only focusing on Chinese word as basic translational unit is not adequate to model 1 -to-n translations. Ma and Way (2009) tackle this problem by using word aligner to bootstrap bilingual segmentation suit-able for machine translation. Lambert and Banchs (2005) detect bilingual multi-word ex-pressions by monotonically segmenting a given Spanish-English sentence pair into bilingual units, where word aligner is also used. IBM model 3, 4, 5 (Brown et al., 1993) and Deng and Byrne (2005) are another kind of re-lated works that allow 1 -to-n alignments, but they rarely questioned if such alignments exist in word units level, that is, they rarely questioned word as basic translational unit. Moreover, m -to-n alignments were not modeled. 
This paper focuses on determining the basic translational units on both language sides without using word aligner before feeding them into PB-SMT pipeline. We call such basic translational unit as pseudo-word to differentiate with word. Pseudo-word is a kind of multi-word expression (includes both unary word and multi-word). Pseudo-word searching problem is the same to decomposition of a given sentence into pseudo-words. We assume that such decomposition is in the Gibbs distribution. We use a measurement, which characterizes pseudo-word as minimal sequence of consecutive wo rds in sense of trans-lation, as potential function in Gibbs distribution. Note that the number of decomposition of one sentence into pseudo-words grows exponentially with sentence length. By fitting decomposition problem into parsing framework, we can find optimal pseudo-word sequence in polynomial time. Then we feed pseudo-words into PB-SMT pipeline, and find that pseudo-words as basic translational units improve translation perform-ance over words as basic translational units. Fur-ther experiments of removing the power of higher order language m odel and longer max phrase length, which are inherent in pseudo-words, show that pseudo-words still improve translational performance significantly over unary words. 
This paper is structured as follows: In section 2, we define the task of searching for pseudo-words and its solution. We present experimental results and analyses of using pseudo-words in PB-SMT model in section 3. The conclusion is presented at section 4. Pseudo-word searching problem is equal to de-composition of a given sentence into pseudo-words. We assume that th e distribution of such decomposition is in the form of Gibbs distribu-tion as below: where X denotes the sentence, Y denotes a de-composition of X . Sig function acts as potential function on each multi-word y k , and Z X acts as partition function. Note that the number of y k is not fixed given X because X can be decomposed into various number of multi-words. 
Given X , Z X is fixed, so searching for optimal decomposition is as below:  X  where Y 1 K denotes K multi-word units from de-composition of X . A multi-word sequence with maximal sum of Sig function values is the search target  X  pseudo-word sequence. From (2) we can see that Sig function is vital for pseudo-word searching. In this paper Sig function calculates sequence significance which is proposed to char-acterize pseudo-word as minimal sequence of consecutive words in sense of translation. The detail of sequence significance is described in the following section. 2.1 Sequence Significance Two kinds of definitions of sequence signifi-cance are proposed. One is monolingual se-quence significance. X and Y are monolingual sentence and monolingual multi-words respec-tively in this monolingual scenario. The other is bilingual sequence significance. X and Y are sen-tence pair and multi-word pairs respectively in this bilingual scenario. 2.1.1 Monolingual Sequence Significance unary word, monolingual sequence significance is defined as: where Freq i, j ( i  X  j ) represents frequency of word sequence w i , ..., w j in the corpus, Sig i, j sents monolingual sequence significance of a word sequence w i , ..., w j . We also denote word as span [ 1, n ]. Each span is also a multi-word ex-pression. 
Monolingual sequence significance of span [ i, j ] is proportional to span [ i, j ] X  X  frequency, while is inversely proportion to frequency of expanded span ( span [ i-1, j+1 ]). Such definition character-izes minimal sequence of consecutive words which we are looking for. Our target is to find pseudo-word sequence which has maximal sum of spans X  significances: where pw denotes pseudo-word, K is equal to or of K spans span 1 K . Equation (4) is the rewrite of equation (2) in monolingual scenario. Searching for pseudo-words pw 1 K is the same to finding optimal segmentation of a sentence into K seg-ments span 1 K ( K is a variable too). Details of searching algorithm are described in section 2.2.1. 
We firstly search for monolingual pseudo-words on source and target side individually. Then we apply word alignment techniques to build pseudo-word alignments. We argue that word alignment techniques will work fine if non-existent word alignments in such as non-compositional phrasal equivalences have been filtered by pseudo-words. 2.1.2 Bilingual Sequence Significance Bilingual sequence significance is proposed to characterize pseudo-word pairs. Co-occurrence of sequences on both language sides is used to define bilingual sequence significance. Given a bilingual sequence pair: span-pair [ i s , j s , i t (source side span [ i s , j s ] and target side span [ i bilingual sequence significance is defined as be-low: where Freq denotes the frequency of a span-pair. Bilingual sequence significance is an extension of monolingual sequence significance. Its value is proportional to frequency of span-pair [ i s , j s , i j ], while is inversely proportional to frequency of expanded span-pair [ i s -1 , j s +1 , i t -1 , j Pseudo-word pairs of one sentence pair are such pairs that maximize the sum of span-pairs  X  bilin-gual sequence significances: pwp represents pseudo-word pair. Equation (6) is the rewrite of equation (2) in bilingual scenario. Searching for pseudo-word pairs pwp 1 K is equal to bilingual segmentation of a sentence pair into optimal span-pair 1 K . Details of searching algo-rithm are presented in section 2.2.2. 2.2 Algorithms of Searching for Pseudo-Pseudo-word searching problem is equal to de-composition of a sentence into pseudo-words. But the number of possible decompositions of the sentence grows exponentially with the sen-tence length in both monolingual scenario and bilingual scenario. By casting such decomposi-tion problem into parsing framework, we can find pseudo-word sequence in polynomial time. According to the two scenarios, searching for pseudo-words can be performed in a monolin-gual way and a synchronous way. Details of the two kinds of searching algorithms are described in the following two sections. 2.2.1 Algorithm of Searching for Monolin-Searching for monolingual pseudo-words is based on the computation of monolingual se-quence significance. Figure 1 presents the search algorithm. It is performed in a way similar to CKY (Cocke-Kasami-Younger) parser. 
In this algorithm, W i, j records maximal sum of monolingual sequence significances of sub spans ized as Sig i,i (note that this sequence is word w only). For all spans that have more than one word ( i  X  j ), W i, j is initialized as zero. 
In the main algorithm, d represents span X  X  length, ranging from 2 to n , i represents start po-sition of a span, j represents end position of a span, k represents decomposition position of sum of monolingual sequence significances is found. 
The algorithm is performed in a bottom-up way. Small span X  X  computation is first. After maximal sum of significances is found in small spans, big span X  X  computation, which uses small spans X  maximal sum, is continued. Maximal sum of significances for whole sentence ( W 1,n , n is sentence X  X  length) is guaranteed in this way, and optimal decomposition is obtained correspond-ingly. 
The method of fitting the decomposition prob-lem into CKY parsing framework is located at steps 7-9. After steps 3-6, all possible decompo-sitions of span [ i, j ] are explored and W i, j timal decomposition of span [ i, j ] is recorded. Then monolingual sequence significance Sig i,j of span [ i, j ] is computed at step 7, and it is com-pared to W i, j at step 8. Update of W i, j is taken at step 9 if Sig i,j is bigger than W i, j , which indicates that span [ i , j ] is non-decomposable. Thus whether span [ i , j ] should be non-decomposable or not is decided through steps 7-9. 2.2.2 Algorithm of Synchronous Searching Synchronous searching for pseudo-words utilizes bilingual sequence significance. Figure 2 pre-sents the search algorithm. It is similar to ITG (Wu, 1997), except that it has no production rules and non-terminal nodes of a synchronous grammar. What it cares about is the span-pairs that maximize the sum of bilingual sequence sig-nificances. 
In the algorithm, records maximal sum of bilingual sequence significances of sub span-pairs of span-pair [ i span-pairs, W s are initialized as bilingual se-quence significances of such span-pairs. For other span-pairs, W s are initialized as zero. 
In the main algorithm, d s /d t denotes the length of a span on source/target side, ranging from 2 to n start position of a span-pair on source/target side, j / j t is the end position of a span-pair on source/target side, k s / k t is the decomposition po-side. Update steps in Figure 2 are similar to that of Figure 1, except that the update is about span-pairs, not monolingual spans. Reversed and non-reversed alignments inside a span-pair are com-pared at step 4. For span-pair [ i s , j s , i bilingual sequence significances is found. 
Fitting the bilingually searching for pseudo-words into ITG framework is located at steps 7-9. Steps 3-6 have explored all possible decomposi-maximal bilingual sequence significance of span-pair [ i i , j t ] is computed at step 7. It is compared to bilingual sequence significance of span-pair [ i i , j t ] is bigger than span-pair [ i Whether the span-pair [ i s , j s , i t , j t ] should be non-decomposable or not is decided through steps 7-9. 
In addition to the initialization step, all span-pairs X  bilingual sequence significances are com-puted. Maximal sum of bilingual sequence sig-nificances for one sentence pair is guaranteed through this bottom-up way, and the optimal de-composition of the sentence pair is obtained cor-respondingly. z Algorithm of Excluded Synchronous The algorithm of SSP in Figure 2 explores all span-pairs, but it neglects NULL alignments, where words and  X  X mpty X  word are aligned. In fact, SSP requires that all parts of a sentence pair should be aligned. This requirement is too strong because NULL alignments are very common in many language pairs. In SSP, words that should be aligned to  X  X mpty X  word are programmed to be aligned to real words. 
Unlike most word alignment methods (Och and Ney, 2003) that add  X  X mpty X  word to ac-count for NULL alignment entries, we propose a method to naturally exclude such NULL align-ments. We call this method as Excluded Syn-chronous Searching for Pseudo-words (ESSP). 
The main difference between ESSP and SSP is in steps 3-6 in Figure 3. We illustrate Figure 3 X  X  span-pair configuration in Figure 4. 
The solid boxes in Figure 4 represent excluded in SSP, there is no excluded part, that is, k s1
We can see that in Figure 4, each monolingual span is configured into three parts, for example: span [ i s , k s1 -1 ], span [ k s1 , k s2 ] and span [ k on source language side. k s1 and k s2 are two new variables gliding between i s and j s , span [ k is source side excluded part of span-pair [ i s , j s , i j ]. Bilingual sequence significance is computed only on pairs of blank boxes, solid boxes are ex-cluded in this computation to represent NULL alignment cases. 
Note that, in Figure 4, solid box on either lan-guage side can be void (i.e., length is zero) if there is no NULL alignment on its side. If all solid boxes are shrunk into void, algorithm of ESSP is the same to SSP. 
Generally, span length of NULL alignment is not very long, so we can set a length threshold for NULL alignments, eg. k s2 -k s 1  X  EL , where EL denotes Excluded Length threshold. Computa-tional complexity of the ESSP remains the same to SSP X  X  complexity O ( n s 3 .n t 3 ), except multiply a constant EL 2 . There is one kind of NULL alignments that ESSP can not consider. Since we limit excluded parts in the middle of a span-pair, the algorithm will end without considering boundary parts of a sentence pair as NULL alignments. In our experiments, pseudo-words are fed into PB-SMT pipeline. The pipeline uses GIZA++ model 4 (Brown et al., 1993; Och and Ney, 2003) for pseudo-word alignment, uses Moses (Koehn et al., 2007) as phrase-based decoder, uses the SRI Language Modeling Toolkit to train lan-guage model with modified Kneser-Ney smooth-ing (Kneser and Ney 1995; Chen and Goodman 1998). Note that MERT (Och, 2003) is still on original words of target language. In our experi-ments, pseudo-word length is limited to no more than six unary words on both sides of the lan-guage pair. We conduct experiments on Chinese-to-English machine translation. Two data sets are adopted, one is small corpus of IWSLT-2008 BTEC task of spoken language translation in travel domain (Paul, 2008), the other is large corpus in news domain, which consists Hong Kong News (LDC2004T08), Sinorama Magazine (LDC2005T10), FBIS (LDC2003E14), Xinhua (LDC2002E18), Chinese News Translation (LDC2003E07), Multiple Translation Chinese (LDC2004T07). Table 1 lists statistics of the corpus used in these experiments. 
For small corpus, we use CSTAR03 as devel-opment set, use IWSLT08 official test set for test. A 5 -gram language model is trained on English side of parallel corpus. For large corpus, we use NIST02 as development set, use NIST03 as test set. Xinhua portion of the English Gigaword3 corpus is used together with English side of large corpus to train a 4 -gram language model. 
Experimental results are evaluated by case-insensitive BLEU-4 (Papineni et al., 2001). Closest reference sentence length is used for brevity penalty. Additionally, NIST score (Dod-dington, 2002) and METEOR (Banerjee and La-vie, 2005) are also used to check the consistency of experimental results. Statistical significance in BLEU score differences was tested by paired bootstrap re-sampling (Koehn, 2004). 3.1 Baseline Performance Our baseline system feed s word into PB-SMT pipeline. We use GIZA++ model 4 for word alignment, use Moses for phrase-based decoding. The setting of language model order for each corpus is not changed. Baseline performances on test sets of small corpus and large corpus are re-ported in table 2. 3.2 Pseudo-word Unpacking Because pseudo-word is a kind of multi-word expression, it has inborn advantage of higher language model order and longer max phrase length over unary word. To see if such inborn advantage is the main contribution to the per-formance or not, we unpack pseudo-word into words after GIZA++ aligning. Aligned pseudo-words are unpacked into m  X  n word alignments. PB-SMT pipeline is executed thereafter. The ad-vantage of longer max phrase length is removed during phrase extraction, and the advantage of higher order of language model is also removed during decoding since we use language model trained on unary words. Performances of pseudo-word unpacking are reported in section 3.3.1 and 3.4.1. Ma and Way (2009) used the unpacking after phrase extraction, then re-estimated phrase translation probability and lexical reordering model. The advantage of longer max phrase length is still used in their method. 3.3 Pseudo-word Performances on Small Table 3 presents performances of SMP, SSP, ESSP on small data set. pw ch pw en denotes that pseudo-words are on both language side of train-ing data, and they are input strings during devel-opment and testing, and translations are also pseudo-words, which will be converted to words pseudo-words are adopted only on Eng-lish/Chinese side of the data set. 
We can see from table 3 that, ESSP attains the best performance, while SSP attains the worst performance. This shows that excluding NULL alignments in synchronous searching for pseudo-words is effective. SSP puts overly strong align-ment constraints on parallel corpus, which im-pacts performance dramatically. ESSP is superior to SMP indicating that bilingually motivated searching for pseudo-words is more effective. Both SMP and ESSP outperform baseline consis-tently in BLEU, NIST and METEOR. There is a common phenomenon among SMP, SSP and ESSP. w ch pw en always performs better than the other two cases. It seems that Chinese word prefers to have English pseudo-word equivalence which has more than or equal to one word. pw ch pw en in ESSP performs similar to the baseline, which reflects that our direct pseudo-word pairs do not work very well with GIZA++ alignments. Such disagreement is weakened by using pseudo-words on on ly one language side ( w ch pw en or pw ch w en ), while the advantage of pseudo-words is still leveraged in the alignments. 
Best ESSP ( w ch pw en ) is significantly better than baseline (p&lt;0.01) in BLEU score, best SMP ( w ch pw en ) is significantly better than baseline (p&lt;0.05) in BLEU score. This indicates that pseudo-words, through either monolingual searching or synchronous searching, are more effective than words as to being basic transla-tional units. 
Figure 5 illustrates examples of pseudo-words of one Chinese-to-English sentence pair. Gold standard word alignments are shown at the bot-tom of figure 5. We can see that  X  X ront desk X  is recognized as one pseudo-word in ESSP. Be-cause SMP performs monolingually, it can not consider  X   X  X  X   X  and  X  X ront desk X  simultaneously. SMP only detects frequent monolingual multi-words as pseudo-words. SSP has a strong con-straint that all parts of a sentence pair should be aligned, so source sentence and target sentence have same length after merging words into pseudo-words. We can see that too many pseudo-words are detected by SSP. 3.3.1 Pseudo-word Unpacking Perform-We test pseudo-word unpacking in ESSP. Table 4 presents its performances on small corpus. 
We can see that pseudo-word unpacking sig-nificantly outperforms baseline. w ch pw en is sig-nificantly better than baseline (p&lt;0.04) in BLEU score. Unpacked pseudo-word performs com-paratively with pseudo-word without unpacking. There is no statistical difference between them. It shows that the improvement derives from pseudo-word itself as basic translational unit, does not rely very much on higher language model order or longer max phrase length setting. 3.4 Pseudo-word Performances on Large Table 5 lists the performance of using pseudo-words on large corpus. We apply SMP on this task. ESSP is not applied because of its high computational complexity. Table 5 shows that all three configurations ( pw ch pw en , w ch pw en , pw ch of SMP outperform the baseline. If we go back to the definition of sequence significance, we can see that it is a data-driven definition that utilizes corpus frequencies. Corpus scale has an influ-ence on computation of sequence significance in long sentences which appear frequently in news domain. SMP benefits from large corpus, and w ch pw en is significantly better than baseline (p&lt;0.01). Similar to performances on small cor-pus, w ch pw en always performs better than the other two cases, which indicates that Chinese word prefers to have English pseudo-word equivalence which has more than or equal to one word. 3.4.1 Pseudo-word Unpacking Perform-Table 6 presents pseudo-word unpacking per-formances on large corpus. All three configura-tions improve performance over baseline after pseudo-word unpacking. pw ch pw en attains the best BLEU among the three configurations, and is significantly better than baseline (p&lt;0.03). w ch pw en is also significantly better than baseline (p&lt;0.04). By comparing table 6 with table 5, we can see that unpacked pseudo-word performs comparatively with pseudo-word without un-packing. There is no statistical difference be-tween them. It shows that the improvement de-rives from pseudo-word itself as basic transla-tional unit, does not rely very much on higher language model order or longer max phrase length setting. In fact, slight improvement in pw ch pw en and pw ch w en is seen after pseudo-word unpacking, which indicates that higher language model order and longer max phrase length im-pact the performance in these two configurations. 3.5 Comparison to English Chunking English chunking is experimented to compare with pseudo-word. We use FlexCRFs (Xuan-Hieu Phan et al., 2005) to get English chunks. Since there is no standard Chinese chunking data and code, only English chunking is executed. The experimental results show that English chunking performs far below baseline, usually 8 absolute BLEU points below. It shows that sim-ple chunks are not suitable for being basic trans-lational units. We have presented pseudo-word as a novel ma-chine translational unit for phrase-based machine translation. It is proposed to replace too fine-grained word as basic translational unit. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecu-tive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in poly-nomial time. Experimental results of Chinese-to-English translation task show that, in phrase-based machine translation model, pseudo-word performs significantly better than word in both spoken language transl ation domain and news domain. Removing the power of higher order language model and longer max phrase length, which are inherent in pseudo-words, shows that pseudo-words still improve translational per-formance significantly over unary words. S. Banerjee, and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with im-proved correlation with human judgments. In P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009. A Gibbs Sampler for Phrasal Synchronous Grammar Induction. In Proceedings of ACL-P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian synchronous grammar induction. In Proceed-
Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computa-
Optimizing Chinese word segmentation for machine translation performance. In Proceed-Chen, Stanley F. and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, C. Cherry, D. Lin. 2007. Inversion transduction grammar for joint phrasal translation model-ing. In Proc. of the HLTNAACL Workshop on D. Chiang. 2007. Hierarchical phrase-based translation . Computational Linguistics, 33(2):201 X  Y. Deng and W. Byrne. 2005. HMM word and phrase alignment for statistical machine trans-lation. In Proc. of HLT-EMNLP, pages 169 X 176. G. Doddington. 2002. Automatic evaluation of ma-chine translation quality using n-gram cooc-currence statistics. In Proceedings of the 2nd In-Kneser, Reinhard and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In 
E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrasebased translation. In Proc. of the 3rd In-P. Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceed-P. Lambert and R. Banchs. 2005. Data Inferred 
Multi-word Expressions for Statistical Ma-chine Translation. In Proceedings of MT Summit Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrap-ping word alignment via word packing. In Pro-Y. Ma, and A. Way. 2009. Bilingually Motivated Word Segmentation for Statistical Machine 
Translation. In ACM Transactions on Asian Lan-D. Marcu,W.Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of the 2002 Conference on F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, F. J. Och and H. Ney. 2003. A systematic compari-son of various statistical alignment models. Nguyen. 2005. FlexCRFs: Flexible Conditional 
Random Field Toolkit, http://flexcrfs.sourceforge. K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, 2001. M. Paul, 2008. Overview of the IWSLT 2008 evaluation campaign. In Proc. of Internationa A. Stolcke. (2002). SRILM -an extensible lan-guage modeling toolkit. In Proceedings of D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377 X  J. Xu, Zens., and H. Ney. 2004. Do we need Chi-nese word segmentation for statistical ma-chine translation? In Proceedings of the ACL 
Bayesian semi-supervised chinese word seg-mentation for statistical machine translation.
Bayesian learning of non-compositional phrases with synchronous parsing. In Proc. of R. Zhang, K. Yasuda, and E. Sumita. 2008. Improved statistical machine translation by multiple 
Chinese word segmentation. In Proceedings of 
