 Before deciding to buy a product, many people tend to consult others X  opinions on it. Web provides a perfect platform which one can get information. Many customers record their comments on products on the Websites, forums or blogs. Reading the comments, one concerned about a product can find out its main advantages and dis-advantages. However, only few comments cannot give a convincing suggestion and persons do not have sufficient energy to browse more. Therefore, how to automati-cally manage the numerous comments and suggest the potential customers is becom-ing a research hotspot recently. The main target is mining the customers X  opinions to the products. The opinions are classified into positive, negative and neutral. Naturally, customers prefer the products with more positive comments than those with more negative ones. 
There are three levels for such opinion mining task: document level, product level and feature level. To the document level, a whole document only generates a single opinion, which is coarse and inaccurate. Because one document may contain not only an opinion polarity for a given product or a brand. Since nothing can be consummate, there are always some advantages and disadvantages for a product. So feature level customers understand the product more clearly and definitely. Feature stands for an attribute or a component of a product. For example:  X   X  X  X  X  X  X  (standby time) X  is an attribute of a mobile telephone, while  X   X  X  X  X  (screen) X  is a component, both of them can be named as  X  feature words  X . In the examples following,  X  X he standby time of Nokia is long enough X  expresses a positive opinion while  X  X he screen is too small X  expresses a negative one. 
The standby time of Nokia is long enough. Ex. 1  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X  X  X  .

Therefore, to analyze a comment, we need to find the feature words the document contains and the opinion words that embellish the feature words. By confirming the polarity of the opinion words, the main viewpoint of the comment is discovered. With so many comments owning clear polarity of opinions for a given product, customers can easily get to know the product deeply. 
We construct a sentiment resource for a given domain to offer efficient and effective utilities to analyze the comments. The sentiment resource can be considered as a dic-tionary that contains a list of feature words and several opinion words with polarity tag words and opinion words. The polarity of opinion words determination is taken as words of the product, the words are not abundant. Web users sometimes take informal words to describe a feature. Therefore, mining the feature words is necessary. 
There are three benefits to construct such a dictionary beforehand. First, after hold-feature words and most opinion words from the new comment through matching the items in the dictionary quickly. Computing the polarity of the opinion words for every new comments offline will save much time. Second, processing a large set of com-ments to find the feature words and opinion words can not only use the NLP (Natural Language Processing) techniques but also the statistical characteristics. It gets a better performance than processing comments one by one online. Supposing that we do not have the dictionary, when we get a new comment to process, we hardly take any sta-tistical characteristics to make the performance better. Third, the dictionary is easy to maintain. When a new feature word or a new opinion word discovered, it can be easily added into the resource. 
We use both NLP technique and statistical method to extract the feature words and the opinion words. Through our life experiences, the feature words are usually nouns or noun phrases, while the opinion words are usually adjectives. Tagging part-of-some patterns to extract the target words with certain natural language characteristics. teristics of the words in the comments and in the whole Web background corpus, most noises can be removed. To get a higher recall, we also use unknown word finding techniques to extract more feature words. After filtering the noises and adding the new feature words, the performance is improved significantly in term of f-measure. 
After describing about the related works in section 2, the data set and the main al-gorithm are introduced in section 3 and section 4 respectively, including feature words extraction, unknown word finding, and the usage of the background corpus, et al. Section 5 shows the evaluation of the performance. The conclusion and the future work are discussed in the last section. There are several researches to opinion mining for the product. The work in [2, 3] are based on document level. Feature level opinion mining also gains the interesting of researchers [4-11]. How to get feature words more exactly is the main problem in this task. 
M. Hu and B. Liu [4, 5, 6] used association rule mining to find all frequent itemsets which are sets of words or phrases that occur together. CBA which was based on the Apriori algorithm is used. The words and the phrases extracted are considered as feature words. After two pruning phases (compactness pruning and redundancy prun-ing) to increase the precision and an infreq uent feature identification phase to increase the recall, they got a good performance finally. A. Popescu and O. Etzioni [7] built a system named OPINE. It is built on top of KnowItAll which was a Web-based domain-independent information extraction sys-tem. The system first extracts noun phrase and then filtered with point-wise mutual information value between the phrase and meronymy discriminators associated with the product class. They increased both precision and recall compared with M. Hu [6]. 
J. Yi and W. Niblack [8] extracted definite base noun phrases at the beginning of sentences followed by a verb phrase as the feature words. A definite base noun phrase method was called bBNP (Beginning definite Base Noun Phrases) heuristic. To filter appeared more in the documents focused on the product than in the ones did not were kept, while others were removed. They only used precision to evaluate their method and got a very high precision at the top 20 feature words. 
C. Scaffidi, K. Bierhoff and et al. [9] considered noun and two nouns that occur successively as the feature word candidates. Comparing with the random section of English text, the feature words often occurred far more frequently in the comment text. Using the distribution of the words in the random section of English text to com-pute the probability that it occurred n times in the comment text, the less the probabil-ity was, the more possible it was a feature word. The Red Opal system can return quite high precision when few feature words returned. There are also some researchers on feature extraction in Chinese. B. Wang and H. Wang [10] created a bootstrapping method to extract feature words and opinion words in Chinese product comments. They used only a few manual tagged training data to characteristics, such as  X  X s there an adverb in the right X . The algorithm is iterative; the terms tagged by former round were added to training set to train the latter round clas-sifier. The experiment indicated that few manual tagged data can also bring high performance on both feature words extraction and opinion words extracting. 
Q. Su, X. Xu and et al. [11] also used noun and noun phrase (two or more adjacent in Chinese comments to help to filter noises. But there are some Chinese own bound-ary indicators such as  X   X  (of) X . They clustered the feature words and the opinion words respectively to eliminate the problem that hard to mine the implicit feature words. The feature words embellished by the same opinion word may be clustered together to stand for a feature. 
Our work is different at these points: first, we also employ verb and verb phrase to characteristic of candidates on background co rpus to filter noises; third, we use unknown word finding techniques to extract more feature words. The experiment was built on mobile telephone domain in Chinese. We gathered comments from 2 web sites: http://www.bibifa.com/ and http://dp.cnmo.com/. There are totally 6405 comments (6.18 MB plain text data). The average length of the com-ments is 994 byte (nearly 500 Chinese characters). There are two types of the data in each comment. One is tagged by the customer with  X  X dvantage X  or  X  X isadvantage X , while the sentences are short but have strong sentiment. The other is descriptive text. 
Unlike English, Chinese does not make use of any white space characters between words. Therefore, we should segment the sentence into words by a tool ICTCLAS example. The tags  X  X  X   X  X  X   X  X  X  stand for noun, adverb and adjective respectively. Original text:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Ex. 3 Segmented text:  X  X  X  /n  X  /d  X  /a  X  /wd  X  X  X  /n  X  X  X  /n  X  /d  X  /a  X  /wt Ex. 4
Translation text: The profile is too big. The battery performance is poor. 4.1 Feature Words Extraction We use both NLP techniques and statistic methods to extract the feature words. First, using NLP techniques to acquire the feature words candidates, and then filter the noises with statistic characteristics. 
In most of the researches, only nouns and noun phrases are extracted as the feature word candidates. But taking the particularity of Chinese into account, sometimes a verb can also be regarded as a feature word. It is not so much far to find an example (Ex. 5).  X  X  X  /v  X  X  /a  X  /wd  X  X  X  /vi  X  /a 
The words  X   X  X  X  (operating) X  and  X   X  X  X  (reaction) X  are both verbs, although they are nouns in English translation. However, postulating them as feature words is prop-erty. Therefore, for extracting feature words with more coverage, we use four patterns shown in Table 1. 
We can establish such an assumption that the term has a higher probability to be a feature word when it occurs more times in th e comments. It is fairly explicit, because the main features of the product must be the hotspots of the customer discussions. Obviously, not all the nouns, verbs and noun phrases are feature words. Extracting all such terms must bring lots of noises. We can simply remove the terms with low oc-currence frequency to filter the noises, but it is not very effective. So we employ three other filter methods to remove the noises: 1) Import a rule that there should be an adjective on the right of the feature word. 2) Use the frequency of the terms in the background corpus. 3) Use the unknown word finding techniques. 4.1.1 Adjective Rule tionary, if no opinion words appear with the feature word, the feature word is not very valuable in our resource. And the most syntax that the customers use to express their opinion is  X  X eature word + adjective X . (Ex.1 Ex.3 Ex.4 and Ex. 5) This syntax is easy to express an opinion and accords to oral language. While the comments on the Web adverb between the feature word and the ad jective. Therefore, before extracting the feature words, stopwords and adverbs should be removed first. The adjective rule can be defined as: if in all the comments there is not any adjective occurs on right of the dates without an adjective on the right are removed to aspire after a higher precision. words. 4.1.2 Background Corpus Because the comments we use to mining feature words are from the Web, it is ineluc-table that many common words that occur everywhere on the Web are mistakenly extracted as feature words. For example:  X   X  X  X  (Website) X ,  X   X  X  X  (people) X . If the feature word candidate occurs too often on the whole Web text, we can doubt it is a true feature word for the domain we are concerned about justly. We regard the whole Web text as a background corpus. To get the occurrence frequency of the terms, So-gou Lab Internet Vocabulary [13] is used. The Internet Vocabulary is from the statis-tic analysis of the Chinese Web corpus indexed by Sogou search engine (http://www.sogou.com/) in October 2006. It is related to over 100 million Web pages containing more than 150,000 words with high frequency. The vocabulary gives the POS tag and occurrence frequency for each word. The common words that may be background corpus to filter the single nouns or verbs. 
We define a feature named TFP ( term frequency proportion ) as follow: mine the feature words, N is the occurrence frequency in the background corpus given by the Internet Vocabulary. With the higher TFP, the candidate has more probability to be a true feature word. For example, the word  X   X  X  X  (life) X  occurs in the comments 168 times and 251,581,894 times in the background corpus, while  X   X  X  X  (murmur) X  occurs 151 times in the comments and 856,288 times in the background corpus.  X   X  X  X  (murmur) X  has a higher TFP value (11.05) than  X   X  X  X  (life) X  (8.69). Although  X   X  X  X  (murmur) X  appears word. Therefore we select a threshold to confine the terms that have low TFP value. If TFP is less than the threshold, the term will be removed from the candidate set. 4.1.3 Unknown Word Finding Technique As we use nouns, verbs and noun phrase to be the feature word candidates, the poten-mistake of Chinese word segmentation will cause the missing more familiarly. The be segmented correctly by the common segmentation tool. It can influence the performance of feature word extraction. For example, the word  X   X  X  X  (Bluetooth) X  is segmented as  X   X  (blue)/a  X  (tooth)/n X , not  X   X  X  X  (Bluetooth)/n X . Although it is a feature word, it cannot be extracted from the comments. To solve this problem, we employ an unknown word finding technique. 
Z. Luo and R. Song [14] use context-entropy to find the unknown words. Accord-ing to their investigation,  X  X ignificant terms in specific collection of texts can be used almost locates in its corresponding upper string (that is, in fixed context) even through it occur frequently. X  We select the feature word  X   X  X  X  (Bluetooth) X  to explain this. Fig.1 shows the contexts of  X   X  X  X  (Bluetooth) X  and  X   X  (tooth) X . We can see on the left of  X   X  X  X  (Bluetooth) X , the contexts are various, while on the left of  X   X  (tooth) X  the contexts are almost only  X   X  (blue) X  which is dominating. It means that in the com-ments of mobile telephone the word  X   X  (tooth) X  is just a substring of the word  X   X  X  X  (Bluetooth) X .  X   X  X  X  (Bluetooth) X  can stand for a significant term. 
To scale the chaos degree of the various contexts, left-context-entropy and right-context-entropy are defined. Assume  X  as a term which appears n times in the corpus,  X  corpus. Left-context-entropy and right-context-entropy of  X  can be defined as: count of co-occurrence of  X  and b i . 
We consider a feature word candidate joint with the word occurred on its left in the comments as a joint word. The difference of the left-context-entropy between original feature word candidate and the joint word can used to judge if the candidate is only a substring of the joint word which can be a true feature word. If the LCE of the origi-nal candidate is low, and the LCE of the joint word is higher, we can regard that the joint word is a feature word more possibly than the original candidate. We can select an LCE threshold; if the LCE of the joint word is over threshold greater than the LCE candidate set. The reason why we do not use the right-context-entropy is we have already employed the adjective rule on th e right of the candidate. The candidate cannot joint the right term. 4.2 Opinion Words Extraction In this task, we regarded only adjectives as opinion words. This step is simple. When extracting the feature words, the opinion wo rds are also generated. The opinion words are extracted when we use adjective rule to filter the feature words noises. The candi-date with an adjective on its right is cons idered as a feature word, while the adjective word must be along with a feature word to express a polarity. Therefore, an adjective without a feature word on the left is not a significant opinion word. We manually pick up the feature words from the comments to organize the test data created by the algorithm is signed as RESULT. Precision, recall and f-measure defined as follow are used. The performance of only using the patterns and adjective rule is shown in Fig. 2. Totally 2596 feature words have been extracted. Ranking the feature words by their are true feature words, which is in accordan ce with the assumption we take before in section 4.1. The recall curve in Fig. 2 is nearly linear, which indicates that the occur-rence frequency of true feature words is nearly uniform distribution in the comments. Cutting the words whose occurrence frequenc y is below any threshold to maintain a high precision may depress the recall markedly. This can be proved by the f-measure curve; when all the results are kept, the f-measure can get the highest value. Fig. 3 shows the performance that uses the background corpus to filter the noises. The result feature word set is ranking the feature words by their TFP value descend-the performance without filtering (ref. Fig. 2), the precision curve and the recall curve are both different. The precision curve is gent ly at the top, which indicates that most of the words with high TFP value are true feature words. They are most the phrases. The recall curve is gently at the bottom of the result list, which indicates most of the words with low TFP value are not true feature words, and cutting them will not influ-ence the recall badly. There is a maximum point in the f-measure curve; keeping the The words with low TFP value are removed. compare the performance between not using verbs and verb phrases, not using verbs and using all. The precision, recall and f-measure in Fig. 4 are the highest performance after filtering noises. The gray vertical bar stands for the performance of only using nouns and noun phrases (nouns + nouns) as candidates. It is the lowest among the three. When we added the verb phrases (verbs + nouns) pattern, the performance increases evidently. The patterns is a little better than the white at f-measure and recall, which indicates that using verbs as feature words can extract more, but the noises are growing as well. 
Next, we evaluate the validity of the unknown word finding technique. We control many terms will be replaced, and then the precision will become lower. Fig. 5 illustrates the curves with the LCE threshold increasing. The horizontal axis stands for the threshold. When the threshold is low, many joint words with a low LCE may replace the original candidate, which makes the precision low. However, when the original candidates which are not. The reca ll is decreasing. When we select a proper LCE threshold 0.8, a higher f-measure 0.7407 is achieved. 
Table 2 gives some examples for the unknown word finding technique using. The original candidates are not complete attrib ute or component of the product in the spe-cific domain (Some of them do not have corresponding English translation in the domain context, which are marked as N/A in the table). We aim at automatically constructing the sentiment resource by mining customers X  product comments. The sentiment resource can be considered as a dictionary for a utes or components of a product. For each feature word, there are several opinion words with polarity tag. The opinion words contain sentiment. The main contribution of this paper is extracting the feature words and the opinion words. Both NLP tech-nique and statistical method are applied. 
We use part-of-speech information and natural language patterns to extract the The first step is based on the characteristic of natural language and the comments on the Web. The adjective restriction rule is shown to enhance the performance. The Web text. The common words that occur too often on the Web text are regarded with low probability to be a feature word. And the unknown word finding technique also method is helpful in the task of extracting feature words. The use of statistical charac-teristic can be taken in the process of sentiment resource constructing. While dealing with a single comment, no statistical characteristic can be used. That is why we emphasize the importance and validity of resource construction. 
Furthermore, although the experiment is built on the comments of mobile tele-phone domain in Chinese, the algorithm does not refer to any information of the domain. So it is a domain independent. 
The future work can be expanded as the following: 1) For the aspect of feature word extr action, the more agile background corpus might be used to improve the timeliness of the vocabulary. 2) Polarity determination of opinion word will be studied in the future. For exam-ple, using the existing nature language tools such as HowNet [1] and considering the context of the opinion words, we can tag the polarity of the opinion words. 3) For the aspect of application of the sentiment resource, how to use the resource to analyze the comments is also an interesting task. 
