 human, largely due to the expressive deep architecture employed by human visual cortex systems. visual patterns than shallow models, because they are considered biologically plausible. have recently proposed training methods by using unlabeled data. These methods perform a greedy shallow models.
 Beside using unlabeled data, in this paper we tackle the problem by leveraging additional prior knowledge . In the last few decades, researchers have developed successful kernel-based systems an extremely valuable source of prior knowledge, which we believe should be exploited in deep to train DNNs with prior knowledge about the recognition task.
 set of auxiliary functions automatically constructed from the kernel function. We apply the described method to train convolutional neural networks (CNNs) for a wide range of of CNNs by a large margin when the training set is small or the learning problem is difficult. examples [( x where  X  the output layer,  X  ularization. Applying the well-known representor theorem, we derive the equivalence to a kernel where the kernel is computed by We assume the network is provided with some prior knowledge, in the form of an m  X  m kernel n . We exploit this prior knowledge via imposing a kernel regularization on K (  X  ) = [ K that the learning problem seeks Problem 2.1. where  X  &gt; 0 and  X (  X  ) is defined by unique minimum at K =  X  if  X  0 , suggesting that minimizing  X (  X  ) encourages K to approach  X  D
KL [p( f |  X ) k p( f | K )] The learning problem in Eq. (3) can be solved by using gradient-based methods. In this paper we emphasize large-scale optimizations using stochastic gradient descent (SGD), because the method facto method to train neural networks for large-scale learning tasks.
 SGD considers the problem where the optimization cost is the sum of the local cost of each indi-vidual training example. A standard batch gradient descent updates the model parameters by using the gradient caused by a single random training example. Therefore, the parameters of the model gradient descent.
 3.1 Shrinkage on the Kernel Matrix where  X  = [  X  then turn the kernel regularization into later development. 3.2 Transformation of the Log-determinant Term By noticing that  X  &gt;  X  = P n of the log determinant for the convenience of SGD.
 Theorem 3.1. Consider min function of  X  , if its local minimum w.r.t.  X  exists, then the problem is equivalent to where g  X  (  X  ) is the conjugate function of g ( a ) , i.e. g  X  (  X  ) = min i.e., g ( a ) = min minimizing L (  X  ) w.r.t.  X  .
 of log det( A ) is log det( X ) + q . We can use the above theorem to transform any loss function Therefore the log-determinant in Eq. (5) is turned into a variational representation where  X   X  S + is a convex function of auxiliary variables  X  and more importantly, it amounts to a sum of local quantities caused by each of the m data examples. 3.3 Transformation of the Trace Term We assume that the kernel matrix  X  is presented in a decomposed form  X  = UU &gt; , with U = [ u problem by introducing an q  X  p auxiliary variable matrix  X  .
 Proposition 3.1. The trace term in Eq. (5) is equivalent to a convex variational representation Proof. We first obtain the analytical solution  X   X  = 1  X  representation reaches its unique minimum. Then, plugging it back into the function, we have where the last step is derived by applying the Woodbury matrix identity.
 over data examples. 3.4 An Equivalent Learning Framework Combining the previous results, we obtain the convex upper bound for the kernel regularization Eq. (5), which amounts to a sum of costs over examples under some regularization
 X (  X  )  X  a unique minimum  X (  X  ) , hence we can replace  X (  X  ) by instead minimizing the upper bound and formulate an equivalent learning problem Clearly this new optimization can be solved by SGD.
 When applying the SGD method, each step based on one example needs to compute the inverse of  X  next result suggests that we can dramatically reduce this complexity from O ( q 3 ) to O ( q ) . Proposition 3.2. Eq. (5) is equivalent to the convex variational problem where  X  = [  X  can pre-restrict  X  to be a diagonal positive definite matrix  X  = diag[  X  change our problem and gives rise to Eq. (8).
 formulate the whole learning problem as Problem 3.1. where L Therefore we intentionally normalize L L term L functions amounts to a summation of local costs caused by individual data examples, the whole learning problem can be conveniently implemented by SGD, as described in Algorithm 1. U can be results of some unsupervised learning (e.g. the self-taught learning [14] based on sparse can be obtained by applying incomplete Cholesky decomposition on an m  X  m kernel matrix  X  . In the third case, when m is so large that the matrix decomposition cannot be computed in the main memory, we apply the Nystr  X  om method [19]: We first randomly sample m such that the computed kernel matrix  X  rank eigenvalue decomposition of  X  and the subset of size m Algorithm 1 Stochastic Gradient Descent with the support vector machine (SVM) using the exactly the same kernel. Throughout all the experiments,  X  X CNN X  denotes CNNs regularized by nonlinear kernels, processed by either Cholesky or Nystr  X  om approximation, with parameters p = 600 , m size of each whole data set. The obtained u 4 recognition tasks based on a 5-fold cross validation using 500 labeled examples. 4.1 Handwritten Digit Recognition on MNIST Dataset The data contains a training set with 60000 examples and a test set with 10000 examples. The CNN RBF kernel, and (2) Graph kernel on 10 nearest neighbor graph [6]. We perform 600 -dimension Cholesky decomposition on the whole 70000  X  70000 graph kernel because it is very sparse. CNNs by leveraging the prior knowledge, and also outperform SVMs that use the same kernels. The 4.2 Gender and Ethnicity Recognition on FRGC Dataset conditions and backgrounds. Beside person identities, each image is annotated with gender and training data 5 times and report the average error rates.
 In this experiment, CNNs operate on images represented by R/G/B planes plus horizontal and ver-borhood. The obtained 16 feature maps of size 25  X  25 are connected to the next layer by 256 filters of size 6  X  6 , with 50% random sparse connections, followed by max pooling within each kernel used in this experiment is the RBF kernel computed directly on images, which has demon-demonstrate that kCNNs significantly boost the recognition accuracy of CNNs for both gender and ethnicity recognition. The difference is prominent when small training sets are presented. 4.3 Object Recognition on Caltech101 Dataset Caltech101 [7] contains 9144 images from 101 object categories and a background category. It is benchmark for object recognition. We follow the common setting to train on 15 and 30 images per recognition accuracy was normalized by class sizes and evaluated over 5 random data splits. The spatial pyramid matching (SPM) kernel developed in [10].
 ularization by SPM kernel, kCNN dramatically improves the accuracy of CNN, and outperforms SVM using the same kernel. This is perhaps the best performance by (trainable and hand-crafted) regularization are visualized in Fig. 1, which helps to understand the difference made by kCNN. consisted of hard-wired Gabor filters, and then a large number of patches were sampled from the classifier.
 his coworkers proposed training deep belief networks with layer-wise unsupervised pre-training, els like CNNs [15], autoassociators [4], and for document coding [16]. In recent work [18], the authors proposed training a deep model jointly with an unsupervised embedding task, which led to approaches, including neighborhood component analysis [8], to further enhance the deep learning. feature mapping, and more directly, our previous work [1], which created pseudo auxiliary tasks based on hand-craft image features to train nonlinear deep networks.
 trained CNN and SVM systems? The reason is computational speed  X  kCNN pays an extra cost to
Figure 1: First-layer filters on the B channel, learned from Caltech101 (30 examples per class) In our Caltech101 experiment, the SVM (SPM) needed several seconds to process a new image on a PC with a 3.0 GHz processor, while kCNN can process about 40 images per second. The latest record on Caltech101 was based on combining multiple kernels [5]. We conjecture that kCNN could be further improved by using multiple kernels without sacrificing recognition speed. To conclude, we proposed using kernels to improve the training of deep models. The approach was the performance of deep models when insufficient labeled data were available in hard recognition problems. The trained model was much faster than kernel systems for making predictions. Acknowledgment : We thank the reviewers and Douglas Gray for helpful comments.
