 Frequent Itemsets and Association Rules Mining (FIM) is a key task in knowledge discovery from data. As the dataset grows, the cost of solving this task is dominated by the component that de-pends on the number of transactions in the dataset. We address this issue by proposing PARMA, a parallel algorithm for the MapRe-duce framework, which scales well with the size of the dataset (as number of transactions) while minimizing data replication and communication cost. PARMA cuts down the dataset-size-dependent part of the cost by using a random sampling approach to FIM. Each machine mines a small random sample of the dataset, of size in-dependent from the dataset size. The results from each machine are then filtered and aggregated to produce a single output collec-tion. The output will be a very close approximation of the collec-tion of Frequent Itemsets (FI X  X ) or Association Rules (AR X  X ) with their frequencies and confidence levels. The quality of the output is probabilistically guaranteed by our analysis to be within the user-specified accuracy and error probability parameters. The sizes of the random samples are independent from the size of the dataset, as is the number of samples. They depend on the user-chosen ac-curacy and error probability parameters and on the parallel compu-tational model. We implemented PARMA in Hadoop MapReduce and show experimentally that it runs faster than previously intro-duced FIM algorithms for the same platform, while 1) scaling al-most linearly, and 2) offering even higher accuracy and confidence than what is guaranteed by the analysis.
 H.2.8 [ Database Management ]: Database Applications X  data min-ing ; H.3.4 [ Systems and Software ]: Distributed Systems Algorithms, Experimentation, Performance, Theory MapReduce, Frequent Itemsets, Association Rules, Sampling
The discovery of (top-K) Frequent Itemsets and Association Rules (FIM) is a fundamental primitive in data mining and databases ap-plications. The computational problem is defined in the general set-ting of a transactional dataset  X  a collection of transactions where each transaction is a set of items. With datasets increasing both in size and complexity, the computation for FIM faces scalability challenges in both space and time. Datasets have now reached the tens of terabytes scale and it is no longer reasonable to assume that such massive amounts of data can be easily processed by a single machine in a sequential fashion.

A typical exact algorithm scans the entire dataset, possibly mul-tiple times, and stores intermediate counts of a large number of possible frequent itemsets candidates [2, 17]. The cost of these al-gorithms can be split in two independent components: the scanning cost and the mining cost. The scanning cost includes all operations that directly handle the transactions in the dataset, and scales with the size of the dataset, i.e., the number of such transactions. Ex-amples include the scanning of the dataset to build the FP-Tree in FP-Growth [17] or to compute the actual frequencies of candidate frequent itemsets in APriori [1]. The mining cost refers to the oper-ations in derived data structures and does not require access to the dataset. Examples include the operations performed on the FP-Tree once it has been generated, and the creation of candidate itemsets of length i + 1 at the end of phase i in APriori. This cost scales with the complexity of the dataset, i.e., the number of items, the number and distribution of frequent itemsets, and the underlying process that generated the transactions. It also depends on parame-ters given to the algorithm, such as the desired frequency threshold.
In this paper we are concerned with the scalability of FIM with respect to the size of the dataset, or the number of transactions. In many practical settings for FIM, the process generating the data changes very slowly or not at all, especially when compared to the data generation rate, therefore the number and frequency distribu-tion of the frequent itemsets grows much slower than the size of the dataset. For example, the number of items available on the cata-log of an e-commerce website grows much slower than the number of purchases by customers, each of which corresponds to a trans-action. Therefore the scanning component grows faster than the mining one, and soon becomes dominant.

We introduce a randomized parallel algorithm for approximate frequent itemset mining, PARMA, that makes the scanning step of FIM embarassingly parallel, thus exhibiting near-linear speedup with the number of machines. PARMA combines random sam-pling and parallelization techniques in a novel fashion. It mines, in parallel, a set of small random samples and then filters and ag-gregates the collections of frequent itemsets or association rules obtained from each sample. Our work is orthogonal to other ap-proaches, like PFP [18], which focuses on parallelizing the min-ing phase in order to decrease the corresponding component of the cost. Due to the use of random sampling, the output of PARMA is an approximation of the collection of FI X  X  or AR X  X  in the dataset, but leveraging on previous work [29], PARMA offers tight prob-abilistic guarantees on the quality of the approximated collections returned in output. In particular it guarantees that the output is an  X  -approximation of the real collection with probability at least 1  X   X  , where  X  and  X  are parameters specified by the user (see Section 3 for formal definitions). PARMA is designed on MapReduce [11], a novel parallel/distributed architecture that has raised significant interest in the research and industry communities. MapReduce is capable of handling very large datasets and efficiently executing parallel algorithms like PARMA.

To our knowledge PARMA is the first algorithm to exploit the combination of random sampling and parallelization for the task of Association Rules Mining. A number of previous works explored either parallel algorithms [4, 8, 12, 13, 22, 25, 30, 34] or random sampling [32, 35, 26, 28, 20, 29] for the FIM task, but the two approaches have been seen somewhat orthogonal until today. In PARMA, the disadvantages of either approach are evened out by the advantages of the other. In the spirit of moving computation to the data to minimize communication, we avoid data replication, and preserve the advantages of parallelization by using of multiple independent small random samples of the dataset which are mined in parallel and have only their results aggregated. Similarly, we are not subject to the inherent trade-off between the size of the random sample and the accuracy of the approximation that can be obtained from it, as PARMA would only have to mine more samples of the same size in parallel to get higher quality approximations.
Although PARMA is not the first algorithm to use MapReduce to solve the Association Rule Mining task, it differs from and en-hances previous works [10, 14, 16, 18, 19, 33, 36] in two crucial aspects. First, it significantly reduces the data that is replicated and transmitted in the shuffle phase of MapReduce. Second, PARMA is not limited to the extraction of Frequent Itemsets but can also di-rectly compute the collection of Association Rules in MapReduce. In previous works, association rules had to be created sequentially after the Frequent Itemsets had been computed in MapReduce.
We conducted an extensive experimental evaluation to test the relative performance, scalability and accuracy of PARMA across a wide range of parameters and datasets. Our results suggest that PARMA can significantly outperform exact mining solutions, has near-linear speedup, and, as data and nodes are scaled together, is able to achieve near constant runtimes. Also, our accuracy evalua-tion shows that PARMA consistently computes approximated col-lections of higher quality than what can be analytically guaranteed.
In this paper: 1. We present PARMA, the first randomized MapReduce al-2. We provide analytical guarantees for the quality of the ap-3. We demonstrate the effectiveness of PARMA on many datasets
The task of mining frequent itemsets is a fundamental primitive in computer science, with applications ranging from market bas-ket analysis to network monitoring. The two most well known al-gorithms for extracting frequent itemsets from a dataset are APri-ori [2] and FP-growth [17].

The use of parallel and/or distributed algorithms for Associa-tion Rules Mining comes from the impossibility to handle very large datasets on a single machine. Early contributions in this area are presented in a survey by Zaki [34]. In recent years, the focus shifted to exploit architecture advantages as much as possible, such as shared memory [30], cluster architecture [4] or the massive par-allelism of GPUs [13]. The main goal is to avoid communication between nodes as much as possible and minimize the amount of data that are moved across the network [8, 12, 22, 25].

The use of sampling to mine an approximation of the frequent itemsets and of association rules is orthogonal to the efforts for parallelizing frequent itemsets mining algorithm, but is driven by the same goal of making the mining of massive datasets possible. It was suggested in [23] almost as soon as the first efficient algo-rithms for Association Rules Mining appeared. Toivonen [32] pre-sented the first algorithm to extract an approximation of the Fre-quent Itemsets using sampling. Many other works used different tools from probability theory to improve the quality of the approx-imations and/or reduce the size of sample. We refer the interested reader to [29] for a review of many contributions in this area.
The MapReduce [11] paradigm enjoys widespread success across both industry and academia. Research communities in many differ-ent fields uses this novel approach to distributed/parallel computa-tion to develop algorithms to solve important problems in computer science [5, 6, 15, 21, 27]. Not only MapReduce can easily perform computation on very large datasets, but it is also extremely suited in executing embarassingly parallel algorithms which make a very limited use of communication. PARMA fits in this description so MapReduce is an appropriate choice for it.

A number of previous works [10, 14, 16, 18, 19, 33, 36] looked at adapting APriori and FP-growth to the MapReduce setting. Some-what naively, some authors [10, 19, 33] suggest a distributed/parallel counting approach, i.e. to compute the support of every itemset in the dataset in a single MapReduce round. This algorithm neces-sarily incurs in a huge data replication, given that an exponential number of messages are sent to the reducers, as each transaction contains a number of itemsets that is exponential in its length. A different adaptation of APriori to MapReduce is presented in [16, Chap.4]: similarly to the original formulation of APriori, at each round i , the support for itemsets of length i is computed, and those that are deemed frequent are then used to generate candidate fre-quent itemsets of length i + 1 , although outside of the MapReduce environment. Apart from this, the major downsides of such ap-proach are that some data replication still occurs, slowing down the shuffling phase, and that the algorithm does not complete until the longest frequent itemset is found. Given that length is not known in advance, the running time of the algorithm can not be computed in advance. Also the entire dataset needs to be scanned at each round, which can be very expensive, even if it is possible to keep additional data structures to speed up this phase.

An adaptation of FP-Growth to MapRreduce called PFP is pre-sented in [18]. First, a parallel/distributed counting approach is used to compute the frequent items, which are then randomly par-titioned into groups. Then, in a single MapReduce round the trans-actions in the dataset are used to generate group-dependent trans-actions. Each group is assigned to a reducer and the corresponding group-dependent transactions are sent to this reducer which then builds the local FP-tree and the conditional FP-trees recursively, computing the frequent patterns. The group-dependent transactions are such that the local FP-trees and the conditional FP-trees built by different reducers are independent. This algorithm suffers from a data replication problem: the number of group-dependent transac-tions generated for each single transaction is potentially equal to the number of groups. This means that the dataset may be replicated up to a number of times equal to the number of groups, resulting in a huge amount of data to be sent to the reducers and therefore in a slower synchronization/communication ( shuffle ) phase, which is usually the most expensive in a MapReduce algorithm. Another practical downside of PFP is that the time needed to mine the de-pendent FP-tree is not uniform across the groups. An empirical solution to this load balancing problem is presented in [36], al-though with no guarantees and by computing the groups outside the MapReduce environment. An implementation of the PFP algo-rithm as presented in [18] is included in Apache Mahout [3].
The authors of [14] presents an high-level library to perform var-ious machine learning and data mining tasks using MapReduce. They show how to implement the Frequent Itemset Mining task us-ing their library. The approach is very similar to that in [18], and the same observations apply about the performances and downsides of this approach.
A dataset D is a collection of transactions , where each trans-action  X  is a subset of a ground set (alphabet) I . There can be multiple identical transactions in D . Members of I are called items and members of 2 I are called itemsets . Given an itemset A  X  2 let T D ( A ) denote the set of transactions in D that contain A . The support of A ,  X  D ( A ) = | T D ( A ) | , is the number of transaction in D that contains A , and the frequency of A , f D ( A ) = | T the fraction of transactions in D that contain A .
 A task of major interest in this setting is finding the Frequent Itemsets with respect to a minimum frequency threshold .
Definition 1. Given a minimum frequency threshold  X  , 0 &lt;  X   X  1 , the Frequent Itemsets mining task with respect to  X  is finding all itemsets with frequency  X   X  , i.e., the set
For the top-K Frequent Itemsets definition we assume a fixed canonical ordering of the itemsets in 2 I by decreasing frequency in D , with ties broken arbitrarily. We label the itemsets A according to this ordering and denote with f ( K ) D the frequency f of the K -th most frequent itemset A K . For a given K , with 1  X  K  X  m , the set of top-K Frequent Itemsets (with their respective frequencies) is defined as
One of the main uses of frequent itemsets is in the discovery of association rules .

Definition 2. An association rule W is an expression  X  A  X  B  X  where A and B are itemsets such that A  X  B =  X  . The support  X  ( W ) (resp. frequency f D ( W ) ) of the association rule W is the support (resp. frequency) of the itemset A  X  B . The confidence c the frequency of A .

Given a minimum frequency threshold  X  and a minimum confi-dence level  X  , we define the set AR ( D , I , X , X  ) of triplets s. t. W is an association rule with f D ( W )  X   X  and c D
In this work we are interested in computing well defined approx-imations of the above sets.

Definition 3. Given two parameters  X  1 , X  2  X  (0 , 1) , an (  X  approximation of FI ( D , I , X  ) is a set C = { ( A,f A , K 2 ,f A  X  K A  X  [0 , 1] } of triplets ( A,f A , K A ) where f imates f D ( A ) and K A is an interval containing f A and f is such that: 1. C contains all itemsets appearing in FI ( D , I , X  ) ; 2. C contains no itemset A with frequency f D ( A ) &lt;  X   X   X  3. For every triplet ( A,f A , K A )  X  X  , it holds If  X  1 =  X  2 =  X  we refer to C as a  X  -approximation of FI ( D , I , X  ) . This definition extends easily to the case of top-K frequent itemsets mining using the equivalence from (1). An (  X  1 , X  2 ) -approximation to FI D , I ,f ( K ) D is an (  X  1 , X  2 ) -approximation to TOPK ( D , I ,K ) . For association rules, we have the following definition.
Definition 4. Given two parameters  X  1 , X  2  X  (0 , 1) an (  X  approximation of AR ( D , I , X , X  ) is a set
C = { ( W,f W , K W ,c W , J W ) | AR W,f W  X  X  W ,c W  X  X  W of tuples ( W,f W , K W ,c W , J W ) where f W and c W approximate f ( W ) and c D ( W ) respectively and belong to K W  X  [0 , 1] and J
W  X  [0 , 1] respectively. C is such that: 1. C contains all association rules appearing in AR ( D , I , X , X  ) ; 2. C contains no association rule W with frequency f D ( W ) &lt; 3. For every tuple ( W,f W , K W ,c W , J W )  X  X  , it holds | f 4. C contains no association rule W with confidence c D ( W ) &lt; 5. For every tuple ( W,f W , K W ,c W , J W )  X  X  , it holds | c If  X  1 =  X  2 =  X  we refer to C as an  X  -approximation of AR ( D , I , X , X  ) .
The following result from [29] is at the core of our algorithm for computing an  X  -approximation to FI ( D , I , X  ) . A similar result also holds for TOPK ( D , I ,K ) [29, Lemma 3].

L EMMA 1. [29, Lemma 1] Let D be a dataset with transactions built on an alphabet I , and let d be the maximum integer such that D contains at least d transactions of size at least d . Let 0 &lt;  X , X , X  &lt; 1 . Let S be a random sample of D containing |S| = random with replacement from those in D , then with probability at least 1  X   X  , the set FI ( S , I , X   X   X  2 ) is a (  X , X / 2) -approximation of FI ( D , I , X  ) .
 For computing a  X  -approximation to AR ( D , I , X  ) , we make use of the following Lemma.

L EMMA 2. [29, Lemma 6] Let D be a dataset with transactions built on an alphabet I , and let d be the maximum integer such that D contains at least d transactions of size at least d . Let 0 &lt;  X , X , X , X  &lt; 1 and let  X  rel =  X  max {  X , X  } . Fix c &gt; 4  X  2  X  c , and p = and uniformly at random. Then AR ( S , I , (1  X   X  )  X , 1  X   X  (  X , X / 2) approximation to AR ( D , I , X , X  ) .
MapReduce is a programming paradigm and an associated par-allel and distributed implementation for developing and executing parallel algorithms to process massive datasets on clusters of com-modity machines [11]. Algorithms are specified in MapReduce us-ing two functions, map and reduce . The input is seen as a se-quence of ordered key-value pairs ( k,v ) . The map function takes as input one such ( key,value ) pair at a time, and can produce a tiset union of all the multisets produced by the map function when applied to all input pairs. We can partition U into sets U by a particular key  X  k . U  X  k contains all and only the values v for which there are pairs (  X  k,v ) with key  X  k produced by the function map ( U  X  k is a multiset, so a particular value v can appear multi-ple times in U  X  k ). The reduce function takes as input a key the multiset U  X  k and produce another set { ( k 1 ,v 1 ) , ( k The output of reduce can be used as input for another (different) map function to develop MapReduce algorithms that complete in multiple rounds . By definition, the map function can be executed in parallel for each input pair. In the same way, the computation of the output of reduce for a specific key k  X  is independent from the computation for any other key k 0 6 = k  X  , so multiple copies of the reduce function can be executed in parallel, one for each key k . We denote the machines executing the map function as map-pers and those executing the reduce function as reducers . The latter will be indexed by the key k assigned to them, i.e., reducer r processes the multiset U r . The data produced by the mappers are split by key and sent to the reducers in the so-called shuffle step. Some implementations, including Hadoop and the one described by Google [11], use sorting in the shuffle step to perform the grouping of map outputs by key. The shuffle is transparent to the algorithm designer but, since it involves the transmission of (possibly very large amount of) data across the network, can be very expensive.
In this section we describe and analyze PARMA, our algorithm for extracting  X  -approximations of FI ( D , I , X  ) , TOPK ( D , I , X  ) , and AR ( D , I , X , X  ) from samples of a dataset D with probability at least 1  X   X  . In this section we present the variant for FI ( D , I , X  ) . The variants for the cases of TOPK ( D , I , X  ) and AR ( D , I , X , X  ) can be easily derived from the one we present here. We outline them in Section 4.4. Detailed presentations for them will appear in the full version of the paper.
We now present the algorithmic design framework on which we developed PARMA and some design decisions we made for speed-ing up the computation.
 Model. When developing solutions for any computational prob-lem, the algorithm designer must always be aware of the trade-off between the available computational resources and the performance (broadly defined) of the algorithm. In the parallel computation set-ting, the resources are usually modeled through the parameters p and m , representing respectively the number of available proces-sors that can run in parallel and the amount of local memory avail-able to a single processor. In our case we will express m in terms of the number of transactions that can be stored in the main mem-ory of a single machine. When dealing with algorithms that use random samples of the input, the performances of the algorithm are usually measured through the parameters  X  and  X  . The former represents the desired accuracy of the results, i.e., the maximum tolerable error (defined according to some distance measure) in the solution computed by the algorithm using the random sample when compared to an exact solution of the computational problem. The parameter  X  represents the maximum acceptable probability that the previously defined error in the solution computed by the algo-rithm is greater than  X  . The measure we will use to evaluate the performances of PARMA in our analysis is based on the concept of  X  -approximation introduced in Definition 3.
 Trade-offs. We are presented with a trade-off between the pa-rameters  X  ,  X  , p , and m . To obtain a  X  -approximation with proba-bility at least 1  X   X  , one must have a certain amount of computa-tional resources, expressed by p and m . On the other hand, given p and m , it is possible to obtain  X  -approximations with probability at least 1  X   X  only for values of  X  and  X  larger than some limits. By fix-ing any three of the parameters, it is possible to find the best value for the fourth by solving an optimization problem. From Lemma 1 we know that there is a trade-off between  X  ,  X  , and the size w of a random sample from which it is possible to extract a (  X , X / 2) -approximation to FI ( D , I , X  ) with probability at least 1  X   X  . If w  X  m , then we can store the sample in a single machine and compute the  X  -approximation there using Lemma 1. For some com-binations of values for  X  and  X  , though, we may have that w &gt; m , i.e. the sample would be too large to fit into the main memory of a single processor, defeating one of the goals of using random sam-pling, that is to store the set of transactions to be mined in main memory in order to avoid expensive disk accesses. To address the issue of a single sample not fitting in memory, PARMA works on multiple samples, say N with N  X  p , each of size w  X  m so that 1) each sample fits in the main memory of a single processor and 2) for each sample, it is possible to extract an (  X , X / 2) -approximation of FI ( D , I , X  ) from it with probability at least 1  X   X  , for some  X  &gt;  X  . In the first stage, the samples are created and mined in par-allel and the so-obtained collections of Frequent Itemset are then aggregated in a second stage to compute the final output. This ap-proach is a perfect match for the MapReduce framework, given the limited number of synchronization and communication steps that are needed. Each stage is performed in a single MapReduce round. The computational and data workflow of PARMA is presented in Figure 1, which we describe in detail in the following paragraphs. Computing N and w . From the above discussion it should be clear that, once p , m ,  X  and  X  have been fixed, there is a trade-off between w and N . In the MapReduce setting, often the most ex-pensive operation is the movement of data between the mappers and the reducers in the shuffle phase. In PARMA, the amount of data to be shuffled corresponds to the sum of the sizes of the samples, i.e., Nw , and to the amount of communication needed in the aggre-gation stage. This second quantity is dependent on the number of frequent itemsets in the dataset, and therefore PARMA has no con-trol over it. PARMA tries to minimize the first quantity when com-puting N and w in order to achieve the maximum speed. It is still possible to minimize for other quantities (e.g.  X  or  X  if they have not been fixed), but we believe the most effective and natural in the MapReduce setting is the minimization of the communication. This intuition was verified in our experimental evaluation, where com-munication proved to be the dominant cost. We can formulate the problem of minimizing Nw as the following Mixed Integer Non Linear Programming (MINLP) problem: Figure 1: A system overview of PARMA. Ellipses represent data, squares represent computations on that data and arrows show the movement of data through the system. Note that, because of our requirement 2) on w , the sample size w is directly determined by  X  through Lemma 1, so the trade-off is really between N and  X  , while w does not appear in the above problem. Since  X  is a probability we restrict its domain to the inter-val (0 , 1) , but it must also be such that the single sample size w is at most m , as required by 1) and expressed by Constraint (3). The limit to the number of samples N is expressed by Constraint (2). The last constraint (4) is a bit more technical and the need for it will be evident in the analysis of the algorithm. Intuitively, it expresses the fact that an itemset must appear in a sufficiently high fraction (at least 1/2, possibly more) of the collections obtained from the sam-ples in the first stage in order to be included in the output collection. Due to the integrality constraint on N , this optimization problem is not convex, although when the constraint it is dropped the feasi-bility region is convex, and the objective function is convex. It is then relatively easy and fast to find an integer optimal solution to the problem using a global MINLP solver like BARON [31]. In the following paragraphs we give a detailed description of PARMA. The reader is also referred to Figure 1 for a schematic representation of PARMA X  X  data/computational workflow.
 have been computed, PARMA enters the first MapReduce round to create the N samples (phase Map 1 in Figure 1) and mine them (Reduce 1). We see the input of the algorithm as a sequence where the  X  i are transactions in D . In the Map phase, the input of the map function is a pair ( tid, X  ) , where tid is a natural from 1 to |D| and  X  is a transaction in D . The map function produces in out-put a pair ( i, ( ` ( i )  X  , X  )) for each sample S i containing  X  . The value `  X  denotes the number of times  X  appears in S i , 1  X  i  X  N . We use random sampling with replacement and ensure that all samples have size w , i.e., P  X   X  X  ` ( i )  X  = w ,  X  i . This is done by computing (serially) how many transactions each mapper must send to each sample. In the Reduce phase, there are N reducers, with associated key i , 1  X  i  X  N . The input to reducer i is ( i, S i ) , 1  X  i  X  N . Reducer i mines the set S i of transactions it receives using an ex-act sequential mining algorithm like Apriori or FP-Growth and a lowered minimum frequency threshold  X  0 =  X   X   X / 2 to obtain C = FI ( S i , I , X  0 ) . For each itemset A  X  C i the Reduce function outputs a pair ( A, ( f S i ( A ) , [ f S i ( A )  X   X / 2 ,f Stage 2: Aggregation. In the second round of MapReduce, PARMA aggregates the result from the first stage to obtain a  X  -approximation to FI ( D , I , X  ) with probability at least 1  X   X  . The Map phase (Map 2 in Figure 1) is just the identity function, so for each pair in the input the same pair is produced in the output. In the Reduce phase (Reduce 2) there is a reducer for each itemset A that appears in at least one of the collections C j (i.e.,  X  A such that there is a C containing a pair related to A ). The reducer receives as input the itemset A and the set F A of pairs for the samples S i such that A  X  X  i . Now let The itemset A is declared globally frequent and will be present in the output if and only if |F A |  X  R . If this is the case, PARMA computes, during the Reduce phase of the second MapReduce round, the estimation  X  f ( A ) for the frequency f D ( A ) of the itemset A in D and the confidence interval K A . The computation for  X  ceeds as follows. Let [ a A ,b A ] be the shortest interval such that there are at least N  X  R + 1 elements from F A that belong to this interval. The estimation  X  f ( A ) for the frequency f itemset A is the central point of this interval: The confidence interval K A is defined as The output of the reducer assigned to the itemset A is The output of PARMA is the union of the outputs from all reducers. We have the following result: L EMMA 3. The output of the PARMA is an  X  -approximation of FI ( D , I , X  ) with probability at least 1  X   X  .

P ROOF . For each sample S i , 1  X  i  X  N we define a ran-dom variable X i that takes the value 1 if C i = FI ( S i (  X , X / 2) -approximation of FI ( D , I , X  ) , X i = 0 otherwise. Given our choices of w and  X  0 , we can apply Lemma 1 and have that Pr( X i = 1)  X  1  X   X  . Let Y = P N r =1 X r and let Z be a random variable with binomial distribution with parameters N and 1  X   X  . For any constant Q &lt; N (1  X   X  ) we have where the last inequality follows from an application of the Cher-noff bound [24, Chap. 4]. We then have, for our choice of  X  and N and for Q = R (defined in Eq. (5)), that with probability at least 1  X   X  , at least R of the collections C i are (  X , X / 2) -approximations of FI ( D , I , X  ) . Denote this event as G . For the rest of the proof we will assume that G indeed occurs.

Then  X  A  X  FI ( D , I , X  ) , A belongs to at least R of the collec-the algorithm. This means that Property 1 from Def. 3 holds.
Consider now any itemset B such that f D ( B ) &lt;  X   X   X  . By def-inition of (  X , X / 2) -approximation we have that B can only appear in the collections C i that are not (  X , X / 2) -approximations. Given that G occurs, then there are at most N  X  R such collections. But from Constraint (4) and the definition of R in (5), we have that N  X  R &lt; R , and therefore B will not be present in the output of PARMA, i.e. Property 2 from Def. 3 holds.
 Let now C be any itemset in the output, and consider the interval S C = [ a C ,b C ] as computed by PARMA. S C contains at least N  X  R + 1 of the f S i ( C ) , otherwise C would not be in the output. By our assumption on the event G , we have that at least R of the f i ( C )  X  X  are such that | f S i ( C )  X  f D ( C ) | X   X / 2 . Then there is an index j such that | f S j ( C )  X  f D ( C ) | X   X / 2 and such that f S . Given also that f S j ( C )  X  a C , then f D ( C )  X  a C analogously, given that f S j ( C )  X  b C , then f D ( C )  X  b This means that which, together with the fact that  X  f C  X  X  C by construction, proves Property 3.b from Def. 3. We now give a bound to | S C | = b a . From our assumption on the event G , there are (at least) R [ f D ( C )  X   X / 2 ,f D ( C ) +  X / 2] contains (at least) R values f Its length  X  is an upper bound to | S C | . Then the length of the interval K C = [ a C  X   X / 2 ,b C +  X / 2] is at most 2  X  , as requested by Property 3.c from Def. 3. From this, from (6), and from the fact that  X  f ( C ) is the center of this interval we have |  X   X  , i.e., Property 3.a from Def. 3 holds.
The above algorithm can be easily adapted to computing, with probability at least 1  X   X  ,  X  -approximations to TOPK ( D , I ,K ) and to AR ( D , I , X , X  ) . The main difference is in the formula to compute the sample size w (Lemma 1), and in the process to ex-tract the local collections from the samples. The case of top-K is presented in [29] and is a minor modification of Lemma 1, while for the association rule case we can use Lemma 2. These are minimal changes to the version of PARMA presented here, and the modified algorithms guarantee the same levels of accuracy and confidence.
The entire PARMA algorithm has been written as a Java library for Hadoop, the popular open source implementation of MapRe-duce. Because all experiments were done using Amazon Web Ser-vice (AWS) Elastic MapReduce, the version of Hadoop used was 0.20.205, the highest supported by AWS. The use of Java makes possible future integration with the Apache Mahout parallel ma-chine learning library [3]. Mahout also includes an implementation of PFP [18] that we used for our evaluation of PARMA.

In PARMA, during the mining phase (i.e. during the reducer of stage 1), any frequent itemset or association rule mining algorithm can be used. We wanted to compare the performances of PARMA against PFP which only produces frequent itemsets, therefore we chose to use a frequent itemset mining algorithm instead of an as-sociation rule mining algorithm. Again, this choice was merely for ease of comparison with existing parallel frequent itemset mining algorithms as no such algorithms for association rule mining exist. While there are many frequent itemset mining algorithms available, we chose the FP-growth implementation provided by [7]. We chose FP-growth due to its relative performance superiority to other Fre-quent Itemsets mining algorithms. Additionally, since FP-growth is the algorithm that PFP has parallelized and uses internally, the choice of FP-growth for the mining phase in PARMA is appropri-ate for a more natural comparison.

We also compare PARMA against the naive distributed count-ing algorithm (DistCount) for computing frequent itemsets. In this approach, there is only a single MapReduce iteration. The map breaks a transaction  X  into its powerset P (  X  ) and emits key/value pairs in the form ( A, 1) where A is an itemset in P ( t ) . The re-ducers simply count how many pairs they receive for each itemset A and output the itemsets with frequency above the minimum fre-quency threshold. This is similar to the canonical wordcount exam-ple for MapReduce. However, because the size of the powerset is exponential in the size of the original transaction (specifically 2 where |  X  | denotes the number of items in a given transaction), this algorithm incurs massive network costs, even when combiners are used. This is very similar to the algorithms presented in [10, 19, 33]. We have built our own implementation of DistCount in Java using the Hadoop API. We evaluate the performance of PARMA using Amazon X  X  Elastic MapReduce platform. We used instances of type m1.xlarge , which contain roughly 17GB of memory and 6.5 EC2 compute units. For data, we created artificial dataset using the synthetic data generator from [9]. This implementation is based on the generator described in [2], which can be parameterized to generate a wide range of data. We used two distinct sets of parameters to generate the datasets: the first set, shown in Table 1, for the experiments comparing PARMA and the distributed counting algorithm (DistCount), and the second set, shown in Table 2, for the experiments comparing PARMA and PFP. The parameters were chosen to mimic real-world datasets on which PARMA would be run. For a full description of the relevant parameters, we refer the reader to [2]. The reason we needed two distinct datasets is that DistCount did not scale to the larger dataset sizes, as the amount of data it generates in the map phase grows exponentially with the length of the individual transactions in the dataset. We found that DistCount would run out of memory using datasets with longer transactions, and we had to generate datasets with both shorter and less transactions for its comparisons.
Because PARMA is an approximate algorithm, the choice of ac-curacy parameters  X  and  X  are important, as is  X  , the minimum fre-quency at which itemsets were mined. In all of our experiments,  X  = 0 . 05 and  X  = 0 . 01 . This means that the collection of itemsets mined by PARMA will be a 0.05-approximation with probability 0.99. In practice, we show later that the results are much more ac-curate than what this. For all experiments other than the minimum number of items 1000 average transaction length 5 average size of maximal potentially large itemsets 5 number of maximal potentially large itemsets 5 correlation among maximal potentially large itemsets 0.1 corruption of maximal potentially large itemsets 0.1 Table 1: Parameters used to generate the datasets for the run-time comparison between DistCount and PARMA in Figure 2. number of items 10000 average transaction length 10 average size of maximal potentially large itemsets 5 number of maximal potentially large itemsets 20 correlation among maximal potentially large itemsets 0.1 corruption of maximal potentially large itemsets 0.1 Table 2: Parameters used to generate the datasets for the run-time comparison between PFP and PARMA in Figure 2. frequency performance comparison in Figure 4 and for the accu-racy comparison in Figures 7 and 8,  X  was kept constant at 0.1.
Due to space limitations, we do not report the results of the ex-periments to evaluate the performances of PARMA as the parame-ters of the optimization problem change.
For the performance analysis of PARMA, we analyze the rel-ative performance against two exact FIM algorithms on MapRe-duce, DistCount and PFP , on a cluster of 8 nodes. We also provide a breakdown of the costs associated with each stage of PARMA. Figure 2 (top) shows the comparison between PARMA and Dist-Count. For DistCount, longer itemsets affect runtime the most, as the number of key/value pairs generated from each transaction is exponential in the size of the transaction. This is not to say that more transactions does not affect runtime, just that the length of those transactions also has a significant impact. Because of this, it is possible to have datasets with fewer transactions but with more  X  X ong X  transactions that take longer to mine. This effect is seen in the first three datasets (1-3 million). Of course, since these datasets were generated independently and with the same parameters, this was purely by chance. However, as the number of transactions con-tinues to increase, the exponential growth in the number of inter-mediate key/value pairs is seen by the sharp increase in runtime. While we tried to test with a dataset with 6 million transaction, DistCount ran out of memory. The lack of ability to handle either long individual transactions or a large number of transactions in a dataset limits DistCount X  X  real-world applicability.

For the performance comparison with PFP, 10 datasets were gen-erated using parameter values from Table 2 and ranging in size from 10 to 50 million transactions. The results are shown in Figure 2 (bottom). For every dataset tested, PARMA was able to mine the dataset roughly 30-55% faster than PFP. The reason for the relative performance advantage of PARMA is twofold. The first (and pri-mary) reason is that for larger datasets the size of the dataset that PARMA has sampled (and mined) is staying the same, whereas PFP is mining more and more transactions as the dataset grows. The second reason is that as the dataset grows, PFP is potentially duplicating more and more transactions as it assigns transactions to groups. A transaction that belongs to multiple groups is sent to multiple reducers, resulting in higher network costs.

The most important aspect of the comparison of PFP to PARMA Figur e 2: A runtime comparison of PARMA with DistCount (top) and PFP (bottom). is that the runtimes as data grows are clearly diverging due to the reasons discussed above. While 50 million transactions is very siz-able, it is not hard to imagine real-world datasets with transactions on the order of billions. Indeed, many point-of-sale datasets would easily break this boundary. In these scenarios a randomized algo-rithm such as PARMA would show increasing performance advan-tages over exact algorithms such as any of the standard non-parallel algorithms or PFP, which must mine the entire dataset. At that scale, even transmitting that data over the network (several times in the case of PFP) would become prohibitive.

To understand the performance of PARMA it is important to an-alyze the runtimes at each of the various stages in the algorithm. To do this, we have implemented runtime timers at very fine gran-ularities throughout our algorithm. The timers X  values are written to Hadoop job logs for analysis. This breakdown allows us to not only analyze the overall runtime, but also the sections of the algo-rithm whose runtimes are affected by an increase in data size. In Figure 3, a breakdown of PARMA runtimes is shown for each of the six segments of the algorithm, which include a map, shuffle and reduce phase for each of the two stages. Due to space limitations, we only show the breakdown for a subset of the datasets we tested. We observed the same patterns for all datasets. This breakdown demonstrates several interesting aspects of PARMA. First, the cost of the mining local frequent itemsets (stage 1, reduce) is relatively constant. For many frequent itemset mining implementations, this cost will grow with the size of the input. This is not the case in PARMA, because local frequent itemset mining is being done on constant-sized sample of the input. Indeed another interesting ob-servation, as expected, is that the only cost that increases as sample size increases is the cost of sampling (stage 1, map). This is because in order to be sampled the input data must be read, so larger input data means larger read times. In practice, this cost is minimal and grows linearly with the input, hence it will never be prohibitive, Figure 3: A comparison of runtimes of the map/reduce/shuffle phases of PARMA, as a function of number of transactions. Run on an 8 node Elastic MapReduce cluster. Figure 4: A comparison of runtimes of the map/reduce/shuffle phases of PARMA, as a function of minimum frequency. Clus-tered by stage. Run on an 8 node Elastic MapReduce cluster. especially considering all other current algorithms must read the entire input data at least once, and in many cases multiple times.
There is one outlier in the graph, which is the dataset with 5 mil-lion transactions. Because each dataset was independently gener-ated, it is possible for a dataset to have a larger number of frequent itemsets than other datasets, even if it has less transactions. This is the case with the 5 million transaction dataset, which takes longer to run mine for both PARMA and PFP due to the relatively greater number of frequent itemsets.

Figure 4 shows the breakdown of PARMA runtimes as the mini-mum frequency at which the data is mined at is changed. Data size was kept constant at 10 million transactions. Minimum frequency is used by the local frequent itemset mining algorithm to prune itemsets; itemsets below the minimum frequency are not consid-ered frequent, nor is any superset since a superset must, by defini-tion, contain the not frequent set and therefore cannot be frequent itself. Intuitively, a lower minimum frequency will mean more fre-quent itemsets are produced. Other than a runtime increase in the local frequent itemset mining phase (stage 1, reduce), the effects of this can be seen in the stage 2 shuffle phase as well, as there is more data to move across the network. Still, the added costs of mining with lower frequencies are relatively small.
To show the speedup of PARMA, we used a two-nodes cluster as the baseline. Because PARMA is intended to be a parallel al-gorithm, the choice of a two-nodes cluster was more appropriate than the standard single node baseline. For the dataset, we used a 10 million transaction database generated using the parameters in Figure 5: The speedup analysis of PARMA, broken down by stages. Figure 6: The scalability of PARMA as both data and cluster size are increased.
 Table 2. The results are shown in Figure 5. The three lines on this graph represent the relative speedup of both stage 1 and stage 2 as well as the overall PARMA algorithm. The graph indicates that stage 1 is highly parallelizable and follows a near-ideal speedup for up to 8 nodes, after which a slight degradation of speedup oc-curs. There are two reasons for this slight degradation. In the map phase of stage 1, due to an implementation decision in Hadoop, the smallest unit of data that can be split is one HDFS block. As we continue to add more nodes to the cluster, we may have more avail-able map slots than HDFS data blocks, resulting in some slots being unused. Theoretically, this could be fixed by allowing smaller gran-ularity splitting in Hadoop. Another cause of the slightly sub-ideal speedup in stage 1 is from the reducer. Because the data in this experiment was held constant, the slight degradation in speedup as more than 8 nodes were added was a result of an inefficient over-splitting of transaction data. If each reducer in stage 1 is mining a very small subset of the transactions, the overhead of building the FP-tree begins to dominate the cost of mining the FP-tree. This is because the cost of mining the FP-tree is relatively fixed. Thus, we can  X  X ver-split X  the data by forcing the reducer to build a large FP-tree only to mine a small set of transactions. For larger samples, the size of the cluster where speedup degradation begins to occur would also increase, meaning PARMA would continue to scale.
Also, as is clearly visible in the graph, the sub-ideal overall speedup is due largely to the poor speedup of stage 2. Stage 2 is bound almost entirely by the communication costs of transmitting the local frequent itemsets from stage 1 to the reducers that will do the aggregation. Because the amount of local frequent itemsets does not change as more nodes are added, the communication for this stage does not change. What does change is the number of itemsets each node must aggregate. During the reduce phase, each node is assigned a set of keys. All key/value pairs emitted from the map phase are sent to the reducer assigned their respective key. The reducer is in charge of aggregating the values and emitting one aggregate value per key assigned to it. As more reducers are added to the cluster, each reducer will have fewer keys assigned to it, and therefore must aggregate across fewer values, resulting in faster aggregation. The small but existent positive change in the line for stage 2 is a result of this slight speedup of the reduce phase.
Figure 6 depicts the scalability of PARMA as both the size of the dataset (i.e. number of transactions) and the number of nodes in the cluster are increased. The data and nodes are scaled proportionally so that the ratio of data to nodes remains constant across all exper-iments. This result shows that as nodes and data are increased pro-portionally, the total runtime actually begins to decrease for larger datasets. This is because as nodes are added to the cluster, the run-time of the Stage 1 reducer (FIM) is decreased while the relative costs of the Stage 1 mapper and Stage 2 remain the same. There is a leveling off of the runtime between the 40M and 80M datasets, which can be explained using Amdhal X  X  law; because only portions of the algorithm are parallelizable, there is a theoretical maximum speedup that is possible. Still, the constant runtime as data is in-creased demonstrates PARMA X  X  potential scalability to real-world cluster and dataset sizes.
The output of PARMA is a collection of frequent itemsets which approximates the collection one can obtain by mining the entire dataset. Although our analysis shows that PARMA offers solid guarantees in terms of accuracy of the output, we conducted an ex-tensive evaluation to assess the actual performances of PARMA in practice, especially in relation to what can be analytically proved.
We compared the results obtained by PARMA with the exact col-lection of itemsets from the entire dataset, for different values of the parameters  X  ,  X  , and  X  , and for different datasets. A first important result is that in all the runs, the collection computed by PARMA was indeed a  X  -approximation to the real one, i.e., all the properties from Definition 3 were satisfied. This fact suggests that the con-fidence in the result obtained by PARMA is actually greater than the level 1  X   X  suggested by the analysis. This can be explained by considering that we had to use potentially loose theoretical bounds in the analysis to make it tractable.

Given that all real frequent itemsets were included in the out-put, we then focused on how many itemsets with real frequency in the interval [  X   X   X , X  ) were included in the output. It is important to notice that these itemsets would be acceptable false positives, as Definition 3 does not forbid them to be present in the output. We stress again that the output of PARMA never contained non-acceptable false positives, i.e. itemsets with real frequency less than the minimum frequency threshold  X  . The number of accept-able false positives included in the output of PARMA depends on the distribution of the real frequencies in the interval [  X   X   X , X  ) , so it should not be judged in absolute terms. In Table 3 we report, for various values of  X  , the number of real frequent itemsets (i.e., with real frequency at least  X  , the number of acceptable false posi-tives (AFP) contained in the output of PARMA, and the number of itemsets with real frequency in the interval [  X   X   X , X  ) , i.e., the max-imum number of acceptable false positives that may be contained in the output of PARMA (Max AFP). These numbers refers to a run of PARMA on (samples of) the 10M dataset, with  X  = 0 . 05
Table 3: Acceptable False Positives in the output of PARMA
Figure 7: Error in frequency estimations as frequency varies. and  X  = 0 . 01 . It is evident that PARMA does a very good job in filtering out even acceptable false positives, especially at lower fre-quencies, when their number increases. This is thanks to the fact that an itemset is included in the output of PARMA if and only if it appears in the majority of the collections obtained in the first stage. Itemsets with real frequencies in [  X   X   X , X  ) are not very likely to be contained in many of these collections.

We conducted an evaluation of the accuracy of two other com-ponents of the output of PARMA, namely the estimated frequen-cies for the itemsets in the output and the width of the confidence bounds for these estimations. In Figure 7 we show the distribution of the absolute error in the estimation, i.e. |  X  f ( X )  X  f itemsets X in the output of PARMA, as  X  varies. The lower end of the  X  X hisker X  indicates the minimum error, the lower bound of the box corresponds to the first quartile, the segment across the box to the median, and the upper bound of the box to the third quar-tile. The top end of the whisker indicates the maximum error, and the central diamond shows the mean. This figure (and also Fig-ure 8) shows the values for a run of PARMA on samples of the 10M dataset, with  X  = 0 . 05 and  X  = 0 . 01 . We can see that even the maximum values are one order of magnitude smaller than the threshold of 0 . 05 guaranteed by the analysis, and many of the er-rors are two or more orders of magnitude smaller. It is also possible to appreciate that the distribution of the error would be heavily con-centrated in a small interval if the maximum error were not so high, effectively an outlier. The fact that the average and the median of the error, together with the entire  X  X ox X  move down as the mini-mum frequency threshold decrease can be explained by the fact that at lower frequencies more itemsets are considered, and this makes the distribution less susceptible to outliers. Not only this is a sign of the high level of accuracy achieved by PARMA, but also of its being consistently accurate on a very large portion of the output.
Finally, in Figure 8 we show the distribution of the widths of the confidence intervals K ( A ) for the frequency estimations the itemsets A in the output of PARMA. Given that  X  = 0 . 05 , the maximum allowed width was 2  X  = 0 . 1 . It is evident from the fig-ures that PARMA returns much narrower intervals, of size almost  X  . Moreover, the distribution of the width is very concentrated, as shown by the small height of the boxes, suggesting that PARMA is extremely consistent in giving very high quality confidence inter-vals for the estimations. We state again that in all runs of PARMA in our tests, all the confidence intervals contained the estimation Figure 8: Width of the confidence intervals as frequency varies. and the real frequency, as requested by Definition 3. As seen in the case of the estimation error, the distribution of the widths shifts down at lower thresholds  X  . This is motivated by the higher num-ber of itemsets in the output of PARMA at those frequencies. Their presence makes the distribution more robust to outliers. We can conclude that PARMA gives very narrow but extremely accurate confidence intervals across the entirety of its output.

This analysis of the accuracy of various aspects of PARMA X  X  output shows that PARMA can be very useful in practice, and the confidence of the end user in the collections of itemsets and estima-tions given in its output can be even higher than what is guaranteed by the analysis.
In this paper, we have described PARMA, a parallel algorithm for mining quasi-optimal collections of frequent itemsets and asso-ciation rules in MapReduce. We showed through theoretical anal-ysis that PARMA offers provable guarantees on the quality of the output collections. Through experimentation on a wide range of datasets ranging in size from 5 million to 50 million transactions, we have demonstrated a 30-55% runtime improvement over PFP, the current state-of-the-art in exact parallel mining algorithms on MapReduce. Empirically we were able to verify the accuracy of the theoretical bounds, as well as show that in practice our results are orders of magnitude more accurate than is analytically guaran-teed. Thus PARMA is an algorithm that can scale to arbitrary data sizes while simultaneously providing nearly perfect results.
The work of Riondato, DeBrabant, and Upfal was supported in part by NSF award IIS-0905553.
