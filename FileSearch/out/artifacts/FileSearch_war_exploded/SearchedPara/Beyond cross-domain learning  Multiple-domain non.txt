 1. Introduction tion from the machine learning community recently. The goal of target domain, but the number of labeled samples in the target target domain. The source domain samples share the same feature space and the class label space as the target domain, but the from that of the target domain. Thus the source domain samples cannot be directly used for the learning problem of the target domain. Instead, the domain transfer, or domain adaptation, is needed to learn from the source domain to the target domain.
Recently, many works have been done in the transfer learning problem ( Daume, 2007 ; Yang et al., 2007 ; Jiang et al., 2008 ; Bruzzone and Marconcini, 2010 ; Duan et al., 2012a , b ). on learning from a single source domain to a target domain great number of labeled samples in the source domain. However, in many real-world applications, there are usually more than two domains, and for each domain there are just a few labeled domain with the help of labeled samples from all other domains, the problem of spam email detection, we may have several email are collected while only a small portion of them are labeled as different domains. Every user needs a classi fi er for the spam detection, but does not have enough labeled emails, thus every one is a target domain. At the same time, each user's data would also be helpful for the learning of other users' classi fi domain learning problem. It has the following features: 1. Several (usually more than two) domains will be considered in the learning procedure. 2. All the domains will be treated equally. No domain will be speci fi ed as source or target domains. Each domain could be a target domain, while could also be a source domain for other domains. 3. A classi fi er is needed for each domain, but the number of labeled samples for each domain is limited.
 Though there are many real-world applications of the multiple-domain learning problem, surprisingly, little attention has been methods have been presented to learn from multiple source domains to a single target domain. For example,
Duan et al. (2012a) proposed the domain adaptation machine (DAM) for the multiple source domain adaption problem, which learns a robust classi fi er for the target domain by leveraging many base classi fi ers which could be learned using the labeled samples from the source domains or the target domains.

Zhuang et al. (2010) proposed a centralized consensus regular-ization (CCR) framework for learning from multiple source domains to a target domain. It trains a local classi fi er by considering both local data available in one source domain and the prediction consensus with the classi fi ers learned from other source domains.
 Yao and Doretto (2010) proposed the multiple source transfer
AdaBoost (MDTAB) by extending the boosting framework for transferring knowledge from multiple sources.

Sun et al. (2011) proposed a two-stage-weighting-based method for multiple source domain adaptation (TSWMSD) ( Sun et al., 2011 ), by combining weighted data samples from multiple probability differences, with the target domain data.
Tu and Sun (2012a) proposed a multiple source domain adaptation method based on ensemble learning. Using with different weights dynamically.

Tu and Sun (2012b) further proposed a novel framework to maximize separability among classes and to minimize separ-ability among domains simultaneously. To this end, the class-separate objectives and the domain-merge objectives are combined to achieve a uni fi ed objective.
 All the aforementioned works tried to deal with the problem of They can be regarded as a special case of our multiple domain learning problem. The methods that learn from multiple source domains can be extended to the multiple-domain learning pro-blem by treating each domain as the target domain and other following limitations:
Its quite time consuming. For each domain, we should perform a learning procedure to learn form all other domains. It is not ef fi cient when the number of domains is large.

It assumes that all the samples in the source domains are labeled, which is not true for the multiple-domain learning problem. In this case, only the labeled samples from other domains will be utilized, while neglecting the unlabeled ones. it could not be applied to other domains. Learning a single classi fi er for all the domains is impossible.

Nonnegative matrix factorization (NMF) has been well studied fi are limited in the single domain problem, and no cross-domain or multiple-domain NMF method has been studied yet. To fi ll these gaps, in this paper, we develop a novel data representation method for the multiple-domain data representation, based on
NMF. We try to map all the samples from multiple-domains with different data distributions into a common representation space with a common distribution by NMF. A distribution mismatch term is constructed and applied to the coding vectors of samples under the framework of NMF. With the common representation of samples from multiple-domains, a robust classi fi er could be trained for the classi fi cation problem of samples of multiple-domains directly.

The rest of this paper is organized as follows: In Section 2 ,we propose the NMF learning algorithm for representation of samples from multiple-domains. In Section 3 , we show the experimental results for the proposed algorithm on two real-world multiple-domain learning problems. Finally, we conclude this paper with the conclusion and possible future works in Section 4 . 2. Multiple-domain nonnegative matrix factorization
In this section, we will introduce the proposed NMF method for representation of data samples from multiple-domains. 2.1. Objective function
To introduce the proposed NMF method, we construct an objective function for the factorization of all the samples from multiple-domains, by considering the following two problems simultaneously:
NMF problem : Given a training data set with N data samples
Multiple-domain distribution mismatch reduction problem : Sup-
By considering the NMF problem and the OTA criterion for multiple-domain distribution mismatch reduction simultaneously, we combine the two optimization problems in (2 ) and ( 12 )to formulate the MDNMF problem: min s : t : H Z 0 ; W Z 0 ;  X  13  X  where  X  is the trade-off parameter. 2.2. Optimization We adopt the Lagrange multiplier method to optimize (13) . We introduce the Lagrange multiplier matrices,  X  and  X  , for the constrain of H Z 0 and W Z 0, respectively. Thus, the Lagrange function L of (13) is L  X  Tr  X  X  X HW  X  X  X HW  X  &gt;  X   X  Tr  X  W  X   X  W &gt;  X   X  Tr  X  W zero, we have  X  L  X 
H  X  2 XW &gt;  X  2 HWW &gt;  X   X   X  0  X  L  X 
W  X  2 H By using the KKT conditions  X   X  H  X  0 and  X   X  W  X  0, where denotes element-wise product, we could have the following equations for the factorization matrices H and W :  X  XW &gt;  X   X  H  X  X  HWW &gt;  X   X  H  X  0  X  H &gt; X  X   X  W  X  X  H &gt; HW  X   X  W  X   X   X  W  X   X   X   X  W  X   X  W These equations lead to the following updating rules: H  X  H  X   X  XW &gt; W  X  W  X   X  H &gt; X  X  where  X  =  X  denotes the element-wise division. 2.3. Algorithm
Based on the optimization results in (17) , we can design the The update rules will be repeated until a maximum interaction number T is reached.
 Algorithm 1. MDNMF learning algorithm.
 INPUT : Training set D of M domains and its data matrix X ; Input : Iteration number T ;
Initialize the basis matrix H 0 and the coding matrix W 0
Initialize matrices  X   X  and  X  as in (11) ; for t  X  1 ; ... ; T do end for OUTPUT : Basis matrix H T and coding matrix W T .

Time complexity analysis : In this learning algorithm, the basis matrix W and the coding matrix W are updated alternately in each
Moreover, before the iterations, the matrix  X  of size N N should is O  X  T K  X  D  X  N  X  X  N 2  X  . 2.4. Representation of new samples
When a test sample from the m -th domain comes, we try to represent it as a new coding vector w using the learned basis assume that the input of this new sample does not affect the coding of the training samples, so w i for training samples x fi xed. We also assume that the average of the coding vector means new sample to the distribution of the m -th domain, the coding vector mean of the m -th domain is re-computed as  X  1 =  X  N  X   X  considering the approximation error of the new sample, we need of the test sample x : min s : t : w Z 0 ;  X  18  X  where  X   X  X   X  1 ; ... ;  X  N &gt; with  X  8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :
We also separate  X  into two negative parts  X   X   X   X   X  , where  X   X 
By substituting  X   X   X   X   X  into (18) , we have the following optimization problem: min  X  Tr  X  X  x Hw  X  X  x Hw  X  &gt;  X   X  Tr 1 N s : t : w Z 0 :  X  20  X  We denote the Lagrange multiplier vector for the constrain w as . The Lagrange function L of problem (20) is
L  X  Tr  X  X  x Hw  X  X  x Hw  X  &gt;  X   X  Tr 1  X  Tr  X   X  w &gt;  X  X  21  X 
By setting the derivative of L regarding to w to zero, we have  X  L  X  w  X  2 H &gt; x  X  2 H &gt; Hw  X  2  X  1 N  X  0  X  22  X  for w :  X  H &gt; Hw  X   X  w  X   X  1 N which leads to the following updating rule for w : w  X  w  X  vectors w i of the training samples x i A D . The algorithm for representation of a new test sample is given in Algorithm 2 . Algorithm 2. MDNMF representation algorithm.
 the coding vector w of size K is updated for T times. Besides, a complexity is O  X  T K  X  N  X  . 3. Experiments method on two multiple-domain learning tasks  X  multiple user spam email detection and multiple-domain glioma diagnosis using gene expression data. 3.1. Experiment I: multiple user spam email detection 3.1.1. Dataset and setup to non-spam or spam. We use a spam email dataset which is
The spam email dataset consists the emails from 15 inboxes of emails received by individual users, we treat each user as an individual domain. Each user may have labeled some of the emails in his/her inboxes, but not all of them. When we try to learn a other users' emails may be helpful. Thus the multiple user spam email detection problem is a typical multiple-domain learning as training samples in the training set, and the remaining 100 we have 300 15  X  4500 samples in the training set, while 100 15  X  1500 samples in the independent test set. Among all these training samples, we randomly select a small part (30%) of them as labeled samples while keeping the remaining ones as unlabeled.

To represent the emails, we fi rst extract the word-frequency features from each email, then weight the word-frequency fea-2009 ) using the coding vectors of all training samples from different domains. To conduct the evaluation, we fi rst perform the 10-fold cross validation on the training set to optimize the parameters for the MDNMF model and the SVM classi fi er. To procedure. To evaluate the performances of the multiple-domain learning methods, we employ the following performance criteria
Accuracy (ACC), and Matthews Correlation Coef fi cient (MCC). We de non-spam, a true negative (TN) as a non-spam email classi correctly as a non-spam, and a false positive (FP) as a non-spam have the following classi fi cation performance measures for the training set: Sensitivity  X  TP TP  X  FN ; Specificity  X  TN FP  X  TN ;
F Score  X  2 TP 2 TP  X  FP  X  FN ; ACC  X  TP  X  TN TP  X  FP  X  TN  X  FN
A better classi fi er will achieve a higher performance measure to a better classi fi er, and 1 and 1 represents worst and best ing characteristic (ROC) ( Ben-David, 2008 ) and Recall  X  curves are reported as performance measures. The ROC curve and FPR are de fi ned as follows: TPR  X  TP TP  X  TN ; FPR  X  FP FP  X  TN  X  26  X 
Moreover, the area under ROC curve (AUC) ( Ben-David, 2008 ) is also used as a single performance measure to compare the by plotting the precision vs. the recall, at various threshold settings, where precision and recall are de fi ned as follows: Precision  X  TP TP  X  FP ; Recall  X  TP TP  X  FN  X  27  X  closer to the upper right corner.

Since there are no other multiple learning methods that we can compare our algorithms with, we compared the proposed MDNMF with the state-of-the-art cross-domain learning methods that 2012b ) requires pre-learned base classi fi ers, and we used the of a baseline method, which helps to identify the cross-domain issues. The baseline method is developed by putting all training samples of different domains to one merged dataset and training one single SVM on the merged dataset. 3.1.2. Results
The boxplot of various performance measures for 10-fold cross validation on the training set is given in Fig.1 . From this easy to see that the proposed MDNMF outperforms other multiple con fi rmed by the improvements of MDNMF over those methods on all the performance criteria. The reason is that the samples from all the domains are mapped into a common coding space via MDNMF, which are combined and further used to train a robust classi fi er to the problem. Because it provides a uni fi domain training set re fi ned by the MDNMF to obtain the fi representation, it is not necessary to design domain transfer classi fi ers. Moreover, it is obvious that with respect to the unlabeled samples, the state-of-the-art methods have no super-curves of the methods on the independent test set of the email spam set. The AUCs of RCOs are summarized in Table 1 . It can be seen that MDNMF achieves the best performance among all the methods compared here. In particular, the DAM achieves AUC of baseline method achieves only 0.8101, while MDNMF achieves 0.9084. The main difference between MDNMF and other methods is that MDNMF exploits the unlabeled samples from all the domains, and maps the samples into a common space. We can see that by the multiple-domain mapping, MDNMF achieves performance improvement w.r.t. concerned performance mea-sures such as TPR, FPR, Recall and Precision. This could also be con fi rmed by Fig. 2 . 3.2. Experiment II: multiple-domain glioma diagnosis
In the second group of experiment we test our data represen-tation method on the multiple-domain glioma diagnosis problem. 3.2.1. Dataset and setup
We collect 399 samples from four different datasets generated in each dataset varies from 50 to 180. Each sample is classi one of the four classes as follows: 1. Glioblastoma (GBM): 248 samples. 2. Anaplastic oligodendroglioma (AO) and anaplastic astrocytoma (AA): 79 samples.
 0.6 0.7 0.8 0.9 Specificity 0.6 0.7 0.8 0.9 ACC MCC 3. Astrocytoma (A): 45 samples. 4. Non-tumor: 27 samples.
 expression of samples to 272 genes commonly shared by the four datasets, and the 272 gene expression data will be used as the original nonnegative features of samples. Due to the different equipments and data pre-processing methods used by different groups, the distributions of the four data sets are signi different, thus we treat the four data sets as four domains. folds are used as the training set. Note that the parameters are validation. We adopt the ACC as a criterion for the classi problem. In the experiments, we adopt the one-to-all protocol to handle the multi-class problem, and we also use the standard SVM in total in this experiment. 3.2.2. Results
The boxplot of ACCs for the 10-fold cross validation on the when compared with the baseline method. Moreover, it seems that TSWMSD and DAM outperforms MDTAB, CCR and the baseline.
The results of TSWMSD, DAM, CCR and our algorithm all rely on a robust SVM classi fi er.Differently,mostmultiplesourcedomain learning algorithms employ multiple source domains to directly re fi ne an SVM model, while MDNMF tries to learn a common data representation space for the learning of SVM. Compared with the
We also compared the training and the testing time with other methods, and the results are reported in Fig. 4 . From this can observe that compared to multiple source domain learning methods, the proposed algorithm can reduce the learning time domain learning methods are not designed for the multiple-domain learning problem, when we use them we have to we treat Precision Time (s) each domain as the target domain in turn. Thus the learning procedure was conducted for every domain, treating it as target domain while leaving the others as source domains. In our algorithm, we only perform the learning procedure for one time for all the domains. since we treat all the domains equally, The any transfer learning procedure, we can also perform one single learning procedure to all the samples regardless which domain they belong to. 4. Conclusion and future works
In this paper, we proposed the multiple-domain learning pro-blem, and developed a novel multiple-domain data representation method. The multiple-domain learning problem is summarized as a are in lack of labeled samples for learning robust classi improved the NMF for the representation of samples of multiple-domains, so that the coding vectors share a common distribution domains. The experiments on two challenging multiple-domain proposed method.

In the further, we will develop novel multiple-domain data representation method by extending other representation meth-odologies to multiple-domains, such as multiple-domain sparse and multiple-domain component analysis ( Aradhya et al., 2008 ). Moreover, we could also directly conduct the multiple-domain or multiple-domain database ranking ( Yang et al., 2012 ), by fi cation prediction results or ranking scores. Recently, the multiple-domain SVM was reported by Ji and Sun (2013) . This method deals with the multiple-domain learning problem and multiple-domain learning method reported in Ji and Sun (2013) domains. Moreover, our framework can only handle the binary in Ji and Sun (2013) is targeted at multi-class learning. Acknowledgments The study was supported by grants from Chongqing Key Laboratory of Computational Intelligence, China (Grant no. CQ-LCI-2013-02), Tianjin Key Laboratory of Cognitive Computing and Application, China, and King Abdullah University of Science and Technology (KAUST), Saudi Arabia.
 References
