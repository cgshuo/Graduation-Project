 The original goals of the RH parser were to obtain accurate parses where (a) application speed was needed, and (b) large amounts of annotated mate-rial for a subject idiom were not available. Addi-tional goals that evolved were (c) that parses for particular documents could be brought to an almost arbitrary level of correctness for research purpose s, by grammar correction, and (d) that information collected during parsing could be modified for an application with a modest amount of effort. Goal (a) ruled out the use of unification-based symbolic parsers, because deep unification is a relatively slow operation, no matter what amount of compu-tational sophistication is employed. Until very re-cently, goal (b) ruled out stochastic parsers, but new results (McClosky et al. 2006) suggest this may no longer be the case. However, the "addi-tional" goals still favor symbolic parsing. 
To meet these goals, the RH parser combines a very efficient shallow parser with an overlay parse r that is "retro", in that the grammar is related to Augmented Transition Networks (Woods, 1970), operating on the shallow-parser output. A major "augmentation" is a preference-scoring component. 
Section 2 below reviews the shallow parser used, and Section 3 describes the overlay parser. Some current results are presented in section 4. Section 5 examines some closely-related work, and Section 6 discusses some implications. XIP is a robust parser developed by Xerox Research Center Europe. It is actually a full pars er that produces a tree of chunks, plus identification of (sometimes alternative) typed dependencies among the chunk heads (Ait-Mokhtar et al. 2002, Gala 2004). But because the XIP dependency analyzer for English was incomplete when RH work began, and because classic parse trees are more convenient for discourse-related applications, we focused on the chunk output. 
XIP is astonishingly fast, contributing very little to RH parse time. It consists of the XIP engine, plus language-specific grammars, each consisting of: (a) a finite state lexicon producing alternativ e tags and morphological analyses for each token, together with subcategorization, control and (some) semantic class features, (b) a part of speec h tagger, and (c) conveniently expressed, layered rule sets that perform the following functions: -Lexicon extension , which adds words and -Lexical disambiguation (including use of the -Multi-word identification for named entities, -Chunking, obtaining basic chunks such as -Dependency Analysis (not used in RH) 
All rule sets have been extended within RH development except for the dependency rule sets.. The overlay parser builds on chunker output to produce a single tree (figure 1) providing syntacti c categories and functions, heads, and head features. The output tree requires further processing to ob-tain long distance dependency information, and make some unambiguous coordination adjustments Some of this has already been done in a post-parse phase. The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahil l et al (2004). The major parser components are a control, the ATN-like grammar networks, and collections of tests. The control is invoked recursively to build non-chunk constituents by following grammar network paths and creating output networks. 
Figure 2 shows the arcs of an excerpt from a grammar network used to build a noun phrase. The Test labels on the arcs resemble specialized cate-gories. The MetaOps (limited in the illustration to Prolog-like cuts) expedite processing by permitting or barring exploration of further ordered arcs originating at the same state. 
An output network , illustrated in figure 3, mirrors the full paths traversed in a grammar net-Figure 2. Some arcs of grammar network for GNP Figure 3. Output network for " The park in Paris " work by one invocation of the control. The arcs refer either to chunks or to final states of other out-put networks. Output networks do not contain cy-cles or converging arcs, so states represent unique paths. They carry head and other path information, and a preference score. The final parser output is a single tree, derived from a highest scoring path of a topmost output network. Ties are broken by low attach considerations. 
Each invocation of the control is given a grammar network entry state and a desired constituent category. After initializing a new output network, the arcs from the given entry state are followed. Processing an arc may begin with an optional pretest. If that succeeds, or there is no pretest, a constructive test follows. The tests ar e indexed by grammar network test labels, and are expressed as blocks of procedural code, for initial flexibility in determining the necessary checks. 
Pretests include fast feasibility checks, and con-texted checks of consistency of the potential new constituent with the current output network path. Constructive tests can make additional feasibility checks. If these checks succeed, either a chunk is returned, or the control is reentered to try to bui ld a subordinate output network. Results are cached, to avoid repeated testing. 
After a chunk or subordinate network ON' is returned from a constructive test, one new arc Ai is added to the current output network ON to represent each full path through ON'. All added arcs have the same origin state in ON , but unique successor states and associated preference scores. The preference score is the sum of the score at the common origin state, plus the score of the repre-sented path in ON' , plus a contexted score for the alternative within ON . The latter is one of &lt;-1, 0, +1&gt;, and expresses the consistency of Ai with the current path with respect to dependency, coordina-tion and apposition. Structural and punctuation aspects are also considered. Preference tests are indexed by syntactic category or syntactic func-tion, and are organized for speed. Most tests are independent of Ai length, and can be applied once and the results assumed for all Ai . 
Before a completed output network is returned, paths ending at those lower scoring final states which cannot ultimately be optimal are pruned. Such pruning is critical to efficiency. To provide a snapshot of current RH parser performance, we compare its current speed and accuracy directly to those of a widely used statistical parser, Collins model 3 (Collins, 1999) , and indirectly to two other parsers. Wall Street Journal section 23 of the Penn Treebank (Marcus et al. 1994) was used in all experiments. "Training" of the RH parser on the Wall Street Journal area (beyond general RH development) occupied about 8 weeks, and involved testing and (non-exhaustively) correcting the parser using two WSJ texts: (a) section 00, and (b) 700 sentences of section 23 used as a dependency bank by King et al. (2003). The latter were used early in RH devel -opment, and so were included in the training set. 4.1 Comparative Speed 
Table 1 compares RH parser speed with Collins model 3, using the same CPU, showing the elapsed times for the entire 2416-line section 23. 
The results are then extrapolated to two other parsers, based on published comparisons with Collins. The extrapolation to XLE, a mature unification-based parser that uses a disambiguating statistical post-processor, is drawn from Kaplan et al. (2004). Results are given for both the full grammar and a reduced version that omits less likely rules. The second comparison is with the fast stochastic parser by Sagae and Lavie (2005). 
Summarizing these results, RH is much faster than Collins model 3 and the reduced version of XLE, but a bit slower than Sagae-Lavie. 
The table also compares coverage, as percent-ages of non-parsed sentences. For RH this was 10% for the test set discussed below, which did not contain any training sentences, and was 10.4% for the full section 23. This is reasonable for a sym-bolic parser with limited training on an idiom, and better than the 21% reported for XLE English. Sagae/ Lavie ~ 4 min 1.1% RH parser 5 min 10% Collins m3 16 min .6% XLE full ~80 minutes ~21% XLE reduced ~24 minutes unknown Table 1: Speeds and Extrapolated speeds Sagae/Lavie unknwn 86% unknwn Collins Lbl 33.6% 88.2% 1.05 CollinsNoLbl 35.4% 89.4 % 1.05 RH NoLbl 46% 86 % .59 
Table 2. Accuracy Comparison 4.2 Comparative Acccuracy Table 2 primarily compares the accuracy of the Collins model 3 and RH parsers. The entries show the proportion of fully accurate parses, the f-scor e average of bracket precision and recall, and average crossing brackets, as obtained by EVALB (Sekine and Collins, 1997). The RH f-score is currently somewhat lower, but the proportion of fully correct parses is significantly higher. 
This data may be biased toward RH, because, of necessity, the test set used is smaller, and a different bracketing method is used. For Collins model 3, the entries show both labeled and unlabeled results for all of WSJ section 23. The Collins results were generated from the bracketed output and Penn Treebank gold standard files provided in a recent Collins download. 
But because RH does not generate treebank style tags, the RH entries reflect a test only on a rando m sample of 100 sentences from the 1716 sentences of section 23 not used as "training" data, using a different, available, gold standard creation and bracketing method. In that method (Newman, 2005), parser results are produced in a "TextTree" form, initially developed for fast visual review of parser output, and then edited to obtain gold standard trees. Both sets of trees are then bracket ed by a script to obtain, e.g., 
For non-parsed sentences in the parser outputs, brackets are applied to the chunks. EVALB is then used to compare the two sets of bracketed results. 
Accuracy for XLE is not given, because the results reported by Kaplan et al. (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. (All f-scores are &lt;= 80%). Several efforts combine a chunker with a dependency analyzer operating on the chunks, including XIP itself. The XIP dependency analyzer is very fast, but we do not have current coverage o r accuracy data for XIP English. 
Other related hybrids do not build on chunks, but, rather, adjust full parsers to require or pre fer results consistent with chunk boundaries. Daum et al. (2003) use chunks to constrain a WCDG grammar for German, reducing parse times by about 2/3 (but the same results are obtained using a tagger alone). They estimate that an ideal chunker would reduce times by about 75%. No absolute numbers are given. Also, Frank et al. (2003) use a German topological field identifier to constrain an HPSG parser. They show speedups of about 2.2 relative to a tagged baseline, on a corpus whose average sentence length is about 9 words. We have shown that the RH hybrid can compete with stochastic parsers in efficiency and, with onl y limited "training" on an idiom, can approach them in accuracy. Also, the test organization prevents speed from degrading as the parser is improved. 
The method is significant in itself, but also leads to questions about the advantages of deep-unification-based parsers for practical NLP. These parsers are relatively slow, and their large number s of results require disambiguation, e.g., by corpus-trained back-ends. They do provide more informa-tion than RH, but there is much evidence that the additional information can be obtained by rapid analysis of a single best parse. Also, it has neve r been shown that their elegant notations actually facilitate grammar development and maintenance. Finally, while unification grammars are reversible for use in generation, good generation methods remain an open research problem. 
