 1. Introduction
The apparent quality can be divided into two categories. First, the fabrics with a large number of area faults that were occurring in the knitting process and eventually make them useless. In the second category, there are inputted faults that originate from yarn faults. The standard method ASTM (Mahli and Batra, 1972) is for grading the short staple spun yarn. This standard is based on human observations and therefore it is not precise enough (Semnani et al., 2005 ). Many researchers like Neval et al. (1996a, 1996b, 1996c) worked to solve this problem that was based on classification of events along a thread of yarn by measuring the percentage of different classes of events. In the Fredrych and Matusiak (2002) method, the nep number in cotton yarn was predicted by Uster tester. In another method ( Rong and Slater, 1995 ; Rong et al.,1995 ), grading of various yarns is done by cluster analysis. The other researchers such as Kang et al. (2001) and Sawhney (2000) used image analysis to detect weaving and yarn faults. Few researchers worked on the classification of weft knitting faults like Abou-liana et al. (2003), whose method was based on structure changes and similar work by Tanako et al. (2004) using image analysis and 3D-CAD for knitting fabrics. None of the works above present a general method of grading fabric and yarn together based on their appearance, so the aim of the Semnani et al. (2005) investigation was to present a general method for inspecting all types of woven and knitted fabrics. In
Consequently, the image of fabric faults as image of yarn faults can be obtained considering the use of two threshold points: upper point T fl = m f s f and lower point T fu = m f s f . The original images of fabric samples well before and after thresholding operation are both shown in Fig. 1 .

To count the faults, their size and adherence was considered for defining a suitable block size that provides maximum deviation in images fault means. The faults of each loop of knitted fabric are separated by neighborhood loops. Therefore, suitable block size of fabric faults image is estimated from loop size, which is approximately four times of yarn diameter.

The faults matrix is divided into blocks of equal size. There are four classes: Class I: m bi Z 1.2 T bm ; Class II: T bm r m bi r 1.2 T bm and
V r T bv ; Class III: T bm r m bi r 1.2 T bm and V bi Z T bv ; and Class IV: any other blocks of faults image of yarn that are not classified in the above classes. T bm is the threshold of mean blocks, T bv the threshold of deviation blocks, m bi and V bi are the mean and standard deviations of i th block, respectively. The N 1 , N 2 , N 3 and N 4 for classes I, II, III and IV are the faults countered, respectively. The fault percent in each class can be calculated by PF i =( N i K K )/ ( M N ), i =1,2,3 and 4 (faults factors). k k and M N are the size of block and the original image, respectively, in the previous research ( Semnani et al., 2005 ). But in the present work PF i is the percent of faults volume in each class. Fig. 2 shows a part of binary image mesh ( Fig. 1 b). The vacant places indicate the faults and so y in k  X  w ok  X  y  X  f  X  y in k  X  X  5  X  where v oj , v ij , f ( x ), w ok and w jk are bias on j th neuron in hidden layer, weight on j th neuron in hidden layer and i th neuron in input layer, activation function, bias on k th neuron in output layer and weight on k th neuron in output layer and j th neuron in hidden layer, respectively ( Krose and Smagt, 1996 ; Fausett, 1994 ). The ANN is trained with the error back propagation algorithm. The used training function is Trainlm, which is based on the Levenberg X  X arquardt optimization theory because the neural network converges much faster than other training functions. The input signals are PF 1 to PF 4 and yarn count and output is fabric apparent quality.

The available data are divided into three groups. The first group is the training set. The second group is the validation set, which is useful when the network begins to over-fit the data so the error on the validation set typically begins to rise; during this time the training is stopped for a specified number of iterations (max fails) and the weights and biases at the minimum of the validation error are returned. The last group is the test set, which is useful to plot the test set error during the training process ( Demuth et al., 2007 ). The error monitored by max fails is called MSE (Mean Square Error) and it is computed with Eq. (6) ( Fausett, 1994 ). MSE  X  1 m 4. Genetic algorithm
There are some variables in the applied neural network where their variation affects on the obtained results are significant. These variables include the number of hidden layers, the number of neurons in hidden layers, the value of max fail and the percentage of validation and testing data.
 There are many methods to optimize like Particle Swarm Optimization (PSO) and Genetic Algorithm (GA). But in this study GA was applied because of its intuitiveness, ease of implementa-tion and the ability to effectively solve highly nonlinear, mixed integer optimization problems ( Uysal and Bulkan, 2008 ; Hassan et al., 2004 ). On the other hand GA provides very efficient search method working on population ( Chamber, 1995 ; Goldberg, 1989 ). GA applies some natural evolution mechanisms like crossover, mutation and survival of the fittest for optimization. Generally, there are some different steps in the GA process. In the first step, population of chromosomes is initialized.

In the second step, the designed neural network was used as a fitness function for genetic algorithm. GA sends the produced population to the fitness function. The fitness function calculates fitness of each individual, which correspond to correlation coefficient ( R value) and returns it to GA ( Demuth et al., 2007 ).
The closer the outputs are to the targets, the greater the R-value achieved. If the outputs are exactly equal to the targets, the R -value becomes 1% or 100%. Consequently, the individual that has a greater R -value is more successful than the other. In the third step, individuals reproduced to form a new population according to each individual X  X  fitness. In the fourth step, after each generation, crossover and mutation is performed on the present generation, these steps are repeated until some condition is satisfied ( Eberhart et al., 1996 ).

Many ANNs were studied with different structures and it was realized that the ANN with two hidden layers has the highest accuracy in prediction;hence, we chose this structure. Therefore percentage of testing data between 10% and 30% and number of max fail between 1 and 10 epochs.

At last, it was deduced that an ANN with properties set by 8 and 7 neurons in the first and second layers, respectively, followed by 10% of data for validation set, 20% of data for test (data selection was randomly) and 5 epochs for max fail, finally resulted in the outputs, which have the highest correlation coefficient with actual data. This structure of ANN gained in the last generation. Fig. 5 shows the training stage for the best ANN.

As there are 2 layers in ANN and error is corrected in the second layer, and thereafter the same error is corrected in the first layer again, it is possible that a sudden change occurs in the error curve slope. This phenomenon eventuated in epoch 12. The training was stopped after 19 iterations because the validation error became greater, even though error growth in testing set is crystal clear. Fig. 6 shows the testing stage of ANN in which the outputs are compared with the apparent quality index (target).
The correlation coefficient between I D and ANNs output is 0.997. To evaluate the ANN accuracy, the testing samples were given to several observers and they were requested to estimate the appearance of samples from 0 to 100 based on ASTM standard
