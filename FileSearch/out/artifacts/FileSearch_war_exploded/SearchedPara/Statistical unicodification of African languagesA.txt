 Kevin P. Scannell Abstract Many languages in Africa are written using Latin-based scripts, but letters themselves (e.g. open vowels  X  X  X  and  X  X  X  in Lingala:  X  ,  X  ). While it is possible to render these characters accurately in Unicode, oftentimes keyboard input methods are not easily accessible or are cumbersome to use, and so the vast majority of electronic texts in many African languages are written in plain ASCII. We call the process of converting an ASCII text to its proper Unicode form unicodi fi cation . This paper describes an open-source package which performs automatic unicodi-fication, implementing a variant of an algorithm described in previous work of De Pauw, Wagacha, and de Schryver. We have trained models for more than 100 languages using web data, and have evaluated each language using a range of feature sets.
 Keywords Diacritic restoration  X  Unicodification  X  Under-resourced languages  X  African languages  X  Machine learning 1 Introduction The problem traditionally known as  X  X iacritic restoration X  in the European context involves inserting appropriate diacritics into an input text given as ASCII characters, in order to restore it to its  X  X roper X  form. In Africa, many languages use Latin-based scripts that have been extended with diacritics not found in European languages, or with variants of the Latin characters themselves (  X  ,  X  ,  X  ,  X  ,...) which are available in Unicode but not in any of the 8-bit ISO 8859 character sets. We therefore propose extending the scope of the diacritic restoration problem to include the restoration of any ASCII text to its proper Unicode form, and dub this more general process  X  X nicodification X .

It is hard to overstate the importance of unicodification in the context of African languages, and under-resourced languages more generally. Much of modern natural language processing (NLP) relies on large corpora for training statistical models, corpora that are unavailable for most African languages. The web offers some hope, as more and more local language communities extend their presence on the web through blogs and other forms of online publishing. Unfortunately, for a variety of reasons (lack of proper keyboards, clumsy input methods, unfamiliarity with proper orthography, etc.), many texts found on the web are not written using proper Unicode characters. Automatic unicodification allows the construction of high-quality corpora from web data, thereby paving the way for the development of statistical NLP tools. Looking to the future, we expect the quality of web corpora themselves to improve, as integration of unicodification into authoring tools helps overcome both the lack of proper keyboards and any unfamiliarity with proper orthography that may exist in some language communities.

This project ties in closely with earlier work on the web crawler  X  X n Cru  X  bada  X  n X  (Scannell 2007 ) which has been used to produce corpora for almost 850 languages. These corpora have proved immensely valuable in developing basic technology for many under-resourced languages; they have been used in software for accessibility, predictive text for mobile devices, spell checkers and grammar checkers, machine translation engines, and even in audiometry for hearing-impaired children (Caldwell 2009 ; Haslam 2009 ). Many other researchers working on African languages are turning to the web for valuable training data as well; see in particular (De Pauw et al. 2011 ) and (Moran 2011 ) in this volume.

The Cru  X  bada  X  n corpora are also the primary source of training data for the models evaluated in this paper. Indeed, this is one of the ancillary research questions we hope to examine here: just how effective are (noisy) web corpora when used as training data for statistical NLP? Most researchers working on major languages are able to make use of high-quality corpora consisting of books, well-edited newspaper text, and the like. This is not realistic for most languages, and therefore the effectiveness of free web corpora as training data becomes an important question.
 Quite a few papers have looked at the problem of diacritic restoration for European languages (see Iftene et al. 2009 ; Spriet et al. 1997 ; Simard 1998 ; Tufis  X  et al. 1999 , 2008 ; Yarowsky 1994 ). These papers all rely on pre-existing NLP resources such as electronic dictionaries and part-of-speech taggers. Mihalcea (Mihalcea 2002 ; Mihalcea et al. 2002 ) introduced a language-independent approach based on statistics at the character level, making it well-suited for under-resourced languages. De Pauw et al. (De Pauw et al. 2007 ; Wagacha et al. 2006 ), examined this approach for a number of African languages. These latter papers were the direct inspiration for the present work; in particular, (De Pauw et al. 2007 ) calls for a  X  X urther investigation [of the machine learning approach] on a larger array of African languages X  which we have attempted to provide here. 2 Unicodification The precise meaning of unicodification rests on the definition of the inverse process of ascii fi cation . This is a deterministic mapping from a subset of all Unicode characters into (strings of zero or more) ASCII characters (Unicode 0000-007F). For reasons of space we do not give the full specification of asciification here, but most of the mappings are self-evident. Since we are focused on Latin-based alphabets, the domain of asciification lies within the following ranges:  X  00A1-00FF: Latin-1 Supplement  X  0100-017F: Latin Extended-A  X  0180-0233: Latin Extended-B  X  0250-02AD: IPA Extensions (some: e.g. 0253 in Hausa, 0254 in Lingala, etc.)  X  02B9-02EE: Spacing Modifier Letters (map to empty string)  X  0300-0362: Combining Diacritical Marks (map to empty string)  X  1E00-1EF9: Latin Extended Additional
Note that some characters (combining diacritics) map to the empty string under asciification, and others map to more than one character ( ! ae, ! ss, etc.). One could conceivably also include standard Latin transliterations of other Unicode scripts in this framework: Cyrillic, Greek, Ethiopic (Ge X  X z), etc. but we have not done so. There is also related work on diacritic restoration for Arabic script, again not considered here.

Unicodification is defined to be the (non-deterministic, language-dependent) inverse to asciification. Note that this definition is problematic for many African languages for which there is no agreed-upon  X  X orrect X  orthography, and several unicodifications of the same text are possible. Therefore the evaluations performed below on  X  X anguages X  should be taken with a grain of salt. Ideally, we would have trained and evaluated models according to  X  X riting systems X  (Streiter et al. 2006 ); e.g. for Hausa we would need to distinguish at least the following training sets:  X  X o length or tone marks X ,  X  X ith tone but no length X ,  X  X ith tone and long vowels doubled X ,  X  X ith tone and long vowels with macrons X ,  X  X ith tone, long vowels unmarked, short vowels marked with cedilla X , variants of these with the  X  X ooked y X  used in Niger, etc. We leave such extensions for future work. 3 The algorithms Our program crams together code for training, evaluating, and unicodification into about 500 lines of Perl. 1 Some of the algorithms described below assume the existence of a lexicon for the language. The lexicon is  X  X ayered X ; this means that at training time, it is possible to specify, in addition to the raw training text, a list of words known to be correct (the first layer) and also a second layer of words that are accepted as correct but perhaps reflect non-standard spellings. The third layer of the lexicon consists of words seen in the raw training text that do not appear in the first two layers.

We performed evaluations of the following algorithms (see Tables 1 , 2 )  X  BL : The baseline algorithm simply leaves all characters as ASCII. This is  X  LL : The lexicon-lookup algorithm assumes the existence of a 3-layer lexicon for  X  LL2 : This is the same as LL , but in ambiguous cases where more than one  X  FS1 : This is the first of the character-level statistical models. It is possible to  X  FS3 : Features (  X  4, 3), (  X  3, 3), (  X  2, 3), (  X  1, 3), (0, 3), (+1, 3), (+2, 3). These  X  FS4 : Features (  X  3, 3), (  X  1, 3), (+1, 3), i.e. the trigrams immediately preceding  X  CMB : This algorithm uses LL2 for words that appear in the lexicon, and for
We implemented a Naive Bayes classifier for both word-level and character-level modeling. Unicodification proceeds from left to right, treating each ambiguous character in the input as an independent classification problem (in particular, ignoring the results of any previous unicodifications). All models were trained on lowercased text, and smoothed using additive smoothing. For simplicity, the same smoothing parameter was used across all languages, independent of the size of the various training corpora, and this likely degraded performance to a certain extent. With additional time and computing power it would be an easy matter to tune the smoothing for individual languages and we plan to do this in the near future. 4 Evaluation 4.1 Experimental setup As mentioned in the introduction, the training corpora were assembled from the web using the Cru  X  bada  X  n web crawler (Scannell 2007 ). Through manual inspection of character frequency profiles, and relying on input from native speakers, we selected only documents that use the correct Unicode characters for each language. These were then segmented into sentences, and any sentences that appeared to contain pollution (English text or boilerplate text) were discarded. The training corpora were randomly sampled from the remaining sentences.

When an open source spell checker existed for a given language, we used it to generate a word list for the first layer of the lexicon. For a small number of morphologically-complex languages (Finnish, Estonian, etc.), the resulting word lists would have been much too large, so we kept only the generated words that also appeared somewhere in the full web corpus for the language.

Finally, we evaluated each of the eight algorithms described in Sect. 3 using ten-fold cross validation. We report word-level accuracy for each algorithm (following (De Pauw et al. 2007 ), where it is argued that this is a more meaningful measure than character-level accuracy). 4.2 The tables A useful measure of the difficulty of the diacritic restoration problem for a given language (the  X  X exical diffusion X  of the language) was introduced in (De Pauw et al. 2007 ). Essentially this is the average number of possible unicodifications for a given ASCII form; more precisely, it is obtained as the number of words in the lexicon divided by the number of distinct word forms after asciifying the lexicon. We found, however, that estimates of the lexical diffusion depended greatly on the corpora we used, with especially inflated values coming from noisy web corpora. Lexical diffusion may also overstate the difficulty of the problem when there exist high and low frequency word pairs with the same asciification (e.g. Romanian  X  i ( X  X nd X ) and si (rarely, a musical note))  X  these count as much as pairs that are harder to disambiguate. For similar reasons, it is also somewhat misleading to use BL as a measure of difficulty, because it makes languages that have common words with diacritics (again, Romanian  X  i ) appear more difficult than they really are.
Instead, we report in column LD1 the percentage of words in the training corpus that are incorrectly resolved by always choosing the most frequent candidate word as the unicodification. Like lexical diffusion, this measure increases when there are many possible unicodifications, but only in proportion to the frequency of the candidates in the corpus. This also makes LD1 more robust with respect to noise and more stable across corpora.

In Tables 1 and 2 , the column labeled  X 639 X  contains the ISO 639-3 code for the language, 2  X  X rain X  indicates the number of words in the training set,  X  X ex X  is number of words in the lexicon (all layers), and the remaining columns represent word-level accuracy scores for the eight algorithms, computed using ten-fold cross validation.

The best-performing feature set ( FS1 -FS4 ) for each language is rendered in italics; this is the algorithm used along with LL2 in the CMB column. As usual, the best-performing algorithm overall is marked in boldface (with some ties broken at hundredths of a percent). 4.3 Analysis of results Because we used different training sets and different machine learning algorithms (naive Bayes vs. memory-based learning), our results are not directly comparable to those reported in (De Pauw et al. 2007 ). Nevertheless, our column FS2 uses the same features as were used in that paper, and we observe that our results are in all cases lower than the ones in (De Pauw et al. 2007 ), sometimes much lower. This seems to be due almost entirely to noise in the web corpora we used for training. As a partial verification of this, we retrained the French model using a high-quality corpus (3.3M words from the Hansards) and obtained results comparable to (De Pauw et al. 2007 ) (86.6% vs. 88.3% in their paper) even with a fraction of the training data and a weaker learner. This gives a partial answer to the question raised in the introduction: the use of web texts for training in statistical NLP can have a substantial negative impact on system performance, and therefore the  X  X eb as corpus X  community would benefit from further research on corpus-cleaning algorithms (cf. the CLEANEVAL competitions (Fairon 2007 )).

The second takeaway message from the evaluation tables is that the trigram models perform consistently better than the models found in (De Pauw et al. 2007 ; Mihalcea 2002 ), and (Mihalcea et al. 2002 ). This confirms an intuition which was based upon consideration of examples like the common Irish word freisin ( X  X lso X ). Algorithms FS1 and FS2 incorrectly restore this word as fr X isin , despite the much greater prior probability of the unaccented  X  X  X . This is due in large part to the fact that there is a greater chance of seeing an  X  X  X  if you assume the previous character is for the  X  X  X  in position +3 (as exhibited by some of these same examples). On the other hand, the trigram model resolves this correctly because the full trigram -isi-almost never occurs following  X  X   X   X  but is quite common after  X  X  X  ( freisin , feisire , speisialta , seisi X n ...).
 A similar example in a more familiar language would be the word traitement in French, which FS1 and FS2 restore incorrectly as trait X ment in part because the the probability that a  X  X  X  precedes an  X  X  X . It is worth emphasizing that these are conditional probabilities: the bigram -te-is, in raw terms, much more common than -t X -, but whereas  X  X  X  is the letter most likely to precede an  X  X   X   X  in French, it is only the fourth most common letter preceding an  X  X  X  (after  X  X  X ,  X  X  X , and  X  X  X ). In contrast, all of our trigram models give the correct output traitement . Examples like these appear to be the rule rather than the exception across languages, and this is borne out by the data in Tables 1 and 2 .

A comparison of the two trigram models ( FS3 and FS4 ) shows that FS3 is superior when there is a small amount of training data available (in particular, dominating in Table 1 ), while FS4 is generally better when more data is available.
For most languages the bigram word model utilized in LL2 offers only a negligible increase in performance over LL . Not surprisingly, we see the biggest performance boost for languages with high LD1 values (more frequent ambiguities) and large training corpora for building an accurate bigram model.

Something perhaps surprising in the results is that LL2 often outperforms the combination CMB . This is saying that for words not in the lexicon , leaving them as pure ASCII is a better option than trying to restore them statistically. This is true despite the fact that all of the statistical restoration models outperformed the baseline when evaluated on the full texts. This apparent paradox is again a consequence of using noisy web data; when the lexicon is large, most of the unseen words will either be pollution (often English, no diacritics), or else words in the language but written incorrectly without diacritics, so leaving these as ASCII indeed leads to the best performance. 5 Applications We foresee many applications for our software. Many of the Cru  X  bada  X  n corpora for African languages consist primarily of ASCII text, and for those languages which are properly written with tone marks or extended Latin characters, our application offers a way to generate large corpora in the correct orthography, automatically. Even in cases where the performance of the unicodification is not perfect, it at least minimizes the amount of manual correction needed to create a high-quality corpus. We have already done this for Lingala, a language having more than a million words of text on the web, but with the vast majority being pure ASCII. The corpus obtained by unicodifying this text was used to create the first Lingala spell checker as well as a predictive text application. This is a good illustration of how the construction of a high-quality corpus opens the door to a world of statistical NLP applications as discussed in the introduction.

A second important application is search. Someone who uses proper Unicode characters in a search query might not find results that are written in ASCII, and conversely ASCII queries will not retrieve results written in the proper encoding. The Irish language offers an extreme example of this: in the 1990 X  X , an acute accent ( s X neadh fada ) in Irish was often typed as a forward slash following the vowel ( si/neadh fada , for example). Because of this, some of the largest repositories of Irish language material on the web are essentially invisible to the standard search engines.

A final obvious application of the software is the simplification of keyboard input. We would like to integrate our unicodification software into free text editors like Vim and OpenOffice.org, allowing users to enter text in plain ASCII and have the correct orthography appear on the screen  X  X agically X , even if they are not completely comfortable with the correct orthography, as is common among speakers of many African languages (Kikongo, Lingala, Kinyarwanda, etc.). We have recently made a start in this direction (together with an undergraduate student, Michael Schade) by creating a free web service and API for unicodification, as well as a Firefox add-on that implements this API. 3 See (Simard 2001 ) for related work on French.

To date, we have trained the system for 115 languages, but as can be seen especially in Table 1 , many models were trained with a minimum of data. We therefore welcome contributions of additional (or cleaner) training data for any of these languages. We are also keen to develop models for as many new languages as possible. There is sufficient training text available on the web for about 50 more Latin script languages (these are listed in the README in the charlifter package). For languages beyond these, we would welcome contributions of texts from local language communities who feel they might benefit from the software.
 References
