 Most existing approaches to outlier detection are based on density estimation methods. There are two notable issues with these methods: one is the lack of explanation for outlier flagging decisions, and the other is the relatively high com-putational requirement. In this paper, we present a novel approach to outlier detection based on classification, in an attempt to address both of these issues. Our approach is based on two key ideas. First, we present a simple reduc-tion of outlier detection to classification, via a procedure that involves applying classification to a labeled data set containing artificially generated examples that play the role of potential outliers. Once the task has been reduced to classification, we then invoke a selective sampling mecha-nism based on active learning to the reduced classification problem. We empirically evaluate the proposed approach using a number of data sets, and find that our method is superior to other methods based on the same reduction to classification, but using standard classification methods. We also show that it is competitive to the state-of-the-art out-lier detection methods in the literature based on density estimation, while significantly improving the computational complexity and explanatory power.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications X  Data Mining General Terms: Algorithms, Performance, Design Keywords: Outlier detection, active learning, ensemble method
Outlier detection is an alternative to supervised learning methods, particularly for applications in which label infor-mation is either hard to obtain or unreliable. Typical ex-18th St New York, NY 10011, USA, jl@yahoo-inc.com amples of such application areas include network intrusion detection, fraud detection and fault detection in manufac-turing, among other things. Most of the existing approaches to the problem of outlier detection in the literature have been based on density estimation methods, and in partic-ular, on nearest-neighbor methods [5, 9, 10, 13]. There are two shortcomings with this type of approach, which we recognize as potential obstacles to real world deployment of these methods. One is that it tends not to provide semantic explanation as to why a particular instance has been flagged as an outlier. The other is the relatively high computational requirement, since nearest-neighbor methods need to store all or a large part of the past examples for effective classifi-cation of future examples.

In the present paper, we exhibit a novel approach to out-lier detection based on classification, which addresses both of these issues. The approach is based on two key ideas. First, we present a simple reduction of outlier detection to classification, via a procedure that involves applying clas-sification to a labeled data set containing artificially gen-erated examples that play the role of potential outliers, in addition to the actual data. Once the task has been re-duced to classification, we can take advantage of the wealth of techniques offered by classification theory. In particular, we invoke the technique of active learning to the reduced classification problem. We do this to address some subtle shortcomings of using the artificial examples approach. In particular, direct application of this approach can fail to work, since the real world examples may adhere to some hidden constraints that the artificial examples violate, and hence it may be trivial to classify the two groups of examples apart. Even though perfect classification may be possible, this would lead to a useless classifier for outlier detection, since it just learned to distinguish the artificial examples from the real ones.

More generally, the performance of outlier detection inher-ently depends on the exact choice of the sampling distribu-tion of the artificial examples. Active learning, by virtue of the way it effectively alters the sampling distribution to fo-cus on the decision boundary between the normal examples and outliers, weakens the dependence on this distribution. Consequently, it also should be free from the shortcomings mentioned above, namely of just picking up the telltale pat-terns of artificial examples.

Specifically, we employ a selective sampling mechanism based on an active learning framework, which may be termed an ensemble-based minimum margin approach (e.g. [2, 11]). The benefits of this approach are two-fold: 1. Selective sampling based on active learning is able to 2. The use of selective sampling provides the data scal-
We empirically evaluate the effectiveness of the proposed approach using a number of data sets that are publicly avail-able. The results demonstrate that our method is superior to other methods based on the reduction to classification but using standard classification methods. In particular, our method outperforms applying bagging and boosting using the same component algorithm on the same reduced prob-lem. Indeed, active learning helps in the present context. The results also indicate that our method is competitive with the state-of-the-art outlier detection methods in the literature, such as the LOF method [5] which has been ex-tensively studied as a method for outlier detection.
We begin by presenting a non-standard model of unsuper-vised learning. Assume that the data are drawn from some probability distribution U on an instance space X . The goal in this model is to choose a  X  X ood X  partition  X  of the space X . The partition  X  divides the space X into two subspaces which we call,  X  and X  X   X  . A  X  X ood X  partition  X  contains  X  X ost X  of the points while minimizing the size of  X  . We then define the notion of error for this form of unsupervised learning as follows. Here we used B to denote the  X  X ackground X  distribution over X , such as the uniform distribution for a finite X . Note that the minimization of the first term attempts to include the set of likely events, while the second term forces exclu-sion of the unlikely events. Intuitively, the goal here is to find a  X  X mall X  (w.r.t. B ) set  X  which contains  X  X ost X  of the data (w.r.t U ).

Next, we review a standard model of supervised learning, in particular a robust model of classification in which no assumption is made about the target concept. We assume a distribution D over the input space X and the binary output space Y = { 0 , 1 } . The goal is to find a classifier h : X  X  Y with a small true error rate:
We can directly connect these two models, and in so doing transfer much of classification theory to this model of unsu-pervised learning. In particular, consider the classification problem: decide whether a point is drawn from a distribu-tion B or a distribution U . An element from the distribution D ( x, y ) is drawn via the following program: 1. Flip a coin with bias 0 . 5. 2. If  X  X eads X  Given this distribution, we can interpret any classifier as a partition, and vice versa, The error rate of the classifier (in the classifier setting) is intrinsically related to the error rate of the partition in the unlabeled data setting. Now, we can state a meta-theorem connecting true errors in classification to this setting.
Proposition 2.1. (Translation) where we let c  X  denote the classifier corresponding to the partition  X  , namely: c  X  ( x ) = 1  X  x  X   X 
Proof. = Pr The above proposition gives us a direct connection between the error rate of a classifier learned on the distribution D and the error rate of our unsupervised learner. This connection implies a direct reduction since any classifier minimizing the error rate e D ( c ) also minimizes the error rate, e U,B  X  is the partition implied by the classifier.
The proposition may seem simple, but the relationship it establishes is useful in practice, since in some sense it allows us to transfer much of what is known about classification to unsupervised learning, including theory and specific al-gorithms. In particular, here we apply the idea of active learning, which is a notion that normally makes sense only for supervised learning (classification), to outlier detection.
More specifically, we apply a selective sampling mecha-nism based on a particular type of active learning method-ology, which may be collectively termed ensemble-based min-imum margin active learning . This approach has its origins in the Query by Bagging procedure due to Abe and Mamit-suka [2], which has been applied and shown effective as a method of selective sampling from a very large data set [11]. Ensemble-based minimum margin active learning combines the ideas of query by committee [14] and ensemble method-ology for classification accuracy enhancement [8, 4]. It is given as a sub-procedure a classification algorithm, such as a decision tree learner like C4.5 or any other classifier of one X  X  choice. It works iteratively, yielding a classifier in each iter-ation by feeding a sub-sample of the input data set obtained by selective sampling to the given classification learner. The selective sampling in each iteration is dictated by a version of uncertainty sampling, which accepts those examples having lower margin , as defined by the ensemble of hypotheses, with higher probability. That is, it scans through the entire data set (or possibly some random subset of it), and performs rejection sampling using an acceptance probability that is calculated as a function of the margin.

Our method, which we call Active-Outlier , is based on the idea illustrated above and is presented in Figure 1. Note in the pseudo code that we use margin ( {  X  f 0 , ...,  X  f k  X  1 note the margin on example x by the ensemble of classifiers, namely the difference between the number of classifiers vot-ing for the most popular label and that for the second most popular label, e.g. for binary classification, We also let gauss (  X ,  X , Z ) denote  X  Z 1
This formulation of sampling probability is based on the following intuition. Consider for simplicity, the binary clas-sification problem. Suppose that n classifiers are chosen at random, each having probability one half of voting one way or another. Then the probability of obtaining k more votes for one label than the other is given by the discrete proba-bility of obtaining ( n + k ) / 2 successes in n Bernoulli trials with bias 1 / 2, and can be approximately calculated using a gaussian distribution. Given that the observed difference is indeed k for a certain example, then by the Bayes theorem, the likelihood of the hypothesis that, for that example, the two labels are equally likely to be predicted by the rest of the classifiers, is proportional to the above probability. Assume further that an inclusion of any example is equally likely to contribute to the correct labeling for that example, by the classifier obtained in the current iteration. Since correct prediction by the classifier output in the current iteration will only make a difference if the classifiers in the rest of the iterations are equally split, it should be optimal to in-clude this example with acceptance probability proportional to the above probability.

In the general scheme just described, there are a number of places that admit alternative implementations.
The choice of the underlying distribution B , or how to generate the synthetic sample S syn : This is an obvious point of confrontation, since this choice may be a potentially domain dependent one. In the present paper, we consider two alternative definitions of B : 1) uniform distri-bution within a bounded sub-space; 2) product distribution of the marginals. Since uniform distribution is not defined on an unbounded domain, we define a bounded subspace by limiting the maximum and minimum to be 10 per cent be-yond the observed maximum and minimum, then generate S syn according to the uniform distribution over the bounded domain. For the product of marginals, we estimate the mar-ginal distributions as gaussian distributions for numerical features, and by using observed frequencies for nominal fea-tures. For integer features, we then round them off to the nearest integers. Our experimental results are based on the former choice, namely the uniform distribution, except for Active-Outlier(Learner A , Samples S real , count t , threshold  X  , underlying distrubtion B ) 1. Generate a synthetic sample, S syn , of size | S real | , ac-2. Let S = { ( x, 0) | x  X  S real } X  X  ( x, 1) | x  X  S syn 3. For i = 1 to t do 4. Output h ( x ) = sign t i =1  X  i h i ( x )  X   X  ( t i =1 Figure 1: Outlier detection method using active learning those on the KDD cup 99 data. We will elaborate on this aspect in the experiments section.

The definition of margin: Normally, the margin is de-fined for classifiers  X  f i outputting { 0 , 1 } predictions. Here we use a generalization of this to probabilistic classifiers outputting conditional probabilities, to take advantage of the finer information provided by the probablities. (See, for example, [12].) More precisely, we define the margin as follows. We find empirically that this extension yields somewhat bet-ter performance.

The choice of ensemble weights  X  i : We introduce this weighting to address certain difficulties that multiple authors have noted in the literature about active learning: that inclusion of inaccurate component models into the en-semble can hurt the overall predictive performance. Here we heuristically adopt the weighting scheme of AdaBoost [8]. Experimentally, the weighting has not shown to make a big difference.

Normalizing constant for sampling probability: The rejection sampling formulation for Active-Outlier rests on the assumption that a virtually unlimited stream of exam-ples are available from which to sample. When this assump-tion is violated, rejection sampling can result in sample sizes that are too small for practical purposes. So we multiply the sampling probability by a normalizing constant ( r/ ( w i where w i is the sampling probability calculated for the i -th example and r is a pre-specified fraction) in each iteration, so that we expect to get roughly the same fraction of exam-ples in each iteration.
The idea of applying classification to outlier detection, using artificially generated examples in place of outliers, is a natural one, and has been employed by multiple authors in the recent past. Specifically, it has been applied in some empirical studies on anomaly detection tasks in concrete do-mains such as intrusion detection and image processing [7, 16]. Some theoretical treatment was also provided in our earlier work [1], as well as other work on the more general problem of density-level learning/detection using classifica-tion [3]. Recent work by Steinwart et al [15] gives a compre-hensive treatment of the classification approach to anomaly detection, based on a theoretical framework of density-level detection. The present work is different from the above body of work because we employ a sampling scheme based on ac-tive learning to classification-based outlier detection for the first time and show that this is often crucial for state-of-the-art performance.
We empirically evaluate the effectiveness of the proposed outlier detection methodology, using a number of publicly available data sets. We conduct a series of experiments to investigate different questions concerning the performance of the approach for outlier detection using active learning.
In the first set of experiments, we compare the accuracy of the proposed method against other methods that are also based on the reduction to classification, presented in Sub-section 2.1. In the second set of experiments, we compare against a well-known outlier detection method based on a modified nearest-neighbor approach, called the LOF method [5]. For the first two comparisons, we mainly use the area under the ROC curve (AUC), as well as the ROC curves themselves as the measure of success.

In the third experimentation, we focus on the specific do-main of network intrusion detection, using the well-known data set for the KDD-Cup 1999 network intrusion detection competition. For this setup, we employ the misclassification costs, which were used in the KDD-Cup 1999 competition, as the evaluation criterion, and compare the performance of the proposed scheme against the reported performance in the literature, of a couple representative methods.
The evaluation of outlier detection methods poses a cer-tain difficulty: there is no clear consensus on what is the definition of an outlier. A natural technical (statistical) de-finition is to say that a sample is an outlier if and only if it has less likelihood than a certain threshold according to the underlying distribution giving rise to the data. While this is a reasonable definition, in practice it is difficult to use this definition for evaluation. This is because we do not know the underlying distribution, unless the evaluation is done by simulation. For this reason, evaluation is usu-ally done with respect to a more pragmatic notion of outlier detection. That is, one tries to see how well an outlier detec-tion method can be used to separate a rare class of events, often associated with a meaningful notion (such as frauds, intrusions or tool anomalies). Here we take this latter view.
This type of evaluation can be conducted by making use of an existing labeled data set: giving an unlabeled training data set to the outlier detection algorithm and then evaluat-ing the accuracy of flagging examples known to belong to a different class than those included in the training data set as outliers in a test data set. As the outlier class, either a rare class or a class with semantic significance is often chosen. We selected a number of labeled data sets, which have been used in the past for evaluating outlier detection methods. In particular, we chose those data sets used by Lazarevic and Kumar [10] in their empirical evaluation, which have exhib-ited some evidence they can be reasonably viewed as outlier detection problems. That is, we choose those data sets on which outlier detection methods were found to be able to beat random guessing with some margin. (On some por-tion of the data sets used in [10], all of the outlier detection methods do no better than random guessing.)
First we present the results of a comparison between the proposed scheme of Active-Outlier using C4.5 as the base classifier learner, against two of the leading classifier learn-ing methods known in the literature, namely Bagging and Boosting, also using C4.5 as the base learner.

The data sets we used for this comparison are Mammogra-phy, 1 two versions of Ann-Thyroid, the Shuttle data, and the KDD-Cup 1999 intrusion detection data. We note that Ann-Thyroid and Shuttle are available from the UCI Machine Learning Repository (at http://www.ics.uci.edu/ mlearn/ MLRepository.html) and the KDD-Cup 1999 data set is available from the UCI KDD Archive (http://kdd.ics.uci.edu/). As we explained above, we choose one of the rare classes as the outlier class in our experiments. In deciding which classes should be the outlier classes, and other details about the experimental set-up, we basically follow [10], so that we can directly compare our results (AUC) with those reported in their paper. For example, for experiments with the KDD-cup 1999 data, we only made use of the test data set, both for training and testing by random split as in [10]. The results of this comparison are summarized in Table 1. Some examples of ROC curves for Active-Outlier and Bag-ging are also exhibited in Figure 2. (We elected not to plot the ROC curves for Boosting, as these were not very infor-mative.) It is seen that Active-Outlier is performing consis-tently, doing close to best in all cases. Bagging can work very well, although it does quite badly for the KDD-Cup 1999 data. It is quite intriguing that Boosting does not work well at all in this context. Upon a second thought, however, this may not be as surprising as it seems. We are using the classification method to solve an artificial classification problem to which we have reduced the original outlier de-tection problem. This reduced classification problem tends to be highly noisy because the artificial examples are in the background of the real ones. As it is known in the literature, Boosting tends to work poorly in the presence of high noise because it puts too much weight on the incorrectly labeled examples. In this subsection, we compare the AUC obtained by Active-Outlier with two existing outlier detection methods. The first is the well-established LOF method [9] and the second is the recent Feature Bagging method [10]. The results of this comparison are shown in Table 1. Note that the AUC figures for the two methods have been approximately calcu-lated from ROC graphs reported in [10]. These results show that in all cases, the proposed method achieves AUC that is either roughly equivalent or significantly better than that
The Mammography data set was made available by the courtesy of Aleksandar Lazarevic. of Feature Bagging, and outperforms LOF with a significant margin in all cases.
The experimental results shown in the earlier sections were with respect to outlier detection rate as summarized by AUC. In this section, we evaluate the competing meth-ods with respect to domain-specific cost information associ-ated with the network intrusion data of KDD-Cup 1999. We use only the data corresponding to the normal connections in the training data. More concretely, we use the approxi-mately 200 , 000 normal connections included in the so-called  X 10 % X  data ( X  X ddcup data 10 percent.gz X ). The synthetic data were generated using the product of the gaussian mar-ginals, rather than the uniform distribution.

We compared the performance of the proposed method against the performance of two other methods reported in the literature, one existing outlier detection method based on density estimation, and one supervised learning method. The outlier detection method we compare against is based on the so-called Parzen Window method, and is documented in [17]. The supervised learning method is the winner of the KDD cup 99 competition (c.f. [6].) We note that the su-pervised learning method is solving a different problem: (1) it uses labeled data; (2) it is solving a multi-class classifica-tion problem with the labels being one of { normal, probe, DOS, U2R, R2L } . Note that the outlier detection meth-ods output only two labels,  X  X ormal X  and  X  X ntrusion X . We therefore use a slightly modified cost matrix to evaluate the costs for both types of methods, following [17]. We note that although the costs calculated using the two cost ma-trices are not identical, the difference is almost negligible. Nonetheless, we use the modified cost matrix to evaluate all of them for a fair comparison.

Table 2 gives the results of this experimentation. What is shown is the total cost incurred on the entire test data, consisting of about 311 , 000 examples, averaged over 5 ran-domized runs. The number in parentheses is the standard error. The result for the KDD-Cup 1999 winner is the re-sult of their submitted classifier, hence it is not averaged. No information on the standard error for the Parzen Window method was readily available in the literature.

We note that in getting these results, some trials and er-rors were made on the choice of the threshold parameter  X  in our method. We wish to point out, however, that the excellent performance of this method is not so dependent on the threshold, as is evidenced by the fact that the AUC figures for this method are also very good (c.f. Figure 3.)
The Parzen Window method is a non-parametric density estimation method that basically puts a gaussian distribu-tion around each of the training examples. As such it is an Table 2: Test set costs (standard error) on the KDD-Cup 1999 data obtained by Active-Outlier(40 itera-tions), Parzen Window, and KDD-Cup 1999 winner. instance of a nearest neighbor type method, and is consid-ered to be well suited and competitive for outlier detection. Note, however, that it requires the storage of all the training data, and hence its requirement on memory is quite heavy. This is to be contrasted with our proposed method, which by virtue of its iterative sampling approach, is extremely memory light.

It is interesting to see that for this problem, outlier de-tection methods based on unsupervised learning outperform the best classification method using more information. (The cup winner used all of the 5 million labeled data, whereas both of the anomaly detection methods used (subsets of) the normal connection data included in the so-called 10% data.) This is due, in part, to the unique characteristic of this data set that the test data are drawn from a significantly different data distribution than the training data, which is realistic but makes the problem challenging. Nonetheless, this in some sense gives a strong argument for employing outlier detection based methods for certain real world anom-aly detection problems (such as network intrusion and fraud detection).
We have proposed a novel scheme for outlier detection based on the reduction of unsupervised learning to classi-fication, and active learning based selective sampling. In addition to the high detection accuracy verified with our ex-periments, the proposed approach has the advantage that it is an inherently resource light approach well suited in stream mining set-ups. In addition, the proposed approach yields high explanatory power, and we are finding that this is crit-ical in many real world applications. [1] N. Abe, C. V. Apte, B. Bhattacharjee, K. A.
 [2] N. Abe and H. Mamitsuka. Query learning strategies [3] S. Ben-David and M. Lindenbaum. Learning [4] L. Breiman. Bagging predictors. Machine Learning , [5] M. M. Breunig, H. P. Kriegel, R. T. Ng, and [6] C. Elkan. Results of the kdd X 99 classification learning [7] W. Fan, M. Miller, S. J. Stolfo, W. Lee, and P. K. [8] Y. Freund and R. E. Schapire. A decision-theoretic [9] E. Knorr and R. Ng. Algorithms for mining distance [10] A. Lazarevic and V. Kumar. Feature bagging for outlier detection. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining , August 2005. [11] H. Mamitsuka and N. Abe. Efficient mining from large databases by query learning. In Proceedings of the Seventeenth International Conference on Machine
Learning , 2000. [12] P. Melville and R. Mooney. Diverse ensemble for active learning. In Proceedings of the 21st
International Conference on Machine Learning , pages 584 X 591, 2004. [13] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for mining outliers from large data sets. In Proceedings of the ACM SIGMOD International
Conference on Management of Data , May 2000. [14] H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proc. 5th Annu. Workshop on Comput. Learning Theory , pages 287 X 294. ACM
Press, New York, NY, 1992. [15] I. Steinwart, D. Hush, and C. Scovel. A classification framework for anomaly detection. Journal of Machine
Learning Research , 6:211 X 232, 2005. [16] T. Theiler and D. M. Cai. Resampling approach for anomaly detection in multispectral images. In
Proceedings of the SPIE 5093 , pages 230 X 240, 2003. [17] D. Y. Yeung and C. Chow. Parzen-window network intrusion detectors. In Proceedings of the 16th
International Conference on Pattern Recognition (ICPR X 02) , pages 385 X 388, 2003.
