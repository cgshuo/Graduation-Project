 Instructors love to solicit feedback from students. Rich information from student responses can reveal complex teaching problems, help teachers adjust their teaching strategies, and create more effective teaching and learning experiences. Text-based stu-dent feedback is often manually analyzed by teach-ing evaluation centers in a costly manner. Albeit useful, the approach does not scale well. It is there-fore desirable to automatically summarize the stu-dent feedback produced in online and offline envi-ronments. In this work, student responses are col-lected from an introductory materials science and engineering course, taught in a classroom setting. Students are presented with prompts after each lec-ture and asked to provide feedback. These prompts solicit  X  reflective feedback  X  (Boud et al., 2013) from the students. An example is presented in Table 1.
In this work, we aim to summarize the student re-sponses. This is formulated as an extractive sum-marization task, where a set of representative sen-tences are extracted from student responses to form a textual summary. One of the challenges of sum-marizing student feedback is its lexical variety. For example, in Table 1,  X  X ike elements X  (S4) and  X  X i-cycle parts X  (S7),  X  X he main topics of this course X  (S1) and  X  X hat we will learn in this class X  (S6) are different expressions that communicate the same or similar meanings. In fact, we observe 97% of the bi-grams appear only once or twice in the student feed-back corpus (  X  4), whereas in a typical news dataset (DUC 2004), it is about 80%. To tackle this chal-lenge, we propose a new approach to summarizing student feedback, which extends the standard ILP framework by approximating the co-occurrence ma-trix using a low-rank alternative. The resulting sys-tem allows sentences authored by different students to share co-occurrence statistics. For example,  X  X he activity with the bicycle parts X  (S7) will be allowed to partially contain  X  X ike elements X  (S4) although the latter did not appear in the sentence. Experi-ments show that our approach produces better re-sults on the student feedback summarization task in terms of both ROUGE scores and human evaluation. Let D be a set of student responses that consist of M sentences in total. Let y j  X  X  0 , 1 } , j = { 1 ,  X  X  X  ,M } indicate if a sentence j is selected ( y j = 1 ) or not ( y j = 0 ) in the summary. Similarly, let N be the number of unique concepts in D . z i  X  { 0 , 1 } , i = { 1 ,  X  X  X  ,N } indicate the appearance of concepts in the summary. Each concept i is assigned a weight of w i , often measured by the number of sentences or documents that contain the concept. The ILP-based summarization approach (Gillick and Favre, 2009) searches for an optimal assignment to the sentence and concept variables so that the selected summary sentences maximize coverage of important concepts. The relationship between concepts and sentences is where A ij = 1 indicates the i -th concept appears in the j -th sentence, and A ij = 0 otherwise. In the lit-erature, bigrams are frequently used as a surrogate for concepts (Gillick et al., 2008; Berg-Kirkpatrick et al., 2011). We follow the convention and use  X  X on-cept X  and  X  X igram X  interchangeably in the paper.
Two sets of linear constraints are specified to en-sure the ILP validity: (1) a concept is selected if and only if at least one sentence carrying it has been se-lected (Eq. 2), and (2) all concepts in a sentence will be selected if that sentence is selected (Eq. 3). Fi-nally, the selected summary sentences are allowed to contain a total of L words or less (Eq. 4). Because of the lexical diversity in student responses, we suspect the co-occurrence matrix A may not es-tablish a faithful correspondence between sentences and concepts. A concept may be conveyed using multiple bigram expressions; however, the current co-occurrence matrix only captures a binary rela-tionship between sentences and bigrams. For exam-ple, we ought to give partial credit to  X  X icycle parts X  (S7) given that a similar expression  X  X ike elements X  (S4) appears in the sentence. Domain-specific syn-onyms may be captured as well. For example, the sentence  X  X  tried to follow along but I couldn X  X  grasp the concepts X  is expected to partially contain the concept  X  X nderstand the X , although the latter did not appear in the sentence.

The existing matrix A is highly sparse. Only 2.7% of the entries are non-zero in our dataset (  X  4). We therefore propose to impute the co-occurrence ma-trix by filling in missing values. This is accom-plished by approximating the original co-occurrence matrix using a low-rank matrix. The low-rankness encourages similar concepts to be shared across sen-tences. The data imputation process makes two no-table changes to the existing ILP framework. First, it extends the domain of A ij from binary to a con-tinuous scale [0 , 1] (Eq. 2), which offers a better sentence-level semantic representation. The binary concept variables ( z i ) are also relaxed to continuous domain [0 , 1] (Eq. 5), which allows the concepts to be  X  X artially X  included in the summary.
 Concretely, given the co-occurrence matrix A  X  positions. Our objective function is where  X  represents the set of observed value po-sitions. k B k  X  denotes the trace norm of B , i.e., k B k  X  = are the singular values. By defining the following projection operator P  X  , our objective function (Eq. 6) can be succinctly rep-resented as where k X k F denotes the Frobenius norm.
 Following (Mazumder et al., 2010), we optimize Eq. 8 using the proximal gradient descent algorithm. The update rule is where  X  k is the step size at iteration k and the proximal function prox t ( B ) is defined as the singular value soft-thresholding operator, prox t ( B ) = U  X  diag ((  X  i  X  t ) + )  X  V &gt; , where B = U diag (  X  1 ,  X  X  X  , X  r ) V &gt; is the singular value decomposition (SVD) of B and ( x ) + = max( x, 0) .
Since the gradient of 1 schitz continuous with L = 1 ( L is the Lipschitz continuous constant), we follow (Mazumder et al., 2010) to choose fixed step size  X  k = 1 , which has a provable convergence rate of O (1 /k ) , where k is the number of iterations. Our dataset is collected from an introductory mate-rials science and engineering class taught in a ma-jor U.S. university. The class has 25 lectures and enrolled 53 undergrad students. The students are asked to provide feedback after each lecture based on three prompts: 1)  X  X escribe what you found most interesting in today X  X  class, X  2)  X  X escribe what was confusing or needed more detail, X  and 3)  X  X escribe what you learned about how you learn. X  These open-ended prompts are carefully designed to encourage students to self-reflect, allowing them to  X  X ecapture experience, think about it and evaluate it X  (Boud et al., 2013). The average response length is 10  X  8.3 words. If we concatenate all the responses to each lecture and prompt into a  X  X seudo-document X , the document contains 378 words on average.

The reference summaries are created by a teach-ing assistant. She is allowed to create abstract sum-maries using her own words in addition to select-ing phrases directly from the responses. Because summary annotation is costly and recruiting anno-tators with proper background is nontrivial, 12 out of the 25 lectures are annotated with reference sum-maries. There is one gold-standard summary per lecture and question prompt, yielding 36 document-contains 30 words, corresponding to 7.9% of the to-tal words in student responses. 43.5% of the bigrams in human summaries appear in the responses. Our proposed approach is compared against a range of baselines. They are 1) M EAD (Radev et al., 2004), a centroid-based summarization system that scores sentences based on length, centroid, and position; 2) L EX R ANK (Erkan and Radev, 2004), a graph-based summarization approach based on eigenvector centrality; 3) S UM B ASIC (Vanderwende et al., 2007), an approach that assumes words oc-curring frequently in a document cluster have a higher chance of being included in the summary; 4) B ASELINE -ILP (Berg-Kirkpatrick et al., 2011), a baseline ILP framework without data imputation.
For the ILP based approaches, we use bigrams as concepts (bigrams consisting of only stopwords weights. We use all the sentences in 25 lectures to construct the concept-sentence co-occurrence ma-trix and perform data imputation. It allows us to leverage the co-occurrence statistics both within and across lectures. For the soft-impute algorithm, we perform grid search (on a scale of [0, 5] with step-size 0.5) to tune the hyper-parameter  X  . To make the most use of annotated lectures, we split them into three folds. In each one, we tune  X  on two folds and test it on the other fold. Finally, we report the av-eraged results. In all experiments, summary length is set to be 30 words or less, corresponding to the average number of words in human summaries.

In Table 2, we present summarization results eval-uated by ROUGE (Lin, 2004) and human judges. ROUGE is a standard evaluation metric that com-pares system and reference summaries based on n-gram overlaps. Our proposed approach outperforms all the baselines based on three standard ROUGE concept co-occurrence matrix, we notice some in-teresting examples that indicate the effectiveness of the proposed approach, shown in Table 3.

Because ROUGE cannot thoroughly capture the semantic similarity between system and reference summaries, we further perform human evaluation. For each lecture and prompt, we present the prompt, a pair of system outputs in a random order, and the human summary to five Amazon turkers. The turk-ers are asked to indicate their preference for system A or B based on the semantic resemblance to the human summary on a 5-Likert scale ( X  X trongly pre-ferred A X ,  X  X lightly preferred A X ,  X  X o preference X ,  X  X lightly preferred B X ,  X  X trongly preferred B X ). They are rewarded $0.08 per task. We use two strategies to control the quality of the human evaluation. First, we require the turkers to have a Human Intelligence Task (HIT) approval rate of 90% or above. Sec-ond, we insert some quality checkpoints by asking the turkers to compare two summaries of same text content but different sentence orders. Turkers who did not pass these tests are filtered out. Due to bud-get constraints, we conduct pairwise comparisons for three systems. The total number of comparisons is 3 system-system pairs  X  12 lectures  X  3 prompts  X  5 turkers = 540 total pairs. We calculate the per-centage of  X  X ins X  (strong or slight preference) for each system among all comparisons with its coun-terparts. Results are reported in the last column of Table 2. O UR A PPROACH is preferred significantly the inter-annotator agreement, we find 74.3% of the individual judgements agree with the majority votes when using a 3-point Likert scale ( X  X referred A X ,  X  X o preference X ,  X  X referred B X ).

Table 4 presents example system outputs. This offers intuitive understanding to our proposed ap-proach. Our previous work (Luo and Litman, 2015) pro-poses to summarize student responses by extract-ing phrases rather than sentences in order to meet the need of aggregating and displaying student re-sponses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level.
The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector re-gression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another impor-tant component of the ILP framework.

Most summarization work focuses on summariz-ing news documents, as driven by the DUC/TAC conferences. Notable systems include maximal marginal relevance (Carbonell and Goldstein, 1998), submodular functions (Lin and Bilmes, 2010), jointly extract and compress sentences (Zajic et al., 2007), optimize content selection and surface real-ization (Woodsend and Lapata, 2012), minimize re-construction error (He et al., 2012), and dual decom-position (Almeida and Martins, 2013). Albeit the encouraging performance of our proposed approach on summarizing student responses, when applied to the DUC 2004 dataset (Hong et al., 2014) and eval-uated using ROUGE we observe only comparable or marginal improvement over the ILP baseline. How-ever, this is not surprising since the lexical variety is low (20% of bigrams appear more than twice com-pared to 3% of bigrams appear more than twice in student responses) and thus less data sparsity, so the DUC data cannot benefit much from imputation. We make the first effort to summarize student feed-back using an integer linear programming frame-work with data imputation. Our approach allows sentences to share co-occurrence statistics and alle-viates sparsity issue. Our experiments show that the proposed approach performs competitively against a range of baselines and shows promise for future au-tomation of student feedback analysis.

In the future, we may take advantage of the high quality student responses (Luo and Litman, 2016) and explore helpfulness-guided summariza-tion (Xiong and Litman, 2014) to improve the sum-marization performance. We will also investigate whether the proposed approach benefits other in-formal text such as product reviews, social media discussions or spontaneous speech conversations, in which we expect the same sparsity issue occurs and the language expression is diverse.
 This research is supported by an internal grant from the Learning Research and Development Center at the University of Pittsburgh. We thank Muhsin Menekse for providing the data set. We thank Jingtao Wang, Fan Zhang, Huy Nguyen and Zahra Rahimi for valuable suggestions about the proposed summarization algorithm. We also thank anony-mous reviewers for insightful comments and sugges-tions.
