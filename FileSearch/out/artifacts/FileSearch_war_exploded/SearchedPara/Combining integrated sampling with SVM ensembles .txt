 1. Introduction
A standard two-class classification method usually makes the simple assumption that the classes to be discriminated should have a comparable number of instances. In accordance, most current classification systems are designed to optimize these systems tend to misclassify the minority class examples as majority, and lead to a high false negative rate. In such day. However, the loss of might be huge if the illegitimacy is associated with the leakage of private internal data or the disorder of website functionalities. To solve this problem, two categories of techniques have been proposed: sampling approaches and algorithm-based approaches. Generally, sampling approaches include methods that over-sample the minority  X  ent characteristics. For example, methods that are particular tailored for Decision Trees, Neural Networks (MLPs), Naive Bayes systems, etc.
 This paper is concerned with improving the performance of the Support Vector Machines (SVMs) on imbalanced data sets.
SVMs have gained success in many applications, such as text mining and hand-writing recognition. However, when the data is highly imbalanced, the decision boundary obtained from the training data is biased toward the minority class. Most ap-proaches proposed to address this problem have been algorithm-based ( Akbani, Kwek, &amp; Japkowicz, 2004; Veropoulos, decision function, including adjusting the kernel function, changing the intercept, etc. We take a complementary approach and study the use of sampling as well as ensemble techniques to improve SVM X  X  performance.

First, our observation indicates that using over-sampling alone as proposed in previous work (e.g. SMOTE Akbani et al., of sampling strategies by starting with over-sampling the minority class to a moderate extent, followed by under-sampling results that the proposed sampling approach outperforms over-sampling alone irrespective of the parameter selection.
We further consider using an ensemble of SVMs, which we call EnSVM , to boost the performance. A collection of SVMs is
SVMs. We then show that the generalization capability of EnSVM can be further improved by retaining only a subset of the component SVM classifiers, and propose a new approach called EnSVM sifier selection.

In summary, we make the following contributions: 1. We carefully design a series of experiments to demonstrate that over-sampling alone could mislead the decision bound-ary of SVM when the data are highly skewed. 2. We propose a novel strategy to combine two types of sampling methods, answering the call to achieve optimal perfor-mance by balancing the class distribution. 3. We propose the ensemble of SVM ( EnSVM ) model to integrate the classification results of weak classifiers constructed individually on the processed data, and develop a genetic algorithm-based model called EnSVM performance of classification through classifier selection. The effectiveness of the proposed models is confirmed by experiments.
 data, and Section 5 presents EnSVM and EnSVM + . In Section 6 , we describe our benchmark data and report the experimental results, and Section 7 concludes the paper. 2. Related work The class imbalance problem has recently attracted considerable attention in the machine learning community.
Approaches to addressing this problem can be divided into two main directions: sampling approaches and algorithm-based it may function as an evaluation standard in a way that if a complex algorithm for handling imbalanced data does not out-sampling approaches have obvious deficiencies: (1) Under-sampling majority instances may lose potential useful informa-dataset, which may induce much more extra computational cost. Current literatures report that under-sampling approaches conclusion has been made while the new instances introduced by over-sampling are generated in an intelligent way. As one successful example, the SMOTE algorithm ( Chawla, Bowyer, Hall, &amp; Philip Kegelmeyer, 2002 ) over-samples the minority minority instance, and then for each neighbor, it randomly selects a point from the line connecting the neighbor and the favorable results in many class imbalance studies ( Chawla et al., 2002, Chawla, Lazarevic, Hall, &amp; Bowyer, 2003 ).
Algorithm-based approaches include methods in which existing classifiers are adjusted to improve the performance for imbalanced datasets. For example, to solve the class imbalance problem, some studies proposed to re-balance the class 2004; Drummond &amp; Holte, 2003; Weiss &amp; Provost, 2003 ).

Meanwhile, SVMs have established themselves as a successful approach for various machine learning tasks, and the class imbalance issue has also been addressed in some studies. Through empirical observations, Wu and Chang (2004) report that when the data is highly imbalanced, the decision boundary determined by the training data is largely biased toward the minority class. As a result, the false negative rate that associates with the minority class might be high. To compensate for the skewness, they propose to enlarge the resolution around the decision boundary by revising kernel functions. Along
Akbani et al. (2004) combine SVMs with the SMOTE over-sampling strategy with the cost function in their approach. Instead of believing that SVMs may also suffer from the skewness of the data, there are also studies argue that SVMs are immune to boundary would change with respect to the different imbalance ratios, and explain the underlying implications accordingly.
Using an ensemble of weak classifiers to boost the classification performance has been reported to be effective in skewed induced from bootstrapping the training data, and Guo and Viktor (2004) apply data boosting to improve the performance on but most minority examples are likely to exist in the leaves of long trees ( Han &amp; Kamber, 2000 ). 3. Background
In this section, we first recall some background knowledge about how SVMs function as a classifier; then we demonstrate how they act in the context of class imbalance problem with empirical studies. 3.1. Support vector machines
In a classification problem, SVMs determine a hyperplane in the space of possible inputs. This hyperplane will attempt to tical to the training data.
 training instances Tr ={ x i , y i }, where x i 2 R N and y linearly separates the positive from negative examples in a feature space. The points x belonging to the hyperplane must satisfy for Lagrange multiplier a i ( i =1, ... , n ) in Lagrangian 1998 ). Furthermore, in the a i optimizing process, Karush Kuhn Tucker (KKT) conditions which require must be satisfied. To predict the class label for a new case x , we need to compute the sign of If the sign function is greater than zero, x belongs to the positive class, and the negative otherwise. boundary; thus form the margin between two sides. If all other training data were removed, and training process was repeated, the same separating hyperplane would still be constructed. Note that there is a Lagrange multiplier a training instance. In this context, SVs correspond to those points for which a of the decision boundary which lies right in the middle of the margin; other training points can be considered redundant. that lie far away from the decision boundary even if there are many of them.
 taining errors due to various reasons. For example, some examples may be labeled incorrectly. Besides, practical problems sequently, in the case of non-separable data, 1-norm soft-margin SVMs minimize the Lagrangian alty to errors, and l i are Lagrange multipliers introduced to enforce n have to be met for the purpose of optimization.

In addition to the above cases where positive and negative examples can be linearly separated, i.e., the decision boundary must be a hyperplane, for many real-life datasets, the decision boundary could be nonlinear. To deal with nonlinearly sep-to a feature space F via a nonlinear mapping / . To avoid inducing huge computational cost through this transformation, we can apply various kernel functions which compute dot product in feature space using only input vectors, such as Polynomial
Kernel and Gaussian RBF Kernel. The additional virtue of this kernel trick is that we would never need to explicitly know what / is. More details of SVM method can be found in many data mining tutorials and textbooks, such as Burges (1998) and Liu (2006) . 3.2. Effects of class imbalance on SVMs
To investigate how decision boundaries are affected by the imbalance ratio, i.e., the ratio between the number of negative and negative examples are the same. We then manipulate the imbalance ratio by bootstrapping different amount of exam-
SVM case with two dimensional data, and believe that the same situation may apply to the nonlinear separable case with the assist of kernel functions although the result cannot be easily displayed due to the curse of dimensionality. a consequence, making predictions with such an approach may lead to a high false negative rate. 4. Re-balancing the data
We have shown that SVMs may perform well while the imbalance ratio is moderate in Section 3.2 . Nonetheless, their per-pling techniques to balance the data. 4.1. Under-sampling
Under-sampling approaches have been reported to outperform over-sampling approaches in previous literatures. How-ever, under-sampling throws away potentially useful information in the majority class; it thus could make the decision ity, it might be undesirable to throw away 99% of majority instances.

Fig. 3 illustrates such a scenario, where the majority class is under-sampled to keep the same number of instances as in the minority class, but a considerable number of support vectors lie far away from the ideal boundary y = 1. Accordingly, making predictions with such SVMs may result in a very low accuracy. 4.2. Over-sampling
Considering that simply replicating the minority instances tends to induce over-fitting, using interpolated data is often preferred in the hope of supplying additional and meaningful information on the positive class. SMOTE is the method that has been mostly cited along this line.

However, the improvement of integrating SVMs with the SMOTE algorithm can be limited because of its dependence on how many new data points will be added into the interpolated dataset. Fig. 4 shows how the decision boundary changes with that the classification boundary is relatively smoothed when K has a small value, e.g., K = 2 in our example; nonetheless, minority class; hence over-sampling in this case should be considered as a type of  X  X  X hantom-transduction X  X . When the boundary, because SMOTE makes the assumption that the instance between a positive class instance and its nearest neigh-boundary, as shown in Fig. 4 c, could be inversely distorted to the majority class. 4.3. Combination of two types of samplings
To address problems arising from using either of the two sampling approaches alone, we integrate them together. Given an imbalance ratio, we first over-sample the minority instances with the SMOTE algorithm to some extent, and then under-sample the majority class so that both sides have the same or similar amount of instances. To under-sample the majority the strength of both strategies, and alleviates the over-fitting and information loss problems.
 a filtering process is taken on each side.
 Algorithm 1. [Impurity Pruning Algorithm (IP)]
Input : Labeled example set E ={ x i , y i }, where y i 2 { 1, 1} is the class label of example x
Output : Purified set E new 5. Ensemble of SVMs
Recently, ensemble techniques have been applied in a broad spectrum of scenarios in order to improve the performance the noise resulting from bootstrapping at random will be reduced, and the ensemble is expected to be more robust than each grows a set of trees with the CART/C4.5 algorithm to improve the average performance. However, this strategy might be problematic in our context. The reason is that decision tree-based algorithms tend to favor  X  X  X hort trees X  X  as developing off, but how to determine the appropriate level of pruning is still an open problem.

In this section, we present methods based on ensembles of SVM classifiers. We first describe a method that integrates an ensemble of SVM classifiers with aforementioned over-sampling and under-sampling strategies, and then further improve it by building selective ensembles that aim to retain only the  X  X  X est X  X  classifiers. 5.1. EnSVM: The base model
SVM in the ensemble is composed of few or even none of the minority instances. Hence, each component learner of the ensemble would suffer from severe skewness, and the improvement of using an ensemble would be limited. Our proposed method, called EnSVM , is formally presented in Algorithm 2 .
 Algorithm 2. [EnSVM]
Input : Training set E train ={ h x 1 , y 1 i , ... , h x
Output : Predicted class label for E test :{ y n +1 , ... , y the minority class is over-sampled with the SMOTE method to smooth the decision boundary ( Chawla et al., 2002 ). That is, bors, and then randomly selects a point on each line to use as a new positive instance. In this way, K n new positive we under-sample the majority class instances M times to generate M bootstrap samples so that each bootstrap sample has the same or similar size as the over-sampled positive instances. The next step is to combine each bootstrap sample (of the majority class) with the over-sampled positive instances to form a training set to train an SVM. Therefore, M SVMs can be obtained from M different training sets. Finally, the M SVMs form an ensemble to make a prediction on a test instance there is a tie in the vote. 1 5.2. EnSVM + : Selective ensembles
In EnSVM , all the individual SVM classifiers are used to reach a final classification decision. As we will show below, in bining many but not all neural networks in an ensemble, we investigate how we can improve the prediction accuracy through selective ensemble.

We first show why using only a subset of the available classifiers in an ensemble could result in better generalization accuracy. To see this, suppose there are L test instances with the target output being y the true class label. Let the actual output of the i th SVM classifier on the j th instance be ^ y
Since majority voting is used, the actual output of the ensemble on the j th instance can be expressed as ^ y x &gt;0,0if x = 0, and 1if x &lt;0.

If we consider the generalization error of a classifier on an instance to be 0 if the actual output agrees with the target the generalization error of the ensemble on the j th instance can be defined as error the ensemble is
Now let us consider what will happen if one of the component SVM classifiers is excluded from the ensemble. For the j th put of the ensemble. Therefore, in the rest of the discussion, we only focus on the cases where Suppose the k th classifier is excluded from the ensemble. Then the total generalization error becomes
Comparing Eqs. (6) and (7) , it is easy to show that there exist cases where E P E the exclusion of the k th component. As another example, consider the case where L =1, y Then using properties of the sgn function, we can show that That is, the generalization error can be reduced if the k th component classifier is excluded.
 To summarize, for an ensemble of SVM classifiers, ensembling many of them may be better than ensembling all of them.
In order to benefit from this observation, we propose EnSVM
It is worth noting that identifying the optimal subset of classifiers to be included in the ensemble can be viewed as an we propose an approximate solution based on the genetic algorithm ( Goldberg, 1989 ), which has been shown to be a suc-cessful approach to stochastic optimization.
 to evaluate the goodness of the individual classifiers. Let Err vector b on the validation set V . Apparently, a smaller Err as the fitness function used by the genetic algorithm. Formally,
The result of the genetic algorithm is an optimal bit vector b , which is used to determine which component SVM to keep in the ensemble. The resulting ensemble can then be used for prediction. 6. Empirical evaluation report the experimental results that compare our proposed approach with other methods. 6.1. Evaluation measures
The evaluation measures used in our experiments are based on the Confusion Matrix . Table 1 illustrates a confusion matrix for a two-class problem with positive and negative class values.
 Traditionally, the performance of a machine learning algorithm is usually evaluated with  X  X  X ccuracy X  X , denoted with two-class problem with class distribution of 95:5, a straightforward method of guessing all instances as negative would achieve an accuracy of 95%. In the context of class imbalance problem, however, the higher rate of correct detection on of Table 1 , i.e., any measurement which employs values from both lines will be sensitive to class skewnesses. Hence,  X  X  X ccuracy X  X  is obviously not suitable any more. Recently, some researchers in this area have realized this problem, and proposed different metrics accordingly. With the help of confusion matrix, our performance measures are expressed as follows:
G-mean  X  balanced the combination scheme is. If a classifier is highly biased toward one class (such as the majority class), the G-mean value is still low. For example, if a + = 0 and a = 1, which means none of the positive examples is identified, illustrates how the accuracy on positive instances drops with the error rate on majority instances. Informally, one point the classifier. 6.2. Benchmark data
We use eight datasets as our testbeds. Four of them are from the UCI Machine Learning Repository ( Frank &amp; Asuncion, same size, where a stratified manner is employed to ensure that the training and test sets have the same imbalance ratio.
The four UCI datasets are spambase, letter-recognition, pima-Indians-diabetes and abalone , which have been extensively adopted in evaluating the performance of various types of classifications. Here the datasets carefully are selected based on the following principles. 1. Obtained from real applications. 2. Demonstrate distinctive feature characteristics. 3. Vary considerably in size and imbalance ratio. 4. Maintain sufficient amount of instances to keep the classification performance.

Besides, we particularly choose four clinical datasets to testify the effectiveness of our proposed method as we believe dataset (mcd) from NCI is for discovering new compounds capable of inhibiting the HIV virus. set exploited in our experiments is tabulated in Table 2 . 6.3. Experimental results tiveness of our proposed data processing strategy, we then compare its performance with other similar methods. Finally, we investigate the algorithm parameter K , which allows users to fine tune the system for optimal performance, and study how the choice of various parameter values affects the classification accuracy wrt. different methods. 6.3.1. Comparison with alternative methods
In this section, we compare the performances of our proposed EnSVM and EnSVM methods: 1. single SVM without re-sampling the data, 2. single SVM with over-sampling using SMOTE ( Akbani et al., 2004 ) (without applying cost functions), 3. random forest with balanced training data from under-sampling ( Chen et al., 2004 ), 4. random forest with our combined sampling method, and 5. single SVM with our combined sampling method.

In our experiments, for all the SVMs, we employed Gaussian RBF kernels of the form K ( x
For each method we repeated our experiments ten times, computed average G-mean values and F-measures. Results in terms of G-mean are shown in Table 3 , where SVM denotes the single SVM method with the original training data, SMOTE represents over-sampling the minority class and then training a system with single SVMs, Rand-Forest sampling the majority class and then making an ensemble with C4.5 decision trees, Rand-Forest our combined method, followed by forming an ensemble with C4.5, AvgSVM denotes the average performance of 10 single
SVMs with our sampling method, EnSVM is our ensemble method with the combined sampling method, and EnSVM selective ensemble method. For the first two datasets, the K values for SMOTE , EnSVM , and EnSVM maximum value is 9 for the spambase data. 3
Tables 3 and 4 show the performance for each method in terms of the G-mean and F-measure respectively. We can see that EnSVM or EnSVM + achieves the highest G-mean and F-mean on all datasets. In particular, with our proposed EnSVM ensembles with EnSVM + may further boost the classification performance compared with using all classifiers in EnSVM , and the algorithms tend to enjoy a better performance while K is large.

Finally, to verify whether there exists a statistically significant difference between our proposal and other methods, we conduct Student X  X  t -test with a form of T = Z / s , where Z is pare the baseline SVM method with EnSVM + under two scenarios, namely EnSVM The t -values of the test are then calculated with G-mean measure, and the outcomes are 2.7255 and 2.5267 respectively. the null hypothesis is rejected in favor of the alternative hypothesis, meaning SVM and EnSVM significant test.
 studied. 1. By comparing the results from the four SVM methods, we can see that (1) using SMOTE to over-sample the data is better than SVM without any sampling; (2) using the ensemble method together with the combined sampling method achieve the best results. 2. By comparing the two Random Forest methods, using the combined sampling method is better than using only the under-sampling method on most datasets. 3. Between the Random Forest method and the ensemble of SVMs method, the ensemble of SVMs performs better. some light on this phenomenon, we studied a + and a separately with real datasets, and demonstrate the result in Table 5 .
Basically, we find that a generally outperforms a + for all datasets, and the fact becomes more prominent as the imbalance cation accuracy of some positive examples. Nonetheless, among various methods explored, EnSVM and EnSVM that got least affected. Comparing with other methods, EnSVM and EnSVM while keeping a decent true negative accuracy. In addition, we notice that compared to SVM-based approaches, under-sampling the majority followed by combining a group of decision trees merely outperforms the regular SVM in most cases. of class imbalance. 6.3.2. Effectiveness of impurity pruning
In Section 4.3 , we introduce an algorithm of filtering out impurity examples that may potentially mislead the decision boundary. In this experiment, we compare the performance of our proposal with an alternative that lacks the preprocessing then implement the following methods: (1) a single SVM without any preprocessing step, denoted as SVM; (2) a single SVM with data impurity pruning as described in Algorithm 1, denoted as SVM + IP; (3) a single SVM with our proposed method which considers both impurity pruning and data sampling, denoted as SVM + IP + Sampling; (4) the baseline EnSVM model, where K is assigned the highest value, denoted as EnSVM. We finally compare the performance in terms of G-mean and F-measure, and the results are shown in Fig. 6 .

For the case where the imbalance ratio is low, conducting various data preprocessing methods on the letter dataset does not have a significant impact in terms of G-mean and F-measure, as their values are already high in the baseline SVM method. However, for the mcd data, where the imbalance ratio is as high as 100, a clear advantage can be observed when impurity pruning is performed. In addition, adopting our proposed sampling strategy could further boost the performance (SVM + IP + Sampling), and the best result is generated by taking an ensemble (EnSVM). 6.3.3. Comparison with other data processing methods
To further validate our claim that data processing plays an important role in improving classifiers X  performance, we carry imbalance ratio of the data set. By combining a bootstrapped set of majority instances with all minority instances, we can SVMs from training sets, and make an ensemble of SVMs. Table 6 shows experimental results from such an ensemble of
SVMs on abalone and mcd datasets. These two datasets are adopted here because from Tables 3 and 4 we have realized that
Tables 3 and 4 , we conclude that this type of ensemble of SVMs does not perform as well as the ensemble of SVMs that uses our proposed sampling method. Besides, we realize that compared to G-mean, F-measure is more likely to get smaller with measure is positively correlated with its component Precision , which can be further described as
With the increase of data volume and imbalance ratio, the value of Actual Negative becomes very large rapidly compared 6.3.4. Parameter selection In addition to the imbalance ratio, the selection of K may also impact on the prediction accuracy of SMOTE , EnSVM , and ance ratio and the underlying data structure. Nonetheless, we found that the ensembled-based solution may generally out-perform other alternative methods irrespective of different K values.

To further justify our claim, we present a ROC analysis result with the spambase dataset. This dataset is considered here in this experiments, we test K from 1 to 9. Through experiments in Section 6.3.1 , we notice that SMOTE achieves a better performance comparing to other state-of-the-art methods; we hence adopt it as a representative to compare with our pro-posal. Finally, we depict the ROC curves of the two approaches in Fig. 7 . Clearly, EnSVM in general. 7. Conclusions
This paper introduces a new approach to learning from imbalanced datasets through making an ensemble of SVM clas-sifiers and combining both over-sampling and under-sampling techniques. We first show in this study that using SVMs for ance. To cope with the problem, re-balancing the data is a promising direction, but both under-sampling and over-sampling have limitations. In our approach, we integrate the two types of sampling strategies together. Over-sampling the minority class provides complementary knowledge for the training data, and under-sampling alleviates the over-fitting problem. In addition, we propose to use ensembles of SVMs to enhance the prediction performance. Through extensive experiments with real application data, our proposed method is shown to be effective and superior to several other methods with different data sampling methods or different ensemble methods. We are now working on a method for automatically determining the value of K based on the data set characteristics in order to optimize the performance of EnSVM and EnSVM Acknowledgments This work was supported by the National Natural Science Foundation of China (NSFC 60903108), the Program for New
Century Excellent Talents in University (NCET-10-0532), Communications and Information Technology Ontario (CITO), and Discovery Grants from the Natural Sciences and Engineering Research Council of Canada (NSERC). References
