 1. Introduction
In the past decade, we have witnessed a revolution in information technology. As a consequence, routine records and terabytes of information are not unusual. For example, Barclaycard (UK) carries out 350 million set is too large to be stored in a primary memory. The memory space in some computing environments can be as large as several terabyte. However, the number of observations that can be stored in primary memory is often restricted. The available memory, though large, is finite. Many computing environments also limit the maximum array size allowed and this can be much smaller and even independent of the available memory.
The large data sets present some obvious difficulties, such as complex relationships among variables, and a large physical memory requirement. In general, a simple job for a small data set may be of major difficulty variables and a response in a manufacturing application.

However, regression analysis is not straightforward for massive datasets. For efficiency, an estimator
Intuitively we may sequentially read the data and store in primary memory block by block, and analyze the procedure within each block under various computing environments. A question arisen here is how to make a estimation in terms of coverage probabilities than the usual least square estimators. 2. Block weighted least square method
This section mainly introduces the block weighted least square method proposed by Fan et al. [9] . In the of n samples such that for block j  X  1 ; ... ; k , the regression model is given by where least square estimators of the regression coefficients proposed by Fan et al. [9] is as follows: (for each i  X  0 ; 1 ; ... ; p 1)
Step 1: Split N sample points into k blocks such that each block contains n data points (and the memory is
Step 2: Estimate b i , in each block by the usual least square estimation and denote the estimator by
Step 3: Compute ^ c ij  X  X  ^ r 2 ij  X  1  X  a ij ^ r 2 j , where a Step 4: The weighted estimator of b i is ~ b H i  X  Fan et al. [9] show that these estimators achieve the minimum variance  X  estimators asymptotically and posses the asymptotic normality property, i.e.,  X   X  ~  X  cients and make variable selection from the result. 3. Main results 3.1. Block testing procedure
If we want to assure that an explanatory variable, say X i on the dependent variable of a regression model, we may consider to test H when dealing with massive datasets, Fan et al. [9] show that the block weighted least square estimator theory, the null hypothesis H 0 : b i  X  0 is rejected if the test statistic  X  the significance level a , H 0 is rejected, if the observed weighted estimate whatever the true value of b i is, (1) is almost always satisfied and thus the conclusion tends to reject H sets practically.
 results from all the k blocks. In the first stage, we individually perform the normal test for H
H 1 : b i 6  X  0 based on the usual least square estimator based on the observations in block j , and H 0 is rejected if j namely P j  X  P H
P  X  P  X  ^ b 0 ij  X  X  1 F  X j ^ b 0 ij j X  , where F  X  X  is the cumulative distribution function of
F  X j ^ b 0 ij j X  uniform  X  0 ; 1  X  ,sois P j ,if H 0 is true. Therefore, for j  X  1 ; 2 ; ... ; k , P ables when H 0 is true, so where k is the total number of blocks and the success probability p  X  P meaning H 0 being rejected in most blocks, the original hypothesis H ical value of the second stage test indeed is the smallest integer c satisfying ever, since k is also large, Z has an approximate N  X  k a ; k a  X  1 a  X  X  distribution under H of the normal approximation. More precisely, letting z a  X  U In summary, our procedure of testing H 0 : b i  X  0 is as follows: Step 1: Split sample data into k blocks with sample size n in each block.

Step 2: Compute ^ b 0 ij for block j and calculate the corresponding p -value P
Step 3: Count Z = the number of P j  X  X  which are less than the predetermined value a of Type I error, for
Step 4: If Z &gt; c H , then H 0 : b i  X  0 should be rejected. 3.2. Variables selection ysis, many techniques are used to choose significant explanatory variables among which stepwise selection, forward selection and backward elimination are the most popular ones (see [7] and the references therein).
Since all these methods are based on hypothesis testing for the regression coefficients, the problem caused selected in the model by the classical stepwise procedure in block j , and Z which indeed is the total number of blocks that variable X selection procedure. By the same reason discussed in Section 3.1 , each Z binomial  X  k ; a  X  for i  X  1 ; 2 ; ... ; p 1. Thus, if Z i and backward elimination criteria as follows: 1. Forward selection : Begin with the largest Z i associated with variable X to proceed the stepwise selection in each block with respect to the rest of the variables with X the model already. Repeat the second stage forward selection until the largest Z
The final integrated regression model consists of all the variables selected before this time. model. Next, eliminate X i from the model and repeat the same procedure to eliminate more possible vari-ables until all the variables remained in the model are significant.
 suggest the forward selection. Otherwise, the backward elimination is advised. 4. Maximum acceptable block sample size
H such scenario. We will consider the maximum acceptable sample size in each block based on the equal weights performed based on the weighted least square estimator, then by (1) , H
Letting the residual sum of squares in block j be e j  X  X  n p  X  ^ r 2 replacing ^ c ij in terms of e j in (2) , it yields an equivalent inequality
On the other hand, if equal weights, i.e. w ij  X  1 = k , are used for each block, then similarly, H or equivalently theorem shows that the maximum acceptable sample size by using equal weights, is no smaller than that by using optimal weights.

Theorem 1. Let n H and n 0 be the maximum sample sizes in a block that can tolerate using ( 3 ) and ( 4 ) , respectively, in testing H 0 : b i  X  0 , we have  X  n 0 p  X  =  X  n H p  X  P 1 , a.s. Proof. For i  X  0 ; 1 ; ... ; p 1, consider Note that the Cauchy X  X chwartz inequality yields that a.s. for each j  X  1 ; 2 ; ... ; k ,as n !1 . Thus, ~ b H i Then the proof is completed. h when the sample size in each block is over the maximum acceptable size, H block so the test may be meaningless. 5. Simulation study and real examples the same notation, let p =6, r 2 j  X  1, b 1  X  0, and the other regression coefficients  X  b from N  X  0 ; 10000  X  to have b 0  X  22 : 23, b 2  X  83 : 66, b data are simulated from the model Y l  X  b 0  X  b 1 X l 1  X  X  b b a  X  0 : 05, we consider four different combinations of k and n by the proposed block testing procedure with that about 95% of all the 1000 simulation data sets include X known to be zero. As in the second allocation, the sample size in each block is still too large such that H is still rejected too often. A finer split as in the last case indeed generates better result.
As comparison of the powers of the tests, recall that the power in testing H
H ). Specifically, it is Pr  X  Reject H 0 j b 1  X  , evaluated at the true value of b functions jump to one rapidly at a very small value of j b j b j &lt; , where is around 0.006, that is negligible in practice. We have also compared the regular stepwise than the maximum sample size allowed.

Next, we apply the proposed variable selection methods to analyze a data set obtained from the UCI web-site ( http://www.ics.uci.edu/mlearn/MLRepository.html ) which is the census of American populations from 1994 to 1995. Each sample is composed by the personal income ( Y ( X ), savings interest ( X 4 ) and the number of members in his/her family ( X approach by compared with the regular regression analysis. We will study the impact on the personal income tive and skewed with large observations, a logarithm transformation Y  X  log  X  Y assumption of the model. Table 2 presents the correlation coefficients between the variables. We see that variables.

Divide the data into 10 blocks, each contains 1654 data. We will apply the two-stage forward selection and backward elimination methods to choose the important variables and compute the block weighted least square set, the maximum acceptable size n 0 is estimated to be about 6438 for testing b b  X  0, so Lindley X  X  paradox never occurs. Since the classical method is a benchmark to be compared, it shows that our proposed procedures are quite satisfactory, especially the backward elimination approach.
To demonstrate the proposed method for massive data set, a real data set of credit cards information was analyzed. The data were taken from a database collected by the Marketing Survey Center at Fu-Jen Catholic University in Taiwan. There were N  X  2 ; 000 ; 000 observations, each contains 9 attributes, namely,
Y = monthly credit card bill, X 1 = frequency of using the credit card, X
X = number of persons in the family, X 5 = age, X 6 = monthly income, X
X = monthly family income, of the card holder. We applied the proposed two-stage block testing procedure selections obtained the same fitted model, namely,
In other words, age is not an important factor on the credit card bill. However, if all variables were used without making variables selection. Note that the selected models do not change much for k n being 200 10 ; 000, 500 4000, or 1000 2000. In contrast our computer cannot access the job for the whole data set without blocking unless we clean out the entire memory space. 6. Conclusions and discussion sets, our approach provides an alternative. We also suggest a maximum sample size used in each block which our approach is as good as the usual procedure under moderate sample size; and it provides reasonable solu-out when dealing with comparisons among regression parameters because the results can be easily extended to multivariate normal situation by the Central Limit Theorem and Slutsky X  X  Theorem [6] . posed blocking test has a competitive power function.
 with the proposed two-stage approach via simulation study. Here we just provide another viewpoint of the blocking test which may be applicable to other problems in massive data sets such as the multiple decision problems, especially when each block has different distribution. One may use the usual testing procedure to make the first stage tests individually and follow the proposed second stage test to integrate the results. reduce the computation. It needs further study in such cases.

References
