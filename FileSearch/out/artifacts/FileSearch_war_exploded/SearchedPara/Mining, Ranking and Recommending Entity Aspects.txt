 Entity queries constitute a large fraction of web search queries and most of these queries are in the form of an entity mention plus some context terms that represent an intent in the context of that entity. We refer to these entity-oriented search intents as entity aspects . Recognizing entity aspects in a query can improve various search applications such as providing direct answers, diversifying search results, and recommending queries. In this paper we focus on the tasks of identifying, ranking, and recommending entity aspects, and propose an approach that mines, clusters, and ranks such aspects from query logs.

We perform large-scale experiments based on users X  search ses-sions from actual query logs to evaluate the aspect ranking and recommendation tasks. In the aspect ranking task, we aim to sat-isfy most users X  entity queries, and evaluate this task in a query-independent fashion. We find that entropy-based methods achieve the best performance compared to maximum likelihood and lan-guage modeling approaches. In the aspect recommendation task, we recommend other aspects related to the aspect currently being queried. We propose two approaches based on semantic relatedness and aspect transitions within user sessions and find that a combined approach gives the best performance. As an additional experiment, we utilize entity aspects for actual query recommendation and find that our approach improves the effectiveness of query recommen-dations built on top of the query-flow graph.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Entity aspects; Query intent; Semantic search
With the proliferation of mobile devices, an increasing amount of available structured data, and the development of advanced search result pages, modern-day web search is increasingly geared to-wards entity-oriented search [2, 26, 29]. A first step and common strategy to address such information needs is to identify entities within queries, commonly known as entity linking [24]. Semantic information that is gleaned from the linked entities (such as en-tity types, attributes, or related entities) is used in various ways by modern search engines, e.g., for presenting an entity card, showing actionable links, and/or recommending related entities [3, 12, 19].
Entities are not typically searched for on their own, however, but often combined with other entities, types, attributes/properties, relationships, or keywords [29]. Such query completions in the context of an entity are commonly referred to as entity-oriented in-tents or entity aspects [27, 38]. In this paper we study the problem of mining and ranking entity aspects in the context of web search. In particular, we study four related tasks in this paper: (1) identi-fying entity aspects, (2) estimating the importance of aspects with respect to an entity, (3) ranking entity aspects with respect to a cur-rent query and/or user session, and (4) leveraging entity aspects for query recommendation.

The first step in identifying entity aspects involves extracting common queries in the context of an entity and grouping them based on their similarity. We perform this process offline and in-vestigate three matching strategies for clustering queries into entity aspects: lexical , semantic , and click-based . Gathering such entity aspects can already be valuable on its own since they can be used to, e.g., discover bursty or consistent entity intents or to determine entity type-specific aspects [27].

In the next step we rank the obtained entity aspects for each en-tity in a query-independent fashion using three distinct strategies. This provides us with a mechanism to retrieve the most relevant aspects for a given entity on its own, which, in turn, can be used to, e.g., summarize the most pertinent information needs around an entity or to help the presentation of entity-oriented search results such as customized entity cards on SERPs [2].

The third task that we consider is aspect recommendation. Given an entity and a certain aspect as input, recommend related aspects. This task is motivated by the increasing proliferation of entity-oriented interface elements for web search that can be improved by, e.g., (re)ranking particular items on these elements. Recom-mending aspects for an entity can also help users discover new and serendipitous information with respect to an entity. We consider two approaches to recommend aspects: semantic and behavioral . In the semantic approach, relatedness is estimated from a semantic representation of aspects. The behavioral approach is based on the  X  X low X  of aspect transitions in actual user sessions, modeled using an adapted version of the query-flow graph [5, 6, 34].

In our final task we leverage entity aspects for actual query rec-ommendation, i.e., helping users refine their query and/or to help users accomplish a complex search task [11, 22]. Most methods for query recommendation are similar to the behavioral approach men-tioned above and based on query transitions within sessions. They do not commonly utilize semantic information, however, which may cause distinct but semantically equivalent suggestions. We aim to ameliorate this problem by utilizing the semantic informa-tion captured through the entity aspects for query recommendation.
We perform large-scale experiments on both a publicly avail-able and a commercial search engine X  X  query log to evaluate our proposed methods for mining, ranking, and recommending entity aspects, as well as for recommending queries. We perform con-trastive experiments using various similarity measures and ranking strategies. We find that entropy-based methods achieve the best performance compared to maximum likelihood and language mod-eling on the task of entity aspect ranking. Concerning aspect rec-ommendation we find that combining aspect transitions within a session and semantic relatedness give the best performance. Fur-thermore, we show that the entity aspects can be effectively utilized for query recommendation.

Our main contributions can be summarized as follows: After a discussion of related work in Section 2, we formalize our task in Section 3. We then detail our approaches to mining, ranking, and recommending entity aspects in Section 4. A detailed account of experiments and data is given in Section 5. We discuss the results of our experiments in Section 6 and conclude in Section 7.
In this section we review related work around three main top-ics: query intent mining, leveraging entity aspects for search, and search task identification.

Intent mining deals with identifying clusters of synonymous or strongly related queries based on intents , which are typically de-fined as  X  X he need behind a query. X  An intent (sometimes also referred to as an aspect ) is commonly defined as a set of search queries that together represent a distinct information need relevant to the original search query. Methods for identifying intents are typically based on the query itself, results returned by a retrieval algorithm, clicked results, or any other actions by the user. Hu et al. [13] leverage two kinds of user behavior for identifying query  X  X ubtopics X  (which can be interpreted as intents): one subtopic per search and subtopic clarification by keyword. They propose a clus-tering algorithm that can effectively leverage the two phenomena to automatically mine the major subtopics of queries. They repre-sent each subtopic as a cluster containing a number of URLs and keywords. Cheung and Li [8] present an unsupervised method for clustering queries with similar intent and producing patterns con-sisting of a sequence of semantic concepts or lexical items for each intent. They refer to this step of identifying patterns as intent sum-marization . They then use the discovered patterns to automatically annotate queries.

Other related work focuses on extracting attributes from queries, either unsupervised or in the context of entities from a knowledge base. For instance, Li et al. [16] propose a clustering framework with similarity kernels to identify synonymous query intent  X  X em-plates X  for a set of canonical templates. They integrate signals from multiple sources of information and tune the weights in an unsu-pervised manner. Li et al. [17] on the other hand, solely aim to discover alternative surface forms of attribute values. They pro-pose a compact clustering framework to jointly identify synonyms for a set of attribute values. In a similar vein, Pasca and Van Durme [28] describe a method for extracting relevant attributes or quan-tifiable properties for various classes of objects . They utilize query logs as a source for these. Yin and Shah [38] propose an approach for building a hierarchical taxonomy of generic search intents for a class of named entities. Their proposed approach finds phrases representing generic intents from user queries and organize these phrases into a tree. They propose three methods for tree building: maximum spanning tree, hierarchical agglomerative clustering, and pachinko allocation model. These approaches are based on search logs only. Moving beyond entity types, Lin et al. [19] introduce the notion of active objects in which entity-bearing queries are paired with actions that can be performed on entities. They pose the prob-lem of finding actions that can be performed on entities as the prob-lem of probabilistic inference in a graphical model that captures how an entity-bearing query is generated.

Another body of related work deals with alternative presenta-tions of search results, e.g., based on intents [9]. For instance, Balasubramanian and Cucerzan [2] propose a method to generate entity-specific topic pages as an alternative to regular search re-sults. Similarly, Song et al. [32] present a model to summarize a query X  X  results using distinct aspects. For this they propose  X  X om-posite queries X  that are used for providing additional information for the original query and its aspects. This works by comparatively mining the search results of different component queries. Wu et al. [37] mine latent query aspects based on users X  query reformulation behavior and present a system that computes aspects for any new query. Their system combines different sources of information to compute aspects. They first discover candidate aspects for queries by analyzing query logs. They then use a knowledge base to com-pute aspects for queries that occur less frequently and to group as-pects that are semantically related. Finally, Spina et al. [33] explore the task of identifying aspects of an entity given a stream of mi-croblog posts. They compare different IR techniques and opinion target identification methods for automatically identifying aspects.
Search task identification deals with determining the specific task a user is aiming to solve. Such information enables a search en-gine to, e.g., suggest relevant queries and/or results. Jones and Klinkner [14] formalize the notion of a search goal as an atomic information need which results in one or more queries. They pro-pose a method for the automated segmentation of a users X  query stream into hierarchical units. While a search goal is atomic, a se-ries of search goals then form a search missions (or complex search tasks ). Lucchese et al. [20] also aim to identify user search tasks within query sessions. They cluster queries in order to find user tasks , defined as a set of queries that is aimed towards the same in-formation need. Later they expand user task detection across user sessions [21], similar to so-called task trails and long-term search tasks [18, 36]. Li et al. [15] model the temporal influence of queries within a search session and then use this temporal influence across multiple sessions to identify and label search tasks.

There exists a large body of work on query recommendation, i.e., suggesting follow-up queries to a user, either in an ad hoc fashion or in the context of a user X  X  session or task. Boldi et al. [5] introduces the notion of query-flow graph for query sugges-tion and Szpektor et al. [34] later expand this model to increase coverage for long tail queries. Bonchi et al. [6] expand it even further to improve coverage. Feild and Allan [10] show that us-ing contextual information can improve query recommendation, as
Table 1: Glossary of the main notation used in this paper. t a term q a query e an entity s a query segment, i.e., a sequence of terms a an entity aspect, consisting of zero or more query seg-
A e the set of entity aspects for e d time span long as the previous queries in the context involve a similar or re-lated task. Hassan Awadallah et al. [11] capitalizes on this idea and propose grouping together similar queries and then using them for query recommendation for complex search tasks, similar to task-specific recommendations [22]. Finally, Verma and Yilmaz [35] extract common tasks in the context of an entity to improve retrieval through query expansion and query term prediction. They extract terms frequently appearing with an entity and aggregate this type of information to an entity type level to obtain a dictionary of entity tasks. They evaluate their work through query term prediction and query expansion.

Our work is different in the following major ways. First, we ex-tract entity aspects from query logs specifically. Second, we weight these aspects and assign their importance with respect to an entity on its own in an ad hoc fashion, i.e., without any user, session or query-based information. Third, we learn their relatedness using semantic and behavioral approaches. Finally, we propose an entity aspect-based query recommendation algorithm building upon the query-flow graph.
In this paper, we study three related entity-oriented tasks that are elemental in modern-day entity-oriented web search: identifying , ranking , and recommending entity aspects. Although they build on one another, we propose effective methods for each of them separately since they are essential building blocks for information access applications on their own as well.

In our methods and experiments we employ user interaction log data in the form of queries and clicks. Formally, such logs can be represented as a sequence of events where each event is an action taken by a user. For each event we store a timestamp, a user ID, and the type of action; these are limited to queries and clicks in our current case. Furthermore, the logs are divided into time-ordered sessions, h  X  H , for each user where we use a common segmen-tation method and begin a new session after a predefined period of inactivity (30 minutes unless indicated otherwise).

We formulate the first task of identifying entity aspects as fol-lows. Given an annotation function  X  e : Q  X  E that assigns entities from the set of all entities E to queries, we detect  X  X ntity-bearing queries, X  i.e., queries Q e containing an entity e . Then, for each entity, we mine a set of entity aspects: A e = { a 1 from Q e representing the key search tasks in the context of that entity. Table 1 details the main notation we use in this paper. We employ the following definition of search tasks and entity aspects. Once entity aspects have been identified we turn to ranking them. That is, we estimate the importance of each aspect with respect to the entity in a query-independent fashion and rank them accord-ingly. The obtained ranking can be interpreted as a distribution of prior probabilities over users X  information needs on the entity and can be used on its own to, e.g., prioritize an entity display. For entity aspect recommendation, we recommend related aspects in the context of the entity given an entity-bearing query. We also study this problem in a context-aware setting, incorporating previ-ous queries within the search session. Finally, for query recommen-dation we drop this restriction and include all queries in a session.
In this section, we introduce our methods for each of the tasks introduced in the previous section. To mine aspects for an entity, we first need to identify all queries Q e that contain entity e from the query log using an annotation function  X  e . Since an entity may be referred to in various ways, we need an effective method for identifying entity mentions in web search queries. Since web search queries are typically short and not grammatically correct [31], we rely on a fairly simple method for entity linking that has been shown to obtain strong performance on such texts [23, 24]. In particular, for each query we generate all possible segmentations and link them to Wikipedia articles. Fol-lowing [24], we use the CMNS method to generate a link for query segment s : where H s,e denotes the set of all links with anchor text s which points to target e in Wikipedia. We start with the longest possible query segments and recurse to smaller n-grams in case no entity mention is detected. In case a segment matches multiple entities, we take the most  X  X ommon X  sense, i.e., the one with the highest CMNS score. We do not specifically evaluate the performance of our linking method for queries in this paper. However, in a recent comprehensive comparison on entity linking for queries, CMNS proved to be a very strong unsupervised baseline for this task [4].
Now that we have the set of all queries containing entity e , we remove the mentions of e from the queries and use the remaining segments as query contexts . If the query contains more than one en-tity, we simply consider each entity on its own with the remainder of the query as its context. In this manner we obtain S e all context segments which appear with entity e . We then cluster the contexts s  X  S e such that context segments which have the same intent are grouped together. We consider the following features for clustering: lexical , semantic , and click similarity, covering spelling differences/errors, related words/synonyms, and behavioral infor-mation. Below we detail the specific methods for each.
 Lexical similarity. We compute the lexical similarity between two query contexts using the Jaro-Winkler distance, computed as fol-lows: where m is the number of matching characters, and m 0 is half the number of transpositions between the two query context segments. Semantic similarity. To compute the semantic similarity between two query contexts s i and s j , we use word2vec [25], sum the vec-tors of each term within the queries, and determine the cosine dis-tance of the resulting vectors: where z ( s ) is a function that calculates the semantic vector of s . Click similarity. Beside lexical and semantic similarity, we also utilize click similarity. In particular, for each query context s  X  S we obtain all clicked hostnames for all queries containing e and s and combine them into a click vector. We then compute the click similarity between two query segments s i and s j using their cosine similarity: where c i and c j are click vectors of query s i and s j , respectively. The final segment similarity score is calculated by taking the max-imum value of the similarity scores.

In order to cluster query contexts into entity aspects, we then em-ploy Hierarchical Agglomerative Clustering (HAC) with complete linkage. We flatten the obtained hierarchical clusters so that for ev-ery object s i , s j belonging to the same cluster, sim ( s By the end of this step, we have obtained the entity aspects A the form of the query context clusters.
The main goal of entity aspect ranking is to estimate the im-portance of each aspect in the context of an entity in a query-independent fashion. Given an entity and its aspects as input, the output of this task is a list of aspects that is ranked according to their pertinence to the entity. We consider three methods for rank-ing aspects given an entity. The first model is based on maximum likelihood where we reward more frequently occurring aspects: Here, n ( s,e ) denotes the number of times query segment s is queried for in the context of e . Note that this method will not simply place clusters with most members at a higher rank, since there might be clusters with few members in which the members occur more fre-quently than in a large cluster.

The second model uses entropy-based scoring where we reward the most  X  X table X  aspects using different time granularities includ-ing months, weeks, and days. For instance, in the case of days we partition the query log into daily chunks and count the number of times completion s is queried for in the context of e on that day. We then determine the entropy: where D is the set of all time units and P ( a | d,e ) the probability of observing any s  X  a in the context of e on time interval d (we omit the minus sign in order to make these scores comparable). In another variant we determine the joint entropy, incorporating a factor p ( d | e ) : where P ( a,d | e ) is the joint probability of observing any s  X  a and time interval d in the context of e .

The third and final model is based on language modeling and aims to rank aspects by how likely they are generated by a statisti-cal language model based on a textual representation of the entity. More formally, we introduce the following three variants: where P ( s |  X  e ) is determined using maximum likelihood estimation with Dirichlet smoothing: Here, r ( e ) is the textual representation of e , for which we consider either the entity X  X  Wikipedia article text, LMW , or the frequency-weighted aggregation of all queries leading to a click on the en-tity X  X  Wikipedia article, LMC . P ( t ) is the probability of term t estimated from all textual representations of the type at hand. We set  X  to the default value of the average document length. Note that the main difference with the MLE method introduced earlier lies in the fact that the LM approaches are based on unigram term proba-bilities whereas the MLE estimates operate at the segment level.
The goal of this task is to recommend aspects related to the en-tity and aspect currently being queried. Given such an entity and aspect pair as input, the output of this task is a ranked list of as-pects. We consider two methods for recommending entity aspects: a semantic and a behavioral approach. In the semantic approach, we determine the aspects X  relatedness based on semantic similarity. In the behavioral approach, we estimate the relatedness from user sessions, inspired by the query-flow graph [5].

We use a graph-based representation for representing entity as-pect relatedness. There are two motivations for this decision. First, having a graph-based representation allows the relatedness of the entity aspects to be computed offline. At retrieval time, we retrieve the candidates and rank them based on previously estimated relat-edness. Secondly, having this data encoded as graph allows ad-vanced graph-based compression and recommendation techniques to be applied in the future. We construct a different type of graph for each approach: an aspect-semantic graph and an aspect-flow graph, respectively.
For the semantic approach we define the aspect-semantic graph for an entity e as an undirected graph G as = ( V,L,w ) where: We construct G as using Algorithm 1 and compute the relatedness between two aspects a i ,a j using: similar to (1). One main difference with (1) is that z is now com-puted from the mean of the semantic vectors of all query contexts belonging to a . If the relatedness score is above a threshold  X  , we construct an edge between aspect a i , and a j and assign score as weight w ij . Algorithm 1 Constructing an aspect-semantic graph for e . Input: Aspect list: A e Output: Aspect-semantic graph: G 1: G  X  initializeGraph ( A e ) 2: for each a i  X  A do 3: for each a j  X  A do 4: w  X  sem ( a i ,a j ) 5: if w &gt;  X  then 6: e  X  createEdge ( a i ,a j ,w ) 7: G  X  G  X  e 8: end if 9: end for 10: end for Algorithm 2 Constructing an aspect-flow graph for e .
 Input: Aspect list: A e , User sessions: H Output: Aspect-flow graph: G 1: G  X  initializeGraph ( A e ) 2: for each a i  X  A do 3: for each a j  X  A do 4: w  X  adj ( H,a i ,a j ) 5: if w &gt;  X  then 6: e  X  createDirectedEdge ( a i ,a j ,w ) 7: G  X  G  X  e 8: end if 9: end for 10: end for
The second approach is based on the query-flow graph. For-mally, we define an aspect-flow graph as a directed graph G ( V,L,w ) where: Here, we estimate the relatedness between query aspects from user sessions. We determine the relatedness from the adjacency of the aspects: where countAdjacent ( h,a i ,a j ) denotes the frequency of query transitions of any query segment s  X  a i to any segment in a in the user session h , i.e., how often a j follows a i . We construct the aspect-flow graph for each entity with Algorithm 2. First, we construct a node for every aspect a that occurs in the user sessions, H . Then, for every pair of aspects ( a i ,a j ) , we compute their ad-jacency in H . We create an edge between two aspects a i if the adjacency is above a threshold  X  . We assign the adjacency count, normalized to transition probability, as the weight w We utilize the aspect-semantic graph G as and aspect-flow graph G af to generate recommendations for an input aspect a in the con-text of entity e . In the first variant, we generate aspect recommen-dations without a user session X  X  context as detailed in Algorithm 3. Algorithm 3 Aspect recommendation.
 Input: Aspect graph: G , Input aspect: a Output: Ranked aspects: R 1: C  X  getCandidatesFromNeighbors ( G,a ) 2: for each ca  X  C do 3: score [ ca ]  X  getWeight ( G,a,ca ) 4: end for 5: R  X  rankCandidates ( score ) Algorithm 4 Context-aware aspect recommendation.
 Input: Aspect graph: G , Input aspect: a , Search context: S Output: Ranked aspects: R 1: C  X  getCandidatesFromNeighbors ( G,a ) 2: for each ca  X  C do 3: score [ ca ]  X  getWeight ( G,a,ca ) 4: end for 5: for each p  X  S do 6: score p  X  decay ( a,p )  X  getWeight ( G,p,ca ) 7: score [ ca ]  X  score [ ca ] + score p 8: end for 9: R  X  rankCandidates ( score ) Here, we retrieve candidate recommendations from all nodes ad-jacent to a in G . For G af , we only retrieve neighboring nodes connected by the outgoing links from a .

We combine the output of both methods to improve the cover-age and effectiveness of our recommendations. First, we combine the outputs with a simple round robin strategy, alternating the re-trieval of recommendations from the behavioral and the semantic approach, respectively. The intuition is that the semantic method will be able to complement the behavioral method, since it will have higher coverage if constructed with a relatively low threshold  X  . We also experiment with another combination method: convex combination . We retrieve the scores generated by the behavioral and semantic approaches and combine them with a weight  X  : score ( a,a 0 ) =  X   X  flow ( a,a 0 ) + (1  X   X  )  X  semantic ( a,a Since the scores are on a different scale, we perform min-max nor-malization to each score before combining them. Due to our graph construction process there might be cases where either method can not provide any score; for these we simply assign a zero score.
In a variant of this method, we incorporate context-awareness by looking at the previous queries in a user X  X  search session as detailed in Algorithm 4. First, we retrieve the recommendation candidates from the neighbors of a in G . Then we compute initial recommen-dation scores for each of them and, lastly, we incorporate scores from any previous aspect a 0 within the search context S , dampened based on their distance: where  X  is a decay constant, and | a  X  a 0 | indicates the distance between a and a 0 . The distance is the number of aspects queried by the user between a and a 0 in the current session S .
So far, we have focused on problems and approaches for rank-ing and recommending aspects involving the same entity. In this section, we detail how we leverage entity aspects for query recom-mendation in general. That is, recommending other entities, other entity aspects, or regular/non-entity queries for a given query. We Algorithm 5 Generating query recommendation (QFG+A).
 Input: Input query: q , Query graph: G Output: Ranked recommendations: R 1: for each q  X  Q do 2: q  X  X  X  annotateQuery ( q ) 3: n q  X  matchToNode ( G,q  X  ) 4: S  X  getCandidates ( G,n ) 5: for each n ca  X  S do 6: score [ n ca ]  X  getWeight ( G,n q ,n ca ) 7: end for 8: R  X  rankCandidates ( score ) 9: end for complement a state-of-the-art query recommendation method X  X he query-flow graph [5] X  X ith information from the entity aspects.
We first apply the information from entity aspects when con-structing the query-flow graph. We preserve all other queries that are not entity queries, thus forming the query nodes as in a regu-lar query-flow graph. For an entity-bearing query q e , we link all mentions of an entity e . Next, if the query contains additional query context, we extract the context segment s from q e we match s to an appropriate aspect a in the aspect model A e . We perform this matching by finding a which contains s as its cluster member. We collapse different mentions of the same entity into one entity node and collapse semantically-equivalent queries into one entity aspect node. This way, we obtain a  X  X emanticized X  query-flow graph.
 Lastly, we introduce our recommendation method, detailed in Algorithm 5. For every input query, we perform entity linking of the query to detect entity bearing queries (the annotateQuery func-tion). Next, we match an entity query to a node (the matchToNode function) with the similar procedure applied during graph construc-tion. Regular queries will be matched straightforwardly. Lastly, we retrieve recommendation candidates from adjacent nodes, scored by their weights in the graph.
In this section we detail our experimental setup, including the data we use, the relevance assessments, 1 and the metrics we em-ploy. Our experiments below address the following research ques-tions: RQ1 When mining entity aspects, how do different similarity mea-RQ2 How do different aspect ranking methods compare on the RQ3 How do the semantic and behavioral approaches compare on RQ4 Does incorporating context improve aspect recommendation? RQ5 Can we leverage the semantic information captured through
To answer our research questions we set up 4 experiments, which we describe below. In our experiments we test for statistical signif-
Our relevance assessments and editorial guidelines are available at http://ridhorei.github.io/entity-aspects/ . icance using a paired t-test, indicating significantly better or worse results at the p &lt; 0.01 level with N and H respectively. Evaluating mining entity aspects. In this experiment, aimed at answering RQ1, we evaluate the quality of the extracted entity as-pects by manually evaluating the generated clusters. We use a set of 50 entities sampled from user logs in a stratified fashion. That is, we bias the sample such that more popular entities are more likely to be included. We then extract the query completions for each entity over a period of time from the dev-contexts collection (introduced in the next section). To obtain ground truth data we manually cluster the query segments by grouping those that repre-sent the same aspect together. To evaluate the quality of each entity aspect we employ commonly used cluster quality metrics: B-cubed recall (B-recall), precision (B-precision), and F1 (B-F1) [1]. Evaluating ranking entity aspects. The second experiment is aimed at answering RQ2. Since manually evaluating aspect rank-ings for entities without any explicit query is not straightforward, we resort to automatic evaluation. We propose an automatic evalu-ation based on what we call  X  X nderspecified X  entity queries, that is, queries that contain only an entity. We rely on the assumption that a good aspect ranking is one that, on average, best satisfies users that issue such underspecified entity queries. Specifically, we con-sider sessions that contain an underspecified entity query and aim to predict any subsequent queries that again contain the entity, plus additional query terms.

For this experiment we consider one month of query logs (the test-aspect-ranking collection) that is disjoint from any log data used for training). Because of this disjointness there might be as-pects that our method is unable to predict, simply because they have not been seen before. This includes spelling variants, reformula-tions, and new aspects. In our experiments below we do keep them as relevant samples in the evaluation data in order to mimic a real-life setting as closely as possible.

We consider the following setup: we aim to predict the next query a user issues in a session, only considering pairs of adja-cent entity-bearing queries in the session where the second query contains the same entity plus additional query terms. We then ob-serve at which position our method ranks this subsequent query and score it accordingly. Since we only have pairs of queries and thus only one relevant suggestion for each, we report on mean reciprocal rank (MRR) and success rate (SR).
 Evaluating recommending entity aspects. The third experiment is aimed at answering RQ3 and RQ4. Here, we evaluate the effec-tiveness of recommending entities and aspects in the context of a user session and constituent queries. We follow a similar evaluation approach to the ranking task above, i.e., we consider the next query in the session as the target to predict. As such we again report on mean reciprocal rank (MRR) and success rate (SR).

Since detecting entity-dominated sessions is not trivial, we sim-ulate them through the following procedure. First, we extend the session demarcation boundary, effectively merging the sessions be-longing to the same user within a 3-day timeframe (the test-aspect-recommendation collection). Then, we consider the first entity within these extended user sessions as the reference entity and eval-uate the recommendation methods by their effectiveness in predict-ing subsequent aspects of the entity throughout the remainder of the session. This setup reflects recommending related entity aspects for complex search tasks in the context of an entity.
 Evaluating query recommendation. Our fourth and final experi-ment addresses query recommendation and is aimed at answering RQ5. Here we evaluate actual query pair predictions, following the automatic evaluation method from [34]. We sample 1,000,000 query sessions from the query logs of a commercial search en-gine (the test-query-recommendation collection) and extract pairs of adjacent queries from the sessions. Queries belonging to same entity aspect are treated as equivalent queries during evaluation. We evaluate this approach in two configurations: looking at all queries within the sessions ( all-pairs ), and using only the first and last queries ( first-last pairs ). Furthermore, we also differentiate be-tween using all query pair occurrences (allowing possible dupli-cates of popular queries pairs) and using distinct occurrences only. Our main evaluation metrics for this experiment are again mean reciprocal rank (MRR) and success rate (SR). To gain additional insights, we also look at the fraction of correct predictions at dif-ferent recommendations cut-off levels: 100 and 10.
We use two sources of data for training and testing, including user logs of the Yahoo web search engine as well as the AOL query logs. From the former we sample a number of datasets. All the development and test datasets that we use are disjoint, i.e., they are sampled from non-overlapping time periods. The dev-contexts dataset is a large, 1-year query log sample containing queries that we use to build the full aspect model for our set of entities. The dev-clicks dataset is a 1-month sample used to compute click sim-ilarity for the context terms. We build our query-flow graph and the aspect-flow graph on the dev-flow dataset (a 1-month sample). The test datasets, test-aspect-ranking , test-aspect-recommendation , test-query-recommendation , are all 1-month samples and unseen query logs that are used in our automatic evaluation methods for our second, third, and fourth experiment, respectively. In addition, we also utilize the publicly available AOL dataset in our second experiment. This last dataset includes queries sampled from March 2006 until May 2006.

We define navigational queries as queries that are in the top-40% in terms of the number of pageviews and that also lead to a click on the top search result in at least 40% of the cases. We detect and subsequently discard navigational queries based on this heuristic.
We perform entity linking and context term extraction using the method described in Section 4.1. Below we focus on Wikipedia entities and we leave using other entity repositories for future work. In order to reduce data sparseness we remove entities that occur in less than 100 queries. This results in a set of about 75k entities of interest.

Our approach involves several parameters. The first parameter,  X  is the similarity threshold used for clustering. From a prelimi-nary experiment on held-out data, we obtain the optimal value of  X  = 0 . 75 . For the minimum relatedness score in the construction of the aspect-semantic graph, we set  X  = 0 . 1 . Following the com-mon practice in constructing a query-flow graph [5, 7], we retain only transitions that appear at least two times, thus  X  = 1 . After a preliminary experiment, we set  X  = 0 . 85 when combining the semantic and flow scores. The decay parameter is set to  X  = 0 . 85 .
In this section we answer the research questions presented in the previous section.
Our first experiment concerns mining entity aspects. We start by evaluating the quality of each cluster and then zoom in on the as-pects generated during the mining process. Table 2 presents the re-sults of using different matching strategies to cluster query context terms into entity aspects. Recall that we perform complete-linkage clustering with a parameter  X  as threshold for grouping objects. Table 2: Entity aspect mining results. Significance is tested against the lexical method (row 1).
 Method B-Recall B-Precision B-F1 Lexical 0.9164 0.8258 0.8338 Semantic 0.9452 N 0.7744 H 0.8117 H Click 0.8977 0.6666 H 0.6880 H Lexical + semantic 0.9216 0.8629 N 0.8607 N Lexical + click 0.8480 H 0.8155 H 0.7842 H Semantic + click 0.8686 H 0.7788 H 0.7680 H Lexical + semantic + click 0.8558 H 0.8465 N 0.8098 H Table 3: Entity aspect mining: clustering output for entity Paris Saint-Germain F.C. .

Cluster Context terms
First, we look at the results of individual similarity measures. As we can see, using just lexical matching already results in a fairly good B-cubed recall and precision score. This can be explained by the fact that the lexical matching strategy allows minor changes caused by spelling variants or spelling errors, and is successful in performing clustering on these cases. Semantic similarity achieves higher recall at the cost of precision. This means that the clustering method with the current threshold clusters the object aggressively, for example, grouping aspects such as  X  X aughter X  and  X  X other X  together. Click similarity has the lowest precision compared to the other two measures for reasons that we will explain below.
Although lexical similarity provides a good start, this strategy fails to group queries that are semantically related. Thus, combin-ing it with semantic similarity improves recall and precision. This combination proves to be the best performing one, compared to us-ing individual measure and all measures.

Adding click similarity with our current strategy does not work well. Upon closer inspection, we find that a lot of unrelated query contexts point to the same host name. For example, contexts related to an entertainer X  X  news are often directed to the same entertain-ment site. Therefore, combining clicks with other measures tends to brings down the performance, in particular precision.

Table 3 shows sample output generated by our aspect mining method for the entity Paris Saint-Germain F.C. (a Parisian soc-cer club). Our manually created clusters are shown in Table 4. The aspect mining produces more clusters than the ground truth. The method fails to group  X  X arca, barca vs, barcelona, barcelone, barcelona vs. X  instead making three clusters from them. With such short strings, the pairwise Jaro-Winkler distance between two ob-jects is bigger than the threshold, thus preventing the objects from Table 4: Entity aspect mining: clustering ground truth for en-tity Paris Saint-Germain F.C. .

Cluster Context terms being clustered with complete linkage. Also, in some cases the lexical clustering method groups queries that should not be clus-tered together because they represent different intent/vertical. For instance, two separate clusters ( X  X onaco X ) and ( X  X onaco direct, monaco streaming X ) should be created instead of putting them to-gether in a single cluster.

Next, we look at the different types of aspect that occur in the context of our example entity Paris Saint-Germain F.C. . Many of the aspects refer to a fairly common transactional intent for a football club such as  X  X ive streaming X . Other sets of intents are re-lational , which concerns the relationship of the topic entity with intents are categorical , that is, they deal with type-related intents, e.g.,  X  X esults, X   X  X atch X   X  X ighlights X  and  X  X ransfers. X  The last as-pect we observe concerns attempts to find something related to a certain time point, such as  X 2013 X .

To conclude this section, we formulate our answer to our first research question RQ1. Combining lexical and semantic similar-ity measures performs best on the task of clustering query context terms for entity aspect mining and we select this method for the re-maining experiments. Integrating click similarity tend to hurt per-formance, particularly precision.
Our second experiment evaluates the importance of each aspect with respect to the entity in a query-independent setting. The re-sults of this experiment is displayed in Tables 5 and 6. From these tables we observe consistent results across the two datasets. First, the simple maximum likelihood approach already performs quite well, thus providing a good baseline. The entropy-based methods, in particular using month as time units, achieve the best perfor-mance overall, outperforming maximum likelihood and language modeling approaches. We further observe that the absolute scores on the AOL dataset are lower overall which is mainly due to the dis-jointness nature of the query logs that we use to mine the aspects with the queries in the AOL logs.

We use different granularities of time-slices, and experiment with two variant of entropy-based methods. For the baseline MLE ap-proach, we simply use the aspect popularity over the whole range of our main query logs that is used for mining (1 year of data). The different granularities do not really show much difference in terms of performance, although computing entropy on the monthly data provides a slight edge.
 Table 5: Entity aspect ranking: results on test-aspect-ranking . Significance is tested against the MLE baseline (row 1).

There are several possible reasons why language modeling does not work well for this task. First, it is the only approach that does not include any query popularity or frequency information. Secondly, what users search for does not always align with what Wikipedia editors may put in a Wikipedia article X  X hich is in line with previous research [37]. This may result in a so-called knowl-edge base gap where a user is searching for an important fact that is not included on a Wikipedia article yet. In our case, this may re-sult in low scores with the language modeling approach. Lastly, the fact that the language model based approaches are unigram-based, while the other methods are segment-based might also contribute to the lower scores.

We also considered a second experimental variant where we look for entity-bearing query pairs in the whole session. That is, we discard any non-entity bearing queries in between. We find that the scores using this variant are comparable to those reported here.
It is important to note that we compare a different and diverse set of features, with appropriate functions defined for each type of fea-tures. However, since the previous step (clustering query contexts into aspects) are kept constant across method, we argue that this comparison is still valuable despite having to compare the combi-nation of features and functions simultaneously. The end-to-end aspect ranking scores should be comparable.

In conclusion, the results show that ranking entity aspects can be done successfully, resulting in sensible absolute MRR and SR scores and we find that entropy-based methods are the best in rank-ing entity aspects in a query-independent scenario.
Our third experiment evaluates the quality of recommending en-tity aspects within a session, comparing the semantic and behav-ioral approaches. The results are shown in Tables 7 and 8. Overall, we see that the behavioral aspect-flow approach outperforms the purely semantic approach. When combined, they give the best per-formance. In our experiments, we experiment with a round-robin and a convex approach to combine the results.

Upon looking at the graphs created by both methods, we see that the semantic method tends to generate larger graphs, thus attaining more coverage. This is not surprising since the flow graph is only constructed with a single month of data. Despite the sparsity and lack of coverage, the behavioral flow-based approach still manages to outperform the semantic approach, providing better quality rec-Table 6: Entity aspect ranking: results on the AOL dataset. Significance is tested against the MLE baseline (row 1).
 Table 7: Aspect recommendation: results. Significance is tested against row 1.
 Method MRR SR Aspect-semantic 0.0431 0.0244 Aspect-flow 0.0602 N 0.0451 N Aspect-combined-rr 0.0674 N 0.0486 N
Aspect-combined-convex (  X  = 0 . 85 ) 0.0650 N 0.0465 N ommendations overall. Both combination approaches succesfully improve over the individual approaches.

As for incorporating user session context (Table 8), we observe that the semantic approach gains a small improvement by incorpo-rating previous queries in the search context. However, the flow-based approach performs slightly worse when previous query is taken into account. This is related to the sparsity/lack of transi-tion data on the flow-based approach. The size of the context have little effect in the current adjancency-based recommendation setup. In conclusion, we find that the behavioral approach is better than the semantic approach for recommending entity aspects, and they can be combined to generate better recommendations.
In this section we investigate whether our aspect model can help to complement the query-flow graph for generic query recommen-dation. Table 9 shows the result of predicting all queries within the sampled user sessions ( all-pairs ). Table 10 shows the results of us-ing the first query as input to predict only the last query of a session (which can be considered as yielding the desired results).
In the all-pairs prediction setup, we see that the aspect-based query recommendation method (labeled QFG+A in the table) suc-cessfully improves upon the baseline query-flow graph in terms of predictions coverage and ranking. The improvement is small, but consistent and significant across the two different configurations of our experiment. Overall, we achieve around 1% improvement on the correct prediction X  X  coverage. For ranking, our method achieves slightly better mean reciprocal rank and average position of the tar-get query (averaged for correct predictions at the top-100). Con-sidering the large number of queries, these improvements are quite substantial. The improvements become more pronounced as we look at the unique query occurrences rather than all occurrences.
We notice consistent results in the first-last experiment also. Im-provements in terms of prediction coverage are around 1%, while the ranking also shows consistent and significant improvements. Table 8: Aspect recommendation: results of context aware ex-periment where m denotes the context size, i.e., the number of queries used as context. Significance is tested against row 1 of each group.
 Method MRR SR Aspect-semantic 0.0431 0.0244 CA-aspect-semantic ( m = 3 ) 0.0436 N 0.0248 N CA-aspect-semantic ( m = 10 ) 0.0436 N 0.0248 N Aspect-flow 0.0602 0.0451 CA-aspect-flow ( m = 3 ) 0.0583 H 0.0438 H CA-aspect-flow ( m = 10 ) 0.0583 H 0.0438 H Table 9: Query recommendation: results on the all-pairs dataset for each configuration.
In this paper we have considered common information access tasks in the context of entity-oriented web search. In particular, we have developed and evaluated methods for mining entity as-pects, ranking their importance, and recommending them directly or leveraging them for query recommendation. We have done so through linking entities within queries, extracting the query con-text terms, and clustering them together into entity aspects if they refer to the same intent.

The first step, mining entity aspects involves extracting common queries in the context of an entity and grouping them based on their similarity. We find that combining the lexical and semantic match-ing strategies performs best for this task. In the next step we rank the obtained entity aspects for each entity in a query-independent fashion using three strategies: maximum likelihood , entropy , and language modeling . In the maximum likelihood method, we reward more frequently occurring aspects. In the entropy-based methods, we aim to reward aspects that are stable over time. With the lan-guage modeling methods, we estimate the probability that the as-pect is generated from a statistical unigram language model of the entity. We find that the entropy-based methods yield the best per-formance. The third task that we consider is aspect recommenda-tion. That is, given an entity and a certain aspect as input, recom-mend related aspects. For this we consider two approaches, seman-tic and behavioral , and find that the latter provides superior results. In our final task we leverage entity aspects for actual query recom-mendation. Here we normalize a query graph into a semantic query graph, and use the entity aspects as an additional source of infor-Table 10: Query recommendation: results on the first-last dataset for each configuration. mation for query recommendation. We find that resolving entities and grouping queries into aspects helps to improve query recom-mendation in a semantic way, by addressing the sparsity of queries and improving the diversity of recommendations.

As to future work, we would like to extend the study in the fol-lowing directions. First, we would like to experiment with more advanced graph-based compression and recommendation methods for query recommendation. Secondly, we would like to incorpo-rate more features and study the performance of aspect mining in an even larger-scale setting. Next, just as some entity relations are known to be fluent [30], we would like to study the temporality of the entity aspects. Finally, we would like to the see whether the different aspect ranking methods perform differently per entity type, or per different query triggers, distinguishing e.g., actual user inputs with auto-completions and related searches.

